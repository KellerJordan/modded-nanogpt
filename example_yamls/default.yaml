# Synthyra Training Configuration
# This YAML file defines all available training parameters
# CLI arguments will override these values where provided
# Security note: tokens (--token, --wandb_token) and log_name must be passed via CLI

# Model Configuration
save_path: "Synthyra/speedrun_yaml_test"

# Distributed Training
seed: 42
clear_cache_every: 1000
grad_clip: 0.0

# Model Architecture
hidden_size: 768
num_attention_heads: 6
num_hidden_layers: 24
num_att_tokens: 512
vocab_size: 33
expansion_ratio: 2.6667  # 8/3
soft_logit_cap: 32.0
attention_soft_cap: 64.0
add_att_soft_cap: true
p_attention: false
tie_embeddings: false
unet: true

# Data Configuration
input_bin: "data/omgprot50/omgprot50_train_*.bin"
input_valid_bin: "data/omgprot50/omgprot50_valid_*.bin"
input_test_bin: "data/omgprot50/omgprot50_test_*.bin"
mlm: false  # Masked Language Modeling
mask_rate: 0.2

# Training Hyperparameters
batch_size: 524288  # 8*64*1024 tokens
grad_accum: 1
num_steps: 50000
cooldown_steps: 5000
max_length: 1024
scheduler_type: "cosine"
lr_warmup_steps: 1000

# Optimizer Configuration
# Adam optimizer (for embeddings, head, scalars when using Muon)
lr: 0.001  # Learning rate when not using Muon
lr_embed: 0.06
lr_head: 0.008
lr_scalar: 0.04

# Muon optimizer (for hidden layers)
use_muon: true
lr_hidden: 0.05
muon_momentum_warmup_steps: 300

# Evaluation and Logging
eval_every: 1000
hf_model_name: "lhallee/speedrun"
save_every: null  # Set to a number to save checkpoints every N steps

# Dataloader Configuration
num_workers: 4
prefetch_factor: 2

# Example alternative configurations (commented out):

# Small model for debugging:
# hidden_size: 128
# num_attention_heads: 2
# num_hidden_layers: 2
# num_att_tokens: 128
# batch_size: 2048
# num_steps: 100
# max_length: 512

# Large model configuration:
# hidden_size: 1024
# num_attention_heads: 8
# num_hidden_layers: 32
# num_att_tokens: 1024
# batch_size: 1048576  # 16*64*1024
# grad_accum: 2
# max_length: 2048

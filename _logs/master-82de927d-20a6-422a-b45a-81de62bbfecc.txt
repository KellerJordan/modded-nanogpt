import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import contextlib
from dataclasses import dataclass
from pathlib import Path

import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.nn.attention.flex_attention import BlockMask, flex_attention #KoszarskyB

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params = list(params)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [
            {
                'params': [p for p in params if p.numel() == size],
                'update_buffer': [
                    torch.empty(size, device='cuda', dtype=torch.bfloat16)
                    for _ in range(self.world_size)
                ],
            }
            for size in sizes
        ]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            assert len(params) % self.world_size == 0
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                p = params[base_i + self.rank]
                g = p.grad
                assert g is not None
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.lerp_(g, 1 - momentum)
                g = g.lerp_(buf, momentum) if nesterov else buf
                g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                update_prev()
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.register_buffer('inv_freq', (1 / base) ** (torch.arange(0, dim, 2) / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            t = torch.arange(seq_len, device=x.device)
            freqs = torch.outer(t, self.inv_freq)
            self.seq_len_cached = seq_len
            self.cos_cached = freqs.cos()
            self.sin_cached = freqs.sin()
        cos, sin = self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
        # apply_rotary_emb(x, cos, sin)
        x1, x2 = x.chunk(2, dim=3)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x, vi, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        v = self.lambdas[0] * v + self.lambdas[1] * vi.view_as(v) # @KoszarskyB & @Grad62304977
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, enable_gqa=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc   = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config.model_dim, config.num_heads)
        self.mlp = MLP(config.model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, vi, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x = x + self.attn(norm(x), vi, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, config: "GPTConfig"):
        super().__init__()
        self.embed = nn.ModuleList([
            nn.Embedding(config.vocab_size, config.model_dim)
            for _ in range(6)
        ])

    def forward(self, inputs) -> "list[torch.Tensor]":
        ve = [emb(inputs) for emb in self.embed]
        ve += reversed(ve)
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    num_layers : int = 12
    num_heads : int = 6 # head dim 128 suggested by @Grad62304977
    model_dim : int = 768

class GPT(nn.Module):

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.num_layers = config.num_layers

        # U-net design by @brendanh0gan
        self.num_encoder_layers = config.num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = config.num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

        self.embed = nn.Embedding(config.vocab_size, config.model_dim)
        self.blocks = nn.ModuleList([Block(config) for _ in range(config.num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(config)
        self.lm_head = CastedLinear(config.model_dim, config.vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(
        self,
        inputs: torch.Tensor,
        targets: torch.Tensor,
        sliding_window_num_blocks: torch.Tensor,
    ):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: torch.Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks: torch.Tensor):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm ^ full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        # forward the GPT model itself
        x = self.embed(inputs[None]) # token embeddings of shape (b, t, model_dim)
        x = norm(x) # @Grad62304977
        x0 = x
        ve = self.value_embeds(inputs)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(file: Path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32)
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    return int(header[2]) # number of tokens (claimed)

def _load_data_shard(path: Path, num_tokens):
    with path.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, seq_len, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.seq_len = seq_len

        # glob files that match the pattern
        self.files = sorted(Path.cwd().glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        self.files_num_tokens = [_peek_data_shard(file) for file in self.files]
        assert min(self.files_num_tokens) >= num_processes * seq_len + 1
        self.total_num_tokens = sum(self.files_num_tokens)

        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.seq_len
        self.tokens = _load_data_shard(self.files[self.current_shard], self.files_num_tokens[self.current_shard])

    def next_batch(self):
        batch_size = self.seq_len * self.num_processes
        buf = self.tokens[self.current_position:self.current_position+self.seq_len+1]
        # host side async is sufficient;
        # no performance improvement was observed when introducing a separate stream.
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # inputs
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # targets
        # advance current position and load next shard if necessary
        self.current_position += batch_size
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8 # batch size, in sequences, across all devices
    sequence_length : int = 64*1024 # sequence length, in tokens
    num_iterations : int = 1480 # number of iterations to run
    warmup_iters : int = 0
    cooldown_iters : int = 600 # number of iterations of linear warmup/cooldown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
device = torch.device(f'cuda:{ddp_local_rank}')
torch.cuda.set_device(device)
print(f'using device: {device}')
dist.init_process_group(backend='nccl', device_id=device)
dist.barrier()
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    Path('logs').mkdir(exist_ok=True)
    # logdir = Path('logs') / f'{run_id}'
    # logdir.mkdir()
    logfile = Path('logs') / f'{run_id}.txt'
    print(logfile.stem)
    # create the log file
    with logfile.open('w') as f:
        # begin the log by printing this file (the Python code)
        print(code, file=f)
        print('=' * 100, file=f)
def print0(s, logonly=False):
    if master_process:
        with logfile.open('a') as f:
            if not logonly:
                print(s)
            print(s, file=f)
# log information about the hardware/software environment this is running on
# and print the full `nvidia-smi` to file
print0(f'Running python {sys.version}')
print0(f'Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:')
import subprocess
result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
print0(f'{result.stdout}', logonly=True)
print0('='*100, logonly=True)

# calculate the number of steps to take in the val loop.
assert args.val_tokens % (args.sequence_length * ddp_world_size) == 0
val_steps = args.val_tokens // (args.sequence_length * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (ddp_world_size) == 0
train_accumulation_steps = args.batch_size // ddp_world_size

# load tokens
train_loader = DistributedDataLoader(args.input_bin, args.sequence_length, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, args.sequence_length, ddp_rank, ddp_world_size)
print0(f"Training DataLoader: total number of tokens: {train_loader.total_num_tokens} across {len(train_loader.files)} files")
print0(f"Validation DataLoader: total number of tokens: {val_loader.total_num_tokens} across {len(val_loader.files)} files")
print0('='*100, logonly=True)
inputs_train, targets_train = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, num_layers=12, num_heads=6, model_dim=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()
config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)
raw_model = model.module # always contains the "raw" unwrapped model

# init the optimizer(s)
embed_params = [*raw_model.embed.parameters(), *raw_model.value_embeds.parameters()]
optimizer1 = torch.optim.Adam(embed_params, lr=0.6, betas=(0.8, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight], lr=0.008, betas=(0.8, 0.95), fused=True)
params = list(raw_model.blocks.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
optimizer3 = Muon(matrix_params, lr=0.05, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.8, 0.95), fused=True)
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and cooldown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.cooldown_iters:
        return 1.0
    # 3) linear cooldown
    else:
        decay_ratio = (args.num_iterations - it) / args.cooldown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device="cuda")
sw_num_blocks_prev = 1
# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the sliding window size over training in chunks of 128 from 128 -> 1856. By @fernbear.bsky.social
    frac_done = step / args.num_iterations # training progress
    sw_num_blocks = int(((1 - frac_done) * 128 + frac_done * 1856) // 128)
    if sw_num_blocks != sw_num_blocks_prev:
        sliding_window_num_blocks.copy_(sw_num_blocks, non_blocking=True)
        sw_num_blocks_prev = sw_num_blocks

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch()
                val_loss += model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        print0(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    # uncomment if you want to save any checkpoints
    #save_every = 1000
    #if master_process and (last_step or (save_every > 0 and step % save_every == 0)):
    #    # stop the clock
    #    torch.cuda.synchronize()
    #    training_time_ms += 1000 * (time.perf_counter() - t0)
    #    # save the state of the training process
    #    log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
    #    torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
    #    # start the clock again
    #    torch.cuda.synchronize()
    #    t0 = time.perf_counter()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps + 1):
        with contextlib.ExitStack() as stack:
            if i < train_accumulation_steps: # there's no need to sync gradients every accumulation step
                stack.enter_context(model.no_sync())
            if step >= 5:
                stack.enter_context(torch.compiler.set_stance(skip_guard_eval_unsafe=True))
            model(inputs_train, targets_train, sliding_window_num_blocks).backward()
            inputs_train, targets_train = train_loader.next_batch()
    if train_accumulation_steps != 1:
        for p in model.parameters():
            p.grad /= train_accumulation_steps
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer3.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{args.num_iterations} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")

print0(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()

====================================================================================================
Running python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running pytorch 2.6.0.dev20241203+cu124 compiled for CUDA 12.4
nvidia-smi:
Thu Dec 26 07:38:48 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 PCIe               On  | 00000000:00:07.0 Off |                    0 |
| N/A   35C    P0              79W / 350W |   4162MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  | 00000000:00:08.0 Off |                    0 |
| N/A   40C    P0              84W / 350W |    923MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  | 00000000:00:09.0 Off |                    0 |
| N/A   33C    P0              78W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  | 00000000:00:0A.0 Off |                    0 |
| N/A   34C    P0              77W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  | 00000000:00:0B.0 Off |                    0 |
| N/A   33C    P0              75W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  | 00000000:00:0C.0 Off |                    0 |
| N/A   32C    P0              76W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  | 00000000:00:0D.0 Off |                    0 |
| N/A   40C    P0              81W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  | 00000000:00:0E.0 Off |                    0 |
| N/A   34C    P0              76W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training DataLoader: total number of tokens: 1000000000 across 10 files
Validation DataLoader: total number of tokens: 100000000 across 1 files
====================================================================================================
step:0/1480 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1480 train_time:454531ms step_avg:nanms
step:2/1480 train_time:458892ms step_avg:nanms
step:3/1480 train_time:459134ms step_avg:nanms
step:4/1480 train_time:459381ms step_avg:nanms
step:5/1480 train_time:460440ms step_avg:nanms
step:6/1480 train_time:460691ms step_avg:nanms
step:7/1480 train_time:460938ms step_avg:nanms
step:8/1480 train_time:461190ms step_avg:nanms
step:9/1480 train_time:461435ms step_avg:nanms
step:10/1480 train_time:461686ms step_avg:nanms
step:11/1480 train_time:248ms step_avg:nanms
step:12/1480 train_time:499ms step_avg:nanms
step:13/1480 train_time:749ms step_avg:249.78ms
step:14/1480 train_time:1000ms step_avg:250.05ms
step:15/1480 train_time:1249ms step_avg:249.71ms
step:16/1480 train_time:1499ms step_avg:249.80ms
step:17/1480 train_time:1749ms step_avg:249.81ms
step:18/1480 train_time:2000ms step_avg:249.94ms
step:19/1480 train_time:2250ms step_avg:249.95ms
step:20/1480 train_time:2502ms step_avg:250.20ms
step:21/1480 train_time:2748ms step_avg:249.85ms
step:22/1480 train_time:2999ms step_avg:249.94ms
step:23/1480 train_time:3249ms step_avg:249.89ms
step:24/1480 train_time:3497ms step_avg:249.78ms
step:25/1480 train_time:3751ms step_avg:250.04ms
step:26/1480 train_time:3999ms step_avg:249.92ms
step:27/1480 train_time:4248ms step_avg:249.90ms
step:28/1480 train_time:4502ms step_avg:250.09ms
step:29/1480 train_time:4750ms step_avg:249.98ms
step:30/1480 train_time:4999ms step_avg:249.96ms
step:31/1480 train_time:5248ms step_avg:249.92ms
step:32/1480 train_time:5501ms step_avg:250.03ms
step:33/1480 train_time:5749ms step_avg:249.96ms
step:34/1480 train_time:5998ms step_avg:249.90ms
step:35/1480 train_time:6249ms step_avg:249.96ms
step:36/1480 train_time:6503ms step_avg:250.10ms
step:37/1480 train_time:6749ms step_avg:249.96ms
step:38/1480 train_time:6999ms step_avg:249.97ms
step:39/1480 train_time:7250ms step_avg:250.00ms
step:40/1480 train_time:7502ms step_avg:250.08ms
step:41/1480 train_time:7749ms step_avg:249.95ms
step:42/1480 train_time:7999ms step_avg:249.96ms
step:43/1480 train_time:8249ms step_avg:249.98ms
step:44/1480 train_time:8500ms step_avg:249.99ms
step:45/1480 train_time:8750ms step_avg:250.01ms
step:46/1480 train_time:9001ms step_avg:250.04ms
step:47/1480 train_time:9250ms step_avg:250.00ms
step:48/1480 train_time:9501ms step_avg:250.04ms
step:49/1480 train_time:9749ms step_avg:249.97ms
step:50/1480 train_time:9999ms step_avg:249.97ms
step:51/1480 train_time:10249ms step_avg:249.99ms
step:52/1480 train_time:10501ms step_avg:250.02ms
step:53/1480 train_time:10748ms step_avg:249.97ms
step:54/1480 train_time:10998ms step_avg:249.95ms
step:55/1480 train_time:11250ms step_avg:250.00ms
step:56/1480 train_time:11501ms step_avg:250.03ms
step:57/1480 train_time:11749ms step_avg:249.98ms
step:58/1480 train_time:12000ms step_avg:250.00ms
step:59/1480 train_time:12250ms step_avg:250.00ms
step:60/1480 train_time:12502ms step_avg:250.04ms
step:61/1480 train_time:12750ms step_avg:249.99ms
step:62/1480 train_time:13000ms step_avg:250.01ms
step:63/1480 train_time:13250ms step_avg:250.01ms
step:64/1480 train_time:13503ms step_avg:250.05ms
step:65/1480 train_time:13750ms step_avg:250.01ms
step:66/1480 train_time:14002ms step_avg:250.03ms
step:67/1480 train_time:14249ms step_avg:249.99ms
step:68/1480 train_time:14501ms step_avg:250.02ms
step:69/1480 train_time:14752ms step_avg:250.03ms
step:70/1480 train_time:15004ms step_avg:250.06ms
step:71/1480 train_time:15252ms step_avg:250.03ms
step:72/1480 train_time:15503ms step_avg:250.04ms
step:73/1480 train_time:15750ms step_avg:250.00ms
step:74/1480 train_time:16000ms step_avg:249.99ms
step:75/1480 train_time:16252ms step_avg:250.03ms
step:76/1480 train_time:16503ms step_avg:250.04ms
step:77/1480 train_time:16750ms step_avg:250.00ms
step:78/1480 train_time:17001ms step_avg:250.01ms
step:79/1480 train_time:17250ms step_avg:250.00ms
step:80/1480 train_time:17500ms step_avg:250.00ms
step:81/1480 train_time:17750ms step_avg:250.01ms
step:82/1480 train_time:18001ms step_avg:250.01ms
step:83/1480 train_time:18250ms step_avg:250.00ms
step:84/1480 train_time:18499ms step_avg:249.99ms
step:85/1480 train_time:18751ms step_avg:250.01ms
step:86/1480 train_time:19004ms step_avg:250.05ms
step:87/1480 train_time:19250ms step_avg:250.00ms
step:88/1480 train_time:19502ms step_avg:250.03ms
step:89/1480 train_time:19750ms step_avg:250.00ms
step:90/1480 train_time:20001ms step_avg:250.01ms
step:91/1480 train_time:20252ms step_avg:250.02ms
step:92/1480 train_time:20502ms step_avg:250.03ms
step:93/1480 train_time:20749ms step_avg:249.98ms
step:94/1480 train_time:20999ms step_avg:249.99ms
step:95/1480 train_time:21250ms step_avg:249.99ms
step:96/1480 train_time:21502ms step_avg:250.03ms
step:97/1480 train_time:21750ms step_avg:250.00ms
step:98/1480 train_time:22000ms step_avg:250.00ms
step:99/1480 train_time:22250ms step_avg:250.00ms
step:100/1480 train_time:22502ms step_avg:250.03ms
step:101/1480 train_time:22750ms step_avg:250.00ms
step:102/1480 train_time:23000ms step_avg:250.00ms
step:103/1480 train_time:23249ms step_avg:249.99ms
step:104/1480 train_time:23503ms step_avg:250.03ms
step:105/1480 train_time:23750ms step_avg:250.00ms
step:106/1480 train_time:24000ms step_avg:250.00ms
step:107/1480 train_time:24251ms step_avg:250.01ms
step:108/1480 train_time:24502ms step_avg:250.02ms
step:109/1480 train_time:24749ms step_avg:249.99ms
step:110/1480 train_time:25001ms step_avg:250.01ms
step:111/1480 train_time:25252ms step_avg:250.02ms
step:112/1480 train_time:25507ms step_avg:250.07ms
step:113/1480 train_time:25760ms step_avg:250.10ms
step:114/1480 train_time:26018ms step_avg:250.17ms
step:115/1480 train_time:26270ms step_avg:250.19ms
step:116/1480 train_time:26526ms step_avg:250.24ms
step:117/1480 train_time:26783ms step_avg:250.30ms
step:118/1480 train_time:27038ms step_avg:250.35ms
step:119/1480 train_time:27295ms step_avg:250.41ms
step:120/1480 train_time:27549ms step_avg:250.45ms
step:121/1480 train_time:27805ms step_avg:250.50ms
step:122/1480 train_time:28059ms step_avg:250.52ms
step:123/1480 train_time:28315ms step_avg:250.57ms
step:124/1480 train_time:28567ms step_avg:250.59ms
step:125/1480 train_time:28822ms step_avg:250.63ms
step:125/1480 val_loss:4.4223 train_time:28947ms step_avg:251.71ms
step:126/1480 train_time:29076ms step_avg:250.65ms
step:127/1480 train_time:29335ms step_avg:250.73ms
step:128/1480 train_time:29588ms step_avg:250.75ms
step:129/1480 train_time:29843ms step_avg:250.78ms
step:130/1480 train_time:30094ms step_avg:250.79ms
step:131/1480 train_time:30348ms step_avg:250.81ms
step:132/1480 train_time:30603ms step_avg:250.85ms
step:133/1480 train_time:30862ms step_avg:250.91ms
step:134/1480 train_time:31115ms step_avg:250.92ms
step:135/1480 train_time:31370ms step_avg:250.96ms
step:136/1480 train_time:31626ms step_avg:251.00ms
step:137/1480 train_time:31879ms step_avg:251.02ms
step:138/1480 train_time:32135ms step_avg:251.05ms
step:139/1480 train_time:32387ms step_avg:251.06ms
step:140/1480 train_time:32642ms step_avg:251.09ms
step:141/1480 train_time:32894ms step_avg:251.10ms
step:142/1480 train_time:33148ms step_avg:251.12ms
step:143/1480 train_time:33404ms step_avg:251.16ms
step:144/1480 train_time:33660ms step_avg:251.19ms
step:145/1480 train_time:33912ms step_avg:251.20ms
step:146/1480 train_time:34168ms step_avg:251.23ms
step:147/1480 train_time:34423ms step_avg:251.26ms
step:148/1480 train_time:34675ms step_avg:251.27ms
step:149/1480 train_time:34931ms step_avg:251.30ms
step:150/1480 train_time:35186ms step_avg:251.33ms
step:151/1480 train_time:35442ms step_avg:251.36ms
step:152/1480 train_time:35698ms step_avg:251.39ms
step:153/1480 train_time:35949ms step_avg:251.39ms
step:154/1480 train_time:36206ms step_avg:251.43ms
step:155/1480 train_time:36461ms step_avg:251.45ms
step:156/1480 train_time:36713ms step_avg:251.46ms
step:157/1480 train_time:36968ms step_avg:251.48ms
step:158/1480 train_time:37225ms step_avg:251.52ms
step:159/1480 train_time:37478ms step_avg:251.53ms
step:160/1480 train_time:37733ms step_avg:251.56ms
step:161/1480 train_time:37988ms step_avg:251.57ms
step:162/1480 train_time:38243ms step_avg:251.60ms
step:163/1480 train_time:38496ms step_avg:251.61ms
step:164/1480 train_time:38748ms step_avg:251.61ms
step:165/1480 train_time:39006ms step_avg:251.65ms
step:166/1480 train_time:39262ms step_avg:251.68ms
step:167/1480 train_time:39514ms step_avg:251.68ms
step:168/1480 train_time:39768ms step_avg:251.70ms
step:169/1480 train_time:40026ms step_avg:251.74ms
step:170/1480 train_time:40278ms step_avg:251.73ms
step:171/1480 train_time:40534ms step_avg:251.77ms
step:172/1480 train_time:40788ms step_avg:251.77ms
step:173/1480 train_time:41043ms step_avg:251.80ms
step:174/1480 train_time:41296ms step_avg:251.81ms
step:175/1480 train_time:41549ms step_avg:251.81ms
step:176/1480 train_time:41805ms step_avg:251.84ms
step:177/1480 train_time:42063ms step_avg:251.88ms
step:178/1480 train_time:42315ms step_avg:251.88ms
step:179/1480 train_time:42569ms step_avg:251.89ms
step:180/1480 train_time:42824ms step_avg:251.91ms
step:181/1480 train_time:43077ms step_avg:251.91ms
step:182/1480 train_time:43333ms step_avg:251.93ms
step:183/1480 train_time:43586ms step_avg:251.94ms
step:184/1480 train_time:43843ms step_avg:251.97ms
step:185/1480 train_time:44099ms step_avg:251.99ms
step:186/1480 train_time:44353ms step_avg:252.01ms
step:187/1480 train_time:44608ms step_avg:252.02ms
step:188/1480 train_time:44865ms step_avg:252.05ms
step:189/1480 train_time:45118ms step_avg:252.05ms
step:190/1480 train_time:45373ms step_avg:252.07ms
step:191/1480 train_time:45629ms step_avg:252.09ms
step:192/1480 train_time:45884ms step_avg:252.11ms
step:193/1480 train_time:46142ms step_avg:252.14ms
step:194/1480 train_time:46396ms step_avg:252.15ms
step:195/1480 train_time:46648ms step_avg:252.15ms
step:196/1480 train_time:46905ms step_avg:252.18ms
step:197/1480 train_time:47161ms step_avg:252.20ms
step:198/1480 train_time:47415ms step_avg:252.21ms
step:199/1480 train_time:47671ms step_avg:252.23ms
step:200/1480 train_time:47926ms step_avg:252.24ms
step:201/1480 train_time:48179ms step_avg:252.25ms
step:202/1480 train_time:48436ms step_avg:252.27ms
step:203/1480 train_time:48687ms step_avg:252.27ms
step:204/1480 train_time:48944ms step_avg:252.29ms
step:205/1480 train_time:49202ms step_avg:252.32ms
step:206/1480 train_time:49457ms step_avg:252.33ms
step:207/1480 train_time:49710ms step_avg:252.34ms
step:208/1480 train_time:49966ms step_avg:252.36ms
step:209/1480 train_time:50222ms step_avg:252.37ms
step:210/1480 train_time:50478ms step_avg:252.39ms
step:211/1480 train_time:50733ms step_avg:252.41ms
step:212/1480 train_time:50987ms step_avg:252.41ms
step:213/1480 train_time:51242ms step_avg:252.42ms
step:214/1480 train_time:51496ms step_avg:252.43ms
step:215/1480 train_time:51750ms step_avg:252.44ms
step:216/1480 train_time:52006ms step_avg:252.46ms
step:217/1480 train_time:52263ms step_avg:252.48ms
step:218/1480 train_time:52517ms step_avg:252.49ms
step:219/1480 train_time:52774ms step_avg:252.51ms
step:220/1480 train_time:53030ms step_avg:252.52ms
step:221/1480 train_time:53288ms step_avg:252.55ms
step:222/1480 train_time:53548ms step_avg:252.58ms
step:223/1480 train_time:53807ms step_avg:252.62ms
step:224/1480 train_time:54069ms step_avg:252.66ms
step:225/1480 train_time:54328ms step_avg:252.69ms
step:226/1480 train_time:54585ms step_avg:252.71ms
step:227/1480 train_time:54846ms step_avg:252.75ms
step:228/1480 train_time:55107ms step_avg:252.79ms
step:229/1480 train_time:55369ms step_avg:252.83ms
step:230/1480 train_time:55629ms step_avg:252.86ms
step:231/1480 train_time:55885ms step_avg:252.88ms
step:232/1480 train_time:56147ms step_avg:252.91ms
step:233/1480 train_time:56407ms step_avg:252.95ms
step:234/1480 train_time:56670ms step_avg:252.99ms
step:235/1480 train_time:56930ms step_avg:253.02ms
step:236/1480 train_time:57188ms step_avg:253.04ms
step:237/1480 train_time:57447ms step_avg:253.07ms
step:238/1480 train_time:57707ms step_avg:253.10ms
step:239/1480 train_time:57969ms step_avg:253.14ms
step:240/1480 train_time:58231ms step_avg:253.18ms
step:241/1480 train_time:58488ms step_avg:253.20ms
step:242/1480 train_time:58748ms step_avg:253.23ms
step:243/1480 train_time:59008ms step_avg:253.26ms
step:244/1480 train_time:59269ms step_avg:253.29ms
step:245/1480 train_time:59528ms step_avg:253.31ms
step:246/1480 train_time:59788ms step_avg:253.34ms
step:247/1480 train_time:60049ms step_avg:253.37ms
step:248/1480 train_time:60307ms step_avg:253.39ms
step:249/1480 train_time:60569ms step_avg:253.43ms
step:250/1480 train_time:60828ms step_avg:253.45ms
step:250/1480 val_loss:3.9975 train_time:60957ms step_avg:253.99ms
step:251/1480 train_time:61088ms step_avg:253.48ms
step:252/1480 train_time:61351ms step_avg:253.52ms
step:253/1480 train_time:61613ms step_avg:253.55ms
step:254/1480 train_time:61874ms step_avg:253.58ms
step:255/1480 train_time:62132ms step_avg:253.60ms
step:256/1480 train_time:62391ms step_avg:253.62ms
step:257/1480 train_time:62652ms step_avg:253.65ms
step:258/1480 train_time:62913ms step_avg:253.68ms
step:259/1480 train_time:63173ms step_avg:253.71ms
step:260/1480 train_time:63433ms step_avg:253.73ms
step:261/1480 train_time:63691ms step_avg:253.75ms
step:262/1480 train_time:63952ms step_avg:253.78ms
step:263/1480 train_time:64211ms step_avg:253.80ms
step:264/1480 train_time:64472ms step_avg:253.83ms
step:265/1480 train_time:64732ms step_avg:253.85ms
step:266/1480 train_time:64991ms step_avg:253.87ms
step:267/1480 train_time:65251ms step_avg:253.89ms
step:268/1480 train_time:65512ms step_avg:253.92ms
step:269/1480 train_time:65773ms step_avg:253.95ms
step:270/1480 train_time:66035ms step_avg:253.98ms
step:271/1480 train_time:66294ms step_avg:254.00ms
step:272/1480 train_time:66553ms step_avg:254.02ms
step:273/1480 train_time:66813ms step_avg:254.04ms
step:274/1480 train_time:67074ms step_avg:254.07ms
step:275/1480 train_time:67334ms step_avg:254.09ms
step:276/1480 train_time:67592ms step_avg:254.11ms
step:277/1480 train_time:67852ms step_avg:254.13ms
step:278/1480 train_time:68112ms step_avg:254.15ms
step:279/1480 train_time:68374ms step_avg:254.18ms
step:280/1480 train_time:68634ms step_avg:254.20ms
step:281/1480 train_time:68891ms step_avg:254.21ms
step:282/1480 train_time:69152ms step_avg:254.24ms
step:283/1480 train_time:69412ms step_avg:254.26ms
step:284/1480 train_time:69674ms step_avg:254.28ms
step:285/1480 train_time:69933ms step_avg:254.30ms
step:286/1480 train_time:70191ms step_avg:254.32ms
step:287/1480 train_time:70452ms step_avg:254.34ms
step:288/1480 train_time:70712ms step_avg:254.36ms
step:289/1480 train_time:70972ms step_avg:254.38ms
step:290/1480 train_time:71233ms step_avg:254.40ms
step:291/1480 train_time:71491ms step_avg:254.42ms
step:292/1480 train_time:71751ms step_avg:254.44ms
step:293/1480 train_time:72012ms step_avg:254.46ms
step:294/1480 train_time:72274ms step_avg:254.49ms
step:295/1480 train_time:72534ms step_avg:254.50ms
step:296/1480 train_time:72791ms step_avg:254.51ms
step:297/1480 train_time:73051ms step_avg:254.53ms
step:298/1480 train_time:73312ms step_avg:254.56ms
step:299/1480 train_time:73574ms step_avg:254.58ms
step:300/1480 train_time:73834ms step_avg:254.60ms
step:301/1480 train_time:74093ms step_avg:254.62ms
step:302/1480 train_time:74353ms step_avg:254.63ms
step:303/1480 train_time:74614ms step_avg:254.65ms
step:304/1480 train_time:74874ms step_avg:254.67ms
step:305/1480 train_time:75133ms step_avg:254.69ms
step:306/1480 train_time:75392ms step_avg:254.70ms
step:307/1480 train_time:75652ms step_avg:254.72ms
step:308/1480 train_time:75912ms step_avg:254.74ms
step:309/1480 train_time:76174ms step_avg:254.76ms
step:310/1480 train_time:76433ms step_avg:254.78ms
step:311/1480 train_time:76691ms step_avg:254.79ms
step:312/1480 train_time:76951ms step_avg:254.80ms
step:313/1480 train_time:77212ms step_avg:254.83ms
step:314/1480 train_time:77474ms step_avg:254.85ms
step:315/1480 train_time:77734ms step_avg:254.87ms
step:316/1480 train_time:77993ms step_avg:254.88ms
step:317/1480 train_time:78252ms step_avg:254.89ms
step:318/1480 train_time:78513ms step_avg:254.91ms
step:319/1480 train_time:78774ms step_avg:254.93ms
step:320/1480 train_time:79034ms step_avg:254.95ms
step:321/1480 train_time:79292ms step_avg:254.96ms
step:322/1480 train_time:79552ms step_avg:254.97ms
step:323/1480 train_time:79812ms step_avg:254.99ms
step:324/1480 train_time:80073ms step_avg:255.01ms
step:325/1480 train_time:80333ms step_avg:255.03ms
step:326/1480 train_time:80591ms step_avg:255.04ms
step:327/1480 train_time:80853ms step_avg:255.06ms
step:328/1480 train_time:81113ms step_avg:255.07ms
step:329/1480 train_time:81375ms step_avg:255.09ms
step:330/1480 train_time:81636ms step_avg:255.11ms
step:331/1480 train_time:81898ms step_avg:255.14ms
step:332/1480 train_time:82162ms step_avg:255.16ms
step:333/1480 train_time:82425ms step_avg:255.18ms
step:334/1480 train_time:82693ms step_avg:255.23ms
step:335/1480 train_time:82955ms step_avg:255.25ms
step:336/1480 train_time:83216ms step_avg:255.27ms
step:337/1480 train_time:83479ms step_avg:255.29ms
step:338/1480 train_time:83740ms step_avg:255.31ms
step:339/1480 train_time:84003ms step_avg:255.33ms
step:340/1480 train_time:84265ms step_avg:255.35ms
step:341/1480 train_time:84534ms step_avg:255.39ms
step:342/1480 train_time:84794ms step_avg:255.40ms
step:343/1480 train_time:85056ms step_avg:255.42ms
step:344/1480 train_time:85317ms step_avg:255.44ms
step:345/1480 train_time:85580ms step_avg:255.46ms
step:346/1480 train_time:85840ms step_avg:255.48ms
step:347/1480 train_time:86100ms step_avg:255.49ms
step:348/1480 train_time:86361ms step_avg:255.51ms
step:349/1480 train_time:86625ms step_avg:255.53ms
step:350/1480 train_time:86893ms step_avg:255.57ms
step:351/1480 train_time:87155ms step_avg:255.59ms
step:352/1480 train_time:87415ms step_avg:255.60ms
step:353/1480 train_time:87678ms step_avg:255.62ms
step:354/1480 train_time:87940ms step_avg:255.64ms
step:355/1480 train_time:88202ms step_avg:255.66ms
step:356/1480 train_time:88464ms step_avg:255.68ms
step:357/1480 train_time:88731ms step_avg:255.71ms
step:358/1480 train_time:88993ms step_avg:255.73ms
step:359/1480 train_time:89255ms step_avg:255.75ms
step:360/1480 train_time:89516ms step_avg:255.76ms
step:361/1480 train_time:89778ms step_avg:255.78ms
step:362/1480 train_time:90039ms step_avg:255.79ms
step:363/1480 train_time:90300ms step_avg:255.81ms
step:364/1480 train_time:90562ms step_avg:255.83ms
step:365/1480 train_time:90828ms step_avg:255.85ms
step:366/1480 train_time:91093ms step_avg:255.88ms
step:367/1480 train_time:91355ms step_avg:255.90ms
step:368/1480 train_time:91616ms step_avg:255.91ms
step:369/1480 train_time:91878ms step_avg:255.93ms
step:370/1480 train_time:92139ms step_avg:255.94ms
step:371/1480 train_time:92399ms step_avg:255.95ms
step:372/1480 train_time:92661ms step_avg:255.97ms
step:373/1480 train_time:92923ms step_avg:255.99ms
step:374/1480 train_time:93192ms step_avg:256.02ms
step:375/1480 train_time:93455ms step_avg:256.04ms
step:375/1480 val_loss:3.8093 train_time:93586ms step_avg:256.40ms
step:376/1480 train_time:93718ms step_avg:256.06ms
step:377/1480 train_time:93988ms step_avg:256.10ms
step:378/1480 train_time:94251ms step_avg:256.12ms
step:379/1480 train_time:94514ms step_avg:256.14ms
step:380/1480 train_time:94776ms step_avg:256.15ms
step:381/1480 train_time:95038ms step_avg:256.17ms
step:382/1480 train_time:95305ms step_avg:256.20ms
step:383/1480 train_time:95568ms step_avg:256.21ms
step:384/1480 train_time:95830ms step_avg:256.23ms
step:385/1480 train_time:96091ms step_avg:256.24ms
step:386/1480 train_time:96355ms step_avg:256.26ms
step:387/1480 train_time:96618ms step_avg:256.28ms
step:388/1480 train_time:96879ms step_avg:256.29ms
step:389/1480 train_time:97148ms step_avg:256.33ms
step:390/1480 train_time:97410ms step_avg:256.34ms
step:391/1480 train_time:97674ms step_avg:256.36ms
step:392/1480 train_time:97935ms step_avg:256.37ms
step:393/1480 train_time:98197ms step_avg:256.39ms
step:394/1480 train_time:98463ms step_avg:256.41ms
step:395/1480 train_time:98729ms step_avg:256.44ms
step:396/1480 train_time:98991ms step_avg:256.45ms
step:397/1480 train_time:99252ms step_avg:256.46ms
step:398/1480 train_time:99514ms step_avg:256.48ms
step:399/1480 train_time:99775ms step_avg:256.49ms
step:400/1480 train_time:100038ms step_avg:256.51ms
step:401/1480 train_time:100304ms step_avg:256.53ms
step:402/1480 train_time:100570ms step_avg:256.56ms
step:403/1480 train_time:100830ms step_avg:256.56ms
step:404/1480 train_time:101091ms step_avg:256.58ms
step:405/1480 train_time:101355ms step_avg:256.59ms
step:406/1480 train_time:101618ms step_avg:256.61ms
step:407/1480 train_time:101880ms step_avg:256.62ms
step:408/1480 train_time:102149ms step_avg:256.65ms
step:409/1480 train_time:102411ms step_avg:256.67ms
step:410/1480 train_time:102673ms step_avg:256.68ms
step:411/1480 train_time:102936ms step_avg:256.70ms
step:412/1480 train_time:103199ms step_avg:256.71ms
step:413/1480 train_time:103468ms step_avg:256.74ms
step:414/1480 train_time:103730ms step_avg:256.76ms
step:415/1480 train_time:103992ms step_avg:256.77ms
step:416/1480 train_time:104253ms step_avg:256.78ms
step:417/1480 train_time:104517ms step_avg:256.80ms
step:418/1480 train_time:104777ms step_avg:256.81ms
step:419/1480 train_time:105038ms step_avg:256.82ms
step:420/1480 train_time:105306ms step_avg:256.84ms
step:421/1480 train_time:105569ms step_avg:256.86ms
step:422/1480 train_time:105830ms step_avg:256.87ms
step:423/1480 train_time:106092ms step_avg:256.88ms
step:424/1480 train_time:106354ms step_avg:256.89ms
step:425/1480 train_time:106615ms step_avg:256.90ms
step:426/1480 train_time:106878ms step_avg:256.92ms
step:427/1480 train_time:107143ms step_avg:256.94ms
step:428/1480 train_time:107411ms step_avg:256.96ms
step:429/1480 train_time:107672ms step_avg:256.97ms
step:430/1480 train_time:107935ms step_avg:256.99ms
step:431/1480 train_time:108197ms step_avg:257.00ms
step:432/1480 train_time:108458ms step_avg:257.01ms
step:433/1480 train_time:108725ms step_avg:257.03ms
step:434/1480 train_time:108989ms step_avg:257.05ms
step:435/1480 train_time:109252ms step_avg:257.06ms
step:436/1480 train_time:109513ms step_avg:257.07ms
step:437/1480 train_time:109775ms step_avg:257.08ms
step:438/1480 train_time:110035ms step_avg:257.09ms
step:439/1480 train_time:110297ms step_avg:257.10ms
step:440/1480 train_time:110564ms step_avg:257.13ms
step:441/1480 train_time:110832ms step_avg:257.15ms
step:442/1480 train_time:111096ms step_avg:257.17ms
step:443/1480 train_time:111359ms step_avg:257.18ms
step:444/1480 train_time:111631ms step_avg:257.21ms
step:445/1480 train_time:111893ms step_avg:257.22ms
step:446/1480 train_time:112160ms step_avg:257.25ms
step:447/1480 train_time:112431ms step_avg:257.28ms
step:448/1480 train_time:112694ms step_avg:257.29ms
step:449/1480 train_time:112961ms step_avg:257.31ms
step:450/1480 train_time:113230ms step_avg:257.34ms
step:451/1480 train_time:113496ms step_avg:257.36ms
step:452/1480 train_time:113760ms step_avg:257.37ms
step:453/1480 train_time:114029ms step_avg:257.40ms
step:454/1480 train_time:114292ms step_avg:257.41ms
step:455/1480 train_time:114559ms step_avg:257.44ms
step:456/1480 train_time:114827ms step_avg:257.46ms
step:457/1480 train_time:115091ms step_avg:257.47ms
step:458/1480 train_time:115355ms step_avg:257.49ms
step:459/1480 train_time:115622ms step_avg:257.51ms
step:460/1480 train_time:115890ms step_avg:257.53ms
step:461/1480 train_time:116157ms step_avg:257.55ms
step:462/1480 train_time:116423ms step_avg:257.57ms
step:463/1480 train_time:116689ms step_avg:257.59ms
step:464/1480 train_time:116953ms step_avg:257.61ms
step:465/1480 train_time:117221ms step_avg:257.63ms
step:466/1480 train_time:117488ms step_avg:257.65ms
step:467/1480 train_time:117753ms step_avg:257.67ms
step:468/1480 train_time:118018ms step_avg:257.68ms
step:469/1480 train_time:118283ms step_avg:257.70ms
step:470/1480 train_time:118552ms step_avg:257.72ms
step:471/1480 train_time:118815ms step_avg:257.73ms
step:472/1480 train_time:119082ms step_avg:257.75ms
step:473/1480 train_time:119348ms step_avg:257.77ms
step:474/1480 train_time:119613ms step_avg:257.79ms
step:475/1480 train_time:119878ms step_avg:257.80ms
step:476/1480 train_time:120144ms step_avg:257.82ms
step:477/1480 train_time:120413ms step_avg:257.84ms
step:478/1480 train_time:120681ms step_avg:257.86ms
step:479/1480 train_time:120951ms step_avg:257.89ms
step:480/1480 train_time:121218ms step_avg:257.91ms
step:481/1480 train_time:121484ms step_avg:257.93ms
step:482/1480 train_time:121751ms step_avg:257.95ms
step:483/1480 train_time:122017ms step_avg:257.96ms
step:484/1480 train_time:122282ms step_avg:257.98ms
step:485/1480 train_time:122552ms step_avg:258.00ms
step:486/1480 train_time:122819ms step_avg:258.02ms
step:487/1480 train_time:123084ms step_avg:258.04ms
step:488/1480 train_time:123354ms step_avg:258.06ms
step:489/1480 train_time:123619ms step_avg:258.08ms
step:490/1480 train_time:123885ms step_avg:258.09ms
step:491/1480 train_time:124151ms step_avg:258.11ms
step:492/1480 train_time:124419ms step_avg:258.13ms
step:493/1480 train_time:124686ms step_avg:258.15ms
step:494/1480 train_time:124954ms step_avg:258.17ms
step:495/1480 train_time:125222ms step_avg:258.19ms
step:496/1480 train_time:125488ms step_avg:258.21ms
step:497/1480 train_time:125752ms step_avg:258.22ms
step:498/1480 train_time:126020ms step_avg:258.24ms
step:499/1480 train_time:126288ms step_avg:258.26ms
step:500/1480 train_time:126554ms step_avg:258.27ms
step:500/1480 val_loss:3.6880 train_time:126688ms step_avg:258.55ms
step:501/1480 train_time:126821ms step_avg:258.29ms
step:502/1480 train_time:127090ms step_avg:258.31ms
step:503/1480 train_time:127359ms step_avg:258.34ms
step:504/1480 train_time:127626ms step_avg:258.35ms
step:505/1480 train_time:127891ms step_avg:258.37ms
step:506/1480 train_time:128158ms step_avg:258.38ms
step:507/1480 train_time:128425ms step_avg:258.40ms
step:508/1480 train_time:128690ms step_avg:258.41ms
step:509/1480 train_time:128954ms step_avg:258.43ms
step:510/1480 train_time:129219ms step_avg:258.44ms
step:511/1480 train_time:129487ms step_avg:258.46ms
step:512/1480 train_time:129756ms step_avg:258.48ms
step:513/1480 train_time:130024ms step_avg:258.50ms
step:514/1480 train_time:130291ms step_avg:258.51ms
step:515/1480 train_time:130558ms step_avg:258.53ms
step:516/1480 train_time:130824ms step_avg:258.55ms
step:517/1480 train_time:131088ms step_avg:258.56ms
step:518/1480 train_time:131357ms step_avg:258.58ms
step:519/1480 train_time:131625ms step_avg:258.60ms
step:520/1480 train_time:131890ms step_avg:258.61ms
step:521/1480 train_time:132156ms step_avg:258.62ms
step:522/1480 train_time:132427ms step_avg:258.65ms
step:523/1480 train_time:132691ms step_avg:258.66ms
step:524/1480 train_time:132956ms step_avg:258.67ms
step:525/1480 train_time:133225ms step_avg:258.69ms
step:526/1480 train_time:133490ms step_avg:258.70ms
step:527/1480 train_time:133757ms step_avg:258.72ms
step:528/1480 train_time:134023ms step_avg:258.73ms
step:529/1480 train_time:134289ms step_avg:258.75ms
step:530/1480 train_time:134556ms step_avg:258.76ms
step:531/1480 train_time:134822ms step_avg:258.78ms
step:532/1480 train_time:135090ms step_avg:258.79ms
step:533/1480 train_time:135356ms step_avg:258.81ms
step:534/1480 train_time:135622ms step_avg:258.82ms
step:535/1480 train_time:135888ms step_avg:258.83ms
step:536/1480 train_time:136157ms step_avg:258.85ms
step:537/1480 train_time:136425ms step_avg:258.87ms
step:538/1480 train_time:136689ms step_avg:258.88ms
step:539/1480 train_time:136957ms step_avg:258.90ms
step:540/1480 train_time:137223ms step_avg:258.91ms
step:541/1480 train_time:137489ms step_avg:258.92ms
step:542/1480 train_time:137756ms step_avg:258.94ms
step:543/1480 train_time:138025ms step_avg:258.96ms
step:544/1480 train_time:138289ms step_avg:258.97ms
step:545/1480 train_time:138556ms step_avg:258.98ms
step:546/1480 train_time:138823ms step_avg:259.00ms
step:547/1480 train_time:139089ms step_avg:259.01ms
step:548/1480 train_time:139358ms step_avg:259.03ms
step:549/1480 train_time:139624ms step_avg:259.04ms
step:550/1480 train_time:139890ms step_avg:259.06ms
step:551/1480 train_time:140159ms step_avg:259.07ms
step:552/1480 train_time:140426ms step_avg:259.09ms
step:553/1480 train_time:140691ms step_avg:259.10ms
step:554/1480 train_time:140962ms step_avg:259.12ms
step:555/1480 train_time:141232ms step_avg:259.14ms
step:556/1480 train_time:141500ms step_avg:259.16ms
step:557/1480 train_time:141770ms step_avg:259.18ms
step:558/1480 train_time:142040ms step_avg:259.20ms
step:559/1480 train_time:142309ms step_avg:259.22ms
step:560/1480 train_time:142580ms step_avg:259.24ms
step:561/1480 train_time:142850ms step_avg:259.26ms
step:562/1480 train_time:143119ms step_avg:259.27ms
step:563/1480 train_time:143388ms step_avg:259.29ms
step:564/1480 train_time:143657ms step_avg:259.31ms
step:565/1480 train_time:143927ms step_avg:259.33ms
step:566/1480 train_time:144195ms step_avg:259.34ms
step:567/1480 train_time:144464ms step_avg:259.36ms
step:568/1480 train_time:144730ms step_avg:259.37ms
step:569/1480 train_time:145001ms step_avg:259.39ms
step:570/1480 train_time:145273ms step_avg:259.42ms
step:571/1480 train_time:145543ms step_avg:259.44ms
step:572/1480 train_time:145809ms step_avg:259.45ms
step:573/1480 train_time:146083ms step_avg:259.47ms
step:574/1480 train_time:146353ms step_avg:259.49ms
step:575/1480 train_time:146624ms step_avg:259.51ms
step:576/1480 train_time:146892ms step_avg:259.53ms
step:577/1480 train_time:147162ms step_avg:259.54ms
step:578/1480 train_time:147430ms step_avg:259.56ms
step:579/1480 train_time:147699ms step_avg:259.58ms
step:580/1480 train_time:147969ms step_avg:259.60ms
step:581/1480 train_time:148238ms step_avg:259.61ms
step:582/1480 train_time:148509ms step_avg:259.63ms
step:583/1480 train_time:148780ms step_avg:259.65ms
step:584/1480 train_time:149049ms step_avg:259.67ms
step:585/1480 train_time:149321ms step_avg:259.69ms
step:586/1480 train_time:149590ms step_avg:259.70ms
step:587/1480 train_time:149862ms step_avg:259.73ms
step:588/1480 train_time:150128ms step_avg:259.74ms
step:589/1480 train_time:150398ms step_avg:259.75ms
step:590/1480 train_time:150667ms step_avg:259.77ms
step:591/1480 train_time:150934ms step_avg:259.78ms
step:592/1480 train_time:151208ms step_avg:259.81ms
step:593/1480 train_time:151479ms step_avg:259.83ms
step:594/1480 train_time:151749ms step_avg:259.84ms
step:595/1480 train_time:152018ms step_avg:259.86ms
step:596/1480 train_time:152289ms step_avg:259.88ms
step:597/1480 train_time:152556ms step_avg:259.89ms
step:598/1480 train_time:152825ms step_avg:259.91ms
step:599/1480 train_time:153091ms step_avg:259.92ms
step:600/1480 train_time:153360ms step_avg:259.93ms
step:601/1480 train_time:153631ms step_avg:259.95ms
step:602/1480 train_time:153901ms step_avg:259.97ms
step:603/1480 train_time:154170ms step_avg:259.98ms
step:604/1480 train_time:154440ms step_avg:260.00ms
step:605/1480 train_time:154710ms step_avg:260.02ms
step:606/1480 train_time:154982ms step_avg:260.04ms
step:607/1480 train_time:155256ms step_avg:260.06ms
step:608/1480 train_time:155526ms step_avg:260.08ms
step:609/1480 train_time:155792ms step_avg:260.09ms
step:610/1480 train_time:156060ms step_avg:260.10ms
step:611/1480 train_time:156330ms step_avg:260.12ms
step:612/1480 train_time:156599ms step_avg:260.13ms
step:613/1480 train_time:156868ms step_avg:260.15ms
step:614/1480 train_time:157136ms step_avg:260.16ms
step:615/1480 train_time:157408ms step_avg:260.18ms
step:616/1480 train_time:157680ms step_avg:260.20ms
step:617/1480 train_time:157948ms step_avg:260.21ms
step:618/1480 train_time:158218ms step_avg:260.23ms
step:619/1480 train_time:158489ms step_avg:260.25ms
step:620/1480 train_time:158759ms step_avg:260.26ms
step:621/1480 train_time:159028ms step_avg:260.27ms
step:622/1480 train_time:159297ms step_avg:260.29ms
step:623/1480 train_time:159572ms step_avg:260.31ms
step:624/1480 train_time:159841ms step_avg:260.33ms
step:625/1480 train_time:160109ms step_avg:260.34ms
step:625/1480 val_loss:3.6085 train_time:160247ms step_avg:260.56ms
step:626/1480 train_time:160382ms step_avg:260.36ms
step:627/1480 train_time:160653ms step_avg:260.38ms
step:628/1480 train_time:160921ms step_avg:260.39ms
step:629/1480 train_time:161192ms step_avg:260.41ms
step:630/1480 train_time:161460ms step_avg:260.42ms
step:631/1480 train_time:161730ms step_avg:260.44ms
step:632/1480 train_time:161999ms step_avg:260.45ms
step:633/1480 train_time:162273ms step_avg:260.47ms
step:634/1480 train_time:162542ms step_avg:260.48ms
step:635/1480 train_time:162811ms step_avg:260.50ms
step:636/1480 train_time:163078ms step_avg:260.51ms
step:637/1480 train_time:163350ms step_avg:260.53ms
step:638/1480 train_time:163615ms step_avg:260.53ms
step:639/1480 train_time:163886ms step_avg:260.55ms
step:640/1480 train_time:164153ms step_avg:260.56ms
step:641/1480 train_time:164423ms step_avg:260.58ms
step:642/1480 train_time:164692ms step_avg:260.59ms
step:643/1480 train_time:164962ms step_avg:260.60ms
step:644/1480 train_time:165231ms step_avg:260.62ms
step:645/1480 train_time:165503ms step_avg:260.64ms
step:646/1480 train_time:165773ms step_avg:260.65ms
step:647/1480 train_time:166041ms step_avg:260.66ms
step:648/1480 train_time:166312ms step_avg:260.68ms
step:649/1480 train_time:166577ms step_avg:260.68ms
step:650/1480 train_time:166846ms step_avg:260.70ms
step:651/1480 train_time:167116ms step_avg:260.71ms
step:652/1480 train_time:167386ms step_avg:260.73ms
step:653/1480 train_time:167653ms step_avg:260.74ms
step:654/1480 train_time:167922ms step_avg:260.75ms
step:655/1480 train_time:168192ms step_avg:260.76ms
step:656/1480 train_time:168462ms step_avg:260.78ms
step:657/1480 train_time:168732ms step_avg:260.79ms
step:658/1480 train_time:169006ms step_avg:260.81ms
step:659/1480 train_time:169276ms step_avg:260.83ms
step:660/1480 train_time:169549ms step_avg:260.85ms
step:661/1480 train_time:169819ms step_avg:260.86ms
step:662/1480 train_time:170091ms step_avg:260.88ms
step:663/1480 train_time:170362ms step_avg:260.89ms
step:664/1480 train_time:170634ms step_avg:260.91ms
step:665/1480 train_time:170908ms step_avg:260.93ms
step:666/1480 train_time:171176ms step_avg:260.94ms
step:667/1480 train_time:171448ms step_avg:260.96ms
step:668/1480 train_time:171721ms step_avg:260.97ms
step:669/1480 train_time:172000ms step_avg:261.00ms
step:670/1480 train_time:172273ms step_avg:261.02ms
step:671/1480 train_time:172545ms step_avg:261.04ms
step:672/1480 train_time:172816ms step_avg:261.05ms
step:673/1480 train_time:173089ms step_avg:261.07ms
step:674/1480 train_time:173362ms step_avg:261.09ms
step:675/1480 train_time:173635ms step_avg:261.11ms
step:676/1480 train_time:173910ms step_avg:261.13ms
step:677/1480 train_time:174179ms step_avg:261.14ms
step:678/1480 train_time:174449ms step_avg:261.15ms
step:679/1480 train_time:174721ms step_avg:261.17ms
step:680/1480 train_time:174993ms step_avg:261.18ms
step:681/1480 train_time:175264ms step_avg:261.20ms
step:682/1480 train_time:175541ms step_avg:261.22ms
step:683/1480 train_time:175811ms step_avg:261.23ms
step:684/1480 train_time:176081ms step_avg:261.25ms
step:685/1480 train_time:176352ms step_avg:261.26ms
step:686/1480 train_time:176622ms step_avg:261.28ms
step:687/1480 train_time:176892ms step_avg:261.29ms
step:688/1480 train_time:177169ms step_avg:261.31ms
step:689/1480 train_time:177439ms step_avg:261.32ms
step:690/1480 train_time:177711ms step_avg:261.34ms
step:691/1480 train_time:177981ms step_avg:261.35ms
step:692/1480 train_time:178251ms step_avg:261.36ms
step:693/1480 train_time:178523ms step_avg:261.38ms
step:694/1480 train_time:178795ms step_avg:261.40ms
step:695/1480 train_time:179068ms step_avg:261.41ms
step:696/1480 train_time:179337ms step_avg:261.42ms
step:697/1480 train_time:179611ms step_avg:261.44ms
step:698/1480 train_time:179881ms step_avg:261.45ms
step:699/1480 train_time:180152ms step_avg:261.47ms
step:700/1480 train_time:180423ms step_avg:261.48ms
step:701/1480 train_time:180694ms step_avg:261.50ms
step:702/1480 train_time:180967ms step_avg:261.51ms
step:703/1480 train_time:181235ms step_avg:261.52ms
step:704/1480 train_time:181507ms step_avg:261.54ms
step:705/1480 train_time:181780ms step_avg:261.55ms
step:706/1480 train_time:182054ms step_avg:261.57ms
step:707/1480 train_time:182326ms step_avg:261.59ms
step:708/1480 train_time:182595ms step_avg:261.60ms
step:709/1480 train_time:182868ms step_avg:261.61ms
step:710/1480 train_time:183136ms step_avg:261.62ms
step:711/1480 train_time:183410ms step_avg:261.64ms
step:712/1480 train_time:183686ms step_avg:261.66ms
step:713/1480 train_time:183958ms step_avg:261.68ms
step:714/1480 train_time:184227ms step_avg:261.69ms
step:715/1480 train_time:184497ms step_avg:261.70ms
step:716/1480 train_time:184769ms step_avg:261.71ms
step:717/1480 train_time:185041ms step_avg:261.73ms
step:718/1480 train_time:185314ms step_avg:261.74ms
step:719/1480 train_time:185585ms step_avg:261.76ms
step:720/1480 train_time:185857ms step_avg:261.77ms
step:721/1480 train_time:186126ms step_avg:261.78ms
step:722/1480 train_time:186399ms step_avg:261.80ms
step:723/1480 train_time:186670ms step_avg:261.81ms
step:724/1480 train_time:186940ms step_avg:261.82ms
step:725/1480 train_time:187212ms step_avg:261.84ms
step:726/1480 train_time:187488ms step_avg:261.85ms
step:727/1480 train_time:187759ms step_avg:261.87ms
step:728/1480 train_time:188030ms step_avg:261.88ms
step:729/1480 train_time:188300ms step_avg:261.89ms
step:730/1480 train_time:188573ms step_avg:261.91ms
step:731/1480 train_time:188845ms step_avg:261.92ms
step:732/1480 train_time:189113ms step_avg:261.93ms
step:733/1480 train_time:189385ms step_avg:261.94ms
step:734/1480 train_time:189656ms step_avg:261.96ms
step:735/1480 train_time:189926ms step_avg:261.97ms
step:736/1480 train_time:190198ms step_avg:261.98ms
step:737/1480 train_time:190470ms step_avg:261.99ms
step:738/1480 train_time:190738ms step_avg:262.00ms
step:739/1480 train_time:191011ms step_avg:262.02ms
step:740/1480 train_time:191285ms step_avg:262.03ms
step:741/1480 train_time:191552ms step_avg:262.04ms
step:742/1480 train_time:191824ms step_avg:262.05ms
step:743/1480 train_time:192096ms step_avg:262.07ms
step:744/1480 train_time:192370ms step_avg:262.08ms
step:745/1480 train_time:192643ms step_avg:262.10ms
step:746/1480 train_time:192914ms step_avg:262.11ms
step:747/1480 train_time:193187ms step_avg:262.13ms
step:748/1480 train_time:193461ms step_avg:262.14ms
step:749/1480 train_time:193734ms step_avg:262.16ms
step:750/1480 train_time:194007ms step_avg:262.17ms
step:750/1480 val_loss:3.5517 train_time:194143ms step_avg:262.36ms
step:751/1480 train_time:194279ms step_avg:262.19ms
step:752/1480 train_time:194553ms step_avg:262.20ms
step:753/1480 train_time:194825ms step_avg:262.21ms
step:754/1480 train_time:195097ms step_avg:262.23ms
step:755/1480 train_time:195371ms step_avg:262.24ms
step:756/1480 train_time:195646ms step_avg:262.26ms
step:757/1480 train_time:195920ms step_avg:262.28ms
step:758/1480 train_time:196191ms step_avg:262.29ms
step:759/1480 train_time:196462ms step_avg:262.30ms
step:760/1480 train_time:196734ms step_avg:262.31ms
step:761/1480 train_time:197006ms step_avg:262.33ms
step:762/1480 train_time:197279ms step_avg:262.34ms
step:763/1480 train_time:197549ms step_avg:262.35ms
step:764/1480 train_time:197821ms step_avg:262.36ms
step:765/1480 train_time:198090ms step_avg:262.37ms
step:766/1480 train_time:198360ms step_avg:262.38ms
step:767/1480 train_time:198630ms step_avg:262.39ms
step:768/1480 train_time:198902ms step_avg:262.40ms
step:769/1480 train_time:199178ms step_avg:262.42ms
step:770/1480 train_time:199449ms step_avg:262.43ms
step:771/1480 train_time:199721ms step_avg:262.45ms
step:772/1480 train_time:199993ms step_avg:262.46ms
step:773/1480 train_time:200262ms step_avg:262.47ms
step:774/1480 train_time:200535ms step_avg:262.48ms
step:775/1480 train_time:200805ms step_avg:262.49ms
step:776/1480 train_time:201080ms step_avg:262.51ms
step:777/1480 train_time:201353ms step_avg:262.52ms
step:778/1480 train_time:201622ms step_avg:262.53ms
step:779/1480 train_time:201896ms step_avg:262.54ms
step:780/1480 train_time:202169ms step_avg:262.56ms
step:781/1480 train_time:202446ms step_avg:262.58ms
step:782/1480 train_time:202720ms step_avg:262.59ms
step:783/1480 train_time:202992ms step_avg:262.60ms
step:784/1480 train_time:203264ms step_avg:262.61ms
step:785/1480 train_time:203536ms step_avg:262.63ms
step:786/1480 train_time:203810ms step_avg:262.64ms
step:787/1480 train_time:204083ms step_avg:262.65ms
step:788/1480 train_time:204358ms step_avg:262.67ms
step:789/1480 train_time:204630ms step_avg:262.68ms
step:790/1480 train_time:204908ms step_avg:262.70ms
step:791/1480 train_time:205188ms step_avg:262.73ms
step:792/1480 train_time:205461ms step_avg:262.74ms
step:793/1480 train_time:205734ms step_avg:262.75ms
step:794/1480 train_time:206008ms step_avg:262.76ms
step:795/1480 train_time:206282ms step_avg:262.78ms
step:796/1480 train_time:206557ms step_avg:262.79ms
step:797/1480 train_time:206827ms step_avg:262.80ms
step:798/1480 train_time:207100ms step_avg:262.82ms
step:799/1480 train_time:207381ms step_avg:262.84ms
step:800/1480 train_time:207651ms step_avg:262.85ms
step:801/1480 train_time:207925ms step_avg:262.86ms
step:802/1480 train_time:208202ms step_avg:262.88ms
step:803/1480 train_time:208476ms step_avg:262.89ms
step:804/1480 train_time:208745ms step_avg:262.90ms
step:805/1480 train_time:209021ms step_avg:262.92ms
step:806/1480 train_time:209295ms step_avg:262.93ms
step:807/1480 train_time:209567ms step_avg:262.94ms
step:808/1480 train_time:209844ms step_avg:262.96ms
step:809/1480 train_time:210118ms step_avg:262.98ms
step:810/1480 train_time:210388ms step_avg:262.98ms
step:811/1480 train_time:210659ms step_avg:263.00ms
step:812/1480 train_time:210932ms step_avg:263.01ms
step:813/1480 train_time:211202ms step_avg:263.02ms
step:814/1480 train_time:211476ms step_avg:263.03ms
step:815/1480 train_time:211748ms step_avg:263.04ms
step:816/1480 train_time:212021ms step_avg:263.05ms
step:817/1480 train_time:212297ms step_avg:263.07ms
step:818/1480 train_time:212565ms step_avg:263.08ms
step:819/1480 train_time:212838ms step_avg:263.09ms
step:820/1480 train_time:213117ms step_avg:263.11ms
step:821/1480 train_time:213386ms step_avg:263.11ms
step:822/1480 train_time:213660ms step_avg:263.13ms
step:823/1480 train_time:213930ms step_avg:263.14ms
step:824/1480 train_time:214207ms step_avg:263.15ms
step:825/1480 train_time:214484ms step_avg:263.17ms
step:826/1480 train_time:214759ms step_avg:263.19ms
step:827/1480 train_time:215033ms step_avg:263.20ms
step:828/1480 train_time:215307ms step_avg:263.21ms
step:829/1480 train_time:215582ms step_avg:263.23ms
step:830/1480 train_time:215857ms step_avg:263.24ms
step:831/1480 train_time:216134ms step_avg:263.26ms
step:832/1480 train_time:216409ms step_avg:263.27ms
step:833/1480 train_time:216684ms step_avg:263.29ms
step:834/1480 train_time:216958ms step_avg:263.30ms
step:835/1480 train_time:217231ms step_avg:263.31ms
step:836/1480 train_time:217507ms step_avg:263.33ms
step:837/1480 train_time:217784ms step_avg:263.34ms
step:838/1480 train_time:218058ms step_avg:263.36ms
step:839/1480 train_time:218333ms step_avg:263.37ms
step:840/1480 train_time:218609ms step_avg:263.38ms
step:841/1480 train_time:218884ms step_avg:263.40ms
step:842/1480 train_time:219159ms step_avg:263.41ms
step:843/1480 train_time:219429ms step_avg:263.42ms
step:844/1480 train_time:219701ms step_avg:263.43ms
step:845/1480 train_time:219977ms step_avg:263.44ms
step:846/1480 train_time:220247ms step_avg:263.45ms
step:847/1480 train_time:220521ms step_avg:263.47ms
step:848/1480 train_time:220794ms step_avg:263.48ms
step:849/1480 train_time:221065ms step_avg:263.49ms
step:850/1480 train_time:221335ms step_avg:263.49ms
step:851/1480 train_time:221611ms step_avg:263.51ms
step:852/1480 train_time:221884ms step_avg:263.52ms
step:853/1480 train_time:222157ms step_avg:263.53ms
step:854/1480 train_time:222430ms step_avg:263.54ms
step:855/1480 train_time:222702ms step_avg:263.55ms
step:856/1480 train_time:222976ms step_avg:263.57ms
step:857/1480 train_time:223251ms step_avg:263.58ms
step:858/1480 train_time:223526ms step_avg:263.59ms
step:859/1480 train_time:223799ms step_avg:263.60ms
step:860/1480 train_time:224069ms step_avg:263.61ms
step:861/1480 train_time:224349ms step_avg:263.63ms
step:862/1480 train_time:224631ms step_avg:263.65ms
step:863/1480 train_time:224914ms step_avg:263.67ms
step:864/1480 train_time:225189ms step_avg:263.69ms
step:865/1480 train_time:225460ms step_avg:263.70ms
step:866/1480 train_time:225736ms step_avg:263.71ms
step:867/1480 train_time:226007ms step_avg:263.72ms
step:868/1480 train_time:226279ms step_avg:263.73ms
step:869/1480 train_time:226552ms step_avg:263.74ms
step:870/1480 train_time:226825ms step_avg:263.75ms
step:871/1480 train_time:227099ms step_avg:263.76ms
step:872/1480 train_time:227375ms step_avg:263.78ms
step:873/1480 train_time:227646ms step_avg:263.78ms
step:874/1480 train_time:227923ms step_avg:263.80ms
step:875/1480 train_time:228197ms step_avg:263.81ms
step:875/1480 val_loss:3.5069 train_time:228336ms step_avg:263.97ms
step:876/1480 train_time:228471ms step_avg:263.82ms
step:877/1480 train_time:228754ms step_avg:263.84ms
step:878/1480 train_time:229029ms step_avg:263.86ms
step:879/1480 train_time:229301ms step_avg:263.87ms
step:880/1480 train_time:229573ms step_avg:263.88ms
step:881/1480 train_time:229846ms step_avg:263.89ms
step:882/1480 train_time:230121ms step_avg:263.90ms
step:883/1480 train_time:230398ms step_avg:263.92ms
step:884/1480 train_time:230675ms step_avg:263.93ms
step:885/1480 train_time:230949ms step_avg:263.94ms
step:886/1480 train_time:231230ms step_avg:263.96ms
step:887/1480 train_time:231508ms step_avg:263.98ms
step:888/1480 train_time:231791ms step_avg:264.00ms
step:889/1480 train_time:232068ms step_avg:264.01ms
step:890/1480 train_time:232338ms step_avg:264.02ms
step:891/1480 train_time:232612ms step_avg:264.03ms
step:892/1480 train_time:232888ms step_avg:264.05ms
step:893/1480 train_time:233159ms step_avg:264.05ms
step:894/1480 train_time:233434ms step_avg:264.07ms
step:895/1480 train_time:233710ms step_avg:264.08ms
step:896/1480 train_time:233985ms step_avg:264.09ms
step:897/1480 train_time:234263ms step_avg:264.11ms
step:898/1480 train_time:234541ms step_avg:264.12ms
step:899/1480 train_time:234819ms step_avg:264.14ms
step:900/1480 train_time:235094ms step_avg:264.15ms
step:901/1480 train_time:235369ms step_avg:264.16ms
step:902/1480 train_time:235645ms step_avg:264.18ms
step:903/1480 train_time:235930ms step_avg:264.20ms
step:904/1480 train_time:236202ms step_avg:264.21ms
step:905/1480 train_time:236470ms step_avg:264.21ms
step:906/1480 train_time:236746ms step_avg:264.23ms
step:907/1480 train_time:237022ms step_avg:264.24ms
step:908/1480 train_time:237297ms step_avg:264.25ms
step:909/1480 train_time:237569ms step_avg:264.26ms
step:910/1480 train_time:237852ms step_avg:264.28ms
step:911/1480 train_time:238128ms step_avg:264.29ms
step:912/1480 train_time:238408ms step_avg:264.31ms
step:913/1480 train_time:238687ms step_avg:264.33ms
step:914/1480 train_time:238959ms step_avg:264.34ms
step:915/1480 train_time:239240ms step_avg:264.35ms
step:916/1480 train_time:239516ms step_avg:264.37ms
step:917/1480 train_time:239792ms step_avg:264.38ms
step:918/1480 train_time:240070ms step_avg:264.39ms
step:919/1480 train_time:240347ms step_avg:264.41ms
step:920/1480 train_time:240625ms step_avg:264.42ms
step:921/1480 train_time:240899ms step_avg:264.43ms
step:922/1480 train_time:241178ms step_avg:264.45ms
step:923/1480 train_time:241451ms step_avg:264.46ms
step:924/1480 train_time:241728ms step_avg:264.47ms
step:925/1480 train_time:242002ms step_avg:264.48ms
step:926/1480 train_time:242274ms step_avg:264.49ms
step:927/1480 train_time:242547ms step_avg:264.50ms
step:928/1480 train_time:242822ms step_avg:264.51ms
step:929/1480 train_time:243095ms step_avg:264.52ms
step:930/1480 train_time:243369ms step_avg:264.53ms
step:931/1480 train_time:243642ms step_avg:264.54ms
step:932/1480 train_time:243922ms step_avg:264.56ms
step:933/1480 train_time:244202ms step_avg:264.57ms
step:934/1480 train_time:244476ms step_avg:264.58ms
step:935/1480 train_time:244759ms step_avg:264.60ms
step:936/1480 train_time:245034ms step_avg:264.62ms
step:937/1480 train_time:245311ms step_avg:264.63ms
step:938/1480 train_time:245587ms step_avg:264.64ms
step:939/1480 train_time:245865ms step_avg:264.66ms
step:940/1480 train_time:246140ms step_avg:264.67ms
step:941/1480 train_time:246418ms step_avg:264.68ms
step:942/1480 train_time:246691ms step_avg:264.69ms
step:943/1480 train_time:246968ms step_avg:264.70ms
step:944/1480 train_time:247251ms step_avg:264.72ms
step:945/1480 train_time:247530ms step_avg:264.74ms
step:946/1480 train_time:247809ms step_avg:264.75ms
step:947/1480 train_time:248085ms step_avg:264.77ms
step:948/1480 train_time:248363ms step_avg:264.78ms
step:949/1480 train_time:248638ms step_avg:264.79ms
step:950/1480 train_time:248909ms step_avg:264.80ms
step:951/1480 train_time:249189ms step_avg:264.81ms
step:952/1480 train_time:249464ms step_avg:264.82ms
step:953/1480 train_time:249745ms step_avg:264.84ms
step:954/1480 train_time:250027ms step_avg:264.86ms
step:955/1480 train_time:250304ms step_avg:264.87ms
step:956/1480 train_time:250579ms step_avg:264.88ms
step:957/1480 train_time:250864ms step_avg:264.90ms
step:958/1480 train_time:251145ms step_avg:264.92ms
step:959/1480 train_time:251416ms step_avg:264.93ms
step:960/1480 train_time:251691ms step_avg:264.94ms
step:961/1480 train_time:251968ms step_avg:264.95ms
step:962/1480 train_time:252244ms step_avg:264.96ms
step:963/1480 train_time:252525ms step_avg:264.98ms
step:964/1480 train_time:252799ms step_avg:264.99ms
step:965/1480 train_time:253070ms step_avg:264.99ms
step:966/1480 train_time:253346ms step_avg:265.01ms
step:967/1480 train_time:253618ms step_avg:265.01ms
step:968/1480 train_time:253898ms step_avg:265.03ms
step:969/1480 train_time:254171ms step_avg:265.04ms
step:970/1480 train_time:254444ms step_avg:265.05ms
step:971/1480 train_time:254721ms step_avg:265.06ms
step:972/1480 train_time:254994ms step_avg:265.07ms
step:973/1480 train_time:255267ms step_avg:265.07ms
step:974/1480 train_time:255547ms step_avg:265.09ms
step:975/1480 train_time:255820ms step_avg:265.10ms
step:976/1480 train_time:256092ms step_avg:265.11ms
step:977/1480 train_time:256368ms step_avg:265.12ms
step:978/1480 train_time:256644ms step_avg:265.13ms
step:979/1480 train_time:256922ms step_avg:265.14ms
step:980/1480 train_time:257199ms step_avg:265.15ms
step:981/1480 train_time:257479ms step_avg:265.17ms
step:982/1480 train_time:257757ms step_avg:265.18ms
step:983/1480 train_time:258034ms step_avg:265.19ms
step:984/1480 train_time:258309ms step_avg:265.20ms
step:985/1480 train_time:258588ms step_avg:265.22ms
step:986/1480 train_time:258863ms step_avg:265.23ms
step:987/1480 train_time:259136ms step_avg:265.24ms
step:988/1480 train_time:259414ms step_avg:265.25ms
step:989/1480 train_time:259691ms step_avg:265.26ms
step:990/1480 train_time:259969ms step_avg:265.27ms
step:991/1480 train_time:260245ms step_avg:265.29ms
step:992/1480 train_time:260528ms step_avg:265.30ms
step:993/1480 train_time:260815ms step_avg:265.33ms
step:994/1480 train_time:261089ms step_avg:265.33ms
step:995/1480 train_time:261363ms step_avg:265.34ms
step:996/1480 train_time:261640ms step_avg:265.35ms
step:997/1480 train_time:261922ms step_avg:265.37ms
step:998/1480 train_time:262194ms step_avg:265.38ms
step:999/1480 train_time:262469ms step_avg:265.39ms
step:1000/1480 train_time:262748ms step_avg:265.40ms
step:1000/1480 val_loss:3.4433 train_time:262890ms step_avg:265.55ms
step:1001/1480 train_time:263027ms step_avg:265.42ms
step:1002/1480 train_time:263309ms step_avg:265.43ms
step:1003/1480 train_time:263594ms step_avg:265.45ms
step:1004/1480 train_time:263869ms step_avg:265.46ms
step:1005/1480 train_time:264147ms step_avg:265.47ms
step:1006/1480 train_time:264423ms step_avg:265.49ms
step:1007/1480 train_time:264701ms step_avg:265.50ms
step:1008/1480 train_time:264979ms step_avg:265.51ms
step:1009/1480 train_time:265263ms step_avg:265.53ms
step:1010/1480 train_time:265540ms step_avg:265.54ms
step:1011/1480 train_time:265812ms step_avg:265.55ms
step:1012/1480 train_time:266088ms step_avg:265.56ms
step:1013/1480 train_time:266371ms step_avg:265.57ms
step:1014/1480 train_time:266651ms step_avg:265.59ms
step:1015/1480 train_time:266938ms step_avg:265.61ms
step:1016/1480 train_time:267218ms step_avg:265.62ms
step:1017/1480 train_time:267499ms step_avg:265.64ms
step:1018/1480 train_time:267779ms step_avg:265.65ms
step:1019/1480 train_time:268059ms step_avg:265.67ms
step:1020/1480 train_time:268339ms step_avg:265.68ms
step:1021/1480 train_time:268614ms step_avg:265.69ms
step:1022/1480 train_time:268896ms step_avg:265.71ms
step:1023/1480 train_time:269172ms step_avg:265.72ms
step:1024/1480 train_time:269452ms step_avg:265.73ms
step:1025/1480 train_time:269733ms step_avg:265.75ms
step:1026/1480 train_time:270005ms step_avg:265.75ms
step:1027/1480 train_time:270279ms step_avg:265.76ms
step:1028/1480 train_time:270560ms step_avg:265.78ms
step:1029/1480 train_time:270842ms step_avg:265.79ms
step:1030/1480 train_time:271119ms step_avg:265.80ms
step:1031/1480 train_time:271393ms step_avg:265.81ms
step:1032/1480 train_time:271679ms step_avg:265.83ms
step:1033/1480 train_time:271955ms step_avg:265.84ms
step:1034/1480 train_time:272234ms step_avg:265.85ms
step:1035/1480 train_time:272514ms step_avg:265.87ms
step:1036/1480 train_time:272791ms step_avg:265.88ms
step:1037/1480 train_time:273067ms step_avg:265.89ms
step:1038/1480 train_time:273340ms step_avg:265.89ms
step:1039/1480 train_time:273624ms step_avg:265.91ms
step:1040/1480 train_time:273900ms step_avg:265.92ms
step:1041/1480 train_time:274180ms step_avg:265.94ms
step:1042/1480 train_time:274456ms step_avg:265.95ms
step:1043/1480 train_time:274732ms step_avg:265.96ms
step:1044/1480 train_time:275006ms step_avg:265.96ms
step:1045/1480 train_time:275284ms step_avg:265.97ms
step:1046/1480 train_time:275560ms step_avg:265.98ms
step:1047/1480 train_time:275839ms step_avg:266.00ms
step:1048/1480 train_time:276115ms step_avg:266.01ms
step:1049/1480 train_time:276392ms step_avg:266.02ms
step:1050/1480 train_time:276677ms step_avg:266.04ms
step:1051/1480 train_time:276958ms step_avg:266.05ms
step:1052/1480 train_time:277237ms step_avg:266.06ms
step:1053/1480 train_time:277512ms step_avg:266.07ms
step:1054/1480 train_time:277787ms step_avg:266.08ms
step:1055/1480 train_time:278061ms step_avg:266.09ms
step:1056/1480 train_time:278340ms step_avg:266.10ms
step:1057/1480 train_time:278620ms step_avg:266.11ms
step:1058/1480 train_time:278900ms step_avg:266.13ms
step:1059/1480 train_time:279182ms step_avg:266.14ms
step:1060/1480 train_time:279459ms step_avg:266.15ms
step:1061/1480 train_time:279733ms step_avg:266.16ms
step:1062/1480 train_time:280010ms step_avg:266.17ms
step:1063/1480 train_time:280287ms step_avg:266.18ms
step:1064/1480 train_time:280560ms step_avg:266.19ms
step:1065/1480 train_time:280839ms step_avg:266.20ms
step:1066/1480 train_time:281115ms step_avg:266.21ms
step:1067/1480 train_time:281392ms step_avg:266.22ms
step:1068/1480 train_time:281664ms step_avg:266.22ms
step:1069/1480 train_time:281948ms step_avg:266.24ms
step:1070/1480 train_time:282219ms step_avg:266.24ms
step:1071/1480 train_time:282501ms step_avg:266.26ms
step:1072/1480 train_time:282778ms step_avg:266.27ms
step:1073/1480 train_time:283054ms step_avg:266.28ms
step:1074/1480 train_time:283331ms step_avg:266.29ms
step:1075/1480 train_time:283615ms step_avg:266.31ms
step:1076/1480 train_time:283897ms step_avg:266.32ms
step:1077/1480 train_time:284177ms step_avg:266.33ms
step:1078/1480 train_time:284461ms step_avg:266.35ms
step:1079/1480 train_time:284739ms step_avg:266.36ms
step:1080/1480 train_time:285020ms step_avg:266.37ms
step:1081/1480 train_time:285298ms step_avg:266.39ms
step:1082/1480 train_time:285578ms step_avg:266.40ms
step:1083/1480 train_time:285857ms step_avg:266.41ms
step:1084/1480 train_time:286129ms step_avg:266.41ms
step:1085/1480 train_time:286410ms step_avg:266.43ms
step:1086/1480 train_time:286689ms step_avg:266.44ms
step:1087/1480 train_time:286967ms step_avg:266.45ms
step:1088/1480 train_time:287243ms step_avg:266.46ms
step:1089/1480 train_time:287524ms step_avg:266.47ms
step:1090/1480 train_time:287805ms step_avg:266.49ms
step:1091/1480 train_time:288083ms step_avg:266.50ms
step:1092/1480 train_time:288365ms step_avg:266.51ms
step:1093/1480 train_time:288641ms step_avg:266.52ms
step:1094/1480 train_time:288918ms step_avg:266.53ms
step:1095/1480 train_time:289195ms step_avg:266.54ms
step:1096/1480 train_time:289472ms step_avg:266.55ms
step:1097/1480 train_time:289753ms step_avg:266.56ms
step:1098/1480 train_time:290033ms step_avg:266.57ms
step:1099/1480 train_time:290316ms step_avg:266.59ms
step:1100/1480 train_time:290600ms step_avg:266.61ms
step:1101/1480 train_time:290881ms step_avg:266.62ms
step:1102/1480 train_time:291162ms step_avg:266.63ms
step:1103/1480 train_time:291450ms step_avg:266.65ms
step:1104/1480 train_time:291724ms step_avg:266.66ms
step:1105/1480 train_time:292003ms step_avg:266.67ms
step:1106/1480 train_time:292279ms step_avg:266.68ms
step:1107/1480 train_time:292562ms step_avg:266.69ms
step:1108/1480 train_time:292838ms step_avg:266.70ms
step:1109/1480 train_time:293114ms step_avg:266.71ms
step:1110/1480 train_time:293392ms step_avg:266.72ms
step:1111/1480 train_time:293668ms step_avg:266.73ms
step:1112/1480 train_time:293948ms step_avg:266.74ms
step:1113/1480 train_time:294242ms step_avg:266.77ms
step:1114/1480 train_time:294523ms step_avg:266.78ms
step:1115/1480 train_time:294804ms step_avg:266.79ms
step:1116/1480 train_time:295080ms step_avg:266.80ms
step:1117/1480 train_time:295365ms step_avg:266.82ms
step:1118/1480 train_time:295652ms step_avg:266.83ms
step:1119/1480 train_time:295930ms step_avg:266.84ms
step:1120/1480 train_time:296208ms step_avg:266.85ms
step:1121/1480 train_time:296489ms step_avg:266.87ms
step:1122/1480 train_time:296771ms step_avg:266.88ms
step:1123/1480 train_time:297046ms step_avg:266.89ms
step:1124/1480 train_time:297328ms step_avg:266.90ms
step:1125/1480 train_time:297605ms step_avg:266.91ms
step:1125/1480 val_loss:3.3872 train_time:297743ms step_avg:267.03ms
step:1126/1480 train_time:297881ms step_avg:266.92ms
step:1127/1480 train_time:298167ms step_avg:266.94ms
step:1128/1480 train_time:298449ms step_avg:266.95ms
step:1129/1480 train_time:298735ms step_avg:266.97ms
step:1130/1480 train_time:299011ms step_avg:266.97ms
step:1131/1480 train_time:299302ms step_avg:267.00ms
step:1132/1480 train_time:299575ms step_avg:267.00ms
step:1133/1480 train_time:299854ms step_avg:267.01ms
step:1134/1480 train_time:300132ms step_avg:267.02ms
step:1135/1480 train_time:300412ms step_avg:267.03ms
step:1136/1480 train_time:300690ms step_avg:267.04ms
step:1137/1480 train_time:300970ms step_avg:267.05ms
step:1138/1480 train_time:301248ms step_avg:267.06ms
step:1139/1480 train_time:301528ms step_avg:267.08ms
step:1140/1480 train_time:301806ms step_avg:267.08ms
step:1141/1480 train_time:302090ms step_avg:267.10ms
step:1142/1480 train_time:302367ms step_avg:267.11ms
step:1143/1480 train_time:302647ms step_avg:267.12ms
step:1144/1480 train_time:302928ms step_avg:267.13ms
step:1145/1480 train_time:303203ms step_avg:267.14ms
step:1146/1480 train_time:303483ms step_avg:267.15ms
step:1147/1480 train_time:303763ms step_avg:267.16ms
step:1148/1480 train_time:304043ms step_avg:267.17ms
step:1149/1480 train_time:304327ms step_avg:267.19ms
step:1150/1480 train_time:304608ms step_avg:267.20ms
step:1151/1480 train_time:304891ms step_avg:267.21ms
step:1152/1480 train_time:305174ms step_avg:267.23ms
step:1153/1480 train_time:305452ms step_avg:267.24ms
step:1154/1480 train_time:305729ms step_avg:267.25ms
step:1155/1480 train_time:306011ms step_avg:267.26ms
step:1156/1480 train_time:306299ms step_avg:267.28ms
step:1157/1480 train_time:306576ms step_avg:267.29ms
step:1158/1480 train_time:306851ms step_avg:267.29ms
step:1159/1480 train_time:307129ms step_avg:267.30ms
step:1160/1480 train_time:307404ms step_avg:267.31ms
step:1161/1480 train_time:307687ms step_avg:267.32ms
step:1162/1480 train_time:307968ms step_avg:267.33ms
step:1163/1480 train_time:308246ms step_avg:267.34ms
step:1164/1480 train_time:308523ms step_avg:267.35ms
step:1165/1480 train_time:308797ms step_avg:267.36ms
step:1166/1480 train_time:309079ms step_avg:267.37ms
step:1167/1480 train_time:309361ms step_avg:267.38ms
step:1168/1480 train_time:309639ms step_avg:267.39ms
step:1169/1480 train_time:309918ms step_avg:267.40ms
step:1170/1480 train_time:310195ms step_avg:267.41ms
step:1171/1480 train_time:310473ms step_avg:267.42ms
step:1172/1480 train_time:310749ms step_avg:267.43ms
step:1173/1480 train_time:311028ms step_avg:267.44ms
step:1174/1480 train_time:311311ms step_avg:267.45ms
step:1175/1480 train_time:311593ms step_avg:267.46ms
step:1176/1480 train_time:311875ms step_avg:267.47ms
step:1177/1480 train_time:312167ms step_avg:267.50ms
step:1178/1480 train_time:312447ms step_avg:267.51ms
step:1179/1480 train_time:312720ms step_avg:267.51ms
step:1180/1480 train_time:313011ms step_avg:267.53ms
step:1181/1480 train_time:313288ms step_avg:267.54ms
step:1182/1480 train_time:313570ms step_avg:267.55ms
step:1183/1480 train_time:313850ms step_avg:267.56ms
step:1184/1480 train_time:314129ms step_avg:267.57ms
step:1185/1480 train_time:314411ms step_avg:267.58ms
step:1186/1480 train_time:314688ms step_avg:267.59ms
step:1187/1480 train_time:314974ms step_avg:267.61ms
step:1188/1480 train_time:315250ms step_avg:267.61ms
step:1189/1480 train_time:315533ms step_avg:267.63ms
step:1190/1480 train_time:315813ms step_avg:267.64ms
step:1191/1480 train_time:316094ms step_avg:267.65ms
step:1192/1480 train_time:316370ms step_avg:267.66ms
step:1193/1480 train_time:316648ms step_avg:267.66ms
step:1194/1480 train_time:316924ms step_avg:267.67ms
step:1195/1480 train_time:317207ms step_avg:267.69ms
step:1196/1480 train_time:317498ms step_avg:267.71ms
step:1197/1480 train_time:317777ms step_avg:267.71ms
step:1198/1480 train_time:318072ms step_avg:267.74ms
step:1199/1480 train_time:318354ms step_avg:267.75ms
step:1200/1480 train_time:318635ms step_avg:267.76ms
step:1201/1480 train_time:318911ms step_avg:267.77ms
step:1202/1480 train_time:319200ms step_avg:267.79ms
step:1203/1480 train_time:319483ms step_avg:267.80ms
step:1204/1480 train_time:319771ms step_avg:267.82ms
step:1205/1480 train_time:320049ms step_avg:267.82ms
step:1206/1480 train_time:320329ms step_avg:267.83ms
step:1207/1480 train_time:320609ms step_avg:267.84ms
step:1208/1480 train_time:320890ms step_avg:267.85ms
step:1209/1480 train_time:321172ms step_avg:267.87ms
step:1210/1480 train_time:321462ms step_avg:267.89ms
step:1211/1480 train_time:321748ms step_avg:267.90ms
step:1212/1480 train_time:322031ms step_avg:267.91ms
step:1213/1480 train_time:322311ms step_avg:267.92ms
step:1214/1480 train_time:322594ms step_avg:267.94ms
step:1215/1480 train_time:322877ms step_avg:267.95ms
step:1216/1480 train_time:323161ms step_avg:267.96ms
step:1217/1480 train_time:323448ms step_avg:267.98ms
step:1218/1480 train_time:323729ms step_avg:267.99ms
step:1219/1480 train_time:324021ms step_avg:268.01ms
step:1220/1480 train_time:324307ms step_avg:268.02ms
step:1221/1480 train_time:324589ms step_avg:268.03ms
step:1222/1480 train_time:324868ms step_avg:268.04ms
step:1223/1480 train_time:325149ms step_avg:268.05ms
step:1224/1480 train_time:325437ms step_avg:268.07ms
step:1225/1480 train_time:325713ms step_avg:268.08ms
step:1226/1480 train_time:325999ms step_avg:268.09ms
step:1227/1480 train_time:326279ms step_avg:268.10ms
step:1228/1480 train_time:326554ms step_avg:268.11ms
step:1229/1480 train_time:326838ms step_avg:268.12ms
step:1230/1480 train_time:327125ms step_avg:268.14ms
step:1231/1480 train_time:327409ms step_avg:268.15ms
step:1232/1480 train_time:327694ms step_avg:268.16ms
step:1233/1480 train_time:327972ms step_avg:268.17ms
step:1234/1480 train_time:328249ms step_avg:268.18ms
step:1235/1480 train_time:328537ms step_avg:268.19ms
step:1236/1480 train_time:328815ms step_avg:268.20ms
step:1237/1480 train_time:329093ms step_avg:268.21ms
step:1238/1480 train_time:329388ms step_avg:268.23ms
step:1239/1480 train_time:329671ms step_avg:268.24ms
step:1240/1480 train_time:329951ms step_avg:268.25ms
step:1241/1480 train_time:330236ms step_avg:268.27ms
step:1242/1480 train_time:330513ms step_avg:268.27ms
step:1243/1480 train_time:330795ms step_avg:268.28ms
step:1244/1480 train_time:331071ms step_avg:268.29ms
step:1245/1480 train_time:331350ms step_avg:268.30ms
step:1246/1480 train_time:331633ms step_avg:268.31ms
step:1247/1480 train_time:331914ms step_avg:268.32ms
step:1248/1480 train_time:332193ms step_avg:268.33ms
step:1249/1480 train_time:332473ms step_avg:268.34ms
step:1250/1480 train_time:332751ms step_avg:268.35ms
step:1250/1480 val_loss:3.3373 train_time:332898ms step_avg:268.47ms
step:1251/1480 train_time:333043ms step_avg:268.37ms
step:1252/1480 train_time:333324ms step_avg:268.38ms
step:1253/1480 train_time:333604ms step_avg:268.39ms
step:1254/1480 train_time:333883ms step_avg:268.39ms
step:1255/1480 train_time:334177ms step_avg:268.42ms
step:1256/1480 train_time:334463ms step_avg:268.43ms
step:1257/1480 train_time:334744ms step_avg:268.44ms
step:1258/1480 train_time:335025ms step_avg:268.45ms
step:1259/1480 train_time:335306ms step_avg:268.46ms
step:1260/1480 train_time:335585ms step_avg:268.47ms
step:1261/1480 train_time:335865ms step_avg:268.48ms
step:1262/1480 train_time:336148ms step_avg:268.49ms
step:1263/1480 train_time:336435ms step_avg:268.50ms
step:1264/1480 train_time:336711ms step_avg:268.51ms
step:1265/1480 train_time:336986ms step_avg:268.51ms
step:1266/1480 train_time:337268ms step_avg:268.53ms
step:1267/1480 train_time:337545ms step_avg:268.53ms
step:1268/1480 train_time:337824ms step_avg:268.54ms
step:1269/1480 train_time:338112ms step_avg:268.56ms
step:1270/1480 train_time:338387ms step_avg:268.56ms
step:1271/1480 train_time:338667ms step_avg:268.57ms
step:1272/1480 train_time:338945ms step_avg:268.58ms
step:1273/1480 train_time:339224ms step_avg:268.59ms
step:1274/1480 train_time:339507ms step_avg:268.60ms
step:1275/1480 train_time:339784ms step_avg:268.60ms
step:1276/1480 train_time:340060ms step_avg:268.61ms
step:1277/1480 train_time:340344ms step_avg:268.62ms
step:1278/1480 train_time:340622ms step_avg:268.63ms
step:1279/1480 train_time:340905ms step_avg:268.64ms
step:1280/1480 train_time:341191ms step_avg:268.65ms
step:1281/1480 train_time:341467ms step_avg:268.66ms
step:1282/1480 train_time:341745ms step_avg:268.67ms
step:1283/1480 train_time:342024ms step_avg:268.68ms
step:1284/1480 train_time:342304ms step_avg:268.68ms
step:1285/1480 train_time:342582ms step_avg:268.69ms
step:1286/1480 train_time:342859ms step_avg:268.70ms
step:1287/1480 train_time:343144ms step_avg:268.71ms
step:1288/1480 train_time:343424ms step_avg:268.72ms
step:1289/1480 train_time:343715ms step_avg:268.74ms
step:1290/1480 train_time:344003ms step_avg:268.75ms
step:1291/1480 train_time:344285ms step_avg:268.76ms
step:1292/1480 train_time:344567ms step_avg:268.77ms
step:1293/1480 train_time:344850ms step_avg:268.78ms
step:1294/1480 train_time:345128ms step_avg:268.79ms
step:1295/1480 train_time:345406ms step_avg:268.80ms
step:1296/1480 train_time:345686ms step_avg:268.81ms
step:1297/1480 train_time:345969ms step_avg:268.82ms
step:1298/1480 train_time:346249ms step_avg:268.83ms
step:1299/1480 train_time:346529ms step_avg:268.84ms
step:1300/1480 train_time:346807ms step_avg:268.84ms
step:1301/1480 train_time:347083ms step_avg:268.85ms
step:1302/1480 train_time:347366ms step_avg:268.86ms
step:1303/1480 train_time:347649ms step_avg:268.87ms
step:1304/1480 train_time:347933ms step_avg:268.88ms
step:1305/1480 train_time:348207ms step_avg:268.89ms
step:1306/1480 train_time:348491ms step_avg:268.90ms
step:1307/1480 train_time:348767ms step_avg:268.90ms
step:1308/1480 train_time:349045ms step_avg:268.91ms
step:1309/1480 train_time:349325ms step_avg:268.92ms
step:1310/1480 train_time:349607ms step_avg:268.93ms
step:1311/1480 train_time:349885ms step_avg:268.94ms
step:1312/1480 train_time:350168ms step_avg:268.95ms
step:1313/1480 train_time:350445ms step_avg:268.95ms
step:1314/1480 train_time:350726ms step_avg:268.96ms
step:1315/1480 train_time:351004ms step_avg:268.97ms
step:1316/1480 train_time:351281ms step_avg:268.97ms
step:1317/1480 train_time:351563ms step_avg:268.98ms
step:1318/1480 train_time:351851ms step_avg:269.00ms
step:1319/1480 train_time:352132ms step_avg:269.01ms
step:1320/1480 train_time:352425ms step_avg:269.03ms
step:1321/1480 train_time:352707ms step_avg:269.04ms
step:1322/1480 train_time:352994ms step_avg:269.05ms
step:1323/1480 train_time:353283ms step_avg:269.07ms
step:1324/1480 train_time:353568ms step_avg:269.08ms
step:1325/1480 train_time:353861ms step_avg:269.10ms
step:1326/1480 train_time:354145ms step_avg:269.11ms
step:1327/1480 train_time:354425ms step_avg:269.12ms
step:1328/1480 train_time:354703ms step_avg:269.12ms
step:1329/1480 train_time:355010ms step_avg:269.15ms
step:1330/1480 train_time:355295ms step_avg:269.16ms
step:1331/1480 train_time:355574ms step_avg:269.17ms
step:1332/1480 train_time:355854ms step_avg:269.18ms
step:1333/1480 train_time:356143ms step_avg:269.19ms
step:1334/1480 train_time:356424ms step_avg:269.20ms
step:1335/1480 train_time:356704ms step_avg:269.21ms
step:1336/1480 train_time:356993ms step_avg:269.23ms
step:1337/1480 train_time:357280ms step_avg:269.24ms
step:1338/1480 train_time:357563ms step_avg:269.25ms
step:1339/1480 train_time:357846ms step_avg:269.26ms
step:1340/1480 train_time:358127ms step_avg:269.27ms
step:1341/1480 train_time:358406ms step_avg:269.28ms
step:1342/1480 train_time:358688ms step_avg:269.29ms
step:1343/1480 train_time:358967ms step_avg:269.29ms
step:1344/1480 train_time:359246ms step_avg:269.30ms
step:1345/1480 train_time:359541ms step_avg:269.32ms
step:1346/1480 train_time:359819ms step_avg:269.33ms
step:1347/1480 train_time:360102ms step_avg:269.34ms
step:1348/1480 train_time:360383ms step_avg:269.34ms
step:1349/1480 train_time:360662ms step_avg:269.35ms
step:1350/1480 train_time:360947ms step_avg:269.36ms
step:1351/1480 train_time:361228ms step_avg:269.37ms
step:1352/1480 train_time:361507ms step_avg:269.38ms
step:1353/1480 train_time:361790ms step_avg:269.39ms
step:1354/1480 train_time:362077ms step_avg:269.40ms
step:1355/1480 train_time:362357ms step_avg:269.41ms
step:1356/1480 train_time:362646ms step_avg:269.42ms
step:1357/1480 train_time:362932ms step_avg:269.44ms
step:1358/1480 train_time:363217ms step_avg:269.45ms
step:1359/1480 train_time:363503ms step_avg:269.46ms
step:1360/1480 train_time:363785ms step_avg:269.47ms
step:1361/1480 train_time:364068ms step_avg:269.48ms
step:1362/1480 train_time:364349ms step_avg:269.49ms
step:1363/1480 train_time:364643ms step_avg:269.51ms
step:1364/1480 train_time:364923ms step_avg:269.51ms
step:1365/1480 train_time:365203ms step_avg:269.52ms
step:1366/1480 train_time:365484ms step_avg:269.53ms
step:1367/1480 train_time:365765ms step_avg:269.54ms
step:1368/1480 train_time:366047ms step_avg:269.55ms
step:1369/1480 train_time:366334ms step_avg:269.56ms
step:1370/1480 train_time:366623ms step_avg:269.58ms
step:1371/1480 train_time:366906ms step_avg:269.59ms
step:1372/1480 train_time:367191ms step_avg:269.60ms
step:1373/1480 train_time:367468ms step_avg:269.60ms
step:1374/1480 train_time:367753ms step_avg:269.61ms
step:1375/1480 train_time:368043ms step_avg:269.63ms
step:1375/1480 val_loss:3.2983 train_time:368181ms step_avg:269.73ms
step:1376/1480 train_time:368322ms step_avg:269.64ms
step:1377/1480 train_time:368606ms step_avg:269.65ms
step:1378/1480 train_time:368889ms step_avg:269.66ms
step:1379/1480 train_time:369173ms step_avg:269.67ms
step:1380/1480 train_time:369454ms step_avg:269.67ms
step:1381/1480 train_time:369745ms step_avg:269.69ms
step:1382/1480 train_time:370022ms step_avg:269.70ms
step:1383/1480 train_time:370304ms step_avg:269.70ms
step:1384/1480 train_time:370595ms step_avg:269.72ms
step:1385/1480 train_time:370872ms step_avg:269.73ms
step:1386/1480 train_time:371158ms step_avg:269.74ms
step:1387/1480 train_time:371439ms step_avg:269.75ms
step:1388/1480 train_time:371717ms step_avg:269.75ms
step:1389/1480 train_time:372003ms step_avg:269.76ms
step:1390/1480 train_time:372280ms step_avg:269.77ms
step:1391/1480 train_time:372559ms step_avg:269.78ms
step:1392/1480 train_time:372839ms step_avg:269.78ms
step:1393/1480 train_time:373121ms step_avg:269.79ms
step:1394/1480 train_time:373400ms step_avg:269.80ms
step:1395/1480 train_time:373678ms step_avg:269.80ms
step:1396/1480 train_time:373959ms step_avg:269.81ms
step:1397/1480 train_time:374236ms step_avg:269.82ms
step:1398/1480 train_time:374515ms step_avg:269.82ms
step:1399/1480 train_time:374799ms step_avg:269.83ms
step:1400/1480 train_time:375087ms step_avg:269.85ms
step:1401/1480 train_time:375364ms step_avg:269.85ms
step:1402/1480 train_time:375639ms step_avg:269.86ms
step:1403/1480 train_time:375924ms step_avg:269.87ms
step:1404/1480 train_time:376201ms step_avg:269.87ms
step:1405/1480 train_time:376481ms step_avg:269.88ms
step:1406/1480 train_time:376768ms step_avg:269.89ms
step:1407/1480 train_time:377044ms step_avg:269.90ms
step:1408/1480 train_time:377319ms step_avg:269.90ms
step:1409/1480 train_time:377617ms step_avg:269.92ms
step:1410/1480 train_time:377899ms step_avg:269.93ms
step:1411/1480 train_time:378178ms step_avg:269.93ms
step:1412/1480 train_time:378461ms step_avg:269.94ms
step:1413/1480 train_time:378744ms step_avg:269.95ms
step:1414/1480 train_time:379029ms step_avg:269.96ms
step:1415/1480 train_time:379309ms step_avg:269.97ms
step:1416/1480 train_time:379603ms step_avg:269.99ms
step:1417/1480 train_time:379891ms step_avg:270.00ms
step:1418/1480 train_time:380177ms step_avg:270.01ms
step:1419/1480 train_time:380461ms step_avg:270.02ms
step:1420/1480 train_time:380741ms step_avg:270.03ms
step:1421/1480 train_time:381024ms step_avg:270.04ms
step:1422/1480 train_time:381303ms step_avg:270.04ms
step:1423/1480 train_time:381580ms step_avg:270.05ms
step:1424/1480 train_time:381868ms step_avg:270.06ms
step:1425/1480 train_time:382159ms step_avg:270.08ms
step:1426/1480 train_time:382439ms step_avg:270.08ms
step:1427/1480 train_time:382724ms step_avg:270.09ms
step:1428/1480 train_time:383004ms step_avg:270.10ms
step:1429/1480 train_time:383282ms step_avg:270.11ms
step:1430/1480 train_time:383567ms step_avg:270.12ms
step:1431/1480 train_time:383856ms step_avg:270.13ms
step:1432/1480 train_time:384145ms step_avg:270.14ms
step:1433/1480 train_time:384437ms step_avg:270.16ms
step:1434/1480 train_time:384729ms step_avg:270.17ms
step:1435/1480 train_time:385011ms step_avg:270.18ms
step:1436/1480 train_time:385299ms step_avg:270.20ms
step:1437/1480 train_time:385579ms step_avg:270.20ms
step:1438/1480 train_time:385860ms step_avg:270.21ms
step:1439/1480 train_time:386143ms step_avg:270.22ms
step:1440/1480 train_time:386420ms step_avg:270.22ms
step:1441/1480 train_time:386702ms step_avg:270.23ms
step:1442/1480 train_time:386988ms step_avg:270.24ms
step:1443/1480 train_time:387297ms step_avg:270.27ms
step:1444/1480 train_time:387577ms step_avg:270.28ms
step:1445/1480 train_time:387861ms step_avg:270.29ms
step:1446/1480 train_time:388148ms step_avg:270.30ms
step:1447/1480 train_time:388438ms step_avg:270.31ms
step:1448/1480 train_time:388720ms step_avg:270.32ms
step:1449/1480 train_time:389003ms step_avg:270.33ms
step:1450/1480 train_time:389287ms step_avg:270.34ms
step:1451/1480 train_time:389570ms step_avg:270.35ms
step:1452/1480 train_time:389857ms step_avg:270.36ms
step:1453/1480 train_time:390137ms step_avg:270.36ms
step:1454/1480 train_time:390419ms step_avg:270.37ms
step:1455/1480 train_time:390706ms step_avg:270.39ms
step:1456/1480 train_time:390996ms step_avg:270.40ms
step:1457/1480 train_time:391279ms step_avg:270.41ms
step:1458/1480 train_time:391560ms step_avg:270.41ms
step:1459/1480 train_time:391846ms step_avg:270.43ms
step:1460/1480 train_time:392130ms step_avg:270.43ms
step:1461/1480 train_time:392413ms step_avg:270.44ms
step:1462/1480 train_time:392696ms step_avg:270.45ms
step:1463/1480 train_time:392981ms step_avg:270.46ms
step:1464/1480 train_time:393262ms step_avg:270.47ms
step:1465/1480 train_time:393543ms step_avg:270.48ms
step:1466/1480 train_time:393824ms step_avg:270.48ms
step:1467/1480 train_time:394107ms step_avg:270.49ms
step:1468/1480 train_time:394391ms step_avg:270.50ms
step:1469/1480 train_time:394673ms step_avg:270.51ms
step:1470/1480 train_time:394962ms step_avg:270.52ms
step:1471/1480 train_time:395261ms step_avg:270.54ms
step:1472/1480 train_time:395558ms step_avg:270.56ms
step:1473/1480 train_time:395837ms step_avg:270.57ms
step:1474/1480 train_time:396128ms step_avg:270.58ms
step:1475/1480 train_time:396422ms step_avg:270.60ms
step:1476/1480 train_time:396701ms step_avg:270.60ms
step:1477/1480 train_time:396995ms step_avg:270.62ms
step:1478/1480 train_time:397289ms step_avg:270.63ms
step:1479/1480 train_time:397579ms step_avg:270.65ms
step:1480/1480 train_time:397861ms step_avg:270.65ms
step:1480/1480 val_loss:3.2792 train_time:398007ms step_avg:270.75ms
peak memory consumption: 34238 MiB

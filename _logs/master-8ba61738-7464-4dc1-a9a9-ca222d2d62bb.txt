import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import contextlib
from dataclasses import dataclass
from pathlib import Path

import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.nn.attention.flex_attention import BlockMask, flex_attention #KoszarskyB

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params = list(params)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [
            {
                'params': [p for p in params if p.numel() == size],
                'update_buffer': [
                    torch.empty(size, device='cuda', dtype=torch.bfloat16)
                    for _ in range(self.world_size)
                ],
            }
            for size in sizes
        ]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            assert len(params) % self.world_size == 0
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                p = params[base_i + self.rank]
                g = p.grad
                assert g is not None
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.lerp_(g, 1 - momentum)
                g = g.lerp_(buf, momentum) if nesterov else buf
                g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                update_prev()
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.register_buffer('inv_freq', (1 / base) ** (torch.arange(0, dim, 2) / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            t = torch.arange(seq_len, device=x.device)
            freqs = torch.outer(t, self.inv_freq)
            self.seq_len_cached = seq_len
            self.cos_cached = freqs.cos()
            self.sin_cached = freqs.sin()
        cos, sin = self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
        # apply_rotary_emb(x, cos, sin)
        x1, x2 = x.chunk(2, dim=3)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x, vi, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        v = self.lambdas[0] * v + self.lambdas[1] * vi.view_as(v) # @KoszarskyB & @Grad62304977
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, enable_gqa=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc   = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config.model_dim, config.num_heads)
        self.mlp = MLP(config.model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, vi, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x = x + self.attn(norm(x), vi, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, config: "GPTConfig"):
        super().__init__()
        self.embed = nn.ModuleList([
            nn.Embedding(config.vocab_size, config.model_dim)
            for _ in range(6)
        ])

    def forward(self, inputs) -> "list[torch.Tensor]":
        ve = [emb(inputs) for emb in self.embed]
        ve += reversed(ve)
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    num_layers : int = 12
    num_heads : int = 6 # head dim 128 suggested by @Grad62304977
    model_dim : int = 768

class GPT(nn.Module):

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.num_layers = config.num_layers

        # U-net design by @brendanh0gan
        self.num_encoder_layers = config.num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = config.num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

        self.embed = nn.Embedding(config.vocab_size, config.model_dim)
        self.blocks = nn.ModuleList([Block(config) for _ in range(config.num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(config)
        self.lm_head = CastedLinear(config.model_dim, config.vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(
        self,
        inputs: torch.Tensor,
        targets: torch.Tensor,
        sliding_window_num_blocks: torch.Tensor,
    ):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: torch.Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks: torch.Tensor):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm ^ full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        # forward the GPT model itself
        x = self.embed(inputs[None]) # token embeddings of shape (b, t, model_dim)
        x = norm(x) # @Grad62304977
        x0 = x
        ve = self.value_embeds(inputs)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(file: Path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32)
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    return int(header[2]) # number of tokens (claimed)

def _load_data_shard(path: Path, num_tokens):
    with path.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, seq_len, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.seq_len = seq_len

        # glob files that match the pattern
        self.files = sorted(Path.cwd().glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        self.files_num_tokens = [_peek_data_shard(file) for file in self.files]
        assert min(self.files_num_tokens) >= num_processes * seq_len + 1
        self.total_num_tokens = sum(self.files_num_tokens)

        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.seq_len
        self.tokens = _load_data_shard(self.files[self.current_shard], self.files_num_tokens[self.current_shard])

    def next_batch(self):
        batch_size = self.seq_len * self.num_processes
        buf = self.tokens[self.current_position:self.current_position+self.seq_len+1]
        # host side async is sufficient;
        # no performance improvement was observed when introducing a separate stream.
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # inputs
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # targets
        # advance current position and load next shard if necessary
        self.current_position += batch_size
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8 # batch size, in sequences, across all devices
    sequence_length : int = 64*1024 # sequence length, in tokens
    num_iterations : int = 1480 # number of iterations to run
    warmup_iters : int = 0
    cooldown_iters : int = 600 # number of iterations of linear warmup/cooldown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
device = torch.device(f'cuda:{ddp_local_rank}')
torch.cuda.set_device(device)
print(f'using device: {device}')
dist.init_process_group(backend='nccl', device_id=device)
dist.barrier()
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    Path('logs').mkdir(exist_ok=True)
    # logdir = Path('logs') / f'{run_id}'
    # logdir.mkdir()
    logfile = Path('logs') / f'{run_id}.txt'
    print(logfile.stem)
    # create the log file
    with logfile.open('w') as f:
        # begin the log by printing this file (the Python code)
        print(code, file=f)
        print('=' * 100, file=f)
def print0(s, logonly=False):
    if master_process:
        with logfile.open('a') as f:
            if not logonly:
                print(s)
            print(s, file=f)
# log information about the hardware/software environment this is running on
# and print the full `nvidia-smi` to file
print0(f'Running python {sys.version}')
print0(f'Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:')
import subprocess
result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
print0(f'{result.stdout}', logonly=True)
print0('='*100, logonly=True)

# calculate the number of steps to take in the val loop.
assert args.val_tokens % (args.sequence_length * ddp_world_size) == 0
val_steps = args.val_tokens // (args.sequence_length * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (ddp_world_size) == 0
train_accumulation_steps = args.batch_size // ddp_world_size

# load tokens
train_loader = DistributedDataLoader(args.input_bin, args.sequence_length, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, args.sequence_length, ddp_rank, ddp_world_size)
print0(f"Training DataLoader: total number of tokens: {train_loader.total_num_tokens} across {len(train_loader.files)} files")
print0(f"Validation DataLoader: total number of tokens: {val_loader.total_num_tokens} across {len(val_loader.files)} files")
print0('='*100, logonly=True)
inputs_train, targets_train = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, num_layers=12, num_heads=6, model_dim=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()
config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)
raw_model = model.module # always contains the "raw" unwrapped model

# init the optimizer(s)
embed_params = [*raw_model.embed.parameters(), *raw_model.value_embeds.parameters()]
optimizer1 = torch.optim.Adam(embed_params, lr=0.6, betas=(0.8, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight], lr=0.008, betas=(0.8, 0.95), fused=True)
params = list(raw_model.blocks.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
optimizer3 = Muon(matrix_params, lr=0.05, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.8, 0.95), fused=True)
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and cooldown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.cooldown_iters:
        return 1.0
    # 3) linear cooldown
    else:
        decay_ratio = (args.num_iterations - it) / args.cooldown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device="cuda")
sw_num_blocks_prev = 1
# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the sliding window size over training in chunks of 128 from 128 -> 1856. By @fernbear.bsky.social
    frac_done = step / args.num_iterations # training progress
    sw_num_blocks = int(((1 - frac_done) * 128 + frac_done * 1856) // 128)
    if sw_num_blocks != sw_num_blocks_prev:
        sliding_window_num_blocks.copy_(sw_num_blocks, non_blocking=True)
        sw_num_blocks_prev = sw_num_blocks

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch()
                val_loss += model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        print0(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    # uncomment if you want to save any checkpoints
    #save_every = 1000
    #if master_process and (last_step or (save_every > 0 and step % save_every == 0)):
    #    # stop the clock
    #    torch.cuda.synchronize()
    #    training_time_ms += 1000 * (time.perf_counter() - t0)
    #    # save the state of the training process
    #    log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
    #    torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
    #    # start the clock again
    #    torch.cuda.synchronize()
    #    t0 = time.perf_counter()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps + 1):
        with contextlib.ExitStack() as stack:
            if i < train_accumulation_steps: # there's no need to sync gradients every accumulation step
                stack.enter_context(model.no_sync())
            if step >= 5:
                stack.enter_context(torch.compiler.set_stance(skip_guard_eval_unsafe=True))
            model(inputs_train, targets_train, sliding_window_num_blocks).backward()
            inputs_train, targets_train = train_loader.next_batch()
    if train_accumulation_steps != 1:
        for p in model.parameters():
            p.grad /= train_accumulation_steps
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer3.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{args.num_iterations} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")

print0(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()

====================================================================================================
Running python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running pytorch 2.6.0.dev20241203+cu124 compiled for CUDA 12.4
nvidia-smi:
Tue Dec 24 08:44:51 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 PCIe               On  | 00000000:00:07.0 Off |                    0 |
| N/A   31C    P0              76W / 350W |   4162MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  | 00000000:00:08.0 Off |                    0 |
| N/A   37C    P0              79W / 350W |    923MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  | 00000000:00:09.0 Off |                    0 |
| N/A   28C    P0              73W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  | 00000000:00:0A.0 Off |                    0 |
| N/A   29C    P0              74W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  | 00000000:00:0B.0 Off |                    0 |
| N/A   30C    P0              75W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  | 00000000:00:0C.0 Off |                    0 |
| N/A   29C    P0              73W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  | 00000000:00:0D.0 Off |                    0 |
| N/A   35C    P0              80W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  | 00000000:00:0E.0 Off |                    0 |
| N/A   30C    P0              76W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     94853      C   /usr/bin/python3                            946MiB |
|    0   N/A  N/A     94854      C   /usr/bin/python3                            452MiB |
|    0   N/A  N/A     94855      C   /usr/bin/python3                            452MiB |
|    0   N/A  N/A     94856      C   /usr/bin/python3                            452MiB |
|    0   N/A  N/A     94857      C   /usr/bin/python3                            452MiB |
|    0   N/A  N/A     94858      C   /usr/bin/python3                            452MiB |
|    0   N/A  N/A     94859      C   /usr/bin/python3                            452MiB |
|    0   N/A  N/A     94860      C   /usr/bin/python3                            452MiB |
|    1   N/A  N/A     94854      C   /usr/bin/python3                            914MiB |
|    2   N/A  N/A     94855      C   /usr/bin/python3                            954MiB |
|    3   N/A  N/A     94856      C   /usr/bin/python3                            954MiB |
|    4   N/A  N/A     94857      C   /usr/bin/python3                            954MiB |
|    5   N/A  N/A     94858      C   /usr/bin/python3                            954MiB |
|    6   N/A  N/A     94859      C   /usr/bin/python3                            954MiB |
|    7   N/A  N/A     94860      C   /usr/bin/python3                            954MiB |
+---------------------------------------------------------------------------------------+

====================================================================================================
Training DataLoader: total number of tokens: 1000000000 across 10 files
Validation DataLoader: total number of tokens: 100000000 across 1 files
====================================================================================================
step:0/1480 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1480 train_time:24755ms step_avg:nanms
step:2/1480 train_time:25331ms step_avg:nanms
step:3/1480 train_time:25572ms step_avg:nanms
step:4/1480 train_time:25819ms step_avg:nanms
step:5/1480 train_time:26069ms step_avg:nanms
step:6/1480 train_time:26317ms step_avg:nanms
step:7/1480 train_time:26567ms step_avg:nanms
step:8/1480 train_time:26816ms step_avg:nanms
step:9/1480 train_time:27063ms step_avg:nanms
step:10/1480 train_time:27313ms step_avg:nanms
step:11/1480 train_time:249ms step_avg:nanms
step:12/1480 train_time:498ms step_avg:nanms
step:13/1480 train_time:746ms step_avg:248.58ms
step:14/1480 train_time:993ms step_avg:248.26ms
step:15/1480 train_time:1245ms step_avg:248.93ms
step:16/1480 train_time:1493ms step_avg:248.76ms
step:17/1480 train_time:1745ms step_avg:249.30ms
step:18/1480 train_time:1992ms step_avg:249.04ms
step:19/1480 train_time:2243ms step_avg:249.23ms
step:20/1480 train_time:2490ms step_avg:249.00ms
step:21/1480 train_time:2741ms step_avg:249.22ms
step:22/1480 train_time:2990ms step_avg:249.20ms
step:23/1480 train_time:3242ms step_avg:249.41ms
step:24/1480 train_time:3490ms step_avg:249.31ms
step:25/1480 train_time:3743ms step_avg:249.53ms
step:26/1480 train_time:3991ms step_avg:249.44ms
step:27/1480 train_time:4243ms step_avg:249.61ms
step:28/1480 train_time:4491ms step_avg:249.51ms
step:29/1480 train_time:4746ms step_avg:249.79ms
step:30/1480 train_time:4994ms step_avg:249.72ms
step:31/1480 train_time:5244ms step_avg:249.73ms
step:32/1480 train_time:5492ms step_avg:249.62ms
step:33/1480 train_time:5745ms step_avg:249.80ms
step:34/1480 train_time:5994ms step_avg:249.75ms
step:35/1480 train_time:6245ms step_avg:249.79ms
step:36/1480 train_time:6492ms step_avg:249.71ms
step:37/1480 train_time:6745ms step_avg:249.82ms
step:38/1480 train_time:6994ms step_avg:249.78ms
step:39/1480 train_time:7245ms step_avg:249.83ms
step:40/1480 train_time:7492ms step_avg:249.74ms
step:41/1480 train_time:7745ms step_avg:249.85ms
step:42/1480 train_time:7994ms step_avg:249.81ms
step:43/1480 train_time:8244ms step_avg:249.83ms
step:44/1480 train_time:8493ms step_avg:249.78ms
step:45/1480 train_time:8746ms step_avg:249.89ms
step:46/1480 train_time:8994ms step_avg:249.83ms
step:47/1480 train_time:9245ms step_avg:249.87ms
step:48/1480 train_time:9494ms step_avg:249.83ms
step:49/1480 train_time:9746ms step_avg:249.89ms
step:50/1480 train_time:9994ms step_avg:249.85ms
step:51/1480 train_time:10244ms step_avg:249.85ms
step:52/1480 train_time:10493ms step_avg:249.83ms
step:53/1480 train_time:10744ms step_avg:249.86ms
step:54/1480 train_time:10992ms step_avg:249.81ms
step:55/1480 train_time:11244ms step_avg:249.86ms
step:56/1480 train_time:11491ms step_avg:249.80ms
step:57/1480 train_time:11745ms step_avg:249.89ms
step:58/1480 train_time:11993ms step_avg:249.86ms
step:59/1480 train_time:12245ms step_avg:249.91ms
step:60/1480 train_time:12494ms step_avg:249.88ms
step:61/1480 train_time:12746ms step_avg:249.91ms
step:62/1480 train_time:12994ms step_avg:249.88ms
step:63/1480 train_time:13246ms step_avg:249.92ms
step:64/1480 train_time:13493ms step_avg:249.88ms
step:65/1480 train_time:13746ms step_avg:249.92ms
step:66/1480 train_time:13993ms step_avg:249.88ms
step:67/1480 train_time:14245ms step_avg:249.91ms
step:68/1480 train_time:14494ms step_avg:249.89ms
step:69/1480 train_time:14746ms step_avg:249.93ms
step:70/1480 train_time:14994ms step_avg:249.90ms
step:71/1480 train_time:15245ms step_avg:249.92ms
step:72/1480 train_time:15495ms step_avg:249.91ms
step:73/1480 train_time:15745ms step_avg:249.93ms
step:74/1480 train_time:15994ms step_avg:249.90ms
step:75/1480 train_time:16246ms step_avg:249.94ms
step:76/1480 train_time:16495ms step_avg:249.93ms
step:77/1480 train_time:16748ms step_avg:249.97ms
step:78/1480 train_time:16998ms step_avg:249.97ms
step:79/1480 train_time:17247ms step_avg:249.95ms
step:80/1480 train_time:17496ms step_avg:249.95ms
step:81/1480 train_time:18305ms step_avg:257.82ms
step:82/1480 train_time:18540ms step_avg:257.50ms
step:83/1480 train_time:18791ms step_avg:257.40ms
step:84/1480 train_time:19043ms step_avg:257.34ms
step:85/1480 train_time:19292ms step_avg:257.22ms
step:86/1480 train_time:19545ms step_avg:257.17ms
step:87/1480 train_time:19793ms step_avg:257.05ms
step:88/1480 train_time:20045ms step_avg:256.99ms
step:89/1480 train_time:20294ms step_avg:256.88ms
step:90/1480 train_time:20546ms step_avg:256.82ms
step:91/1480 train_time:20794ms step_avg:256.72ms
step:92/1480 train_time:21045ms step_avg:256.65ms
step:93/1480 train_time:21294ms step_avg:256.55ms
step:94/1480 train_time:21545ms step_avg:256.49ms
step:95/1480 train_time:21795ms step_avg:256.41ms
step:96/1480 train_time:22046ms step_avg:256.34ms
step:97/1480 train_time:22294ms step_avg:256.26ms
step:98/1480 train_time:22546ms step_avg:256.21ms
step:99/1480 train_time:22795ms step_avg:256.12ms
step:100/1480 train_time:23046ms step_avg:256.07ms
step:101/1480 train_time:23295ms step_avg:255.99ms
step:102/1480 train_time:23546ms step_avg:255.94ms
step:103/1480 train_time:23795ms step_avg:255.86ms
step:104/1480 train_time:24046ms step_avg:255.81ms
step:105/1480 train_time:24296ms step_avg:255.74ms
step:106/1480 train_time:24546ms step_avg:255.69ms
step:107/1480 train_time:24795ms step_avg:255.62ms
step:108/1480 train_time:25046ms step_avg:255.57ms
step:109/1480 train_time:25296ms step_avg:255.52ms
step:110/1480 train_time:25545ms step_avg:255.45ms
step:111/1480 train_time:25797ms step_avg:255.42ms
step:112/1480 train_time:26052ms step_avg:255.41ms
step:113/1480 train_time:26306ms step_avg:255.40ms
step:114/1480 train_time:26561ms step_avg:255.40ms
step:115/1480 train_time:26814ms step_avg:255.37ms
step:116/1480 train_time:27070ms step_avg:255.38ms
step:117/1480 train_time:27328ms step_avg:255.40ms
step:118/1480 train_time:27582ms step_avg:255.39ms
step:119/1480 train_time:27836ms step_avg:255.37ms
step:120/1480 train_time:28095ms step_avg:255.41ms
step:121/1480 train_time:28351ms step_avg:255.41ms
step:122/1480 train_time:28606ms step_avg:255.41ms
step:123/1480 train_time:28862ms step_avg:255.41ms
step:124/1480 train_time:29117ms step_avg:255.41ms
step:125/1480 train_time:29372ms step_avg:255.41ms
step:125/1480 val_loss:4.4388 train_time:29503ms step_avg:256.55ms
step:126/1480 train_time:29632ms step_avg:255.45ms
step:127/1480 train_time:29892ms step_avg:255.49ms
step:128/1480 train_time:30144ms step_avg:255.46ms
step:129/1480 train_time:30400ms step_avg:255.47ms
step:130/1480 train_time:30655ms step_avg:255.46ms
step:131/1480 train_time:30911ms step_avg:255.46ms
step:132/1480 train_time:31165ms step_avg:255.45ms
step:133/1480 train_time:31418ms step_avg:255.43ms
step:134/1480 train_time:31675ms step_avg:255.45ms
step:135/1480 train_time:31933ms step_avg:255.47ms
step:136/1480 train_time:32187ms step_avg:255.46ms
step:137/1480 train_time:32442ms step_avg:255.45ms
step:138/1480 train_time:32699ms step_avg:255.46ms
step:139/1480 train_time:32958ms step_avg:255.49ms
step:140/1480 train_time:33212ms step_avg:255.48ms
step:141/1480 train_time:33467ms step_avg:255.47ms
step:142/1480 train_time:33724ms step_avg:255.48ms
step:143/1480 train_time:33979ms step_avg:255.48ms
step:144/1480 train_time:34237ms step_avg:255.50ms
step:145/1480 train_time:34499ms step_avg:255.55ms
step:146/1480 train_time:34756ms step_avg:255.56ms
step:147/1480 train_time:35012ms step_avg:255.56ms
step:148/1480 train_time:35267ms step_avg:255.56ms
step:149/1480 train_time:35523ms step_avg:255.56ms
step:150/1480 train_time:35780ms step_avg:255.57ms
step:151/1480 train_time:36034ms step_avg:255.56ms
step:152/1480 train_time:36289ms step_avg:255.55ms
step:153/1480 train_time:36543ms step_avg:255.55ms
step:154/1480 train_time:36797ms step_avg:255.54ms
step:155/1480 train_time:37053ms step_avg:255.53ms
step:156/1480 train_time:37308ms step_avg:255.53ms
step:157/1480 train_time:37563ms step_avg:255.53ms
step:158/1480 train_time:37818ms step_avg:255.52ms
step:159/1480 train_time:38073ms step_avg:255.52ms
step:160/1480 train_time:38329ms step_avg:255.53ms
step:161/1480 train_time:38583ms step_avg:255.52ms
step:162/1480 train_time:38838ms step_avg:255.51ms
step:163/1480 train_time:39093ms step_avg:255.51ms
step:164/1480 train_time:39347ms step_avg:255.50ms
step:165/1480 train_time:39603ms step_avg:255.50ms
step:166/1480 train_time:39857ms step_avg:255.49ms
step:167/1480 train_time:40111ms step_avg:255.48ms
step:168/1480 train_time:40365ms step_avg:255.48ms
step:169/1480 train_time:40619ms step_avg:255.46ms
step:170/1480 train_time:40874ms step_avg:255.46ms
step:171/1480 train_time:41130ms step_avg:255.46ms
step:172/1480 train_time:41385ms step_avg:255.46ms
step:173/1480 train_time:41639ms step_avg:255.45ms
step:174/1480 train_time:41899ms step_avg:255.48ms
step:175/1480 train_time:42156ms step_avg:255.49ms
step:176/1480 train_time:42410ms step_avg:255.48ms
step:177/1480 train_time:42665ms step_avg:255.48ms
step:178/1480 train_time:42918ms step_avg:255.47ms
step:179/1480 train_time:43174ms step_avg:255.47ms
step:180/1480 train_time:43429ms step_avg:255.46ms
step:181/1480 train_time:43683ms step_avg:255.45ms
step:182/1480 train_time:43938ms step_avg:255.45ms
step:183/1480 train_time:44196ms step_avg:255.47ms
step:184/1480 train_time:44449ms step_avg:255.45ms
step:185/1480 train_time:44705ms step_avg:255.45ms
step:186/1480 train_time:44958ms step_avg:255.44ms
step:187/1480 train_time:45212ms step_avg:255.44ms
step:188/1480 train_time:45467ms step_avg:255.43ms
step:189/1480 train_time:45720ms step_avg:255.42ms
step:190/1480 train_time:45975ms step_avg:255.42ms
step:191/1480 train_time:46231ms step_avg:255.42ms
step:192/1480 train_time:46486ms step_avg:255.42ms
step:193/1480 train_time:46739ms step_avg:255.41ms
step:194/1480 train_time:46999ms step_avg:255.43ms
step:195/1480 train_time:47254ms step_avg:255.43ms
step:196/1480 train_time:47510ms step_avg:255.43ms
step:197/1480 train_time:47765ms step_avg:255.43ms
step:198/1480 train_time:48020ms step_avg:255.43ms
step:199/1480 train_time:48275ms step_avg:255.42ms
step:200/1480 train_time:48533ms step_avg:255.44ms
step:201/1480 train_time:48788ms step_avg:255.43ms
step:202/1480 train_time:49041ms step_avg:255.42ms
step:203/1480 train_time:49302ms step_avg:255.45ms
step:204/1480 train_time:49556ms step_avg:255.45ms
step:205/1480 train_time:49811ms step_avg:255.44ms
step:206/1480 train_time:50065ms step_avg:255.43ms
step:207/1480 train_time:50318ms step_avg:255.42ms
step:208/1480 train_time:50574ms step_avg:255.42ms
step:209/1480 train_time:50831ms step_avg:255.43ms
step:210/1480 train_time:51086ms step_avg:255.43ms
step:211/1480 train_time:51340ms step_avg:255.42ms
step:212/1480 train_time:51598ms step_avg:255.44ms
step:213/1480 train_time:51855ms step_avg:255.44ms
step:214/1480 train_time:52109ms step_avg:255.44ms
step:215/1480 train_time:52363ms step_avg:255.43ms
step:216/1480 train_time:52616ms step_avg:255.42ms
step:217/1480 train_time:52872ms step_avg:255.42ms
step:218/1480 train_time:53129ms step_avg:255.43ms
step:219/1480 train_time:53384ms step_avg:255.42ms
step:220/1480 train_time:53638ms step_avg:255.42ms
step:221/1480 train_time:53899ms step_avg:255.44ms
step:222/1480 train_time:54158ms step_avg:255.46ms
step:223/1480 train_time:54416ms step_avg:255.47ms
step:224/1480 train_time:54674ms step_avg:255.48ms
step:225/1480 train_time:54932ms step_avg:255.50ms
step:226/1480 train_time:55192ms step_avg:255.52ms
step:227/1480 train_time:55449ms step_avg:255.53ms
step:228/1480 train_time:55711ms step_avg:255.56ms
step:229/1480 train_time:55970ms step_avg:255.57ms
step:230/1480 train_time:56230ms step_avg:255.59ms
step:231/1480 train_time:56490ms step_avg:255.61ms
step:232/1480 train_time:56749ms step_avg:255.62ms
step:233/1480 train_time:57009ms step_avg:255.65ms
step:234/1480 train_time:57268ms step_avg:255.66ms
step:235/1480 train_time:57528ms step_avg:255.68ms
step:236/1480 train_time:57789ms step_avg:255.70ms
step:237/1480 train_time:58047ms step_avg:255.71ms
step:238/1480 train_time:58309ms step_avg:255.74ms
step:239/1480 train_time:58569ms step_avg:255.76ms
step:240/1480 train_time:58831ms step_avg:255.79ms
step:241/1480 train_time:59090ms step_avg:255.80ms
step:242/1480 train_time:59349ms step_avg:255.81ms
step:243/1480 train_time:59609ms step_avg:255.83ms
step:244/1480 train_time:59869ms step_avg:255.85ms
step:245/1480 train_time:60131ms step_avg:255.88ms
step:246/1480 train_time:60391ms step_avg:255.89ms
step:247/1480 train_time:60649ms step_avg:255.90ms
step:248/1480 train_time:60910ms step_avg:255.93ms
step:249/1480 train_time:61169ms step_avg:255.94ms
step:250/1480 train_time:61430ms step_avg:255.96ms
step:250/1480 val_loss:3.9939 train_time:61559ms step_avg:256.49ms
step:251/1480 train_time:61689ms step_avg:255.97ms
step:252/1480 train_time:61958ms step_avg:256.03ms
step:253/1480 train_time:62220ms step_avg:256.05ms
step:254/1480 train_time:62482ms step_avg:256.07ms
step:255/1480 train_time:62739ms step_avg:256.08ms
step:256/1480 train_time:63001ms step_avg:256.10ms
step:257/1480 train_time:63260ms step_avg:256.11ms
step:258/1480 train_time:63522ms step_avg:256.14ms
step:259/1480 train_time:63781ms step_avg:256.15ms
step:260/1480 train_time:64038ms step_avg:256.15ms
step:261/1480 train_time:64298ms step_avg:256.17ms
step:262/1480 train_time:64557ms step_avg:256.18ms
step:263/1480 train_time:64818ms step_avg:256.20ms
step:264/1480 train_time:65081ms step_avg:256.23ms
step:265/1480 train_time:65339ms step_avg:256.23ms
step:266/1480 train_time:65600ms step_avg:256.25ms
step:267/1480 train_time:65858ms step_avg:256.26ms
step:268/1480 train_time:66119ms step_avg:256.27ms
step:269/1480 train_time:66381ms step_avg:256.30ms
step:270/1480 train_time:66639ms step_avg:256.30ms
step:271/1480 train_time:66897ms step_avg:256.31ms
step:272/1480 train_time:67156ms step_avg:256.32ms
step:273/1480 train_time:67415ms step_avg:256.33ms
step:274/1480 train_time:67675ms step_avg:256.34ms
step:275/1480 train_time:67933ms step_avg:256.35ms
step:276/1480 train_time:68196ms step_avg:256.38ms
step:277/1480 train_time:68455ms step_avg:256.38ms
step:278/1480 train_time:68714ms step_avg:256.39ms
step:279/1480 train_time:68973ms step_avg:256.40ms
step:280/1480 train_time:69234ms step_avg:256.42ms
step:281/1480 train_time:69496ms step_avg:256.44ms
step:282/1480 train_time:69756ms step_avg:256.46ms
step:283/1480 train_time:70016ms step_avg:256.47ms
step:284/1480 train_time:70277ms step_avg:256.49ms
step:285/1480 train_time:70536ms step_avg:256.50ms
step:286/1480 train_time:70795ms step_avg:256.50ms
step:287/1480 train_time:71057ms step_avg:256.52ms
step:288/1480 train_time:71317ms step_avg:256.53ms
step:289/1480 train_time:71578ms step_avg:256.55ms
step:290/1480 train_time:71836ms step_avg:256.56ms
step:291/1480 train_time:72096ms step_avg:256.57ms
step:292/1480 train_time:72357ms step_avg:256.58ms
step:293/1480 train_time:72618ms step_avg:256.60ms
step:294/1480 train_time:72880ms step_avg:256.62ms
step:295/1480 train_time:73140ms step_avg:256.63ms
step:296/1480 train_time:73400ms step_avg:256.64ms
step:297/1480 train_time:73659ms step_avg:256.65ms
step:298/1480 train_time:73920ms step_avg:256.67ms
step:299/1480 train_time:74181ms step_avg:256.68ms
step:300/1480 train_time:74440ms step_avg:256.69ms
step:301/1480 train_time:74700ms step_avg:256.70ms
step:302/1480 train_time:74960ms step_avg:256.71ms
step:303/1480 train_time:75222ms step_avg:256.73ms
step:304/1480 train_time:75481ms step_avg:256.74ms
step:305/1480 train_time:75739ms step_avg:256.74ms
step:306/1480 train_time:76000ms step_avg:256.76ms
step:307/1480 train_time:76259ms step_avg:256.76ms
step:308/1480 train_time:76520ms step_avg:256.78ms
step:309/1480 train_time:76782ms step_avg:256.80ms
step:310/1480 train_time:77041ms step_avg:256.80ms
step:311/1480 train_time:77301ms step_avg:256.81ms
step:312/1480 train_time:77560ms step_avg:256.82ms
step:313/1480 train_time:77822ms step_avg:256.84ms
step:314/1480 train_time:78081ms step_avg:256.85ms
step:315/1480 train_time:78340ms step_avg:256.85ms
step:316/1480 train_time:78600ms step_avg:256.86ms
step:317/1480 train_time:78858ms step_avg:256.87ms
step:318/1480 train_time:79118ms step_avg:256.88ms
step:319/1480 train_time:79378ms step_avg:256.89ms
step:320/1480 train_time:79637ms step_avg:256.89ms
step:321/1480 train_time:79896ms step_avg:256.90ms
step:322/1480 train_time:80157ms step_avg:256.91ms
step:323/1480 train_time:80418ms step_avg:256.93ms
step:324/1480 train_time:80682ms step_avg:256.95ms
step:325/1480 train_time:80939ms step_avg:256.95ms
step:326/1480 train_time:81201ms step_avg:256.97ms
step:327/1480 train_time:81459ms step_avg:256.97ms
step:328/1480 train_time:81720ms step_avg:256.98ms
step:329/1480 train_time:81982ms step_avg:257.00ms
step:330/1480 train_time:82242ms step_avg:257.01ms
step:331/1480 train_time:82504ms step_avg:257.02ms
step:332/1480 train_time:82764ms step_avg:257.03ms
step:333/1480 train_time:83025ms step_avg:257.04ms
step:334/1480 train_time:83286ms step_avg:257.06ms
step:335/1480 train_time:83546ms step_avg:257.07ms
step:336/1480 train_time:83806ms step_avg:257.07ms
step:337/1480 train_time:84069ms step_avg:257.09ms
step:338/1480 train_time:84334ms step_avg:257.12ms
step:339/1480 train_time:84602ms step_avg:257.15ms
step:340/1480 train_time:84864ms step_avg:257.16ms
step:341/1480 train_time:85125ms step_avg:257.17ms
step:342/1480 train_time:85385ms step_avg:257.18ms
step:343/1480 train_time:85646ms step_avg:257.19ms
step:344/1480 train_time:85906ms step_avg:257.20ms
step:345/1480 train_time:86167ms step_avg:257.21ms
step:346/1480 train_time:86428ms step_avg:257.23ms
step:347/1480 train_time:86690ms step_avg:257.24ms
step:348/1480 train_time:86951ms step_avg:257.25ms
step:349/1480 train_time:87216ms step_avg:257.27ms
step:350/1480 train_time:87479ms step_avg:257.29ms
step:351/1480 train_time:87742ms step_avg:257.31ms
step:352/1480 train_time:88004ms step_avg:257.32ms
step:353/1480 train_time:88265ms step_avg:257.33ms
step:354/1480 train_time:88526ms step_avg:257.34ms
step:355/1480 train_time:88787ms step_avg:257.35ms
step:356/1480 train_time:89047ms step_avg:257.36ms
step:357/1480 train_time:89309ms step_avg:257.37ms
step:358/1480 train_time:89569ms step_avg:257.38ms
step:359/1480 train_time:89834ms step_avg:257.40ms
step:360/1480 train_time:90102ms step_avg:257.44ms
step:361/1480 train_time:90364ms step_avg:257.45ms
step:362/1480 train_time:90625ms step_avg:257.46ms
step:363/1480 train_time:90887ms step_avg:257.47ms
step:364/1480 train_time:91148ms step_avg:257.48ms
step:365/1480 train_time:91409ms step_avg:257.49ms
step:366/1480 train_time:91672ms step_avg:257.51ms
step:367/1480 train_time:91936ms step_avg:257.52ms
step:368/1480 train_time:92202ms step_avg:257.55ms
step:369/1480 train_time:92464ms step_avg:257.56ms
step:370/1480 train_time:92726ms step_avg:257.57ms
step:371/1480 train_time:92987ms step_avg:257.58ms
step:372/1480 train_time:93247ms step_avg:257.59ms
step:373/1480 train_time:93507ms step_avg:257.60ms
step:374/1480 train_time:93768ms step_avg:257.60ms
step:375/1480 train_time:94031ms step_avg:257.62ms
step:375/1480 val_loss:3.8118 train_time:94170ms step_avg:258.00ms
step:376/1480 train_time:94302ms step_avg:257.66ms
step:377/1480 train_time:94570ms step_avg:257.68ms
step:378/1480 train_time:94836ms step_avg:257.71ms
step:379/1480 train_time:95099ms step_avg:257.72ms
step:380/1480 train_time:95363ms step_avg:257.74ms
step:381/1480 train_time:95627ms step_avg:257.75ms
step:382/1480 train_time:95887ms step_avg:257.76ms
step:383/1480 train_time:96148ms step_avg:257.77ms
step:384/1480 train_time:96410ms step_avg:257.78ms
step:385/1480 train_time:96673ms step_avg:257.79ms
step:386/1480 train_time:96937ms step_avg:257.81ms
step:387/1480 train_time:97201ms step_avg:257.83ms
step:388/1480 train_time:97464ms step_avg:257.84ms
step:389/1480 train_time:97725ms step_avg:257.85ms
step:390/1480 train_time:97986ms step_avg:257.86ms
step:391/1480 train_time:98248ms step_avg:257.87ms
step:392/1480 train_time:98508ms step_avg:257.87ms
step:393/1480 train_time:98768ms step_avg:257.88ms
step:394/1480 train_time:99030ms step_avg:257.89ms
step:395/1480 train_time:99291ms step_avg:257.90ms
step:396/1480 train_time:99560ms step_avg:257.93ms
step:397/1480 train_time:99823ms step_avg:257.94ms
step:398/1480 train_time:100086ms step_avg:257.95ms
step:399/1480 train_time:100347ms step_avg:257.96ms
step:400/1480 train_time:100606ms step_avg:257.96ms
step:401/1480 train_time:100866ms step_avg:257.97ms
step:402/1480 train_time:101129ms step_avg:257.98ms
step:403/1480 train_time:101395ms step_avg:258.00ms
step:404/1480 train_time:101662ms step_avg:258.02ms
step:405/1480 train_time:101924ms step_avg:258.04ms
step:406/1480 train_time:102186ms step_avg:258.04ms
step:407/1480 train_time:102447ms step_avg:258.05ms
step:408/1480 train_time:102706ms step_avg:258.06ms
step:409/1480 train_time:102966ms step_avg:258.06ms
step:410/1480 train_time:103229ms step_avg:258.07ms
step:411/1480 train_time:103492ms step_avg:258.08ms
step:412/1480 train_time:103761ms step_avg:258.11ms
step:413/1480 train_time:104024ms step_avg:258.13ms
step:414/1480 train_time:104287ms step_avg:258.14ms
step:415/1480 train_time:104547ms step_avg:258.14ms
step:416/1480 train_time:104808ms step_avg:258.15ms
step:417/1480 train_time:105069ms step_avg:258.15ms
step:418/1480 train_time:105330ms step_avg:258.16ms
step:419/1480 train_time:105594ms step_avg:258.18ms
step:420/1480 train_time:105862ms step_avg:258.20ms
step:421/1480 train_time:106125ms step_avg:258.21ms
step:422/1480 train_time:106386ms step_avg:258.22ms
step:423/1480 train_time:106648ms step_avg:258.23ms
step:424/1480 train_time:106908ms step_avg:258.23ms
step:425/1480 train_time:107168ms step_avg:258.24ms
step:426/1480 train_time:107430ms step_avg:258.24ms
step:427/1480 train_time:107695ms step_avg:258.26ms
step:428/1480 train_time:107962ms step_avg:258.28ms
step:429/1480 train_time:108225ms step_avg:258.29ms
step:430/1480 train_time:108488ms step_avg:258.30ms
step:431/1480 train_time:108750ms step_avg:258.31ms
step:432/1480 train_time:109011ms step_avg:258.32ms
step:433/1480 train_time:109275ms step_avg:258.33ms
step:434/1480 train_time:109541ms step_avg:258.35ms
step:435/1480 train_time:109803ms step_avg:258.36ms
step:436/1480 train_time:110067ms step_avg:258.37ms
step:437/1480 train_time:110327ms step_avg:258.38ms
step:438/1480 train_time:110588ms step_avg:258.38ms
step:439/1480 train_time:110849ms step_avg:258.39ms
step:440/1480 train_time:111113ms step_avg:258.40ms
step:441/1480 train_time:111385ms step_avg:258.43ms
step:442/1480 train_time:111652ms step_avg:258.45ms
step:443/1480 train_time:111918ms step_avg:258.47ms
step:444/1480 train_time:112185ms step_avg:258.49ms
step:445/1480 train_time:112449ms step_avg:258.50ms
step:446/1480 train_time:112713ms step_avg:258.52ms
step:447/1480 train_time:112980ms step_avg:258.53ms
step:448/1480 train_time:113247ms step_avg:258.55ms
step:449/1480 train_time:113510ms step_avg:258.57ms
step:450/1480 train_time:113775ms step_avg:258.58ms
step:451/1480 train_time:114044ms step_avg:258.60ms
step:452/1480 train_time:114309ms step_avg:258.62ms
step:453/1480 train_time:114573ms step_avg:258.63ms
step:454/1480 train_time:114842ms step_avg:258.65ms
step:455/1480 train_time:115107ms step_avg:258.67ms
step:456/1480 train_time:115369ms step_avg:258.68ms
step:457/1480 train_time:115637ms step_avg:258.70ms
step:458/1480 train_time:115904ms step_avg:258.71ms
step:459/1480 train_time:116169ms step_avg:258.73ms
step:460/1480 train_time:116432ms step_avg:258.74ms
step:461/1480 train_time:116704ms step_avg:258.77ms
step:462/1480 train_time:116968ms step_avg:258.78ms
step:463/1480 train_time:117230ms step_avg:258.79ms
step:464/1480 train_time:117498ms step_avg:258.81ms
step:465/1480 train_time:117765ms step_avg:258.82ms
step:466/1480 train_time:118030ms step_avg:258.84ms
step:467/1480 train_time:118297ms step_avg:258.86ms
step:468/1480 train_time:118565ms step_avg:258.88ms
step:469/1480 train_time:118828ms step_avg:258.88ms
step:470/1480 train_time:119095ms step_avg:258.90ms
step:471/1480 train_time:119365ms step_avg:258.93ms
step:472/1480 train_time:119628ms step_avg:258.94ms
step:473/1480 train_time:119894ms step_avg:258.95ms
step:474/1480 train_time:120164ms step_avg:258.97ms
step:475/1480 train_time:120428ms step_avg:258.99ms
step:476/1480 train_time:120693ms step_avg:259.00ms
step:477/1480 train_time:120961ms step_avg:259.02ms
step:478/1480 train_time:121226ms step_avg:259.03ms
step:479/1480 train_time:121489ms step_avg:259.04ms
step:480/1480 train_time:121756ms step_avg:259.05ms
step:481/1480 train_time:122026ms step_avg:259.08ms
step:482/1480 train_time:122290ms step_avg:259.09ms
step:483/1480 train_time:122553ms step_avg:259.10ms
step:484/1480 train_time:122824ms step_avg:259.12ms
step:485/1480 train_time:123089ms step_avg:259.13ms
step:486/1480 train_time:123354ms step_avg:259.15ms
step:487/1480 train_time:123622ms step_avg:259.16ms
step:488/1480 train_time:123888ms step_avg:259.18ms
step:489/1480 train_time:124154ms step_avg:259.19ms
step:490/1480 train_time:124422ms step_avg:259.21ms
step:491/1480 train_time:124689ms step_avg:259.23ms
step:492/1480 train_time:124951ms step_avg:259.23ms
step:493/1480 train_time:125218ms step_avg:259.25ms
step:494/1480 train_time:125486ms step_avg:259.27ms
step:495/1480 train_time:125752ms step_avg:259.28ms
step:496/1480 train_time:126020ms step_avg:259.30ms
step:497/1480 train_time:126286ms step_avg:259.31ms
step:498/1480 train_time:126553ms step_avg:259.33ms
step:499/1480 train_time:126819ms step_avg:259.34ms
step:500/1480 train_time:127086ms step_avg:259.36ms
step:500/1480 val_loss:3.6871 train_time:127220ms step_avg:259.63ms
step:501/1480 train_time:127352ms step_avg:259.37ms
step:502/1480 train_time:127626ms step_avg:259.40ms
step:503/1480 train_time:127890ms step_avg:259.41ms
step:504/1480 train_time:128153ms step_avg:259.42ms
step:505/1480 train_time:128421ms step_avg:259.44ms
step:506/1480 train_time:128691ms step_avg:259.46ms
step:507/1480 train_time:128954ms step_avg:259.46ms
step:508/1480 train_time:129223ms step_avg:259.48ms
step:509/1480 train_time:129490ms step_avg:259.50ms
step:510/1480 train_time:129751ms step_avg:259.50ms
step:511/1480 train_time:130015ms step_avg:259.51ms
step:512/1480 train_time:130287ms step_avg:259.54ms
step:513/1480 train_time:130550ms step_avg:259.54ms
step:514/1480 train_time:130817ms step_avg:259.56ms
step:515/1480 train_time:131084ms step_avg:259.57ms
step:516/1480 train_time:131349ms step_avg:259.58ms
step:517/1480 train_time:131612ms step_avg:259.59ms
step:518/1480 train_time:131877ms step_avg:259.60ms
step:519/1480 train_time:132143ms step_avg:259.61ms
step:520/1480 train_time:132409ms step_avg:259.63ms
step:521/1480 train_time:132675ms step_avg:259.64ms
step:522/1480 train_time:132944ms step_avg:259.66ms
step:523/1480 train_time:133209ms step_avg:259.67ms
step:524/1480 train_time:133472ms step_avg:259.67ms
step:525/1480 train_time:133739ms step_avg:259.69ms
step:526/1480 train_time:134006ms step_avg:259.70ms
step:527/1480 train_time:134270ms step_avg:259.71ms
step:528/1480 train_time:134536ms step_avg:259.72ms
step:529/1480 train_time:134805ms step_avg:259.74ms
step:530/1480 train_time:135071ms step_avg:259.75ms
step:531/1480 train_time:135339ms step_avg:259.77ms
step:532/1480 train_time:135608ms step_avg:259.79ms
step:533/1480 train_time:135870ms step_avg:259.79ms
step:534/1480 train_time:136137ms step_avg:259.80ms
step:535/1480 train_time:136405ms step_avg:259.82ms
step:536/1480 train_time:136670ms step_avg:259.83ms
step:537/1480 train_time:136937ms step_avg:259.84ms
step:538/1480 train_time:137205ms step_avg:259.86ms
step:539/1480 train_time:137471ms step_avg:259.87ms
step:540/1480 train_time:137738ms step_avg:259.88ms
step:541/1480 train_time:138005ms step_avg:259.90ms
step:542/1480 train_time:138270ms step_avg:259.91ms
step:543/1480 train_time:138535ms step_avg:259.92ms
step:544/1480 train_time:138806ms step_avg:259.94ms
step:545/1480 train_time:139070ms step_avg:259.94ms
step:546/1480 train_time:139337ms step_avg:259.96ms
step:547/1480 train_time:139606ms step_avg:259.97ms
step:548/1480 train_time:139872ms step_avg:259.99ms
step:549/1480 train_time:140137ms step_avg:260.00ms
step:550/1480 train_time:140408ms step_avg:260.02ms
step:551/1480 train_time:140674ms step_avg:260.03ms
step:552/1480 train_time:140946ms step_avg:260.05ms
step:553/1480 train_time:141213ms step_avg:260.06ms
step:554/1480 train_time:141486ms step_avg:260.08ms
step:555/1480 train_time:141755ms step_avg:260.10ms
step:556/1480 train_time:142027ms step_avg:260.12ms
step:557/1480 train_time:142294ms step_avg:260.14ms
step:558/1480 train_time:142562ms step_avg:260.15ms
step:559/1480 train_time:142829ms step_avg:260.16ms
step:560/1480 train_time:143096ms step_avg:260.17ms
step:561/1480 train_time:143367ms step_avg:260.19ms
step:562/1480 train_time:143636ms step_avg:260.21ms
step:563/1480 train_time:143909ms step_avg:260.23ms
step:564/1480 train_time:144173ms step_avg:260.24ms
step:565/1480 train_time:144441ms step_avg:260.25ms
step:566/1480 train_time:144710ms step_avg:260.27ms
step:567/1480 train_time:144975ms step_avg:260.28ms
step:568/1480 train_time:145247ms step_avg:260.30ms
step:569/1480 train_time:145512ms step_avg:260.31ms
step:570/1480 train_time:145780ms step_avg:260.32ms
step:571/1480 train_time:146049ms step_avg:260.34ms
step:572/1480 train_time:146315ms step_avg:260.35ms
step:573/1480 train_time:146585ms step_avg:260.36ms
step:574/1480 train_time:146855ms step_avg:260.38ms
step:575/1480 train_time:147127ms step_avg:260.40ms
step:576/1480 train_time:147393ms step_avg:260.41ms
step:577/1480 train_time:147663ms step_avg:260.43ms
step:578/1480 train_time:147932ms step_avg:260.44ms
step:579/1480 train_time:148204ms step_avg:260.46ms
step:580/1480 train_time:148472ms step_avg:260.48ms
step:581/1480 train_time:148741ms step_avg:260.49ms
step:582/1480 train_time:149010ms step_avg:260.51ms
step:583/1480 train_time:149276ms step_avg:260.52ms
step:584/1480 train_time:149549ms step_avg:260.54ms
step:585/1480 train_time:149814ms step_avg:260.55ms
step:586/1480 train_time:150088ms step_avg:260.57ms
step:587/1480 train_time:150353ms step_avg:260.58ms
step:588/1480 train_time:150623ms step_avg:260.59ms
step:589/1480 train_time:150892ms step_avg:260.61ms
step:590/1480 train_time:151160ms step_avg:260.62ms
step:591/1480 train_time:151430ms step_avg:260.64ms
step:592/1480 train_time:151697ms step_avg:260.65ms
step:593/1480 train_time:151969ms step_avg:260.67ms
step:594/1480 train_time:152237ms step_avg:260.68ms
step:595/1480 train_time:152510ms step_avg:260.70ms
step:596/1480 train_time:152777ms step_avg:260.71ms
step:597/1480 train_time:153047ms step_avg:260.73ms
step:598/1480 train_time:153312ms step_avg:260.73ms
step:599/1480 train_time:153585ms step_avg:260.75ms
step:600/1480 train_time:153850ms step_avg:260.76ms
step:601/1480 train_time:154120ms step_avg:260.78ms
step:602/1480 train_time:154391ms step_avg:260.80ms
step:603/1480 train_time:154658ms step_avg:260.81ms
step:604/1480 train_time:154928ms step_avg:260.82ms
step:605/1480 train_time:155195ms step_avg:260.83ms
step:606/1480 train_time:155467ms step_avg:260.85ms
step:607/1480 train_time:155739ms step_avg:260.87ms
step:608/1480 train_time:156009ms step_avg:260.89ms
step:609/1480 train_time:156275ms step_avg:260.89ms
step:610/1480 train_time:156545ms step_avg:260.91ms
step:611/1480 train_time:156813ms step_avg:260.92ms
step:612/1480 train_time:157085ms step_avg:260.94ms
step:613/1480 train_time:157355ms step_avg:260.95ms
step:614/1480 train_time:157626ms step_avg:260.97ms
step:615/1480 train_time:157892ms step_avg:260.98ms
step:616/1480 train_time:158159ms step_avg:260.99ms
step:617/1480 train_time:158431ms step_avg:261.01ms
step:618/1480 train_time:158696ms step_avg:261.01ms
step:619/1480 train_time:158967ms step_avg:261.03ms
step:620/1480 train_time:159236ms step_avg:261.04ms
step:621/1480 train_time:159510ms step_avg:261.06ms
step:622/1480 train_time:159777ms step_avg:261.07ms
step:623/1480 train_time:160048ms step_avg:261.09ms
step:624/1480 train_time:160315ms step_avg:261.10ms
step:625/1480 train_time:160588ms step_avg:261.12ms
step:625/1480 val_loss:3.6052 train_time:160723ms step_avg:261.34ms
step:626/1480 train_time:160859ms step_avg:261.13ms
step:627/1480 train_time:161135ms step_avg:261.16ms
step:628/1480 train_time:161402ms step_avg:261.17ms
step:629/1480 train_time:161672ms step_avg:261.18ms
step:630/1480 train_time:161939ms step_avg:261.19ms
step:631/1480 train_time:162203ms step_avg:261.20ms
step:632/1480 train_time:162473ms step_avg:261.21ms
step:633/1480 train_time:162739ms step_avg:261.22ms
step:634/1480 train_time:163005ms step_avg:261.23ms
step:635/1480 train_time:163275ms step_avg:261.24ms
step:636/1480 train_time:163542ms step_avg:261.25ms
step:637/1480 train_time:163812ms step_avg:261.26ms
step:638/1480 train_time:164078ms step_avg:261.27ms
step:639/1480 train_time:164344ms step_avg:261.28ms
step:640/1480 train_time:164619ms step_avg:261.30ms
step:641/1480 train_time:164885ms step_avg:261.31ms
step:642/1480 train_time:165158ms step_avg:261.33ms
step:643/1480 train_time:165425ms step_avg:261.34ms
step:644/1480 train_time:165694ms step_avg:261.35ms
step:645/1480 train_time:165961ms step_avg:261.36ms
step:646/1480 train_time:166231ms step_avg:261.37ms
step:647/1480 train_time:166498ms step_avg:261.38ms
step:648/1480 train_time:166767ms step_avg:261.39ms
step:649/1480 train_time:167038ms step_avg:261.41ms
step:650/1480 train_time:167304ms step_avg:261.41ms
step:651/1480 train_time:167575ms step_avg:261.43ms
step:652/1480 train_time:167844ms step_avg:261.44ms
step:653/1480 train_time:168116ms step_avg:261.46ms
step:654/1480 train_time:168383ms step_avg:261.46ms
step:655/1480 train_time:168652ms step_avg:261.48ms
step:656/1480 train_time:168924ms step_avg:261.49ms
step:657/1480 train_time:169194ms step_avg:261.50ms
step:658/1480 train_time:169462ms step_avg:261.51ms
step:659/1480 train_time:169730ms step_avg:261.53ms
step:660/1480 train_time:170002ms step_avg:261.54ms
step:661/1480 train_time:170275ms step_avg:261.56ms
step:662/1480 train_time:170542ms step_avg:261.57ms
step:663/1480 train_time:170815ms step_avg:261.59ms
step:664/1480 train_time:171086ms step_avg:261.60ms
step:665/1480 train_time:171359ms step_avg:261.62ms
step:666/1480 train_time:171628ms step_avg:261.63ms
step:667/1480 train_time:171901ms step_avg:261.65ms
step:668/1480 train_time:172170ms step_avg:261.66ms
step:669/1480 train_time:172441ms step_avg:261.67ms
step:670/1480 train_time:172710ms step_avg:261.68ms
step:671/1480 train_time:172979ms step_avg:261.69ms
step:672/1480 train_time:173252ms step_avg:261.71ms
step:673/1480 train_time:173524ms step_avg:261.73ms
step:674/1480 train_time:173796ms step_avg:261.74ms
step:675/1480 train_time:174067ms step_avg:261.76ms
step:676/1480 train_time:174339ms step_avg:261.77ms
step:677/1480 train_time:174609ms step_avg:261.78ms
step:678/1480 train_time:174882ms step_avg:261.80ms
step:679/1480 train_time:175153ms step_avg:261.81ms
step:680/1480 train_time:175424ms step_avg:261.83ms
step:681/1480 train_time:175695ms step_avg:261.84ms
step:682/1480 train_time:175967ms step_avg:261.86ms
step:683/1480 train_time:176240ms step_avg:261.87ms
step:684/1480 train_time:176509ms step_avg:261.88ms
step:685/1480 train_time:176780ms step_avg:261.90ms
step:686/1480 train_time:177051ms step_avg:261.91ms
step:687/1480 train_time:177321ms step_avg:261.92ms
step:688/1480 train_time:177591ms step_avg:261.93ms
step:689/1480 train_time:177860ms step_avg:261.94ms
step:690/1480 train_time:178131ms step_avg:261.96ms
step:691/1480 train_time:178402ms step_avg:261.97ms
step:692/1480 train_time:178671ms step_avg:261.98ms
step:693/1480 train_time:178940ms step_avg:261.99ms
step:694/1480 train_time:179208ms step_avg:262.00ms
step:695/1480 train_time:179478ms step_avg:262.01ms
step:696/1480 train_time:179747ms step_avg:262.02ms
step:697/1480 train_time:180021ms step_avg:262.04ms
step:698/1480 train_time:180290ms step_avg:262.05ms
step:699/1480 train_time:180562ms step_avg:262.06ms
step:700/1480 train_time:180832ms step_avg:262.08ms
step:701/1480 train_time:181103ms step_avg:262.09ms
step:702/1480 train_time:181375ms step_avg:262.10ms
step:703/1480 train_time:181646ms step_avg:262.12ms
step:704/1480 train_time:181920ms step_avg:262.13ms
step:705/1480 train_time:182193ms step_avg:262.15ms
step:706/1480 train_time:182468ms step_avg:262.17ms
step:707/1480 train_time:182743ms step_avg:262.18ms
step:708/1480 train_time:183018ms step_avg:262.20ms
step:709/1480 train_time:183287ms step_avg:262.21ms
step:710/1480 train_time:183558ms step_avg:262.23ms
step:711/1480 train_time:183828ms step_avg:262.24ms
step:712/1480 train_time:184102ms step_avg:262.25ms
step:713/1480 train_time:184380ms step_avg:262.28ms
step:714/1480 train_time:184649ms step_avg:262.29ms
step:715/1480 train_time:184921ms step_avg:262.30ms
step:716/1480 train_time:185189ms step_avg:262.31ms
step:717/1480 train_time:185461ms step_avg:262.32ms
step:718/1480 train_time:185729ms step_avg:262.33ms
step:719/1480 train_time:186001ms step_avg:262.34ms
step:720/1480 train_time:186272ms step_avg:262.35ms
step:721/1480 train_time:186540ms step_avg:262.36ms
step:722/1480 train_time:186810ms step_avg:262.37ms
step:723/1480 train_time:187079ms step_avg:262.38ms
step:724/1480 train_time:187349ms step_avg:262.39ms
step:725/1480 train_time:187622ms step_avg:262.41ms
step:726/1480 train_time:187895ms step_avg:262.42ms
step:727/1480 train_time:188169ms step_avg:262.44ms
step:728/1480 train_time:188442ms step_avg:262.45ms
step:729/1480 train_time:188714ms step_avg:262.47ms
step:730/1480 train_time:188988ms step_avg:262.48ms
step:731/1480 train_time:189259ms step_avg:262.50ms
step:732/1480 train_time:189527ms step_avg:262.50ms
step:733/1480 train_time:189799ms step_avg:262.52ms
step:734/1480 train_time:190070ms step_avg:262.53ms
step:735/1480 train_time:190340ms step_avg:262.54ms
step:736/1480 train_time:190610ms step_avg:262.55ms
step:737/1480 train_time:190880ms step_avg:262.56ms
step:738/1480 train_time:191149ms step_avg:262.57ms
step:739/1480 train_time:191420ms step_avg:262.58ms
step:740/1480 train_time:191691ms step_avg:262.59ms
step:741/1480 train_time:191962ms step_avg:262.60ms
step:742/1480 train_time:192234ms step_avg:262.61ms
step:743/1480 train_time:192504ms step_avg:262.62ms
step:744/1480 train_time:192777ms step_avg:262.64ms
step:745/1480 train_time:193050ms step_avg:262.65ms
step:746/1480 train_time:193320ms step_avg:262.66ms
step:747/1480 train_time:193592ms step_avg:262.68ms
step:748/1480 train_time:193866ms step_avg:262.69ms
step:749/1480 train_time:194138ms step_avg:262.70ms
step:750/1480 train_time:194406ms step_avg:262.71ms
step:750/1480 val_loss:3.5514 train_time:194546ms step_avg:262.90ms
step:751/1480 train_time:194680ms step_avg:262.73ms
step:752/1480 train_time:194953ms step_avg:262.74ms
step:753/1480 train_time:195225ms step_avg:262.75ms
step:754/1480 train_time:195493ms step_avg:262.76ms
step:755/1480 train_time:195768ms step_avg:262.78ms
step:756/1480 train_time:196038ms step_avg:262.78ms
step:757/1480 train_time:196309ms step_avg:262.80ms
step:758/1480 train_time:196577ms step_avg:262.80ms
step:759/1480 train_time:196851ms step_avg:262.82ms
step:760/1480 train_time:197121ms step_avg:262.83ms
step:761/1480 train_time:197395ms step_avg:262.84ms
step:762/1480 train_time:197668ms step_avg:262.86ms
step:763/1480 train_time:197937ms step_avg:262.86ms
step:764/1480 train_time:198211ms step_avg:262.88ms
step:765/1480 train_time:198480ms step_avg:262.89ms
step:766/1480 train_time:198752ms step_avg:262.90ms
step:767/1480 train_time:199021ms step_avg:262.91ms
step:768/1480 train_time:199292ms step_avg:262.92ms
step:769/1480 train_time:199569ms step_avg:262.94ms
step:770/1480 train_time:199841ms step_avg:262.95ms
step:771/1480 train_time:200114ms step_avg:262.96ms
step:772/1480 train_time:200388ms step_avg:262.98ms
step:773/1480 train_time:200659ms step_avg:262.99ms
step:774/1480 train_time:200932ms step_avg:263.00ms
step:775/1480 train_time:201202ms step_avg:263.01ms
step:776/1480 train_time:201475ms step_avg:263.02ms
step:777/1480 train_time:201749ms step_avg:263.04ms
step:778/1480 train_time:202020ms step_avg:263.05ms
step:779/1480 train_time:202293ms step_avg:263.06ms
step:780/1480 train_time:202569ms step_avg:263.08ms
step:781/1480 train_time:202841ms step_avg:263.09ms
step:782/1480 train_time:203113ms step_avg:263.10ms
step:783/1480 train_time:203386ms step_avg:263.11ms
step:784/1480 train_time:203659ms step_avg:263.13ms
step:785/1480 train_time:203931ms step_avg:263.14ms
step:786/1480 train_time:204205ms step_avg:263.15ms
step:787/1480 train_time:204478ms step_avg:263.16ms
step:788/1480 train_time:204753ms step_avg:263.18ms
step:789/1480 train_time:205031ms step_avg:263.20ms
step:790/1480 train_time:205302ms step_avg:263.21ms
step:791/1480 train_time:205580ms step_avg:263.23ms
step:792/1480 train_time:205856ms step_avg:263.24ms
step:793/1480 train_time:206130ms step_avg:263.26ms
step:794/1480 train_time:206403ms step_avg:263.27ms
step:795/1480 train_time:206677ms step_avg:263.28ms
step:796/1480 train_time:206953ms step_avg:263.30ms
step:797/1480 train_time:207226ms step_avg:263.31ms
step:798/1480 train_time:207501ms step_avg:263.33ms
step:799/1480 train_time:207777ms step_avg:263.34ms
step:800/1480 train_time:208051ms step_avg:263.36ms
step:801/1480 train_time:208324ms step_avg:263.37ms
step:802/1480 train_time:208604ms step_avg:263.39ms
step:803/1480 train_time:208876ms step_avg:263.40ms
step:804/1480 train_time:209153ms step_avg:263.42ms
step:805/1480 train_time:209428ms step_avg:263.43ms
step:806/1480 train_time:209698ms step_avg:263.44ms
step:807/1480 train_time:209971ms step_avg:263.45ms
step:808/1480 train_time:210244ms step_avg:263.46ms
step:809/1480 train_time:210517ms step_avg:263.48ms
step:810/1480 train_time:210789ms step_avg:263.49ms
step:811/1480 train_time:211061ms step_avg:263.50ms
step:812/1480 train_time:211334ms step_avg:263.51ms
step:813/1480 train_time:211607ms step_avg:263.52ms
step:814/1480 train_time:211878ms step_avg:263.53ms
step:815/1480 train_time:212152ms step_avg:263.54ms
step:816/1480 train_time:212429ms step_avg:263.56ms
step:817/1480 train_time:212700ms step_avg:263.57ms
step:818/1480 train_time:212971ms step_avg:263.58ms
step:819/1480 train_time:213242ms step_avg:263.59ms
step:820/1480 train_time:213515ms step_avg:263.60ms
step:821/1480 train_time:213787ms step_avg:263.61ms
step:822/1480 train_time:214060ms step_avg:263.62ms
step:823/1480 train_time:214334ms step_avg:263.63ms
step:824/1480 train_time:214605ms step_avg:263.64ms
step:825/1480 train_time:214880ms step_avg:263.66ms
step:826/1480 train_time:215153ms step_avg:263.67ms
step:827/1480 train_time:215426ms step_avg:263.68ms
step:828/1480 train_time:215701ms step_avg:263.69ms
step:829/1480 train_time:215975ms step_avg:263.71ms
step:830/1480 train_time:216252ms step_avg:263.72ms
step:831/1480 train_time:216526ms step_avg:263.73ms
step:832/1480 train_time:216800ms step_avg:263.75ms
step:833/1480 train_time:217077ms step_avg:263.76ms
step:834/1480 train_time:217351ms step_avg:263.78ms
step:835/1480 train_time:217624ms step_avg:263.79ms
step:836/1480 train_time:217901ms step_avg:263.80ms
step:837/1480 train_time:218174ms step_avg:263.81ms
step:838/1480 train_time:218446ms step_avg:263.82ms
step:839/1480 train_time:218717ms step_avg:263.83ms
step:840/1480 train_time:218991ms step_avg:263.84ms
step:841/1480 train_time:219263ms step_avg:263.85ms
step:842/1480 train_time:219538ms step_avg:263.87ms
step:843/1480 train_time:219814ms step_avg:263.88ms
step:844/1480 train_time:220089ms step_avg:263.90ms
step:845/1480 train_time:220364ms step_avg:263.91ms
step:846/1480 train_time:220637ms step_avg:263.92ms
step:847/1480 train_time:220912ms step_avg:263.93ms
step:848/1480 train_time:221183ms step_avg:263.94ms
step:849/1480 train_time:221456ms step_avg:263.95ms
step:850/1480 train_time:221730ms step_avg:263.96ms
step:851/1480 train_time:222002ms step_avg:263.97ms
step:852/1480 train_time:222274ms step_avg:263.98ms
step:853/1480 train_time:222545ms step_avg:263.99ms
step:854/1480 train_time:222816ms step_avg:264.00ms
step:855/1480 train_time:223093ms step_avg:264.02ms
step:856/1480 train_time:223368ms step_avg:264.03ms
step:857/1480 train_time:223645ms step_avg:264.04ms
step:858/1480 train_time:223922ms step_avg:264.06ms
step:859/1480 train_time:224193ms step_avg:264.07ms
step:860/1480 train_time:224466ms step_avg:264.08ms
step:861/1480 train_time:224742ms step_avg:264.09ms
step:862/1480 train_time:225020ms step_avg:264.11ms
step:863/1480 train_time:225298ms step_avg:264.12ms
step:864/1480 train_time:225573ms step_avg:264.14ms
step:865/1480 train_time:225842ms step_avg:264.14ms
step:866/1480 train_time:226119ms step_avg:264.16ms
step:867/1480 train_time:226391ms step_avg:264.17ms
step:868/1480 train_time:226661ms step_avg:264.17ms
step:869/1480 train_time:226933ms step_avg:264.18ms
step:870/1480 train_time:227206ms step_avg:264.19ms
step:871/1480 train_time:227477ms step_avg:264.20ms
step:872/1480 train_time:227756ms step_avg:264.22ms
step:873/1480 train_time:228033ms step_avg:264.23ms
step:874/1480 train_time:228307ms step_avg:264.24ms
step:875/1480 train_time:228578ms step_avg:264.25ms
step:875/1480 val_loss:3.5026 train_time:228722ms step_avg:264.42ms
step:876/1480 train_time:228858ms step_avg:264.27ms
step:877/1480 train_time:229141ms step_avg:264.29ms
step:878/1480 train_time:229413ms step_avg:264.30ms
step:879/1480 train_time:229685ms step_avg:264.31ms
step:880/1480 train_time:229958ms step_avg:264.32ms
step:881/1480 train_time:230228ms step_avg:264.33ms
step:882/1480 train_time:230503ms step_avg:264.34ms
step:883/1480 train_time:230780ms step_avg:264.35ms
step:884/1480 train_time:231056ms step_avg:264.37ms
step:885/1480 train_time:231330ms step_avg:264.38ms
step:886/1480 train_time:231605ms step_avg:264.39ms
step:887/1480 train_time:231884ms step_avg:264.41ms
step:888/1480 train_time:232163ms step_avg:264.42ms
step:889/1480 train_time:232440ms step_avg:264.44ms
step:890/1480 train_time:232710ms step_avg:264.44ms
step:891/1480 train_time:232983ms step_avg:264.45ms
step:892/1480 train_time:233257ms step_avg:264.46ms
step:893/1480 train_time:233530ms step_avg:264.47ms
step:894/1480 train_time:233811ms step_avg:264.49ms
step:895/1480 train_time:234091ms step_avg:264.51ms
step:896/1480 train_time:234367ms step_avg:264.52ms
step:897/1480 train_time:234644ms step_avg:264.54ms
step:898/1480 train_time:234919ms step_avg:264.55ms
step:899/1480 train_time:235192ms step_avg:264.56ms
step:900/1480 train_time:235464ms step_avg:264.57ms
step:901/1480 train_time:235739ms step_avg:264.58ms
step:902/1480 train_time:236010ms step_avg:264.59ms
step:903/1480 train_time:236294ms step_avg:264.61ms
step:904/1480 train_time:236570ms step_avg:264.62ms
step:905/1480 train_time:236843ms step_avg:264.63ms
step:906/1480 train_time:237120ms step_avg:264.64ms
step:907/1480 train_time:237398ms step_avg:264.66ms
step:908/1480 train_time:237671ms step_avg:264.67ms
step:909/1480 train_time:237943ms step_avg:264.68ms
step:910/1480 train_time:238221ms step_avg:264.69ms
step:911/1480 train_time:238494ms step_avg:264.70ms
step:912/1480 train_time:238771ms step_avg:264.71ms
step:913/1480 train_time:239051ms step_avg:264.73ms
step:914/1480 train_time:239325ms step_avg:264.74ms
step:915/1480 train_time:239612ms step_avg:264.76ms
step:916/1480 train_time:239889ms step_avg:264.78ms
step:917/1480 train_time:240162ms step_avg:264.79ms
step:918/1480 train_time:240441ms step_avg:264.80ms
step:919/1480 train_time:240718ms step_avg:264.82ms
step:920/1480 train_time:240990ms step_avg:264.82ms
step:921/1480 train_time:241264ms step_avg:264.83ms
step:922/1480 train_time:241544ms step_avg:264.85ms
step:923/1480 train_time:241822ms step_avg:264.87ms
step:924/1480 train_time:242097ms step_avg:264.88ms
step:925/1480 train_time:242374ms step_avg:264.89ms
step:926/1480 train_time:242650ms step_avg:264.90ms
step:927/1480 train_time:242924ms step_avg:264.91ms
step:928/1480 train_time:243202ms step_avg:264.93ms
step:929/1480 train_time:243477ms step_avg:264.94ms
step:930/1480 train_time:243754ms step_avg:264.95ms
step:931/1480 train_time:244027ms step_avg:264.96ms
step:932/1480 train_time:244308ms step_avg:264.98ms
step:933/1480 train_time:244586ms step_avg:264.99ms
step:934/1480 train_time:244868ms step_avg:265.01ms
step:935/1480 train_time:245151ms step_avg:265.03ms
step:936/1480 train_time:245427ms step_avg:265.04ms
step:937/1480 train_time:245707ms step_avg:265.06ms
step:938/1480 train_time:245981ms step_avg:265.07ms
step:939/1480 train_time:246260ms step_avg:265.08ms
step:940/1480 train_time:246536ms step_avg:265.09ms
step:941/1480 train_time:246811ms step_avg:265.10ms
step:942/1480 train_time:247088ms step_avg:265.12ms
step:943/1480 train_time:247368ms step_avg:265.13ms
step:944/1480 train_time:247652ms step_avg:265.15ms
step:945/1480 train_time:247926ms step_avg:265.16ms
step:946/1480 train_time:248207ms step_avg:265.18ms
step:947/1480 train_time:248483ms step_avg:265.19ms
step:948/1480 train_time:248758ms step_avg:265.20ms
step:949/1480 train_time:249035ms step_avg:265.21ms
step:950/1480 train_time:249305ms step_avg:265.22ms
step:951/1480 train_time:249589ms step_avg:265.24ms
step:952/1480 train_time:249865ms step_avg:265.25ms
step:953/1480 train_time:250145ms step_avg:265.27ms
step:954/1480 train_time:250425ms step_avg:265.28ms
step:955/1480 train_time:250698ms step_avg:265.29ms
step:956/1480 train_time:250971ms step_avg:265.30ms
step:957/1480 train_time:251248ms step_avg:265.31ms
step:958/1480 train_time:251531ms step_avg:265.33ms
step:959/1480 train_time:251805ms step_avg:265.34ms
step:960/1480 train_time:252081ms step_avg:265.35ms
step:961/1480 train_time:252354ms step_avg:265.36ms
step:962/1480 train_time:252628ms step_avg:265.37ms
step:963/1480 train_time:252905ms step_avg:265.38ms
step:964/1480 train_time:253181ms step_avg:265.39ms
step:965/1480 train_time:253455ms step_avg:265.40ms
step:966/1480 train_time:253730ms step_avg:265.41ms
step:967/1480 train_time:254008ms step_avg:265.42ms
step:968/1480 train_time:254285ms step_avg:265.43ms
step:969/1480 train_time:254560ms step_avg:265.44ms
step:970/1480 train_time:254832ms step_avg:265.45ms
step:971/1480 train_time:255107ms step_avg:265.46ms
step:972/1480 train_time:255381ms step_avg:265.47ms
step:973/1480 train_time:255653ms step_avg:265.48ms
step:974/1480 train_time:255935ms step_avg:265.49ms
step:975/1480 train_time:256208ms step_avg:265.50ms
step:976/1480 train_time:256482ms step_avg:265.51ms
step:977/1480 train_time:256755ms step_avg:265.52ms
step:978/1480 train_time:257032ms step_avg:265.53ms
step:979/1480 train_time:257305ms step_avg:265.54ms
step:980/1480 train_time:257582ms step_avg:265.55ms
step:981/1480 train_time:257862ms step_avg:265.56ms
step:982/1480 train_time:258132ms step_avg:265.57ms
step:983/1480 train_time:258409ms step_avg:265.58ms
step:984/1480 train_time:258686ms step_avg:265.59ms
step:985/1480 train_time:258964ms step_avg:265.60ms
step:986/1480 train_time:259237ms step_avg:265.61ms
step:987/1480 train_time:259508ms step_avg:265.62ms
step:988/1480 train_time:259786ms step_avg:265.63ms
step:989/1480 train_time:260061ms step_avg:265.64ms
step:990/1480 train_time:260341ms step_avg:265.65ms
step:991/1480 train_time:260617ms step_avg:265.66ms
step:992/1480 train_time:260901ms step_avg:265.68ms
step:993/1480 train_time:261189ms step_avg:265.71ms
step:994/1480 train_time:261465ms step_avg:265.72ms
step:995/1480 train_time:261739ms step_avg:265.72ms
step:996/1480 train_time:262013ms step_avg:265.73ms
step:997/1480 train_time:262291ms step_avg:265.75ms
step:998/1480 train_time:262565ms step_avg:265.75ms
step:999/1480 train_time:262845ms step_avg:265.77ms
step:1000/1480 train_time:263126ms step_avg:265.78ms
step:1000/1480 val_loss:3.4401 train_time:263268ms step_avg:265.93ms
step:1001/1480 train_time:263408ms step_avg:265.80ms
step:1002/1480 train_time:263685ms step_avg:265.81ms
step:1003/1480 train_time:263963ms step_avg:265.82ms
step:1004/1480 train_time:264246ms step_avg:265.84ms
step:1005/1480 train_time:264525ms step_avg:265.85ms
step:1006/1480 train_time:264805ms step_avg:265.87ms
step:1007/1480 train_time:265082ms step_avg:265.88ms
step:1008/1480 train_time:265359ms step_avg:265.89ms
step:1009/1480 train_time:265649ms step_avg:265.91ms
step:1010/1480 train_time:265923ms step_avg:265.92ms
step:1011/1480 train_time:266197ms step_avg:265.93ms
step:1012/1480 train_time:266473ms step_avg:265.94ms
step:1013/1480 train_time:266753ms step_avg:265.96ms
step:1014/1480 train_time:267037ms step_avg:265.97ms
step:1015/1480 train_time:267319ms step_avg:265.99ms
step:1016/1480 train_time:267598ms step_avg:266.00ms
step:1017/1480 train_time:267880ms step_avg:266.02ms
step:1018/1480 train_time:268159ms step_avg:266.03ms
step:1019/1480 train_time:268442ms step_avg:266.05ms
step:1020/1480 train_time:268724ms step_avg:266.06ms
step:1021/1480 train_time:268998ms step_avg:266.07ms
step:1022/1480 train_time:269276ms step_avg:266.08ms
step:1023/1480 train_time:269554ms step_avg:266.09ms
step:1024/1480 train_time:269832ms step_avg:266.11ms
step:1025/1480 train_time:270116ms step_avg:266.12ms
step:1026/1480 train_time:270390ms step_avg:266.13ms
step:1027/1480 train_time:270663ms step_avg:266.14ms
step:1028/1480 train_time:270947ms step_avg:266.16ms
step:1029/1480 train_time:271230ms step_avg:266.17ms
step:1030/1480 train_time:271509ms step_avg:266.19ms
step:1031/1480 train_time:271781ms step_avg:266.19ms
step:1032/1480 train_time:272062ms step_avg:266.21ms
step:1033/1480 train_time:272340ms step_avg:266.22ms
step:1034/1480 train_time:272622ms step_avg:266.23ms
step:1035/1480 train_time:272901ms step_avg:266.24ms
step:1036/1480 train_time:273178ms step_avg:266.26ms
step:1037/1480 train_time:273455ms step_avg:266.27ms
step:1038/1480 train_time:273735ms step_avg:266.28ms
step:1039/1480 train_time:274021ms step_avg:266.30ms
step:1040/1480 train_time:274299ms step_avg:266.31ms
step:1041/1480 train_time:274576ms step_avg:266.32ms
step:1042/1480 train_time:274851ms step_avg:266.33ms
step:1043/1480 train_time:275127ms step_avg:266.34ms
step:1044/1480 train_time:275402ms step_avg:266.35ms
step:1045/1480 train_time:275683ms step_avg:266.36ms
step:1046/1480 train_time:275963ms step_avg:266.37ms
step:1047/1480 train_time:276240ms step_avg:266.38ms
step:1048/1480 train_time:276514ms step_avg:266.39ms
step:1049/1480 train_time:276790ms step_avg:266.40ms
step:1050/1480 train_time:277069ms step_avg:266.41ms
step:1051/1480 train_time:277348ms step_avg:266.42ms
step:1052/1480 train_time:277623ms step_avg:266.43ms
step:1053/1480 train_time:277899ms step_avg:266.44ms
step:1054/1480 train_time:278180ms step_avg:266.46ms
step:1055/1480 train_time:278454ms step_avg:266.46ms
step:1056/1480 train_time:278732ms step_avg:266.47ms
step:1057/1480 train_time:279013ms step_avg:266.49ms
step:1058/1480 train_time:279289ms step_avg:266.50ms
step:1059/1480 train_time:279569ms step_avg:266.51ms
step:1060/1480 train_time:279848ms step_avg:266.52ms
step:1061/1480 train_time:280120ms step_avg:266.53ms
step:1062/1480 train_time:280398ms step_avg:266.54ms
step:1063/1480 train_time:280672ms step_avg:266.54ms
step:1064/1480 train_time:280946ms step_avg:266.55ms
step:1065/1480 train_time:281224ms step_avg:266.56ms
step:1066/1480 train_time:281506ms step_avg:266.58ms
step:1067/1480 train_time:281782ms step_avg:266.59ms
step:1068/1480 train_time:282056ms step_avg:266.59ms
step:1069/1480 train_time:282340ms step_avg:266.61ms
step:1070/1480 train_time:282615ms step_avg:266.62ms
step:1071/1480 train_time:282896ms step_avg:266.63ms
step:1072/1480 train_time:283173ms step_avg:266.64ms
step:1073/1480 train_time:283448ms step_avg:266.65ms
step:1074/1480 train_time:283723ms step_avg:266.66ms
step:1075/1480 train_time:284006ms step_avg:266.67ms
step:1076/1480 train_time:284284ms step_avg:266.68ms
step:1077/1480 train_time:284562ms step_avg:266.69ms
step:1078/1480 train_time:284848ms step_avg:266.71ms
step:1079/1480 train_time:285129ms step_avg:266.72ms
step:1080/1480 train_time:285409ms step_avg:266.74ms
step:1081/1480 train_time:285685ms step_avg:266.75ms
step:1082/1480 train_time:285959ms step_avg:266.75ms
step:1083/1480 train_time:286238ms step_avg:266.76ms
step:1084/1480 train_time:286515ms step_avg:266.77ms
step:1085/1480 train_time:286792ms step_avg:266.78ms
step:1086/1480 train_time:287072ms step_avg:266.80ms
step:1087/1480 train_time:287348ms step_avg:266.80ms
step:1088/1480 train_time:287627ms step_avg:266.82ms
step:1089/1480 train_time:287908ms step_avg:266.83ms
step:1090/1480 train_time:288188ms step_avg:266.84ms
step:1091/1480 train_time:288464ms step_avg:266.85ms
step:1092/1480 train_time:288743ms step_avg:266.86ms
step:1093/1480 train_time:289020ms step_avg:266.87ms
step:1094/1480 train_time:289295ms step_avg:266.88ms
step:1095/1480 train_time:289573ms step_avg:266.89ms
step:1096/1480 train_time:289851ms step_avg:266.90ms
step:1097/1480 train_time:290131ms step_avg:266.91ms
step:1098/1480 train_time:290415ms step_avg:266.93ms
step:1099/1480 train_time:290699ms step_avg:266.94ms
step:1100/1480 train_time:290985ms step_avg:266.96ms
step:1101/1480 train_time:291265ms step_avg:266.97ms
step:1102/1480 train_time:291548ms step_avg:266.99ms
step:1103/1480 train_time:291835ms step_avg:267.00ms
step:1104/1480 train_time:292115ms step_avg:267.02ms
step:1105/1480 train_time:292401ms step_avg:267.03ms
step:1106/1480 train_time:292677ms step_avg:267.04ms
step:1107/1480 train_time:292955ms step_avg:267.05ms
step:1108/1480 train_time:293230ms step_avg:267.06ms
step:1109/1480 train_time:293509ms step_avg:267.07ms
step:1110/1480 train_time:293785ms step_avg:267.08ms
step:1111/1480 train_time:294064ms step_avg:267.09ms
step:1112/1480 train_time:294342ms step_avg:267.10ms
step:1113/1480 train_time:294631ms step_avg:267.12ms
step:1114/1480 train_time:294915ms step_avg:267.13ms
step:1115/1480 train_time:295195ms step_avg:267.14ms
step:1116/1480 train_time:295472ms step_avg:267.15ms
step:1117/1480 train_time:295754ms step_avg:267.17ms
step:1118/1480 train_time:296040ms step_avg:267.18ms
step:1119/1480 train_time:296315ms step_avg:267.19ms
step:1120/1480 train_time:296593ms step_avg:267.20ms
step:1121/1480 train_time:296879ms step_avg:267.22ms
step:1122/1480 train_time:297157ms step_avg:267.23ms
step:1123/1480 train_time:297438ms step_avg:267.24ms
step:1124/1480 train_time:297719ms step_avg:267.25ms
step:1125/1480 train_time:297994ms step_avg:267.26ms
step:1125/1480 val_loss:3.3848 train_time:298135ms step_avg:267.39ms
step:1126/1480 train_time:298275ms step_avg:267.27ms
step:1127/1480 train_time:298561ms step_avg:267.29ms
step:1128/1480 train_time:298841ms step_avg:267.30ms
step:1129/1480 train_time:299126ms step_avg:267.32ms
step:1130/1480 train_time:299403ms step_avg:267.32ms
step:1131/1480 train_time:299694ms step_avg:267.35ms
step:1132/1480 train_time:299969ms step_avg:267.35ms
step:1133/1480 train_time:300251ms step_avg:267.37ms
step:1134/1480 train_time:300531ms step_avg:267.38ms
step:1135/1480 train_time:300810ms step_avg:267.39ms
step:1136/1480 train_time:301089ms step_avg:267.40ms
step:1137/1480 train_time:301369ms step_avg:267.41ms
step:1138/1480 train_time:301649ms step_avg:267.42ms
step:1139/1480 train_time:301926ms step_avg:267.43ms
step:1140/1480 train_time:302206ms step_avg:267.44ms
step:1141/1480 train_time:302489ms step_avg:267.45ms
step:1142/1480 train_time:302764ms step_avg:267.46ms
step:1143/1480 train_time:303049ms step_avg:267.48ms
step:1144/1480 train_time:303330ms step_avg:267.49ms
step:1145/1480 train_time:303605ms step_avg:267.49ms
step:1146/1480 train_time:303886ms step_avg:267.51ms
step:1147/1480 train_time:304169ms step_avg:267.52ms
step:1148/1480 train_time:304446ms step_avg:267.53ms
step:1149/1480 train_time:304727ms step_avg:267.54ms
step:1150/1480 train_time:305008ms step_avg:267.55ms
step:1151/1480 train_time:305291ms step_avg:267.56ms
step:1152/1480 train_time:305573ms step_avg:267.58ms
step:1153/1480 train_time:305854ms step_avg:267.59ms
step:1154/1480 train_time:306128ms step_avg:267.59ms
step:1155/1480 train_time:306410ms step_avg:267.61ms
step:1156/1480 train_time:306698ms step_avg:267.62ms
step:1157/1480 train_time:306976ms step_avg:267.63ms
step:1158/1480 train_time:307252ms step_avg:267.64ms
step:1159/1480 train_time:307528ms step_avg:267.65ms
step:1160/1480 train_time:307803ms step_avg:267.65ms
step:1161/1480 train_time:308089ms step_avg:267.67ms
step:1162/1480 train_time:308367ms step_avg:267.68ms
step:1163/1480 train_time:308644ms step_avg:267.69ms
step:1164/1480 train_time:308919ms step_avg:267.69ms
step:1165/1480 train_time:309195ms step_avg:267.70ms
step:1166/1480 train_time:309484ms step_avg:267.72ms
step:1167/1480 train_time:309765ms step_avg:267.73ms
step:1168/1480 train_time:310045ms step_avg:267.74ms
step:1169/1480 train_time:310322ms step_avg:267.75ms
step:1170/1480 train_time:310601ms step_avg:267.76ms
step:1171/1480 train_time:310878ms step_avg:267.77ms
step:1172/1480 train_time:311152ms step_avg:267.77ms
step:1173/1480 train_time:311426ms step_avg:267.78ms
step:1174/1480 train_time:311716ms step_avg:267.80ms
step:1175/1480 train_time:312002ms step_avg:267.81ms
step:1176/1480 train_time:312282ms step_avg:267.82ms
step:1177/1480 train_time:312571ms step_avg:267.84ms
step:1178/1480 train_time:312849ms step_avg:267.85ms
step:1179/1480 train_time:313124ms step_avg:267.86ms
step:1180/1480 train_time:313416ms step_avg:267.88ms
step:1181/1480 train_time:313695ms step_avg:267.89ms
step:1182/1480 train_time:313975ms step_avg:267.90ms
step:1183/1480 train_time:314255ms step_avg:267.91ms
step:1184/1480 train_time:314533ms step_avg:267.92ms
step:1185/1480 train_time:314816ms step_avg:267.93ms
step:1186/1480 train_time:315092ms step_avg:267.94ms
step:1187/1480 train_time:315381ms step_avg:267.95ms
step:1188/1480 train_time:315654ms step_avg:267.96ms
step:1189/1480 train_time:315936ms step_avg:267.97ms
step:1190/1480 train_time:316215ms step_avg:267.98ms
step:1191/1480 train_time:316499ms step_avg:267.99ms
step:1192/1480 train_time:316776ms step_avg:268.00ms
step:1193/1480 train_time:317056ms step_avg:268.01ms
step:1194/1480 train_time:317333ms step_avg:268.02ms
step:1195/1480 train_time:317615ms step_avg:268.03ms
step:1196/1480 train_time:317911ms step_avg:268.05ms
step:1197/1480 train_time:318191ms step_avg:268.06ms
step:1198/1480 train_time:318481ms step_avg:268.08ms
step:1199/1480 train_time:318768ms step_avg:268.10ms
step:1200/1480 train_time:319048ms step_avg:268.11ms
step:1201/1480 train_time:319324ms step_avg:268.11ms
step:1202/1480 train_time:319615ms step_avg:268.13ms
step:1203/1480 train_time:319898ms step_avg:268.15ms
step:1204/1480 train_time:320183ms step_avg:268.16ms
step:1205/1480 train_time:320463ms step_avg:268.17ms
step:1206/1480 train_time:320743ms step_avg:268.18ms
step:1207/1480 train_time:321021ms step_avg:268.19ms
step:1208/1480 train_time:321297ms step_avg:268.19ms
step:1209/1480 train_time:321579ms step_avg:268.21ms
step:1210/1480 train_time:321866ms step_avg:268.22ms
step:1211/1480 train_time:322152ms step_avg:268.24ms
step:1212/1480 train_time:322433ms step_avg:268.25ms
step:1213/1480 train_time:322713ms step_avg:268.26ms
step:1214/1480 train_time:322995ms step_avg:268.27ms
step:1215/1480 train_time:323279ms step_avg:268.28ms
step:1216/1480 train_time:323559ms step_avg:268.29ms
step:1217/1480 train_time:323848ms step_avg:268.31ms
step:1218/1480 train_time:324128ms step_avg:268.32ms
step:1219/1480 train_time:324419ms step_avg:268.34ms
step:1220/1480 train_time:324700ms step_avg:268.35ms
step:1221/1480 train_time:324983ms step_avg:268.36ms
step:1222/1480 train_time:325257ms step_avg:268.36ms
step:1223/1480 train_time:325540ms step_avg:268.38ms
step:1224/1480 train_time:325829ms step_avg:268.39ms
step:1225/1480 train_time:326110ms step_avg:268.40ms
step:1226/1480 train_time:326391ms step_avg:268.41ms
step:1227/1480 train_time:326675ms step_avg:268.43ms
step:1228/1480 train_time:326953ms step_avg:268.43ms
step:1229/1480 train_time:327235ms step_avg:268.45ms
step:1230/1480 train_time:327521ms step_avg:268.46ms
step:1231/1480 train_time:327805ms step_avg:268.47ms
step:1232/1480 train_time:328092ms step_avg:268.49ms
step:1233/1480 train_time:328367ms step_avg:268.49ms
step:1234/1480 train_time:328645ms step_avg:268.50ms
step:1235/1480 train_time:328931ms step_avg:268.52ms
step:1236/1480 train_time:329211ms step_avg:268.52ms
step:1237/1480 train_time:329488ms step_avg:268.53ms
step:1238/1480 train_time:329780ms step_avg:268.55ms
step:1239/1480 train_time:330066ms step_avg:268.56ms
step:1240/1480 train_time:330348ms step_avg:268.58ms
step:1241/1480 train_time:330632ms step_avg:268.59ms
step:1242/1480 train_time:330911ms step_avg:268.60ms
step:1243/1480 train_time:331188ms step_avg:268.60ms
step:1244/1480 train_time:331466ms step_avg:268.61ms
step:1245/1480 train_time:331746ms step_avg:268.62ms
step:1246/1480 train_time:332026ms step_avg:268.63ms
step:1247/1480 train_time:332310ms step_avg:268.64ms
step:1248/1480 train_time:332589ms step_avg:268.65ms
step:1249/1480 train_time:332869ms step_avg:268.66ms
step:1250/1480 train_time:333147ms step_avg:268.67ms
step:1250/1480 val_loss:3.3342 train_time:333289ms step_avg:268.78ms
step:1251/1480 train_time:333431ms step_avg:268.68ms
step:1252/1480 train_time:333709ms step_avg:268.69ms
step:1253/1480 train_time:333987ms step_avg:268.69ms
step:1254/1480 train_time:334269ms step_avg:268.70ms
step:1255/1480 train_time:334563ms step_avg:268.73ms
step:1256/1480 train_time:334851ms step_avg:268.74ms
step:1257/1480 train_time:335132ms step_avg:268.75ms
step:1258/1480 train_time:335416ms step_avg:268.76ms
step:1259/1480 train_time:335695ms step_avg:268.77ms
step:1260/1480 train_time:335974ms step_avg:268.78ms
step:1261/1480 train_time:336259ms step_avg:268.79ms
step:1262/1480 train_time:336544ms step_avg:268.81ms
step:1263/1480 train_time:336829ms step_avg:268.82ms
step:1264/1480 train_time:337108ms step_avg:268.83ms
step:1265/1480 train_time:337386ms step_avg:268.83ms
step:1266/1480 train_time:337667ms step_avg:268.84ms
step:1267/1480 train_time:337950ms step_avg:268.85ms
step:1268/1480 train_time:338228ms step_avg:268.86ms
step:1269/1480 train_time:338518ms step_avg:268.88ms
step:1270/1480 train_time:338796ms step_avg:268.89ms
step:1271/1480 train_time:339081ms step_avg:268.90ms
step:1272/1480 train_time:339357ms step_avg:268.90ms
step:1273/1480 train_time:339637ms step_avg:268.91ms
step:1274/1480 train_time:339919ms step_avg:268.92ms
step:1275/1480 train_time:340196ms step_avg:268.93ms
step:1276/1480 train_time:340473ms step_avg:268.94ms
step:1277/1480 train_time:340759ms step_avg:268.95ms
step:1278/1480 train_time:341040ms step_avg:268.96ms
step:1279/1480 train_time:341320ms step_avg:268.97ms
step:1280/1480 train_time:341606ms step_avg:268.98ms
step:1281/1480 train_time:341886ms step_avg:268.99ms
step:1282/1480 train_time:342164ms step_avg:269.00ms
step:1283/1480 train_time:342447ms step_avg:269.01ms
step:1284/1480 train_time:342728ms step_avg:269.02ms
step:1285/1480 train_time:343006ms step_avg:269.02ms
step:1286/1480 train_time:343283ms step_avg:269.03ms
step:1287/1480 train_time:343564ms step_avg:269.04ms
step:1288/1480 train_time:343847ms step_avg:269.05ms
step:1289/1480 train_time:344142ms step_avg:269.07ms
step:1290/1480 train_time:344426ms step_avg:269.08ms
step:1291/1480 train_time:344706ms step_avg:269.09ms
step:1292/1480 train_time:344986ms step_avg:269.10ms
step:1293/1480 train_time:345270ms step_avg:269.11ms
step:1294/1480 train_time:345552ms step_avg:269.12ms
step:1295/1480 train_time:345829ms step_avg:269.13ms
step:1296/1480 train_time:346115ms step_avg:269.14ms
step:1297/1480 train_time:346397ms step_avg:269.15ms
step:1298/1480 train_time:346681ms step_avg:269.16ms
step:1299/1480 train_time:346961ms step_avg:269.17ms
step:1300/1480 train_time:347238ms step_avg:269.18ms
step:1301/1480 train_time:347519ms step_avg:269.19ms
step:1302/1480 train_time:347802ms step_avg:269.20ms
step:1303/1480 train_time:348084ms step_avg:269.21ms
step:1304/1480 train_time:348365ms step_avg:269.22ms
step:1305/1480 train_time:348641ms step_avg:269.22ms
step:1306/1480 train_time:348923ms step_avg:269.23ms
step:1307/1480 train_time:349200ms step_avg:269.24ms
step:1308/1480 train_time:349478ms step_avg:269.24ms
step:1309/1480 train_time:349760ms step_avg:269.25ms
step:1310/1480 train_time:350036ms step_avg:269.26ms
step:1311/1480 train_time:350313ms step_avg:269.26ms
step:1312/1480 train_time:350597ms step_avg:269.28ms
step:1313/1480 train_time:350876ms step_avg:269.28ms
step:1314/1480 train_time:351158ms step_avg:269.29ms
step:1315/1480 train_time:351439ms step_avg:269.30ms
step:1316/1480 train_time:351716ms step_avg:269.31ms
step:1317/1480 train_time:351997ms step_avg:269.32ms
step:1318/1480 train_time:352283ms step_avg:269.33ms
step:1319/1480 train_time:352565ms step_avg:269.34ms
step:1320/1480 train_time:352857ms step_avg:269.36ms
step:1321/1480 train_time:353140ms step_avg:269.37ms
step:1322/1480 train_time:353427ms step_avg:269.38ms
step:1323/1480 train_time:353713ms step_avg:269.39ms
step:1324/1480 train_time:354000ms step_avg:269.41ms
step:1325/1480 train_time:354295ms step_avg:269.43ms
step:1326/1480 train_time:354581ms step_avg:269.44ms
step:1327/1480 train_time:354860ms step_avg:269.45ms
step:1328/1480 train_time:355138ms step_avg:269.45ms
step:1329/1480 train_time:355443ms step_avg:269.48ms
step:1330/1480 train_time:355730ms step_avg:269.49ms
step:1331/1480 train_time:356011ms step_avg:269.50ms
step:1332/1480 train_time:356294ms step_avg:269.51ms
step:1333/1480 train_time:356582ms step_avg:269.53ms
step:1334/1480 train_time:356864ms step_avg:269.53ms
step:1335/1480 train_time:357146ms step_avg:269.54ms
step:1336/1480 train_time:357440ms step_avg:269.56ms
step:1337/1480 train_time:357723ms step_avg:269.57ms
step:1338/1480 train_time:358006ms step_avg:269.58ms
step:1339/1480 train_time:358289ms step_avg:269.59ms
step:1340/1480 train_time:358577ms step_avg:269.61ms
step:1341/1480 train_time:358853ms step_avg:269.61ms
step:1342/1480 train_time:359136ms step_avg:269.62ms
step:1343/1480 train_time:359414ms step_avg:269.63ms
step:1344/1480 train_time:359696ms step_avg:269.64ms
step:1345/1480 train_time:359987ms step_avg:269.65ms
step:1346/1480 train_time:360266ms step_avg:269.66ms
step:1347/1480 train_time:360547ms step_avg:269.67ms
step:1348/1480 train_time:360830ms step_avg:269.68ms
step:1349/1480 train_time:361111ms step_avg:269.69ms
step:1350/1480 train_time:361396ms step_avg:269.70ms
step:1351/1480 train_time:361679ms step_avg:269.71ms
step:1352/1480 train_time:361958ms step_avg:269.72ms
step:1353/1480 train_time:362244ms step_avg:269.73ms
step:1354/1480 train_time:362529ms step_avg:269.74ms
step:1355/1480 train_time:362809ms step_avg:269.75ms
step:1356/1480 train_time:363095ms step_avg:269.76ms
step:1357/1480 train_time:363380ms step_avg:269.77ms
step:1358/1480 train_time:363661ms step_avg:269.78ms
step:1359/1480 train_time:363944ms step_avg:269.79ms
step:1360/1480 train_time:364233ms step_avg:269.80ms
step:1361/1480 train_time:364521ms step_avg:269.82ms
step:1362/1480 train_time:364803ms step_avg:269.82ms
step:1363/1480 train_time:365093ms step_avg:269.84ms
step:1364/1480 train_time:365378ms step_avg:269.85ms
step:1365/1480 train_time:365653ms step_avg:269.85ms
step:1366/1480 train_time:365936ms step_avg:269.86ms
step:1367/1480 train_time:366219ms step_avg:269.87ms
step:1368/1480 train_time:366498ms step_avg:269.88ms
step:1369/1480 train_time:366788ms step_avg:269.90ms
step:1370/1480 train_time:367078ms step_avg:269.91ms
step:1371/1480 train_time:367358ms step_avg:269.92ms
step:1372/1480 train_time:367645ms step_avg:269.93ms
step:1373/1480 train_time:367930ms step_avg:269.94ms
step:1374/1480 train_time:368217ms step_avg:269.95ms
step:1375/1480 train_time:368494ms step_avg:269.96ms
step:1375/1480 val_loss:3.2957 train_time:368634ms step_avg:270.06ms
step:1376/1480 train_time:368775ms step_avg:269.97ms
step:1377/1480 train_time:369063ms step_avg:269.98ms
step:1378/1480 train_time:369341ms step_avg:269.99ms
step:1379/1480 train_time:369623ms step_avg:270.00ms
step:1380/1480 train_time:369903ms step_avg:270.00ms
step:1381/1480 train_time:370195ms step_avg:270.02ms
step:1382/1480 train_time:370476ms step_avg:270.03ms
step:1383/1480 train_time:370761ms step_avg:270.04ms
step:1384/1480 train_time:371048ms step_avg:270.05ms
step:1385/1480 train_time:371327ms step_avg:270.06ms
step:1386/1480 train_time:371610ms step_avg:270.07ms
step:1387/1480 train_time:371894ms step_avg:270.08ms
step:1388/1480 train_time:372173ms step_avg:270.08ms
step:1389/1480 train_time:372458ms step_avg:270.09ms
step:1390/1480 train_time:372739ms step_avg:270.10ms
step:1391/1480 train_time:373023ms step_avg:270.11ms
step:1392/1480 train_time:373309ms step_avg:270.12ms
step:1393/1480 train_time:373591ms step_avg:270.13ms
step:1394/1480 train_time:373871ms step_avg:270.14ms
step:1395/1480 train_time:374147ms step_avg:270.14ms
step:1396/1480 train_time:374427ms step_avg:270.15ms
step:1397/1480 train_time:374704ms step_avg:270.15ms
step:1398/1480 train_time:374983ms step_avg:270.16ms
step:1399/1480 train_time:375261ms step_avg:270.17ms
step:1400/1480 train_time:375553ms step_avg:270.18ms
step:1401/1480 train_time:375830ms step_avg:270.19ms
step:1402/1480 train_time:376107ms step_avg:270.19ms
step:1403/1480 train_time:376395ms step_avg:270.20ms
step:1404/1480 train_time:376675ms step_avg:270.21ms
step:1405/1480 train_time:376962ms step_avg:270.22ms
step:1406/1480 train_time:377249ms step_avg:270.24ms
step:1407/1480 train_time:377532ms step_avg:270.24ms
step:1408/1480 train_time:377808ms step_avg:270.25ms
step:1409/1480 train_time:378103ms step_avg:270.27ms
step:1410/1480 train_time:378384ms step_avg:270.27ms
step:1411/1480 train_time:378666ms step_avg:270.28ms
step:1412/1480 train_time:378948ms step_avg:270.29ms
step:1413/1480 train_time:379231ms step_avg:270.30ms
step:1414/1480 train_time:379513ms step_avg:270.31ms
step:1415/1480 train_time:379794ms step_avg:270.32ms
step:1416/1480 train_time:380090ms step_avg:270.33ms
step:1417/1480 train_time:380373ms step_avg:270.34ms
step:1418/1480 train_time:380661ms step_avg:270.36ms
step:1419/1480 train_time:380943ms step_avg:270.36ms
step:1420/1480 train_time:381227ms step_avg:270.37ms
step:1421/1480 train_time:381511ms step_avg:270.38ms
step:1422/1480 train_time:381793ms step_avg:270.39ms
step:1423/1480 train_time:382069ms step_avg:270.40ms
step:1424/1480 train_time:382357ms step_avg:270.41ms
step:1425/1480 train_time:382648ms step_avg:270.42ms
step:1426/1480 train_time:382930ms step_avg:270.43ms
step:1427/1480 train_time:383214ms step_avg:270.44ms
step:1428/1480 train_time:383494ms step_avg:270.45ms
step:1429/1480 train_time:383773ms step_avg:270.45ms
step:1430/1480 train_time:384066ms step_avg:270.47ms
step:1431/1480 train_time:384353ms step_avg:270.48ms
step:1432/1480 train_time:384636ms step_avg:270.49ms
step:1433/1480 train_time:384924ms step_avg:270.50ms
step:1434/1480 train_time:385215ms step_avg:270.52ms
step:1435/1480 train_time:385498ms step_avg:270.52ms
step:1436/1480 train_time:385791ms step_avg:270.54ms
step:1437/1480 train_time:386070ms step_avg:270.55ms
step:1438/1480 train_time:386347ms step_avg:270.55ms
step:1439/1480 train_time:386632ms step_avg:270.56ms
step:1440/1480 train_time:386909ms step_avg:270.57ms
step:1441/1480 train_time:387192ms step_avg:270.57ms
step:1442/1480 train_time:387480ms step_avg:270.59ms
step:1443/1480 train_time:387780ms step_avg:270.61ms
step:1444/1480 train_time:388062ms step_avg:270.62ms
step:1445/1480 train_time:388344ms step_avg:270.62ms
step:1446/1480 train_time:388632ms step_avg:270.63ms
step:1447/1480 train_time:388917ms step_avg:270.64ms
step:1448/1480 train_time:389201ms step_avg:270.65ms
step:1449/1480 train_time:389487ms step_avg:270.67ms
step:1450/1480 train_time:389770ms step_avg:270.67ms
step:1451/1480 train_time:390051ms step_avg:270.68ms
step:1452/1480 train_time:390333ms step_avg:270.69ms
step:1453/1480 train_time:390611ms step_avg:270.69ms
step:1454/1480 train_time:390892ms step_avg:270.70ms
step:1455/1480 train_time:391176ms step_avg:270.71ms
step:1456/1480 train_time:391461ms step_avg:270.72ms
step:1457/1480 train_time:391742ms step_avg:270.73ms
step:1458/1480 train_time:392027ms step_avg:270.74ms
step:1459/1480 train_time:392312ms step_avg:270.75ms
step:1460/1480 train_time:392593ms step_avg:270.75ms
step:1461/1480 train_time:392875ms step_avg:270.76ms
step:1462/1480 train_time:393154ms step_avg:270.77ms
step:1463/1480 train_time:393443ms step_avg:270.78ms
step:1464/1480 train_time:393730ms step_avg:270.79ms
step:1465/1480 train_time:394010ms step_avg:270.80ms
step:1466/1480 train_time:394291ms step_avg:270.80ms
step:1467/1480 train_time:394575ms step_avg:270.81ms
step:1468/1480 train_time:394857ms step_avg:270.82ms
step:1469/1480 train_time:395138ms step_avg:270.83ms
step:1470/1480 train_time:395427ms step_avg:270.84ms
step:1471/1480 train_time:395723ms step_avg:270.86ms
step:1472/1480 train_time:396014ms step_avg:270.87ms
step:1473/1480 train_time:396293ms step_avg:270.88ms
step:1474/1480 train_time:396589ms step_avg:270.89ms
step:1475/1480 train_time:396876ms step_avg:270.91ms
step:1476/1480 train_time:397160ms step_avg:270.91ms
step:1477/1480 train_time:397455ms step_avg:270.93ms
step:1478/1480 train_time:397749ms step_avg:270.95ms
step:1479/1480 train_time:398034ms step_avg:270.96ms
step:1480/1480 train_time:398313ms step_avg:270.96ms
step:1480/1480 val_loss:3.2769 train_time:398464ms step_avg:271.06ms
peak memory consumption: 34237 MiB

import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import contextlib
from dataclasses import dataclass
from pathlib import Path

import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.nn.attention.flex_attention import BlockMask, flex_attention #KoszarskyB

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params = list(params)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [
            {
                'params': [p for p in params if p.numel() == size],
                'update_buffer': [
                    torch.empty(size, device='cuda', dtype=torch.bfloat16)
                    for _ in range(self.world_size)
                ],
            }
            for size in sizes
        ]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            assert len(params) % self.world_size == 0
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                p = params[base_i + self.rank]
                g = p.grad
                assert g is not None
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.lerp_(g, 1 - momentum)
                g = g.lerp_(buf, momentum) if nesterov else buf
                g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                update_prev()
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features, bias=False):
        super().__init__(in_features, out_features, bias)

    def forward(self, x):
        return F.linear(
            x,
            self.weight.to(x.dtype),
            None if self.bias is None else self.bias.to(x.dtype)
        )

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.register_buffer('inv_freq', (1 / base) ** (torch.arange(0, dim, 2) / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            t = torch.arange(seq_len, device=x.device)
            freqs = torch.outer(t, self.inv_freq)
            self.seq_len_cached = seq_len
            self.cos_cached = freqs.cos()
            self.sin_cached = freqs.sin()
        cos, sin = self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
        # apply_rotary_emb(x, cos, sin)
        x1, x2 = x.chunk(2, dim=3)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x, vi, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        v = self.lambdas[0] * v + self.lambdas[1] * vi.view_as(v) # @KoszarskyB & @Grad62304977
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, enable_gqa=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc   = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config.model_dim, config.num_heads)
        self.mlp = MLP(config.model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, vi, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x = x + self.attn(norm(x), vi, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, config: "GPTConfig"):
        super().__init__()
        self.embed = nn.ModuleList([
            nn.Embedding(config.vocab_size, config.model_dim)
            for _ in range(6)
        ])

    def forward(self, inputs) -> "list[torch.Tensor]":
        ve = [emb(inputs) for emb in self.embed]
        ve += reversed(ve)
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    num_layers : int = 12
    num_heads : int = 6 # head dim 128 suggested by @Grad62304977
    model_dim : int = 768

class GPT(nn.Module):

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.num_layers = config.num_layers

        # U-net design by @brendanh0gan
        self.num_encoder_layers = config.num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = config.num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

        self.embed = nn.Embedding(config.vocab_size, config.model_dim)
        self.blocks = nn.ModuleList([Block(config) for _ in range(config.num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(config)
        self.lm_head = CastedLinear(config.model_dim, config.vocab_size, bias=True)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(
        self,
        inputs: torch.Tensor,
        targets: torch.Tensor,
        sliding_window_num_blocks: torch.Tensor,
    ):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: torch.Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks: torch.Tensor):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm ^ full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        # forward the GPT model itself
        x = self.embed(inputs[None]) # token embeddings of shape (b, t, model_dim)
        x = norm(x) # @Grad62304977
        x0 = x
        ve = self.value_embeds(inputs)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(file: Path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32)
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    return int(header[2]) # number of tokens (claimed)

def _load_data_shard(path: Path, num_tokens):
    with path.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, seq_len, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.seq_len = seq_len

        # glob files that match the pattern
        self.files = sorted(Path.cwd().glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        self.files_num_tokens = [_peek_data_shard(file) for file in self.files]
        assert min(self.files_num_tokens) >= num_processes * seq_len + 1
        self.total_num_tokens = sum(self.files_num_tokens)

        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.seq_len
        self.tokens = _load_data_shard(self.files[self.current_shard], self.files_num_tokens[self.current_shard])

    def next_batch(self):
        batch_size = self.seq_len * self.num_processes
        buf = self.tokens[self.current_position:self.current_position+self.seq_len+1]
        # host side async is sufficient;
        # no performance improvement was observed when introducing a separate stream.
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # inputs
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # targets
        # advance current position and load next shard if necessary
        self.current_position += batch_size
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        return inputs, targets

def set_output_layer_bias(model, dataloader, n_batches):
    # Use token prevalence to initialize output layer bias & avoid initial shock to network of having to find it.
    t0 = time.perf_counter()
    num_vocab = model.lm_head.bias.size(0)
    for i in range(n_batches):
        _, targets_train = dataloader.next_batch()
        if i == 0:
            total_counts = torch.zeros(num_vocab, dtype=torch.int32, device=targets_train.device)
        ids, counts = torch.unique(targets_train, sorted=True, return_counts=True)
        total_counts[ids] += counts

    target_probs = total_counts / total_counts.sum()
    target_probs = (target_probs + 1e-12)
    target_probs = target_probs / target_probs.sum()

    with torch.no_grad():
        model.lm_head.bias.copy_(target_probs.log())

    old_init_loss = torch.tensor(1 / num_vocab).log().item()
    new_init_loss = (target_probs * target_probs.log()).sum().item()
    total_time = time.perf_counter() - t0
    print0(f"Centered output layer, initial loss {old_init_loss:.3f} => {new_init_loss:.3f} ({total_time:.1f}s)")

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8 # batch size, in sequences, across all devices
    sequence_length : int = 64*1024 # sequence length, in tokens
    num_iterations : int = 1480 # number of iterations to run
    warmup_iters : int = 0
    cooldown_iters : int = 600 # number of iterations of linear warmup/cooldown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
device = torch.device(f'cuda:{ddp_local_rank}')
torch.cuda.set_device(device)
print(f'using device: {device}')
dist.init_process_group(backend='nccl', device_id=device)
dist.barrier()
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    Path('logs').mkdir(exist_ok=True)
    # logdir = Path('logs') / f'{run_id}'
    # logdir.mkdir()
    logfile = Path('logs') / f'{run_id}.txt'
    print(logfile.stem)
    # create the log file
    with logfile.open('w') as f:
        # begin the log by printing this file (the Python code)
        print(code, file=f)
        print('=' * 100, file=f)
def print0(s, logonly=False):
    if master_process:
        with logfile.open('a') as f:
            if not logonly:
                print(s)
            print(s, file=f)
# log information about the hardware/software environment this is running on
# and print the full `nvidia-smi` to file
print0(f'Running python {sys.version}')
print0(f'Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:')
import subprocess
result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
print0(f'{result.stdout}', logonly=True)
print0('='*100, logonly=True)

# calculate the number of steps to take in the val loop.
assert args.val_tokens % (args.sequence_length * ddp_world_size) == 0
val_steps = args.val_tokens // (args.sequence_length * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (ddp_world_size) == 0
train_accumulation_steps = args.batch_size // ddp_world_size

# load tokens
train_loader = DistributedDataLoader(args.input_bin, args.sequence_length, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, args.sequence_length, ddp_rank, ddp_world_size)
print0(f"Training DataLoader: total number of tokens: {train_loader.total_num_tokens} across {len(train_loader.files)} files")
print0(f"Validation DataLoader: total number of tokens: {val_loader.total_num_tokens} across {len(val_loader.files)} files")
print0('='*100, logonly=True)
inputs_train, targets_train = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, num_layers=12, num_heads=6, model_dim=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee

set_output_layer_bias(model, train_loader, n_batches=100)

model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)
raw_model = model.module # always contains the "raw" unwrapped model

# init the optimizer(s)
embed_params = [*raw_model.embed.parameters(), *raw_model.value_embeds.parameters()]
optimizer1 = torch.optim.Adam(embed_params, lr=0.6, betas=(0.8, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight, raw_model.lm_head.bias], lr=0.008, betas=(0.8, 0.95), fused=True)
params = list(raw_model.blocks.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
optimizer3 = Muon(matrix_params, lr=0.05, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.8, 0.95), fused=True)
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and cooldown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.cooldown_iters:
        return 1.0
    # 3) linear cooldown
    else:
        decay_ratio = (args.num_iterations - it) / args.cooldown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device="cuda")
sw_num_blocks_prev = 1
# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the sliding window size over training in chunks of 128 from 128 -> 1856. By @fernbear.bsky.social
    frac_done = step / args.num_iterations # training progress
    sw_num_blocks = int(((1 - frac_done) * 128 + frac_done * 1856) // 128)
    if sw_num_blocks != sw_num_blocks_prev:
        sliding_window_num_blocks.copy_(sw_num_blocks, non_blocking=True)
        sw_num_blocks_prev = sw_num_blocks

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch()
                val_loss += model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        print0(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    # uncomment if you want to save any checkpoints
    #save_every = 1000
    #if master_process and (last_step or (save_every > 0 and step % save_every == 0)):
    #    # stop the clock
    #    torch.cuda.synchronize()
    #    training_time_ms += 1000 * (time.perf_counter() - t0)
    #    # save the state of the training process
    #    log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
    #    torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
    #    # start the clock again
    #    torch.cuda.synchronize()
    #    t0 = time.perf_counter()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps + 1):
        with contextlib.ExitStack() as stack:
            if i < train_accumulation_steps: # there's no need to sync gradients every accumulation step
                stack.enter_context(model.no_sync())
            if step >= 5:
                stack.enter_context(torch.compiler.set_stance(skip_guard_eval_unsafe=True))
            model(inputs_train, targets_train, sliding_window_num_blocks).backward()
            inputs_train, targets_train = train_loader.next_batch()
    if train_accumulation_steps != 1:
        for p in model.parameters():
            p.grad /= train_accumulation_steps
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer3.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{args.num_iterations} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")

print0(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()

====================================================================================================
Running python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running pytorch 2.6.0.dev20241203+cu124 compiled for CUDA 12.4
nvidia-smi:
Thu Dec 26 08:22:04 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 PCIe               On  | 00000000:00:07.0 Off |                    0 |
| N/A   42C    P0              82W / 350W |   4162MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  | 00000000:00:08.0 Off |                    0 |
| N/A   46C    P0              87W / 350W |    923MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  | 00000000:00:09.0 Off |                    0 |
| N/A   39C    P0              82W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  | 00000000:00:0A.0 Off |                    0 |
| N/A   41C    P0              81W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  | 00000000:00:0B.0 Off |                    0 |
| N/A   41C    P0              78W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  | 00000000:00:0C.0 Off |                    0 |
| N/A   39C    P0              78W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  | 00000000:00:0D.0 Off |                    0 |
| N/A   46C    P0              84W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  | 00000000:00:0E.0 Off |                    0 |
| N/A   41C    P0              80W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training DataLoader: total number of tokens: 1000000000 across 10 files
Validation DataLoader: total number of tokens: 100000000 across 1 files
====================================================================================================
Centered output layer, initial loss -10.826 => -7.657 (0.1s)
step:0/1480 val_loss:7.7109 train_time:0ms step_avg:nanms
step:1/1480 train_time:23309ms step_avg:nanms
step:2/1480 train_time:24123ms step_avg:nanms
step:3/1480 train_time:24373ms step_avg:nanms
step:4/1480 train_time:24627ms step_avg:nanms
step:5/1480 train_time:24879ms step_avg:nanms
step:6/1480 train_time:25136ms step_avg:nanms
step:7/1480 train_time:25394ms step_avg:nanms
step:8/1480 train_time:25650ms step_avg:nanms
step:9/1480 train_time:25901ms step_avg:nanms
step:10/1480 train_time:26158ms step_avg:nanms
step:11/1480 train_time:255ms step_avg:nanms
step:12/1480 train_time:514ms step_avg:nanms
step:13/1480 train_time:770ms step_avg:256.74ms
step:14/1480 train_time:1024ms step_avg:256.00ms
step:15/1480 train_time:1280ms step_avg:256.10ms
step:16/1480 train_time:1539ms step_avg:256.56ms
step:17/1480 train_time:1796ms step_avg:256.62ms
step:18/1480 train_time:2053ms step_avg:256.69ms
step:19/1480 train_time:2308ms step_avg:256.42ms
step:20/1480 train_time:2562ms step_avg:256.23ms
step:21/1480 train_time:2820ms step_avg:256.34ms
step:22/1480 train_time:3077ms step_avg:256.40ms
step:23/1480 train_time:3336ms step_avg:256.61ms
step:24/1480 train_time:3592ms step_avg:256.60ms
step:25/1480 train_time:3846ms step_avg:256.40ms
step:26/1480 train_time:4102ms step_avg:256.37ms
step:27/1480 train_time:4358ms step_avg:256.37ms
step:28/1480 train_time:4616ms step_avg:256.46ms
step:29/1480 train_time:4874ms step_avg:256.50ms
step:30/1480 train_time:5130ms step_avg:256.51ms
step:31/1480 train_time:5383ms step_avg:256.35ms
step:32/1480 train_time:5642ms step_avg:256.46ms
step:33/1480 train_time:5899ms step_avg:256.48ms
step:34/1480 train_time:6156ms step_avg:256.51ms
step:35/1480 train_time:6416ms step_avg:256.66ms
step:36/1480 train_time:6674ms step_avg:256.68ms
step:37/1480 train_time:6931ms step_avg:256.69ms
step:38/1480 train_time:7184ms step_avg:256.56ms
step:39/1480 train_time:7442ms step_avg:256.61ms
step:40/1480 train_time:7701ms step_avg:256.68ms
step:41/1480 train_time:7957ms step_avg:256.66ms
step:42/1480 train_time:8217ms step_avg:256.79ms
step:43/1480 train_time:8476ms step_avg:256.84ms
step:44/1480 train_time:8733ms step_avg:256.87ms
step:45/1480 train_time:8985ms step_avg:256.72ms
step:46/1480 train_time:9242ms step_avg:256.73ms
step:47/1480 train_time:9501ms step_avg:256.78ms
step:48/1480 train_time:9758ms step_avg:256.80ms
step:49/1480 train_time:10016ms step_avg:256.83ms
step:50/1480 train_time:10276ms step_avg:256.90ms
step:51/1480 train_time:10537ms step_avg:257.01ms
step:52/1480 train_time:10794ms step_avg:256.99ms
step:53/1480 train_time:11048ms step_avg:256.93ms
step:54/1480 train_time:11304ms step_avg:256.91ms
step:55/1480 train_time:11561ms step_avg:256.91ms
step:56/1480 train_time:11819ms step_avg:256.94ms
step:57/1480 train_time:12077ms step_avg:256.96ms
step:58/1480 train_time:12338ms step_avg:257.04ms
step:59/1480 train_time:12595ms step_avg:257.04ms
step:60/1480 train_time:12849ms step_avg:256.97ms
step:61/1480 train_time:13107ms step_avg:257.00ms
step:62/1480 train_time:13363ms step_avg:256.97ms
step:63/1480 train_time:13620ms step_avg:256.99ms
step:64/1480 train_time:13879ms step_avg:257.02ms
step:65/1480 train_time:14140ms step_avg:257.08ms
step:66/1480 train_time:14397ms step_avg:257.09ms
step:67/1480 train_time:14654ms step_avg:257.09ms
step:68/1480 train_time:14909ms step_avg:257.05ms
step:69/1480 train_time:15165ms step_avg:257.03ms
step:70/1480 train_time:15422ms step_avg:257.03ms
step:71/1480 train_time:15679ms step_avg:257.04ms
step:72/1480 train_time:15939ms step_avg:257.09ms
step:73/1480 train_time:16197ms step_avg:257.10ms
step:74/1480 train_time:16455ms step_avg:257.11ms
step:75/1480 train_time:16713ms step_avg:257.12ms
step:76/1480 train_time:16971ms step_avg:257.14ms
step:77/1480 train_time:17231ms step_avg:257.18ms
step:78/1480 train_time:17486ms step_avg:257.15ms
step:79/1480 train_time:17742ms step_avg:257.13ms
step:80/1480 train_time:18001ms step_avg:257.16ms
step:81/1480 train_time:18259ms step_avg:257.16ms
step:82/1480 train_time:18517ms step_avg:257.19ms
step:83/1480 train_time:18777ms step_avg:257.21ms
step:84/1480 train_time:19038ms step_avg:257.27ms
step:85/1480 train_time:19296ms step_avg:257.28ms
step:86/1480 train_time:19552ms step_avg:257.26ms
step:87/1480 train_time:19808ms step_avg:257.25ms
step:88/1480 train_time:20063ms step_avg:257.22ms
step:89/1480 train_time:20321ms step_avg:257.23ms
step:90/1480 train_time:20579ms step_avg:257.24ms
step:91/1480 train_time:20840ms step_avg:257.29ms
step:92/1480 train_time:21098ms step_avg:257.29ms
step:93/1480 train_time:21356ms step_avg:257.30ms
step:94/1480 train_time:21615ms step_avg:257.33ms
step:95/1480 train_time:21878ms step_avg:257.38ms
step:96/1480 train_time:22139ms step_avg:257.43ms
step:97/1480 train_time:22396ms step_avg:257.43ms
step:98/1480 train_time:22654ms step_avg:257.43ms
step:99/1480 train_time:22912ms step_avg:257.44ms
step:100/1480 train_time:23174ms step_avg:257.48ms
step:101/1480 train_time:23431ms step_avg:257.48ms
step:102/1480 train_time:23685ms step_avg:257.45ms
step:103/1480 train_time:23942ms step_avg:257.45ms
step:104/1480 train_time:24202ms step_avg:257.47ms
step:105/1480 train_time:24460ms step_avg:257.47ms
step:106/1480 train_time:24718ms step_avg:257.48ms
step:107/1480 train_time:24978ms step_avg:257.50ms
step:108/1480 train_time:25235ms step_avg:257.50ms
step:109/1480 train_time:25490ms step_avg:257.48ms
step:110/1480 train_time:25745ms step_avg:257.45ms
step:111/1480 train_time:26004ms step_avg:257.47ms
step:112/1480 train_time:26265ms step_avg:257.50ms
step:113/1480 train_time:26526ms step_avg:257.53ms
step:114/1480 train_time:26786ms step_avg:257.56ms
step:115/1480 train_time:27046ms step_avg:257.59ms
step:116/1480 train_time:27309ms step_avg:257.63ms
step:117/1480 train_time:27572ms step_avg:257.68ms
step:118/1480 train_time:27834ms step_avg:257.73ms
step:119/1480 train_time:28096ms step_avg:257.76ms
step:120/1480 train_time:28357ms step_avg:257.79ms
step:121/1480 train_time:28620ms step_avg:257.84ms
step:122/1480 train_time:28881ms step_avg:257.86ms
step:123/1480 train_time:29143ms step_avg:257.91ms
step:124/1480 train_time:29404ms step_avg:257.93ms
step:125/1480 train_time:29669ms step_avg:257.99ms
step:125/1480 val_loss:4.3838 train_time:29795ms step_avg:259.08ms
step:126/1480 train_time:29934ms step_avg:258.05ms
step:127/1480 train_time:30199ms step_avg:258.11ms
step:128/1480 train_time:30460ms step_avg:258.14ms
step:129/1480 train_time:30721ms step_avg:258.16ms
step:130/1480 train_time:30983ms step_avg:258.19ms
step:131/1480 train_time:31244ms step_avg:258.22ms
step:132/1480 train_time:31508ms step_avg:258.26ms
step:133/1480 train_time:31771ms step_avg:258.30ms
step:134/1480 train_time:32032ms step_avg:258.33ms
step:135/1480 train_time:32295ms step_avg:258.36ms
step:136/1480 train_time:32557ms step_avg:258.39ms
step:137/1480 train_time:32818ms step_avg:258.41ms
step:138/1480 train_time:33080ms step_avg:258.43ms
step:139/1480 train_time:33343ms step_avg:258.47ms
step:140/1480 train_time:33606ms step_avg:258.51ms
step:141/1480 train_time:33868ms step_avg:258.54ms
step:142/1480 train_time:34130ms step_avg:258.56ms
step:143/1480 train_time:34391ms step_avg:258.58ms
step:144/1480 train_time:34655ms step_avg:258.62ms
step:145/1480 train_time:34916ms step_avg:258.64ms
step:146/1480 train_time:35180ms step_avg:258.67ms
step:147/1480 train_time:35442ms step_avg:258.70ms
step:148/1480 train_time:35704ms step_avg:258.73ms
step:149/1480 train_time:35965ms step_avg:258.74ms
step:150/1480 train_time:36225ms step_avg:258.75ms
step:151/1480 train_time:36487ms step_avg:258.77ms
step:152/1480 train_time:36750ms step_avg:258.80ms
step:153/1480 train_time:37013ms step_avg:258.83ms
step:154/1480 train_time:37276ms step_avg:258.86ms
step:155/1480 train_time:37538ms step_avg:258.88ms
step:156/1480 train_time:37800ms step_avg:258.90ms
step:157/1480 train_time:38060ms step_avg:258.91ms
step:158/1480 train_time:38322ms step_avg:258.93ms
step:159/1480 train_time:38582ms step_avg:258.94ms
step:160/1480 train_time:38844ms step_avg:258.96ms
step:161/1480 train_time:39106ms step_avg:258.98ms
step:162/1480 train_time:39368ms step_avg:259.00ms
step:163/1480 train_time:39631ms step_avg:259.03ms
step:164/1480 train_time:39894ms step_avg:259.05ms
step:165/1480 train_time:40158ms step_avg:259.08ms
step:166/1480 train_time:40419ms step_avg:259.10ms
step:167/1480 train_time:40682ms step_avg:259.12ms
step:168/1480 train_time:40947ms step_avg:259.16ms
step:169/1480 train_time:41211ms step_avg:259.19ms
step:170/1480 train_time:41476ms step_avg:259.22ms
step:171/1480 train_time:41736ms step_avg:259.23ms
step:172/1480 train_time:41999ms step_avg:259.25ms
step:173/1480 train_time:42260ms step_avg:259.26ms
step:174/1480 train_time:42521ms step_avg:259.27ms
step:175/1480 train_time:42782ms step_avg:259.29ms
step:176/1480 train_time:43044ms step_avg:259.30ms
step:177/1480 train_time:43310ms step_avg:259.34ms
step:178/1480 train_time:43574ms step_avg:259.37ms
step:179/1480 train_time:43836ms step_avg:259.39ms
step:180/1480 train_time:44099ms step_avg:259.40ms
step:181/1480 train_time:44359ms step_avg:259.41ms
step:182/1480 train_time:44619ms step_avg:259.41ms
step:183/1480 train_time:44880ms step_avg:259.42ms
step:184/1480 train_time:45142ms step_avg:259.44ms
step:185/1480 train_time:45407ms step_avg:259.47ms
step:186/1480 train_time:45673ms step_avg:259.51ms
step:187/1480 train_time:45935ms step_avg:259.52ms
step:188/1480 train_time:46197ms step_avg:259.53ms
step:189/1480 train_time:46459ms step_avg:259.55ms
step:190/1480 train_time:46719ms step_avg:259.55ms
step:191/1480 train_time:46981ms step_avg:259.56ms
step:192/1480 train_time:47245ms step_avg:259.59ms
step:193/1480 train_time:47510ms step_avg:259.61ms
step:194/1480 train_time:47775ms step_avg:259.65ms
step:195/1480 train_time:48037ms step_avg:259.66ms
step:196/1480 train_time:48299ms step_avg:259.67ms
step:197/1480 train_time:48559ms step_avg:259.67ms
step:198/1480 train_time:48819ms step_avg:259.67ms
step:199/1480 train_time:49081ms step_avg:259.69ms
step:200/1480 train_time:49344ms step_avg:259.70ms
step:201/1480 train_time:49609ms step_avg:259.73ms
step:202/1480 train_time:49875ms step_avg:259.77ms
step:203/1480 train_time:50137ms step_avg:259.78ms
step:204/1480 train_time:50398ms step_avg:259.78ms
step:205/1480 train_time:50659ms step_avg:259.79ms
step:206/1480 train_time:50919ms step_avg:259.79ms
step:207/1480 train_time:51181ms step_avg:259.80ms
step:208/1480 train_time:51446ms step_avg:259.83ms
step:209/1480 train_time:51710ms step_avg:259.85ms
step:210/1480 train_time:51976ms step_avg:259.88ms
step:211/1480 train_time:52236ms step_avg:259.88ms
step:212/1480 train_time:52499ms step_avg:259.89ms
step:213/1480 train_time:52758ms step_avg:259.89ms
step:214/1480 train_time:53020ms step_avg:259.90ms
step:215/1480 train_time:53282ms step_avg:259.91ms
step:216/1480 train_time:53546ms step_avg:259.93ms
step:217/1480 train_time:53814ms step_avg:259.97ms
step:218/1480 train_time:54078ms step_avg:259.99ms
step:219/1480 train_time:54339ms step_avg:259.99ms
step:220/1480 train_time:54600ms step_avg:260.00ms
step:221/1480 train_time:54864ms step_avg:260.02ms
step:222/1480 train_time:55133ms step_avg:260.06ms
step:223/1480 train_time:55398ms step_avg:260.08ms
step:224/1480 train_time:55663ms step_avg:260.11ms
step:225/1480 train_time:55931ms step_avg:260.15ms
step:226/1480 train_time:56197ms step_avg:260.17ms
step:227/1480 train_time:56463ms step_avg:260.20ms
step:228/1480 train_time:56731ms step_avg:260.23ms
step:229/1480 train_time:56996ms step_avg:260.25ms
step:230/1480 train_time:57260ms step_avg:260.27ms
step:231/1480 train_time:57527ms step_avg:260.30ms
step:232/1480 train_time:57793ms step_avg:260.33ms
step:233/1480 train_time:58059ms step_avg:260.35ms
step:234/1480 train_time:58323ms step_avg:260.37ms
step:235/1480 train_time:58591ms step_avg:260.40ms
step:236/1480 train_time:58858ms step_avg:260.44ms
step:237/1480 train_time:59121ms step_avg:260.45ms
step:238/1480 train_time:59389ms step_avg:260.48ms
step:239/1480 train_time:59657ms step_avg:260.51ms
step:240/1480 train_time:59920ms step_avg:260.52ms
step:241/1480 train_time:60189ms step_avg:260.56ms
step:242/1480 train_time:60456ms step_avg:260.59ms
step:243/1480 train_time:60720ms step_avg:260.60ms
step:244/1480 train_time:60986ms step_avg:260.62ms
step:245/1480 train_time:61255ms step_avg:260.66ms
step:246/1480 train_time:61519ms step_avg:260.68ms
step:247/1480 train_time:61788ms step_avg:260.71ms
step:248/1480 train_time:62055ms step_avg:260.73ms
step:249/1480 train_time:62320ms step_avg:260.75ms
step:250/1480 train_time:62589ms step_avg:260.79ms
step:250/1480 val_loss:4.0061 train_time:62718ms step_avg:261.32ms
step:251/1480 train_time:62856ms step_avg:260.81ms
step:252/1480 train_time:63128ms step_avg:260.86ms
step:253/1480 train_time:63395ms step_avg:260.88ms
step:254/1480 train_time:63665ms step_avg:260.92ms
step:255/1480 train_time:63929ms step_avg:260.93ms
step:256/1480 train_time:64199ms step_avg:260.97ms
step:257/1480 train_time:64468ms step_avg:261.01ms
step:258/1480 train_time:64732ms step_avg:261.02ms
step:259/1480 train_time:65004ms step_avg:261.06ms
step:260/1480 train_time:65270ms step_avg:261.08ms
step:261/1480 train_time:65538ms step_avg:261.11ms
step:262/1480 train_time:65806ms step_avg:261.13ms
step:263/1480 train_time:66070ms step_avg:261.15ms
step:264/1480 train_time:66341ms step_avg:261.18ms
step:265/1480 train_time:66609ms step_avg:261.21ms
step:266/1480 train_time:66874ms step_avg:261.23ms
step:267/1480 train_time:67143ms step_avg:261.26ms
step:268/1480 train_time:67410ms step_avg:261.28ms
step:269/1480 train_time:67677ms step_avg:261.30ms
step:270/1480 train_time:67946ms step_avg:261.33ms
step:271/1480 train_time:68210ms step_avg:261.34ms
step:272/1480 train_time:68477ms step_avg:261.36ms
step:273/1480 train_time:68745ms step_avg:261.39ms
step:274/1480 train_time:69009ms step_avg:261.40ms
step:275/1480 train_time:69277ms step_avg:261.42ms
step:276/1480 train_time:69544ms step_avg:261.45ms
step:277/1480 train_time:69810ms step_avg:261.46ms
step:278/1480 train_time:70075ms step_avg:261.47ms
step:279/1480 train_time:70344ms step_avg:261.50ms
step:280/1480 train_time:70608ms step_avg:261.51ms
step:281/1480 train_time:70875ms step_avg:261.53ms
step:282/1480 train_time:71143ms step_avg:261.55ms
step:283/1480 train_time:71409ms step_avg:261.57ms
step:284/1480 train_time:71674ms step_avg:261.58ms
step:285/1480 train_time:71944ms step_avg:261.61ms
step:286/1480 train_time:72210ms step_avg:261.63ms
step:287/1480 train_time:72477ms step_avg:261.65ms
step:288/1480 train_time:72745ms step_avg:261.67ms
step:289/1480 train_time:73010ms step_avg:261.68ms
step:290/1480 train_time:73276ms step_avg:261.70ms
step:291/1480 train_time:73545ms step_avg:261.73ms
step:292/1480 train_time:73810ms step_avg:261.74ms
step:293/1480 train_time:74077ms step_avg:261.76ms
step:294/1480 train_time:74344ms step_avg:261.78ms
step:295/1480 train_time:74609ms step_avg:261.79ms
step:296/1480 train_time:74873ms step_avg:261.80ms
step:297/1480 train_time:75145ms step_avg:261.83ms
step:298/1480 train_time:75409ms step_avg:261.84ms
step:299/1480 train_time:75677ms step_avg:261.86ms
step:300/1480 train_time:75946ms step_avg:261.88ms
step:301/1480 train_time:76209ms step_avg:261.89ms
step:302/1480 train_time:76478ms step_avg:261.91ms
step:303/1480 train_time:76746ms step_avg:261.93ms
step:304/1480 train_time:77012ms step_avg:261.94ms
step:305/1480 train_time:77281ms step_avg:261.97ms
step:306/1480 train_time:77547ms step_avg:261.98ms
step:307/1480 train_time:77810ms step_avg:261.99ms
step:308/1480 train_time:78079ms step_avg:262.01ms
step:309/1480 train_time:78347ms step_avg:262.03ms
step:310/1480 train_time:78610ms step_avg:262.03ms
step:311/1480 train_time:78876ms step_avg:262.05ms
step:312/1480 train_time:79144ms step_avg:262.07ms
step:313/1480 train_time:79409ms step_avg:262.08ms
step:314/1480 train_time:79677ms step_avg:262.09ms
step:315/1480 train_time:79945ms step_avg:262.11ms
step:316/1480 train_time:80209ms step_avg:262.12ms
step:317/1480 train_time:80477ms step_avg:262.14ms
step:318/1480 train_time:80747ms step_avg:262.16ms
step:319/1480 train_time:81011ms step_avg:262.17ms
step:320/1480 train_time:81279ms step_avg:262.19ms
step:321/1480 train_time:81547ms step_avg:262.21ms
step:322/1480 train_time:81812ms step_avg:262.22ms
step:323/1480 train_time:82080ms step_avg:262.24ms
step:324/1480 train_time:82347ms step_avg:262.25ms
step:325/1480 train_time:82611ms step_avg:262.26ms
step:326/1480 train_time:82880ms step_avg:262.28ms
step:327/1480 train_time:83147ms step_avg:262.29ms
step:328/1480 train_time:83411ms step_avg:262.30ms
step:329/1480 train_time:83680ms step_avg:262.32ms
step:330/1480 train_time:83948ms step_avg:262.34ms
step:331/1480 train_time:84220ms step_avg:262.37ms
step:332/1480 train_time:84488ms step_avg:262.39ms
step:333/1480 train_time:84763ms step_avg:262.42ms
step:334/1480 train_time:85030ms step_avg:262.44ms
step:335/1480 train_time:85301ms step_avg:262.47ms
step:336/1480 train_time:85571ms step_avg:262.49ms
step:337/1480 train_time:85845ms step_avg:262.52ms
step:338/1480 train_time:86112ms step_avg:262.54ms
step:339/1480 train_time:86385ms step_avg:262.57ms
step:340/1480 train_time:86654ms step_avg:262.59ms
step:341/1480 train_time:86927ms step_avg:262.62ms
step:342/1480 train_time:87197ms step_avg:262.64ms
step:343/1480 train_time:87469ms step_avg:262.67ms
step:344/1480 train_time:87739ms step_avg:262.69ms
step:345/1480 train_time:88009ms step_avg:262.71ms
step:346/1480 train_time:88280ms step_avg:262.74ms
step:347/1480 train_time:88549ms step_avg:262.76ms
step:348/1480 train_time:88819ms step_avg:262.78ms
step:349/1480 train_time:89089ms step_avg:262.80ms
step:350/1480 train_time:89362ms step_avg:262.83ms
step:351/1480 train_time:89631ms step_avg:262.85ms
step:352/1480 train_time:89907ms step_avg:262.89ms
step:353/1480 train_time:90178ms step_avg:262.91ms
step:354/1480 train_time:90450ms step_avg:262.94ms
step:355/1480 train_time:90722ms step_avg:262.96ms
step:356/1480 train_time:90989ms step_avg:262.97ms
step:357/1480 train_time:91263ms step_avg:263.01ms
step:358/1480 train_time:91531ms step_avg:263.02ms
step:359/1480 train_time:91803ms step_avg:263.05ms
step:360/1480 train_time:92075ms step_avg:263.07ms
step:361/1480 train_time:92348ms step_avg:263.10ms
step:362/1480 train_time:92616ms step_avg:263.11ms
step:363/1480 train_time:92886ms step_avg:263.13ms
step:364/1480 train_time:93158ms step_avg:263.16ms
step:365/1480 train_time:93428ms step_avg:263.18ms
step:366/1480 train_time:93701ms step_avg:263.21ms
step:367/1480 train_time:93971ms step_avg:263.22ms
step:368/1480 train_time:94246ms step_avg:263.26ms
step:369/1480 train_time:94513ms step_avg:263.27ms
step:370/1480 train_time:94787ms step_avg:263.30ms
step:371/1480 train_time:95056ms step_avg:263.31ms
step:372/1480 train_time:95327ms step_avg:263.34ms
step:373/1480 train_time:95601ms step_avg:263.36ms
step:374/1480 train_time:95871ms step_avg:263.38ms
step:375/1480 train_time:96145ms step_avg:263.41ms
step:375/1480 val_loss:3.8346 train_time:96276ms step_avg:263.77ms
step:376/1480 train_time:96417ms step_avg:263.43ms
step:377/1480 train_time:96693ms step_avg:263.47ms
step:378/1480 train_time:96961ms step_avg:263.48ms
step:379/1480 train_time:97235ms step_avg:263.51ms
step:380/1480 train_time:97502ms step_avg:263.52ms
step:381/1480 train_time:97777ms step_avg:263.55ms
step:382/1480 train_time:98044ms step_avg:263.56ms
step:383/1480 train_time:98315ms step_avg:263.58ms
step:384/1480 train_time:98588ms step_avg:263.60ms
step:385/1480 train_time:98856ms step_avg:263.62ms
step:386/1480 train_time:99131ms step_avg:263.65ms
step:387/1480 train_time:99399ms step_avg:263.66ms
step:388/1480 train_time:99672ms step_avg:263.68ms
step:389/1480 train_time:99941ms step_avg:263.70ms
step:390/1480 train_time:100216ms step_avg:263.73ms
step:391/1480 train_time:100482ms step_avg:263.73ms
step:392/1480 train_time:100755ms step_avg:263.76ms
step:393/1480 train_time:101027ms step_avg:263.78ms
step:394/1480 train_time:101298ms step_avg:263.80ms
step:395/1480 train_time:101572ms step_avg:263.82ms
step:396/1480 train_time:101841ms step_avg:263.84ms
step:397/1480 train_time:102116ms step_avg:263.87ms
step:398/1480 train_time:102387ms step_avg:263.88ms
step:399/1480 train_time:102657ms step_avg:263.90ms
step:400/1480 train_time:102932ms step_avg:263.93ms
step:401/1480 train_time:103200ms step_avg:263.94ms
step:402/1480 train_time:103473ms step_avg:263.96ms
step:403/1480 train_time:103743ms step_avg:263.98ms
step:404/1480 train_time:104016ms step_avg:264.00ms
step:405/1480 train_time:104286ms step_avg:264.02ms
step:406/1480 train_time:104556ms step_avg:264.03ms
step:407/1480 train_time:104829ms step_avg:264.05ms
step:408/1480 train_time:105099ms step_avg:264.07ms
step:409/1480 train_time:105372ms step_avg:264.09ms
step:410/1480 train_time:105643ms step_avg:264.11ms
step:411/1480 train_time:105917ms step_avg:264.13ms
step:412/1480 train_time:106187ms step_avg:264.15ms
step:413/1480 train_time:106456ms step_avg:264.16ms
step:414/1480 train_time:106731ms step_avg:264.19ms
step:415/1480 train_time:106999ms step_avg:264.20ms
step:416/1480 train_time:107273ms step_avg:264.22ms
step:417/1480 train_time:107543ms step_avg:264.23ms
step:418/1480 train_time:107818ms step_avg:264.26ms
step:419/1480 train_time:108085ms step_avg:264.27ms
step:420/1480 train_time:108355ms step_avg:264.28ms
step:421/1480 train_time:108628ms step_avg:264.30ms
step:422/1480 train_time:108898ms step_avg:264.31ms
step:423/1480 train_time:109172ms step_avg:264.34ms
step:424/1480 train_time:109442ms step_avg:264.35ms
step:425/1480 train_time:109715ms step_avg:264.37ms
step:426/1480 train_time:109989ms step_avg:264.40ms
step:427/1480 train_time:110258ms step_avg:264.41ms
step:428/1480 train_time:110529ms step_avg:264.42ms
step:429/1480 train_time:110799ms step_avg:264.44ms
step:430/1480 train_time:111072ms step_avg:264.46ms
step:431/1480 train_time:111342ms step_avg:264.47ms
step:432/1480 train_time:111617ms step_avg:264.49ms
step:433/1480 train_time:111885ms step_avg:264.50ms
step:434/1480 train_time:112156ms step_avg:264.52ms
step:435/1480 train_time:112428ms step_avg:264.54ms
step:436/1480 train_time:112697ms step_avg:264.55ms
step:437/1480 train_time:112971ms step_avg:264.57ms
step:438/1480 train_time:113241ms step_avg:264.58ms
step:439/1480 train_time:113516ms step_avg:264.61ms
step:440/1480 train_time:113784ms step_avg:264.61ms
step:441/1480 train_time:114058ms step_avg:264.64ms
step:442/1480 train_time:114335ms step_avg:264.66ms
step:443/1480 train_time:114610ms step_avg:264.69ms
step:444/1480 train_time:114883ms step_avg:264.71ms
step:445/1480 train_time:115157ms step_avg:264.73ms
step:446/1480 train_time:115433ms step_avg:264.75ms
step:447/1480 train_time:115704ms step_avg:264.77ms
step:448/1480 train_time:115979ms step_avg:264.79ms
step:449/1480 train_time:116254ms step_avg:264.82ms
step:450/1480 train_time:116531ms step_avg:264.84ms
step:451/1480 train_time:116800ms step_avg:264.85ms
step:452/1480 train_time:117078ms step_avg:264.88ms
step:453/1480 train_time:117354ms step_avg:264.91ms
step:454/1480 train_time:117628ms step_avg:264.93ms
step:455/1480 train_time:117901ms step_avg:264.95ms
step:456/1480 train_time:118177ms step_avg:264.97ms
step:457/1480 train_time:118454ms step_avg:265.00ms
step:458/1480 train_time:118726ms step_avg:265.01ms
step:459/1480 train_time:118999ms step_avg:265.03ms
step:460/1480 train_time:119274ms step_avg:265.05ms
step:461/1480 train_time:119549ms step_avg:265.08ms
step:462/1480 train_time:119821ms step_avg:265.09ms
step:463/1480 train_time:120096ms step_avg:265.11ms
step:464/1480 train_time:120375ms step_avg:265.14ms
step:465/1480 train_time:120653ms step_avg:265.17ms
step:466/1480 train_time:120922ms step_avg:265.18ms
step:467/1480 train_time:121198ms step_avg:265.20ms
step:468/1480 train_time:121473ms step_avg:265.23ms
step:469/1480 train_time:121748ms step_avg:265.25ms
step:470/1480 train_time:122020ms step_avg:265.26ms
step:471/1480 train_time:122295ms step_avg:265.28ms
step:472/1480 train_time:122570ms step_avg:265.30ms
step:473/1480 train_time:122841ms step_avg:265.32ms
step:474/1480 train_time:123118ms step_avg:265.34ms
step:475/1480 train_time:123396ms step_avg:265.37ms
step:476/1480 train_time:123668ms step_avg:265.38ms
step:477/1480 train_time:123941ms step_avg:265.40ms
step:478/1480 train_time:124219ms step_avg:265.42ms
step:479/1480 train_time:124492ms step_avg:265.44ms
step:480/1480 train_time:124761ms step_avg:265.45ms
step:481/1480 train_time:125039ms step_avg:265.47ms
step:482/1480 train_time:125314ms step_avg:265.50ms
step:483/1480 train_time:125589ms step_avg:265.52ms
step:484/1480 train_time:125860ms step_avg:265.53ms
step:485/1480 train_time:126136ms step_avg:265.55ms
step:486/1480 train_time:126410ms step_avg:265.57ms
step:487/1480 train_time:126683ms step_avg:265.58ms
step:488/1480 train_time:126957ms step_avg:265.60ms
step:489/1480 train_time:127233ms step_avg:265.62ms
step:490/1480 train_time:127509ms step_avg:265.64ms
step:491/1480 train_time:127780ms step_avg:265.65ms
step:492/1480 train_time:128056ms step_avg:265.68ms
step:493/1480 train_time:128333ms step_avg:265.70ms
step:494/1480 train_time:128604ms step_avg:265.71ms
step:495/1480 train_time:128879ms step_avg:265.73ms
step:496/1480 train_time:129156ms step_avg:265.75ms
step:497/1480 train_time:129431ms step_avg:265.77ms
step:498/1480 train_time:129701ms step_avg:265.78ms
step:499/1480 train_time:129978ms step_avg:265.80ms
step:500/1480 train_time:130255ms step_avg:265.83ms
step:500/1480 val_loss:3.7189 train_time:130389ms step_avg:266.10ms
step:501/1480 train_time:130531ms step_avg:265.85ms
step:502/1480 train_time:130808ms step_avg:265.87ms
step:503/1480 train_time:131086ms step_avg:265.89ms
step:504/1480 train_time:131360ms step_avg:265.91ms
step:505/1480 train_time:131631ms step_avg:265.92ms
step:506/1480 train_time:131908ms step_avg:265.94ms
step:507/1480 train_time:132182ms step_avg:265.96ms
step:508/1480 train_time:132456ms step_avg:265.98ms
step:509/1480 train_time:132728ms step_avg:265.99ms
step:510/1480 train_time:133005ms step_avg:266.01ms
step:511/1480 train_time:133279ms step_avg:266.03ms
step:512/1480 train_time:133551ms step_avg:266.04ms
step:513/1480 train_time:133826ms step_avg:266.06ms
step:514/1480 train_time:134103ms step_avg:266.08ms
step:515/1480 train_time:134373ms step_avg:266.09ms
step:516/1480 train_time:134648ms step_avg:266.10ms
step:517/1480 train_time:134926ms step_avg:266.13ms
step:518/1480 train_time:135201ms step_avg:266.14ms
step:519/1480 train_time:135472ms step_avg:266.15ms
step:520/1480 train_time:135748ms step_avg:266.17ms
step:521/1480 train_time:136023ms step_avg:266.19ms
step:522/1480 train_time:136299ms step_avg:266.21ms
step:523/1480 train_time:136572ms step_avg:266.22ms
step:524/1480 train_time:136848ms step_avg:266.24ms
step:525/1480 train_time:137122ms step_avg:266.26ms
step:526/1480 train_time:137397ms step_avg:266.27ms
step:527/1480 train_time:137669ms step_avg:266.29ms
step:528/1480 train_time:137946ms step_avg:266.31ms
step:529/1480 train_time:138223ms step_avg:266.33ms
step:530/1480 train_time:138496ms step_avg:266.34ms
step:531/1480 train_time:138769ms step_avg:266.35ms
step:532/1480 train_time:139045ms step_avg:266.37ms
step:533/1480 train_time:139320ms step_avg:266.39ms
step:534/1480 train_time:139591ms step_avg:266.40ms
step:535/1480 train_time:139867ms step_avg:266.41ms
step:536/1480 train_time:140144ms step_avg:266.43ms
step:537/1480 train_time:140420ms step_avg:266.45ms
step:538/1480 train_time:140691ms step_avg:266.46ms
step:539/1480 train_time:140964ms step_avg:266.47ms
step:540/1480 train_time:141237ms step_avg:266.48ms
step:541/1480 train_time:141511ms step_avg:266.50ms
step:542/1480 train_time:141786ms step_avg:266.52ms
step:543/1480 train_time:142062ms step_avg:266.53ms
step:544/1480 train_time:142332ms step_avg:266.54ms
step:545/1480 train_time:142607ms step_avg:266.56ms
step:546/1480 train_time:142882ms step_avg:266.57ms
step:547/1480 train_time:143153ms step_avg:266.58ms
step:548/1480 train_time:143429ms step_avg:266.60ms
step:549/1480 train_time:143704ms step_avg:266.61ms
step:550/1480 train_time:143978ms step_avg:266.63ms
step:551/1480 train_time:144253ms step_avg:266.64ms
step:552/1480 train_time:144530ms step_avg:266.66ms
step:553/1480 train_time:144808ms step_avg:266.68ms
step:554/1480 train_time:145085ms step_avg:266.70ms
step:555/1480 train_time:145366ms step_avg:266.73ms
step:556/1480 train_time:145641ms step_avg:266.74ms
step:557/1480 train_time:145917ms step_avg:266.76ms
step:558/1480 train_time:146191ms step_avg:266.77ms
step:559/1480 train_time:146469ms step_avg:266.79ms
step:560/1480 train_time:146748ms step_avg:266.81ms
step:561/1480 train_time:147026ms step_avg:266.83ms
step:562/1480 train_time:147304ms step_avg:266.86ms
step:563/1480 train_time:147576ms step_avg:266.86ms
step:564/1480 train_time:147852ms step_avg:266.88ms
step:565/1480 train_time:148130ms step_avg:266.90ms
step:566/1480 train_time:148407ms step_avg:266.92ms
step:567/1480 train_time:148685ms step_avg:266.94ms
step:568/1480 train_time:148964ms step_avg:266.96ms
step:569/1480 train_time:149245ms step_avg:266.99ms
step:570/1480 train_time:149520ms step_avg:267.00ms
step:571/1480 train_time:149798ms step_avg:267.02ms
step:572/1480 train_time:150072ms step_avg:267.03ms
step:573/1480 train_time:150350ms step_avg:267.05ms
step:574/1480 train_time:150628ms step_avg:267.07ms
step:575/1480 train_time:150908ms step_avg:267.09ms
step:576/1480 train_time:151186ms step_avg:267.11ms
step:577/1480 train_time:151463ms step_avg:267.13ms
step:578/1480 train_time:151736ms step_avg:267.14ms
step:579/1480 train_time:152012ms step_avg:267.16ms
step:580/1480 train_time:152290ms step_avg:267.17ms
step:581/1480 train_time:152568ms step_avg:267.19ms
step:582/1480 train_time:152847ms step_avg:267.21ms
step:583/1480 train_time:153126ms step_avg:267.24ms
step:584/1480 train_time:153404ms step_avg:267.25ms
step:585/1480 train_time:153684ms step_avg:267.28ms
step:586/1480 train_time:153962ms step_avg:267.30ms
step:587/1480 train_time:154242ms step_avg:267.32ms
step:588/1480 train_time:154519ms step_avg:267.33ms
step:589/1480 train_time:154791ms step_avg:267.34ms
step:590/1480 train_time:155070ms step_avg:267.36ms
step:591/1480 train_time:155348ms step_avg:267.38ms
step:592/1480 train_time:155627ms step_avg:267.40ms
step:593/1480 train_time:155907ms step_avg:267.42ms
step:594/1480 train_time:156183ms step_avg:267.44ms
step:595/1480 train_time:156463ms step_avg:267.46ms
step:596/1480 train_time:156737ms step_avg:267.47ms
step:597/1480 train_time:157014ms step_avg:267.49ms
step:598/1480 train_time:157291ms step_avg:267.50ms
step:599/1480 train_time:157569ms step_avg:267.52ms
step:600/1480 train_time:157846ms step_avg:267.53ms
step:601/1480 train_time:158124ms step_avg:267.55ms
step:602/1480 train_time:158405ms step_avg:267.58ms
step:603/1480 train_time:158681ms step_avg:267.59ms
step:604/1480 train_time:158953ms step_avg:267.60ms
step:605/1480 train_time:159232ms step_avg:267.62ms
step:606/1480 train_time:159511ms step_avg:267.64ms
step:607/1480 train_time:159790ms step_avg:267.66ms
step:608/1480 train_time:160069ms step_avg:267.67ms
step:609/1480 train_time:160347ms step_avg:267.69ms
step:610/1480 train_time:160627ms step_avg:267.71ms
step:611/1480 train_time:160907ms step_avg:267.73ms
step:612/1480 train_time:161188ms step_avg:267.75ms
step:613/1480 train_time:161468ms step_avg:267.77ms
step:614/1480 train_time:161746ms step_avg:267.79ms
step:615/1480 train_time:162025ms step_avg:267.81ms
step:616/1480 train_time:162305ms step_avg:267.83ms
step:617/1480 train_time:162579ms step_avg:267.84ms
step:618/1480 train_time:162853ms step_avg:267.85ms
step:619/1480 train_time:163129ms step_avg:267.86ms
step:620/1480 train_time:163408ms step_avg:267.88ms
step:621/1480 train_time:163686ms step_avg:267.90ms
step:622/1480 train_time:163966ms step_avg:267.92ms
step:623/1480 train_time:164244ms step_avg:267.93ms
step:624/1480 train_time:164523ms step_avg:267.95ms
step:625/1480 train_time:164804ms step_avg:267.97ms
step:625/1480 val_loss:3.6372 train_time:164940ms step_avg:268.20ms
step:626/1480 train_time:165081ms step_avg:267.99ms
step:627/1480 train_time:165362ms step_avg:268.01ms
step:628/1480 train_time:165641ms step_avg:268.03ms
step:629/1480 train_time:165918ms step_avg:268.04ms
step:630/1480 train_time:166198ms step_avg:268.06ms
step:631/1480 train_time:166474ms step_avg:268.07ms
step:632/1480 train_time:166749ms step_avg:268.08ms
step:633/1480 train_time:167022ms step_avg:268.09ms
step:634/1480 train_time:167301ms step_avg:268.11ms
step:635/1480 train_time:167578ms step_avg:268.13ms
step:636/1480 train_time:167859ms step_avg:268.14ms
step:637/1480 train_time:168133ms step_avg:268.15ms
step:638/1480 train_time:168412ms step_avg:268.17ms
step:639/1480 train_time:168684ms step_avg:268.18ms
step:640/1480 train_time:168963ms step_avg:268.20ms
step:641/1480 train_time:169240ms step_avg:268.21ms
step:642/1480 train_time:169518ms step_avg:268.22ms
step:643/1480 train_time:169797ms step_avg:268.24ms
step:644/1480 train_time:170078ms step_avg:268.26ms
step:645/1480 train_time:170358ms step_avg:268.28ms
step:646/1480 train_time:170634ms step_avg:268.29ms
step:647/1480 train_time:170908ms step_avg:268.30ms
step:648/1480 train_time:171185ms step_avg:268.31ms
step:649/1480 train_time:171463ms step_avg:268.33ms
step:650/1480 train_time:171741ms step_avg:268.35ms
step:651/1480 train_time:172020ms step_avg:268.36ms
step:652/1480 train_time:172298ms step_avg:268.38ms
step:653/1480 train_time:172579ms step_avg:268.40ms
step:654/1480 train_time:172860ms step_avg:268.42ms
step:655/1480 train_time:173137ms step_avg:268.43ms
step:656/1480 train_time:173415ms step_avg:268.44ms
step:657/1480 train_time:173692ms step_avg:268.46ms
step:658/1480 train_time:173969ms step_avg:268.47ms
step:659/1480 train_time:174245ms step_avg:268.48ms
step:660/1480 train_time:174523ms step_avg:268.50ms
step:661/1480 train_time:174803ms step_avg:268.51ms
step:662/1480 train_time:175082ms step_avg:268.53ms
step:663/1480 train_time:175362ms step_avg:268.55ms
step:664/1480 train_time:175641ms step_avg:268.56ms
step:665/1480 train_time:175921ms step_avg:268.58ms
step:666/1480 train_time:176200ms step_avg:268.60ms
step:667/1480 train_time:176481ms step_avg:268.62ms
step:668/1480 train_time:176762ms step_avg:268.64ms
step:669/1480 train_time:177041ms step_avg:268.65ms
step:670/1480 train_time:177320ms step_avg:268.67ms
step:671/1480 train_time:177599ms step_avg:268.68ms
step:672/1480 train_time:177880ms step_avg:268.70ms
step:673/1480 train_time:178160ms step_avg:268.72ms
step:674/1480 train_time:178439ms step_avg:268.73ms
step:675/1480 train_time:178719ms step_avg:268.75ms
step:676/1480 train_time:178999ms step_avg:268.77ms
step:677/1480 train_time:179280ms step_avg:268.79ms
step:678/1480 train_time:179559ms step_avg:268.80ms
step:679/1480 train_time:179839ms step_avg:268.82ms
step:680/1480 train_time:180121ms step_avg:268.84ms
step:681/1480 train_time:180400ms step_avg:268.85ms
step:682/1480 train_time:180682ms step_avg:268.87ms
step:683/1480 train_time:180962ms step_avg:268.89ms
step:684/1480 train_time:181239ms step_avg:268.90ms
step:685/1480 train_time:181518ms step_avg:268.91ms
step:686/1480 train_time:181800ms step_avg:268.93ms
step:687/1480 train_time:182080ms step_avg:268.95ms
step:688/1480 train_time:182363ms step_avg:268.97ms
step:689/1480 train_time:182642ms step_avg:268.99ms
step:690/1480 train_time:182921ms step_avg:269.00ms
step:691/1480 train_time:183203ms step_avg:269.02ms
step:692/1480 train_time:183483ms step_avg:269.04ms
step:693/1480 train_time:183762ms step_avg:269.05ms
step:694/1480 train_time:184040ms step_avg:269.07ms
step:695/1480 train_time:184320ms step_avg:269.08ms
step:696/1480 train_time:184603ms step_avg:269.10ms
step:697/1480 train_time:184881ms step_avg:269.11ms
step:698/1480 train_time:185161ms step_avg:269.13ms
step:699/1480 train_time:185441ms step_avg:269.14ms
step:700/1480 train_time:185719ms step_avg:269.16ms
step:701/1480 train_time:186000ms step_avg:269.17ms
step:702/1480 train_time:186283ms step_avg:269.20ms
step:703/1480 train_time:186562ms step_avg:269.21ms
step:704/1480 train_time:186841ms step_avg:269.22ms
step:705/1480 train_time:187122ms step_avg:269.24ms
step:706/1480 train_time:187399ms step_avg:269.25ms
step:707/1480 train_time:187680ms step_avg:269.27ms
step:708/1480 train_time:187962ms step_avg:269.29ms
step:709/1480 train_time:188241ms step_avg:269.30ms
step:710/1480 train_time:188520ms step_avg:269.31ms
step:711/1480 train_time:188801ms step_avg:269.33ms
step:712/1480 train_time:189082ms step_avg:269.35ms
step:713/1480 train_time:189360ms step_avg:269.36ms
step:714/1480 train_time:189640ms step_avg:269.37ms
step:715/1480 train_time:189920ms step_avg:269.39ms
step:716/1480 train_time:190201ms step_avg:269.41ms
step:717/1480 train_time:190481ms step_avg:269.42ms
step:718/1480 train_time:190760ms step_avg:269.44ms
step:719/1480 train_time:191039ms step_avg:269.45ms
step:720/1480 train_time:191319ms step_avg:269.46ms
step:721/1480 train_time:191598ms step_avg:269.48ms
step:722/1480 train_time:191879ms step_avg:269.49ms
step:723/1480 train_time:192161ms step_avg:269.51ms
step:724/1480 train_time:192438ms step_avg:269.52ms
step:725/1480 train_time:192720ms step_avg:269.54ms
step:726/1480 train_time:193001ms step_avg:269.55ms
step:727/1480 train_time:193282ms step_avg:269.57ms
step:728/1480 train_time:193561ms step_avg:269.58ms
step:729/1480 train_time:193840ms step_avg:269.60ms
step:730/1480 train_time:194121ms step_avg:269.61ms
step:731/1480 train_time:194402ms step_avg:269.63ms
step:732/1480 train_time:194682ms step_avg:269.64ms
step:733/1480 train_time:194963ms step_avg:269.66ms
step:734/1480 train_time:195240ms step_avg:269.67ms
step:735/1480 train_time:195520ms step_avg:269.68ms
step:736/1480 train_time:195801ms step_avg:269.70ms
step:737/1480 train_time:196080ms step_avg:269.71ms
step:738/1480 train_time:196362ms step_avg:269.73ms
step:739/1480 train_time:196640ms step_avg:269.74ms
step:740/1480 train_time:196918ms step_avg:269.75ms
step:741/1480 train_time:197197ms step_avg:269.76ms
step:742/1480 train_time:197477ms step_avg:269.78ms
step:743/1480 train_time:197756ms step_avg:269.79ms
step:744/1480 train_time:198030ms step_avg:269.80ms
step:745/1480 train_time:198312ms step_avg:269.81ms
step:746/1480 train_time:198587ms step_avg:269.82ms
step:747/1480 train_time:198865ms step_avg:269.83ms
step:748/1480 train_time:199143ms step_avg:269.84ms
step:749/1480 train_time:199422ms step_avg:269.85ms
step:750/1480 train_time:199700ms step_avg:269.87ms
step:750/1480 val_loss:3.5790 train_time:199838ms step_avg:270.05ms
step:751/1480 train_time:199981ms step_avg:269.88ms
step:752/1480 train_time:200261ms step_avg:269.89ms
step:753/1480 train_time:200539ms step_avg:269.90ms
step:754/1480 train_time:200818ms step_avg:269.92ms
step:755/1480 train_time:201100ms step_avg:269.93ms
step:756/1480 train_time:201380ms step_avg:269.95ms
step:757/1480 train_time:201660ms step_avg:269.96ms
step:758/1480 train_time:201940ms step_avg:269.97ms
step:759/1480 train_time:202219ms step_avg:269.99ms
step:760/1480 train_time:202498ms step_avg:270.00ms
step:761/1480 train_time:202776ms step_avg:270.01ms
step:762/1480 train_time:203061ms step_avg:270.03ms
step:763/1480 train_time:203341ms step_avg:270.04ms
step:764/1480 train_time:203620ms step_avg:270.05ms
step:765/1480 train_time:203900ms step_avg:270.07ms
step:766/1480 train_time:204181ms step_avg:270.08ms
step:767/1480 train_time:204459ms step_avg:270.09ms
step:768/1480 train_time:204738ms step_avg:270.10ms
step:769/1480 train_time:205018ms step_avg:270.12ms
step:770/1480 train_time:205300ms step_avg:270.13ms
step:771/1480 train_time:205579ms step_avg:270.14ms
step:772/1480 train_time:205861ms step_avg:270.16ms
step:773/1480 train_time:206141ms step_avg:270.17ms
step:774/1480 train_time:206425ms step_avg:270.19ms
step:775/1480 train_time:206703ms step_avg:270.20ms
step:776/1480 train_time:206982ms step_avg:270.21ms
step:777/1480 train_time:207264ms step_avg:270.23ms
step:778/1480 train_time:207544ms step_avg:270.24ms
step:779/1480 train_time:207826ms step_avg:270.25ms
step:780/1480 train_time:208104ms step_avg:270.27ms
step:781/1480 train_time:208383ms step_avg:270.28ms
step:782/1480 train_time:208665ms step_avg:270.29ms
step:783/1480 train_time:208949ms step_avg:270.31ms
step:784/1480 train_time:209229ms step_avg:270.32ms
step:785/1480 train_time:209512ms step_avg:270.34ms
step:786/1480 train_time:209797ms step_avg:270.36ms
step:787/1480 train_time:210081ms step_avg:270.37ms
step:788/1480 train_time:210364ms step_avg:270.39ms
step:789/1480 train_time:210641ms step_avg:270.40ms
step:790/1480 train_time:210921ms step_avg:270.41ms
step:791/1480 train_time:211202ms step_avg:270.43ms
step:792/1480 train_time:211483ms step_avg:270.44ms
step:793/1480 train_time:211761ms step_avg:270.45ms
step:794/1480 train_time:212044ms step_avg:270.46ms
step:795/1480 train_time:212329ms step_avg:270.48ms
step:796/1480 train_time:212611ms step_avg:270.50ms
step:797/1480 train_time:212894ms step_avg:270.51ms
step:798/1480 train_time:213176ms step_avg:270.53ms
step:799/1480 train_time:213457ms step_avg:270.54ms
step:800/1480 train_time:213738ms step_avg:270.55ms
step:801/1480 train_time:214019ms step_avg:270.57ms
step:802/1480 train_time:214301ms step_avg:270.58ms
step:803/1480 train_time:214586ms step_avg:270.60ms
step:804/1480 train_time:214864ms step_avg:270.61ms
step:805/1480 train_time:215144ms step_avg:270.62ms
step:806/1480 train_time:215424ms step_avg:270.63ms
step:807/1480 train_time:215712ms step_avg:270.66ms
step:808/1480 train_time:215992ms step_avg:270.67ms
step:809/1480 train_time:216272ms step_avg:270.68ms
step:810/1480 train_time:216558ms step_avg:270.70ms
step:811/1480 train_time:216839ms step_avg:270.71ms
step:812/1480 train_time:217120ms step_avg:270.72ms
step:813/1480 train_time:217402ms step_avg:270.74ms
step:814/1480 train_time:217681ms step_avg:270.75ms
step:815/1480 train_time:217970ms step_avg:270.77ms
step:816/1480 train_time:218252ms step_avg:270.78ms
step:817/1480 train_time:218531ms step_avg:270.79ms
step:818/1480 train_time:218812ms step_avg:270.81ms
step:819/1480 train_time:219095ms step_avg:270.82ms
step:820/1480 train_time:219375ms step_avg:270.83ms
step:821/1480 train_time:219659ms step_avg:270.85ms
step:822/1480 train_time:219938ms step_avg:270.86ms
step:823/1480 train_time:220221ms step_avg:270.87ms
step:824/1480 train_time:220501ms step_avg:270.89ms
step:825/1480 train_time:220782ms step_avg:270.90ms
step:826/1480 train_time:221061ms step_avg:270.91ms
step:827/1480 train_time:221341ms step_avg:270.92ms
step:828/1480 train_time:221622ms step_avg:270.93ms
step:829/1480 train_time:221902ms step_avg:270.94ms
step:830/1480 train_time:222187ms step_avg:270.96ms
step:831/1480 train_time:222463ms step_avg:270.97ms
step:832/1480 train_time:222745ms step_avg:270.98ms
step:833/1480 train_time:223033ms step_avg:271.00ms
step:834/1480 train_time:223314ms step_avg:271.01ms
step:835/1480 train_time:223601ms step_avg:271.03ms
step:836/1480 train_time:223882ms step_avg:271.04ms
step:837/1480 train_time:224165ms step_avg:271.06ms
step:838/1480 train_time:224449ms step_avg:271.07ms
step:839/1480 train_time:224731ms step_avg:271.09ms
step:840/1480 train_time:225012ms step_avg:271.10ms
step:841/1480 train_time:225294ms step_avg:271.11ms
step:842/1480 train_time:225576ms step_avg:271.12ms
step:843/1480 train_time:225860ms step_avg:271.14ms
step:844/1480 train_time:226143ms step_avg:271.15ms
step:845/1480 train_time:226422ms step_avg:271.16ms
step:846/1480 train_time:226703ms step_avg:271.18ms
step:847/1480 train_time:226983ms step_avg:271.19ms
step:848/1480 train_time:227263ms step_avg:271.20ms
step:849/1480 train_time:227543ms step_avg:271.21ms
step:850/1480 train_time:227823ms step_avg:271.22ms
step:851/1480 train_time:228108ms step_avg:271.23ms
step:852/1480 train_time:228387ms step_avg:271.24ms
step:853/1480 train_time:228672ms step_avg:271.26ms
step:854/1480 train_time:228956ms step_avg:271.27ms
step:855/1480 train_time:229237ms step_avg:271.29ms
step:856/1480 train_time:229518ms step_avg:271.30ms
step:857/1480 train_time:229802ms step_avg:271.31ms
step:858/1480 train_time:230083ms step_avg:271.32ms
step:859/1480 train_time:230363ms step_avg:271.33ms
step:860/1480 train_time:230643ms step_avg:271.35ms
step:861/1480 train_time:230923ms step_avg:271.35ms
step:862/1480 train_time:231202ms step_avg:271.36ms
step:863/1480 train_time:231485ms step_avg:271.38ms
step:864/1480 train_time:231766ms step_avg:271.39ms
step:865/1480 train_time:232042ms step_avg:271.39ms
step:866/1480 train_time:232323ms step_avg:271.41ms
step:867/1480 train_time:232604ms step_avg:271.42ms
step:868/1480 train_time:232891ms step_avg:271.44ms
step:869/1480 train_time:233173ms step_avg:271.45ms
step:870/1480 train_time:233457ms step_avg:271.46ms
step:871/1480 train_time:233739ms step_avg:271.47ms
step:872/1480 train_time:234021ms step_avg:271.49ms
step:873/1480 train_time:234301ms step_avg:271.50ms
step:874/1480 train_time:234584ms step_avg:271.51ms
step:875/1480 train_time:234864ms step_avg:271.52ms
step:875/1480 val_loss:3.5363 train_time:235001ms step_avg:271.68ms
step:876/1480 train_time:235145ms step_avg:271.53ms
step:877/1480 train_time:235427ms step_avg:271.54ms
step:878/1480 train_time:235710ms step_avg:271.56ms
step:879/1480 train_time:235988ms step_avg:271.56ms
step:880/1480 train_time:236271ms step_avg:271.58ms
step:881/1480 train_time:236561ms step_avg:271.60ms
step:882/1480 train_time:236845ms step_avg:271.61ms
step:883/1480 train_time:237128ms step_avg:271.62ms
step:884/1480 train_time:237415ms step_avg:271.64ms
step:885/1480 train_time:237701ms step_avg:271.66ms
step:886/1480 train_time:237983ms step_avg:271.67ms
step:887/1480 train_time:238263ms step_avg:271.68ms
step:888/1480 train_time:238548ms step_avg:271.69ms
step:889/1480 train_time:238834ms step_avg:271.71ms
step:890/1480 train_time:239122ms step_avg:271.73ms
step:891/1480 train_time:239400ms step_avg:271.74ms
step:892/1480 train_time:239687ms step_avg:271.75ms
step:893/1480 train_time:239982ms step_avg:271.78ms
step:894/1480 train_time:240264ms step_avg:271.79ms
step:895/1480 train_time:240543ms step_avg:271.80ms
step:896/1480 train_time:240825ms step_avg:271.81ms
step:897/1480 train_time:241110ms step_avg:271.83ms
step:898/1480 train_time:241389ms step_avg:271.83ms
step:899/1480 train_time:241671ms step_avg:271.85ms
step:900/1480 train_time:241958ms step_avg:271.86ms
step:901/1480 train_time:242239ms step_avg:271.87ms
step:902/1480 train_time:242524ms step_avg:271.89ms
step:903/1480 train_time:242806ms step_avg:271.90ms
step:904/1480 train_time:243088ms step_avg:271.91ms
step:905/1480 train_time:243371ms step_avg:271.92ms
step:906/1480 train_time:243658ms step_avg:271.94ms
step:907/1480 train_time:243940ms step_avg:271.95ms
step:908/1480 train_time:244223ms step_avg:271.96ms
step:909/1480 train_time:244507ms step_avg:271.98ms
step:910/1480 train_time:244787ms step_avg:271.99ms
step:911/1480 train_time:245065ms step_avg:271.99ms
step:912/1480 train_time:245348ms step_avg:272.00ms
step:913/1480 train_time:245638ms step_avg:272.02ms
step:914/1480 train_time:245923ms step_avg:272.04ms
step:915/1480 train_time:246207ms step_avg:272.05ms
step:916/1480 train_time:246489ms step_avg:272.06ms
step:917/1480 train_time:246780ms step_avg:272.08ms
step:918/1480 train_time:247064ms step_avg:272.10ms
step:919/1480 train_time:247348ms step_avg:272.11ms
step:920/1480 train_time:247634ms step_avg:272.13ms
step:921/1480 train_time:247917ms step_avg:272.14ms
step:922/1480 train_time:248203ms step_avg:272.15ms
step:923/1480 train_time:248485ms step_avg:272.16ms
step:924/1480 train_time:248767ms step_avg:272.17ms
step:925/1480 train_time:249055ms step_avg:272.19ms
step:926/1480 train_time:249335ms step_avg:272.20ms
step:927/1480 train_time:249616ms step_avg:272.21ms
step:928/1480 train_time:249904ms step_avg:272.23ms
step:929/1480 train_time:250194ms step_avg:272.25ms
step:930/1480 train_time:250480ms step_avg:272.26ms
step:931/1480 train_time:250762ms step_avg:272.27ms
step:932/1480 train_time:251048ms step_avg:272.29ms
step:933/1480 train_time:251328ms step_avg:272.29ms
step:934/1480 train_time:251615ms step_avg:272.31ms
step:935/1480 train_time:251900ms step_avg:272.32ms
step:936/1480 train_time:252182ms step_avg:272.33ms
step:937/1480 train_time:252464ms step_avg:272.35ms
step:938/1480 train_time:252746ms step_avg:272.36ms
step:939/1480 train_time:253036ms step_avg:272.37ms
step:940/1480 train_time:253323ms step_avg:272.39ms
step:941/1480 train_time:253605ms step_avg:272.40ms
step:942/1480 train_time:253885ms step_avg:272.41ms
step:943/1480 train_time:254166ms step_avg:272.42ms
step:944/1480 train_time:254448ms step_avg:272.43ms
step:945/1480 train_time:254735ms step_avg:272.44ms
step:946/1480 train_time:255022ms step_avg:272.46ms
step:947/1480 train_time:255304ms step_avg:272.47ms
step:948/1480 train_time:255585ms step_avg:272.48ms
step:949/1480 train_time:255865ms step_avg:272.49ms
step:950/1480 train_time:256147ms step_avg:272.50ms
step:951/1480 train_time:256435ms step_avg:272.51ms
step:952/1480 train_time:256721ms step_avg:272.53ms
step:953/1480 train_time:257004ms step_avg:272.54ms
step:954/1480 train_time:257290ms step_avg:272.55ms
step:955/1480 train_time:257568ms step_avg:272.56ms
step:956/1480 train_time:257852ms step_avg:272.57ms
step:957/1480 train_time:258135ms step_avg:272.58ms
step:958/1480 train_time:258421ms step_avg:272.60ms
step:959/1480 train_time:258707ms step_avg:272.61ms
step:960/1480 train_time:258986ms step_avg:272.62ms
step:961/1480 train_time:259264ms step_avg:272.62ms
step:962/1480 train_time:259549ms step_avg:272.64ms
step:963/1480 train_time:259829ms step_avg:272.64ms
step:964/1480 train_time:260111ms step_avg:272.65ms
step:965/1480 train_time:260393ms step_avg:272.66ms
step:966/1480 train_time:260680ms step_avg:272.68ms
step:967/1480 train_time:260963ms step_avg:272.69ms
step:968/1480 train_time:261244ms step_avg:272.70ms
step:969/1480 train_time:261529ms step_avg:272.71ms
step:970/1480 train_time:261807ms step_avg:272.72ms
step:971/1480 train_time:262095ms step_avg:272.73ms
step:972/1480 train_time:262382ms step_avg:272.75ms
step:973/1480 train_time:262663ms step_avg:272.75ms
step:974/1480 train_time:262944ms step_avg:272.76ms
step:975/1480 train_time:263228ms step_avg:272.78ms
step:976/1480 train_time:263513ms step_avg:272.79ms
step:977/1480 train_time:263798ms step_avg:272.80ms
step:978/1480 train_time:264086ms step_avg:272.82ms
step:979/1480 train_time:264369ms step_avg:272.83ms
step:980/1480 train_time:264658ms step_avg:272.84ms
step:981/1480 train_time:264942ms step_avg:272.85ms
step:982/1480 train_time:265225ms step_avg:272.87ms
step:983/1480 train_time:265507ms step_avg:272.87ms
step:984/1480 train_time:265789ms step_avg:272.88ms
step:985/1480 train_time:266077ms step_avg:272.90ms
step:986/1480 train_time:266363ms step_avg:272.91ms
step:987/1480 train_time:266645ms step_avg:272.92ms
step:988/1480 train_time:266928ms step_avg:272.93ms
step:989/1480 train_time:267223ms step_avg:272.96ms
step:990/1480 train_time:267507ms step_avg:272.97ms
step:991/1480 train_time:267793ms step_avg:272.98ms
step:992/1480 train_time:268081ms step_avg:272.99ms
step:993/1480 train_time:268366ms step_avg:273.01ms
step:994/1480 train_time:268649ms step_avg:273.02ms
step:995/1480 train_time:268931ms step_avg:273.03ms
step:996/1480 train_time:269222ms step_avg:273.04ms
step:997/1480 train_time:269507ms step_avg:273.06ms
step:998/1480 train_time:269797ms step_avg:273.07ms
step:999/1480 train_time:270086ms step_avg:273.09ms
step:1000/1480 train_time:270380ms step_avg:273.11ms
step:1000/1480 val_loss:3.4688 train_time:270525ms step_avg:273.26ms
step:1001/1480 train_time:270673ms step_avg:273.13ms
step:1002/1480 train_time:270969ms step_avg:273.15ms
step:1003/1480 train_time:271257ms step_avg:273.17ms
step:1004/1480 train_time:271538ms step_avg:273.18ms
step:1005/1480 train_time:271832ms step_avg:273.20ms
step:1006/1480 train_time:272115ms step_avg:273.21ms
step:1007/1480 train_time:272400ms step_avg:273.22ms
step:1008/1480 train_time:272685ms step_avg:273.23ms
step:1009/1480 train_time:272966ms step_avg:273.24ms
step:1010/1480 train_time:273248ms step_avg:273.25ms
step:1011/1480 train_time:273534ms step_avg:273.26ms
step:1012/1480 train_time:273817ms step_avg:273.27ms
step:1013/1480 train_time:274113ms step_avg:273.29ms
step:1014/1480 train_time:274400ms step_avg:273.31ms
step:1015/1480 train_time:274686ms step_avg:273.32ms
step:1016/1480 train_time:274972ms step_avg:273.33ms
step:1017/1480 train_time:275257ms step_avg:273.34ms
step:1018/1480 train_time:275550ms step_avg:273.36ms
step:1019/1480 train_time:275834ms step_avg:273.37ms
step:1020/1480 train_time:276117ms step_avg:273.38ms
step:1021/1480 train_time:276407ms step_avg:273.40ms
step:1022/1480 train_time:276694ms step_avg:273.41ms
step:1023/1480 train_time:276976ms step_avg:273.42ms
step:1024/1480 train_time:277258ms step_avg:273.43ms
step:1025/1480 train_time:277540ms step_avg:273.44ms
step:1026/1480 train_time:277823ms step_avg:273.45ms
step:1027/1480 train_time:278110ms step_avg:273.46ms
step:1028/1480 train_time:278397ms step_avg:273.47ms
step:1029/1480 train_time:278684ms step_avg:273.49ms
step:1030/1480 train_time:278967ms step_avg:273.50ms
step:1031/1480 train_time:279258ms step_avg:273.51ms
step:1032/1480 train_time:279537ms step_avg:273.52ms
step:1033/1480 train_time:279832ms step_avg:273.54ms
step:1034/1480 train_time:280117ms step_avg:273.55ms
step:1035/1480 train_time:280402ms step_avg:273.56ms
step:1036/1480 train_time:280691ms step_avg:273.58ms
step:1037/1480 train_time:280976ms step_avg:273.59ms
step:1038/1480 train_time:281258ms step_avg:273.60ms
step:1039/1480 train_time:281543ms step_avg:273.61ms
step:1040/1480 train_time:281828ms step_avg:273.62ms
step:1041/1480 train_time:282117ms step_avg:273.63ms
step:1042/1480 train_time:282398ms step_avg:273.64ms
step:1043/1480 train_time:282692ms step_avg:273.66ms
step:1044/1480 train_time:282976ms step_avg:273.67ms
step:1045/1480 train_time:283258ms step_avg:273.68ms
step:1046/1480 train_time:283548ms step_avg:273.70ms
step:1047/1480 train_time:283835ms step_avg:273.71ms
step:1048/1480 train_time:284116ms step_avg:273.72ms
step:1049/1480 train_time:284408ms step_avg:273.73ms
step:1050/1480 train_time:284696ms step_avg:273.75ms
step:1051/1480 train_time:284982ms step_avg:273.76ms
step:1052/1480 train_time:285269ms step_avg:273.77ms
step:1053/1480 train_time:285556ms step_avg:273.78ms
step:1054/1480 train_time:285838ms step_avg:273.79ms
step:1055/1480 train_time:286130ms step_avg:273.81ms
step:1056/1480 train_time:286426ms step_avg:273.83ms
step:1057/1480 train_time:286713ms step_avg:273.84ms
step:1058/1480 train_time:286995ms step_avg:273.85ms
step:1059/1480 train_time:287280ms step_avg:273.86ms
step:1060/1480 train_time:287561ms step_avg:273.87ms
step:1061/1480 train_time:287849ms step_avg:273.88ms
step:1062/1480 train_time:288135ms step_avg:273.89ms
step:1063/1480 train_time:288416ms step_avg:273.90ms
step:1064/1480 train_time:288698ms step_avg:273.91ms
step:1065/1480 train_time:288977ms step_avg:273.91ms
step:1066/1480 train_time:289270ms step_avg:273.93ms
step:1067/1480 train_time:289556ms step_avg:273.94ms
step:1068/1480 train_time:289837ms step_avg:273.95ms
step:1069/1480 train_time:290122ms step_avg:273.96ms
step:1070/1480 train_time:290409ms step_avg:273.97ms
step:1071/1480 train_time:290694ms step_avg:273.98ms
step:1072/1480 train_time:290977ms step_avg:273.99ms
step:1073/1480 train_time:291259ms step_avg:274.00ms
step:1074/1480 train_time:291553ms step_avg:274.02ms
step:1075/1480 train_time:291841ms step_avg:274.03ms
step:1076/1480 train_time:292133ms step_avg:274.05ms
step:1077/1480 train_time:292420ms step_avg:274.06ms
step:1078/1480 train_time:292705ms step_avg:274.07ms
step:1079/1480 train_time:292989ms step_avg:274.08ms
step:1080/1480 train_time:293280ms step_avg:274.09ms
step:1081/1480 train_time:293561ms step_avg:274.10ms
step:1082/1480 train_time:293850ms step_avg:274.11ms
step:1083/1480 train_time:294138ms step_avg:274.13ms
step:1084/1480 train_time:294421ms step_avg:274.14ms
step:1085/1480 train_time:294713ms step_avg:274.15ms
step:1086/1480 train_time:294996ms step_avg:274.16ms
step:1087/1480 train_time:295285ms step_avg:274.17ms
step:1088/1480 train_time:295567ms step_avg:274.18ms
step:1089/1480 train_time:295853ms step_avg:274.19ms
step:1090/1480 train_time:296137ms step_avg:274.20ms
step:1091/1480 train_time:296421ms step_avg:274.21ms
step:1092/1480 train_time:296702ms step_avg:274.22ms
step:1093/1480 train_time:296989ms step_avg:274.23ms
step:1094/1480 train_time:297273ms step_avg:274.24ms
step:1095/1480 train_time:297560ms step_avg:274.25ms
step:1096/1480 train_time:297856ms step_avg:274.27ms
step:1097/1480 train_time:298139ms step_avg:274.28ms
step:1098/1480 train_time:298439ms step_avg:274.30ms
step:1099/1480 train_time:298731ms step_avg:274.32ms
step:1100/1480 train_time:299015ms step_avg:274.33ms
step:1101/1480 train_time:299298ms step_avg:274.33ms
step:1102/1480 train_time:299597ms step_avg:274.36ms
step:1103/1480 train_time:299888ms step_avg:274.37ms
step:1104/1480 train_time:300181ms step_avg:274.39ms
step:1105/1480 train_time:300469ms step_avg:274.40ms
step:1106/1480 train_time:300755ms step_avg:274.41ms
step:1107/1480 train_time:301039ms step_avg:274.42ms
step:1108/1480 train_time:301324ms step_avg:274.43ms
step:1109/1480 train_time:301614ms step_avg:274.44ms
step:1110/1480 train_time:301900ms step_avg:274.45ms
step:1111/1480 train_time:302195ms step_avg:274.47ms
step:1112/1480 train_time:302480ms step_avg:274.48ms
step:1113/1480 train_time:302770ms step_avg:274.50ms
step:1114/1480 train_time:303060ms step_avg:274.51ms
step:1115/1480 train_time:303352ms step_avg:274.53ms
step:1116/1480 train_time:303639ms step_avg:274.54ms
step:1117/1480 train_time:303934ms step_avg:274.56ms
step:1118/1480 train_time:304217ms step_avg:274.56ms
step:1119/1480 train_time:304515ms step_avg:274.59ms
step:1120/1480 train_time:304800ms step_avg:274.59ms
step:1121/1480 train_time:305084ms step_avg:274.60ms
step:1122/1480 train_time:305373ms step_avg:274.62ms
step:1123/1480 train_time:305658ms step_avg:274.63ms
step:1124/1480 train_time:305952ms step_avg:274.64ms
step:1125/1480 train_time:306237ms step_avg:274.65ms
step:1125/1480 val_loss:3.4106 train_time:306384ms step_avg:274.78ms
step:1126/1480 train_time:306531ms step_avg:274.67ms
step:1127/1480 train_time:306819ms step_avg:274.68ms
step:1128/1480 train_time:307104ms step_avg:274.69ms
step:1129/1480 train_time:307393ms step_avg:274.70ms
step:1130/1480 train_time:307689ms step_avg:274.72ms
step:1131/1480 train_time:307974ms step_avg:274.73ms
step:1132/1480 train_time:308269ms step_avg:274.75ms
step:1133/1480 train_time:308553ms step_avg:274.76ms
step:1134/1480 train_time:308835ms step_avg:274.76ms
step:1135/1480 train_time:309130ms step_avg:274.78ms
step:1136/1480 train_time:309414ms step_avg:274.79ms
step:1137/1480 train_time:309699ms step_avg:274.80ms
step:1138/1480 train_time:309993ms step_avg:274.82ms
step:1139/1480 train_time:310282ms step_avg:274.83ms
step:1140/1480 train_time:310568ms step_avg:274.84ms
step:1141/1480 train_time:310853ms step_avg:274.85ms
step:1142/1480 train_time:311136ms step_avg:274.85ms
step:1143/1480 train_time:311426ms step_avg:274.87ms
step:1144/1480 train_time:311707ms step_avg:274.87ms
step:1145/1480 train_time:311993ms step_avg:274.88ms
step:1146/1480 train_time:312285ms step_avg:274.90ms
step:1147/1480 train_time:312570ms step_avg:274.91ms
step:1148/1480 train_time:312855ms step_avg:274.92ms
step:1149/1480 train_time:313140ms step_avg:274.92ms
step:1150/1480 train_time:313426ms step_avg:274.93ms
step:1151/1480 train_time:313712ms step_avg:274.94ms
step:1152/1480 train_time:313994ms step_avg:274.95ms
step:1153/1480 train_time:314285ms step_avg:274.97ms
step:1154/1480 train_time:314570ms step_avg:274.97ms
step:1155/1480 train_time:314868ms step_avg:274.99ms
step:1156/1480 train_time:315158ms step_avg:275.01ms
step:1157/1480 train_time:315448ms step_avg:275.02ms
step:1158/1480 train_time:315737ms step_avg:275.03ms
step:1159/1480 train_time:316023ms step_avg:275.04ms
step:1160/1480 train_time:316307ms step_avg:275.05ms
step:1161/1480 train_time:316594ms step_avg:275.06ms
step:1162/1480 train_time:316888ms step_avg:275.08ms
step:1163/1480 train_time:317181ms step_avg:275.09ms
step:1164/1480 train_time:317462ms step_avg:275.10ms
step:1165/1480 train_time:317744ms step_avg:275.10ms
step:1166/1480 train_time:318032ms step_avg:275.11ms
step:1167/1480 train_time:318323ms step_avg:275.13ms
step:1168/1480 train_time:318608ms step_avg:275.14ms
step:1169/1480 train_time:318898ms step_avg:275.15ms
step:1170/1480 train_time:319185ms step_avg:275.16ms
step:1171/1480 train_time:319471ms step_avg:275.17ms
step:1172/1480 train_time:319752ms step_avg:275.17ms
step:1173/1480 train_time:320037ms step_avg:275.18ms
step:1174/1480 train_time:320329ms step_avg:275.20ms
step:1175/1480 train_time:320613ms step_avg:275.20ms
step:1176/1480 train_time:320892ms step_avg:275.21ms
step:1177/1480 train_time:321180ms step_avg:275.22ms
step:1178/1480 train_time:321467ms step_avg:275.23ms
step:1179/1480 train_time:321755ms step_avg:275.24ms
step:1180/1480 train_time:322049ms step_avg:275.26ms
step:1181/1480 train_time:322333ms step_avg:275.26ms
step:1182/1480 train_time:322617ms step_avg:275.27ms
step:1183/1480 train_time:322907ms step_avg:275.28ms
step:1184/1480 train_time:323191ms step_avg:275.29ms
step:1185/1480 train_time:323474ms step_avg:275.30ms
step:1186/1480 train_time:323761ms step_avg:275.31ms
step:1187/1480 train_time:324052ms step_avg:275.32ms
step:1188/1480 train_time:324337ms step_avg:275.33ms
step:1189/1480 train_time:324637ms step_avg:275.35ms
step:1190/1480 train_time:324932ms step_avg:275.37ms
step:1191/1480 train_time:325215ms step_avg:275.37ms
step:1192/1480 train_time:325501ms step_avg:275.38ms
step:1193/1480 train_time:325789ms step_avg:275.39ms
step:1194/1480 train_time:326073ms step_avg:275.40ms
step:1195/1480 train_time:326353ms step_avg:275.40ms
step:1196/1480 train_time:326638ms step_avg:275.41ms
step:1197/1480 train_time:326930ms step_avg:275.43ms
step:1198/1480 train_time:327214ms step_avg:275.43ms
step:1199/1480 train_time:327503ms step_avg:275.44ms
step:1200/1480 train_time:327788ms step_avg:275.45ms
step:1201/1480 train_time:328072ms step_avg:275.46ms
step:1202/1480 train_time:328366ms step_avg:275.47ms
step:1203/1480 train_time:328652ms step_avg:275.48ms
step:1204/1480 train_time:328938ms step_avg:275.49ms
step:1205/1480 train_time:329224ms step_avg:275.50ms
step:1206/1480 train_time:329512ms step_avg:275.51ms
step:1207/1480 train_time:329797ms step_avg:275.52ms
step:1208/1480 train_time:330087ms step_avg:275.53ms
step:1209/1480 train_time:330379ms step_avg:275.55ms
step:1210/1480 train_time:330665ms step_avg:275.55ms
step:1211/1480 train_time:330948ms step_avg:275.56ms
step:1212/1480 train_time:331242ms step_avg:275.58ms
step:1213/1480 train_time:331529ms step_avg:275.59ms
step:1214/1480 train_time:331821ms step_avg:275.60ms
step:1215/1480 train_time:332110ms step_avg:275.61ms
step:1216/1480 train_time:332393ms step_avg:275.62ms
step:1217/1480 train_time:332685ms step_avg:275.63ms
step:1218/1480 train_time:332978ms step_avg:275.64ms
step:1219/1480 train_time:333268ms step_avg:275.66ms
step:1220/1480 train_time:333563ms step_avg:275.67ms
step:1221/1480 train_time:333847ms step_avg:275.68ms
step:1222/1480 train_time:334135ms step_avg:275.69ms
step:1223/1480 train_time:334428ms step_avg:275.70ms
step:1224/1480 train_time:334717ms step_avg:275.71ms
step:1225/1480 train_time:335013ms step_avg:275.73ms
step:1226/1480 train_time:335304ms step_avg:275.74ms
step:1227/1480 train_time:335592ms step_avg:275.75ms
step:1228/1480 train_time:335873ms step_avg:275.76ms
step:1229/1480 train_time:336185ms step_avg:275.79ms
step:1230/1480 train_time:336475ms step_avg:275.80ms
step:1231/1480 train_time:336764ms step_avg:275.81ms
step:1232/1480 train_time:337052ms step_avg:275.82ms
step:1233/1480 train_time:337346ms step_avg:275.84ms
step:1234/1480 train_time:337635ms step_avg:275.85ms
step:1235/1480 train_time:337923ms step_avg:275.86ms
step:1236/1480 train_time:338220ms step_avg:275.87ms
step:1237/1480 train_time:338511ms step_avg:275.88ms
step:1238/1480 train_time:338798ms step_avg:275.89ms
step:1239/1480 train_time:339090ms step_avg:275.91ms
step:1240/1480 train_time:339382ms step_avg:275.92ms
step:1241/1480 train_time:339667ms step_avg:275.93ms
step:1242/1480 train_time:339953ms step_avg:275.94ms
step:1243/1480 train_time:340236ms step_avg:275.94ms
step:1244/1480 train_time:340528ms step_avg:275.95ms
step:1245/1480 train_time:340825ms step_avg:275.97ms
step:1246/1480 train_time:341109ms step_avg:275.98ms
step:1247/1480 train_time:341395ms step_avg:275.99ms
step:1248/1480 train_time:341683ms step_avg:276.00ms
step:1249/1480 train_time:341968ms step_avg:276.00ms
step:1250/1480 train_time:342262ms step_avg:276.02ms
step:1250/1480 val_loss:3.3593 train_time:342406ms step_avg:276.13ms
step:1251/1480 train_time:342554ms step_avg:276.03ms
step:1252/1480 train_time:342847ms step_avg:276.04ms
step:1253/1480 train_time:343136ms step_avg:276.05ms
step:1254/1480 train_time:343426ms step_avg:276.07ms
step:1255/1480 train_time:343711ms step_avg:276.07ms
step:1256/1480 train_time:344000ms step_avg:276.08ms
step:1257/1480 train_time:344286ms step_avg:276.09ms
step:1258/1480 train_time:344569ms step_avg:276.10ms
step:1259/1480 train_time:344860ms step_avg:276.11ms
step:1260/1480 train_time:345147ms step_avg:276.12ms
step:1261/1480 train_time:345440ms step_avg:276.13ms
step:1262/1480 train_time:345727ms step_avg:276.14ms
step:1263/1480 train_time:346024ms step_avg:276.16ms
step:1264/1480 train_time:346306ms step_avg:276.16ms
step:1265/1480 train_time:346591ms step_avg:276.17ms
step:1266/1480 train_time:346882ms step_avg:276.18ms
step:1267/1480 train_time:347166ms step_avg:276.19ms
step:1268/1480 train_time:347456ms step_avg:276.20ms
step:1269/1480 train_time:347750ms step_avg:276.21ms
step:1270/1480 train_time:348045ms step_avg:276.23ms
step:1271/1480 train_time:348331ms step_avg:276.23ms
step:1272/1480 train_time:348626ms step_avg:276.25ms
step:1273/1480 train_time:348918ms step_avg:276.26ms
step:1274/1480 train_time:349204ms step_avg:276.27ms
step:1275/1480 train_time:349492ms step_avg:276.28ms
step:1276/1480 train_time:349778ms step_avg:276.29ms
step:1277/1480 train_time:350065ms step_avg:276.29ms
step:1278/1480 train_time:350350ms step_avg:276.30ms
step:1279/1480 train_time:350641ms step_avg:276.31ms
step:1280/1480 train_time:350927ms step_avg:276.32ms
step:1281/1480 train_time:351226ms step_avg:276.34ms
step:1282/1480 train_time:351509ms step_avg:276.34ms
step:1283/1480 train_time:351801ms step_avg:276.36ms
step:1284/1480 train_time:352095ms step_avg:276.37ms
step:1285/1480 train_time:352379ms step_avg:276.38ms
step:1286/1480 train_time:352665ms step_avg:276.38ms
step:1287/1480 train_time:352955ms step_avg:276.39ms
step:1288/1480 train_time:353242ms step_avg:276.40ms
step:1289/1480 train_time:353534ms step_avg:276.41ms
step:1290/1480 train_time:353820ms step_avg:276.42ms
step:1291/1480 train_time:354104ms step_avg:276.43ms
step:1292/1480 train_time:354391ms step_avg:276.44ms
step:1293/1480 train_time:354681ms step_avg:276.45ms
step:1294/1480 train_time:354964ms step_avg:276.45ms
step:1295/1480 train_time:355248ms step_avg:276.46ms
step:1296/1480 train_time:355534ms step_avg:276.46ms
step:1297/1480 train_time:355821ms step_avg:276.47ms
step:1298/1480 train_time:356103ms step_avg:276.48ms
step:1299/1480 train_time:356387ms step_avg:276.48ms
step:1300/1480 train_time:356685ms step_avg:276.50ms
step:1301/1480 train_time:356967ms step_avg:276.50ms
step:1302/1480 train_time:357258ms step_avg:276.52ms
step:1303/1480 train_time:357550ms step_avg:276.53ms
step:1304/1480 train_time:357832ms step_avg:276.53ms
step:1305/1480 train_time:358124ms step_avg:276.54ms
step:1306/1480 train_time:358418ms step_avg:276.56ms
step:1307/1480 train_time:358702ms step_avg:276.56ms
step:1308/1480 train_time:358984ms step_avg:276.57ms
step:1309/1480 train_time:359285ms step_avg:276.59ms
step:1310/1480 train_time:359576ms step_avg:276.60ms
step:1311/1480 train_time:359864ms step_avg:276.61ms
step:1312/1480 train_time:360153ms step_avg:276.62ms
step:1313/1480 train_time:360441ms step_avg:276.62ms
step:1314/1480 train_time:360731ms step_avg:276.63ms
step:1315/1480 train_time:361023ms step_avg:276.65ms
step:1316/1480 train_time:361323ms step_avg:276.66ms
step:1317/1480 train_time:361615ms step_avg:276.68ms
step:1318/1480 train_time:361905ms step_avg:276.69ms
step:1319/1480 train_time:362197ms step_avg:276.70ms
step:1320/1480 train_time:362487ms step_avg:276.71ms
step:1321/1480 train_time:362778ms step_avg:276.72ms
step:1322/1480 train_time:363066ms step_avg:276.73ms
step:1323/1480 train_time:363348ms step_avg:276.73ms
step:1324/1480 train_time:363645ms step_avg:276.75ms
step:1325/1480 train_time:363943ms step_avg:276.76ms
step:1326/1480 train_time:364229ms step_avg:276.77ms
step:1327/1480 train_time:364526ms step_avg:276.79ms
step:1328/1480 train_time:364809ms step_avg:276.79ms
step:1329/1480 train_time:365098ms step_avg:276.80ms
step:1330/1480 train_time:365388ms step_avg:276.81ms
step:1331/1480 train_time:365682ms step_avg:276.82ms
step:1332/1480 train_time:365969ms step_avg:276.83ms
step:1333/1480 train_time:366266ms step_avg:276.85ms
step:1334/1480 train_time:366565ms step_avg:276.86ms
step:1335/1480 train_time:366853ms step_avg:276.87ms
step:1336/1480 train_time:367145ms step_avg:276.88ms
step:1337/1480 train_time:367431ms step_avg:276.89ms
step:1338/1480 train_time:367720ms step_avg:276.90ms
step:1339/1480 train_time:368013ms step_avg:276.91ms
step:1340/1480 train_time:368298ms step_avg:276.92ms
step:1341/1480 train_time:368584ms step_avg:276.92ms
step:1342/1480 train_time:368882ms step_avg:276.94ms
step:1343/1480 train_time:369186ms step_avg:276.96ms
step:1344/1480 train_time:369474ms step_avg:276.97ms
step:1345/1480 train_time:369764ms step_avg:276.98ms
step:1346/1480 train_time:370058ms step_avg:276.99ms
step:1347/1480 train_time:370349ms step_avg:277.00ms
step:1348/1480 train_time:370641ms step_avg:277.01ms
step:1349/1480 train_time:370931ms step_avg:277.02ms
step:1350/1480 train_time:371226ms step_avg:277.03ms
step:1351/1480 train_time:371519ms step_avg:277.05ms
step:1352/1480 train_time:371806ms step_avg:277.05ms
step:1353/1480 train_time:372087ms step_avg:277.06ms
step:1354/1480 train_time:372377ms step_avg:277.07ms
step:1355/1480 train_time:372668ms step_avg:277.08ms
step:1356/1480 train_time:372962ms step_avg:277.09ms
step:1357/1480 train_time:373250ms step_avg:277.10ms
step:1358/1480 train_time:373542ms step_avg:277.11ms
step:1359/1480 train_time:373835ms step_avg:277.12ms
step:1360/1480 train_time:374126ms step_avg:277.13ms
step:1361/1480 train_time:374414ms step_avg:277.14ms
step:1362/1480 train_time:374703ms step_avg:277.15ms
step:1363/1480 train_time:374991ms step_avg:277.16ms
step:1364/1480 train_time:375284ms step_avg:277.17ms
step:1365/1480 train_time:375574ms step_avg:277.18ms
step:1366/1480 train_time:375863ms step_avg:277.19ms
step:1367/1480 train_time:376157ms step_avg:277.20ms
step:1368/1480 train_time:376444ms step_avg:277.20ms
step:1369/1480 train_time:376733ms step_avg:277.21ms
step:1370/1480 train_time:377029ms step_avg:277.23ms
step:1371/1480 train_time:377330ms step_avg:277.24ms
step:1372/1480 train_time:377629ms step_avg:277.26ms
step:1373/1480 train_time:377911ms step_avg:277.26ms
step:1374/1480 train_time:378205ms step_avg:277.28ms
step:1375/1480 train_time:378502ms step_avg:277.29ms
step:1375/1480 val_loss:3.3179 train_time:378645ms step_avg:277.40ms
step:1376/1480 train_time:378790ms step_avg:277.30ms
step:1377/1480 train_time:379097ms step_avg:277.32ms
step:1378/1480 train_time:379402ms step_avg:277.34ms
step:1379/1480 train_time:379691ms step_avg:277.35ms
step:1380/1480 train_time:379980ms step_avg:277.36ms
step:1381/1480 train_time:380268ms step_avg:277.37ms
step:1382/1480 train_time:380563ms step_avg:277.38ms
step:1383/1480 train_time:380863ms step_avg:277.39ms
step:1384/1480 train_time:381161ms step_avg:277.41ms
step:1385/1480 train_time:381449ms step_avg:277.42ms
step:1386/1480 train_time:381744ms step_avg:277.43ms
step:1387/1480 train_time:382035ms step_avg:277.44ms
step:1388/1480 train_time:382331ms step_avg:277.45ms
step:1389/1480 train_time:382617ms step_avg:277.46ms
step:1390/1480 train_time:382910ms step_avg:277.47ms
step:1391/1480 train_time:383205ms step_avg:277.48ms
step:1392/1480 train_time:383489ms step_avg:277.49ms
step:1393/1480 train_time:383775ms step_avg:277.49ms
step:1394/1480 train_time:384066ms step_avg:277.50ms
step:1395/1480 train_time:384360ms step_avg:277.52ms
step:1396/1480 train_time:384649ms step_avg:277.52ms
step:1397/1480 train_time:384946ms step_avg:277.54ms
step:1398/1480 train_time:385235ms step_avg:277.55ms
step:1399/1480 train_time:385530ms step_avg:277.56ms
step:1400/1480 train_time:385836ms step_avg:277.58ms
step:1401/1480 train_time:386131ms step_avg:277.59ms
step:1402/1480 train_time:386447ms step_avg:277.62ms
step:1403/1480 train_time:386732ms step_avg:277.62ms
step:1404/1480 train_time:387026ms step_avg:277.64ms
step:1405/1480 train_time:387318ms step_avg:277.65ms
step:1406/1480 train_time:387608ms step_avg:277.66ms
step:1407/1480 train_time:387911ms step_avg:277.67ms
step:1408/1480 train_time:388207ms step_avg:277.69ms
step:1409/1480 train_time:388491ms step_avg:277.69ms
step:1410/1480 train_time:388778ms step_avg:277.70ms
step:1411/1480 train_time:389070ms step_avg:277.71ms
step:1412/1480 train_time:389356ms step_avg:277.71ms
step:1413/1480 train_time:389650ms step_avg:277.73ms
step:1414/1480 train_time:389940ms step_avg:277.73ms
step:1415/1480 train_time:390226ms step_avg:277.74ms
step:1416/1480 train_time:390522ms step_avg:277.75ms
step:1417/1480 train_time:390809ms step_avg:277.76ms
step:1418/1480 train_time:391101ms step_avg:277.77ms
step:1419/1480 train_time:391393ms step_avg:277.78ms
step:1420/1480 train_time:391687ms step_avg:277.79ms
step:1421/1480 train_time:391973ms step_avg:277.80ms
step:1422/1480 train_time:392267ms step_avg:277.81ms
step:1423/1480 train_time:392551ms step_avg:277.81ms
step:1424/1480 train_time:392841ms step_avg:277.82ms
step:1425/1480 train_time:393131ms step_avg:277.83ms
step:1426/1480 train_time:393432ms step_avg:277.85ms
step:1427/1480 train_time:393723ms step_avg:277.86ms
step:1428/1480 train_time:394013ms step_avg:277.87ms
step:1429/1480 train_time:394303ms step_avg:277.87ms
step:1430/1480 train_time:394589ms step_avg:277.88ms
step:1431/1480 train_time:394880ms step_avg:277.89ms
step:1432/1480 train_time:395175ms step_avg:277.90ms
step:1433/1480 train_time:395470ms step_avg:277.91ms
step:1434/1480 train_time:395770ms step_avg:277.93ms
step:1435/1480 train_time:396063ms step_avg:277.94ms
step:1436/1480 train_time:396354ms step_avg:277.95ms
step:1437/1480 train_time:396641ms step_avg:277.95ms
step:1438/1480 train_time:396946ms step_avg:277.97ms
step:1439/1480 train_time:397243ms step_avg:277.99ms
step:1440/1480 train_time:397535ms step_avg:278.00ms
step:1441/1480 train_time:397826ms step_avg:278.01ms
step:1442/1480 train_time:398121ms step_avg:278.02ms
step:1443/1480 train_time:398412ms step_avg:278.03ms
step:1444/1480 train_time:398706ms step_avg:278.04ms
step:1445/1480 train_time:398990ms step_avg:278.04ms
step:1446/1480 train_time:399293ms step_avg:278.06ms
step:1447/1480 train_time:399589ms step_avg:278.07ms
step:1448/1480 train_time:399891ms step_avg:278.09ms
step:1449/1480 train_time:400186ms step_avg:278.10ms
step:1450/1480 train_time:400471ms step_avg:278.10ms
step:1451/1480 train_time:400765ms step_avg:278.12ms
step:1452/1480 train_time:401055ms step_avg:278.12ms
step:1453/1480 train_time:401341ms step_avg:278.13ms
step:1454/1480 train_time:401629ms step_avg:278.14ms
step:1455/1480 train_time:401926ms step_avg:278.15ms
step:1456/1480 train_time:402211ms step_avg:278.15ms
step:1457/1480 train_time:402501ms step_avg:278.16ms
step:1458/1480 train_time:402789ms step_avg:278.17ms
step:1459/1480 train_time:403088ms step_avg:278.18ms
step:1460/1480 train_time:403373ms step_avg:278.19ms
step:1461/1480 train_time:403666ms step_avg:278.20ms
step:1462/1480 train_time:403950ms step_avg:278.20ms
step:1463/1480 train_time:404240ms step_avg:278.21ms
step:1464/1480 train_time:404538ms step_avg:278.22ms
step:1465/1480 train_time:404831ms step_avg:278.23ms
step:1466/1480 train_time:405122ms step_avg:278.24ms
step:1467/1480 train_time:405411ms step_avg:278.25ms
step:1468/1480 train_time:405709ms step_avg:278.26ms
step:1469/1480 train_time:406021ms step_avg:278.29ms
step:1470/1480 train_time:406318ms step_avg:278.30ms
step:1471/1480 train_time:406612ms step_avg:278.31ms
step:1472/1480 train_time:406906ms step_avg:278.32ms
step:1473/1480 train_time:407193ms step_avg:278.33ms
step:1474/1480 train_time:407480ms step_avg:278.33ms
step:1475/1480 train_time:407769ms step_avg:278.34ms
step:1476/1480 train_time:408058ms step_avg:278.35ms
step:1477/1480 train_time:408353ms step_avg:278.36ms
step:1478/1480 train_time:408647ms step_avg:278.37ms
step:1479/1480 train_time:408941ms step_avg:278.38ms
step:1480/1480 train_time:409229ms step_avg:278.39ms
step:1480/1480 val_loss:3.2988 train_time:409369ms step_avg:278.48ms
peak memory consumption: 34262 MiB

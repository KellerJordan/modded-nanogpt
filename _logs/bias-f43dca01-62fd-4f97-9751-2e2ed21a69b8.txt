import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import contextlib
from dataclasses import dataclass
from pathlib import Path

import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.nn.attention.flex_attention import BlockMask, flex_attention #KoszarskyB

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params = list(params)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [
            {
                'params': [p for p in params if p.numel() == size],
                'update_buffer': [
                    torch.empty(size, device='cuda', dtype=torch.bfloat16)
                    for _ in range(self.world_size)
                ],
            }
            for size in sizes
        ]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            assert len(params) % self.world_size == 0
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                p = params[base_i + self.rank]
                g = p.grad
                assert g is not None
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.lerp_(g, 1 - momentum)
                g = g.lerp_(buf, momentum) if nesterov else buf
                g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                update_prev()
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features, bias=False):
        super().__init__(in_features, out_features, bias)

    def forward(self, x):
        return F.linear(
            x,
            self.weight.to(x.dtype),
            None if self.bias is None else self.bias.to(x.dtype)
        )

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.register_buffer('inv_freq', (1 / base) ** (torch.arange(0, dim, 2) / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            t = torch.arange(seq_len, device=x.device)
            freqs = torch.outer(t, self.inv_freq)
            self.seq_len_cached = seq_len
            self.cos_cached = freqs.cos()
            self.sin_cached = freqs.sin()
        cos, sin = self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
        # apply_rotary_emb(x, cos, sin)
        x1, x2 = x.chunk(2, dim=3)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x, vi, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        v = self.lambdas[0] * v + self.lambdas[1] * vi.view_as(v) # @KoszarskyB & @Grad62304977
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, enable_gqa=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc   = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config.model_dim, config.num_heads)
        self.mlp = MLP(config.model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, vi, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x = x + self.attn(norm(x), vi, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, config: "GPTConfig"):
        super().__init__()
        self.embed = nn.ModuleList([
            nn.Embedding(config.vocab_size, config.model_dim)
            for _ in range(6)
        ])

    def forward(self, inputs) -> "list[torch.Tensor]":
        ve = [emb(inputs) for emb in self.embed]
        ve += reversed(ve)
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    num_layers : int = 12
    num_heads : int = 6 # head dim 128 suggested by @Grad62304977
    model_dim : int = 768

class GPT(nn.Module):

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.num_layers = config.num_layers

        # U-net design by @brendanh0gan
        self.num_encoder_layers = config.num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = config.num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

        self.embed = nn.Embedding(config.vocab_size, config.model_dim)
        self.blocks = nn.ModuleList([Block(config) for _ in range(config.num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(config)
        self.lm_head = CastedLinear(config.model_dim, config.vocab_size, bias=True)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(
        self,
        inputs: torch.Tensor,
        targets: torch.Tensor,
        sliding_window_num_blocks: torch.Tensor,
    ):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: torch.Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks: torch.Tensor):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm ^ full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        # forward the GPT model itself
        x = self.embed(inputs[None]) # token embeddings of shape (b, t, model_dim)
        x = norm(x) # @Grad62304977
        x0 = x
        ve = self.value_embeds(inputs)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(file: Path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32)
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    return int(header[2]) # number of tokens (claimed)

def _load_data_shard(path: Path, num_tokens):
    with path.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, seq_len, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.seq_len = seq_len

        # glob files that match the pattern
        self.files = sorted(Path.cwd().glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        self.files_num_tokens = [_peek_data_shard(file) for file in self.files]
        assert min(self.files_num_tokens) >= num_processes * seq_len + 1
        self.total_num_tokens = sum(self.files_num_tokens)

        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.seq_len
        self.tokens = _load_data_shard(self.files[self.current_shard], self.files_num_tokens[self.current_shard])

    def next_batch(self):
        batch_size = self.seq_len * self.num_processes
        buf = self.tokens[self.current_position:self.current_position+self.seq_len+1]
        # host side async is sufficient;
        # no performance improvement was observed when introducing a separate stream.
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # inputs
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # targets
        # advance current position and load next shard if necessary
        self.current_position += batch_size
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        return inputs, targets

def set_output_layer_bias(model, dataloader, n_batches):
    # Use token prevalence to initialize output layer bias & avoid initial shock to network of having to find it.
    t0 = time.perf_counter()
    num_vocab = model.lm_head.bias.size(0)
    for i in range(n_batches):
        _, targets_train = dataloader.next_batch()
        if i == 0:
            total_counts = torch.zeros(num_vocab, dtype=torch.int32, device=targets_train.device)
        ids, counts = torch.unique(targets_train, sorted=True, return_counts=True)
        total_counts[ids] += counts

    target_probs = total_counts / total_counts.sum()
    target_probs = (target_probs + 1e-12)
    target_probs = target_probs / target_probs.sum()

    with torch.no_grad():
        model.lm_head.bias.copy_(target_probs.log())

    old_init_loss = torch.tensor(1 / num_vocab).log().item()
    new_init_loss = (target_probs * target_probs.log()).sum().item()
    total_time = time.perf_counter() - t0
    print0(f"Centered output layer, initial loss {old_init_loss:.3f} => {new_init_loss:.3f} ({total_time:.1f}s)")

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8 # batch size, in sequences, across all devices
    sequence_length : int = 64*1024 # sequence length, in tokens
    num_iterations : int = 1480 # number of iterations to run
    warmup_iters : int = 0
    cooldown_iters : int = 600 # number of iterations of linear warmup/cooldown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
device = torch.device(f'cuda:{ddp_local_rank}')
torch.cuda.set_device(device)
print(f'using device: {device}')
dist.init_process_group(backend='nccl', device_id=device)
dist.barrier()
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    Path('logs').mkdir(exist_ok=True)
    # logdir = Path('logs') / f'{run_id}'
    # logdir.mkdir()
    logfile = Path('logs') / f'{run_id}.txt'
    print(logfile.stem)
    # create the log file
    with logfile.open('w') as f:
        # begin the log by printing this file (the Python code)
        print(code, file=f)
        print('=' * 100, file=f)
def print0(s, logonly=False):
    if master_process:
        with logfile.open('a') as f:
            if not logonly:
                print(s)
            print(s, file=f)
# log information about the hardware/software environment this is running on
# and print the full `nvidia-smi` to file
print0(f'Running python {sys.version}')
print0(f'Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:')
import subprocess
result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
print0(f'{result.stdout}', logonly=True)
print0('='*100, logonly=True)

# calculate the number of steps to take in the val loop.
assert args.val_tokens % (args.sequence_length * ddp_world_size) == 0
val_steps = args.val_tokens // (args.sequence_length * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (ddp_world_size) == 0
train_accumulation_steps = args.batch_size // ddp_world_size

# load tokens
train_loader = DistributedDataLoader(args.input_bin, args.sequence_length, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, args.sequence_length, ddp_rank, ddp_world_size)
print0(f"Training DataLoader: total number of tokens: {train_loader.total_num_tokens} across {len(train_loader.files)} files")
print0(f"Validation DataLoader: total number of tokens: {val_loader.total_num_tokens} across {len(val_loader.files)} files")
print0('='*100, logonly=True)
inputs_train, targets_train = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, num_layers=12, num_heads=6, model_dim=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee

set_output_layer_bias(model, train_loader, n_batches=100)

model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)
raw_model = model.module # always contains the "raw" unwrapped model

# init the optimizer(s)
embed_params = [*raw_model.embed.parameters(), *raw_model.value_embeds.parameters()]
optimizer1 = torch.optim.Adam(embed_params, lr=0.6, betas=(0.8, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight, raw_model.lm_head.bias], lr=0.008, betas=(0.8, 0.95), fused=True)
params = list(raw_model.blocks.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
optimizer3 = Muon(matrix_params, lr=0.05, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.8, 0.95), fused=True)
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and cooldown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.cooldown_iters:
        return 1.0
    # 3) linear cooldown
    else:
        decay_ratio = (args.num_iterations - it) / args.cooldown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device="cuda")
sw_num_blocks_prev = 1
# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the sliding window size over training in chunks of 128 from 128 -> 1856. By @fernbear.bsky.social
    frac_done = step / args.num_iterations # training progress
    sw_num_blocks = int(((1 - frac_done) * 128 + frac_done * 1856) // 128)
    if sw_num_blocks != sw_num_blocks_prev:
        sliding_window_num_blocks.copy_(sw_num_blocks, non_blocking=True)
        sw_num_blocks_prev = sw_num_blocks

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch()
                val_loss += model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        print0(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    # uncomment if you want to save any checkpoints
    #save_every = 1000
    #if master_process and (last_step or (save_every > 0 and step % save_every == 0)):
    #    # stop the clock
    #    torch.cuda.synchronize()
    #    training_time_ms += 1000 * (time.perf_counter() - t0)
    #    # save the state of the training process
    #    log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
    #    torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
    #    # start the clock again
    #    torch.cuda.synchronize()
    #    t0 = time.perf_counter()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps + 1):
        with contextlib.ExitStack() as stack:
            if i < train_accumulation_steps: # there's no need to sync gradients every accumulation step
                stack.enter_context(model.no_sync())
            if step >= 5:
                stack.enter_context(torch.compiler.set_stance(skip_guard_eval_unsafe=True))
            model(inputs_train, targets_train, sliding_window_num_blocks).backward()
            inputs_train, targets_train = train_loader.next_batch()
    if train_accumulation_steps != 1:
        for p in model.parameters():
            p.grad /= train_accumulation_steps
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer3.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{args.num_iterations} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")

print0(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()

====================================================================================================
Running python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running pytorch 2.6.0.dev20241203+cu124 compiled for CUDA 12.4
nvidia-smi:
Thu Dec 26 08:12:27 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 PCIe               On  | 00000000:00:07.0 Off |                    0 |
| N/A   40C    P0              82W / 350W |   4162MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  | 00000000:00:08.0 Off |                    0 |
| N/A   45C    P0              87W / 350W |    923MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  | 00000000:00:09.0 Off |                    0 |
| N/A   38C    P0              81W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  | 00000000:00:0A.0 Off |                    0 |
| N/A   39C    P0              79W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  | 00000000:00:0B.0 Off |                    0 |
| N/A   39C    P0              78W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  | 00000000:00:0C.0 Off |                    0 |
| N/A   37C    P0              77W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  | 00000000:00:0D.0 Off |                    0 |
| N/A   44C    P0              84W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  | 00000000:00:0E.0 Off |                    0 |
| N/A   39C    P0              79W / 350W |    963MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training DataLoader: total number of tokens: 1000000000 across 10 files
Validation DataLoader: total number of tokens: 100000000 across 1 files
====================================================================================================
Centered output layer, initial loss -10.826 => -7.657 (0.1s)
step:0/1480 val_loss:7.7109 train_time:0ms step_avg:nanms
step:1/1480 train_time:26796ms step_avg:nanms
step:2/1480 train_time:26933ms step_avg:nanms
step:3/1480 train_time:27181ms step_avg:nanms
step:4/1480 train_time:27437ms step_avg:nanms
step:5/1480 train_time:27694ms step_avg:nanms
step:6/1480 train_time:27948ms step_avg:nanms
step:7/1480 train_time:28200ms step_avg:nanms
step:8/1480 train_time:28459ms step_avg:nanms
step:9/1480 train_time:28713ms step_avg:nanms
step:10/1480 train_time:28965ms step_avg:nanms
step:11/1480 train_time:256ms step_avg:nanms
step:12/1480 train_time:513ms step_avg:nanms
step:13/1480 train_time:769ms step_avg:256.41ms
step:14/1480 train_time:1025ms step_avg:256.25ms
step:15/1480 train_time:1279ms step_avg:255.84ms
step:16/1480 train_time:1535ms step_avg:255.83ms
step:17/1480 train_time:1794ms step_avg:256.25ms
step:18/1480 train_time:2050ms step_avg:256.21ms
step:19/1480 train_time:2305ms step_avg:256.11ms
step:20/1480 train_time:2559ms step_avg:255.95ms
step:21/1480 train_time:2817ms step_avg:256.09ms
step:22/1480 train_time:3074ms step_avg:256.14ms
step:23/1480 train_time:3331ms step_avg:256.24ms
step:24/1480 train_time:3587ms step_avg:256.22ms
step:25/1480 train_time:3839ms step_avg:255.96ms
step:26/1480 train_time:4097ms step_avg:256.03ms
step:27/1480 train_time:4355ms step_avg:256.16ms
step:28/1480 train_time:4611ms step_avg:256.19ms
step:29/1480 train_time:4869ms step_avg:256.27ms
step:30/1480 train_time:5122ms step_avg:256.10ms
step:31/1480 train_time:5377ms step_avg:256.07ms
step:32/1480 train_time:5635ms step_avg:256.14ms
step:33/1480 train_time:5894ms step_avg:256.26ms
step:34/1480 train_time:6150ms step_avg:256.25ms
step:35/1480 train_time:6405ms step_avg:256.20ms
step:36/1480 train_time:6660ms step_avg:256.14ms
step:37/1480 train_time:6917ms step_avg:256.18ms
step:38/1480 train_time:7174ms step_avg:256.22ms
step:39/1480 train_time:7431ms step_avg:256.25ms
step:40/1480 train_time:7690ms step_avg:256.32ms
step:41/1480 train_time:7945ms step_avg:256.30ms
step:42/1480 train_time:8199ms step_avg:256.22ms
step:43/1480 train_time:8457ms step_avg:256.26ms
step:44/1480 train_time:8714ms step_avg:256.30ms
step:45/1480 train_time:8971ms step_avg:256.33ms
step:46/1480 train_time:9227ms step_avg:256.30ms
step:47/1480 train_time:9483ms step_avg:256.29ms
step:48/1480 train_time:9737ms step_avg:256.24ms
step:49/1480 train_time:9998ms step_avg:256.35ms
step:50/1480 train_time:10256ms step_avg:256.40ms
step:51/1480 train_time:10513ms step_avg:256.41ms
step:52/1480 train_time:10770ms step_avg:256.44ms
step:53/1480 train_time:11031ms step_avg:256.53ms
step:54/1480 train_time:11286ms step_avg:256.50ms
step:55/1480 train_time:11542ms step_avg:256.49ms
step:56/1480 train_time:11799ms step_avg:256.50ms
step:57/1480 train_time:12057ms step_avg:256.53ms
step:58/1480 train_time:12315ms step_avg:256.56ms
step:59/1480 train_time:12572ms step_avg:256.58ms
step:60/1480 train_time:12831ms step_avg:256.62ms
step:61/1480 train_time:13091ms step_avg:256.69ms
step:62/1480 train_time:13346ms step_avg:256.65ms
step:63/1480 train_time:13600ms step_avg:256.60ms
step:64/1480 train_time:13858ms step_avg:256.63ms
step:65/1480 train_time:14115ms step_avg:256.64ms
step:66/1480 train_time:14373ms step_avg:256.67ms
step:67/1480 train_time:14632ms step_avg:256.71ms
step:68/1480 train_time:14888ms step_avg:256.70ms
step:69/1480 train_time:15141ms step_avg:256.63ms
step:70/1480 train_time:15399ms step_avg:256.65ms
step:71/1480 train_time:15658ms step_avg:256.69ms
step:72/1480 train_time:15914ms step_avg:256.68ms
step:73/1480 train_time:16173ms step_avg:256.71ms
step:74/1480 train_time:16432ms step_avg:256.75ms
step:75/1480 train_time:16690ms step_avg:256.77ms
step:76/1480 train_time:16943ms step_avg:256.70ms
step:77/1480 train_time:17199ms step_avg:256.70ms
step:78/1480 train_time:17457ms step_avg:256.72ms
step:79/1480 train_time:17713ms step_avg:256.72ms
step:80/1480 train_time:17971ms step_avg:256.73ms
step:81/1480 train_time:18227ms step_avg:256.72ms
step:82/1480 train_time:18481ms step_avg:256.69ms
step:83/1480 train_time:18738ms step_avg:256.69ms
step:84/1480 train_time:18996ms step_avg:256.71ms
step:85/1480 train_time:19255ms step_avg:256.73ms
step:86/1480 train_time:19513ms step_avg:256.75ms
step:87/1480 train_time:19772ms step_avg:256.78ms
step:88/1480 train_time:20029ms step_avg:256.78ms
step:89/1480 train_time:20286ms step_avg:256.78ms
step:90/1480 train_time:20540ms step_avg:256.74ms
step:91/1480 train_time:20799ms step_avg:256.77ms
step:92/1480 train_time:21058ms step_avg:256.80ms
step:93/1480 train_time:21315ms step_avg:256.81ms
step:94/1480 train_time:21574ms step_avg:256.83ms
step:95/1480 train_time:21832ms step_avg:256.84ms
step:96/1480 train_time:22088ms step_avg:256.83ms
step:97/1480 train_time:22341ms step_avg:256.79ms
step:98/1480 train_time:22599ms step_avg:256.81ms
step:99/1480 train_time:22858ms step_avg:256.83ms
step:100/1480 train_time:23115ms step_avg:256.83ms
step:101/1480 train_time:23373ms step_avg:256.84ms
step:102/1480 train_time:23630ms step_avg:256.85ms
step:103/1480 train_time:23885ms step_avg:256.83ms
step:104/1480 train_time:24139ms step_avg:256.79ms
step:105/1480 train_time:24400ms step_avg:256.84ms
step:106/1480 train_time:24658ms step_avg:256.85ms
step:107/1480 train_time:24915ms step_avg:256.85ms
step:108/1480 train_time:25174ms step_avg:256.87ms
step:109/1480 train_time:25430ms step_avg:256.87ms
step:110/1480 train_time:25686ms step_avg:256.86ms
step:111/1480 train_time:25942ms step_avg:256.85ms
step:112/1480 train_time:26204ms step_avg:256.90ms
step:113/1480 train_time:26466ms step_avg:256.95ms
step:114/1480 train_time:26726ms step_avg:256.98ms
step:115/1480 train_time:26985ms step_avg:257.00ms
step:116/1480 train_time:27244ms step_avg:257.02ms
step:117/1480 train_time:27505ms step_avg:257.06ms
step:118/1480 train_time:27765ms step_avg:257.08ms
step:119/1480 train_time:28027ms step_avg:257.13ms
step:120/1480 train_time:28292ms step_avg:257.20ms
step:121/1480 train_time:28556ms step_avg:257.26ms
step:122/1480 train_time:28817ms step_avg:257.29ms
step:123/1480 train_time:29079ms step_avg:257.33ms
step:124/1480 train_time:29340ms step_avg:257.37ms
step:125/1480 train_time:29602ms step_avg:257.41ms
step:125/1480 val_loss:4.3804 train_time:29731ms step_avg:258.53ms
step:126/1480 train_time:29866ms step_avg:257.46ms
step:127/1480 train_time:30129ms step_avg:257.51ms
step:128/1480 train_time:30393ms step_avg:257.57ms
step:129/1480 train_time:30655ms step_avg:257.61ms
step:130/1480 train_time:30917ms step_avg:257.64ms
step:131/1480 train_time:31181ms step_avg:257.69ms
step:132/1480 train_time:31444ms step_avg:257.74ms
step:133/1480 train_time:31706ms step_avg:257.78ms
step:134/1480 train_time:31966ms step_avg:257.79ms
step:135/1480 train_time:32228ms step_avg:257.82ms
step:136/1480 train_time:32490ms step_avg:257.86ms
step:137/1480 train_time:32753ms step_avg:257.90ms
step:138/1480 train_time:33015ms step_avg:257.93ms
step:139/1480 train_time:33275ms step_avg:257.94ms
step:140/1480 train_time:33536ms step_avg:257.97ms
step:141/1480 train_time:33798ms step_avg:258.00ms
step:142/1480 train_time:34062ms step_avg:258.04ms
step:143/1480 train_time:34325ms step_avg:258.08ms
step:144/1480 train_time:34586ms step_avg:258.10ms
step:145/1480 train_time:34847ms step_avg:258.13ms
step:146/1480 train_time:35108ms step_avg:258.15ms
step:147/1480 train_time:35368ms step_avg:258.16ms
step:148/1480 train_time:35629ms step_avg:258.18ms
step:149/1480 train_time:35890ms step_avg:258.20ms
step:150/1480 train_time:36151ms step_avg:258.22ms
step:151/1480 train_time:36410ms step_avg:258.23ms
step:152/1480 train_time:36669ms step_avg:258.24ms
step:153/1480 train_time:36931ms step_avg:258.26ms
step:154/1480 train_time:37191ms step_avg:258.27ms
step:155/1480 train_time:37450ms step_avg:258.28ms
step:156/1480 train_time:37709ms step_avg:258.28ms
step:157/1480 train_time:37970ms step_avg:258.30ms
step:158/1480 train_time:38230ms step_avg:258.31ms
step:159/1480 train_time:38492ms step_avg:258.33ms
step:160/1480 train_time:38751ms step_avg:258.34ms
step:161/1480 train_time:39009ms step_avg:258.34ms
step:162/1480 train_time:39269ms step_avg:258.35ms
step:163/1480 train_time:39530ms step_avg:258.36ms
step:164/1480 train_time:39791ms step_avg:258.38ms
step:165/1480 train_time:40052ms step_avg:258.40ms
step:166/1480 train_time:40314ms step_avg:258.43ms
step:167/1480 train_time:40574ms step_avg:258.43ms
step:168/1480 train_time:40834ms step_avg:258.45ms
step:169/1480 train_time:41100ms step_avg:258.49ms
step:170/1480 train_time:41364ms step_avg:258.52ms
step:171/1480 train_time:41627ms step_avg:258.55ms
step:172/1480 train_time:41889ms step_avg:258.58ms
step:173/1480 train_time:42151ms step_avg:258.59ms
step:174/1480 train_time:42411ms step_avg:258.60ms
step:175/1480 train_time:42670ms step_avg:258.61ms
step:176/1480 train_time:42931ms step_avg:258.62ms
step:177/1480 train_time:43191ms step_avg:258.63ms
step:178/1480 train_time:43455ms step_avg:258.66ms
step:179/1480 train_time:43716ms step_avg:258.67ms
step:180/1480 train_time:43978ms step_avg:258.69ms
step:181/1480 train_time:44240ms step_avg:258.71ms
step:182/1480 train_time:44503ms step_avg:258.74ms
step:183/1480 train_time:44765ms step_avg:258.76ms
step:184/1480 train_time:45028ms step_avg:258.78ms
step:185/1480 train_time:45290ms step_avg:258.80ms
step:186/1480 train_time:45553ms step_avg:258.83ms
step:187/1480 train_time:45814ms step_avg:258.84ms
step:188/1480 train_time:46073ms step_avg:258.84ms
step:189/1480 train_time:46334ms step_avg:258.85ms
step:190/1480 train_time:46598ms step_avg:258.88ms
step:191/1480 train_time:46863ms step_avg:258.91ms
step:192/1480 train_time:47127ms step_avg:258.94ms
step:193/1480 train_time:47388ms step_avg:258.95ms
step:194/1480 train_time:47649ms step_avg:258.96ms
step:195/1480 train_time:47911ms step_avg:258.98ms
step:196/1480 train_time:48172ms step_avg:258.99ms
step:197/1480 train_time:48433ms step_avg:259.00ms
step:198/1480 train_time:48695ms step_avg:259.01ms
step:199/1480 train_time:48961ms step_avg:259.05ms
step:200/1480 train_time:49226ms step_avg:259.08ms
step:201/1480 train_time:49487ms step_avg:259.09ms
step:202/1480 train_time:49747ms step_avg:259.10ms
step:203/1480 train_time:50008ms step_avg:259.11ms
step:204/1480 train_time:50269ms step_avg:259.12ms
step:205/1480 train_time:50530ms step_avg:259.13ms
step:206/1480 train_time:50792ms step_avg:259.14ms
step:207/1480 train_time:51057ms step_avg:259.17ms
step:208/1480 train_time:51318ms step_avg:259.18ms
step:209/1480 train_time:51583ms step_avg:259.21ms
step:210/1480 train_time:51844ms step_avg:259.22ms
step:211/1480 train_time:52107ms step_avg:259.24ms
step:212/1480 train_time:52366ms step_avg:259.24ms
step:213/1480 train_time:52628ms step_avg:259.25ms
step:214/1480 train_time:52890ms step_avg:259.26ms
step:215/1480 train_time:53154ms step_avg:259.29ms
step:216/1480 train_time:53417ms step_avg:259.31ms
step:217/1480 train_time:53681ms step_avg:259.33ms
step:218/1480 train_time:53943ms step_avg:259.34ms
step:219/1480 train_time:54206ms step_avg:259.36ms
step:220/1480 train_time:54466ms step_avg:259.36ms
step:221/1480 train_time:54729ms step_avg:259.38ms
step:222/1480 train_time:54997ms step_avg:259.42ms
step:223/1480 train_time:55266ms step_avg:259.46ms
step:224/1480 train_time:55530ms step_avg:259.49ms
step:225/1480 train_time:55800ms step_avg:259.54ms
step:226/1480 train_time:56066ms step_avg:259.57ms
step:227/1480 train_time:56330ms step_avg:259.59ms
step:228/1480 train_time:56600ms step_avg:259.63ms
step:229/1480 train_time:56866ms step_avg:259.66ms
step:230/1480 train_time:57131ms step_avg:259.69ms
step:231/1480 train_time:57401ms step_avg:259.73ms
step:232/1480 train_time:57668ms step_avg:259.77ms
step:233/1480 train_time:57932ms step_avg:259.78ms
step:234/1480 train_time:58199ms step_avg:259.82ms
step:235/1480 train_time:58466ms step_avg:259.85ms
step:236/1480 train_time:58731ms step_avg:259.87ms
step:237/1480 train_time:59000ms step_avg:259.91ms
step:238/1480 train_time:59266ms step_avg:259.94ms
step:239/1480 train_time:59532ms step_avg:259.97ms
step:240/1480 train_time:59801ms step_avg:260.01ms
step:241/1480 train_time:60069ms step_avg:260.04ms
step:242/1480 train_time:60335ms step_avg:260.07ms
step:243/1480 train_time:60604ms step_avg:260.10ms
step:244/1480 train_time:60868ms step_avg:260.12ms
step:245/1480 train_time:61132ms step_avg:260.14ms
step:246/1480 train_time:61401ms step_avg:260.17ms
step:247/1480 train_time:61667ms step_avg:260.20ms
step:248/1480 train_time:61933ms step_avg:260.22ms
step:249/1480 train_time:62202ms step_avg:260.26ms
step:250/1480 train_time:62467ms step_avg:260.28ms
step:250/1480 val_loss:4.0066 train_time:62597ms step_avg:260.82ms
step:251/1480 train_time:62736ms step_avg:260.32ms
step:252/1480 train_time:63004ms step_avg:260.35ms
step:253/1480 train_time:63269ms step_avg:260.36ms
step:254/1480 train_time:63537ms step_avg:260.40ms
step:255/1480 train_time:63803ms step_avg:260.42ms
step:256/1480 train_time:64069ms step_avg:260.44ms
step:257/1480 train_time:64337ms step_avg:260.47ms
step:258/1480 train_time:64602ms step_avg:260.49ms
step:259/1480 train_time:64869ms step_avg:260.52ms
step:260/1480 train_time:65136ms step_avg:260.54ms
step:261/1480 train_time:65403ms step_avg:260.57ms
step:262/1480 train_time:65667ms step_avg:260.58ms
step:263/1480 train_time:65935ms step_avg:260.61ms
step:264/1480 train_time:66202ms step_avg:260.64ms
step:265/1480 train_time:66467ms step_avg:260.66ms
step:266/1480 train_time:66736ms step_avg:260.69ms
step:267/1480 train_time:67003ms step_avg:260.71ms
step:268/1480 train_time:67268ms step_avg:260.73ms
step:269/1480 train_time:67534ms step_avg:260.75ms
step:270/1480 train_time:67802ms step_avg:260.78ms
step:271/1480 train_time:68069ms step_avg:260.80ms
step:272/1480 train_time:68335ms step_avg:260.82ms
step:273/1480 train_time:68602ms step_avg:260.85ms
step:274/1480 train_time:68866ms step_avg:260.86ms
step:275/1480 train_time:69135ms step_avg:260.89ms
step:276/1480 train_time:69401ms step_avg:260.91ms
step:277/1480 train_time:69664ms step_avg:260.92ms
step:278/1480 train_time:69932ms step_avg:260.94ms
step:279/1480 train_time:70201ms step_avg:260.97ms
step:280/1480 train_time:70464ms step_avg:260.98ms
step:281/1480 train_time:70731ms step_avg:261.00ms
step:282/1480 train_time:70999ms step_avg:261.03ms
step:283/1480 train_time:71264ms step_avg:261.04ms
step:284/1480 train_time:71531ms step_avg:261.06ms
step:285/1480 train_time:71800ms step_avg:261.09ms
step:286/1480 train_time:72063ms step_avg:261.10ms
step:287/1480 train_time:72329ms step_avg:261.12ms
step:288/1480 train_time:72597ms step_avg:261.14ms
step:289/1480 train_time:72862ms step_avg:261.16ms
step:290/1480 train_time:73126ms step_avg:261.17ms
step:291/1480 train_time:73395ms step_avg:261.19ms
step:292/1480 train_time:73663ms step_avg:261.22ms
step:293/1480 train_time:73927ms step_avg:261.23ms
step:294/1480 train_time:74193ms step_avg:261.24ms
step:295/1480 train_time:74461ms step_avg:261.27ms
step:296/1480 train_time:74727ms step_avg:261.28ms
step:297/1480 train_time:74996ms step_avg:261.31ms
step:298/1480 train_time:75263ms step_avg:261.33ms
step:299/1480 train_time:75526ms step_avg:261.34ms
step:300/1480 train_time:75795ms step_avg:261.36ms
step:301/1480 train_time:76062ms step_avg:261.38ms
step:302/1480 train_time:76328ms step_avg:261.40ms
step:303/1480 train_time:76596ms step_avg:261.42ms
step:304/1480 train_time:76862ms step_avg:261.44ms
step:305/1480 train_time:77127ms step_avg:261.45ms
step:306/1480 train_time:77396ms step_avg:261.47ms
step:307/1480 train_time:77662ms step_avg:261.49ms
step:308/1480 train_time:77928ms step_avg:261.50ms
step:309/1480 train_time:78196ms step_avg:261.52ms
step:310/1480 train_time:78462ms step_avg:261.54ms
step:311/1480 train_time:78728ms step_avg:261.55ms
step:312/1480 train_time:78997ms step_avg:261.58ms
step:313/1480 train_time:79263ms step_avg:261.59ms
step:314/1480 train_time:79528ms step_avg:261.61ms
step:315/1480 train_time:79797ms step_avg:261.63ms
step:316/1480 train_time:80064ms step_avg:261.65ms
step:317/1480 train_time:80332ms step_avg:261.67ms
step:318/1480 train_time:80600ms step_avg:261.69ms
step:319/1480 train_time:80865ms step_avg:261.70ms
step:320/1480 train_time:81135ms step_avg:261.73ms
step:321/1480 train_time:81403ms step_avg:261.75ms
step:322/1480 train_time:81667ms step_avg:261.75ms
step:323/1480 train_time:81934ms step_avg:261.77ms
step:324/1480 train_time:82201ms step_avg:261.79ms
step:325/1480 train_time:82464ms step_avg:261.79ms
step:326/1480 train_time:82734ms step_avg:261.82ms
step:327/1480 train_time:83002ms step_avg:261.83ms
step:328/1480 train_time:83267ms step_avg:261.84ms
step:329/1480 train_time:83532ms step_avg:261.86ms
step:330/1480 train_time:83802ms step_avg:261.88ms
step:331/1480 train_time:84071ms step_avg:261.90ms
step:332/1480 train_time:84342ms step_avg:261.93ms
step:333/1480 train_time:84613ms step_avg:261.96ms
step:334/1480 train_time:84885ms step_avg:261.99ms
step:335/1480 train_time:85157ms step_avg:262.02ms
step:336/1480 train_time:85426ms step_avg:262.04ms
step:337/1480 train_time:85698ms step_avg:262.07ms
step:338/1480 train_time:85965ms step_avg:262.09ms
step:339/1480 train_time:86238ms step_avg:262.12ms
step:340/1480 train_time:86507ms step_avg:262.14ms
step:341/1480 train_time:86779ms step_avg:262.17ms
step:342/1480 train_time:87051ms step_avg:262.20ms
step:343/1480 train_time:87325ms step_avg:262.24ms
step:344/1480 train_time:87598ms step_avg:262.27ms
step:345/1480 train_time:87866ms step_avg:262.29ms
step:346/1480 train_time:88140ms step_avg:262.32ms
step:347/1480 train_time:88410ms step_avg:262.35ms
step:348/1480 train_time:88682ms step_avg:262.37ms
step:349/1480 train_time:88955ms step_avg:262.40ms
step:350/1480 train_time:89225ms step_avg:262.43ms
step:351/1480 train_time:89497ms step_avg:262.45ms
step:352/1480 train_time:89766ms step_avg:262.47ms
step:353/1480 train_time:90039ms step_avg:262.50ms
step:354/1480 train_time:90308ms step_avg:262.52ms
step:355/1480 train_time:90581ms step_avg:262.55ms
step:356/1480 train_time:90851ms step_avg:262.57ms
step:357/1480 train_time:91123ms step_avg:262.60ms
step:358/1480 train_time:91393ms step_avg:262.62ms
step:359/1480 train_time:91663ms step_avg:262.64ms
step:360/1480 train_time:91934ms step_avg:262.67ms
step:361/1480 train_time:92203ms step_avg:262.69ms
step:362/1480 train_time:92475ms step_avg:262.71ms
step:363/1480 train_time:92745ms step_avg:262.73ms
step:364/1480 train_time:93017ms step_avg:262.76ms
step:365/1480 train_time:93285ms step_avg:262.77ms
step:366/1480 train_time:93559ms step_avg:262.81ms
step:367/1480 train_time:93828ms step_avg:262.82ms
step:368/1480 train_time:94099ms step_avg:262.85ms
step:369/1480 train_time:94368ms step_avg:262.86ms
step:370/1480 train_time:94640ms step_avg:262.89ms
step:371/1480 train_time:94911ms step_avg:262.91ms
step:372/1480 train_time:95182ms step_avg:262.93ms
step:373/1480 train_time:95453ms step_avg:262.96ms
step:374/1480 train_time:95725ms step_avg:262.98ms
step:375/1480 train_time:95996ms step_avg:263.00ms
step:375/1480 val_loss:3.8302 train_time:96128ms step_avg:263.36ms
step:376/1480 train_time:96269ms step_avg:263.03ms
step:377/1480 train_time:96544ms step_avg:263.06ms
step:378/1480 train_time:96813ms step_avg:263.08ms
step:379/1480 train_time:97086ms step_avg:263.11ms
step:380/1480 train_time:97354ms step_avg:263.12ms
step:381/1480 train_time:97628ms step_avg:263.15ms
step:382/1480 train_time:97898ms step_avg:263.17ms
step:383/1480 train_time:98170ms step_avg:263.19ms
step:384/1480 train_time:98440ms step_avg:263.21ms
step:385/1480 train_time:98710ms step_avg:263.23ms
step:386/1480 train_time:98984ms step_avg:263.26ms
step:387/1480 train_time:99252ms step_avg:263.27ms
step:388/1480 train_time:99526ms step_avg:263.30ms
step:389/1480 train_time:99794ms step_avg:263.31ms
step:390/1480 train_time:100067ms step_avg:263.33ms
step:391/1480 train_time:100339ms step_avg:263.36ms
step:392/1480 train_time:100608ms step_avg:263.37ms
step:393/1480 train_time:100879ms step_avg:263.39ms
step:394/1480 train_time:101150ms step_avg:263.41ms
step:395/1480 train_time:101419ms step_avg:263.43ms
step:396/1480 train_time:101692ms step_avg:263.45ms
step:397/1480 train_time:101960ms step_avg:263.46ms
step:398/1480 train_time:102232ms step_avg:263.48ms
step:399/1480 train_time:102503ms step_avg:263.50ms
step:400/1480 train_time:102772ms step_avg:263.52ms
step:401/1480 train_time:103044ms step_avg:263.54ms
step:402/1480 train_time:103313ms step_avg:263.55ms
step:403/1480 train_time:103589ms step_avg:263.58ms
step:404/1480 train_time:103855ms step_avg:263.59ms
step:405/1480 train_time:104129ms step_avg:263.62ms
step:406/1480 train_time:104403ms step_avg:263.64ms
step:407/1480 train_time:104672ms step_avg:263.66ms
step:408/1480 train_time:104943ms step_avg:263.68ms
step:409/1480 train_time:105212ms step_avg:263.69ms
step:410/1480 train_time:105483ms step_avg:263.71ms
step:411/1480 train_time:105753ms step_avg:263.72ms
step:412/1480 train_time:106027ms step_avg:263.75ms
step:413/1480 train_time:106298ms step_avg:263.77ms
step:414/1480 train_time:106570ms step_avg:263.79ms
step:415/1480 train_time:106842ms step_avg:263.81ms
step:416/1480 train_time:107111ms step_avg:263.82ms
step:417/1480 train_time:107381ms step_avg:263.84ms
step:418/1480 train_time:107652ms step_avg:263.85ms
step:419/1480 train_time:107922ms step_avg:263.87ms
step:420/1480 train_time:108192ms step_avg:263.88ms
step:421/1480 train_time:108466ms step_avg:263.91ms
step:422/1480 train_time:108734ms step_avg:263.92ms
step:423/1480 train_time:109006ms step_avg:263.94ms
step:424/1480 train_time:109275ms step_avg:263.95ms
step:425/1480 train_time:109547ms step_avg:263.97ms
step:426/1480 train_time:109815ms step_avg:263.98ms
step:427/1480 train_time:110089ms step_avg:264.00ms
step:428/1480 train_time:110356ms step_avg:264.01ms
step:429/1480 train_time:110628ms step_avg:264.03ms
step:430/1480 train_time:110899ms step_avg:264.04ms
step:431/1480 train_time:111171ms step_avg:264.06ms
step:432/1480 train_time:111440ms step_avg:264.08ms
step:433/1480 train_time:111710ms step_avg:264.09ms
step:434/1480 train_time:111982ms step_avg:264.11ms
step:435/1480 train_time:112251ms step_avg:264.12ms
step:436/1480 train_time:112523ms step_avg:264.14ms
step:437/1480 train_time:112795ms step_avg:264.16ms
step:438/1480 train_time:113066ms step_avg:264.17ms
step:439/1480 train_time:113333ms step_avg:264.18ms
step:440/1480 train_time:113606ms step_avg:264.20ms
step:441/1480 train_time:113878ms step_avg:264.22ms
step:442/1480 train_time:114153ms step_avg:264.24ms
step:443/1480 train_time:114428ms step_avg:264.27ms
step:444/1480 train_time:114707ms step_avg:264.30ms
step:445/1480 train_time:114979ms step_avg:264.32ms
step:446/1480 train_time:115252ms step_avg:264.34ms
step:447/1480 train_time:115526ms step_avg:264.36ms
step:448/1480 train_time:115804ms step_avg:264.39ms
step:449/1480 train_time:116073ms step_avg:264.40ms
step:450/1480 train_time:116349ms step_avg:264.43ms
step:451/1480 train_time:116621ms step_avg:264.45ms
step:452/1480 train_time:116894ms step_avg:264.47ms
step:453/1480 train_time:117169ms step_avg:264.49ms
step:454/1480 train_time:117445ms step_avg:264.52ms
step:455/1480 train_time:117718ms step_avg:264.53ms
step:456/1480 train_time:117990ms step_avg:264.55ms
step:457/1480 train_time:118266ms step_avg:264.58ms
step:458/1480 train_time:118538ms step_avg:264.59ms
step:459/1480 train_time:118810ms step_avg:264.61ms
step:460/1480 train_time:119086ms step_avg:264.63ms
step:461/1480 train_time:119356ms step_avg:264.65ms
step:462/1480 train_time:119631ms step_avg:264.67ms
step:463/1480 train_time:119906ms step_avg:264.69ms
step:464/1480 train_time:120181ms step_avg:264.72ms
step:465/1480 train_time:120453ms step_avg:264.73ms
step:466/1480 train_time:120728ms step_avg:264.75ms
step:467/1480 train_time:121001ms step_avg:264.77ms
step:468/1480 train_time:121274ms step_avg:264.79ms
step:469/1480 train_time:121549ms step_avg:264.81ms
step:470/1480 train_time:121824ms step_avg:264.83ms
step:471/1480 train_time:122098ms step_avg:264.85ms
step:472/1480 train_time:122371ms step_avg:264.87ms
step:473/1480 train_time:122647ms step_avg:264.90ms
step:474/1480 train_time:122921ms step_avg:264.92ms
step:475/1480 train_time:123196ms step_avg:264.94ms
step:476/1480 train_time:123470ms step_avg:264.96ms
step:477/1480 train_time:123745ms step_avg:264.98ms
step:478/1480 train_time:124017ms step_avg:264.99ms
step:479/1480 train_time:124291ms step_avg:265.01ms
step:480/1480 train_time:124568ms step_avg:265.04ms
step:481/1480 train_time:124840ms step_avg:265.05ms
step:482/1480 train_time:125114ms step_avg:265.07ms
step:483/1480 train_time:125389ms step_avg:265.09ms
step:484/1480 train_time:125659ms step_avg:265.10ms
step:485/1480 train_time:125933ms step_avg:265.12ms
step:486/1480 train_time:126208ms step_avg:265.14ms
step:487/1480 train_time:126481ms step_avg:265.16ms
step:488/1480 train_time:126754ms step_avg:265.18ms
step:489/1480 train_time:127030ms step_avg:265.20ms
step:490/1480 train_time:127306ms step_avg:265.22ms
step:491/1480 train_time:127579ms step_avg:265.24ms
step:492/1480 train_time:127850ms step_avg:265.25ms
step:493/1480 train_time:128127ms step_avg:265.27ms
step:494/1480 train_time:128405ms step_avg:265.30ms
step:495/1480 train_time:128677ms step_avg:265.31ms
step:496/1480 train_time:128951ms step_avg:265.33ms
step:497/1480 train_time:129226ms step_avg:265.35ms
step:498/1480 train_time:129497ms step_avg:265.36ms
step:499/1480 train_time:129772ms step_avg:265.38ms
step:500/1480 train_time:130047ms step_avg:265.40ms
step:500/1480 val_loss:3.7171 train_time:130180ms step_avg:265.67ms
step:501/1480 train_time:130323ms step_avg:265.42ms
step:502/1480 train_time:130597ms step_avg:265.44ms
step:503/1480 train_time:130876ms step_avg:265.47ms
step:504/1480 train_time:131152ms step_avg:265.49ms
step:505/1480 train_time:131422ms step_avg:265.50ms
step:506/1480 train_time:131697ms step_avg:265.52ms
step:507/1480 train_time:131974ms step_avg:265.54ms
step:508/1480 train_time:132246ms step_avg:265.55ms
step:509/1480 train_time:132519ms step_avg:265.57ms
step:510/1480 train_time:132796ms step_avg:265.59ms
step:511/1480 train_time:133070ms step_avg:265.61ms
step:512/1480 train_time:133344ms step_avg:265.63ms
step:513/1480 train_time:133616ms step_avg:265.64ms
step:514/1480 train_time:133892ms step_avg:265.66ms
step:515/1480 train_time:134165ms step_avg:265.67ms
step:516/1480 train_time:134436ms step_avg:265.68ms
step:517/1480 train_time:134712ms step_avg:265.70ms
step:518/1480 train_time:134986ms step_avg:265.72ms
step:519/1480 train_time:135257ms step_avg:265.73ms
step:520/1480 train_time:135534ms step_avg:265.75ms
step:521/1480 train_time:135810ms step_avg:265.77ms
step:522/1480 train_time:136085ms step_avg:265.79ms
step:523/1480 train_time:136357ms step_avg:265.80ms
step:524/1480 train_time:136635ms step_avg:265.83ms
step:525/1480 train_time:136907ms step_avg:265.84ms
step:526/1480 train_time:137177ms step_avg:265.85ms
step:527/1480 train_time:137453ms step_avg:265.87ms
step:528/1480 train_time:137723ms step_avg:265.88ms
step:529/1480 train_time:137996ms step_avg:265.89ms
step:530/1480 train_time:138271ms step_avg:265.91ms
step:531/1480 train_time:138544ms step_avg:265.92ms
step:532/1480 train_time:138817ms step_avg:265.93ms
step:533/1480 train_time:139093ms step_avg:265.95ms
step:534/1480 train_time:139370ms step_avg:265.97ms
step:535/1480 train_time:139644ms step_avg:265.99ms
step:536/1480 train_time:139917ms step_avg:266.00ms
step:537/1480 train_time:140192ms step_avg:266.02ms
step:538/1480 train_time:140464ms step_avg:266.03ms
step:539/1480 train_time:140738ms step_avg:266.04ms
step:540/1480 train_time:141014ms step_avg:266.06ms
step:541/1480 train_time:141288ms step_avg:266.08ms
step:542/1480 train_time:141559ms step_avg:266.09ms
step:543/1480 train_time:141835ms step_avg:266.11ms
step:544/1480 train_time:142110ms step_avg:266.12ms
step:545/1480 train_time:142378ms step_avg:266.13ms
step:546/1480 train_time:142657ms step_avg:266.15ms
step:547/1480 train_time:142928ms step_avg:266.16ms
step:548/1480 train_time:143202ms step_avg:266.17ms
step:549/1480 train_time:143475ms step_avg:266.19ms
step:550/1480 train_time:143753ms step_avg:266.21ms
step:551/1480 train_time:144028ms step_avg:266.23ms
step:552/1480 train_time:144301ms step_avg:266.24ms
step:553/1480 train_time:144577ms step_avg:266.26ms
step:554/1480 train_time:144856ms step_avg:266.28ms
step:555/1480 train_time:145133ms step_avg:266.30ms
step:556/1480 train_time:145408ms step_avg:266.32ms
step:557/1480 train_time:145687ms step_avg:266.34ms
step:558/1480 train_time:145961ms step_avg:266.35ms
step:559/1480 train_time:146238ms step_avg:266.37ms
step:560/1480 train_time:146515ms step_avg:266.39ms
step:561/1480 train_time:146795ms step_avg:266.42ms
step:562/1480 train_time:147072ms step_avg:266.44ms
step:563/1480 train_time:147349ms step_avg:266.45ms
step:564/1480 train_time:147623ms step_avg:266.47ms
step:565/1480 train_time:147901ms step_avg:266.49ms
step:566/1480 train_time:148175ms step_avg:266.50ms
step:567/1480 train_time:148455ms step_avg:266.53ms
step:568/1480 train_time:148733ms step_avg:266.55ms
step:569/1480 train_time:149010ms step_avg:266.57ms
step:570/1480 train_time:149286ms step_avg:266.58ms
step:571/1480 train_time:149558ms step_avg:266.59ms
step:572/1480 train_time:149839ms step_avg:266.62ms
step:573/1480 train_time:150116ms step_avg:266.64ms
step:574/1480 train_time:150393ms step_avg:266.65ms
step:575/1480 train_time:150670ms step_avg:266.67ms
step:576/1480 train_time:150947ms step_avg:266.69ms
step:577/1480 train_time:151220ms step_avg:266.70ms
step:578/1480 train_time:151498ms step_avg:266.72ms
step:579/1480 train_time:151775ms step_avg:266.74ms
step:580/1480 train_time:152053ms step_avg:266.76ms
step:581/1480 train_time:152329ms step_avg:266.78ms
step:582/1480 train_time:152607ms step_avg:266.79ms
step:583/1480 train_time:152880ms step_avg:266.81ms
step:584/1480 train_time:153158ms step_avg:266.83ms
step:585/1480 train_time:153436ms step_avg:266.84ms
step:586/1480 train_time:153713ms step_avg:266.86ms
step:587/1480 train_time:153992ms step_avg:266.88ms
step:588/1480 train_time:154270ms step_avg:266.90ms
step:589/1480 train_time:154544ms step_avg:266.92ms
step:590/1480 train_time:154820ms step_avg:266.93ms
step:591/1480 train_time:155096ms step_avg:266.95ms
step:592/1480 train_time:155375ms step_avg:266.97ms
step:593/1480 train_time:155654ms step_avg:266.99ms
step:594/1480 train_time:155928ms step_avg:267.00ms
step:595/1480 train_time:156207ms step_avg:267.02ms
step:596/1480 train_time:156481ms step_avg:267.03ms
step:597/1480 train_time:156758ms step_avg:267.05ms
step:598/1480 train_time:157036ms step_avg:267.07ms
step:599/1480 train_time:157315ms step_avg:267.09ms
step:600/1480 train_time:157593ms step_avg:267.11ms
step:601/1480 train_time:157870ms step_avg:267.12ms
step:602/1480 train_time:158149ms step_avg:267.14ms
step:603/1480 train_time:158421ms step_avg:267.15ms
step:604/1480 train_time:158697ms step_avg:267.17ms
step:605/1480 train_time:158976ms step_avg:267.19ms
step:606/1480 train_time:159255ms step_avg:267.21ms
step:607/1480 train_time:159531ms step_avg:267.22ms
step:608/1480 train_time:159807ms step_avg:267.24ms
step:609/1480 train_time:160084ms step_avg:267.25ms
step:610/1480 train_time:160359ms step_avg:267.26ms
step:611/1480 train_time:160638ms step_avg:267.28ms
step:612/1480 train_time:160915ms step_avg:267.30ms
step:613/1480 train_time:161196ms step_avg:267.32ms
step:614/1480 train_time:161473ms step_avg:267.34ms
step:615/1480 train_time:161752ms step_avg:267.36ms
step:616/1480 train_time:162026ms step_avg:267.37ms
step:617/1480 train_time:162301ms step_avg:267.38ms
step:618/1480 train_time:162576ms step_avg:267.39ms
step:619/1480 train_time:162855ms step_avg:267.41ms
step:620/1480 train_time:163131ms step_avg:267.43ms
step:621/1480 train_time:163407ms step_avg:267.44ms
step:622/1480 train_time:163680ms step_avg:267.45ms
step:623/1480 train_time:163956ms step_avg:267.47ms
step:624/1480 train_time:164235ms step_avg:267.48ms
step:625/1480 train_time:164514ms step_avg:267.50ms
step:625/1480 val_loss:3.6396 train_time:164649ms step_avg:267.72ms
step:626/1480 train_time:164793ms step_avg:267.52ms
step:627/1480 train_time:165073ms step_avg:267.54ms
step:628/1480 train_time:165351ms step_avg:267.56ms
step:629/1480 train_time:165628ms step_avg:267.57ms
step:630/1480 train_time:165907ms step_avg:267.59ms
step:631/1480 train_time:166180ms step_avg:267.60ms
step:632/1480 train_time:166455ms step_avg:267.61ms
step:633/1480 train_time:166730ms step_avg:267.62ms
step:634/1480 train_time:167010ms step_avg:267.64ms
step:635/1480 train_time:167285ms step_avg:267.66ms
step:636/1480 train_time:167565ms step_avg:267.68ms
step:637/1480 train_time:167838ms step_avg:267.68ms
step:638/1480 train_time:168114ms step_avg:267.70ms
step:639/1480 train_time:168390ms step_avg:267.71ms
step:640/1480 train_time:168669ms step_avg:267.73ms
step:641/1480 train_time:168947ms step_avg:267.74ms
step:642/1480 train_time:169226ms step_avg:267.76ms
step:643/1480 train_time:169503ms step_avg:267.78ms
step:644/1480 train_time:169781ms step_avg:267.79ms
step:645/1480 train_time:170062ms step_avg:267.81ms
step:646/1480 train_time:170334ms step_avg:267.82ms
step:647/1480 train_time:170613ms step_avg:267.84ms
step:648/1480 train_time:170893ms step_avg:267.86ms
step:649/1480 train_time:171172ms step_avg:267.87ms
step:650/1480 train_time:171449ms step_avg:267.89ms
step:651/1480 train_time:171729ms step_avg:267.91ms
step:652/1480 train_time:172006ms step_avg:267.92ms
step:653/1480 train_time:172281ms step_avg:267.93ms
step:654/1480 train_time:172554ms step_avg:267.94ms
step:655/1480 train_time:172833ms step_avg:267.96ms
step:656/1480 train_time:173111ms step_avg:267.97ms
step:657/1480 train_time:173389ms step_avg:267.99ms
step:658/1480 train_time:173666ms step_avg:268.00ms
step:659/1480 train_time:173947ms step_avg:268.02ms
step:660/1480 train_time:174227ms step_avg:268.04ms
step:661/1480 train_time:174511ms step_avg:268.07ms
step:662/1480 train_time:174791ms step_avg:268.08ms
step:663/1480 train_time:175070ms step_avg:268.10ms
step:664/1480 train_time:175351ms step_avg:268.12ms
step:665/1480 train_time:175629ms step_avg:268.14ms
step:666/1480 train_time:175911ms step_avg:268.16ms
step:667/1480 train_time:176192ms step_avg:268.18ms
step:668/1480 train_time:176472ms step_avg:268.20ms
step:669/1480 train_time:176753ms step_avg:268.21ms
step:670/1480 train_time:177032ms step_avg:268.23ms
step:671/1480 train_time:177313ms step_avg:268.25ms
step:672/1480 train_time:177591ms step_avg:268.26ms
step:673/1480 train_time:177871ms step_avg:268.28ms
step:674/1480 train_time:178149ms step_avg:268.30ms
step:675/1480 train_time:178426ms step_avg:268.31ms
step:676/1480 train_time:178709ms step_avg:268.33ms
step:677/1480 train_time:178989ms step_avg:268.35ms
step:678/1480 train_time:179265ms step_avg:268.36ms
step:679/1480 train_time:179543ms step_avg:268.38ms
step:680/1480 train_time:179825ms step_avg:268.40ms
step:681/1480 train_time:180103ms step_avg:268.41ms
step:682/1480 train_time:180385ms step_avg:268.43ms
step:683/1480 train_time:180660ms step_avg:268.44ms
step:684/1480 train_time:180937ms step_avg:268.45ms
step:685/1480 train_time:181215ms step_avg:268.47ms
step:686/1480 train_time:181494ms step_avg:268.48ms
step:687/1480 train_time:181772ms step_avg:268.50ms
step:688/1480 train_time:182052ms step_avg:268.51ms
step:689/1480 train_time:182331ms step_avg:268.53ms
step:690/1480 train_time:182614ms step_avg:268.55ms
step:691/1480 train_time:182895ms step_avg:268.57ms
step:692/1480 train_time:183175ms step_avg:268.59ms
step:693/1480 train_time:183453ms step_avg:268.60ms
step:694/1480 train_time:183733ms step_avg:268.62ms
step:695/1480 train_time:184014ms step_avg:268.63ms
step:696/1480 train_time:184291ms step_avg:268.65ms
step:697/1480 train_time:184571ms step_avg:268.66ms
step:698/1480 train_time:184851ms step_avg:268.68ms
step:699/1480 train_time:185131ms step_avg:268.70ms
step:700/1480 train_time:185411ms step_avg:268.71ms
step:701/1480 train_time:185691ms step_avg:268.73ms
step:702/1480 train_time:185975ms step_avg:268.75ms
step:703/1480 train_time:186251ms step_avg:268.76ms
step:704/1480 train_time:186529ms step_avg:268.77ms
step:705/1480 train_time:186811ms step_avg:268.79ms
step:706/1480 train_time:187089ms step_avg:268.81ms
step:707/1480 train_time:187370ms step_avg:268.82ms
step:708/1480 train_time:187651ms step_avg:268.84ms
step:709/1480 train_time:187932ms step_avg:268.86ms
step:710/1480 train_time:188211ms step_avg:268.87ms
step:711/1480 train_time:188491ms step_avg:268.89ms
step:712/1480 train_time:188771ms step_avg:268.90ms
step:713/1480 train_time:189048ms step_avg:268.92ms
step:714/1480 train_time:189330ms step_avg:268.94ms
step:715/1480 train_time:189611ms step_avg:268.95ms
step:716/1480 train_time:189890ms step_avg:268.97ms
step:717/1480 train_time:190171ms step_avg:268.98ms
step:718/1480 train_time:190450ms step_avg:269.00ms
step:719/1480 train_time:190728ms step_avg:269.01ms
step:720/1480 train_time:191010ms step_avg:269.03ms
step:721/1480 train_time:191287ms step_avg:269.04ms
step:722/1480 train_time:191568ms step_avg:269.06ms
step:723/1480 train_time:191849ms step_avg:269.07ms
step:724/1480 train_time:192128ms step_avg:269.09ms
step:725/1480 train_time:192411ms step_avg:269.11ms
step:726/1480 train_time:192690ms step_avg:269.12ms
step:727/1480 train_time:192972ms step_avg:269.14ms
step:728/1480 train_time:193251ms step_avg:269.15ms
step:729/1480 train_time:193531ms step_avg:269.17ms
step:730/1480 train_time:193811ms step_avg:269.18ms
step:731/1480 train_time:194092ms step_avg:269.20ms
step:732/1480 train_time:194372ms step_avg:269.21ms
step:733/1480 train_time:194652ms step_avg:269.23ms
step:734/1480 train_time:194930ms step_avg:269.24ms
step:735/1480 train_time:195211ms step_avg:269.26ms
step:736/1480 train_time:195490ms step_avg:269.27ms
step:737/1480 train_time:195769ms step_avg:269.28ms
step:738/1480 train_time:196049ms step_avg:269.30ms
step:739/1480 train_time:196327ms step_avg:269.31ms
step:740/1480 train_time:196610ms step_avg:269.33ms
step:741/1480 train_time:196888ms step_avg:269.34ms
step:742/1480 train_time:197169ms step_avg:269.36ms
step:743/1480 train_time:197448ms step_avg:269.37ms
step:744/1480 train_time:197722ms step_avg:269.38ms
step:745/1480 train_time:198003ms step_avg:269.39ms
step:746/1480 train_time:198280ms step_avg:269.40ms
step:747/1480 train_time:198559ms step_avg:269.42ms
step:748/1480 train_time:198835ms step_avg:269.42ms
step:749/1480 train_time:199114ms step_avg:269.44ms
step:750/1480 train_time:199394ms step_avg:269.45ms
step:750/1480 val_loss:3.5809 train_time:199529ms step_avg:269.63ms
step:751/1480 train_time:199671ms step_avg:269.46ms
step:752/1480 train_time:199953ms step_avg:269.48ms
step:753/1480 train_time:200231ms step_avg:269.49ms
step:754/1480 train_time:200510ms step_avg:269.50ms
step:755/1480 train_time:200790ms step_avg:269.52ms
step:756/1480 train_time:201069ms step_avg:269.53ms
step:757/1480 train_time:201350ms step_avg:269.55ms
step:758/1480 train_time:201631ms step_avg:269.56ms
step:759/1480 train_time:201910ms step_avg:269.57ms
step:760/1480 train_time:202189ms step_avg:269.59ms
step:761/1480 train_time:202469ms step_avg:269.60ms
step:762/1480 train_time:202753ms step_avg:269.62ms
step:763/1480 train_time:203033ms step_avg:269.63ms
step:764/1480 train_time:203310ms step_avg:269.64ms
step:765/1480 train_time:203588ms step_avg:269.65ms
step:766/1480 train_time:203869ms step_avg:269.67ms
step:767/1480 train_time:204150ms step_avg:269.68ms
step:768/1480 train_time:204427ms step_avg:269.69ms
step:769/1480 train_time:204707ms step_avg:269.71ms
step:770/1480 train_time:204989ms step_avg:269.72ms
step:771/1480 train_time:205270ms step_avg:269.74ms
step:772/1480 train_time:205552ms step_avg:269.75ms
step:773/1480 train_time:205833ms step_avg:269.77ms
step:774/1480 train_time:206116ms step_avg:269.79ms
step:775/1480 train_time:206393ms step_avg:269.80ms
step:776/1480 train_time:206672ms step_avg:269.81ms
step:777/1480 train_time:206952ms step_avg:269.82ms
step:778/1480 train_time:207233ms step_avg:269.83ms
step:779/1480 train_time:207512ms step_avg:269.85ms
step:780/1480 train_time:207792ms step_avg:269.86ms
step:781/1480 train_time:208072ms step_avg:269.87ms
step:782/1480 train_time:208352ms step_avg:269.89ms
step:783/1480 train_time:208632ms step_avg:269.90ms
step:784/1480 train_time:208914ms step_avg:269.91ms
step:785/1480 train_time:209193ms step_avg:269.93ms
step:786/1480 train_time:209475ms step_avg:269.94ms
step:787/1480 train_time:209758ms step_avg:269.96ms
step:788/1480 train_time:210044ms step_avg:269.98ms
step:789/1480 train_time:210326ms step_avg:269.99ms
step:790/1480 train_time:210606ms step_avg:270.01ms
step:791/1480 train_time:210888ms step_avg:270.02ms
step:792/1480 train_time:211170ms step_avg:270.04ms
step:793/1480 train_time:211450ms step_avg:270.05ms
step:794/1480 train_time:211732ms step_avg:270.07ms
step:795/1480 train_time:212016ms step_avg:270.08ms
step:796/1480 train_time:212298ms step_avg:270.10ms
step:797/1480 train_time:212579ms step_avg:270.11ms
step:798/1480 train_time:212860ms step_avg:270.13ms
step:799/1480 train_time:213138ms step_avg:270.14ms
step:800/1480 train_time:213418ms step_avg:270.15ms
step:801/1480 train_time:213700ms step_avg:270.16ms
step:802/1480 train_time:213976ms step_avg:270.17ms
step:803/1480 train_time:214267ms step_avg:270.20ms
step:804/1480 train_time:214551ms step_avg:270.22ms
step:805/1480 train_time:214828ms step_avg:270.22ms
step:806/1480 train_time:215111ms step_avg:270.24ms
step:807/1480 train_time:215392ms step_avg:270.25ms
step:808/1480 train_time:215669ms step_avg:270.26ms
step:809/1480 train_time:215951ms step_avg:270.28ms
step:810/1480 train_time:216236ms step_avg:270.30ms
step:811/1480 train_time:216513ms step_avg:270.30ms
step:812/1480 train_time:216795ms step_avg:270.32ms
step:813/1480 train_time:217079ms step_avg:270.34ms
step:814/1480 train_time:217356ms step_avg:270.34ms
step:815/1480 train_time:217643ms step_avg:270.36ms
step:816/1480 train_time:217920ms step_avg:270.37ms
step:817/1480 train_time:218198ms step_avg:270.38ms
step:818/1480 train_time:218481ms step_avg:270.40ms
step:819/1480 train_time:218763ms step_avg:270.41ms
step:820/1480 train_time:219042ms step_avg:270.42ms
step:821/1480 train_time:219324ms step_avg:270.44ms
step:822/1480 train_time:219607ms step_avg:270.45ms
step:823/1480 train_time:219887ms step_avg:270.46ms
step:824/1480 train_time:220170ms step_avg:270.48ms
step:825/1480 train_time:220451ms step_avg:270.49ms
step:826/1480 train_time:220730ms step_avg:270.50ms
step:827/1480 train_time:221010ms step_avg:270.51ms
step:828/1480 train_time:221291ms step_avg:270.53ms
step:829/1480 train_time:221571ms step_avg:270.54ms
step:830/1480 train_time:221854ms step_avg:270.55ms
step:831/1480 train_time:222132ms step_avg:270.56ms
step:832/1480 train_time:222414ms step_avg:270.58ms
step:833/1480 train_time:222694ms step_avg:270.59ms
step:834/1480 train_time:222974ms step_avg:270.60ms
step:835/1480 train_time:223263ms step_avg:270.62ms
step:836/1480 train_time:223547ms step_avg:270.64ms
step:837/1480 train_time:223829ms step_avg:270.65ms
step:838/1480 train_time:224110ms step_avg:270.66ms
step:839/1480 train_time:224392ms step_avg:270.68ms
step:840/1480 train_time:224672ms step_avg:270.69ms
step:841/1480 train_time:224953ms step_avg:270.70ms
step:842/1480 train_time:225235ms step_avg:270.71ms
step:843/1480 train_time:225514ms step_avg:270.72ms
step:844/1480 train_time:225798ms step_avg:270.74ms
step:845/1480 train_time:226076ms step_avg:270.75ms
step:846/1480 train_time:226360ms step_avg:270.77ms
step:847/1480 train_time:226641ms step_avg:270.78ms
step:848/1480 train_time:226922ms step_avg:270.79ms
step:849/1480 train_time:227206ms step_avg:270.81ms
step:850/1480 train_time:227484ms step_avg:270.81ms
step:851/1480 train_time:227766ms step_avg:270.83ms
step:852/1480 train_time:228049ms step_avg:270.84ms
step:853/1480 train_time:228331ms step_avg:270.86ms
step:854/1480 train_time:228613ms step_avg:270.87ms
step:855/1480 train_time:228892ms step_avg:270.88ms
step:856/1480 train_time:229171ms step_avg:270.89ms
step:857/1480 train_time:229453ms step_avg:270.90ms
step:858/1480 train_time:229735ms step_avg:270.91ms
step:859/1480 train_time:230014ms step_avg:270.92ms
step:860/1480 train_time:230295ms step_avg:270.94ms
step:861/1480 train_time:230574ms step_avg:270.94ms
step:862/1480 train_time:230855ms step_avg:270.96ms
step:863/1480 train_time:231136ms step_avg:270.97ms
step:864/1480 train_time:231416ms step_avg:270.98ms
step:865/1480 train_time:231692ms step_avg:270.98ms
step:866/1480 train_time:231973ms step_avg:271.00ms
step:867/1480 train_time:232253ms step_avg:271.01ms
step:868/1480 train_time:232534ms step_avg:271.02ms
step:869/1480 train_time:232813ms step_avg:271.03ms
step:870/1480 train_time:233092ms step_avg:271.04ms
step:871/1480 train_time:233374ms step_avg:271.05ms
step:872/1480 train_time:233654ms step_avg:271.06ms
step:873/1480 train_time:233933ms step_avg:271.07ms
step:874/1480 train_time:234215ms step_avg:271.08ms
step:875/1480 train_time:234496ms step_avg:271.09ms
step:875/1480 val_loss:3.5384 train_time:234632ms step_avg:271.25ms
step:876/1480 train_time:234779ms step_avg:271.11ms
step:877/1480 train_time:235062ms step_avg:271.12ms
step:878/1480 train_time:235345ms step_avg:271.13ms
step:879/1480 train_time:235623ms step_avg:271.14ms
step:880/1480 train_time:235906ms step_avg:271.16ms
step:881/1480 train_time:236195ms step_avg:271.18ms
step:882/1480 train_time:236478ms step_avg:271.19ms
step:883/1480 train_time:236761ms step_avg:271.20ms
step:884/1480 train_time:237042ms step_avg:271.22ms
step:885/1480 train_time:237324ms step_avg:271.23ms
step:886/1480 train_time:237604ms step_avg:271.24ms
step:887/1480 train_time:237882ms step_avg:271.24ms
step:888/1480 train_time:238168ms step_avg:271.26ms
step:889/1480 train_time:238449ms step_avg:271.27ms
step:890/1480 train_time:238736ms step_avg:271.29ms
step:891/1480 train_time:239018ms step_avg:271.30ms
step:892/1480 train_time:239303ms step_avg:271.32ms
step:893/1480 train_time:239598ms step_avg:271.35ms
step:894/1480 train_time:239880ms step_avg:271.36ms
step:895/1480 train_time:240160ms step_avg:271.37ms
step:896/1480 train_time:240440ms step_avg:271.38ms
step:897/1480 train_time:240721ms step_avg:271.39ms
step:898/1480 train_time:241001ms step_avg:271.40ms
step:899/1480 train_time:241282ms step_avg:271.41ms
step:900/1480 train_time:241572ms step_avg:271.43ms
step:901/1480 train_time:241852ms step_avg:271.44ms
step:902/1480 train_time:242135ms step_avg:271.45ms
step:903/1480 train_time:242420ms step_avg:271.47ms
step:904/1480 train_time:242703ms step_avg:271.48ms
step:905/1480 train_time:242986ms step_avg:271.49ms
step:906/1480 train_time:243274ms step_avg:271.51ms
step:907/1480 train_time:243554ms step_avg:271.52ms
step:908/1480 train_time:243838ms step_avg:271.53ms
step:909/1480 train_time:244121ms step_avg:271.55ms
step:910/1480 train_time:244400ms step_avg:271.56ms
step:911/1480 train_time:244681ms step_avg:271.57ms
step:912/1480 train_time:244963ms step_avg:271.58ms
step:913/1480 train_time:245251ms step_avg:271.60ms
step:914/1480 train_time:245538ms step_avg:271.61ms
step:915/1480 train_time:245823ms step_avg:271.63ms
step:916/1480 train_time:246105ms step_avg:271.64ms
step:917/1480 train_time:246395ms step_avg:271.66ms
step:918/1480 train_time:246681ms step_avg:271.68ms
step:919/1480 train_time:246968ms step_avg:271.69ms
step:920/1480 train_time:247252ms step_avg:271.71ms
step:921/1480 train_time:247532ms step_avg:271.71ms
step:922/1480 train_time:247817ms step_avg:271.73ms
step:923/1480 train_time:248099ms step_avg:271.74ms
step:924/1480 train_time:248382ms step_avg:271.75ms
step:925/1480 train_time:248668ms step_avg:271.77ms
step:926/1480 train_time:248948ms step_avg:271.78ms
step:927/1480 train_time:249226ms step_avg:271.78ms
step:928/1480 train_time:249514ms step_avg:271.80ms
step:929/1480 train_time:249804ms step_avg:271.82ms
step:930/1480 train_time:250086ms step_avg:271.83ms
step:931/1480 train_time:250365ms step_avg:271.84ms
step:932/1480 train_time:250653ms step_avg:271.86ms
step:933/1480 train_time:250939ms step_avg:271.87ms
step:934/1480 train_time:251222ms step_avg:271.88ms
step:935/1480 train_time:251504ms step_avg:271.90ms
step:936/1480 train_time:251787ms step_avg:271.91ms
step:937/1480 train_time:252072ms step_avg:271.92ms
step:938/1480 train_time:252351ms step_avg:271.93ms
step:939/1480 train_time:252638ms step_avg:271.95ms
step:940/1480 train_time:252922ms step_avg:271.96ms
step:941/1480 train_time:253204ms step_avg:271.97ms
step:942/1480 train_time:253482ms step_avg:271.98ms
step:943/1480 train_time:253766ms step_avg:271.99ms
step:944/1480 train_time:254048ms step_avg:272.00ms
step:945/1480 train_time:254334ms step_avg:272.01ms
step:946/1480 train_time:254619ms step_avg:272.03ms
step:947/1480 train_time:254901ms step_avg:272.04ms
step:948/1480 train_time:255184ms step_avg:272.05ms
step:949/1480 train_time:255471ms step_avg:272.07ms
step:950/1480 train_time:255757ms step_avg:272.08ms
step:951/1480 train_time:256042ms step_avg:272.10ms
step:952/1480 train_time:256323ms step_avg:272.11ms
step:953/1480 train_time:256604ms step_avg:272.11ms
step:954/1480 train_time:256890ms step_avg:272.13ms
step:955/1480 train_time:257167ms step_avg:272.13ms
step:956/1480 train_time:257449ms step_avg:272.14ms
step:957/1480 train_time:257731ms step_avg:272.16ms
step:958/1480 train_time:258015ms step_avg:272.17ms
step:959/1480 train_time:258302ms step_avg:272.18ms
step:960/1480 train_time:258582ms step_avg:272.19ms
step:961/1480 train_time:258861ms step_avg:272.20ms
step:962/1480 train_time:259143ms step_avg:272.21ms
step:963/1480 train_time:259425ms step_avg:272.22ms
step:964/1480 train_time:259703ms step_avg:272.23ms
step:965/1480 train_time:259985ms step_avg:272.24ms
step:966/1480 train_time:260273ms step_avg:272.25ms
step:967/1480 train_time:260557ms step_avg:272.26ms
step:968/1480 train_time:260839ms step_avg:272.27ms
step:969/1480 train_time:261125ms step_avg:272.29ms
step:970/1480 train_time:261401ms step_avg:272.29ms
step:971/1480 train_time:261686ms step_avg:272.31ms
step:972/1480 train_time:261975ms step_avg:272.32ms
step:973/1480 train_time:262253ms step_avg:272.33ms
step:974/1480 train_time:262537ms step_avg:272.34ms
step:975/1480 train_time:262821ms step_avg:272.35ms
step:976/1480 train_time:263104ms step_avg:272.36ms
step:977/1480 train_time:263383ms step_avg:272.37ms
step:978/1480 train_time:263677ms step_avg:272.39ms
step:979/1480 train_time:263961ms step_avg:272.41ms
step:980/1480 train_time:264243ms step_avg:272.42ms
step:981/1480 train_time:264525ms step_avg:272.43ms
step:982/1480 train_time:264805ms step_avg:272.43ms
step:983/1480 train_time:265089ms step_avg:272.45ms
step:984/1480 train_time:265378ms step_avg:272.46ms
step:985/1480 train_time:265661ms step_avg:272.47ms
step:986/1480 train_time:265943ms step_avg:272.48ms
step:987/1480 train_time:266228ms step_avg:272.50ms
step:988/1480 train_time:266516ms step_avg:272.51ms
step:989/1480 train_time:266802ms step_avg:272.52ms
step:990/1480 train_time:267087ms step_avg:272.54ms
step:991/1480 train_time:267377ms step_avg:272.56ms
step:992/1480 train_time:267663ms step_avg:272.57ms
step:993/1480 train_time:267946ms step_avg:272.58ms
step:994/1480 train_time:268234ms step_avg:272.60ms
step:995/1480 train_time:268518ms step_avg:272.61ms
step:996/1480 train_time:268800ms step_avg:272.62ms
step:997/1480 train_time:269084ms step_avg:272.63ms
step:998/1480 train_time:269372ms step_avg:272.64ms
step:999/1480 train_time:269659ms step_avg:272.66ms
step:1000/1480 train_time:269949ms step_avg:272.68ms
step:1000/1480 val_loss:3.4709 train_time:270093ms step_avg:272.82ms
step:1001/1480 train_time:270242ms step_avg:272.70ms
step:1002/1480 train_time:270531ms step_avg:272.71ms
step:1003/1480 train_time:270825ms step_avg:272.73ms
step:1004/1480 train_time:271107ms step_avg:272.74ms
step:1005/1480 train_time:271393ms step_avg:272.76ms
step:1006/1480 train_time:271676ms step_avg:272.77ms
step:1007/1480 train_time:271963ms step_avg:272.78ms
step:1008/1480 train_time:272244ms step_avg:272.79ms
step:1009/1480 train_time:272528ms step_avg:272.80ms
step:1010/1480 train_time:272811ms step_avg:272.81ms
step:1011/1480 train_time:273097ms step_avg:272.82ms
step:1012/1480 train_time:273385ms step_avg:272.84ms
step:1013/1480 train_time:273679ms step_avg:272.86ms
step:1014/1480 train_time:273966ms step_avg:272.87ms
step:1015/1480 train_time:274251ms step_avg:272.89ms
step:1016/1480 train_time:274538ms step_avg:272.90ms
step:1017/1480 train_time:274823ms step_avg:272.91ms
step:1018/1480 train_time:275116ms step_avg:272.93ms
step:1019/1480 train_time:275399ms step_avg:272.94ms
step:1020/1480 train_time:275685ms step_avg:272.96ms
step:1021/1480 train_time:275969ms step_avg:272.97ms
step:1022/1480 train_time:276251ms step_avg:272.97ms
step:1023/1480 train_time:276538ms step_avg:272.99ms
step:1024/1480 train_time:276825ms step_avg:273.00ms
step:1025/1480 train_time:277107ms step_avg:273.01ms
step:1026/1480 train_time:277387ms step_avg:273.02ms
step:1027/1480 train_time:277672ms step_avg:273.03ms
step:1028/1480 train_time:277963ms step_avg:273.05ms
step:1029/1480 train_time:278251ms step_avg:273.06ms
step:1030/1480 train_time:278537ms step_avg:273.08ms
step:1031/1480 train_time:278828ms step_avg:273.09ms
step:1032/1480 train_time:279106ms step_avg:273.10ms
step:1033/1480 train_time:279393ms step_avg:273.11ms
step:1034/1480 train_time:279679ms step_avg:273.12ms
step:1035/1480 train_time:279961ms step_avg:273.13ms
step:1036/1480 train_time:280245ms step_avg:273.14ms
step:1037/1480 train_time:280527ms step_avg:273.15ms
step:1038/1480 train_time:280810ms step_avg:273.16ms
step:1039/1480 train_time:281097ms step_avg:273.17ms
step:1040/1480 train_time:281384ms step_avg:273.19ms
step:1041/1480 train_time:281670ms step_avg:273.20ms
step:1042/1480 train_time:281952ms step_avg:273.21ms
step:1043/1480 train_time:282243ms step_avg:273.23ms
step:1044/1480 train_time:282526ms step_avg:273.24ms
step:1045/1480 train_time:282807ms step_avg:273.24ms
step:1046/1480 train_time:283098ms step_avg:273.26ms
step:1047/1480 train_time:283383ms step_avg:273.27ms
step:1048/1480 train_time:283666ms step_avg:273.28ms
step:1049/1480 train_time:283952ms step_avg:273.29ms
step:1050/1480 train_time:284241ms step_avg:273.31ms
step:1051/1480 train_time:284528ms step_avg:273.32ms
step:1052/1480 train_time:284815ms step_avg:273.33ms
step:1053/1480 train_time:285101ms step_avg:273.35ms
step:1054/1480 train_time:285386ms step_avg:273.36ms
step:1055/1480 train_time:285671ms step_avg:273.37ms
step:1056/1480 train_time:285966ms step_avg:273.39ms
step:1057/1480 train_time:286251ms step_avg:273.40ms
step:1058/1480 train_time:286539ms step_avg:273.42ms
step:1059/1480 train_time:286826ms step_avg:273.43ms
step:1060/1480 train_time:287106ms step_avg:273.43ms
step:1061/1480 train_time:287390ms step_avg:273.44ms
step:1062/1480 train_time:287679ms step_avg:273.46ms
step:1063/1480 train_time:287962ms step_avg:273.47ms
step:1064/1480 train_time:288245ms step_avg:273.48ms
step:1065/1480 train_time:288526ms step_avg:273.48ms
step:1066/1480 train_time:288813ms step_avg:273.50ms
step:1067/1480 train_time:289101ms step_avg:273.51ms
step:1068/1480 train_time:289385ms step_avg:273.52ms
step:1069/1480 train_time:289668ms step_avg:273.53ms
step:1070/1480 train_time:289951ms step_avg:273.54ms
step:1071/1480 train_time:290240ms step_avg:273.55ms
step:1072/1480 train_time:290522ms step_avg:273.56ms
step:1073/1480 train_time:290806ms step_avg:273.57ms
step:1074/1480 train_time:291096ms step_avg:273.59ms
step:1075/1480 train_time:291387ms step_avg:273.60ms
step:1076/1480 train_time:291674ms step_avg:273.62ms
step:1077/1480 train_time:291967ms step_avg:273.63ms
step:1078/1480 train_time:292250ms step_avg:273.64ms
step:1079/1480 train_time:292536ms step_avg:273.65ms
step:1080/1480 train_time:292829ms step_avg:273.67ms
step:1081/1480 train_time:293110ms step_avg:273.68ms
step:1082/1480 train_time:293394ms step_avg:273.69ms
step:1083/1480 train_time:293685ms step_avg:273.70ms
step:1084/1480 train_time:293967ms step_avg:273.71ms
step:1085/1480 train_time:294250ms step_avg:273.72ms
step:1086/1480 train_time:294532ms step_avg:273.73ms
step:1087/1480 train_time:294828ms step_avg:273.75ms
step:1088/1480 train_time:295106ms step_avg:273.75ms
step:1089/1480 train_time:295392ms step_avg:273.76ms
step:1090/1480 train_time:295681ms step_avg:273.78ms
step:1091/1480 train_time:295969ms step_avg:273.79ms
step:1092/1480 train_time:296250ms step_avg:273.80ms
step:1093/1480 train_time:296533ms step_avg:273.81ms
step:1094/1480 train_time:296821ms step_avg:273.82ms
step:1095/1480 train_time:297108ms step_avg:273.83ms
step:1096/1480 train_time:297406ms step_avg:273.85ms
step:1097/1480 train_time:297688ms step_avg:273.86ms
step:1098/1480 train_time:297988ms step_avg:273.89ms
step:1099/1480 train_time:298278ms step_avg:273.90ms
step:1100/1480 train_time:298564ms step_avg:273.91ms
step:1101/1480 train_time:298848ms step_avg:273.92ms
step:1102/1480 train_time:299147ms step_avg:273.94ms
step:1103/1480 train_time:299440ms step_avg:273.96ms
step:1104/1480 train_time:299731ms step_avg:273.98ms
step:1105/1480 train_time:300018ms step_avg:273.99ms
step:1106/1480 train_time:300304ms step_avg:274.00ms
step:1107/1480 train_time:300590ms step_avg:274.01ms
step:1108/1480 train_time:300871ms step_avg:274.02ms
step:1109/1480 train_time:301163ms step_avg:274.03ms
step:1110/1480 train_time:301449ms step_avg:274.04ms
step:1111/1480 train_time:301744ms step_avg:274.06ms
step:1112/1480 train_time:302030ms step_avg:274.07ms
step:1113/1480 train_time:302316ms step_avg:274.09ms
step:1114/1480 train_time:302606ms step_avg:274.10ms
step:1115/1480 train_time:302897ms step_avg:274.11ms
step:1116/1480 train_time:303187ms step_avg:274.13ms
step:1117/1480 train_time:303481ms step_avg:274.15ms
step:1118/1480 train_time:303766ms step_avg:274.16ms
step:1119/1480 train_time:304063ms step_avg:274.18ms
step:1120/1480 train_time:304348ms step_avg:274.19ms
step:1121/1480 train_time:304632ms step_avg:274.20ms
step:1122/1480 train_time:304921ms step_avg:274.21ms
step:1123/1480 train_time:305205ms step_avg:274.22ms
step:1124/1480 train_time:305497ms step_avg:274.23ms
step:1125/1480 train_time:305784ms step_avg:274.25ms
step:1125/1480 val_loss:3.4139 train_time:305928ms step_avg:274.37ms
step:1126/1480 train_time:306074ms step_avg:274.26ms
step:1127/1480 train_time:306369ms step_avg:274.28ms
step:1128/1480 train_time:306651ms step_avg:274.28ms
step:1129/1480 train_time:306940ms step_avg:274.30ms
step:1130/1480 train_time:307230ms step_avg:274.31ms
step:1131/1480 train_time:307518ms step_avg:274.32ms
step:1132/1480 train_time:307808ms step_avg:274.34ms
step:1133/1480 train_time:308091ms step_avg:274.35ms
step:1134/1480 train_time:308375ms step_avg:274.35ms
step:1135/1480 train_time:308666ms step_avg:274.37ms
step:1136/1480 train_time:308950ms step_avg:274.38ms
step:1137/1480 train_time:309232ms step_avg:274.39ms
step:1138/1480 train_time:309530ms step_avg:274.41ms
step:1139/1480 train_time:309818ms step_avg:274.42ms
step:1140/1480 train_time:310104ms step_avg:274.43ms
step:1141/1480 train_time:310391ms step_avg:274.44ms
step:1142/1480 train_time:310675ms step_avg:274.45ms
step:1143/1480 train_time:310961ms step_avg:274.46ms
step:1144/1480 train_time:311247ms step_avg:274.47ms
step:1145/1480 train_time:311530ms step_avg:274.48ms
step:1146/1480 train_time:311822ms step_avg:274.49ms
step:1147/1480 train_time:312108ms step_avg:274.50ms
step:1148/1480 train_time:312395ms step_avg:274.51ms
step:1149/1480 train_time:312681ms step_avg:274.52ms
step:1150/1480 train_time:312964ms step_avg:274.53ms
step:1151/1480 train_time:313255ms step_avg:274.54ms
step:1152/1480 train_time:313537ms step_avg:274.55ms
step:1153/1480 train_time:313824ms step_avg:274.56ms
step:1154/1480 train_time:314110ms step_avg:274.57ms
step:1155/1480 train_time:314406ms step_avg:274.59ms
step:1156/1480 train_time:314696ms step_avg:274.60ms
step:1157/1480 train_time:314987ms step_avg:274.62ms
step:1158/1480 train_time:315273ms step_avg:274.63ms
step:1159/1480 train_time:315559ms step_avg:274.64ms
step:1160/1480 train_time:315845ms step_avg:274.65ms
step:1161/1480 train_time:316130ms step_avg:274.66ms
step:1162/1480 train_time:316422ms step_avg:274.67ms
step:1163/1480 train_time:316712ms step_avg:274.69ms
step:1164/1480 train_time:316993ms step_avg:274.69ms
step:1165/1480 train_time:317277ms step_avg:274.70ms
step:1166/1480 train_time:317567ms step_avg:274.71ms
step:1167/1480 train_time:317852ms step_avg:274.72ms
step:1168/1480 train_time:318140ms step_avg:274.73ms
step:1169/1480 train_time:318430ms step_avg:274.75ms
step:1170/1480 train_time:318711ms step_avg:274.75ms
step:1171/1480 train_time:318997ms step_avg:274.76ms
step:1172/1480 train_time:319282ms step_avg:274.77ms
step:1173/1480 train_time:319569ms step_avg:274.78ms
step:1174/1480 train_time:319860ms step_avg:274.79ms
step:1175/1480 train_time:320148ms step_avg:274.80ms
step:1176/1480 train_time:320427ms step_avg:274.81ms
step:1177/1480 train_time:320716ms step_avg:274.82ms
step:1178/1480 train_time:321003ms step_avg:274.83ms
step:1179/1480 train_time:321291ms step_avg:274.84ms
step:1180/1480 train_time:321585ms step_avg:274.86ms
step:1181/1480 train_time:321869ms step_avg:274.87ms
step:1182/1480 train_time:322151ms step_avg:274.87ms
step:1183/1480 train_time:322437ms step_avg:274.88ms
step:1184/1480 train_time:322725ms step_avg:274.89ms
step:1185/1480 train_time:323009ms step_avg:274.90ms
step:1186/1480 train_time:323290ms step_avg:274.91ms
step:1187/1480 train_time:323583ms step_avg:274.92ms
step:1188/1480 train_time:323866ms step_avg:274.93ms
step:1189/1480 train_time:324165ms step_avg:274.95ms
step:1190/1480 train_time:324454ms step_avg:274.96ms
step:1191/1480 train_time:324746ms step_avg:274.98ms
step:1192/1480 train_time:325030ms step_avg:274.98ms
step:1193/1480 train_time:325319ms step_avg:274.99ms
step:1194/1480 train_time:325603ms step_avg:275.00ms
step:1195/1480 train_time:325888ms step_avg:275.01ms
step:1196/1480 train_time:326173ms step_avg:275.02ms
step:1197/1480 train_time:326464ms step_avg:275.03ms
step:1198/1480 train_time:326750ms step_avg:275.04ms
step:1199/1480 train_time:327037ms step_avg:275.05ms
step:1200/1480 train_time:327323ms step_avg:275.06ms
step:1201/1480 train_time:327608ms step_avg:275.07ms
step:1202/1480 train_time:327899ms step_avg:275.08ms
step:1203/1480 train_time:328189ms step_avg:275.10ms
step:1204/1480 train_time:328473ms step_avg:275.10ms
step:1205/1480 train_time:328757ms step_avg:275.11ms
step:1206/1480 train_time:329049ms step_avg:275.12ms
step:1207/1480 train_time:329333ms step_avg:275.13ms
step:1208/1480 train_time:329622ms step_avg:275.14ms
step:1209/1480 train_time:329912ms step_avg:275.16ms
step:1210/1480 train_time:330196ms step_avg:275.16ms
step:1211/1480 train_time:330484ms step_avg:275.17ms
step:1212/1480 train_time:330777ms step_avg:275.19ms
step:1213/1480 train_time:331064ms step_avg:275.20ms
step:1214/1480 train_time:331353ms step_avg:275.21ms
step:1215/1480 train_time:331641ms step_avg:275.22ms
step:1216/1480 train_time:331926ms step_avg:275.23ms
step:1217/1480 train_time:332216ms step_avg:275.24ms
step:1218/1480 train_time:332509ms step_avg:275.26ms
step:1219/1480 train_time:332796ms step_avg:275.27ms
step:1220/1480 train_time:333091ms step_avg:275.28ms
step:1221/1480 train_time:333381ms step_avg:275.29ms
step:1222/1480 train_time:333670ms step_avg:275.31ms
step:1223/1480 train_time:333959ms step_avg:275.32ms
step:1224/1480 train_time:334250ms step_avg:275.33ms
step:1225/1480 train_time:334549ms step_avg:275.35ms
step:1226/1480 train_time:334838ms step_avg:275.36ms
step:1227/1480 train_time:335126ms step_avg:275.37ms
step:1228/1480 train_time:335408ms step_avg:275.38ms
step:1229/1480 train_time:335720ms step_avg:275.41ms
step:1230/1480 train_time:336011ms step_avg:275.42ms
step:1231/1480 train_time:336295ms step_avg:275.43ms
step:1232/1480 train_time:336587ms step_avg:275.44ms
step:1233/1480 train_time:336877ms step_avg:275.45ms
step:1234/1480 train_time:337167ms step_avg:275.46ms
step:1235/1480 train_time:337450ms step_avg:275.47ms
step:1236/1480 train_time:337746ms step_avg:275.49ms
step:1237/1480 train_time:338031ms step_avg:275.49ms
step:1238/1480 train_time:338318ms step_avg:275.50ms
step:1239/1480 train_time:338608ms step_avg:275.52ms
step:1240/1480 train_time:338901ms step_avg:275.53ms
step:1241/1480 train_time:339190ms step_avg:275.54ms
step:1242/1480 train_time:339476ms step_avg:275.55ms
step:1243/1480 train_time:339760ms step_avg:275.56ms
step:1244/1480 train_time:340048ms step_avg:275.57ms
step:1245/1480 train_time:340345ms step_avg:275.58ms
step:1246/1480 train_time:340631ms step_avg:275.59ms
step:1247/1480 train_time:340918ms step_avg:275.60ms
step:1248/1480 train_time:341207ms step_avg:275.61ms
step:1249/1480 train_time:341492ms step_avg:275.62ms
step:1250/1480 train_time:341786ms step_avg:275.63ms
step:1250/1480 val_loss:3.3625 train_time:341927ms step_avg:275.75ms
step:1251/1480 train_time:342078ms step_avg:275.65ms
step:1252/1480 train_time:342368ms step_avg:275.66ms
step:1253/1480 train_time:342661ms step_avg:275.67ms
step:1254/1480 train_time:342949ms step_avg:275.68ms
step:1255/1480 train_time:343237ms step_avg:275.69ms
step:1256/1480 train_time:343527ms step_avg:275.70ms
step:1257/1480 train_time:343817ms step_avg:275.72ms
step:1258/1480 train_time:344100ms step_avg:275.72ms
step:1259/1480 train_time:344388ms step_avg:275.73ms
step:1260/1480 train_time:344681ms step_avg:275.74ms
step:1261/1480 train_time:344975ms step_avg:275.76ms
step:1262/1480 train_time:345261ms step_avg:275.77ms
step:1263/1480 train_time:345557ms step_avg:275.78ms
step:1264/1480 train_time:345842ms step_avg:275.79ms
step:1265/1480 train_time:346126ms step_avg:275.80ms
step:1266/1480 train_time:346416ms step_avg:275.81ms
step:1267/1480 train_time:346701ms step_avg:275.82ms
step:1268/1480 train_time:346989ms step_avg:275.83ms
step:1269/1480 train_time:347286ms step_avg:275.84ms
step:1270/1480 train_time:347580ms step_avg:275.86ms
step:1271/1480 train_time:347868ms step_avg:275.87ms
step:1272/1480 train_time:348161ms step_avg:275.88ms
step:1273/1480 train_time:348448ms step_avg:275.89ms
step:1274/1480 train_time:348738ms step_avg:275.90ms
step:1275/1480 train_time:349024ms step_avg:275.91ms
step:1276/1480 train_time:349308ms step_avg:275.91ms
step:1277/1480 train_time:349598ms step_avg:275.93ms
step:1278/1480 train_time:349883ms step_avg:275.93ms
step:1279/1480 train_time:350177ms step_avg:275.95ms
step:1280/1480 train_time:350462ms step_avg:275.95ms
step:1281/1480 train_time:350759ms step_avg:275.97ms
step:1282/1480 train_time:351044ms step_avg:275.98ms
step:1283/1480 train_time:351337ms step_avg:275.99ms
step:1284/1480 train_time:351627ms step_avg:276.00ms
step:1285/1480 train_time:351909ms step_avg:276.01ms
step:1286/1480 train_time:352199ms step_avg:276.02ms
step:1287/1480 train_time:352489ms step_avg:276.03ms
step:1288/1480 train_time:352778ms step_avg:276.04ms
step:1289/1480 train_time:353075ms step_avg:276.06ms
step:1290/1480 train_time:353359ms step_avg:276.06ms
step:1291/1480 train_time:353642ms step_avg:276.07ms
step:1292/1480 train_time:353936ms step_avg:276.08ms
step:1293/1480 train_time:354222ms step_avg:276.09ms
step:1294/1480 train_time:354505ms step_avg:276.09ms
step:1295/1480 train_time:354793ms step_avg:276.10ms
step:1296/1480 train_time:355080ms step_avg:276.11ms
step:1297/1480 train_time:355362ms step_avg:276.12ms
step:1298/1480 train_time:355644ms step_avg:276.12ms
step:1299/1480 train_time:355930ms step_avg:276.13ms
step:1300/1480 train_time:356222ms step_avg:276.14ms
step:1301/1480 train_time:356506ms step_avg:276.15ms
step:1302/1480 train_time:356792ms step_avg:276.16ms
step:1303/1480 train_time:357083ms step_avg:276.17ms
step:1304/1480 train_time:357366ms step_avg:276.17ms
step:1305/1480 train_time:357656ms step_avg:276.18ms
step:1306/1480 train_time:357949ms step_avg:276.20ms
step:1307/1480 train_time:358237ms step_avg:276.20ms
step:1308/1480 train_time:358521ms step_avg:276.21ms
step:1309/1480 train_time:358821ms step_avg:276.23ms
step:1310/1480 train_time:359112ms step_avg:276.24ms
step:1311/1480 train_time:359400ms step_avg:276.25ms
step:1312/1480 train_time:359686ms step_avg:276.26ms
step:1313/1480 train_time:359978ms step_avg:276.27ms
step:1314/1480 train_time:360263ms step_avg:276.28ms
step:1315/1480 train_time:360554ms step_avg:276.29ms
step:1316/1480 train_time:360854ms step_avg:276.30ms
step:1317/1480 train_time:361146ms step_avg:276.32ms
step:1318/1480 train_time:361438ms step_avg:276.33ms
step:1319/1480 train_time:361728ms step_avg:276.34ms
step:1320/1480 train_time:362019ms step_avg:276.35ms
step:1321/1480 train_time:362309ms step_avg:276.36ms
step:1322/1480 train_time:362599ms step_avg:276.37ms
step:1323/1480 train_time:362882ms step_avg:276.38ms
step:1324/1480 train_time:363180ms step_avg:276.39ms
step:1325/1480 train_time:363482ms step_avg:276.41ms
step:1326/1480 train_time:363771ms step_avg:276.42ms
step:1327/1480 train_time:364064ms step_avg:276.43ms
step:1328/1480 train_time:364352ms step_avg:276.44ms
step:1329/1480 train_time:364638ms step_avg:276.45ms
step:1330/1480 train_time:364928ms step_avg:276.46ms
step:1331/1480 train_time:365219ms step_avg:276.47ms
step:1332/1480 train_time:365511ms step_avg:276.48ms
step:1333/1480 train_time:365803ms step_avg:276.49ms
step:1334/1480 train_time:366098ms step_avg:276.51ms
step:1335/1480 train_time:366384ms step_avg:276.52ms
step:1336/1480 train_time:366680ms step_avg:276.53ms
step:1337/1480 train_time:366966ms step_avg:276.54ms
step:1338/1480 train_time:367253ms step_avg:276.55ms
step:1339/1480 train_time:367544ms step_avg:276.56ms
step:1340/1480 train_time:367834ms step_avg:276.57ms
step:1341/1480 train_time:368118ms step_avg:276.57ms
step:1342/1480 train_time:368415ms step_avg:276.59ms
step:1343/1480 train_time:368721ms step_avg:276.61ms
step:1344/1480 train_time:369010ms step_avg:276.62ms
step:1345/1480 train_time:369299ms step_avg:276.63ms
step:1346/1480 train_time:369593ms step_avg:276.64ms
step:1347/1480 train_time:369886ms step_avg:276.65ms
step:1348/1480 train_time:370176ms step_avg:276.66ms
step:1349/1480 train_time:370464ms step_avg:276.67ms
step:1350/1480 train_time:370758ms step_avg:276.68ms
step:1351/1480 train_time:371043ms step_avg:276.69ms
step:1352/1480 train_time:371337ms step_avg:276.70ms
step:1353/1480 train_time:371620ms step_avg:276.71ms
step:1354/1480 train_time:371905ms step_avg:276.72ms
step:1355/1480 train_time:372199ms step_avg:276.73ms
step:1356/1480 train_time:372492ms step_avg:276.74ms
step:1357/1480 train_time:372781ms step_avg:276.75ms
step:1358/1480 train_time:373072ms step_avg:276.76ms
step:1359/1480 train_time:373363ms step_avg:276.77ms
step:1360/1480 train_time:373650ms step_avg:276.78ms
step:1361/1480 train_time:373940ms step_avg:276.79ms
step:1362/1480 train_time:374221ms step_avg:276.79ms
step:1363/1480 train_time:374517ms step_avg:276.80ms
step:1364/1480 train_time:374804ms step_avg:276.81ms
step:1365/1480 train_time:375095ms step_avg:276.82ms
step:1366/1480 train_time:375381ms step_avg:276.83ms
step:1367/1480 train_time:375674ms step_avg:276.84ms
step:1368/1480 train_time:375960ms step_avg:276.85ms
step:1369/1480 train_time:376247ms step_avg:276.86ms
step:1370/1480 train_time:376543ms step_avg:276.87ms
step:1371/1480 train_time:376850ms step_avg:276.89ms
step:1372/1480 train_time:377144ms step_avg:276.90ms
step:1373/1480 train_time:377428ms step_avg:276.91ms
step:1374/1480 train_time:377720ms step_avg:276.92ms
step:1375/1480 train_time:378018ms step_avg:276.94ms
step:1375/1480 val_loss:3.3208 train_time:378162ms step_avg:277.04ms
step:1376/1480 train_time:378310ms step_avg:276.95ms
step:1377/1480 train_time:378617ms step_avg:276.97ms
step:1378/1480 train_time:378918ms step_avg:276.99ms
step:1379/1480 train_time:379212ms step_avg:277.00ms
step:1380/1480 train_time:379501ms step_avg:277.01ms
step:1381/1480 train_time:379795ms step_avg:277.02ms
step:1382/1480 train_time:380084ms step_avg:277.03ms
step:1383/1480 train_time:380387ms step_avg:277.05ms
step:1384/1480 train_time:380682ms step_avg:277.06ms
step:1385/1480 train_time:380972ms step_avg:277.07ms
step:1386/1480 train_time:381264ms step_avg:277.08ms
step:1387/1480 train_time:381558ms step_avg:277.09ms
step:1388/1480 train_time:381844ms step_avg:277.10ms
step:1389/1480 train_time:382134ms step_avg:277.11ms
step:1390/1480 train_time:382422ms step_avg:277.12ms
step:1391/1480 train_time:382714ms step_avg:277.13ms
step:1392/1480 train_time:383001ms step_avg:277.14ms
step:1393/1480 train_time:383288ms step_avg:277.14ms
step:1394/1480 train_time:383575ms step_avg:277.15ms
step:1395/1480 train_time:383868ms step_avg:277.16ms
step:1396/1480 train_time:384162ms step_avg:277.17ms
step:1397/1480 train_time:384459ms step_avg:277.19ms
step:1398/1480 train_time:384744ms step_avg:277.19ms
step:1399/1480 train_time:385040ms step_avg:277.21ms
step:1400/1480 train_time:385344ms step_avg:277.23ms
step:1401/1480 train_time:385639ms step_avg:277.24ms
step:1402/1480 train_time:385946ms step_avg:277.26ms
step:1403/1480 train_time:386235ms step_avg:277.27ms
step:1404/1480 train_time:386522ms step_avg:277.28ms
step:1405/1480 train_time:386813ms step_avg:277.29ms
step:1406/1480 train_time:387099ms step_avg:277.29ms
step:1407/1480 train_time:387404ms step_avg:277.31ms
step:1408/1480 train_time:387698ms step_avg:277.32ms
step:1409/1480 train_time:387982ms step_avg:277.33ms
step:1410/1480 train_time:388268ms step_avg:277.33ms
step:1411/1480 train_time:388561ms step_avg:277.35ms
step:1412/1480 train_time:388849ms step_avg:277.35ms
step:1413/1480 train_time:389140ms step_avg:277.36ms
step:1414/1480 train_time:389427ms step_avg:277.37ms
step:1415/1480 train_time:389717ms step_avg:277.38ms
step:1416/1480 train_time:390013ms step_avg:277.39ms
step:1417/1480 train_time:390300ms step_avg:277.40ms
step:1418/1480 train_time:390596ms step_avg:277.41ms
step:1419/1480 train_time:390888ms step_avg:277.42ms
step:1420/1480 train_time:391181ms step_avg:277.43ms
step:1421/1480 train_time:391467ms step_avg:277.44ms
step:1422/1480 train_time:391763ms step_avg:277.45ms
step:1423/1480 train_time:392046ms step_avg:277.46ms
step:1424/1480 train_time:392337ms step_avg:277.47ms
step:1425/1480 train_time:392629ms step_avg:277.48ms
step:1426/1480 train_time:392934ms step_avg:277.50ms
step:1427/1480 train_time:393224ms step_avg:277.50ms
step:1428/1480 train_time:393519ms step_avg:277.52ms
step:1429/1480 train_time:393807ms step_avg:277.52ms
step:1430/1480 train_time:394096ms step_avg:277.53ms
step:1431/1480 train_time:394384ms step_avg:277.54ms
step:1432/1480 train_time:394681ms step_avg:277.55ms
step:1433/1480 train_time:394974ms step_avg:277.56ms
step:1434/1480 train_time:395276ms step_avg:277.58ms
step:1435/1480 train_time:395564ms step_avg:277.59ms
step:1436/1480 train_time:395863ms step_avg:277.60ms
step:1437/1480 train_time:396149ms step_avg:277.61ms
step:1438/1480 train_time:396452ms step_avg:277.63ms
step:1439/1480 train_time:396751ms step_avg:277.64ms
step:1440/1480 train_time:397042ms step_avg:277.65ms
step:1441/1480 train_time:397333ms step_avg:277.66ms
step:1442/1480 train_time:397626ms step_avg:277.67ms
step:1443/1480 train_time:397920ms step_avg:277.68ms
step:1444/1480 train_time:398210ms step_avg:277.69ms
step:1445/1480 train_time:398498ms step_avg:277.70ms
step:1446/1480 train_time:398800ms step_avg:277.72ms
step:1447/1480 train_time:399097ms step_avg:277.73ms
step:1448/1480 train_time:399400ms step_avg:277.75ms
step:1449/1480 train_time:399696ms step_avg:277.76ms
step:1450/1480 train_time:399981ms step_avg:277.76ms
step:1451/1480 train_time:400275ms step_avg:277.78ms
step:1452/1480 train_time:400566ms step_avg:277.79ms
step:1453/1480 train_time:400854ms step_avg:277.79ms
step:1454/1480 train_time:401142ms step_avg:277.80ms
step:1455/1480 train_time:401438ms step_avg:277.81ms
step:1456/1480 train_time:401725ms step_avg:277.82ms
step:1457/1480 train_time:402017ms step_avg:277.83ms
step:1458/1480 train_time:402303ms step_avg:277.83ms
step:1459/1480 train_time:402602ms step_avg:277.85ms
step:1460/1480 train_time:402886ms step_avg:277.85ms
step:1461/1480 train_time:403177ms step_avg:277.86ms
step:1462/1480 train_time:403463ms step_avg:277.87ms
step:1463/1480 train_time:403752ms step_avg:277.87ms
step:1464/1480 train_time:404050ms step_avg:277.89ms
step:1465/1480 train_time:404342ms step_avg:277.90ms
step:1466/1480 train_time:404633ms step_avg:277.91ms
step:1467/1480 train_time:404922ms step_avg:277.91ms
step:1468/1480 train_time:405217ms step_avg:277.93ms
step:1469/1480 train_time:405524ms step_avg:277.95ms
step:1470/1480 train_time:405822ms step_avg:277.96ms
step:1471/1480 train_time:406119ms step_avg:277.97ms
step:1472/1480 train_time:406404ms step_avg:277.98ms
step:1473/1480 train_time:406701ms step_avg:277.99ms
step:1474/1480 train_time:406986ms step_avg:278.00ms
step:1475/1480 train_time:407278ms step_avg:278.01ms
step:1476/1480 train_time:407567ms step_avg:278.01ms
step:1477/1480 train_time:407864ms step_avg:278.03ms
step:1478/1480 train_time:408159ms step_avg:278.04ms
step:1479/1480 train_time:408453ms step_avg:278.05ms
step:1480/1480 train_time:408740ms step_avg:278.05ms
step:1480/1480 val_loss:3.3015 train_time:408880ms step_avg:278.15ms
peak memory consumption: 34262 MiB

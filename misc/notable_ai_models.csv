Model,Publication date,Parameters,Parameters notes,Training compute (FLOP),Training compute notes,Epochs,Training time (hours),Training time notes,Training hardware,Hardware quantity,Training compute cost (2023 USD)
Claude Sonnet 4,5/22/2025,,,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,,,,,,
Claude Opus 4,5/22/2025,,,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,,,,,,
Veo 3,5/21/2025,,,,,,,,,,
gpt-image-1,4/23/2025,,,,,,,,,,
Pangu Ultra,4/10/2025,1.35E+11,,1.07E+25,"When compared to Llama 3.1 405B, Pangu Ultra achieves better scores on most of the challenging benchmarks, while utilizing only about 29% of the training FLOPs required by Llama 405B.

Compute = 6 FLOP/token/param *  135e9 params *13.2e12 tokens = 1.069200e+25 FLOP
This is consistent with 29% of Llama 405B's compute: 3.8e25*0.29=1.1e25.",,,,Huawei Ascend 910B,8192,
Llama 4 Behemoth (preview),4/5/2025,2E+12,"""Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs.""",5.18E+25,"Behemoth's training dataset is at least 30T tokens:
https://ai.meta.com/blog/llama-4-multimodal-intelligence/ 

6 FLOP / parameter / token * 288 * 10^9 activated parameters * 30 * 10^12 tokens = 5.184e+25 FLOP",,,"Based on the model cards for Llama 4 Scout and Maverick, they seem to be using H100-80GB GPUs, despite the article saying that 390 TFLOPS/GPU was a high MFU (it is high throughput, but <20% MFU in FP8).",,32000,
EXAONE Deep 32B,3/16/2025,32000000000,32B,1.26E+24,"1.25 × 10^24 (base model reported training compute) + 7.04 × 10^21 (finetune compute) = 1.26 × 10^24 FLOP

Table 1",,2160,512 H100 GPUs were used for three months,NVIDIA H100 SXM5 80GB,512,
ERNIE-4.5-VL-424B-A47B (文心大模型4.5),3/16/2025,4.24E+11,"MoE:
total parameters - 424B
active parameters - 47B

""ViT encoder comprising 630 million parameters""",,"Unlikely over 1e25 FLOP, as the ERNIE-4.5 LLM pretraining used 3e24 FLOP.",,,,,,
Hunyuan-TurboS,3/11/2025,,,,,,,,,,
QwQ-32B,3/6/2025,32500000000,"Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias
Number of Parameters: 32.5B
Number of Paramaters (Non-Embedding): 31.0B
Number of Layers: 64
Number of Attention Heads (GQA): 40 for Q and 8 for KV",3.51E+24,"Assuming the same dataset size as for Qwen2.5 training (18T tokens):

6ND = 6 * 32500000000 parameters * 18 * 10^12 tokens =  3.51 × 10^24

'Speculative' confidence",,,,,,
Mistral OCR,3/6/2025,,,,,,,,,,
Mercury,2/27/2025,,,,,,,,,,
GPT-4.5,2/27/2025,,,2.10E+26,"Speculative estimate based on GPT-N typically incrementing  by about 100x each version number (https://www.youtube.com/watch?v=6nJZopACRuQ) and 10x each 0.5 version number (https://x.com/karpathy/status/1895213020982472863).

OpenAI said GPT-4.5 was a “new order of magnitude in compute”, which they could have meant somewhat loosely. But seems quite likely >1e26 based on that statement, plus 4.5's high inference costs, and <5e26 because OpenAI probably didn’t have a substantially >100k H100 cluster in mid-2024.",,,,,,
Wan 2.1 14B,2/25/2025,14000000000,14B,2.50E+23,"""Through extensive experimentation, the model is validated at scale, reaching 14 billion parameters. Subsequently, Wan has seen large-scale data comprising billions of
images and videos, amounting to O(1) trillions of tokens in total.""

So likely between 1T and 10T tokens. Assume 3T.

Transformer architecture, so 6ND should be a decent approximation.

6ND = 6 * 14e9 * 3e12 ~= 2.5e+23 FLOP",,,,,,
Claude 3.7 Sonnet,2/24/2025,,,3.35E+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,,,,,,
Grok-3,2/17/2025,,,4.64E+26,"Estimate based on training time for a cluster of 100,000 H100s, and xAI's statement that Grok 2 was trained on more compute than GPT-4 (2.1e25) and that Grok 3 was trained on around 15 times more compute than Grok 2. 

Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing",,2400,Estimated to be between 3 and 4 months. We use 100 days in our estimate,NVIDIA H100 SXM5 80GB,100000,470000000
Eurus-2-7B-PRIME,2/3/2025,7000000000,,,,3,,,,,
o3-mini,1/31/2025,,"Can't get an exact estimate, but we suspect total parameter count around 60B-120B, active parameters around 10B-30B. 

Given these models are served at 150-200 tok/s, at $4.40/Mtok output, inference economics (https://epoch.ai/blog/inference-economics-of-language-models) suggests total parameter count around 60-120B parameters, with mixture-of-experts active parameters around 10-30B. MoEs make a given model roughly comparable to a ~50% smaller dense model (https://epoch.ai/gradient-updates/moe-vs-dense-models-inference), which lines up decently with Magistral Small pricing (24B dense, served at a similar speed for the cheaper $1.50/Mtok). ",,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",,,,,,
Computer-Using Agent (CUA),1/23/2025,,,,,,,,,,
Kimi k1.5,1/22/2025,,,,,,,,,,
Doubao-1.5-pro,1/22/2025,,"Not directly reported. We are told it is a MoE model, and that it matches the performance of a dense model trained on the same data, while using 1/7th of the activated parameters. Additionally they say ""The number of parameters of the Doubao dense model is also much smaller than that of Llama3.1-405B"", which suggests that the number of activated parameters on the forward pass is ""much less"" than 405B/7 = 58B parameters.",,"The model appears to have been trained on 9T tokens; since we believe the MoE model uses ""much less"" than 58B parameters (see parameter notes), training compute is likely to be less than 6 * 9T * 58B = 3.132e24

It is possible the 9T token training run was for comparison sake against the dense model (they label it as ""doubao-MoE"", not doubao-1.5-pro), and that they continued training beyond this. They would need to train for at least 29T tokens to ",1,,,,,
Eagle 2,1/20/2025,8930000000,"Table 4

https://huggingface.co/nvidia/Eagle2-9B
""8.93B params""
",4.72E+22,"Appendix A. Compute: ""We show our training resource for Eagle2-9B in Tab. A. In actual development, we rarely iterate the Stage-1 model. Usually, we iterate Stage-1.5 once after iterating Stage-2 >10 times.""
Assume "">10 times"" -> 12
Assume ""H100"" -> NVIDIA H100 SXM5 80GB
Assume bf16
Assume 0.4 utilization (NVIDIA in-house)
H100 SXM5 performance = 989400000000000 FLOP/s = 9.894e14 FLOP/s

Stage 1:     (H100 * 128) * (2.5 hr * 1)
Stage 1.5:  (H100 * 256) * (28 hr * 2)
Stage 2:     (H100 * 256) * (6 hr * 12)

Stage 1:      0.4 * (9.894e14 FLOP/s * 128) * (3600 s / 1 hr) * (2.5 hr *1) ~= 4.56e20 FLOP
Stage 1.5:   0.4 * (9.894e14 FLOP/s * 256) * (3600 s / 1 hr) * (28 hr * 2) ~= 2.04e22 FLOP
Stage 2:      0.4 * (9.894e14 FLOP/s * 256) * (3600 s / 1 hr) * (6 hr * 12) ~= 2.63e22 FLOP

(4.56e20 + 2.04e22 + 2.63e22) FLOP ~= 4.72e22 FLOP
",,130.5,"Appendix A. Compute: ""We show our training resource for Eagle2-9B in Tab. A. In actual development, we rarely iterate the Stage-1 model. Usually, we iterate Stage-1.5 once after iterating Stage-2 >10 times.
Assume "">10 times"" -> 12
Stage 1.0:  2.5 hr * 1
Stage 1.5   28 hr * 2
Stage 2.0:  6 hr * 12
(2.5 hr * 1) + (28 hr * 2) + (6 hr * 12) = 130.5 hr
 ",NVIDIA H100 SXM5 80GB,256,
DeepSeek-R1,1/20/2025,6.71E+11,"671B total
37B activated
https://github.com/deepseek-ai/DeepSeek-R1/tree/main",4.02E+24,"Estimates by Ege Erdil in Gradient Updates:
https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1
""A dataset size of 14.8 trillion tokens is reasonable and in line with other models of this scale. Assuming that’s valid, the pretraining of this model would have required 6 * (37 billion) * (14.8 trillion) = 3e24 FLOP. If we assume DeepSeek’s training cluster consists of H800s with the PCIe form factor, then each should be capable of 1.5e15 FP8 per second, and the implied model FLOP utilization (MFU) of DeepSeek v3’s 55 day training run ends up being around 23%.""

6 FLOP/token/param * 14.8T tokens * 37B active params = 3.29e24 FLOP (pretraining)
1.2e23 FLOP (post-training)
6.1e23 FLOP (fine-tuning)

Total compute: 3.29e24 + 1.2e23 + 6.1e23 = 4.02e24",,,,,,6770000
INTELLECT-MATH,1/17/2025,7000000000,,,,,,,,,
DeepSeek-V3,12/24/2024,6.71E+11,Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.,3.41E+24,"""At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.""

6 * 37B (active params) * 14.8T = 3.2856e24 for pretraining.

We know they trained in FP8. H800s get 1.513e15 FLOP/s in FP8:
2.688M * 3600 * 1.513e15 * MFU = 3.2856e24

Suggests a MFU of 0.2244 in pre-training. If we assume MFU was the same in post-training, that adds an additional:

0.1M * 3600 * 1.513e15 * 0.2244 = 1.222e23 FLOP from post-training

Total: 3.2856e24 + 1.222e23 = 3.4078e24 FLOP",,,"""DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training""",NVIDIA H800 SXM5,2048,5390000
o3,12/20/2024,,,,,,,,,,
Veo 2,12/16/2024,,,,,,,,,,
Gemini 2.0 Pro,12/11/2024,,,,Flagship model from a leading developer in early 2025; very likely it used >1e25 FLOP.,,,,,,
EXAONE 3.5 32B,12/9/2024,32000000000,32B,1.25E+24,1.25 × 10^24 (Table 2) ,,,,,,
Llama 3.3 70B,12/6/2024,70000000000,70B,6.86E+24,"6ND = 6 FLOP / parameter / token * 70*10^9 parameters * 15*10^12 tokens = 6.3e+24 FLOP

7000000 GPU-hours * 3600 sec / hour * 989500000000000 FLOP / second * 0.3 [assumed utilization]= 7.48062e+24 FLOP

sqrt(7.48062e+24*6.3e+24) = 6.8649768e+24",,,"""Training utilized a cumulative of 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.""

Llama 3.3 70B: Training Time (GPU hours): 7M
",NVIDIA H100 SXM5 80GB,,
o1,12/5/2024,,,,,,,,,,
Infinity,12/5/2024,2000000000,,,,,,,,,
Amazon Nova Pro,12/3/2024,,,6.00E+24,"""probably just below 1e25 stemming from the Llama 70B serving speed.  If Llama 70B is trained proportionally to 405B, then it's at ~ 6.6e24. Nova Pro is served at 100tk/s, while Llama 70B is served at 70tk/s on average, and 100tk/s by together.ai at FP8. So Nova Pro would be >1e25 if they roughly 2x the amount of training compared to Llama 70B which [seems unlikely]""",,,,"Amazon Trainium1,NVIDIA A100,NVIDIA H100 SXM5 80GB",,
Fugatto 1,11/25/2024,2500000000,,,,,,,NVIDIA A100,32,
Suno v4,11/19/2024,,,,,,,,,,
Pixtral Large,11/18/2024,1.24E+11,"123B multimodal decoder, 1B parameter vision encoder",,,,,,,,
k0-math,11/16/2024,,This tweet mentions that this LLM has 100B params but I have not found the information anywhere else. https://x.com/kimmonismus/status/1902710969534460109,,,,,,,,
Hunyuan-Large,11/6/2024,3.89E+11,"""a total of 389 billion parameters and 52 billion activation parameters""",3.49E+24,"52B activated parameters

6ND = 6*52*10^9*7*10^12 = 2.184 × 10^24

They also suggest more precise formula to calculate MoE compute budget:

9.59ND + 2.3 × 10^8D = 9.59*52*10^9*7*10^12 + 2.3 × 10^8 ×  7*10^12 = 3.49237×10^24

which seems closer to projected compute on Figure 3",,,,,,
Doubao-pro,10/28/2024,5E+11,"[Speculative] Doubao's large language model has scaled up from 35 billion parameters to 800 billion, with 500 billion and 800 billion parameter models currently under training.
https://xueqiu.com/9637001584/309910396?md5__1038=7qmx2DyDuie4cDBqDTQEWqDtMvO4iTphD
",2.51E+25,6ND = 6 * 500*10^9 * 8350*10^9 = 2.505e+25,,,,,,
NVLM-X 72B,10/22/2024,72000000000,72B,3.04E+24,3.02e24 FLOP (Qwen2-72B compute) + 19818086000000000000000 = 3.0398181e+24,1,,,NVIDIA H100 SXM5 80GB,128,
NVLM-H 72B,10/22/2024,72000000000,72B,3.02E+24,Additional compute in this paper is negligible relative to the compute used to train the language model backbone (Qwen2-72B at 3.02e24 FLOP),1,,,NVIDIA H100 SXM5 80GB,128,
NVLM-D 72B,10/22/2024,72000000000,72B,3.02E+24,"Uses Qwen2-72B as a backbone, which trained with 3.02e24 FLOP, as well as InternViT-6B. It's unclear how many FLOP were spent training but probably negligible; e.g. PaLI trained ViT-e with ~4B parameters using 1.07e23 FLOP.

Fine-tuning FLOPs:
57,016,320,000 image/text tokens over all stages
6 * 72B * 57,016,320,000 = 2.463e22
",1,,,NVIDIA H100 SXM5 80GB,128,
Yi-Lightning,10/18/2024,,,1.50E+24,"The CEO of 01.AI tweeted that Yi-Lightning was trained for 1 month on 2000 H100s: https://x.com/kaifulee/status/1846310645849047524
Assuming this is accurate:
(9.9e14 * 2000) FLOP/s * 1 month * 30.5 days/month * 24hr/day * 3600 s/hr * 0.3 utilization assumption = 1.565e24",,720,"https://x.com/kaifulee/status/1846310645849047524
""it was trained on 2000 H100s for 1 month""",NVIDIA H100 SXM5 80GB,2000,
CHAI-1,10/15/2024,,,7.76E+21,"From paper: 128 A100s for 30 days; assumptions: 30% utilization rate, FP16 precision",,720,Taken from paper: 128 A100s for 30 days,NVIDIA A100,128,
RDT-1B,10/10/2024,1200000000,"Model Training and Inference: ""We scale the size of RDT up to 1.2B parameters, establishing it as the currently largest diffusion-based robotic foundation model.""",3.69E+22,"Model Training and Inference: ""The model is pre-trained on 48 H100 80GB GPUs for a month, giving a total of 1M training iteration steps. It takes three days to fine-tune this model using the same GPUs for 130K steps.""
Table 10: ""Mixed Precision, bf16""
Assume 48xGPU, ""scheduling reasons""  -> NVIDIA H100 SXM5 -> 9.894e14 FLOP/s/GPU
Assume 0.3 utilization
Assume 1 month -> 30 days = 720 hr = 2.592e6 s
0.3 * 48 GPU * 2.592e6 s * 9.894e14 FLOP/s/GPU ~=  3.69e22 FLOP

",,720,"Model Training and Inference: ""The model is pre-trained on 48 H100 80GB GPUs for a month, giving a total of 1M training iteration steps. It takes three days to fine-tune this model using the same GPUs for 130K steps.""",NVIDIA H100 PCIe,48,
Palmyra X 004,10/9/2024,1.5E+11,Source: https://venturebeat.com/ai/writers-palmyra-x-004-takes-the-lead-in-ai-function-calling-surpassing-tech-giants/,,,,,,,,
GR-2,10/8/2024,230000000,"the default GR-2 model contains 230M parameters, of which 95M are trainable",,,,,,,,
Movie Gen Video,10/4/2024,30000000000,30B,1.65E+24,"Model size = 30B
Broken down by training stage (table 3):
256px T2I: samples seen = 1.94E9; sample token length = 256; flops = 6ND = 8.94E22
256px T2I/V: samples seen = 3.95E8; sample token length = 8192; flops = 6ND = 5.82E23
768px T2I/V: samples seen = 7.38E7; sample token length = 73,728; flops = 6ND = 9.79E23
Total flops = 1.65E24",,331,"54 hours for 256px T2I
128 hours for 256px T2I/V
149 hours for 768px T2I/V",NVIDIA H100 SXM5 80GB,6144,
Llama 3.2 11B,9/24/2024,10600000000,https://huggingface.co/meta-llama/Llama-3.2-11B-Vision,5.79E+23,"Tensor type is BF16 (https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct).

“Training utilized a cumulative of 2.02M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency… Training time: Stage 1 pretraining: 147K H100 hours Stage 2 annealing: 98K H100 hours SFT: 896 H100 hours RLHF: 224 H100 hours.” (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#hardware-and-software).

The Nvidia H100 80GB is the H100 SXM. BFLOAT16 Tensor Core peak FLOPS with sparsity is 1,979 teraFLOPS (https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet?ncid=no-ncid).

Assuming 33% utilization rate,
Training compute
~= 0.33 * ( 147000 + 98000 + 896 + 224 ) hours * 3600 s / hour * 1979e12 FLOPS / GPU
~= 5.79e23 FLOPS",,,"""Stage 1 pretraining: 147K H100 hours Stage 2 annealing: 98K H100 hours SFT: 896 H100 hours RLHF: 224 H100 hours""

https://huggingface.co/meta-llama/Llama-3.2-11B-Vision",NVIDIA H100 SXM5 80GB,,
Qwen2.5-72B,9/19/2024,72700000000,72.7B,7.80E+24,"Training dataset size was 18 trillion

6ND = 6 * 72.7 billion parameters * 18 trillion tokens = 7.8e24",1,,,,,
Qwen2.5 Instruct (72B),9/19/2024,72700000000,"Number of Parameters: 72.7B
Number of Paramaters (Non-Embedding): 70.0B",7.85E+24,6ND = 6*72700000000 parameters *18000000000000 tokens = 7.8516e+24,,,,,,
Oryx 34B,9/19/2024,34000000000,,,,,,,NVIDIA A800 SXM,64,
Qwen2.5-32B,9/17/2024,32500000000,32.5B,3.51E+24,6 FLOP / parameter / token * 32.5B parameters * 18 trillion tokens = 3.51 × 10^24 FLOP,,,,,,
o1-preview,9/12/2024,,,,,,,,,,
o1-mini,9/12/2024,,"Can't get an exact estimate, but we suspect total parameter count around 60B-120B, active parameters around 10B-30B. 

Given these models are served at 150-200 tok/s, at $4.40/Mtok output, inference economics (https://epoch.ai/blog/inference-economics-of-language-models) suggests total parameter count around 60-120B parameters, with mixture-of-experts active parameters around 10-30B. MoEs make a given model roughly comparable to a ~50% smaller dense model (https://epoch.ai/gradient-updates/moe-vs-dense-models-inference), which lines up decently with Magistral Small pricing (24B dense, served at a similar speed for the cheaper $1.50/Mtok). ",,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",,,,,,
DeepSeek-V2.5,9/6/2024,2.36E+11,"21B active params, 236B total",1.79E+24,"V2.5 is a merge of V2-coder and V2-chat
V2-coder is trained for 6T additional tokens from an intermediate checkpoint of V2, which had been trained for 4.2T tokens. Total: 10.2T
V2-chat is fine-tuned from V2, saw 8.2T tokens in pre-training
Unique steps: 8.2T + 6T = 14.2T
FLOPs: 6 * 21B * 14.2T = 1.7892e24",,,,,,
Hunyuan Turbo,9/5/2024,,,,,,,,,,
AlphaProteo,9/5/2024,,,,,,,,,,
Hairuo,8/29/2024,,,,,,,,,,
GLM-4-Plus,8/29/2024,,,3.60E+25,Estimated using benchmark imputation,,,,,,
Jamba 1.5-Large,8/22/2024,3.98E+11,94B active/398B total,,,,,,NVIDIA H100 SXM5 80GB,,
Grok-2,8/13/2024,,,2.96E+25,Estimate based on xAI statements comparing Grok-2 compute to GPT-4 and Grok-3. Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing,,,,NVIDIA H100 SXM5 80GB,,30499862.98
Table Tennis Agent,8/7/2024,185000,"17 low level controllers with 10k parameters each: 
""Each policy is a dilated-gated CNN
[22] following the architecture in [23] with 10k parameters... The final
system contained 17 LLCs""

One high-level controller with 4.5k parameters: ""The style policy architecture, similar to the LLC but with
only 4.5k parameters, has a (8, 128) observation space""

spin classifier that is a 2-layer MLP of hidden sizes (128, 64) and input size 18, which is 10k parameters per o1 and Claude.

So ~185k parameters total",,unclear,,,,,,
LLaVA-OV-72B,8/6/2024,72000000000,,3.04E+24,"FineTune: 6*72000000000*38314782000=1.655199e+22

Base model: 3.02e+24
Total: 3.036552e+24",1,,,,,
AFM-server,7/29/2024,,,4.30E+24,"""The AFM base models are dense decoder-only models that build on the
Transformer architecture""

""We train AFM-server from scratch for 6.3T tokens on 8192
TPUv4 chips, using a sequence length of 4096 and a batch-size of 4096 sequences.""

""For both models we perform continued pre-training at a sequence length of
8192, with another 1T tokens from a mixture that upweights math and code,
and down-weights the bulk web-crawl.""

""The sustained model-flop-utilization (MFU) for this training run was approximately 52%.""

Parameter count is not specified other than it being ""larger"" than 3 billion.

Counting FLOP: Chinchilla scaling laws would suggest 7.3T / 20 = 365B parameters. 

365B parameters * 7.3T tokens * 6 ~= 1.6e25 FLOP.

However, the attention to inference optimization in the technical report suggests a smaller size, even for this ""server"" model. One point of reference is Llama 3 70B being overtrained by a factor of 10. If this is true of AFM-server, the parameter count would be ~37B and training compute would be 1.6e24 FLOP.

GPU-time: assume a wall-clock training time of 30 days based on the current trend value for notable models.

8192 chips * 275e12 FLOP/s per chip * 0.52 utilization * 30 * 24 * 60 * 60 s ~= 3.0e24 FLOP

The geometric mean of these three estimates is 4.3e24 FLOP.",1,,,Google TPU v4,8192,
AFM-on-device,7/29/2024,2730000000,"Table 1, sum of non-embedding and embedding parameters",4.51E+23,"Model was initialized from a pruned version of a 6.4B parameter model trained using the same recipe as AFM-server. Assuming ""same recipe"" involves training for the full 6.3T tokens, this implies 6 * 6.3T * 6.4B = 2.42e23 FLOP. 

The pruning masks are learned by training over 188B tokens, which suggests 6 * 188B * 6.4B = 7.22e21 FLOPs.

Pretraining is then run over 6.3T tokens; however, labels are a convex combination of true labels and the predicted labels from the unpruned 6.4B model. Since this involves running the 6.3T tokens forward through both the 6.4B and the 2.73B model, but only calculating gradients for the smaller model, FLOPs here are equal to (6 * 6.3T * 2.73B) + (2 * 6.3T * 6.4B) = 1.84e23. 

Finally, there is a 1T ""continuation"" pretraining stage without distillation loss, for 6 * 1T * 2.73B = 1.64e22 FLOP, and a 100B context-lengthening stage for another 6 * 100B * 2.73B = 1.64e21 FLOP

In total: 2.42e23 + 7.22e21 + 1.84e23 + 1.64e22 + 1.64e21 = 4.51e23 FLOP",1,,"Trained on ""one slice of 2048 TPUv5p chips""; wall-time not given.",Google TPU v5p,2048,
Mistral Large 2,7/24/2024,1.23E+11,,2.13E+25,"Details are sparse, but we can hazard a guess based on evidence about the training cluster they may have used, the scale up in compute they likely would have used relative to Mistral Large 1, and from the model's MMLU score. Extended reasoning given here: https://docs.google.com/document/d/1I2ZWBLFMpRZYcdMMUfKAGZFJrOJpduNDS9ZeVFIHnd8/edit?usp=sharing",,,,,,
Llama 3.1-405B,7/23/2024,4.05E+11,405B,3.80E+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",1,2142,"Trained on 30.84M GPU hours (https://huggingface.co/blog/llama31) and used ""up to 16K H100 GPU[s]"" so training took at least
30.84M / 16k = 1927.5 hours or ~80 days. 

Section 3.3.4 gives reliability details over a 54 day period during training, for which they had ""higher than 90% effective training time""
1927.5 / 0.9 = 2142 hours

Probably, full training time is somewhat longer, since it sounds like there were periods where not all 16k H100s were running.",NVIDIA H100 SXM5 80GB,16384,51040524.06
GPT-4o mini,7/18/2024,,,7.36E+24,"Training compute estimated from benchmark scores.

90% CI [3.23e+24, 2.05e+25]",,,,,,
Mathstral,7/16/2024,7000000000,,,,,,,,,
DeepL LLM,7/16/2024,,,,,,,,,,
ESM3 (98B),6/25/2024,98500000000,98.5 billion (Table S1),1.07E+24,"""ESM3 at its largest scale was trained with 1.07×10^24 FLOPs on 2.78 billion proteins and 771 billion unique tokens, and has 98 billion parameters.""

per Table 1, trained 98B model on 1.8T training tokens. 98 billion * 1800 billion * 6 = 1.06e24. Likely some rounding, so will go with developer's reported count.",2.3,,,,,
Cambrian-1-34B,6/24/2024,34000000000,,,,,,Our final Cambrian-1 models are trained in less than 4 days on a TPU-V4-512.,Google TPU v4,512,
Claude 3.5 Sonnet,6/20/2024,,,2.70E+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",,,,,,
DeepSeek-Coder-V2 236B,6/17/2024,2.36E+11,Mixture of experts model. 21B parameters activated per token.,1.29E+24,"Trained on a total of 10.2T tokens
6NC: 6 * 10.2T * 21B active parameters = 1.285e24",,,,,,
Nemotron-4 340B,6/14/2024,3.4E+11,340B,1.80E+25,"9 trillion tokens for training
6 * 340B * 9T = 1.8E25

alternatively, can do a hardware estimate with a few extra steps:

According to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. 
Unfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. 
Nemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. 
A batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. 
So, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) 
Multiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. 
",,2200,"see training compute notes, this is an inferred estimate",NVIDIA H100 SXM5 80GB,6144,20528977.87
OpenVLA,6/13/2024,7188100000,"Based on a Prismatic-7B VLM backbone, which itself is comprised of 600M parameter vision encoder (DinoV2 + SigLIP) plus Llama-2 7B. Table 1 indicates 7.1881 billion trainable parameters",1.10E+23,"Majority of compute is from VLA pre-training embedded in Prismatic-7B and it's constituent models. 

The fine-tuning compute used in this paper is ""64 A100 GPUs for 14 days, or a total of 21,500 A100-hours""
21500 * 3600 * 3.12e14 * 0.4 = 9.66e21

Prismatic-7B training took ""less than 9 hours"" on 8 A100s: 9 * 3600 * 8 * 3.12e14 * 0.4 = 3.23e19

Add in the pre-trained components:
- DinoV2 = 7.42e21, per our database
- The SigLIP model in question is SoViT-400m/14 from the cited Alabdulmohsin et al., 2023) and ""is pretrained on 40 billion examples, which amounts to 9T GFLOPs and 230K TPUv3 core-hours"" = 9e21
- Llama 2-7B = 8.4e22, per our database

Total
9.66e21 + 3.23e19 + 7.42e21 + 9e21 + 8.4e22 = 1.10e23",27,336,"""The final OpenVLA model is trained on a cluster of 64 A100 GPUs for 14 days""
14 days * 24 hr/day = 336 hours",NVIDIA A100,64,
Qwen2-72B,6/7/2024,72710000000,"72.71B parameters in total, of which 70.21B are non-embedding parameters",3.02E+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",,,,,,
"ALLaM adapted13B
",5/21/2024,13000000000,,1.72E+23,"Finetuning compute: 
6*13000000000*1200000000000=9.36e+22

Llama 13B training FLOP: 7.8e+22

Total: 1.716e+23",1,,,NVIDIA A100,,
ALLaM adapted 70B,5/21/2024,70000000000,,1.06E+24,"Llama 70B: 8.1e+23
Finetune: 6*70000000000*600000000000=252000000000000000000000
Total: 1062000000000000000000000",1,,,NVIDIA A100,,
ALLaM 34B,5/21/2024,34000000000,,1.06E+24,6*34000000000*5200000000000=1.060800e+24,1,,,NVIDIA A100,,
ALLaM 7B,5/21/2024,7000000000,,9.04E+22,"FT: 6*7000000000*1200000000000=5.04e+22

Llama 7B compute: 4e+22

Total: 9.04e+22",1,,,NVIDIA A100,,
Octo-Base,5/20/2024,93000000,Source: https://arxiv.org/abs/2405.12213 ,5.85E+20,"Training compute 
= peak FLOPs * utilization rate * training time
~= 128 TPUs * 275e12 FLOPs / TPU * 0.33 * 14 hours * 3600 s / hour
~= 585446400e12 FLOPs
= 5.85e20 FLOPs,
assuming utilization rate = 0.33.",,14,"“We trained two variants of our model: Octo-Small with a transformer backbone that mirrors the size of a ViT-S, andOcto-Base with a transformer backbone that mirrors the size of a ViT-B. The ViT-B was trained for 300k steps with a batch size of 2048 using a TPU v4-128 pod, which took 14 hours."" (Source: https://arxiv.org/abs/2405.12213)
",Google TPU v4,128,
GLM-4 (0520),5/20/2024,,,,"- “the GLM-4 models are pre-trained on ten trillions of tokens”
- I did not find any information about parameters or compute. Here they speculatively estimate GLM-4 to be 200B parameters (which seems plausible), though no source provided: https://lifearchitect.ai/models-table/
- “GLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)”  none of these models has parameters disclosed or compute estimation.

6*10000000000000*200000000000 = 1.2e+25 FLOPs with “Likely” confidence (+/- 1 OOM)",,,,,,
Yi-Large,5/13/2024,1E+11,"""Yi-Large is a software over-the-air-driven closed-source large model with a parameter of over 100 billion tokens."" from https://www.chinadaily.com.cn/a/202405/13/WS6641abd1a31082fc043c6ccd.html",1.80E+24,"6ND = 6*100000000000*3000000000000=1.8e+24

(speculative confidence because training dataset size is very uncertain)",,,,,,
GPT-4o,5/13/2024,,"Not known.

Inference costs in the API are 2x cheaper than GPT-4 Turbo",3.81E+25,Training compute estimated from benchmark scores.,,,,,,
VILA1.5-13B,5/3/2024,13493916736,"https://huggingface.co/Efficient-Large-Model/VILA1.5-13b/blob/main/llm/model.safetensors.index.json
https://huggingface.co/Efficient-Large-Model/VILA1.5-13b/tree/main/vision_tower
https://huggingface.co/Efficient-Large-Model/VILA1.5-13b/blob/main/mm_projector/model.safetensors

llm: 13015864320
vision_tower: 428225600
mm_projector: 49826816

total: 13493916736
",2.30E+21,"not directly reported (arXiv preprint is for VILA-13B), but assumed to be similar to VILA-13B (given similar size/architecture)",,,,NVIDIA A100,128,
Llama 3-70B,4/18/2024,70000000000,,7.86E+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",,,,NVIDIA H100 SXM5 80GB,,
Reka Core,4/15/2024,67000000000,,8.40E+24,"No direct information about Reka Core model (""Reka Core has not finished training and is still improving."")

The smaller dense model Reka Flash has 21B parameters and was trained on 5 trillion language tokens.

There is information about compute: ""Our setup comprises of clusters from a mixture of vendors with our peak compute being approximately
2.5K H100s and 2.5K A100s.""

If we assume 2 months of training with 2.5k H100s and 2.5k A100s at utilization 0.5 we get 8.4e24 FLOP (2500*9.9e14+2500*3.12e14)*60*60*24*60*0.5.",,,,"NVIDIA A100,NVIDIA H100 SXM5 80GB",,
ReALM,3/29/2024,3000000000,Fine-tuned FLAN-T5 models ranging from 80M to 3B,,Fine-tuned from FLAN-T5,,,,,,
DBRX,3/27/2024,1.32E+11,132B mixture of experts. 36B parameters active per inference,2.60E+24,"Mixture of Experts (MoE)

36 billion active params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).",1,,,NVIDIA H100 SXM5 80GB,,
MM1-30B,3/14/2024,30000000000,30B,4.86E+23,"Pre-trained on ~2B image-text pairs and 2T tokens (Table 2). Each image is 144 tokens, so the images are ~300B tokens.
Then additional multimodal training for 400B tokens, for a total of ~2.7T tokens.

This is the final training recipe: ""We initialize both the image encoder and the underlying LLM decoder weights for MM1 from in-house pre-trained models2. We then perform multimodal pre-training on the above data mix for 200k steps (approx. 400B tokens).""

Compute  = 6ND = 6 * 2.7 trillion * 30 billion = 4.86e23

maybe the size of the visual connector is relevant",,,,,,
ManiGaussian,3/13/2024,,,,,,,"""All the compared methods are trained on two NVIDIA RTX 4090 GPUs for 100k iterations with a batch size of 2.""

https://github.com/GuanxingLu/ManiGaussian
Training: ""We train our ManiGaussian on two NVIDIA RTX 4090 GPUs for <2 days.""

Paper didn't specify precision, but GitHub code/weights might confirm - I gave a cursory look over the repo but didn't see anything obvious.

Assume BF16 -> 3.3e14 FLOP/s/GPU 
Assume <2 days = <48 hr  ->  42 hr = 151200 s
Assume 0.3 utilization
0.3 * 2 GPU * 3.3e14 FLOP/s/GPU * 151200 s ~= 2.99e19 FLOP


 ",NVIDIA GeForce RTX 4090,2,
Inflection-2.5,3/7/2024,,,1.00E+25,"""Inflection-1 used approximately 4% the training FLOPs of GPT-4 and, on average, performed at approximately 72% GPT-4 level on a diverse range of IQ-oriented tasks. Inflection-2.5, now powering Pi, achieves more than 94% the average performance of GPT-4 despite using only 40% the training FLOPs.""

This is a weird one - we estimated GPT-4 at 2.1e25 FLOP (which could be off somewhat, or Inflection could believe a different number). 40% of that is ~8e24. But Inflection 2, the previous model, was trained on ~1e25 FLOP per Inflection. Inflection-2.5 also does better on benchmarks than 2. Intuitively Inflection-2.5 would be trained on appreciably more compute. 

1e25 seems like a rough, perhaps conservative guess given all this.",,,,NVIDIA H100 SXM5 80GB,,11804593.33
Claude 3 Sonnet,3/4/2024,,,,,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,,
Claude 3 Opus,3/4/2024,,,1.64E+25,Training compute estimated from benchmark scores.,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,,
Aramco Metabrain AI,3/4/2024,2.5E+11,"""It has 250 billion parameters that are adjustable during training to generate outputs or make predictions.""",1.05E+25,6*250B*7T=1.05e+25,,,,,,
Mistral Large,2/26/2024,,,1.12E+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",,2500,Speculation by Emad Mostaque: 20M euro spent at Scaleway (1.9 euro per H100-hour) would be around 3 months on 4000 H100s.,NVIDIA H100 SXM5 80GB,,14110111.93
MegaScale (Production),2/23/2024,5.3E+11,"Production run is stated to have ""hundreds of billions of parameters"". Since the authors also do a number of experiments with a 530B model, I speculate they've used 530B for the production model.",3.90E+24,"Speculative. The model is stated to have trained for ""several weeks"". Assuming 530B parameters and ""several"" = 3, compute can be estimated from the 175B model's stated PFLOP/sec:
2166.3 aggregate PFlops/sec * 3 weeks * 7 days/week * 24 hours/day * 3600 seconds/hour = 3.9e+24.
As an upper bound, say 8e+24. ",,504,"Speculative. Authors state ""several weeks"". For analysis, I've assumed this means around 3 weeks.",NVIDIA A100,12288,2614019.245
Sora Turbo,2/15/2024,,,,,,,,,,
Sora,2/15/2024,,,,,,,,,,
Gemini 1.5 Pro,2/15/2024,,MoE architecture,1.58E+25,Training compute imputed from benchmark scores.,,,,Google TPU v4,,7753789.965
Aya,2/12/2024,13000000000,13B  - fine tune of mT5 - from last page - model card ,,"13B parameters, batch size = 256, sequence length = 1024 (for both input and output), 30K updates
- aproximation 6ND = 6 * 13B * 2 * 1024 * 30K * 256= 1226833920000000000000 = 1.22683392e+21
""We finetune mT5 models using the Adafactor optimizer [Shazeer & Stern, 2018] with a learning rate of 3 × 10−4 and a batch size of 256. We find that using a smaller learning rate compared to 1 × 10−3 leads to a better downstream performance, which is potentially due to the diverse nature of our IFT mixture. Both input and target sequence length are set to 1024.""
""We train all the models for 30,000 update steps with data packing enabled.16 This results in a training budget of 25M samples. """,,,,Google TPU v4,128,
Qwen1.5-72B,2/4/2024,72000000000,72B,1.30E+24,"3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 

6 * 72 billion * 3 trillion = ~1.3e24",,,,,,
Qwen-VL-Max,1/25/2024,7000000000,"Not stated. Qwen-VL (less capable, presumably smaller version) is 9.6B

Upd: 7B parameters mentioned here
https://github.com/QwenLM/Qwen-VL#qwen-vl-plus",,,,,,,,
AlphaGeometry,1/17/2024,151000000,"""Overall, the transformer has 151 million parameters, excluding embedding layers at its input and output heads.""",,"Training details. Don't think there's enough info for a FLOP estimate.

""Our customized tokenizer is trained with ‘word’ mode using
SentencePiece36 and has a vocabulary size of 757. We limit the maximum context length to 1,024 tokens and use T5-style relative position embedding37. Sequence packing38,39 is also used because more
than 90% of our sequences are under 200 in length. During training, a
dropout40 rate of 5% is applied pre-attention and post-dense. A 4 × 4 slice of TPUv3 (ref. 41) is used as its hardware accelerator. For pretraining, we train the transformer with a batch size of 16 per core
and a cosine learning-rate schedule that decays from 0.01 to 0.001
in 10,000,000 steps. For fine-tuning, we maintain the final learning rate of 0.001 for another 1,000,000 steps""",,,,Google TPU v3,,
Palmyra X 003,1/1/2024,72000000000,,,,,,,,,
Kimi Explorer,1/1/2024,,,,,,,,,,
CoRe,12/29/2023,12400000000,"""Since the default setting consists of two GPT-J (6B) and a DeBERTa-large (0.4B), we note our backbone as “GPT-J 12B”, which implies around 12.4 billion parameters in total. """,,,,,,NVIDIA A100 SXM4 40 GB,,
nekomata-14b,12/21/2023,14200000000,Source: https://huggingface.co/rinna/nekomata-14b,2.56E+23,"Begins from Qwen-14B (2.5e23 FLOP). They continue pretraining on a mix of Japanese and English text for 66B tokens.
(assuming 1 epoch, and using the C=6ND approximation)
= # of active parameters / forward pass * # of tokens * 6 FLOPs / token
~= 14.2e9 active parameters * 66e9 tokens * 6 FLOPs / token
~= 5623.2e18 FLOPs
~= 5.62e21 FLOPs

In total, 2.5562e23",,168,The pre-training job was completed within a timeframe of approximately 7 days (source: https://huggingface.co/rinna/nekomata-14b).,Amazon Trainium1,256,
Gemini Nano-2,12/19/2023,3250000000,3.25B,,"More tokens than Chinchilla-optimal:

""The number of tokens used to train the largest models were determined following the approach in Hoffmann et al. (2022). The smaller models are trained for significantly more tokens to improve performance for a given inference budget, similar to the approach advocated in Touvron et al. (2023a)""

Chinchilla was 1.4T tokens for 70B params, so Chinchilla-optimal for 3.25B params would be ~1.4T/20 = 70B tokens.

So compute was significantly greater than 3.25B * 70B * 6, which is 1.4e21. 

Touvron et al. is the Llama 1 paper, in which a 6.7B model is trained for 1T tokens. Using the same ratio, a 3.25B model would be trained on ~500B tokens. 3.25 * 500B * 6 = 9.75e21. No guarantee that the exact ratio for Nano is close to Llama's, of course.",,,,Google TPU v5e,,
Gemini Nano-1,12/19/2023,1800000000,1.8B,,"More tokens than Chinchilla-optimal:

""The number of tokens used to train the largest models were determined following the approach in Hoffmann et al. (2022). The smaller models are trained for significantly more tokens to improve performance for a given inference budget, similar to the approach advocated in Touvron et al. (2023a)""",,,,Google TPU v5e,,
FunSearch,12/14/2023,15000000000,"From the section called ""Pretrained LLM"": ""We use Codey, an LLM built on top of the PaLM2 model family... Because FunSearch relies on sampling from an LLM extensively, an important performance-defining tradeoff is between the quality of the samples and the inference speed of the LLM. In practice, we have chosen to work with a fast-inference model (rather than slower-inference, higher-quality)""

Unclear which PaLM2 model was used (of Gecko, Otter, Bison, and Unicorn); above quote indicates it was perhaps Otter or Bison, but not Unicorn. Exact parameter counts are not publicly disclosed for any of these models. In comparisons where FunSearch uses StarCoder-15B, Codey is an improvement but not obviously of an entirely different model class.

I report the 15B parameters from StarCoder-15B, used as an open-source comparison",3.87E+23,"Appendix A.5: ""Finding the full-sized symmetric admissible set I(15, 10) required the generation and analysis of approximately two million programs... To reproduce admissible set experiments done above (generating 2 million samples) one would have to use 15 instances of StarCoder-15B running on A100 40 GB GPU each and 5 CPU servers (each running 32 evaluators in parallel) for two days. We estimate that when running on Google Cloud, the price of an experiment is around $800 – $1400, and the energy usage around 250 – 500 kWh; i.e., 0.5% of the energy used for training StarCoder""

15 GPUs * 7.80E+13 FLOP/GPU-sec * 2 days * 24 hours/day * 3600 sec/hour = 2.02e20 FLOP for the GPU servers

We should also add the compute used to train the PaLM2 variant used as the base LLM. Since we don't have any details about this model, I use the compute from StarCoder-15B (used as the open source comparison point): 3.87e+23 FLOP

Unclear how to evaluate the compute from the CPU servers implementing the evolutionary algorithm, but this is very likely dwarfed by the pre-training compute for the LLM.",,48,"Appendix A.5: ""To reproduce admissible set experiments done above (generating 2 million samples) one would have to use 15 instances of StarCoder-15B running on A100 40 GB GPU each and 5 CPU servers (each running 32 evaluators in parallel) for two days""",,,
CogAgent,12/14/2023,18000000000,,6.71E+22,"States 12.6 TFLOP per 1120x1120 image forward pass. Trained 60k steps with 4608 batch size, and then 10k with 1024 batch size.
12.6 TFLOP * (60000*4608 + 10000*1024) = 3.76e21

Uses pretrained CogVLM as base (6.331e22 FLOP), along with EVA2-CLIP-L. EVA2-CLIP-L's FLOPs are potentially estimable, but based on details about EVA2-CLIP-g/14 (a larger model), they likely contribute negligibly to CogAgent.

Sum: 6.707e22",,,,,,
VILA-13B,12/12/2023,13350839296,"https://huggingface.co/Efficient-Large-Model/VILA-13b/tree/main?show_file_info=model.safetensors.index.json
",2.30E+21,"Appendix B: ""We perform training on 16 A100 GPU nodes, each node has 8 GPUs. The training hours for each stage of the 7B model are: projector initialization: 4 hours; visual language pre-training: 30 hours; visual instruction-tuning: 6 hours. The training corresponds to a total of 5.1k GPU hours. Most of the computation is spent on the pre-training stage.""
16 nodes * 8 GPU/node = 128 GPU
128 GPU * (4 h + 30 h + 6 h) = 5120 GPU*h
8 GPU/node -> A100 SMX4

https://huggingface.co/Efficient-Large-Model/VILA-13b/tree/main?show_file_info=model.safetensors.index.json
-> BF16

Assume 80 GB variant (A100 SMX4 80 GB)
pk-BF16: 312000000000000 FLOP/s = 3.12e14 FLOP/s

Assume 0.4 utilization (NVIDIA in-house)

0.4 * 3.12e14 FLOP/s/GPU * (3600 s / 1 h) * 5120 GPU*h = 2.3003136e21 FLOP
",,,,NVIDIA A100 SXM4 80 GB,128,
Mixtral 8x7B,12/11/2023,46700000000,"46.7B *sparse* params. 12.9B params used on average:

""Concretely, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model.""",7.74E+23,"Assuming the model was trained on ~1-10 trillions of tokens (same OOM as the models from the comparison in Figure 1. Llama 2 was trained on 2T tokens) + Mistral Small 3 was trained on 8T of tokens, we can estimate training compute with ""speculative"" confidence:

6 FLOP / token / parameter * 12.9 * 10^9 active parameters * 10*10^12 tokens [speculatively] = 7.74e+23 FLOP",,,,,,
SeamlessM4T,12/8/2023,2300000000,2.3B,,,,,,NVIDIA V100,,
Llama Guard,12/7/2023,7000000000,7B,1.60E+23,"1.7e17 finetune compute, plus Llama 2-13B pretrain compute (1.6e+23)",1,,,NVIDIA A100 SXM4 80 GB,,
Gemini 1.0 Ultra,12/6/2023,,,5.00E+25,"This number is an estimate based on limited evidence. In particular, we combine information about the performance of Gemini Ultra on various benchmarks compared to other models, and guesstimates about the hardware setup used for training to arrive at our estimate. Our reasoning and calculations are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",,2400,"Dylan Patel, author of SemiAnalysis, speculates that the training duration of Gemini may have been 100 days.",Google TPU v4,57000,29827341.92
Gemini 1.0 Pro,12/6/2023,,,1.83E+24,"Training compute estimated from benchmark scores.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c

",,,,Google TPU v4,,
Mamba-24M (SC09),12/1/2023,23400000,Table 4,,,100,,,,,
Qwen-72B,11/30/2023,72000000000,72B,1.30E+24,"72 billion params, 3 trillion tokens
72b * 3T * 6 = 1.3e24",,,,,,
PPLX-70B-Online,11/29/2023,70000000000,70B,,,,,,,,
GNoME for crystal discovery,11/29/2023,16240000,"""The pretrained potential has 16.24 million parameters.""
This refers to the GNoME network, which is a ""Gaussian Network Model of Energy"" for predicting crystal potential energy of new crystals.",,"Pretraining involved 36.43M steps:
""The learning rate was decreased to a new value of 2 × 10−4 after approximately 23 million steps, to 5 × 10−5 after a further approximately 11 million steps and then trained for a final 2.43 million steps. Training was performed on four TPU v3 chips.""

""Inference on an A100 GPU on a 50-atom system takes approximately 14 ms""
3.12e14 FLOP/s * 0.014 s = 4.368e12 FLOP per 50 atom system

Batch size was 32. Multiply inference FLOPs by 3 to account for forward and backward passes during training.
32 * 4.368e12 * 36.43 million * 3 = 1.53e22

This seems implausible – on 4 TPUv3 chips this would take
(1.53e22 / (4 * 1.23e14)) / (3600 * 24) = 360 days",1000,,,Google TPU v4,4,
Inflection-2,11/22/2023,,,1.00E+25,"""Inflection-2 was trained on 5,000 NVIDIA H100 GPUs in fp8 mixed precision for ~10²⁵ FLOPs""

(the second 1 is there because of airtable being wonky, it's not a real sig fig)",,,,NVIDIA H100 SXM5 80GB,5000,12991551.78
Claude 2.1,11/21/2023,,,,,,,,,,
AndesGPT,11/16/2023,,,,,,,,,,
Nemotron-3-8B,11/15/2023,8000000000,,1.80E+23,"https://huggingface.co/nvidia/nemotron-3-8b-base-4k

""This model was trained on a dataset containing 3.8 Trillion tokens of text""

8 billion * 3.8 trillion * 6 = 1.8e23

Also, using the hardware method: ""1,024 A100s were used for 19 days to train the model.""

19*1024 * 312 trillion * 24 * 3600 * 0.3 = 1.57e23",,456,19 days,NVIDIA A100,1024,214467.0201
Qwen-Audio-Chat,11/14/2023,8460000000,"the model has two components - audio and language.
670M + 7.7B = 8.46B
""The audio encoder is composed of 640M parameters""

""Qwen-Audio incorporates a large language model as its foundational component.
The model is initialized using pre-trained weights derived from Qwen-7B (Bai et al., 2023a). Qwen-7B is a 32-layer Transformer decoder model with a hidden size of 4096, encompassing a total of 7.7B parameters.""",,,,,,,,
GraphCast,11/14/2023,,Not mentioned in paper.,2.10E+22,"""Training GraphCast took roughly four weeks on 32 Cloud TPU v4 devices using batch parallelism.""

4.6: ""we use bfloat16 floating point precision""

2.1e22 = 2.75E+14 FLOP/s * 32 * 60* 60 * 24 * 7 * 4",,,,,,
Volcano 13B,11/13/2023,13000000000,13B,4.56E+22,"Base model is LLaVa-1.5 13B, which used 4.55e22 FLOP (mostly coming from Llama base)

""For this research, we used an NVIDIA A100-SXM4-80GB GPU and an AMD EPYC 7513 32-Core Processor running at 2.0778 GHz. Training
VOLCANO 7B required 8 GPUs and took a total of 15 hours, while training VOLCANO 13B took 30 hours.""
3.12e14 * 8 * 30 * 3600 * 0.3 = 8.1e19 finetune compute",1,30,,NVIDIA A100 SXM4 80 GB,,
SPHINX (Llama 2 13B),11/13/2023,19900000000,"SPHINX + Llama 2 13B
SPHINX component involves four vision encoders:
- CLIP - ViT
- CLIP - ConvNeXt V2 (89M to 659M params, depending on size)
- DinoV2 - ViT (22M to 1.14B params, depending on size)
- Q-former (188M params)
Also involves to projection networks

Huggingface Hub model files appear to be 39.8GB. Assuming models are stored in fp16 there are 2 bytes per parameter, so 39.8 / 2 = 19.9B parameters.",3.04E+22,"""The pre-training time is around 125 hours on 32 A100 GPUs with a 7B
language model and about twice the time with a 13B language model... The fine-tuning takes about 38 hours with 16 A100 GPUs with a 13B
language model.""

((125*2 * 32) + (38 * 16)) * 3.12e14 * 3600 * 0.3 = 2.9e21

Component vision encoders were initialized from pre-trained:
- CLIP ViT: 1.5e22 FLOPs for L/14@336
- ConvNeXt V2: 6.8e21 FLOPs for largest
- DinoV2: 7.42e+21 FLOPs for largest
- Q-former: 1.2e21 FLOPs for largest

(Based on full parameter count, SPHINX probably uses largest models)

Sum: 3.04e22 FLOPs",,290,"""The pre-training time is around 125 hours on 32 A100 GPUs with a 7B
language model and about twice the time with a 13B language model.""
"" The fine-tuning takes about 38 hours with 16 A100 GPUs with a 13B
language model.""",NVIDIA A100 SXM4 40 GB,32,239188.6875
MultiBand Diffusion,11/8/2023,,,2.60E+19,"""It takes around 2 days on 4 Nvidia V100 with 16 GB to train one of the 4 models.""

125 tflop/s for V100 SXM (not clear which they used, could be PCI given small number - still same OOM thus confident)
4 * 125 trillion * 2 * 24 * 3600 * 0.3 = 2.6e19",,48,around 2 days,NVIDIA V100,,22.8103233
OmniVec,11/7/2023,,,,,2000,,,,,
mPLUG-Owl2,11/7/2023,7120000000,"""As depicted in Figure 2, our model, referred to as mPLUGOwl2, is composed of three main components: a fundamental vision encoder, a visual abstractor, and a language decoder. Specifically, we utilize ViT-L/14 as the
vision encoder and LLaMA-2-7B [58] as the language decoder""
ViT-L/14 has 123M parameters and Llama 2 7B has 7B parameters.",,"ViT-L/14 and Llama 2-7b compute, plus 1.7e19 joint pretrain FLOP (6 * 400M * 7.1B) and 4e16 joint finetune FLOP. Everything is a negligible fraction except the Llama 2 compute.",1,,,,,
GPT-4 Turbo,11/6/2023,,Not known. Maybe smaller/sparser than GPT-4.,2.20E+25,Estimated using benchmark imputation,,,,,,
CogVLM-17B,11/6/2023,17000000000,"CogVLM-17B has 10 billion vision parameters and 7 billion language parameters. However, ""the total number of trainable parameters is 6.5B"".

""CogVLM model comprises four fundamental components: a vision transformer (ViT) encoder, an MLP adapter, a pretrained large language model (GPT), and a visual expert module.""

ViT: EVA2-CLIP-E, last layer removed (5B params with last layer, non-trainable)
MLP adapter: 2 layers, parameter count unavailable
GPT: Vicuna1.5-7B (7B params)
Visual expert module: parameter count unclear",6.33E+22,"from table 8 on page 17

230.1 FLOPS*days 
so 
10**15*24*3600*230.1= 1.988e22

Since this training uses pretrained weights from EVA02-CLIP-E and Vicuna1.5-7B, we report the full number of FLOPs baked into the model.

EVA02-CLIP-g/14 is stated to have taken 25 days to train 12B samples using 64 A100-40GB GPUs, implying: 
25 days * 24 hr/day * 3600 sec/hr * 64 GPU * 7.80E+13 FLOP/GPU-sec * 30% efficiency = 3.23e21

EVA02-CLIP-E doesn't give a training time; it saw 1/4 as many samples as the g/14 model but has 4.27x more parameters; as a rough estimate, assume it took the same number of FLOPs to train.

Vicuna1.5-7B's training compute is ~entirely embedded in the base Llama-7b weights, which took 4.02e+22 FLOPs

Total: 1.988e22 + 3.23e21 + 4.02e22 = 6.331E22",,,,,,
LLaVA 1.5,11/5/2023,13000000000,"from abstract ""Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. """,7.81E+22,"""Due to the increased image input resolution to 336^2, the training of LLaVA-1.5 is ∼2× as long as LLaVA: ∼6 hours of pretraining and ∼20 hours of visual instruction tuning using 8× A100s.""
26 * 3600 * 8 * 3.12e14 * 0.3 = 7.0e19

Fine-tuned from Vicuna-13B (itself finetuned from LLaMa-13B), which used 7.8e22 FLOPs

7.0e19 + 7.8e22",1,24,"from abstract ""Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. """,NVIDIA A100,8,
Grok-1,11/4/2023,3.14E+11,"""314B parameter Mixture-of-Experts model with 25% of the weights active on a given token"". So effectively 78B parameters

Mixture of 8 experts: https://github.com/xai-org/grok-1",2.90E+24,"""On these benchmarks, Grok-1 displayed strong results, surpassing all other models in its compute class, including ChatGPT-3.5 and Inflection-1. It is only surpassed by models that were trained with a significantly larger amount of training data and compute resources like GPT-4""

Per table, Grok-1 is surpassed by Palm 2, Claude 2, GPT-4, so it required less compute than these three models. Palm 2 was trained on 7e24 FLOP.

GPT-3.5 is ~2.6e24. Inflection-1's compute is not public/known by us but Inflection says Inflection-1 compute was <= Palm-540B's (which was ~2.5e24). 

For optimal training, our current working hypothesis is that you still need something like Chinchilla scaling on the total number of parameters in the model, even for MoE models, so optimal dataset size would be 20*310B tokens. With 25%*314B params active per forward pass, this would be around 3e24 FLOP.
https://www.wolframalpha.com/input?i=20*310+billion+*+6+*+25%25+*+314+billion",,,,,,
RT-Trajectory,11/3/2023,,seems to be based on the RT-1 architecture (35M parameters) with some modifications (section 3.3),,"Given the architecture seems to use 35M parameters, it seems unlikely this is above 1e23 FLOP.",,,,,,
BLUUMI,11/3/2023,1.76E+11,176 billion,,,8,,,AMD Radeon Instinct MI250X,,
Yi-34B,11/2/2023,34000000000,34b,6.10E+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",,,,NVIDIA A100,128,
Cohere Embed,11/2/2023,,,,"https://docs.cohere.com/docs/environmental-impact

Embed v2 (older version) produced 6689.76 kg CO2 to train. Using the calculator Cohere links (https://mlco2.github.io/impact/) that's the equivalent of 80,000 TPUv3-hours in the ""us-west1"" region. That's 3.5e22 FLOP without considering utilization. However, I have no idea which region Cohere's GPUs are in (looks like CO2/energy can vary a lot by region), and they probably used a more recent GPU.",,,,,,
Skywork-13B,10/30/2023,13000000000,13B,2.50E+23,"""Our Skywork-13B is trained on a cluster of 64 NVIDIA-HGX-A800 nodes, a total of 512 A800-80G SXM GPUs... The training process of Skywork-13B spanned a total of 39 days.""

They note that ""we achieved a token throughput of 1873 per GPU per second and a model flops utilization (MFU) of 56.5%... "". 

""MFU"" was coined in the Palm paper (https://arxiv.org/pdf/2204.02311.pdf) and only counts operations used to train the model, not all operations observed on the hardware. MFU is lower than traditionally measured utilization.

Using the 56.5% number, and a peak tensor performance of 623.8 TFLOPS for the A800, this suggests 512 * 623.8 TFLOPS * 39 days * 86400 seconds/day * 0.565 = 6.08e23 FLOP.

Based on C=6ND, with 13B parameters and 3.2T tokens, we have C=6*(13B)*(3.2T)=2.5e23 FLOP.

Since the reported MFU is quite high, and would imply a higher compute usage than 6ND, it seems they may have trained on mixed precision and with the GPUs not always operating in the 623.8 TFLOPS mode.",1,940,39 days,NVIDIA A800 PCIe 40 GB,512,
ChatGLM3-6B,10/27/2023,6000000000,6B from https://arxiv.org/abs/2406.12793,5.04E+22,"Highly speculative.
Assume 1 epoch on 1.4T tokens.
6 FLOP/token/param * 1.4T tokens * 6B params=50.4 * 10 ^(12+9) = 5.04*10^(22)",,,,,,
DiT-XL/2 + CADS,10/26/2023,675000000,original parameter count for DiT-XL/2,,,,,,,,
CODEFUSION (Python),10/26/2023,75000000,Table 1,7.92E+18,"V100 performance: 125 teraFLOPS according to https://www.nvidia.com/en-us/data-center/v100/

11 hours * 4 GPUs * 125 teraFLOPS/GPU * 0.40 utilization = 7.92e18 FLOP",,11,"""The system used to run the experiments uses an Intel Core i7 processor (base at 1.8 GHz) along with 4 V100 GPU units, a 64-bit operating system, and 56 GB RAM. CODEFUSION took 8 hours to pre-train and 3 hours to fine-tune on average for each dataset.""",NVIDIA V100,4,8.542235671
DALL·E 3,10/19/2023,,,,,,,,,,
ERNIE 4.0,10/17/2023,,"""similar architecture with 3.5 version""  -interpreter dub at 01:25:08 https://www.youtube.com/watch?v=wYozcsavRuM",,"Unlikely to be >1e25 FLOP, ERNIE 4.5 was <1e25 FLOP.

may be mentioned here https://www.youtube.com/watch?v=wYozcsavRuM",,,,,,
RT-2-X,10/13/2023,55000000000,55B,,,,,,,,
Ferret (13B),10/11/2023,13000000000,13B,,"Fine-tuned from Vicuna-13B, which we don't have an estimate for. Finetuning cost is ~4e19.

""Training Details. We initialize the image encoder with CLIP-ViT-L/14@336p, the LLM with Vicuna, and the projection layer with LLaVA’s first-stage weights, leaving the visual sampler randomly initialized. After the initialization, Ferret is trained on the aforementioned GRIT data for three epochs, optimized by Loshchilov & Hutter (2017) with a learning rate of 2e − 5 and a batch size of 128. The training takes ∼5/2.5 days on 8 A100 GPU for a Ferret-13B/7B.""

5 * 24 * 3600 * 0.3 utilization (assumption) * 312 TFLOP/s = 4.04e19",3,120,"""The training takes ∼5/2.5 days on 8 A100 GPU for a Ferret-13B/7B.""",NVIDIA A100,8,
FinGPT-13B,10/7/2023,13000000000,"Finetunes using LoRA, so only trains 3.67 million parameters",1.60E+23,From Llama 2-13B,,17.25,https://github.com/AI4Finance-Foundation/FinGPT?tab=readme-ov-file,NVIDIA GeForce RTX 3090,1,
CTM (CIFAR-10),10/1/2023,,,,"Almost certainly <1e23 FLOP due to the small scale experiments.

""We use 4×V100 (16G) GPUs for CIFAR-10 experiment""
100K training iterations
",1.7,,,NVIDIA V100,4,
Amazon Titan,9/28/2023,2E+11,"200B dense model
https://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon",4.80E+24,"trained using NVIDIA NeMo: https://blogs.nvidia.com/blog/nemo-amazon-titan/

13,760 NVIDIA A100 chips (using 1,720 P4d nodes). It took 48 days to train.
from https://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon

counting operations: 6*200000000000*4000000000000=4.8e+24

gpu usage: 312000000000000(FLOP/s)*0.3*13760*1152*3600=5.3413281792e+24",,1152,,NVIDIA A100,13760,7656704.716
Show-1,9/27/2023,,,,,,,,NVIDIA A100,,
GPT-4V,9/25/2023,,,,,,,,,,
AlphaMissense,9/22/2023,93000000,"""The model architecture is similar to that of AlphaFold (21), with minor modifications""
Reference is to the AlphaFold 2 paper; that model had 93 million parameters",,"From supplementary materials: ""We independently trained three AlphaFold models and fine-tuned them independently on variants. We followed the training procedure described in (21), (only the “Initial training” stage) ... AF training is carried out for about 7e6 steps on single-chain structures ... Fine-tuning is carried out @until auROC of the evaluation set converges (about 350k samples, each training sample contains maximum 50 variants)""

Table S4 gives details. Total samples seen across the three pretraining models are (7.8M + 7.5M + 5.85M) = 21.15M

Each sequence is cropped to 256 elements long, which suggests 5.4B tokens seen in training.",4,,,,,
Robot Parkour,9/12/2023,500000,"Parkour policy details on page 8, table 11.",,"The paper provides some details on the training time and hardware used:

Each specialized skill policy (climbing, leaping, etc) was pre-trained with soft dynamics constraints for 12 hours using 1 Nvidia RTX 3090 GPU.
The skills were then fine-tuned with hard dynamics constraints for 6 hours each.
The final parkour policy distillation process used 4 computers with 1 RTX 3090 GPU each, training for an unspecified amount of time.
So the total training time was at least 12 + 6 x 5 = 42 hours for the initial skills, plus an additional unknown time for the distillation.

The hardware used was high-end Nvidia RTX 3090 GPUs, which at the time of paper writing would have been top of the line GPUs. Multiple GPUs were used in parallel during the distillation stage.",,,,NVIDIA GeForce RTX 3090,,
Falcon-180B,9/6/2023,1.8E+11,"""Falcon 180B is a super-powerful language model with 180 billion parameters""",3.76E+24,"43,500 petaflop-days per Table 1 of the paper

43500 * 1e15 * 24 * 3600 = 3.76e24


C = 6ND = 6 FLOP/token/parameter * 3.5 trillion tokens * 180 billion parameters = 3.78*10^24 FLOP",1,4320,"Stanford CRFM foundation model ecosystem graph data page https://crfm.stanford.edu/ecosystem-graphs/index.html?asset=Falcon-180B says 9 months, which is the maximum possible amount of time: training began sometime in 2023, and it was released in September. 

However, 6 months is more realistic. That is the length of the gap between Falcon 40B and Falcon 180B. Additionally, the amount of compute is specified in the paper, so there is only one degree of freedom in the uncertain values of training duration and hardware utilization rate. At six months, the utilization is unusually low, so the training was probably not longer than that.",NVIDIA A100 SXM4 40 GB,4096,10368712.43
Swift,8/30/2023,56804,"The control network is an MLP with input dimension 31, two hidden layers of size 128, and an output of dimension 4.
(31+1)*128+(128+1)*128+(128+1)*4 = 21124

Gate detector is a 6 layer U-net with 
8*(3^3*3+1) + 16*(3^2*8+1) + 16*(3^2*16+1) + 16*(5^2*16+1) + 16*(7^2*16+1) + 16*(7^2*16+1) = 35680

35680 + 21124 = 56804",5.34E+16,"Policies are trained for a total of 1 × 108 environment interactions, which takes 50 min on a workstation (i9 12900K, RTX 3090, 32 GB RAM DDR5). Fine-tuning is performed for 2 × 107 environment interactions.

35.58 TFLOPS * 50 min * 60 s/min * 0.50 utilization = 5.337*10^16 FLOP",,0.833,"50 minutes (training details, page 8)",NVIDIA GeForce RTX 3090,1,
Jais,8/29/2023,13000000000,"""With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic""",3.08E+22,C = 6ND = 6 * 13 billion params * 395 billion tokens = 3.081e+22 FLOP,,600,2023 June 25 to July 18 = 25 days = 600 hours,,,
PeptideBERT,8/28/2023,,,4.90E+16,"""Compute for fine-tuning ProtBERT: 1 NVidia GeForce GTX 1080Ti, 30 epochs, batch size 32, model trained for individual tasks with training time ranging from 58-116 minutes, assuming 
from Table 1 we have 244 minutes
11.34e12 FLOPs and 0.3 utilization rate FLOP = 244 min * 60 sec/min * 11.34e12 FLOP/sec *0.3 = 4.9e16 FLOP,",30,4.067,244 minues from Table 1,NVIDIA GeForce GTX 1080 Ti,1,
Qwen-VL,8/24/2023,9600000000,9.6B total - Table 1,,"Qwen-7B and ViT as base models, trained on 1.5B image-text pairs",1,,,,,
GGNN,8/5/2023,,"ESM-2 650M is used as the main PLM, they run ablations with versions up to 3B. Unclear how many parameters are are in the geometric graph neural network module.",7.56E+21,"ESM-2 650M is very likely the majority of FLOPs, since they only used 2 A100s (ESM-2 650M used 512 V100s for 8 days). As such I'm reporting the compute from ESM-2 650M here only.",,,,NVIDIA A100 SXM4 80 GB,2,
RT-2,7/28/2023,55000000000,"""We train two specific instantiations of RT-2 that leverage pre-trained VLMs: (1) RT-2-PaLI-X is built from 5B and 55B PaLI-X (Chen et al., 2023a), and (2) RT-2-PaLM-E is built from 12B PaLM-E (Driess et al., 2023).""

55B and 12B have similar overall performance",,"""""For RT-2-PaLI-X-55B, we use learning rate 1e-3 and batch size 2048 and co-fine-tune the model for 80K gradient steps""
Sequence length not stated",,,,,,
AudioLM,7/26/2023,1500000000,"""We use identical decoder-only Transformers in
all stages, with 12 layers, 16 attention heads, embedding
dimension of 1024, feed-forward layer dimension of 4096
and dropout of 0.1, together with T5-style relative positional
embeddings [38], resulting in a model parameter size of
0.3B per stage.""

Three stages (figure 2), and 300M per stage. Plus 600M parameters for w2v-BERT XL, so 1.5B total",3.90E+18,"""We train each stage on 16 TPUv4s with batch size of 256 for 1M steps.""

That's for the 900M-param transformers

If there's 256 passes in each batch, then using 6ND that's 900m * 256m * 6 = 1.3e18. sanity check: 16 tpu4s is 4.4e15 FLOP/s. 1.3e18 FLOP / 4.4e15 FLOP/s is 295 seconds. adjusting for utilization it would be ~1000 seconds or 15 minutes? probably too short, so 1.3e18 seems too low.

upd there are 3 stages -> 1.3e18*3 = 3.9e+18 (Speculative due to reasoning above)",,,,Google TPU v4,,
Llama 2-70B,7/18/2023,70000000000,"Llama has been released in 7B, 13B, 34B, and 70B variants.",8.10E+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",1,1728,"Model was trained from January 2023 to July 2023, which is six months. However, the training run duration did not take up this whole period. According to a Meta employee interviewed by Epoch, Llama 2 34B and 70B were trained on different clusters, with overlapping training periods. Based on an estimate of 1000 GPUs, it would have taken 72 days.",NVIDIA A100 SXM4 80 GB,1000,1102561.194
Llama 2-7B,7/18/2023,7000000000,"Llama has been released in 7B, 13B, and 70B variants.",8.40E+22,"Trained on 2 trillion tokens per Table 1. 

C = 6ND = 6 FLOP / token / parameter * 7B parameters * 2T tokens = 8.4e+22 FLOP.

Also, 7B model was trained on 184320 GPU-hours

312 trillion * 184320 GPU-hours * 3600 sec/hour * 0.3 [utilization] = 6.21e22 FLOP",1,,,NVIDIA A100 SXM4 80 GB,,114259.3853
GPT3-2.7B (FlashAttention-2),7/18/2023,2700000000,,,"8x A100 SXMs used, but no indication of dataset size, how long training took, or compute costs",,,,NVIDIA A100 SXM4 80 GB,8,
Claude 2,7/11/2023,,,3.87E+24,https://colab.research.google.com/drive/1MdPuhS4Emaf23VXYZ-ooExDW-5GXZkw0#scrollTo=Ds0Q5X8aMnOY,,,,,,
xTrimoPGLM -100B,7/6/2023,1E+11,"Abstract: ""training xTrimoPGLM at an unprecedented scale of 100 billion
parameters and 1 trillion training tokens""",6.20E+23,"""xTrimoPGLM-100B is trained on a cluster of 96 DGX-A100 GPU (8×40G) servers in FP16 precision from January 18 to June 30, 2023. During this time, xTrimoPGLM-100B has consumed 1 trillion tokens from the dataset consisting of Uniref90 and ColAbFoldDB. As of the current date, xTrimoPGLM-100B continues its pre-training process to pass through as many tokens as possible""

6 * 100 billion params * 1T tokens = 6e23

8*96 * 312 trillion * 163 days * 24 * 3600 * 0.3 ~= 1e24

directly given in the paper (Table 9, or Table 4 in new version): 6.2E+23 ",,3912,163 days,NVIDIA A100 SXM4 40 GB,768,1823415.258
InternLM,7/6/2023,1.04E+11,"""We present InternLM, a multilingual foundational language model with 104B parameters""",9.98E+23,6 * 104b * 1.6T = 9.984e23,,,Training performance for the open-source InternLM-7B: https://github.com/InternLM/InternLM/blob/main/doc/en/train_performance.md,NVIDIA A100 SXM4 80 GB,,1505257.378
Pangu-Weather,7/5/2023,256000000,"4*64 million = 256M params

""We trained four deep networks with lead times (the time difference
between input and output) at 1 h, 3 h, 6 h and 24 h, respectively... 

This modification increases the number of bias parameters by a factor of 527, with each 3D deep network containing approximately 64 million parameters.""",3.98E+22,"""Each of the four deep networks was trained for 100 epochs, and
each of them takes approximately 16 days on a cluster of 192 NVIDIA
Tesla-V100 GPUs.""

192 * 4 * 16 * 24 * 3600 * 125 teraflops * 0.3 utilization = 3.98e22",100,1536,"4*16 = 64 days
""Each of the four deep networks was trained for 100 epochs, andeach of them takes approximately 16 days on a cluster of 192 NVIDIA Tesla-V100 GPUs.""
",NVIDIA V100,192,51279.01752
Stable Diffusion XL (SDXL),7/4/2023,3400000000,"""...result in a model size of 2.6B parameters in the UNet, see Tab. 1. The text encoders have a total size of 817M parameters.""",,,,,,,,
HyenaDNA,6/27/2023,6600000,"Table A.1 shows details, largest experiment is on far right.",1.81E+21,"8 Nvidia A100 (80GB) GPUs, ~4 weeks
(4 * 7 * 24 * 3600) seconds * (8 * 3.12e14) FLOP/sec * 0.3 (utilization) = 1.811e21",679.1171477,672,"""For example, the largest model with context length 1M was trained on 2T tokens over 4 weeks.""",NVIDIA A100,8,5000
ERNIE 3.5,6/27/2023,,,,,,,,,,
RoboCat,6/20/2023,1180000000,"""Most of the experimental results are based on models with a 1.18B-parameter decoder-only transformer (Vaswani et al., 2017) with 24 layers, an embedding size of 2048, and a post-attention feedforward hidden size of 8196."" page 8",,,,,,,,
MusicGen,6/8/2023,3359000000,"""We train autoregressive transformer models at different sizes: 300M, 1.5B, 3.3B parameters""

Uses EnCodec 32kHz (HF version has 59M params) for audio tokenization.",,"We train the 300M, 1.5B and 3.3B parameter models, using respectively 32, 64 and 96 GPUs, with mixed precision.

Unclear how many epochs used so FLOP calculation is not feasible.",,,,,,
LTM-1,6/6/2023,,,,"Must be below 1e23 FLOP, as it's trained with a single A100.",,,,,,
PaLI-X,5/29/2023,55000000000,55B (table 1),,,,,,,,
Goat-7B,5/23/2023,7000000000,7B,,2.78e+22 for base LLaMA-7B,1,,,NVIDIA A10 PCIe,,
CodeT5+,5/20/2023,16000000000,"""We implemented a family of CodeT5+ models, with model sizes ranging from 220M to 16B""",,,10.8,,,NVIDIA A100,,
ONE-PEACE,5/18/2023,4000000000,"""we propose ONE-PEACE, a model with 4B parameters""",1.80E+20,"4 billion params * 7.5 billion data * 6 = 1.8e20.

see training dataset size notes. this estimate required some more assumptions than usual.",4.7,,,,,
LIMA,5/18/2023,65000000000,"""We train LIMA (Less Is More for Alignment) using the following protocol. Starting from LLaMa 65B [Touvron et al., 2023], we fine-tune on our 1,000-example alignment training set,"" according to page 4 of https://arxiv.org/pdf/2305.11206.",5.50E+23,"Finetune:  4.39e18 FLOP
Base model: 5.5e+23 FLOP 
Total: 4.39e18+5.5e+23=5.5e+23 FLOP",15,,,,,
CoEdiT-xxl,5/17/2023,11000000000,11B,,finetuned from Flan-T5,5,,,NVIDIA A100,,
Med-PaLM 2,5/16/2023,3.4E+11,from PaLM 2,,,,,,,,
InstructBLIP,5/11/2023,13000000000,13B form 2.6,1.94E+20,"""All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.""
16 * 3.12e14 * 1.5 * 24 * 3600 * 0.3 = 1.94e20",,36,"""All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.""",NVIDIA A100 SXM4 40 GB,16,
PaLM 2,5/10/2023,3.4E+11,"Model Architecture: ""PaLM-2 is a new state-of-the-art language model. We have small, medium, and large variants that use stacked layers based on the Transformer architecture, with varying parameters depending on model size. Further details of model size and architecture are withheld from external publication.""
However, the parameter count was leaked to CNBC: https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",7.34E+24,"Compute Requirements ""Not reported.""
Paper suggests heuristic of  C=6ND. Based on 340B parameters and 3.6T tokens, training compute would be around 7.3*10^24 FLOP.",,,,Google TPU v4,,4865570.064
StarCoder,5/9/2023,15500000000,"""We trained a 15.5B parameter model""",8.46E+22,"FLOP reported here, 8.46e22
https://huggingface.co/bigcode/starcoder


""We trained our model on a GPU cluster with 512 A100 80 GB GPUs... Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU... The fine-tuned model adds 3.5% of training time""

320256 * 312 tFLOP/s * 3600 * 1.035 * 0.3 (utilization assumption) = 1.12e23",1,625.5,"625.5 hours = 320256 /512
512 GPUs from ""We trained our model on a GPU cluster with 512 A100 80 GB GPUs ""

320256 GPU hours from ""Based on the total number of GPU hours that training took (320,256)""
citations from sections 5.6 and 5.7",NVIDIA A100 SXM4 80 GB,512,212217.6508
ImageBind,5/9/2023,932000000,used ViT-Huge 630M as an image/video encoder and OpenCLIP-302m as text encoder,,,64,,,"NVIDIA V100,NVIDIA A100",,
Agile Soccer Robot,4/26/2023,,,,,,240,"14+158+68 hours:
""Training the get-up and soccer teachers took 14 and 158 hours (6.5 days), respectively, and distillation and self-play
took 68 hours (see Appendix B for details)""",,,
LLaVA,4/17/2023,13000000000,13B,7.80E+22,"8 * 3.12e14 * (18 * 3600) * 0.3 = 4.9e19
num gpus * peak flops * time *assumed utilization rate 
""We train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours."" so 18 hours of time in total.

However, they use Vicuna as their LLM backbone, which used 7.8e22 FLOPs in training. Total FLOPs are then 4.9e19 + 7.8e22 = 7.8049e22",,10,"""We train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.""",NVIDIA A100,8,42.46267261
DINOv2,4/14/2023,1140000000,1.14B from https://huggingface.co/facebook/dinov2-giant,7.42E+21,"table 14

22016 * 3600 * 312 * 10 ** 12 * 3/10 = 7.41851136e+21
gpu hours in seconds * flops of A100 * assumed utilization  rate",,,,NVIDIA A100 SXM4 40 GB,,10203.60518
Incoder-6.7B,4/9/2023,6700000000,6.7B,3.00E+21,"per table 5, required 3 zettaflop (3e21) to train.

also, ""INCODER-6.7B was trained on 248 V100 GPUs for 24 days""

hardware method: 125 trillion * 248 * 24 * 24 * 3600 * 0.3 = 2e22. suggests their utilization was quite low, or 24 days was just calendar time.
",1,576,24,NVIDIA V100,,3129.077137
Segment Anything Model,4/5/2023,636000000,"From Facebook website: https://segment-anything.com/
""How big is the model? The image encoder has 632M parameters.
The prompt encoder and mask decoder have 4M parameters.""",7.80E+21,"""SAM was trained on 256 A100 GPUS for 68 hours. We acknowledge the environmental impact and cost of training
large scale models. The environmental impact of training the released SAM model is approximately 6963 kWh""

68*256 A100-hours = 
17408 hours * 3600 * 312 trillion * 0.4 (utilization assumption for image models)
= 7.82e21

max A100 power is 400W. 6,963,000 watt-hours / 400 watts = 17407.5 hours (so they probably just calculated backwards from power rating, and this doesn't give any info on utilization)",2,68,"""SAM was trained on 256 A100 GPUS for 68 hours""",NVIDIA A100,256,15888.41123
BloombergGPT,3/30/2023,50558868480,,2.36E+23,"2.36e23 per Table 4

(using our usual hardware method, 512 A100s over 53 days would be 512 * 312 teraFLOP/s * 53 * 24 * 3600 * 0.3 = 2.19e23)",0.8,1270,"""~53 days""",NVIDIA A100,512,369586.1353
VideoMAE V2,3/29/2023,1000000000,1B,9.70E+21,"finetuned on ViT-g (smaller than ViT-G with 1B params)

""It takes more than two weeks to pre-train a ViT-g model with VideoMAE
on 64 A100 GPUs""

64 * 312 trillion * 2 * 7 * 24 * 3600 * 0.4 (utilization assumption) = 9.7e21",1200,336,2 weeks,NVIDIA A100 SXM4 80 GB,64,18339.96928
SigLIP 400M,3/27/2023,400000000,Table 3,4.95E+21,"Operation Counting: 
6ND = 6 FLOP / token / parameter*400*10^6 parameters * 6705000000000 tokens [see Dataset size notes] = 1.6092e+22 FLOP

Hardware:
275000000000000 FLOP/s/GPU * 32 GPUs * 120 hours * 3600 sec / hour * 0.4 = 1.52064e+21 FLOPs

geometric mean (1.6092e+22, 1.52064e+21) = 4.9467301e+21",,120,5 days = 120 hours,Google TPU v4,32,
Firefly,3/21/2023,,,,,,,,,,
PanGu-Σ,3/20/2023,1.085E+12,"""In this work, we present PanGu-Σ , a large language model with sparse architecture containing 1.085 trillion parameters.""",4.67E+23,"It has sparse architecture, so we can't use C=6ND.
""We develop PanGu-Σ model under the framework of MindSpore and train it on a cluster with only 512 Ascend 910 AI Accelerators with 329 billion tokens over 100 days.""
100 days * 512 processors * 320 teraFLOPS/processor * 33% utilization = 4.67e+23 FLOP
https://www.wolframalpha.com/input?i=100+days+*+512+*+320+terahertz+*+0.33",1.836,2400,"We develop PanGu-Σ model under the framework of MindSpore 5
and train it on a cluster with only 512 Ascend 910 AI Accelerators [28] with 329 billion tokens over 100 days.",Huawei Ascend 910,512,
Gen-2,3/20/2023,,,,,,,,,,
LEP-AD,3/15/2023,3007381000,"Uses ESM-2 3B. Table 2 gives details on the non-ESM layers. The GCN appears to have about 3.31M parameters and the linear layers should have 771k and 3.3M, respectively. So total is ~3.007B",,No indication of the training used here. ESM-2 3B used 3e22.,,,,,,
GPT-4,3/15/2023,,,2.10E+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",2,2280,(Speculative) SemiAnalysis conjectures that GPT-4 training took 90-100 days with utilization of 32-36%.,NVIDIA A100 SXM4 40 GB,25000,40695706.4
Falcon-40B,3/15/2023,40000000000,Model comes in 7B and 40B variants.,2.40E+23,"C = 6ND = 6 * 40B * 1000B = 2.4e+23 FLOP (assuming one epoch)

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

2,800 petaflop-days * 1e15 * 24 * 3600 = 2.4192e+23 FLOPs",,1440,"""Falcon-40B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances.""
""Training started in December 2022 and took two months.""",NVIDIA A100,384,319783.1572
Claude,3/14/2023,,,,,,,,,,
PaLM-E,3/6/2023,5.62E+11,562B,,"Based on Palm-540B and ViT-22B and then trained on robotics data.

",,,,,,
AudioGen,3/5/2023,1000000000,"""We trained two sets of ALMs, one with 285M parameters (base) and the other with 1B parameters (large).""",9.50E+21,"""the large model was trained on 128 A100 GPUs for 200k steps (∼1 week)""
A100s are 312 teraflop/s
128 * 312 trillion * 7 * 24 * 3600 * 0.3 (utilization assumption) = 7.2e21

Text encoding uses T5-Large, which used 2.3e21 FLOP in pre-training per Flan paper: https://arxiv.org/abs/2210.11416 ",,168,1 week,NVIDIA A100,,9429.740911
DiT-XL/2,3/2/2023,675000000,"675M

Table 4: ""Parameter and flop counts exclude the VAE model which contains 84M parameters across the encoder and decoder.""
675e6 not including VAE (likely frozen), 759e6 including VAE",6.00E+20,"~6e20, based on eyeballing Figure 9. It's between 1e11 and 1e12 gigaflop (1 gigaflop = 1e9 flop), and about 80% of the way towards 1e12 on a log scale. 10^0.8 is about 6. 

3M iterations with a batch size of 256.

""Compute. We implement all models in JAX [1] and train
them using TPU-v3 pods. DiT-XL/2, our most computeintensive model, trains at roughly 5.7 iterations/second on a
TPU v3-256 pod with a global batch size of 256""
256*123000000000000 FLOPs/s * 800000 training steps / 5.7 iterations/second * 0.3 = 1.3258105e+21",,,,Google TPU v3,,111048.1961
LLaMA-65B,2/24/2023,65200000000,"Model card, table 1: https://github.com/facebookresearch/llama/blob/53011c3d7946dadb8274a4c5c7586ab54edf792d/MODEL_CARD.md",5.50E+23,"1.4e12 tokens * 6.52e10 parameters * 6 FLOP/token/parameter = 5.5e23 FLOP

Compared to 2048 A100 GPUs each with 311.84 TFLOPS maximum performance for 21 days, this implies 47% utilization.
https://www.wolframalpha.com/input?i=5.5*10%5E23+FLOP+%2F+%282048+*+311.84+teraFLOPS+*+21+days%29",1.09,500,"""When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days.""",NVIDIA A100,2048,578026.3043
BASIC-L + Lion,2/13/2023,3070000000,parameter count of original BASIC-L,,"This model is BASIC-L retrained with a different optimizer, Lion. Lion seems more compute-efficient, so we should expect compute to be less than BASIC-L.",,,,,,
ViT-22B,2/10/2023,21743000000,"21.743B, Table 1",1.93E+23,"""ViT-22B was trained using 256 visual tokens per image, where each token represents a 14 × 14 patch extracted from 224 × 224 sized images. ViT-22B is trained for 177k steps with batch size of 65k: approximately 3 epochs""

""ViT-22B was trained on 1024 TPU V4 chips for 177K steps""

""Using these techniques, ViT-22B processes 1.15k tokens per second per core during training (forward and backward pass) on TPUv4 [...] ViT-22B’s model flops utilization (MFU) is 54.9%""

256 * 177k * 65k = 2.945T tokens

So training time is 2.945T tokens / (1.15k * 2 * 1024) tokens/s = 1.25M seconds = 347.4 hours

So 1024 TPUv4 chips for 1.25M seconds at 54.9% MFU:
1024 * 2.75e14 * 1.25M * 0.549 = 1.93248e23",2.9,347.4,"""Using these techniques, ViT-22B processes 1.15k tokens per second per core during training (forward and backward pass)""
From model card we know they trained with 1024 TPUv4 chips, and there are 2 cores per chip. Total number of tokens was 177K steps * 65k images/step * 256 tokens/image = 2.945T tokens

So training time is 2.945T tokens / (1.15k * 2 * 1024) tokens/s = 1.25M seconds = 347.4 hours",Google TPU v4,1024,285555.5702
ProteinDT,2/9/2023,,,,,,,,,,
Gen-1,2/6/2023,,,,,,,,,,
Flan T5-XXL + BLIP-2,1/30/2023,12100000000,"12.1B, per Table 2. 

only 108M trainable params (i.e. params trained during the BLIP process)",,"fine-tuned from Flan-T5 XXL (11B) and ViT-g

fine-tuning compute:

""using a single 16-A100(40G) machine, our largest model with
ViT-g and FlanT5-XXL requires less than 6 days for the first
stage and less than 3 days for the second stage.""

16 * 9 days * 24 * 3600 * 312 teraflops * 0.3 ~= 1.2e21",,200,"""less than 6 days for the first
stage and less than 3 days for the second stage""
9*24 is 216, rounding down a bit is 200 hours",NVIDIA A100 SXM4 40 GB,,99690.24664
BLIP-2 (Q-Former),1/30/2023,1480000000,"Q-Former has 188M params. The BLIP-2 system overall has ""54x fewer trainable parameters"" than Flamingo80B.",1.20E+21,https://www.wolframalpha.com/input?i=312+teraFLOPS+*+16+*+200+hours+*+0.33,,200,"""For example, using
a single 16-A100(40G) machine, our largest model with
ViT-g and FlanT5-XXL requires less than 6 days for the first
stage and less than 3 days for the second stage.""
9 days = 216 hours",NVIDIA A100 SXM4 40 GB,16,1960.822538
DDPM-IP (CelebA),1/27/2023,295000000,"295M for CelebA model, per Table 9",3.50E+20,"""We use Pytorch 1.8 (Paszke et al., 2019) and trained all the models on different NVIDIA Tesla V100s (16G memory). In
more detail, we use 2 GPUs to train the models on CIFAR10 for 2 days, and 4 GPUs to train the models on ImageNet 32×32
for 34 days. For LSUN tower 64×64, CelebA 64×64 and FFHQ 128×128, we used 16 GPUs to train the models for 3 days,
5 days and 4 days, respectively""

5*16 V100-days for CelebA.

5 * 16 * 24 * 3600 * 125 teraflops * 0.4 ~= 3.5e20",681,120,5 days,NVIDIA V100,,390.4861318
MusicLM,1/26/2023,860000000,"""We use decoder-only Transformers for modeling the semantic stage and the acoustic stages of AudioLM. The models
share the same architecture, composed of 24 layers, 16 attention heads, an embedding dimension of 1024, feed-forward
layers of dimensionality 4096, dropout of 0.1, and relative
positional embeddings (Raffel et al., 2020), resulting in
430M parameters per stage.""

""stage"" seems to mean semantic + acoustic, so 860M total",,,,,,,,
Ankh_large,1/16/2023,1900000000,"Figure 1 indicates 1.15B parameters, but both the huggingface model and a replication (https://huggingface.co/ElnaggarLab/ankh-large and https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf) indicate 1.9B parameters.
Notebook for counting params: https://colab.research.google.com/drive/1EGI5_vDl4pOBUukJexMHQR16BFKJe4a5?usp=sharing",6.50E+21,"Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf

Can also be manually estimated based on the details in Table 11 and 4.6.1 Exp 4. 14B residues * 68 epochs = 952B tokens seen in forward passes. However, only 20% of tokens are masked as individual targets; other tokens in consecutive spans are collapsed into single-token targets to reduce computations. For masking rate of 20%, the average sequence will have 36% as many targets as input tokens under this strategy. This is the relevant number of backward passes:
(2 * 952B * 19B) + (4 * 952B * 0.36 * 19B) = 6.22e22

36% figure verified here: https://colab.research.google.com/drive/1ETsmp_KRMK8kIRA5kdfcO9QiPK28cBQ6?usp=sharing ",68,,,Google TPU v4,64,4802.398249
Nucleotide Transformer,1/15/2023,2500000000,"""We built four distinct foundation language models of different sizes, ranging from 500M up to 2.5B parameters""",8.08E+21,"""Training the largest parameter model required a total of 128 GPUs across 16 compute nodes for 28 days""

In repo, they default to jnp.float32, but recommend fp16 or bp16 for activations in the docstring. JAX defaults to TF32 so they should be utilizing tensor cores.

Assuming 1.56e14 FLOP/s for 32-bit calculations and 0.3 utilization rate

Estimate: 1.56e14 FLOP/s * 128 GPUs * 28 days * 24 h/day * 3600 s/h * 0.3 utilization rate = 1.45e22

Or, with 6ND:
""the model processed a total of 300B tokens during training""
6 * 300B * 2.5B = 4.5e21

Geometric mean: sqrt(1.45e22 * 4.5e21) = 8.08e21",1,672,"""Training the largest parameter model required a total of 128 GPUs across 16 compute nodes for 28 days""",NVIDIA A100,128,51064.70621
DreamerV3,1/10/2023,200000000,Table B1,2.20E+20,"16 environment instances, each with 1 V100 running for 17 days (table A1) - it's not entirely clear if the GPU days already account for multiple environment instances.

Assuming no:
Compute: 17*24*60*60*125000000000000*0.3=5.508e+19
Assuming yes:
Compute: 17*24*60*60*16*125000000000000*0.3=8.8128e+20

Geometric mean: 220320000000000000000",,6528,,NVIDIA V100,16,
VALL-E,1/5/2023,353000000,"""Both the AR model and the NAR model have the same transformer architecture with 12
layers, 16 attention heads, an embedding dimension of 1024, a feed-forward layer dimension of 4096, and a dropout of 0.1""

Ben's script says that's 353M parameters, using n_block 12, d_model 1024, d_ff 4096, encoder only False

https://github.com/bencottier/ml-parameter-count/blob/main/parameter_count.py",1.01E+19,"""The models are trained using 16 NVIDIA TESLA V100 32GB GPUs with a batch size of 6k acoustic
tokens per GPU for 800k steps""

353M * 800k * 6k * 6 = 1.01e19

16 V100s is 2080 teraFLOP or 2e15 FLOP so 1e19 would take 1.5 hours at 100% utilization or ~5 hours at 30%. Is that plausible?",,,,NVIDIA V100,,11.40575196
Hybrid H3-2.7B,12/28/2022,2700000000,2.7B,6.48E+21,"6 FLOP/token/parameter * 400000000000 training tokens * 2700000000 parameters = 6.48e+21 FLOP

___________________
in the algorithmic progress paper the estimation was 8.49 × 10^20 based on the assumption of WT-103 dataset and 509 epochs",509.02,,"""All models were trained on either a single 16xA100-40GB node or a cluster of 8xA100-80GB nodes.""",NVIDIA A100 SXM4 80 GB,8,
CaLM,12/19/2022,86000000,"""We trained a large language model with 86M parameters""",2.90E+19,"""4 NVIDIA Quadro RTX4000 GPUs for 40 days""

Calculation assuming FP32, utilization 30%:
= (40 * 24 * 3600) s * 7.1e12 FLOP/s * 0.3 * 4 GPU = 2.999808e+19

alternative calculation:
""Gradients were accumulated to an effective batch size of 1,000 examples, or approximately 256,000 tokens. ""
""(66,000 gradient steps, 14 full epochs)""

256000*66000*14*86000000*6=1.220567e+20",14,960,"""The model reported in this work was trained on 4 NVIDIA Quadro
RTX4000 GPUs for 40 days (66,000 gradient steps, 14 full epochs)""",NVIDIA Quadro RTX 4000,4,
RT-1,12/13/2022,35000000,"""we also limit the size of the model compared to
the original publication, which was 1.2B parameters (resulting in on robot inference time of 1.9s),
to be of similar size to RT-1 (37M parameters for Gato vs. 35M for RT-1""

16M params for image tokenizer, 19M for the transformer",,,,,,,,
TranceptEve,12/10/2022,,,,,,,,,,
Vega v2,12/4/2022,6000000000,,7.76E+22,"Pretraining took 1 month on 320 A100 (Section 3.1)
720*60*60*320*312000000000000*0.3=7.7635584e+22",,720,,NVIDIA A100,320,
DeepNash,12/1/2022,,,,"""The final agent was trained using 768 MXU’s (matrix multiplication unit) for Learners and
256 MXU’s for Actors (using 256 TPU’s in total).""
Some more details in Table S1 (in supplementary materials)",,,,,,
GPT-3.5 Turbo,11/30/2022,20000000000,20B parameters according to Table 1 in Microsoft's CODEFUSION paper: https://arxiv.org/pdf/2310.17680.pdf,,,,,,,,
GPT-3.5,11/28/2022,,"Parameter count may be 175B based on OpenAI's statements that text-davinci-003 is in the GPT-3.5 series of models. It was also stated to be 175B in the Microsoft CODEFUSION paper, but the paper was reportedly retracted because the authors did not know the parameter count.",2.58E+24,https://colab.research.google.com/drive/1QSxa8YCWjEBQU7mrXLhw6TP1VX5oqgdW#scrollTo=Gt6Z6oZ26clI,,,,NVIDIA A100 SXM4 40 GB,,4637986.173
DiT-XL/2 + Discriminator Guidance,11/28/2022,,,,"This is a finetune of DiT-XL/2, so its compute won't be much higher.",10,,,NVIDIA A100,,
Discriminator Guidance,11/28/2022,,,2.16E+20,481 hours * 312 TFLOPS (A100) * 40% utilization,10,481,Table 6,NVIDIA A100 PCIe,,337.8811363
ALM 1.0,11/28/2022,335000000,335M parameters: https://github.com/FlagAI-Open/FlagAI/blob/master/examples/ALM/README.md,,,,,,,,
CICERO,11/22/2022,,"""We took R2C2 (22) as our base model – a 2.7B parameter Transformer-based (23) encoder-decoder model pre-trained on text from the Internet using a BART de-noising objective (24).""",,,,,,,,
AR-LDM,11/20/2022,1500000000,Table 1,5.10E+20,8 NVIDIA A100 GPUs for 8 days,50,194,8 NVIDIA A100 GPUs for 8 days,NVIDIA A100,,745.8360576
Fusion in Encoder,11/18/2022,330000000,330M,1.30E+20,"""The experiments were run on 8x80GB Nvidia A100s with 800GB RAM and 4x32-core CPUs, and each experiment took around 1 day for NQ and 2 days for TriviaQA with large models. Inference was run on the same system, and took 2 minutes.""

2 days * 24 * 3600 * 8 * 312 teraflop/s * 0.3 utilization = 1.3e20",,48,2 days,NVIDIA A100 SXM4 80 GB,,233.0630322
Galactica,11/16/2022,1.2E+11,"""The largest 120B model we train runs on a single NVIDIA A100 node""",3.24E+23,"Authors state the model is trained on 450b tokens. Using 6 FLOP/token/parameter, this is 6*120b*450b = 3.24e23",4,,,NVIDIA A100 SXM4 80 GB,128,591076.8944
EVA-01,11/14/2022,1011000000,1011M from table 3,1.50E+22,"flops = (128) * (3.12e14) * (14.5 * 24 * 3600) * (0.3) = 1.501e22
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from Table 3, time and num gpus, GPU model is on page 4 (A100), precision is fp16 and likely utilizes tensor cores",150,348,from Table 3 14.5 days = 348 hours,NVIDIA A100 SXM4 40 GB,128,29374.46909
AltCLIP_M9,11/12/2022,,,,,10,,,,,
InternImage,11/10/2022,1080000000,"1.08B, table 1",2.41E+21,"InternImage-H is pre-trained on a 427 million joint dataset of public Laion-400M [61], YFCC-15M [62], and CC12M [63] for 30 epochs, and then fine-tuned the model on ImageNet-1K for 20 epochs. ImageNet-1K has 1,281,167 images.

Table 2 says InternImage-H uses 188 GFLOP per forward pass at 224 resolution, and 1478 GFLOP at 640

Table 7 indicates training InternImage-H was done at a scale of ""224/640"" so presumably there was pretraining at 224x224 resolution and then some fine-tuning at 640x640. It's not clear how much training was done at each resolution, but typically this is a small fraction of total training (e.g. Noisy Student finds it sufficient to train for 350 epochs at smaller resolution, and then fine-tune at the higher resolution for 1.5 epochs). We'll ignore the additional FLOPs from high resolution training.

Total training FLOPs:
188e9 FLOP/image * (427M images * 30 epochs) + (1.281M images * 20 epochs) * 3 (additional FLOPs for backward pass) = 2.408e21",30,,,,,
mT0-13B,11/3/2022,13000000000,13B,,"fine-tuned from mT5

1.37e22 fine-tune compute",,,,,,
Mogrifier RLSTM (WT2),11/3/2022,35000000,Table 1,1.40E+17,6ND = 6*35000000*2666667*250 = 1.4000002e+17,250,,,,,
BLOOMZ-176B,11/3/2022,1.76E+11,176B,,"fine-tuned from BLOOM-176B

1.37e22 fine-tune compute",,,,,,
eDiff-I,11/2/2022,9100000000,"9.1B for config D, Table 1",5.46E+19,"6ND = 6*9100000000*1000000000=5.46e+19 (likely, might change because of several epochs / dataset division)

""The base model was trained using 256 NVIDIA A100 GPUs, while the two super-resolution models were trained with 128 NVIDIA A100 GPUs each"" 
no info on duration",,,,NVIDIA A100,,
EnCodec,10/24/2022,,,,"""We train all models for 300 epochs, with one epoch being 2,000 updates with the Adam optimizer with a batch size of 64 examples of 1 second each, a learning rate of 3 · 10−4 , β1 = 0.5, and β2 = 0.9. All the models are traind using 8 A100 GPUs""",300,,,NVIDIA A100,,
U-PaLM (540B),10/20/2022,5.4E+11,,2.53E+24,"""The total number of extra tokens we train on for the 540B
model is approximately 1.3 Billion which constitutes 0.16% extra computation... Training an U-PaLM 540B model only consumes 512 TPUv4 chips and finishes in about 5 days which is considered to be lightweight.""

original PaLM was 2.527e+24. adding 0.16% is ~2.53e24",,120,5 days,Google TPU v4,512,
LMSI-Palm,10/20/2022,5.4E+11,540B,,"(fine-tuned from Palm-540B, which was 2.52e24)",,,,,,
Flan-T5 11B,10/20/2022,11000000000,11B,3.30E+22,"Table 2: 0.2% greater than T5 xxl, which used 3.3e22 FLOP",,,,Google TPU v4,,98374.29476
Flan-PaLM 540B,10/20/2022,5.4E+11,540B,2.50E+24,"0.2% greater than Palm 540B, which used 2.5e24",,37,"""we only use 0.2% of the pre-training compute to instruction-finetune Flan-PaLM 540B (approximately 512 v4 TPU chips for 37 hours)""",Google TPU v4,512,
GenSLM,10/11/2022,25000000000,See Table 3,1.42E+21,"See Table 3
Overall ZettaFlops 1.42",,,,,,
Diplodocus,10/11/2022,,may be estimated from https://github.com/facebookresearch/diplomacy_cicero?tab=readme-ov-file,,,,,,,,
Phenaki,10/5/2022,1800000000,"Unless specified otherwise, we train a 1.8B parameter Phenaki model on a corpus of ∼15M textvideo pairs at 8 FPS mixed with ∼50M text-images plus ∼400M pairs of LAION-400M [41] (more
details in Appendix B.3). The model used in the visualisations in this paper was trained for 1 million
steps at a batch size of 512, which took less than 5 days. In this setup 80% of the training data came
from the video dataset and each image dataset contributed 10%.",,,,,,,,
DiffDock,10/4/2022,20240000,"""For determining the hyperparameters of DIFFDOCK’s score model, we trained
smaller models (3.97 million parameters) that fit into 48GB of GPU RAM before scaling it up to the final model (20.24 million parameters) that was trained on four 48GB GPUs""

There's a separate 4.77M ""confidence model"" that helps make predictions along with the score model",7.20E+19,"""We trained our final score model on four 48GB RTX A6000 GPUs for 850 epochs (around 18 days).""

4 * 38.7 teraflops * 18 days * 24 * 3600 * 0.3 = 7.2e19

https://www.techpowerup.com/gpu-specs/rtx-a6000.c3686",850,432,18 days,NVIDIA RTX A6000,,
Make-A-Video,9/29/2022,,,,,,,,,,
Whisper,9/21/2022,1550000000,Table 1,4.21E+21,See figure 9,3,,,,,
PaLI,9/14/2022,16900000000,"3.9b Image Encoder, 
14b Multimodal Encoder-Decoder",1.69E+23,"Pre-training the ViT component involved 1.1 million steps (they train over 1M steps but run the last 100k twice and then average the two resulting models). Batch size is 16384 and the inputs are 224x224. Table 8 indicates a forward pass with ViT-e/14 on a 224 image takes 1980 GFLOPs, so total training compute for the ViT-e/14 model is:
1980e9 * 16384 * 1.1 million * 3 (account for backward passes) = 1.07e23

In the ""overal model"" section, they then say: ""The largest model, PaLI-17B, is pretrained using 1,024 GCP-TPUv4 chips for 7 days"". It is then trained for another 3 days on 512 chips at higher resolution. 

I assume the stated TPUv4 training does not include the ViT pretraining, since it amounts to fewer FLOPs than we estimate above for the ViT.

275 teraFLOP/s * ((1024 * 7) + (512 * 3)) * 24 * 3600 * 0.3 (utilization assumption) = 6.2e22

Total: 1.07e23 + 6.2e22 = 1.69e23",1,240,10,Google TPU v4,1024,50878.10777
BEIT-3,8/22/2022,1900000000,1.9B from Table 2,7.00E+19,"from Table 11, 1M training steps with batch size 6144. 
From Table 2 we have that model have 1.9B parameters.
Model is VIT",,,,,,
BlenderBot 3,8/10/2022,1.75E+11,,4.30E+23,(taken from OPT-175 base),,,,NVIDIA A100 SXM4 40 GB,128,
GLM-130B,8/4/2022,1.3E+11,Dense model,3.55E+23,"""96 NVIDIA A100 (40G * 8) servers for 2 months""

312 TFLOPS/GPU * 96 servers * 8 GPU/server * 2 months * 32.5% utilization = 4.037e23

utilization rate - citation from the paper: ""we report hardware FLOPs utilization (HFU) of 43.3% and model FLOPs utilization (MFU) of 32.5% due to re-materialization.""

Aligns pretty well with 6ND:
6 * 400B * 130B = 3.12E23

Geometric mean: sqrt(4.037e23 * 3.12e23) = 3.549e23",1,1440,"""During the 60-day access to the cluster, we manage to train GLM-130B for 400 billion tokens""
60 days * 24 = 1,440 hours",NVIDIA A100 SXM4 40 GB,768,820296.6313
AlexaTM 20B,8/2/2022,19750000000,See Table 1 on p.3 of the paper,2.04E+23,"Training throughput is reported as 154 TFLOP/s - see p.5 of the paper.
""We relied on an internal and optimized version of DeepSpeed that we have since open-sourced (Chiu & Zheng, 2022) to obtain training throughput of up to 154 TFLOPS/GPU on 16 AWS p4d.24xlarge compute instances.""

Accelerator compute days are reported as 15,360 days - see Table 17 on p.18 of the paper.",,2880,"See p.5 of the paper: ""We trained AlexaTM 20B for 120 days on 128 A100 GPUs...""",NVIDIA A100,128,267943.2113
OmegaPLM,7/22/2022,670000000,"""Our model contains 66 layers with around 670 million parameters without sharing parameters, which doubles the layer count of ESM-1b but roughly retains the parameter count.""",1.04E+22,"""OmegaPLM is implemented in PyTorch (44) and trained for 2,560 GPU Nvidia A100 80G days."" 
""Default precision format in Nvidia A100 GPUs is set to TensorFloat-32 for matrix operations.""

Assume 0.3 utilization for language model

Estimate: (2560 * 24 * 3600) s * 156e12 FLOP/s * 0.3 * = 1.04e22",,,"2,560 GPU Nvidia A100 80G days",NVIDIA A100 SXM4 80 GB,,52400.32119
ESM2-15B,7/21/2022,15000000000,"""we train models up to 15B parameters""",7.35E+22,"from xTrimoPGLM paper Table 9 (https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1): 5.1e22 FLOP

from Arb Research (https://arbresearch.com/files/gen_bio.pdf): ""ESM-2-15B: 270000 updates x 3.2M batch size x 15 B “connections” x 6. : 7.8e22 FLOP

from the paper's Supplementary Materials: 
""We trained each model over 512 NVIDIA V100 GPUs. ESM2 700M took 8 days to train. The 3B parameter LM took 30 days. The 15B model took 60 days.""
60 days x 512 V100s x an imputed 30% utilization"": 1e23 FLOP

Geometric mean: 7.35e22",72,1440,,NVIDIA V100,512,163467.8202
BLOOM-176B,7/11/2022,1.76247E+11,"See ""Technical Specifications"" on Hugging Face:
https://huggingface.co/bigscience/bloom",3.66E+23,"https://bigscience.huggingface.co/blog/bloom Blog post says 117 days.

384 A100 GPUs * 314 TFLOPS throughput per GPU * 117 days * 0.3 (utilization assumption) = 3.65664e23
https://www.wolframalpha.com/input?i=384+*+314+TFLOPS+*+117+days+*+0.3",1,2808,117 days * 24 hours/day,NVIDIA A100 SXM4 80 GB,384,995819.1171
NLLB,7/6/2022,54500000000,"Section 8.2.4: ""The model has a total of 54.5B parameters
and FLOPs similar to that of a 3.3B dense model""",1.75E+22,"Section 8.8:
"" To train NLLB-200, a cumulative
of 51968 GPU hours of computation was performed on hardware of type A100-SXM-80GB""
See also Table 48

Section 8.2.4 states they use FP16

NVIDIA Datasheet states 312TFLOPS for FP16
https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-nvidia-us-2188504-web.pdf

Assuming 0.3 utilization:

312e12*3600*51968*0.3

Also:
""Our final model is a Transformer
encoder-decoder model in which we replace the Feed Forward Network (FFN) layer in
every 4th Transformer block with a Sparsely Gated Mixture of Experts layer containing 128
experts. We use model dimension 2048, FFN dimension 8192, 16 attention heads, 24 encoder
layers and 24 decoder layers. We use Pre-LayerNorm (Xiong et al., 2020) as described in
Section 6.1.1. We share the embedding weights of the encoder input embedding, decoder
input embedding and decoder output embedding layers. We use an overall dropout of 0.3,
attention dropout 0.1 and EOM with peom=0.2. The model has a total of 54.5B parameters
and FLOPs similar to that of a 3.3B dense model.""",,,,NVIDIA A100 SXM4 80 GB,,50667.25034
CodeT5-large,7/5/2022,770000000,"""We pretrain a CodeT5-large model (770M) from scratch following T5-large’s architecture""",2.72E+21,"""We perform our experiments on a kubernetes with 16 A100-40G GPUs on Google Cloud Platform and the total pretraining duration is around 21 days""

16 * 312tFLOP/s * 21 * 24 * 3600 * 0.3 (utilization assumption) = 2.72e21",150,504,21 days,NVIDIA A100,,4478.145684
Minerva (540B),6/29/2022,5.4035E+11,"""To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).""

Our approach is to start with the PaLM pretrained decoder-only transformer language models Chowdhery et al. (2022), and further train (finetune) them on our mathematical dataset using an autoregressive objective.
Table 2 contains the main model and training hyperparameters.

See Table 2",2.74E+24,"Minerva was fine-tuned from PaLM using the same hardware. Assume the same model FLOPs utilization rate for pre-training and fine-tuning.
""the 540B model was trained for 29 days on a v4-1024""

PaLM pretraining time: 6144 TPU for 1200 hours + 3072 TPU for 336 hours = @8404992 TPU-hours
Minerva finetuning time: 1024 TPU for 696 hours = 712704 TPU-hours
So fine-tuning added 8.5% more compute.

Minerva total compute = PaLM pretraining compute * (712704+8404992)/(8404992) = 2.7415*10^24 FLOP
https://www.wolframalpha.com/input?i=%28712704%2B8404992%29%2F%288404992%29+*+2.5272*10%5E24

Palm pretraining: 2.5272e+24",,696,,Google TPU v4,1024,
ProGen2-xlarge,6/27/2022,6400000000,"""We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters""",1.35E+22,"Estimate 1:
""350,000 steps x 1m batch size x 6.4 B “connections” x 6"" - Arb Research (https://arbresearch.com/files/gen_bio.pdf)
Steps and batches from Table 1. 
FLOP estimate: 1.3e22

Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf
FLOP estimate: 1.4e22

Geometric mean = 1.35e22 FLOP",,,,Google TPU v3,,11850.17841
Parti,6/22/2022,20000000000,"Abstract: ""we achieve consistent quality improvements
by scaling the encoder-decoder Transformer model up to 20B parameters""",5.10E+23,"Calculated from architecture. Does not take into account the encoding and decoding of text and images, only the transformer stack.

Table 1 shows for the 20B model
16 encoder layers
64 decoder layers
Dmodel = 4096
Dhidden = 16384
Num heads = 64

Just below table 1:
""We use a maximum length of text tokens of 128, and the length of image tokens are fixed to 1024""

I take the length of the sequence to be 100 for the encoder stack and 1024 for the decoder stack.

Section 3, Training: ""a total
of 450,000 steps and final ratio of 0.025. We use a global batch size of 8192 during training.""

6* 20B parameters * (1024+128) sequence length*450000 steps*8192 batch size= 5.096079e+23",,,,Google TPU v4,,427178.712
CoCa,6/14/2022,2100000000,"""Our largest CoCa model (""CoCa"" in short) follows the ViT-giant setup in [21] with 1B-parameters in the image encoder and 2.1B-parameters altogether with the text decoder""",7.30E+22,"""Pretraining CoCa takes about 5 days on 2,048 CloudTPUv4 chips""

275 teraFLOP/s * 2048 * 5 * 24 * 3600 * 0.3 (assumed utilization) = 7.3e22",6.875,120,5 days,Google TPU v4,2048,78043.37569
MetaLM,6/13/2022,,,,,,,,,,
DITTO,6/6/2022,750000000,"""We train a Transformer model (750M parameters, similar to GPT-2 Large)""

""Specifically, we use a 16-layer Transformer with 8 attention heads, hidden size 1024 and fully-connected dimension 4096.""",3.32E+18,6 FLOP / token / parameter * 160000 steps * 3 samples per batch * 1536 tokens per sample *  750000000 parameters = 3.31776 × 10^18 FLOP,7.158,,,NVIDIA V100,8,
Diffusion-GAN,6/5/2022,,,,"Must be <1e23 FLOP, all experiments were done with 4 or 8 V100s.",,,,NVIDIA V100,,
CogVideo,5/29/2022,9400000000,,,,,,,,,
Tranception,5/27/2022,700000000,"""Our largest transformer model, Tranception L, has 700M parameters and is trained on UniRef100 (Suzek et al., 2014)""",7.24E+21,"Trained using 64 A100 GPUs for two weeks.
64 * 312 teraFLOP/s * 14 days * 24 hours/day * 3600 seconds/hour * 0.3 utilization (assumption)
= 7.24e21",,336,2 weeks,NVIDIA A100,64,15247.43609
Imagen,5/23/2022,7762000000,"2B 64x64 generation model, 600M 64->256 super-resolution model, 400M 256->1024 super-resolution model
Uses encodings from a frozen T5-XXL, which should be included in total parameter count. Loading the model directly, there are 4,762,310,656 parameters in the encoder.
2B + 4.762B + 600M + 400M = 7.762 billion

here they claim it is 3B parameters: https://arxiv.org/pdf/2407.15811",1.46E+22,"256 TPU v4 chips for 64x64, for 4 days
128 TPU v4 chips for 64->256, for 2 days
128 TPU v4 chips for 256->1024, for 2 days

256 TPUs * 275 teraFLOPS/TPU * 4 days + 2 * (128 TPUs * 275 teraFLOPS/TPU * 2 days) * 40% utilization = 1.46e+22 FLOP",,96,4 days,Google TPU v4,256,7915.823806
SimCSE,5/18/2022,,,,,3,,,,,
Gato,5/12/2022,1180000000,"""This section focuses on in-simulation evaluation.
Figure 10 compares the full 1.18B parameter Gato"" p.10",4.02E+21,"256 (16x16x) TPUv3 chips x 123e12 FLOPS/chip x 4 days x 86400 seconds/day * 0.4 utilization = 4.35e21 FLOPs

Similar value by 6NC:
6 * 524288000000 * 1.18B = 3.71e21

Using geometric mean:
sqrt(4.35e21 * 3.71e21) = 4.02e21",,96,4 days,Google TPU v3,256,3523.06498
UL2,5/10/2022,20000000000,Taken from Directory of LLMs,1.20E+23,"Trained on 1T tokens
20B * 1T * 6 = 1.2e23 

Second source: Section 5.1 says model was trained on 512 TPUv4 chips, and took slightly over 1 month
512 * 2.75e14 * 31 * 24 * 3600 * 0.3 = 1.13e23",,744,"around 31 days from 'Pre-training took approximately slight more than one month for about 1 trillion
tokens.' from section 5.1
so around 31*24 = 744
",Google TPU v4,512,126785.762
DeBERTaV3large + KEAR,5/4/2022,418000000,"DeBERTaV3-large had 418M params, per Table 2",,this is a fine-tuned version of DeBERTaV3-large,10,,,,,
OPT-175B,5/2/2022,1.75E+11,"""In line with Meta AI’s commitment to open science, we are sharing Open Pretrained Transformer (OPT-175B), a language model with 175 billion parameters trained on publicly available data sets""",4.30E+23,"https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/final_update.md

""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute""",1.6667,793.5,"4.3*10^23 FLOP / (147 TFLOPS) = 813000 A100-hours
https://www.wolframalpha.com/input?i=4.3*10%5E23+FLOP+%2F+%28147+TFLOPS%29

""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute, or roughly ~33 days of continuous training on 1024 80GB A100s (assuming no hardware issues, no numerical instabilities, etc.).""",NVIDIA A100 SXM4 80 GB,1024,733634.637
Flamingo,4/29/2022,80000000000,"""We obtain three models, Flamingo-3B, Flamingo-9B and Flamingo-80B""

"" The Flamingo-80B model builds on top of the frozen Chinchilla 70B language model [42]. Starting from the very first layer and before every seventh transformer blocks, we add a GATED XATTN-DENSE layer attending to the visual inputs; this accounts for 10B additional learned parameters. For simplicity, we refer to this model as simply Flamingo throughout the paper""",2.19E+23,"1536 TPU v4 chips for 15 days. Assuming 40% utilization:
C = 1536 TPU * 275*10^12 FLOP/s/TPU * 15 day * 86400 s/day * 0.40 = 2.2*10^23 FLOP

""All training and evaluation was performed on TPUv4 instances. The largest model containing 80 billion parameters is trained on QUSV chips for 15 days and sharded across 16 devices.""

""All trained parameters and optimizer accumulators are stored and updated in float32; all activations and gradients are computed in bfloat16 after downcasting of parameters from float32 to bfloat16""",,360,1536 TPU v4 chips for 15 days,Google TPU v4,1536,183423.1633
Sparse all-MLP,4/14/2022,9410000000,"Table 2: ""In Section 4.4, we run our large model (9.41B parameters)""",5.32E+20,"112 hours on 32 V100 GPUs
assumed 0.33 util rate

112 hours *3600 seconds / hour *0.33 utilization *32 gpus *125000000000000 FLOPs=532224000000000000000
",,112,,NVIDIA V100,,
Stable Diffusion (LDM-KL-8-G),4/13/2022,1450000000,See Table 2,5.00E+22,"""I get 5e22 FLOP. 150k hours on A100 [1] gives 150*10^3 hours * 3600 seconds/hour * 3.12E+14 peak performance of A100 * 0.33 utilisation = 5e22  FLOP""

[1] https://twitter.com/EMostaque/status/1563870674111832066",,585.9375,"total chip-hours divided by number of GPUs
150k/256",NVIDIA A100,256,111248.217
BERT-RBP,4/7/2022,110000000,"Base model is BERT base (110M parameters), pre-trained on human reference genome (DNABert: https://academic.oup.com/bioinformatics/article/37/15/2112/6128680)",1.40E+20,"See DNABert entry:

""Since the pre-training of DNABERT model is resource-intensive (about 25 days on 8 NVIDIA 2080Ti GPUs)""

Assuming FP16 and 30% utilization

Calculation = (25 * 24 *3600) s * 2.7e13 FLOP/s per GPU * 8 GPUs * 0.3 utilization = 1.4e20 FLOP",3,,,,,
DALL·E 2,4/6/2022,3500000000,"""Our decoder architecture is the 3.5 billion parameter GLIDE model""",3.37E+23,"Decoder architecture is similar to Imagen (1.46E+22), but trained on 1.6e9 datapoints (Table 3) rather than Imagen's 5.1e9 datapoints.

DALL-E 2 uses two models as priors. I estimate the prior model's FLOP as 6*N*D = 6 * 1e9 * 4096 * 1e6 = 2.5e19 FLOP. However, this seems low compared to CLIP.

So it may be possible to estimate DALL-E 2's compute by analogy to Imagen, but there is a lot of uncertainty and more research would be needed.

here (https://arxiv.org/pdf/2407.15811) they claim the DALL-E.2 model was trained on the equivalent of 5208.3 days on 8*A100 GPUs:

312000000000000 FLOP / sec / GPU * 8 GPUs * 5208.3 days * 24 hours / day * 3600 sec / hour * 0.3 [assumed utilization] = 3.3695784e+23 FLOP",,,,,,
PaLM (540B),4/4/2022,5.4035E+11,"""To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).""",2.53E+24,"See Table 20.

6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.
Equivalent to 6144 TPUv4 for 1368 hours.

46.2% model FLOPs utilization

""The 540B-parameter PaLM model sustained a remarkable 57.8% of the peak hardware floating point performance over 50 days while training on TPU v4 supercomputers. "" https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains",1,1536,"6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.

Equivalent to 6144 TPUv4 for 1368 hours.",Google TPU v4,6144,2945949.763
Chinchilla,3/29/2022,70000000000,"""We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4× more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.""",5.76E+23,"""Both Chinchilla and Gopher have been trained for the same number of FLOPs but differ in the size of the model and the number of training tokens.""

We see the number of flops in table 3",1,,,"Google TPU v4,Google TPU v3",,
"Segatron-XL large, M=384 + HCP",3/21/2022,257000000,"257M
(Table 1)

18 layers, 16 heads, hidden size
1024 batch size 128.",2.65E+19,6 FLOP / token / parameter * 257000000 parameters * 384 tokens/batch × 128 [batch size] * 350000 steps = 2.6527334e+19 FLOP,167.02,,,,,
ViT-G (model soup),3/10/2022,1843000000,This is from the original ViT-G paper,3.40E+21,"This is a fine-tuned version of ViT-G, which required 3.4e21 to train per PCD/Akronomicon.

Fine-tuning compute is likely minor in comparision:
""Models are fine-tuned at a batch size of 512 for either 10,000 or 20,000 steps (approximately 4 or 8 epochs)... all models are fine-tuned at 518 × 518 resolution""
At 20k steps, we have (518^2) * 512 * 20k = 2.75e12 pixels seen in fine-tuning, compared to (224^2) * 32768 * 5M = 8.22e15 in pre-training.",8,,,,,
MegaSyn,3/7/2022,,"model details here: https://chemrxiv.org/engage/chemrxiv/article-details/61551803d1fc335b7cf8fd45

""The variational autoencoder utilizes an encoder-decoder architecture to map chemical space into a latent vector 34. The encoder is composed of 3 LSTM layers of 512 units each followed by a linear layer of 64 units (the latent space).
Our decoder is comprised of 3 LSTM layers of 512 units each with dropout of 0.2 between
all layers""",,,,,,,,
Statement Curriculum Learning,3/2/2022,774000000,,,Probably below 1e23 FLOP given the small model size.,,,,,,
DeepNet,3/1/2022,3200000000,"""Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction""

EDIT 05/05/2022: The 12B model was presented in an earlier paper. This paper presents a 3.2B model",,"They show results on par with the original Transformer, so probably less than 2.3e19 FLOP.",,,,,,
PolyCoder,2/26/2022,2700000000,2.7B for largest model,1.10E+21,"""We use GPT-NeoX toolkit 11 to
train the model efficiently in parallel with 8 Nvidia RTX 8000 GPUs on a single machine. The wall
time used to train the largest 2.7B model is about 6 weeks""

8 * 130 TFLOP/s * 6 * 7 * 24 * 3600 * 0.3 (utilization) ~= 1.1e21",,1000,6 weeks,NVIDIA Quadro RTX 8000,,
ST-MoE,2/17/2022,2.69E+11,269B. it's called ST-MoE-32B because it's equivalent to a 32B dense model.,2.90E+23,"The paper claims ""scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder"". If this is true for training cost, then 6*32e9*1.5e12 = 2.9e23",0.84,,,,,
Midjourney V1,2/15/2022,,,,,,,,,,
ProteinBERT,2/10/2022,16000000,"""Altogether, it includes ∼16M trainable parameters, making it substantially smaller than other protein language models""",6.50E+19,"""Pretraining speed on a single GPU (Nvidia Quadro RTX 5000) was 280 protein records per second. We trained the model for 28 days over ∼670M records""

28 * 24 * 3600 * 89 TFLOP/s * 0.3 (assumed utilization) = 6.5e19
https://www.wolframalpha.com/input?i=28+days+*+89+TFLOP%2Fs+*+0.3",6.4,672,28 days,NVIDIA Quadro RTX 5000,1,
LaMDA,2/10/2022,1.37E+11,"""LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters""",3.55E+23,"""The total FLOPS is 56.5% * 123 TFLOPS/s * 1024 chips * 57.7 days
= 3.55E+23""
From https://arxiv.org/pdf/2201.08239.pdf p.18
",,1385,57.7 days * 24,Google TPU v3,1024,229949.9863
GPT-NeoX-20B,2/9/2022,20000000000,,9.32E+22,Trained for 3 months on 96 A100s (according to correspondence with author). Let's say 0.4 utilization rate.,1.4,2160,"see other notes
",NVIDIA A100 SXM4 40 GB,96,184272.8074
RETRO-7B,2/7/2022,7500000000,"""Retro provides a constant gain for models ranging from 150M to 7B parameters, and Retro can be improved at evaluation time by increasing the database size and the number of retrieved neighbours. """,1.68E+22,C=6ND = 6 * 7e9 * 400e9 = 1.7e22 ,,,,,,
AlphaCode,2/2/2022,41100000000,41.1B. Table 3,1.64E+23,"Figure 7 (a) shows a maximum training compute budget of approx 23000 TPU-days per model.
23000 days * 24 h/day * 3600 sec/h * 2.75e14 FLOP/s * 0.3 utilization = 1.64e23 FLOP",,147.2,"Figure 7 (a) shows that the models were trained for around 23000 TPU-days. We know they trained on TPUv4s, and in appendix D.1 they say they have 3750 TPUv4 and TPUv4i. Assuming they trained only on the 3750 TPUv4s, that suggests 23000 / 3750 = 6.13 days, or 147.2 hours.",Google TPU v4,3750,
InstructGPT 175B,1/27/2022,1.75E+11,"""We train three model sizes (1.3B, 6B, and 175B parameters)""",3.19E+23,"""training our 175B PPO-ptx model requires 60 petaflops/s-days, compared to 3,640 petaflops/s-days for GPT-3 (Brown et al., 2020)""

60/3640 = +1.65% to base model compute

base model was reported 3.14e+23 FLOP

3.14e+23 * 1.0165 = 319181000000000000000000",,,,,,
InstructGPT 6B,1/27/2022,6000000000,"""We train three model sizes (1.3B, 6B, and 175B parameters)""",,,,,,,,
InstructGPT 1.3B,1/27/2022,1300000000,"""We train three model sizes (1.3B, 6B, and 175B parameters)""",,,,,,,,
OntoProtein,1/23/2022,420000000,"""For the protein encoder, we use the pre-trained ProtBert from Elnaggar et al. (2020).""",,,,,,,,
AbLang (heavy sequences),1/22/2022,355000000,"""The hyperparameters were selected to be similar to those used
in the RoBERTa paper (Liu et al., 2019).""

Liu et al., 2019 link: https://arxiv.org/pdf/1907.11692.pdf
""We begin by training RoBERTa following the BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters)""",,,20,,,,,
data2vec (vision),1/20/2022,705134592,"Section 4: ""We experiment with two model sizes: data2vec Base and
data2vec Large, containing either L = 12 or L = 24 Trans-
former blocks with H = 768 or H = 1024 hidden dimen-
sion (with 4 × H feed-forward inner-dimension)""
",,,800,,,,,
data2vec (speech),1/20/2022,705134592,"Section 4: ""We experiment with two model sizes: data2vec Base and
data2vec Large, containing either L = 12 or L = 24 Trans-
former blocks with H = 768 or H = 1024 hidden dimen-
sion (with 4 × H feed-forward inner-dimension)""
",,,,,,,,
data2vec (language),1/20/2022,705134592,"Section 4: ""We experiment with two model sizes: data2vec Base and
data2vec Large, containing either L = 12 or L = 24 Trans-
former blocks with H = 768 or H = 1024 hidden dimen-
sion (with 4 × H feed-forward inner-dimension)""
",,,,,,,,
Detic,1/7/2022,88000000,"from https://github.com/microsoft/Swin-Transformer Swin-B have 88M, 
from page 8 :  'Training our ResNet50 model takes ∼ 22 hours on 8 V100 GPUs. The large 21K Swin-B model trains in ∼ 24 hours on 32 GPUs.'",2.34E+19,"28.26e12* 32 * 24*3600*0.3 =2.34e19 = peak flops * num gpus * num seconds * assumed utilization rate
for Swin-B model from page 8 :  'Training our ResNet50 model takes ∼ 22 hours on 8 V100 GPUs. The large 21K Swin-B model trains in ∼ 24 hours on 32 GPUs.'",,24,"from page 8 :  'Training our ResNet50 model takes ∼ 22 hours on 8 V100
GPUs. The large 21K Swin-B model trains in ∼ 24 hours on 32 GPUs.'",NVIDIA V100,32,191.4458183
ERNIE-ViLG,12/31/2021,10000000000,"""To explore the landscape of large-scale pre-training for bidirectional text-image generation, we pre-train a 10-billion parameter model on a large-scale dataset of 145 million high-quality Chinese image-text pairs.""",,,,,,,,
ERNIE 3.0 Titan,12/23/2021,2.6E+11,"""[We] developed... distributed training technology, including fine-grained parallelism, heterogeneous hardware-aware training, and fault tolerance mechanism to train the 260B model on both Nvidia V100 GPU and Ascend 910 NPU clusters.""
See also:
https://twitter.com/BaiduResearch/status/1468633977242243078?t=6q4zuLNdTSc4GUBe9OM5Aw&s=19",1.04E+24,"The paper suggests that ERNIE 3.0 Titan uses more compute than GPT-3. This is consistent with the 6ND approximation.

C = 6ND = 6 (FLOP/param/token) * (260B params) * (668B tokens) = 1.0421*10^24 FLOP",,,,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",1920,
XGLM-7.5B,12/20/2021,7500000000,"""Our largest model with 7.5 billion parameters sets new state of the art""",2.25E+22,"""The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second""

256 * 312 teraFLOP/s * 21 * 24 * 3600 * 0.3 utilization assumption ~= 4.3e22

also, it was trained for 500B tokens. Using Compute = 6ND, we have
6 * 500B * 7.5B = 2.25e22

311k tokens per second * 7.5B params * 6 is 1.35e16 FLOP/s. divide that by 312 teraFLOP/s, which is A100 peak compute, gets 43, suggesting low utilization (17%) of the 256-GPU cluster, or somewhat higher if there's more than one token per word. So I'll use the 6ND number.",1,504,"appendix A : ""The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second""",NVIDIA A100,256,104152.2259
LDM-1.45B,12/20/2021,1450000000,1.45B,,,0.66,,,NVIDIA A100,,
GLIDE,12/20/2021,3500000000,"""Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking""",4.70E+22,"""Note that GLIDE was
trained with roughly the same training compute as DALL-E
but with a much smaller model (3.5 billion vs. 12 billion
parameters)""",,,,,,
Contriever,12/16/2021,110000000,"Based on BERT base, which had 110m params.

""We initialize the network with the publicly available BERT base uncased model.""",1.57E+20,"Pre-training:
""We use the random cropping data augmentation, with documents of 256 tokens... batch size of 2,048 and 500,000 steps""
256 * 2048 * 500k * 100M * 6 = 1.57e20

Fine-tuning looks unlikely to move final sum much beyond this.",,,,,,
LongT5,12/15/2021,3000000000,3B from section 4.1,,"architecture is sparse so we cannot use 6ND method,
from 3.1.1 ""we simply replace the encoder
self-attention operation in T5 with a sparse sliding-
window local attention operation following the im-
plementation in ETC ""
at the end of section 3.1.2 there is information about 
complexity O(l(r + l/k)) of local attention
from 4.1.1 ""We pre-train LongT5 models for 1M steps on
4096 input sequence length and 910 output se-
quence length.
batch size is 128 (from 4.1 configurations section)
so with l = 4096, k = 16, r = 127, 
so l(r+l/k) = 1568768, but we are not sure about constant.

if normal attention have complexity O(l^2), and l^2 = 16777216
16777216/1568768 = 10.7
We can try to estimate that LongT5 would have 10 times less compute that normal architecture.",3.2,,,Google TPU v3,128,
GLaM,12/13/2021,1.2E+12,1.2 trillion parameters,3.64E+23,"The network activates 96.6 billion parameters per token and trained for 600B tokens.

6 * 600B * 96.6B = 3.478e23

Digitizing figure 4 (d) indicates 139.67 TPU-years of training. 
2.75e14 * 139.67 * 365.25 * 24 * 3600 * 0.3 = 3.636e23

Since these are close, we will use the 6NC estimate and derive hardware utilization from the training time information.

Later they say they measured 326W power usage per chip, which could maybe be used to estimate utilization.",,1366,"Note that they give several energy estimates. Use the complete training figures for 600B tokens, not the GPT-3 comparison values with 280B tokens.

""326W measured system power per TPU-v4 chip""
""The complete GLaM training using 600B tokens consumes only
456 MWh""
1024 TPU v4 chips
(456 MWh) / (326W/chip * 1024 chips) = 1366 hours",Google TPU v4,1024,541437.4162
Gopher (280B),12/8/2021,2.8E+11,"Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.",6.31E+23,"Table A26
6.31E+08 Train PFLOPs",1,920,"""We trained Gopher for 920 hours in November and December 2020 in Google’s Georgia datacentre. The PUE of the datacenter at this time was 1.08; the net tCO2e per MWh in October 2020 was 0.33. Using an estimate of 283W drawn per chip, this leads to a total of 380 net tCO2e""",Google TPU v3,4096,616611.1392
Student of Games,12/6/2021,,,3.67E+22,"""We trained a version of AlphaZero using its original settings in chess and Go, e.g. , using 800 MCTS simulations during training, with 3500 concurrent actors each on a single TPUv4, for a total of 800k training steps. SOG was trained using a similar amount of TPU resources.""",,,,,,
T-NLRv5 XXL,12/3/2021,5400000000,Table 1 of the blogpost,,,,,,,,
NÜWA,11/24/2021,870000000,Section 4.1,7.25E+21,"From AI Tracker:
""Compute cost: End of Sec 4.1: ""We pre-train on 64 A100 GPUs for two weeks"". 

Half precision FLOPs of A100: 312000000000000

64 gpus *312000000000000 FLOPs *0.3 utilization * 14 day* (24*60*60) seconds / day=7.245988e+21

",,,,NVIDIA A100,,
Florence,11/22/2021,893000000,"""Our Florence pretrained model has in total 893M parameters, including the language transformer with 256M parameters and the CoSwin-H transformer with 637M parameters.""",4.83E+22,"""The model takes 10 days to train on 512 NVIDIA A100 GPUs with 40GB memory per GPU.""
512 * 312 teraFLOPS * 10 days * 35% utilization = 4.831e22 FLOP",,240,10 days on 512 A100 40GB,NVIDIA A100 SXM4 40 GB,512,106950.6157
BASIC-L,11/19/2021,3070000000,2.4B image model + 670M text model,4.12E+22,"6.9k + 1k + 0.8k = 8.7k TPUv4 core-days for BASIC-L, per Table 8

Two cores per chip, and 275 teraflop/s per chip 
(https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v4)

275 teraflops * 8700/2 * 24 * 3600 * 0.4 (assumed utilization) = 8.3e22",,,,Google TPU v4,,1684.770712
Swin Transformer V2 (SwinV2-G),11/18/2021,3000000000,,1.10E+21,"trained on ""<0.5k"" TPUv3 core-days per Table 2 (not trained on TPUs, this is a comparison with other papers)

A core is 123/2 teraflops

500 core-days
= 500 * 123/2 trillion * 24 * 3600 * 0.4 utilization
~= 1.1e21",,,,NVIDIA A100 SXM4 40 GB,,2326.66365
ViT-G/14 (LiT),11/15/2021,3005000000,Table 7,,"They start with the ViT-G/14 image model and train their own text model. ViT-G/14 is 3.4e21. 

They also say ""We use 128 TPU cores by default for the above experiments, and 256 TPU cores for our best run with 18 billion seen image-text pairs"" which may be relevant.",4.5,,,Google TPU v3,,
Masked Autoencoders ViT-H,11/11/2021,632000000,"Three models:
ViT-B (86M), ViT-L (304M), ViT-H (632M)",4.60E+20,"128 TPU-v3 cores trained for 1600 epochs. Times are given for 800 epochs in Table 2; largest model (ViT-H) took 34.5 hrs for 800.
128 TPU-v3 cores * 0.5 chips/core * 34.5 hours * 2 * 1.23E+14 FLOP/sec / chip * 3600 sec/hour  * 40% utilization = 7.84e20 FLOP

Note that the operations counting method disagrees:
2 × 632000000 connections × 3 × 1281167 training examples × 1600 of epochs  = 7.8e18 FLOP

Manual calculation with `calflops` package roughly agrees with hardware-time calculation: 
286.21 GFLOPS/observation * 1281167 observations * 1600 epochs = 5.86e20 FLOP

See reproduction here: https://colab.research.google.com/drive/1KCsmrfPzT9BgGO_YQthnz4oP3QRqbw5o?usp=sharing

Weighting three estimates equally:
(7.84e20 + 7.8e18 + 5.86e20)/3 = 4.6e20",1600,69,"Table 2 gives wall times for training ViT-L and ViT-H to 800 epochs; later it is stated that the systems are each trained for 1600 epochs.
(34.5 hours / 800 epochs) * 1600 epochs = 69 hours",,,
Projected GAN,11/1/2021,,Possibly calculable from Appendix Table 8,1.05E+19,"""With this setting, each experiment takes roughly 100-200 GPU hours on a NVIDIA V100,
for more details we refer to the appendix.""

""We conduct our experiments on an internal cluster with several nodes, each with up to 8 Quadro RTX
6000 or NVIDIA V100 using PyTorch 1.7.1 and CUDA 11.0.""

In appendix table 7, takes 10.1 seconds per 1k images on 8 Quadro RTX 6000s. Longest training run for Projected GAN appears to be in Figure 4 (left), at 14M images, though this is overtrained and the largest checkpoint used for evaluations was 10M.
10M images * 10.1 s/1000 images * 8 * 3.26e13 FLOP/s * 0.4 = 1.05e19",,,,"NVIDIA V100,NVIDIA Quadro RTX 6000",,
CodeT5-base,11/1/2021,220000000,"""We build CodeT5 based on Huggingface’s T5 (Raffel et al., 2020) PyTorch implementation and employ two sizes of CodeT5-small (60M) and CodeT5-base (220M)""",1.56E+21,"""We pre-train the model with the denoising objective for 100 epochs and bimodal dual training for further 50 epochs on a cluster of 16 NVIDIA A100 GPUs with 40G memory. The total training time for CodeT5-small and CodeT5- base is 5 and 12 days, respectively""

16 * 312 teraFLOP/s * 12 * 24 * 3600 * 0.3 (utilization assumption) = 1.56e21",150,288,"""The total training time for CodeT5-small and CodeT5- base is 5 and 12 days, respectively""",NVIDIA A100,,3114.869095
S4,10/31/2021,249000000,249M (Table 8),7.83E+19,"6 FLOP / token / parameter * 249000000 parameters * 8 GPUs * 8192 tokens/step/GPU * 800000 steps = 7.8328627e+19 FLOP
 
""our S4 model was trained with the simpler AdamW optimizer with a single cosine learning rate cycle with a maximum of 800000 steps.
The initial learning rate was set to 0.0005. We used 8 A100 GPUs with a batch size of 1 per gpu and context size 8192.""

",509.02,,,NVIDIA A100,8,
EfficientZero,10/30/2021,,,,"""Our implementation is computationally friendly. To train an Atari agent for 100k steps, it only needs 4 GPUs to train 7 hours.""",,,,,,
Eve,10/27/2021,15010300,"""The Bayesian VAE architecture in EVE is comprised of a symmetric 3-layer encoder & decoder architecture (with 2,000-1,000-300 and 300-1,000-2,000 units respectively) and a latent space of dimension 50 [...] We use a single set of parameters for the encoder (ϕp) and learn a fully-factorized gaussian distribution over the weights of the decoder (θp)""
They train a new VAE for each protein, and it doesn't seem like they trim the input sequence length, so the largest model will be the one trained for the largest input protein. Supplementary materials 1 gives statistics for each protein; the longest is 5202, which would indicate a network of size 15,010,300",,,,,,,,
base LM+GNN+kNN,10/17/2021,274000000,"274M (table 1)

""We use the base version of deep Transformer language model with adaptive embeddings (Baevski & Auli, 2018) as our base LM. This model has 16 decoder layers. The dimensionality of word representations is 1,024, the number of multi-attention heads is 16, and the inner dimensionality of feedforward layers is 4,096.""",5.26E+19,"base model compute: 4.47*10^19 FLOP 
fine-tune lower bound estimation: 1.69332e+17 FLOP -> 4.47*10^19 FLOP + 1.69332e+17 FLOP = 4.4869332e+19 FLOP total 
fine-tune upper bound estimation: 1.69332e+19 FLOP -> 4.47*10^19 FLOP + 1.69332e+19 FLOP = 6.16332e+19 FLOP total

geometric mean: sqrt(4.4869332e+19*6.16332e+19) = 5.2587456e+19 
",,,,,,
T0-XXL,10/15/2021,11000000000,"""Unless specified otherwise, we use the XXL version which
has 11B parameters.""",9.18E+20,"From Table 1 and section B.1, a single run uses 27 hours of a 512 core slice of a TPU-v3 pod. 
512 * 0.5 * 1.23e14 * 3600 * 27 * 0.3 = 9.18e20
(cores) * (chip/core) * (FLOP/chip-sec) * (sec/hour) * (hours) * (utilization assumption)",,27,"For main model, 27 hours (Table 1)

Total time taken to train for all experiments was 270 hours ""These training runs corresponded to about 270 total hours of training on a v3-512 Cloud TPU device.""",Google TPU v3,256,11671.84084
Yuan 1.0,10/12/2021,2.4573E+11,"Table 2: Parameters of Yuan models.
""Parameters (billion)""",3.54E+23,"Table 9: 4095 petaFLOPS-days which equals 3.538*10^23 FLOP

https://www.wolframalpha.com/input?i=4095+petaFLOPS+*+1+day
",,,,,2128,
Megatron-Turing NLG 530B,10/11/2021,5.3E+11,,8.59E+23,"https://www.lesswrong.com/posts/bGuMrzhJdENCo8BxX/nvidia-and-microsoft-releases-530b-parameter-transformer?commentId=HSJSNspKp94tFcSCx

source: https://lair.lighton.ai/akronomicon/
9938 PF-days * 3600 * 24 * 10^15  = 8.586432e+23

6ND estimate: 6 * 530B * 270B = 8.586000e+23",,770,"Total compute was 1.17*10^24 FLOP.
They don't directly report the utilization and training speed when using the full Selene supercomputer with 560 DGX * 8 A100/DGX = 4480 GPUs. See section 2.3 Hardware Setup.

At 280 DGX, the utilization is 126/312 = 40% and a batch takes 60 seconds; at 350, it is 39% for 50 seconds; at 420, it is 36% for 44 seconds.

The overall utilization was 30.2% and the full cluster has 560 DGX. Dividing the total compute by the total performance of 4480 A100 at 30.2% utilization gives 770 hours.",NVIDIA A100 SXM4 80 GB,4480,3714250.001
AlphaFold-Multimer,10/4/2021,,"""Multiple changes to the AlphaFold system were made to adapt it to training on protein complexes, which are detailed below. Summarizing briefly, we [...] make various small adjustments to the structure losses and the model architecture."" [2. Methods]

Hence, this will have approximately the same amount of parameters as AlphaFold2",4.35E+21,"Section: 2.5. Training Regimen
""We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores [...]. Then we [...] run two separate fine-tuning stages (one further day of training each)""

Assuming: FP16 and utilization 0.4

Calculation: (14+2) days * 24 hours/day * 60 min/hour * 60 sec/min * (128 TPU cores/2 cores per chip) * 1.23e14 FLOP/s per chip * 0.4 utilization = 4.35e21 FLOPs",68,384,"Section: 2.5. Training Regimen
""We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores [...]. Then we [...] run two separate fine-tuning stages (one further day of training each)""",Google TPU v3,64,7966.233013
TrOCR,9/21/2021,558000000,558M table 5,,May be computed from github and datasets details. Uses pretrained BEiT and DeiT models.,,,,NVIDIA V100,32,
PLATO-XL,9/20/2021,11000000000,,9.90E+21,"""In PLATO-XL, each model was trained for a total of 150B tokens, with
a batch size of 2M tokens.""

150B * 11B * 6 = 9.9e21",,,,NVIDIA V100,256,
HyperCLOVA 204B,9/10/2021,2.04E+11,https://www.navercorp.com/navercorp_/ir/announce/2023/NAVER_CEO%20letter%20to%20shareholders_Aug%202023_Eng.pdf,2.00E+23,"Estimations for 82B model (marked as lower bound estimations)

""For experiments in Section 4, the model trained with 150B is used for fair comparison, because not all models are finished training at the same iteration. However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.""

82e9 connections * 2 FLOP/connection * 300e9 tokens * 3 backward pass = 1.476e23 FLOP

Calculation using GPU time corroborates this:
- ""Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.""
- ""It takes 13.4 days to train a model with 82B parameters with 150B tokens."" Assume 300B tokens takes twice as long, 26.8 days.
- Assume the default of 30% utilization rate for large language models.

1024 A100 GPUs * 312e12 FLOP/second * 0.3 utilization * 26.8 days * 24 * 60 * 60 seconds/day = 2.219e+23 FLOP",,,,NVIDIA A100,,426358.7992
PermuteFormer,9/6/2021,149697024,"Parameterization appears to be similar to a vanilla transformer. 6 layers, hidden dimension of 512, feed forward dimension of 1024, 8 attention heads. This would imply 20,447,232 parameters without embedding weights, and 512*vocab_size embedding weights (assuming tied embedding and unembedding projections)

They appear to use word-level tokenization: ""We evaluate unidirectional PermuteFormer on WikiText-103 (Merity et al., 2017). It is a language modeling dataset with about 103 million tokens,"" and I confirmed that word-level tokenization results in about 102M words across train-test-validation.

If this is the case, there are 267,735 unique words, so the embedding layer alone would be 137,080,320 parameters, for a total of 149,697,024.",2.78E+18,"6 * (30 * 103M) * 149,697,024 = 2.775e18

This seems a bit small relative to their statement: ""It takes about 10 days on 8 V100 GPUs to get all the figures in this paper"" which suggests about 2.7e20 FLOPs at 30% MFU.

Table 2 indicates that Performer and PermuteFormer take 0.23x to 0.58x as long to train as a Transformer model. Figure 2 appears to be the most compute intensive figure, and would take about 4 * (2.775e18) + 1 * (2.775e18 / 0.365) = 1.9e19 FLOPs. ",30,,Running all code needed to produce plots took about 10 days on 8 V100s,NVIDIA V100,8,
MEB,9/4/2021,1.35E+11,See paper title,,,,,,,,
FLAN 137B,9/3/2021,1.37E+11,"Abstract:
""We take a 137B parameter pretrained language model and instruction tune it on
over 60 NLP datasets verbalized via natural language instruction templates. We
evaluate this instruction-tuned model, which we call FLAN, on unseen task types.""

Many models seem to be using the same 137B base transformer model?",2.05E+24,"From section 2.4: Pretraining was done over 2.49T tokens.
6 * 2.49T * 137B = 2.047e24 
Also, ""instruction tuning takes around 60 hours on a TPUv3 with 128 cores."" 128 TPUv3 cores = 64 TPUv3 chips. Environmental considerations section claims this took less than 2% of total time
1.23e14 * 64 * 60 * 3600 * 0.3 = 5.10e20",1,,,Google TPU v3,,230526.7644
XLMR-XXL,8/17/2021,10700000000,"Section 2.1:
"" ...XLM-RXXL (L= 48, H = 4096, A = 32, 10.7B params)""",3.37E+22,"Trained for 500k steps at a batch size of 2048 with sequence length of 512 = 524,288,000,000 tokens seen.
6 * 10700000000 * 524,288,000,000 = 3.366e22",3.139449102,,,,,
DNABERT,8/15/2021,110000000,"""We used the same model architecture as the BERT base, which consists of 12 Transformer layers with 768 hidden units and 12 attention heads in each layer, and the same parameter setting across all the four DNABERT models during pre-training""

Known to have 110 million parameters as reported in: https://arxiv.org/pdf/1810.04805v2.pdf
""We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) [...]""",1.07E+20,"""Since the pre-training of DNABERT model is resource-intensive (about 25 days on 8 NVIDIA 2080Ti GPUs)""

Assuming FP16 and 30% utilization

Calculation = (25 * 24 *3600) s * 2.7e13 FLOP/s per GPU * 8 GPUs * 0.3 utilization = 1.4e20 FLOP

Alternatively:
""DNABERT takes a sequence with a max length of 512 as input... We pre-trained DNABERT for 120k steps with a batch size of 2000""
6 * 512 * 2000 * 120k * 110M = 8.11e19

Geometric mean: 1.07e20",4.04,600,"""Since the pre-training of DNABERT model is resource-intensive (about 25 days on 8 NVIDIA 2080Ti GPUs)""",NVIDIA GeForce RTX 2080 Ti 11GB,,
Zidong Taichu,8/11/2021,3200000000,共32亿参数 translated as A total of 3.2 billion parameters ,8.02E+20,4.175e10 * 3.2e9 * 6 = 8.016e20 FLOP,,,,,,
Jurassic-1-Jumbo,8/11/2021,1.78E+11,"""Jurassic-1 models come in two sizes, where the Jumbo version, at 178B parameters, is the largest and most sophisticated language model ever released for general use by developers.""",3.70E+23,"see here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit

6 * 178B * 300B = 3.204000e+23",,,,NVIDIA A100,,807511.6596
W2v-BERT,8/7/2021,1000000000,1B for XXL model,,,,,,,,
YOLOX-X,8/6/2021,99100000,"99.1M, table 3",6.34E+20,"""We train the models for a total of 300 epochs with 5 epochs warmup on COCO train2017 [17]. We use stochastic gradient descent (SGD) for training ... The batch size is 128 by default to typical 8-GPU devices ... input size is evenly drawn from 448 to 832 with 32 strides""

Training is done on 300 epochs of the 2.5 million image-label pairs in COCO train2017.

Table 3 indicates 281.9 GFLOP per forward pass on a 640x640 image. The mean image width/height is 640, though using this to estimate training FLOPs is probably a slight underestimate as FLOPs will scale roughly linearly in the number of pixels (which scale at width^2).

Ignoring this slight issue: 281.9e9 * 2.5M * 300 * 3 = 6.34e20",300,,,NVIDIA V100,8,
6-Act Tether,8/3/2021,5000000,"""Agent parameter counts were all 5 − 6 million parameters, excluding parameters in auxiliary modules""",,"""In our experiments, we train each of our agents for 8 GPU-weeks (192 GPU-hours)"". No GPU specified.",,,,,,
SEER,7/29/2021,1300000000,"From abstract:
"" Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters...""",1.80E+22,"Numbers from section 3.2, they specifically mention using mixed precision training.
6125 ms / batch * 114890 batches = 8.14 days (they round to 8 in the text)

512 GPUs * 8.14 days * 24h/day * 3600s/h * 125 TFLOP/s * 0.4 (assumed utilization) = 1.800e22

""on 512 V100 32GB NVIDIA GPUs. Training this model on 1 billion images requires 114, 890 training iterations for a batch size of 8, 704 images, summing to 8 days of training over 512 GPUs.""",,195.5,6125 ms / batch * 114890 batches = 195.5 hours,NVIDIA V100,512,34114.25246
HuBERT,7/27/2021,1000000000,"From abstract:
""Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets""",5.54E+21,"GPU NOT SPECIFIED - for the sake of argument I assume something on the order of 1 TFLOP/s

Numbers from Section IV part C
0.1 * (960h * 32GPUs + 60000h * 256 GPUs) * 3600s/h * 1 TFLOP/s/GPU",,,,,,
GOAT,7/27/2021,3472816,estimate described here: https://docs.google.com/document/d/1S9xZyCeITDOs-P1W_-liNW0WgVN-OLsSudVrPXMaLqw/edit?usp=sharing,2.41E+22,"[Final calculation]
(8 TPUs) * (1.23e14 FLOP/TPU-s) * (0.1 utilization) / (50k steps/s) = 1.968e9 FLOP/step

(32 agents) * (383B steps/agent) * (1.968e9 FLOP/step) = 2.412e22 FLOPs

==========================
NOTES BELOW

6.1: Each agent is trained using 8 TPUv3s and consumes approximately 50,000 agent steps (observations) per second.
Multiple agents interacting probably mean a fairly low utilization rate, so let’s assume 0.10
8 * 1.23e14 * 0.1 / 50k = 1.968e9 FLOPs per step

The paper doesn’t say exactly how many agents they train in each population. The original PBT paper uses 32 agents for one task (in general it uses between 10 and 80), so as a guesstimate let’s go with that.

Figure 16: They train over 5 generations. Summing the number of steps, it looks like there were roughly 383B steps
32 * 383B * 1.968e9 = 2.412e22

Final estimate:
2.412e22

I do a confidence interval analysis here and find a 90% CI of 6.9e21 to 1.3e23, so we can call this estimate ""likely"" (within 1 OOM): https://colab.research.google.com/drive/1wGSTQxBExY6Fa0-d7msVumf5-KnsWLe6?usp=sharing",,,"see other notes
",Google TPU v3,,84799.78517
Codex,7/7/2021,12000000000,"""With just a single sample, a 12B parameter Codex solves 28.8% of these problems, and a 300M parameter Codex solves 13.2% of these problems""",7.34E+22,"""The original training of GPT-3-12B consumed hundreds of petaflop/sdays of compute, while fine-tuning it to create Codex-12B
consumed a similar amount of compute.""
1 PFLOP/s-day = 8.64e19 FLOPs.
""Hundreds"" is likely between 200 and 900, geometric mean = 425.
2 * 425 * 8.64e19 = 7.344e22
",,,,,,
ERNIE 3.0,7/5/2021,10000000000,"""We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph.""",2.25E+22,"Section 3.3.3: 
""""The model is trained for
a total of 375 billion tokens""

Total compute approximated as 6*N*D",,,,NVIDIA V100,384,39104.2671
Adaptive Input Transformer + RD,6/28/2021,247000000,,8.58E+19,"""We train Transformer model for 50k steps and Adaptive Input Transformer for 286k steps""

assuming 1 step took 1 second:

125000000000000 FLOP / sec [assumed precision: bf16] * 286000 seconds * 8 GPUs * 0.3 [assumed utilization] = 8.58e+19 FLOP",,79.4,""" The training is on 8 Tesla V100 GPU cards""
286k steps ~ [assumption] 286000 seconds = 79.4 hours",NVIDIA V100,8,
EfficientNetV2-XL,6/23/2021,208000000,"208M for XL version (Table 7, page 7)",9.56E+19,"Table 7, page 7: 45 hours on 32 TPUv3 cores.

""Each v3 TPU chip contains two TensorCores.""
TPU performance per chip = 123e12 FLOP/s
32 cores = 16 chips

123e12 FLOP/s per chip * (32 cores / 2 cores per chip) * 45 hours * 3600 seconds/hour * 0.30 utilization = 9.56e19 FLOP

https://www.wolframalpha.com/input?i=123+terahertz+*+16+*+45+hours+*+0.3",30,45,Table 7,Google TPU v3,16,104.340134
Denoising Diffusion Probabilistic Models (LSUN Bedroom),6/11/2021,256000000,"Appendix B: 
"" Our CIFAR10 model has 35.7 million parameters, and our LSUN and
CelebA-HQ models have 114 million parameters. We also trained a larger variant of the LSUN Bedroom model with approximately 256 million parameters by increasing filter count.""",7.84E+19,"Numbers in Appendix B
""Our CelebA-HQ/LSUN (2562) models train at 2.2 steps per second at batch size 64, [...] The larger LSUN Bedroom model was trained for 1.15M steps.""
10.6h for the CIFAR model (batch size 128, 21 step/s)
2.2 step/s for the LSUN model, 1.15M steps so 702.8 hours

1 step takes 1/2.2 =0.4545 seconds
1.15M steps * 0.4545 seconds = 522675 seconds = 145 hours

This is for TPUv3-8's, which seems to mean 8 cores (standard chip is 125 teraflop/s for 2 cores) -> 4 chips
https://cloud.google.com/tpu/docs/regions-zones

1.25E14 FLOP/s * (4 chips) * 522675 seconds * 0.3 = 78401250000000010000",,,,Google TPU v3,,436.3084845
ALIGN,6/11/2021,820000000,"From author communication

480M (image tower) + 340 M (text tower)",2.60E+22,"From author communication
14.82K TPUv3 core-days
Precision: bfloat16

Estimation
TPUv3 at float16: 123 TFLOPS/chip

123*10^12 TFLOPS/chip * (1 chip / 2 cores) * 14820 TPU core-days * 86400 s/day * 33% utilization = 2.599*10^22 FLOP
https://www.wolframalpha.com/input?i=14820+days+*+123+teraFLOPS+%2F+2+*+0.33",,347.3,14820 TPU core-hours / 1024 TPU cores = 347.3 hours,Google TPU v3,512,32852.9166
DeBERTa,6/10/2021,1500000000,"""...we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters""

Other versions are smaller and use a smaller pre-training dataset. These are distinguished in the paper (e.g. DeBERTa1.5B is the version of DeBERTa with 1.5 billion parameters).",2.59E+22,"Table 8: 16 DGX-2 nodes (x16 V100s each) for 30 days
16 * 16 * 1.3e14 * 30 * 24 * 3600 * 0.3 = 2.588e22",49.2,720,30 days (Table 8),NVIDIA V100,256,6682.228999
EMDR,6/9/2021,440000000,Table 2,1.91E+21,"""We run all of our experiments on a machine with 96 CPUs, 1.3TB physical memory, and 16 A100 GPUs. We use PyTorch (Paszke et al., 2019) to implement our proposed model. With this hardware setup, our experiments on NQ and TriviaQA took approximately 25 hours to complete, while experiments on WebQ took roughly 8 hours to complete. Before supervised training, we also perform a one-time unsupervised MSS pre-training for 82,000 steps that took roughly 1 week.""

1 week + 25 hours * 16 A100s
= ~193 * 16 A100-hours
= 193 * 16 * 3600 * 312 trillion * 0.3 = 1.04e21

Additionally, the model uses BERT, ICT, and T5 models. These required:
- BERT: 6 * 110M parameters * (1M * 256 * 256) inputs = 4.33e19 FLOP
- ICT: 6 * 220M parameters * (100k * 4096 * 256) inputs = 1.38e20 FLOP
- T5: 6 * 220M parameters * (1M * 2048 * 256) inputs = 6.92e20 FLOP

Total: 1.04e21 + 4.33e19 + 1.38e20 + 6.92e20 = 1.91e21",4.05,355,"""We run all of our experiments on a machine with 96 CPUs, 1.3TB physical memory, and 16 A100 GPUs [...] our experiments on NQ and TriviaQA took approximately 25 hours to complete, while experiments on WebQ took roughly 8 hours to complete. Before supervised training, we also perform a one-time unsupervised MSS pre-training for 82,000 steps that took roughly 1 week""

Additionally, they pre-trained BERT, ICT, and T5 models, which took a combined 8.733e20 FLOPs. On 16 A100s at 0.3 utilization, that would have taken approximately 162 hours.

So total time for the largest experiment (NQ or TriviaQA) is around:
25 + 168 + 162 = 355",NVIDIA A100,,2773.312431
CoAtNet,6/9/2021,2440000000,,4.27E+22,"20.1K TPU-v3 core-days

TPUs have two cores per chip, and a chip is 123 teraflop/s 
https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v3

123 teraflop/s * 20100/2 * 24 * 3600 * 0.4 (utilization assumption for non-language models) = 4.27e22",1.83,,,Google TPU v3,,1887.163593
ViT-G/14,6/8/2021,1843000000,Table 2 of paper,5.85E+22,"Digitizing Figure 9 indicates training used 27,200 TPUv3 core-days. TPUv3 is 123 teraflop/s per chip, 2 cores per chip.

1.23e14 * (1/2) * 27,200 * 24 * 3600 * 0.4 = 5.78e22

Alternatively, Table 2 indicates 965.3e9 FLOPs per forward pass on a 224^2 image. Table 4 indicates 5 million steps at a (normalized) batch size of 4096, and total flops including backward pass would be 3x the FLOPs from forward passes alone, so we get:
4096 * 5e6 * 965.3e9 * 3 = 5.93e22

(Note that actual batch size appears to have been 32,768)

Geometric mean: sqrt(5.78e22*5.93e22) = 5.85e22

However note that this leaderboard claims ViT-G/14 took 34 PF-days, or 2.94e21 FLOPs: https://web.archive.org/web/20211218185755/https://lair.lighton.ai/akronomicon/",54.6,,,Google TPU v3,2048,3847.861392
ByT5-XXL,5/28/2021,12900000000,"12.9B, from Table 1",8.10E+22,"""Like mT5, we set our sequence length to 1024 (bytes rather than tokens), and train for 1 million steps over batches of 2^20 tokens.""

12.9 billion * 1 million * 2^20 * 6 = ~8.1e22",,,"Table 9 indicates pretraining completes 25 sequences per second on a TPUv3-64 device. 

""we set our sequence length to 1024 (bytes rather than tokens), and train for 1 million steps over batches of 2^20 tokens."" 

So 1024 sequences per step * 1M steps = 1.024 billion sequences
1.024 B / 25 = 40.96M seconds = 11378 hours or 474 days. This seems implausible, so probably they just used a bigger TPU slice for the full training, but this is not indicated.",Google TPU v3,,92453.37636
Transformer local-attention (NesT-B),5/26/2021,90100000,"Table A2, NesT-B is the largest size.",2.41E+19,"17.9 GFLOPS per forward pass
300 epochs
1.28M training examples
3.5 f_to_b pass ratio
(From Imagenet paper-data, Besiroglu et al., forthcoming) 

17.9e9 FLOP *300 epoch *1.28M images *3.5 forward-backward-ratio = 24057600000000000000",,,,,,
CogView,5/26/2021,4000000000,"""We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem.""",2.68E+22,"source: https://lair.lighton.ai/akronomicon/
archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",,,,NVIDIA Tesla V100 DGXS 16 GB,512,60071.70666
MedBERT,5/20/2021,17000000,"17M from ""This is possibly due to the fact that the untrained Med-BERT is an over-parameterized model (around 17 million parameters) with a huge
number of configurations, so it might overfit to the training data""",9.47E+18,"flops = (1) * (3.13e13) * (24*7 * 3600) * (0.5) = 9.47e18
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)
I assume higher utilization rate, because only 1 GPU is used.
Citation from the text:
""We used a single Nvidia Tesla V100GPU of 32 GB graphics memory capacity, and we trained the model for a week for more than 45 million steps, for which each step consists of 32 patients (batch size)."" - page 11

Note that public code appears not to make use of the tensor core speed up, thus I use 3.13e13 FLOP/sec",50.5,168,"""We used a single Nvidia Tesla V100GPU of 32 GB graphics memory capacity, and we trained the model for a week for more than 45 million steps, for which each step consists of 32 patients (batch size)."" - page 11",NVIDIA Tesla V100 DGXS 32 GB,1,62.48645494
ADM,5/11/2021,559000000,"Largest model is denoted ImageNet 512, has 559M parameters",6.20E+21,"Largest run with their architecture improvements is the ImageNet 512 variant. Table 7 suggests utilization is around 30% for largest models (though we only see 256 x 256 and 128 -> 512)

Table 10: ImageNet 512 variant took 1914 V100-days of training
125e12 FLOP/sec * 1914 days * 24 h/day * 3600 sec/h * 0.3 = 6.2e21",381,,,NVIDIA V100,,11274.48433
ProtT5-XXL-BFD,5/4/2021,11000000000,Table 2,3.70E+22,"FLOP = 11B*2*(920k*512*4096) +  11B*4*(920k*512*4096), 920k steps using seq length 512 batch size 4096, ",5,,,Google TPU v3,512,43025.05719
ProtT5-XXL,5/4/2021,11000000000,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",7.37E+22,"7.37e22 from:
source: https://lair.lighton.ai/akronomicon/
archived: https://github.com/lightonai/akronomicon/blob/main/akrodb/Technical%20University%20of%20Munich/ProtT5-XXL.json

3.7E+22 from Table 9 https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1

Manual calculation: forward passes on 512 * (4096*920k + 2048*343k) = 2.3T tokens
Backward passes on 15% of those, 2.3T * 0.15 = 343B tokens.
Total FLOPs: (2 * 11B * 2.3T) + (4 * 11B * 343B) = 6.57e22",,,,Google TPU v3,512,85701.26256
ProtT5-XL-U50,5/4/2021,3000000000,Table 2,1.87E+22,"991K steps, 2048 batch, 512 sequence length (Table 2)

Total tokens: 991K*2048*512=1039138816000

FLOP: 6*1039138816000*3000000000=18704498688000000000000",,,,,256,
ProtBERT-BFD,5/4/2021,420000000,Table 2,3.90E+22,"FLOP = 420M * 6 * (800k*512*32k + 200k*2048*6k) 
1M steps total split into two phases, (1) 800k steps, seq length 512 (batch size 32k) and (2) 200k steps, seq length 2048 (batch size 6k)
single TPU Pod V3-1024 (64 nodes and 1024 TPUs) info from paper and https://huggingface.co/Rostlab/prot_bert_bfd",,,"figure 3 shows 19 hours per epoch, though this was on a different GPU setup than the one used for training.",Google TPU v3,1024,45350.73596
ViT + DINO,4/29/2021,85000000,"85M, table 1",2.10E+20,"""Overall, training DINO with Vision Transformers
achieves 76.1 top-1 accuracy using two 8-GPU servers for 3
days""

GPU is V100

16 * 125 teraflops * 3 days * 0.4 utilization
= 2.1e20

However, this isn't the best result in the paper (which is 80.1% with ViT-B/8). 76.1% is the result from ViT-B/16 per Table 2, which may be 5x cheaper than ViT-B/8 based on Table 1?

upd:
 ""Table 8: Time and memory requirements. We show total running
time and peak memory per GPU (“mem.”) when running ViT-S/16
DINO models on two 8-GPU machines.""

2*8*125 teraflops*72.6h*3600*0.4=2.09088e+20",300,,,NVIDIA V100,,380.2849049
M6-T,3/5/2021,1.0027E+12,Table 5. Note model is sparse MoE with 960 experts; not all parameters are activated on the forward pass.,5.50E+21,Estimate taken from https://www.governance.ai/research-paper/recent-trends-chinas-llm-landscape,,,,NVIDIA Tesla V100 DGXS 32 GB,480,13156.86124
Generative BST,3/5/2021,9431810048,The largest model is a transformer with 9.4B parameters (Table 2),1.45E+22,"""Both our 2.7B and 9.4B parameter models were trained with batches of approximately 500k label BPE tokens per batch [...] The 9.4B parameter model was trained [...] for a total of 200k SGD steps.""

Also note that the full dataset contains 56.8B label BPE tokens and 88.8B context tokens, so for each batch of 500k label tokens, there are likely 500k * 88.8B / 56.8B = 780k context tokens.

6 * 9.4318B * 200k * (500k + 780k) = 1.449e22",1.76056338,,,,,
Meta Pseudo Labels,3/1/2021,480000000,"Table 4
 480M",4.79E+22,"From communication with author:

22671 TPU days on specific hardware.

Which hardware did you use and in which configuration?
2048 cores of TPU v3.

Precision: Mixed. bfloat16 for activations, float32 for weights and optimizer slots.

2048 TPUv3 cores means 1024 TPUv3 chips, and the spec is 123e12 FLOP/second per chip with bfloat16 precision (Source: https://cloud.google.com/tpu/docs/system-architecture-tpu-vm)

So the compute estimate is:
1024 chips * 123e12 FLOP/second * 0.4 utilization * 11 days * 24 * 60 * 60 = 4.788191232e+22 FLOP",,264,"11 days from section 4:
""We train the model for 1 million steps in total,
which takes about 11 days for EfficientNet-L2 and 10 days
for EfficientNet-B6-Wide. ""

""Specifically, our training process runs on a cluster of 2,048
TPUv3 cores. ""
",Google TPU v3,1024,53844.28059
SRU++ Large,2/24/2021,234000000,Table 5,2.12E+19,"6 FLOP / token / parameter * 234000000 parameters * 1024 tokens per sample * 8*8 samples per batch * 400000 steps = 3.6805018e+19 FLOP

31330000000000 FLOP / sec * 360 GPU-hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.2181104e+19 FLOP

sqrt(3.6805018e+19*1.2181104e+19) = 2.1173704e+19 

",254.5,, 15† GPU-days = 360 GPU-hours,NVIDIA V100,,
Rational DQN Average,2/18/2021,1683456,See figure 7,,,,,,,,
MSA Transformer,2/13/2021,100000000,"""We train an MSA Transformer model with 100M parameters..."" ",5.49E+21,"Based on: https://docs.google.com/spreadsheets/d/1enan21dFx03TkwufHgOwTVNBtuYlqNY9uurjIK6YS-8/edit#gid=0

Number of steps 4.5e5, batch size (tokens) 6.1e7, parameters 1e8

Calculation = 4e8 FLOP/bp * 4.5e5 bp + 2e8 FLOP/fp * 2.75e13 fp

Batch size: 512
Seq length: 100 * 1192 tokens
All models are trained on 32 V100 GPUs for 100k updates. The four models with best contact precision are then further trained to 150k updates. Finally, the best model at 150k updates is trained to 450k updates.

450k * 512 * 100 * 1192 * 100M * 6 = 1.65e22",,,,NVIDIA Tesla V100 DGXS 32 GB,32,13256.9373
top-down frozen classifier,2/9/2021,,,,,,,,,,
DeiT-B,1/15/2021,86000000,(DeiT-B),7.88E+19,"2*86000000 parameters*3*1280000 training examples*300 epochs=1.98144e+17 FLOPs

compute [FLOP] = training time [s] × # of GPUs/TPUs × peak FLOP/s × utilization rate

(53h+20h)*3600*8*125000000000000 peak FLOP/s*0.3=7.884e+19

",300,53,"A typical training of 300 epochs takes 37 hours with 2 nodes or 53 hours on a single node for the DeiT-B.
In this paper, we train a vision transformer on a single 8-GPU node in two
to three days (53 hours of pre-training, and optionally 20 hours of fine-tuning) that is competitive with convnets having a similar number of parameters and efficiency. It uses Imagenet as the sole training set.",NVIDIA V100,,
Switch,1/11/2021,1.571E+12,"""Combining expert, model and data parallelism, we design two large Switch Transformer models, one with 395 billion and 1.6 trillion parameters""
Table 9 gives more precise count of 1571B parameters",8.22E+22,"Table 4
https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf",,648,"see table 4 in https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
",Google TPU v3,1024,139663.5594
BigSSL,1/10/2021,8000000000,"""... we study the utility of large models, with the parameter count ranging from 600M to 8B...""",,,,,,,,
DALL-E,1/5/2021,12000000000,DALL·E is a 12-billion parameter version of GPT-3 trained to generate images from text descriptions,4.70E+22,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb

VAE training ""on 64 16 GB NVIDIA V100 GPUs, with a per-GPU batch size of 8, resulting in a total batch size of 512. It is trained for a total of 3,000,000 updates.""

Transformer training: ""We trained the model using 1024, 16 GB NVIDIA V100 GPUs and a total batch size of 1024, for a total of 430,000 updates.""; ""We concatenate up to 256 BPE-encoded text tokens with the 32 × 32 = 1024 image tokens""
Total tokens: 430000 steps * 1024 batch size * 1280 sequence length = 563609600000

Transformer FLOP: 6 * 12B parameter * 563609600000 tokens = 4.057989e+22

Estimating the VAE at ~15% seems reasonable",1.76,,"""We trained the model using 1024, 16 GB NVIDIA V100 GPUs and a total batch size of 1024, for a total of 430,000 updates.
At the start of training, we use a linear schedule to ramp up the step size to 4.5 · 10−4 over 5000 updates, and halved the
step size each time the training loss appeared to plateau. We did this a total of five times, ending training with a final step
size that was 32 times smaller than the initial one. """,NVIDIA Tesla V100 DGXS 16 GB,1024,125818.6901
CLIP (ViT L/14@336px),1/5/2021,370000000,"Image encoder
Vision Transformer
Table 1 in https://arxiv.org/pdf/2010.11929.pdf
Authors fine-tuned ViT L/14 at additional 336px resolution, hence the @336 (See ViT)
307M params

Text encoder
~Transformer (from paper)
63M params",1.05E+22,"https://docs.google.com/document/d/156miAJkFN9DDX06C3s03UDsretCtymCKiGDddLBCgQE/edit?usp=sharing

",,288,"“The largest ResNet model, RN50x64, took 18 days to train on 592 V100 GPUs while the largest Vision Transformer took 12 days on 256 V100 GPUs”",NVIDIA V100,256,24638.51841
CLIP (ResNet-50),1/5/2021,88600000,"Image encoder
~ResNet-50 (from paper)
25.6M params

Text encoder
~Transformer (from paper)
63M params",,,,,,,,
ERNIE-Doc (247M),12/31/2020,247000000,,3.03E+19,6 FLOP / parameter / token * 247000000 parameters * 416000 steps * 128 sequences per batch * 384 tokens per sequence = 3.0302798e+19 FLOP,198.5,,,,,
CT-MoS (WT2),12/25/2020,45000000,"45M
Table 2",5.40E+17,6 FLOP / parameter / token * 45000000 parameters * 2000000 tokens * 1000 epochs = 5.4e+17 FLOP,1000,,,NVIDIA GeForce GTX 1080 Ti,4,
DensePhrases,12/23/2020,,may be possible to estimate from batch size (8) and maximum memory of GPUs (96GB),2.10E+18," flops = (8) * (1215 * 10**10) * (20 * 3600) * 3 // 10 = 2099520000000000000
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

model of GPU from appendix B (Titan Xp)
number of GPUs from table in appendix A
flops from https://www.techpowerup.com/gpu-specs/titan-xp.c2948",4,20,appendix A row 3,NVIDIA TITAN Xp,8,
VQGAN + CLIP,12/17/2020,,,,,,,,,,
ESM1b,12/15/2020,652400000,See Table 9,5.10E+21,"Information: 
128 NVIDIA V100 GPUs [Pre-training details]
8.5 hours on 64 GPUs per epoch, 56 epochs of UR50/S [Appendix B, ESM-1b Hyperparameter optimization, Experimental set-up]
128 NVIDIA V100 GPU, assuming  V100 PCIe half precision 130 TFLOPS and 0.3 utilization rate

Estimate: (8.5*56*3600) s * 1.3e14 FLOP/s * 0.3 *64 = 4.277e21 FLOP

6NC method:
UR50/S has 27.1M sequences, which are capped at 1024 amino acids. 
27.1M * 1024 * 56 * 652.4M * 6 = 6.08e21 FLOP

Geometric mean: 5.1e21",56,,,NVIDIA V100,128,924.3248891
CPM-Large,12/1/2020,2600000000,"""To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language mode""",2.61E+20,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb

6*2600000000 parameter *16700000000 tokens=2.605200e+20",,336,"""It takes two weeks to train our largest model using 64 NVIDIA V100.""",NVIDIA V100,64,7340.099045
AlphaFold 2,11/30/2020,93000000,"https://arxiv.org/abs/2207.05477 reimplements AlphaFold 2 in a more efficient way, and states there are 93M parameters in the original version (Table 1)",2.99E+21,"123 teraFLOPS / TPU v3 chip * 128 cores * (1 chip / 2 cores) * 11 days * 40% utilization = 2.99e21 FLOP
https://www.wolframalpha.com/input?i=123+teraFLOPS+*+128+*+11+days+*+0.4

""Training regimen"" section: 
""We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. [...] The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days.""",,264,7 days pretrain and 4 days finetune,Google TPU v3,,3841.772612
KEPLER,11/23/2020,125000000,,1.66E+21,"From author communication

""About 128 GPU-days using Nvidia V100 (16GB). ""

precision: float16

V100 GPU for float16: 28000000000000 (2.8E+13)

0.4 * 28TFLOP/s * 128 GPU-days * 24h/day * 3600s/h
= 1.24E+20

""and use the released roberta.base parameters for
initialization, which is a common practice to save
pre-training time""

Roberta base FLOP: 1.536e+21
Total:1.660000e+21
",,,,,,
SimCLRv2,10/26/2020,795000000,"From author communication

We trained different model sizes (from 24M to 795M), and they're summarized in Table 1 of the paper (https://arxiv.org/pdf/2006.10029.pdf).",,,,,,,,
wave2vec 2.0 LARGE,10/22/2020,317000000,"Section 5.1:
""We consider two model sizes: BASE (95m parameters) and LARGE (317m parameters)
",3.87E+21,"From surveying the authors:

We trained the base model on 64 V100 GPUs for 400k updates. This takes about 3 days to complete. The large model is trained on 128 V100 GPUs for 1 million updates, and this takes about 7 days to complete.

V100 GPU peak: 125TFLOP/s (https://www.nvidia.com/en-gb/data-center/tesla-v100/)
Assume 40% utilization based on default for non-Language domain (https://epoch.ai/blog/estimating-training-compute)

128 GPUs * 40% * 125TFLOP/s * 7 days * 24h/day * 3600s/h
~= 3.870720e+21",,,,NVIDIA Tesla V100 DGXS 32 GB,,5021.241075
ViT-Huge/14,10/22/2020,632000000,Table 1 https://arxiv.org/pdf/2010.11929.pdf,4.26E+21,"Table 6: 4.262e21 FLOPs

Agrees with Table 2 (2.5k TPUv3-core days), if MFU is around 0.32. 

2500 * 24 * 3600 * (0.5 * 1.23e14) * 0.32 = 4.25e21",14,,,Google TPU v3,,6724.023241
ViT-Base/32,10/22/2020,86000000,Table 1 https://arxiv.org/pdf/2010.11929.pdf,,,7,,,,,
German ELECTRA Large,10/21/2020,335000000,335M from Table 5,1.43E+21,"flops = (64) * (123* 10**12) * (7 * 24 * 3600) * (0.3) = 1.4e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

'large models were trained on pods of 16 TPUs v3 (128 cores).' - from section 4.1 it was trained for 7 days from Table 2

Agrees with 6CN:
Tokens seen: 512 (seq len) * 1024 (batch size) * 1 million (steps) = 5.24e11
FLOPs: 6 * 335M * 5.24e11 = 1.05e21",,168,7 days from Table 2,Google TPU v3,64,2392.288335
GBERT-Large,10/21/2020,335000000,335M from Table 5,2.24E+21,"flops = (64) * (123* 10**12) * (11 * 24 * 3600) * (0.3) = 2.24e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

'large models were trained on pods of 16 TPUs v3 (128 cores).' - from section 4.1it was trained for 11 days from Table 2",,264,11 days from Table 2,Google TPU v3,64,3771.133385
mT5-XXL,10/20/2020,13000000000,13 billion,8.20E+22,"""We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total.""

1 million steps * 1024 batchsize * 1024 length * 13 billion params * 6 = 8.2e22

Ignores fine-tuning compute; this is likely a small fraction of pre-training compute.",1,,,,,
Conformer + Wav2vec 2.0 + Noisy Student,10/20/2020,1000000000,1B for XXL model,7.60E+21,"""We train with global batch size 2048 on 256/512 Google TPU V3 cores for 3-4 days for the XL/XXL models respectively...
We fine-tune the pre-trained checkpoints (400k steps) with global batch
size 1024/512 on 256/512 Google TPU v3 cores for 1-3 days for the XL/XXL models""

TPU v3 chips are 123 teraflop/s. 2 chips per core

512 cores * 7 days * 24 * 3600 * 123 tflops * (1 chip/2 cores) * 0.4 (assumed utilization) = 7.6e21",,168,7 days,Google TPU v3,256,9449.541661
LUKE,10/2/2020,483000000,"""The total number of parameters is approximately 483 M, consisting of 355 M in RoBERTa and 128 M in our entity embeddings""",1.81E+22,"Uses RoBERTa Large as a base model, which used 1.66e22 FLOPs in training.

LUKE's additional training was:
(16) * (1.25e14) * (30 * 24 * 3600) * (0.3) = 1.5552e21
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from appendix A: ""Werun the pretraining on NVIDIA’s PyTorch Docker
container 19.02 hosted on a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training takes approximately 30 days.""

Assuming 16 bit tensor core computations, 1.25e14 FLOP/s per V100

Total: 1.65888e22 + 1.5552e21 = 1.8144e22",119.46,720,see compute notes,NVIDIA V100,16,4186.383566
ProBERTa,9/1/2020,44000000,"""In total, our model has approximately 44M trainable parameters.""",9.72E+18,"""we pre-train PRoBERTa on 4 NVIDIA V100 GPUs in 18 hours""
4 * 125 tFLOP/s * 18 * 3600 * 0.3 (assumed utilization) = 9.72e18",,18,,NVIDIA V100,4,26.20699244
ERNIE-GEN (large),8/6/2020,340000000,"""We train a base model ERNIEGENBASE (L=12, H=768, A=12, Total Parameters=110M)1
and a large model ERNIE-GENLARGE (L=24, H=1024,
A=16, Total Parameters=340M) with parameters initialized
by BERTBASE and BERTLARGE respectively""",2.00E+20,"430GB text for 1 epoch

approx 430 * 200 million words = 86B words, or 100B tokens per https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0

6 * 340 million params * 100 billion tokens ~= 2e20",,,,,,
DeLighT,8/3/2020,99000000,99M (Table 4b),3.80E+18,"6 FLOP / parameter / token * 99 * 10^6 parameters * 100000 steps * 64000 tokens per batch = 3.8016e+18 FLOP

31330000000000 FLOP / second / GPU * 8 GPUs * 30 hours [assumed based on smaller models reported training time] * 3600 sec / hour * 0.3 [assumed utilization] = 8.120736e+18 FLOP

Operation counting method uses less assumptions",62.14,30,"Table 5 reports training time for 54M translation model (23h)
it should be more for the 99M language modeling model.",NVIDIA V100,8,
EfficientDet,7/27/2020,77000000,"""EfficientDet-D7 achieves stateof-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs""",,,600,,,,,
Hopfield Networks (2020),7/16/2020,,,,,,,,,,
SemExp,7/2/2020,,,,,,,,,,
GShard (dense),6/30/2020,2300000000,"""Our best quality dense single Transformer model (2.3B parameters) achieving ∆BLEU of 6.1, was trained with GPipe [15] on 2048 TPU v3 cores for 6 weeks or total of 235.5 TPU v3 core-years.""",4.77E+22,"Trained for a total of 235.5 TPU v3 core-years.
Hardware estimate: 235.5 * 365.25 * 24 * 3600 * (1.23e14 / 2) * 0.3 = 1.371e23

Footnote 10 indicates 300k steps and 4M tokens/step -> 1.2T tokens
Arithmetic estimate: 6 * 2.3B * 1.2T = 1.656e22 FLOPs

Geometric mean: sqrt(1.371e23 * 1.656e22) = 4.765e22",,1008,6 weeks = 1008 hours,Google TPU v3,1024,256224.7686
iGPT-XL,6/17/2020,6801000000,source: https://openai.com/blog/image-gpt/#rfref53,3.30E+22,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening (""There's no compute data for the largest model, iGPT-XL. But based on the FLOP/s increase from GPT-3 XL (same num of params as iGPT-L) to GPT-3 6.7B (same num of params as iGPT-XL), I think it required 5 times more compute: 3.3 * 10^22 FLOP."")",,,,NVIDIA Tesla V100 DGXS 32 GB,,104609.5126
iGPT-L,6/17/2020,1362000000,source: https://openai.com/blog/image-gpt/#rfref53,8.91E+21,"We have that ""iGPT-L was trained for roughly 2500 V100-days"" [1]

I assume this is the NVIDIA Tesla V100 GPU. In the specifications, the NVIDIA Tesla V100 has 7 to 8.2 TFLOPS of peak double precision performance and 14 to 16.4 TFLOPS of peak single precision performance and 112 to 130 TFLOPS of peak tensor performance [2].

I suppose the one that makes sense using if peak tensor performance, for ~125 TFLOPS peak tensor performance more or less.
Following OpenAIs AI and compute we apply a 0.33 utitilization factor [3].

In total we get 2500 V100-days * (24*60*60) seconds/day * 125 TFLOPS * 0.33 = 8.91e+21 FLOPS = 89.1 PF-days.

[1] https://openai.com/blog/image-gpt/
[2] https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf
[3] https://openai.com/blog/ai-and-compute/",,,,NVIDIA Tesla V100 DGXS 32 GB,,30093.44468
GPT-3 175B (davinci),5/28/2020,1.75E+11,"""we train GPT-3, an autoregressive language model with 175 billion parameters""",3.14E+23,"Table D.1
https://arxiv.org/abs/2005.14165",0.6,355.2,14.8 days according to https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf,NVIDIA Tesla V100 DGXS 32 GB,10000,2198104.565
DETR,5/26/2020,60000000,60M per Table 1,4.00E+20,"""Training the baseline model for 300 epochs on 16 V100 GPUs takes 3 days, with 4 images per GPU (hence a total batch size of 64). For the longer schedule used to compare with Faster R-CNN we train for 500 epochs with learning rate drop after 400 epochs. This schedule adds 1.5 AP compared to the shorter schedule.""

48 V100-days for baseline DETR model. Larger model had 1.5x the params and 5/3 as many epochs, so required ~2.5x as much training compute.

125 teraflop/s * 2.5 * 48 * 24 * 3600 * 0.3 (assumed utilization) ~ 4e20",500,,,NVIDIA V100,,959.9097627
Retrieval-Augmented Generator,5/22/2020,626000000,"""Our RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable parameters""",,"not enough info, e.g. no training time reported:

""We train with mixed precision floating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU""",,,,NVIDIA Tesla V100 PCIe 32 GB,,
Conformer,5/16/2020,118800000,118.8M for Conformer(L),,,,,,,,
ContextNet,5/7/2020,112700000,Table 5,,"Uses pre-trained joint network from https://arxiv.org/pdf/1811.06621, so total training compute should factor this in.",,,,,,
NAS+ESS (156M),5/6/2020,156000000,156M (Table 2),2.89E+18,6 FLOP / token / parameter * 156 * 10^6 parameters * 103000000 tokens * 30 epochs = 2.89224e+18 FLOP,30,,2890000000000000000 FLOP / (11340000000000 FLOP / sec [fp32] * 1 GPU * 3600 sec / hour * 0.3 [assumed utilization]) = 235 hours,NVIDIA GeForce GTX 1080 Ti,1,
UnifiedQA,5/2/2020,11000000000,11B (based on T5-11B),1.65E+19,"A.2: ""In the experiments, we use v3-8 TPUs for T5 models... pretraining UNIFIEDQA approximately takes about 36 hours on T5(11B)""

4 * 1.23e14 * 36 * 3600 * 0.3 = 1.91e19

Alternatively, input (ouput) size of 512 (100) tokens, batch size of 8, trained for 100k steps. Input tokens only need the forward pass.
((2 * 11B * 512 * 8) + (6 * 11B * 100 * 8)) * 100k = 1.43e19

Took geometric mean of these estimates:
sqrt(1.91e19*1.43e19) = 1.65e19",1.88,36,"""pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.""",Google TPU v3,8,
ATLAS,5/2/2020,11000000000,"11B from appendix A.2 : Model sizes: ""Most of the experiments are done on T5(11B) which has 11 billion parameters. We also report experiments with BART (large) with 440 million parameters.""",3.83E+19,"flops = (8) * (123 * 10**12) * (36 * 3600) * (0.3)
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from Appendix A.2: ""Time spent to build UNIFIEDQA: pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.""
so 36h for T5

""Infrastructure: In the experiments, we use v3-8 TPUs for T5 models, and eight 32GB GPUs for BART models.""

from https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_chip
tpu chip have peak flops 123 teraflops
so 8 chips have peak flops 123 * 8`",,36,"Appendix A.2: Time spent to build UNIFIEDQA: pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.",Google TPU v3,,59.12208691
Once for All,4/29/2020,7700000,"""Since
all of these sub-networks share the same weights (i.e., Wo) (Cheung et al., 2019), we only require
7.7M parameters to store all of them. Without sharing, the total model size will be prohibitive""",6.24E+20,"4.2k V100-hours (table 1)
0.33 utilization rate
V100 FP16 Tensor FLOPs: 125000000000000

4200 hours*60*60* 125000000000000 FLOP/s *0.33 utilization =623700000000000000000
",180,,,NVIDIA V100,,1753.925568
Go-explore,4/27/2020,,,,,,,,,,
CURL,4/8/2020,907264,,,,,,,,,
Agent57,3/30/2020,,,,,,,,,,
MetNet,3/24/2020,,,,,,,,,,
ELECTRA,3/23/2020,335000000,https://github.com/google-research/electra,3.10E+21,"Table 8: ""ELECTRA-1.75M"" used 3.1e21 train FLOPs. Note that the actual parameter count is 335M. The 1.75M refers to the number of training steps.

This doesn't quite line up with a 6ND estimate, 
6 * 335M * (1.75M * 2048 * 128) = 9.22e20 FLOPs
I'm inferring 128 sequence length, possibly this is 256 or 512?",,,table 1,,,
Tensor-Transformer(1core)+PN (WT103),3/17/2020,85300000,"six layers tensorized transformer core-1 for Wikitext-103, following (Ma et al., 2019).

https://arxiv.org/abs/1906.09777",1.58E+18,6 FLOP / parameter / token * 85300000 parameters * 103000000 tokens * 30 epochs = 1.581462e+18 FLOP,30,,,,,
Routing Transformer (WT-103),3/12/2020,79500000,,,,,,,,,
TransformerXL + spectrum control,3/11/2020,151000000,"151M (Table 2)

"" On the large WikiText-103 dataset, we implement our method based on the state-of-the-art Transformer-XL based models (Dai et al., 2019). We follow the same settings reported in (Dai et al., 2019), and our implementation is based on the
official code for Transformer-XL.""",2.63E+19,"6 FLOP / parameter / token *  151000000 parameters * 103000000 tokens * 250 epochs = 2.33295e+19 FLOP

31330000000000 FLOP / sec / GPU [fp16] * 4 GPUs * 3152 sec per epoch * 250 epochs * 0.3 [assumed precision] = 2.9625648e+19 FLOP

sqrt(2.33295e+19*2.9625648e+19) = 2.6289761e+19 FLOP

'speculative' confidence due to the assumption of the amount of epochs",250,219,3152 sec per epoch (Table 7) * 250 epochs / 3600 sec/hour = 219 hours,NVIDIA V100,4,
TCAN (WT2),2/28/2020,33000000,,,,,,,,,
Feedback Transformer,2/21/2020,126000000,"Table 3 shows 126M.

There is another instance of the Feedback Transformer mentioned in Table 9 with 139M parameters.",7.69E+18,"6 FLOP / token / parameter * 126*10^6 parameters * 256 tokens per sequences * 512 sequences per batch * 210000 steps = 2.0808991e+19 FLOP

assuming V100 GPU fp16:

31330000000000 FLOP/sec/GPU * 1 GPU * 84 hours * 3600 sec / hour * 0.3 [assumed utilization] = 2.8422576e+18 FLOP

sqrt(2.0808991e+19*2.8422576e+18) = 7.690547e+18 FLOP

___________
in the Algorithmic progress paper they used estimation of 4.41e+19 FLOP also with low confidence",267.23,84,""" Our Feedback architecture takes 3.5 days to train""
3.5*24 = 84 hours",,,
Turing-NLG,2/13/2020,17000000000,,1.57E+22,"source: https://lair.lighton.ai/akronomicon/
157 PF-days * 3600 * 24 * 10^15  = 1.35648e+22

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb

6ND=6*17000000000*46400000000=4.7328e+21 (confidence regarding dataset size - likely)

Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 28,000,000 PFLOP = 2.8*10^22 FLOP",3.39,,,NVIDIA Tesla V100 DGXS 32 GB,256,51659.71329
SimCLR,2/13/2020,375000000,source: https://openai.com/blog/image-gpt/,,,1000,,,Google TPU v3,,
ALBERT-xxlarge,2/9/2020,235000000,,2.39E+21,"32 hours of training
512 TPU V3s
0.33 utilization rate

123000000000000 FLOP / chip / sec * 512 TPUs * 32 hours * 3600 sec / hour * 0.33 [assumed utilization] = 2.3940956e+21 FLOP

""We train all models for 125,000 steps unless otherwise specified""
""All the model updates use a batch size of 4096 ""
""We always limit the maximum input length to 512, and randomly generate input sequences shorter than 512 with a probability of 10%.""

6 FLOP / parameter / token * 235000000 parameters * 512 tokens per sequence * 4096 sequences per batch * 125000 steps =  3.6962304e+20 FLOP

Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 31,000,000 PFLOP = 3.1*10^22 FLOP",79.4,32,,Google TPU v3,512,4439.921299
TaLK Convolution,2/8/2020,240000000,"Table 5

""For the language model, we followed the same configuration
as Baevski & Auli (2019). We used 17 decoding layers, each
layer with a 1024 hidden size, a 4096 feed-forward hidden
size and 8 heads. The adaptive input factor was set to 4.""",2.70E+19,6 FLOP / parameter / token * 240000000 parameters * 286000 steps * 65536 tokens per batch = 2.6990346e+19 FLOP,182,,,NVIDIA GeForce RTX 2080 Ti 11GB,8,
Perceiver IO (optical flow),2/8/2020,27900000,"Optical flow model (SOTA) was 27.9M params. There are other, larger models described in this paper, e.g. for language.

""For the pixel- and patch-based models, total computational
complexity for a forward pass on a 368 × 496 image is roughly 987 billion FLOPs, and there are
roughly 27.9 million parameters.""",,,480,,,,,
Theseus 6/768,2/7/2020,66000000,"66M, Table 1",,,,,,NVIDIA V100,,
Meena,1/28/2020,2600000000,"""We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token.""",1.12E+23,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4

In the paper: ""We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores) on the Meena dataset containing 40B words (or 61B BPE tokens) [...] by the end of training, the model had traversed the full
training set 164 times (or epochs) and observed a total of about 10T tokens""

Hardware: 30 * 24 * 3600 * (2048/2) * 1.23e14 * 0.3 = 9.794e22
Ops counting: 6 * 10T * 2.6B = 1.56E23
Geometric mean: sqrt(9.79e22*1.56E23) = 1.24e23, very close to the figure in the link above.",164,720,"We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores)",Google TPU v3,1024,206760.3813
ContextNet + Noisy Student,1/19/2020,,,8.16E+21,"""We train 6 generations of models numbered 0 to 5, where
we count the baseline model trained with the supervised set
as the zeroth generation. Each generation is trained ... on 32 Google
Cloud TPU chips for 10 days.""

The TPU version is likely v3 given this is a 2020 paper.

we get 6 * 10 * 24 * 3600 * 32 * 123 tflops * 0.4  (assumed utilization) = 8.16e21",,1440,roughly 10 days,Google TPU v3,,14226.05446
AlphaFold,1/15/2020,16340840,"""Neural network hyperparameters"" section of https://www.nature.com/articles/s41586-019-1923-7:
“7 × 4 Blocks with 256 channels, cycling through dilations 1, 2, 4, 8”
“48 × 4 Blocks with 128 channels, cycling through dilations 1, 2, 4, 8”

""Distogram prediction"" section:
""For the final layer, a position-specific bias was used""

Extended Data Fig.1 (b): 
Shows that each block consists of 9 layers:
(1) Batch norm
(2) Elu
(3) Project down (halves number of dimensions)
(4) Batch norm
(5) Elu
(6) 3x3 kernel with dilation
(7) Batch norm
(8) Elu
(9) Project up (doubles number of dimensions)

Dilations don't change the number of parameters in each filter
Assuming that projection layers are convolutional layers with 1x1 kernels

Parameter estimate for each layer in a 256 channel block:
(1) 256*2            = 512
(2) 0
(3) 1*1*256*128 = 32768
(4) 128*2            = 256 
(5) 0
(6) 3*3*128*128 = 147456
(7) 128*2            = 256 
(8) 0
(9) 1*1*128*256 + 256 = 33024
Total                             = 214272

Parameter estimate for each layer in a 128 channel block:
(1) 128*2            = 256
(2) 0
(3) 1*1*128*64   = 8192
(4) 64*2              = 128
(5) 0
(6) 3*3*64*64     = 36864
(7) 64*2              = 128
(8) 0
(9) 1*1*64*128 + 128 = 8320
Total                   = 53897

Estimate total network = 7*4*214272 + 48*4*53897 = 5992616 + 10348224
                                     = 16340840
                                     ~ 16e6

Within a factor of 2 of the estimate of 21M parameters stated in: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305407/

[Previous approximation: 7 * 4 * 256 * 3 * 3 * 256 + 48 * 4 * 128 * 3 * 3 * 128 = 44826624]",1.00E+20,"Estimated in the blogpost below

https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening

""AlphaFold: they say they trained on GPU and not TPU. Assuming V100 GPU, it's 5 days * 24 hours/day * 3600 sec/hour * 8 V100 GPU * 100*10^12 FLOP/s * 33% actual GPU utilization = 10^20 FLOP.""",,120,"""Training time: about 5 days for 600,000 steps""",,,
Big Transfer (BiT-L),12/24/2019,928000000,,,,40,,,Google TPU v3,,
DD-PPO,12/19/2019,,"no parameter count but some architecture details: ""The policy is parameterized by a 2-layer LSTM with a 512-dimensional hidden state. It takes three inputs: the previous action, the target relative to the current state, and the output of the visual encoder. The LSTM’s output is used to produce a softmax distribution over the action space and an estimate of the value function. See Appendix C for full details.""",7.80E+20,"""Using DD-PPO, we train agents for 2.5 Billion steps of experience with 64 Tesla V100 GPUs in 2.75 days – 180 GPU-days of training""

125 teraFLOP/s (exact V100 model not specified) * 180 * 24 * 3600 * 0.4 (assumed utilization) = 7.8e20",,66,2.75 days,NVIDIA V100,64,1926.89929
OpenAI Five Rerun,12/13/2019,159000000,"""We define a policy (π) as a function from the history of observations to a probability distribution
over actions, which we parameterize as a recurrent neural network with approximately 159 million
parameters (θ)."" pg. 3 of paper

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",1.30E+22,"THIS CALCULATION IS FOR RERUN

""Rerun took 2 months and 150 ± 5 PFlops/s·days of compute (see Figure 4)""



source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",,,,NVIDIA P100,512,321105.9055
OpenAI Five,12/13/2019,159000000,"""We define a policy (π) as a function from the history of observations to a probability distribution over actions, which we parameterize as a recurrent neural network with approximately 159 million parameters (θ)."" pg. 3 of paper

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",6.70E+22,"""770±50 PFlops/s·days of compute"" for the model that played against world champions. They did a single training run that took 10 months.

While the model was playing against world champions, they continued training for a few days, so that the resulting model used even more training compute: 820±50 PFlops/s·days.

Finally, they also trained a Rerun model with 150±5 PFlops/s·days of compute.

Source: Dota 2 with Large Scale Deep Reinforcement Learning
https://arxiv.org/abs/1912.06680

You cannot multiply the hardware quantity by training time to get the quantity of GPU-hours! Page 5: "" the number of GPUs (up to 1536 at the peak)""

From this NVIDIA blogpost, it appears they were using P100s:
https://developer.nvidia.com/blog/ai-learns-to-play-dota-2-with-human-precision/#:~:text=AI%20Learns%20to%20Play%20Dota,The%20neural",,7104,"""OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. "" --> 296 days",NVIDIA P100,1536,4177317.756
MMLSTM,12/5/2019,75000000,Table VII,2.32E+18,6 FLOP / token / parameter * 75000000 parameters * 103000000 tokens * 50 epochs [assumption] = 2.3175e+18 FLOP,50,,,,,
StarGAN v2,12/4/2019,,,,,,,,,,
Transformer-XL DeFINE (141M),11/27/2019,141000000,Table 2b,1.74E+18,"6 FLOP / token / parameter * 141000000 parameters * 103000000 tokens * 20 epochs [assumption based on number of epochs for LSTMs] =  1.74276e+18 FLOP

_______________
older estimation:  6.2 × 10^18 (no explantion how it was calculated)",20,,,,,
Photo-Geometric Autoencoder,11/25/2019,,,,,30,,,,,
Transformer - LibriVox + Decoding/Rescoring,11/19/2019,296000000,Table 2,,"""Models are trained on 64 GPUs each with an overall batch size of 256 for ResNet and TDS and 320 for Transformer. With only LIBRISPEECH, all models converged in under a week; with pseudo-labels from LIBRIVOX, training required 2-3 weeks""

GPU not specified",,,,,,
MuZero,11/19/2019,36864000,"Both the representation and dynamics function use the same architecture asAlphaZero, but with 16 instead of20 residual blocks [15]. We use 3x3 kernels and 256 hidden planes for each convolution.

Previous downsampling:
•  1 convolution with stride 2 and 128 output planes, output resolution 48x48.•  2 residual blocks with 128 planes•  1 convolution with stride 2 and 256 output planes, output resolution 24x24.•  3 residual blocks with 256 planes.•  Average pooling with stride 2, output resolution 12x12.•  3 residual blocks with 256 planes.•  Average pooling with stride 2, output resolution 6x6.",4.80E+19,"third-generation Google Cloud TPU
(For each board game, we used 16 TPUs for training and 1000 TPUs for self-play)
For each game in Atari, we used 8 TPUs for training and 32 TPUs for self-play
Training for 12 hours (for Atari)
Data from Parameter, Compute and Data Trends in Machine Learning
Google v3 TPU: 1.23E+14 FLOP/s (although with the caveat that it might be not applicable)
Utilization rate 
In LaMDA: Language Models for Dialog Applications, they report for TPU V3: 56.5%
Calculations for Atari:
12 hours → 43200 seconds
(8 TPUs for training) * (1.23*10^14 FLOP/s) * (43.2 *10^3 s) * (0.565 utilization rate) = 2.4017472 * 10^19 FLOP
Training time missing for boardgames
Assumption also 12 hours 
Also: 2.4017472 * 10^19 FLOP
Total cost ≈ 4.8 * 10^19 FLOP",,,,,,
MoCo,11/13/2019,375000000,https://openai.com/blog/image-gpt/#rfref53,,,,,,,,
Noisy Student (L2),11/11/2019,480000000,,2.61E+22,"""Our largest model, EfficientNet-L2, needs to be trained for 6 days on a Cloud TPU v3 Pod, which has 2048 cores, if the unlabeled batch size is 14x the labeled batch size""
TPU v3 gets 1.23e14 FLOP/s per chip, with 2 cores per chip

1024 * 1.23e14 * 6 * 24 * 3600 * 0.4 = 2.612e22",,144,6 days,Google TPU v3,1024,43900.60644
Sandwich Transformer,11/10/2019,209000000,"209M
""All of our experiments use the same hyperparameters as Baevski and Auli’s original model.""",2.35E+19,"6 FLOP / token / parameter * 209000000 parameters * 286000 steps * 65536 tokens per batch [same as Baevski and Auli (2019) = 2.3504093e+19 FLOP

__________

in the Algorithmic progress paper they assumed 180 epochs (same as Baevski and Auli 2019 Transformer, but that one was trained on WT 103 not the book corpus) -> training compute was estimated to be 1.58E+20 FLOP",27,,,,,
CamemBERT,11/10/2019,335000000,"CamemBERT Large, Table 4",8.30E+20,"""Unless otherwise specified, our models use the BASE architecture, and are pretrained for 100k backpropagation steps on 256 Nvidia V100 GPUs (32GB each) for a day""

256 V100-days

256 * 125 teraflops * 24 * 3600 * 0.3 (assumed utilization)
= 8.3e20


""Following (Liu et al., 2019), we optimize the model using Adam (Kingma and Ba, 2014) (β1 = 0.9, β2 = 0.98) for 100k steps with large batch sizes of 8192 sequences, each sequence containing at most 512 tokens""

Using compute = 6*N*D, that's 6 * (100k * 8192 * 512) * 335M= 8.43e20",13,24,1 day for each model (may not have been a full 24 hours),NVIDIA V100,,2319.541948
XLM-RoBERTa,11/5/2019,550000000,"The number of parameters in the model is specified as ""550M params"" for XLM-R.",2.08E+22,"""We use the multilingual MLM loss and train our XLM-R model for
1.5 Million updates on five-hundred 32GB Nvidia
V100 GPUs with a batch size of 8192. ""

6ND:
Sequence length was probably 512, based on follow up paper (XLM-R XXL)
6 * 550e6 * 1.5e6 * 8192 * 512 = 2.076e22
",,,,NVIDIA Tesla V100 DGXS 32 GB,500,80889.66935
Base LM + kNN LM + Continuous Cache,11/1/2019,247000000,"""we take the exact architecture and optimization described by Baevski & Auli (2019) and use it to create a kNN-LM for inference. This model consists of 16 layers, each with 16 self-attention heads, 1024 dimensional hidden states, and 4096 dimensional feedforward layers, amounting to 247M trainable parameters.""",3.05E+19,"6 FLOP / parameter / token * 247*10^6 parameters * 103000000 tokens * 200 epochs = 3.05292e+19 FLOP


__________
for the Algorithmic progress paper 7.3 × 10^18 FLOP was estimated similar to supposedly base model (transformer) ",200,,,,,
AlphaStar,10/30/2019,139000000,"AlphaStar has 139 million weights, but only 55 million weights are required during inference.",5.93E+22,"384 TPUv3 chips for 44 days. Assume 33% utilization.
https://www.wolframalpha.com/input?i=123+teraFLOPS+*+384+*+0.33+*+44+days

""Each agent was trained using 32 third-generation tensor processing units (TPUs23) over 44 days.""

12 agents * 34 chips = 384 chicks
",,1056,"""Each agent was trained using 32 third-generation tensor
processing units (TPUs) over 44 days""",Google TPU v3,384,125758.0981
BART-large,10/29/2019,406291456,"""In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.""

I counted the parameters in the huggingface model
https://huggingface.co/facebook/bart-large/tree/main

from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(""facebook/bart-large"")
model = AutoModel.from_pretrained(""facebook/bart-large"")
sum(p.numel() for p in model.parameters() if p.requires_grad)",,,,,,,,
T5-11B,10/23/2019,11000000000,The full 11-billion parameter model,3.30E+22,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4, 4.05e22

update: 3.3e22 per FLAN paper from Google 
https://arxiv.org/pdf/2210.11416.pdf

6ND rule suggests somewhat more FLOPs:
6 * 1T * 11B = 6.6e22",,481.9,"4.05*10^22 FLOP at 37.073% utilization on 512 TPU v3 chips (123 TFLOPS) -> 482 hours
https://www.wolframalpha.com/input?i=4.05*10%5E22+seconds+%2F+%28512*123*10%5E12%29+*%28123%2F45.6%29",Google TPU v3,512,75524.39074
T5-3B,10/23/2019,2800000000,"page 37, 3B and 11B. ""To further explore what kind of performance is possible when using larger models, we consider two additional variants. In both cases, we use d_model = 1024, a 24 layer encoder and decoder, and dkv = 128. For the “3B” variant, we use dff = 16,384 with 32-headed attention, which results in around 2.8 billion parameters; for “11B” we use dff = 65,536 with 128-headed attention producing a model with about 11 billion parameters""",9.00E+21,"Akronomicon states 1.04e+22 FLOP. Archived source: https://github.com/lightonai/akronomicon/tree/main/akrodb
However, this seems dubiously high.

""We pre-train each model for 2^19 = 524,288 steps on C4 before fine-tuning.""
""In total, this batch size and number of steps corresponds to pre-training on 2^35 ≈ 34B tokens.""
""To compare these mixing strategies on equal footing with our baseline pre-train-then-fine-tune results, we train multi-task models for the same total number of steps: 2^19 + 2^18 = 786,432""
Using the 6DN approximation gives: 6 FLOP/token/param * 2^35 pretrain tokens * (1+1/2 finetune tokens per pretrain token) * 1 iteration of training data* 2.8 billion parameters = 8.659e20 FLOP
https://www.wolframalpha.com/input?i=6+*+2%5E35+*+2.8+billion+*+1.5

update: 9.0E+21 per FLAN paper from Google 
https://arxiv.org/pdf/2210.11416.pdf",0.17,,,Google TPU v3,,16182.2859
M4-50B,10/11/2019,50000000000,"(sparse architecture)

""By modifying the Transformer architecture through the substitution of the vanilla feed-forward layers with sparsely-gated mixture of experts, we drastically scale up the model capacity, allowing us to successfully train and pass 50 billion parameters, which further improved translation quality across the board.""",,"Sparse architecture, so training compute is uncertain",,,,,,
DistilBERT,10/2/2019,66000000,Table 3,1.24E+19,"Section 3: DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours.

1.6e13*8*60**2*90*0.3 = 1.2e19",,,,NVIDIA Tesla V100 DGXS 16 GB,,
AlphaX-1,10/2/2019,5400000,"Table 3: multiadds for AlphaX-1 579M, parameters 5.4M",8.89E+17,""" Our models for ImageNet use polynomial learning rate
schedule, starting with 0.05 and decay through 200 epochs.""

1280000 images * 200 epochs *3 forward-backward adjustment * 1158000000 forward FLOP =889344000000000000",,,,NVIDIA GeForce GTX 1080 Ti,,
ALBERT,9/26/2019,18000000,Section 3.2 of paper,,,79.4,,,Google TPU v3,,
Adaptive Inputs + LayerDrop,9/25/2019,423000000,,,,,,,,,
Megatron-LM (8.3B),9/17/2019,8300000000,"Source: https://lair.lighton.ai/akronomicon/

Archived source: https://web.archive.org/web/20211220142906/https://lair.lighton.ai/akronomicon/

Data also available on GitHub: https://github.com/lightonai/akronomicon/blob/main/akrodb/NVIDIA/Megatron-LM.json",9.10E+21,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb

other estimates:

8.3B is a GPT-2-based model (Table 2). ""For GPT-2 models, all training is performed with sequences of 1024 subword units at a batch size of 512 for 300k iterations"" 

I interpret the above as 1024*512*300k = 157B training tokens 

6 * 157 billion * 8.3 billion  = 7.8e21

Also, their training setup achieved 15.1 petaFLOPS or 1.5e16 FLOPS.
(512 V100s is 512 * 125 teraflops = 64 petaFLOPS so they had ~25% utilization)
2.1 days per epoch, ~4.4 epochs
2.1 * 4.4 * 24 * 3600 * 1.5e16 = 1.197e22

These are both close to the akronomicon estimate

Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 8,100,000 PFLOP = 8.1*10^21 FLOP",4.4,327,"Reported throughput is 15.1 teraFLOPS per GPU on 512 GPUs
Assume total compute is 9.1e21 FLOP.
Then training time is 327 hours.
https://www.wolframalpha.com/input?i=9.1*10%5E21+FLOP+%2F+%28512*15.1+teraFLOPS%29",NVIDIA Tesla V100 DGXS 32 GB,512,113502.8196
Megatron-BERT,9/17/2019,3900000000,"2.1Source: https://lair.lighton.ai/akronomicon/

Archive on GitHub: https://github.com/lightonai/akronomicon/tree/main/akrodb",2.20E+22,"A third-party source: https://lair.lighton.ai/akronomicon/ claims 5.7e22

The authors report experimenting on 1 V100 GPU and achieving throughput of 39 TFLOPS which is 30% of the peak throughput. Therefore the GPU has a peak throughput of 130 TFLOPS so it is specifically the NVIDIA V100S PCIe.
https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf

Param-based calculation:
6ND = 6*3.9e9*(2e6+1e4)*1024*512 = 2.5e22 FLOP

1024 is the batch size, 512 is the sequence length (not explicitly stated but they say non-specified hyperparameters follow cited papers).

Time-based calculation:
The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512. An epoch was 68.5k iterations with sequence length 1024.

Halving the model size should ~halve the iteration time.
Doubling the batch size should ~double the iteration time.
Halving the sequence length should ~quarter the iteration time (quadratic scaling).

Hence 3.1e-5 days/iteration * 2 * 1/2 * 1/4 = 7.8e-6 days/iteration.

2e6 iterations => seems like 15.6 days training.

On 512 GPUs they achieve a peak throughput of 15.1 PFLOPS.
C=15.1 PFLOPS * 58 days = 2.0e22 FLOP.

If we disregard the Akronomicon estimate and aggregate our two, geometric mean is 2.2e22 FLOP",,374,"The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512, sequence length 1024. An epoch was 68.5k iterations.

BERT: batch size 1024, sequence length 512, 2e6 iterations total.

Halving the model size should ~halve the iteration time.
Doubling the batch size should ~double the iteration time.
Halving the sequence length should ~quarter the iteration time (quadratic scaling).

Hence 3.1e-5 days/iteration * 2 * 1/2 * 1/4 = 7.8e-6 days/iteration.

2e6 iterations => seems like 15.6 days training.",NVIDIA Tesla V100S PCIe 32 GB,512,178608.4119
UDSMProt,9/4/2019,28303800,"Python code:  
# Given LSTM parameters
emb_sz = 400  # embedding size, typically equal to the input size for the first layer
nh = 1150     # number of hidden units
nl = 3        # number of layers

# The formula for a single LSTM layer parameters is:
# P = 4 * ((input_dim + hidden_dim) * hidden_dim + hidden_dim)

# First layer parameters (input_dim is the embedding size)
first_layer_params = 4 * ((emb_sz + nh) * nh + nh)

# For subsequent layers, input_dim is equal to hidden_dim (nh)
subsequent_layer_params = 4 * ((nh + nh) * nh + nh)

# Total parameters for all layers
total_params = first_layer_params + (nl - 1) * subsequent_layer_params

print(total_params)",6.37E+17,"Pretraining:
Table 7 gives max of 499k sequences each at (seemingly) L=1024:
499k * 1024 * 28.3M * 6 = 8.7e16

Finetuning:
Largest downstream task has 104940 sequences (Table 5), each sequence has L=1024 residues, 28.3M parameters, and 30 epochs.
105k * 1024 * 30 * 28.3 * 6 = 5.5e17.",30,,,,,
"Mogrifier (d2, MoS2, MC) + dynamic eval",9/4/2019,35000000,,,,145,,,,,
EN^2AS with performance reward,7/22/2019,23000000,,,,,,,,,
Pluribus,7/11/2019,,,6.60E+16,"Trained in 8 days on a 64 core CPU
https://ai.facebook.com/blog/pluribus-first-ai-to-beat-pros-in-6-player-poker/

""We trained the blueprint strategy for Pluribus in eight days on a 64-core server and required less than 512 GB of RAM. No GPUs were used. At typical cloud computing instance rates, it would cost less than $150 to train.""

Guess: trained on i7 Intel CPU, approx 5e9 FLOP/s for each core.

 https://epoch.ai/blog/estimating-training-compute
8 days, 64 cores, 5e9 FLOP/s, 30% utilization",,,,,,
BigBiGAN,7/4/2019,86000000,https://openai.com/blog/image-gpt/#rfref53,,,,,,,,
RoBERTa Large,7/1/2019,355000000,"355M 
https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.md",8.51E+21,"Section 5: We pretrain our model using 1024 V100 GPUs for approximately one day.

Note this is the base pretraining comparable to BERT, 100k steps. Subsequently they do more: ""increasing the number of pretraining steps
from 100K to 300K, and then further to 500K"".

So assume 5x the 1024 V100 GPUs for 1d estimate. Mixed precision tensor cores get 1.25e14 FLOP/s.

1024 * 1.25e14 * 5 * 24 * 3600 * 0.3 = 1.65888e22

6ND estimate: batches are 8k sequences of 512 tokens; 500k updates means the model saw 500k * 8k * 512 = 2.048T tokens
6 * 2.048T * 355M = 4.36224e21

geometric mean: sqrt(1.65888e22 * 4.36224e21) = 8.5067e21

Authors of ""AI and Memory Wall"" estimated model's training compute as 4,300,000 PFLOP = 4.3*10^21 FLOP
(https://github.com/amirgholami/ai_and_memory_wall)",,120,"First the model is pretrained for 100k steps on 1024 GPUs for 1 day, then pretraining is increased to 500k steps, so assuming they used the same number of GPUs, this would have taken 5 days.",NVIDIA Tesla V100 DGXS 32 GB,1024,88691.29948
Tensorized Transformer (257M),6/24/2019,257000000,257M (Table 5) ,4.76E+18,"6 FLOP / parameter / token * 257000000 parameters * 103000000 tokens * 30 epochs = 4.76478e+18 FLOP
",30,,,NVIDIA P40,2,
Walking Minotaur robot,6/19/2019,,,,,,,,,,
LaNet-L (CIFAR-10),6/17/2019,44100000,44.1M,,"LaNet-L was trained on 150 GPU-days, however the GPU was not specified",600,,,,,
PG-SWGAN,6/15/2019,,,,,,,,,,
FixRes ResNeXt-101 WSL,6/14/2019,829000000,,,,,,,,,
Char-CNN-BiLSTM,6/13/2019,,,,,,,,,,
AWD-LSTM + MoS + Partial Shuffled,6/10/2019,35000000,35M (Table 2),3.15E+17,6 FLOP / parameter / token * 35000000 parameters * 2000000 tokens * 750 epochs = 3.15e+17 FLOP,750,,,,,
Transformer-XL Large + Phrase Induction,6/4/2019,257000000,,3.78E+20,"Fine-tuned from pre-trained Transformer-XL Large (upd 3.7832771e+20 FLOP, old estimation 1.09e19 FLOP). 

Total: 3.7832771e20 + 1.588e17 = 3.7848651e+20 FLOP (Speculative confidence same as Transformer XL)",1,,,,,
AMDIM,6/3/2019,626000000,source: https://openai.com/blog/image-gpt/#rfref13e,,,,,,,,
XLNet,6/1/2019,340000000,"Same size as BERT-Large, which was 340M",6.19E+21,"""Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days.""

123 teraflops * 5.5 days * 24 * 3600 * 512 * 0.3 utilization (assumption) ~= 8977858560*10^12=8.9*10^21

Alternatively, 500k steps * batch size 8192 * sequence length 512 = 2.1T training passes. 340 million * 6 * 2 trillion = 4.3e21 FLOP. 

Geometric mean: sqrt(8.9e21 * 4.3e21) = 6.19e21",63.76260261,,,Google TPU v3,,12730.06373
XLM,6/1/2019,665000000,,,,,,,,,
DLRM-2020,5/31/2019,1E+11,"Figure 1

https://arxiv.org/abs/2104.05158",4.00E+18,"Figure 1

https://arxiv.org/abs/2104.05158",,,,,,
MnasNet-A3,5/29/2019,5200000,From https://arxiv.org/pdf/1807.11626.pdf,1.50E+21,"""each architecture search takes 4.5 days on 64 TPUv2 devices""
This seems to be referring to a TPUv2 pod, consisting of 64 four-chip modules. The total performance is 11.5 petaFLOPS.
https://en.wikipedia.org/wiki/Tensor_Processing_Unit#Second_generation_TPU
Assuming a 33% utilization rate:

4.5 days * 64 * 180 teraFLOPS * 0.33 = 1.48*10^21 FLOP

However, it is unclear if ""64 TPUv2 devices"" refers to chips or modules, so the true compute might be 1/4 of this amount.",,108,,Google TPU v3,256,9551.59162
MnasNet-A1 + SSDLite,5/29/2019,4900000,From https://arxiv.org/pdf/1807.11626.pdf,1.50E+21,"""each architecture search takes 4.5 days on 64 TPUv2 devices""
This seems to be referring to a TPUv2 pod, consisting of 64 four-chip modules. The total performance is 11.5 petaFLOPS.
https://en.wikipedia.org/wiki/Tensor_Processing_Unit#Second_generation_TPU
Assuming a 33% utilization rate:

4.5 days * 64 * 180 teraFLOPS * 0.33 = 1.48*10^21 FLOP

However, it is unclear if ""64 TPUv2 devices"" refers to chips or modules, so the true compute might be 1/4 of this amount.",,108,,Google TPU v3,256,9551.59162
EfficientNet-L2,5/28/2019,480000000,,,,,,,,,
CPC v2,5/22/2019,303000000,source: https://openai.com/blog/image-gpt/#rfref25d,,,,,,,,
AWD-LSTM-DRILL + dynamic evaluation† (WT2),5/14/2019,34000000,"34M, Table 2",4.08E+17,6 FLOP / parameter / token * 34000000 parameters * 2000000 tokens * 1000 epochs = 4.08e+17 FLOP,1000,29,106 sec per epoch (Table 3) -> 106000 seconds = 29 hours,,,
ResNeXt-101 Billion-scale,5/2/2019,193000000,,,,,,,,,
ResNet-50 Billion-scale,5/2/2019,25000000,25M parameters vanilla ResNet50,,,,,,,,
Neuro-Symbolic Concept Learner,4/26/2019,,,,,,,,,,
DANet,4/21/2019,,,,,,,,,,
BERT-Large-CAS (PTB+WT2+WT103),4/20/2019,395000000,395M (Table 6),1.54E+20,"6 FLOP / token / parameter * 395000000 parameters * 1300000000 parameters * 50 epochs = 1.5405e+20 FLOP

________
in the Algorithmic progress paper, the estimation was 5.21E+20 FLOP",50,,,,,
SpecAugment,4/18/2019,,,,,,,,,,
Transformer-XL + RMS dynamic eval,4/17/2019,257000000,,,,,,,,,
MEGNet (molecule model),4/10/2019,8720,Calculations here: https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.0#heading=h.dcux1bvmijlm,4.54E+17,"Calculations here: 
https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.0#heading=h.c0h2t2icf5xr

Assuming a single GPU (the model is quite small):
1.0e+3*100*1.1e+13*0.4=4.5e+17",1000,28,,NVIDIA GeForce GTX 1080 Ti,1,
MEGNet (crystal formation energy model),4/10/2019,26128,Calculations here: https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.0#heading=h.913mrln2g0cv ,4.54E+17,"Calculations here: https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.0#heading=h.obxgi47b0fxe

Updated calculation assuming 100s per epoch and a single GPU.
1000*100*11340000000000*0.4=453600000000000000",1000,28,,NVIDIA GeForce GTX 1080 Ti,,
MEGNet (crystal elasticity model),4/10/2019,26128,"Calculations here:
https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.0#heading=h.913mrln2g0cv
",4.54E+17,"Calculations here:
https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.0#heading=h.3ria9ya1ty1o


Updated calculation assuming 100s per epoch and a single GPU.
1000*100*11340000000000*0.4=453600000000000000
",1000,28,,NVIDIA GeForce GTX 1080 Ti,,
MEGNet (crystal band gap model),4/10/2019,26128,"Calculations here:
https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.0#heading=h.913mrln2g0cv",4.54E+17,"Calculations here:
https://docs.google.com/document/d/1BTmyZ9KVTIwkp9z9tRRFsWM5A0QcXiufXrIhujffyso/edit?tab=t.0#heading=h.lk6l1te7vrkv


Updated calculation assuming 100s per epoch and a single GPU.
1000*100*11340000000000*0.4=453600000000000000",1000,28,,NVIDIA GeForce GTX 1080 Ti,1,
WeNet (Penn Treebank),4/8/2019,23000000,Table 1,7.30E+17,"PTB has 912344 tokens. The model has 23M parameters and was trained for 6k epochs. If the model was dense, 6 FLOP/token/param/epoch * 6k epochs * 23M params * 912k tokens = 1.05e18 FLOP.

Alternatively, the model was trained on 1 V100 GPU and ""In terms of efficiency, the overall cost... is within 1 GPU day"" so the training time was around or below 24 hours. Half precision and 30% utilization would be a pretty good match for the arithmetic estimate: 24 hours * 30% * 28 TFLOPS = 7.3e17 FLOP.",6000,24,,NVIDIA V100,1,
True-Regularization+Finetune+Dynamic-Eval,4/8/2019,7000000,,,,,,,,,
Cross-lingual alignment,4/4/2019,,,2.56E+18,"From author communication:

Precision: float32

Hardware: 4 GPU  NVIDIA 1080Ti

NVIDIA 1080Ti: 1.06E+13

Compute: 7 GPU-days

0.4 * 1.06E+13 FLOP/s * 7 days * 24h/day * 3600s/h
= 2.56E+18",,,,NVIDIA GeForce GTX 1080 Ti,,
FAIRSEQ Adaptive Inputs,4/1/2019,247000000,"""The first model has 16 blocks, inner dimension 4K and embedding dimension 1K""

247M as in Baevski and Auli (2019) Transformer",3.18E+19,"6 FLOP / parameter / token * 247000000 parameters * 103000000 tokens * 180 epochs = 2.747628e+19 FLOP 

for translation model, Table 2:

31330000000000 FLOP / second / GPU * 128 GPUs * 8.5 hours * 3600 seconds / hour * 0.3 [assumed precision] = 3.6814003e+19 FLOP

sqrt(2.747628e+19*3.6814003e+19) = 3.1804274e+19 FLOP

Speculative confidence since amount of parameters and epochs are assumed as well as hardware estimation is given for another model in the paper

__________
In the Algorithmic progress paper the estimation was 7.30E+18 FLOP",180,,,NVIDIA V100,,
SciBERT,3/26/2019,110000000,"110M
size of bert base from https://huggingface.co/google-bert/bert-base-uncased
relevant citation: 
""We use the original BERT code to
train SCIBERT on our corpus with the same con-
figuration and size as BERT-Base. We train 4
different versions of SCIBERT: (i) cased or un-
cased and (ii) BASEVOCAB or SCIVOCAB. The
two models that use BASEVOCAB are finetuned
from the corresponding BERT-Base models. The
other two models that use the new SCIVOCAB are
trained from scratch.""",8.93E+19,"4*123e12*0.3*(7*24*3600) = 8.926848e+19
(num gpu) * (peak compute) * (assumed utilization rate) * (time in seconds)
We have:
 4 TPUv3 chips.123teraFLOPS per chip.
7 days of training
""We use a single TPU v3 with 8 cores. Training the SCIVOCAB models from scratch on our corpus takes 1 week (5 days with max length 128, then 2 days with max length 512). ""

If this compute estimate is accurate and BERT is approximately dense, then C=6eND -> e=C/6ND ~= 40 epochs.",,168,1 week,Google TPU v3,4,247.2628901
NMT Transformer 437M,2/28/2019,437700000,"""Regarding the model, for these experiments we
use a larger Transformer model with 6 layers in
both the encoder and the decoder, model dimension set to 1024, hidden dimension size of 8192,
and 16 attention heads. This results in a model
with approximately 473.7M parameters.""",,,,,,,,
KataGo,2/27/2019,2500000,https://arxiv.org/abs/2210.00849 gives parameter count for AlphaZero in Fig 1b.,2.32E+19,"""[KataGo] surpasses the strength of ELF OpenGo after training on about 27 V100 GPUs for 19 days""
14.13 teraFLOP/s * 19 days = 2.32e+19 FLOP",,456,27 processors for 19 days,NVIDIA Tesla V100 DGXS 16 GB,,104.9142585
ProxylessNAS,2/23/2019,,,3.72E+18,"For their searched Imagenet models, they used 200 GPU hours on a V100 GPU.

At FP32, a V100 GPU has a peak performance of 1.56E+13 FLOPS.

Utilization rate of 0.33.

200 h * 3600 second / hour* 1.6e+13 flop /second * 0.33 = 3.7e+18 flop",,,,NVIDIA V100,,122.7411066
GPT-2 (1.5B),2/14/2019,1500000000,"""GPT-2 is a large transformer-based language model with 1.5 billion parameters""",1.92E+21,"Estimating based on compute = 6 FLOP/token/param * epochs * parameters * tokens.

40GB dataset is approximately 8B words, or 1/0.75 * 8B = 10.66B tokens.

The number of epochs is not reported, but another paper [1] claims in table 1 that it is 20 or 100 epochs, and another paper [2] claims 12 epochs based on communication with the GPT-2 authors (page 4).

12 epochs is the modal, most credible value. Mean of probability mass is probably around 20 epochs, so calculating from that value:

6 * (40 * 200 million * 1/0.75 * 20) * 1.5 billion parameters = 1.92e21
https://www.wolframalpha.com/input?i=6+FLOP+*+20+*+%2840+billion+%2F+5+*+%284%2F3%29%29+*+1.5+billion

[1] https://arxiv.org/abs/1906.06669 One Epoch Is All You Need
[2] https://www.usenix.org/system/files/sec21-carlini-extracting.pdf Extracting Data From Large Language Models

It also appears the model was trained on TPU v3 chips:
https://huggingface.co/openai-community/gpt2",20,,,Google TPU v3,,4348.443645
Hanabi 4 player,2/1/2019,764000,source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389,4.30E+18,14.13e+12 FLOP/s * 7 days * 86400 s/day * 0.50 utilization = 4.3e+18 FLOP,,,,NVIDIA V100,,
MT-DNN,1/31/2019,330000000,,,,,,,,,
Transformer-XL (257M),1/9/2019,257000000,"Transformer-XL Large, Table 1",3.78E+20,"6 FLOP / token / parameter * 257000000 parameters * 103000000 tokens * 1908 epochs [see dataset size notes] = 3.0304001e+20 FLOP

from training code (https://github.com/kimiyoung/transformer-xl/blob/master/tf/scripts/wt103_large_tpu.sh) they used 64 tpv3 cores 

123000000000000 FLOP/s/chip* (64 cores / 2 cores per chip) * 4000000 steps *  0.1 sec / step [assumption] * 0.3 [assumed utilization] = 4.7232e+20 FLOP

geometric mean
sqrt(3.0304001e+20 * 4.7232e+20) = 3.7832771e+20

speculative confidence given assumptions used
_________
previous estimation in the algorithmic progress paper was 1.09 × 10^19 FLOP without explanation 
",1908,,,Google TPU v3,32,
Decoupled weight decay regularization,1/4/2019,36500000,"From author communication

WideResNet 28-10 models with 36.5 million parameters (3.65E+07)",4.72E+17,"From author communication

Per image: 5.24 billion FLOPs (5.24E+09)  Per training run: 50k times 5.24E+09 times 1800 epochs

5240000000*50000*1800=471600000000000000=4.72e17",,,,,,
Transformer ELMo,1/1/2019,56000000,,,,,,,,,
GPipe (Transformer),11/16/2018,6000000000,Section 5: ,,,,,,,,
GPipe (Amoeba),11/16/2018,557000000,Section 4,,,,,,,,
Multi-cell LSTM,11/15/2018,7200000,"Based on the details in the paper, the number of parameters in the Multi-cell LSTM model can be calculated as follows:

The model has 2 hidden LSTM layers, each with 1500 hidden units

Each LSTM unit is a multi-cell LSTM with 10 memory cells per unit

For a standard LSTM layer with n hidden units:

W matrix: n x input_size
U matrix: n x n
4 bias vectors of size n (for input, forget, cell, output gates)
So for each multi-cell LSTM layer with 1500 units and 10 cells per unit:

W matrix: 1500 x input_size
U matrix: 1500 x 1500
4 bias vectors of size 1500
Number of parameters is same as standard LSTM layer

For the 2 hidden layers:

Input size for Layer 1: embedding dimension (estimated 300 in paper)

Input size for Layer 2: 1500 (output of layer 1)

Total params =
Layer 1: 1500 x (300 + 1500 + 4) = 2,706,000
Layer 2: 1500 x (1500 + 1500 + 4) = 4,506,000

Total Parameters = 2,706,000 + 4,506,000 = 7,212,000

So the total number of parameters for the Multi-cell LSTM model with 2 layers of 1500 units and 10 cells per unit is approximately 7.2 million.",2.00664E+15,6 FLOP / parameter / token * 7200000 parameters * 929000 tokens * 50 epochs = 2.00664e+15 FLOP,50,,,,,
Fine-tuned-AWD-LSTM-DOC (fin),11/12/2018,46000000,"This is the model trained on Penn Treebank, which uses as a base model the 23M model from Table 7 in https://aclanthology.org/D18-1489.pdf

They additionally train a discriminator with the same architecture, so total parameters is 2*23M = 46M",5.19E+16,"Base model uses 4.323e16 FLOPs.
They then train a discriminator using the same architecture for 30 epochs, and then use the discriminator to fine-tune the base model for another 15 epochs. Both of these latter training steps require running forward passes on both the discriminator and the language model, but only doing a backward pass on one of them.

Discriminator training: 
2*23M*30*1044112 + 6*23M*30*1044112 = 5.763e15

LM fine-tuning:
2*23M*15*1044112 + 6*23M*15*1044112 = 2.882e15

Total:
4.323e16 + 5.763e15 + 2.882e15 = 5.188e16",15,,,,,
Mesh-TensorFlow Transformer 4.9B (language),11/5/2018,4900000000,4.9B from section 9.1 : ''The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.',1.62E+20,"flops = (256) * ( 45 * 10**12) * (13 * 3600) * (0.3) = 1.6e20
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from section 9.1 : ''The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.'
from https://en.wikipedia.org/wiki/Tensor_Processing_Unit 
45TFLOPs per chips",10,13,"from section 9.1 ""For the billion-word language modeling benchmark, we trained the models for 10 epochs. The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.""",Google TPU v2,256,935.3300509
Mesh-TensorFlow Transformer 2.9B (translation),11/5/2018,2900000000,"2.9B from section 9.1 : ""On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model
(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.""",6.84E+19,"flops = (64) * ( 45 * 10**12) * (22 * 3600) * (0.3) = 6.8e19
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)

from section 9.1 : ""On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model
(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.""
from https://en.wikipedia.org/wiki/Tensor_Processing_Unit 
45TFLOPs per chips",10,22,"from section 9.1 ""On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model
(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.""",Google TPU v2,64,395.71656
MemoReader,10/31/2018,,,,"""Our model does require more memory than existing methods, but a single GPU (e.g., M40 with 12GB memory) was enough to train model within a reasonable amount of time""

""Reasonable"" could mean anything, maybe hours to a few days.",,,"""reasonable amount of time"" with a single GPU",NVIDIA M40,,
TrellisNet,10/15/2018,180000000,"180M, Table 2",2.78E+18,6 FLOP / parameter / token * 180000000 parameters * 103000000 tokens * 25 epochs = 2.781e+18 FLOP,25,,,,,
MetaMimic,10/11/2018,22000000,"""This representational demand motivates the introduction of high-capacity deep neural networks. We found the architecture, shown in Figure 3, with residual connections, 20 convolution layers with 512 channels
for a total of 22 million parameters, and instance normalization to drastically improve performance, as shown in Figure 6 of the Experiments section.""",,,,,,,,
BERT-Large,10/11/2018,340000000,340M,2.85E+20,"more info here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit?usp=sharing
285000000000000000000 = 2.85 × 10^20

""AI and Memory Wall"" paper (https://github.com/amirgholami/ai_and_memory_wall) made an estimation of 250,000 PFLOPS = 2.5*10^20 FLOP",40,96,"from appendix A.2: ""Training of BERTLARGE was performed
on 16 Cloud TPUs (64 TPU chips total). Each pre-
training took 4 days to complete.""",Google TPU v2,64,1751.477009
Transformer (Adaptive Input Embeddings) WT103,9/28/2018,247000000,Table 2,4.47E+19,"8 V100s * 67 hours per Table 2.
125e12 FLOP/sec * 8 * 67 * 3600 * 0.3 (utilization assumption) = 7.2e19 FLOP

They also say they trained for 286k steps in batches of 65,536 tokens.
6 * 247M * (286k * 65536) = 2.78e19

geometric mean: sqrt(7.2e19 * 2.78e19) = 4.47e19",180,67,,NVIDIA V100,8,2880.917279
BigGAN-deep 512x512,9/28/2018,112694781,"I used the publicly available implementation available at [1]

There I loaded the biggan-deep512/1 model, and ran script [2] to compute the number of parameters

[1] https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb

[2]
n_params = 0
for var in module.variables:
  n_params += np.prod(var.shape.as_list())
  pass

print(n_params)",1.80E+21,"3e21, estimate taken from:

https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening",,48,"""We train on a Google TPU v3 Pod, with the number of cores proportional to the resolution: 128 for 128×128, 256 for 256×256, and 512 for 512×512. Training takes between 24 and 48 hours for most models""",Google TPU v3,256,5170.456706
LSTM+NeuralCache,9/24/2018,2100000,"Given:

Hidden size (H) = 512
Number of hidden layers (L) = 1
Input size (I) is not mentioned, so let's denote it as I
The total number of parameters P in an LSTM can be calculated as follows:

P = 4 * ((I * H) + (H * H) + H)

This is for one layer of LSTM cells. Since the LSTM model described has only one layer, we don't need to multiply by the number of layers.

To calculate the exact number of parameters, we would need to know the input size I. However, if you are looking for the number of parameters just within a single LSTM cell (assuming I is equal to H), then you can substitute I with H in the above formula:

P = 4 * ((H * H) + (H * H) + H)
= 4 * (2 * (H^2) + H)

For H = 512, this becomes:

P = 4 * (2 * (512^2) + 512)
= 4 * (2 * 262144 + 512)
= 4 * (524288 + 512)
= 4 * 524800
≈ 2,099,200",9.828E+14,6 FLOP / parameter / token * 2100000 parameters * 2000000 tokens * 39 epochs = 9.828e+14 FLOP,39,,,,,
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)",9/18/2018,35000000,,,,,,,,,
Transformer + Simple Recurrent Unit,9/17/2018,90000000,"5-layer model, Table 3",1.10E+19,"""We use a single NVIDIA Tesla V100 GPU for each model. The published results were obtained
using 8 GPUs in parallel, which provide a large effective batch size during training. To approximate
the setup, we update the model parameters every 5×5120 tokens and use 16,000 warm-up steps
following OpenNMT suggestions. We train each
model for 40 epochs (250,000 steps), and perform
3 independent trials for each model configuration.
A single run takes about 3.5 days with a Tesla V100 GPU.""

125 trillion * 3.5 * 24 * 3600 * 0.3 = 1.1e19",40,,,NVIDIA V100,8,45.37321924
ESRGAN,9/1/2018,,,,,,,,,,
(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2),8/30/2018,185000000,185M (table 8),6.66E+17,6 FLOP / parameter / token * 185000000 parameters * 2000000 tokens * 300 epochs = 6.66e+17 FLOP,300,,,,,
Big Transformer for Back-Translation,8/28/2018,,"""We re-implemented the Transformer model in py-
torch using the fairseq toolkit.1 All experiments
are based on the Big Transformer architecture with
6 blocks in the encoder and decoder. We use the
same hyper-parameters for all experiments, i.e.,
word representations of size 1024, feed-forward
layers with inner dimension 4096. ""

I am not sure what authors mean by 'Big Transformer architecture'",4.78E+20,"(128) * (1.25e14) * (27*3600 + 40*60) * (0.3)  = 4.7808e20
(number of gpus) * (peak flops) * (seconds) * (assumed utilization rate)  

""We run experiments on DGX-1 machines with 8Nvidia V100 GPUs and machines are interconnected by Infiniband. Experiments are run on 16
machines and we perform 30K synchronous updates.""
""We also use the NCCL2 library [...] with 16-bit floating point
operations""

NCCL2 supported tensor core operations at 1.25e14 FLOP/s on a V100 for FP16 

in section 5.6 we have

""train this system we perform 300K training up-
dates in 27h 40min on 128 GPUs;""",,27.666,"""training updates in 27h 40min on 128 GPUs""",NVIDIA Tesla V100 DGXS 16 GB,128,2442.161878
AWD-LSTM-MoS+PDR + dynamic evaluation (WT2),8/14/2018,35000000,,,,,,,,,
Big-Little Net (speech),7/10/2018,3320000,table 3,4.29E+17,980000000 (number of FLOPs from table 3) * 27360000 (dataset size) * 16 (number of epochs from appendix B.1) = 429004800000000000,16,,,,,
Big-Little Net,7/10/2018,77360000,Table 2,2.46E+17,"Using the 6ND formula: 
6×number of tokens×number of parameters×number of epochs
6×1.28×10^6×77360000×110=6.5353728e+16 FLOPs

9.32*10^9 (flops per inference)*1.28×10^6(dataset size)/16 (batch size) * 110 epochs * 3 (to account for backpropagation)= 2.46048e+17 FLOPs",110,,,NVIDIA Tesla K80,,
RCAN,7/8/2018,16000000,"""EDSR has much larger number of parameters (43 M) than ours
(16 M), but our RCAN obtains much better performance.""",,,,,,,,
ShuffleNet v2,6/30/2018,2280000,,,,,,,,,
QT-Opt,6/27/2018,1200000,"""The Q-function Qθ(s, a) is represented in our system by a large convolutional neural network with 1.2M parameters""",3.49E+19,"""We distribute training across 10 GPUs, using asynchronous SGD with momentum... This system allows us to train the Q-function at 40 steps per second with a batch size of 32 across 10 NVIDIA P100 GPUs.""

""We found empirically that a large number of gradient steps (up to 15M) were needed to train an effective Q-function...""

15M steps * 0.025 seconds/step *  9.30E+12 FLOP/sec/GPU * 10 GPU = 3.4875E+19",80,104.2,"""We distribute training across 10 GPUs, using asynchronous SGD with momentum... This system allows us to train the Q-function at 40 steps per second with a batch size of 32 across 10 NVIDIA P100 GPUs.""

""We found empirically that a large number of gradient steps (up to 15M) were needed to train an effective Q-function...""

15M steps * 0.025 seconds/step *  1/3600 hours/second = 104.2 hours",NVIDIA P100,,1317.803586
DARTS,6/24/2018,33000000,"33M (Table 4) - parameters reported for PTB, but they say it is the same for WT-2

"" WIKITEXT-2
We use embedding and hidden sizes 700, weight decay 5×10−7, and hidden-node variational dropout 0.15. Other hyperparameters remain the same as in our PTB experiments.""",3.24E+17,"6 FLOP / parameter / token * 33000000 parameters * 2000000 tokens * 300 epochs = 1.188e+17 FLOP

11340000000000 FLOP / second / GPU * 1 GPU * 72 hours * 3600 sec / hour * 0.3 [assumed utilization] = 8.817984e+17 FLOP

sqrt(1.188e+17*8.817984e+17) = 3.2366286e+17 FLOP

'Speculative' confidence since many variables are assumed based on PTB model training",300,72,"for PTB (they say WT-2 training is similar): The training takes 3 days on a single 1080Ti GPU with our PyTorch implementation

3*24 = 72 hours",NVIDIA GeForce GTX 1080 Ti,1,
MobileNetV2,6/18/2018,3400000,Rados,,,,,,,,
Relational Memory Core,6/5/2018,,,,,,,,,,
GPT-1,6/1/2018,117000000,"""The model had 117M parameters in total.""

source: https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2",1.76E+19,"COMPUTE = FORWARD COMPUTE PER TOKEN * 3 BACKWARD FORWARD ADJUSTMENT * EPOCHS * DATASET SIZE

""We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.""

Authors of ""AI and Memory Wall"" estimated model's training compute as 57,000 PFLOPS = 5.7*10^19 FLOP
(https://github.com/amirgholami/ai_and_memory_wall)",,720,"""1 month on 8 GPUs."" from the reference link",NVIDIA Quadro P600,8,
aLSTM(depth-2)+RecurrentPolicy (WT2),5/22/2018,32000000,32M (Table 3),7.30E+16,6 FLOP / token / parameter * 32000000 parameters * 2000000 tokens * 190 epochs = 7.296e+16 FLOP,190,,,,,
Dropout-LSTM+Noise(Bernoulli) (WT2),5/3/2018,51000000,"""The large network has 2 layers with 1500 hidden units each. This leads to a model complexity of 51 million parameters.""",1.27E+17,"6 FLOP / parameter / token * 51000000 parameters * 2000000 tokens * 200 epochs = 1.224e+17 FLOP

'Likely' confidence because I am not very sure that 51M paramters and 200 epochs relate to WT-2 model, but it is very likely",200,,,,,
ResNeXt-101 32x48d,5/2/2018,829000000,"Table 6
",8.74E+21,"Table 6: 153e9 mult-adds.
Section 2.4: ""minibatches of 8,064 images"".

Compute = 2 * 3 * mult-adds * dataset size = 2 * 3 * 153e9 * 9525e6 = 8.74e21 FLOP

Likely trained on V100s, since Facebook had just upgraded their Big Basin GPU cluster to V100s as of March 2018. The previous iteration of Big Basin had 32 clusters of 8xP100s, while Big Basin v2 had 42 clusters of 8xV100s, which matches the 336 GPUs used in this paper.",,496,"""Mahajan et al. (2018) required 19 GPU years to train their ResNeXt101-32x48d"" https://arxiv.org/abs/2103.00020
Models were trained on 336 GPUs, so that suggests 20.65 days or 496 hours",NVIDIA V100,336,139304.6861
Diffractive Deep Neural Network,4/14/2018,8000000000,"""For example, using five 3D-printed transmission layers, containing a total of 0.2 million neurons and ~8.0 billion connections that are trained using deep learning, we experimentally demonstrated the function of a handwritten digit classifier.""

My understanding is that every connection correspond to the parameter to learn.",,,,,,,,
YOLOv3,4/8/2018,56933216,"Feature extractor (ignoring biases)
32*3*3*3 +
64*3*3*32 +
32*1*1*64 +
64*3*3*32 +
128*3*3*64 +
2*(64*1*1*128 +
128*3*3*64) +
256*3*3*128 +
8*(128*1*1*256 +
256*3*3*128) +
512*3*3*256 + 
8*(256*1*1*512 + 
512*3*3*256) + 
1024*3*3*512 + 
4*(512*1*1*1024 +
1024*3*3*512) +
4*4*1024*1000

source: table 1
This is assuming the average pooling step changes the output size from 8x8 to 4x4.

The weights file is 237MB. If the weights are saved as float32, 4 bytes per weight, then there are approximately 237M/4=59M parameters, consistent with the calculation above.",1.34E+19,"We use the formula training_compute = ops_per_forward_pass * 3.5 * n_epochs * n_examples

Assuming 160 epochs of training as in https://arxiv.org/pdf/1612.08242.pdf

Table 2: 18700000000 operations 

18700000000 ops * 3.5 *160 epochs * 1281167 images",,,,"NVIDIA M40,NVIDIA GeForce GTX TITAN X",,
"LSTM (Hebbian, Cache, MbPA)",3/27/2018,530442240,"Single layer LSTM with hidden dimension of 2048. Vocabulary for Gutenberg is 242,621; input and output embeddings are tied.

Embedding layer (tied): 242,621 * 2048 = 496,887,808
LSTM layer: 4 * (2048 + 2048) * 2048 = 33,554,432

Total: 496,887,808 + 33,554,432 = 530,442,240",3.33E+19,"They do training runs on a vision task and three language datasets. The largest dataset by size is GigaWord, but the largest training run is on the Gutenberg dataset, at 15B tokens. 

I assume the input embedding is done with an embedding lookup for efficiency rather than a dense matrix multiplication, so we only count FLOPs on the de-embedding.

Ops counting:
6 * 15B * 530,442,240 = 4.774e19

Hardware:
(8 * 1.87e13) * (6 * 24 * 3600) * 0.3 = 2.327e19

Geometric mean: sqrt(4.774e19 * 2.327e19) = 3.33e19

(Note they say 15B steps, but they also say it is 80 epochs on a 175M token dataset, and that it took 6 days on 8 P100s, both of which would agree with 15B tokens, not 15B steps)",80,144,6 days,NVIDIA P100,8,590.5320553
4 layer QRNN (h=2500),3/22/2018,151000000,Table 6,5.92E+17,"20670000000000 FLOP / sec / GPU [fp16 assumed] * 1 GPU * 12 hours * 3600 sec / hour * 0.3 [assumed utilization] = 2.678832e+17FLOP

6 FLOP / token / parameter * 151000000 parameters * 103000000 tokens * 14 epochs = 1.306452e+18 FLOP

sqrt(2.678832e+17*1.306452e+18) = 5.9158815e+17 FLOP
__________________
in the algorithmic progress paper the estimation was 2.4 × 10^17 FLOP under assumption of 26M parameters",14,12,"""Results are obtained in only 12 hours (WikiText-103) to 2 days (enwik8) using a single modern GPU""",NVIDIA Quadro GP100,1,
Rotation,3/21/2018,86000000,https://openai.com/blog/image-gpt/#rfref53,,,,,,,,
LSTM (2018),3/4/2018,13000000,,,,,,,,,
Chinese - English translation,3/1/2018,,,,,,,,,,
Residual Dense Network,2/24/2018,,,,,200,,,,,
Spectrally Normalized GAN,2/16/2018,,,,,,,,,,
TCN (P-MNIST),2/15/2018,42000,,,,,,,,,
ENAS,2/9/2018,24000000,"24M
Table 1",2.01E+16,"Training on PTB:
6 FLOP / token / parameter * 24000000 parameters * 929000 tokens * 150 epochs = 2.00664e+16 FLOP",150,,,NVIDIA GeForce GTX 1080 Ti,,
DeepLabV3+,2/7/2018,,,,,,,,,,
AmoebaNet-A (F=448),2/5/2018,469000000,Table 2,3.85E+20,"450 K40 GPUs for 20k models (approx. 7 days).
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",,168,"""Each experiment ran on 450 K40 GPUs for 20k models (approx. 7 days).""",NVIDIA Tesla K40s,450,11766.33968
AmoebaNet-A (F=190),2/5/2018,87000000,Table 2,,,,,,,,
QRNN,2/1/2018,135000000,"Based on the details provided in the paper, the number of parameters in the QRNN model can be calculated as follows:

Embedding layer size: 400
Number of QRNN layers: 4
Number of nodes per QRNN layer: 2500
Vocabulary size: 267,735 (for WikiText-103 dataset)
Adaptive softmax layer
Parameters:

Embedding layer: 400 x 267,735 = 107,094,000
QRNN layers:
Input to hidden weights per layer: 400 x 2500 = 1,000,000
Hidden to hidden weights per layer: 2500 x 2500 = 6,250,000
Biases per layer: 2500 = 2,500
Total QRNN layers parameters: 4 x (1,000,000 + 6,250,000 + 2,500) = 28,000,000
Adaptive softmax layer: No extra parameters due to weight tying
Total Parameters = Embedding + QRNN layers
= 107,094,000 + 28,000,000
= 135,094,000

So the total number of parameters in the QRNN model is approximately 135 million.

The majority of parameters are in the embedding layer, while the 4 QRNN layers contribute 28 million parameters. The adaptive softmax does not add any extra parameters due to weight tying.",6.89E+17,"6 FLOP / parameter / token * 135000000 parameters ['Likely' confidence] * 103000000 tokens * 14 epochs = 1.16802e+18 FLOP

31330000000000 FLOP / sec / GPU [fp16 assumed] * 1 GPU * 12 hours * 3600 sec / hour * 0.3 [assumed utilization] = 4.060368e+17 FLOP 

sqrt(4.060368e+17*1.16802e+18) = 6.8866472e+17 FLOP 
",14,12,"""The model trains at 2980 seconds per epoch on the NVIDIA V100
and 5460 seconds per epoch on the NVIDIA P100.""

""Using this approach we reduce our per-epoch time substantially and achieve a new state-of-the-art on WikiText-103 despite training for 14 epochs, a total time of only 12 hours.""",NVIDIA V100,,
ELMo,2/1/2018,94000000,,3.3001E+15,3300e12 - https://github.com/amirgholami/ai_and_memory_wall,,,,,,
ULM-FiT,1/18/2018,441000000,https://files.fast.ai/models/wt103/?C=S;O=D,2.73E+17,TRUE,,,,,,
Refined Part Pooling,1/9/2018,,,2.62E+16,12150000000000*3600*2*0.3=2.6244e+16 FLOP,,1,"""With two NVIDIA TITAN XP GPUs and Pytorch as the platform, training an IDE model and a standard
PCB on Market-1501 (12,936 training images) consumes
about 40 and 50 minutes, respectively""",NVIDIA TITAN Xp,2,
Tacotron 2,12/19/2017,,"some architecture details:

""Input characters are represented using a learned 512-dimensional
character embedding, which are passed through a stack of 3 convolutional layers each containing 512 filters with shape 5 × 1, i.e., where
each filter spans 5 characters, followed by batch normalization [18]
and ReLU activations. As in Tacotron, these convolutional layers
model longer-term context (e.g., N-grams) in the input character
sequence. The output of the final convolutional layer is passed into a
single bi-directional [19] LSTM [20] layer containing 512 units (256
in each direction) to generate the encoded features.""",,,,,,,,
AlphaZero,12/5/2017,,,3.67E+22,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,24,,"Google TPU v2,Google TPU v1",5064,229918.6147
2-layer-LSTM+Deep-Gradient-Compression,12/5/2017,6020000,"Here is a summary of the calculations to determine the number of parameters in the 2-layer LSTM model with Deep Gradient Compression described in the paper:

Model has 2 LSTM layers with 1500 hidden units each

For an LSTM layer:

W matrix: number of hidden units * input dimension
U matrix: number of hidden units * number of hidden units
4 bias vectors with size = number of hidden units
So params per LSTM layer = hidden_units * (input_dim + hidden_units + 4)

For layer 1:

Input dim is word embedding size (estimated 300)
Params = 1500 * (300 + 1500 + 4) = 1,512,000
For layer 2:

Input is previous layer output (1500)
Params = 1500 * (1500 + 1500 + 4) = 4,506,000
Total Params = Layer 1 + Layer 2
= 1,512,000 + 4,506,000
= 6,018,000

The Deep Gradient Compression technique does not change number of parameters.

So total parameters for the model is approximately 6 million.",1.34E+15,6 FLOP / parameter / token * 6020000 parameters ['Likely' confidence] * 929000 tokens * 40 epochs = 1.3422192e+15 FLOP,40,,,,,
PNASNet-5,12/2/2017,86100000,Table 5,6.63E+19,"8 times less compute than Zoph (2018), which used 500 p100s for 4 days.
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",,,,,,
PNAS-net,12/2/2017,86000000,,,,,,,,,
TriNet,11/21/2017,,,,,,,,,,
"AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",11/10/2017,35000000,35M (Table 2),3.36E+18,"6 FLOP / parameter / token * 35000000 parameters * 2000000 tokens * 8000 epochs = 3.36e+18 FLOP 

_____________
in the algorithmic progress paper the estimation was 4.37 × 10^17 FLOP based on 1000 epochs assumption",8000,,,,,
Fraternal dropout + AWD-LSTM 3-layer (WT2),10/31/2017,34000000,34M (Table 2),3.06E+17,"6 FLOP / token / parameter * 34000000 parameters * 2000000 tokens * 750 epochs = 3.06e+17 FLOP

_________________
In the Algorithmic Progress paper, the compute was estimated to be 9.85 × 10¹⁶ FLOP, assuming 520 epochs reported for the PTB dataset.",750,,,,,
DCN+,10/31/2017,,"Not directly repoted - It may be possible to extract number from:
https://github.com/lmn-extracts/dcn_plus/tree/master/question_answering",,"in Figure 4 we see that network was trained on 140k iterations
from https://github.com/lmn-extracts/dcn_plus/tree/master we see that batch size is 64
It should be possible to compute inference FLOPs from repository and estimate training compute",,,,,,
S-Norm,10/29/2017,,Not stated. Probably obtainable from github: https://github.com/allenai/document-qa/tree/master,,,1,,,,,
PhraseCond,10/28/2017,,Unclear how many layers they use for self-attention (N) and fusion (L and K). Could calculate if these were known.,,,,,,,,
ProgressiveGAN,10/27/2017,,,,,,,,,,
CapsNet (MultiMNIST),10/26/2017,11360000,"""This model has 24.56M parameters which is 2 times more parameters
than CapsNet with 11.36M parameters.""",,,,,,,,
CapsNet (MNIST),10/26/2017,8200000,"""In terms of number of parameters the baseline has 35.4M while CapsNet
has 8.2M parameters and 6.8M parameters without the reconstruction subnetwork""",,"It should be feasible to estimate this from the information in the paper, but it would require carefully checking the FLOP involved for capsules.",,,,,,
LRSO-GAN,10/22/2017,,,,,,,,,,
AlphaGo Master,10/19/2017,,,2.00E+23,"This is a guess. There was no single journal publication that accompanied this model, that gave information about architecture/model training time etc. All I could find was that it has the same architecture as AlphaGo Zero, and that it had roughly the same power consumption as AGZ. See for instance: 
https://deepmind.com/blog/article/alphago-zero-starting-scratch

Since AGZ reaches the ELO of AlphaGo Master in about 25-30 days (60-75% of the total training time), I estimate the compute to be around 60-75% that of AGZ. I round this to 2e23, and I expect this to only be accurate within an OOM.",,72,"""Training started from completely random behaviour and continued without human intervention for approximately three days.""",Google TPU v1,,471445.3248
AlphaGo Zero,10/18/2017,46400244,"Input size: 19*19*17=6137, internal dimension 19*19
1 conv block
17*3*3*256=39168
39 residual blocks
2*3*3*256*256=1179648
Policy head
256*2+2*19*19*(19*19+2)=262598
Value head
256*1+1*19*19*256+256*1=92928
Total: 39168+39*1179648+262598+92928=46400966",3.41E+23,"source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389


AGZ had two models, one of which was small and another of which was large. The compute for AGZ is for the large model, which has 40 residual blocks instead of 20.

A second way of looking at this... we believe multiple TPUs were used for training. 29 million games * 211 moves per game on average * 0.8 seconds per move = 4.8952E+09 seconds of player-time across all TPUs.

4.8952E+09 seconds of player-time / (40 days * 24 * 60 * 60 seconds of real time) ~= 1,416 players

4 TPUs per player => 4.8952E+09 * 4 = 1.95808E+10 TPU-seconds
Total compute = 1.95808E+10 TPU-seconds * 92E+12 FLOP/(TPU-second) * 0.4 = 7.2e23 FLOP

So similar to the Cotra and Davidson estimate (within a factor of 2).

Alternative operation counting estimate: 
Number of connections (to calculate forward FLOP)
Input size: 19*19*17, Assuming internal dimension stays at 19*19
1 conv block
19*19*17*3*3*256=14139648
40 residual blocks
19*19*2*3*3*256*256=425852928
Policy head
19*19*256*2+2*19*19*(19*19+2)=446918
Value head
19*19*256*1+1*19*19*256+256*1=185088
Total: 14139648+40*425852928+446918+185088=17048888774
Forward FLOP: 2*17048888774=34097777548
Parameter updates “Parameters were updated from 3.1 million mini-batches of 2,048 positions each.”
Total updates: 3100000*2048=6348800000
Paramter update FLOP: 3*6348800000*34097777548=649439910290227200000
MCTS move generation FLOP
“Over the course of training, 29 million games of selfplay were generated.”
From the main training details (not 40 block specific): “using 1,600 simulations for each MCTS”
Assuming each MCTS simulation requires 1 forward pass
Assuming 200 moves on average per game
MCTS FLOP: 34097777548*29000000*200*1600=3.1642737564544e+23
Total: 3.1642737564544e+23+649439910290227200000=3.170768e+23
",,480,,Google TPU v1,,613480.6258
AWD-LSTM+WT+Cache+IOG (WT2),9/26/2017,53000000,53M (Table 3),3.18E+15,6 FLOP / parameter / token * 53000000 parameters * 2000000 tokens * 5 epochs = 3.18e+15 FLOP,5,,,,,
LSTM + dynamic eval,9/21/2017,50000000,table 2,,,,,,,,
ISS,9/15/2017,11100000,11.1M (Table 2),3.4E+15,6 FLOP / parameter / token * 11100000 parameters * 929000 tokens * 55 epochs = 3.402927e+15 FLOP,55,,,,,
PyramidNet,9/6/2017,26000000,best model had 26M params,2.34E+15,6ND=6*26000000*50000*300=2.34e+15,300,,,,,
SENet (ImageNet),9/5/2017,28100000,Table 16,,,,,,,,
GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2),8/29/2017,38000000,38M (Table 2),4.56E+17,6 FLOP / parameter / token * 38000000 parameters * 2000000 tokens * 1000 epochs = 4.56e+17 FLOP,1000,,,,,
Libratus,8/19/2017,,,5.51E+20,"""In total, Libratus used about 25 million core hours. Of those, about 13 million core hours were used for exploratory experiments and evaluation. About 6 million core hours were spent on the initial abstraction and equilibrium finding component, another 3 million were used for nested subgame solving, and about 3 million were used on the self-improvement algorithm.""

""Like many data-centric supercomputers, Bridges offers a relatively a modest number of FLOPS, but lots of memory: 895 teraflops and 130 TB, respectively.""

I just used the first bullet point (as those are usually independent systems and you only benchmark one of them).
The first system has 752 nodes a 2CPUs a 14cores each.

source: https://www.top500.org/news/bridges-supercomputer-boots-up-at-pittsburgh/



1. 12M core hours for 196 cores
2. We have  895 TFLOPS for 752 nodes a 2 CPUs a 14 cores
2.1 That's 42.5 GFLOPS per core.
3. Running this for 12M h
3.1 12 * 10^6 * 60 * 60 * 42.5 * 10^9 FLOP/S = 1.823e21 FLOPs
4. Assuming 30% utilization
 1.823e21 * 0.3
→ 5.51e20 FLOPs",,,"""In total, Libratus used about 25 million core hours. Of those, about 13 million core hours were used for exploratory experiments and evaluation. About 6 million core hours were spent on the initial abstraction and equilibrium finding component, another 3 million were used for nested subgame solving, and about 3 million were used on the self-improvement algorithm.""",,,
Adversarial Joint Adaptation Network (ResNet),8/17/2017,60000000,"Model is based on ResNet (60m params), might have more parameters though

""We implement all deep methods based on the Caffe framework, and fine-tune from Caffe-provided models of AlexNet (Krizhevsky et al., 2012) and ResNet (He et al., 2016), both are pre-trained on the ImageNet 2012 dataset.""",,,,,,,,
NeuMF (Pinterest),8/16/2017,,,,,,,,,,
Cutout-regularized net,8/15/2017,,,,,,,,,,
EI-REHN-1000D,8/14/2017,19000000,19M (Table 4),1.06E+16,6 FLOP / parameter / token * 19000000 parameters * 929000 tokens * 100 epochs = 1.05906e+16 FLOP,100,,,,,
OpenAI TI7 DOTA 1v1,8/11/2017,,"Section 4 states: ""we used a model with over 150 million parameters"" but this is for the 5v5 agent, not the 1v1.",6.05E+20,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,,,,,
RetinaNet-R101,8/7/2017,53000000,source: table 2 in https://arxiv.org/pdf/1911.09070.pdf,2.07E+18,"""We use synchronized SGD over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise specified, all models are trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations. We use horizontal image flipping as the only form of data augmentation unless otherwise noted. Weight decay of 0.0001 and momentum of 0.9 are used. The training loss is the sum the focal loss and the standard smooth L1 loss used for box regression [10]. Training time ranges between 10 and 35 hours for the models in Table 1e.""

NVIDIA M40 GPU

35*60**2*0.3*8*6.83E+12 = 2.07e18",,35,"""We use synchronized SGD over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise specified, all models are trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations. We use horizontal image flipping as the only form of data augmentation unless otherwise noted. Weight decay of 0.0001 and momentum of 0.9 are used. The training loss is the sum the focal loss and the standard smooth L1 loss used for box regression [10]. Training time ranges between 10 and 35 hours for the models in Table 1e.""

NVIDIA M40 GPU

35*60**2*0.3*8*6.83E+12 = 2.07e18",NVIDIA M40,8,
RetinaNet-R50,8/7/2017,34000000,source: table 2 in https://arxiv.org/pdf/1911.09070.pdf,,,,,,,,
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),8/7/2017,33000000,33M (Table 2),2.97E+17,6 FLOP / parameter / token * 33000000 parameters * 2000000 tokens * 750 epochs = 2.97e+17 FLOP,750,,,,,
GSM,7/30/2017,,It could be possible to estimate it from section 3.,,,,,,,,
ConvS2S (ensemble of 8 models),7/25/2017,,,5.64E+19,"All models are implemented in Torch (Collobert et al., 2011) and trained on a single Nvidia M40 GPU except for WMT’14 English-French for which we use a multi-GPU setup on a single
machine. We train on up to eight GPUs synchronously by
maintaining copies of the model on each card and split the
batch so that each worker computes 1/8-th of the gradients;
at the end we sum the gradients via Nvidia NCCL.

1. English-Romanian: ""Training took between 6 and 7.5 days on a single GPU.""
7 days * 24 * 3600 * 6.8e12 FLOP/s (Nvidia M40, fp32) * 0.3 = 1.2e18 FLOP

2. English-German: "" We trained this model on a single GPU over a
period of 18.5 days with a batch size of 48"".
18.5 days * 24 * 3600 * 6.8e12 FLOP/s (Nvidia M40, fp32) * 0.3 = 3.3e18 FLOP

3. English-French: ""Our results are based on training
with 8 GPUs for about 37 days and batch size 32 on each
worker.6 ""
37 days * 24 * 3600 * 8 * 6.8e12 FLOP/s (Nvidia M40, fp32) * 0.3 = 5.2e19 FLOP

the minimum compute needed to train ensemble model: 1.2e18 FLOP + 3.3e18 FLOP + 5.2e19 FLOP = 5.65e19 FLOP

I am not sure how much to add more (they say ensemble model consists of 8 models), probably summarization training takes at least 1.2e18 FLOP more. 

",,,,NVIDIA M40,,
PSPNet,7/21/2017,,,,,,,,,,
NASNet-A,7/21/2017,89000000,,,,,,,,,
AWD-LSTM,7/18/2017,24000000,,,,,,,,,
JFT,7/10/2017,44654504,"Uses ResNet-101 architecture, which has 44,654,504 parameters:
https://resources.wolframcloud.com/NeuralNetRepository/resources/ResNet-101-Trained-on-ImageNet-Competition-Data/",8.43E+20,"Tesla K80 performance: 8.13 TFLOP/s

Assume 40% utilization

60 days * 50 GPUs * 40% utilization * 8.13 TFLOP/s/GPU = 8.43*10^20 FLOP",4,1440,,NVIDIA Tesla K80,50,17239.59303
ShuffleNet v1,7/3/2017,2430000,,,,,,,,,
NoisyNet-Dueling,6/30/2017,,,,,,,,,,
DeepLabV3,6/17/2017,,,,,,,,,,
HRA,6/13/2017,,,,,,,,,,
Transformer,6/12/2017,213000000,"This page suggests the transformer has 213M parameters.

""Although there are others architectures that make use of attention layers, none achieves so good results so fast. Not only that, but the only model that can compite against Transformer is the Slicenet22, proposed just fifteen days before. It takes much longer to train, due to the huge amount of parameters it requires (348 million against the 213 millions of Transformer), and the BLEU scores it achieves are slightly worse on average. In short, up to date it offers no profit over Transformer.""

https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html",7.42E+18,"""The model was trained during 300000 steps, roughly 3.5 days, using 8 NVIDIA P100 GPUs.""

source: https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html

NVIDIA Tesla P100 has 9.3 teraFLOPS single-precision performance

source: https://www.nvidia.com/en-gb/data-center/tesla-p100/

We assume 0.33 utilization performance, in line with OpenAI's ""AI and compute"" article

source: https://openai.com/blog/ai-and-compute/

9.3*10^12 FLOP / GPU / sec * 8 GPUs * 3.5 days * 24 hour / day * 3600 sec / hour * 0.33 [assumed utilization] = 7.4245248e+18 FLOP

In the ""AI and Memory Wall"" paper (https://github.com/amirgholami/ai_and_memory_wall) the estimation is 23,000 PFLOPS = 2.3*10^19 FLOPs. They estimate number of parameters as 65M

Note that Table 2 provides a training compute estimate, but appears not to account for utilization.",3,84,"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using
the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the
bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps
(3.5 days).",NVIDIA P100,8,438.0356518
EDSR,6/10/2017,,,,,,,,,,
Reading Twice for NLU,6/8/2017,,,,,,,,,,
PointNet++,6/7/2017,,,,,,,,,,
Inflated 3D ConvNet,6/1/2017,,,,,,,,,,
SRGAN,5/25/2017,,,,,,,,,,
Mnemonic Reader,5/8/2017,,may be possible to estimate architecture description,,may be possible to estimate from architecture description,,,,,,
DeepLab (2017),4/27/2017,,,,,,,,,,
MobileNet,4/17/2017,4200000,,,,,,,,,
WGAN-GP,3/31/2017,,,,,,,,,,
Mask R-CNN,3/30/2017,,,,,,,"Training with
ResNet-50-FPN on COCO trainval35k takes 32 hours
in our synchronized 8-GPU implementation (0.72s per 16-
image mini-batch), and 44 hours with ResNet-101-FPN",,,
Prototypical networks,3/15/2017,,,,,,,,,,
DnCNN,2/1/2017,,,,,,,,,,
MoE-Multi,1/23/2017,8700000000,"Table 5

https://arxiv.org/abs/1701.06538",9.39E+19,"12 days 
64 NVIDIA K40 GPUs (see hardware data sheet for performance)
0.33 util rate
 ",,288,12 days,NVIDIA Tesla K40t,64,3874.12265
OR-WideResNet,1/7/2017,18200000,18.2M for largest OR-WideResNet model.,,,,,,NVIDIA Tesla K80,,
DeepStack,1/6/2017,2500000,"Figure 3, p.9

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",1.45E+19,"The largest source of compute necessary for training seems to be the data generation job on 20 GPUs. We count this towards the training compute because it requires simulation using the network. This is analogous to the AlphaGo systems simulating Go games.

From p.26: ""For the flop network, one million poker flop situations (from after the flop cards are dealt) were generated and solved. These situations were solved using DeepStack’s depth limited solver with the turn network used for the counterfactual values at public states immediately after the turn card. We used a cluster of 20 GPUS and one-half of a GPU year of computation time.""

Assume they used P100 GPUs because they were common at the time (P100 was released in 2016 and this paper was published in 2017).

But assume low utilization of 10% to hedge on (a) lower-performing GPUs being used, (b) non-FLOP computations taking up a lot of the data generation job.

Calculation:
6 months * 30 days * 24 hours * 3600 seconds * 9.3e12 FLOP/s * 0.1 utilization = 1.446336e+19 FLOP.",,218,from compute notes - around 9 days  - half a year of GPU compute using 20 GPUs,,20,
YOLOv2,12/25/2016,51000000,Source: https://resources.wolframcloud.com/NeuralNetRepository/resources/YOLO-V2-Trained-on-MS-COCO-Data_1,,,,,,,,
GCNN-14,12/23/2016,,,,,35,,,,,
Diabetic Retinopathy Detection Net,12/13/2016,,,,,,,,,,
GAN-Advancer,12/5/2016,,,,,,,,,,
PointNet,12/2/2016,,,,,,,,,,
Elastic weight consolidation,12/2/2016,,,,,,,,,,
Image-to-image cGAN,11/21/2016,,,,,,,,,,
RefineNet,11/20/2016,,,,,,,,,,
PolyNet,11/17/2016,92000000,,6.40E+19,"Section 5: ""ResNet-500 [has] similar computation
costs to our Very Deep PolyNet"".

ResNet-152 has 11.3e9 FLOP per forward pass (https://arxiv.org/abs/1512.03385, Table 1). Hence ResNet-500 has approx 3.7e10 = 11.3e9*500/152 FLOP per forward pass.

560k iterations, batch size 512:
Train compute = 3.7e10*3*2*560e3 * 512 = 6.4e19",,,,NVIDIA GeForce GTX TITAN X,32,617.1106542
ResNeXt-50,11/16/2016,25000000,"""If you’re thinking about ResNets, yes, they are related. ResNeXt-50 has 25M parameters (ResNet-50 has 25.5M).""

https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d",,,,,,,,
Deeply-recursive ConvNet,11/11/2016,,,,,,,,,,
NASv3 (CIFAR-10),11/5/2016,37400000,Table 1,2.20E+21,"50 epochs * 50,000 images * 10.0 GFLOPSs * 12800 networks * 2 add-multiply * 3 backward pass 
= 1.9e6 PF = 22 pfs-days

source: https://openai.com/blog/ai-and-compute/",,,,,800,
NAS with base 8 and shared embeddings,11/5/2016,54000000,54M (Table 2),1.05E+16,6 FLOP / parameter / token * 54000000 parameters * 929000 tokens * 35 epochs = 1.053486e+16 FLOP,35,,,,,
BIDAF,11/5/2016,2600000,"There are two similar models described in sections ""Models details""
citation from the paper about model for SQuAD ""The model has about 2.6 million parameters""
citation about model for cloze test
""The model architecture used for this task is very similar to that for SQuAD (Section 4) with only a few small changes to adapt it to the cloze test. ""
",3.47E+18,"flops = (8) * (6691 * 10**9) * (60 * 3600) * 3 // 10
(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate) =

citation from the section about cloze test experiments ""The entire training process takes roughly 60 hours on eight Titan X GPUs. The other hyper-parameters are identical to the model described in Section 4"" (section 4 is about SQuAD experiments and cloze test experiments require more compute and data).
flops  6.691 TFLOPS from https://www.techpowerup.com/gpu-specs/geforce-gtx-titan-x.c2632",8,60,see compute notes,NVIDIA GeForce GTX TITAN X,8,41.25014557
VD-LSTM+REAL Large,11/4/2016,51000000,51M (Table 3),2.13E+16,6 FLOP / parameter / token * 51000000 parameters * 929000 tokens * 75 epochs = 2.132055e+16 FLOP,75,,,,,
SPIDER2,10/28/2016,409536,"Three networks, each three layers. First takes in 459 inputs and outputs 12, second and third take in 459 + (12*17) = 663 inputs.

Network 1: (459 * 150 + 150) + (150 * 150 + 150) + (150 * 150 + 150) + (150 * 12 + 12) = 116,112
Networks 2 and 3: (663 * 150 + 150) + (150 * 150 + 150) + (150 * 150 + 150) + (150 * 12 + 12) = 146,712
Total: 116,112 + (2 * 146,712) = 409,536",1.82E+16,"120 epochs, dataset 5789 proteins. There are about 300 residues per protein (115,479 residues / 418 proteins) according to https://www.ncbi.nlm.nih.gov/pmc/articles/PMC22960/. 
First network gets 27 features per residue, second and third get 39.
FLOPs from first: 6 * 116112 * (27 * 300 * 5789 * 120) = 3.92e15
FLOPs from 2nd and 3rd: 2 *6 * 146712 * (39 * 300 * 5789 * 120) = 1.43e16
Total: 1.822E16",120,,"The authors had a website where sequences could be submitted for processing through the model: ""Each prediction is usually completed within 10 min, but may take up to a few hours depending on how busy the server is and how long the protein chain is [...] Using an external PSSM file can skip the most time consuming step of generating the evolution profile by PSIBLAST, and the executive time reduce to a few seconds"" 

Rough estimate:
It looks like the PSIBLAST step only needs doing once per input, and this takes the majority of the time. If the inference server uses the same hardware that was used for training, 10 mins * 5789 sequences =  965 hours for PSIBLAST calculation. Then assume training on a sequence takes 3x as long as inference (forward + backward pass uses 6 FLOPs per parameter, vs 2 for forward only), so 120 epochs would take:
3 seconds * 3 * 5789 * 120 = 1,737 hours
Total: around 2,702 hours
(This seems on the long side – probably they had better hardware for training, or else there's an incorrect assumption here)",,,
Differentiable neural computer,10/12/2016,,,,,,,,,,
Xception,10/7/2016,22855952,Table 3,4.36E+20,"60 K80 GPUs * 30 days * 8.5 TFLOPS/GPU * 0.33 utilization  = 4.36e20

Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 450,000 PFLOP = 4.5*10^20 FLOP",,720,"""while the JFT experiments took over one month each.""",NVIDIA Tesla K80,60,12651.54832
Zoneout + Variational LSTM (WT2),9/26/2016,21000000,21M (Table 3),1.61E+16,6 FLOP / parameter / token * 21000000 parameters * 2000000 tokens * 64 epochs = 1.6128e+16 FLOP,64,,,NVIDIA Tesla K80,,
Pointer Sentinel-LSTM (medium),9/26/2016,21000000,,7.49E+15,6 FLOP / parameter / token * 21000000 parameters * 929000 tokens * 64 epochs = 7.491456e+15 FLOP,64,,,,,
GNMT,9/26/2016,278000000,"Table 5 in 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer'

https://arxiv.org/abs/1701.06538",6.62E+21,"From AI and Compute:
""sqrt(10 * 100) factor added because production model used 2-3 orders of magnitude more data, but only 1 epoch rather than 10.
96 K80 GPU’s * 9 days * 8.5 TFLOPS * 0.33 utilization * sqrt(10 * 100)  
= 6.9e6 PF = 79 pfs-days""
source: https://openai.com/blog/ai-and-compute/

https://www.wolframalpha.com/input?i=96+*+9+days+*+8.5+TFLOPS+*+0.33+*+sqrt%281000%29
",1,,"Test model used 96 K80 for 9 days, then this was scaled up by 31x for the production model, but unclear how many GPUs were used or how long it was trained for. The production run used 96 * 9 days * sqrt(1000) ~= 655730 chip-hours.",NVIDIA Tesla K80,,194308.0917
Wide Residual Network,9/19/2016,,,,,,,,,,
TSN,9/17/2016,,,,,,,,,,
Stacked hourglass network,9/17/2016,,,,,,,,,,
ResNet-1001,9/17/2016,10200000,,,"""On CIFAR, ResNet-1001 takes about 27 h to train on 2 GPUs""",,,,,,
ResNet-200,9/17/2016,,,2.97E+19,"""ResNet-200 takes about 3 weeks to train on 8 GPUs"". didn't specify which GPU
upd: 
common GPU performance for 2016 is 6.83E+12 FLOPs/s (https://epoch.ai/blog/estimating-training-compute#forward-pass-compute-and-parameter-counts-of-common-layers) 
then 6.83E+12*3*7*24*3600*8*0.3=2.9741645e+19 (Speculative)",,500,"""about 3 weeks""",,,
MS-CNN,9/17/2016,,,,,,,,,,
Youtube recommendation model,9/15/2016,,,,,,,,,,
WaveNet,9/12/2016,,,,,,,,,,
Multi-task Cascaded CNN,8/26/2016,,,,,,,,,,
DenseNet-264,8/25/2016,34000000,,,,,,,,,
SimpleNet,8/22/2016,5480000,SOTA CIFAR-10 model was 5.48m params,,,,,,NVIDIA GeForce GTX 980,,
Character-enriched word2vec,7/15/2016,,,,,,,,,,
VD-RHN,7/12/2016,32000000,"""To examine the effect of recurrence depth we train RHNs with fixed total parameters (32 M)""",3.57E+15,6 FLOP / parameter / token * 32000000 parameters * 929000 tokens * 20 epochs = 3.56736e+15 FLOP,20,,,,,
fastText,7/6/2016,,,,,,,,,,
Wide & Deep,6/24/2016,,,,,,,,,,
R-FCN,6/21/2016,,,7.19E+17,"1,464  images in 2012 VOC (https://paperswithcode.com/dataset/pascal-voc)/
9,963 images in 2007 VOC (https://www.tensorflow.org/datasets/catalog/voc)
83K training images in MS COCO  (https://paperswithcode.com/dataset/coco)

They used a Nvidia K40 GPU and report training time/image in seconds (table 3)

Assumed a 0.33 util rate

Section 4.2 (MS COCO):
""Next we evaluate on the MS COCO dataset [ 13 ] that has 80 object categories. Our experiments involve the 80k train set, 40k val set, and 20k test-dev set. We set the learning rate as 0.001 for 90k iterations and 0.0001 for next 30k iterations, with an effective mini-batch size of 8.""
0.45s K40 training time per image (table 3)
K40 has 5046000000000 FLOP/s

Total examples: 120000*8=960000 (12 epochs)
5046000000000 FLOP/s * 960000 images * 0.45 s/image * 0.33 utilization = 719357760000000000 FLOP",,,,,,
DMN,6/20/2016,,,,,,,,,,
PixelCNN,6/16/2016,,,,,,60,We were able to achieve similar performance to the PixelRNN (Row LSTM [30]) in less than half the training time (60 hours using 32 GPUs).,,32,
Spatiotemporal fusion ConvNet,6/1/2016,,,,,,,,,,
Part-of-sentence tagging model,5/29/2016,,Architecture described in Table 1,1.45E+17,"12 hours of training for POS tagging
GeForce GTX TITAN X GPU
0.33 utilization rate
",50,12,"""the model training requires about 12 hours for POS tagging and 8
hours for NER""",NVIDIA GeForce GTX TITAN X,1,
Named Entity Recognition model,5/29/2016,,Architecture in Table 1,9.69E+16,"8 hours of training for NER
GeForce GTX TITAN X GPU
0.33 utilization rate
",50,8,"""the model training requires about 12 hours for POS tagging and 8
hours for NER""",NVIDIA GeForce GTX TITAN X,1,
Gated HORNN (3rd order),4/30/2016,8970000,,,,,,,,,
Symmetric Residual Encoder-Decoder Net,3/30/2016,,,,,,,,,,
Binarized Neural Network (MNIST),3/17/2016,37000000,"Parameter count is not explicitly stated, but they give details:

""The MLP we train on MNIST consists of 3 hidden layers of 4096 binary units (see Section 1) and a L2-SVM output layer""

Approximately 37m, based on 784 pixels * 4096 + 2 * 4096^2",,,1000,,,,,
SqueezeNet,2/24/2016,1200000,"The paper says ""SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.""

AlexNet has 60 million parameters.",,,,,,,,
Inceptionv4,2/23/2016,43000000,"""The folks from Google strike again with Inception-v4, 43M parameters.""

https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d",,,,,,,,
Inception-ResNet-V2,2/23/2016,56000000,,,,,,,,,
A3C FF hs,2/4/2016,,,,,,,,,,
Convolutional Pose Machines,1/30/2016,,,,,,,,,,
AlphaGo Lee,1/27/2016,,,1.90E+21,"This number is pretty uncertain. I expect it to be right to around a factor of 3, at least compared to AlphaGo Fan.

The architecture used was pretty much the same as AlphaGo Fan, but it was ""trained for longer"" and had around 5.33x the number of convolutional layers of AlphaGo Fan (256/48 = 5.33). 

The convolutional layers are the major contributor to the training compute, so I somewhat arbitrarily just multiply the compute for AlphaGo Fan by 5. Thus 3.8e20 * 5 = 1.9e21

Otherwise there has been little said about this model specifically - I've mainly relied on the source for AlphaGo Zero and AlphaGo Fan, linked below

AlphaGo Fan: https://www.nature.com/articles/nature16961

AlphaGo Zero: https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ",,696,"Training times are given for several components:
- Policy network classifier: 3 weeks
- Policy network RL: 1 day
- Value network regression: 1 week
- Rollout policy: ""Similar to the policy network, the weights π of the rollout policy are trained from 8 million positions from human games on the Tygem server to maximize log likelihood by stochastic gradient descent. Rollouts execute at approximately 1,000 simulations per second per CPU thread on an empty board."" could suggest (8M sims / 1000 sims/sec) / 3600 sec/hr = 2.2 hours, if each position corresponds to only one simulation (unclear)

Total: 29 days or 696 hours",,,
"Variational (untied weights, MC) LSTM (Large)",12/16/2015,66000000,66M according to https://arxiv.org/pdf/1611.01462,5.88614E+15,6 FLOP / parameter / token * 66000000 parameters * 929000 tokens * 16 epochs = 5.886144e+15 FLOP,16,,,,,
Advantage Learning,12/15/2015,,,,,,,,,,
BPL,12/11/2015,,,,,,,,,,
ResNet-152 (ImageNet),12/10/2015,60200000,Taken from https://arxiv.org/abs/1605.07146,1.04E+19,"11.3 *10^9 mult-adds per forward pass (Table 1)
2 FLOPS/ mult-add
3 for forward & backward pass
1.2 * 10^6 examples in dataset
128 epochs

-> 1.041408 × 10^19 FLOP

Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 11,000 PFLOP = 1.1*10^19 FLOP",120,,,,,
ResNet-110 (CIFAR-10),12/10/2015,1700000,Table 6,,,,,,,,
SSD,12/8/2015,,Can be calculated from the VGG-16 paper and Figure 2,,,,,"Note on training hardware below: unclear if the Titan X was for testing or training. ""We measure the speed with batch size 8 using Titan X and cuDNN v4 with Intel Xeon E5-2667v3@3.20GHz."" I don't know if it's common to use the same hardware for both testing and training.

",NVIDIA GeForce GTX TITAN X,,
DeepSpeech2 (English),12/8/2015,38000000,All networks have 38 million parameters.,2.60E+19,"1 timestep = (1280 hidden units)^2 * (7 RNN layers * 4 matrices for bidirectional + 2 DNN layers) * (2 for doubling parameters from 36M to 72M) = 98 MFLOP
20 epochs * 12,000 hours * 3600 seconds/hour * 50 samples/sec * 98 MFLOP * 3 add-multiply * 2 backprop 
= 26,000 PF = 0.30 pfs-days

See also AI and Compute by Dario Amodei and OpenAI https://openai.com/research/ai-and-compute",,120,"""5 days"" from AI and Compute https://openai.com/index/ai-and-compute/",NVIDIA GeForce GTX TITAN X,16,206.8604146
Inception v3,12/2/2015,23626728,Table 3 from Xception paper,1.00E+20,"Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 100,000 PFLOP = 1*10^20 FLOP",,,,,,
Netflix Recommender System,12/1/2015,,,,,,,,,,
Multi-scale Dilated CNN,11/23/2015,,,,,,,,,,
AlphaGo Fan,10/1/2015,8209984,"The input to the policy network is a 19 × 19 × 48 image stack consisting of 48 feature planes. The first hidden layer zero pads the input into a 23 × 23 image, then convolves k filters of kernel size 5 × 5 with stride 1 with the input image and applies a rectifier nonlinearity. Each of the subsequent hidden layers 2 to 12 zero pads the respective previous hidden layer into a 21 × 21 image, then convolves k filters of kernel size 3 × 3 with stride 1, again followed by a rectifier nonlinearity. The final layer convolves 1 filter of kernel size 1 × 1 with stride 1, with a different bias for each position, and applies a softmax function. The match version of AlphaGo used k = 192 filters; Fig. 2b and Extended Data Table 3 additionally show the results of training with k = 128, 256 and 384 filters.

The input to the value network is also a 19 × 19 × 48 image stack, with an additional binary feature plane describing the current colour to play. Hidden layers 2 to 11 are identical to the policy network, hidden layer 12 is an additional convolution layer, hidden layer 13 convolves 1 filter of kernel size 1 × 1 with stride 1, and hidden layer 14 is a fully connected linear layer with 256 rectifier units. The output layer is a fully connected linear layer with a single tanh unit.",3.80E+20,"Assume 0.3 utilisation rate, 1e13 GPU FLOP/s [single precision]. Trained in three stages using 50 GPUs over 3 weeks + 1 day + 1 week

Training compute = (50 GPUs)(29 days)(86400s/day)(0.3 utilisation rate)(1e13 FLOP/s) = 3.8e20 FLOPs
",,,,,,
Deep Deterministic Policy Gradients,9/9/2015,,,,,,,,,,
BPE,8/31/2015,,,,,,,,,,
LSTM-Char-Large,8/26/2015,19000000,19M (Table 3),2.65E+15,6 FLOP / parameter / token * 19000000 parameters * 929000 tokens * 25 epochs = 2.64765e+15 FLOP,25,,,,,
"Listen, Attend and Spell",8/20/2015,,,,,,,,,,
Search-Proven Best LSTM,7/6/2015,20000000,20M (Table 3),3.34E+15,6 FLOP / parameter / token * 20000000 parameters * 929000 tokens * 30 epochs = 3.3444e+15 FLOP,30,,,,,
BatchNorm,6/15/2015,13600000,"""The network contains 13.6 · 106 parameters""",,,72,,,,,
YOLO,6/8/2015,271684800,"Calculation based on figure 3 of the paper:
7 * 7 * 3 * 64 + 3 * 3 * 64 * 192 + 1 * 1 * 192 * 128 + 3 * 3 * 128 * 256 + 1 * 1 * 256 * 256 + 3 * 3 * 256 * 512 + 4 * (1 * 1 * 512 * 256 + 3 * 3 * 256 * 512) + 1 * 1 * 512 * 512 + 3 * 3 * 512 * 1024 + 2 * (1 * 1 * 1024 * 512 + 3 * 3 * 512 * 1024) + 4 * (3 * 3 * 1024 * 1024) + 7 * 7 * 1024 * 4096 + 4096 * 7 * 7 * 30",,,,,,,,
Faster R-CNN,6/4/2015,,,,,,,,,,
Trajectory-pooled conv nets,5/19/2015,9106245,"The input layer takes either a single RGB frame (224x224x3) for the spatial stream or a stack of 10 optical flow frames (224x224x20) for the temporal stream.
The first convolutional layer has 96 filters of size 7x7 with stride 2.
This is followed by max pooling with size 3x3 and stride 2.
The second convolutional layer has 256 filters of size 5x5 with stride 2.
After that is another max pooling layer (3x3, stride 2).
The third convolutional layer has 512 filters of size 3x3 with stride 1.
The fourth convolutional layer has 512 filters of size 3x3 with stride 1.
The fifth convolutional layer has 512 filters of size 3x3 with stride 1.
Next is a max pooling layer (3x3, stride 2).
The fully-connected layers have 4096, 2048, and 101 neurons respectively.

(7*7*20+1)*96 + (5*5*20+1)*256 + (3*3*20+1)*512 + (3*3*20+1)*512 + (3*3*20+1)*512 + 2*4096 + (4096+1)*2048 + (2048+1)*101 = 9106245",,,,,,,,
Deep LSTM video classifier,5/1/2015,,,,,,,,,,
Fast R-CNN,4/30/2015,,,,,,,,,,
genCNN + dyn eval,3/17/2015,8000000,8M according to https://arxiv.org/pdf/1508.06615,3.42E+16,"5046000000000 FLOP / sec/ GPU * 1 GPU * 48 hours [""Likely"" confidence since 2 days of training refer to another dataset] * 3600 sec / hour * 0.3 [assumed utilization] = 2.6158464e+17 FLOP

Assuming (!) 100 epochs (-> ""Speculative"" confidence""):

6 FLOP / parameter / token * 8000000 parameters * 929000 tokens * 100 epochs = 4.4592e+15 FLOP

sqrt(2.6158464e+17*4.4592e+15 ) = 3.4153451e+16 FLOP

________
in the algorithmic progress report paper the estimation was 7.3 × 10^16 FLOP (hardware-based estimation assuming another Tesla K40 chip)
",,48,""" The optimization is done mainly on a Tesla K40 GPU, which takes about 2 days for the training on a dataset containing 1M sentences.""
this training doesn't refer to the PTB dataset but to wiki dataset so it is likely an upper bound",NVIDIA Tesla K40s,,
Constituency-Tree LSTM,2/28/2015,205190,"Table 1

https://arxiv.org/abs/1503.00075",,,,,,,,
DQN-2015,2/25/2015,1693362,"""The input to the neural network consists of an 84x84x4 image produced by the preprocess-ing mapw. The first hidden layer convolves 32 filters of 8x8 with stride 4 with theinput image and applies a rectifier nonlinearity. The second hidden layer con-volves 64 filters of 4x4 with stride 2, again followed by a rectifier nonlinearity.This is followedby a thirdconvolutional layer thatconvolves 64 filtersof 3x3 withstride 1 followed by a rectifier. The final hidden layer is fully-connected and con-sists of 512 rectifier units. The output layer is a fully-connected linear layer with asingle output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered.""

Example num params here: https://colab.research.google.com/drive/1Ty6SFYWd7EcKoxJohucL2OdiLR_3oXnI?usp=sharing",,"This should be calculatable, just needs careful reasoning about compute per frame.",,,,,,
TRPO,2/19/2015,33500,,,,,30,"""The 500 iterations of our algorithm took about 30 hours (with slight variation between games) on a 16-core computer.""",,,
CRF-RNN,2/11/2015,,,,,,,,,,
"MSRA (C, PReLU)",2/6/2015,87048800,"I used the architecture in table 3
I ignored biases, and assumed a SPP bin size of 256

3*7*7*96+96*3*3*384+384*3*3*384*5+384*3*3*768+768*3*3*768*5+768*3*3*896+896*3*3*896*5+896*(7*7+3*3+2*2+1)*4096+4096*4096+4096*1000=330581792



",2.40E+19,"""training C on eight K40 GPUs, takes about 3-4 weeks""
0.33 util rate
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",,588,,NVIDIA Tesla K40t,,
DeepLab,12/22/2014,,,,,,,,,,
Fractional Max-Pooling,12/18/2014,27000000,27M weights in largest CIFAR-100 model,1.00E+17,"For the 12M param model, training required ""18 hours on a GeForce GTX 780"". So would be somewhat larger for 27M.

4 TFLOPS * 18 * 3600 * 0.4 = 1e17",250,18,,NVIDIA GeForce GTX 780,,
NTM,12/10/2014,,,,,,,,,,
SNM-skip,12/3/2014,62000000000,62B from Table 2,2.98E+20,https://www.wolframalpha.com/input?i=0.8+billion+*+62+billion+*+6+FLOP,,,,,,
Cascaded LNet-ANet,11/28/2014,,,,,,,,,,
Fully Convolutional Networks,11/14/2014,,,,,,,,,,
SC-NLM,11/10/2014,,,,,,,,,,
LRCN,11/7/2014,142552000,"1st model: CaffeNet fc6 feature extractor (4096-length vectors) -> LSTM with 1024 hidden units

2nd model: CaffeNet fc6 feature extractor (4096-length vectors) -> 2 layer LSTM with 1000 hidden units

3rd mode: Like the second, but has encoder and decoder LSTMs (both with 2 layers)

AlexNet (close relative to CaffeNet) has 61M params.

LSTM RNN number of parameters is given by L*(n*m + n^2 + n) where L:= Number of layers, n:= hidden units, m:= input vector length
",,,,,,,,
Spatially-Sparse CNN,9/23/2014,,Parameter count not stated but is probably derivable from the paper.,,,,,,,,
Deeply-supervised nets,9/18/2014,,,,,,,,,,
GoogLeNet / InceptionV1,9/17/2014,6797700,Computed summing the parameters on table 1 of section 5,1.51E+18,"AI and Compute  (https://openai.com/blog/ai-and-compute/) charts imply a value of 1.51e18 (value extracted using WebPlotDigitizer  https://automeris.io/WebPlotDigitizer/ ).

Based on the paper, there are 1.5B multiply-adds per inference, and 1.2M images in the training set, but an unknown number of epochs. They decrease the learning rate by 4% every 8 epochs, so there are likely many. If the figure from AI and Compute is taken as true, there were likely 140 epochs",827,,"""Although we used CPU based implementation only, a rough estimate suggests that the GoogLeNet network could be trained to convergence using few high-end GPUs within a week""",,,
SPN-4+KN5,9/14/2014,5000000,"Estimate from table 2 of https://arxiv.org/abs/1609.07843

The authors of the linked paper draw on estimates from table 3 of https://arxiv.org/pdf/1508.06615.pdf",4.40E+16,"40h, 1 GPU, 1028e9 Peak FLOP/s, 30%

1028000000000 FLOP/s/GPU * 1GPU * 40 hours * 3600 s/hour * 0.3 [assumed utilization] = 4.44096e+16 FLOP",,40,"""""e stopped training our SPN when its performance on the validation set
stops improving at two consecutive evaluation points, or when it has run for 40 hours, whichever occurred first. (It turned out that both SPN-3 and SPN-4 ran for the maximum of 40 hours.)
We parallelized our SPN code2 to run on a GPU, and ran our experiments on a machine with a 2.4 GHz CPU and an NVIDIA Tesla C2075 GPU (448 CUDA cores, 5GB of device memory).""",NVIDIA Tesla C2075,1,
Seq2Seq LSTM,9/10/2014,1920000000,"The resulting LSTM has 384M parameters of which 64M are pure recurrent connections (32M for the “encoder” LSTM and 32M
for the “decoder” LSTM).
The paper uses an ensemble of 5 LSTMs.",5.60E+19,"384E+6 parameters * 2 FLOP/parameter * (348E+6 + 304E+6 points per epoch) * 7.5 epochs * 3 FLOP/point ~= 1.126656e+19 FLOP
Times 5 independent models in ensemble => 5.6E+19 FLOP

If we assume NVIDIA K40 (in use at the time): 10 days * 24 * 60 * 60 seconds/day * 8 GPUs * 33% * 5e12 FLOP/s * 5 models in ensemble ~= 5.7E+19 FLOP

Authors of ""AI and Memory Wall"" estimated model's training compute as 11,000 PFLOPS = 1.1*10^19 FLOPS
(https://github.com/amirgholami/ai_and_memory_wall)",7.5,240,Training took about 10 days,,,
Large regularized LSTM,9/8/2014,66000000,"""The large LSTM has 1500 units per layer and its parameters are initialized uniformly in [−0.04, 0.04]""

66M according to Table 3 of https://arxiv.org/pdf/1611.01462",4.30E+16,"3520000000000 FLOP / s/ GPU [tesla k20c assumed, the paper says just ""Nvidia k20""] * 1 GPU * 24 hours [could be less, they report ""the entire day""] * 3600 s / hour * 0.3 [assumed utilization] = 9.12384e+16 FLOP 

6 FLOP / parameter / token * 66000000 parameters * 929000 tokens * 55 epochs = 2.023362e+16 FLOP

sqrt(2.023362e+16*9.12384e+16) = 4.2966069e+16 ",55,24,"""Training this network takes an entire day on an NVIDIA K20 GPU.""",NVIDIA Tesla K20c,1,
VGG19,9/4/2014,144000000,"Source: Table 2
https://arxiv.org/abs/1409.1556",1.10E+19,"Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 11,000 PFLOP = 1.1*10^19 FLOP",,,,,,
VGG16,9/4/2014,138000000,"Source: Table 2
https://arxiv.org/abs/1409.1556",1.23E+19,"3 weeks * 4 Titan Black GPUs * 0.30 utilization

Section 3.3: ""On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture.""

Titan Black performance: 5.645 TFLOPS (assuming FP32)

https://www.wolframalpha.com/input?i=5.645+TFLOPS+*+3+weeks+*+4+*+0.3


",74,504,,NVIDIA GTX Titan Black,4,
RNNsearch-50*,9/1/2014,,,1.56E+18,"From https://openai.com/blog/ai-and-compute/ Appendix.

0.018 pfs-days
(86400*10^15*0.018)

252 hours in a Quadro K-6000 GPU (assumed utilization: 0.33)

5196000000000 FLOP/s *252 hours * 3600 second/hour * 0.33 utilization = 1555200000000000000 FLOP",,,,NVIDIA Quadro K6000,,
SmooCT,7/1/2014,,,6.90E+16,"""Each three-player agent was trained for about 12 billion episodes, requiring about 48 hours of training time [...] on a modern computer without using parallelization""

Assume an Intel i7 so 400e9 FLOP/s.
6.9e16 = 400e9*60*60*48",,48,,,,
Multiresolution CNN,6/23/2014,126125568,"""Using shorthand notation, the full [single frame] architecture is C(96, 11, 3)-N-P-C(256, 5, 1)-N-P-C(384, 3, 1)-C(384, 3, 1)-C(256, 3, 1)-P-FC(4096)-FC(4096), where C(d, f, s) indicates a convolutional layer with d filters of spatial size f ×f, applied to the input with stride s""

Two such single-frame architectures are concatenated as shown in figure 2

""Since the input is only of half the
spatial size as the full-frame models, we take out the last
pooling layer to ensure that both streams still terminate in a
layer of size 7×7×256. ""

We assume the input are T=10 frames with C=3 color channels each

2*(256*(10*3*5*5+1) + 384*(256*3*3+1) + 384*(384*3*3+1) + 256*(384*3*3+1)) + (2*7*7*256 + 1)*4096 + (4096+1)*4096



",,,,,,,,
DeepFace,6/23/2014,,,,,,,,,,
RNN-WER,6/22/2014,26500000,"""The network had five levels of bidirectional LSTM hidden layers, with 500 cells in each layer, giving a total of ∼ 26.5M weights.""",,,,,,,,
Fragment embedding,6/21/2014,144496000,"Model contains a word embedding. a matrix combining two word embeddings, and image embedding (built upon a pretrained RCNN image model.
Word embedding: 400000 * 200 =80000000 (""Here, We is a d × 400, 000 matrix that encodes a 1-of-k vector into a d-dimensional word vector representation (we use d = 200).""
Embedding dimension: 1000 (""The size of the embedded space is cross-validated, and we found that values of approximately 1000 generally work well.""
Word combination matrix: 400* 1000=400000
Image embedding: 4096*1000=4096000 (""We use the Caffe [41] implementation of the ImageNet Detection RCNN model [27] to detect objects in all images. On our machine with a Tesla K40 GPU, the RCNN processes one image in approximately 25 seconds. We discard the predictions for 200 ImageNet detection classes and only keep the 4096-D activations"")
CNN: 60,000,000 ""The CNN architecture is identical to the one described in Girhsick et al. [26]. It contains approximately 60 million parameters""
Total parameters: 4096000+80000000+400000+60000000=144,496,000",,,20,,,,,
SPPNet,6/18/2014,,,3.41E+18,"""All networks in this paper can be
trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.""
4.7e12 FLOP/s * 4* 7*24*60*60 seconds * 0.3 utilisation",,672,"""All networks in this paper can be trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.""",NVIDIA GeForce GTX TITAN,,
GANs,6/10/2014,,The paper outlines the G-D framework but doesn't provide information about the structures of their generator and discriminator.,5.18E+17,"From https://openai.com/blog/ai-and-compute/ Appendix

""Less than 0.006 pfs-days""
(86400*10^15*0.006)

Seems extremely speculative, unless someone at OpenAI privately corresponded with the authors. There is no information about compute or training in the GANs paper.",,,,,,
Two-stream ConvNets for action recognition,6/9/2014,,,,,,,,,,
GRUs,6/3/2014,,,,,,,,,,
AdaRNN,6/1/2014,13040,"D=25 ""For recursive neural models, the dimension of word vector is set to 25, and f = tanh is used as the nonlinearity function. We employ 10 composition matrices in AdaRNN.""
Composition matrices: ""W ∈ R D×2D is the composition matrix, and b is the bias vector.""
C=10 ""We employ 10 composition matrices in AdaRNN.""
Combination matrix: ""S ∈ R C×(2D+|e|) is the matrix used to determine which composition function we use, vl , vr are the left and right child vectors, and e are external feature vector. In this work, e is a one-hot binary feature vector which indicates what the dependency type is.""
|e| > 4: (see Figure 2)
Weights: 10 * 25 * 50 + 10 * (50+4) =13040 (ignoring embedding to the 25 dimension embedding space)",,,,,,,,
Paragraph Vector,5/14/2014,32000000," 75000*400+5000*400=32000000
""We learn the word vectors and paragraph vectors using 75,000 training documents""
""In PV-DM, the learned vector representations have 400 dimensions for both words and documents""
Paragraph embedding of dimension number of paragraphs * embedding size
Word embedding of dimension |V|*embedding size
Assuming vocabulary of 5000 since results are compared directly to Maas et. al., 2011",,,,,,,,
HyperNEAT,3/5/2014,239712,"""The ANN consists of three layers (Fig. 3): a substrate layer inwhich information from the game screen (raw pixels, objects, ornoise) is given as input to the network; a processing layer whichadds a nonlinear internal representation; and a nonlinear outputlayer from which actions are read and conveyed to the Atari em-ulator. Both the input and output layers are fully connected tothe processing layer. The substrate dimensionality of the inputand processinglayers is 810 in the case of the object repre-sentation and 1621 for the pixel and noise representations.3The output layer consists of a 33 substrate mirroring the ninepossible directions of the Atari 2600 joystick and a single noderepresenting thefire button""",,,,,,,,
GloVe (32B),1/1/2014,120000000,400k vocab * 300 vector dimensions,,"""The total run-time is split between populating X
and training the model. The former depends on
many factors, including window size, vocabulary
size, and corpus size. Though we did not do so,
this step could easily be parallelized across multiple machines (see, e.g., Lebret and Collobert
(2014) for some benchmarks). Using a single
thread of a dual 2.1GHz Intel Xeon E5-2658 machine, populating X with a 10 word symmetric
context window, a 400,000 word vocabulary, and
a 6 billion token corpus takes about 85 minutes.
Given X, the time it takes to train the model depends on the vector size and the number of iterations. For 300-dimensional vectors with the above settings (and using all 32 cores of the above machine), a single iteration takes 14 minutes. See Fig. 4 for a plot of the learning curve""

""We run 50 iterations for vectors smaller than
300 dimensions, and 100 iterations otherwise (see
Section 4.6 for more details about the convergence
rate).""

But we are interested in the 42B token model",,,"Section 4.6 in original paper (https://nlp.stanford.edu/pubs/glove.pdf)

85 min to populate coocurrence matrix
+ 25 training iterations

Each iteration takes 14 minutes on 32 cores ",,,
GloVe (6B),1/1/2014,120000000,400k vocab * 300 vector dimensions,,"""The total run-time is split between populating X
and training the model. The former depends on
many factors, including window size, vocabulary
size, and corpus size. Though we did not do so,
this step could easily be parallelized across multiple machines (see, e.g., Lebret and Collobert
(2014) for some benchmarks). Using a single
thread of a dual 2.1GHz Intel Xeon E5-2658 machine, populating X with a 10 word symmetric
context window, a 400,000 word vocabulary, and
a 6 billion token corpus takes about 85 minutes.
Given X, the time it takes to train the model depends on the vector size and the number of iterations. For 300-dimensional vectors with the above settings (and using all 32 cores of the above machine), a single iteration takes 14 minutes. See Fig. 4 for a plot of the learning curve""

""We run 50 iterations for vectors smaller than
300 dimensions, and 100 iterations otherwise (see
Section 4.6 for more details about the convergence
rate).""

Details of dual 2.1GHz Intel Xeon E5-2658 machine:
https://www.intel.com/content/www/us/en/products/sku/61428/intel-xeon-processor-e52658-20m-2-10-ghz-8-0-gts-intel-qpi/specifications.html",,,"Section 4.6 in original paper (https://nlp.stanford.edu/pubs/glove.pdf)

85 min to populate coocurrence matrix
+ 25 training iterations

Each iteration takes 14 minutes on 32 cores ",,,
OverFeat,12/21/2013,144000000,144M (Table 4),,,80,,,NVIDIA Tesla K20X,,
Image generation,12/20/2013,784000,"""We trained generative models (decoders) and corresponding encoders
(a.k.a. recognition models) having 500 hidden units in case of MNIST""

784*500*2=784000 (Ignoring latent dimension)",4.752E+14,"From https://openai.com/blog/ai-and-compute/ Appendix

""less than 0.0000055 pfs-days""
(86400*10^15*0.0000055)

Figure 2 shows evaluations with 10^8 training samples

6*784000*100000000=470400000000000",,,,,,
DOT(S)-RNN,12/20/2013,6160000,,,,,,,,,
DQN,12/19/2013,836096,"""The input to the neural network consists is an 84 × 84 × 4 image produced by φ. The first hidden layer convolves 16 8 × 8 filters with stride 4 with the input image and applies a rectifier nonlinearity [10, 18]. The second hidden layer convolves 32 4 × 4 filters with stride 2, again followed by a rectifier nonlinearity. The final hidden layer is fully-connected and consists of 256 rectifier units. The output layer is a fully connected linear layer with a single output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered.""

Parameter: 4*16*8*8+16*32*4*4+10*10*32*256+18*256=836096",2.84688E+15,"Network is 84x84x4 input, 16, 8x8, stride 4, 32 4x4 stride 2, 256 fully connected
First layer: 20*20*4*16*8*8 = 1638400
Second layer: 9*9*16*32*4*4 = 663552
Third layer: 9*9*32*256 = 663552
Total ~ 2965504
2965504 * 5M updates * 32 batch size * 2 multiply-add * 3 backward pass
= 2965504*50000*100*32*6 = 2846883840000000

",,,,,,
Network in Network,12/16/2013,,,,,,,,,,
RNN for 1B words,12/11/2013,20000000000,20B from Table 1,,"240 hours on 24 CPUs from Table 1. CPU model is not given, but there is mention of using SIMD instructions. 1 SIMD operation is around 4 FLOP. CPU can have around 3e9 operations per second. so around 12e9*24 * 240*3600 = 2.4e17 operations. This estimation doesn't include use of multiple threads. Including use of threads we would probably have around 10 times more operations  so around 2.4e18 FLOPs. This estimation is speculative.",,240,"from Table 1,240 hours on 24 CPUs",,24,
DBLSTM,12/8/2013,29900000,"""The DBLSTM network had five bidirectional hidden levels, with 500 LSTM cells in each of the forward and backward
layers, and a size 3385 softmax output layer, giving a total of
29.9M weights.""",,,,,,,,
TransE,12/5/2013,942000000,"Based on the TransE architecture, the authors give a formula for how the model size scales with the dimensionality of the dataset. The model scale is proportional to: k*(n_e+n_r) where k is the embeddings dimension, n_e is the number of entities, and n_r is the number of relationships.

They studied using the TransE model for two datasets: FB15k and FB1M. The FB15k model has 810000 parameters.

FB15k has 14951 entities and 1345 relationships. FB1M has 1000000 entities and 23382 relationships. Therefore, the FB1M model will be bigger than the FB15k model by a factor of (23382e6)/(14951*1345) => N = 8.1e5 * (23382e6)/(14951*1345) = 942e6.",1.34E+18,"8 GPUs (they don't specify which, so I used the average for FP32 for 2017 from the write-up table)
8 hours 
0.33 util rate",,,,,,
DeViSE,12/5/2013,,,,,,,,,,
TensorReasoner,12/1/2013,,,,,,,,,,
Visualizing CNNs,11/12/2013,,,5.32E+17,"1 GPU * 12 days * 1.54 TFLOPS/GTX 580 * 0.33 utilization 
= 532 PF = 0.0062 pfs-days

Source: https://openai.com/blog/ai-and-compute

""We stopped training after 70 epochs,
which took around 12 days on a single GTX580 GPU""",,,,NVIDIA GeForce GTX 580,,
R-CNN (T-net),11/11/2013,69003872,"Computed from architecture description in Caffee

https://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/detection.ipynb",,,,,,,,
Word2Vec (small),10/16/2013,207600000,"""We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K [...] Starting with the same news data as in the previous experiments, we first constructed the phrase based training corpus and then we trained several Skip-gram models using different hyperparameters. As before, we used vector dimensionality 300 and context size 5.""",,,,,,,,
RNTN,10/1/2013,,,1.42E+16,"""The RNTN would usually achieve its best performance on the dev set after training for 3 - 5 hours.""
",,5,The RNTN would usually achieve its best performance on the dev set after training for 3 - 5 hours.,,,
RCTM,10/1/2013,,,9.3312E+15,"""The training of an RCTM takes about 15 hours on 3 multicore CPUs""
Given the publication year, a rough estimate for the CPU performance is 16 FP32 per cycle, 4 cores, clock speed 4GHz, utilization of 0.3.
15*60*60*3*4*12*4000000000*0.3=9331200000000000=9.33e15",,15,The training of an RCTM takes about 15 hours on 3 multicore CPUs.,,3,
Mitosis,9/22/2013,37230,Sum numbers of weights in Table 1.b,1.37E+17,"""Training each network requires one day of computation with an optimized GPU implementation""

Assuming 1.58E+12 FLOP/second on FP32 (from the table in the Estimating compute post), we get

3600*24*1.58E+12 = 1.37E+17 FLOP",,24,"""Training each network requires one day of computation with an optimized GPU implementation""",,,
RNN+weight noise+dynamic eval,8/4/2013,54000000,"""the word-level network had 10,000 inputs and outputs and around
54M weights""",4.21E+15,6 FLOP / parameter / token * 54000000 parameters * 929000 tokens * 14 epochs = 4.213944e+15 FLOP,14,,,,,
Fisher Vector image classifier,6/12/2013,,,9.08424E+13,"They use a Intel Xeon E5-2470 Processor for 2 hours. This can do 12,617 MOps/Sec (average test results, assuming they achieved a similar utilization)
https://www.cpubenchmark.net/cpu.php?cpu=Intel+Xeon+E5-2470+%40+2.30GHz&id=2003

12617000000*2*60*60=90842400000000",,2,,,,
SemVec,6/9/2013,,,,,,,,,,
ReLU-Speech,5/26/2013,101706240,"""The overall input dimensionality is 1040,""
""All layers of our networks have 2560 hidden units ""
""used to generate 7969 context-dependent tied acoustic states""
Largest model: 12 hidden layers (Fig 4)
Parameters: 1040*2560+12*2560*2560+2560*7969=101706240
",1.28E+17,"""across 4 machines using up to 4 CPUs each""
CPU model not specified, I assumed a Sandy Bridge with 16 FLOP/cycle and 3.3GhZ based on the publication year (4*16*3300000000=211200000000 FLOP/s per machine)
Compute: 4*211200000000*168*60*60*0.3 = 1.53e17

Alternatively, the training set is ""several hundred hours of speech"", with inputs consisting of 26 frames, each frame is 10ms apart.
If we assume 400h of training data, 400h/10ms/26= 5,538,461 inputs
6 * 101706240 * 5,538,461 = 3.38e15 FLOPs per epoch. Number of epochs unstated.",,168,"""The results we report are obtained by training for one week.""",,,
Multilingual DNN,5/26/2013,206899200,"""The input for the DNN is eleven contiguous frames of 40-dimensional log-filterbank features. The DNN consists of four hidden layers each with 2560 nodes""
Network structure: 3 multilingual shared layers, 1 language specific hidden layer + output layer (Figure 2)
Language specific layer output sizes: 1600, 3300, 2900, 5700, 3500, 5500, 6200, 4700, 5100, 4900, 3700 (Table 1)
Shared: 11*40*2560+2560*2560+2560*2560=14233600
Language heads: 11*2560*2560+2560*1600+2560*3300+2560*2900+2560*5700+2560*3500+2560*5500+2560*6200+2560*4700+2560*5100+2560*4900+2560*3700=192665600
Total: 14233600+192665600=206899200=2e8",,Could be estimated if we knew framerate of input filterbanks.,,672,"""increased training time of roughly four weeks""
4*7*24=672 hours of training",,,
Selective Search,4/2/2013,,,,,,,,,,
PreTrans-3L-250H,3/22/2013,43000000,Table 1,,,,,,,,
Maxout Networks,2/18/2013,,,,,,,,,,
Textual Imager,1/16/2013,,,,,,,,,,
DistBelief NNLM,1/16/2013,,,2.61E+18,"Trained for 14 days on 180 CPU cores (Table 6)
Roughly estimating the performance of CPUs in a HPC around 2013: 16 FP32 operations per cycle, 2.5GHz, 0.3 utilization
Time: 14*24*60*60=1209600s
FLOPs: 0.3*180*16*2500000000=2160000000000
Training compute: 1209600s * 2160000000000 = 2612736000000000000 = 2.61e18
https://www.wolframalpha.com/input?i=16+FLOP+*+2.5+GHz+*+180+*+14+days+*+0.3",,336,Trained for 14 days (Table 6),,180,
DistBelief Vision,12/3/2012,1700000000,"""we used Downpour SGD to train the 1.7 billion parameter image model""",,,,,,,,
DistBelief Speech,12/3/2012,47185920,"""We used a deep network with five layers: four hidden layer with sigmoidal activations and 2560 nodes each, and a softmax output layer with 8192 nodes.""
""The network was fully-connected layer-to-layer, for a total of approximately 42 million model parameters.""
2560*2560*4+2560*8192=47185920",3.11E+17,"https://www.wolframalpha.com/input?i=6+FLOP+*+47185920+*+1.1+billion
Number of epochs unknown but most likely 1 and probably under 30.
We could narrow down the uncertainty further if we knew something about the hardware.",,120,Figure 4,,,
Bayesian automated hyperparameter tuning,12/2/2012,,,,,,,,,,
RNN+LDA+KN5+cache,12/1/2012,9000000,,,,,,,,,
AlexNet,9/30/2012,60000000,"""Our neural network architecture has 60 million parameters.""",4.70E+17,"1.2M images * 90 epochs * 0.75 GFLOP * (2 add-multiply) * (3 backward pass) 
= 470 PF = 0.0054 pfs-days

Source: https://openai.com/blog/ai-and-compute/

Hardware method:
2 GTX 580 3GB GPUs for ""between five and six days"". Assuming 5.5 days and 32-bit training:
1.581 TFLOPS * 5.5 days * 2 = 1.5e18 FLOP
Comparing to the operation counting method, this implies around 31% MFU.

Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 460 PFLOP = 4.6*10^17 FLOP",,132,"""Our network takes between five and six days to train on two GTX 580 3GB GPUs.""",NVIDIA GeForce GTX 580,,
LSTM LM,9/9/2012,102720000,"Multiple models were trained, the largest on transcribed French podcast data.
""We trained an LSTM LM using 300 hidden nodes and 27 M running words of indomain training data""
""Corpus sizes in number of running words; the vocabulary size of the Treebank corpus is 10 K, for Quaero French it is 170 K""
Embedding and unembedding: 2*170000*300=102000000
LSTM: 4*600*300=720000
Total: 102000000+720000=102720000=1.03e8
(Assuming the embedding dimension is the same as the LSTM layer)",1.66E+16,"FLOP per input for LSTM layer is 4*2*(M+N)*M, for N inputs and M outputs.

Embedding FLOPs: 2 * 170000 * 300 = 102,000,000
LSTM FLOPs: 4 * 2 * (300 + 300) * 300 = 1,440,000
Unembedding FLOPs: 2 * 170000 * 300 = 102,000,000
Total: 205,440,000 FLOPs per word per forward pass
For 27M training input words and including backward passes: 27M * 3 * 205,440,000 = 1.66e16

However, it sounds like they're doing something with a secondary acoustic model, so this may be an underestimate.",,,,,,
LSTM-300units,9/1/2012,12000000,,,,,,,,,
Context-dependent RNN,7/27/2012,,,,,,,,,,
Unsupervised High-level Feature Learner,7/12/2012,1000000000,"""To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet)""",6.00E+17,"Assuming 1 epoch, 10 million images and 1 billion parameters, 6*N*D = 6*10^17 FLOP",,72,"""We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. """,,,
MV-RNN,7/12/2012,3510255,"""We represent a word as both a continuous vector and a matrix of parameters. We initialize all word vectors x ∈ Rn with pre-trained 50-dimensional word vectors from the unsupervised model of Collobert and Weston (2008). [...] Every word is also associated with a matrix X.  [...] If the vectors have dimensionality n, then each word’s matrix has dimensionality X ∈ Rn×n.""

""We propose the following combination function which is input dependent:
p = fA,B(a, b) = f(Ba, Ab) = g(W x (Ba Ab)) ,(2)
where A, B are matrices for single words, the global W ∈ Rn×2n is a matrix that maps both transformed words back into the same n-dimensional space.""

""For computing nonterminal phrase matrices, we define the function
P = fM(A, B) = WMA, B, (3)
where WM ∈ Rn×2n, so P ∈ Rn×n just like each input matrix.""

""If every word is represented by an n-dimensional vector and additionally by an n × n matrix, the dimensionality of the whole model may become too large with commonly used vector sizes of n = 100. In order to reduce the number of parameters, we represent word matrices by the following low-rank plus diagonal approximation: A = UV + diag(a), (5)
where U ∈ Rn×r, V ∈ Rr×n, a ∈ Rnand we set the rank for all experiments to r = 3.""

""We train these representations by adding on top of each parent node a simple softmax classifier
to predict a class distribution over, e.g., sentiment or relationship classes: d(p) = softmax(Wlabelp). If there are K labels, then d ∈ RK is a K-dimensional multinomial distribution""

In total there are V*(n+n*r + r*n) + n*2n + n*2n + (n+1)*k parameters, where n is the vector dimension, r is the low-rank decomposition dimension, V is the vocabulary size and k is the number of classes.

In the experiments we have that n=50, r=3, k=? and V=?. I'm guesstimating k=5 and V=10k.",,,,,,,,
Dropout (TIMIT),6/3/2012,48840185,The input to the net is 21 adjacent frames with an advance of 10ms per frame. The neural net has 4 fully-connected hidden layers of 4000 units per layer and 185 “softmax” output units that are subsequently merged into the 39 distinct classes used for the benchmark.,,,,,,NVIDIA GeForce GTX 580,,
Dropout (MNIST),6/3/2012,5594010,"We show results for 4 nets (784-800-800-10, 784-1200-1200-10, 784-2000-2000-10, 784-1200-1200-1200-10)

784*2000+2000*2000+10*2000+6010=5594010",6.03937E+15,"Num mul-add / forward pass
2 FLOPs / mult-add
3 total mult-add / fp mult-add
3000 epochs
60000 training samples",,,,NVIDIA GeForce GTX 580,,
Dropout (ImageNet),6/3/2012,,"""We achieved comparable performance of 48.6% error using a single neural network with five convolutional hidden layers interleaved with “max-pooling” layer followed by two globally
connected layers and a final 1000-way softmax layer""",2.73E+17,"""a single NVIDIA GTX 580 GPU... Training on ImageNet takes
roughly four days with dropout and two days without.""
1.581 TFLOP/s * 4 day * 86400 s/day * 0.5 utilization",,96,4 days with dropout; 2 days without dropout,NVIDIA GeForce GTX 580,,
Dropout (CIFAR),6/3/2012,,,4.2687E+15,"""a single NVIDIA GTX 580 GPU. Training on CIFAR-10 takes roughly 90 minutes"" p17
1.581 TFLOP/s * 90 min * 60 s/min * 0.5 utilization",,1.5,90 minutes,NVIDIA GeForce GTX 580,,
HOGWILD!,11/11/2011,,,,,,,,,,
NLP from scratch,11/8/2011,5000000,"""The capacity of our network architectures lies mainly in the word lookup table, which contains 50 × 100,000 parameters to train. [...] most of the trainable parameters are located in the lookup tables.""",,,,72,"""Chunking and NER take about one hour to train, POS takes few hours, and SRL takes about three days.""
SRL is the longest task.",,,
Domain Adaptation,11/6/2011,15260,"Did not take into account initial image feature extraction, only novel stuff.

1. Perform PCA on the feature matrices from both domains. Learnable parameters are projection matrices.
= 800 (# features) x 200 (reduced dimension) x 2 (once per subdomain)

2. Perform partial least squares regression. Learnable parameters are

Matrix P with dimensions 200 (# features) x 30 (dimension of latent space)
Matrix Q with dimensions 1 (# responses) x 30 (dimension of latent space)
Projection matrix of X onto latent space:  200 (# features) x 30 (dimension of latent space)
Projection matrix of Y onto latent space:  1 (# responses) x 30 (dimension of latent space)
",,,,,,,,
Adaptive Subgrad,10/3/2011,,,,,,,,,,
Recursive sentiment autoencoder,7/1/2011,,,,,,,,,,
Recursive Neural Network,6/28/2011,,,,,,,,,,
Vector Space Model,6/19/2011,255000,"""We build a fixed dictionary of the 5,000 most frequent tokens""
""For all word vector models, we use 50-dimensional vectors""
Parameters: 5000*50 + 5000=255000
",,,,,,,,
Cross-Lingual POS Tagger,6/19/2011,,,,,,,,,,
RNN-SpeedUp,5/22/2011,,,,,,,,,,
Deep Autoencoders,4/29/2011,139808256,"2*(3072*8192+8192*4096+4096*2048+2048*1024+1024*512+512*256+256*128+128*64+64*28)=139808256
""n each autoencoder, the hidden layers halve in size until they reach the desired size, except that we use 28 instead of 32""",3.67E+16,"48*60*60*708500000000*0.3=36728640000000000=3.7e16
GTX 285 with 708.5 GFLOP/s",85,48,"""The entire training procedure for each autoencoder took about 2 days on an Nvidia GTX 285 GPU.""",NVIDIA GeForce GTX 285,1,
Deep rectifier networks,4/13/2011,,,,,,,,,,
Optimized Single-layer Net,4/11/2011,,,,,,,,,,
YouTube Video Recommendation System,9/26/2010,,,,,,,,,,
RNN LM,9/26/2010,70265000,"This database entry refers to the 3xRNN rows in Table 2 (static and dynamic likely use the same model ensemble, but allow the model weights to update once when testing the dynamic version).

I assume the 3xRNN represents interpolation between the three largest models shown explicitly (RNN 250/5, RNN 250/2, and RNN 400/10). This seems likely, since smaller models do considerably worse on their own.

In the following colab notebook, I estimate vocabulary sizes for the NYT Gigaword data at around 54.4k, 41.4k, and 27.6k for merge thresholds of 2, 5, and 10, respectively: https://colab.research.google.com/drive/1K5qH0EqXtFwTLESNtp4oelCM28GpGXt6#scrollTo=tedUkbgklNJ3

So the total number of parameters in each constituent model is:
- RNN 250/2: (250 + 54.4k) * 250 + (250 * 54.4k) = 27,262,500
- RNN 250/5: (250 + 41.4k) * 250 + (250 * 41.4k) = 20,762,500
- RNN 400/10: (400 + 27.6k) * 400 + (400 * 27.6k) = 22,240,000

In total: 70,265,000 parameters",5.40E+16,"""Convergence is usually achieved after 10-20 epochs.""
Training was done over a 6.4M subset of the NYT section of English Gigaword.

6 * 70,265,000 * 20 * 6.4M = 5.396e16",1,504,"""it takes several weeks to train the most complex models.""
Rough guess, 3 weeks = 504 hours

Assuming these models trained at the same time on different machines.",,,
Fisher-Boost,9/5/2010,,,,,,21.5,"""Extracting and projecting the SIFT features for the 350K training images takes approx. 15h (150ms / image), learning the GMM on a random subset of 1M descriptors approx. 30 min, computing the Fisher vectors. Improving the Fisher Kernel for Large-Scale Image Classification 155
approx. 4h (40ms / image) and learning the 18 classifiers approx. 2h (7 min / class). ""
15 + 0.5 + 4 + 2 = 21.5 hours",,,
ReLU (NORB),6/15/2010,16210006,"""The stereo-pair images are subsampled from their original resolution of 108 × 108 × 2 to 32 × 32 × 2 to speed up experiments [...]  the architecture
with the best results have 4000 units in the first layer
and 2000 in the second [...] there are 58,320 test
cases (9,720 cases per class) ""

So the architecture has (32*32*2+1)x4000 + (4000+1)*2000 + (2000+1)*58,320/9,720 parameters",,,,,,,,
ReLU (LFW),6/15/2010,,,,,,,,,,
Mid-level Features,6/13/2010,,This is extracting low-level SIFT features then max-pooling them and using in a linear SVM. The training compute could be estimated loosely for the SVM part.,,,,,,,,
Deconvolutional Network,6/13/2010,,,,,,,,,,
Word Representations,6/1/2010,,,,,,,,,,
Feedforward NN,5/13/2010,7082000,"pg250 of the paper, section 2.3: 
""We optimized feedforward neural networks with one to
five hidden layers, with one thousand hidden units per
layer""

Input is a flattened 32x32 image, which corresponds to an input vector of length 3072

Output is a number from 0-9, so 10 neurons

No. of params: 3072*1000 + 4*1000*1000 + 1000*10 = 7,082,000
",3.5E+14,"Roughly two times the number of parameters for ops per forward pass. 

So 2*7082000 params*3.5*140 epochs * 50k training images = 3.5e14",,,,,,
6-layer MLP (MNIST),3/1/2010,12110000,Table 1,1.30788E+14,"""Networks with up to 12 million weights can successfully be trained by plain gradient descent to achieve test errors below 1% after 20-30 epochs in less than 2 hours of training.""

60k images in each MNIST epoch.

Architecture-based estimate: 6 * 12.11M * 60k * 30 = 1.31e14

We can also get a rough hardware estimate. The authors use single precision, GTX280 gets 6.221e11 FLOPs in single precision. Training 30 epochs takes less than 2 hours, but on each epoch the training set is augmented in online fashion, which takes 93 seconds.
(2*3600 - 93*30) * 6.221e11 * 0.3 = 8.23e14

The architecture approach seems less uncertain.",,2,"""less than 2 hours of
training""","NVIDIA GeForce GTX 280,Intel Core 2 Quad Q9450",,
Stacked Denoising Autoencoders,1/3/2010,,,,,,,,,,
Super-vector coding,1/1/2010,1025,"Somewhat low confidence, but it seems like the number of learnable parameters is the size of the codebook, plus parameters in the SVM used for classification. Since it is a linear SVM, there will be one parameter per input feature, plus a single bias term.

So in total, 512 learnable codebook values, plus 513 SVM parameters = 1025 parameters",,,,,,,,
3D city reconstruction,9/29/2009,,,,,,,,,,
BellKor 2007,9/21/2009,,,,,,,,,,
MatrixFac for Recommenders,8/7/2009,,,,,,,,,,
Pragmatic Theory solution (Netflix 2009),8/1/2009,,"This is an ensemble of many smaller models. Ideally, the number of parameters of all the sub-models should be added up and recorded here.",,"This is an ensemble of many smaller models. Ideally, the training compute of all the sub-models should be added up and recorded here.",,,,,,
BigChaos OptiBlend,8/1/2009,,,,,,,,,,
BellKor 2009,8/1/2009,,,,,,,,,,
BellKor 2008,8/1/2009,,,,,,,,,,
GPU DBNs,6/15/2009,100000000,"""For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day.""",1E+15,"https://www.getguesstimate.com/models/19602

6435 GPU seconds for 1M examples
Single GTX 280 with 622.1 GFLOPS
All results are reported for 1M examples, unclear if they ran larger training experiments.",,,,,,
Conv-DBN,6/14/2009,,,,,,,,,,
Deep Boltzmann Machines,4/16/2009,,,,,,,,,,
RBM Image Classifier,4/8/2009,80000000,"Best performing model (see Figure 3.1) had 10,000 hidden units in one hidden layer and 8000 visible units",,,,,,,,
Semantic Hashing,12/10/2008,2600000,,,,,,,,,
HLBL,12/8/2008,1846400,"""Except for where stated otherwise, the models used for the experiments used 100 dimensional feature vectors and a context size of 5.""
""The vocabulary size for this dataset is 17964.""
Embedding: 17964 * 100 = 1796400
Context matrices: 5 * 100 * 100 = 50000
Unembedding: 0 (tied embedding “while the matrix of weights from the hidden layer to the output layer is simply the feature vector matrix R”)
Total: 1796400 + 50000 = 1846400
",,"6ND: 6 * 1846400 * 14000000 = 155,097,600,000,000 FLOP per epoch.
Not stated how many epochs for training.",,,HLBL with largest tree (T7) takes 32 minutes per epoch. Unstated how many epochs they trained for.,,,
ADAPTIVE NLPM,12/8/2008,12198000,"None given but it does say "" Since each non-leaf node in a tree has its own feature vector, the number of free parameters associated with the tree is linear in this quantity"", and the largest model (T7: ADAPATIVE(0.4) x 4) has 121980 of them. The feature vectors are 100-dimensional. I've done the dubious thing of multiplying the two to give an estimate.",,,,,"32 minutes per epoch for the largest model, but unfortunately no epoch count given.

""Models were trained using the learning rate of 10^−3 until the perplexity on the validation set started to increase. Then the learning rate was reduced to 3 × 10^−5 and training was resumed until the validation perplexity started increasing again.""",,,
BigChaos 2008,11/25/2008,,,,,,,,,,
Sparse digit recognition SVM,11/19/2008,,,,,,,,,,
Boss (DARPA Urban Challenge),7/23/2008,,,,,,,,,,
Semi-Supervised Embedding for DL,7/5/2008,,,,,,,,,,
Denoising Autoencoders,7/5/2008,,,,,,,,,,
Deep Multitask NLP Network,7/5/2008,1500000,"With a word vector size of 50 and a vocabulary size of 30,000, the embedding matrix has 1,500,000 parameters. There are also some small convolutional and dense layers with far fewer parameters.",,,,168,1 week on 1 computer,,,
Multiscale deformable part model,6/23/2008,,,,,,,,,,
BLSTM for handwriting (2),12/3/2007,100881,"For the raw input representation,
there were 4 input units and a total of 100,881 weights",,,,,,,,
Enhanced Neighborhood-Based Filtering,10/28/2007,,,,,,,,,,
BLSTM for handwriting (1),9/23/2007,,,,,,,,,,
Regularized SVD for Collaborative Filtering,8/12/2007,,,,,,,,,,
Fisher Kernel GMM,7/16/2007,,,,2.5 hours on an AMD Opteron 2.4GHz with 4GB RAM,,2.5,"""With Fisher kernels, the training
cost is reduced down to approximately 2h30.""",,,
SB-LM,6/22/2007,3E+11,Table 2,1.45E+18,"Assuming a Nehalem based processor with 8 FLOP/cycle (https://www.agner.org/optimize/microarchitecture.pdf#page=105.06) , 2 cores and 2.33 GHz clock speed: 8*2*2330000000=37280000000 FLOP/s
Trained for 1 day on 1500 machines (Table 2)
Compute: 1500*37280000000*1*24*60*60*0.3=1449446400000000000=1.4e18
",,24,Table 2,,1500,
KN-LM,6/22/2007,21000000000,Table 2,7.73E+17,"Trained for 2 days on 400 machines (Table 2)
Assuming a Nehalem based processor with 8 FLOP/cycle (https://www.agner.org/optimize/microarchitecture.pdf#page=105.06) , 2 cores and 2.33 GHz clock speed: 8*2*2330000000=37280000000 FLOP/s
Compute: 400*37280000000*2*24*60*60*0.3=773038080000000000=7.7e17",,48,Table 2,,400,
Restricted Bolzmann machines,6/20/2007,,,,,,,,,,
λ-WASP,6/1/2007,,,,,,,,,,
Empirical evaluation of deep architectures,6/1/2007,,,,,,,,,,
Sparse Energy-Based Model,12/4/2006,,,,,,,,,,
Greedy layer-wise DNN training,12/4/2006,,,,,,,,,,
Local Binary Patterns for facial recognition,12/1/2006,,"Shallowly investigated, couldn't find much.
",,,,,,,,
Sparse Vision Encoding,11/1/2006,,,9.6048E+12,"(Just for natural images)

""All experiments were conducted on a Linux machine with AMD Opteron 2GHz CPU and 2GB RAM. ... For example, we were able to learn a set of 1,024 bases (each 14×14 pixels)in about 2 hours and a set of 2,000 bases (each 20×20 pixels) in about 10 hours.""

I filtered for 2GHz Opteron models that came out in 2005, of which there are five: https://www.techpowerup.com/cpu-specs/?mfgr=AMD&released=2005&generation=AMD%20Opteron&sort=name

Found a source which indicates 3 cycles per 32-bit multiply, and 5 per 64 bit: https://www.cse.wustl.edu/~roger/569M/m2066.pdf 
Assuming 32-bit precision, 2e9 cycles/s / 3 cycle/FLOP = 6.67e8 FLOP/s

10 * 3600 * 6.67e8 * 0.4 = 9.6048e12 FLOPs",,10,"""...in about 2 hours and a set of 2,000 bases (each 20×20 pixels) in about 10 hours.""",,,
Dimensionality Reduction,7/18/2006,3800000,,,,,,,,,
Deep Belief Nets,7/18/2006,1600000,,,,,,,,,
CTC-Trained LSTM,6/25/2006,114662,"""The hidden layers were fully connected to themselves
and the output layer, and fully connected from the input layer. The input layer was size 26, the softmax output layer size 62 (61 phoneme categories plus the blank label), and the total number of weights was
114, 662.""

https://www.cs.toronto.edu/~graves/icml_2006.pdf",,,,,,,,
Spatial Pyramid Matching,6/17/2006,,,,,,,,,,
DrLIM,6/17/2006,37097,Architecture described in figure 3,,,,,,,,
FAST,5/7/2006,,,,,,,,,,
TFE SVM,2/2/2006,,,,,,,,,,
Stanley (DARPA Grand Challenge 2),1/1/2006,,"""Our  approach  and  the underlying  probabilistic  Markov  model  possess  anumber  of  unknown  parameters.  These  parameters include the height threshold, the statistical acceptance  probability  threshold,  and  various  Markov chain error parameters the noise covariances of theprocess noise and the measurement noise. Stanley uses a discriminative learning algorithm for  locally  optimizing  these  parameters.""",,,,,,,,
Monocular Depth Prediction,12/5/2005,1472256,"""In detail, we use different parameters (θr, σ1r, σ2r) for each row in the image, because the images we consider are taken from a horizontally mounted camera, and thus different rows of the image have different statistical properties.""
""We collected a total of 425 image+depthmap pairs, with an image resolution of 1704x2272 and a depthmap resolution of 86x107""

The dimensionality of each parameter set isn't totally clear, but from equation (1) it seems like θ is the same length as the absolute depth input features (646) and σ1 and σ2 are scalar. If so, then we should have:
2272 * (646 + 1 + 1) = 1,472,256 parameters",,,,,,,,
RankNet,8/7/2005,5711,"Model is ""a two layer net with 10 hidden units""
Input is of size 569 ""In all, we use 569 features""
Parameters:
569*10 + 10 for hidden layer
10*1 + 1 for output layer",3.48208E+12,"FLOPs per forward pass: 2*parameters = 11422
FLOPs per pair (data point): two forward passes and one backward pass (""A forward prop is performed for the first sample; each node’s activation and gradient value are stored; a forward prop is then performed for the second sample, and the activations and gradients are again stored. The gradient of the cost is then *formula*"") = 2*11422 + 2*11422 = 45688
Total FLOPs = (FLOPs per pair = 45688)*(pairs = 3,464,289)*(epochs = 22) = 3.48E12",22,5.85,Table 6,,,
BiLSTM for Speech,8/1/2005,152061,"""The hidden layer sizes were chosen to ensure that all networks had roughly the same number of weights W (≈100,000). However, for the MLPs the network grew with the time-window size, and W varied between 22,061 and 152,061.""",2.41246E+13,"Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.

Forward FLOP: 2*200000=400000
Trained on 4158 utterances (TIMIT)
""We found that large networks, of around 200,000 weights,
gave good performance""
TIMIT has around 5 hours total. Estimated utterance length: 5*60*60/(4620+1680)=2.86s
The frame size was 5 ms
Frames per utterance: 2.86/0.005=572
Training FLOP for one epoch: 3*400000*572*4158=2854051200000

Epochs unclear, based on OpenAIs estimate it would be 8. ",,,,,,
Histograms of Oriented Gradients,6/25/2005,,,,,,,,,,
ConvNet similarity metric,6/20/2005,,,,,,,,,,
Hiero,6/1/2005,120000000,"Very unsure, but the paper mentions 
""We ran the training process of Section 3 on the same data, obtaining a grammar of 24M rules"" 
and 
""For our experiments we used the following features, analogous to Pharaoh’s default feature set:
• P(γ | α) and P(α | γ), the latter of which is not
found in the noisy-channel model, but has been
previously found to be a helpful feature (Och
and Ney, 2002);
• the lexical weights Pw(γ | α) and Pw(α | γ) (Koehn et al., 2003), which estimate how well the words in α translate the words in γ;
2
• a phrase penalty exp(1), which allows the
model to learn a preference for longer or
shorter derivations, analogous to Koehn’sphrase penalty (Koehn, 2003).""

Suggesting 24M rules * 5 features per rule (?)",,,,,,,,
SACHS,4/22/2005,178,From https://www.bnlearn.com/bnrepository/,,,,,,,,
Hierarchical LM,1/6/2005,,,1.15848E+14,"""The computations were performed on Athlon processors with a 1.2 GHz clock""
FP32 per cycle: 4 (""The bottom line is that the Athlon is capable of delivering as many as four 32-bit, single-precision floating-point results per clock cycle"", https://www.pctechguide.com/amd-technology/amd-athlon) 
Training time per epoch: 1609s (table 1)
Epochs: 30 ""Training is performed over about 20 to 30 epochs according to validation set perplexity (early stopping).""
Assumed utilization: 0.5 
Compute estimate: 0.5*1200000000*4*30*1609=115848000000000=1.16e14",30,13.4,"Training time per epoch: 1609s (table 1)
Total training time 30*1609/60/60=13.408h
",,1,
LMICA,12/1/2004,4096000,"64*64*1000=4096000
""100000 samples of natural scenes of 64 × 64 pixels were given as X""
""LMICA was carried out in 1000 layers""
",2.78208E+15,"69*60*60*8*2800000000*0.5=2782080000000000=2.78e15
""it consumed about 69 hours with Intel 2.8GHz CPU""
- Assuming they used an Intel Pentium 4 processor with 8 FLOP/cycle (https://en.wikipedia.org/wiki/FLOPS)",,,,,,
Invariant CNN,6/27/2004,90575,"""The network has a total of 90,575 trainable parameters.""",9.7423E+11,"""A full propagation through the network requires 3,896,920 multiply-adds."" - it's not entirely clear whether this refers to a forward pass or forward + backward pass (I assumed the latter)
""We used a stochastic version of the Levenberg-Marquardt algorithm with diagonal approximation of the Hessian [7], for approximately 250,000 online updates.""
3896920*250000=974230000000=9.7e11",10,,,,,
Max-Margin Markov Networks,3/1/2004,,,,,,,,,,
CNN Best Practices,8/6/2003,,,,,,,,,,
Unsupervised Scale-Invariant Learning,6/18/2003,451,"See Table 1
",,,,,,,,
Phrase-based translation,5/1/2003,9178890,"There are various components to the system:

- Translation probability model phi
- The distortion probability distribution d
- A langage model p_LM
- A length factor w

Several translation probability models are considered. The most performant one is the AP word alignment model. The sentence length preferred by the authors is 3 words maximum. In the biggest corpus considered (320k phrase pairs) it produces a phrase translation probability table of 1996k entries.

The distortion probability model d is taken from  (Marcu and Wong, 2002).

The distortion probability model must have ~10 parameters at most

The language model p_LM is a back off trigram model from (Seymore and Rosenfeld,1997). AFAIK the cutoff used is not specified. Based on the example on section 4.3 of (Seymore and Rosefeld, 1997), a trigram probability model has about 3866964 + 2674322 + 641604 parameters.

""For each possible phrase translation anywhere in the sentence (we call it a translation option), we multiply its phrase translation probability with the language model probability for the generated English phrase. As language model probability we use the unigram probability for the first word, the bigram probability for the second, and the trigram probability for all following words""

The length factor w is an additional single parameter.

""In order to calibrate the output length, we introduce a
factor w for each generated English word in addition to
the trigram language model ""

In summary, the parameter count seems to be dominated by the trigram language model and the word alignment phrase translation model. ",,,,,,,,
NPLM (Brown),3/15/2003,4124233,"""The number of free parameters is |V|(1 + nm + h) + h(1 + (n − 1)m) [...] For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100""

Brown corpus: n=5, h=100, m=30, |V|=16383
16383*(1+5*30+100)+100*(1+(5-1)*30)=4124233",1.32076E+14,"""For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100. The total number of numerical operations to process a single training example is approximately |V|(1+nm+h)+h(1+nm)+nm""

Brown corpus: n=5, h=100, m=30, |V|=16383, dataset size = 800000, epochs=20
Forward FLOP: 16383*(1+5*30+100)+100*(1+5*30)+5*30=4127383
Adjusting for backward pass with 1:1 ratio, as the by far largest layer (embedding) doesn't require gradients w.r.t. inputs.
Total FLOP: 2*4127383*800000*20=1.3207626e+14",,,,,,
NPLM (AP News),3/15/2003,11904264,"""The number of free parameters is |V|(1 + nm + h) + h(1 + (n − 1)m) [...] For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100""

AP News: n=6, h=60, m=100, |V|=17964
17964*(1+6*100+60)+60*(1+(6-1)*100)=11904264",1.66687E+15,"""For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100. The total number of numerical operations to process a single training example is approximately |V|(1+nm+h)+h(1+nm)+nm""

AP News: n=6, h=60, m=100, |V|=17964, dataset=13994528, epochs=5
Forward FLOP: 17964*(1+6*100+60)+60*(1+6*100)+6*100=11910864
Adjusting for backward pass with 1:1 ratio, as the by far largest layer (embedding) doesn't require gradients w.r.t. inputs.
Total FLOP: 2*11910864*13994528*5=1.6668692e+15",,,,,,
LDA,2/2/2003,,,,,,,,,,
Statistical Shape Constellations,1/1/2003,,,,,,,,,,
Maximum Entropy Models for machine translation,7/6/2002,,,,,,,,,,
Tagging via Viterbi Decoding,6/1/2002,,,,,,,,,,
NEAT,6/1/2002,,,,,,,,,,
Thumbs Up?,5/28/2002,,,,,,,,,,
Decision tree (classification),12/8/2001,12000,"Iteratively learns decision tree features, where each feature has 2 parameters (threshold and parity). They learn 6000 features total: ""The complete face detection cascade has 38 stages with over
6000 features""

Additional weights (see Table 1) are a temporary variable of the optimization and not part of the final model. ",6.3E+13,"
The training compute can be tediously worked out from the pseudocode. I think for dataset size D, number of filters T, the training compute is roughly 180k * D * 3 * T = 6.3e13 FLOPs

The training evaluates each of 180k candidate features at each step, repeated for 6000 steps (as 1 feature is selected per round). 
The operations for one feature evaluation are unclear, but should be low (they only compare specific integer positions in the image). Estimated at 10op. 
180000*6000*10*14460=1.56168e+14",,,,,,
Gradient Boosting Machine,10/1/2001,,,,,,,,,,
Immediate trihead,7/6/2001,,,,,,,,,,
PoE MNIST,11/28/2000,3925310,"10 models, one for each digit. Largest models: 500 epochs, 500 hidden units (Table 2)
""The largest network was the best, even though each digit model contains 392,500 parameters trained on only 4,400 images""
""the classification network had 30 inputs and therefore 300 weights and 10 output biases.""

Total: 392500*10 + 310 = 3,925,310",5.181E+13,"Each model was trained on 4400 examples: ""The largest network was the best, even though each digit model contains 392,500 parameters trained on only 4,400 images.""

Table 2, largest network trained 500 epochs.
10 * 6 * 392500 * 4400 * 500 = 51,810,000,000,000",500,,,,,
Neural LM,11/28/2000,6906980,"(30959*100) + (8*100*120) + (120*30959) = 6,906,980
""This is obtained with a network with the direct architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 words of context.""
""The Hansard corpus (Canadian parliament proceedings, French version) is a stream of about 34 million words, of which 32 millions (set A) was used for training, 1.1 million (set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests. The original data has 106, 936 different words, and those with frequency <= 10 were merged into a single token, yielding IVI = 30,959 different words.""
",6.339E+15,"The authors use a trick to avoid having to calculate the final layer for all possible words in the vocabulary. They precompute a ""short list"" of the most common word following any 2 precursor words with a smoothed trigram model, and then only calculate the softmax over words on the short list. This means only a negligible fraction of the unembedding parameters get used, so the effective number of parameters appears to be (30959*100) + (8*100*120) = 3,191,900

""Apparent convergence of the stochastic gradient descent procedure was obtained after around 10 epochs for Hansard""

6ND:
6*3191900*33100000*10=6.339e15
",10,,,,,
FrameNet role labeling,9/1/2000,,,,,,,,,,
SVD in recommender systems,7/14/2000,,,,,,,,,,
Perceptron for Large Margin Classification,12/1/1999,,,,,,,,,,
IBM Model 4,7/2/1999,,,,,,,,,,
LSTM with forget gates,1/2/1999,276,See Table 1,,,,,,,,
LeNet-5,11/1/1998,60000,"""[LeNet5] contains 390408 connections, but only 60000 trainable free parameters because of the weight sharing""",2.81094E+12,"""[LeNet5] contains 390408 connections"" = multiply-adds
MNIST - 60,000 data points
20 epochs
390408*60000*6*20=2.810938e+12",,,,,,
Social and content-based classification,7/1/1998,,,,,,,,,,
Sparse coding model for V1 receptive fields,12/1/1997,,,,,,,,,,
LSTM,11/15/1997,10504,"Table 2

http://www.bioinf.jku.at/publications/older/2604.pdf",3.1512E+13,"""Due to limited computation time, training is stopped after 5 million sequence presentations""

Each sequence has p=100 elements in the long-delay setting.

COMPUTE = PRESENTATIONS * PRESENTATION LENGTH * UPDATE COMPUTE PER TOKEN

5000000*100*6*10,504.0=",,,,,,
Bidirectional RNN,11/1/1997,13000,"Page 7: ""The structures of all networks are adjusted so that
each of them has about the same number of free parameters
(approximately 13 000 here""",,,,,,,,
SVM for face detection,6/17/1997,,,,,,,,,,
Deep Blue,5/1/1997,8000,"""The new chess chip had a completely redesigned evaluation function, going from around 6400 features to over 8000""",,"The 8000 features were tuned using a mix of human judgment and automated tools using data on chess matches. Unclear how much total ""compute"" went into this.",,,,,,
HMM Word Alignment,8/5/1996,,,,,,,,,,
AdaBoost.M2 Digit Recognition,7/3/1996,,,,,,,,,,
System 11,6/18/1996,6452,"System 11 is a combination of Network 1 and Network 2

Network 1 has 2095 connections and network 2 has 4357 connections (see table 1)",25859616000,"Since there is no parameter sharing, the forward compute is roughly twice that of the number of parameters. We use a 2:1 forward-backward ratio as this is a shallow network, with most connections in the first layer.

Number of passes (Section 2.1):
* ""Nearly 1,050 face examples were gathered from face databases [...]""
* ""Fifteen face examples are generated for the training set from each original image""

Training loop:
1. ""initial set of nonface images by generating 1,000 random images""
2. Train (presumably on whole set)
3. Run + collect false positives
4. ""Select up to 250 of these subimages [...] and add them into the training set [...] Go to step 2""

""A typical training run selects approximately 8,000 nonface images ""

Selecting 8,000 nonface images implies 8000/250 = 32 loops.

Assuming compute is 3 * N * D, we have
* Loop 1: D = 15*1050 + 1000
* Loop 2: D = 15*1050 + 1000 + 250
* So on.

Hence D overall is 32*(15*1050 + 1000) + 250*32/2*(32+1) = 668,000.

Hence compute = 3 * 12904 * 668e3 = 25859616000",,,,,,
MUSIC perceptron,6/3/1996,13607,230*55+56*15+16*6+7*3=13607 (Figure 2),8.81734E+11,"2*13607*3*10800000=881733600000=8.8e11
Training steps: 400*27000=10800000
""After 400 epochs the error of the network""",400,,,,,
LISSOM,11/27/1995,432800,"Total connections 32*32*20*20+20*20*48+20*20*10=432800
Input: 32*32, Lissom: 20*20, Output: 10 (Figure 1a), up to 48 lateral connections per Lissom neuron (Figure 1b)
",1.95545E+11,"Lissom connections: 32*32*20*20+20*20*48=428800
Lissom compute: 2*428800*3*38*2000=195532800000=1.96e11
Perceptron connections: 20*20*10=4000
Perceptron compute: 2*4000*3*500*1700=20400000000=2e10
Total compute: 195532800000+12000000=195544800000=1.96e11
""LISSOM was trained with 2000 patterns""
""The initial self-organizing map was formed in 8 epochs over the training set, gradually reducing the neighborhood radius from 20 to 8. The lateral connections were then added to the system, and over another 30 epochs,""
""Of these, 1700 were used to train the perceptron layer, ""
""After the SOM and LISSOM maps were organized, a complete set of activation patterns on the two maps were collected. These patterns then formed the training input for the perceptron layer. Two separate versions were each trained for 500 epochs,""

",38,,,,,
Support Vector Machines,9/1/1995,100000000,"Section 6.2.2: ""...polynomials
of degree 4 (that have more than 10^8 free parameters)...""
They used 4-degree polynomials for MNIST",,,,,,,,
Random Decision Forests,8/14/1995,,,,,,,,,,
Iterative Bootstrapping WSD,6/26/1995,,,,,,,,,,
Predictive Coding NN,12/2/1994,206910,"5*80*430+430+430*80+80=206910
""P has nk input units and k output units. n is called the ""time-window size""
""Note that the time-window was quite small (n = 5).""
""alphabet consisted of k = 80 possible characters""
""P had 430 hidden units""",1.86219E+13,"2*206910*3*15000000=18621900000000=1.86e13
""The training phase consisted of 25 sweeps through the training set""",25,,,,,
NeuroChess,12/2/1994,72251,"""Prior to learning an evaluation function, the model M (175 input, 165 hidden, and 175 output units)"" = 58,090 parameters
""NeuroChess then learns an evaluation network V (175 input units, 0 to 80 hidden units, and one output units)."" = 14,161 parameters
Total: 58,090 + 14,161 = 72,251",8.58731E+11,"Lower bound: 0.3*2*24*60*60*1400000=72576000000=7.26e10
Upper bound: 0.3*14*24*60*60*1400000*20=10160640000000=1.02e13
Geometric mean: 858730812676=8.59e11 (speculative)
""Thus far, experiments lasted for 2 days to 2 weeks on I to 20 SUN Sparc Stations. ""
SparcStation has 1.4 MFLOPS (https://ieeexplore.ieee.org/document/63671)
",,,,,,
Mixture of linear models,12/2/1994,384000,"“In the example we describe, 7000 training images are sufficient to fit 384,000 parameters“",4.536E+11,"0.3*12*60*60*35000000=453600000000=4.54e11
Assuming a utilization of 0.3 and interpreting ""overnight"" as 12 hours.
“the training procedure is fast enough to do the fitting overnight on an R4400-based machine. “
R4400 has 35MFLOPS (“Compare this to the 200MHz R4400 which is rated at about 35MFLOPS”, http://www.sgidepot.co.uk/perf.html)",,12,"""the training procedure is fast enough to do the fitting overnight on an R4400-based machine.""",,,
JPMAX,12/2/1994,4446,"Inputs are 12x12 pixels (Figure 3). They first train the architecture in Figure 2 a), then freeze it and train the additional layer in Figure 3 b).

Figure 2 a): 2*(12*12*15) + 2*15 = 4,350
Figure 2 b): 2*(12*12*15) + 2*15 + 2*(15*3) + 2*3 = 4,446",80828280,"Training 2a):  ""The learning took about 3000 iterations of steepest descent"" Assuming each iteration refers to a single image.
6 * 4446 * 3000 = 80,028,000

Training 2b): ""While keeping the first layer of weights frozen, this network was trained using exactly the same cost function as the first layer for about 30 iterations using a gradient-based learning method.""

6 * 4446 * 30 = 800,280

Total: 80,828,280",,,,,,
GroupLens,10/22/1994,,"For each pair of users, the system computes the correlation between their scores in the articles they have rated.

Then to make the prediction of a score for a given article and user the system computes a weighted average taking into account the correlations with each other user, the average rating of each user and the average rating of the article.

So the system in total has n+m+n*n ~= n*n parameters, where n is the number of users and m is the number of articles.

To address scaling issues, the system is partioned into clusters of users. It's very unclear what is the number of users per cluster, though the Daily ratings traffic table provided suggests that is around 10k users ",,,,,,,,
Ceramic-MLP,1/7/1994,1888,"Parameters: 100*16 + 16*16 + 16*2 = 1888
Architecture: ""The topology of the classifier was X-Y-Y-2, where X is the number of input components, Y is the number of neurons in each hidden layer and the number of neurons in the output layer is two, which is the number of classes. The two hidden layers were considered to have the same number of nodes for simplification purposes. ""
Input size: ""Each pattern consists of a 10 x 10 pixel sub-image.""
Hidden size: ""Experiments have been made on networks with 6, 9, 12 and 16 hidden nodes. """,4531200000,"Compute estimate: 2*1888*3*400000=4531200000=4.53e9
Training steps: ""In Fig. 6 we report the classification results obtained on the testing set in the 12 and 16 component compressed data after 400000 training iterations""",5000,,,,,
ANN Eye Tracker,11/29/1993,5620,"15*15*20+20+50*10*2+100=5620
Hidden layer is split, 15*15 image input, 2*50 output neurons (see Figure 2)
Hidden size up to 20 neurons (""This architecture was used with varying numbers of hidden units in the single, divided, hidden layer; experiments with 10, 16 and 20 hidden units were performed. "")",17534400000,"2*5620*3*520000=17534400000
Training examples: 2000*260=520000
""As mentioned before, 2000 image/position pairs were gathered for training""
""All of the networks described in this paper are trained with the same parameters for 260 epochs""",260,0.6,"""Training the 8x2 hidden layer network using the 15x40 input retina, with 2000 images, takes approximately 30-40 minutes on a Sun SPARC 10 machine. """,,,
Siamese-TDNN,8/1/1993,744,"""The input is 8 by 200 units, the first convolutional layer is 6 by 192 units with each unit's receptive field covering 8 by 9 units of the input. The first averaging layer is 6 by 64 units, the second convolution layer is 4 by 57 with 6 by 8 receptive fields and the second averaging layer is 4 by 19""
""Two separate sub-networks based on Time Delay Neural Networks (Lang and Hinton, 1988, Guyon et al. 1990) act on each input pattern to extract features,""
""All weights could be learnt, but the two sub-networks were constrained to have identical weights.""
L1: H=1, W=200, C=8, K=9, D=6
L2: H=1, W=64, C=6, K=8, D=4
Parameters:  7*9*8+5*8*6=744",1.28696E+13,"8073216*3*7701*69=12869570138112=1.29e13
Forward pass flop: 2*(2*200*200*8*6+2*64*64*6*4)=8073216
""We used up to 7,701 signature pairs""
Epochs: 69 (Table 1)
",69,,,,,
IBM-5,6/15/1993,1658364,"The model is initiallized with 2.44E+09 translation probabilities, which are progressively culled until 1,658,364 remain. There are other parameters in the models (eg the fertility probabilities that relate each word in the input to the number of words it will align to) but the parameter count is dominated by the translation probabilities.",,,,,,,,
Boosting,11/30/1992,2578,"“The network has 4645 neurons, 2578 different weights, and 98442 connections.“",,,,,,,,
Cancer drug mechanism prediction,10/16/1992,594,"“The network shown has 60 input PEs, one for each cell line, and 6 output PEs“
“Neural networks with three to nine hidden layer PEs used”
9*60 + 6*9 = 594",53460000,"2*594*3*15000=53460000=5.35e7
“The extent of training was 15,000 presentations“",,,,,,
Golem,10/1/1992,,,,,,,,,,
Fuzzy NN,9/1/1992,1166,"Table II: ""he neural network has three hidden layers, with m hidden nodes in each layer"", m = 20, input dim. = 9, output dim. = 6

9*20+20*20+20*20+6*20+66=1166",1403117760,1166 params * 2 FLOP/param * (3 for forward + backward pass) * 460 epochs * 436 examples,,,,,,
TD-Gammon,5/1/1992,25000,"""The best performance was obtained with a network containing 80 hidden units and over 25,000 weights.""",1.82322E+13,"Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.

OpenAI estimate: 1.8e13

Hardware estimate (likely overestimates due to simulation effort)
""on an IBM RS/6000 workstation, the smallest network was trained in several hours, while the largest net required two weeks of simulation time.""
IBM RS/6000 achieves 1.5 GFLOPS on Linpack (https://link.springer.com/rwe/10.1007/978-0-387-09766-4_232) 
14*24*60*60*0.5*1500000000=9.072e+14

Operation counting estimate: 
Forward FLOP: 50000
Each legal move had to be evaluated separately, assuming an average of 10 move options (+2 for backward passes of the chosen move):
50000*12*300000*21=3.78e+12

Keeping the OpenAI estimate as the median estimate. ",,,,,,
Weight Decay,12/2/1991,8386," 7*26*40+40+40*26+26=8386
""The network had 7 x 26 input units, 40 hidden units and 26 output units""",75474000000,"2*8386*3*1500000=75474000000=7.55e10
""It was trained on 400 to 5000 random words from the data base of around 20.000 words,""
""The top full line corresponds to the generalization error after 300 epochs""
",300,,,,,
SRN-Encoded Grammatical Structures,9/1/1991,,,,,,,,,,
RAAM,11/1/1990,1536,"Largest model:
""A 48-16-48 RAAM learned to construct representations ""
Parameters 48*16*2 = 1536",,,,,,,,
SexNet compression,10/1/1990,72940,"900*40*2+40+900=72940
“Images sampled at 30x30 were compressed using a 900x40x900 fully connected back-propagation network”",78775200000,"2*72940*3*90*2000=78775200000
“The compression network trained for 2000 runs on each of 90 faces”",2000,,,,,
SexNet classification,10/1/1990,1640,Largest classification model: 40*40 + 40=1640 (Figure 2),,,,,,,,
ISR network,10/1/1990,,,,,,,,,,
Bankruptcy-NN,6/17/1990,36,"""The input layer consisted of the five nodes, one for each of the ratios. The hidden layer consisted of 5 node.;. The output layer consisted of only one neuron""
30 weights + 6 biases = 36",3059337600," 2*36*3*74*191400=3,059,337,600=3.06e9
""Convergence was reached after 191,400 iterations""",191400,24,,,,
Zip CNN,12/1/1989,9760,"""In summary, the network has 1256 units, 64,660 connections, and 9760 independent parameters""",1.49634E+12,"Its a deep CNN so we assume a backward-forward ratio of 2:1
 2*64660*3*23*167693=1496338054440
""The network was trained for 23
passes through the training set (167,693 pattern presentations).""",,,,,,
Innervator,12/1/1989,10,Each net has 5 units,120000000,10 params * 6 FLOP/param/pass * 4 datapoints * 1000 epochs * 50 individuals * 10 generations,,,,,,
ALVINN,12/1/1989,36627,"1217*29 + 29*46 =36627
“Each of these 1217 input units is fully connected to the hidden layer of 29 units, which is in turn fully connected to the output layer. The output layer consists of 46 units, divided into two groups.”",10548576000,"2 * 36627 * 3 * 40 * 1200 = 10548576000 = 1.05e10
36627 parameters
""After 40 epochs of training on the 1200 simulated road snapshots""",40,,,,,
Speaker-independent vowel classification,11/27/1989,3040,"“The MLP consisted of 64 inputs (the DFf coefficients. each nonnalized between zero and one), a single hidden layer of 40 units, and 12 output units;”",7485696000,"2*3040*3*410400=7485696000=7.49e9
“The network was trained on 100 iterations through the 4104 training vectors.”",100,,,,,
Handwritten Digit Recognition System,11/27/1989,2578,"""In summary, the network has 4635 units, 98442 connections, and 2578 independent parameters.“",1.8144E+11,"1.4e6 * 3 * 24 * 60* 60 * 0.5 = 181440000000 = 1.81e11
""A complete training session (30 passes through the training set plus test) takes about 3 days on a SUN SPARCstation 1""
Sparcstation 1 has an estimated compute of 1.4 MFLOPS (source: https://ieeexplore.ieee.org/document/63671 )",30,72,"""A complete training session (30 passes through the training set plus test) takes about 3 days on a SUN SPARCstation 1""",,,
Truck backer-upper,6/18/1989,805,6*25+25+8*45+45*6=805 (see Figure 6),,,,,,,,
Invariant image recognition,6/18/1989,,,27000000000,"0.5*6*60*60*2.5e6 = 27000000000 = 2.7e10
Trained for 6h on a SUN-4 (section 4)
Assumed utilization of 0.5
SUN-4 is estimated at 2.5e6 FLOP/s (Nordhaus, 2007)",,6,Section 4,,,
Time-delay neural networks,3/3/1989,,,,,,,,,,
Q-learning,1/1/1989,,,,,,,,,,
MLP baggage detector,1/1/1989,,"3 layer network, input layer (<20), hidden layer, output layer (3)",,,2000,,,,,
MLN-ASR,8/1/1988,10000,"“For an MLN of about 10,000 links, the time was 115 CPU msecs for the recognition of a spoken letter and 317 msecs for the learning of a spoken letter on the SUN 4/280. A 20% reduction was obtained on the VAX 8650”",296425000,"“For an MLN of about 10,000 links, the time was 115 CPU msecs for the recognition of a spoken letter and 317 msecs for the learning of a spoken letter on the SUN 4/280. A 20% reduction was obtained on the VAX 8650”, “Learning and recognition were performed on a VAX 8650.”
Dataset: 70*10*2=1400 (Train) 10*10*2=200 (Test)
“The ten words of the El set were pronounced twice by 80 speakers (40 males and 40 females)”
“The data from 70 speakers were used as a training set while the data from the remaining 10 speakers (6 males and 4 females) were used for the test”
VAX 8650 FLOPS  = 1.67E+06 (Nordhaus)
Training time: 317ms * 1400  * 0.8 = 355040ms = 355s
Estimate: 0.5 * 1.67e6 * 355 = 296425000 = 2.96e8",,0.1,,,,
NetTalk (transcription),6/6/1987,18629,"""The connections in the network are specified by a total of 18629
weight parameters (including a variable threshold for each unit)""",28328002560,18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1024 words/epoch * 4.5 letters/word,55,,,,,
NetTalk (dictionary),6/6/1987,18629,"""The connections in the network are specified by a total of 18629 weight parameters (including a variable threshold for each unit)""",27664065000,18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1000 words/epoch * 4.5 letters/word (estimated number of letters),55,,,,,
Optimized Multi-Scale Edge Detection,11/1/1986,,,,,,,,,,
Back-propagation,10/1/1986,720,"Architecture in Figure 3: 

24+12 input -> 6 + 6 hidden -> 12 hidden -> 6 hidden -> 24 output

Parameters: 6*24+6*12+12*12+6*12+24*12=720",673920000,"We assume that the number of mult-adds per pass is equal to the number of parameters -> 2*720=1440 FLOP per forward pass.

""We trained the network for 1500 sweeps""
There are 104 relationship triplets (""[...] of the 104 possible triplets"")

FLOP: 1500*104*3*1440=673920000

",,,,,,
Distributed representation NN,8/15/1986,432,"Parameters: 24*6 + 12*6 + 12*12 + 12*6 =432
""Figure 5: The activity levels in a five-layer network after it has learned. The bottom layer has 24 input units on the left for representing person 1 and 12 units on the right for representing the relationship. The white squares inside these two groups show the activity levels of the units. There is one active unit in the first group (representing Colin) and one in the second group (representing has-aunt). Each of the two groups of input units is totally connected to its own group of 6 units in the second layer. These two groups of 6 must learn to encode the input terms as distributed patterns of activity. The second layer is totally connected to the central layer of 12 units, and this layer is connected to the penultimate layer of 6 units.""
",388800000," 2*432*3*1500*100=388800000=3.9e8
""After 1500 sweeps through all 100 training examples the weights were very stable """,1500,,,,,
PDP model for serial order,1/5/1986,,,,,,,,,,
Error Propagation,1/3/1986,,,,,,,,,,
Learnability theory of language development,7/1/1984,,,,,,,,,,
Hierarchical Cognitron,4/1/1984,9315,"Parameters 5*5*9*3*3 + 3*3*9*3*3*9 + 9*3*3*9 = 9315
""The numbers of excitatory cells in these four layers were: 7x7 in U0, 5x5 in  U1, 3x3 in U2, and 9 in U3""
""Each feature-extracting cell in layer U1 receives excitatory modifiable afferent connections from 3x3 cells in layer U0""
""On the other hand, each feature, each extracting cell in layers U2 and U3 receives excitatory modifiable connections from all 9 cells in each of the 3 x 3 hypercolumns in the layer preceding it. Therefore, it receives 3 x 3 x 9 afferent excitatory modifiable connections altogether""
",,,,,,,,
ASE+ACE,9/1/1983,324,"The system consists of two parts: ACE and ASE, each with 162 weights (=324 parameters). Found in Figures 2 and 3.",324000000,"324 * 2 * 500000 = 324000000 = 3.24e8. The calculation assumes ""compute per forward pass"" = ""number of parameters"" = ""compute per backward pass"". Their model only has a single layer and is trained with simple update rules instead of gradient descent. Training details are described in Section IX.

Note that this is the compute for a single run; they appear to have repeated training 10 times for the ASE+ACE system.",,2.8,"""Runs consisted of 100 trials unless the run's duration exceeded 500 000 time steps (approximately 2.8 h of simulated real time)"" 
""Almost all runs of the ASE/ACE system [...], were terminated after 500 000"" (Section IX)",,,
Hopfield network,4/1/1982,9900,"My understanding is that the biggest Hopfield networks they studied had N=100 units. 

Each unit has 99 synapses Tij from each other unit, for a total of 100*99 parameters",,,,,,,,
Kohonen network,7/25/1981,4096,"The input vectors are 3D.
I could not find the grid size, but from the images it looks 8x8.
So the network was 8x8x3 parameters.",,,,,,,,
Neocognitron,4/1/1980,1140576,"""The synaptic connections from S-layers to C-layers
are fixed and unmodifiable. [...]
The numbers of excitatory cells in these seven layers are: 16x16 in U0, 16x16x24 in Us1, 10x10x 24 in Uc1, 8x8x24 in Us2, 6x 6x 24 in Uc2, 2x2x24 in Us3, and 24 in Uc3 
[...]
 the number of input synapses to each S-cell is 5 x 5 in layer Us1 and 5x5x24 in layers Us2 and Us3
[...]
The number of excitatory input synapses to each C-cell is 5x5 in layers Uc1 and Uc2, and is 2x2 in
layer Uc3
""

The number of synapses into each S-layer is:

S1: (16*16*24)*(5*5) 
S2: (8*8*24)*(5*5*24)
S3: (2*2*24)*(5*5*24)

We assume one parameter a per synapse into each cell in a S-layer, and one parameter b per each cell in a S-layer.",273738240,"""It does not necessarily mean that all of these input synapses are
always fully reinforced. In usual situations, only some of these input synapses are reinforced, and the rest of them remains in small values [...] Each of the five stimulus patterns has been presented 20 times to the network. By that time, self organization of the network has almost been completed.""

We multiply by 2 to account for multadds in the forward pass. 
There is no real backward pass, weights are only updated sparsely. Estimating 20% additional weight update compute compared to the forward pass:
2*1,140,576.0*1.2*100=273738240
",,,,,,
Statistical continuous speech recognizer,4/30/1976,,,,,,,,,,
Cognitron,9/1/1975,21600,"4 layers, 288 neurons per layer, weights connect each neuron to only 25 neurons in the previous layer.
Only 3 layers with learnable weights, the first layer is just an input representation (see Fig 5)
3*288*25 parameters",5184000,"No real backward pass as weights are sparsely updated. Assuming 20% additional compute for the weight update.
Total compute estimate: 100*2*3*288*25*1.2 = 5184000
",20,,,,,
Piecewise linear model,11/1/1973,357,"16 input features + bias = 17 input features
7*6/2 = 21 Hyperplanes
17*21 = 357 parameters
""For the multicategory problem involving NR categories, a total of NR(NR - 1)/2 hyperplanes are used to partition the pattern space.""
""The input variables to the classifier consisted of the mean variance of the four textural features (f1,f2,f3, andfg obtained from the distance 1 gray-tone spatial-dependence matrices) and eight spectral features (comprised of the mean variance of the image gray-tone values) in each of the four spectral bands""
",,,,,,,,
Graph-based structural reasoning,9/1/1970,,,,,,,,,,
Decision tree adaline,5/1/1969,2450,"5 adaline were trained on binary decisions (p. 1)
Each adaline had up to 490 input weights (“meshes”)
Total parameters = 5*490=2450
",,,,,,,,
GLEE,7/1/1968,,,,,,,,,,
BOXES,7/1/1968,,,,,,,,,,
LTE speaker verification system,11/1/1966,2061,"2 connected systems, 1st level LTE and 2nd level LTE.
1st Level: 1810 parameters (""Thus, every 20 msec after the beginning of the utterance, the 15 filter amplitudes were each represented by a 12-bit code, resulting in a 180-bit time sample of the spectrum for that interval. Each time sample was fed to the first-level LTE's, which reduced it to a 10-bit code"")
2nd Level: 251 parameters (""This resulted in a 250-bit input pattern to the second level for the first half-second of each utterance. Each 250-bit pattern was then classified by the LTE into one of two classes"")",105917060,"1st and 2nd level system are trained separately, multiple versions of both are trained, I chose the largest clearly described training runs.

1st level LTE compute: 2*1810*28700=103894000=1.04e8
1st level steps: 28700 (""Only 287 samples were selected to train the 10 LTE's. The same algorithm was used as that used with the 100-class gain. Two LTE's
converged before 100 training passes."")

2nd level LTE compute: 2*251*4030=2023060=2e6
2nd level steps: 4030 (31 epochs, 130 training examples, see Table 3)

Total compute: 103894000+2023060=105917060=1.06e8 (assuming no backward pass since they didn't use backpropagation)
",131,,,,,
Heuristic Reinforcement Learning,10/1/1965,,,1080000,"Figure 10 shows their largest system is trained for 3h and was trained on an analog IBM 1620 that was simulated on a digital IBM 1710.
Nordhaus, 2007 lists the IBM 1620 at 200 multiplications per second and doesn’t contain the 1710
Flops estimate: 0.5 * 3 * 60 * 60 * 200 = 1080000 = 1.08e6
Assumed utilization of 0.5
",,3,Figure 10,,,
Print Recognition Logic,1/1/1963,,,22500000,"0.5*2.5*60*60*5000 = 22500000 = 2.25e7
Assumed utilization of 0.5
Trained for 2-3h on an IBM 7090 (from Introduction)
Estimated IBM 7090 at 5000 FLOP/s based on multiplications per second (Nordhaus, 2007)
Note: the Nordhaus estimate is very different from Wikipedia's estimate of 100000 FLOP/s, which cites a PowerPoint as source.",,2.5,2-3h (from Introduction),,,
MADALINE I,7/1/1962,,,,,,,,,,
Linear Decision Functions,6/1/1962,,,1559250,"0.5*45*35*1980 = 1559250 = 1.56e6
Trained using IBM punched cards, computation took 45 * 35s for all 10 digits (Section Estimating the Linear Decision Function).
Multiplications per second estimate based on publication year: 1.98e3 (regression on Nordhaus data).
Assumed utilization of 0.5",,0.4375,"""Forty-five hyperplanes are required in the complete linear decision function""
""About 35 seconds, on the average, was required to determine a hyperplane, given an initial position.""",,,
ADALINE,6/30/1960,17,"""The machine's total experience is stored in the values of the weights a0,...,a16""",6600,"""The method of searching that has proven most useful is the method of steepest descent""

Apparently each pattern was only shown once to the system.

So the training compute is (forward pass compute) * (3 for backprop) * dataset size

This is a single layer (and single neuron) which does not require gradients w.r.t. inputs - 1:1 forward-backward ratio

",,,,,,
Perceptron (1960),3/30/1960,1000,""" The first program was designed to handle
up to 1000 A units, and a 72 by 72 sensory mosaic. It
was found that this large sensory system presented
stimuli with a fineness of grain considerably better than
the limits of discrimination of a thousand-unit percep-
tron, and at the same time, required an excessive
amount of time for stimulus transformations, since each
illuminated point in the stimulus must be transformed
individually into its image point.""",720000000,"4000 * 12000 * 15
from the text ""This program uses the IBM 704 computer to simulate per-ceptual learning, recognition, and spontaneous classification of visual stimuli in the perceptron,""
from https://en.wikipedia.org/wiki/IBM_704 The 704 can execute up to 12,000 floating-point additions per second.
"" For the first system, the computing time averaged about 15 seconds per stimulus cycle, ""
In Fig 10 we see up to 4000 stimuli",,,,,,
Pattern recognition and reading by machine,12/1/1959,2625,A two bit state is recorded for each of the 75 cell pairs and each of the 25+10 characters recognized.,,,,,,,,
Samuel Neural Checkers,7/1/1959,16,"""with 16 terms for generalization learning""

""Mention has been made several times of the procedure
for replacing terms in the scoring polynomial. The program, as it is currently running, contains 38 different
terms (in addition to the piece-advantage term), 16 of
these being included in the scoring polynomial at anyone
time and the remaining 22 being kept in reserve.""",428400000,"""it can learn to do this in a remarkably short period of time 8 or 10 hours of machine-playing time)""

""The availability of a larger and faster machine (the IBM 704), coupled with many detailed changes in the programming procedure, leads to a fairly interesting game being played, even without any learning.""

""The Type 704 is the first large-scale, commercially available computer to employ fully automatic floating point arithmetic commands. [...]. Floating point addition or subtraction operations require 84 microseconds.""

source: https://www.ibm.com/ibm/history/exhibits/mainframe/mainframe_PP704.html

""An idea of the learning ability of this procedure can be gained by analyzing an initial test series of 28 games""

""Each game averaged 68 moves (34 to a side), of which approximately 20 caused changes to be made in the scoring polynomial.""

10*3600*1000000/84=428571428",,,,,,
Pandemonium (morse),2/1/1959,,"The paper mentions 11 function types. Unclear how many times they are called (number of ""demons"" in their Pandemonium implementation).",600000000,"The paper mentions using an IBM 704, which can execute up to 12,000 floating-point additions per second (https://wikiless.org/wiki/IBM_704). My best guess as to how long it ran for ranges between 1h to 2 days, which when plugged into guesstimate (https://www.getguesstimate.com/models/19625), i.e., taking the log mean, gives a mean estimate of 600M",,,,,,
Perceptron Mark I,1/1/1957,1000,"""Figure 4.8 Illustration of the Mark 1 perceptron hardware. The photograph on the left shows how the inputs were obtained using a simple camera system in which an input scene, in this case a printed character, was illuminated by powerful lights, and an image focussed onto a 20 × 20 array of cadmium sulphide photocells, giving a primitive 400 pixel image. The perceptron also had a patch board, shown in the middle photograph, which allowed different configurations of input features to be tried. Often these were wired up at random to demonstrate the ability of the perceptron to learn without the need for precise wiring, in contrast to a modern digital computer. The photograph on the right shows one of the racks of adaptive weights. Each weight was implemented using a rotary variable resistor, also called a potentiometer, driven by an electric motor thereby allowing the value of the weight to be adjusted automatically by the learning algorithm.""

source: Bishop, Christopher M. (2006). Pattern Recognition and Machine Learning

The Perceptron had a 400-pixel visual input and 1000 neurons in the hidden layer. https://twitter.com/DiegoKuonen/status/1130352233223262208",694894.9377,"Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.

Additional experiment described in https://babel.hathitrust.org/cgi/pt?id=coo.31924004657973&seq=70 
- 400 input (20x20) - 512 hidden with 40 fixed connections each (not learned) - 1 output (learned)
Parameters: 512*1=512
Forward flop: 41*512=20992
Forward + “backward flop”: 43*512=22016 (only last layer was adjusted)
100*22016=2201600
",,,,,,
Sequence-based pattern recognition,3/1/1955,,,,,,,,,,
Self Organizing System,3/1/1955,225,Figure 4 contains the learnt weight matrix,,,,,,,,
Genetic algorithm,7/2/1954,,,,,,,,,,
SNARC,1/8/1952,40,"The link below seems to suggest the SNARC had 40 cells, each with a dial that acts as a configurable weight.

https://www.webofstories.com/play/marvin.minsky/137",,,,,,,,
Theseus,7/2/1950,40,"The learned part is the maze configuration. There are 25 squares of the maze. The 16 squares to the left top corner have each one adjacent square down and one adjacent square up, for a total of 16*2 walls. We only need to count the 8 spare walls connecting the squares in the right side and the bottom side. In total there are 16*2+8 walls.",40,"The ""training"" consists on the mouse running around and checking each wall (assuming each relay switch is one operation).",,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,

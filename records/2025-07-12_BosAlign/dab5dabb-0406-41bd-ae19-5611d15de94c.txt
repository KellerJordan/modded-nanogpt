import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn, autocast
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
#import wandb

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)

        params = list(params)
        sizes = {p.shape for p in params}

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params,))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * self.world_size
            for base_i in range(0, len(params), self.world_size):
                if base_i + self.rank < len(params):
                    grad = params[base_i + self.rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + self.world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * self.world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), self.world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                futures.append(dist.all_gather(params_pad[base_i:base_i + self.world_size], params_pad[base_i + self.rank], async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01, rank: int = 0, world_size: int = 1):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        self.rank = rank
        self.world_size = world_size

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(
                params=group_params,
            ))
        super().__init__(param_groups, defaults)

    @torch.compile
    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // self.world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)

                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']

                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)

                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t

                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        for param in self.embed.parameters():
            param.lr_mul = 75.
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embeds in self.value_embeds:
            for param in self.value_embeds.parameters():
                param.lr_mul = 75.
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.lr_mul = 27.5
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % world_size
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            #return causal_mask
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, world_size: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos:pos+max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()

    starts = []
    batch_end=None
    for i in range(len(boundary_positions) - 1):
        end = boundary_positions[i + 1].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == world_size:
                batch_end = end
                break
            start = end
    assert batch_end is not None # increase max_batch_span if necessary
    batch_span = batch_end-pos
    return starts, batch_span

def distributed_data_generator(filename_pattern: str, batch_size: int, rank: int, world_size: int, align_to_bos: bool):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    batch_span = batch_size
    max_batch_span = 2*batch_size if align_to_bos else batch_size #provide buffer to handle samples up to length local_batch_size

    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, world_size, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    train_align_to_bos = True # align local batch start indicies with next bos_token
    val_align_to_bos = False # False to maintain same eval as prior records
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

#if master_process:
#    wandb.init(project="modded-nanogpt-tiny", name=f"run-{os.path.basename(__file__)}", save_code=True)

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=True):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
if master_process:
    print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=next_multiple_of_n(50257, n=128), num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094

optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0, rank=rank, world_size=world_size)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]

for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

for n, p in model.named_parameters():
    wd_mul = getattr(p, "wd_mul", 1.0)
    lr_mul = getattr(p, "lr_mul", 1.0)

    print0(f"{n}: {p.shape} {p.dtype} {wd_mul} {lr_mul}")

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
embedding_params = sum(p.numel() for n, p in model.named_parameters() if "embed" in n)
non_embedding_params = total_params - embedding_params

print0(f"")
print0(f"Model parameters:")
print0(f"  Total parameters: {total_params:,}")
print0(f"  Embedding parameters: {embedding_params:,}")
print0(f"  Non-embedding parameters: {non_embedding_params:,}")

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    w = min((1 - x) / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.05
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

torch.cuda.synchronize()
dist.barrier()
with torch.profiler.profile() as prof:
    for _ in range(warmup_steps):
        inputs, targets = next(train_loader)
        model(inputs, targets, get_window_size_blocks(1)).backward()
        for opt in optimizers:
            opt.step()
    model.zero_grad(set_to_none=True)
    torch.cuda.synchronize()
    dist.barrier()
os.makedirs("traces", exist_ok=True)
prof.export_chrome_trace(f"traces/trace_{rank}.json")

model.load_state_dict(initial_state['model'])
for opt, opt_state in zip(optimizers, initial_state['optimizers']):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size, args.val_align_to_bos)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        #if master_process:
        #    wandb.log({"val/loss": val_loss}, step=step)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250524+cu126 compiled for CUDA 12.6
Sun Jul 13 01:26:06 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    5856MiB /  81559MiB |      4%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   38C    P0            125W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1517MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           81676      C   /usr/bin/python3                       1508MiB |
|    0   N/A  N/A           81677      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           81678      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           81679      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           81680      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           81681      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           81682      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           81683      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           81677      C   /usr/bin/python3                       1508MiB |
|    2   N/A  N/A           81678      C   /usr/bin/python3                       1508MiB |
|    3   N/A  N/A           81679      C   /usr/bin/python3                       1508MiB |
|    4   N/A  N/A           81680      C   /usr/bin/python3                       1508MiB |
|    5   N/A  N/A           81681      C   /usr/bin/python3                       1508MiB |
|    6   N/A  N/A           81682      C   /usr/bin/python3                       1508MiB |
|    7   N/A  N/A           81683      C   /usr/bin/python3                       1508MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
scalars: torch.Size([64]) torch.float32 1.0 5.0
embed.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.0.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.1.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.2.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
blocks.0.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.0.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.1.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.1.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.2.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.2.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.3.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.3.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.4.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.4.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.5.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.5.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.6.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.6.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.7.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.7.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.8.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.8.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.9.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.9.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.10.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.10.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.11.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.11.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
lm_head.weight: torch.Size([50304, 768]) torch.float32 1.0 27.5

Model parameters:
  Total parameters: 275,742,784
  Embedding parameters: 154,533,888
  Non-embedding parameters: 121,208,896
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1750 train_time:148ms step_avg:148.31ms
step:2/1750 train_time:173ms step_avg:86.50ms
step:3/1750 train_time:246ms step_avg:82.09ms
step:4/1750 train_time:338ms step_avg:84.49ms
step:5/1750 train_time:431ms step_avg:86.16ms
step:6/1750 train_time:523ms step_avg:87.13ms
step:7/1750 train_time:615ms step_avg:87.85ms
step:8/1750 train_time:708ms step_avg:88.48ms
step:9/1750 train_time:800ms step_avg:88.92ms
step:10/1750 train_time:893ms step_avg:89.28ms
step:11/1750 train_time:985ms step_avg:89.56ms
step:12/1750 train_time:1079ms step_avg:89.90ms
step:13/1750 train_time:1174ms step_avg:90.30ms
step:14/1750 train_time:1268ms step_avg:90.60ms
step:15/1750 train_time:1363ms step_avg:90.84ms
step:16/1750 train_time:1456ms step_avg:90.98ms
step:17/1750 train_time:1549ms step_avg:91.11ms
step:18/1750 train_time:1642ms step_avg:91.21ms
step:19/1750 train_time:1735ms step_avg:91.29ms
step:20/1750 train_time:1828ms step_avg:91.39ms
step:21/1750 train_time:1921ms step_avg:91.46ms
step:22/1750 train_time:2013ms step_avg:91.52ms
step:23/1750 train_time:2107ms step_avg:91.61ms
step:24/1750 train_time:2201ms step_avg:91.70ms
step:25/1750 train_time:2295ms step_avg:91.81ms
step:26/1750 train_time:2389ms step_avg:91.90ms
step:27/1750 train_time:2482ms step_avg:91.94ms
step:28/1750 train_time:2577ms step_avg:92.03ms
step:29/1750 train_time:2670ms step_avg:92.07ms
step:30/1750 train_time:2763ms step_avg:92.10ms
step:31/1750 train_time:2856ms step_avg:92.12ms
step:32/1750 train_time:2949ms step_avg:92.15ms
step:33/1750 train_time:3041ms step_avg:92.16ms
step:34/1750 train_time:3135ms step_avg:92.19ms
step:35/1750 train_time:3229ms step_avg:92.24ms
step:36/1750 train_time:3323ms step_avg:92.30ms
step:37/1750 train_time:3416ms step_avg:92.31ms
step:38/1750 train_time:3508ms step_avg:92.33ms
step:39/1750 train_time:3602ms step_avg:92.36ms
step:40/1750 train_time:3695ms step_avg:92.38ms
step:41/1750 train_time:3788ms step_avg:92.40ms
step:42/1750 train_time:3881ms step_avg:92.40ms
step:43/1750 train_time:3975ms step_avg:92.45ms
step:44/1750 train_time:4068ms step_avg:92.46ms
step:45/1750 train_time:4160ms step_avg:92.44ms
step:46/1750 train_time:4254ms step_avg:92.47ms
step:47/1750 train_time:4347ms step_avg:92.49ms
step:48/1750 train_time:4441ms step_avg:92.51ms
step:49/1750 train_time:4535ms step_avg:92.55ms
step:50/1750 train_time:4628ms step_avg:92.56ms
step:51/1750 train_time:4721ms step_avg:92.57ms
step:52/1750 train_time:4814ms step_avg:92.58ms
step:53/1750 train_time:4907ms step_avg:92.59ms
step:54/1750 train_time:5000ms step_avg:92.60ms
step:55/1750 train_time:5094ms step_avg:92.61ms
step:56/1750 train_time:5187ms step_avg:92.62ms
step:57/1750 train_time:5280ms step_avg:92.63ms
step:58/1750 train_time:5374ms step_avg:92.65ms
step:59/1750 train_time:5468ms step_avg:92.67ms
step:60/1750 train_time:5561ms step_avg:92.69ms
step:61/1750 train_time:5655ms step_avg:92.70ms
step:62/1750 train_time:5748ms step_avg:92.72ms
step:63/1750 train_time:5841ms step_avg:92.72ms
step:64/1750 train_time:5935ms step_avg:92.73ms
step:65/1750 train_time:6028ms step_avg:92.74ms
step:66/1750 train_time:6121ms step_avg:92.74ms
step:67/1750 train_time:6215ms step_avg:92.76ms
step:68/1750 train_time:6308ms step_avg:92.76ms
step:69/1750 train_time:6402ms step_avg:92.78ms
step:70/1750 train_time:6495ms step_avg:92.79ms
step:71/1750 train_time:6589ms step_avg:92.80ms
step:72/1750 train_time:6683ms step_avg:92.81ms
step:73/1750 train_time:6776ms step_avg:92.82ms
step:74/1750 train_time:6869ms step_avg:92.83ms
step:75/1750 train_time:6963ms step_avg:92.85ms
step:76/1750 train_time:7057ms step_avg:92.85ms
step:77/1750 train_time:7150ms step_avg:92.86ms
step:78/1750 train_time:7243ms step_avg:92.86ms
step:79/1750 train_time:7336ms step_avg:92.86ms
step:80/1750 train_time:7429ms step_avg:92.86ms
step:81/1750 train_time:7522ms step_avg:92.87ms
step:82/1750 train_time:7616ms step_avg:92.88ms
step:83/1750 train_time:7709ms step_avg:92.88ms
step:84/1750 train_time:7802ms step_avg:92.88ms
step:85/1750 train_time:7895ms step_avg:92.88ms
step:86/1750 train_time:7989ms step_avg:92.89ms
step:87/1750 train_time:8082ms step_avg:92.90ms
step:88/1750 train_time:8176ms step_avg:92.90ms
step:89/1750 train_time:8269ms step_avg:92.91ms
step:90/1750 train_time:8362ms step_avg:92.91ms
step:91/1750 train_time:8456ms step_avg:92.93ms
step:92/1750 train_time:8551ms step_avg:92.94ms
step:93/1750 train_time:8644ms step_avg:92.94ms
step:94/1750 train_time:8736ms step_avg:92.94ms
step:95/1750 train_time:8830ms step_avg:92.95ms
step:96/1750 train_time:8923ms step_avg:92.94ms
step:97/1750 train_time:9015ms step_avg:92.94ms
step:98/1750 train_time:9109ms step_avg:92.95ms
step:99/1750 train_time:9202ms step_avg:92.95ms
step:100/1750 train_time:9295ms step_avg:92.95ms
step:101/1750 train_time:9389ms step_avg:92.96ms
step:102/1750 train_time:9481ms step_avg:92.95ms
step:103/1750 train_time:9575ms step_avg:92.96ms
step:104/1750 train_time:9668ms step_avg:92.96ms
step:105/1750 train_time:9762ms step_avg:92.97ms
step:106/1750 train_time:9855ms step_avg:92.97ms
step:107/1750 train_time:9948ms step_avg:92.97ms
step:108/1750 train_time:10042ms step_avg:92.98ms
step:109/1750 train_time:10135ms step_avg:92.99ms
step:110/1750 train_time:10229ms step_avg:92.99ms
step:111/1750 train_time:10322ms step_avg:92.99ms
step:112/1750 train_time:10415ms step_avg:92.99ms
step:113/1750 train_time:10508ms step_avg:92.99ms
step:114/1750 train_time:10602ms step_avg:93.00ms
step:115/1750 train_time:10695ms step_avg:93.00ms
step:116/1750 train_time:10788ms step_avg:93.00ms
step:117/1750 train_time:10881ms step_avg:93.00ms
step:118/1750 train_time:10975ms step_avg:93.01ms
step:119/1750 train_time:11068ms step_avg:93.01ms
step:120/1750 train_time:11161ms step_avg:93.01ms
step:121/1750 train_time:11255ms step_avg:93.02ms
step:122/1750 train_time:11349ms step_avg:93.02ms
step:123/1750 train_time:11442ms step_avg:93.02ms
step:124/1750 train_time:11535ms step_avg:93.03ms
step:125/1750 train_time:11629ms step_avg:93.03ms
step:125/1750 val_loss:4.6316 train_time:11717ms step_avg:93.73ms
step:126/1750 train_time:11745ms step_avg:93.21ms
step:127/1750 train_time:11822ms step_avg:93.09ms
step:128/1750 train_time:11924ms step_avg:93.15ms
step:129/1750 train_time:12017ms step_avg:93.16ms
step:130/1750 train_time:12111ms step_avg:93.16ms
step:131/1750 train_time:12204ms step_avg:93.16ms
step:132/1750 train_time:12297ms step_avg:93.16ms
step:133/1750 train_time:12389ms step_avg:93.15ms
step:134/1750 train_time:12483ms step_avg:93.15ms
step:135/1750 train_time:12576ms step_avg:93.15ms
step:136/1750 train_time:12668ms step_avg:93.15ms
step:137/1750 train_time:12761ms step_avg:93.15ms
step:138/1750 train_time:12856ms step_avg:93.16ms
step:139/1750 train_time:12951ms step_avg:93.17ms
step:140/1750 train_time:13046ms step_avg:93.19ms
step:141/1750 train_time:13141ms step_avg:93.20ms
step:142/1750 train_time:13236ms step_avg:93.21ms
step:143/1750 train_time:13329ms step_avg:93.21ms
step:144/1750 train_time:13421ms step_avg:93.20ms
step:145/1750 train_time:13514ms step_avg:93.20ms
step:146/1750 train_time:13607ms step_avg:93.20ms
step:147/1750 train_time:13700ms step_avg:93.20ms
step:148/1750 train_time:13794ms step_avg:93.20ms
step:149/1750 train_time:13889ms step_avg:93.22ms
step:150/1750 train_time:13984ms step_avg:93.23ms
step:151/1750 train_time:14079ms step_avg:93.24ms
step:152/1750 train_time:14174ms step_avg:93.25ms
step:153/1750 train_time:14268ms step_avg:93.25ms
step:154/1750 train_time:14362ms step_avg:93.26ms
step:155/1750 train_time:14455ms step_avg:93.26ms
step:156/1750 train_time:14548ms step_avg:93.26ms
step:157/1750 train_time:14642ms step_avg:93.26ms
step:158/1750 train_time:14735ms step_avg:93.26ms
step:159/1750 train_time:14828ms step_avg:93.26ms
step:160/1750 train_time:14923ms step_avg:93.27ms
step:161/1750 train_time:15016ms step_avg:93.27ms
step:162/1750 train_time:15110ms step_avg:93.27ms
step:163/1750 train_time:15205ms step_avg:93.28ms
step:164/1750 train_time:15300ms step_avg:93.29ms
step:165/1750 train_time:15394ms step_avg:93.29ms
step:166/1750 train_time:15487ms step_avg:93.30ms
step:167/1750 train_time:15580ms step_avg:93.29ms
step:168/1750 train_time:15674ms step_avg:93.30ms
step:169/1750 train_time:15768ms step_avg:93.30ms
step:170/1750 train_time:15862ms step_avg:93.30ms
step:171/1750 train_time:15955ms step_avg:93.30ms
step:172/1750 train_time:16048ms step_avg:93.30ms
step:173/1750 train_time:16142ms step_avg:93.31ms
step:174/1750 train_time:16236ms step_avg:93.31ms
step:175/1750 train_time:16330ms step_avg:93.31ms
step:176/1750 train_time:16424ms step_avg:93.32ms
step:177/1750 train_time:16519ms step_avg:93.33ms
step:178/1750 train_time:16612ms step_avg:93.33ms
step:179/1750 train_time:16706ms step_avg:93.33ms
step:180/1750 train_time:16801ms step_avg:93.34ms
step:181/1750 train_time:16895ms step_avg:93.34ms
step:182/1750 train_time:16988ms step_avg:93.34ms
step:183/1750 train_time:17082ms step_avg:93.34ms
step:184/1750 train_time:17175ms step_avg:93.34ms
step:185/1750 train_time:17269ms step_avg:93.35ms
step:186/1750 train_time:17363ms step_avg:93.35ms
step:187/1750 train_time:17456ms step_avg:93.35ms
step:188/1750 train_time:17550ms step_avg:93.35ms
step:189/1750 train_time:17644ms step_avg:93.35ms
step:190/1750 train_time:17738ms step_avg:93.36ms
step:191/1750 train_time:17831ms step_avg:93.36ms
step:192/1750 train_time:17925ms step_avg:93.36ms
step:193/1750 train_time:18019ms step_avg:93.36ms
step:194/1750 train_time:18113ms step_avg:93.37ms
step:195/1750 train_time:18207ms step_avg:93.37ms
step:196/1750 train_time:18301ms step_avg:93.37ms
step:197/1750 train_time:18395ms step_avg:93.38ms
step:198/1750 train_time:18488ms step_avg:93.38ms
step:199/1750 train_time:18582ms step_avg:93.38ms
step:200/1750 train_time:18676ms step_avg:93.38ms
step:201/1750 train_time:18770ms step_avg:93.38ms
step:202/1750 train_time:18864ms step_avg:93.39ms
step:203/1750 train_time:18958ms step_avg:93.39ms
step:204/1750 train_time:19052ms step_avg:93.39ms
step:205/1750 train_time:19146ms step_avg:93.39ms
step:206/1750 train_time:19239ms step_avg:93.39ms
step:207/1750 train_time:19333ms step_avg:93.40ms
step:208/1750 train_time:19427ms step_avg:93.40ms
step:209/1750 train_time:19521ms step_avg:93.40ms
step:210/1750 train_time:19614ms step_avg:93.40ms
step:211/1750 train_time:19707ms step_avg:93.40ms
step:212/1750 train_time:19801ms step_avg:93.40ms
step:213/1750 train_time:19895ms step_avg:93.40ms
step:214/1750 train_time:19989ms step_avg:93.41ms
step:215/1750 train_time:20083ms step_avg:93.41ms
step:216/1750 train_time:20176ms step_avg:93.41ms
step:217/1750 train_time:20270ms step_avg:93.41ms
step:218/1750 train_time:20364ms step_avg:93.41ms
step:219/1750 train_time:20458ms step_avg:93.42ms
step:220/1750 train_time:20552ms step_avg:93.42ms
step:221/1750 train_time:20645ms step_avg:93.42ms
step:222/1750 train_time:20739ms step_avg:93.42ms
step:223/1750 train_time:20832ms step_avg:93.42ms
step:224/1750 train_time:20926ms step_avg:93.42ms
step:225/1750 train_time:21020ms step_avg:93.42ms
step:226/1750 train_time:21114ms step_avg:93.43ms
step:227/1750 train_time:21208ms step_avg:93.43ms
step:228/1750 train_time:21302ms step_avg:93.43ms
step:229/1750 train_time:21395ms step_avg:93.43ms
step:230/1750 train_time:21489ms step_avg:93.43ms
step:231/1750 train_time:21582ms step_avg:93.43ms
step:232/1750 train_time:21676ms step_avg:93.43ms
step:233/1750 train_time:21770ms step_avg:93.43ms
step:234/1750 train_time:21864ms step_avg:93.44ms
step:235/1750 train_time:21957ms step_avg:93.43ms
step:236/1750 train_time:22051ms step_avg:93.44ms
step:237/1750 train_time:22145ms step_avg:93.44ms
step:238/1750 train_time:22239ms step_avg:93.44ms
step:239/1750 train_time:22333ms step_avg:93.44ms
step:240/1750 train_time:22427ms step_avg:93.45ms
step:241/1750 train_time:22520ms step_avg:93.44ms
step:242/1750 train_time:22613ms step_avg:93.44ms
step:243/1750 train_time:22707ms step_avg:93.44ms
step:244/1750 train_time:22801ms step_avg:93.45ms
step:245/1750 train_time:22895ms step_avg:93.45ms
step:246/1750 train_time:22990ms step_avg:93.45ms
step:247/1750 train_time:23083ms step_avg:93.46ms
step:248/1750 train_time:23177ms step_avg:93.46ms
step:249/1750 train_time:23271ms step_avg:93.46ms
step:250/1750 train_time:23365ms step_avg:93.46ms
step:250/1750 val_loss:4.0840 train_time:23453ms step_avg:93.81ms
step:251/1750 train_time:23480ms step_avg:93.55ms
step:252/1750 train_time:23559ms step_avg:93.49ms
step:253/1750 train_time:23660ms step_avg:93.52ms
step:254/1750 train_time:23754ms step_avg:93.52ms
step:255/1750 train_time:23847ms step_avg:93.52ms
step:256/1750 train_time:23940ms step_avg:93.51ms
step:257/1750 train_time:24033ms step_avg:93.51ms
step:258/1750 train_time:24126ms step_avg:93.51ms
step:259/1750 train_time:24219ms step_avg:93.51ms
step:260/1750 train_time:24312ms step_avg:93.51ms
step:261/1750 train_time:24405ms step_avg:93.51ms
step:262/1750 train_time:24500ms step_avg:93.51ms
step:263/1750 train_time:24596ms step_avg:93.52ms
step:264/1750 train_time:24692ms step_avg:93.53ms
step:265/1750 train_time:24786ms step_avg:93.53ms
step:266/1750 train_time:24880ms step_avg:93.53ms
step:267/1750 train_time:24974ms step_avg:93.53ms
step:268/1750 train_time:25068ms step_avg:93.54ms
step:269/1750 train_time:25161ms step_avg:93.54ms
step:270/1750 train_time:25255ms step_avg:93.54ms
step:271/1750 train_time:25348ms step_avg:93.54ms
step:272/1750 train_time:25442ms step_avg:93.54ms
step:273/1750 train_time:25537ms step_avg:93.54ms
step:274/1750 train_time:25633ms step_avg:93.55ms
step:275/1750 train_time:25728ms step_avg:93.56ms
step:276/1750 train_time:25823ms step_avg:93.56ms
step:277/1750 train_time:25916ms step_avg:93.56ms
step:278/1750 train_time:26010ms step_avg:93.56ms
step:279/1750 train_time:26104ms step_avg:93.56ms
step:280/1750 train_time:26198ms step_avg:93.56ms
step:281/1750 train_time:26291ms step_avg:93.56ms
step:282/1750 train_time:26385ms step_avg:93.56ms
step:283/1750 train_time:26479ms step_avg:93.56ms
step:284/1750 train_time:26573ms step_avg:93.57ms
step:285/1750 train_time:26667ms step_avg:93.57ms
step:286/1750 train_time:26762ms step_avg:93.57ms
step:287/1750 train_time:26855ms step_avg:93.57ms
step:288/1750 train_time:26950ms step_avg:93.58ms
step:289/1750 train_time:27044ms step_avg:93.58ms
step:290/1750 train_time:27138ms step_avg:93.58ms
step:291/1750 train_time:27233ms step_avg:93.59ms
step:292/1750 train_time:27328ms step_avg:93.59ms
step:293/1750 train_time:27422ms step_avg:93.59ms
step:294/1750 train_time:27516ms step_avg:93.59ms
step:295/1750 train_time:27610ms step_avg:93.59ms
step:296/1750 train_time:27705ms step_avg:93.60ms
step:297/1750 train_time:27799ms step_avg:93.60ms
step:298/1750 train_time:27893ms step_avg:93.60ms
step:299/1750 train_time:27987ms step_avg:93.60ms
step:300/1750 train_time:28081ms step_avg:93.60ms
step:301/1750 train_time:28175ms step_avg:93.61ms
step:302/1750 train_time:28271ms step_avg:93.61ms
step:303/1750 train_time:28364ms step_avg:93.61ms
step:304/1750 train_time:28458ms step_avg:93.61ms
step:305/1750 train_time:28552ms step_avg:93.61ms
step:306/1750 train_time:28646ms step_avg:93.61ms
step:307/1750 train_time:28740ms step_avg:93.62ms
step:308/1750 train_time:28834ms step_avg:93.62ms
step:309/1750 train_time:28929ms step_avg:93.62ms
step:310/1750 train_time:29023ms step_avg:93.62ms
step:311/1750 train_time:29117ms step_avg:93.62ms
step:312/1750 train_time:29211ms step_avg:93.63ms
step:313/1750 train_time:29305ms step_avg:93.63ms
step:314/1750 train_time:29399ms step_avg:93.63ms
step:315/1750 train_time:29493ms step_avg:93.63ms
step:316/1750 train_time:29587ms step_avg:93.63ms
step:317/1750 train_time:29680ms step_avg:93.63ms
step:318/1750 train_time:29774ms step_avg:93.63ms
step:319/1750 train_time:29868ms step_avg:93.63ms
step:320/1750 train_time:29962ms step_avg:93.63ms
step:321/1750 train_time:30056ms step_avg:93.63ms
step:322/1750 train_time:30150ms step_avg:93.63ms
step:323/1750 train_time:30243ms step_avg:93.63ms
step:324/1750 train_time:30338ms step_avg:93.63ms
step:325/1750 train_time:30432ms step_avg:93.64ms
step:326/1750 train_time:30526ms step_avg:93.64ms
step:327/1750 train_time:30620ms step_avg:93.64ms
step:328/1750 train_time:30714ms step_avg:93.64ms
step:329/1750 train_time:30808ms step_avg:93.64ms
step:330/1750 train_time:30902ms step_avg:93.64ms
step:331/1750 train_time:30997ms step_avg:93.65ms
step:332/1750 train_time:31091ms step_avg:93.65ms
step:333/1750 train_time:31184ms step_avg:93.65ms
step:334/1750 train_time:31278ms step_avg:93.65ms
step:335/1750 train_time:31373ms step_avg:93.65ms
step:336/1750 train_time:31468ms step_avg:93.65ms
step:337/1750 train_time:31561ms step_avg:93.65ms
step:338/1750 train_time:31656ms step_avg:93.66ms
step:339/1750 train_time:31750ms step_avg:93.66ms
step:340/1750 train_time:31844ms step_avg:93.66ms
step:341/1750 train_time:31938ms step_avg:93.66ms
step:342/1750 train_time:32032ms step_avg:93.66ms
step:343/1750 train_time:32127ms step_avg:93.67ms
step:344/1750 train_time:32221ms step_avg:93.67ms
step:345/1750 train_time:32314ms step_avg:93.67ms
step:346/1750 train_time:32409ms step_avg:93.67ms
step:347/1750 train_time:32502ms step_avg:93.67ms
step:348/1750 train_time:32597ms step_avg:93.67ms
step:349/1750 train_time:32691ms step_avg:93.67ms
step:350/1750 train_time:32785ms step_avg:93.67ms
step:351/1750 train_time:32878ms step_avg:93.67ms
step:352/1750 train_time:32973ms step_avg:93.67ms
step:353/1750 train_time:33067ms step_avg:93.67ms
step:354/1750 train_time:33162ms step_avg:93.68ms
step:355/1750 train_time:33256ms step_avg:93.68ms
step:356/1750 train_time:33350ms step_avg:93.68ms
step:357/1750 train_time:33444ms step_avg:93.68ms
step:358/1750 train_time:33538ms step_avg:93.68ms
step:359/1750 train_time:33633ms step_avg:93.68ms
step:360/1750 train_time:33728ms step_avg:93.69ms
step:361/1750 train_time:33822ms step_avg:93.69ms
step:362/1750 train_time:33916ms step_avg:93.69ms
step:363/1750 train_time:34010ms step_avg:93.69ms
step:364/1750 train_time:34104ms step_avg:93.69ms
step:365/1750 train_time:34198ms step_avg:93.69ms
step:366/1750 train_time:34293ms step_avg:93.70ms
step:367/1750 train_time:34387ms step_avg:93.70ms
step:368/1750 train_time:34481ms step_avg:93.70ms
step:369/1750 train_time:34575ms step_avg:93.70ms
step:370/1750 train_time:34669ms step_avg:93.70ms
step:371/1750 train_time:34763ms step_avg:93.70ms
step:372/1750 train_time:34857ms step_avg:93.70ms
step:373/1750 train_time:34952ms step_avg:93.70ms
step:374/1750 train_time:35046ms step_avg:93.71ms
step:375/1750 train_time:35140ms step_avg:93.71ms
step:375/1750 val_loss:3.8838 train_time:35229ms step_avg:93.94ms
step:376/1750 train_time:35256ms step_avg:93.77ms
step:377/1750 train_time:35336ms step_avg:93.73ms
step:378/1750 train_time:35434ms step_avg:93.74ms
step:379/1750 train_time:35529ms step_avg:93.74ms
step:380/1750 train_time:35623ms step_avg:93.74ms
step:381/1750 train_time:35716ms step_avg:93.74ms
step:382/1750 train_time:35810ms step_avg:93.74ms
step:383/1750 train_time:35904ms step_avg:93.74ms
step:384/1750 train_time:35997ms step_avg:93.74ms
step:385/1750 train_time:36090ms step_avg:93.74ms
step:386/1750 train_time:36184ms step_avg:93.74ms
step:387/1750 train_time:36279ms step_avg:93.74ms
step:388/1750 train_time:36374ms step_avg:93.75ms
step:389/1750 train_time:36469ms step_avg:93.75ms
step:390/1750 train_time:36564ms step_avg:93.75ms
step:391/1750 train_time:36661ms step_avg:93.76ms
step:392/1750 train_time:36758ms step_avg:93.77ms
step:393/1750 train_time:36853ms step_avg:93.77ms
step:394/1750 train_time:36949ms step_avg:93.78ms
step:395/1750 train_time:37045ms step_avg:93.78ms
step:396/1750 train_time:37141ms step_avg:93.79ms
step:397/1750 train_time:37237ms step_avg:93.80ms
step:398/1750 train_time:37333ms step_avg:93.80ms
step:399/1750 train_time:37430ms step_avg:93.81ms
step:400/1750 train_time:37526ms step_avg:93.82ms
step:401/1750 train_time:37623ms step_avg:93.82ms
step:402/1750 train_time:37719ms step_avg:93.83ms
step:403/1750 train_time:37815ms step_avg:93.83ms
step:404/1750 train_time:37911ms step_avg:93.84ms
step:405/1750 train_time:38007ms step_avg:93.84ms
step:406/1750 train_time:38103ms step_avg:93.85ms
step:407/1750 train_time:38200ms step_avg:93.86ms
step:408/1750 train_time:38296ms step_avg:93.86ms
step:409/1750 train_time:38393ms step_avg:93.87ms
step:410/1750 train_time:38489ms step_avg:93.88ms
step:411/1750 train_time:38586ms step_avg:93.88ms
step:412/1750 train_time:38682ms step_avg:93.89ms
step:413/1750 train_time:38779ms step_avg:93.90ms
step:414/1750 train_time:38875ms step_avg:93.90ms
step:415/1750 train_time:38970ms step_avg:93.90ms
step:416/1750 train_time:39066ms step_avg:93.91ms
step:417/1750 train_time:39162ms step_avg:93.91ms
step:418/1750 train_time:39259ms step_avg:93.92ms
step:419/1750 train_time:39355ms step_avg:93.93ms
step:420/1750 train_time:39451ms step_avg:93.93ms
step:421/1750 train_time:39548ms step_avg:93.94ms
step:422/1750 train_time:39644ms step_avg:93.94ms
step:423/1750 train_time:39741ms step_avg:93.95ms
step:424/1750 train_time:39837ms step_avg:93.96ms
step:425/1750 train_time:39934ms step_avg:93.96ms
step:426/1750 train_time:40030ms step_avg:93.97ms
step:427/1750 train_time:40126ms step_avg:93.97ms
step:428/1750 train_time:40223ms step_avg:93.98ms
step:429/1750 train_time:40319ms step_avg:93.98ms
step:430/1750 train_time:40416ms step_avg:93.99ms
step:431/1750 train_time:40512ms step_avg:94.00ms
step:432/1750 train_time:40608ms step_avg:94.00ms
step:433/1750 train_time:40705ms step_avg:94.01ms
step:434/1750 train_time:40801ms step_avg:94.01ms
step:435/1750 train_time:40898ms step_avg:94.02ms
step:436/1750 train_time:40993ms step_avg:94.02ms
step:437/1750 train_time:41089ms step_avg:94.02ms
step:438/1750 train_time:41185ms step_avg:94.03ms
step:439/1750 train_time:41282ms step_avg:94.04ms
step:440/1750 train_time:41378ms step_avg:94.04ms
step:441/1750 train_time:41475ms step_avg:94.05ms
step:442/1750 train_time:41571ms step_avg:94.05ms
step:443/1750 train_time:41667ms step_avg:94.06ms
step:444/1750 train_time:41764ms step_avg:94.06ms
step:445/1750 train_time:41861ms step_avg:94.07ms
step:446/1750 train_time:41957ms step_avg:94.07ms
step:447/1750 train_time:42053ms step_avg:94.08ms
step:448/1750 train_time:42149ms step_avg:94.08ms
step:449/1750 train_time:42246ms step_avg:94.09ms
step:450/1750 train_time:42342ms step_avg:94.09ms
step:451/1750 train_time:42439ms step_avg:94.10ms
step:452/1750 train_time:42536ms step_avg:94.11ms
step:453/1750 train_time:42632ms step_avg:94.11ms
step:454/1750 train_time:42728ms step_avg:94.12ms
step:455/1750 train_time:42825ms step_avg:94.12ms
step:456/1750 train_time:42921ms step_avg:94.13ms
step:457/1750 train_time:43018ms step_avg:94.13ms
step:458/1750 train_time:43114ms step_avg:94.13ms
step:459/1750 train_time:43210ms step_avg:94.14ms
step:460/1750 train_time:43306ms step_avg:94.14ms
step:461/1750 train_time:43403ms step_avg:94.15ms
step:462/1750 train_time:43501ms step_avg:94.16ms
step:463/1750 train_time:43597ms step_avg:94.16ms
step:464/1750 train_time:43694ms step_avg:94.17ms
step:465/1750 train_time:43790ms step_avg:94.17ms
step:466/1750 train_time:43887ms step_avg:94.18ms
step:467/1750 train_time:43985ms step_avg:94.19ms
step:468/1750 train_time:44081ms step_avg:94.19ms
step:469/1750 train_time:44177ms step_avg:94.19ms
step:470/1750 train_time:44273ms step_avg:94.20ms
step:471/1750 train_time:44368ms step_avg:94.20ms
step:472/1750 train_time:44465ms step_avg:94.21ms
step:473/1750 train_time:44562ms step_avg:94.21ms
step:474/1750 train_time:44659ms step_avg:94.22ms
step:475/1750 train_time:44755ms step_avg:94.22ms
step:476/1750 train_time:44851ms step_avg:94.22ms
step:477/1750 train_time:44947ms step_avg:94.23ms
step:478/1750 train_time:45043ms step_avg:94.23ms
step:479/1750 train_time:45140ms step_avg:94.24ms
step:480/1750 train_time:45235ms step_avg:94.24ms
step:481/1750 train_time:45332ms step_avg:94.24ms
step:482/1750 train_time:45428ms step_avg:94.25ms
step:483/1750 train_time:45524ms step_avg:94.25ms
step:484/1750 train_time:45621ms step_avg:94.26ms
step:485/1750 train_time:45718ms step_avg:94.26ms
step:486/1750 train_time:45815ms step_avg:94.27ms
step:487/1750 train_time:45911ms step_avg:94.27ms
step:488/1750 train_time:46006ms step_avg:94.28ms
step:489/1750 train_time:46103ms step_avg:94.28ms
step:490/1750 train_time:46199ms step_avg:94.28ms
step:491/1750 train_time:46295ms step_avg:94.29ms
step:492/1750 train_time:46392ms step_avg:94.29ms
step:493/1750 train_time:46488ms step_avg:94.30ms
step:494/1750 train_time:46584ms step_avg:94.30ms
step:495/1750 train_time:46681ms step_avg:94.30ms
step:496/1750 train_time:46777ms step_avg:94.31ms
step:497/1750 train_time:46873ms step_avg:94.31ms
step:498/1750 train_time:46969ms step_avg:94.32ms
step:499/1750 train_time:47065ms step_avg:94.32ms
step:500/1750 train_time:47161ms step_avg:94.32ms
step:500/1750 val_loss:3.7433 train_time:47252ms step_avg:94.50ms
step:501/1750 train_time:47279ms step_avg:94.37ms
step:502/1750 train_time:47363ms step_avg:94.35ms
step:503/1750 train_time:47461ms step_avg:94.36ms
step:504/1750 train_time:47558ms step_avg:94.36ms
step:505/1750 train_time:47654ms step_avg:94.36ms
step:506/1750 train_time:47750ms step_avg:94.37ms
step:507/1750 train_time:47846ms step_avg:94.37ms
step:508/1750 train_time:47941ms step_avg:94.37ms
step:509/1750 train_time:48037ms step_avg:94.37ms
step:510/1750 train_time:48132ms step_avg:94.38ms
step:511/1750 train_time:48229ms step_avg:94.38ms
step:512/1750 train_time:48327ms step_avg:94.39ms
step:513/1750 train_time:48424ms step_avg:94.39ms
step:514/1750 train_time:48521ms step_avg:94.40ms
step:515/1750 train_time:48617ms step_avg:94.40ms
step:516/1750 train_time:48713ms step_avg:94.41ms
step:517/1750 train_time:48809ms step_avg:94.41ms
step:518/1750 train_time:48906ms step_avg:94.41ms
step:519/1750 train_time:49001ms step_avg:94.41ms
step:520/1750 train_time:49099ms step_avg:94.42ms
step:521/1750 train_time:49193ms step_avg:94.42ms
step:522/1750 train_time:49291ms step_avg:94.43ms
step:523/1750 train_time:49388ms step_avg:94.43ms
step:524/1750 train_time:49486ms step_avg:94.44ms
step:525/1750 train_time:49584ms step_avg:94.45ms
step:526/1750 train_time:49680ms step_avg:94.45ms
step:527/1750 train_time:49777ms step_avg:94.45ms
step:528/1750 train_time:49874ms step_avg:94.46ms
step:529/1750 train_time:49970ms step_avg:94.46ms
step:530/1750 train_time:50066ms step_avg:94.46ms
step:531/1750 train_time:50163ms step_avg:94.47ms
step:532/1750 train_time:50259ms step_avg:94.47ms
step:533/1750 train_time:50357ms step_avg:94.48ms
step:534/1750 train_time:50454ms step_avg:94.48ms
step:535/1750 train_time:50551ms step_avg:94.49ms
step:536/1750 train_time:50649ms step_avg:94.49ms
step:537/1750 train_time:50746ms step_avg:94.50ms
step:538/1750 train_time:50842ms step_avg:94.50ms
step:539/1750 train_time:50939ms step_avg:94.51ms
step:540/1750 train_time:51035ms step_avg:94.51ms
step:541/1750 train_time:51132ms step_avg:94.51ms
step:542/1750 train_time:51229ms step_avg:94.52ms
step:543/1750 train_time:51326ms step_avg:94.52ms
step:544/1750 train_time:51423ms step_avg:94.53ms
step:545/1750 train_time:51520ms step_avg:94.53ms
step:546/1750 train_time:51618ms step_avg:94.54ms
step:547/1750 train_time:51714ms step_avg:94.54ms
step:548/1750 train_time:51811ms step_avg:94.55ms
step:549/1750 train_time:51908ms step_avg:94.55ms
step:550/1750 train_time:52005ms step_avg:94.56ms
step:551/1750 train_time:52102ms step_avg:94.56ms
step:552/1750 train_time:52198ms step_avg:94.56ms
step:553/1750 train_time:52294ms step_avg:94.57ms
step:554/1750 train_time:52391ms step_avg:94.57ms
step:555/1750 train_time:52488ms step_avg:94.57ms
step:556/1750 train_time:52586ms step_avg:94.58ms
step:557/1750 train_time:52683ms step_avg:94.58ms
step:558/1750 train_time:52780ms step_avg:94.59ms
step:559/1750 train_time:52876ms step_avg:94.59ms
step:560/1750 train_time:52973ms step_avg:94.60ms
step:561/1750 train_time:53071ms step_avg:94.60ms
step:562/1750 train_time:53167ms step_avg:94.60ms
step:563/1750 train_time:53263ms step_avg:94.61ms
step:564/1750 train_time:53360ms step_avg:94.61ms
step:565/1750 train_time:53457ms step_avg:94.61ms
step:566/1750 train_time:53554ms step_avg:94.62ms
step:567/1750 train_time:53651ms step_avg:94.62ms
step:568/1750 train_time:53747ms step_avg:94.63ms
step:569/1750 train_time:53845ms step_avg:94.63ms
step:570/1750 train_time:53941ms step_avg:94.63ms
step:571/1750 train_time:54039ms step_avg:94.64ms
step:572/1750 train_time:54135ms step_avg:94.64ms
step:573/1750 train_time:54232ms step_avg:94.65ms
step:574/1750 train_time:54329ms step_avg:94.65ms
step:575/1750 train_time:54426ms step_avg:94.65ms
step:576/1750 train_time:54522ms step_avg:94.66ms
step:577/1750 train_time:54618ms step_avg:94.66ms
step:578/1750 train_time:54715ms step_avg:94.66ms
step:579/1750 train_time:54811ms step_avg:94.67ms
step:580/1750 train_time:54908ms step_avg:94.67ms
step:581/1750 train_time:55006ms step_avg:94.67ms
step:582/1750 train_time:55103ms step_avg:94.68ms
step:583/1750 train_time:55200ms step_avg:94.68ms
step:584/1750 train_time:55296ms step_avg:94.68ms
step:585/1750 train_time:55392ms step_avg:94.69ms
step:586/1750 train_time:55489ms step_avg:94.69ms
step:587/1750 train_time:55586ms step_avg:94.70ms
step:588/1750 train_time:55683ms step_avg:94.70ms
step:589/1750 train_time:55780ms step_avg:94.70ms
step:590/1750 train_time:55877ms step_avg:94.71ms
step:591/1750 train_time:55975ms step_avg:94.71ms
step:592/1750 train_time:56073ms step_avg:94.72ms
step:593/1750 train_time:56171ms step_avg:94.72ms
step:594/1750 train_time:56267ms step_avg:94.73ms
step:595/1750 train_time:56363ms step_avg:94.73ms
step:596/1750 train_time:56460ms step_avg:94.73ms
step:597/1750 train_time:56557ms step_avg:94.73ms
step:598/1750 train_time:56655ms step_avg:94.74ms
step:599/1750 train_time:56750ms step_avg:94.74ms
step:600/1750 train_time:56847ms step_avg:94.75ms
step:601/1750 train_time:56944ms step_avg:94.75ms
step:602/1750 train_time:57041ms step_avg:94.75ms
step:603/1750 train_time:57138ms step_avg:94.76ms
step:604/1750 train_time:57235ms step_avg:94.76ms
step:605/1750 train_time:57331ms step_avg:94.76ms
step:606/1750 train_time:57428ms step_avg:94.77ms
step:607/1750 train_time:57525ms step_avg:94.77ms
step:608/1750 train_time:57621ms step_avg:94.77ms
step:609/1750 train_time:57718ms step_avg:94.78ms
step:610/1750 train_time:57815ms step_avg:94.78ms
step:611/1750 train_time:57912ms step_avg:94.78ms
step:612/1750 train_time:58009ms step_avg:94.79ms
step:613/1750 train_time:58107ms step_avg:94.79ms
step:614/1750 train_time:58203ms step_avg:94.79ms
step:615/1750 train_time:58300ms step_avg:94.80ms
step:616/1750 train_time:58397ms step_avg:94.80ms
step:617/1750 train_time:58494ms step_avg:94.80ms
step:618/1750 train_time:58591ms step_avg:94.81ms
step:619/1750 train_time:58688ms step_avg:94.81ms
step:620/1750 train_time:58785ms step_avg:94.81ms
step:621/1750 train_time:58882ms step_avg:94.82ms
step:622/1750 train_time:58979ms step_avg:94.82ms
step:623/1750 train_time:59075ms step_avg:94.82ms
step:624/1750 train_time:59173ms step_avg:94.83ms
step:625/1750 train_time:59270ms step_avg:94.83ms
step:625/1750 val_loss:3.6551 train_time:59361ms step_avg:94.98ms
step:626/1750 train_time:59390ms step_avg:94.87ms
step:627/1750 train_time:59473ms step_avg:94.85ms
step:628/1750 train_time:59573ms step_avg:94.86ms
step:629/1750 train_time:59670ms step_avg:94.86ms
step:630/1750 train_time:59766ms step_avg:94.87ms
step:631/1750 train_time:59862ms step_avg:94.87ms
step:632/1750 train_time:59958ms step_avg:94.87ms
step:633/1750 train_time:60054ms step_avg:94.87ms
step:634/1750 train_time:60150ms step_avg:94.87ms
step:635/1750 train_time:60246ms step_avg:94.88ms
step:636/1750 train_time:60343ms step_avg:94.88ms
step:637/1750 train_time:60441ms step_avg:94.88ms
step:638/1750 train_time:60539ms step_avg:94.89ms
step:639/1750 train_time:60637ms step_avg:94.89ms
step:640/1750 train_time:60733ms step_avg:94.90ms
step:641/1750 train_time:60829ms step_avg:94.90ms
step:642/1750 train_time:60926ms step_avg:94.90ms
step:643/1750 train_time:61022ms step_avg:94.90ms
step:644/1750 train_time:61118ms step_avg:94.90ms
step:645/1750 train_time:61214ms step_avg:94.91ms
step:646/1750 train_time:61311ms step_avg:94.91ms
step:647/1750 train_time:61409ms step_avg:94.91ms
step:648/1750 train_time:61506ms step_avg:94.92ms
step:649/1750 train_time:61603ms step_avg:94.92ms
step:650/1750 train_time:61701ms step_avg:94.92ms
step:651/1750 train_time:61799ms step_avg:94.93ms
step:652/1750 train_time:61898ms step_avg:94.94ms
step:653/1750 train_time:61997ms step_avg:94.94ms
step:654/1750 train_time:62095ms step_avg:94.95ms
step:655/1750 train_time:62194ms step_avg:94.95ms
step:656/1750 train_time:62293ms step_avg:94.96ms
step:657/1750 train_time:62391ms step_avg:94.96ms
step:658/1750 train_time:62489ms step_avg:94.97ms
step:659/1750 train_time:62588ms step_avg:94.97ms
step:660/1750 train_time:62686ms step_avg:94.98ms
step:661/1750 train_time:62785ms step_avg:94.98ms
step:662/1750 train_time:62882ms step_avg:94.99ms
step:663/1750 train_time:62981ms step_avg:94.99ms
step:664/1750 train_time:63079ms step_avg:95.00ms
step:665/1750 train_time:63178ms step_avg:95.00ms
step:666/1750 train_time:63277ms step_avg:95.01ms
step:667/1750 train_time:63376ms step_avg:95.02ms
step:668/1750 train_time:63475ms step_avg:95.02ms
step:669/1750 train_time:63573ms step_avg:95.03ms
step:670/1750 train_time:63672ms step_avg:95.03ms
step:671/1750 train_time:63770ms step_avg:95.04ms
step:672/1750 train_time:63870ms step_avg:95.04ms
step:673/1750 train_time:63968ms step_avg:95.05ms
step:674/1750 train_time:64067ms step_avg:95.06ms
step:675/1750 train_time:64166ms step_avg:95.06ms
step:676/1750 train_time:64264ms step_avg:95.07ms
step:677/1750 train_time:64363ms step_avg:95.07ms
step:678/1750 train_time:64462ms step_avg:95.08ms
step:679/1750 train_time:64560ms step_avg:95.08ms
step:680/1750 train_time:64659ms step_avg:95.09ms
step:681/1750 train_time:64758ms step_avg:95.09ms
step:682/1750 train_time:64857ms step_avg:95.10ms
step:683/1750 train_time:64955ms step_avg:95.10ms
step:684/1750 train_time:65054ms step_avg:95.11ms
step:685/1750 train_time:65153ms step_avg:95.11ms
step:686/1750 train_time:65252ms step_avg:95.12ms
step:687/1750 train_time:65350ms step_avg:95.12ms
step:688/1750 train_time:65448ms step_avg:95.13ms
step:689/1750 train_time:65547ms step_avg:95.13ms
step:690/1750 train_time:65646ms step_avg:95.14ms
step:691/1750 train_time:65745ms step_avg:95.14ms
step:692/1750 train_time:65844ms step_avg:95.15ms
step:693/1750 train_time:65942ms step_avg:95.15ms
step:694/1750 train_time:66041ms step_avg:95.16ms
step:695/1750 train_time:66139ms step_avg:95.16ms
step:696/1750 train_time:66238ms step_avg:95.17ms
step:697/1750 train_time:66336ms step_avg:95.17ms
step:698/1750 train_time:66436ms step_avg:95.18ms
step:699/1750 train_time:66536ms step_avg:95.19ms
step:700/1750 train_time:66635ms step_avg:95.19ms
step:701/1750 train_time:66734ms step_avg:95.20ms
step:702/1750 train_time:66833ms step_avg:95.20ms
step:703/1750 train_time:66932ms step_avg:95.21ms
step:704/1750 train_time:67031ms step_avg:95.21ms
step:705/1750 train_time:67129ms step_avg:95.22ms
step:706/1750 train_time:67227ms step_avg:95.22ms
step:707/1750 train_time:67325ms step_avg:95.23ms
step:708/1750 train_time:67424ms step_avg:95.23ms
step:709/1750 train_time:67523ms step_avg:95.24ms
step:710/1750 train_time:67622ms step_avg:95.24ms
step:711/1750 train_time:67722ms step_avg:95.25ms
step:712/1750 train_time:67821ms step_avg:95.25ms
step:713/1750 train_time:67920ms step_avg:95.26ms
step:714/1750 train_time:68019ms step_avg:95.27ms
step:715/1750 train_time:68118ms step_avg:95.27ms
step:716/1750 train_time:68217ms step_avg:95.28ms
step:717/1750 train_time:68317ms step_avg:95.28ms
step:718/1750 train_time:68415ms step_avg:95.29ms
step:719/1750 train_time:68514ms step_avg:95.29ms
step:720/1750 train_time:68613ms step_avg:95.30ms
step:721/1750 train_time:68712ms step_avg:95.30ms
step:722/1750 train_time:68812ms step_avg:95.31ms
step:723/1750 train_time:68910ms step_avg:95.31ms
step:724/1750 train_time:69009ms step_avg:95.32ms
step:725/1750 train_time:69107ms step_avg:95.32ms
step:726/1750 train_time:69206ms step_avg:95.33ms
step:727/1750 train_time:69305ms step_avg:95.33ms
step:728/1750 train_time:69404ms step_avg:95.33ms
step:729/1750 train_time:69502ms step_avg:95.34ms
step:730/1750 train_time:69600ms step_avg:95.34ms
step:731/1750 train_time:69699ms step_avg:95.35ms
step:732/1750 train_time:69798ms step_avg:95.35ms
step:733/1750 train_time:69897ms step_avg:95.36ms
step:734/1750 train_time:69995ms step_avg:95.36ms
step:735/1750 train_time:70094ms step_avg:95.37ms
step:736/1750 train_time:70192ms step_avg:95.37ms
step:737/1750 train_time:70291ms step_avg:95.37ms
step:738/1750 train_time:70390ms step_avg:95.38ms
step:739/1750 train_time:70489ms step_avg:95.38ms
step:740/1750 train_time:70587ms step_avg:95.39ms
step:741/1750 train_time:70685ms step_avg:95.39ms
step:742/1750 train_time:70785ms step_avg:95.40ms
step:743/1750 train_time:70884ms step_avg:95.40ms
step:744/1750 train_time:70982ms step_avg:95.41ms
step:745/1750 train_time:71081ms step_avg:95.41ms
step:746/1750 train_time:71181ms step_avg:95.42ms
step:747/1750 train_time:71279ms step_avg:95.42ms
step:748/1750 train_time:71378ms step_avg:95.43ms
step:749/1750 train_time:71477ms step_avg:95.43ms
step:750/1750 train_time:71575ms step_avg:95.43ms
step:750/1750 val_loss:3.5929 train_time:71668ms step_avg:95.56ms
step:751/1750 train_time:71695ms step_avg:95.47ms
step:752/1750 train_time:71780ms step_avg:95.45ms
step:753/1750 train_time:71880ms step_avg:95.46ms
step:754/1750 train_time:71979ms step_avg:95.46ms
step:755/1750 train_time:72077ms step_avg:95.47ms
step:756/1750 train_time:72175ms step_avg:95.47ms
step:757/1750 train_time:72273ms step_avg:95.47ms
step:758/1750 train_time:72371ms step_avg:95.48ms
step:759/1750 train_time:72470ms step_avg:95.48ms
step:760/1750 train_time:72568ms step_avg:95.48ms
step:761/1750 train_time:72666ms step_avg:95.49ms
step:762/1750 train_time:72767ms step_avg:95.49ms
step:763/1750 train_time:72868ms step_avg:95.50ms
step:764/1750 train_time:72968ms step_avg:95.51ms
step:765/1750 train_time:73067ms step_avg:95.51ms
step:766/1750 train_time:73165ms step_avg:95.52ms
step:767/1750 train_time:73263ms step_avg:95.52ms
step:768/1750 train_time:73361ms step_avg:95.52ms
step:769/1750 train_time:73460ms step_avg:95.53ms
step:770/1750 train_time:73558ms step_avg:95.53ms
step:771/1750 train_time:73656ms step_avg:95.53ms
step:772/1750 train_time:73754ms step_avg:95.54ms
step:773/1750 train_time:73854ms step_avg:95.54ms
step:774/1750 train_time:73954ms step_avg:95.55ms
step:775/1750 train_time:74053ms step_avg:95.55ms
step:776/1750 train_time:74153ms step_avg:95.56ms
step:777/1750 train_time:74253ms step_avg:95.56ms
step:778/1750 train_time:74353ms step_avg:95.57ms
step:779/1750 train_time:74451ms step_avg:95.57ms
step:780/1750 train_time:74550ms step_avg:95.58ms
step:781/1750 train_time:74650ms step_avg:95.58ms
step:782/1750 train_time:74749ms step_avg:95.59ms
step:783/1750 train_time:74848ms step_avg:95.59ms
step:784/1750 train_time:74948ms step_avg:95.60ms
step:785/1750 train_time:75048ms step_avg:95.60ms
step:786/1750 train_time:75148ms step_avg:95.61ms
step:787/1750 train_time:75248ms step_avg:95.61ms
step:788/1750 train_time:75348ms step_avg:95.62ms
step:789/1750 train_time:75447ms step_avg:95.62ms
step:790/1750 train_time:75546ms step_avg:95.63ms
step:791/1750 train_time:75645ms step_avg:95.63ms
step:792/1750 train_time:75743ms step_avg:95.64ms
step:793/1750 train_time:75842ms step_avg:95.64ms
step:794/1750 train_time:75941ms step_avg:95.64ms
step:795/1750 train_time:76040ms step_avg:95.65ms
step:796/1750 train_time:76139ms step_avg:95.65ms
step:797/1750 train_time:76238ms step_avg:95.66ms
step:798/1750 train_time:76337ms step_avg:95.66ms
step:799/1750 train_time:76436ms step_avg:95.66ms
step:800/1750 train_time:76537ms step_avg:95.67ms
step:801/1750 train_time:76636ms step_avg:95.68ms
step:802/1750 train_time:76736ms step_avg:95.68ms
step:803/1750 train_time:76835ms step_avg:95.69ms
step:804/1750 train_time:76934ms step_avg:95.69ms
step:805/1750 train_time:77033ms step_avg:95.69ms
step:806/1750 train_time:77132ms step_avg:95.70ms
step:807/1750 train_time:77231ms step_avg:95.70ms
step:808/1750 train_time:77331ms step_avg:95.71ms
step:809/1750 train_time:77431ms step_avg:95.71ms
step:810/1750 train_time:77530ms step_avg:95.72ms
step:811/1750 train_time:77630ms step_avg:95.72ms
step:812/1750 train_time:77730ms step_avg:95.73ms
step:813/1750 train_time:77829ms step_avg:95.73ms
step:814/1750 train_time:77929ms step_avg:95.74ms
step:815/1750 train_time:78028ms step_avg:95.74ms
step:816/1750 train_time:78126ms step_avg:95.74ms
step:817/1750 train_time:78226ms step_avg:95.75ms
step:818/1750 train_time:78325ms step_avg:95.75ms
step:819/1750 train_time:78424ms step_avg:95.76ms
step:820/1750 train_time:78524ms step_avg:95.76ms
step:821/1750 train_time:78623ms step_avg:95.76ms
step:822/1750 train_time:78722ms step_avg:95.77ms
step:823/1750 train_time:78821ms step_avg:95.77ms
step:824/1750 train_time:78920ms step_avg:95.78ms
step:825/1750 train_time:79019ms step_avg:95.78ms
step:826/1750 train_time:79118ms step_avg:95.78ms
step:827/1750 train_time:79218ms step_avg:95.79ms
step:828/1750 train_time:79317ms step_avg:95.79ms
step:829/1750 train_time:79416ms step_avg:95.80ms
step:830/1750 train_time:79515ms step_avg:95.80ms
step:831/1750 train_time:79614ms step_avg:95.80ms
step:832/1750 train_time:79714ms step_avg:95.81ms
step:833/1750 train_time:79813ms step_avg:95.81ms
step:834/1750 train_time:79912ms step_avg:95.82ms
step:835/1750 train_time:80011ms step_avg:95.82ms
step:836/1750 train_time:80111ms step_avg:95.83ms
step:837/1750 train_time:80210ms step_avg:95.83ms
step:838/1750 train_time:80309ms step_avg:95.83ms
step:839/1750 train_time:80408ms step_avg:95.84ms
step:840/1750 train_time:80508ms step_avg:95.84ms
step:841/1750 train_time:80607ms step_avg:95.85ms
step:842/1750 train_time:80707ms step_avg:95.85ms
step:843/1750 train_time:80806ms step_avg:95.86ms
step:844/1750 train_time:80906ms step_avg:95.86ms
step:845/1750 train_time:81005ms step_avg:95.86ms
step:846/1750 train_time:81104ms step_avg:95.87ms
step:847/1750 train_time:81202ms step_avg:95.87ms
step:848/1750 train_time:81301ms step_avg:95.87ms
step:849/1750 train_time:81400ms step_avg:95.88ms
step:850/1750 train_time:81499ms step_avg:95.88ms
step:851/1750 train_time:81598ms step_avg:95.88ms
step:852/1750 train_time:81697ms step_avg:95.89ms
step:853/1750 train_time:81796ms step_avg:95.89ms
step:854/1750 train_time:81895ms step_avg:95.90ms
step:855/1750 train_time:81994ms step_avg:95.90ms
step:856/1750 train_time:82094ms step_avg:95.90ms
step:857/1750 train_time:82194ms step_avg:95.91ms
step:858/1750 train_time:82294ms step_avg:95.91ms
step:859/1750 train_time:82394ms step_avg:95.92ms
step:860/1750 train_time:82493ms step_avg:95.92ms
step:861/1750 train_time:82593ms step_avg:95.93ms
step:862/1750 train_time:82692ms step_avg:95.93ms
step:863/1750 train_time:82791ms step_avg:95.93ms
step:864/1750 train_time:82890ms step_avg:95.94ms
step:865/1750 train_time:82989ms step_avg:95.94ms
step:866/1750 train_time:83088ms step_avg:95.94ms
step:867/1750 train_time:83187ms step_avg:95.95ms
step:868/1750 train_time:83286ms step_avg:95.95ms
step:869/1750 train_time:83385ms step_avg:95.96ms
step:870/1750 train_time:83483ms step_avg:95.96ms
step:871/1750 train_time:83582ms step_avg:95.96ms
step:872/1750 train_time:83681ms step_avg:95.96ms
step:873/1750 train_time:83780ms step_avg:95.97ms
step:874/1750 train_time:83879ms step_avg:95.97ms
step:875/1750 train_time:83977ms step_avg:95.97ms
step:875/1750 val_loss:3.5452 train_time:84071ms step_avg:96.08ms
step:876/1750 train_time:84098ms step_avg:96.00ms
step:877/1750 train_time:84183ms step_avg:95.99ms
step:878/1750 train_time:84285ms step_avg:96.00ms
step:879/1750 train_time:84384ms step_avg:96.00ms
step:880/1750 train_time:84482ms step_avg:96.00ms
step:881/1750 train_time:84580ms step_avg:96.00ms
step:882/1750 train_time:84679ms step_avg:96.01ms
step:883/1750 train_time:84777ms step_avg:96.01ms
step:884/1750 train_time:84875ms step_avg:96.01ms
step:885/1750 train_time:84972ms step_avg:96.01ms
step:886/1750 train_time:85071ms step_avg:96.02ms
step:887/1750 train_time:85171ms step_avg:96.02ms
step:888/1750 train_time:85271ms step_avg:96.03ms
step:889/1750 train_time:85370ms step_avg:96.03ms
step:890/1750 train_time:85469ms step_avg:96.03ms
step:891/1750 train_time:85567ms step_avg:96.04ms
step:892/1750 train_time:85666ms step_avg:96.04ms
step:893/1750 train_time:85766ms step_avg:96.04ms
step:894/1750 train_time:85865ms step_avg:96.05ms
step:895/1750 train_time:85963ms step_avg:96.05ms
step:896/1750 train_time:86063ms step_avg:96.05ms
step:897/1750 train_time:86163ms step_avg:96.06ms
step:898/1750 train_time:86263ms step_avg:96.06ms
step:899/1750 train_time:86363ms step_avg:96.07ms
step:900/1750 train_time:86462ms step_avg:96.07ms
step:901/1750 train_time:86562ms step_avg:96.07ms
step:902/1750 train_time:86661ms step_avg:96.08ms
step:903/1750 train_time:86760ms step_avg:96.08ms
step:904/1750 train_time:86858ms step_avg:96.08ms
step:905/1750 train_time:86956ms step_avg:96.08ms
step:906/1750 train_time:87054ms step_avg:96.09ms
step:907/1750 train_time:87152ms step_avg:96.09ms
step:908/1750 train_time:87251ms step_avg:96.09ms
step:909/1750 train_time:87350ms step_avg:96.09ms
step:910/1750 train_time:87451ms step_avg:96.10ms
step:911/1750 train_time:87551ms step_avg:96.10ms
step:912/1750 train_time:87652ms step_avg:96.11ms
step:913/1750 train_time:87753ms step_avg:96.12ms
step:914/1750 train_time:87853ms step_avg:96.12ms
step:915/1750 train_time:87953ms step_avg:96.12ms
step:916/1750 train_time:88053ms step_avg:96.13ms
step:917/1750 train_time:88152ms step_avg:96.13ms
step:918/1750 train_time:88252ms step_avg:96.14ms
step:919/1750 train_time:88353ms step_avg:96.14ms
step:920/1750 train_time:88454ms step_avg:96.15ms
step:921/1750 train_time:88555ms step_avg:96.15ms
step:922/1750 train_time:88656ms step_avg:96.16ms
step:923/1750 train_time:88756ms step_avg:96.16ms
step:924/1750 train_time:88856ms step_avg:96.16ms
step:925/1750 train_time:88956ms step_avg:96.17ms
step:926/1750 train_time:89056ms step_avg:96.17ms
step:927/1750 train_time:89156ms step_avg:96.18ms
step:928/1750 train_time:89256ms step_avg:96.18ms
step:929/1750 train_time:89356ms step_avg:96.19ms
step:930/1750 train_time:89457ms step_avg:96.19ms
step:931/1750 train_time:89558ms step_avg:96.20ms
step:932/1750 train_time:89659ms step_avg:96.20ms
step:933/1750 train_time:89760ms step_avg:96.21ms
step:934/1750 train_time:89860ms step_avg:96.21ms
step:935/1750 train_time:89960ms step_avg:96.21ms
step:936/1750 train_time:90060ms step_avg:96.22ms
step:937/1750 train_time:90160ms step_avg:96.22ms
step:938/1750 train_time:90259ms step_avg:96.23ms
step:939/1750 train_time:90360ms step_avg:96.23ms
step:940/1750 train_time:90459ms step_avg:96.23ms
step:941/1750 train_time:90561ms step_avg:96.24ms
step:942/1750 train_time:90662ms step_avg:96.24ms
step:943/1750 train_time:90763ms step_avg:96.25ms
step:944/1750 train_time:90863ms step_avg:96.25ms
step:945/1750 train_time:90964ms step_avg:96.26ms
step:946/1750 train_time:91064ms step_avg:96.26ms
step:947/1750 train_time:91164ms step_avg:96.27ms
step:948/1750 train_time:91264ms step_avg:96.27ms
step:949/1750 train_time:91365ms step_avg:96.27ms
step:950/1750 train_time:91465ms step_avg:96.28ms
step:951/1750 train_time:91566ms step_avg:96.28ms
step:952/1750 train_time:91667ms step_avg:96.29ms
step:953/1750 train_time:91767ms step_avg:96.29ms
step:954/1750 train_time:91868ms step_avg:96.30ms
step:955/1750 train_time:91968ms step_avg:96.30ms
step:956/1750 train_time:92069ms step_avg:96.31ms
step:957/1750 train_time:92170ms step_avg:96.31ms
step:958/1750 train_time:92270ms step_avg:96.32ms
step:959/1750 train_time:92371ms step_avg:96.32ms
step:960/1750 train_time:92472ms step_avg:96.32ms
step:961/1750 train_time:92572ms step_avg:96.33ms
step:962/1750 train_time:92672ms step_avg:96.33ms
step:963/1750 train_time:92773ms step_avg:96.34ms
step:964/1750 train_time:92873ms step_avg:96.34ms
step:965/1750 train_time:92973ms step_avg:96.35ms
step:966/1750 train_time:93074ms step_avg:96.35ms
step:967/1750 train_time:93174ms step_avg:96.35ms
step:968/1750 train_time:93274ms step_avg:96.36ms
step:969/1750 train_time:93374ms step_avg:96.36ms
step:970/1750 train_time:93474ms step_avg:96.37ms
step:971/1750 train_time:93575ms step_avg:96.37ms
step:972/1750 train_time:93675ms step_avg:96.37ms
step:973/1750 train_time:93775ms step_avg:96.38ms
step:974/1750 train_time:93874ms step_avg:96.38ms
step:975/1750 train_time:93975ms step_avg:96.38ms
step:976/1750 train_time:94074ms step_avg:96.39ms
step:977/1750 train_time:94175ms step_avg:96.39ms
step:978/1750 train_time:94276ms step_avg:96.40ms
step:979/1750 train_time:94375ms step_avg:96.40ms
step:980/1750 train_time:94476ms step_avg:96.40ms
step:981/1750 train_time:94577ms step_avg:96.41ms
step:982/1750 train_time:94677ms step_avg:96.41ms
step:983/1750 train_time:94777ms step_avg:96.42ms
step:984/1750 train_time:94877ms step_avg:96.42ms
step:985/1750 train_time:94977ms step_avg:96.42ms
step:986/1750 train_time:95078ms step_avg:96.43ms
step:987/1750 train_time:95179ms step_avg:96.43ms
step:988/1750 train_time:95280ms step_avg:96.44ms
step:989/1750 train_time:95381ms step_avg:96.44ms
step:990/1750 train_time:95483ms step_avg:96.45ms
step:991/1750 train_time:95584ms step_avg:96.45ms
step:992/1750 train_time:95684ms step_avg:96.46ms
step:993/1750 train_time:95784ms step_avg:96.46ms
step:994/1750 train_time:95884ms step_avg:96.46ms
step:995/1750 train_time:95984ms step_avg:96.47ms
step:996/1750 train_time:96085ms step_avg:96.47ms
step:997/1750 train_time:96186ms step_avg:96.48ms
step:998/1750 train_time:96288ms step_avg:96.48ms
step:999/1750 train_time:96388ms step_avg:96.48ms
step:1000/1750 train_time:96489ms step_avg:96.49ms
step:1000/1750 val_loss:3.5037 train_time:96584ms step_avg:96.58ms
step:1001/1750 train_time:96612ms step_avg:96.52ms
step:1002/1750 train_time:96699ms step_avg:96.51ms
step:1003/1750 train_time:96802ms step_avg:96.51ms
step:1004/1750 train_time:96902ms step_avg:96.52ms
step:1005/1750 train_time:97002ms step_avg:96.52ms
step:1006/1750 train_time:97102ms step_avg:96.52ms
step:1007/1750 train_time:97202ms step_avg:96.53ms
step:1008/1750 train_time:97301ms step_avg:96.53ms
step:1009/1750 train_time:97401ms step_avg:96.53ms
step:1010/1750 train_time:97502ms step_avg:96.54ms
step:1011/1750 train_time:97604ms step_avg:96.54ms
step:1012/1750 train_time:97706ms step_avg:96.55ms
step:1013/1750 train_time:97807ms step_avg:96.55ms
step:1014/1750 train_time:97908ms step_avg:96.56ms
step:1015/1750 train_time:98008ms step_avg:96.56ms
step:1016/1750 train_time:98108ms step_avg:96.56ms
step:1017/1750 train_time:98207ms step_avg:96.57ms
step:1018/1750 train_time:98307ms step_avg:96.57ms
step:1019/1750 train_time:98407ms step_avg:96.57ms
step:1020/1750 train_time:98508ms step_avg:96.58ms
step:1021/1750 train_time:98608ms step_avg:96.58ms
step:1022/1750 train_time:98709ms step_avg:96.58ms
step:1023/1750 train_time:98810ms step_avg:96.59ms
step:1024/1750 train_time:98912ms step_avg:96.59ms
step:1025/1750 train_time:99012ms step_avg:96.60ms
step:1026/1750 train_time:99112ms step_avg:96.60ms
step:1027/1750 train_time:99212ms step_avg:96.60ms
step:1028/1750 train_time:99312ms step_avg:96.61ms
step:1029/1750 train_time:99413ms step_avg:96.61ms
step:1030/1750 train_time:99515ms step_avg:96.62ms
step:1031/1750 train_time:99616ms step_avg:96.62ms
step:1032/1750 train_time:99717ms step_avg:96.62ms
step:1033/1750 train_time:99817ms step_avg:96.63ms
step:1034/1750 train_time:99917ms step_avg:96.63ms
step:1035/1750 train_time:100018ms step_avg:96.64ms
step:1036/1750 train_time:100118ms step_avg:96.64ms
step:1037/1750 train_time:100219ms step_avg:96.64ms
step:1038/1750 train_time:100320ms step_avg:96.65ms
step:1039/1750 train_time:100421ms step_avg:96.65ms
step:1040/1750 train_time:100521ms step_avg:96.66ms
step:1041/1750 train_time:100622ms step_avg:96.66ms
step:1042/1750 train_time:100724ms step_avg:96.66ms
step:1043/1750 train_time:100825ms step_avg:96.67ms
step:1044/1750 train_time:100926ms step_avg:96.67ms
step:1045/1750 train_time:101027ms step_avg:96.68ms
step:1046/1750 train_time:101127ms step_avg:96.68ms
step:1047/1750 train_time:101229ms step_avg:96.68ms
step:1048/1750 train_time:101329ms step_avg:96.69ms
step:1049/1750 train_time:101430ms step_avg:96.69ms
step:1050/1750 train_time:101530ms step_avg:96.70ms
step:1051/1750 train_time:101631ms step_avg:96.70ms
step:1052/1750 train_time:101731ms step_avg:96.70ms
step:1053/1750 train_time:101832ms step_avg:96.71ms
step:1054/1750 train_time:101932ms step_avg:96.71ms
step:1055/1750 train_time:102033ms step_avg:96.71ms
step:1056/1750 train_time:102133ms step_avg:96.72ms
step:1057/1750 train_time:102233ms step_avg:96.72ms
step:1058/1750 train_time:102333ms step_avg:96.72ms
step:1059/1750 train_time:102434ms step_avg:96.73ms
step:1060/1750 train_time:102535ms step_avg:96.73ms
step:1061/1750 train_time:102636ms step_avg:96.73ms
step:1062/1750 train_time:102736ms step_avg:96.74ms
step:1063/1750 train_time:102838ms step_avg:96.74ms
step:1064/1750 train_time:102939ms step_avg:96.75ms
step:1065/1750 train_time:103041ms step_avg:96.75ms
step:1066/1750 train_time:103141ms step_avg:96.76ms
step:1067/1750 train_time:103242ms step_avg:96.76ms
step:1068/1750 train_time:103343ms step_avg:96.76ms
step:1069/1750 train_time:103444ms step_avg:96.77ms
step:1070/1750 train_time:103545ms step_avg:96.77ms
step:1071/1750 train_time:103647ms step_avg:96.78ms
step:1072/1750 train_time:103748ms step_avg:96.78ms
step:1073/1750 train_time:103849ms step_avg:96.78ms
step:1074/1750 train_time:103950ms step_avg:96.79ms
step:1075/1750 train_time:104050ms step_avg:96.79ms
step:1076/1750 train_time:104151ms step_avg:96.79ms
step:1077/1750 train_time:104252ms step_avg:96.80ms
step:1078/1750 train_time:104352ms step_avg:96.80ms
step:1079/1750 train_time:104454ms step_avg:96.81ms
step:1080/1750 train_time:104554ms step_avg:96.81ms
step:1081/1750 train_time:104655ms step_avg:96.81ms
step:1082/1750 train_time:104755ms step_avg:96.82ms
step:1083/1750 train_time:104855ms step_avg:96.82ms
step:1084/1750 train_time:104956ms step_avg:96.82ms
step:1085/1750 train_time:105056ms step_avg:96.83ms
step:1086/1750 train_time:105157ms step_avg:96.83ms
step:1087/1750 train_time:105256ms step_avg:96.83ms
step:1088/1750 train_time:105357ms step_avg:96.84ms
step:1089/1750 train_time:105457ms step_avg:96.84ms
step:1090/1750 train_time:105558ms step_avg:96.84ms
step:1091/1750 train_time:105660ms step_avg:96.85ms
step:1092/1750 train_time:105761ms step_avg:96.85ms
step:1093/1750 train_time:105861ms step_avg:96.85ms
step:1094/1750 train_time:105964ms step_avg:96.86ms
step:1095/1750 train_time:106065ms step_avg:96.86ms
step:1096/1750 train_time:106166ms step_avg:96.87ms
step:1097/1750 train_time:106266ms step_avg:96.87ms
step:1098/1750 train_time:106367ms step_avg:96.87ms
step:1099/1750 train_time:106467ms step_avg:96.88ms
step:1100/1750 train_time:106567ms step_avg:96.88ms
step:1101/1750 train_time:106668ms step_avg:96.88ms
step:1102/1750 train_time:106768ms step_avg:96.89ms
step:1103/1750 train_time:106869ms step_avg:96.89ms
step:1104/1750 train_time:106970ms step_avg:96.89ms
step:1105/1750 train_time:107071ms step_avg:96.90ms
step:1106/1750 train_time:107172ms step_avg:96.90ms
step:1107/1750 train_time:107272ms step_avg:96.90ms
step:1108/1750 train_time:107372ms step_avg:96.91ms
step:1109/1750 train_time:107472ms step_avg:96.91ms
step:1110/1750 train_time:107573ms step_avg:96.91ms
step:1111/1750 train_time:107673ms step_avg:96.92ms
step:1112/1750 train_time:107775ms step_avg:96.92ms
step:1113/1750 train_time:107877ms step_avg:96.92ms
step:1114/1750 train_time:107977ms step_avg:96.93ms
step:1115/1750 train_time:108078ms step_avg:96.93ms
step:1116/1750 train_time:108179ms step_avg:96.93ms
step:1117/1750 train_time:108278ms step_avg:96.94ms
step:1118/1750 train_time:108379ms step_avg:96.94ms
step:1119/1750 train_time:108480ms step_avg:96.94ms
step:1120/1750 train_time:108581ms step_avg:96.95ms
step:1121/1750 train_time:108682ms step_avg:96.95ms
step:1122/1750 train_time:108782ms step_avg:96.95ms
step:1123/1750 train_time:108883ms step_avg:96.96ms
step:1124/1750 train_time:108985ms step_avg:96.96ms
step:1125/1750 train_time:109085ms step_avg:96.96ms
step:1125/1750 val_loss:3.4516 train_time:109181ms step_avg:97.05ms
step:1126/1750 train_time:109207ms step_avg:96.99ms
step:1127/1750 train_time:109298ms step_avg:96.98ms
step:1128/1750 train_time:109402ms step_avg:96.99ms
step:1129/1750 train_time:109502ms step_avg:96.99ms
step:1130/1750 train_time:109602ms step_avg:96.99ms
step:1131/1750 train_time:109701ms step_avg:96.99ms
step:1132/1750 train_time:109801ms step_avg:97.00ms
step:1133/1750 train_time:109901ms step_avg:97.00ms
step:1134/1750 train_time:110001ms step_avg:97.00ms
step:1135/1750 train_time:110101ms step_avg:97.01ms
step:1136/1750 train_time:110203ms step_avg:97.01ms
step:1137/1750 train_time:110305ms step_avg:97.01ms
step:1138/1750 train_time:110407ms step_avg:97.02ms
step:1139/1750 train_time:110507ms step_avg:97.02ms
step:1140/1750 train_time:110607ms step_avg:97.02ms
step:1141/1750 train_time:110707ms step_avg:97.03ms
step:1142/1750 train_time:110807ms step_avg:97.03ms
step:1143/1750 train_time:110907ms step_avg:97.03ms
step:1144/1750 train_time:111007ms step_avg:97.03ms
step:1145/1750 train_time:111107ms step_avg:97.04ms
step:1146/1750 train_time:111208ms step_avg:97.04ms
step:1147/1750 train_time:111309ms step_avg:97.04ms
step:1148/1750 train_time:111409ms step_avg:97.05ms
step:1149/1750 train_time:111510ms step_avg:97.05ms
step:1150/1750 train_time:111610ms step_avg:97.05ms
step:1151/1750 train_time:111711ms step_avg:97.06ms
step:1152/1750 train_time:111811ms step_avg:97.06ms
step:1153/1750 train_time:111912ms step_avg:97.06ms
step:1154/1750 train_time:112013ms step_avg:97.06ms
step:1155/1750 train_time:112114ms step_avg:97.07ms
step:1156/1750 train_time:112214ms step_avg:97.07ms
step:1157/1750 train_time:112317ms step_avg:97.08ms
step:1158/1750 train_time:112417ms step_avg:97.08ms
step:1159/1750 train_time:112518ms step_avg:97.08ms
step:1160/1750 train_time:112619ms step_avg:97.08ms
step:1161/1750 train_time:112720ms step_avg:97.09ms
step:1162/1750 train_time:112821ms step_avg:97.09ms
step:1163/1750 train_time:112922ms step_avg:97.10ms
step:1164/1750 train_time:113023ms step_avg:97.10ms
step:1165/1750 train_time:113124ms step_avg:97.10ms
step:1166/1750 train_time:113226ms step_avg:97.11ms
step:1167/1750 train_time:113326ms step_avg:97.11ms
step:1168/1750 train_time:113427ms step_avg:97.11ms
step:1169/1750 train_time:113530ms step_avg:97.12ms
step:1170/1750 train_time:113632ms step_avg:97.12ms
step:1171/1750 train_time:113733ms step_avg:97.12ms
step:1172/1750 train_time:113835ms step_avg:97.13ms
step:1173/1750 train_time:113936ms step_avg:97.13ms
step:1174/1750 train_time:114038ms step_avg:97.14ms
step:1175/1750 train_time:114140ms step_avg:97.14ms
step:1176/1750 train_time:114242ms step_avg:97.14ms
step:1177/1750 train_time:114344ms step_avg:97.15ms
step:1178/1750 train_time:114445ms step_avg:97.15ms
step:1179/1750 train_time:114549ms step_avg:97.16ms
step:1180/1750 train_time:114651ms step_avg:97.16ms
step:1181/1750 train_time:114753ms step_avg:97.17ms
step:1182/1750 train_time:114856ms step_avg:97.17ms
step:1183/1750 train_time:114957ms step_avg:97.17ms
step:1184/1750 train_time:115059ms step_avg:97.18ms
step:1185/1750 train_time:115162ms step_avg:97.18ms
step:1186/1750 train_time:115264ms step_avg:97.19ms
step:1187/1750 train_time:115366ms step_avg:97.19ms
step:1188/1750 train_time:115467ms step_avg:97.19ms
step:1189/1750 train_time:115568ms step_avg:97.20ms
step:1190/1750 train_time:115670ms step_avg:97.20ms
step:1191/1750 train_time:115772ms step_avg:97.21ms
step:1192/1750 train_time:115873ms step_avg:97.21ms
step:1193/1750 train_time:115975ms step_avg:97.21ms
step:1194/1750 train_time:116077ms step_avg:97.22ms
step:1195/1750 train_time:116179ms step_avg:97.22ms
step:1196/1750 train_time:116281ms step_avg:97.22ms
step:1197/1750 train_time:116382ms step_avg:97.23ms
step:1198/1750 train_time:116485ms step_avg:97.23ms
step:1199/1750 train_time:116588ms step_avg:97.24ms
step:1200/1750 train_time:116688ms step_avg:97.24ms
step:1201/1750 train_time:116790ms step_avg:97.24ms
step:1202/1750 train_time:116893ms step_avg:97.25ms
step:1203/1750 train_time:116996ms step_avg:97.25ms
step:1204/1750 train_time:117097ms step_avg:97.26ms
step:1205/1750 train_time:117199ms step_avg:97.26ms
step:1206/1750 train_time:117301ms step_avg:97.26ms
step:1207/1750 train_time:117403ms step_avg:97.27ms
step:1208/1750 train_time:117506ms step_avg:97.27ms
step:1209/1750 train_time:117607ms step_avg:97.28ms
step:1210/1750 train_time:117708ms step_avg:97.28ms
step:1211/1750 train_time:117810ms step_avg:97.28ms
step:1212/1750 train_time:117911ms step_avg:97.29ms
step:1213/1750 train_time:118014ms step_avg:97.29ms
step:1214/1750 train_time:118115ms step_avg:97.29ms
step:1215/1750 train_time:118218ms step_avg:97.30ms
step:1216/1750 train_time:118320ms step_avg:97.30ms
step:1217/1750 train_time:118422ms step_avg:97.31ms
step:1218/1750 train_time:118524ms step_avg:97.31ms
step:1219/1750 train_time:118626ms step_avg:97.31ms
step:1220/1750 train_time:118727ms step_avg:97.32ms
step:1221/1750 train_time:118829ms step_avg:97.32ms
step:1222/1750 train_time:118930ms step_avg:97.32ms
step:1223/1750 train_time:119032ms step_avg:97.33ms
step:1224/1750 train_time:119134ms step_avg:97.33ms
step:1225/1750 train_time:119237ms step_avg:97.34ms
step:1226/1750 train_time:119339ms step_avg:97.34ms
step:1227/1750 train_time:119441ms step_avg:97.34ms
step:1228/1750 train_time:119543ms step_avg:97.35ms
step:1229/1750 train_time:119645ms step_avg:97.35ms
step:1230/1750 train_time:119746ms step_avg:97.35ms
step:1231/1750 train_time:119849ms step_avg:97.36ms
step:1232/1750 train_time:119951ms step_avg:97.36ms
step:1233/1750 train_time:120053ms step_avg:97.37ms
step:1234/1750 train_time:120154ms step_avg:97.37ms
step:1235/1750 train_time:120256ms step_avg:97.37ms
step:1236/1750 train_time:120360ms step_avg:97.38ms
step:1237/1750 train_time:120462ms step_avg:97.38ms
step:1238/1750 train_time:120563ms step_avg:97.39ms
step:1239/1750 train_time:120665ms step_avg:97.39ms
step:1240/1750 train_time:120766ms step_avg:97.39ms
step:1241/1750 train_time:120868ms step_avg:97.40ms
step:1242/1750 train_time:120971ms step_avg:97.40ms
step:1243/1750 train_time:121072ms step_avg:97.40ms
step:1244/1750 train_time:121174ms step_avg:97.41ms
step:1245/1750 train_time:121275ms step_avg:97.41ms
step:1246/1750 train_time:121379ms step_avg:97.41ms
step:1247/1750 train_time:121480ms step_avg:97.42ms
step:1248/1750 train_time:121582ms step_avg:97.42ms
step:1249/1750 train_time:121684ms step_avg:97.43ms
step:1250/1750 train_time:121785ms step_avg:97.43ms
step:1250/1750 val_loss:3.4066 train_time:121881ms step_avg:97.51ms
step:1251/1750 train_time:121910ms step_avg:97.45ms
step:1252/1750 train_time:121997ms step_avg:97.44ms
step:1253/1750 train_time:122098ms step_avg:97.44ms
step:1254/1750 train_time:122200ms step_avg:97.45ms
step:1255/1750 train_time:122302ms step_avg:97.45ms
step:1256/1750 train_time:122404ms step_avg:97.46ms
step:1257/1750 train_time:122504ms step_avg:97.46ms
step:1258/1750 train_time:122605ms step_avg:97.46ms
step:1259/1750 train_time:122706ms step_avg:97.46ms
step:1260/1750 train_time:122807ms step_avg:97.47ms
step:1261/1750 train_time:122910ms step_avg:97.47ms
step:1262/1750 train_time:123014ms step_avg:97.48ms
step:1263/1750 train_time:123116ms step_avg:97.48ms
step:1264/1750 train_time:123216ms step_avg:97.48ms
step:1265/1750 train_time:123318ms step_avg:97.48ms
step:1266/1750 train_time:123418ms step_avg:97.49ms
step:1267/1750 train_time:123520ms step_avg:97.49ms
step:1268/1750 train_time:123622ms step_avg:97.49ms
step:1269/1750 train_time:123723ms step_avg:97.50ms
step:1270/1750 train_time:123825ms step_avg:97.50ms
step:1271/1750 train_time:123929ms step_avg:97.50ms
step:1272/1750 train_time:124029ms step_avg:97.51ms
step:1273/1750 train_time:124132ms step_avg:97.51ms
step:1274/1750 train_time:124234ms step_avg:97.52ms
step:1275/1750 train_time:124336ms step_avg:97.52ms
step:1276/1750 train_time:124438ms step_avg:97.52ms
step:1277/1750 train_time:124540ms step_avg:97.53ms
step:1278/1750 train_time:124641ms step_avg:97.53ms
step:1279/1750 train_time:124743ms step_avg:97.53ms
step:1280/1750 train_time:124845ms step_avg:97.53ms
step:1281/1750 train_time:124947ms step_avg:97.54ms
step:1282/1750 train_time:125048ms step_avg:97.54ms
step:1283/1750 train_time:125150ms step_avg:97.54ms
step:1284/1750 train_time:125252ms step_avg:97.55ms
step:1285/1750 train_time:125353ms step_avg:97.55ms
step:1286/1750 train_time:125455ms step_avg:97.55ms
step:1287/1750 train_time:125557ms step_avg:97.56ms
step:1288/1750 train_time:125659ms step_avg:97.56ms
step:1289/1750 train_time:125761ms step_avg:97.56ms
step:1290/1750 train_time:125863ms step_avg:97.57ms
step:1291/1750 train_time:125965ms step_avg:97.57ms
step:1292/1750 train_time:126067ms step_avg:97.58ms
step:1293/1750 train_time:126169ms step_avg:97.58ms
step:1294/1750 train_time:126272ms step_avg:97.58ms
step:1295/1750 train_time:126374ms step_avg:97.59ms
step:1296/1750 train_time:126475ms step_avg:97.59ms
step:1297/1750 train_time:126576ms step_avg:97.59ms
step:1298/1750 train_time:126678ms step_avg:97.59ms
step:1299/1750 train_time:126780ms step_avg:97.60ms
step:1300/1750 train_time:126882ms step_avg:97.60ms
step:1301/1750 train_time:126984ms step_avg:97.60ms
step:1302/1750 train_time:127086ms step_avg:97.61ms
step:1303/1750 train_time:127190ms step_avg:97.61ms
step:1304/1750 train_time:127291ms step_avg:97.62ms
step:1305/1750 train_time:127393ms step_avg:97.62ms
step:1306/1750 train_time:127495ms step_avg:97.62ms
step:1307/1750 train_time:127596ms step_avg:97.63ms
step:1308/1750 train_time:127698ms step_avg:97.63ms
step:1309/1750 train_time:127800ms step_avg:97.63ms
step:1310/1750 train_time:127902ms step_avg:97.63ms
step:1311/1750 train_time:128004ms step_avg:97.64ms
step:1312/1750 train_time:128107ms step_avg:97.64ms
step:1313/1750 train_time:128209ms step_avg:97.65ms
step:1314/1750 train_time:128311ms step_avg:97.65ms
step:1315/1750 train_time:128413ms step_avg:97.65ms
step:1316/1750 train_time:128514ms step_avg:97.66ms
step:1317/1750 train_time:128615ms step_avg:97.66ms
step:1318/1750 train_time:128716ms step_avg:97.66ms
step:1319/1750 train_time:128819ms step_avg:97.66ms
step:1320/1750 train_time:128921ms step_avg:97.67ms
step:1321/1750 train_time:129022ms step_avg:97.67ms
step:1322/1750 train_time:129125ms step_avg:97.67ms
step:1323/1750 train_time:129227ms step_avg:97.68ms
step:1324/1750 train_time:129330ms step_avg:97.68ms
step:1325/1750 train_time:129432ms step_avg:97.68ms
step:1326/1750 train_time:129535ms step_avg:97.69ms
step:1327/1750 train_time:129636ms step_avg:97.69ms
step:1328/1750 train_time:129737ms step_avg:97.69ms
step:1329/1750 train_time:129839ms step_avg:97.70ms
step:1330/1750 train_time:129941ms step_avg:97.70ms
step:1331/1750 train_time:130043ms step_avg:97.70ms
step:1332/1750 train_time:130145ms step_avg:97.71ms
step:1333/1750 train_time:130248ms step_avg:97.71ms
step:1334/1750 train_time:130351ms step_avg:97.71ms
step:1335/1750 train_time:130452ms step_avg:97.72ms
step:1336/1750 train_time:130554ms step_avg:97.72ms
step:1337/1750 train_time:130656ms step_avg:97.72ms
step:1338/1750 train_time:130757ms step_avg:97.73ms
step:1339/1750 train_time:130860ms step_avg:97.73ms
step:1340/1750 train_time:130961ms step_avg:97.73ms
step:1341/1750 train_time:131064ms step_avg:97.74ms
step:1342/1750 train_time:131165ms step_avg:97.74ms
step:1343/1750 train_time:131267ms step_avg:97.74ms
step:1344/1750 train_time:131369ms step_avg:97.74ms
step:1345/1750 train_time:131471ms step_avg:97.75ms
step:1346/1750 train_time:131573ms step_avg:97.75ms
step:1347/1750 train_time:131675ms step_avg:97.75ms
step:1348/1750 train_time:131777ms step_avg:97.76ms
step:1349/1750 train_time:131879ms step_avg:97.76ms
step:1350/1750 train_time:131981ms step_avg:97.76ms
step:1351/1750 train_time:132084ms step_avg:97.77ms
step:1352/1750 train_time:132184ms step_avg:97.77ms
step:1353/1750 train_time:132286ms step_avg:97.77ms
step:1354/1750 train_time:132388ms step_avg:97.78ms
step:1355/1750 train_time:132490ms step_avg:97.78ms
step:1356/1750 train_time:132592ms step_avg:97.78ms
step:1357/1750 train_time:132694ms step_avg:97.78ms
step:1358/1750 train_time:132796ms step_avg:97.79ms
step:1359/1750 train_time:132898ms step_avg:97.79ms
step:1360/1750 train_time:133001ms step_avg:97.79ms
step:1361/1750 train_time:133102ms step_avg:97.80ms
step:1362/1750 train_time:133203ms step_avg:97.80ms
step:1363/1750 train_time:133305ms step_avg:97.80ms
step:1364/1750 train_time:133408ms step_avg:97.81ms
step:1365/1750 train_time:133511ms step_avg:97.81ms
step:1366/1750 train_time:133612ms step_avg:97.81ms
step:1367/1750 train_time:133714ms step_avg:97.82ms
step:1368/1750 train_time:133816ms step_avg:97.82ms
step:1369/1750 train_time:133917ms step_avg:97.82ms
step:1370/1750 train_time:134018ms step_avg:97.82ms
step:1371/1750 train_time:134120ms step_avg:97.83ms
step:1372/1750 train_time:134222ms step_avg:97.83ms
step:1373/1750 train_time:134324ms step_avg:97.83ms
step:1374/1750 train_time:134427ms step_avg:97.84ms
step:1375/1750 train_time:134531ms step_avg:97.84ms
step:1375/1750 val_loss:3.3658 train_time:134626ms step_avg:97.91ms
step:1376/1750 train_time:134654ms step_avg:97.86ms
step:1377/1750 train_time:134746ms step_avg:97.85ms
step:1378/1750 train_time:134850ms step_avg:97.86ms
step:1379/1750 train_time:134952ms step_avg:97.86ms
step:1380/1750 train_time:135053ms step_avg:97.86ms
step:1381/1750 train_time:135156ms step_avg:97.87ms
step:1382/1750 train_time:135257ms step_avg:97.87ms
step:1383/1750 train_time:135358ms step_avg:97.87ms
step:1384/1750 train_time:135459ms step_avg:97.88ms
step:1385/1750 train_time:135560ms step_avg:97.88ms
step:1386/1750 train_time:135663ms step_avg:97.88ms
step:1387/1750 train_time:135765ms step_avg:97.88ms
step:1388/1750 train_time:135867ms step_avg:97.89ms
step:1389/1750 train_time:135970ms step_avg:97.89ms
step:1390/1750 train_time:136071ms step_avg:97.89ms
step:1391/1750 train_time:136174ms step_avg:97.90ms
step:1392/1750 train_time:136276ms step_avg:97.90ms
step:1393/1750 train_time:136378ms step_avg:97.90ms
step:1394/1750 train_time:136479ms step_avg:97.90ms
step:1395/1750 train_time:136582ms step_avg:97.91ms
step:1396/1750 train_time:136683ms step_avg:97.91ms
step:1397/1750 train_time:136786ms step_avg:97.91ms
step:1398/1750 train_time:136888ms step_avg:97.92ms
step:1399/1750 train_time:136990ms step_avg:97.92ms
step:1400/1750 train_time:137092ms step_avg:97.92ms
step:1401/1750 train_time:137194ms step_avg:97.93ms
step:1402/1750 train_time:137296ms step_avg:97.93ms
step:1403/1750 train_time:137399ms step_avg:97.93ms
step:1404/1750 train_time:137501ms step_avg:97.94ms
step:1405/1750 train_time:137603ms step_avg:97.94ms
step:1406/1750 train_time:137705ms step_avg:97.94ms
step:1407/1750 train_time:137807ms step_avg:97.94ms
step:1408/1750 train_time:137908ms step_avg:97.95ms
step:1409/1750 train_time:138012ms step_avg:97.95ms
step:1410/1750 train_time:138114ms step_avg:97.95ms
step:1411/1750 train_time:138215ms step_avg:97.96ms
step:1412/1750 train_time:138317ms step_avg:97.96ms
step:1413/1750 train_time:138418ms step_avg:97.96ms
step:1414/1750 train_time:138521ms step_avg:97.96ms
step:1415/1750 train_time:138623ms step_avg:97.97ms
step:1416/1750 train_time:138724ms step_avg:97.97ms
step:1417/1750 train_time:138826ms step_avg:97.97ms
step:1418/1750 train_time:138928ms step_avg:97.97ms
step:1419/1750 train_time:139030ms step_avg:97.98ms
step:1420/1750 train_time:139131ms step_avg:97.98ms
step:1421/1750 train_time:139234ms step_avg:97.98ms
step:1422/1750 train_time:139336ms step_avg:97.99ms
step:1423/1750 train_time:139438ms step_avg:97.99ms
step:1424/1750 train_time:139541ms step_avg:97.99ms
step:1425/1750 train_time:139644ms step_avg:98.00ms
step:1426/1750 train_time:139745ms step_avg:98.00ms
step:1427/1750 train_time:139847ms step_avg:98.00ms
step:1428/1750 train_time:139952ms step_avg:98.01ms
step:1429/1750 train_time:140054ms step_avg:98.01ms
step:1430/1750 train_time:140157ms step_avg:98.01ms
step:1431/1750 train_time:140260ms step_avg:98.02ms
step:1432/1750 train_time:140363ms step_avg:98.02ms
step:1433/1750 train_time:140466ms step_avg:98.02ms
step:1434/1750 train_time:140569ms step_avg:98.03ms
step:1435/1750 train_time:140674ms step_avg:98.03ms
step:1436/1750 train_time:140779ms step_avg:98.04ms
step:1437/1750 train_time:140885ms step_avg:98.04ms
step:1438/1750 train_time:140986ms step_avg:98.04ms
step:1439/1750 train_time:141089ms step_avg:98.05ms
step:1440/1750 train_time:141193ms step_avg:98.05ms
step:1441/1750 train_time:141298ms step_avg:98.06ms
step:1442/1750 train_time:141401ms step_avg:98.06ms
step:1443/1750 train_time:141503ms step_avg:98.06ms
step:1444/1750 train_time:141606ms step_avg:98.07ms
step:1445/1750 train_time:141709ms step_avg:98.07ms
step:1446/1750 train_time:141812ms step_avg:98.07ms
step:1447/1750 train_time:141916ms step_avg:98.08ms
step:1448/1750 train_time:142019ms step_avg:98.08ms
step:1449/1750 train_time:142121ms step_avg:98.08ms
step:1450/1750 train_time:142224ms step_avg:98.09ms
step:1451/1750 train_time:142326ms step_avg:98.09ms
step:1452/1750 train_time:142429ms step_avg:98.09ms
step:1453/1750 train_time:142533ms step_avg:98.10ms
step:1454/1750 train_time:142639ms step_avg:98.10ms
step:1455/1750 train_time:142742ms step_avg:98.10ms
step:1456/1750 train_time:142845ms step_avg:98.11ms
step:1457/1750 train_time:142948ms step_avg:98.11ms
step:1458/1750 train_time:143050ms step_avg:98.11ms
step:1459/1750 train_time:143153ms step_avg:98.12ms
step:1460/1750 train_time:143257ms step_avg:98.12ms
step:1461/1750 train_time:143359ms step_avg:98.12ms
step:1462/1750 train_time:143462ms step_avg:98.13ms
step:1463/1750 train_time:143568ms step_avg:98.13ms
step:1464/1750 train_time:143670ms step_avg:98.14ms
step:1465/1750 train_time:143774ms step_avg:98.14ms
step:1466/1750 train_time:143877ms step_avg:98.14ms
step:1467/1750 train_time:143979ms step_avg:98.15ms
step:1468/1750 train_time:144083ms step_avg:98.15ms
step:1469/1750 train_time:144186ms step_avg:98.15ms
step:1470/1750 train_time:144289ms step_avg:98.16ms
step:1471/1750 train_time:144392ms step_avg:98.16ms
step:1472/1750 train_time:144494ms step_avg:98.16ms
step:1473/1750 train_time:144598ms step_avg:98.17ms
step:1474/1750 train_time:144702ms step_avg:98.17ms
step:1475/1750 train_time:144804ms step_avg:98.17ms
step:1476/1750 train_time:144908ms step_avg:98.18ms
step:1477/1750 train_time:145011ms step_avg:98.18ms
step:1478/1750 train_time:145114ms step_avg:98.18ms
step:1479/1750 train_time:145217ms step_avg:98.19ms
step:1480/1750 train_time:145320ms step_avg:98.19ms
step:1481/1750 train_time:145423ms step_avg:98.19ms
step:1482/1750 train_time:145526ms step_avg:98.20ms
step:1483/1750 train_time:145629ms step_avg:98.20ms
step:1484/1750 train_time:145734ms step_avg:98.20ms
step:1485/1750 train_time:145837ms step_avg:98.21ms
step:1486/1750 train_time:145941ms step_avg:98.21ms
step:1487/1750 train_time:146042ms step_avg:98.21ms
step:1488/1750 train_time:146146ms step_avg:98.22ms
step:1489/1750 train_time:146249ms step_avg:98.22ms
step:1490/1750 train_time:146351ms step_avg:98.22ms
step:1491/1750 train_time:146455ms step_avg:98.23ms
step:1492/1750 train_time:146559ms step_avg:98.23ms
step:1493/1750 train_time:146662ms step_avg:98.23ms
step:1494/1750 train_time:146765ms step_avg:98.24ms
step:1495/1750 train_time:146867ms step_avg:98.24ms
step:1496/1750 train_time:146971ms step_avg:98.24ms
step:1497/1750 train_time:147072ms step_avg:98.24ms
step:1498/1750 train_time:147176ms step_avg:98.25ms
step:1499/1750 train_time:147278ms step_avg:98.25ms
step:1500/1750 train_time:147381ms step_avg:98.25ms
step:1500/1750 val_loss:3.3295 train_time:147479ms step_avg:98.32ms
step:1501/1750 train_time:147508ms step_avg:98.27ms
step:1502/1750 train_time:147595ms step_avg:98.27ms
step:1503/1750 train_time:147698ms step_avg:98.27ms
step:1504/1750 train_time:147800ms step_avg:98.27ms
step:1505/1750 train_time:147902ms step_avg:98.27ms
step:1506/1750 train_time:148005ms step_avg:98.28ms
step:1507/1750 train_time:148107ms step_avg:98.28ms
step:1508/1750 train_time:148209ms step_avg:98.28ms
step:1509/1750 train_time:148311ms step_avg:98.28ms
step:1510/1750 train_time:148413ms step_avg:98.29ms
step:1511/1750 train_time:148518ms step_avg:98.29ms
step:1512/1750 train_time:148620ms step_avg:98.29ms
step:1513/1750 train_time:148723ms step_avg:98.30ms
step:1514/1750 train_time:148827ms step_avg:98.30ms
step:1515/1750 train_time:148934ms step_avg:98.31ms
step:1516/1750 train_time:149038ms step_avg:98.31ms
step:1517/1750 train_time:149139ms step_avg:98.31ms
step:1518/1750 train_time:149242ms step_avg:98.31ms
step:1519/1750 train_time:149346ms step_avg:98.32ms
step:1520/1750 train_time:149450ms step_avg:98.32ms
step:1521/1750 train_time:149552ms step_avg:98.32ms
step:1522/1750 train_time:149655ms step_avg:98.33ms
step:1523/1750 train_time:149758ms step_avg:98.33ms
step:1524/1750 train_time:149863ms step_avg:98.34ms
step:1525/1750 train_time:149968ms step_avg:98.34ms
step:1526/1750 train_time:150070ms step_avg:98.34ms
step:1527/1750 train_time:150174ms step_avg:98.35ms
step:1528/1750 train_time:150278ms step_avg:98.35ms
step:1529/1750 train_time:150380ms step_avg:98.35ms
step:1530/1750 train_time:150485ms step_avg:98.36ms
step:1531/1750 train_time:150587ms step_avg:98.36ms
step:1532/1750 train_time:150690ms step_avg:98.36ms
step:1533/1750 train_time:150794ms step_avg:98.37ms
step:1534/1750 train_time:150897ms step_avg:98.37ms
step:1535/1750 train_time:151000ms step_avg:98.37ms
step:1536/1750 train_time:151103ms step_avg:98.37ms
step:1537/1750 train_time:151205ms step_avg:98.38ms
step:1538/1750 train_time:151308ms step_avg:98.38ms
step:1539/1750 train_time:151410ms step_avg:98.38ms
step:1540/1750 train_time:151514ms step_avg:98.39ms
step:1541/1750 train_time:151616ms step_avg:98.39ms
step:1542/1750 train_time:151720ms step_avg:98.39ms
step:1543/1750 train_time:151823ms step_avg:98.39ms
step:1544/1750 train_time:151926ms step_avg:98.40ms
step:1545/1750 train_time:152029ms step_avg:98.40ms
step:1546/1750 train_time:152132ms step_avg:98.40ms
step:1547/1750 train_time:152236ms step_avg:98.41ms
step:1548/1750 train_time:152341ms step_avg:98.41ms
step:1549/1750 train_time:152444ms step_avg:98.41ms
step:1550/1750 train_time:152547ms step_avg:98.42ms
step:1551/1750 train_time:152651ms step_avg:98.42ms
step:1552/1750 train_time:152754ms step_avg:98.42ms
step:1553/1750 train_time:152857ms step_avg:98.43ms
step:1554/1750 train_time:152960ms step_avg:98.43ms
step:1555/1750 train_time:153063ms step_avg:98.43ms
step:1556/1750 train_time:153166ms step_avg:98.44ms
step:1557/1750 train_time:153269ms step_avg:98.44ms
step:1558/1750 train_time:153374ms step_avg:98.44ms
step:1559/1750 train_time:153477ms step_avg:98.45ms
step:1560/1750 train_time:153580ms step_avg:98.45ms
step:1561/1750 train_time:153684ms step_avg:98.45ms
step:1562/1750 train_time:153787ms step_avg:98.45ms
step:1563/1750 train_time:153892ms step_avg:98.46ms
step:1564/1750 train_time:153994ms step_avg:98.46ms
step:1565/1750 train_time:154098ms step_avg:98.47ms
step:1566/1750 train_time:154201ms step_avg:98.47ms
step:1567/1750 train_time:154305ms step_avg:98.47ms
step:1568/1750 train_time:154408ms step_avg:98.47ms
step:1569/1750 train_time:154511ms step_avg:98.48ms
step:1570/1750 train_time:154616ms step_avg:98.48ms
step:1571/1750 train_time:154718ms step_avg:98.48ms
step:1572/1750 train_time:154821ms step_avg:98.49ms
step:1573/1750 train_time:154924ms step_avg:98.49ms
step:1574/1750 train_time:155026ms step_avg:98.49ms
step:1575/1750 train_time:155129ms step_avg:98.49ms
step:1576/1750 train_time:155232ms step_avg:98.50ms
step:1577/1750 train_time:155338ms step_avg:98.50ms
step:1578/1750 train_time:155441ms step_avg:98.50ms
step:1579/1750 train_time:155545ms step_avg:98.51ms
step:1580/1750 train_time:155649ms step_avg:98.51ms
step:1581/1750 train_time:155752ms step_avg:98.51ms
step:1582/1750 train_time:155855ms step_avg:98.52ms
step:1583/1750 train_time:155959ms step_avg:98.52ms
step:1584/1750 train_time:156064ms step_avg:98.53ms
step:1585/1750 train_time:156167ms step_avg:98.53ms
step:1586/1750 train_time:156272ms step_avg:98.53ms
step:1587/1750 train_time:156376ms step_avg:98.54ms
step:1588/1750 train_time:156479ms step_avg:98.54ms
step:1589/1750 train_time:156582ms step_avg:98.54ms
step:1590/1750 train_time:156686ms step_avg:98.54ms
step:1591/1750 train_time:156790ms step_avg:98.55ms
step:1592/1750 train_time:156893ms step_avg:98.55ms
step:1593/1750 train_time:156996ms step_avg:98.55ms
step:1594/1750 train_time:157101ms step_avg:98.56ms
step:1595/1750 train_time:157204ms step_avg:98.56ms
step:1596/1750 train_time:157306ms step_avg:98.56ms
step:1597/1750 train_time:157410ms step_avg:98.57ms
step:1598/1750 train_time:157515ms step_avg:98.57ms
step:1599/1750 train_time:157618ms step_avg:98.57ms
step:1600/1750 train_time:157722ms step_avg:98.58ms
step:1601/1750 train_time:157825ms step_avg:98.58ms
step:1602/1750 train_time:157927ms step_avg:98.58ms
step:1603/1750 train_time:158031ms step_avg:98.58ms
step:1604/1750 train_time:158133ms step_avg:98.59ms
step:1605/1750 train_time:158237ms step_avg:98.59ms
step:1606/1750 train_time:158340ms step_avg:98.59ms
step:1607/1750 train_time:158443ms step_avg:98.60ms
step:1608/1750 train_time:158546ms step_avg:98.60ms
step:1609/1750 train_time:158648ms step_avg:98.60ms
step:1610/1750 train_time:158753ms step_avg:98.60ms
step:1611/1750 train_time:158857ms step_avg:98.61ms
step:1612/1750 train_time:158962ms step_avg:98.61ms
step:1613/1750 train_time:159065ms step_avg:98.61ms
step:1614/1750 train_time:159167ms step_avg:98.62ms
step:1615/1750 train_time:159269ms step_avg:98.62ms
step:1616/1750 train_time:159373ms step_avg:98.62ms
step:1617/1750 train_time:159477ms step_avg:98.63ms
step:1618/1750 train_time:159580ms step_avg:98.63ms
step:1619/1750 train_time:159683ms step_avg:98.63ms
step:1620/1750 train_time:159786ms step_avg:98.63ms
step:1621/1750 train_time:159889ms step_avg:98.64ms
step:1622/1750 train_time:159992ms step_avg:98.64ms
step:1623/1750 train_time:160096ms step_avg:98.64ms
step:1624/1750 train_time:160200ms step_avg:98.65ms
step:1625/1750 train_time:160304ms step_avg:98.65ms
step:1625/1750 val_loss:3.2999 train_time:160402ms step_avg:98.71ms
step:1626/1750 train_time:160429ms step_avg:98.66ms
step:1627/1750 train_time:160519ms step_avg:98.66ms
step:1628/1750 train_time:160621ms step_avg:98.66ms
step:1629/1750 train_time:160724ms step_avg:98.66ms
step:1630/1750 train_time:160828ms step_avg:98.67ms
step:1631/1750 train_time:160930ms step_avg:98.67ms
step:1632/1750 train_time:161034ms step_avg:98.67ms
step:1633/1750 train_time:161136ms step_avg:98.67ms
step:1634/1750 train_time:161240ms step_avg:98.68ms
step:1635/1750 train_time:161342ms step_avg:98.68ms
step:1636/1750 train_time:161448ms step_avg:98.68ms
step:1637/1750 train_time:161552ms step_avg:98.69ms
step:1638/1750 train_time:161655ms step_avg:98.69ms
step:1639/1750 train_time:161757ms step_avg:98.69ms
step:1640/1750 train_time:161859ms step_avg:98.69ms
step:1641/1750 train_time:161963ms step_avg:98.70ms
step:1642/1750 train_time:162065ms step_avg:98.70ms
step:1643/1750 train_time:162168ms step_avg:98.70ms
step:1644/1750 train_time:162272ms step_avg:98.71ms
step:1645/1750 train_time:162375ms step_avg:98.71ms
step:1646/1750 train_time:162478ms step_avg:98.71ms
step:1647/1750 train_time:162583ms step_avg:98.71ms
step:1648/1750 train_time:162686ms step_avg:98.72ms
step:1649/1750 train_time:162789ms step_avg:98.72ms
step:1650/1750 train_time:162892ms step_avg:98.72ms
step:1651/1750 train_time:162995ms step_avg:98.72ms
step:1652/1750 train_time:163098ms step_avg:98.73ms
step:1653/1750 train_time:163202ms step_avg:98.73ms
step:1654/1750 train_time:163305ms step_avg:98.73ms
step:1655/1750 train_time:163411ms step_avg:98.74ms
step:1656/1750 train_time:163514ms step_avg:98.74ms
step:1657/1750 train_time:163616ms step_avg:98.74ms
step:1658/1750 train_time:163719ms step_avg:98.75ms
step:1659/1750 train_time:163826ms step_avg:98.75ms
step:1660/1750 train_time:163929ms step_avg:98.75ms
step:1661/1750 train_time:164034ms step_avg:98.76ms
step:1662/1750 train_time:164138ms step_avg:98.76ms
step:1663/1750 train_time:164242ms step_avg:98.76ms
step:1664/1750 train_time:164345ms step_avg:98.77ms
step:1665/1750 train_time:164450ms step_avg:98.77ms
step:1666/1750 train_time:164554ms step_avg:98.77ms
step:1667/1750 train_time:164656ms step_avg:98.77ms
step:1668/1750 train_time:164761ms step_avg:98.78ms
step:1669/1750 train_time:164865ms step_avg:98.78ms
step:1670/1750 train_time:164967ms step_avg:98.78ms
step:1671/1750 train_time:165072ms step_avg:98.79ms
step:1672/1750 train_time:165176ms step_avg:98.79ms
step:1673/1750 train_time:165278ms step_avg:98.79ms
step:1674/1750 train_time:165382ms step_avg:98.79ms
step:1675/1750 train_time:165486ms step_avg:98.80ms
step:1676/1750 train_time:165589ms step_avg:98.80ms
step:1677/1750 train_time:165693ms step_avg:98.80ms
step:1678/1750 train_time:165797ms step_avg:98.81ms
step:1679/1750 train_time:165900ms step_avg:98.81ms
step:1680/1750 train_time:166003ms step_avg:98.81ms
step:1681/1750 train_time:166107ms step_avg:98.81ms
step:1682/1750 train_time:166212ms step_avg:98.82ms
step:1683/1750 train_time:166314ms step_avg:98.82ms
step:1684/1750 train_time:166418ms step_avg:98.82ms
step:1685/1750 train_time:166523ms step_avg:98.83ms
step:1686/1750 train_time:166627ms step_avg:98.83ms
step:1687/1750 train_time:166730ms step_avg:98.83ms
step:1688/1750 train_time:166834ms step_avg:98.84ms
step:1689/1750 train_time:166938ms step_avg:98.84ms
step:1690/1750 train_time:167043ms step_avg:98.84ms
step:1691/1750 train_time:167147ms step_avg:98.85ms
step:1692/1750 train_time:167250ms step_avg:98.85ms
step:1693/1750 train_time:167354ms step_avg:98.85ms
step:1694/1750 train_time:167458ms step_avg:98.85ms
step:1695/1750 train_time:167563ms step_avg:98.86ms
step:1696/1750 train_time:167667ms step_avg:98.86ms
step:1697/1750 train_time:167776ms step_avg:98.87ms
step:1698/1750 train_time:167879ms step_avg:98.87ms
step:1699/1750 train_time:167983ms step_avg:98.87ms
step:1700/1750 train_time:168088ms step_avg:98.88ms
step:1701/1750 train_time:168191ms step_avg:98.88ms
step:1702/1750 train_time:168298ms step_avg:98.88ms
step:1703/1750 train_time:168402ms step_avg:98.89ms
step:1704/1750 train_time:168505ms step_avg:98.89ms
step:1705/1750 train_time:168609ms step_avg:98.89ms
step:1706/1750 train_time:168715ms step_avg:98.90ms
step:1707/1750 train_time:168819ms step_avg:98.90ms
step:1708/1750 train_time:168924ms step_avg:98.90ms
step:1709/1750 train_time:169028ms step_avg:98.90ms
step:1710/1750 train_time:169134ms step_avg:98.91ms
step:1711/1750 train_time:169238ms step_avg:98.91ms
step:1712/1750 train_time:169341ms step_avg:98.91ms
step:1713/1750 train_time:169446ms step_avg:98.92ms
step:1714/1750 train_time:169550ms step_avg:98.92ms
step:1715/1750 train_time:169656ms step_avg:98.92ms
step:1716/1750 train_time:169760ms step_avg:98.93ms
step:1717/1750 train_time:169864ms step_avg:98.93ms
step:1718/1750 train_time:169967ms step_avg:98.93ms
step:1719/1750 train_time:170073ms step_avg:98.94ms
step:1720/1750 train_time:170177ms step_avg:98.94ms
step:1721/1750 train_time:170280ms step_avg:98.94ms
step:1722/1750 train_time:170385ms step_avg:98.95ms
step:1723/1750 train_time:170489ms step_avg:98.95ms
step:1724/1750 train_time:170594ms step_avg:98.95ms
step:1725/1750 train_time:170698ms step_avg:98.96ms
step:1726/1750 train_time:170801ms step_avg:98.96ms
step:1727/1750 train_time:170906ms step_avg:98.96ms
step:1728/1750 train_time:171012ms step_avg:98.97ms
step:1729/1750 train_time:171116ms step_avg:98.97ms
step:1730/1750 train_time:171219ms step_avg:98.97ms
step:1731/1750 train_time:171324ms step_avg:98.97ms
step:1732/1750 train_time:171428ms step_avg:98.98ms
step:1733/1750 train_time:171533ms step_avg:98.98ms
step:1734/1750 train_time:171638ms step_avg:98.98ms
step:1735/1750 train_time:171741ms step_avg:98.99ms
step:1736/1750 train_time:171845ms step_avg:98.99ms
step:1737/1750 train_time:171951ms step_avg:98.99ms
step:1738/1750 train_time:172055ms step_avg:99.00ms
step:1739/1750 train_time:172159ms step_avg:99.00ms
step:1740/1750 train_time:172263ms step_avg:99.00ms
step:1741/1750 train_time:172372ms step_avg:99.01ms
step:1742/1750 train_time:172475ms step_avg:99.01ms
step:1743/1750 train_time:172581ms step_avg:99.01ms
step:1744/1750 train_time:172686ms step_avg:99.02ms
step:1745/1750 train_time:172791ms step_avg:99.02ms
step:1746/1750 train_time:172895ms step_avg:99.02ms
step:1747/1750 train_time:173000ms step_avg:99.03ms
step:1748/1750 train_time:173103ms step_avg:99.03ms
step:1749/1750 train_time:173207ms step_avg:99.03ms
step:1750/1750 train_time:173313ms step_avg:99.04ms
step:1750/1750 val_loss:3.2789 train_time:173413ms step_avg:99.09ms
peak memory allocated: 33277 MiB reserved: 48512 MiB

import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn, autocast
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
#import wandb

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)

        params = list(params)
        sizes = {p.shape for p in params}

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params,))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * self.world_size
            for base_i in range(0, len(params), self.world_size):
                if base_i + self.rank < len(params):
                    grad = params[base_i + self.rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + self.world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * self.world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), self.world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                futures.append(dist.all_gather(params_pad[base_i:base_i + self.world_size], params_pad[base_i + self.rank], async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01, rank: int = 0, world_size: int = 1):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        self.rank = rank
        self.world_size = world_size

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(
                params=group_params,
            ))
        super().__init__(param_groups, defaults)

    @torch.compile
    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // self.world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)

                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']

                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)

                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t

                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        for param in self.embed.parameters():
            param.lr_mul = 75.
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embeds in self.value_embeds:
            for param in self.value_embeds.parameters():
                param.lr_mul = 75.
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.lr_mul = 27.5
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % world_size
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            #return causal_mask
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, world_size: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos:pos+max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()

    starts = []
    batch_end=None
    for i in range(len(boundary_positions) - 1):
        end = boundary_positions[i + 1].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == world_size:
                batch_end = end
                break
            start = end
    assert batch_end is not None # increase max_batch_span if necessary
    batch_span = batch_end-pos
    return starts, batch_span

def distributed_data_generator(filename_pattern: str, batch_size: int, rank: int, world_size: int, align_to_bos: bool):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    batch_span = batch_size
    max_batch_span = 2*batch_size if align_to_bos else batch_size #provide buffer to handle samples up to length local_batch_size

    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, world_size, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    train_align_to_bos = True # align local batch start indicies with next bos_token
    val_align_to_bos = False # False to maintain same eval as prior records
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

#if master_process:
#    wandb.init(project="modded-nanogpt-tiny", name=f"run-{os.path.basename(__file__)}", save_code=True)

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=True):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
if master_process:
    print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=next_multiple_of_n(50257, n=128), num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094

optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0, rank=rank, world_size=world_size)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]

for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

for n, p in model.named_parameters():
    wd_mul = getattr(p, "wd_mul", 1.0)
    lr_mul = getattr(p, "lr_mul", 1.0)

    print0(f"{n}: {p.shape} {p.dtype} {wd_mul} {lr_mul}")

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
embedding_params = sum(p.numel() for n, p in model.named_parameters() if "embed" in n)
non_embedding_params = total_params - embedding_params

print0(f"")
print0(f"Model parameters:")
print0(f"  Total parameters: {total_params:,}")
print0(f"  Embedding parameters: {embedding_params:,}")
print0(f"  Non-embedding parameters: {non_embedding_params:,}")

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    w = min((1 - x) / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.05
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

torch.cuda.synchronize()
dist.barrier()
with torch.profiler.profile() as prof:
    for _ in range(warmup_steps):
        inputs, targets = next(train_loader)
        model(inputs, targets, get_window_size_blocks(1)).backward()
        for opt in optimizers:
            opt.step()
    model.zero_grad(set_to_none=True)
    torch.cuda.synchronize()
    dist.barrier()
os.makedirs("traces", exist_ok=True)
prof.export_chrome_trace(f"traces/trace_{rank}.json")

model.load_state_dict(initial_state['model'])
for opt, opt_state in zip(optimizers, initial_state['optimizers']):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size, args.val_align_to_bos)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        #if master_process:
        #    wandb.log({"val/loss": val_loss}, step=step)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250524+cu126 compiled for CUDA 12.6
Sun Jul 13 00:35:53 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   28C    P0            114W /  700W |    5856MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1517MiB /  81559MiB |      3%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   28C    P0            115W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   33C    P0            119W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   28C    P0            113W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           55716      C   /usr/bin/python3                       1508MiB |
|    0   N/A  N/A           55717      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           55718      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           55719      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           55720      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           55721      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           55722      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           55723      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           55717      C   /usr/bin/python3                       1508MiB |
|    2   N/A  N/A           55718      C   /usr/bin/python3                       1508MiB |
|    3   N/A  N/A           55719      C   /usr/bin/python3                       1508MiB |
|    4   N/A  N/A           55720      C   /usr/bin/python3                       1508MiB |
|    5   N/A  N/A           55721      C   /usr/bin/python3                       1508MiB |
|    6   N/A  N/A           55722      C   /usr/bin/python3                       1508MiB |
|    7   N/A  N/A           55723      C   /usr/bin/python3                       1508MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
scalars: torch.Size([64]) torch.float32 1.0 5.0
embed.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.0.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.1.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.2.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
blocks.0.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.0.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.1.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.1.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.2.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.2.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.3.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.3.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.4.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.4.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.5.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.5.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.6.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.6.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.7.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.7.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.8.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.8.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.9.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.9.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.10.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.10.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.11.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.11.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
lm_head.weight: torch.Size([50304, 768]) torch.float32 1.0 27.5

Model parameters:
  Total parameters: 275,742,784
  Embedding parameters: 154,533,888
  Non-embedding parameters: 121,208,896
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1750 train_time:152ms step_avg:151.91ms
step:2/1750 train_time:177ms step_avg:88.54ms
step:3/1750 train_time:249ms step_avg:83.13ms
step:4/1750 train_time:341ms step_avg:85.22ms
step:5/1750 train_time:433ms step_avg:86.63ms
step:6/1750 train_time:525ms step_avg:87.56ms
step:7/1750 train_time:617ms step_avg:88.20ms
step:8/1750 train_time:709ms step_avg:88.67ms
step:9/1750 train_time:802ms step_avg:89.11ms
step:10/1750 train_time:894ms step_avg:89.44ms
step:11/1750 train_time:986ms step_avg:89.68ms
step:12/1750 train_time:1080ms step_avg:89.99ms
step:13/1750 train_time:1176ms step_avg:90.46ms
step:14/1750 train_time:1271ms step_avg:90.76ms
step:15/1750 train_time:1363ms step_avg:90.88ms
step:16/1750 train_time:1456ms step_avg:91.00ms
step:17/1750 train_time:1549ms step_avg:91.09ms
step:18/1750 train_time:1642ms step_avg:91.19ms
step:19/1750 train_time:1734ms step_avg:91.29ms
step:20/1750 train_time:1827ms step_avg:91.34ms
step:21/1750 train_time:1920ms step_avg:91.44ms
step:22/1750 train_time:2013ms step_avg:91.50ms
step:23/1750 train_time:2106ms step_avg:91.57ms
step:24/1750 train_time:2200ms step_avg:91.68ms
step:25/1750 train_time:2295ms step_avg:91.80ms
step:26/1750 train_time:2388ms step_avg:91.86ms
step:27/1750 train_time:2481ms step_avg:91.90ms
step:28/1750 train_time:2574ms step_avg:91.92ms
step:29/1750 train_time:2667ms step_avg:91.96ms
step:30/1750 train_time:2759ms step_avg:91.98ms
step:31/1750 train_time:2852ms step_avg:92.00ms
step:32/1750 train_time:2944ms step_avg:92.00ms
step:33/1750 train_time:3037ms step_avg:92.02ms
step:34/1750 train_time:3130ms step_avg:92.05ms
step:35/1750 train_time:3223ms step_avg:92.08ms
step:36/1750 train_time:3317ms step_avg:92.13ms
step:37/1750 train_time:3410ms step_avg:92.16ms
step:38/1750 train_time:3503ms step_avg:92.18ms
step:39/1750 train_time:3596ms step_avg:92.21ms
step:40/1750 train_time:3689ms step_avg:92.22ms
step:41/1750 train_time:3781ms step_avg:92.22ms
step:42/1750 train_time:3874ms step_avg:92.24ms
step:43/1750 train_time:3967ms step_avg:92.25ms
step:44/1750 train_time:4060ms step_avg:92.28ms
step:45/1750 train_time:4153ms step_avg:92.30ms
step:46/1750 train_time:4247ms step_avg:92.32ms
step:47/1750 train_time:4341ms step_avg:92.36ms
step:48/1750 train_time:4434ms step_avg:92.38ms
step:49/1750 train_time:4528ms step_avg:92.40ms
step:50/1750 train_time:4621ms step_avg:92.42ms
step:51/1750 train_time:4715ms step_avg:92.45ms
step:52/1750 train_time:4808ms step_avg:92.46ms
step:53/1750 train_time:4901ms step_avg:92.47ms
step:54/1750 train_time:4993ms step_avg:92.47ms
step:55/1750 train_time:5086ms step_avg:92.47ms
step:56/1750 train_time:5178ms step_avg:92.47ms
step:57/1750 train_time:5273ms step_avg:92.50ms
step:58/1750 train_time:5365ms step_avg:92.51ms
step:59/1750 train_time:5459ms step_avg:92.52ms
step:60/1750 train_time:5552ms step_avg:92.53ms
step:61/1750 train_time:5644ms step_avg:92.53ms
step:62/1750 train_time:5738ms step_avg:92.55ms
step:63/1750 train_time:5831ms step_avg:92.56ms
step:64/1750 train_time:5924ms step_avg:92.57ms
step:65/1750 train_time:6018ms step_avg:92.58ms
step:66/1750 train_time:6110ms step_avg:92.58ms
step:67/1750 train_time:6204ms step_avg:92.59ms
step:68/1750 train_time:6297ms step_avg:92.61ms
step:69/1750 train_time:6390ms step_avg:92.61ms
step:70/1750 train_time:6483ms step_avg:92.62ms
step:71/1750 train_time:6577ms step_avg:92.63ms
step:72/1750 train_time:6671ms step_avg:92.65ms
step:73/1750 train_time:6762ms step_avg:92.63ms
step:74/1750 train_time:6855ms step_avg:92.64ms
step:75/1750 train_time:6949ms step_avg:92.65ms
step:76/1750 train_time:7041ms step_avg:92.64ms
step:77/1750 train_time:7134ms step_avg:92.65ms
step:78/1750 train_time:7227ms step_avg:92.66ms
step:79/1750 train_time:7321ms step_avg:92.68ms
step:80/1750 train_time:7415ms step_avg:92.68ms
step:81/1750 train_time:7508ms step_avg:92.69ms
step:82/1750 train_time:7601ms step_avg:92.70ms
step:83/1750 train_time:7695ms step_avg:92.71ms
step:84/1750 train_time:7788ms step_avg:92.71ms
step:85/1750 train_time:7883ms step_avg:92.74ms
step:86/1750 train_time:7975ms step_avg:92.73ms
step:87/1750 train_time:8068ms step_avg:92.73ms
step:88/1750 train_time:8161ms step_avg:92.74ms
step:89/1750 train_time:8253ms step_avg:92.73ms
step:90/1750 train_time:8346ms step_avg:92.73ms
step:91/1750 train_time:8438ms step_avg:92.73ms
step:92/1750 train_time:8532ms step_avg:92.74ms
step:93/1750 train_time:8624ms step_avg:92.73ms
step:94/1750 train_time:8718ms step_avg:92.75ms
step:95/1750 train_time:8811ms step_avg:92.75ms
step:96/1750 train_time:8904ms step_avg:92.75ms
step:97/1750 train_time:8997ms step_avg:92.76ms
step:98/1750 train_time:9090ms step_avg:92.76ms
step:99/1750 train_time:9183ms step_avg:92.76ms
step:100/1750 train_time:9276ms step_avg:92.76ms
step:101/1750 train_time:9369ms step_avg:92.77ms
step:102/1750 train_time:9462ms step_avg:92.76ms
step:103/1750 train_time:9556ms step_avg:92.77ms
step:104/1750 train_time:9649ms step_avg:92.78ms
step:105/1750 train_time:9742ms step_avg:92.78ms
step:106/1750 train_time:9836ms step_avg:92.80ms
step:107/1750 train_time:9929ms step_avg:92.79ms
step:108/1750 train_time:10022ms step_avg:92.80ms
step:109/1750 train_time:10115ms step_avg:92.80ms
step:110/1750 train_time:10208ms step_avg:92.80ms
step:111/1750 train_time:10302ms step_avg:92.81ms
step:112/1750 train_time:10395ms step_avg:92.81ms
step:113/1750 train_time:10489ms step_avg:92.82ms
step:114/1750 train_time:10582ms step_avg:92.82ms
step:115/1750 train_time:10675ms step_avg:92.83ms
step:116/1750 train_time:10767ms step_avg:92.82ms
step:117/1750 train_time:10861ms step_avg:92.83ms
step:118/1750 train_time:10954ms step_avg:92.83ms
step:119/1750 train_time:11047ms step_avg:92.83ms
step:120/1750 train_time:11140ms step_avg:92.83ms
step:121/1750 train_time:11233ms step_avg:92.84ms
step:122/1750 train_time:11326ms step_avg:92.84ms
step:123/1750 train_time:11420ms step_avg:92.85ms
step:124/1750 train_time:11514ms step_avg:92.85ms
step:125/1750 train_time:11607ms step_avg:92.85ms
step:125/1750 val_loss:4.6324 train_time:11695ms step_avg:93.56ms
step:126/1750 train_time:11722ms step_avg:93.03ms
step:127/1750 train_time:11799ms step_avg:92.91ms
step:128/1750 train_time:11897ms step_avg:92.94ms
step:129/1750 train_time:11992ms step_avg:92.96ms
step:130/1750 train_time:12084ms step_avg:92.95ms
step:131/1750 train_time:12177ms step_avg:92.95ms
step:132/1750 train_time:12270ms step_avg:92.95ms
step:133/1750 train_time:12363ms step_avg:92.95ms
step:134/1750 train_time:12456ms step_avg:92.95ms
step:135/1750 train_time:12549ms step_avg:92.95ms
step:136/1750 train_time:12642ms step_avg:92.96ms
step:137/1750 train_time:12735ms step_avg:92.96ms
step:138/1750 train_time:12830ms step_avg:92.97ms
step:139/1750 train_time:12926ms step_avg:93.00ms
step:140/1750 train_time:13021ms step_avg:93.01ms
step:141/1750 train_time:13115ms step_avg:93.02ms
step:142/1750 train_time:13209ms step_avg:93.02ms
step:143/1750 train_time:13301ms step_avg:93.02ms
step:144/1750 train_time:13394ms step_avg:93.02ms
step:145/1750 train_time:13488ms step_avg:93.02ms
step:146/1750 train_time:13581ms step_avg:93.02ms
step:147/1750 train_time:13674ms step_avg:93.02ms
step:148/1750 train_time:13768ms step_avg:93.03ms
step:149/1750 train_time:13863ms step_avg:93.04ms
step:150/1750 train_time:13958ms step_avg:93.05ms
step:151/1750 train_time:14052ms step_avg:93.06ms
step:152/1750 train_time:14147ms step_avg:93.07ms
step:153/1750 train_time:14240ms step_avg:93.07ms
step:154/1750 train_time:14333ms step_avg:93.07ms
step:155/1750 train_time:14427ms step_avg:93.08ms
step:156/1750 train_time:14520ms step_avg:93.08ms
step:157/1750 train_time:14614ms step_avg:93.08ms
step:158/1750 train_time:14707ms step_avg:93.09ms
step:159/1750 train_time:14801ms step_avg:93.09ms
step:160/1750 train_time:14895ms step_avg:93.09ms
step:161/1750 train_time:14989ms step_avg:93.10ms
step:162/1750 train_time:15084ms step_avg:93.11ms
step:163/1750 train_time:15177ms step_avg:93.11ms
step:164/1750 train_time:15270ms step_avg:93.11ms
step:165/1750 train_time:15364ms step_avg:93.11ms
step:166/1750 train_time:15457ms step_avg:93.12ms
step:167/1750 train_time:15551ms step_avg:93.12ms
step:168/1750 train_time:15645ms step_avg:93.12ms
step:169/1750 train_time:15738ms step_avg:93.12ms
step:170/1750 train_time:15831ms step_avg:93.13ms
step:171/1750 train_time:15925ms step_avg:93.13ms
step:172/1750 train_time:16020ms step_avg:93.14ms
step:173/1750 train_time:16114ms step_avg:93.15ms
step:174/1750 train_time:16208ms step_avg:93.15ms
step:175/1750 train_time:16302ms step_avg:93.15ms
step:176/1750 train_time:16395ms step_avg:93.16ms
step:177/1750 train_time:16489ms step_avg:93.16ms
step:178/1750 train_time:16582ms step_avg:93.16ms
step:179/1750 train_time:16675ms step_avg:93.16ms
step:180/1750 train_time:16769ms step_avg:93.16ms
step:181/1750 train_time:16863ms step_avg:93.16ms
step:182/1750 train_time:16956ms step_avg:93.17ms
step:183/1750 train_time:17050ms step_avg:93.17ms
step:184/1750 train_time:17145ms step_avg:93.18ms
step:185/1750 train_time:17239ms step_avg:93.18ms
step:186/1750 train_time:17332ms step_avg:93.18ms
step:187/1750 train_time:17426ms step_avg:93.19ms
step:188/1750 train_time:17520ms step_avg:93.19ms
step:189/1750 train_time:17613ms step_avg:93.19ms
step:190/1750 train_time:17707ms step_avg:93.19ms
step:191/1750 train_time:17800ms step_avg:93.20ms
step:192/1750 train_time:17895ms step_avg:93.20ms
step:193/1750 train_time:17988ms step_avg:93.20ms
step:194/1750 train_time:18082ms step_avg:93.21ms
step:195/1750 train_time:18175ms step_avg:93.21ms
step:196/1750 train_time:18269ms step_avg:93.21ms
step:197/1750 train_time:18363ms step_avg:93.21ms
step:198/1750 train_time:18456ms step_avg:93.21ms
step:199/1750 train_time:18549ms step_avg:93.21ms
step:200/1750 train_time:18643ms step_avg:93.22ms
step:201/1750 train_time:18736ms step_avg:93.22ms
step:202/1750 train_time:18830ms step_avg:93.22ms
step:203/1750 train_time:18924ms step_avg:93.22ms
step:204/1750 train_time:19018ms step_avg:93.23ms
step:205/1750 train_time:19111ms step_avg:93.23ms
step:206/1750 train_time:19205ms step_avg:93.23ms
step:207/1750 train_time:19299ms step_avg:93.23ms
step:208/1750 train_time:19393ms step_avg:93.23ms
step:209/1750 train_time:19487ms step_avg:93.24ms
step:210/1750 train_time:19581ms step_avg:93.24ms
step:211/1750 train_time:19674ms step_avg:93.24ms
step:212/1750 train_time:19768ms step_avg:93.25ms
step:213/1750 train_time:19863ms step_avg:93.25ms
step:214/1750 train_time:19956ms step_avg:93.25ms
step:215/1750 train_time:20050ms step_avg:93.25ms
step:216/1750 train_time:20144ms step_avg:93.26ms
step:217/1750 train_time:20237ms step_avg:93.26ms
step:218/1750 train_time:20331ms step_avg:93.26ms
step:219/1750 train_time:20424ms step_avg:93.26ms
step:220/1750 train_time:20518ms step_avg:93.26ms
step:221/1750 train_time:20611ms step_avg:93.26ms
step:222/1750 train_time:20705ms step_avg:93.26ms
step:223/1750 train_time:20799ms step_avg:93.27ms
step:224/1750 train_time:20892ms step_avg:93.27ms
step:225/1750 train_time:20987ms step_avg:93.27ms
step:226/1750 train_time:21081ms step_avg:93.28ms
step:227/1750 train_time:21174ms step_avg:93.28ms
step:228/1750 train_time:21269ms step_avg:93.28ms
step:229/1750 train_time:21362ms step_avg:93.29ms
step:230/1750 train_time:21456ms step_avg:93.29ms
step:231/1750 train_time:21549ms step_avg:93.29ms
step:232/1750 train_time:21643ms step_avg:93.29ms
step:233/1750 train_time:21736ms step_avg:93.29ms
step:234/1750 train_time:21829ms step_avg:93.29ms
step:235/1750 train_time:21924ms step_avg:93.29ms
step:236/1750 train_time:22017ms step_avg:93.29ms
step:237/1750 train_time:22112ms step_avg:93.30ms
step:238/1750 train_time:22206ms step_avg:93.30ms
step:239/1750 train_time:22299ms step_avg:93.30ms
step:240/1750 train_time:22393ms step_avg:93.30ms
step:241/1750 train_time:22486ms step_avg:93.30ms
step:242/1750 train_time:22580ms step_avg:93.31ms
step:243/1750 train_time:22674ms step_avg:93.31ms
step:244/1750 train_time:22767ms step_avg:93.31ms
step:245/1750 train_time:22861ms step_avg:93.31ms
step:246/1750 train_time:22955ms step_avg:93.31ms
step:247/1750 train_time:23049ms step_avg:93.31ms
step:248/1750 train_time:23143ms step_avg:93.32ms
step:249/1750 train_time:23236ms step_avg:93.32ms
step:250/1750 train_time:23330ms step_avg:93.32ms
step:250/1750 val_loss:4.0955 train_time:23419ms step_avg:93.67ms
step:251/1750 train_time:23446ms step_avg:93.41ms
step:252/1750 train_time:23525ms step_avg:93.35ms
step:253/1750 train_time:23625ms step_avg:93.38ms
step:254/1750 train_time:23720ms step_avg:93.39ms
step:255/1750 train_time:23813ms step_avg:93.38ms
step:256/1750 train_time:23906ms step_avg:93.38ms
step:257/1750 train_time:23998ms step_avg:93.38ms
step:258/1750 train_time:24091ms step_avg:93.37ms
step:259/1750 train_time:24184ms step_avg:93.37ms
step:260/1750 train_time:24277ms step_avg:93.37ms
step:261/1750 train_time:24370ms step_avg:93.37ms
step:262/1750 train_time:24465ms step_avg:93.38ms
step:263/1750 train_time:24562ms step_avg:93.39ms
step:264/1750 train_time:24657ms step_avg:93.40ms
step:265/1750 train_time:24752ms step_avg:93.40ms
step:266/1750 train_time:24846ms step_avg:93.41ms
step:267/1750 train_time:24941ms step_avg:93.41ms
step:268/1750 train_time:25035ms step_avg:93.41ms
step:269/1750 train_time:25129ms step_avg:93.41ms
step:270/1750 train_time:25222ms step_avg:93.42ms
step:271/1750 train_time:25316ms step_avg:93.42ms
step:272/1750 train_time:25410ms step_avg:93.42ms
step:273/1750 train_time:25504ms step_avg:93.42ms
step:274/1750 train_time:25599ms step_avg:93.43ms
step:275/1750 train_time:25694ms step_avg:93.43ms
step:276/1750 train_time:25788ms step_avg:93.44ms
step:277/1750 train_time:25883ms step_avg:93.44ms
step:278/1750 train_time:25978ms step_avg:93.45ms
step:279/1750 train_time:26073ms step_avg:93.45ms
step:280/1750 train_time:26167ms step_avg:93.45ms
step:281/1750 train_time:26261ms step_avg:93.46ms
step:282/1750 train_time:26354ms step_avg:93.45ms
step:283/1750 train_time:26448ms step_avg:93.46ms
step:284/1750 train_time:26543ms step_avg:93.46ms
step:285/1750 train_time:26637ms step_avg:93.46ms
step:286/1750 train_time:26731ms step_avg:93.47ms
step:287/1750 train_time:26826ms step_avg:93.47ms
step:288/1750 train_time:26920ms step_avg:93.47ms
step:289/1750 train_time:27014ms step_avg:93.47ms
step:290/1750 train_time:27108ms step_avg:93.48ms
step:291/1750 train_time:27202ms step_avg:93.48ms
step:292/1750 train_time:27296ms step_avg:93.48ms
step:293/1750 train_time:27390ms step_avg:93.48ms
step:294/1750 train_time:27484ms step_avg:93.48ms
step:295/1750 train_time:27579ms step_avg:93.49ms
step:296/1750 train_time:27673ms step_avg:93.49ms
step:297/1750 train_time:27768ms step_avg:93.49ms
step:298/1750 train_time:27862ms step_avg:93.50ms
step:299/1750 train_time:27957ms step_avg:93.50ms
step:300/1750 train_time:28051ms step_avg:93.50ms
step:301/1750 train_time:28146ms step_avg:93.51ms
step:302/1750 train_time:28240ms step_avg:93.51ms
step:303/1750 train_time:28334ms step_avg:93.51ms
step:304/1750 train_time:28428ms step_avg:93.51ms
step:305/1750 train_time:28522ms step_avg:93.51ms
step:306/1750 train_time:28615ms step_avg:93.51ms
step:307/1750 train_time:28709ms step_avg:93.52ms
step:308/1750 train_time:28804ms step_avg:93.52ms
step:309/1750 train_time:28899ms step_avg:93.52ms
step:310/1750 train_time:28993ms step_avg:93.53ms
step:311/1750 train_time:29087ms step_avg:93.53ms
step:312/1750 train_time:29181ms step_avg:93.53ms
step:313/1750 train_time:29274ms step_avg:93.53ms
step:314/1750 train_time:29369ms step_avg:93.53ms
step:315/1750 train_time:29463ms step_avg:93.53ms
step:316/1750 train_time:29557ms step_avg:93.54ms
step:317/1750 train_time:29651ms step_avg:93.54ms
step:318/1750 train_time:29746ms step_avg:93.54ms
step:319/1750 train_time:29840ms step_avg:93.54ms
step:320/1750 train_time:29934ms step_avg:93.54ms
step:321/1750 train_time:30028ms step_avg:93.55ms
step:322/1750 train_time:30122ms step_avg:93.55ms
step:323/1750 train_time:30216ms step_avg:93.55ms
step:324/1750 train_time:30311ms step_avg:93.55ms
step:325/1750 train_time:30405ms step_avg:93.55ms
step:326/1750 train_time:30499ms step_avg:93.56ms
step:327/1750 train_time:30594ms step_avg:93.56ms
step:328/1750 train_time:30687ms step_avg:93.56ms
step:329/1750 train_time:30782ms step_avg:93.56ms
step:330/1750 train_time:30876ms step_avg:93.56ms
step:331/1750 train_time:30970ms step_avg:93.57ms
step:332/1750 train_time:31064ms step_avg:93.57ms
step:333/1750 train_time:31159ms step_avg:93.57ms
step:334/1750 train_time:31253ms step_avg:93.57ms
step:335/1750 train_time:31348ms step_avg:93.58ms
step:336/1750 train_time:31442ms step_avg:93.58ms
step:337/1750 train_time:31537ms step_avg:93.58ms
step:338/1750 train_time:31631ms step_avg:93.58ms
step:339/1750 train_time:31726ms step_avg:93.59ms
step:340/1750 train_time:31820ms step_avg:93.59ms
step:341/1750 train_time:31914ms step_avg:93.59ms
step:342/1750 train_time:32008ms step_avg:93.59ms
step:343/1750 train_time:32102ms step_avg:93.59ms
step:344/1750 train_time:32196ms step_avg:93.59ms
step:345/1750 train_time:32291ms step_avg:93.60ms
step:346/1750 train_time:32385ms step_avg:93.60ms
step:347/1750 train_time:32479ms step_avg:93.60ms
step:348/1750 train_time:32574ms step_avg:93.60ms
step:349/1750 train_time:32668ms step_avg:93.60ms
step:350/1750 train_time:32762ms step_avg:93.61ms
step:351/1750 train_time:32857ms step_avg:93.61ms
step:352/1750 train_time:32951ms step_avg:93.61ms
step:353/1750 train_time:33046ms step_avg:93.61ms
step:354/1750 train_time:33140ms step_avg:93.62ms
step:355/1750 train_time:33234ms step_avg:93.62ms
step:356/1750 train_time:33327ms step_avg:93.62ms
step:357/1750 train_time:33422ms step_avg:93.62ms
step:358/1750 train_time:33517ms step_avg:93.62ms
step:359/1750 train_time:33611ms step_avg:93.62ms
step:360/1750 train_time:33706ms step_avg:93.63ms
step:361/1750 train_time:33801ms step_avg:93.63ms
step:362/1750 train_time:33895ms step_avg:93.63ms
step:363/1750 train_time:33989ms step_avg:93.63ms
step:364/1750 train_time:34083ms step_avg:93.63ms
step:365/1750 train_time:34177ms step_avg:93.64ms
step:366/1750 train_time:34271ms step_avg:93.64ms
step:367/1750 train_time:34366ms step_avg:93.64ms
step:368/1750 train_time:34461ms step_avg:93.64ms
step:369/1750 train_time:34555ms step_avg:93.64ms
step:370/1750 train_time:34648ms step_avg:93.64ms
step:371/1750 train_time:34743ms step_avg:93.65ms
step:372/1750 train_time:34837ms step_avg:93.65ms
step:373/1750 train_time:34931ms step_avg:93.65ms
step:374/1750 train_time:35025ms step_avg:93.65ms
step:375/1750 train_time:35120ms step_avg:93.65ms
step:375/1750 val_loss:3.9007 train_time:35209ms step_avg:93.89ms
step:376/1750 train_time:35236ms step_avg:93.71ms
step:377/1750 train_time:35316ms step_avg:93.68ms
step:378/1750 train_time:35412ms step_avg:93.68ms
step:379/1750 train_time:35508ms step_avg:93.69ms
step:380/1750 train_time:35601ms step_avg:93.69ms
step:381/1750 train_time:35695ms step_avg:93.69ms
step:382/1750 train_time:35788ms step_avg:93.69ms
step:383/1750 train_time:35882ms step_avg:93.69ms
step:384/1750 train_time:35975ms step_avg:93.69ms
step:385/1750 train_time:36069ms step_avg:93.68ms
step:386/1750 train_time:36162ms step_avg:93.68ms
step:387/1750 train_time:36257ms step_avg:93.69ms
step:388/1750 train_time:36353ms step_avg:93.69ms
step:389/1750 train_time:36448ms step_avg:93.70ms
step:390/1750 train_time:36543ms step_avg:93.70ms
step:391/1750 train_time:36640ms step_avg:93.71ms
step:392/1750 train_time:36736ms step_avg:93.71ms
step:393/1750 train_time:36832ms step_avg:93.72ms
step:394/1750 train_time:36927ms step_avg:93.72ms
step:395/1750 train_time:37023ms step_avg:93.73ms
step:396/1750 train_time:37119ms step_avg:93.73ms
step:397/1750 train_time:37216ms step_avg:93.74ms
step:398/1750 train_time:37313ms step_avg:93.75ms
step:399/1750 train_time:37410ms step_avg:93.76ms
step:400/1750 train_time:37507ms step_avg:93.77ms
step:401/1750 train_time:37604ms step_avg:93.77ms
step:402/1750 train_time:37700ms step_avg:93.78ms
step:403/1750 train_time:37797ms step_avg:93.79ms
step:404/1750 train_time:37892ms step_avg:93.79ms
step:405/1750 train_time:37988ms step_avg:93.80ms
step:406/1750 train_time:38083ms step_avg:93.80ms
step:407/1750 train_time:38179ms step_avg:93.81ms
step:408/1750 train_time:38277ms step_avg:93.82ms
step:409/1750 train_time:38374ms step_avg:93.82ms
step:410/1750 train_time:38471ms step_avg:93.83ms
step:411/1750 train_time:38568ms step_avg:93.84ms
step:412/1750 train_time:38664ms step_avg:93.84ms
step:413/1750 train_time:38761ms step_avg:93.85ms
step:414/1750 train_time:38858ms step_avg:93.86ms
step:415/1750 train_time:38954ms step_avg:93.87ms
step:416/1750 train_time:39051ms step_avg:93.87ms
step:417/1750 train_time:39147ms step_avg:93.88ms
step:418/1750 train_time:39243ms step_avg:93.88ms
step:419/1750 train_time:39340ms step_avg:93.89ms
step:420/1750 train_time:39436ms step_avg:93.90ms
step:421/1750 train_time:39533ms step_avg:93.90ms
step:422/1750 train_time:39630ms step_avg:93.91ms
step:423/1750 train_time:39726ms step_avg:93.92ms
step:424/1750 train_time:39821ms step_avg:93.92ms
step:425/1750 train_time:39918ms step_avg:93.92ms
step:426/1750 train_time:40015ms step_avg:93.93ms
step:427/1750 train_time:40111ms step_avg:93.94ms
step:428/1750 train_time:40207ms step_avg:93.94ms
step:429/1750 train_time:40303ms step_avg:93.95ms
step:430/1750 train_time:40400ms step_avg:93.95ms
step:431/1750 train_time:40497ms step_avg:93.96ms
step:432/1750 train_time:40593ms step_avg:93.96ms
step:433/1750 train_time:40689ms step_avg:93.97ms
step:434/1750 train_time:40785ms step_avg:93.97ms
step:435/1750 train_time:40881ms step_avg:93.98ms
step:436/1750 train_time:40979ms step_avg:93.99ms
step:437/1750 train_time:41075ms step_avg:93.99ms
step:438/1750 train_time:41172ms step_avg:94.00ms
step:439/1750 train_time:41268ms step_avg:94.01ms
step:440/1750 train_time:41364ms step_avg:94.01ms
step:441/1750 train_time:41461ms step_avg:94.02ms
step:442/1750 train_time:41558ms step_avg:94.02ms
step:443/1750 train_time:41654ms step_avg:94.03ms
step:444/1750 train_time:41750ms step_avg:94.03ms
step:445/1750 train_time:41846ms step_avg:94.04ms
step:446/1750 train_time:41942ms step_avg:94.04ms
step:447/1750 train_time:42039ms step_avg:94.05ms
step:448/1750 train_time:42136ms step_avg:94.05ms
step:449/1750 train_time:42233ms step_avg:94.06ms
step:450/1750 train_time:42330ms step_avg:94.07ms
step:451/1750 train_time:42426ms step_avg:94.07ms
step:452/1750 train_time:42522ms step_avg:94.07ms
step:453/1750 train_time:42619ms step_avg:94.08ms
step:454/1750 train_time:42715ms step_avg:94.09ms
step:455/1750 train_time:42812ms step_avg:94.09ms
step:456/1750 train_time:42908ms step_avg:94.10ms
step:457/1750 train_time:43004ms step_avg:94.10ms
step:458/1750 train_time:43101ms step_avg:94.11ms
step:459/1750 train_time:43198ms step_avg:94.11ms
step:460/1750 train_time:43294ms step_avg:94.12ms
step:461/1750 train_time:43390ms step_avg:94.12ms
step:462/1750 train_time:43486ms step_avg:94.13ms
step:463/1750 train_time:43583ms step_avg:94.13ms
step:464/1750 train_time:43680ms step_avg:94.14ms
step:465/1750 train_time:43777ms step_avg:94.14ms
step:466/1750 train_time:43874ms step_avg:94.15ms
step:467/1750 train_time:43969ms step_avg:94.15ms
step:468/1750 train_time:44065ms step_avg:94.16ms
step:469/1750 train_time:44162ms step_avg:94.16ms
step:470/1750 train_time:44259ms step_avg:94.17ms
step:471/1750 train_time:44355ms step_avg:94.17ms
step:472/1750 train_time:44452ms step_avg:94.18ms
step:473/1750 train_time:44549ms step_avg:94.18ms
step:474/1750 train_time:44644ms step_avg:94.19ms
step:475/1750 train_time:44741ms step_avg:94.19ms
step:476/1750 train_time:44837ms step_avg:94.20ms
step:477/1750 train_time:44933ms step_avg:94.20ms
step:478/1750 train_time:45029ms step_avg:94.20ms
step:479/1750 train_time:45125ms step_avg:94.21ms
step:480/1750 train_time:45222ms step_avg:94.21ms
step:481/1750 train_time:45319ms step_avg:94.22ms
step:482/1750 train_time:45415ms step_avg:94.22ms
step:483/1750 train_time:45512ms step_avg:94.23ms
step:484/1750 train_time:45609ms step_avg:94.23ms
step:485/1750 train_time:45705ms step_avg:94.24ms
step:486/1750 train_time:45801ms step_avg:94.24ms
step:487/1750 train_time:45897ms step_avg:94.24ms
step:488/1750 train_time:45993ms step_avg:94.25ms
step:489/1750 train_time:46089ms step_avg:94.25ms
step:490/1750 train_time:46185ms step_avg:94.25ms
step:491/1750 train_time:46281ms step_avg:94.26ms
step:492/1750 train_time:46378ms step_avg:94.26ms
step:493/1750 train_time:46475ms step_avg:94.27ms
step:494/1750 train_time:46571ms step_avg:94.27ms
step:495/1750 train_time:46667ms step_avg:94.28ms
step:496/1750 train_time:46763ms step_avg:94.28ms
step:497/1750 train_time:46859ms step_avg:94.28ms
step:498/1750 train_time:46956ms step_avg:94.29ms
step:499/1750 train_time:47053ms step_avg:94.29ms
step:500/1750 train_time:47149ms step_avg:94.30ms
step:500/1750 val_loss:3.7484 train_time:47239ms step_avg:94.48ms
step:501/1750 train_time:47266ms step_avg:94.34ms
step:502/1750 train_time:47350ms step_avg:94.32ms
step:503/1750 train_time:47450ms step_avg:94.33ms
step:504/1750 train_time:47548ms step_avg:94.34ms
step:505/1750 train_time:47644ms step_avg:94.34ms
step:506/1750 train_time:47740ms step_avg:94.35ms
step:507/1750 train_time:47835ms step_avg:94.35ms
step:508/1750 train_time:47931ms step_avg:94.35ms
step:509/1750 train_time:48027ms step_avg:94.35ms
step:510/1750 train_time:48122ms step_avg:94.36ms
step:511/1750 train_time:48219ms step_avg:94.36ms
step:512/1750 train_time:48317ms step_avg:94.37ms
step:513/1750 train_time:48415ms step_avg:94.38ms
step:514/1750 train_time:48512ms step_avg:94.38ms
step:515/1750 train_time:48608ms step_avg:94.38ms
step:516/1750 train_time:48705ms step_avg:94.39ms
step:517/1750 train_time:48801ms step_avg:94.39ms
step:518/1750 train_time:48898ms step_avg:94.40ms
step:519/1750 train_time:48994ms step_avg:94.40ms
step:520/1750 train_time:49090ms step_avg:94.40ms
step:521/1750 train_time:49186ms step_avg:94.41ms
step:522/1750 train_time:49284ms step_avg:94.41ms
step:523/1750 train_time:49382ms step_avg:94.42ms
step:524/1750 train_time:49480ms step_avg:94.43ms
step:525/1750 train_time:49578ms step_avg:94.43ms
step:526/1750 train_time:49675ms step_avg:94.44ms
step:527/1750 train_time:49771ms step_avg:94.44ms
step:528/1750 train_time:49869ms step_avg:94.45ms
step:529/1750 train_time:49966ms step_avg:94.45ms
step:530/1750 train_time:50062ms step_avg:94.46ms
step:531/1750 train_time:50159ms step_avg:94.46ms
step:532/1750 train_time:50255ms step_avg:94.46ms
step:533/1750 train_time:50352ms step_avg:94.47ms
step:534/1750 train_time:50448ms step_avg:94.47ms
step:535/1750 train_time:50546ms step_avg:94.48ms
step:536/1750 train_time:50644ms step_avg:94.48ms
step:537/1750 train_time:50741ms step_avg:94.49ms
step:538/1750 train_time:50838ms step_avg:94.49ms
step:539/1750 train_time:50935ms step_avg:94.50ms
step:540/1750 train_time:51031ms step_avg:94.50ms
step:541/1750 train_time:51128ms step_avg:94.51ms
step:542/1750 train_time:51225ms step_avg:94.51ms
step:543/1750 train_time:51322ms step_avg:94.52ms
step:544/1750 train_time:51419ms step_avg:94.52ms
step:545/1750 train_time:51516ms step_avg:94.52ms
step:546/1750 train_time:51612ms step_avg:94.53ms
step:547/1750 train_time:51709ms step_avg:94.53ms
step:548/1750 train_time:51806ms step_avg:94.54ms
step:549/1750 train_time:51904ms step_avg:94.54ms
step:550/1750 train_time:52001ms step_avg:94.55ms
step:551/1750 train_time:52098ms step_avg:94.55ms
step:552/1750 train_time:52194ms step_avg:94.56ms
step:553/1750 train_time:52291ms step_avg:94.56ms
step:554/1750 train_time:52388ms step_avg:94.56ms
step:555/1750 train_time:52486ms step_avg:94.57ms
step:556/1750 train_time:52584ms step_avg:94.57ms
step:557/1750 train_time:52681ms step_avg:94.58ms
step:558/1750 train_time:52778ms step_avg:94.58ms
step:559/1750 train_time:52875ms step_avg:94.59ms
step:560/1750 train_time:52972ms step_avg:94.59ms
step:561/1750 train_time:53069ms step_avg:94.60ms
step:562/1750 train_time:53166ms step_avg:94.60ms
step:563/1750 train_time:53263ms step_avg:94.61ms
step:564/1750 train_time:53360ms step_avg:94.61ms
step:565/1750 train_time:53457ms step_avg:94.61ms
step:566/1750 train_time:53555ms step_avg:94.62ms
step:567/1750 train_time:53650ms step_avg:94.62ms
step:568/1750 train_time:53747ms step_avg:94.63ms
step:569/1750 train_time:53845ms step_avg:94.63ms
step:570/1750 train_time:53942ms step_avg:94.64ms
step:571/1750 train_time:54039ms step_avg:94.64ms
step:572/1750 train_time:54137ms step_avg:94.64ms
step:573/1750 train_time:54233ms step_avg:94.65ms
step:574/1750 train_time:54330ms step_avg:94.65ms
step:575/1750 train_time:54427ms step_avg:94.66ms
step:576/1750 train_time:54524ms step_avg:94.66ms
step:577/1750 train_time:54621ms step_avg:94.66ms
step:578/1750 train_time:54718ms step_avg:94.67ms
step:579/1750 train_time:54815ms step_avg:94.67ms
step:580/1750 train_time:54911ms step_avg:94.67ms
step:581/1750 train_time:55008ms step_avg:94.68ms
step:582/1750 train_time:55105ms step_avg:94.68ms
step:583/1750 train_time:55202ms step_avg:94.69ms
step:584/1750 train_time:55299ms step_avg:94.69ms
step:585/1750 train_time:55397ms step_avg:94.70ms
step:586/1750 train_time:55494ms step_avg:94.70ms
step:587/1750 train_time:55591ms step_avg:94.70ms
step:588/1750 train_time:55687ms step_avg:94.71ms
step:589/1750 train_time:55785ms step_avg:94.71ms
step:590/1750 train_time:55882ms step_avg:94.71ms
step:591/1750 train_time:55978ms step_avg:94.72ms
step:592/1750 train_time:56075ms step_avg:94.72ms
step:593/1750 train_time:56172ms step_avg:94.72ms
step:594/1750 train_time:56268ms step_avg:94.73ms
step:595/1750 train_time:56366ms step_avg:94.73ms
step:596/1750 train_time:56462ms step_avg:94.74ms
step:597/1750 train_time:56559ms step_avg:94.74ms
step:598/1750 train_time:56658ms step_avg:94.75ms
step:599/1750 train_time:56756ms step_avg:94.75ms
step:600/1750 train_time:56852ms step_avg:94.75ms
step:601/1750 train_time:56949ms step_avg:94.76ms
step:602/1750 train_time:57046ms step_avg:94.76ms
step:603/1750 train_time:57143ms step_avg:94.77ms
step:604/1750 train_time:57241ms step_avg:94.77ms
step:605/1750 train_time:57338ms step_avg:94.77ms
step:606/1750 train_time:57435ms step_avg:94.78ms
step:607/1750 train_time:57531ms step_avg:94.78ms
step:608/1750 train_time:57629ms step_avg:94.78ms
step:609/1750 train_time:57727ms step_avg:94.79ms
step:610/1750 train_time:57824ms step_avg:94.79ms
step:611/1750 train_time:57921ms step_avg:94.80ms
step:612/1750 train_time:58017ms step_avg:94.80ms
step:613/1750 train_time:58114ms step_avg:94.80ms
step:614/1750 train_time:58210ms step_avg:94.80ms
step:615/1750 train_time:58307ms step_avg:94.81ms
step:616/1750 train_time:58405ms step_avg:94.81ms
step:617/1750 train_time:58501ms step_avg:94.82ms
step:618/1750 train_time:58599ms step_avg:94.82ms
step:619/1750 train_time:58696ms step_avg:94.82ms
step:620/1750 train_time:58792ms step_avg:94.83ms
step:621/1750 train_time:58889ms step_avg:94.83ms
step:622/1750 train_time:58986ms step_avg:94.83ms
step:623/1750 train_time:59084ms step_avg:94.84ms
step:624/1750 train_time:59181ms step_avg:94.84ms
step:625/1750 train_time:59278ms step_avg:94.84ms
step:625/1750 val_loss:3.6650 train_time:59370ms step_avg:94.99ms
step:626/1750 train_time:59396ms step_avg:94.88ms
step:627/1750 train_time:59485ms step_avg:94.87ms
step:628/1750 train_time:59584ms step_avg:94.88ms
step:629/1750 train_time:59681ms step_avg:94.88ms
step:630/1750 train_time:59777ms step_avg:94.88ms
step:631/1750 train_time:59874ms step_avg:94.89ms
step:632/1750 train_time:59970ms step_avg:94.89ms
step:633/1750 train_time:60067ms step_avg:94.89ms
step:634/1750 train_time:60163ms step_avg:94.89ms
step:635/1750 train_time:60259ms step_avg:94.90ms
step:636/1750 train_time:60356ms step_avg:94.90ms
step:637/1750 train_time:60454ms step_avg:94.90ms
step:638/1750 train_time:60553ms step_avg:94.91ms
step:639/1750 train_time:60651ms step_avg:94.92ms
step:640/1750 train_time:60748ms step_avg:94.92ms
step:641/1750 train_time:60845ms step_avg:94.92ms
step:642/1750 train_time:60942ms step_avg:94.92ms
step:643/1750 train_time:61038ms step_avg:94.93ms
step:644/1750 train_time:61134ms step_avg:94.93ms
step:645/1750 train_time:61230ms step_avg:94.93ms
step:646/1750 train_time:61327ms step_avg:94.93ms
step:647/1750 train_time:61425ms step_avg:94.94ms
step:648/1750 train_time:61523ms step_avg:94.94ms
step:649/1750 train_time:61620ms step_avg:94.95ms
step:650/1750 train_time:61718ms step_avg:94.95ms
step:651/1750 train_time:61817ms step_avg:94.96ms
step:652/1750 train_time:61916ms step_avg:94.96ms
step:653/1750 train_time:62014ms step_avg:94.97ms
step:654/1750 train_time:62112ms step_avg:94.97ms
step:655/1750 train_time:62210ms step_avg:94.98ms
step:656/1750 train_time:62308ms step_avg:94.98ms
step:657/1750 train_time:62407ms step_avg:94.99ms
step:658/1750 train_time:62506ms step_avg:94.99ms
step:659/1750 train_time:62606ms step_avg:95.00ms
step:660/1750 train_time:62705ms step_avg:95.01ms
step:661/1750 train_time:62804ms step_avg:95.01ms
step:662/1750 train_time:62903ms step_avg:95.02ms
step:663/1750 train_time:63002ms step_avg:95.03ms
step:664/1750 train_time:63101ms step_avg:95.03ms
step:665/1750 train_time:63200ms step_avg:95.04ms
step:666/1750 train_time:63299ms step_avg:95.04ms
step:667/1750 train_time:63397ms step_avg:95.05ms
step:668/1750 train_time:63495ms step_avg:95.05ms
step:669/1750 train_time:63593ms step_avg:95.06ms
step:670/1750 train_time:63693ms step_avg:95.06ms
step:671/1750 train_time:63791ms step_avg:95.07ms
step:672/1750 train_time:63891ms step_avg:95.08ms
step:673/1750 train_time:63991ms step_avg:95.08ms
step:674/1750 train_time:64091ms step_avg:95.09ms
step:675/1750 train_time:64190ms step_avg:95.10ms
step:676/1750 train_time:64289ms step_avg:95.10ms
step:677/1750 train_time:64388ms step_avg:95.11ms
step:678/1750 train_time:64487ms step_avg:95.11ms
step:679/1750 train_time:64585ms step_avg:95.12ms
step:680/1750 train_time:64684ms step_avg:95.12ms
step:681/1750 train_time:64783ms step_avg:95.13ms
step:682/1750 train_time:64882ms step_avg:95.13ms
step:683/1750 train_time:64980ms step_avg:95.14ms
step:684/1750 train_time:65079ms step_avg:95.14ms
step:685/1750 train_time:65177ms step_avg:95.15ms
step:686/1750 train_time:65276ms step_avg:95.15ms
step:687/1750 train_time:65374ms step_avg:95.16ms
step:688/1750 train_time:65473ms step_avg:95.16ms
step:689/1750 train_time:65571ms step_avg:95.17ms
step:690/1750 train_time:65669ms step_avg:95.17ms
step:691/1750 train_time:65768ms step_avg:95.18ms
step:692/1750 train_time:65867ms step_avg:95.18ms
step:693/1750 train_time:65966ms step_avg:95.19ms
step:694/1750 train_time:66065ms step_avg:95.19ms
step:695/1750 train_time:66164ms step_avg:95.20ms
step:696/1750 train_time:66262ms step_avg:95.20ms
step:697/1750 train_time:66361ms step_avg:95.21ms
step:698/1750 train_time:66460ms step_avg:95.21ms
step:699/1750 train_time:66559ms step_avg:95.22ms
step:700/1750 train_time:66657ms step_avg:95.22ms
step:701/1750 train_time:66756ms step_avg:95.23ms
step:702/1750 train_time:66854ms step_avg:95.23ms
step:703/1750 train_time:66952ms step_avg:95.24ms
step:704/1750 train_time:67051ms step_avg:95.24ms
step:705/1750 train_time:67149ms step_avg:95.25ms
step:706/1750 train_time:67247ms step_avg:95.25ms
step:707/1750 train_time:67345ms step_avg:95.25ms
step:708/1750 train_time:67444ms step_avg:95.26ms
step:709/1750 train_time:67543ms step_avg:95.26ms
step:710/1750 train_time:67642ms step_avg:95.27ms
step:711/1750 train_time:67742ms step_avg:95.28ms
step:712/1750 train_time:67841ms step_avg:95.28ms
step:713/1750 train_time:67940ms step_avg:95.29ms
step:714/1750 train_time:68038ms step_avg:95.29ms
step:715/1750 train_time:68136ms step_avg:95.30ms
step:716/1750 train_time:68236ms step_avg:95.30ms
step:717/1750 train_time:68334ms step_avg:95.31ms
step:718/1750 train_time:68432ms step_avg:95.31ms
step:719/1750 train_time:68531ms step_avg:95.31ms
step:720/1750 train_time:68630ms step_avg:95.32ms
step:721/1750 train_time:68730ms step_avg:95.33ms
step:722/1750 train_time:68829ms step_avg:95.33ms
step:723/1750 train_time:68929ms step_avg:95.34ms
step:724/1750 train_time:69029ms step_avg:95.34ms
step:725/1750 train_time:69128ms step_avg:95.35ms
step:726/1750 train_time:69227ms step_avg:95.35ms
step:727/1750 train_time:69326ms step_avg:95.36ms
step:728/1750 train_time:69425ms step_avg:95.36ms
step:729/1750 train_time:69523ms step_avg:95.37ms
step:730/1750 train_time:69622ms step_avg:95.37ms
step:731/1750 train_time:69720ms step_avg:95.38ms
step:732/1750 train_time:69818ms step_avg:95.38ms
step:733/1750 train_time:69917ms step_avg:95.38ms
step:734/1750 train_time:70015ms step_avg:95.39ms
step:735/1750 train_time:70113ms step_avg:95.39ms
step:736/1750 train_time:70212ms step_avg:95.40ms
step:737/1750 train_time:70310ms step_avg:95.40ms
step:738/1750 train_time:70409ms step_avg:95.41ms
step:739/1750 train_time:70508ms step_avg:95.41ms
step:740/1750 train_time:70607ms step_avg:95.41ms
step:741/1750 train_time:70706ms step_avg:95.42ms
step:742/1750 train_time:70805ms step_avg:95.42ms
step:743/1750 train_time:70904ms step_avg:95.43ms
step:744/1750 train_time:71003ms step_avg:95.43ms
step:745/1750 train_time:71103ms step_avg:95.44ms
step:746/1750 train_time:71203ms step_avg:95.45ms
step:747/1750 train_time:71301ms step_avg:95.45ms
step:748/1750 train_time:71400ms step_avg:95.45ms
step:749/1750 train_time:71499ms step_avg:95.46ms
step:750/1750 train_time:71597ms step_avg:95.46ms
step:750/1750 val_loss:3.5989 train_time:71692ms step_avg:95.59ms
step:751/1750 train_time:71719ms step_avg:95.50ms
step:752/1750 train_time:71805ms step_avg:95.49ms
step:753/1750 train_time:71907ms step_avg:95.49ms
step:754/1750 train_time:72006ms step_avg:95.50ms
step:755/1750 train_time:72104ms step_avg:95.50ms
step:756/1750 train_time:72202ms step_avg:95.51ms
step:757/1750 train_time:72300ms step_avg:95.51ms
step:758/1750 train_time:72398ms step_avg:95.51ms
step:759/1750 train_time:72496ms step_avg:95.52ms
step:760/1750 train_time:72594ms step_avg:95.52ms
step:761/1750 train_time:72692ms step_avg:95.52ms
step:762/1750 train_time:72792ms step_avg:95.53ms
step:763/1750 train_time:72892ms step_avg:95.53ms
step:764/1750 train_time:72992ms step_avg:95.54ms
step:765/1750 train_time:73092ms step_avg:95.55ms
step:766/1750 train_time:73192ms step_avg:95.55ms
step:767/1750 train_time:73291ms step_avg:95.56ms
step:768/1750 train_time:73390ms step_avg:95.56ms
step:769/1750 train_time:73489ms step_avg:95.56ms
step:770/1750 train_time:73587ms step_avg:95.57ms
step:771/1750 train_time:73685ms step_avg:95.57ms
step:772/1750 train_time:73783ms step_avg:95.57ms
step:773/1750 train_time:73881ms step_avg:95.58ms
step:774/1750 train_time:73981ms step_avg:95.58ms
step:775/1750 train_time:74080ms step_avg:95.59ms
step:776/1750 train_time:74179ms step_avg:95.59ms
step:777/1750 train_time:74279ms step_avg:95.60ms
step:778/1750 train_time:74377ms step_avg:95.60ms
step:779/1750 train_time:74475ms step_avg:95.60ms
step:780/1750 train_time:74574ms step_avg:95.61ms
step:781/1750 train_time:74673ms step_avg:95.61ms
step:782/1750 train_time:74771ms step_avg:95.62ms
step:783/1750 train_time:74870ms step_avg:95.62ms
step:784/1750 train_time:74969ms step_avg:95.62ms
step:785/1750 train_time:75068ms step_avg:95.63ms
step:786/1750 train_time:75167ms step_avg:95.63ms
step:787/1750 train_time:75266ms step_avg:95.64ms
step:788/1750 train_time:75364ms step_avg:95.64ms
step:789/1750 train_time:75463ms step_avg:95.64ms
step:790/1750 train_time:75562ms step_avg:95.65ms
step:791/1750 train_time:75661ms step_avg:95.65ms
step:792/1750 train_time:75760ms step_avg:95.66ms
step:793/1750 train_time:75859ms step_avg:95.66ms
step:794/1750 train_time:75958ms step_avg:95.66ms
step:795/1750 train_time:76058ms step_avg:95.67ms
step:796/1750 train_time:76158ms step_avg:95.68ms
step:797/1750 train_time:76258ms step_avg:95.68ms
step:798/1750 train_time:76358ms step_avg:95.69ms
step:799/1750 train_time:76458ms step_avg:95.69ms
step:800/1750 train_time:76558ms step_avg:95.70ms
step:801/1750 train_time:76657ms step_avg:95.70ms
step:802/1750 train_time:76756ms step_avg:95.71ms
step:803/1750 train_time:76856ms step_avg:95.71ms
step:804/1750 train_time:76955ms step_avg:95.71ms
step:805/1750 train_time:77054ms step_avg:95.72ms
step:806/1750 train_time:77153ms step_avg:95.72ms
step:807/1750 train_time:77252ms step_avg:95.73ms
step:808/1750 train_time:77351ms step_avg:95.73ms
step:809/1750 train_time:77451ms step_avg:95.74ms
step:810/1750 train_time:77550ms step_avg:95.74ms
step:811/1750 train_time:77649ms step_avg:95.74ms
step:812/1750 train_time:77748ms step_avg:95.75ms
step:813/1750 train_time:77847ms step_avg:95.75ms
step:814/1750 train_time:77945ms step_avg:95.76ms
step:815/1750 train_time:78043ms step_avg:95.76ms
step:816/1750 train_time:78141ms step_avg:95.76ms
step:817/1750 train_time:78240ms step_avg:95.77ms
step:818/1750 train_time:78339ms step_avg:95.77ms
step:819/1750 train_time:78438ms step_avg:95.77ms
step:820/1750 train_time:78537ms step_avg:95.78ms
step:821/1750 train_time:78636ms step_avg:95.78ms
step:822/1750 train_time:78735ms step_avg:95.79ms
step:823/1750 train_time:78835ms step_avg:95.79ms
step:824/1750 train_time:78934ms step_avg:95.79ms
step:825/1750 train_time:79033ms step_avg:95.80ms
step:826/1750 train_time:79132ms step_avg:95.80ms
step:827/1750 train_time:79231ms step_avg:95.81ms
step:828/1750 train_time:79330ms step_avg:95.81ms
step:829/1750 train_time:79429ms step_avg:95.81ms
step:830/1750 train_time:79528ms step_avg:95.82ms
step:831/1750 train_time:79626ms step_avg:95.82ms
step:832/1750 train_time:79725ms step_avg:95.82ms
step:833/1750 train_time:79823ms step_avg:95.83ms
step:834/1750 train_time:79922ms step_avg:95.83ms
step:835/1750 train_time:80020ms step_avg:95.83ms
step:836/1750 train_time:80120ms step_avg:95.84ms
step:837/1750 train_time:80218ms step_avg:95.84ms
step:838/1750 train_time:80318ms step_avg:95.84ms
step:839/1750 train_time:80417ms step_avg:95.85ms
step:840/1750 train_time:80517ms step_avg:95.85ms
step:841/1750 train_time:80617ms step_avg:95.86ms
step:842/1750 train_time:80717ms step_avg:95.86ms
step:843/1750 train_time:80816ms step_avg:95.87ms
step:844/1750 train_time:80916ms step_avg:95.87ms
step:845/1750 train_time:81015ms step_avg:95.88ms
step:846/1750 train_time:81114ms step_avg:95.88ms
step:847/1750 train_time:81214ms step_avg:95.88ms
step:848/1750 train_time:81313ms step_avg:95.89ms
step:849/1750 train_time:81412ms step_avg:95.89ms
step:850/1750 train_time:81511ms step_avg:95.90ms
step:851/1750 train_time:81611ms step_avg:95.90ms
step:852/1750 train_time:81710ms step_avg:95.90ms
step:853/1750 train_time:81809ms step_avg:95.91ms
step:854/1750 train_time:81908ms step_avg:95.91ms
step:855/1750 train_time:82006ms step_avg:95.91ms
step:856/1750 train_time:82104ms step_avg:95.92ms
step:857/1750 train_time:82203ms step_avg:95.92ms
step:858/1750 train_time:82303ms step_avg:95.92ms
step:859/1750 train_time:82402ms step_avg:95.93ms
step:860/1750 train_time:82501ms step_avg:95.93ms
step:861/1750 train_time:82599ms step_avg:95.93ms
step:862/1750 train_time:82698ms step_avg:95.94ms
step:863/1750 train_time:82798ms step_avg:95.94ms
step:864/1750 train_time:82896ms step_avg:95.94ms
step:865/1750 train_time:82996ms step_avg:95.95ms
step:866/1750 train_time:83095ms step_avg:95.95ms
step:867/1750 train_time:83194ms step_avg:95.96ms
step:868/1750 train_time:83293ms step_avg:95.96ms
step:869/1750 train_time:83392ms step_avg:95.96ms
step:870/1750 train_time:83492ms step_avg:95.97ms
step:871/1750 train_time:83592ms step_avg:95.97ms
step:872/1750 train_time:83692ms step_avg:95.98ms
step:873/1750 train_time:83792ms step_avg:95.98ms
step:874/1750 train_time:83891ms step_avg:95.99ms
step:875/1750 train_time:83990ms step_avg:95.99ms
step:875/1750 val_loss:3.5487 train_time:84083ms step_avg:96.09ms
step:876/1750 train_time:84109ms step_avg:96.02ms
step:877/1750 train_time:84196ms step_avg:96.00ms
step:878/1750 train_time:84297ms step_avg:96.01ms
step:879/1750 train_time:84396ms step_avg:96.01ms
step:880/1750 train_time:84494ms step_avg:96.02ms
step:881/1750 train_time:84592ms step_avg:96.02ms
step:882/1750 train_time:84690ms step_avg:96.02ms
step:883/1750 train_time:84788ms step_avg:96.02ms
step:884/1750 train_time:84886ms step_avg:96.03ms
step:885/1750 train_time:84984ms step_avg:96.03ms
step:886/1750 train_time:85083ms step_avg:96.03ms
step:887/1750 train_time:85182ms step_avg:96.03ms
step:888/1750 train_time:85282ms step_avg:96.04ms
step:889/1750 train_time:85383ms step_avg:96.04ms
step:890/1750 train_time:85484ms step_avg:96.05ms
step:891/1750 train_time:85583ms step_avg:96.05ms
step:892/1750 train_time:85682ms step_avg:96.06ms
step:893/1750 train_time:85782ms step_avg:96.06ms
step:894/1750 train_time:85879ms step_avg:96.06ms
step:895/1750 train_time:85977ms step_avg:96.06ms
step:896/1750 train_time:86076ms step_avg:96.07ms
step:897/1750 train_time:86175ms step_avg:96.07ms
step:898/1750 train_time:86275ms step_avg:96.07ms
step:899/1750 train_time:86375ms step_avg:96.08ms
step:900/1750 train_time:86474ms step_avg:96.08ms
step:901/1750 train_time:86574ms step_avg:96.09ms
step:902/1750 train_time:86673ms step_avg:96.09ms
step:903/1750 train_time:86772ms step_avg:96.09ms
step:904/1750 train_time:86871ms step_avg:96.10ms
step:905/1750 train_time:86971ms step_avg:96.10ms
step:906/1750 train_time:87068ms step_avg:96.10ms
step:907/1750 train_time:87168ms step_avg:96.11ms
step:908/1750 train_time:87266ms step_avg:96.11ms
step:909/1750 train_time:87366ms step_avg:96.11ms
step:910/1750 train_time:87466ms step_avg:96.12ms
step:911/1750 train_time:87567ms step_avg:96.12ms
step:912/1750 train_time:87668ms step_avg:96.13ms
step:913/1750 train_time:87769ms step_avg:96.13ms
step:914/1750 train_time:87869ms step_avg:96.14ms
step:915/1750 train_time:87969ms step_avg:96.14ms
step:916/1750 train_time:88069ms step_avg:96.14ms
step:917/1750 train_time:88169ms step_avg:96.15ms
step:918/1750 train_time:88270ms step_avg:96.15ms
step:919/1750 train_time:88370ms step_avg:96.16ms
step:920/1750 train_time:88471ms step_avg:96.16ms
step:921/1750 train_time:88571ms step_avg:96.17ms
step:922/1750 train_time:88671ms step_avg:96.17ms
step:923/1750 train_time:88771ms step_avg:96.18ms
step:924/1750 train_time:88870ms step_avg:96.18ms
step:925/1750 train_time:88970ms step_avg:96.18ms
step:926/1750 train_time:89070ms step_avg:96.19ms
step:927/1750 train_time:89169ms step_avg:96.19ms
step:928/1750 train_time:89269ms step_avg:96.20ms
step:929/1750 train_time:89369ms step_avg:96.20ms
step:930/1750 train_time:89469ms step_avg:96.20ms
step:931/1750 train_time:89570ms step_avg:96.21ms
step:932/1750 train_time:89670ms step_avg:96.21ms
step:933/1750 train_time:89771ms step_avg:96.22ms
step:934/1750 train_time:89871ms step_avg:96.22ms
step:935/1750 train_time:89971ms step_avg:96.23ms
step:936/1750 train_time:90071ms step_avg:96.23ms
step:937/1750 train_time:90171ms step_avg:96.23ms
step:938/1750 train_time:90271ms step_avg:96.24ms
step:939/1750 train_time:90372ms step_avg:96.24ms
step:940/1750 train_time:90472ms step_avg:96.25ms
step:941/1750 train_time:90572ms step_avg:96.25ms
step:942/1750 train_time:90672ms step_avg:96.26ms
step:943/1750 train_time:90773ms step_avg:96.26ms
step:944/1750 train_time:90872ms step_avg:96.26ms
step:945/1750 train_time:90972ms step_avg:96.27ms
step:946/1750 train_time:91071ms step_avg:96.27ms
step:947/1750 train_time:91172ms step_avg:96.27ms
step:948/1750 train_time:91272ms step_avg:96.28ms
step:949/1750 train_time:91373ms step_avg:96.28ms
step:950/1750 train_time:91472ms step_avg:96.29ms
step:951/1750 train_time:91573ms step_avg:96.29ms
step:952/1750 train_time:91673ms step_avg:96.29ms
step:953/1750 train_time:91772ms step_avg:96.30ms
step:954/1750 train_time:91872ms step_avg:96.30ms
step:955/1750 train_time:91972ms step_avg:96.31ms
step:956/1750 train_time:92072ms step_avg:96.31ms
step:957/1750 train_time:92173ms step_avg:96.31ms
step:958/1750 train_time:92272ms step_avg:96.32ms
step:959/1750 train_time:92372ms step_avg:96.32ms
step:960/1750 train_time:92472ms step_avg:96.33ms
step:961/1750 train_time:92573ms step_avg:96.33ms
step:962/1750 train_time:92673ms step_avg:96.33ms
step:963/1750 train_time:92772ms step_avg:96.34ms
step:964/1750 train_time:92873ms step_avg:96.34ms
step:965/1750 train_time:92973ms step_avg:96.35ms
step:966/1750 train_time:93073ms step_avg:96.35ms
step:967/1750 train_time:93173ms step_avg:96.35ms
step:968/1750 train_time:93272ms step_avg:96.36ms
step:969/1750 train_time:93373ms step_avg:96.36ms
step:970/1750 train_time:93472ms step_avg:96.36ms
step:971/1750 train_time:93572ms step_avg:96.37ms
step:972/1750 train_time:93672ms step_avg:96.37ms
step:973/1750 train_time:93772ms step_avg:96.37ms
step:974/1750 train_time:93871ms step_avg:96.38ms
step:975/1750 train_time:93971ms step_avg:96.38ms
step:976/1750 train_time:94070ms step_avg:96.38ms
step:977/1750 train_time:94170ms step_avg:96.39ms
step:978/1750 train_time:94271ms step_avg:96.39ms
step:979/1750 train_time:94372ms step_avg:96.40ms
step:980/1750 train_time:94471ms step_avg:96.40ms
step:981/1750 train_time:94571ms step_avg:96.40ms
step:982/1750 train_time:94671ms step_avg:96.41ms
step:983/1750 train_time:94772ms step_avg:96.41ms
step:984/1750 train_time:94871ms step_avg:96.41ms
step:985/1750 train_time:94971ms step_avg:96.42ms
step:986/1750 train_time:95072ms step_avg:96.42ms
step:987/1750 train_time:95172ms step_avg:96.43ms
step:988/1750 train_time:95272ms step_avg:96.43ms
step:989/1750 train_time:95372ms step_avg:96.43ms
step:990/1750 train_time:95472ms step_avg:96.44ms
step:991/1750 train_time:95572ms step_avg:96.44ms
step:992/1750 train_time:95672ms step_avg:96.44ms
step:993/1750 train_time:95772ms step_avg:96.45ms
step:994/1750 train_time:95872ms step_avg:96.45ms
step:995/1750 train_time:95972ms step_avg:96.45ms
step:996/1750 train_time:96072ms step_avg:96.46ms
step:997/1750 train_time:96173ms step_avg:96.46ms
step:998/1750 train_time:96272ms step_avg:96.47ms
step:999/1750 train_time:96373ms step_avg:96.47ms
step:1000/1750 train_time:96472ms step_avg:96.47ms
step:1000/1750 val_loss:3.5085 train_time:96567ms step_avg:96.57ms
step:1001/1750 train_time:96594ms step_avg:96.50ms
step:1002/1750 train_time:96683ms step_avg:96.49ms
step:1003/1750 train_time:96784ms step_avg:96.49ms
step:1004/1750 train_time:96884ms step_avg:96.50ms
step:1005/1750 train_time:96983ms step_avg:96.50ms
step:1006/1750 train_time:97083ms step_avg:96.50ms
step:1007/1750 train_time:97183ms step_avg:96.51ms
step:1008/1750 train_time:97282ms step_avg:96.51ms
step:1009/1750 train_time:97383ms step_avg:96.51ms
step:1010/1750 train_time:97482ms step_avg:96.52ms
step:1011/1750 train_time:97584ms step_avg:96.52ms
step:1012/1750 train_time:97686ms step_avg:96.53ms
step:1013/1750 train_time:97787ms step_avg:96.53ms
step:1014/1750 train_time:97888ms step_avg:96.54ms
step:1015/1750 train_time:97988ms step_avg:96.54ms
step:1016/1750 train_time:98087ms step_avg:96.54ms
step:1017/1750 train_time:98186ms step_avg:96.55ms
step:1018/1750 train_time:98287ms step_avg:96.55ms
step:1019/1750 train_time:98387ms step_avg:96.55ms
step:1020/1750 train_time:98488ms step_avg:96.56ms
step:1021/1750 train_time:98587ms step_avg:96.56ms
step:1022/1750 train_time:98687ms step_avg:96.56ms
step:1023/1750 train_time:98788ms step_avg:96.57ms
step:1024/1750 train_time:98890ms step_avg:96.57ms
step:1025/1750 train_time:98990ms step_avg:96.58ms
step:1026/1750 train_time:99089ms step_avg:96.58ms
step:1027/1750 train_time:99189ms step_avg:96.58ms
step:1028/1750 train_time:99289ms step_avg:96.59ms
step:1029/1750 train_time:99391ms step_avg:96.59ms
step:1030/1750 train_time:99492ms step_avg:96.59ms
step:1031/1750 train_time:99594ms step_avg:96.60ms
step:1032/1750 train_time:99694ms step_avg:96.60ms
step:1033/1750 train_time:99794ms step_avg:96.61ms
step:1034/1750 train_time:99895ms step_avg:96.61ms
step:1035/1750 train_time:99996ms step_avg:96.61ms
step:1036/1750 train_time:100096ms step_avg:96.62ms
step:1037/1750 train_time:100198ms step_avg:96.62ms
step:1038/1750 train_time:100299ms step_avg:96.63ms
step:1039/1750 train_time:100400ms step_avg:96.63ms
step:1040/1750 train_time:100501ms step_avg:96.64ms
step:1041/1750 train_time:100602ms step_avg:96.64ms
step:1042/1750 train_time:100702ms step_avg:96.64ms
step:1043/1750 train_time:100803ms step_avg:96.65ms
step:1044/1750 train_time:100903ms step_avg:96.65ms
step:1045/1750 train_time:101004ms step_avg:96.65ms
step:1046/1750 train_time:101105ms step_avg:96.66ms
step:1047/1750 train_time:101206ms step_avg:96.66ms
step:1048/1750 train_time:101306ms step_avg:96.67ms
step:1049/1750 train_time:101407ms step_avg:96.67ms
step:1050/1750 train_time:101507ms step_avg:96.67ms
step:1051/1750 train_time:101607ms step_avg:96.68ms
step:1052/1750 train_time:101707ms step_avg:96.68ms
step:1053/1750 train_time:101807ms step_avg:96.68ms
step:1054/1750 train_time:101907ms step_avg:96.69ms
step:1055/1750 train_time:102008ms step_avg:96.69ms
step:1056/1750 train_time:102108ms step_avg:96.69ms
step:1057/1750 train_time:102209ms step_avg:96.70ms
step:1058/1750 train_time:102311ms step_avg:96.70ms
step:1059/1750 train_time:102413ms step_avg:96.71ms
step:1060/1750 train_time:102514ms step_avg:96.71ms
step:1061/1750 train_time:102614ms step_avg:96.71ms
step:1062/1750 train_time:102715ms step_avg:96.72ms
step:1063/1750 train_time:102816ms step_avg:96.72ms
step:1064/1750 train_time:102916ms step_avg:96.73ms
step:1065/1750 train_time:103017ms step_avg:96.73ms
step:1066/1750 train_time:103119ms step_avg:96.73ms
step:1067/1750 train_time:103220ms step_avg:96.74ms
step:1068/1750 train_time:103323ms step_avg:96.74ms
step:1069/1750 train_time:103424ms step_avg:96.75ms
step:1070/1750 train_time:103524ms step_avg:96.75ms
step:1071/1750 train_time:103624ms step_avg:96.75ms
step:1072/1750 train_time:103724ms step_avg:96.76ms
step:1073/1750 train_time:103824ms step_avg:96.76ms
step:1074/1750 train_time:103924ms step_avg:96.76ms
step:1075/1750 train_time:104025ms step_avg:96.77ms
step:1076/1750 train_time:104125ms step_avg:96.77ms
step:1077/1750 train_time:104226ms step_avg:96.77ms
step:1078/1750 train_time:104325ms step_avg:96.78ms
step:1079/1750 train_time:104426ms step_avg:96.78ms
step:1080/1750 train_time:104526ms step_avg:96.78ms
step:1081/1750 train_time:104626ms step_avg:96.79ms
step:1082/1750 train_time:104727ms step_avg:96.79ms
step:1083/1750 train_time:104827ms step_avg:96.79ms
step:1084/1750 train_time:104927ms step_avg:96.80ms
step:1085/1750 train_time:105027ms step_avg:96.80ms
step:1086/1750 train_time:105127ms step_avg:96.80ms
step:1087/1750 train_time:105227ms step_avg:96.80ms
step:1088/1750 train_time:105327ms step_avg:96.81ms
step:1089/1750 train_time:105426ms step_avg:96.81ms
step:1090/1750 train_time:105527ms step_avg:96.81ms
step:1091/1750 train_time:105627ms step_avg:96.82ms
step:1092/1750 train_time:105727ms step_avg:96.82ms
step:1093/1750 train_time:105828ms step_avg:96.82ms
step:1094/1750 train_time:105929ms step_avg:96.83ms
step:1095/1750 train_time:106028ms step_avg:96.83ms
step:1096/1750 train_time:106129ms step_avg:96.83ms
step:1097/1750 train_time:106231ms step_avg:96.84ms
step:1098/1750 train_time:106332ms step_avg:96.84ms
step:1099/1750 train_time:106432ms step_avg:96.84ms
step:1100/1750 train_time:106533ms step_avg:96.85ms
step:1101/1750 train_time:106634ms step_avg:96.85ms
step:1102/1750 train_time:106734ms step_avg:96.85ms
step:1103/1750 train_time:106835ms step_avg:96.86ms
step:1104/1750 train_time:106936ms step_avg:96.86ms
step:1105/1750 train_time:107036ms step_avg:96.86ms
step:1106/1750 train_time:107138ms step_avg:96.87ms
step:1107/1750 train_time:107240ms step_avg:96.87ms
step:1108/1750 train_time:107342ms step_avg:96.88ms
step:1109/1750 train_time:107443ms step_avg:96.88ms
step:1110/1750 train_time:107543ms step_avg:96.89ms
step:1111/1750 train_time:107642ms step_avg:96.89ms
step:1112/1750 train_time:107743ms step_avg:96.89ms
step:1113/1750 train_time:107843ms step_avg:96.89ms
step:1114/1750 train_time:107945ms step_avg:96.90ms
step:1115/1750 train_time:108045ms step_avg:96.90ms
step:1116/1750 train_time:108146ms step_avg:96.91ms
step:1117/1750 train_time:108246ms step_avg:96.91ms
step:1118/1750 train_time:108347ms step_avg:96.91ms
step:1119/1750 train_time:108446ms step_avg:96.91ms
step:1120/1750 train_time:108546ms step_avg:96.92ms
step:1121/1750 train_time:108646ms step_avg:96.92ms
step:1122/1750 train_time:108747ms step_avg:96.92ms
step:1123/1750 train_time:108847ms step_avg:96.93ms
step:1124/1750 train_time:108946ms step_avg:96.93ms
step:1125/1750 train_time:109047ms step_avg:96.93ms
step:1125/1750 val_loss:3.4560 train_time:109141ms step_avg:97.01ms
step:1126/1750 train_time:109168ms step_avg:96.95ms
step:1127/1750 train_time:109257ms step_avg:96.95ms
step:1128/1750 train_time:109358ms step_avg:96.95ms
step:1129/1750 train_time:109458ms step_avg:96.95ms
step:1130/1750 train_time:109558ms step_avg:96.95ms
step:1131/1750 train_time:109658ms step_avg:96.96ms
step:1132/1750 train_time:109758ms step_avg:96.96ms
step:1133/1750 train_time:109858ms step_avg:96.96ms
step:1134/1750 train_time:109957ms step_avg:96.96ms
step:1135/1750 train_time:110057ms step_avg:96.97ms
step:1136/1750 train_time:110159ms step_avg:96.97ms
step:1137/1750 train_time:110260ms step_avg:96.97ms
step:1138/1750 train_time:110361ms step_avg:96.98ms
step:1139/1750 train_time:110461ms step_avg:96.98ms
step:1140/1750 train_time:110560ms step_avg:96.98ms
step:1141/1750 train_time:110660ms step_avg:96.99ms
step:1142/1750 train_time:110760ms step_avg:96.99ms
step:1143/1750 train_time:110860ms step_avg:96.99ms
step:1144/1750 train_time:110960ms step_avg:96.99ms
step:1145/1750 train_time:111061ms step_avg:97.00ms
step:1146/1750 train_time:111162ms step_avg:97.00ms
step:1147/1750 train_time:111262ms step_avg:97.00ms
step:1148/1750 train_time:111364ms step_avg:97.01ms
step:1149/1750 train_time:111464ms step_avg:97.01ms
step:1150/1750 train_time:111565ms step_avg:97.01ms
step:1151/1750 train_time:111666ms step_avg:97.02ms
step:1152/1750 train_time:111766ms step_avg:97.02ms
step:1153/1750 train_time:111867ms step_avg:97.02ms
step:1154/1750 train_time:111968ms step_avg:97.03ms
step:1155/1750 train_time:112068ms step_avg:97.03ms
step:1156/1750 train_time:112169ms step_avg:97.03ms
step:1157/1750 train_time:112271ms step_avg:97.04ms
step:1158/1750 train_time:112372ms step_avg:97.04ms
step:1159/1750 train_time:112473ms step_avg:97.04ms
step:1160/1750 train_time:112575ms step_avg:97.05ms
step:1161/1750 train_time:112676ms step_avg:97.05ms
step:1162/1750 train_time:112778ms step_avg:97.05ms
step:1163/1750 train_time:112880ms step_avg:97.06ms
step:1164/1750 train_time:112981ms step_avg:97.06ms
step:1165/1750 train_time:113081ms step_avg:97.07ms
step:1166/1750 train_time:113181ms step_avg:97.07ms
step:1167/1750 train_time:113282ms step_avg:97.07ms
step:1168/1750 train_time:113382ms step_avg:97.07ms
step:1169/1750 train_time:113485ms step_avg:97.08ms
step:1170/1750 train_time:113588ms step_avg:97.08ms
step:1171/1750 train_time:113690ms step_avg:97.09ms
step:1172/1750 train_time:113793ms step_avg:97.09ms
step:1173/1750 train_time:113895ms step_avg:97.10ms
step:1174/1750 train_time:113997ms step_avg:97.10ms
step:1175/1750 train_time:114098ms step_avg:97.11ms
step:1176/1750 train_time:114201ms step_avg:97.11ms
step:1177/1750 train_time:114302ms step_avg:97.11ms
step:1178/1750 train_time:114404ms step_avg:97.12ms
step:1179/1750 train_time:114507ms step_avg:97.12ms
step:1180/1750 train_time:114609ms step_avg:97.13ms
step:1181/1750 train_time:114712ms step_avg:97.13ms
step:1182/1750 train_time:114814ms step_avg:97.14ms
step:1183/1750 train_time:114916ms step_avg:97.14ms
step:1184/1750 train_time:115019ms step_avg:97.14ms
step:1185/1750 train_time:115121ms step_avg:97.15ms
step:1186/1750 train_time:115222ms step_avg:97.15ms
step:1187/1750 train_time:115323ms step_avg:97.15ms
step:1188/1750 train_time:115425ms step_avg:97.16ms
step:1189/1750 train_time:115526ms step_avg:97.16ms
step:1190/1750 train_time:115628ms step_avg:97.17ms
step:1191/1750 train_time:115731ms step_avg:97.17ms
step:1192/1750 train_time:115833ms step_avg:97.18ms
step:1193/1750 train_time:115936ms step_avg:97.18ms
step:1194/1750 train_time:116037ms step_avg:97.18ms
step:1195/1750 train_time:116140ms step_avg:97.19ms
step:1196/1750 train_time:116241ms step_avg:97.19ms
step:1197/1750 train_time:116343ms step_avg:97.20ms
step:1198/1750 train_time:116444ms step_avg:97.20ms
step:1199/1750 train_time:116546ms step_avg:97.20ms
step:1200/1750 train_time:116648ms step_avg:97.21ms
step:1201/1750 train_time:116750ms step_avg:97.21ms
step:1202/1750 train_time:116853ms step_avg:97.22ms
step:1203/1750 train_time:116955ms step_avg:97.22ms
step:1204/1750 train_time:117058ms step_avg:97.22ms
step:1205/1750 train_time:117160ms step_avg:97.23ms
step:1206/1750 train_time:117261ms step_avg:97.23ms
step:1207/1750 train_time:117363ms step_avg:97.24ms
step:1208/1750 train_time:117465ms step_avg:97.24ms
step:1209/1750 train_time:117567ms step_avg:97.24ms
step:1210/1750 train_time:117669ms step_avg:97.25ms
step:1211/1750 train_time:117771ms step_avg:97.25ms
step:1212/1750 train_time:117874ms step_avg:97.26ms
step:1213/1750 train_time:117975ms step_avg:97.26ms
step:1214/1750 train_time:118077ms step_avg:97.26ms
step:1215/1750 train_time:118180ms step_avg:97.27ms
step:1216/1750 train_time:118282ms step_avg:97.27ms
step:1217/1750 train_time:118383ms step_avg:97.27ms
step:1218/1750 train_time:118485ms step_avg:97.28ms
step:1219/1750 train_time:118587ms step_avg:97.28ms
step:1220/1750 train_time:118689ms step_avg:97.29ms
step:1221/1750 train_time:118792ms step_avg:97.29ms
step:1222/1750 train_time:118894ms step_avg:97.29ms
step:1223/1750 train_time:118997ms step_avg:97.30ms
step:1224/1750 train_time:119099ms step_avg:97.30ms
step:1225/1750 train_time:119201ms step_avg:97.31ms
step:1226/1750 train_time:119302ms step_avg:97.31ms
step:1227/1750 train_time:119404ms step_avg:97.31ms
step:1228/1750 train_time:119505ms step_avg:97.32ms
step:1229/1750 train_time:119606ms step_avg:97.32ms
step:1230/1750 train_time:119708ms step_avg:97.32ms
step:1231/1750 train_time:119811ms step_avg:97.33ms
step:1232/1750 train_time:119915ms step_avg:97.33ms
step:1233/1750 train_time:120016ms step_avg:97.34ms
step:1234/1750 train_time:120119ms step_avg:97.34ms
step:1235/1750 train_time:120220ms step_avg:97.34ms
step:1236/1750 train_time:120323ms step_avg:97.35ms
step:1237/1750 train_time:120424ms step_avg:97.35ms
step:1238/1750 train_time:120526ms step_avg:97.36ms
step:1239/1750 train_time:120628ms step_avg:97.36ms
step:1240/1750 train_time:120730ms step_avg:97.36ms
step:1241/1750 train_time:120832ms step_avg:97.37ms
step:1242/1750 train_time:120933ms step_avg:97.37ms
step:1243/1750 train_time:121036ms step_avg:97.37ms
step:1244/1750 train_time:121138ms step_avg:97.38ms
step:1245/1750 train_time:121239ms step_avg:97.38ms
step:1246/1750 train_time:121341ms step_avg:97.38ms
step:1247/1750 train_time:121442ms step_avg:97.39ms
step:1248/1750 train_time:121543ms step_avg:97.39ms
step:1249/1750 train_time:121645ms step_avg:97.39ms
step:1250/1750 train_time:121747ms step_avg:97.40ms
step:1250/1750 val_loss:3.4083 train_time:121845ms step_avg:97.48ms
step:1251/1750 train_time:121871ms step_avg:97.42ms
step:1252/1750 train_time:121959ms step_avg:97.41ms
step:1253/1750 train_time:122061ms step_avg:97.42ms
step:1254/1750 train_time:122164ms step_avg:97.42ms
step:1255/1750 train_time:122265ms step_avg:97.42ms
step:1256/1750 train_time:122365ms step_avg:97.42ms
step:1257/1750 train_time:122467ms step_avg:97.43ms
step:1258/1750 train_time:122568ms step_avg:97.43ms
step:1259/1750 train_time:122668ms step_avg:97.43ms
step:1260/1750 train_time:122770ms step_avg:97.44ms
step:1261/1750 train_time:122873ms step_avg:97.44ms
step:1262/1750 train_time:122976ms step_avg:97.45ms
step:1263/1750 train_time:123080ms step_avg:97.45ms
step:1264/1750 train_time:123181ms step_avg:97.45ms
step:1265/1750 train_time:123282ms step_avg:97.46ms
step:1266/1750 train_time:123383ms step_avg:97.46ms
step:1267/1750 train_time:123484ms step_avg:97.46ms
step:1268/1750 train_time:123585ms step_avg:97.46ms
step:1269/1750 train_time:123686ms step_avg:97.47ms
step:1270/1750 train_time:123790ms step_avg:97.47ms
step:1271/1750 train_time:123896ms step_avg:97.48ms
step:1272/1750 train_time:123997ms step_avg:97.48ms
step:1273/1750 train_time:124099ms step_avg:97.49ms
step:1274/1750 train_time:124200ms step_avg:97.49ms
step:1275/1750 train_time:124302ms step_avg:97.49ms
step:1276/1750 train_time:124404ms step_avg:97.50ms
step:1277/1750 train_time:124505ms step_avg:97.50ms
step:1278/1750 train_time:124606ms step_avg:97.50ms
step:1279/1750 train_time:124709ms step_avg:97.50ms
step:1280/1750 train_time:124810ms step_avg:97.51ms
step:1281/1750 train_time:124913ms step_avg:97.51ms
step:1282/1750 train_time:125015ms step_avg:97.52ms
step:1283/1750 train_time:125117ms step_avg:97.52ms
step:1284/1750 train_time:125218ms step_avg:97.52ms
step:1285/1750 train_time:125320ms step_avg:97.52ms
step:1286/1750 train_time:125421ms step_avg:97.53ms
step:1287/1750 train_time:125523ms step_avg:97.53ms
step:1288/1750 train_time:125625ms step_avg:97.53ms
step:1289/1750 train_time:125727ms step_avg:97.54ms
step:1290/1750 train_time:125829ms step_avg:97.54ms
step:1291/1750 train_time:125931ms step_avg:97.54ms
step:1292/1750 train_time:126033ms step_avg:97.55ms
step:1293/1750 train_time:126136ms step_avg:97.55ms
step:1294/1750 train_time:126239ms step_avg:97.56ms
step:1295/1750 train_time:126340ms step_avg:97.56ms
step:1296/1750 train_time:126441ms step_avg:97.56ms
step:1297/1750 train_time:126543ms step_avg:97.57ms
step:1298/1750 train_time:126644ms step_avg:97.57ms
step:1299/1750 train_time:126746ms step_avg:97.57ms
step:1300/1750 train_time:126848ms step_avg:97.58ms
step:1301/1750 train_time:126950ms step_avg:97.58ms
step:1302/1750 train_time:127053ms step_avg:97.58ms
step:1303/1750 train_time:127156ms step_avg:97.59ms
step:1304/1750 train_time:127259ms step_avg:97.59ms
step:1305/1750 train_time:127362ms step_avg:97.60ms
step:1306/1750 train_time:127465ms step_avg:97.60ms
step:1307/1750 train_time:127567ms step_avg:97.60ms
step:1308/1750 train_time:127669ms step_avg:97.61ms
step:1309/1750 train_time:127770ms step_avg:97.61ms
step:1310/1750 train_time:127873ms step_avg:97.61ms
step:1311/1750 train_time:127975ms step_avg:97.62ms
step:1312/1750 train_time:128077ms step_avg:97.62ms
step:1313/1750 train_time:128180ms step_avg:97.62ms
step:1314/1750 train_time:128282ms step_avg:97.63ms
step:1315/1750 train_time:128384ms step_avg:97.63ms
step:1316/1750 train_time:128486ms step_avg:97.63ms
step:1317/1750 train_time:128588ms step_avg:97.64ms
step:1318/1750 train_time:128689ms step_avg:97.64ms
step:1319/1750 train_time:128792ms step_avg:97.64ms
step:1320/1750 train_time:128895ms step_avg:97.65ms
step:1321/1750 train_time:128997ms step_avg:97.65ms
step:1322/1750 train_time:129099ms step_avg:97.65ms
step:1323/1750 train_time:129200ms step_avg:97.66ms
step:1324/1750 train_time:129303ms step_avg:97.66ms
step:1325/1750 train_time:129405ms step_avg:97.66ms
step:1326/1750 train_time:129507ms step_avg:97.67ms
step:1327/1750 train_time:129609ms step_avg:97.67ms
step:1328/1750 train_time:129711ms step_avg:97.67ms
step:1329/1750 train_time:129812ms step_avg:97.68ms
step:1330/1750 train_time:129915ms step_avg:97.68ms
step:1331/1750 train_time:130018ms step_avg:97.68ms
step:1332/1750 train_time:130121ms step_avg:97.69ms
step:1333/1750 train_time:130223ms step_avg:97.69ms
step:1334/1750 train_time:130324ms step_avg:97.69ms
step:1335/1750 train_time:130426ms step_avg:97.70ms
step:1336/1750 train_time:130527ms step_avg:97.70ms
step:1337/1750 train_time:130630ms step_avg:97.70ms
step:1338/1750 train_time:130732ms step_avg:97.71ms
step:1339/1750 train_time:130834ms step_avg:97.71ms
step:1340/1750 train_time:130937ms step_avg:97.71ms
step:1341/1750 train_time:131040ms step_avg:97.72ms
step:1342/1750 train_time:131141ms step_avg:97.72ms
step:1343/1750 train_time:131242ms step_avg:97.72ms
step:1344/1750 train_time:131344ms step_avg:97.73ms
step:1345/1750 train_time:131445ms step_avg:97.73ms
step:1346/1750 train_time:131548ms step_avg:97.73ms
step:1347/1750 train_time:131651ms step_avg:97.74ms
step:1348/1750 train_time:131753ms step_avg:97.74ms
step:1349/1750 train_time:131855ms step_avg:97.74ms
step:1350/1750 train_time:131958ms step_avg:97.75ms
step:1351/1750 train_time:132060ms step_avg:97.75ms
step:1352/1750 train_time:132162ms step_avg:97.75ms
step:1353/1750 train_time:132263ms step_avg:97.76ms
step:1354/1750 train_time:132365ms step_avg:97.76ms
step:1355/1750 train_time:132467ms step_avg:97.76ms
step:1356/1750 train_time:132569ms step_avg:97.76ms
step:1357/1750 train_time:132671ms step_avg:97.77ms
step:1358/1750 train_time:132774ms step_avg:97.77ms
step:1359/1750 train_time:132877ms step_avg:97.78ms
step:1360/1750 train_time:132979ms step_avg:97.78ms
step:1361/1750 train_time:133080ms step_avg:97.78ms
step:1362/1750 train_time:133181ms step_avg:97.78ms
step:1363/1750 train_time:133283ms step_avg:97.79ms
step:1364/1750 train_time:133385ms step_avg:97.79ms
step:1365/1750 train_time:133487ms step_avg:97.79ms
step:1366/1750 train_time:133588ms step_avg:97.80ms
step:1367/1750 train_time:133690ms step_avg:97.80ms
step:1368/1750 train_time:133793ms step_avg:97.80ms
step:1369/1750 train_time:133894ms step_avg:97.80ms
step:1370/1750 train_time:133997ms step_avg:97.81ms
step:1371/1750 train_time:134099ms step_avg:97.81ms
step:1372/1750 train_time:134200ms step_avg:97.81ms
step:1373/1750 train_time:134302ms step_avg:97.82ms
step:1374/1750 train_time:134404ms step_avg:97.82ms
step:1375/1750 train_time:134507ms step_avg:97.82ms
step:1375/1750 val_loss:3.3675 train_time:134603ms step_avg:97.89ms
step:1376/1750 train_time:134629ms step_avg:97.84ms
step:1377/1750 train_time:134723ms step_avg:97.84ms
step:1378/1750 train_time:134828ms step_avg:97.84ms
step:1379/1750 train_time:134929ms step_avg:97.85ms
step:1380/1750 train_time:135032ms step_avg:97.85ms
step:1381/1750 train_time:135132ms step_avg:97.85ms
step:1382/1750 train_time:135234ms step_avg:97.85ms
step:1383/1750 train_time:135335ms step_avg:97.86ms
step:1384/1750 train_time:135435ms step_avg:97.86ms
step:1385/1750 train_time:135537ms step_avg:97.86ms
step:1386/1750 train_time:135641ms step_avg:97.86ms
step:1387/1750 train_time:135743ms step_avg:97.87ms
step:1388/1750 train_time:135845ms step_avg:97.87ms
step:1389/1750 train_time:135947ms step_avg:97.87ms
step:1390/1750 train_time:136050ms step_avg:97.88ms
step:1391/1750 train_time:136152ms step_avg:97.88ms
step:1392/1750 train_time:136253ms step_avg:97.88ms
step:1393/1750 train_time:136355ms step_avg:97.89ms
step:1394/1750 train_time:136456ms step_avg:97.89ms
step:1395/1750 train_time:136558ms step_avg:97.89ms
step:1396/1750 train_time:136660ms step_avg:97.89ms
step:1397/1750 train_time:136763ms step_avg:97.90ms
step:1398/1750 train_time:136865ms step_avg:97.90ms
step:1399/1750 train_time:136967ms step_avg:97.90ms
step:1400/1750 train_time:137070ms step_avg:97.91ms
step:1401/1750 train_time:137172ms step_avg:97.91ms
step:1402/1750 train_time:137274ms step_avg:97.91ms
step:1403/1750 train_time:137376ms step_avg:97.92ms
step:1404/1750 train_time:137477ms step_avg:97.92ms
step:1405/1750 train_time:137579ms step_avg:97.92ms
step:1406/1750 train_time:137681ms step_avg:97.92ms
step:1407/1750 train_time:137783ms step_avg:97.93ms
step:1408/1750 train_time:137886ms step_avg:97.93ms
step:1409/1750 train_time:137989ms step_avg:97.93ms
step:1410/1750 train_time:138092ms step_avg:97.94ms
step:1411/1750 train_time:138194ms step_avg:97.94ms
step:1412/1750 train_time:138296ms step_avg:97.94ms
step:1413/1750 train_time:138397ms step_avg:97.95ms
step:1414/1750 train_time:138499ms step_avg:97.95ms
step:1415/1750 train_time:138602ms step_avg:97.95ms
step:1416/1750 train_time:138703ms step_avg:97.95ms
step:1417/1750 train_time:138805ms step_avg:97.96ms
step:1418/1750 train_time:138907ms step_avg:97.96ms
step:1419/1750 train_time:139010ms step_avg:97.96ms
step:1420/1750 train_time:139113ms step_avg:97.97ms
step:1421/1750 train_time:139215ms step_avg:97.97ms
step:1422/1750 train_time:139317ms step_avg:97.97ms
step:1423/1750 train_time:139418ms step_avg:97.97ms
step:1424/1750 train_time:139520ms step_avg:97.98ms
step:1425/1750 train_time:139621ms step_avg:97.98ms
step:1426/1750 train_time:139724ms step_avg:97.98ms
step:1427/1750 train_time:139826ms step_avg:97.99ms
step:1428/1750 train_time:139932ms step_avg:97.99ms
step:1429/1750 train_time:140033ms step_avg:97.99ms
step:1430/1750 train_time:140137ms step_avg:98.00ms
step:1431/1750 train_time:140239ms step_avg:98.00ms
step:1432/1750 train_time:140341ms step_avg:98.00ms
step:1433/1750 train_time:140444ms step_avg:98.01ms
step:1434/1750 train_time:140547ms step_avg:98.01ms
step:1435/1750 train_time:140651ms step_avg:98.01ms
step:1436/1750 train_time:140756ms step_avg:98.02ms
step:1437/1750 train_time:140859ms step_avg:98.02ms
step:1438/1750 train_time:140961ms step_avg:98.03ms
step:1439/1750 train_time:141064ms step_avg:98.03ms
step:1440/1750 train_time:141169ms step_avg:98.03ms
step:1441/1750 train_time:141275ms step_avg:98.04ms
step:1442/1750 train_time:141376ms step_avg:98.04ms
step:1443/1750 train_time:141478ms step_avg:98.04ms
step:1444/1750 train_time:141581ms step_avg:98.05ms
step:1445/1750 train_time:141684ms step_avg:98.05ms
step:1446/1750 train_time:141787ms step_avg:98.05ms
step:1447/1750 train_time:141889ms step_avg:98.06ms
step:1448/1750 train_time:141993ms step_avg:98.06ms
step:1449/1750 train_time:142094ms step_avg:98.06ms
step:1450/1750 train_time:142198ms step_avg:98.07ms
step:1451/1750 train_time:142301ms step_avg:98.07ms
step:1452/1750 train_time:142404ms step_avg:98.07ms
step:1453/1750 train_time:142508ms step_avg:98.08ms
step:1454/1750 train_time:142612ms step_avg:98.08ms
step:1455/1750 train_time:142715ms step_avg:98.09ms
step:1456/1750 train_time:142817ms step_avg:98.09ms
step:1457/1750 train_time:142920ms step_avg:98.09ms
step:1458/1750 train_time:143023ms step_avg:98.10ms
step:1459/1750 train_time:143127ms step_avg:98.10ms
step:1460/1750 train_time:143230ms step_avg:98.10ms
step:1461/1750 train_time:143333ms step_avg:98.11ms
step:1462/1750 train_time:143436ms step_avg:98.11ms
step:1463/1750 train_time:143539ms step_avg:98.11ms
step:1464/1750 train_time:143642ms step_avg:98.12ms
step:1465/1750 train_time:143745ms step_avg:98.12ms
step:1466/1750 train_time:143847ms step_avg:98.12ms
step:1467/1750 train_time:143951ms step_avg:98.13ms
step:1468/1750 train_time:144056ms step_avg:98.13ms
step:1469/1750 train_time:144159ms step_avg:98.13ms
step:1470/1750 train_time:144261ms step_avg:98.14ms
step:1471/1750 train_time:144364ms step_avg:98.14ms
step:1472/1750 train_time:144467ms step_avg:98.14ms
step:1473/1750 train_time:144570ms step_avg:98.15ms
step:1474/1750 train_time:144673ms step_avg:98.15ms
step:1475/1750 train_time:144776ms step_avg:98.15ms
step:1476/1750 train_time:144879ms step_avg:98.16ms
step:1477/1750 train_time:144982ms step_avg:98.16ms
step:1478/1750 train_time:145086ms step_avg:98.16ms
step:1479/1750 train_time:145189ms step_avg:98.17ms
step:1480/1750 train_time:145292ms step_avg:98.17ms
step:1481/1750 train_time:145395ms step_avg:98.17ms
step:1482/1750 train_time:145498ms step_avg:98.18ms
step:1483/1750 train_time:145600ms step_avg:98.18ms
step:1484/1750 train_time:145704ms step_avg:98.18ms
step:1485/1750 train_time:145808ms step_avg:98.19ms
step:1486/1750 train_time:145911ms step_avg:98.19ms
step:1487/1750 train_time:146015ms step_avg:98.19ms
step:1488/1750 train_time:146119ms step_avg:98.20ms
step:1489/1750 train_time:146222ms step_avg:98.20ms
step:1490/1750 train_time:146324ms step_avg:98.20ms
step:1491/1750 train_time:146428ms step_avg:98.21ms
step:1492/1750 train_time:146530ms step_avg:98.21ms
step:1493/1750 train_time:146633ms step_avg:98.21ms
step:1494/1750 train_time:146736ms step_avg:98.22ms
step:1495/1750 train_time:146839ms step_avg:98.22ms
step:1496/1750 train_time:146941ms step_avg:98.22ms
step:1497/1750 train_time:147045ms step_avg:98.23ms
step:1498/1750 train_time:147148ms step_avg:98.23ms
step:1499/1750 train_time:147250ms step_avg:98.23ms
step:1500/1750 train_time:147354ms step_avg:98.24ms
step:1500/1750 val_loss:3.3315 train_time:147452ms step_avg:98.30ms
step:1501/1750 train_time:147478ms step_avg:98.25ms
step:1502/1750 train_time:147571ms step_avg:98.25ms
step:1503/1750 train_time:147674ms step_avg:98.25ms
step:1504/1750 train_time:147776ms step_avg:98.26ms
step:1505/1750 train_time:147879ms step_avg:98.26ms
step:1506/1750 train_time:147980ms step_avg:98.26ms
step:1507/1750 train_time:148082ms step_avg:98.26ms
step:1508/1750 train_time:148184ms step_avg:98.27ms
step:1509/1750 train_time:148287ms step_avg:98.27ms
step:1510/1750 train_time:148390ms step_avg:98.27ms
step:1511/1750 train_time:148494ms step_avg:98.28ms
step:1512/1750 train_time:148598ms step_avg:98.28ms
step:1513/1750 train_time:148701ms step_avg:98.28ms
step:1514/1750 train_time:148807ms step_avg:98.29ms
step:1515/1750 train_time:148914ms step_avg:98.29ms
step:1516/1750 train_time:149017ms step_avg:98.30ms
step:1517/1750 train_time:149118ms step_avg:98.30ms
step:1518/1750 train_time:149221ms step_avg:98.30ms
step:1519/1750 train_time:149326ms step_avg:98.31ms
step:1520/1750 train_time:149429ms step_avg:98.31ms
step:1521/1750 train_time:149532ms step_avg:98.31ms
step:1522/1750 train_time:149635ms step_avg:98.31ms
step:1523/1750 train_time:149738ms step_avg:98.32ms
step:1524/1750 train_time:149842ms step_avg:98.32ms
step:1525/1750 train_time:149946ms step_avg:98.33ms
step:1526/1750 train_time:150049ms step_avg:98.33ms
step:1527/1750 train_time:150153ms step_avg:98.33ms
step:1528/1750 train_time:150259ms step_avg:98.34ms
step:1529/1750 train_time:150360ms step_avg:98.34ms
step:1530/1750 train_time:150463ms step_avg:98.34ms
step:1531/1750 train_time:150567ms step_avg:98.35ms
step:1532/1750 train_time:150669ms step_avg:98.35ms
step:1533/1750 train_time:150772ms step_avg:98.35ms
step:1534/1750 train_time:150875ms step_avg:98.35ms
step:1535/1750 train_time:150978ms step_avg:98.36ms
step:1536/1750 train_time:151080ms step_avg:98.36ms
step:1537/1750 train_time:151184ms step_avg:98.36ms
step:1538/1750 train_time:151287ms step_avg:98.37ms
step:1539/1750 train_time:151390ms step_avg:98.37ms
step:1540/1750 train_time:151494ms step_avg:98.37ms
step:1541/1750 train_time:151598ms step_avg:98.38ms
step:1542/1750 train_time:151701ms step_avg:98.38ms
step:1543/1750 train_time:151806ms step_avg:98.38ms
step:1544/1750 train_time:151909ms step_avg:98.39ms
step:1545/1750 train_time:152012ms step_avg:98.39ms
step:1546/1750 train_time:152115ms step_avg:98.39ms
step:1547/1750 train_time:152219ms step_avg:98.40ms
step:1548/1750 train_time:152323ms step_avg:98.40ms
step:1549/1750 train_time:152427ms step_avg:98.40ms
step:1550/1750 train_time:152530ms step_avg:98.41ms
step:1551/1750 train_time:152634ms step_avg:98.41ms
step:1552/1750 train_time:152737ms step_avg:98.41ms
step:1553/1750 train_time:152840ms step_avg:98.42ms
step:1554/1750 train_time:152943ms step_avg:98.42ms
step:1555/1750 train_time:153046ms step_avg:98.42ms
step:1556/1750 train_time:153150ms step_avg:98.43ms
step:1557/1750 train_time:153254ms step_avg:98.43ms
step:1558/1750 train_time:153358ms step_avg:98.43ms
step:1559/1750 train_time:153461ms step_avg:98.44ms
step:1560/1750 train_time:153565ms step_avg:98.44ms
step:1561/1750 train_time:153669ms step_avg:98.44ms
step:1562/1750 train_time:153772ms step_avg:98.45ms
step:1563/1750 train_time:153877ms step_avg:98.45ms
step:1564/1750 train_time:153979ms step_avg:98.45ms
step:1565/1750 train_time:154081ms step_avg:98.45ms
step:1566/1750 train_time:154184ms step_avg:98.46ms
step:1567/1750 train_time:154287ms step_avg:98.46ms
step:1568/1750 train_time:154390ms step_avg:98.46ms
step:1569/1750 train_time:154494ms step_avg:98.47ms
step:1570/1750 train_time:154598ms step_avg:98.47ms
step:1571/1750 train_time:154700ms step_avg:98.47ms
step:1572/1750 train_time:154803ms step_avg:98.48ms
step:1573/1750 train_time:154906ms step_avg:98.48ms
step:1574/1750 train_time:155009ms step_avg:98.48ms
step:1575/1750 train_time:155112ms step_avg:98.48ms
step:1576/1750 train_time:155215ms step_avg:98.49ms
step:1577/1750 train_time:155319ms step_avg:98.49ms
step:1578/1750 train_time:155421ms step_avg:98.49ms
step:1579/1750 train_time:155526ms step_avg:98.50ms
step:1580/1750 train_time:155630ms step_avg:98.50ms
step:1581/1750 train_time:155733ms step_avg:98.50ms
step:1582/1750 train_time:155835ms step_avg:98.51ms
step:1583/1750 train_time:155940ms step_avg:98.51ms
step:1584/1750 train_time:156046ms step_avg:98.51ms
step:1585/1750 train_time:156149ms step_avg:98.52ms
step:1586/1750 train_time:156254ms step_avg:98.52ms
step:1587/1750 train_time:156356ms step_avg:98.52ms
step:1588/1750 train_time:156459ms step_avg:98.53ms
step:1589/1750 train_time:156563ms step_avg:98.53ms
step:1590/1750 train_time:156667ms step_avg:98.53ms
step:1591/1750 train_time:156771ms step_avg:98.54ms
step:1592/1750 train_time:156874ms step_avg:98.54ms
step:1593/1750 train_time:156977ms step_avg:98.54ms
step:1594/1750 train_time:157082ms step_avg:98.55ms
step:1595/1750 train_time:157185ms step_avg:98.55ms
step:1596/1750 train_time:157288ms step_avg:98.55ms
step:1597/1750 train_time:157392ms step_avg:98.55ms
step:1598/1750 train_time:157497ms step_avg:98.56ms
step:1599/1750 train_time:157598ms step_avg:98.56ms
step:1600/1750 train_time:157703ms step_avg:98.56ms
step:1601/1750 train_time:157807ms step_avg:98.57ms
step:1602/1750 train_time:157910ms step_avg:98.57ms
step:1603/1750 train_time:158013ms step_avg:98.57ms
step:1604/1750 train_time:158116ms step_avg:98.58ms
step:1605/1750 train_time:158219ms step_avg:98.58ms
step:1606/1750 train_time:158321ms step_avg:98.58ms
step:1607/1750 train_time:158425ms step_avg:98.58ms
step:1608/1750 train_time:158528ms step_avg:98.59ms
step:1609/1750 train_time:158632ms step_avg:98.59ms
step:1610/1750 train_time:158736ms step_avg:98.59ms
step:1611/1750 train_time:158839ms step_avg:98.60ms
step:1612/1750 train_time:158944ms step_avg:98.60ms
step:1613/1750 train_time:159047ms step_avg:98.60ms
step:1614/1750 train_time:159150ms step_avg:98.61ms
step:1615/1750 train_time:159252ms step_avg:98.61ms
step:1616/1750 train_time:159356ms step_avg:98.61ms
step:1617/1750 train_time:159459ms step_avg:98.61ms
step:1618/1750 train_time:159562ms step_avg:98.62ms
step:1619/1750 train_time:159665ms step_avg:98.62ms
step:1620/1750 train_time:159770ms step_avg:98.62ms
step:1621/1750 train_time:159873ms step_avg:98.63ms
step:1622/1750 train_time:159976ms step_avg:98.63ms
step:1623/1750 train_time:160079ms step_avg:98.63ms
step:1624/1750 train_time:160182ms step_avg:98.63ms
step:1625/1750 train_time:160286ms step_avg:98.64ms
step:1625/1750 val_loss:3.3014 train_time:160384ms step_avg:98.70ms
step:1626/1750 train_time:160411ms step_avg:98.65ms
step:1627/1750 train_time:160499ms step_avg:98.65ms
step:1628/1750 train_time:160601ms step_avg:98.65ms
step:1629/1750 train_time:160703ms step_avg:98.65ms
step:1630/1750 train_time:160806ms step_avg:98.65ms
step:1631/1750 train_time:160909ms step_avg:98.66ms
step:1632/1750 train_time:161011ms step_avg:98.66ms
step:1633/1750 train_time:161113ms step_avg:98.66ms
step:1634/1750 train_time:161217ms step_avg:98.66ms
step:1635/1750 train_time:161319ms step_avg:98.67ms
step:1636/1750 train_time:161426ms step_avg:98.67ms
step:1637/1750 train_time:161529ms step_avg:98.67ms
step:1638/1750 train_time:161631ms step_avg:98.68ms
step:1639/1750 train_time:161733ms step_avg:98.68ms
step:1640/1750 train_time:161837ms step_avg:98.68ms
step:1641/1750 train_time:161942ms step_avg:98.68ms
step:1642/1750 train_time:162043ms step_avg:98.69ms
step:1643/1750 train_time:162147ms step_avg:98.69ms
step:1644/1750 train_time:162250ms step_avg:98.69ms
step:1645/1750 train_time:162353ms step_avg:98.69ms
step:1646/1750 train_time:162456ms step_avg:98.70ms
step:1647/1750 train_time:162560ms step_avg:98.70ms
step:1648/1750 train_time:162665ms step_avg:98.70ms
step:1649/1750 train_time:162768ms step_avg:98.71ms
step:1650/1750 train_time:162871ms step_avg:98.71ms
step:1651/1750 train_time:162974ms step_avg:98.71ms
step:1652/1750 train_time:163077ms step_avg:98.71ms
step:1653/1750 train_time:163181ms step_avg:98.72ms
step:1654/1750 train_time:163284ms step_avg:98.72ms
step:1655/1750 train_time:163388ms step_avg:98.72ms
step:1656/1750 train_time:163491ms step_avg:98.73ms
step:1657/1750 train_time:163594ms step_avg:98.73ms
step:1658/1750 train_time:163697ms step_avg:98.73ms
step:1659/1750 train_time:163804ms step_avg:98.74ms
step:1660/1750 train_time:163907ms step_avg:98.74ms
step:1661/1750 train_time:164012ms step_avg:98.74ms
step:1662/1750 train_time:164116ms step_avg:98.75ms
step:1663/1750 train_time:164219ms step_avg:98.75ms
step:1664/1750 train_time:164323ms step_avg:98.75ms
step:1665/1750 train_time:164426ms step_avg:98.75ms
step:1666/1750 train_time:164530ms step_avg:98.76ms
step:1667/1750 train_time:164633ms step_avg:98.76ms
step:1668/1750 train_time:164737ms step_avg:98.76ms
step:1669/1750 train_time:164842ms step_avg:98.77ms
step:1670/1750 train_time:164946ms step_avg:98.77ms
step:1671/1750 train_time:165048ms step_avg:98.77ms
step:1672/1750 train_time:165152ms step_avg:98.77ms
step:1673/1750 train_time:165254ms step_avg:98.78ms
step:1674/1750 train_time:165357ms step_avg:98.78ms
step:1675/1750 train_time:165462ms step_avg:98.78ms
step:1676/1750 train_time:165565ms step_avg:98.79ms
step:1677/1750 train_time:165668ms step_avg:98.79ms
step:1678/1750 train_time:165771ms step_avg:98.79ms
step:1679/1750 train_time:165874ms step_avg:98.79ms
step:1680/1750 train_time:165978ms step_avg:98.80ms
step:1681/1750 train_time:166082ms step_avg:98.80ms
step:1682/1750 train_time:166187ms step_avg:98.80ms
step:1683/1750 train_time:166289ms step_avg:98.81ms
step:1684/1750 train_time:166393ms step_avg:98.81ms
step:1685/1750 train_time:166495ms step_avg:98.81ms
step:1686/1750 train_time:166599ms step_avg:98.81ms
step:1687/1750 train_time:166702ms step_avg:98.82ms
step:1688/1750 train_time:166806ms step_avg:98.82ms
step:1689/1750 train_time:166910ms step_avg:98.82ms
step:1690/1750 train_time:167012ms step_avg:98.82ms
step:1691/1750 train_time:167117ms step_avg:98.83ms
step:1692/1750 train_time:167222ms step_avg:98.83ms
step:1693/1750 train_time:167325ms step_avg:98.83ms
step:1694/1750 train_time:167430ms step_avg:98.84ms
step:1695/1750 train_time:167535ms step_avg:98.84ms
step:1696/1750 train_time:167637ms step_avg:98.84ms
step:1697/1750 train_time:167746ms step_avg:98.85ms
step:1698/1750 train_time:167849ms step_avg:98.85ms
step:1699/1750 train_time:167953ms step_avg:98.85ms
step:1700/1750 train_time:168056ms step_avg:98.86ms
step:1701/1750 train_time:168159ms step_avg:98.86ms
step:1702/1750 train_time:168266ms step_avg:98.86ms
step:1703/1750 train_time:168371ms step_avg:98.87ms
step:1704/1750 train_time:168474ms step_avg:98.87ms
step:1705/1750 train_time:168577ms step_avg:98.87ms
step:1706/1750 train_time:168681ms step_avg:98.88ms
step:1707/1750 train_time:168786ms step_avg:98.88ms
step:1708/1750 train_time:168891ms step_avg:98.88ms
step:1709/1750 train_time:168994ms step_avg:98.88ms
step:1710/1750 train_time:169100ms step_avg:98.89ms
step:1711/1750 train_time:169205ms step_avg:98.89ms
step:1712/1750 train_time:169309ms step_avg:98.90ms
step:1713/1750 train_time:169414ms step_avg:98.90ms
step:1714/1750 train_time:169517ms step_avg:98.90ms
step:1715/1750 train_time:169622ms step_avg:98.91ms
step:1716/1750 train_time:169726ms step_avg:98.91ms
step:1717/1750 train_time:169831ms step_avg:98.91ms
step:1718/1750 train_time:169935ms step_avg:98.91ms
step:1719/1750 train_time:170041ms step_avg:98.92ms
step:1720/1750 train_time:170145ms step_avg:98.92ms
step:1721/1750 train_time:170249ms step_avg:98.92ms
step:1722/1750 train_time:170354ms step_avg:98.93ms
step:1723/1750 train_time:170458ms step_avg:98.93ms
step:1724/1750 train_time:170563ms step_avg:98.93ms
step:1725/1750 train_time:170668ms step_avg:98.94ms
step:1726/1750 train_time:170771ms step_avg:98.94ms
step:1727/1750 train_time:170875ms step_avg:98.94ms
step:1728/1750 train_time:170980ms step_avg:98.95ms
step:1729/1750 train_time:171084ms step_avg:98.95ms
step:1730/1750 train_time:171187ms step_avg:98.95ms
step:1731/1750 train_time:171291ms step_avg:98.96ms
step:1732/1750 train_time:171395ms step_avg:98.96ms
step:1733/1750 train_time:171499ms step_avg:98.96ms
step:1734/1750 train_time:171605ms step_avg:98.96ms
step:1735/1750 train_time:171707ms step_avg:98.97ms
step:1736/1750 train_time:171811ms step_avg:98.97ms
step:1737/1750 train_time:171915ms step_avg:98.97ms
step:1738/1750 train_time:172020ms step_avg:98.98ms
step:1739/1750 train_time:172125ms step_avg:98.98ms
step:1740/1750 train_time:172229ms step_avg:98.98ms
step:1741/1750 train_time:172338ms step_avg:98.99ms
step:1742/1750 train_time:172442ms step_avg:98.99ms
step:1743/1750 train_time:172547ms step_avg:98.99ms
step:1744/1750 train_time:172651ms step_avg:99.00ms
step:1745/1750 train_time:172755ms step_avg:99.00ms
step:1746/1750 train_time:172858ms step_avg:99.00ms
step:1747/1750 train_time:172962ms step_avg:99.01ms
step:1748/1750 train_time:173067ms step_avg:99.01ms
step:1749/1750 train_time:173170ms step_avg:99.01ms
step:1750/1750 train_time:173276ms step_avg:99.01ms
step:1750/1750 val_loss:3.2805 train_time:173375ms step_avg:99.07ms
peak memory allocated: 33511 MiB reserved: 49012 MiB

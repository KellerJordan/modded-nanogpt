import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: torch.Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_masks(sliding_window_num_blocks: int):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            global_block_mask = BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
            local_window_bm = q_idx - kv_idx < max(1, sliding_window_num_blocks // 2)
            local_window_full_bm = local_window_bm
            local_nonzero_bm = causal_bm & local_window_bm & document_bm
            local_full_bm = causal_full_bm & local_window_full_bm & document_full_bm
            local_kv_num_blocks, local_kv_indices = dense_to_ordered(local_nonzero_bm & ~local_full_bm)
            local_full_kv_num_blocks, local_full_kv_indices = dense_to_ordered(local_full_bm)
            local_block_mask = BlockMask.from_kv_blocks(
                local_kv_num_blocks,
                local_kv_indices,
                local_full_kv_num_blocks,
                local_full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
            return global_block_mask, local_block_mask

        global_block_mask, local_block_mask = create_doc_swc_block_masks(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            block_mask = global_block_mask if i % 2 == 0 else local_block_mask
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            block_mask = local_block_mask if i % 2 == 0 else global_block_mask
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1405 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Wed Jan 15 09:30:54 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   28C    P0             117W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   27C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   28C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   26C    P0             109W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   25C    P0             114W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1405 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1405 train_time:28344ms step_avg:nanms
step:2/1405 train_time:28412ms step_avg:nanms
step:3/1405 train_time:28622ms step_avg:nanms
step:4/1405 train_time:28755ms step_avg:nanms
step:5/1405 train_time:28890ms step_avg:nanms
step:6/1405 train_time:29024ms step_avg:nanms
step:7/1405 train_time:29156ms step_avg:nanms
step:8/1405 train_time:29290ms step_avg:nanms
step:9/1405 train_time:29423ms step_avg:nanms
step:10/1405 train_time:29561ms step_avg:nanms
step:11/1405 train_time:138ms step_avg:nanms
step:12/1405 train_time:273ms step_avg:nanms
step:13/1405 train_time:409ms step_avg:136.46ms
step:14/1405 train_time:544ms step_avg:135.88ms
step:15/1405 train_time:678ms step_avg:135.57ms
step:16/1405 train_time:812ms step_avg:135.35ms
step:17/1405 train_time:947ms step_avg:135.30ms
step:18/1405 train_time:1081ms step_avg:135.17ms
step:19/1405 train_time:1218ms step_avg:135.30ms
step:20/1405 train_time:1354ms step_avg:135.37ms
step:21/1405 train_time:1489ms step_avg:135.37ms
step:22/1405 train_time:1623ms step_avg:135.26ms
step:23/1405 train_time:1759ms step_avg:135.28ms
step:24/1405 train_time:1894ms step_avg:135.27ms
step:25/1405 train_time:2030ms step_avg:135.36ms
step:26/1405 train_time:2164ms step_avg:135.27ms
step:27/1405 train_time:2300ms step_avg:135.29ms
step:28/1405 train_time:2435ms step_avg:135.30ms
step:29/1405 train_time:2572ms step_avg:135.36ms
step:30/1405 train_time:2706ms step_avg:135.28ms
step:31/1405 train_time:2840ms step_avg:135.24ms
step:32/1405 train_time:2975ms step_avg:135.21ms
step:33/1405 train_time:3112ms step_avg:135.31ms
step:34/1405 train_time:3247ms step_avg:135.28ms
step:35/1405 train_time:3381ms step_avg:135.25ms
step:36/1405 train_time:3517ms step_avg:135.25ms
step:37/1405 train_time:3652ms step_avg:135.26ms
step:38/1405 train_time:3785ms step_avg:135.19ms
step:39/1405 train_time:3920ms step_avg:135.17ms
step:40/1405 train_time:4055ms step_avg:135.18ms
step:41/1405 train_time:4191ms step_avg:135.20ms
step:42/1405 train_time:4326ms step_avg:135.18ms
step:43/1405 train_time:4460ms step_avg:135.16ms
step:44/1405 train_time:4596ms step_avg:135.16ms
step:45/1405 train_time:4731ms step_avg:135.18ms
step:46/1405 train_time:4864ms step_avg:135.11ms
step:47/1405 train_time:5000ms step_avg:135.15ms
step:48/1405 train_time:5134ms step_avg:135.12ms
step:49/1405 train_time:5269ms step_avg:135.11ms
step:50/1405 train_time:5404ms step_avg:135.11ms
step:51/1405 train_time:5538ms step_avg:135.08ms
step:52/1405 train_time:5673ms step_avg:135.07ms
step:53/1405 train_time:5808ms step_avg:135.07ms
step:54/1405 train_time:5944ms step_avg:135.09ms
step:55/1405 train_time:6079ms step_avg:135.10ms
step:56/1405 train_time:6215ms step_avg:135.11ms
step:57/1405 train_time:6352ms step_avg:135.14ms
step:58/1405 train_time:6485ms step_avg:135.11ms
step:59/1405 train_time:6619ms step_avg:135.08ms
step:60/1405 train_time:6753ms step_avg:135.06ms
step:61/1405 train_time:6889ms step_avg:135.08ms
step:62/1405 train_time:7023ms step_avg:135.06ms
step:63/1405 train_time:7160ms step_avg:135.09ms
step:64/1405 train_time:7296ms step_avg:135.11ms
step:65/1405 train_time:7432ms step_avg:135.13ms
step:66/1405 train_time:7567ms step_avg:135.13ms
step:67/1405 train_time:7702ms step_avg:135.12ms
step:68/1405 train_time:7837ms step_avg:135.12ms
step:69/1405 train_time:7972ms step_avg:135.12ms
step:70/1405 train_time:8108ms step_avg:135.14ms
step:71/1405 train_time:8243ms step_avg:135.13ms
step:72/1405 train_time:8377ms step_avg:135.11ms
step:73/1405 train_time:8513ms step_avg:135.13ms
step:74/1405 train_time:8648ms step_avg:135.13ms
step:75/1405 train_time:8782ms step_avg:135.10ms
step:76/1405 train_time:8917ms step_avg:135.10ms
step:77/1405 train_time:9053ms step_avg:135.12ms
step:78/1405 train_time:9187ms step_avg:135.11ms
step:79/1405 train_time:9322ms step_avg:135.10ms
step:80/1405 train_time:9458ms step_avg:135.11ms
step:81/1405 train_time:9594ms step_avg:135.12ms
step:82/1405 train_time:9730ms step_avg:135.14ms
step:83/1405 train_time:9865ms step_avg:135.14ms
step:84/1405 train_time:10000ms step_avg:135.14ms
step:85/1405 train_time:10136ms step_avg:135.15ms
step:86/1405 train_time:10271ms step_avg:135.14ms
step:87/1405 train_time:10406ms step_avg:135.15ms
step:88/1405 train_time:10541ms step_avg:135.14ms
step:89/1405 train_time:10676ms step_avg:135.14ms
step:90/1405 train_time:10812ms step_avg:135.15ms
step:91/1405 train_time:10948ms step_avg:135.15ms
step:92/1405 train_time:11081ms step_avg:135.14ms
step:93/1405 train_time:11217ms step_avg:135.14ms
step:94/1405 train_time:11353ms step_avg:135.15ms
step:95/1405 train_time:11488ms step_avg:135.15ms
step:96/1405 train_time:11622ms step_avg:135.14ms
step:97/1405 train_time:11758ms step_avg:135.15ms
step:98/1405 train_time:11894ms step_avg:135.16ms
step:99/1405 train_time:12030ms step_avg:135.17ms
step:100/1405 train_time:12165ms step_avg:135.17ms
step:101/1405 train_time:12300ms step_avg:135.17ms
step:102/1405 train_time:12435ms step_avg:135.17ms
step:103/1405 train_time:12573ms step_avg:135.19ms
step:104/1405 train_time:12712ms step_avg:135.23ms
step:105/1405 train_time:12847ms step_avg:135.23ms
step:106/1405 train_time:12982ms step_avg:135.23ms
step:107/1405 train_time:13119ms step_avg:135.25ms
step:108/1405 train_time:13256ms step_avg:135.26ms
step:109/1405 train_time:13392ms step_avg:135.28ms
step:110/1405 train_time:13528ms step_avg:135.28ms
step:111/1405 train_time:13665ms step_avg:135.30ms
step:112/1405 train_time:13801ms step_avg:135.31ms
step:113/1405 train_time:13939ms step_avg:135.33ms
step:114/1405 train_time:14076ms step_avg:135.35ms
step:115/1405 train_time:14213ms step_avg:135.37ms
step:116/1405 train_time:14350ms step_avg:135.37ms
step:117/1405 train_time:14486ms step_avg:135.38ms
step:118/1405 train_time:14623ms step_avg:135.40ms
step:119/1405 train_time:14762ms step_avg:135.43ms
step:120/1405 train_time:14898ms step_avg:135.44ms
step:121/1405 train_time:15036ms step_avg:135.46ms
step:122/1405 train_time:15173ms step_avg:135.47ms
step:123/1405 train_time:15312ms step_avg:135.50ms
step:124/1405 train_time:15446ms step_avg:135.49ms
step:125/1405 train_time:15583ms step_avg:135.51ms
step:125/1405 val_loss:4.4035 train_time:15650ms step_avg:136.09ms
step:126/1405 train_time:15723ms step_avg:135.55ms
step:127/1405 train_time:15868ms step_avg:135.62ms
step:128/1405 train_time:16006ms step_avg:135.64ms
step:129/1405 train_time:16140ms step_avg:135.63ms
step:130/1405 train_time:16276ms step_avg:135.63ms
step:131/1405 train_time:16412ms step_avg:135.63ms
step:132/1405 train_time:16547ms step_avg:135.63ms
step:133/1405 train_time:16686ms step_avg:135.66ms
step:134/1405 train_time:16825ms step_avg:135.69ms
step:135/1405 train_time:16964ms step_avg:135.71ms
step:136/1405 train_time:17100ms step_avg:135.71ms
step:137/1405 train_time:17237ms step_avg:135.72ms
step:138/1405 train_time:17374ms step_avg:135.73ms
step:139/1405 train_time:17510ms step_avg:135.74ms
step:140/1405 train_time:17645ms step_avg:135.73ms
step:141/1405 train_time:17784ms step_avg:135.75ms
step:142/1405 train_time:17922ms step_avg:135.77ms
step:143/1405 train_time:18060ms step_avg:135.79ms
step:144/1405 train_time:18197ms step_avg:135.80ms
step:145/1405 train_time:18333ms step_avg:135.80ms
step:146/1405 train_time:18471ms step_avg:135.82ms
step:147/1405 train_time:18609ms step_avg:135.83ms
step:148/1405 train_time:18744ms step_avg:135.83ms
step:149/1405 train_time:18883ms step_avg:135.85ms
step:150/1405 train_time:19021ms step_avg:135.87ms
step:151/1405 train_time:19158ms step_avg:135.88ms
step:152/1405 train_time:19295ms step_avg:135.88ms
step:153/1405 train_time:19432ms step_avg:135.89ms
step:154/1405 train_time:19570ms step_avg:135.90ms
step:155/1405 train_time:19708ms step_avg:135.92ms
step:156/1405 train_time:19847ms step_avg:135.94ms
step:157/1405 train_time:19984ms step_avg:135.95ms
step:158/1405 train_time:20122ms step_avg:135.96ms
step:159/1405 train_time:20259ms step_avg:135.97ms
step:160/1405 train_time:20397ms step_avg:135.98ms
step:161/1405 train_time:20533ms step_avg:135.98ms
step:162/1405 train_time:20672ms step_avg:136.00ms
step:163/1405 train_time:20811ms step_avg:136.02ms
step:164/1405 train_time:20949ms step_avg:136.03ms
step:165/1405 train_time:21087ms step_avg:136.05ms
step:166/1405 train_time:21225ms step_avg:136.06ms
step:167/1405 train_time:21362ms step_avg:136.06ms
step:168/1405 train_time:21501ms step_avg:136.08ms
step:169/1405 train_time:21638ms step_avg:136.09ms
step:170/1405 train_time:21777ms step_avg:136.10ms
step:171/1405 train_time:21916ms step_avg:136.12ms
step:172/1405 train_time:22055ms step_avg:136.14ms
step:173/1405 train_time:22194ms step_avg:136.16ms
step:174/1405 train_time:22330ms step_avg:136.16ms
step:175/1405 train_time:22469ms step_avg:136.18ms
step:176/1405 train_time:22606ms step_avg:136.18ms
step:177/1405 train_time:22742ms step_avg:136.18ms
step:178/1405 train_time:22880ms step_avg:136.19ms
step:179/1405 train_time:23019ms step_avg:136.21ms
step:180/1405 train_time:23158ms step_avg:136.22ms
step:181/1405 train_time:23295ms step_avg:136.23ms
step:182/1405 train_time:23433ms step_avg:136.24ms
step:183/1405 train_time:23572ms step_avg:136.25ms
step:184/1405 train_time:23711ms step_avg:136.27ms
step:185/1405 train_time:23848ms step_avg:136.28ms
step:186/1405 train_time:23985ms step_avg:136.28ms
step:187/1405 train_time:24122ms step_avg:136.28ms
step:188/1405 train_time:24259ms step_avg:136.29ms
step:189/1405 train_time:24398ms step_avg:136.30ms
step:190/1405 train_time:24535ms step_avg:136.31ms
step:191/1405 train_time:24715ms step_avg:136.55ms
step:192/1405 train_time:24851ms step_avg:136.55ms
step:193/1405 train_time:24988ms step_avg:136.55ms
step:194/1405 train_time:25125ms step_avg:136.55ms
step:195/1405 train_time:25263ms step_avg:136.55ms
step:196/1405 train_time:25400ms step_avg:136.56ms
step:197/1405 train_time:25538ms step_avg:136.56ms
step:198/1405 train_time:25679ms step_avg:136.59ms
step:199/1405 train_time:25819ms step_avg:136.61ms
step:200/1405 train_time:25957ms step_avg:136.62ms
step:201/1405 train_time:26096ms step_avg:136.63ms
step:202/1405 train_time:26233ms step_avg:136.63ms
step:203/1405 train_time:26371ms step_avg:136.64ms
step:204/1405 train_time:26509ms step_avg:136.65ms
step:205/1405 train_time:26648ms step_avg:136.66ms
step:206/1405 train_time:26787ms step_avg:136.67ms
step:207/1405 train_time:26925ms step_avg:136.68ms
step:208/1405 train_time:27064ms step_avg:136.69ms
step:209/1405 train_time:27200ms step_avg:136.68ms
step:210/1405 train_time:27338ms step_avg:136.69ms
step:211/1405 train_time:27477ms step_avg:136.70ms
step:212/1405 train_time:27616ms step_avg:136.71ms
step:213/1405 train_time:27757ms step_avg:136.73ms
step:214/1405 train_time:27897ms step_avg:136.75ms
step:215/1405 train_time:28037ms step_avg:136.77ms
step:216/1405 train_time:28177ms step_avg:136.78ms
step:217/1405 train_time:28315ms step_avg:136.79ms
step:218/1405 train_time:28453ms step_avg:136.80ms
step:219/1405 train_time:28592ms step_avg:136.80ms
step:220/1405 train_time:28731ms step_avg:136.81ms
step:221/1405 train_time:28872ms step_avg:136.83ms
step:222/1405 train_time:29012ms step_avg:136.85ms
step:223/1405 train_time:29151ms step_avg:136.86ms
step:224/1405 train_time:29290ms step_avg:136.87ms
step:225/1405 train_time:29429ms step_avg:136.88ms
step:226/1405 train_time:29569ms step_avg:136.89ms
step:227/1405 train_time:29708ms step_avg:136.90ms
step:228/1405 train_time:29847ms step_avg:136.92ms
step:229/1405 train_time:29987ms step_avg:136.93ms
step:230/1405 train_time:30126ms step_avg:136.94ms
step:231/1405 train_time:30266ms step_avg:136.95ms
step:232/1405 train_time:30405ms step_avg:136.96ms
step:233/1405 train_time:30544ms step_avg:136.97ms
step:234/1405 train_time:30683ms step_avg:136.98ms
step:235/1405 train_time:30823ms step_avg:136.99ms
step:236/1405 train_time:30961ms step_avg:136.99ms
step:237/1405 train_time:31100ms step_avg:137.00ms
step:238/1405 train_time:31238ms step_avg:137.01ms
step:239/1405 train_time:31379ms step_avg:137.03ms
step:240/1405 train_time:31518ms step_avg:137.04ms
step:241/1405 train_time:31657ms step_avg:137.04ms
step:242/1405 train_time:31796ms step_avg:137.05ms
step:243/1405 train_time:31936ms step_avg:137.07ms
step:244/1405 train_time:32076ms step_avg:137.08ms
step:245/1405 train_time:32216ms step_avg:137.09ms
step:246/1405 train_time:32357ms step_avg:137.11ms
step:247/1405 train_time:32496ms step_avg:137.11ms
step:248/1405 train_time:32635ms step_avg:137.12ms
step:249/1405 train_time:32775ms step_avg:137.13ms
step:250/1405 train_time:32916ms step_avg:137.15ms
step:250/1405 val_loss:3.9688 train_time:32983ms step_avg:137.43ms
step:251/1405 train_time:33057ms step_avg:137.16ms
step:252/1405 train_time:33201ms step_avg:137.20ms
step:253/1405 train_time:33341ms step_avg:137.20ms
step:254/1405 train_time:33479ms step_avg:137.21ms
step:255/1405 train_time:33617ms step_avg:137.21ms
step:256/1405 train_time:33755ms step_avg:137.22ms
step:257/1405 train_time:33893ms step_avg:137.22ms
step:258/1405 train_time:34035ms step_avg:137.24ms
step:259/1405 train_time:34176ms step_avg:137.25ms
step:260/1405 train_time:34317ms step_avg:137.27ms
step:261/1405 train_time:34455ms step_avg:137.27ms
step:262/1405 train_time:34595ms step_avg:137.28ms
step:263/1405 train_time:34733ms step_avg:137.28ms
step:264/1405 train_time:34871ms step_avg:137.29ms
step:265/1405 train_time:35010ms step_avg:137.29ms
step:266/1405 train_time:35152ms step_avg:137.31ms
step:267/1405 train_time:35292ms step_avg:137.32ms
step:268/1405 train_time:35432ms step_avg:137.33ms
step:269/1405 train_time:35570ms step_avg:137.34ms
step:270/1405 train_time:35708ms step_avg:137.34ms
step:271/1405 train_time:35849ms step_avg:137.35ms
step:272/1405 train_time:35989ms step_avg:137.36ms
step:273/1405 train_time:36130ms step_avg:137.38ms
step:274/1405 train_time:36269ms step_avg:137.38ms
step:275/1405 train_time:36408ms step_avg:137.39ms
step:276/1405 train_time:36549ms step_avg:137.40ms
step:277/1405 train_time:36688ms step_avg:137.41ms
step:278/1405 train_time:36826ms step_avg:137.41ms
step:279/1405 train_time:36964ms step_avg:137.41ms
step:280/1405 train_time:37103ms step_avg:137.42ms
step:281/1405 train_time:37244ms step_avg:137.43ms
step:282/1405 train_time:37385ms step_avg:137.44ms
step:283/1405 train_time:37524ms step_avg:137.45ms
step:284/1405 train_time:37662ms step_avg:137.45ms
step:285/1405 train_time:37803ms step_avg:137.46ms
step:286/1405 train_time:37942ms step_avg:137.47ms
step:287/1405 train_time:38081ms step_avg:137.48ms
step:288/1405 train_time:38222ms step_avg:137.49ms
step:289/1405 train_time:38362ms step_avg:137.50ms
step:290/1405 train_time:38502ms step_avg:137.51ms
step:291/1405 train_time:38641ms step_avg:137.51ms
step:292/1405 train_time:38781ms step_avg:137.52ms
step:293/1405 train_time:38921ms step_avg:137.53ms
step:294/1405 train_time:39060ms step_avg:137.53ms
step:295/1405 train_time:39200ms step_avg:137.54ms
step:296/1405 train_time:39340ms step_avg:137.55ms
step:297/1405 train_time:39479ms step_avg:137.56ms
step:298/1405 train_time:39620ms step_avg:137.57ms
step:299/1405 train_time:39758ms step_avg:137.57ms
step:300/1405 train_time:39899ms step_avg:137.58ms
step:301/1405 train_time:40039ms step_avg:137.59ms
step:302/1405 train_time:40176ms step_avg:137.59ms
step:303/1405 train_time:40315ms step_avg:137.59ms
step:304/1405 train_time:40455ms step_avg:137.60ms
step:305/1405 train_time:40594ms step_avg:137.61ms
step:306/1405 train_time:40735ms step_avg:137.62ms
step:307/1405 train_time:40873ms step_avg:137.62ms
step:308/1405 train_time:41011ms step_avg:137.62ms
step:309/1405 train_time:41151ms step_avg:137.63ms
step:310/1405 train_time:41292ms step_avg:137.64ms
step:311/1405 train_time:41431ms step_avg:137.64ms
step:312/1405 train_time:41570ms step_avg:137.65ms
step:313/1405 train_time:41710ms step_avg:137.66ms
step:314/1405 train_time:41850ms step_avg:137.66ms
step:315/1405 train_time:41992ms step_avg:137.68ms
step:316/1405 train_time:42134ms step_avg:137.69ms
step:317/1405 train_time:42275ms step_avg:137.71ms
step:318/1405 train_time:42418ms step_avg:137.72ms
step:319/1405 train_time:42560ms step_avg:137.73ms
step:320/1405 train_time:42702ms step_avg:137.75ms
step:321/1405 train_time:42845ms step_avg:137.76ms
step:322/1405 train_time:42987ms step_avg:137.78ms
step:323/1405 train_time:43128ms step_avg:137.79ms
step:324/1405 train_time:43269ms step_avg:137.80ms
step:325/1405 train_time:43411ms step_avg:137.81ms
step:326/1405 train_time:43554ms step_avg:137.83ms
step:327/1405 train_time:43695ms step_avg:137.84ms
step:328/1405 train_time:43838ms step_avg:137.85ms
step:329/1405 train_time:43980ms step_avg:137.87ms
step:330/1405 train_time:44122ms step_avg:137.88ms
step:331/1405 train_time:44262ms step_avg:137.89ms
step:332/1405 train_time:44403ms step_avg:137.90ms
step:333/1405 train_time:44545ms step_avg:137.91ms
step:334/1405 train_time:44686ms step_avg:137.92ms
step:335/1405 train_time:44828ms step_avg:137.93ms
step:336/1405 train_time:44969ms step_avg:137.94ms
step:337/1405 train_time:45110ms step_avg:137.95ms
step:338/1405 train_time:45252ms step_avg:137.96ms
step:339/1405 train_time:45394ms step_avg:137.97ms
step:340/1405 train_time:45535ms step_avg:137.98ms
step:341/1405 train_time:45676ms step_avg:137.99ms
step:342/1405 train_time:45818ms step_avg:138.01ms
step:343/1405 train_time:45959ms step_avg:138.01ms
step:344/1405 train_time:46102ms step_avg:138.03ms
step:345/1405 train_time:46243ms step_avg:138.04ms
step:346/1405 train_time:46384ms step_avg:138.05ms
step:347/1405 train_time:46526ms step_avg:138.06ms
step:348/1405 train_time:46666ms step_avg:138.06ms
step:349/1405 train_time:46809ms step_avg:138.08ms
step:350/1405 train_time:46950ms step_avg:138.09ms
step:351/1405 train_time:47092ms step_avg:138.10ms
step:352/1405 train_time:47233ms step_avg:138.11ms
step:353/1405 train_time:47374ms step_avg:138.12ms
step:354/1405 train_time:47516ms step_avg:138.13ms
step:355/1405 train_time:47657ms step_avg:138.14ms
step:356/1405 train_time:47800ms step_avg:138.15ms
step:357/1405 train_time:47943ms step_avg:138.16ms
step:358/1405 train_time:48085ms step_avg:138.17ms
step:359/1405 train_time:48226ms step_avg:138.18ms
step:360/1405 train_time:48369ms step_avg:138.20ms
step:361/1405 train_time:48510ms step_avg:138.20ms
step:362/1405 train_time:48653ms step_avg:138.22ms
step:363/1405 train_time:48796ms step_avg:138.23ms
step:364/1405 train_time:48939ms step_avg:138.25ms
step:365/1405 train_time:49081ms step_avg:138.25ms
step:366/1405 train_time:49221ms step_avg:138.26ms
step:367/1405 train_time:49361ms step_avg:138.27ms
step:368/1405 train_time:49503ms step_avg:138.28ms
step:369/1405 train_time:49645ms step_avg:138.29ms
step:370/1405 train_time:49787ms step_avg:138.30ms
step:371/1405 train_time:49929ms step_avg:138.31ms
step:372/1405 train_time:50072ms step_avg:138.32ms
step:373/1405 train_time:50213ms step_avg:138.33ms
step:374/1405 train_time:50354ms step_avg:138.33ms
step:375/1405 train_time:50494ms step_avg:138.34ms
step:375/1405 val_loss:3.7835 train_time:50562ms step_avg:138.53ms
step:376/1405 train_time:50637ms step_avg:138.35ms
step:377/1405 train_time:50781ms step_avg:138.37ms
step:378/1405 train_time:50921ms step_avg:138.37ms
step:379/1405 train_time:51063ms step_avg:138.38ms
step:380/1405 train_time:51205ms step_avg:138.39ms
step:381/1405 train_time:51392ms step_avg:138.52ms
step:382/1405 train_time:51531ms step_avg:138.52ms
step:383/1405 train_time:51672ms step_avg:138.53ms
step:384/1405 train_time:51812ms step_avg:138.54ms
step:385/1405 train_time:51954ms step_avg:138.54ms
step:386/1405 train_time:52094ms step_avg:138.55ms
step:387/1405 train_time:52235ms step_avg:138.56ms
step:388/1405 train_time:52382ms step_avg:138.58ms
step:389/1405 train_time:52523ms step_avg:138.58ms
step:390/1405 train_time:52665ms step_avg:138.59ms
step:391/1405 train_time:52804ms step_avg:138.59ms
step:392/1405 train_time:52944ms step_avg:138.60ms
step:393/1405 train_time:53084ms step_avg:138.60ms
step:394/1405 train_time:53225ms step_avg:138.61ms
step:395/1405 train_time:53369ms step_avg:138.62ms
step:396/1405 train_time:53513ms step_avg:138.63ms
step:397/1405 train_time:53654ms step_avg:138.64ms
step:398/1405 train_time:53796ms step_avg:138.65ms
step:399/1405 train_time:53935ms step_avg:138.65ms
step:400/1405 train_time:54077ms step_avg:138.66ms
step:401/1405 train_time:54218ms step_avg:138.66ms
step:402/1405 train_time:54359ms step_avg:138.67ms
step:403/1405 train_time:54503ms step_avg:138.68ms
step:404/1405 train_time:54644ms step_avg:138.69ms
step:405/1405 train_time:54785ms step_avg:138.70ms
step:406/1405 train_time:54926ms step_avg:138.70ms
step:407/1405 train_time:55067ms step_avg:138.71ms
step:408/1405 train_time:55209ms step_avg:138.72ms
step:409/1405 train_time:55351ms step_avg:138.72ms
step:410/1405 train_time:55492ms step_avg:138.73ms
step:411/1405 train_time:55635ms step_avg:138.74ms
step:412/1405 train_time:55774ms step_avg:138.74ms
step:413/1405 train_time:55916ms step_avg:138.75ms
step:414/1405 train_time:56058ms step_avg:138.76ms
step:415/1405 train_time:56198ms step_avg:138.76ms
step:416/1405 train_time:56342ms step_avg:138.77ms
step:417/1405 train_time:56483ms step_avg:138.78ms
step:418/1405 train_time:56626ms step_avg:138.79ms
step:419/1405 train_time:56767ms step_avg:138.80ms
step:420/1405 train_time:56911ms step_avg:138.81ms
step:421/1405 train_time:57052ms step_avg:138.81ms
step:422/1405 train_time:57195ms step_avg:138.82ms
step:423/1405 train_time:57337ms step_avg:138.83ms
step:424/1405 train_time:57479ms step_avg:138.84ms
step:425/1405 train_time:57622ms step_avg:138.85ms
step:426/1405 train_time:57763ms step_avg:138.85ms
step:427/1405 train_time:57906ms step_avg:138.86ms
step:428/1405 train_time:58050ms step_avg:138.88ms
step:429/1405 train_time:58192ms step_avg:138.88ms
step:430/1405 train_time:58336ms step_avg:138.90ms
step:431/1405 train_time:58479ms step_avg:138.91ms
step:432/1405 train_time:58623ms step_avg:138.92ms
step:433/1405 train_time:58767ms step_avg:138.93ms
step:434/1405 train_time:58909ms step_avg:138.94ms
step:435/1405 train_time:59053ms step_avg:138.95ms
step:436/1405 train_time:59194ms step_avg:138.95ms
step:437/1405 train_time:59337ms step_avg:138.96ms
step:438/1405 train_time:59477ms step_avg:138.97ms
step:439/1405 train_time:59621ms step_avg:138.98ms
step:440/1405 train_time:59764ms step_avg:138.98ms
step:441/1405 train_time:59907ms step_avg:139.00ms
step:442/1405 train_time:60050ms step_avg:139.01ms
step:443/1405 train_time:60192ms step_avg:139.01ms
step:444/1405 train_time:60335ms step_avg:139.02ms
step:445/1405 train_time:60476ms step_avg:139.03ms
step:446/1405 train_time:60619ms step_avg:139.03ms
step:447/1405 train_time:60762ms step_avg:139.04ms
step:448/1405 train_time:60906ms step_avg:139.05ms
step:449/1405 train_time:61046ms step_avg:139.06ms
step:450/1405 train_time:61188ms step_avg:139.06ms
step:451/1405 train_time:61332ms step_avg:139.07ms
step:452/1405 train_time:61474ms step_avg:139.08ms
step:453/1405 train_time:61616ms step_avg:139.09ms
step:454/1405 train_time:61761ms step_avg:139.10ms
step:455/1405 train_time:61905ms step_avg:139.11ms
step:456/1405 train_time:62047ms step_avg:139.12ms
step:457/1405 train_time:62188ms step_avg:139.12ms
step:458/1405 train_time:62331ms step_avg:139.13ms
step:459/1405 train_time:62474ms step_avg:139.14ms
step:460/1405 train_time:62615ms step_avg:139.15ms
step:461/1405 train_time:62758ms step_avg:139.15ms
step:462/1405 train_time:62901ms step_avg:139.16ms
step:463/1405 train_time:63043ms step_avg:139.17ms
step:464/1405 train_time:63184ms step_avg:139.17ms
step:465/1405 train_time:63326ms step_avg:139.18ms
step:466/1405 train_time:63467ms step_avg:139.18ms
step:467/1405 train_time:63610ms step_avg:139.19ms
step:468/1405 train_time:63752ms step_avg:139.20ms
step:469/1405 train_time:63894ms step_avg:139.20ms
step:470/1405 train_time:64037ms step_avg:139.21ms
step:471/1405 train_time:64179ms step_avg:139.22ms
step:472/1405 train_time:64321ms step_avg:139.22ms
step:473/1405 train_time:64464ms step_avg:139.23ms
step:474/1405 train_time:64605ms step_avg:139.23ms
step:475/1405 train_time:64745ms step_avg:139.24ms
step:476/1405 train_time:64889ms step_avg:139.25ms
step:477/1405 train_time:65031ms step_avg:139.25ms
step:478/1405 train_time:65172ms step_avg:139.26ms
step:479/1405 train_time:65315ms step_avg:139.26ms
step:480/1405 train_time:65457ms step_avg:139.27ms
step:481/1405 train_time:65599ms step_avg:139.28ms
step:482/1405 train_time:65741ms step_avg:139.28ms
step:483/1405 train_time:65883ms step_avg:139.29ms
step:484/1405 train_time:66025ms step_avg:139.29ms
step:485/1405 train_time:66167ms step_avg:139.30ms
step:486/1405 train_time:66310ms step_avg:139.31ms
step:487/1405 train_time:66453ms step_avg:139.31ms
step:488/1405 train_time:66595ms step_avg:139.32ms
step:489/1405 train_time:66737ms step_avg:139.33ms
step:490/1405 train_time:66879ms step_avg:139.33ms
step:491/1405 train_time:67022ms step_avg:139.34ms
step:492/1405 train_time:67165ms step_avg:139.35ms
step:493/1405 train_time:67308ms step_avg:139.35ms
step:494/1405 train_time:67451ms step_avg:139.36ms
step:495/1405 train_time:67593ms step_avg:139.37ms
step:496/1405 train_time:67735ms step_avg:139.37ms
step:497/1405 train_time:67876ms step_avg:139.38ms
step:498/1405 train_time:68019ms step_avg:139.38ms
step:499/1405 train_time:68162ms step_avg:139.39ms
step:500/1405 train_time:68305ms step_avg:139.40ms
step:500/1405 val_loss:3.6643 train_time:68375ms step_avg:139.54ms
step:501/1405 train_time:68449ms step_avg:139.41ms
step:502/1405 train_time:68594ms step_avg:139.42ms
step:503/1405 train_time:68736ms step_avg:139.42ms
step:504/1405 train_time:68876ms step_avg:139.43ms
step:505/1405 train_time:69019ms step_avg:139.43ms
step:506/1405 train_time:69161ms step_avg:139.44ms
step:507/1405 train_time:69301ms step_avg:139.44ms
step:508/1405 train_time:69446ms step_avg:139.45ms
step:509/1405 train_time:69589ms step_avg:139.46ms
step:510/1405 train_time:69732ms step_avg:139.46ms
step:511/1405 train_time:69874ms step_avg:139.47ms
step:512/1405 train_time:70017ms step_avg:139.48ms
step:513/1405 train_time:70159ms step_avg:139.48ms
step:514/1405 train_time:70301ms step_avg:139.49ms
step:515/1405 train_time:70444ms step_avg:139.49ms
step:516/1405 train_time:70588ms step_avg:139.50ms
step:517/1405 train_time:70730ms step_avg:139.51ms
step:518/1405 train_time:70872ms step_avg:139.51ms
step:519/1405 train_time:71015ms step_avg:139.52ms
step:520/1405 train_time:71158ms step_avg:139.52ms
step:521/1405 train_time:71300ms step_avg:139.53ms
step:522/1405 train_time:71442ms step_avg:139.54ms
step:523/1405 train_time:71586ms step_avg:139.54ms
step:524/1405 train_time:71731ms step_avg:139.55ms
step:525/1405 train_time:71877ms step_avg:139.57ms
step:526/1405 train_time:72021ms step_avg:139.58ms
step:527/1405 train_time:72165ms step_avg:139.59ms
step:528/1405 train_time:72310ms step_avg:139.59ms
step:529/1405 train_time:72456ms step_avg:139.61ms
step:530/1405 train_time:72600ms step_avg:139.62ms
step:531/1405 train_time:72745ms step_avg:139.63ms
step:532/1405 train_time:72890ms step_avg:139.64ms
step:533/1405 train_time:73034ms step_avg:139.64ms
step:534/1405 train_time:73177ms step_avg:139.65ms
step:535/1405 train_time:73322ms step_avg:139.66ms
step:536/1405 train_time:73465ms step_avg:139.67ms
step:537/1405 train_time:73609ms step_avg:139.68ms
step:538/1405 train_time:73754ms step_avg:139.68ms
step:539/1405 train_time:73898ms step_avg:139.69ms
step:540/1405 train_time:74041ms step_avg:139.70ms
step:541/1405 train_time:74185ms step_avg:139.71ms
step:542/1405 train_time:74329ms step_avg:139.72ms
step:543/1405 train_time:74475ms step_avg:139.73ms
step:544/1405 train_time:74620ms step_avg:139.74ms
step:545/1405 train_time:74765ms step_avg:139.75ms
step:546/1405 train_time:74909ms step_avg:139.76ms
step:547/1405 train_time:75053ms step_avg:139.76ms
step:548/1405 train_time:75196ms step_avg:139.77ms
step:549/1405 train_time:75341ms step_avg:139.78ms
step:550/1405 train_time:75487ms step_avg:139.79ms
step:551/1405 train_time:75632ms step_avg:139.80ms
step:552/1405 train_time:75777ms step_avg:139.81ms
step:553/1405 train_time:75924ms step_avg:139.82ms
step:554/1405 train_time:76066ms step_avg:139.83ms
step:555/1405 train_time:76211ms step_avg:139.84ms
step:556/1405 train_time:76354ms step_avg:139.84ms
step:557/1405 train_time:76498ms step_avg:139.85ms
step:558/1405 train_time:76643ms step_avg:139.86ms
step:559/1405 train_time:76786ms step_avg:139.86ms
step:560/1405 train_time:76931ms step_avg:139.87ms
step:561/1405 train_time:77075ms step_avg:139.88ms
step:562/1405 train_time:77218ms step_avg:139.89ms
step:563/1405 train_time:77362ms step_avg:139.89ms
step:564/1405 train_time:77506ms step_avg:139.90ms
step:565/1405 train_time:77651ms step_avg:139.91ms
step:566/1405 train_time:77795ms step_avg:139.92ms
step:567/1405 train_time:77941ms step_avg:139.93ms
step:568/1405 train_time:78085ms step_avg:139.94ms
step:569/1405 train_time:78229ms step_avg:139.94ms
step:570/1405 train_time:78373ms step_avg:139.95ms
step:571/1405 train_time:78561ms step_avg:140.04ms
step:572/1405 train_time:78703ms step_avg:140.04ms
step:573/1405 train_time:78846ms step_avg:140.05ms
step:574/1405 train_time:78989ms step_avg:140.05ms
step:575/1405 train_time:79132ms step_avg:140.06ms
step:576/1405 train_time:79276ms step_avg:140.06ms
step:577/1405 train_time:79421ms step_avg:140.07ms
step:578/1405 train_time:79569ms step_avg:140.09ms
step:579/1405 train_time:79714ms step_avg:140.10ms
step:580/1405 train_time:79858ms step_avg:140.10ms
step:581/1405 train_time:80000ms step_avg:140.11ms
step:582/1405 train_time:80143ms step_avg:140.11ms
step:583/1405 train_time:80285ms step_avg:140.11ms
step:584/1405 train_time:80430ms step_avg:140.12ms
step:585/1405 train_time:80575ms step_avg:140.13ms
step:586/1405 train_time:80721ms step_avg:140.14ms
step:587/1405 train_time:80865ms step_avg:140.15ms
step:588/1405 train_time:81009ms step_avg:140.15ms
step:589/1405 train_time:81153ms step_avg:140.16ms
step:590/1405 train_time:81295ms step_avg:140.16ms
step:591/1405 train_time:81440ms step_avg:140.17ms
step:592/1405 train_time:81585ms step_avg:140.18ms
step:593/1405 train_time:81730ms step_avg:140.19ms
step:594/1405 train_time:81875ms step_avg:140.20ms
step:595/1405 train_time:82019ms step_avg:140.20ms
step:596/1405 train_time:82163ms step_avg:140.21ms
step:597/1405 train_time:82306ms step_avg:140.22ms
step:598/1405 train_time:82451ms step_avg:140.22ms
step:599/1405 train_time:82595ms step_avg:140.23ms
step:600/1405 train_time:82740ms step_avg:140.24ms
step:601/1405 train_time:82885ms step_avg:140.25ms
step:602/1405 train_time:83028ms step_avg:140.25ms
step:603/1405 train_time:83175ms step_avg:140.26ms
step:604/1405 train_time:83320ms step_avg:140.27ms
step:605/1405 train_time:83463ms step_avg:140.27ms
step:606/1405 train_time:83606ms step_avg:140.28ms
step:607/1405 train_time:83750ms step_avg:140.28ms
step:608/1405 train_time:83894ms step_avg:140.29ms
step:609/1405 train_time:84039ms step_avg:140.30ms
step:610/1405 train_time:84183ms step_avg:140.31ms
step:611/1405 train_time:84328ms step_avg:140.31ms
step:612/1405 train_time:84474ms step_avg:140.32ms
step:613/1405 train_time:84620ms step_avg:140.33ms
step:614/1405 train_time:84764ms step_avg:140.34ms
step:615/1405 train_time:84908ms step_avg:140.34ms
step:616/1405 train_time:85053ms step_avg:140.35ms
step:617/1405 train_time:85198ms step_avg:140.36ms
step:618/1405 train_time:85342ms step_avg:140.36ms
step:619/1405 train_time:85487ms step_avg:140.37ms
step:620/1405 train_time:85631ms step_avg:140.38ms
step:621/1405 train_time:85776ms step_avg:140.39ms
step:622/1405 train_time:85921ms step_avg:140.39ms
step:623/1405 train_time:86064ms step_avg:140.40ms
step:624/1405 train_time:86207ms step_avg:140.40ms
step:625/1405 train_time:86352ms step_avg:140.41ms
step:625/1405 val_loss:3.5812 train_time:86422ms step_avg:140.52ms
step:626/1405 train_time:86497ms step_avg:140.42ms
step:627/1405 train_time:86644ms step_avg:140.43ms
step:628/1405 train_time:86788ms step_avg:140.43ms
step:629/1405 train_time:86932ms step_avg:140.44ms
step:630/1405 train_time:87076ms step_avg:140.45ms
step:631/1405 train_time:87219ms step_avg:140.45ms
step:632/1405 train_time:87366ms step_avg:140.46ms
step:633/1405 train_time:87515ms step_avg:140.47ms
step:634/1405 train_time:87661ms step_avg:140.48ms
step:635/1405 train_time:87806ms step_avg:140.49ms
step:636/1405 train_time:87951ms step_avg:140.50ms
step:637/1405 train_time:88094ms step_avg:140.50ms
step:638/1405 train_time:88237ms step_avg:140.51ms
step:639/1405 train_time:88381ms step_avg:140.51ms
step:640/1405 train_time:88528ms step_avg:140.52ms
step:641/1405 train_time:88673ms step_avg:140.53ms
step:642/1405 train_time:88820ms step_avg:140.54ms
step:643/1405 train_time:88964ms step_avg:140.54ms
step:644/1405 train_time:89108ms step_avg:140.55ms
step:645/1405 train_time:89252ms step_avg:140.55ms
step:646/1405 train_time:89395ms step_avg:140.56ms
step:647/1405 train_time:89539ms step_avg:140.56ms
step:648/1405 train_time:89685ms step_avg:140.57ms
step:649/1405 train_time:89831ms step_avg:140.58ms
step:650/1405 train_time:89977ms step_avg:140.59ms
step:651/1405 train_time:90122ms step_avg:140.60ms
step:652/1405 train_time:90267ms step_avg:140.60ms
step:653/1405 train_time:90412ms step_avg:140.61ms
step:654/1405 train_time:90556ms step_avg:140.62ms
step:655/1405 train_time:90702ms step_avg:140.62ms
step:656/1405 train_time:90846ms step_avg:140.63ms
step:657/1405 train_time:90991ms step_avg:140.64ms
step:658/1405 train_time:91138ms step_avg:140.65ms
step:659/1405 train_time:91283ms step_avg:140.65ms
step:660/1405 train_time:91428ms step_avg:140.66ms
step:661/1405 train_time:91572ms step_avg:140.66ms
step:662/1405 train_time:91718ms step_avg:140.67ms
step:663/1405 train_time:91865ms step_avg:140.68ms
step:664/1405 train_time:92011ms step_avg:140.69ms
step:665/1405 train_time:92158ms step_avg:140.70ms
step:666/1405 train_time:92303ms step_avg:140.71ms
step:667/1405 train_time:92447ms step_avg:140.71ms
step:668/1405 train_time:92591ms step_avg:140.72ms
step:669/1405 train_time:92737ms step_avg:140.72ms
step:670/1405 train_time:92884ms step_avg:140.73ms
step:671/1405 train_time:93029ms step_avg:140.74ms
step:672/1405 train_time:93174ms step_avg:140.75ms
step:673/1405 train_time:93320ms step_avg:140.75ms
step:674/1405 train_time:93464ms step_avg:140.76ms
step:675/1405 train_time:93608ms step_avg:140.76ms
step:676/1405 train_time:93754ms step_avg:140.77ms
step:677/1405 train_time:93899ms step_avg:140.78ms
step:678/1405 train_time:94044ms step_avg:140.78ms
step:679/1405 train_time:94189ms step_avg:140.79ms
step:680/1405 train_time:94334ms step_avg:140.80ms
step:681/1405 train_time:94481ms step_avg:140.81ms
step:682/1405 train_time:94625ms step_avg:140.81ms
step:683/1405 train_time:94769ms step_avg:140.82ms
step:684/1405 train_time:94915ms step_avg:140.82ms
step:685/1405 train_time:95061ms step_avg:140.83ms
step:686/1405 train_time:95206ms step_avg:140.84ms
step:687/1405 train_time:95351ms step_avg:140.84ms
step:688/1405 train_time:95496ms step_avg:140.85ms
step:689/1405 train_time:95640ms step_avg:140.85ms
step:690/1405 train_time:95784ms step_avg:140.86ms
step:691/1405 train_time:95929ms step_avg:140.87ms
step:692/1405 train_time:96074ms step_avg:140.87ms
step:693/1405 train_time:96218ms step_avg:140.88ms
step:694/1405 train_time:96363ms step_avg:140.88ms
step:695/1405 train_time:96508ms step_avg:140.89ms
step:696/1405 train_time:96653ms step_avg:140.89ms
step:697/1405 train_time:96797ms step_avg:140.90ms
step:698/1405 train_time:96941ms step_avg:140.90ms
step:699/1405 train_time:97087ms step_avg:140.91ms
step:700/1405 train_time:97231ms step_avg:140.91ms
step:701/1405 train_time:97375ms step_avg:140.92ms
step:702/1405 train_time:97522ms step_avg:140.93ms
step:703/1405 train_time:97667ms step_avg:140.93ms
step:704/1405 train_time:97811ms step_avg:140.94ms
step:705/1405 train_time:97957ms step_avg:140.95ms
step:706/1405 train_time:98104ms step_avg:140.95ms
step:707/1405 train_time:98249ms step_avg:140.96ms
step:708/1405 train_time:98393ms step_avg:140.96ms
step:709/1405 train_time:98538ms step_avg:140.97ms
step:710/1405 train_time:98684ms step_avg:140.98ms
step:711/1405 train_time:98830ms step_avg:140.98ms
step:712/1405 train_time:98976ms step_avg:140.99ms
step:713/1405 train_time:99120ms step_avg:141.00ms
step:714/1405 train_time:99266ms step_avg:141.00ms
step:715/1405 train_time:99411ms step_avg:141.01ms
step:716/1405 train_time:99556ms step_avg:141.01ms
step:717/1405 train_time:99702ms step_avg:141.02ms
step:718/1405 train_time:99847ms step_avg:141.03ms
step:719/1405 train_time:99992ms step_avg:141.03ms
step:720/1405 train_time:100140ms step_avg:141.04ms
step:721/1405 train_time:100285ms step_avg:141.05ms
step:722/1405 train_time:100431ms step_avg:141.06ms
step:723/1405 train_time:100578ms step_avg:141.06ms
step:724/1405 train_time:100722ms step_avg:141.07ms
step:725/1405 train_time:100866ms step_avg:141.07ms
step:726/1405 train_time:101012ms step_avg:141.08ms
step:727/1405 train_time:101157ms step_avg:141.08ms
step:728/1405 train_time:101301ms step_avg:141.09ms
step:729/1405 train_time:101448ms step_avg:141.10ms
step:730/1405 train_time:101593ms step_avg:141.10ms
step:731/1405 train_time:101741ms step_avg:141.11ms
step:732/1405 train_time:101888ms step_avg:141.12ms
step:733/1405 train_time:102034ms step_avg:141.13ms
step:734/1405 train_time:102183ms step_avg:141.14ms
step:735/1405 train_time:102330ms step_avg:141.15ms
step:736/1405 train_time:102476ms step_avg:141.15ms
step:737/1405 train_time:102624ms step_avg:141.16ms
step:738/1405 train_time:102770ms step_avg:141.17ms
step:739/1405 train_time:102917ms step_avg:141.18ms
step:740/1405 train_time:103063ms step_avg:141.18ms
step:741/1405 train_time:103211ms step_avg:141.19ms
step:742/1405 train_time:103358ms step_avg:141.20ms
step:743/1405 train_time:103505ms step_avg:141.21ms
step:744/1405 train_time:103652ms step_avg:141.22ms
step:745/1405 train_time:103801ms step_avg:141.23ms
step:746/1405 train_time:103948ms step_avg:141.23ms
step:747/1405 train_time:104094ms step_avg:141.24ms
step:748/1405 train_time:104242ms step_avg:141.25ms
step:749/1405 train_time:104389ms step_avg:141.26ms
step:750/1405 train_time:104535ms step_avg:141.26ms
step:750/1405 val_loss:3.5279 train_time:104609ms step_avg:141.36ms
step:751/1405 train_time:104684ms step_avg:141.27ms
step:752/1405 train_time:104833ms step_avg:141.28ms
step:753/1405 train_time:104978ms step_avg:141.29ms
step:754/1405 train_time:105123ms step_avg:141.29ms
step:755/1405 train_time:105270ms step_avg:141.30ms
step:756/1405 train_time:105415ms step_avg:141.31ms
step:757/1405 train_time:105563ms step_avg:141.32ms
step:758/1405 train_time:105712ms step_avg:141.33ms
step:759/1405 train_time:105859ms step_avg:141.33ms
step:760/1405 train_time:106005ms step_avg:141.34ms
step:761/1405 train_time:106194ms step_avg:141.40ms
step:762/1405 train_time:106344ms step_avg:141.41ms
step:763/1405 train_time:106490ms step_avg:141.42ms
step:764/1405 train_time:106636ms step_avg:141.43ms
step:765/1405 train_time:106780ms step_avg:141.43ms
step:766/1405 train_time:106927ms step_avg:141.44ms
step:767/1405 train_time:107075ms step_avg:141.45ms
step:768/1405 train_time:107224ms step_avg:141.46ms
step:769/1405 train_time:107372ms step_avg:141.47ms
step:770/1405 train_time:107518ms step_avg:141.47ms
step:771/1405 train_time:107664ms step_avg:141.48ms
step:772/1405 train_time:107809ms step_avg:141.48ms
step:773/1405 train_time:107955ms step_avg:141.49ms
step:774/1405 train_time:108104ms step_avg:141.50ms
step:775/1405 train_time:108251ms step_avg:141.51ms
step:776/1405 train_time:108400ms step_avg:141.51ms
step:777/1405 train_time:108548ms step_avg:141.52ms
step:778/1405 train_time:108693ms step_avg:141.53ms
step:779/1405 train_time:108839ms step_avg:141.53ms
step:780/1405 train_time:108986ms step_avg:141.54ms
step:781/1405 train_time:109134ms step_avg:141.55ms
step:782/1405 train_time:109279ms step_avg:141.55ms
step:783/1405 train_time:109426ms step_avg:141.56ms
step:784/1405 train_time:109573ms step_avg:141.57ms
step:785/1405 train_time:109719ms step_avg:141.57ms
step:786/1405 train_time:109866ms step_avg:141.58ms
step:787/1405 train_time:110013ms step_avg:141.59ms
step:788/1405 train_time:110160ms step_avg:141.59ms
step:789/1405 train_time:110307ms step_avg:141.60ms
step:790/1405 train_time:110454ms step_avg:141.61ms
step:791/1405 train_time:110602ms step_avg:141.62ms
step:792/1405 train_time:110748ms step_avg:141.62ms
step:793/1405 train_time:110895ms step_avg:141.63ms
step:794/1405 train_time:111041ms step_avg:141.63ms
step:795/1405 train_time:111189ms step_avg:141.64ms
step:796/1405 train_time:111336ms step_avg:141.65ms
step:797/1405 train_time:111482ms step_avg:141.65ms
step:798/1405 train_time:111629ms step_avg:141.66ms
step:799/1405 train_time:111776ms step_avg:141.67ms
step:800/1405 train_time:111922ms step_avg:141.67ms
step:801/1405 train_time:112068ms step_avg:141.68ms
step:802/1405 train_time:112216ms step_avg:141.69ms
step:803/1405 train_time:112361ms step_avg:141.69ms
step:804/1405 train_time:112508ms step_avg:141.70ms
step:805/1405 train_time:112657ms step_avg:141.71ms
step:806/1405 train_time:112804ms step_avg:141.71ms
step:807/1405 train_time:112951ms step_avg:141.72ms
step:808/1405 train_time:113098ms step_avg:141.73ms
step:809/1405 train_time:113245ms step_avg:141.73ms
step:810/1405 train_time:113391ms step_avg:141.74ms
step:811/1405 train_time:113537ms step_avg:141.74ms
step:812/1405 train_time:113683ms step_avg:141.75ms
step:813/1405 train_time:113830ms step_avg:141.76ms
step:814/1405 train_time:113975ms step_avg:141.76ms
step:815/1405 train_time:114122ms step_avg:141.77ms
step:816/1405 train_time:114269ms step_avg:141.77ms
step:817/1405 train_time:114415ms step_avg:141.78ms
step:818/1405 train_time:114562ms step_avg:141.78ms
step:819/1405 train_time:114709ms step_avg:141.79ms
step:820/1405 train_time:114855ms step_avg:141.80ms
step:821/1405 train_time:115001ms step_avg:141.80ms
step:822/1405 train_time:115147ms step_avg:141.81ms
step:823/1405 train_time:115294ms step_avg:141.81ms
step:824/1405 train_time:115440ms step_avg:141.82ms
step:825/1405 train_time:115589ms step_avg:141.83ms
step:826/1405 train_time:115736ms step_avg:141.83ms
step:827/1405 train_time:115881ms step_avg:141.84ms
step:828/1405 train_time:116028ms step_avg:141.84ms
step:829/1405 train_time:116175ms step_avg:141.85ms
step:830/1405 train_time:116322ms step_avg:141.86ms
step:831/1405 train_time:116468ms step_avg:141.86ms
step:832/1405 train_time:116616ms step_avg:141.87ms
step:833/1405 train_time:116763ms step_avg:141.87ms
step:834/1405 train_time:116910ms step_avg:141.88ms
step:835/1405 train_time:117056ms step_avg:141.89ms
step:836/1405 train_time:117206ms step_avg:141.90ms
step:837/1405 train_time:117354ms step_avg:141.90ms
step:838/1405 train_time:117502ms step_avg:141.91ms
step:839/1405 train_time:117649ms step_avg:141.92ms
step:840/1405 train_time:117796ms step_avg:141.92ms
step:841/1405 train_time:117943ms step_avg:141.93ms
step:842/1405 train_time:118091ms step_avg:141.94ms
step:843/1405 train_time:118237ms step_avg:141.94ms
step:844/1405 train_time:118384ms step_avg:141.95ms
step:845/1405 train_time:118532ms step_avg:141.95ms
step:846/1405 train_time:118677ms step_avg:141.96ms
step:847/1405 train_time:118827ms step_avg:141.97ms
step:848/1405 train_time:118975ms step_avg:141.97ms
step:849/1405 train_time:119123ms step_avg:141.98ms
step:850/1405 train_time:119272ms step_avg:141.99ms
step:851/1405 train_time:119420ms step_avg:142.00ms
step:852/1405 train_time:119567ms step_avg:142.00ms
step:853/1405 train_time:119713ms step_avg:142.01ms
step:854/1405 train_time:119862ms step_avg:142.02ms
step:855/1405 train_time:120010ms step_avg:142.02ms
step:856/1405 train_time:120155ms step_avg:142.03ms
step:857/1405 train_time:120303ms step_avg:142.03ms
step:858/1405 train_time:120452ms step_avg:142.04ms
step:859/1405 train_time:120598ms step_avg:142.05ms
step:860/1405 train_time:120745ms step_avg:142.05ms
step:861/1405 train_time:120893ms step_avg:142.06ms
step:862/1405 train_time:121039ms step_avg:142.06ms
step:863/1405 train_time:121187ms step_avg:142.07ms
step:864/1405 train_time:121334ms step_avg:142.08ms
step:865/1405 train_time:121479ms step_avg:142.08ms
step:866/1405 train_time:121627ms step_avg:142.09ms
step:867/1405 train_time:121774ms step_avg:142.09ms
step:868/1405 train_time:121919ms step_avg:142.10ms
step:869/1405 train_time:122066ms step_avg:142.10ms
step:870/1405 train_time:122214ms step_avg:142.11ms
step:871/1405 train_time:122362ms step_avg:142.12ms
step:872/1405 train_time:122508ms step_avg:142.12ms
step:873/1405 train_time:122654ms step_avg:142.13ms
step:874/1405 train_time:122804ms step_avg:142.13ms
step:875/1405 train_time:122952ms step_avg:142.14ms
step:875/1405 val_loss:3.4769 train_time:123025ms step_avg:142.23ms
step:876/1405 train_time:123100ms step_avg:142.15ms
step:877/1405 train_time:123247ms step_avg:142.15ms
step:878/1405 train_time:123395ms step_avg:142.16ms
step:879/1405 train_time:123542ms step_avg:142.17ms
step:880/1405 train_time:123690ms step_avg:142.17ms
step:881/1405 train_time:123834ms step_avg:142.17ms
step:882/1405 train_time:123983ms step_avg:142.18ms
step:883/1405 train_time:124130ms step_avg:142.19ms
step:884/1405 train_time:124278ms step_avg:142.19ms
step:885/1405 train_time:124427ms step_avg:142.20ms
step:886/1405 train_time:124574ms step_avg:142.21ms
step:887/1405 train_time:124721ms step_avg:142.21ms
step:888/1405 train_time:124869ms step_avg:142.22ms
step:889/1405 train_time:125016ms step_avg:142.23ms
step:890/1405 train_time:125163ms step_avg:142.23ms
step:891/1405 train_time:125310ms step_avg:142.24ms
step:892/1405 train_time:125458ms step_avg:142.24ms
step:893/1405 train_time:125606ms step_avg:142.25ms
step:894/1405 train_time:125754ms step_avg:142.26ms
step:895/1405 train_time:125902ms step_avg:142.26ms
step:896/1405 train_time:126051ms step_avg:142.27ms
step:897/1405 train_time:126198ms step_avg:142.28ms
step:898/1405 train_time:126346ms step_avg:142.28ms
step:899/1405 train_time:126493ms step_avg:142.29ms
step:900/1405 train_time:126640ms step_avg:142.29ms
step:901/1405 train_time:126789ms step_avg:142.30ms
step:902/1405 train_time:126934ms step_avg:142.30ms
step:903/1405 train_time:127083ms step_avg:142.31ms
step:904/1405 train_time:127230ms step_avg:142.31ms
step:905/1405 train_time:127374ms step_avg:142.32ms
step:906/1405 train_time:127523ms step_avg:142.32ms
step:907/1405 train_time:127672ms step_avg:142.33ms
step:908/1405 train_time:127818ms step_avg:142.34ms
step:909/1405 train_time:127967ms step_avg:142.34ms
step:910/1405 train_time:128116ms step_avg:142.35ms
step:911/1405 train_time:128263ms step_avg:142.36ms
step:912/1405 train_time:128411ms step_avg:142.36ms
step:913/1405 train_time:128557ms step_avg:142.37ms
step:914/1405 train_time:128704ms step_avg:142.37ms
step:915/1405 train_time:128853ms step_avg:142.38ms
step:916/1405 train_time:129002ms step_avg:142.39ms
step:917/1405 train_time:129149ms step_avg:142.39ms
step:918/1405 train_time:129297ms step_avg:142.40ms
step:919/1405 train_time:129446ms step_avg:142.40ms
step:920/1405 train_time:129593ms step_avg:142.41ms
step:921/1405 train_time:129741ms step_avg:142.42ms
step:922/1405 train_time:129890ms step_avg:142.42ms
step:923/1405 train_time:130038ms step_avg:142.43ms
step:924/1405 train_time:130186ms step_avg:142.44ms
step:925/1405 train_time:130334ms step_avg:142.44ms
step:926/1405 train_time:130482ms step_avg:142.45ms
step:927/1405 train_time:130628ms step_avg:142.45ms
step:928/1405 train_time:130776ms step_avg:142.46ms
step:929/1405 train_time:130923ms step_avg:142.46ms
step:930/1405 train_time:131072ms step_avg:142.47ms
step:931/1405 train_time:131219ms step_avg:142.47ms
step:932/1405 train_time:131367ms step_avg:142.48ms
step:933/1405 train_time:131514ms step_avg:142.48ms
step:934/1405 train_time:131660ms step_avg:142.49ms
step:935/1405 train_time:131807ms step_avg:142.49ms
step:936/1405 train_time:131954ms step_avg:142.50ms
step:937/1405 train_time:132101ms step_avg:142.50ms
step:938/1405 train_time:132249ms step_avg:142.51ms
step:939/1405 train_time:132398ms step_avg:142.52ms
step:940/1405 train_time:132548ms step_avg:142.52ms
step:941/1405 train_time:132696ms step_avg:142.53ms
step:942/1405 train_time:132845ms step_avg:142.54ms
step:943/1405 train_time:132995ms step_avg:142.55ms
step:944/1405 train_time:133148ms step_avg:142.56ms
step:945/1405 train_time:133295ms step_avg:142.56ms
step:946/1405 train_time:133445ms step_avg:142.57ms
step:947/1405 train_time:133594ms step_avg:142.58ms
step:948/1405 train_time:133742ms step_avg:142.58ms
step:949/1405 train_time:133892ms step_avg:142.59ms
step:950/1405 train_time:134041ms step_avg:142.60ms
step:951/1405 train_time:134231ms step_avg:142.65ms
step:952/1405 train_time:134377ms step_avg:142.65ms
step:953/1405 train_time:134527ms step_avg:142.66ms
step:954/1405 train_time:134676ms step_avg:142.67ms
step:955/1405 train_time:134823ms step_avg:142.67ms
step:956/1405 train_time:134972ms step_avg:142.68ms
step:957/1405 train_time:135120ms step_avg:142.68ms
step:958/1405 train_time:135274ms step_avg:142.69ms
step:959/1405 train_time:135424ms step_avg:142.70ms
step:960/1405 train_time:135574ms step_avg:142.71ms
step:961/1405 train_time:135722ms step_avg:142.72ms
step:962/1405 train_time:135872ms step_avg:142.72ms
step:963/1405 train_time:136022ms step_avg:142.73ms
step:964/1405 train_time:136171ms step_avg:142.74ms
step:965/1405 train_time:136320ms step_avg:142.74ms
step:966/1405 train_time:136470ms step_avg:142.75ms
step:967/1405 train_time:136617ms step_avg:142.76ms
step:968/1405 train_time:136764ms step_avg:142.76ms
step:969/1405 train_time:136913ms step_avg:142.77ms
step:970/1405 train_time:137059ms step_avg:142.77ms
step:971/1405 train_time:137210ms step_avg:142.78ms
step:972/1405 train_time:137358ms step_avg:142.78ms
step:973/1405 train_time:137507ms step_avg:142.79ms
step:974/1405 train_time:137655ms step_avg:142.80ms
step:975/1405 train_time:137805ms step_avg:142.80ms
step:976/1405 train_time:137953ms step_avg:142.81ms
step:977/1405 train_time:138102ms step_avg:142.81ms
step:978/1405 train_time:138252ms step_avg:142.82ms
step:979/1405 train_time:138399ms step_avg:142.83ms
step:980/1405 train_time:138548ms step_avg:142.83ms
step:981/1405 train_time:138695ms step_avg:142.84ms
step:982/1405 train_time:138845ms step_avg:142.84ms
step:983/1405 train_time:138992ms step_avg:142.85ms
step:984/1405 train_time:139140ms step_avg:142.85ms
step:985/1405 train_time:139289ms step_avg:142.86ms
step:986/1405 train_time:139438ms step_avg:142.87ms
step:987/1405 train_time:139586ms step_avg:142.87ms
step:988/1405 train_time:139733ms step_avg:142.88ms
step:989/1405 train_time:139883ms step_avg:142.88ms
step:990/1405 train_time:140031ms step_avg:142.89ms
step:991/1405 train_time:140179ms step_avg:142.89ms
step:992/1405 train_time:140330ms step_avg:142.90ms
step:993/1405 train_time:140484ms step_avg:142.91ms
step:994/1405 train_time:140632ms step_avg:142.92ms
step:995/1405 train_time:140780ms step_avg:142.92ms
step:996/1405 train_time:140928ms step_avg:142.93ms
step:997/1405 train_time:141075ms step_avg:142.93ms
step:998/1405 train_time:141224ms step_avg:142.94ms
step:999/1405 train_time:141372ms step_avg:142.94ms
step:1000/1405 train_time:141521ms step_avg:142.95ms
step:1000/1405 val_loss:3.4115 train_time:141595ms step_avg:143.03ms
step:1001/1405 train_time:141670ms step_avg:142.96ms
step:1002/1405 train_time:141822ms step_avg:142.97ms
step:1003/1405 train_time:141972ms step_avg:142.97ms
step:1004/1405 train_time:142122ms step_avg:142.98ms
step:1005/1405 train_time:142269ms step_avg:142.98ms
step:1006/1405 train_time:142418ms step_avg:142.99ms
step:1007/1405 train_time:142567ms step_avg:143.00ms
step:1008/1405 train_time:142718ms step_avg:143.00ms
step:1009/1405 train_time:142871ms step_avg:143.01ms
step:1010/1405 train_time:143019ms step_avg:143.02ms
step:1011/1405 train_time:143167ms step_avg:143.02ms
step:1012/1405 train_time:143315ms step_avg:143.03ms
step:1013/1405 train_time:143463ms step_avg:143.03ms
step:1014/1405 train_time:143612ms step_avg:143.04ms
step:1015/1405 train_time:143761ms step_avg:143.05ms
step:1016/1405 train_time:143910ms step_avg:143.05ms
step:1017/1405 train_time:144060ms step_avg:143.06ms
step:1018/1405 train_time:144208ms step_avg:143.06ms
step:1019/1405 train_time:144357ms step_avg:143.07ms
step:1020/1405 train_time:144506ms step_avg:143.08ms
step:1021/1405 train_time:144656ms step_avg:143.08ms
step:1022/1405 train_time:144803ms step_avg:143.09ms
step:1023/1405 train_time:144952ms step_avg:143.09ms
step:1024/1405 train_time:145100ms step_avg:143.10ms
step:1025/1405 train_time:145248ms step_avg:143.10ms
step:1026/1405 train_time:145395ms step_avg:143.11ms
step:1027/1405 train_time:145543ms step_avg:143.11ms
step:1028/1405 train_time:145691ms step_avg:143.12ms
step:1029/1405 train_time:145841ms step_avg:143.12ms
step:1030/1405 train_time:145990ms step_avg:143.13ms
step:1031/1405 train_time:146138ms step_avg:143.13ms
step:1032/1405 train_time:146285ms step_avg:143.14ms
step:1033/1405 train_time:146435ms step_avg:143.14ms
step:1034/1405 train_time:146583ms step_avg:143.15ms
step:1035/1405 train_time:146732ms step_avg:143.15ms
step:1036/1405 train_time:146882ms step_avg:143.16ms
step:1037/1405 train_time:147030ms step_avg:143.16ms
step:1038/1405 train_time:147180ms step_avg:143.17ms
step:1039/1405 train_time:147329ms step_avg:143.18ms
step:1040/1405 train_time:147477ms step_avg:143.18ms
step:1041/1405 train_time:147625ms step_avg:143.19ms
step:1042/1405 train_time:147774ms step_avg:143.19ms
step:1043/1405 train_time:147922ms step_avg:143.20ms
step:1044/1405 train_time:148072ms step_avg:143.20ms
step:1045/1405 train_time:148223ms step_avg:143.21ms
step:1046/1405 train_time:148372ms step_avg:143.22ms
step:1047/1405 train_time:148521ms step_avg:143.22ms
step:1048/1405 train_time:148671ms step_avg:143.23ms
step:1049/1405 train_time:148822ms step_avg:143.24ms
step:1050/1405 train_time:148973ms step_avg:143.24ms
step:1051/1405 train_time:149123ms step_avg:143.25ms
step:1052/1405 train_time:149273ms step_avg:143.26ms
step:1053/1405 train_time:149421ms step_avg:143.26ms
step:1054/1405 train_time:149572ms step_avg:143.27ms
step:1055/1405 train_time:149721ms step_avg:143.27ms
step:1056/1405 train_time:149872ms step_avg:143.28ms
step:1057/1405 train_time:150021ms step_avg:143.29ms
step:1058/1405 train_time:150172ms step_avg:143.29ms
step:1059/1405 train_time:150322ms step_avg:143.30ms
step:1060/1405 train_time:150471ms step_avg:143.31ms
step:1061/1405 train_time:150619ms step_avg:143.31ms
step:1062/1405 train_time:150769ms step_avg:143.32ms
step:1063/1405 train_time:150919ms step_avg:143.32ms
step:1064/1405 train_time:151067ms step_avg:143.33ms
step:1065/1405 train_time:151216ms step_avg:143.33ms
step:1066/1405 train_time:151366ms step_avg:143.34ms
step:1067/1405 train_time:151516ms step_avg:143.35ms
step:1068/1405 train_time:151663ms step_avg:143.35ms
step:1069/1405 train_time:151817ms step_avg:143.36ms
step:1070/1405 train_time:151964ms step_avg:143.36ms
step:1071/1405 train_time:152115ms step_avg:143.37ms
step:1072/1405 train_time:152264ms step_avg:143.37ms
step:1073/1405 train_time:152413ms step_avg:143.38ms
step:1074/1405 train_time:152562ms step_avg:143.39ms
step:1075/1405 train_time:152711ms step_avg:143.39ms
step:1076/1405 train_time:152860ms step_avg:143.40ms
step:1077/1405 train_time:153008ms step_avg:143.40ms
step:1078/1405 train_time:153160ms step_avg:143.41ms
step:1079/1405 train_time:153308ms step_avg:143.41ms
step:1080/1405 train_time:153460ms step_avg:143.42ms
step:1081/1405 train_time:153607ms step_avg:143.42ms
step:1082/1405 train_time:153756ms step_avg:143.43ms
step:1083/1405 train_time:153903ms step_avg:143.43ms
step:1084/1405 train_time:154055ms step_avg:143.44ms
step:1085/1405 train_time:154202ms step_avg:143.44ms
step:1086/1405 train_time:154354ms step_avg:143.45ms
step:1087/1405 train_time:154504ms step_avg:143.46ms
step:1088/1405 train_time:154654ms step_avg:143.46ms
step:1089/1405 train_time:154804ms step_avg:143.47ms
step:1090/1405 train_time:154958ms step_avg:143.48ms
step:1091/1405 train_time:155107ms step_avg:143.48ms
step:1092/1405 train_time:155256ms step_avg:143.49ms
step:1093/1405 train_time:155405ms step_avg:143.49ms
step:1094/1405 train_time:155555ms step_avg:143.50ms
step:1095/1405 train_time:155703ms step_avg:143.51ms
step:1096/1405 train_time:155855ms step_avg:143.51ms
step:1097/1405 train_time:156004ms step_avg:143.52ms
step:1098/1405 train_time:156154ms step_avg:143.52ms
step:1099/1405 train_time:156302ms step_avg:143.53ms
step:1100/1405 train_time:156452ms step_avg:143.53ms
step:1101/1405 train_time:156602ms step_avg:143.54ms
step:1102/1405 train_time:156753ms step_avg:143.55ms
step:1103/1405 train_time:156902ms step_avg:143.55ms
step:1104/1405 train_time:157052ms step_avg:143.56ms
step:1105/1405 train_time:157201ms step_avg:143.56ms
step:1106/1405 train_time:157349ms step_avg:143.57ms
step:1107/1405 train_time:157498ms step_avg:143.57ms
step:1108/1405 train_time:157647ms step_avg:143.58ms
step:1109/1405 train_time:157797ms step_avg:143.58ms
step:1110/1405 train_time:157948ms step_avg:143.59ms
step:1111/1405 train_time:158101ms step_avg:143.60ms
step:1112/1405 train_time:158251ms step_avg:143.60ms
step:1113/1405 train_time:158400ms step_avg:143.61ms
step:1114/1405 train_time:158549ms step_avg:143.61ms
step:1115/1405 train_time:158699ms step_avg:143.62ms
step:1116/1405 train_time:158846ms step_avg:143.62ms
step:1117/1405 train_time:158998ms step_avg:143.63ms
step:1118/1405 train_time:159149ms step_avg:143.64ms
step:1119/1405 train_time:159301ms step_avg:143.64ms
step:1120/1405 train_time:159451ms step_avg:143.65ms
step:1121/1405 train_time:159600ms step_avg:143.65ms
step:1122/1405 train_time:159748ms step_avg:143.66ms
step:1123/1405 train_time:159899ms step_avg:143.66ms
step:1124/1405 train_time:160049ms step_avg:143.67ms
step:1125/1405 train_time:160201ms step_avg:143.68ms
step:1125/1405 val_loss:3.3592 train_time:160274ms step_avg:143.74ms
step:1126/1405 train_time:160350ms step_avg:143.68ms
step:1127/1405 train_time:160501ms step_avg:143.69ms
step:1128/1405 train_time:160651ms step_avg:143.70ms
step:1129/1405 train_time:160800ms step_avg:143.70ms
step:1130/1405 train_time:160950ms step_avg:143.70ms
step:1131/1405 train_time:161098ms step_avg:143.71ms
step:1132/1405 train_time:161247ms step_avg:143.71ms
step:1133/1405 train_time:161397ms step_avg:143.72ms
step:1134/1405 train_time:161547ms step_avg:143.73ms
step:1135/1405 train_time:161696ms step_avg:143.73ms
step:1136/1405 train_time:161846ms step_avg:143.74ms
step:1137/1405 train_time:161994ms step_avg:143.74ms
step:1138/1405 train_time:162143ms step_avg:143.74ms
step:1139/1405 train_time:162292ms step_avg:143.75ms
step:1140/1405 train_time:162442ms step_avg:143.75ms
step:1141/1405 train_time:162634ms step_avg:143.80ms
step:1142/1405 train_time:162785ms step_avg:143.80ms
step:1143/1405 train_time:162934ms step_avg:143.81ms
step:1144/1405 train_time:163083ms step_avg:143.81ms
step:1145/1405 train_time:163231ms step_avg:143.82ms
step:1146/1405 train_time:163381ms step_avg:143.82ms
step:1147/1405 train_time:163532ms step_avg:143.83ms
step:1148/1405 train_time:163685ms step_avg:143.84ms
step:1149/1405 train_time:163836ms step_avg:143.84ms
step:1150/1405 train_time:163986ms step_avg:143.85ms
step:1151/1405 train_time:164136ms step_avg:143.85ms
step:1152/1405 train_time:164289ms step_avg:143.86ms
step:1153/1405 train_time:164439ms step_avg:143.87ms
step:1154/1405 train_time:164589ms step_avg:143.87ms
step:1155/1405 train_time:164742ms step_avg:143.88ms
step:1156/1405 train_time:164894ms step_avg:143.89ms
step:1157/1405 train_time:165049ms step_avg:143.90ms
step:1158/1405 train_time:165198ms step_avg:143.90ms
step:1159/1405 train_time:165348ms step_avg:143.91ms
step:1160/1405 train_time:165497ms step_avg:143.91ms
step:1161/1405 train_time:165650ms step_avg:143.92ms
step:1162/1405 train_time:165800ms step_avg:143.92ms
step:1163/1405 train_time:165952ms step_avg:143.93ms
step:1164/1405 train_time:166103ms step_avg:143.94ms
step:1165/1405 train_time:166254ms step_avg:143.94ms
step:1166/1405 train_time:166405ms step_avg:143.95ms
step:1167/1405 train_time:166554ms step_avg:143.95ms
step:1168/1405 train_time:166704ms step_avg:143.96ms
step:1169/1405 train_time:166856ms step_avg:143.97ms
step:1170/1405 train_time:167007ms step_avg:143.97ms
step:1171/1405 train_time:167158ms step_avg:143.98ms
step:1172/1405 train_time:167309ms step_avg:143.98ms
step:1173/1405 train_time:167459ms step_avg:143.99ms
step:1174/1405 train_time:167614ms step_avg:144.00ms
step:1175/1405 train_time:167768ms step_avg:144.01ms
step:1176/1405 train_time:167920ms step_avg:144.01ms
step:1177/1405 train_time:168074ms step_avg:144.02ms
step:1178/1405 train_time:168226ms step_avg:144.03ms
step:1179/1405 train_time:168375ms step_avg:144.03ms
step:1180/1405 train_time:168532ms step_avg:144.04ms
step:1181/1405 train_time:168683ms step_avg:144.05ms
step:1182/1405 train_time:168833ms step_avg:144.06ms
step:1183/1405 train_time:168982ms step_avg:144.06ms
step:1184/1405 train_time:169134ms step_avg:144.07ms
step:1185/1405 train_time:169288ms step_avg:144.07ms
step:1186/1405 train_time:169438ms step_avg:144.08ms
step:1187/1405 train_time:169591ms step_avg:144.09ms
step:1188/1405 train_time:169740ms step_avg:144.09ms
step:1189/1405 train_time:169895ms step_avg:144.10ms
step:1190/1405 train_time:170048ms step_avg:144.11ms
step:1191/1405 train_time:170198ms step_avg:144.11ms
step:1192/1405 train_time:170349ms step_avg:144.12ms
step:1193/1405 train_time:170497ms step_avg:144.12ms
step:1194/1405 train_time:170650ms step_avg:144.13ms
step:1195/1405 train_time:170799ms step_avg:144.13ms
step:1196/1405 train_time:170951ms step_avg:144.14ms
step:1197/1405 train_time:171103ms step_avg:144.15ms
step:1198/1405 train_time:171256ms step_avg:144.15ms
step:1199/1405 train_time:171407ms step_avg:144.16ms
step:1200/1405 train_time:171556ms step_avg:144.16ms
step:1201/1405 train_time:171707ms step_avg:144.17ms
step:1202/1405 train_time:171865ms step_avg:144.18ms
step:1203/1405 train_time:172018ms step_avg:144.19ms
step:1204/1405 train_time:172169ms step_avg:144.19ms
step:1205/1405 train_time:172318ms step_avg:144.20ms
step:1206/1405 train_time:172468ms step_avg:144.20ms
step:1207/1405 train_time:172618ms step_avg:144.21ms
step:1208/1405 train_time:172769ms step_avg:144.21ms
step:1209/1405 train_time:172920ms step_avg:144.22ms
step:1210/1405 train_time:173075ms step_avg:144.23ms
step:1211/1405 train_time:173226ms step_avg:144.24ms
step:1212/1405 train_time:173376ms step_avg:144.24ms
step:1213/1405 train_time:173528ms step_avg:144.25ms
step:1214/1405 train_time:173678ms step_avg:144.25ms
step:1215/1405 train_time:173829ms step_avg:144.26ms
step:1216/1405 train_time:173978ms step_avg:144.26ms
step:1217/1405 train_time:174129ms step_avg:144.27ms
step:1218/1405 train_time:174279ms step_avg:144.27ms
step:1219/1405 train_time:174429ms step_avg:144.28ms
step:1220/1405 train_time:174577ms step_avg:144.28ms
step:1221/1405 train_time:174728ms step_avg:144.28ms
step:1222/1405 train_time:174877ms step_avg:144.29ms
step:1223/1405 train_time:175030ms step_avg:144.30ms
step:1224/1405 train_time:175182ms step_avg:144.30ms
step:1225/1405 train_time:175333ms step_avg:144.31ms
step:1226/1405 train_time:175484ms step_avg:144.31ms
step:1227/1405 train_time:175635ms step_avg:144.32ms
step:1228/1405 train_time:175786ms step_avg:144.32ms
step:1229/1405 train_time:175936ms step_avg:144.33ms
step:1230/1405 train_time:176090ms step_avg:144.34ms
step:1231/1405 train_time:176240ms step_avg:144.34ms
step:1232/1405 train_time:176392ms step_avg:144.35ms
step:1233/1405 train_time:176543ms step_avg:144.35ms
step:1234/1405 train_time:176694ms step_avg:144.36ms
step:1235/1405 train_time:176845ms step_avg:144.36ms
step:1236/1405 train_time:176995ms step_avg:144.37ms
step:1237/1405 train_time:177146ms step_avg:144.37ms
step:1238/1405 train_time:177300ms step_avg:144.38ms
step:1239/1405 train_time:177452ms step_avg:144.39ms
step:1240/1405 train_time:177605ms step_avg:144.39ms
step:1241/1405 train_time:177758ms step_avg:144.40ms
step:1242/1405 train_time:177909ms step_avg:144.41ms
step:1243/1405 train_time:178059ms step_avg:144.41ms
step:1244/1405 train_time:178210ms step_avg:144.42ms
step:1245/1405 train_time:178362ms step_avg:144.42ms
step:1246/1405 train_time:178513ms step_avg:144.43ms
step:1247/1405 train_time:178662ms step_avg:144.43ms
step:1248/1405 train_time:178814ms step_avg:144.44ms
step:1249/1405 train_time:178964ms step_avg:144.44ms
step:1250/1405 train_time:179114ms step_avg:144.45ms
step:1250/1405 val_loss:3.3118 train_time:179190ms step_avg:144.51ms
step:1251/1405 train_time:179269ms step_avg:144.46ms
step:1252/1405 train_time:179419ms step_avg:144.46ms
step:1253/1405 train_time:179571ms step_avg:144.47ms
step:1254/1405 train_time:179718ms step_avg:144.47ms
step:1255/1405 train_time:179874ms step_avg:144.48ms
step:1256/1405 train_time:180025ms step_avg:144.48ms
step:1257/1405 train_time:180175ms step_avg:144.49ms
step:1258/1405 train_time:180329ms step_avg:144.49ms
step:1259/1405 train_time:180482ms step_avg:144.50ms
step:1260/1405 train_time:180633ms step_avg:144.51ms
step:1261/1405 train_time:180786ms step_avg:144.51ms
step:1262/1405 train_time:180938ms step_avg:144.52ms
step:1263/1405 train_time:181089ms step_avg:144.52ms
step:1264/1405 train_time:181238ms step_avg:144.53ms
step:1265/1405 train_time:181388ms step_avg:144.53ms
step:1266/1405 train_time:181541ms step_avg:144.54ms
step:1267/1405 train_time:181693ms step_avg:144.54ms
step:1268/1405 train_time:181846ms step_avg:144.55ms
step:1269/1405 train_time:181999ms step_avg:144.56ms
step:1270/1405 train_time:182150ms step_avg:144.56ms
step:1271/1405 train_time:182301ms step_avg:144.57ms
step:1272/1405 train_time:182451ms step_avg:144.57ms
step:1273/1405 train_time:182601ms step_avg:144.58ms
step:1274/1405 train_time:182753ms step_avg:144.58ms
step:1275/1405 train_time:182904ms step_avg:144.59ms
step:1276/1405 train_time:183053ms step_avg:144.59ms
step:1277/1405 train_time:183205ms step_avg:144.60ms
step:1278/1405 train_time:183353ms step_avg:144.60ms
step:1279/1405 train_time:183506ms step_avg:144.61ms
step:1280/1405 train_time:183661ms step_avg:144.61ms
step:1281/1405 train_time:183812ms step_avg:144.62ms
step:1282/1405 train_time:183962ms step_avg:144.62ms
step:1283/1405 train_time:184113ms step_avg:144.63ms
step:1284/1405 train_time:184266ms step_avg:144.64ms
step:1285/1405 train_time:184416ms step_avg:144.64ms
step:1286/1405 train_time:184569ms step_avg:144.65ms
step:1287/1405 train_time:184718ms step_avg:144.65ms
step:1288/1405 train_time:184871ms step_avg:144.66ms
step:1289/1405 train_time:185024ms step_avg:144.66ms
step:1290/1405 train_time:185176ms step_avg:144.67ms
step:1291/1405 train_time:185329ms step_avg:144.68ms
step:1292/1405 train_time:185479ms step_avg:144.68ms
step:1293/1405 train_time:185630ms step_avg:144.68ms
step:1294/1405 train_time:185783ms step_avg:144.69ms
step:1295/1405 train_time:185934ms step_avg:144.70ms
step:1296/1405 train_time:186087ms step_avg:144.70ms
step:1297/1405 train_time:186240ms step_avg:144.71ms
step:1298/1405 train_time:186392ms step_avg:144.71ms
step:1299/1405 train_time:186542ms step_avg:144.72ms
step:1300/1405 train_time:186692ms step_avg:144.72ms
step:1301/1405 train_time:186844ms step_avg:144.73ms
step:1302/1405 train_time:186993ms step_avg:144.73ms
step:1303/1405 train_time:187147ms step_avg:144.74ms
step:1304/1405 train_time:187299ms step_avg:144.74ms
step:1305/1405 train_time:187450ms step_avg:144.75ms
step:1306/1405 train_time:187602ms step_avg:144.75ms
step:1307/1405 train_time:187752ms step_avg:144.76ms
step:1308/1405 train_time:187903ms step_avg:144.76ms
step:1309/1405 train_time:188053ms step_avg:144.77ms
step:1310/1405 train_time:188206ms step_avg:144.77ms
step:1311/1405 train_time:188356ms step_avg:144.78ms
step:1312/1405 train_time:188505ms step_avg:144.78ms
step:1313/1405 train_time:188654ms step_avg:144.78ms
step:1314/1405 train_time:188807ms step_avg:144.79ms
step:1315/1405 train_time:188957ms step_avg:144.79ms
step:1316/1405 train_time:189108ms step_avg:144.80ms
step:1317/1405 train_time:189256ms step_avg:144.80ms
step:1318/1405 train_time:189411ms step_avg:144.81ms
step:1319/1405 train_time:189561ms step_avg:144.81ms
step:1320/1405 train_time:189712ms step_avg:144.82ms
step:1321/1405 train_time:189865ms step_avg:144.82ms
step:1322/1405 train_time:190020ms step_avg:144.83ms
step:1323/1405 train_time:190172ms step_avg:144.84ms
step:1324/1405 train_time:190322ms step_avg:144.84ms
step:1325/1405 train_time:190474ms step_avg:144.85ms
step:1326/1405 train_time:190626ms step_avg:144.85ms
step:1327/1405 train_time:190776ms step_avg:144.86ms
step:1328/1405 train_time:190926ms step_avg:144.86ms
step:1329/1405 train_time:191088ms step_avg:144.87ms
step:1330/1405 train_time:191238ms step_avg:144.88ms
step:1331/1405 train_time:191432ms step_avg:144.91ms
step:1332/1405 train_time:191584ms step_avg:144.92ms
step:1333/1405 train_time:191734ms step_avg:144.92ms
step:1334/1405 train_time:191885ms step_avg:144.93ms
step:1335/1405 train_time:192033ms step_avg:144.93ms
step:1336/1405 train_time:192188ms step_avg:144.94ms
step:1337/1405 train_time:192341ms step_avg:144.94ms
step:1338/1405 train_time:192493ms step_avg:144.95ms
step:1339/1405 train_time:192646ms step_avg:144.96ms
step:1340/1405 train_time:192795ms step_avg:144.96ms
step:1341/1405 train_time:192945ms step_avg:144.96ms
step:1342/1405 train_time:193095ms step_avg:144.97ms
step:1343/1405 train_time:193246ms step_avg:144.97ms
step:1344/1405 train_time:193396ms step_avg:144.97ms
step:1345/1405 train_time:193548ms step_avg:144.98ms
step:1346/1405 train_time:193699ms step_avg:144.98ms
step:1347/1405 train_time:193851ms step_avg:144.99ms
step:1348/1405 train_time:194002ms step_avg:144.99ms
step:1349/1405 train_time:194153ms step_avg:145.00ms
step:1350/1405 train_time:194304ms step_avg:145.00ms
step:1351/1405 train_time:194454ms step_avg:145.01ms
step:1352/1405 train_time:194608ms step_avg:145.01ms
step:1353/1405 train_time:194759ms step_avg:145.02ms
step:1354/1405 train_time:194913ms step_avg:145.02ms
step:1355/1405 train_time:195065ms step_avg:145.03ms
step:1356/1405 train_time:195216ms step_avg:145.03ms
step:1357/1405 train_time:195369ms step_avg:145.04ms
step:1358/1405 train_time:195524ms step_avg:145.05ms
step:1359/1405 train_time:195675ms step_avg:145.05ms
step:1360/1405 train_time:195830ms step_avg:145.06ms
step:1361/1405 train_time:195984ms step_avg:145.07ms
step:1362/1405 train_time:196136ms step_avg:145.07ms
step:1363/1405 train_time:196292ms step_avg:145.08ms
step:1364/1405 train_time:196444ms step_avg:145.08ms
step:1365/1405 train_time:196595ms step_avg:145.09ms
step:1366/1405 train_time:196747ms step_avg:145.09ms
step:1367/1405 train_time:196901ms step_avg:145.10ms
step:1368/1405 train_time:197053ms step_avg:145.11ms
step:1369/1405 train_time:197210ms step_avg:145.11ms
step:1370/1405 train_time:197361ms step_avg:145.12ms
step:1371/1405 train_time:197513ms step_avg:145.12ms
step:1372/1405 train_time:197671ms step_avg:145.13ms
step:1373/1405 train_time:197821ms step_avg:145.14ms
step:1374/1405 train_time:197974ms step_avg:145.14ms
step:1375/1405 train_time:198126ms step_avg:145.15ms
step:1375/1405 val_loss:3.2808 train_time:198199ms step_avg:145.20ms
step:1376/1405 train_time:198276ms step_avg:145.15ms
step:1377/1405 train_time:198429ms step_avg:145.16ms
step:1378/1405 train_time:198580ms step_avg:145.16ms
step:1379/1405 train_time:198734ms step_avg:145.17ms
step:1380/1405 train_time:198884ms step_avg:145.17ms
step:1381/1405 train_time:199038ms step_avg:145.18ms
step:1382/1405 train_time:199192ms step_avg:145.18ms
step:1383/1405 train_time:199344ms step_avg:145.19ms
step:1384/1405 train_time:199500ms step_avg:145.20ms
step:1385/1405 train_time:199651ms step_avg:145.20ms
step:1386/1405 train_time:199803ms step_avg:145.21ms
step:1387/1405 train_time:199958ms step_avg:145.21ms
step:1388/1405 train_time:200110ms step_avg:145.22ms
step:1389/1405 train_time:200266ms step_avg:145.23ms
step:1390/1405 train_time:200420ms step_avg:145.23ms
step:1391/1405 train_time:200573ms step_avg:145.24ms
step:1392/1405 train_time:200726ms step_avg:145.24ms
step:1393/1405 train_time:200877ms step_avg:145.25ms
step:1394/1405 train_time:201029ms step_avg:145.25ms
step:1395/1405 train_time:201179ms step_avg:145.26ms
step:1396/1405 train_time:201331ms step_avg:145.26ms
step:1397/1405 train_time:201482ms step_avg:145.26ms
step:1398/1405 train_time:201634ms step_avg:145.27ms
step:1399/1405 train_time:201786ms step_avg:145.27ms
step:1400/1405 train_time:201942ms step_avg:145.28ms
step:1401/1405 train_time:202093ms step_avg:145.29ms
step:1402/1405 train_time:202244ms step_avg:145.29ms
step:1403/1405 train_time:202400ms step_avg:145.30ms
step:1404/1405 train_time:202551ms step_avg:145.30ms
step:1405/1405 train_time:202701ms step_avg:145.31ms
step:1405/1405 val_loss:3.2782 train_time:202778ms step_avg:145.36ms
peak memory consumption: 31565 MiB

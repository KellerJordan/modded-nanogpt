import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: torch.Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_masks(sliding_window_num_blocks: int):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            global_block_mask = BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
            local_window_bm = q_idx - kv_idx < max(1, sliding_window_num_blocks // 2)
            local_window_full_bm = local_window_bm
            local_nonzero_bm = causal_bm & local_window_bm & document_bm
            local_full_bm = causal_full_bm & local_window_full_bm & document_full_bm
            local_kv_num_blocks, local_kv_indices = dense_to_ordered(local_nonzero_bm & ~local_full_bm)
            local_full_kv_num_blocks, local_full_kv_indices = dense_to_ordered(local_full_bm)
            local_block_mask = BlockMask.from_kv_blocks(
                local_kv_num_blocks,
                local_kv_indices,
                local_full_kv_num_blocks,
                local_full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
            return global_block_mask, local_block_mask

        global_block_mask, local_block_mask = create_doc_swc_block_masks(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            block_mask = global_block_mask if i % 2 == 0 else local_block_mask
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            block_mask = local_block_mask if i % 2 == 0 else global_block_mask
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1405 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Wed Jan 15 09:21:49 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             127W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             129W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             123W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             117W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1405 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1405 train_time:28133ms step_avg:nanms
step:2/1405 train_time:28201ms step_avg:nanms
step:3/1405 train_time:28392ms step_avg:nanms
step:4/1405 train_time:28524ms step_avg:nanms
step:5/1405 train_time:28658ms step_avg:nanms
step:6/1405 train_time:28792ms step_avg:nanms
step:7/1405 train_time:28924ms step_avg:nanms
step:8/1405 train_time:29058ms step_avg:nanms
step:9/1405 train_time:29192ms step_avg:nanms
step:10/1405 train_time:29331ms step_avg:nanms
step:11/1405 train_time:136ms step_avg:nanms
step:12/1405 train_time:273ms step_avg:nanms
step:13/1405 train_time:405ms step_avg:135.10ms
step:14/1405 train_time:540ms step_avg:134.90ms
step:15/1405 train_time:673ms step_avg:134.66ms
step:16/1405 train_time:809ms step_avg:134.86ms
step:17/1405 train_time:944ms step_avg:134.85ms
step:18/1405 train_time:1082ms step_avg:135.30ms
step:19/1405 train_time:1218ms step_avg:135.38ms
step:20/1405 train_time:1355ms step_avg:135.46ms
step:21/1405 train_time:1489ms step_avg:135.40ms
step:22/1405 train_time:1625ms step_avg:135.38ms
step:23/1405 train_time:1758ms step_avg:135.25ms
step:24/1405 train_time:1893ms step_avg:135.20ms
step:25/1405 train_time:2029ms step_avg:135.28ms
step:26/1405 train_time:2167ms step_avg:135.42ms
step:27/1405 train_time:2301ms step_avg:135.36ms
step:28/1405 train_time:2437ms step_avg:135.40ms
step:29/1405 train_time:2573ms step_avg:135.41ms
step:30/1405 train_time:2709ms step_avg:135.44ms
step:31/1405 train_time:2843ms step_avg:135.38ms
step:32/1405 train_time:2979ms step_avg:135.42ms
step:33/1405 train_time:3116ms step_avg:135.47ms
step:34/1405 train_time:3254ms step_avg:135.57ms
step:35/1405 train_time:3389ms step_avg:135.55ms
step:36/1405 train_time:3525ms step_avg:135.56ms
step:37/1405 train_time:3661ms step_avg:135.58ms
step:38/1405 train_time:3795ms step_avg:135.54ms
step:39/1405 train_time:3932ms step_avg:135.60ms
step:40/1405 train_time:4070ms step_avg:135.66ms
step:41/1405 train_time:4203ms step_avg:135.59ms
step:42/1405 train_time:4340ms step_avg:135.63ms
step:43/1405 train_time:4475ms step_avg:135.61ms
step:44/1405 train_time:4610ms step_avg:135.59ms
step:45/1405 train_time:4745ms step_avg:135.57ms
step:46/1405 train_time:4881ms step_avg:135.58ms
step:47/1405 train_time:5018ms step_avg:135.61ms
step:48/1405 train_time:5153ms step_avg:135.60ms
step:49/1405 train_time:5287ms step_avg:135.56ms
step:50/1405 train_time:5424ms step_avg:135.60ms
step:51/1405 train_time:5562ms step_avg:135.65ms
step:52/1405 train_time:5697ms step_avg:135.65ms
step:53/1405 train_time:5833ms step_avg:135.64ms
step:54/1405 train_time:5970ms step_avg:135.67ms
step:55/1405 train_time:6106ms step_avg:135.68ms
step:56/1405 train_time:6242ms step_avg:135.68ms
step:57/1405 train_time:6378ms step_avg:135.70ms
step:58/1405 train_time:6513ms step_avg:135.69ms
step:59/1405 train_time:6648ms step_avg:135.68ms
step:60/1405 train_time:6784ms step_avg:135.67ms
step:61/1405 train_time:6920ms step_avg:135.68ms
step:62/1405 train_time:7055ms step_avg:135.68ms
step:63/1405 train_time:7190ms step_avg:135.66ms
step:64/1405 train_time:7325ms step_avg:135.65ms
step:65/1405 train_time:7462ms step_avg:135.67ms
step:66/1405 train_time:7597ms step_avg:135.66ms
step:67/1405 train_time:7734ms step_avg:135.68ms
step:68/1405 train_time:7869ms step_avg:135.67ms
step:69/1405 train_time:8005ms step_avg:135.67ms
step:70/1405 train_time:8140ms step_avg:135.67ms
step:71/1405 train_time:8276ms step_avg:135.67ms
step:72/1405 train_time:8411ms step_avg:135.67ms
step:73/1405 train_time:8548ms step_avg:135.68ms
step:74/1405 train_time:8684ms step_avg:135.69ms
step:75/1405 train_time:8819ms step_avg:135.68ms
step:76/1405 train_time:8955ms step_avg:135.68ms
step:77/1405 train_time:9090ms step_avg:135.67ms
step:78/1405 train_time:9227ms step_avg:135.69ms
step:79/1405 train_time:9363ms step_avg:135.70ms
step:80/1405 train_time:9499ms step_avg:135.70ms
step:81/1405 train_time:9635ms step_avg:135.70ms
step:82/1405 train_time:9770ms step_avg:135.70ms
step:83/1405 train_time:9906ms step_avg:135.69ms
step:84/1405 train_time:10041ms step_avg:135.69ms
step:85/1405 train_time:10177ms step_avg:135.70ms
step:86/1405 train_time:10313ms step_avg:135.70ms
step:87/1405 train_time:10448ms step_avg:135.69ms
step:88/1405 train_time:10584ms step_avg:135.69ms
step:89/1405 train_time:10718ms step_avg:135.67ms
step:90/1405 train_time:10855ms step_avg:135.68ms
step:91/1405 train_time:10990ms step_avg:135.68ms
step:92/1405 train_time:11125ms step_avg:135.67ms
step:93/1405 train_time:11261ms step_avg:135.67ms
step:94/1405 train_time:11396ms step_avg:135.67ms
step:95/1405 train_time:11534ms step_avg:135.69ms
step:96/1405 train_time:11672ms step_avg:135.72ms
step:97/1405 train_time:11806ms step_avg:135.70ms
step:98/1405 train_time:11943ms step_avg:135.71ms
step:99/1405 train_time:12079ms step_avg:135.72ms
step:100/1405 train_time:12215ms step_avg:135.72ms
step:101/1405 train_time:12353ms step_avg:135.75ms
step:102/1405 train_time:12489ms step_avg:135.75ms
step:103/1405 train_time:12625ms step_avg:135.75ms
step:104/1405 train_time:12761ms step_avg:135.75ms
step:105/1405 train_time:12896ms step_avg:135.74ms
step:106/1405 train_time:13035ms step_avg:135.78ms
step:107/1405 train_time:13171ms step_avg:135.79ms
step:108/1405 train_time:13308ms step_avg:135.79ms
step:109/1405 train_time:13446ms step_avg:135.81ms
step:110/1405 train_time:13584ms step_avg:135.84ms
step:111/1405 train_time:13722ms step_avg:135.86ms
step:112/1405 train_time:13859ms step_avg:135.88ms
step:113/1405 train_time:13996ms step_avg:135.88ms
step:114/1405 train_time:14135ms step_avg:135.91ms
step:115/1405 train_time:14273ms step_avg:135.93ms
step:116/1405 train_time:14412ms step_avg:135.97ms
step:117/1405 train_time:14551ms step_avg:135.99ms
step:118/1405 train_time:14689ms step_avg:136.01ms
step:119/1405 train_time:14827ms step_avg:136.03ms
step:120/1405 train_time:14963ms step_avg:136.03ms
step:121/1405 train_time:15099ms step_avg:136.03ms
step:122/1405 train_time:15236ms step_avg:136.04ms
step:123/1405 train_time:15376ms step_avg:136.07ms
step:124/1405 train_time:15514ms step_avg:136.09ms
step:125/1405 train_time:15653ms step_avg:136.11ms
step:125/1405 val_loss:4.3879 train_time:15719ms step_avg:136.69ms
step:126/1405 train_time:15791ms step_avg:136.13ms
step:127/1405 train_time:15937ms step_avg:136.21ms
step:128/1405 train_time:16076ms step_avg:136.24ms
step:129/1405 train_time:16213ms step_avg:136.25ms
step:130/1405 train_time:16348ms step_avg:136.23ms
step:131/1405 train_time:16485ms step_avg:136.24ms
step:132/1405 train_time:16622ms step_avg:136.25ms
step:133/1405 train_time:16761ms step_avg:136.27ms
step:134/1405 train_time:16902ms step_avg:136.31ms
step:135/1405 train_time:17040ms step_avg:136.32ms
step:136/1405 train_time:17178ms step_avg:136.33ms
step:137/1405 train_time:17316ms step_avg:136.34ms
step:138/1405 train_time:17452ms step_avg:136.35ms
step:139/1405 train_time:17589ms step_avg:136.35ms
step:140/1405 train_time:17726ms step_avg:136.36ms
step:141/1405 train_time:17865ms step_avg:136.37ms
step:142/1405 train_time:18003ms step_avg:136.39ms
step:143/1405 train_time:18141ms step_avg:136.40ms
step:144/1405 train_time:18279ms step_avg:136.41ms
step:145/1405 train_time:18418ms step_avg:136.43ms
step:146/1405 train_time:18556ms step_avg:136.44ms
step:147/1405 train_time:18695ms step_avg:136.46ms
step:148/1405 train_time:18833ms step_avg:136.47ms
step:149/1405 train_time:18973ms step_avg:136.49ms
step:150/1405 train_time:19110ms step_avg:136.50ms
step:151/1405 train_time:19247ms step_avg:136.50ms
step:152/1405 train_time:19385ms step_avg:136.51ms
step:153/1405 train_time:19523ms step_avg:136.53ms
step:154/1405 train_time:19661ms step_avg:136.53ms
step:155/1405 train_time:19800ms step_avg:136.55ms
step:156/1405 train_time:19936ms step_avg:136.55ms
step:157/1405 train_time:20075ms step_avg:136.56ms
step:158/1405 train_time:20213ms step_avg:136.58ms
step:159/1405 train_time:20350ms step_avg:136.58ms
step:160/1405 train_time:20489ms step_avg:136.60ms
step:161/1405 train_time:20627ms step_avg:136.60ms
step:162/1405 train_time:20765ms step_avg:136.61ms
step:163/1405 train_time:20903ms step_avg:136.62ms
step:164/1405 train_time:21040ms step_avg:136.63ms
step:165/1405 train_time:21181ms step_avg:136.65ms
step:166/1405 train_time:21320ms step_avg:136.66ms
step:167/1405 train_time:21456ms step_avg:136.66ms
step:168/1405 train_time:21595ms step_avg:136.68ms
step:169/1405 train_time:21733ms step_avg:136.68ms
step:170/1405 train_time:21869ms step_avg:136.68ms
step:171/1405 train_time:22005ms step_avg:136.68ms
step:172/1405 train_time:22143ms step_avg:136.69ms
step:173/1405 train_time:22283ms step_avg:136.71ms
step:174/1405 train_time:22422ms step_avg:136.72ms
step:175/1405 train_time:22561ms step_avg:136.73ms
step:176/1405 train_time:22699ms step_avg:136.74ms
step:177/1405 train_time:22836ms step_avg:136.74ms
step:178/1405 train_time:22975ms step_avg:136.75ms
step:179/1405 train_time:23112ms step_avg:136.76ms
step:180/1405 train_time:23250ms step_avg:136.77ms
step:181/1405 train_time:23390ms step_avg:136.78ms
step:182/1405 train_time:23528ms step_avg:136.79ms
step:183/1405 train_time:23667ms step_avg:136.80ms
step:184/1405 train_time:23806ms step_avg:136.82ms
step:185/1405 train_time:23944ms step_avg:136.82ms
step:186/1405 train_time:24081ms step_avg:136.83ms
step:187/1405 train_time:24219ms step_avg:136.83ms
step:188/1405 train_time:24356ms step_avg:136.83ms
step:189/1405 train_time:24495ms step_avg:136.84ms
step:190/1405 train_time:24633ms step_avg:136.85ms
step:191/1405 train_time:24814ms step_avg:137.10ms
step:192/1405 train_time:24950ms step_avg:137.09ms
step:193/1405 train_time:25087ms step_avg:137.09ms
step:194/1405 train_time:25224ms step_avg:137.09ms
step:195/1405 train_time:25360ms step_avg:137.08ms
step:196/1405 train_time:25497ms step_avg:137.08ms
step:197/1405 train_time:25633ms step_avg:137.07ms
step:198/1405 train_time:25776ms step_avg:137.11ms
step:199/1405 train_time:25914ms step_avg:137.11ms
step:200/1405 train_time:26052ms step_avg:137.11ms
step:201/1405 train_time:26189ms step_avg:137.12ms
step:202/1405 train_time:26327ms step_avg:137.12ms
step:203/1405 train_time:26463ms step_avg:137.11ms
step:204/1405 train_time:26600ms step_avg:137.11ms
step:205/1405 train_time:26739ms step_avg:137.12ms
step:206/1405 train_time:26882ms step_avg:137.15ms
step:207/1405 train_time:27020ms step_avg:137.16ms
step:208/1405 train_time:27157ms step_avg:137.16ms
step:209/1405 train_time:27295ms step_avg:137.16ms
step:210/1405 train_time:27432ms step_avg:137.16ms
step:211/1405 train_time:27570ms step_avg:137.16ms
step:212/1405 train_time:27707ms step_avg:137.17ms
step:213/1405 train_time:27846ms step_avg:137.17ms
step:214/1405 train_time:27986ms step_avg:137.19ms
step:215/1405 train_time:28126ms step_avg:137.20ms
step:216/1405 train_time:28265ms step_avg:137.21ms
step:217/1405 train_time:28403ms step_avg:137.21ms
step:218/1405 train_time:28542ms step_avg:137.22ms
step:219/1405 train_time:28680ms step_avg:137.23ms
step:220/1405 train_time:28820ms step_avg:137.24ms
step:221/1405 train_time:28959ms step_avg:137.25ms
step:222/1405 train_time:29100ms step_avg:137.26ms
step:223/1405 train_time:29238ms step_avg:137.27ms
step:224/1405 train_time:29378ms step_avg:137.28ms
step:225/1405 train_time:29516ms step_avg:137.28ms
step:226/1405 train_time:29655ms step_avg:137.29ms
step:227/1405 train_time:29793ms step_avg:137.30ms
step:228/1405 train_time:29933ms step_avg:137.31ms
step:229/1405 train_time:30072ms step_avg:137.32ms
step:230/1405 train_time:30211ms step_avg:137.32ms
step:231/1405 train_time:30351ms step_avg:137.33ms
step:232/1405 train_time:30489ms step_avg:137.34ms
step:233/1405 train_time:30627ms step_avg:137.34ms
step:234/1405 train_time:30767ms step_avg:137.35ms
step:235/1405 train_time:30907ms step_avg:137.36ms
step:236/1405 train_time:31045ms step_avg:137.37ms
step:237/1405 train_time:31184ms step_avg:137.38ms
step:238/1405 train_time:31323ms step_avg:137.38ms
step:239/1405 train_time:31461ms step_avg:137.39ms
step:240/1405 train_time:31600ms step_avg:137.39ms
step:241/1405 train_time:31739ms step_avg:137.40ms
step:242/1405 train_time:31880ms step_avg:137.41ms
step:243/1405 train_time:32019ms step_avg:137.42ms
step:244/1405 train_time:32159ms step_avg:137.43ms
step:245/1405 train_time:32298ms step_avg:137.44ms
step:246/1405 train_time:32435ms step_avg:137.44ms
step:247/1405 train_time:32575ms step_avg:137.45ms
step:248/1405 train_time:32714ms step_avg:137.45ms
step:249/1405 train_time:32852ms step_avg:137.46ms
step:250/1405 train_time:32991ms step_avg:137.46ms
step:250/1405 val_loss:3.9656 train_time:33058ms step_avg:137.74ms
step:251/1405 train_time:33131ms step_avg:137.47ms
step:252/1405 train_time:33273ms step_avg:137.49ms
step:253/1405 train_time:33413ms step_avg:137.50ms
step:254/1405 train_time:33550ms step_avg:137.50ms
step:255/1405 train_time:33689ms step_avg:137.50ms
step:256/1405 train_time:33827ms step_avg:137.51ms
step:257/1405 train_time:33965ms step_avg:137.51ms
step:258/1405 train_time:34106ms step_avg:137.52ms
step:259/1405 train_time:34247ms step_avg:137.54ms
step:260/1405 train_time:34389ms step_avg:137.55ms
step:261/1405 train_time:34528ms step_avg:137.56ms
step:262/1405 train_time:34665ms step_avg:137.56ms
step:263/1405 train_time:34805ms step_avg:137.57ms
step:264/1405 train_time:34943ms step_avg:137.57ms
step:265/1405 train_time:35082ms step_avg:137.58ms
step:266/1405 train_time:35222ms step_avg:137.59ms
step:267/1405 train_time:35362ms step_avg:137.60ms
step:268/1405 train_time:35504ms step_avg:137.61ms
step:269/1405 train_time:35641ms step_avg:137.61ms
step:270/1405 train_time:35779ms step_avg:137.61ms
step:271/1405 train_time:35918ms step_avg:137.62ms
step:272/1405 train_time:36057ms step_avg:137.62ms
step:273/1405 train_time:36198ms step_avg:137.63ms
step:274/1405 train_time:36337ms step_avg:137.64ms
step:275/1405 train_time:36475ms step_avg:137.64ms
step:276/1405 train_time:36616ms step_avg:137.66ms
step:277/1405 train_time:36755ms step_avg:137.66ms
step:278/1405 train_time:36894ms step_avg:137.67ms
step:279/1405 train_time:37034ms step_avg:137.67ms
step:280/1405 train_time:37172ms step_avg:137.68ms
step:281/1405 train_time:37313ms step_avg:137.69ms
step:282/1405 train_time:37452ms step_avg:137.69ms
step:283/1405 train_time:37592ms step_avg:137.70ms
step:284/1405 train_time:37732ms step_avg:137.71ms
step:285/1405 train_time:37869ms step_avg:137.71ms
step:286/1405 train_time:38011ms step_avg:137.72ms
step:287/1405 train_time:38151ms step_avg:137.73ms
step:288/1405 train_time:38292ms step_avg:137.74ms
step:289/1405 train_time:38431ms step_avg:137.75ms
step:290/1405 train_time:38570ms step_avg:137.75ms
step:291/1405 train_time:38709ms step_avg:137.75ms
step:292/1405 train_time:38849ms step_avg:137.76ms
step:293/1405 train_time:38987ms step_avg:137.76ms
step:294/1405 train_time:39126ms step_avg:137.77ms
step:295/1405 train_time:39266ms step_avg:137.78ms
step:296/1405 train_time:39406ms step_avg:137.78ms
step:297/1405 train_time:39546ms step_avg:137.79ms
step:298/1405 train_time:39685ms step_avg:137.79ms
step:299/1405 train_time:39823ms step_avg:137.80ms
step:300/1405 train_time:39962ms step_avg:137.80ms
step:301/1405 train_time:40101ms step_avg:137.80ms
step:302/1405 train_time:40240ms step_avg:137.81ms
step:303/1405 train_time:40378ms step_avg:137.81ms
step:304/1405 train_time:40517ms step_avg:137.81ms
step:305/1405 train_time:40656ms step_avg:137.82ms
step:306/1405 train_time:40796ms step_avg:137.82ms
step:307/1405 train_time:40935ms step_avg:137.83ms
step:308/1405 train_time:41075ms step_avg:137.84ms
step:309/1405 train_time:41214ms step_avg:137.84ms
step:310/1405 train_time:41354ms step_avg:137.85ms
step:311/1405 train_time:41493ms step_avg:137.85ms
step:312/1405 train_time:41633ms step_avg:137.86ms
step:313/1405 train_time:41772ms step_avg:137.86ms
step:314/1405 train_time:41914ms step_avg:137.87ms
step:315/1405 train_time:42056ms step_avg:137.89ms
step:316/1405 train_time:42199ms step_avg:137.90ms
step:317/1405 train_time:42340ms step_avg:137.92ms
step:318/1405 train_time:42481ms step_avg:137.93ms
step:319/1405 train_time:42623ms step_avg:137.94ms
step:320/1405 train_time:42764ms step_avg:137.95ms
step:321/1405 train_time:42907ms step_avg:137.96ms
step:322/1405 train_time:43048ms step_avg:137.97ms
step:323/1405 train_time:43190ms step_avg:137.99ms
step:324/1405 train_time:43331ms step_avg:138.00ms
step:325/1405 train_time:43472ms step_avg:138.01ms
step:326/1405 train_time:43614ms step_avg:138.02ms
step:327/1405 train_time:43755ms step_avg:138.03ms
step:328/1405 train_time:43896ms step_avg:138.04ms
step:329/1405 train_time:44038ms step_avg:138.05ms
step:330/1405 train_time:44179ms step_avg:138.06ms
step:331/1405 train_time:44321ms step_avg:138.07ms
step:332/1405 train_time:44462ms step_avg:138.08ms
step:333/1405 train_time:44605ms step_avg:138.10ms
step:334/1405 train_time:44747ms step_avg:138.11ms
step:335/1405 train_time:44888ms step_avg:138.12ms
step:336/1405 train_time:45029ms step_avg:138.13ms
step:337/1405 train_time:45170ms step_avg:138.13ms
step:338/1405 train_time:45312ms step_avg:138.15ms
step:339/1405 train_time:45454ms step_avg:138.16ms
step:340/1405 train_time:45597ms step_avg:138.17ms
step:341/1405 train_time:45739ms step_avg:138.18ms
step:342/1405 train_time:45882ms step_avg:138.20ms
step:343/1405 train_time:46022ms step_avg:138.21ms
step:344/1405 train_time:46163ms step_avg:138.21ms
step:345/1405 train_time:46307ms step_avg:138.23ms
step:346/1405 train_time:46448ms step_avg:138.24ms
step:347/1405 train_time:46589ms step_avg:138.25ms
step:348/1405 train_time:46730ms step_avg:138.25ms
step:349/1405 train_time:46872ms step_avg:138.27ms
step:350/1405 train_time:47013ms step_avg:138.27ms
step:351/1405 train_time:47154ms step_avg:138.28ms
step:352/1405 train_time:47296ms step_avg:138.29ms
step:353/1405 train_time:47438ms step_avg:138.30ms
step:354/1405 train_time:47579ms step_avg:138.31ms
step:355/1405 train_time:47722ms step_avg:138.32ms
step:356/1405 train_time:47863ms step_avg:138.33ms
step:357/1405 train_time:48007ms step_avg:138.35ms
step:358/1405 train_time:48148ms step_avg:138.36ms
step:359/1405 train_time:48290ms step_avg:138.37ms
step:360/1405 train_time:48432ms step_avg:138.38ms
step:361/1405 train_time:48573ms step_avg:138.39ms
step:362/1405 train_time:48715ms step_avg:138.39ms
step:363/1405 train_time:48859ms step_avg:138.41ms
step:364/1405 train_time:49000ms step_avg:138.42ms
step:365/1405 train_time:49140ms step_avg:138.42ms
step:366/1405 train_time:49284ms step_avg:138.44ms
step:367/1405 train_time:49427ms step_avg:138.45ms
step:368/1405 train_time:49568ms step_avg:138.46ms
step:369/1405 train_time:49710ms step_avg:138.47ms
step:370/1405 train_time:49852ms step_avg:138.48ms
step:371/1405 train_time:49994ms step_avg:138.49ms
step:372/1405 train_time:50136ms step_avg:138.50ms
step:373/1405 train_time:50276ms step_avg:138.50ms
step:374/1405 train_time:50420ms step_avg:138.52ms
step:375/1405 train_time:50562ms step_avg:138.53ms
step:375/1405 val_loss:3.7783 train_time:50631ms step_avg:138.71ms
step:376/1405 train_time:50708ms step_avg:138.55ms
step:377/1405 train_time:50850ms step_avg:138.55ms
step:378/1405 train_time:50991ms step_avg:138.56ms
step:379/1405 train_time:51131ms step_avg:138.57ms
step:380/1405 train_time:51273ms step_avg:138.57ms
step:381/1405 train_time:51461ms step_avg:138.71ms
step:382/1405 train_time:51602ms step_avg:138.71ms
step:383/1405 train_time:51743ms step_avg:138.72ms
step:384/1405 train_time:51882ms step_avg:138.72ms
step:385/1405 train_time:52023ms step_avg:138.73ms
step:386/1405 train_time:52163ms step_avg:138.73ms
step:387/1405 train_time:52306ms step_avg:138.74ms
step:388/1405 train_time:52451ms step_avg:138.76ms
step:389/1405 train_time:52592ms step_avg:138.76ms
step:390/1405 train_time:52733ms step_avg:138.77ms
step:391/1405 train_time:52875ms step_avg:138.78ms
step:392/1405 train_time:53016ms step_avg:138.79ms
step:393/1405 train_time:53158ms step_avg:138.79ms
step:394/1405 train_time:53297ms step_avg:138.80ms
step:395/1405 train_time:53441ms step_avg:138.81ms
step:396/1405 train_time:53582ms step_avg:138.81ms
step:397/1405 train_time:53725ms step_avg:138.83ms
step:398/1405 train_time:53869ms step_avg:138.84ms
step:399/1405 train_time:54010ms step_avg:138.84ms
step:400/1405 train_time:54150ms step_avg:138.85ms
step:401/1405 train_time:54291ms step_avg:138.85ms
step:402/1405 train_time:54433ms step_avg:138.86ms
step:403/1405 train_time:54577ms step_avg:138.87ms
step:404/1405 train_time:54718ms step_avg:138.88ms
step:405/1405 train_time:54861ms step_avg:138.89ms
step:406/1405 train_time:55001ms step_avg:138.89ms
step:407/1405 train_time:55142ms step_avg:138.90ms
step:408/1405 train_time:55282ms step_avg:138.90ms
step:409/1405 train_time:55425ms step_avg:138.91ms
step:410/1405 train_time:55567ms step_avg:138.92ms
step:411/1405 train_time:55708ms step_avg:138.92ms
step:412/1405 train_time:55851ms step_avg:138.93ms
step:413/1405 train_time:55993ms step_avg:138.94ms
step:414/1405 train_time:56133ms step_avg:138.94ms
step:415/1405 train_time:56274ms step_avg:138.95ms
step:416/1405 train_time:56416ms step_avg:138.96ms
step:417/1405 train_time:56557ms step_avg:138.96ms
step:418/1405 train_time:56700ms step_avg:138.97ms
step:419/1405 train_time:56842ms step_avg:138.98ms
step:420/1405 train_time:56984ms step_avg:138.99ms
step:421/1405 train_time:57126ms step_avg:138.99ms
step:422/1405 train_time:57269ms step_avg:139.00ms
step:423/1405 train_time:57412ms step_avg:139.01ms
step:424/1405 train_time:57554ms step_avg:139.02ms
step:425/1405 train_time:57695ms step_avg:139.02ms
step:426/1405 train_time:57839ms step_avg:139.04ms
step:427/1405 train_time:57981ms step_avg:139.04ms
step:428/1405 train_time:58124ms step_avg:139.05ms
step:429/1405 train_time:58266ms step_avg:139.06ms
step:430/1405 train_time:58407ms step_avg:139.06ms
step:431/1405 train_time:58549ms step_avg:139.07ms
step:432/1405 train_time:58691ms step_avg:139.08ms
step:433/1405 train_time:58832ms step_avg:139.08ms
step:434/1405 train_time:58976ms step_avg:139.10ms
step:435/1405 train_time:59120ms step_avg:139.11ms
step:436/1405 train_time:59262ms step_avg:139.11ms
step:437/1405 train_time:59405ms step_avg:139.12ms
step:438/1405 train_time:59547ms step_avg:139.13ms
step:439/1405 train_time:59689ms step_avg:139.13ms
step:440/1405 train_time:59830ms step_avg:139.14ms
step:441/1405 train_time:59973ms step_avg:139.15ms
step:442/1405 train_time:60117ms step_avg:139.16ms
step:443/1405 train_time:60259ms step_avg:139.17ms
step:444/1405 train_time:60400ms step_avg:139.17ms
step:445/1405 train_time:60543ms step_avg:139.18ms
step:446/1405 train_time:60685ms step_avg:139.19ms
step:447/1405 train_time:60827ms step_avg:139.19ms
step:448/1405 train_time:60968ms step_avg:139.20ms
step:449/1405 train_time:61111ms step_avg:139.20ms
step:450/1405 train_time:61254ms step_avg:139.21ms
step:451/1405 train_time:61397ms step_avg:139.22ms
step:452/1405 train_time:61540ms step_avg:139.23ms
step:453/1405 train_time:61681ms step_avg:139.23ms
step:454/1405 train_time:61824ms step_avg:139.24ms
step:455/1405 train_time:61967ms step_avg:139.25ms
step:456/1405 train_time:62109ms step_avg:139.26ms
step:457/1405 train_time:62253ms step_avg:139.27ms
step:458/1405 train_time:62393ms step_avg:139.27ms
step:459/1405 train_time:62537ms step_avg:139.28ms
step:460/1405 train_time:62679ms step_avg:139.29ms
step:461/1405 train_time:62823ms step_avg:139.30ms
step:462/1405 train_time:62965ms step_avg:139.30ms
step:463/1405 train_time:63107ms step_avg:139.31ms
step:464/1405 train_time:63250ms step_avg:139.32ms
step:465/1405 train_time:63391ms step_avg:139.32ms
step:466/1405 train_time:63534ms step_avg:139.33ms
step:467/1405 train_time:63676ms step_avg:139.34ms
step:468/1405 train_time:63820ms step_avg:139.34ms
step:469/1405 train_time:63962ms step_avg:139.35ms
step:470/1405 train_time:64103ms step_avg:139.36ms
step:471/1405 train_time:64245ms step_avg:139.36ms
step:472/1405 train_time:64387ms step_avg:139.37ms
step:473/1405 train_time:64530ms step_avg:139.37ms
step:474/1405 train_time:64675ms step_avg:139.39ms
step:475/1405 train_time:64817ms step_avg:139.39ms
step:476/1405 train_time:64960ms step_avg:139.40ms
step:477/1405 train_time:65103ms step_avg:139.41ms
step:478/1405 train_time:65245ms step_avg:139.41ms
step:479/1405 train_time:65387ms step_avg:139.42ms
step:480/1405 train_time:65530ms step_avg:139.43ms
step:481/1405 train_time:65673ms step_avg:139.43ms
step:482/1405 train_time:65815ms step_avg:139.44ms
step:483/1405 train_time:65955ms step_avg:139.44ms
step:484/1405 train_time:66097ms step_avg:139.44ms
step:485/1405 train_time:66241ms step_avg:139.45ms
step:486/1405 train_time:66382ms step_avg:139.46ms
step:487/1405 train_time:66526ms step_avg:139.47ms
step:488/1405 train_time:66670ms step_avg:139.48ms
step:489/1405 train_time:66813ms step_avg:139.48ms
step:490/1405 train_time:66956ms step_avg:139.49ms
step:491/1405 train_time:67097ms step_avg:139.50ms
step:492/1405 train_time:67239ms step_avg:139.50ms
step:493/1405 train_time:67380ms step_avg:139.50ms
step:494/1405 train_time:67523ms step_avg:139.51ms
step:495/1405 train_time:67666ms step_avg:139.52ms
step:496/1405 train_time:67809ms step_avg:139.52ms
step:497/1405 train_time:67950ms step_avg:139.53ms
step:498/1405 train_time:68091ms step_avg:139.53ms
step:499/1405 train_time:68235ms step_avg:139.54ms
step:500/1405 train_time:68378ms step_avg:139.55ms
step:500/1405 val_loss:3.6614 train_time:68447ms step_avg:139.69ms
step:501/1405 train_time:68521ms step_avg:139.55ms
step:502/1405 train_time:68665ms step_avg:139.56ms
step:503/1405 train_time:68806ms step_avg:139.57ms
step:504/1405 train_time:68949ms step_avg:139.57ms
step:505/1405 train_time:69090ms step_avg:139.57ms
step:506/1405 train_time:69231ms step_avg:139.58ms
step:507/1405 train_time:69374ms step_avg:139.59ms
step:508/1405 train_time:69516ms step_avg:139.59ms
step:509/1405 train_time:69659ms step_avg:139.60ms
step:510/1405 train_time:69800ms step_avg:139.60ms
step:511/1405 train_time:69943ms step_avg:139.61ms
step:512/1405 train_time:70084ms step_avg:139.61ms
step:513/1405 train_time:70227ms step_avg:139.62ms
step:514/1405 train_time:70370ms step_avg:139.62ms
step:515/1405 train_time:70512ms step_avg:139.63ms
step:516/1405 train_time:70653ms step_avg:139.63ms
step:517/1405 train_time:70795ms step_avg:139.64ms
step:518/1405 train_time:70938ms step_avg:139.64ms
step:519/1405 train_time:71081ms step_avg:139.65ms
step:520/1405 train_time:71222ms step_avg:139.65ms
step:521/1405 train_time:71364ms step_avg:139.65ms
step:522/1405 train_time:71505ms step_avg:139.66ms
step:523/1405 train_time:71651ms step_avg:139.67ms
step:524/1405 train_time:71794ms step_avg:139.68ms
step:525/1405 train_time:71938ms step_avg:139.69ms
step:526/1405 train_time:72083ms step_avg:139.70ms
step:527/1405 train_time:72228ms step_avg:139.71ms
step:528/1405 train_time:72372ms step_avg:139.71ms
step:529/1405 train_time:72516ms step_avg:139.72ms
step:530/1405 train_time:72660ms step_avg:139.73ms
step:531/1405 train_time:72805ms step_avg:139.74ms
step:532/1405 train_time:72949ms step_avg:139.75ms
step:533/1405 train_time:73094ms step_avg:139.76ms
step:534/1405 train_time:73239ms step_avg:139.77ms
step:535/1405 train_time:73382ms step_avg:139.78ms
step:536/1405 train_time:73527ms step_avg:139.78ms
step:537/1405 train_time:73673ms step_avg:139.80ms
step:538/1405 train_time:73817ms step_avg:139.80ms
step:539/1405 train_time:73961ms step_avg:139.81ms
step:540/1405 train_time:74106ms step_avg:139.82ms
step:541/1405 train_time:74252ms step_avg:139.83ms
step:542/1405 train_time:74396ms step_avg:139.84ms
step:543/1405 train_time:74542ms step_avg:139.85ms
step:544/1405 train_time:74687ms step_avg:139.86ms
step:545/1405 train_time:74831ms step_avg:139.87ms
step:546/1405 train_time:74976ms step_avg:139.88ms
step:547/1405 train_time:75119ms step_avg:139.89ms
step:548/1405 train_time:75263ms step_avg:139.89ms
step:549/1405 train_time:75408ms step_avg:139.90ms
step:550/1405 train_time:75553ms step_avg:139.91ms
step:551/1405 train_time:75696ms step_avg:139.92ms
step:552/1405 train_time:75841ms step_avg:139.93ms
step:553/1405 train_time:75988ms step_avg:139.94ms
step:554/1405 train_time:76132ms step_avg:139.95ms
step:555/1405 train_time:76276ms step_avg:139.96ms
step:556/1405 train_time:76421ms step_avg:139.96ms
step:557/1405 train_time:76566ms step_avg:139.97ms
step:558/1405 train_time:76711ms step_avg:139.98ms
step:559/1405 train_time:76856ms step_avg:139.99ms
step:560/1405 train_time:77001ms step_avg:140.00ms
step:561/1405 train_time:77146ms step_avg:140.01ms
step:562/1405 train_time:77290ms step_avg:140.02ms
step:563/1405 train_time:77435ms step_avg:140.03ms
step:564/1405 train_time:77578ms step_avg:140.03ms
step:565/1405 train_time:77721ms step_avg:140.04ms
step:566/1405 train_time:77867ms step_avg:140.05ms
step:567/1405 train_time:78012ms step_avg:140.06ms
step:568/1405 train_time:78156ms step_avg:140.06ms
step:569/1405 train_time:78300ms step_avg:140.07ms
step:570/1405 train_time:78444ms step_avg:140.08ms
step:571/1405 train_time:78636ms step_avg:140.17ms
step:572/1405 train_time:78780ms step_avg:140.18ms
step:573/1405 train_time:78926ms step_avg:140.19ms
step:574/1405 train_time:79071ms step_avg:140.20ms
step:575/1405 train_time:79215ms step_avg:140.20ms
step:576/1405 train_time:79357ms step_avg:140.21ms
step:577/1405 train_time:79501ms step_avg:140.21ms
step:578/1405 train_time:79648ms step_avg:140.23ms
step:579/1405 train_time:79794ms step_avg:140.23ms
step:580/1405 train_time:79938ms step_avg:140.24ms
step:581/1405 train_time:80083ms step_avg:140.25ms
step:582/1405 train_time:80228ms step_avg:140.26ms
step:583/1405 train_time:80372ms step_avg:140.26ms
step:584/1405 train_time:80516ms step_avg:140.27ms
step:585/1405 train_time:80660ms step_avg:140.28ms
step:586/1405 train_time:80805ms step_avg:140.29ms
step:587/1405 train_time:80949ms step_avg:140.29ms
step:588/1405 train_time:81093ms step_avg:140.30ms
step:589/1405 train_time:81238ms step_avg:140.31ms
step:590/1405 train_time:81382ms step_avg:140.31ms
step:591/1405 train_time:81527ms step_avg:140.32ms
step:592/1405 train_time:81673ms step_avg:140.33ms
step:593/1405 train_time:81817ms step_avg:140.34ms
step:594/1405 train_time:81962ms step_avg:140.35ms
step:595/1405 train_time:82106ms step_avg:140.35ms
step:596/1405 train_time:82251ms step_avg:140.36ms
step:597/1405 train_time:82395ms step_avg:140.37ms
step:598/1405 train_time:82539ms step_avg:140.37ms
step:599/1405 train_time:82683ms step_avg:140.38ms
step:600/1405 train_time:82829ms step_avg:140.39ms
step:601/1405 train_time:82973ms step_avg:140.39ms
step:602/1405 train_time:83116ms step_avg:140.40ms
step:603/1405 train_time:83260ms step_avg:140.41ms
step:604/1405 train_time:83406ms step_avg:140.41ms
step:605/1405 train_time:83552ms step_avg:140.42ms
step:606/1405 train_time:83694ms step_avg:140.43ms
step:607/1405 train_time:83840ms step_avg:140.43ms
step:608/1405 train_time:83985ms step_avg:140.44ms
step:609/1405 train_time:84130ms step_avg:140.45ms
step:610/1405 train_time:84273ms step_avg:140.46ms
step:611/1405 train_time:84418ms step_avg:140.46ms
step:612/1405 train_time:84563ms step_avg:140.47ms
step:613/1405 train_time:84708ms step_avg:140.48ms
step:614/1405 train_time:84853ms step_avg:140.49ms
step:615/1405 train_time:84997ms step_avg:140.49ms
step:616/1405 train_time:85142ms step_avg:140.50ms
step:617/1405 train_time:85285ms step_avg:140.50ms
step:618/1405 train_time:85431ms step_avg:140.51ms
step:619/1405 train_time:85575ms step_avg:140.52ms
step:620/1405 train_time:85718ms step_avg:140.52ms
step:621/1405 train_time:85864ms step_avg:140.53ms
step:622/1405 train_time:86009ms step_avg:140.54ms
step:623/1405 train_time:86154ms step_avg:140.55ms
step:624/1405 train_time:86298ms step_avg:140.55ms
step:625/1405 train_time:86441ms step_avg:140.55ms
step:625/1405 val_loss:3.5787 train_time:86512ms step_avg:140.67ms
step:626/1405 train_time:86587ms step_avg:140.56ms
step:627/1405 train_time:86734ms step_avg:140.57ms
step:628/1405 train_time:86879ms step_avg:140.58ms
step:629/1405 train_time:87023ms step_avg:140.59ms
step:630/1405 train_time:87166ms step_avg:140.59ms
step:631/1405 train_time:87311ms step_avg:140.60ms
step:632/1405 train_time:87457ms step_avg:140.61ms
step:633/1405 train_time:87604ms step_avg:140.62ms
step:634/1405 train_time:87750ms step_avg:140.62ms
step:635/1405 train_time:87896ms step_avg:140.63ms
step:636/1405 train_time:88041ms step_avg:140.64ms
step:637/1405 train_time:88186ms step_avg:140.65ms
step:638/1405 train_time:88330ms step_avg:140.65ms
step:639/1405 train_time:88476ms step_avg:140.66ms
step:640/1405 train_time:88623ms step_avg:140.67ms
step:641/1405 train_time:88767ms step_avg:140.68ms
step:642/1405 train_time:88912ms step_avg:140.68ms
step:643/1405 train_time:89058ms step_avg:140.69ms
step:644/1405 train_time:89202ms step_avg:140.70ms
step:645/1405 train_time:89346ms step_avg:140.70ms
step:646/1405 train_time:89490ms step_avg:140.71ms
step:647/1405 train_time:89636ms step_avg:140.72ms
step:648/1405 train_time:89783ms step_avg:140.73ms
step:649/1405 train_time:89926ms step_avg:140.73ms
step:650/1405 train_time:90071ms step_avg:140.74ms
step:651/1405 train_time:90219ms step_avg:140.75ms
step:652/1405 train_time:90364ms step_avg:140.75ms
step:653/1405 train_time:90510ms step_avg:140.76ms
step:654/1405 train_time:90655ms step_avg:140.77ms
step:655/1405 train_time:90800ms step_avg:140.78ms
step:656/1405 train_time:90945ms step_avg:140.78ms
step:657/1405 train_time:91089ms step_avg:140.79ms
step:658/1405 train_time:91234ms step_avg:140.79ms
step:659/1405 train_time:91381ms step_avg:140.80ms
step:660/1405 train_time:91526ms step_avg:140.81ms
step:661/1405 train_time:91671ms step_avg:140.82ms
step:662/1405 train_time:91816ms step_avg:140.82ms
step:663/1405 train_time:91962ms step_avg:140.83ms
step:664/1405 train_time:92107ms step_avg:140.84ms
step:665/1405 train_time:92252ms step_avg:140.84ms
step:666/1405 train_time:92395ms step_avg:140.85ms
step:667/1405 train_time:92541ms step_avg:140.85ms
step:668/1405 train_time:92686ms step_avg:140.86ms
step:669/1405 train_time:92832ms step_avg:140.87ms
step:670/1405 train_time:92976ms step_avg:140.87ms
step:671/1405 train_time:93122ms step_avg:140.88ms
step:672/1405 train_time:93267ms step_avg:140.89ms
step:673/1405 train_time:93411ms step_avg:140.89ms
step:674/1405 train_time:93557ms step_avg:140.90ms
step:675/1405 train_time:93702ms step_avg:140.91ms
step:676/1405 train_time:93847ms step_avg:140.91ms
step:677/1405 train_time:93990ms step_avg:140.91ms
step:678/1405 train_time:94139ms step_avg:140.93ms
step:679/1405 train_time:94283ms step_avg:140.93ms
step:680/1405 train_time:94428ms step_avg:140.94ms
step:681/1405 train_time:94572ms step_avg:140.94ms
step:682/1405 train_time:94718ms step_avg:140.95ms
step:683/1405 train_time:94863ms step_avg:140.96ms
step:684/1405 train_time:95009ms step_avg:140.96ms
step:685/1405 train_time:95153ms step_avg:140.97ms
step:686/1405 train_time:95297ms step_avg:140.97ms
step:687/1405 train_time:95442ms step_avg:140.98ms
step:688/1405 train_time:95587ms step_avg:140.98ms
step:689/1405 train_time:95731ms step_avg:140.99ms
step:690/1405 train_time:95877ms step_avg:141.00ms
step:691/1405 train_time:96023ms step_avg:141.00ms
step:692/1405 train_time:96167ms step_avg:141.01ms
step:693/1405 train_time:96310ms step_avg:141.01ms
step:694/1405 train_time:96456ms step_avg:141.02ms
step:695/1405 train_time:96601ms step_avg:141.02ms
step:696/1405 train_time:96746ms step_avg:141.03ms
step:697/1405 train_time:96889ms step_avg:141.03ms
step:698/1405 train_time:97036ms step_avg:141.04ms
step:699/1405 train_time:97181ms step_avg:141.05ms
step:700/1405 train_time:97325ms step_avg:141.05ms
step:701/1405 train_time:97469ms step_avg:141.05ms
step:702/1405 train_time:97616ms step_avg:141.06ms
step:703/1405 train_time:97761ms step_avg:141.07ms
step:704/1405 train_time:97907ms step_avg:141.08ms
step:705/1405 train_time:98053ms step_avg:141.08ms
step:706/1405 train_time:98199ms step_avg:141.09ms
step:707/1405 train_time:98345ms step_avg:141.10ms
step:708/1405 train_time:98488ms step_avg:141.10ms
step:709/1405 train_time:98633ms step_avg:141.11ms
step:710/1405 train_time:98779ms step_avg:141.11ms
step:711/1405 train_time:98925ms step_avg:141.12ms
step:712/1405 train_time:99071ms step_avg:141.13ms
step:713/1405 train_time:99218ms step_avg:141.14ms
step:714/1405 train_time:99364ms step_avg:141.14ms
step:715/1405 train_time:99509ms step_avg:141.15ms
step:716/1405 train_time:99654ms step_avg:141.15ms
step:717/1405 train_time:99799ms step_avg:141.16ms
step:718/1405 train_time:99944ms step_avg:141.16ms
step:719/1405 train_time:100089ms step_avg:141.17ms
step:720/1405 train_time:100235ms step_avg:141.18ms
step:721/1405 train_time:100381ms step_avg:141.18ms
step:722/1405 train_time:100525ms step_avg:141.19ms
step:723/1405 train_time:100669ms step_avg:141.19ms
step:724/1405 train_time:100815ms step_avg:141.20ms
step:725/1405 train_time:100960ms step_avg:141.20ms
step:726/1405 train_time:101105ms step_avg:141.21ms
step:727/1405 train_time:101250ms step_avg:141.21ms
step:728/1405 train_time:101396ms step_avg:141.22ms
step:729/1405 train_time:101541ms step_avg:141.23ms
step:730/1405 train_time:101688ms step_avg:141.23ms
step:731/1405 train_time:101835ms step_avg:141.24ms
step:732/1405 train_time:101983ms step_avg:141.25ms
step:733/1405 train_time:102129ms step_avg:141.26ms
step:734/1405 train_time:102277ms step_avg:141.27ms
step:735/1405 train_time:102425ms step_avg:141.28ms
step:736/1405 train_time:102572ms step_avg:141.28ms
step:737/1405 train_time:102718ms step_avg:141.29ms
step:738/1405 train_time:102865ms step_avg:141.30ms
step:739/1405 train_time:103012ms step_avg:141.31ms
step:740/1405 train_time:103160ms step_avg:141.32ms
step:741/1405 train_time:103305ms step_avg:141.32ms
step:742/1405 train_time:103452ms step_avg:141.33ms
step:743/1405 train_time:103599ms step_avg:141.34ms
step:744/1405 train_time:103745ms step_avg:141.34ms
step:745/1405 train_time:103894ms step_avg:141.35ms
step:746/1405 train_time:104042ms step_avg:141.36ms
step:747/1405 train_time:104186ms step_avg:141.37ms
step:748/1405 train_time:104333ms step_avg:141.37ms
step:749/1405 train_time:104479ms step_avg:141.38ms
step:750/1405 train_time:104626ms step_avg:141.39ms
step:750/1405 val_loss:3.5265 train_time:104697ms step_avg:141.48ms
step:751/1405 train_time:104772ms step_avg:141.39ms
step:752/1405 train_time:104920ms step_avg:141.40ms
step:753/1405 train_time:105067ms step_avg:141.41ms
step:754/1405 train_time:105212ms step_avg:141.41ms
step:755/1405 train_time:105359ms step_avg:141.42ms
step:756/1405 train_time:105505ms step_avg:141.43ms
step:757/1405 train_time:105651ms step_avg:141.43ms
step:758/1405 train_time:105800ms step_avg:141.44ms
step:759/1405 train_time:105948ms step_avg:141.45ms
step:760/1405 train_time:106093ms step_avg:141.46ms
step:761/1405 train_time:106288ms step_avg:141.53ms
step:762/1405 train_time:106434ms step_avg:141.53ms
step:763/1405 train_time:106580ms step_avg:141.54ms
step:764/1405 train_time:106727ms step_avg:141.55ms
step:765/1405 train_time:106872ms step_avg:141.55ms
step:766/1405 train_time:107018ms step_avg:141.56ms
step:767/1405 train_time:107167ms step_avg:141.57ms
step:768/1405 train_time:107316ms step_avg:141.58ms
step:769/1405 train_time:107465ms step_avg:141.59ms
step:770/1405 train_time:107611ms step_avg:141.59ms
step:771/1405 train_time:107756ms step_avg:141.60ms
step:772/1405 train_time:107902ms step_avg:141.60ms
step:773/1405 train_time:108048ms step_avg:141.61ms
step:774/1405 train_time:108196ms step_avg:141.62ms
step:775/1405 train_time:108342ms step_avg:141.62ms
step:776/1405 train_time:108491ms step_avg:141.63ms
step:777/1405 train_time:108638ms step_avg:141.64ms
step:778/1405 train_time:108784ms step_avg:141.65ms
step:779/1405 train_time:108930ms step_avg:141.65ms
step:780/1405 train_time:109076ms step_avg:141.66ms
step:781/1405 train_time:109224ms step_avg:141.67ms
step:782/1405 train_time:109370ms step_avg:141.67ms
step:783/1405 train_time:109516ms step_avg:141.68ms
step:784/1405 train_time:109664ms step_avg:141.68ms
step:785/1405 train_time:109810ms step_avg:141.69ms
step:786/1405 train_time:109957ms step_avg:141.70ms
step:787/1405 train_time:110105ms step_avg:141.71ms
step:788/1405 train_time:110251ms step_avg:141.71ms
step:789/1405 train_time:110399ms step_avg:141.72ms
step:790/1405 train_time:110547ms step_avg:141.73ms
step:791/1405 train_time:110694ms step_avg:141.73ms
step:792/1405 train_time:110840ms step_avg:141.74ms
step:793/1405 train_time:110987ms step_avg:141.75ms
step:794/1405 train_time:111133ms step_avg:141.75ms
step:795/1405 train_time:111280ms step_avg:141.76ms
step:796/1405 train_time:111427ms step_avg:141.77ms
step:797/1405 train_time:111573ms step_avg:141.77ms
step:798/1405 train_time:111721ms step_avg:141.78ms
step:799/1405 train_time:111871ms step_avg:141.79ms
step:800/1405 train_time:112018ms step_avg:141.80ms
step:801/1405 train_time:112164ms step_avg:141.80ms
step:802/1405 train_time:112311ms step_avg:141.81ms
step:803/1405 train_time:112456ms step_avg:141.81ms
step:804/1405 train_time:112604ms step_avg:141.82ms
step:805/1405 train_time:112753ms step_avg:141.83ms
step:806/1405 train_time:112900ms step_avg:141.83ms
step:807/1405 train_time:113048ms step_avg:141.84ms
step:808/1405 train_time:113193ms step_avg:141.85ms
step:809/1405 train_time:113339ms step_avg:141.85ms
step:810/1405 train_time:113486ms step_avg:141.86ms
step:811/1405 train_time:113633ms step_avg:141.86ms
step:812/1405 train_time:113779ms step_avg:141.87ms
step:813/1405 train_time:113927ms step_avg:141.88ms
step:814/1405 train_time:114073ms step_avg:141.88ms
step:815/1405 train_time:114218ms step_avg:141.89ms
step:816/1405 train_time:114365ms step_avg:141.89ms
step:817/1405 train_time:114512ms step_avg:141.90ms
step:818/1405 train_time:114658ms step_avg:141.90ms
step:819/1405 train_time:114806ms step_avg:141.91ms
step:820/1405 train_time:114952ms step_avg:141.92ms
step:821/1405 train_time:115099ms step_avg:141.92ms
step:822/1405 train_time:115248ms step_avg:141.93ms
step:823/1405 train_time:115394ms step_avg:141.94ms
step:824/1405 train_time:115540ms step_avg:141.94ms
step:825/1405 train_time:115688ms step_avg:141.95ms
step:826/1405 train_time:115835ms step_avg:141.96ms
step:827/1405 train_time:115981ms step_avg:141.96ms
step:828/1405 train_time:116127ms step_avg:141.96ms
step:829/1405 train_time:116272ms step_avg:141.97ms
step:830/1405 train_time:116420ms step_avg:141.98ms
step:831/1405 train_time:116567ms step_avg:141.98ms
step:832/1405 train_time:116713ms step_avg:141.99ms
step:833/1405 train_time:116860ms step_avg:141.99ms
step:834/1405 train_time:117008ms step_avg:142.00ms
step:835/1405 train_time:117156ms step_avg:142.01ms
step:836/1405 train_time:117304ms step_avg:142.01ms
step:837/1405 train_time:117451ms step_avg:142.02ms
step:838/1405 train_time:117598ms step_avg:142.03ms
step:839/1405 train_time:117745ms step_avg:142.03ms
step:840/1405 train_time:117892ms step_avg:142.04ms
step:841/1405 train_time:118038ms step_avg:142.04ms
step:842/1405 train_time:118187ms step_avg:142.05ms
step:843/1405 train_time:118335ms step_avg:142.06ms
step:844/1405 train_time:118482ms step_avg:142.06ms
step:845/1405 train_time:118629ms step_avg:142.07ms
step:846/1405 train_time:118776ms step_avg:142.08ms
step:847/1405 train_time:118925ms step_avg:142.08ms
step:848/1405 train_time:119071ms step_avg:142.09ms
step:849/1405 train_time:119220ms step_avg:142.10ms
step:850/1405 train_time:119369ms step_avg:142.11ms
step:851/1405 train_time:119516ms step_avg:142.11ms
step:852/1405 train_time:119665ms step_avg:142.12ms
step:853/1405 train_time:119812ms step_avg:142.13ms
step:854/1405 train_time:119958ms step_avg:142.13ms
step:855/1405 train_time:120105ms step_avg:142.14ms
step:856/1405 train_time:120253ms step_avg:142.14ms
step:857/1405 train_time:120401ms step_avg:142.15ms
step:858/1405 train_time:120551ms step_avg:142.16ms
step:859/1405 train_time:120697ms step_avg:142.16ms
step:860/1405 train_time:120845ms step_avg:142.17ms
step:861/1405 train_time:120992ms step_avg:142.18ms
step:862/1405 train_time:121138ms step_avg:142.18ms
step:863/1405 train_time:121286ms step_avg:142.19ms
step:864/1405 train_time:121434ms step_avg:142.19ms
step:865/1405 train_time:121580ms step_avg:142.20ms
step:866/1405 train_time:121730ms step_avg:142.21ms
step:867/1405 train_time:121876ms step_avg:142.21ms
step:868/1405 train_time:122024ms step_avg:142.22ms
step:869/1405 train_time:122172ms step_avg:142.23ms
step:870/1405 train_time:122319ms step_avg:142.23ms
step:871/1405 train_time:122467ms step_avg:142.24ms
step:872/1405 train_time:122614ms step_avg:142.24ms
step:873/1405 train_time:122762ms step_avg:142.25ms
step:874/1405 train_time:122910ms step_avg:142.26ms
step:875/1405 train_time:123055ms step_avg:142.26ms
step:875/1405 val_loss:3.4766 train_time:123128ms step_avg:142.34ms
step:876/1405 train_time:123204ms step_avg:142.27ms
step:877/1405 train_time:123353ms step_avg:142.28ms
step:878/1405 train_time:123501ms step_avg:142.28ms
step:879/1405 train_time:123647ms step_avg:142.29ms
step:880/1405 train_time:123793ms step_avg:142.29ms
step:881/1405 train_time:123938ms step_avg:142.29ms
step:882/1405 train_time:124087ms step_avg:142.30ms
step:883/1405 train_time:124235ms step_avg:142.31ms
step:884/1405 train_time:124385ms step_avg:142.32ms
step:885/1405 train_time:124531ms step_avg:142.32ms
step:886/1405 train_time:124679ms step_avg:142.33ms
step:887/1405 train_time:124825ms step_avg:142.33ms
step:888/1405 train_time:124972ms step_avg:142.34ms
step:889/1405 train_time:125118ms step_avg:142.34ms
step:890/1405 train_time:125266ms step_avg:142.35ms
step:891/1405 train_time:125414ms step_avg:142.35ms
step:892/1405 train_time:125562ms step_avg:142.36ms
step:893/1405 train_time:125711ms step_avg:142.37ms
step:894/1405 train_time:125858ms step_avg:142.37ms
step:895/1405 train_time:126008ms step_avg:142.38ms
step:896/1405 train_time:126154ms step_avg:142.39ms
step:897/1405 train_time:126303ms step_avg:142.39ms
step:898/1405 train_time:126450ms step_avg:142.40ms
step:899/1405 train_time:126596ms step_avg:142.40ms
step:900/1405 train_time:126745ms step_avg:142.41ms
step:901/1405 train_time:126894ms step_avg:142.42ms
step:902/1405 train_time:127040ms step_avg:142.42ms
step:903/1405 train_time:127188ms step_avg:142.43ms
step:904/1405 train_time:127334ms step_avg:142.43ms
step:905/1405 train_time:127482ms step_avg:142.44ms
step:906/1405 train_time:127628ms step_avg:142.44ms
step:907/1405 train_time:127779ms step_avg:142.45ms
step:908/1405 train_time:127928ms step_avg:142.46ms
step:909/1405 train_time:128076ms step_avg:142.47ms
step:910/1405 train_time:128227ms step_avg:142.47ms
step:911/1405 train_time:128373ms step_avg:142.48ms
step:912/1405 train_time:128520ms step_avg:142.48ms
step:913/1405 train_time:128667ms step_avg:142.49ms
step:914/1405 train_time:128814ms step_avg:142.49ms
step:915/1405 train_time:128963ms step_avg:142.50ms
step:916/1405 train_time:129111ms step_avg:142.51ms
step:917/1405 train_time:129260ms step_avg:142.51ms
step:918/1405 train_time:129408ms step_avg:142.52ms
step:919/1405 train_time:129557ms step_avg:142.53ms
step:920/1405 train_time:129705ms step_avg:142.53ms
step:921/1405 train_time:129852ms step_avg:142.54ms
step:922/1405 train_time:130001ms step_avg:142.54ms
step:923/1405 train_time:130147ms step_avg:142.55ms
step:924/1405 train_time:130294ms step_avg:142.55ms
step:925/1405 train_time:130444ms step_avg:142.56ms
step:926/1405 train_time:130590ms step_avg:142.57ms
step:927/1405 train_time:130736ms step_avg:142.57ms
step:928/1405 train_time:130884ms step_avg:142.58ms
step:929/1405 train_time:131031ms step_avg:142.58ms
step:930/1405 train_time:131181ms step_avg:142.59ms
step:931/1405 train_time:131328ms step_avg:142.59ms
step:932/1405 train_time:131476ms step_avg:142.60ms
step:933/1405 train_time:131624ms step_avg:142.60ms
step:934/1405 train_time:131772ms step_avg:142.61ms
step:935/1405 train_time:131919ms step_avg:142.61ms
step:936/1405 train_time:132067ms step_avg:142.62ms
step:937/1405 train_time:132214ms step_avg:142.63ms
step:938/1405 train_time:132361ms step_avg:142.63ms
step:939/1405 train_time:132510ms step_avg:142.64ms
step:940/1405 train_time:132658ms step_avg:142.64ms
step:941/1405 train_time:132806ms step_avg:142.65ms
step:942/1405 train_time:132953ms step_avg:142.65ms
step:943/1405 train_time:133104ms step_avg:142.66ms
step:944/1405 train_time:133254ms step_avg:142.67ms
step:945/1405 train_time:133405ms step_avg:142.68ms
step:946/1405 train_time:133555ms step_avg:142.69ms
step:947/1405 train_time:133706ms step_avg:142.70ms
step:948/1405 train_time:133852ms step_avg:142.70ms
step:949/1405 train_time:134004ms step_avg:142.71ms
step:950/1405 train_time:134151ms step_avg:142.71ms
step:951/1405 train_time:134344ms step_avg:142.77ms
step:952/1405 train_time:134491ms step_avg:142.77ms
step:953/1405 train_time:134640ms step_avg:142.78ms
step:954/1405 train_time:134788ms step_avg:142.78ms
step:955/1405 train_time:134936ms step_avg:142.79ms
step:956/1405 train_time:135084ms step_avg:142.80ms
step:957/1405 train_time:135232ms step_avg:142.80ms
step:958/1405 train_time:135387ms step_avg:142.81ms
step:959/1405 train_time:135536ms step_avg:142.82ms
step:960/1405 train_time:135686ms step_avg:142.83ms
step:961/1405 train_time:135833ms step_avg:142.83ms
step:962/1405 train_time:135981ms step_avg:142.84ms
step:963/1405 train_time:136132ms step_avg:142.85ms
step:964/1405 train_time:136283ms step_avg:142.85ms
step:965/1405 train_time:136430ms step_avg:142.86ms
step:966/1405 train_time:136580ms step_avg:142.87ms
step:967/1405 train_time:136728ms step_avg:142.87ms
step:968/1405 train_time:136874ms step_avg:142.88ms
step:969/1405 train_time:137024ms step_avg:142.88ms
step:970/1405 train_time:137171ms step_avg:142.89ms
step:971/1405 train_time:137321ms step_avg:142.89ms
step:972/1405 train_time:137468ms step_avg:142.90ms
step:973/1405 train_time:137618ms step_avg:142.91ms
step:974/1405 train_time:137767ms step_avg:142.91ms
step:975/1405 train_time:137916ms step_avg:142.92ms
step:976/1405 train_time:138065ms step_avg:142.92ms
step:977/1405 train_time:138214ms step_avg:142.93ms
step:978/1405 train_time:138364ms step_avg:142.94ms
step:979/1405 train_time:138511ms step_avg:142.94ms
step:980/1405 train_time:138660ms step_avg:142.95ms
step:981/1405 train_time:138808ms step_avg:142.95ms
step:982/1405 train_time:138956ms step_avg:142.96ms
step:983/1405 train_time:139104ms step_avg:142.96ms
step:984/1405 train_time:139251ms step_avg:142.97ms
step:985/1405 train_time:139400ms step_avg:142.97ms
step:986/1405 train_time:139548ms step_avg:142.98ms
step:987/1405 train_time:139696ms step_avg:142.98ms
step:988/1405 train_time:139846ms step_avg:142.99ms
step:989/1405 train_time:139995ms step_avg:143.00ms
step:990/1405 train_time:140144ms step_avg:143.00ms
step:991/1405 train_time:140292ms step_avg:143.01ms
step:992/1405 train_time:140441ms step_avg:143.02ms
step:993/1405 train_time:140595ms step_avg:143.03ms
step:994/1405 train_time:140746ms step_avg:143.03ms
step:995/1405 train_time:140895ms step_avg:143.04ms
step:996/1405 train_time:141043ms step_avg:143.05ms
step:997/1405 train_time:141192ms step_avg:143.05ms
step:998/1405 train_time:141342ms step_avg:143.06ms
step:999/1405 train_time:141490ms step_avg:143.06ms
step:1000/1405 train_time:141638ms step_avg:143.07ms
step:1000/1405 val_loss:3.4111 train_time:141711ms step_avg:143.14ms
step:1001/1405 train_time:141786ms step_avg:143.07ms
step:1002/1405 train_time:141938ms step_avg:143.08ms
step:1003/1405 train_time:142088ms step_avg:143.09ms
step:1004/1405 train_time:142236ms step_avg:143.09ms
step:1005/1405 train_time:142384ms step_avg:143.10ms
step:1006/1405 train_time:142532ms step_avg:143.10ms
step:1007/1405 train_time:142682ms step_avg:143.11ms
step:1008/1405 train_time:142831ms step_avg:143.12ms
step:1009/1405 train_time:142985ms step_avg:143.13ms
step:1010/1405 train_time:143135ms step_avg:143.13ms
step:1011/1405 train_time:143283ms step_avg:143.14ms
step:1012/1405 train_time:143431ms step_avg:143.14ms
step:1013/1405 train_time:143581ms step_avg:143.15ms
step:1014/1405 train_time:143730ms step_avg:143.16ms
step:1015/1405 train_time:143879ms step_avg:143.16ms
step:1016/1405 train_time:144029ms step_avg:143.17ms
step:1017/1405 train_time:144179ms step_avg:143.18ms
step:1018/1405 train_time:144327ms step_avg:143.18ms
step:1019/1405 train_time:144476ms step_avg:143.19ms
step:1020/1405 train_time:144627ms step_avg:143.20ms
step:1021/1405 train_time:144776ms step_avg:143.20ms
step:1022/1405 train_time:144925ms step_avg:143.21ms
step:1023/1405 train_time:145075ms step_avg:143.21ms
step:1024/1405 train_time:145224ms step_avg:143.22ms
step:1025/1405 train_time:145372ms step_avg:143.22ms
step:1026/1405 train_time:145519ms step_avg:143.23ms
step:1027/1405 train_time:145667ms step_avg:143.23ms
step:1028/1405 train_time:145816ms step_avg:143.24ms
step:1029/1405 train_time:145966ms step_avg:143.24ms
step:1030/1405 train_time:146114ms step_avg:143.25ms
step:1031/1405 train_time:146262ms step_avg:143.25ms
step:1032/1405 train_time:146410ms step_avg:143.26ms
step:1033/1405 train_time:146558ms step_avg:143.26ms
step:1034/1405 train_time:146707ms step_avg:143.27ms
step:1035/1405 train_time:146857ms step_avg:143.28ms
step:1036/1405 train_time:147008ms step_avg:143.28ms
step:1037/1405 train_time:147157ms step_avg:143.29ms
step:1038/1405 train_time:147306ms step_avg:143.29ms
step:1039/1405 train_time:147455ms step_avg:143.30ms
step:1040/1405 train_time:147603ms step_avg:143.30ms
step:1041/1405 train_time:147753ms step_avg:143.31ms
step:1042/1405 train_time:147900ms step_avg:143.31ms
step:1043/1405 train_time:148050ms step_avg:143.32ms
step:1044/1405 train_time:148199ms step_avg:143.33ms
step:1045/1405 train_time:148351ms step_avg:143.33ms
step:1046/1405 train_time:148498ms step_avg:143.34ms
step:1047/1405 train_time:148647ms step_avg:143.34ms
step:1048/1405 train_time:148797ms step_avg:143.35ms
step:1049/1405 train_time:148949ms step_avg:143.36ms
step:1050/1405 train_time:149097ms step_avg:143.36ms
step:1051/1405 train_time:149249ms step_avg:143.37ms
step:1052/1405 train_time:149398ms step_avg:143.38ms
step:1053/1405 train_time:149547ms step_avg:143.38ms
step:1054/1405 train_time:149698ms step_avg:143.39ms
step:1055/1405 train_time:149849ms step_avg:143.40ms
step:1056/1405 train_time:149997ms step_avg:143.40ms
step:1057/1405 train_time:150148ms step_avg:143.41ms
step:1058/1405 train_time:150298ms step_avg:143.41ms
step:1059/1405 train_time:150449ms step_avg:143.42ms
step:1060/1405 train_time:150597ms step_avg:143.43ms
step:1061/1405 train_time:150746ms step_avg:143.43ms
step:1062/1405 train_time:150897ms step_avg:143.44ms
step:1063/1405 train_time:151047ms step_avg:143.44ms
step:1064/1405 train_time:151196ms step_avg:143.45ms
step:1065/1405 train_time:151346ms step_avg:143.46ms
step:1066/1405 train_time:151498ms step_avg:143.46ms
step:1067/1405 train_time:151651ms step_avg:143.47ms
step:1068/1405 train_time:151798ms step_avg:143.48ms
step:1069/1405 train_time:151951ms step_avg:143.49ms
step:1070/1405 train_time:152097ms step_avg:143.49ms
step:1071/1405 train_time:152246ms step_avg:143.49ms
step:1072/1405 train_time:152394ms step_avg:143.50ms
step:1073/1405 train_time:152545ms step_avg:143.50ms
step:1074/1405 train_time:152694ms step_avg:143.51ms
step:1075/1405 train_time:152844ms step_avg:143.52ms
step:1076/1405 train_time:152993ms step_avg:143.52ms
step:1077/1405 train_time:153143ms step_avg:143.53ms
step:1078/1405 train_time:153294ms step_avg:143.53ms
step:1079/1405 train_time:153445ms step_avg:143.54ms
step:1080/1405 train_time:153595ms step_avg:143.55ms
step:1081/1405 train_time:153743ms step_avg:143.55ms
step:1082/1405 train_time:153893ms step_avg:143.56ms
step:1083/1405 train_time:154043ms step_avg:143.56ms
step:1084/1405 train_time:154193ms step_avg:143.57ms
step:1085/1405 train_time:154342ms step_avg:143.57ms
step:1086/1405 train_time:154492ms step_avg:143.58ms
step:1087/1405 train_time:154642ms step_avg:143.59ms
step:1088/1405 train_time:154792ms step_avg:143.59ms
step:1089/1405 train_time:154944ms step_avg:143.60ms
step:1090/1405 train_time:155095ms step_avg:143.61ms
step:1091/1405 train_time:155245ms step_avg:143.61ms
step:1092/1405 train_time:155394ms step_avg:143.62ms
step:1093/1405 train_time:155544ms step_avg:143.62ms
step:1094/1405 train_time:155694ms step_avg:143.63ms
step:1095/1405 train_time:155846ms step_avg:143.64ms
step:1096/1405 train_time:155998ms step_avg:143.64ms
step:1097/1405 train_time:156149ms step_avg:143.65ms
step:1098/1405 train_time:156298ms step_avg:143.66ms
step:1099/1405 train_time:156448ms step_avg:143.66ms
step:1100/1405 train_time:156596ms step_avg:143.67ms
step:1101/1405 train_time:156747ms step_avg:143.67ms
step:1102/1405 train_time:156897ms step_avg:143.68ms
step:1103/1405 train_time:157046ms step_avg:143.68ms
step:1104/1405 train_time:157194ms step_avg:143.69ms
step:1105/1405 train_time:157345ms step_avg:143.69ms
step:1106/1405 train_time:157493ms step_avg:143.70ms
step:1107/1405 train_time:157643ms step_avg:143.70ms
step:1108/1405 train_time:157793ms step_avg:143.71ms
step:1109/1405 train_time:157943ms step_avg:143.72ms
step:1110/1405 train_time:158093ms step_avg:143.72ms
step:1111/1405 train_time:158244ms step_avg:143.73ms
step:1112/1405 train_time:158391ms step_avg:143.73ms
step:1113/1405 train_time:158542ms step_avg:143.74ms
step:1114/1405 train_time:158693ms step_avg:143.74ms
step:1115/1405 train_time:158844ms step_avg:143.75ms
step:1116/1405 train_time:158993ms step_avg:143.75ms
step:1117/1405 train_time:159144ms step_avg:143.76ms
step:1118/1405 train_time:159294ms step_avg:143.77ms
step:1119/1405 train_time:159446ms step_avg:143.77ms
step:1120/1405 train_time:159595ms step_avg:143.78ms
step:1121/1405 train_time:159746ms step_avg:143.79ms
step:1122/1405 train_time:159894ms step_avg:143.79ms
step:1123/1405 train_time:160043ms step_avg:143.79ms
step:1124/1405 train_time:160194ms step_avg:143.80ms
step:1125/1405 train_time:160343ms step_avg:143.81ms
step:1125/1405 val_loss:3.3589 train_time:160416ms step_avg:143.87ms
step:1126/1405 train_time:160492ms step_avg:143.81ms
step:1127/1405 train_time:160645ms step_avg:143.82ms
step:1128/1405 train_time:160794ms step_avg:143.82ms
step:1129/1405 train_time:160943ms step_avg:143.83ms
step:1130/1405 train_time:161091ms step_avg:143.83ms
step:1131/1405 train_time:161239ms step_avg:143.84ms
step:1132/1405 train_time:161389ms step_avg:143.84ms
step:1133/1405 train_time:161542ms step_avg:143.85ms
step:1134/1405 train_time:161692ms step_avg:143.85ms
step:1135/1405 train_time:161840ms step_avg:143.86ms
step:1136/1405 train_time:161991ms step_avg:143.86ms
step:1137/1405 train_time:162139ms step_avg:143.87ms
step:1138/1405 train_time:162287ms step_avg:143.87ms
step:1139/1405 train_time:162438ms step_avg:143.88ms
step:1140/1405 train_time:162589ms step_avg:143.88ms
step:1141/1405 train_time:162787ms step_avg:143.93ms
step:1142/1405 train_time:162935ms step_avg:143.94ms
step:1143/1405 train_time:163086ms step_avg:143.94ms
step:1144/1405 train_time:163235ms step_avg:143.95ms
step:1145/1405 train_time:163382ms step_avg:143.95ms
step:1146/1405 train_time:163533ms step_avg:143.96ms
step:1147/1405 train_time:163684ms step_avg:143.96ms
step:1148/1405 train_time:163837ms step_avg:143.97ms
step:1149/1405 train_time:163989ms step_avg:143.98ms
step:1150/1405 train_time:164139ms step_avg:143.98ms
step:1151/1405 train_time:164290ms step_avg:143.99ms
step:1152/1405 train_time:164441ms step_avg:143.99ms
step:1153/1405 train_time:164592ms step_avg:144.00ms
step:1154/1405 train_time:164743ms step_avg:144.01ms
step:1155/1405 train_time:164895ms step_avg:144.01ms
step:1156/1405 train_time:165048ms step_avg:144.02ms
step:1157/1405 train_time:165199ms step_avg:144.03ms
step:1158/1405 train_time:165351ms step_avg:144.03ms
step:1159/1405 train_time:165502ms step_avg:144.04ms
step:1160/1405 train_time:165652ms step_avg:144.04ms
step:1161/1405 train_time:165804ms step_avg:144.05ms
step:1162/1405 train_time:165955ms step_avg:144.06ms
step:1163/1405 train_time:166106ms step_avg:144.06ms
step:1164/1405 train_time:166257ms step_avg:144.07ms
step:1165/1405 train_time:166407ms step_avg:144.08ms
step:1166/1405 train_time:166558ms step_avg:144.08ms
step:1167/1405 train_time:166708ms step_avg:144.09ms
step:1168/1405 train_time:166858ms step_avg:144.09ms
step:1169/1405 train_time:167012ms step_avg:144.10ms
step:1170/1405 train_time:167162ms step_avg:144.10ms
step:1171/1405 train_time:167314ms step_avg:144.11ms
step:1172/1405 train_time:167466ms step_avg:144.12ms
step:1173/1405 train_time:167617ms step_avg:144.12ms
step:1174/1405 train_time:167770ms step_avg:144.13ms
step:1175/1405 train_time:167924ms step_avg:144.14ms
step:1176/1405 train_time:168079ms step_avg:144.15ms
step:1177/1405 train_time:168232ms step_avg:144.16ms
step:1178/1405 train_time:168382ms step_avg:144.16ms
step:1179/1405 train_time:168533ms step_avg:144.17ms
step:1180/1405 train_time:168688ms step_avg:144.18ms
step:1181/1405 train_time:168837ms step_avg:144.18ms
step:1182/1405 train_time:168987ms step_avg:144.19ms
step:1183/1405 train_time:169138ms step_avg:144.19ms
step:1184/1405 train_time:169290ms step_avg:144.20ms
step:1185/1405 train_time:169443ms step_avg:144.21ms
step:1186/1405 train_time:169593ms step_avg:144.21ms
step:1187/1405 train_time:169747ms step_avg:144.22ms
step:1188/1405 train_time:169896ms step_avg:144.22ms
step:1189/1405 train_time:170050ms step_avg:144.23ms
step:1190/1405 train_time:170201ms step_avg:144.24ms
step:1191/1405 train_time:170353ms step_avg:144.24ms
step:1192/1405 train_time:170503ms step_avg:144.25ms
step:1193/1405 train_time:170653ms step_avg:144.25ms
step:1194/1405 train_time:170804ms step_avg:144.26ms
step:1195/1405 train_time:170955ms step_avg:144.27ms
step:1196/1405 train_time:171107ms step_avg:144.27ms
step:1197/1405 train_time:171257ms step_avg:144.28ms
step:1198/1405 train_time:171411ms step_avg:144.29ms
step:1199/1405 train_time:171560ms step_avg:144.29ms
step:1200/1405 train_time:171710ms step_avg:144.29ms
step:1201/1405 train_time:171860ms step_avg:144.30ms
step:1202/1405 train_time:172018ms step_avg:144.31ms
step:1203/1405 train_time:172170ms step_avg:144.32ms
step:1204/1405 train_time:172322ms step_avg:144.32ms
step:1205/1405 train_time:172472ms step_avg:144.33ms
step:1206/1405 train_time:172624ms step_avg:144.33ms
step:1207/1405 train_time:172773ms step_avg:144.34ms
step:1208/1405 train_time:172924ms step_avg:144.34ms
step:1209/1405 train_time:173075ms step_avg:144.35ms
step:1210/1405 train_time:173229ms step_avg:144.36ms
step:1211/1405 train_time:173381ms step_avg:144.36ms
step:1212/1405 train_time:173530ms step_avg:144.37ms
step:1213/1405 train_time:173682ms step_avg:144.37ms
step:1214/1405 train_time:173833ms step_avg:144.38ms
step:1215/1405 train_time:173983ms step_avg:144.38ms
step:1216/1405 train_time:174134ms step_avg:144.39ms
step:1217/1405 train_time:174284ms step_avg:144.39ms
step:1218/1405 train_time:174435ms step_avg:144.40ms
step:1219/1405 train_time:174586ms step_avg:144.41ms
step:1220/1405 train_time:174737ms step_avg:144.41ms
step:1221/1405 train_time:174887ms step_avg:144.41ms
step:1222/1405 train_time:175037ms step_avg:144.42ms
step:1223/1405 train_time:175190ms step_avg:144.43ms
step:1224/1405 train_time:175342ms step_avg:144.43ms
step:1225/1405 train_time:175493ms step_avg:144.44ms
step:1226/1405 train_time:175643ms step_avg:144.44ms
step:1227/1405 train_time:175796ms step_avg:144.45ms
step:1228/1405 train_time:175944ms step_avg:144.45ms
step:1229/1405 train_time:176094ms step_avg:144.46ms
step:1230/1405 train_time:176247ms step_avg:144.46ms
step:1231/1405 train_time:176397ms step_avg:144.47ms
step:1232/1405 train_time:176550ms step_avg:144.48ms
step:1233/1405 train_time:176699ms step_avg:144.48ms
step:1234/1405 train_time:176851ms step_avg:144.49ms
step:1235/1405 train_time:177004ms step_avg:144.49ms
step:1236/1405 train_time:177155ms step_avg:144.50ms
step:1237/1405 train_time:177304ms step_avg:144.50ms
step:1238/1405 train_time:177458ms step_avg:144.51ms
step:1239/1405 train_time:177609ms step_avg:144.51ms
step:1240/1405 train_time:177760ms step_avg:144.52ms
step:1241/1405 train_time:177914ms step_avg:144.53ms
step:1242/1405 train_time:178065ms step_avg:144.53ms
step:1243/1405 train_time:178216ms step_avg:144.54ms
step:1244/1405 train_time:178365ms step_avg:144.54ms
step:1245/1405 train_time:178516ms step_avg:144.55ms
step:1246/1405 train_time:178668ms step_avg:144.55ms
step:1247/1405 train_time:178818ms step_avg:144.56ms
step:1248/1405 train_time:178969ms step_avg:144.56ms
step:1249/1405 train_time:179120ms step_avg:144.57ms
step:1250/1405 train_time:179271ms step_avg:144.57ms
step:1250/1405 val_loss:3.3119 train_time:179345ms step_avg:144.63ms
step:1251/1405 train_time:179423ms step_avg:144.58ms
step:1252/1405 train_time:179573ms step_avg:144.58ms
step:1253/1405 train_time:179725ms step_avg:144.59ms
step:1254/1405 train_time:179873ms step_avg:144.59ms
step:1255/1405 train_time:180031ms step_avg:144.60ms
step:1256/1405 train_time:180181ms step_avg:144.61ms
step:1257/1405 train_time:180332ms step_avg:144.61ms
step:1258/1405 train_time:180485ms step_avg:144.62ms
step:1259/1405 train_time:180637ms step_avg:144.63ms
step:1260/1405 train_time:180788ms step_avg:144.63ms
step:1261/1405 train_time:180940ms step_avg:144.64ms
step:1262/1405 train_time:181092ms step_avg:144.64ms
step:1263/1405 train_time:181244ms step_avg:144.65ms
step:1264/1405 train_time:181393ms step_avg:144.65ms
step:1265/1405 train_time:181545ms step_avg:144.66ms
step:1266/1405 train_time:181697ms step_avg:144.66ms
step:1267/1405 train_time:181847ms step_avg:144.67ms
step:1268/1405 train_time:182000ms step_avg:144.67ms
step:1269/1405 train_time:182154ms step_avg:144.68ms
step:1270/1405 train_time:182306ms step_avg:144.69ms
step:1271/1405 train_time:182458ms step_avg:144.69ms
step:1272/1405 train_time:182609ms step_avg:144.70ms
step:1273/1405 train_time:182759ms step_avg:144.70ms
step:1274/1405 train_time:182909ms step_avg:144.71ms
step:1275/1405 train_time:183061ms step_avg:144.71ms
step:1276/1405 train_time:183210ms step_avg:144.72ms
step:1277/1405 train_time:183363ms step_avg:144.72ms
step:1278/1405 train_time:183513ms step_avg:144.73ms
step:1279/1405 train_time:183665ms step_avg:144.73ms
step:1280/1405 train_time:183819ms step_avg:144.74ms
step:1281/1405 train_time:183971ms step_avg:144.75ms
step:1282/1405 train_time:184122ms step_avg:144.75ms
step:1283/1405 train_time:184274ms step_avg:144.76ms
step:1284/1405 train_time:184426ms step_avg:144.76ms
step:1285/1405 train_time:184577ms step_avg:144.77ms
step:1286/1405 train_time:184727ms step_avg:144.77ms
step:1287/1405 train_time:184878ms step_avg:144.78ms
step:1288/1405 train_time:185029ms step_avg:144.78ms
step:1289/1405 train_time:185183ms step_avg:144.79ms
step:1290/1405 train_time:185336ms step_avg:144.79ms
step:1291/1405 train_time:185488ms step_avg:144.80ms
step:1292/1405 train_time:185640ms step_avg:144.81ms
step:1293/1405 train_time:185791ms step_avg:144.81ms
step:1294/1405 train_time:185944ms step_avg:144.82ms
step:1295/1405 train_time:186093ms step_avg:144.82ms
step:1296/1405 train_time:186245ms step_avg:144.83ms
step:1297/1405 train_time:186399ms step_avg:144.83ms
step:1298/1405 train_time:186550ms step_avg:144.84ms
step:1299/1405 train_time:186701ms step_avg:144.84ms
step:1300/1405 train_time:186850ms step_avg:144.85ms
step:1301/1405 train_time:187002ms step_avg:144.85ms
step:1302/1405 train_time:187155ms step_avg:144.86ms
step:1303/1405 train_time:187308ms step_avg:144.86ms
step:1304/1405 train_time:187461ms step_avg:144.87ms
step:1305/1405 train_time:187611ms step_avg:144.87ms
step:1306/1405 train_time:187766ms step_avg:144.88ms
step:1307/1405 train_time:187916ms step_avg:144.89ms
step:1308/1405 train_time:188067ms step_avg:144.89ms
step:1309/1405 train_time:188217ms step_avg:144.89ms
step:1310/1405 train_time:188367ms step_avg:144.90ms
step:1311/1405 train_time:188519ms step_avg:144.90ms
step:1312/1405 train_time:188669ms step_avg:144.91ms
step:1313/1405 train_time:188820ms step_avg:144.91ms
step:1314/1405 train_time:188972ms step_avg:144.92ms
step:1315/1405 train_time:189122ms step_avg:144.92ms
step:1316/1405 train_time:189272ms step_avg:144.93ms
step:1317/1405 train_time:189423ms step_avg:144.93ms
step:1318/1405 train_time:189576ms step_avg:144.94ms
step:1319/1405 train_time:189728ms step_avg:144.94ms
step:1320/1405 train_time:189880ms step_avg:144.95ms
step:1321/1405 train_time:190031ms step_avg:144.95ms
step:1322/1405 train_time:190184ms step_avg:144.96ms
step:1323/1405 train_time:190333ms step_avg:144.96ms
step:1324/1405 train_time:190485ms step_avg:144.97ms
step:1325/1405 train_time:190635ms step_avg:144.97ms
step:1326/1405 train_time:190790ms step_avg:144.98ms
step:1327/1405 train_time:190939ms step_avg:144.98ms
step:1328/1405 train_time:191090ms step_avg:144.98ms
step:1329/1405 train_time:191250ms step_avg:145.00ms
step:1330/1405 train_time:191402ms step_avg:145.00ms
step:1331/1405 train_time:191600ms step_avg:145.04ms
step:1332/1405 train_time:191750ms step_avg:145.05ms
step:1333/1405 train_time:191902ms step_avg:145.05ms
step:1334/1405 train_time:192051ms step_avg:145.05ms
step:1335/1405 train_time:192200ms step_avg:145.06ms
step:1336/1405 train_time:192352ms step_avg:145.06ms
step:1337/1405 train_time:192505ms step_avg:145.07ms
step:1338/1405 train_time:192659ms step_avg:145.07ms
step:1339/1405 train_time:192813ms step_avg:145.08ms
step:1340/1405 train_time:192964ms step_avg:145.09ms
step:1341/1405 train_time:193114ms step_avg:145.09ms
step:1342/1405 train_time:193266ms step_avg:145.09ms
step:1343/1405 train_time:193417ms step_avg:145.10ms
step:1344/1405 train_time:193567ms step_avg:145.10ms
step:1345/1405 train_time:193718ms step_avg:145.11ms
step:1346/1405 train_time:193871ms step_avg:145.11ms
step:1347/1405 train_time:194023ms step_avg:145.12ms
step:1348/1405 train_time:194173ms step_avg:145.12ms
step:1349/1405 train_time:194326ms step_avg:145.13ms
step:1350/1405 train_time:194477ms step_avg:145.13ms
step:1351/1405 train_time:194626ms step_avg:145.14ms
step:1352/1405 train_time:194780ms step_avg:145.14ms
step:1353/1405 train_time:194933ms step_avg:145.15ms
step:1354/1405 train_time:195085ms step_avg:145.15ms
step:1355/1405 train_time:195239ms step_avg:145.16ms
step:1356/1405 train_time:195391ms step_avg:145.16ms
step:1357/1405 train_time:195545ms step_avg:145.17ms
step:1358/1405 train_time:195699ms step_avg:145.18ms
step:1359/1405 train_time:195851ms step_avg:145.18ms
step:1360/1405 train_time:196005ms step_avg:145.19ms
step:1361/1405 train_time:196159ms step_avg:145.20ms
step:1362/1405 train_time:196310ms step_avg:145.20ms
step:1363/1405 train_time:196467ms step_avg:145.21ms
step:1364/1405 train_time:196618ms step_avg:145.21ms
step:1365/1405 train_time:196768ms step_avg:145.22ms
step:1366/1405 train_time:196921ms step_avg:145.22ms
step:1367/1405 train_time:197074ms step_avg:145.23ms
step:1368/1405 train_time:197227ms step_avg:145.23ms
step:1369/1405 train_time:197383ms step_avg:145.24ms
step:1370/1405 train_time:197535ms step_avg:145.25ms
step:1371/1405 train_time:197686ms step_avg:145.25ms
step:1372/1405 train_time:197842ms step_avg:145.26ms
step:1373/1405 train_time:197992ms step_avg:145.26ms
step:1374/1405 train_time:198145ms step_avg:145.27ms
step:1375/1405 train_time:198297ms step_avg:145.27ms
step:1375/1405 val_loss:3.2812 train_time:198371ms step_avg:145.33ms
step:1376/1405 train_time:198448ms step_avg:145.28ms
step:1377/1405 train_time:198601ms step_avg:145.28ms
step:1378/1405 train_time:198753ms step_avg:145.29ms
step:1379/1405 train_time:198907ms step_avg:145.29ms
step:1380/1405 train_time:199056ms step_avg:145.30ms
step:1381/1405 train_time:199210ms step_avg:145.30ms
step:1382/1405 train_time:199363ms step_avg:145.31ms
step:1383/1405 train_time:199515ms step_avg:145.31ms
step:1384/1405 train_time:199671ms step_avg:145.32ms
step:1385/1405 train_time:199825ms step_avg:145.33ms
step:1386/1405 train_time:199979ms step_avg:145.33ms
step:1387/1405 train_time:200134ms step_avg:145.34ms
step:1388/1405 train_time:200287ms step_avg:145.35ms
step:1389/1405 train_time:200443ms step_avg:145.35ms
step:1390/1405 train_time:200595ms step_avg:145.36ms
step:1391/1405 train_time:200747ms step_avg:145.36ms
step:1392/1405 train_time:200901ms step_avg:145.37ms
step:1393/1405 train_time:201054ms step_avg:145.38ms
step:1394/1405 train_time:201207ms step_avg:145.38ms
step:1395/1405 train_time:201359ms step_avg:145.39ms
step:1396/1405 train_time:201509ms step_avg:145.39ms
step:1397/1405 train_time:201660ms step_avg:145.39ms
step:1398/1405 train_time:201811ms step_avg:145.40ms
step:1399/1405 train_time:201962ms step_avg:145.40ms
step:1400/1405 train_time:202119ms step_avg:145.41ms
step:1401/1405 train_time:202270ms step_avg:145.41ms
step:1402/1405 train_time:202423ms step_avg:145.42ms
step:1403/1405 train_time:202577ms step_avg:145.43ms
step:1404/1405 train_time:202729ms step_avg:145.43ms
step:1405/1405 train_time:202881ms step_avg:145.43ms
step:1405/1405 val_loss:3.2786 train_time:202958ms step_avg:145.49ms
peak memory consumption: 31569 MiB

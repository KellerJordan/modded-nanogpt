import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: torch.Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_masks(sliding_window_num_blocks: int):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            global_block_mask = BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
            local_window_bm = q_idx - kv_idx < max(1, sliding_window_num_blocks // 2)
            local_window_full_bm = local_window_bm
            local_nonzero_bm = causal_bm & local_window_bm & document_bm
            local_full_bm = causal_full_bm & local_window_full_bm & document_full_bm
            local_kv_num_blocks, local_kv_indices = dense_to_ordered(local_nonzero_bm & ~local_full_bm)
            local_full_kv_num_blocks, local_full_kv_indices = dense_to_ordered(local_full_bm)
            local_block_mask = BlockMask.from_kv_blocks(
                local_kv_num_blocks,
                local_kv_indices,
                local_full_kv_num_blocks,
                local_full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
            return global_block_mask, local_block_mask

        global_block_mask, local_block_mask = create_doc_swc_block_masks(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            block_mask = global_block_mask if i % 2 == 0 else local_block_mask
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            block_mask = local_block_mask if i % 2 == 0 else global_block_mask
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1405 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Wed Jan 15 09:12:02 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   27C    P0             117W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   26C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   28C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   25C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   25C    P0             108W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   25C    P0             114W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1405 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1405 train_time:27949ms step_avg:nanms
step:2/1405 train_time:28025ms step_avg:nanms
step:3/1405 train_time:28205ms step_avg:nanms
step:4/1405 train_time:28337ms step_avg:nanms
step:5/1405 train_time:28473ms step_avg:nanms
step:6/1405 train_time:28605ms step_avg:nanms
step:7/1405 train_time:28738ms step_avg:nanms
step:8/1405 train_time:28873ms step_avg:nanms
step:9/1405 train_time:29006ms step_avg:nanms
step:10/1405 train_time:29143ms step_avg:nanms
step:11/1405 train_time:136ms step_avg:nanms
step:12/1405 train_time:270ms step_avg:nanms
step:13/1405 train_time:406ms step_avg:135.28ms
step:14/1405 train_time:539ms step_avg:134.84ms
step:15/1405 train_time:673ms step_avg:134.59ms
step:16/1405 train_time:811ms step_avg:135.15ms
step:17/1405 train_time:945ms step_avg:135.07ms
step:18/1405 train_time:1083ms step_avg:135.38ms
step:19/1405 train_time:1216ms step_avg:135.13ms
step:20/1405 train_time:1352ms step_avg:135.18ms
step:21/1405 train_time:1487ms step_avg:135.19ms
step:22/1405 train_time:1621ms step_avg:135.09ms
step:23/1405 train_time:1755ms step_avg:134.98ms
step:24/1405 train_time:1890ms step_avg:135.02ms
step:25/1405 train_time:2026ms step_avg:135.07ms
step:26/1405 train_time:2162ms step_avg:135.12ms
step:27/1405 train_time:2296ms step_avg:135.09ms
step:28/1405 train_time:2432ms step_avg:135.12ms
step:29/1405 train_time:2569ms step_avg:135.19ms
step:30/1405 train_time:2706ms step_avg:135.30ms
step:31/1405 train_time:2839ms step_avg:135.18ms
step:32/1405 train_time:2974ms step_avg:135.17ms
step:33/1405 train_time:3109ms step_avg:135.19ms
step:34/1405 train_time:3245ms step_avg:135.22ms
step:35/1405 train_time:3381ms step_avg:135.23ms
step:36/1405 train_time:3516ms step_avg:135.22ms
step:37/1405 train_time:3652ms step_avg:135.26ms
step:38/1405 train_time:3788ms step_avg:135.27ms
step:39/1405 train_time:3922ms step_avg:135.26ms
step:40/1405 train_time:4056ms step_avg:135.20ms
step:41/1405 train_time:4191ms step_avg:135.20ms
step:42/1405 train_time:4325ms step_avg:135.17ms
step:43/1405 train_time:4460ms step_avg:135.14ms
step:44/1405 train_time:4595ms step_avg:135.15ms
step:45/1405 train_time:4730ms step_avg:135.14ms
step:46/1405 train_time:4866ms step_avg:135.15ms
step:47/1405 train_time:5001ms step_avg:135.15ms
step:48/1405 train_time:5136ms step_avg:135.15ms
step:49/1405 train_time:5270ms step_avg:135.13ms
step:50/1405 train_time:5406ms step_avg:135.14ms
step:51/1405 train_time:5540ms step_avg:135.13ms
step:52/1405 train_time:5676ms step_avg:135.14ms
step:53/1405 train_time:5811ms step_avg:135.14ms
step:54/1405 train_time:5945ms step_avg:135.12ms
step:55/1405 train_time:6082ms step_avg:135.15ms
step:56/1405 train_time:6217ms step_avg:135.14ms
step:57/1405 train_time:6351ms step_avg:135.13ms
step:58/1405 train_time:6488ms step_avg:135.18ms
step:59/1405 train_time:6624ms step_avg:135.19ms
step:60/1405 train_time:6757ms step_avg:135.14ms
step:61/1405 train_time:6892ms step_avg:135.13ms
step:62/1405 train_time:7027ms step_avg:135.14ms
step:63/1405 train_time:7162ms step_avg:135.13ms
step:64/1405 train_time:7296ms step_avg:135.11ms
step:65/1405 train_time:7432ms step_avg:135.13ms
step:66/1405 train_time:7567ms step_avg:135.12ms
step:67/1405 train_time:7703ms step_avg:135.13ms
step:68/1405 train_time:7836ms step_avg:135.10ms
step:69/1405 train_time:7972ms step_avg:135.12ms
step:70/1405 train_time:8107ms step_avg:135.12ms
step:71/1405 train_time:8242ms step_avg:135.12ms
step:72/1405 train_time:8376ms step_avg:135.10ms
step:73/1405 train_time:8512ms step_avg:135.11ms
step:74/1405 train_time:8647ms step_avg:135.10ms
step:75/1405 train_time:8782ms step_avg:135.11ms
step:76/1405 train_time:8918ms step_avg:135.12ms
step:77/1405 train_time:9053ms step_avg:135.11ms
step:78/1405 train_time:9189ms step_avg:135.13ms
step:79/1405 train_time:9325ms step_avg:135.15ms
step:80/1405 train_time:9460ms step_avg:135.15ms
step:81/1405 train_time:9595ms step_avg:135.14ms
step:82/1405 train_time:9729ms step_avg:135.13ms
step:83/1405 train_time:9865ms step_avg:135.14ms
step:84/1405 train_time:10000ms step_avg:135.13ms
step:85/1405 train_time:10135ms step_avg:135.13ms
step:86/1405 train_time:10271ms step_avg:135.14ms
step:87/1405 train_time:10406ms step_avg:135.15ms
step:88/1405 train_time:10541ms step_avg:135.14ms
step:89/1405 train_time:10676ms step_avg:135.14ms
step:90/1405 train_time:10811ms step_avg:135.14ms
step:91/1405 train_time:10945ms step_avg:135.13ms
step:92/1405 train_time:11081ms step_avg:135.14ms
step:93/1405 train_time:11217ms step_avg:135.14ms
step:94/1405 train_time:11352ms step_avg:135.15ms
step:95/1405 train_time:11489ms step_avg:135.17ms
step:96/1405 train_time:11625ms step_avg:135.18ms
step:97/1405 train_time:11761ms step_avg:135.19ms
step:98/1405 train_time:11896ms step_avg:135.18ms
step:99/1405 train_time:12031ms step_avg:135.18ms
step:100/1405 train_time:12167ms step_avg:135.19ms
step:101/1405 train_time:12301ms step_avg:135.18ms
step:102/1405 train_time:12436ms step_avg:135.18ms
step:103/1405 train_time:12572ms step_avg:135.18ms
step:104/1405 train_time:12709ms step_avg:135.20ms
step:105/1405 train_time:12843ms step_avg:135.19ms
step:106/1405 train_time:12980ms step_avg:135.21ms
step:107/1405 train_time:13116ms step_avg:135.21ms
step:108/1405 train_time:13253ms step_avg:135.23ms
step:109/1405 train_time:13390ms step_avg:135.25ms
step:110/1405 train_time:13528ms step_avg:135.28ms
step:111/1405 train_time:13664ms step_avg:135.28ms
step:112/1405 train_time:13799ms step_avg:135.28ms
step:113/1405 train_time:13935ms step_avg:135.29ms
step:114/1405 train_time:14073ms step_avg:135.32ms
step:115/1405 train_time:14210ms step_avg:135.34ms
step:116/1405 train_time:14346ms step_avg:135.34ms
step:117/1405 train_time:14484ms step_avg:135.37ms
step:118/1405 train_time:14621ms step_avg:135.38ms
step:119/1405 train_time:14757ms step_avg:135.38ms
step:120/1405 train_time:14893ms step_avg:135.39ms
step:121/1405 train_time:15032ms step_avg:135.42ms
step:122/1405 train_time:15171ms step_avg:135.46ms
step:123/1405 train_time:15309ms step_avg:135.48ms
step:124/1405 train_time:15445ms step_avg:135.48ms
step:125/1405 train_time:15583ms step_avg:135.50ms
step:125/1405 val_loss:4.4095 train_time:15648ms step_avg:136.07ms
step:126/1405 train_time:15722ms step_avg:135.53ms
step:127/1405 train_time:15865ms step_avg:135.60ms
step:128/1405 train_time:16002ms step_avg:135.61ms
step:129/1405 train_time:16138ms step_avg:135.62ms
step:130/1405 train_time:16273ms step_avg:135.61ms
step:131/1405 train_time:16409ms step_avg:135.61ms
step:132/1405 train_time:16545ms step_avg:135.61ms
step:133/1405 train_time:16685ms step_avg:135.65ms
step:134/1405 train_time:16826ms step_avg:135.69ms
step:135/1405 train_time:16964ms step_avg:135.71ms
step:136/1405 train_time:17102ms step_avg:135.73ms
step:137/1405 train_time:17239ms step_avg:135.74ms
step:138/1405 train_time:17376ms step_avg:135.75ms
step:139/1405 train_time:17512ms step_avg:135.75ms
step:140/1405 train_time:17649ms step_avg:135.76ms
step:141/1405 train_time:17788ms step_avg:135.79ms
step:142/1405 train_time:17925ms step_avg:135.80ms
step:143/1405 train_time:18064ms step_avg:135.82ms
step:144/1405 train_time:18199ms step_avg:135.82ms
step:145/1405 train_time:18338ms step_avg:135.84ms
step:146/1405 train_time:18474ms step_avg:135.84ms
step:147/1405 train_time:18611ms step_avg:135.85ms
step:148/1405 train_time:18748ms step_avg:135.86ms
step:149/1405 train_time:18887ms step_avg:135.88ms
step:150/1405 train_time:19026ms step_avg:135.90ms
step:151/1405 train_time:19164ms step_avg:135.91ms
step:152/1405 train_time:19299ms step_avg:135.91ms
step:153/1405 train_time:19437ms step_avg:135.92ms
step:154/1405 train_time:19574ms step_avg:135.93ms
step:155/1405 train_time:19711ms step_avg:135.94ms
step:156/1405 train_time:19850ms step_avg:135.96ms
step:157/1405 train_time:19988ms step_avg:135.97ms
step:158/1405 train_time:20126ms step_avg:135.99ms
step:159/1405 train_time:20265ms step_avg:136.01ms
step:160/1405 train_time:20401ms step_avg:136.01ms
step:161/1405 train_time:20540ms step_avg:136.03ms
step:162/1405 train_time:20678ms step_avg:136.04ms
step:163/1405 train_time:20818ms step_avg:136.06ms
step:164/1405 train_time:20956ms step_avg:136.08ms
step:165/1405 train_time:21092ms step_avg:136.08ms
step:166/1405 train_time:21229ms step_avg:136.08ms
step:167/1405 train_time:21367ms step_avg:136.10ms
step:168/1405 train_time:21505ms step_avg:136.11ms
step:169/1405 train_time:21643ms step_avg:136.12ms
step:170/1405 train_time:21782ms step_avg:136.14ms
step:171/1405 train_time:21919ms step_avg:136.14ms
step:172/1405 train_time:22059ms step_avg:136.16ms
step:173/1405 train_time:22195ms step_avg:136.17ms
step:174/1405 train_time:22334ms step_avg:136.18ms
step:175/1405 train_time:22470ms step_avg:136.18ms
step:176/1405 train_time:22608ms step_avg:136.19ms
step:177/1405 train_time:22746ms step_avg:136.20ms
step:178/1405 train_time:22883ms step_avg:136.21ms
step:179/1405 train_time:23023ms step_avg:136.23ms
step:180/1405 train_time:23160ms step_avg:136.23ms
step:181/1405 train_time:23298ms step_avg:136.24ms
step:182/1405 train_time:23436ms step_avg:136.25ms
step:183/1405 train_time:23575ms step_avg:136.27ms
step:184/1405 train_time:23712ms step_avg:136.28ms
step:185/1405 train_time:23850ms step_avg:136.29ms
step:186/1405 train_time:23987ms step_avg:136.29ms
step:187/1405 train_time:24125ms step_avg:136.30ms
step:188/1405 train_time:24262ms step_avg:136.30ms
step:189/1405 train_time:24398ms step_avg:136.30ms
step:190/1405 train_time:24537ms step_avg:136.32ms
step:191/1405 train_time:24713ms step_avg:136.54ms
step:192/1405 train_time:24850ms step_avg:136.54ms
step:193/1405 train_time:24987ms step_avg:136.54ms
step:194/1405 train_time:25124ms step_avg:136.54ms
step:195/1405 train_time:25260ms step_avg:136.54ms
step:196/1405 train_time:25396ms step_avg:136.54ms
step:197/1405 train_time:25533ms step_avg:136.54ms
step:198/1405 train_time:25679ms step_avg:136.59ms
step:199/1405 train_time:25818ms step_avg:136.60ms
step:200/1405 train_time:25955ms step_avg:136.61ms
step:201/1405 train_time:26092ms step_avg:136.61ms
step:202/1405 train_time:26228ms step_avg:136.61ms
step:203/1405 train_time:26366ms step_avg:136.61ms
step:204/1405 train_time:26503ms step_avg:136.61ms
step:205/1405 train_time:26642ms step_avg:136.63ms
step:206/1405 train_time:26783ms step_avg:136.65ms
step:207/1405 train_time:26921ms step_avg:136.65ms
step:208/1405 train_time:27059ms step_avg:136.66ms
step:209/1405 train_time:27195ms step_avg:136.66ms
step:210/1405 train_time:27336ms step_avg:136.68ms
step:211/1405 train_time:27475ms step_avg:136.69ms
step:212/1405 train_time:27615ms step_avg:136.71ms
step:213/1405 train_time:27754ms step_avg:136.72ms
step:214/1405 train_time:27893ms step_avg:136.73ms
step:215/1405 train_time:28035ms step_avg:136.76ms
step:216/1405 train_time:28175ms step_avg:136.77ms
step:217/1405 train_time:28315ms step_avg:136.79ms
step:218/1405 train_time:28453ms step_avg:136.80ms
step:219/1405 train_time:28592ms step_avg:136.80ms
step:220/1405 train_time:28733ms step_avg:136.82ms
step:221/1405 train_time:28873ms step_avg:136.84ms
step:222/1405 train_time:29013ms step_avg:136.85ms
step:223/1405 train_time:29152ms step_avg:136.87ms
step:224/1405 train_time:29292ms step_avg:136.88ms
step:225/1405 train_time:29430ms step_avg:136.88ms
step:226/1405 train_time:29568ms step_avg:136.89ms
step:227/1405 train_time:29707ms step_avg:136.90ms
step:228/1405 train_time:29847ms step_avg:136.91ms
step:229/1405 train_time:29987ms step_avg:136.92ms
step:230/1405 train_time:30126ms step_avg:136.94ms
step:231/1405 train_time:30265ms step_avg:136.95ms
step:232/1405 train_time:30403ms step_avg:136.95ms
step:233/1405 train_time:30541ms step_avg:136.96ms
step:234/1405 train_time:30680ms step_avg:136.97ms
step:235/1405 train_time:30819ms step_avg:136.97ms
step:236/1405 train_time:30958ms step_avg:136.98ms
step:237/1405 train_time:31096ms step_avg:136.99ms
step:238/1405 train_time:31236ms step_avg:137.00ms
step:239/1405 train_time:31373ms step_avg:137.00ms
step:240/1405 train_time:31513ms step_avg:137.02ms
step:241/1405 train_time:31653ms step_avg:137.03ms
step:242/1405 train_time:31792ms step_avg:137.03ms
step:243/1405 train_time:31930ms step_avg:137.04ms
step:244/1405 train_time:32069ms step_avg:137.05ms
step:245/1405 train_time:32206ms step_avg:137.05ms
step:246/1405 train_time:32346ms step_avg:137.06ms
step:247/1405 train_time:32486ms step_avg:137.07ms
step:248/1405 train_time:32626ms step_avg:137.08ms
step:249/1405 train_time:32765ms step_avg:137.09ms
step:250/1405 train_time:32904ms step_avg:137.10ms
step:250/1405 val_loss:3.9692 train_time:32972ms step_avg:137.38ms
step:251/1405 train_time:33046ms step_avg:137.12ms
step:252/1405 train_time:33189ms step_avg:137.14ms
step:253/1405 train_time:33328ms step_avg:137.15ms
step:254/1405 train_time:33468ms step_avg:137.16ms
step:255/1405 train_time:33606ms step_avg:137.17ms
step:256/1405 train_time:33744ms step_avg:137.17ms
step:257/1405 train_time:33883ms step_avg:137.18ms
step:258/1405 train_time:34022ms step_avg:137.19ms
step:259/1405 train_time:34164ms step_avg:137.21ms
step:260/1405 train_time:34305ms step_avg:137.22ms
step:261/1405 train_time:34443ms step_avg:137.22ms
step:262/1405 train_time:34583ms step_avg:137.23ms
step:263/1405 train_time:34721ms step_avg:137.24ms
step:264/1405 train_time:34860ms step_avg:137.24ms
step:265/1405 train_time:34999ms step_avg:137.25ms
step:266/1405 train_time:35137ms step_avg:137.25ms
step:267/1405 train_time:35277ms step_avg:137.26ms
step:268/1405 train_time:35417ms step_avg:137.27ms
step:269/1405 train_time:35557ms step_avg:137.28ms
step:270/1405 train_time:35695ms step_avg:137.29ms
step:271/1405 train_time:35832ms step_avg:137.29ms
step:272/1405 train_time:35972ms step_avg:137.30ms
step:273/1405 train_time:36111ms step_avg:137.30ms
step:274/1405 train_time:36252ms step_avg:137.32ms
step:275/1405 train_time:36392ms step_avg:137.33ms
step:276/1405 train_time:36530ms step_avg:137.33ms
step:277/1405 train_time:36670ms step_avg:137.34ms
step:278/1405 train_time:36809ms step_avg:137.35ms
step:279/1405 train_time:36948ms step_avg:137.35ms
step:280/1405 train_time:37087ms step_avg:137.36ms
step:281/1405 train_time:37227ms step_avg:137.37ms
step:282/1405 train_time:37366ms step_avg:137.38ms
step:283/1405 train_time:37505ms step_avg:137.38ms
step:284/1405 train_time:37644ms step_avg:137.39ms
step:285/1405 train_time:37783ms step_avg:137.39ms
step:286/1405 train_time:37922ms step_avg:137.40ms
step:287/1405 train_time:38062ms step_avg:137.41ms
step:288/1405 train_time:38201ms step_avg:137.41ms
step:289/1405 train_time:38340ms step_avg:137.42ms
step:290/1405 train_time:38479ms step_avg:137.43ms
step:291/1405 train_time:38618ms step_avg:137.43ms
step:292/1405 train_time:38758ms step_avg:137.44ms
step:293/1405 train_time:38898ms step_avg:137.45ms
step:294/1405 train_time:39035ms step_avg:137.45ms
step:295/1405 train_time:39176ms step_avg:137.46ms
step:296/1405 train_time:39315ms step_avg:137.47ms
step:297/1405 train_time:39454ms step_avg:137.47ms
step:298/1405 train_time:39593ms step_avg:137.47ms
step:299/1405 train_time:39732ms step_avg:137.48ms
step:300/1405 train_time:39870ms step_avg:137.48ms
step:301/1405 train_time:40008ms step_avg:137.48ms
step:302/1405 train_time:40147ms step_avg:137.49ms
step:303/1405 train_time:40289ms step_avg:137.50ms
step:304/1405 train_time:40429ms step_avg:137.51ms
step:305/1405 train_time:40568ms step_avg:137.52ms
step:306/1405 train_time:40707ms step_avg:137.52ms
step:307/1405 train_time:40846ms step_avg:137.53ms
step:308/1405 train_time:40985ms step_avg:137.53ms
step:309/1405 train_time:41124ms step_avg:137.54ms
step:310/1405 train_time:41264ms step_avg:137.55ms
step:311/1405 train_time:41404ms step_avg:137.56ms
step:312/1405 train_time:41545ms step_avg:137.57ms
step:313/1405 train_time:41684ms step_avg:137.57ms
step:314/1405 train_time:41825ms step_avg:137.58ms
step:315/1405 train_time:41966ms step_avg:137.59ms
step:316/1405 train_time:42109ms step_avg:137.61ms
step:317/1405 train_time:42250ms step_avg:137.62ms
step:318/1405 train_time:42394ms step_avg:137.64ms
step:319/1405 train_time:42535ms step_avg:137.65ms
step:320/1405 train_time:42678ms step_avg:137.67ms
step:321/1405 train_time:42818ms step_avg:137.68ms
step:322/1405 train_time:42960ms step_avg:137.69ms
step:323/1405 train_time:43102ms step_avg:137.70ms
step:324/1405 train_time:43242ms step_avg:137.71ms
step:325/1405 train_time:43386ms step_avg:137.73ms
step:326/1405 train_time:43527ms step_avg:137.74ms
step:327/1405 train_time:43668ms step_avg:137.76ms
step:328/1405 train_time:43810ms step_avg:137.77ms
step:329/1405 train_time:43954ms step_avg:137.79ms
step:330/1405 train_time:44095ms step_avg:137.80ms
step:331/1405 train_time:44234ms step_avg:137.80ms
step:332/1405 train_time:44377ms step_avg:137.82ms
step:333/1405 train_time:44518ms step_avg:137.83ms
step:334/1405 train_time:44659ms step_avg:137.84ms
step:335/1405 train_time:44800ms step_avg:137.85ms
step:336/1405 train_time:44941ms step_avg:137.86ms
step:337/1405 train_time:45083ms step_avg:137.87ms
step:338/1405 train_time:45224ms step_avg:137.88ms
step:339/1405 train_time:45366ms step_avg:137.89ms
step:340/1405 train_time:45507ms step_avg:137.90ms
step:341/1405 train_time:45649ms step_avg:137.91ms
step:342/1405 train_time:45790ms step_avg:137.92ms
step:343/1405 train_time:45931ms step_avg:137.93ms
step:344/1405 train_time:46074ms step_avg:137.95ms
step:345/1405 train_time:46217ms step_avg:137.96ms
step:346/1405 train_time:46358ms step_avg:137.97ms
step:347/1405 train_time:46498ms step_avg:137.98ms
step:348/1405 train_time:46639ms step_avg:137.98ms
step:349/1405 train_time:46782ms step_avg:138.00ms
step:350/1405 train_time:46924ms step_avg:138.01ms
step:351/1405 train_time:47065ms step_avg:138.02ms
step:352/1405 train_time:47207ms step_avg:138.03ms
step:353/1405 train_time:47349ms step_avg:138.04ms
step:354/1405 train_time:47491ms step_avg:138.05ms
step:355/1405 train_time:47632ms step_avg:138.06ms
step:356/1405 train_time:47776ms step_avg:138.08ms
step:357/1405 train_time:47917ms step_avg:138.09ms
step:358/1405 train_time:48059ms step_avg:138.10ms
step:359/1405 train_time:48201ms step_avg:138.11ms
step:360/1405 train_time:48343ms step_avg:138.12ms
step:361/1405 train_time:48485ms step_avg:138.13ms
step:362/1405 train_time:48626ms step_avg:138.14ms
step:363/1405 train_time:48767ms step_avg:138.15ms
step:364/1405 train_time:48909ms step_avg:138.16ms
step:365/1405 train_time:49050ms step_avg:138.17ms
step:366/1405 train_time:49192ms step_avg:138.18ms
step:367/1405 train_time:49333ms step_avg:138.19ms
step:368/1405 train_time:49475ms step_avg:138.20ms
step:369/1405 train_time:49617ms step_avg:138.21ms
step:370/1405 train_time:49758ms step_avg:138.22ms
step:371/1405 train_time:49898ms step_avg:138.22ms
step:372/1405 train_time:50039ms step_avg:138.23ms
step:373/1405 train_time:50181ms step_avg:138.24ms
step:374/1405 train_time:50323ms step_avg:138.25ms
step:375/1405 train_time:50466ms step_avg:138.26ms
step:375/1405 val_loss:3.7758 train_time:50534ms step_avg:138.45ms
step:376/1405 train_time:50610ms step_avg:138.28ms
step:377/1405 train_time:50752ms step_avg:138.29ms
step:378/1405 train_time:50893ms step_avg:138.30ms
step:379/1405 train_time:51033ms step_avg:138.30ms
step:380/1405 train_time:51173ms step_avg:138.31ms
step:381/1405 train_time:51351ms step_avg:138.41ms
step:382/1405 train_time:51491ms step_avg:138.42ms
step:383/1405 train_time:51632ms step_avg:138.42ms
step:384/1405 train_time:51772ms step_avg:138.43ms
step:385/1405 train_time:51912ms step_avg:138.43ms
step:386/1405 train_time:52051ms step_avg:138.43ms
step:387/1405 train_time:52195ms step_avg:138.45ms
step:388/1405 train_time:52340ms step_avg:138.47ms
step:389/1405 train_time:52482ms step_avg:138.48ms
step:390/1405 train_time:52624ms step_avg:138.48ms
step:391/1405 train_time:52766ms step_avg:138.49ms
step:392/1405 train_time:52907ms step_avg:138.50ms
step:393/1405 train_time:53047ms step_avg:138.51ms
step:394/1405 train_time:53190ms step_avg:138.52ms
step:395/1405 train_time:53334ms step_avg:138.53ms
step:396/1405 train_time:53477ms step_avg:138.54ms
step:397/1405 train_time:53620ms step_avg:138.55ms
step:398/1405 train_time:53760ms step_avg:138.56ms
step:399/1405 train_time:53902ms step_avg:138.56ms
step:400/1405 train_time:54043ms step_avg:138.57ms
step:401/1405 train_time:54185ms step_avg:138.58ms
step:402/1405 train_time:54327ms step_avg:138.59ms
step:403/1405 train_time:54471ms step_avg:138.60ms
step:404/1405 train_time:54612ms step_avg:138.61ms
step:405/1405 train_time:54754ms step_avg:138.62ms
step:406/1405 train_time:54895ms step_avg:138.62ms
step:407/1405 train_time:55037ms step_avg:138.63ms
step:408/1405 train_time:55179ms step_avg:138.64ms
step:409/1405 train_time:55321ms step_avg:138.65ms
step:410/1405 train_time:55462ms step_avg:138.65ms
step:411/1405 train_time:55603ms step_avg:138.66ms
step:412/1405 train_time:55745ms step_avg:138.67ms
step:413/1405 train_time:55887ms step_avg:138.68ms
step:414/1405 train_time:56030ms step_avg:138.69ms
step:415/1405 train_time:56171ms step_avg:138.69ms
step:416/1405 train_time:56313ms step_avg:138.70ms
step:417/1405 train_time:56455ms step_avg:138.71ms
step:418/1405 train_time:56599ms step_avg:138.72ms
step:419/1405 train_time:56742ms step_avg:138.73ms
step:420/1405 train_time:56885ms step_avg:138.74ms
step:421/1405 train_time:57026ms step_avg:138.75ms
step:422/1405 train_time:57170ms step_avg:138.76ms
step:423/1405 train_time:57312ms step_avg:138.77ms
step:424/1405 train_time:57453ms step_avg:138.78ms
step:425/1405 train_time:57596ms step_avg:138.79ms
step:426/1405 train_time:57740ms step_avg:138.80ms
step:427/1405 train_time:57882ms step_avg:138.81ms
step:428/1405 train_time:58025ms step_avg:138.81ms
step:429/1405 train_time:58166ms step_avg:138.82ms
step:430/1405 train_time:58309ms step_avg:138.83ms
step:431/1405 train_time:58451ms step_avg:138.84ms
step:432/1405 train_time:58594ms step_avg:138.85ms
step:433/1405 train_time:58736ms step_avg:138.86ms
step:434/1405 train_time:58879ms step_avg:138.87ms
step:435/1405 train_time:59021ms step_avg:138.87ms
step:436/1405 train_time:59162ms step_avg:138.88ms
step:437/1405 train_time:59304ms step_avg:138.89ms
step:438/1405 train_time:59446ms step_avg:138.89ms
step:439/1405 train_time:59590ms step_avg:138.90ms
step:440/1405 train_time:59731ms step_avg:138.91ms
step:441/1405 train_time:59875ms step_avg:138.92ms
step:442/1405 train_time:60017ms step_avg:138.93ms
step:443/1405 train_time:60160ms step_avg:138.94ms
step:444/1405 train_time:60304ms step_avg:138.95ms
step:445/1405 train_time:60448ms step_avg:138.96ms
step:446/1405 train_time:60592ms step_avg:138.97ms
step:447/1405 train_time:60733ms step_avg:138.98ms
step:448/1405 train_time:60876ms step_avg:138.99ms
step:449/1405 train_time:61018ms step_avg:138.99ms
step:450/1405 train_time:61160ms step_avg:139.00ms
step:451/1405 train_time:61303ms step_avg:139.01ms
step:452/1405 train_time:61445ms step_avg:139.02ms
step:453/1405 train_time:61589ms step_avg:139.03ms
step:454/1405 train_time:61732ms step_avg:139.04ms
step:455/1405 train_time:61874ms step_avg:139.04ms
step:456/1405 train_time:62015ms step_avg:139.05ms
step:457/1405 train_time:62158ms step_avg:139.06ms
step:458/1405 train_time:62301ms step_avg:139.07ms
step:459/1405 train_time:62444ms step_avg:139.07ms
step:460/1405 train_time:62587ms step_avg:139.08ms
step:461/1405 train_time:62730ms step_avg:139.09ms
step:462/1405 train_time:62874ms step_avg:139.10ms
step:463/1405 train_time:63016ms step_avg:139.11ms
step:464/1405 train_time:63158ms step_avg:139.11ms
step:465/1405 train_time:63301ms step_avg:139.12ms
step:466/1405 train_time:63444ms step_avg:139.13ms
step:467/1405 train_time:63586ms step_avg:139.14ms
step:468/1405 train_time:63728ms step_avg:139.14ms
step:469/1405 train_time:63869ms step_avg:139.15ms
step:470/1405 train_time:64011ms step_avg:139.15ms
step:471/1405 train_time:64153ms step_avg:139.16ms
step:472/1405 train_time:64295ms step_avg:139.17ms
step:473/1405 train_time:64439ms step_avg:139.18ms
step:474/1405 train_time:64581ms step_avg:139.18ms
step:475/1405 train_time:64723ms step_avg:139.19ms
step:476/1405 train_time:64866ms step_avg:139.20ms
step:477/1405 train_time:65009ms step_avg:139.21ms
step:478/1405 train_time:65150ms step_avg:139.21ms
step:479/1405 train_time:65294ms step_avg:139.22ms
step:480/1405 train_time:65438ms step_avg:139.23ms
step:481/1405 train_time:65582ms step_avg:139.24ms
step:482/1405 train_time:65724ms step_avg:139.25ms
step:483/1405 train_time:65865ms step_avg:139.25ms
step:484/1405 train_time:66009ms step_avg:139.26ms
step:485/1405 train_time:66151ms step_avg:139.27ms
step:486/1405 train_time:66294ms step_avg:139.27ms
step:487/1405 train_time:66437ms step_avg:139.28ms
step:488/1405 train_time:66582ms step_avg:139.29ms
step:489/1405 train_time:66724ms step_avg:139.30ms
step:490/1405 train_time:66867ms step_avg:139.31ms
step:491/1405 train_time:67008ms step_avg:139.31ms
step:492/1405 train_time:67151ms step_avg:139.32ms
step:493/1405 train_time:67294ms step_avg:139.32ms
step:494/1405 train_time:67436ms step_avg:139.33ms
step:495/1405 train_time:67581ms step_avg:139.34ms
step:496/1405 train_time:67725ms step_avg:139.35ms
step:497/1405 train_time:67866ms step_avg:139.35ms
step:498/1405 train_time:68009ms step_avg:139.36ms
step:499/1405 train_time:68151ms step_avg:139.37ms
step:500/1405 train_time:68294ms step_avg:139.38ms
step:500/1405 val_loss:3.6582 train_time:68362ms step_avg:139.51ms
step:501/1405 train_time:68438ms step_avg:139.38ms
step:502/1405 train_time:68581ms step_avg:139.39ms
step:503/1405 train_time:68723ms step_avg:139.40ms
step:504/1405 train_time:68864ms step_avg:139.40ms
step:505/1405 train_time:69007ms step_avg:139.41ms
step:506/1405 train_time:69149ms step_avg:139.41ms
step:507/1405 train_time:69292ms step_avg:139.42ms
step:508/1405 train_time:69437ms step_avg:139.43ms
step:509/1405 train_time:69580ms step_avg:139.44ms
step:510/1405 train_time:69722ms step_avg:139.44ms
step:511/1405 train_time:69863ms step_avg:139.45ms
step:512/1405 train_time:70008ms step_avg:139.46ms
step:513/1405 train_time:70151ms step_avg:139.47ms
step:514/1405 train_time:70294ms step_avg:139.47ms
step:515/1405 train_time:70437ms step_avg:139.48ms
step:516/1405 train_time:70579ms step_avg:139.48ms
step:517/1405 train_time:70720ms step_avg:139.49ms
step:518/1405 train_time:70861ms step_avg:139.49ms
step:519/1405 train_time:71005ms step_avg:139.50ms
step:520/1405 train_time:71148ms step_avg:139.51ms
step:521/1405 train_time:71290ms step_avg:139.51ms
step:522/1405 train_time:71435ms step_avg:139.52ms
step:523/1405 train_time:71579ms step_avg:139.53ms
step:524/1405 train_time:71722ms step_avg:139.54ms
step:525/1405 train_time:71866ms step_avg:139.55ms
step:526/1405 train_time:72013ms step_avg:139.56ms
step:527/1405 train_time:72157ms step_avg:139.57ms
step:528/1405 train_time:72301ms step_avg:139.58ms
step:529/1405 train_time:72447ms step_avg:139.59ms
step:530/1405 train_time:72593ms step_avg:139.60ms
step:531/1405 train_time:72737ms step_avg:139.61ms
step:532/1405 train_time:72882ms step_avg:139.62ms
step:533/1405 train_time:73026ms step_avg:139.63ms
step:534/1405 train_time:73169ms step_avg:139.63ms
step:535/1405 train_time:73314ms step_avg:139.65ms
step:536/1405 train_time:73459ms step_avg:139.66ms
step:537/1405 train_time:73604ms step_avg:139.67ms
step:538/1405 train_time:73749ms step_avg:139.68ms
step:539/1405 train_time:73894ms step_avg:139.69ms
step:540/1405 train_time:74039ms step_avg:139.70ms
step:541/1405 train_time:74182ms step_avg:139.70ms
step:542/1405 train_time:74326ms step_avg:139.71ms
step:543/1405 train_time:74472ms step_avg:139.72ms
step:544/1405 train_time:74617ms step_avg:139.73ms
step:545/1405 train_time:74761ms step_avg:139.74ms
step:546/1405 train_time:74906ms step_avg:139.75ms
step:547/1405 train_time:75052ms step_avg:139.76ms
step:548/1405 train_time:75197ms step_avg:139.77ms
step:549/1405 train_time:75340ms step_avg:139.78ms
step:550/1405 train_time:75485ms step_avg:139.79ms
step:551/1405 train_time:75631ms step_avg:139.80ms
step:552/1405 train_time:75776ms step_avg:139.81ms
step:553/1405 train_time:75921ms step_avg:139.82ms
step:554/1405 train_time:76063ms step_avg:139.82ms
step:555/1405 train_time:76209ms step_avg:139.83ms
step:556/1405 train_time:76353ms step_avg:139.84ms
step:557/1405 train_time:76498ms step_avg:139.85ms
step:558/1405 train_time:76641ms step_avg:139.86ms
step:559/1405 train_time:76787ms step_avg:139.87ms
step:560/1405 train_time:76931ms step_avg:139.87ms
step:561/1405 train_time:77077ms step_avg:139.89ms
step:562/1405 train_time:77221ms step_avg:139.89ms
step:563/1405 train_time:77364ms step_avg:139.90ms
step:564/1405 train_time:77510ms step_avg:139.91ms
step:565/1405 train_time:77656ms step_avg:139.92ms
step:566/1405 train_time:77801ms step_avg:139.93ms
step:567/1405 train_time:77945ms step_avg:139.94ms
step:568/1405 train_time:78092ms step_avg:139.95ms
step:569/1405 train_time:78236ms step_avg:139.96ms
step:570/1405 train_time:78380ms step_avg:139.96ms
step:571/1405 train_time:78562ms step_avg:140.04ms
step:572/1405 train_time:78705ms step_avg:140.05ms
step:573/1405 train_time:78850ms step_avg:140.05ms
step:574/1405 train_time:78995ms step_avg:140.06ms
step:575/1405 train_time:79139ms step_avg:140.07ms
step:576/1405 train_time:79282ms step_avg:140.07ms
step:577/1405 train_time:79426ms step_avg:140.08ms
step:578/1405 train_time:79573ms step_avg:140.09ms
step:579/1405 train_time:79720ms step_avg:140.10ms
step:580/1405 train_time:79863ms step_avg:140.11ms
step:581/1405 train_time:80009ms step_avg:140.12ms
step:582/1405 train_time:80153ms step_avg:140.13ms
step:583/1405 train_time:80297ms step_avg:140.13ms
step:584/1405 train_time:80444ms step_avg:140.15ms
step:585/1405 train_time:80591ms step_avg:140.16ms
step:586/1405 train_time:80737ms step_avg:140.17ms
step:587/1405 train_time:80880ms step_avg:140.17ms
step:588/1405 train_time:81024ms step_avg:140.18ms
step:589/1405 train_time:81167ms step_avg:140.18ms
step:590/1405 train_time:81312ms step_avg:140.19ms
step:591/1405 train_time:81457ms step_avg:140.20ms
step:592/1405 train_time:81605ms step_avg:140.21ms
step:593/1405 train_time:81752ms step_avg:140.23ms
step:594/1405 train_time:81897ms step_avg:140.24ms
step:595/1405 train_time:82041ms step_avg:140.24ms
step:596/1405 train_time:82184ms step_avg:140.25ms
step:597/1405 train_time:82329ms step_avg:140.25ms
step:598/1405 train_time:82475ms step_avg:140.26ms
step:599/1405 train_time:82620ms step_avg:140.27ms
step:600/1405 train_time:82764ms step_avg:140.28ms
step:601/1405 train_time:82909ms step_avg:140.29ms
step:602/1405 train_time:83056ms step_avg:140.30ms
step:603/1405 train_time:83200ms step_avg:140.30ms
step:604/1405 train_time:83343ms step_avg:140.31ms
step:605/1405 train_time:83488ms step_avg:140.32ms
step:606/1405 train_time:83633ms step_avg:140.32ms
step:607/1405 train_time:83778ms step_avg:140.33ms
step:608/1405 train_time:83922ms step_avg:140.34ms
step:609/1405 train_time:84067ms step_avg:140.35ms
step:610/1405 train_time:84213ms step_avg:140.36ms
step:611/1405 train_time:84356ms step_avg:140.36ms
step:612/1405 train_time:84502ms step_avg:140.37ms
step:613/1405 train_time:84648ms step_avg:140.38ms
step:614/1405 train_time:84792ms step_avg:140.38ms
step:615/1405 train_time:84938ms step_avg:140.39ms
step:616/1405 train_time:85082ms step_avg:140.40ms
step:617/1405 train_time:85228ms step_avg:140.41ms
step:618/1405 train_time:85374ms step_avg:140.42ms
step:619/1405 train_time:85519ms step_avg:140.43ms
step:620/1405 train_time:85662ms step_avg:140.43ms
step:621/1405 train_time:85809ms step_avg:140.44ms
step:622/1405 train_time:85954ms step_avg:140.45ms
step:623/1405 train_time:86099ms step_avg:140.46ms
step:624/1405 train_time:86244ms step_avg:140.46ms
step:625/1405 train_time:86389ms step_avg:140.47ms
step:625/1405 val_loss:3.5785 train_time:86460ms step_avg:140.58ms
step:626/1405 train_time:86535ms step_avg:140.48ms
step:627/1405 train_time:86684ms step_avg:140.49ms
step:628/1405 train_time:86828ms step_avg:140.50ms
step:629/1405 train_time:86971ms step_avg:140.50ms
step:630/1405 train_time:87116ms step_avg:140.51ms
step:631/1405 train_time:87261ms step_avg:140.52ms
step:632/1405 train_time:87410ms step_avg:140.53ms
step:633/1405 train_time:87553ms step_avg:140.54ms
step:634/1405 train_time:87700ms step_avg:140.54ms
step:635/1405 train_time:87845ms step_avg:140.55ms
step:636/1405 train_time:87990ms step_avg:140.56ms
step:637/1405 train_time:88135ms step_avg:140.57ms
step:638/1405 train_time:88281ms step_avg:140.57ms
step:639/1405 train_time:88426ms step_avg:140.58ms
step:640/1405 train_time:88571ms step_avg:140.59ms
step:641/1405 train_time:88717ms step_avg:140.60ms
step:642/1405 train_time:88863ms step_avg:140.61ms
step:643/1405 train_time:89009ms step_avg:140.62ms
step:644/1405 train_time:89153ms step_avg:140.62ms
step:645/1405 train_time:89301ms step_avg:140.63ms
step:646/1405 train_time:89446ms step_avg:140.64ms
step:647/1405 train_time:89592ms step_avg:140.65ms
step:648/1405 train_time:89740ms step_avg:140.66ms
step:649/1405 train_time:89886ms step_avg:140.67ms
step:650/1405 train_time:90031ms step_avg:140.67ms
step:651/1405 train_time:90177ms step_avg:140.68ms
step:652/1405 train_time:90322ms step_avg:140.69ms
step:653/1405 train_time:90467ms step_avg:140.70ms
step:654/1405 train_time:90613ms step_avg:140.70ms
step:655/1405 train_time:90759ms step_avg:140.71ms
step:656/1405 train_time:90907ms step_avg:140.72ms
step:657/1405 train_time:91051ms step_avg:140.73ms
step:658/1405 train_time:91198ms step_avg:140.74ms
step:659/1405 train_time:91343ms step_avg:140.74ms
step:660/1405 train_time:91488ms step_avg:140.75ms
step:661/1405 train_time:91632ms step_avg:140.76ms
step:662/1405 train_time:91778ms step_avg:140.76ms
step:663/1405 train_time:91923ms step_avg:140.77ms
step:664/1405 train_time:92070ms step_avg:140.78ms
step:665/1405 train_time:92217ms step_avg:140.79ms
step:666/1405 train_time:92362ms step_avg:140.80ms
step:667/1405 train_time:92509ms step_avg:140.80ms
step:668/1405 train_time:92653ms step_avg:140.81ms
step:669/1405 train_time:92801ms step_avg:140.82ms
step:670/1405 train_time:92946ms step_avg:140.83ms
step:671/1405 train_time:93091ms step_avg:140.83ms
step:672/1405 train_time:93235ms step_avg:140.84ms
step:673/1405 train_time:93381ms step_avg:140.85ms
step:674/1405 train_time:93526ms step_avg:140.85ms
step:675/1405 train_time:93671ms step_avg:140.86ms
step:676/1405 train_time:93817ms step_avg:140.87ms
step:677/1405 train_time:93964ms step_avg:140.87ms
step:678/1405 train_time:94110ms step_avg:140.88ms
step:679/1405 train_time:94255ms step_avg:140.89ms
step:680/1405 train_time:94402ms step_avg:140.90ms
step:681/1405 train_time:94545ms step_avg:140.90ms
step:682/1405 train_time:94690ms step_avg:140.91ms
step:683/1405 train_time:94837ms step_avg:140.92ms
step:684/1405 train_time:94983ms step_avg:140.92ms
step:685/1405 train_time:95129ms step_avg:140.93ms
step:686/1405 train_time:95273ms step_avg:140.94ms
step:687/1405 train_time:95419ms step_avg:140.94ms
step:688/1405 train_time:95565ms step_avg:140.95ms
step:689/1405 train_time:95710ms step_avg:140.96ms
step:690/1405 train_time:95855ms step_avg:140.96ms
step:691/1405 train_time:96002ms step_avg:140.97ms
step:692/1405 train_time:96145ms step_avg:140.98ms
step:693/1405 train_time:96291ms step_avg:140.98ms
step:694/1405 train_time:96439ms step_avg:140.99ms
step:695/1405 train_time:96583ms step_avg:141.00ms
step:696/1405 train_time:96728ms step_avg:141.00ms
step:697/1405 train_time:96871ms step_avg:141.01ms
step:698/1405 train_time:97017ms step_avg:141.01ms
step:699/1405 train_time:97163ms step_avg:141.02ms
step:700/1405 train_time:97309ms step_avg:141.03ms
step:701/1405 train_time:97454ms step_avg:141.03ms
step:702/1405 train_time:97602ms step_avg:141.04ms
step:703/1405 train_time:97747ms step_avg:141.05ms
step:704/1405 train_time:97892ms step_avg:141.05ms
step:705/1405 train_time:98039ms step_avg:141.06ms
step:706/1405 train_time:98186ms step_avg:141.07ms
step:707/1405 train_time:98330ms step_avg:141.08ms
step:708/1405 train_time:98476ms step_avg:141.08ms
step:709/1405 train_time:98623ms step_avg:141.09ms
step:710/1405 train_time:98768ms step_avg:141.10ms
step:711/1405 train_time:98915ms step_avg:141.11ms
step:712/1405 train_time:99060ms step_avg:141.11ms
step:713/1405 train_time:99207ms step_avg:141.12ms
step:714/1405 train_time:99351ms step_avg:141.12ms
step:715/1405 train_time:99498ms step_avg:141.13ms
step:716/1405 train_time:99642ms step_avg:141.14ms
step:717/1405 train_time:99788ms step_avg:141.14ms
step:718/1405 train_time:99933ms step_avg:141.15ms
step:719/1405 train_time:100080ms step_avg:141.16ms
step:720/1405 train_time:100226ms step_avg:141.16ms
step:721/1405 train_time:100370ms step_avg:141.17ms
step:722/1405 train_time:100516ms step_avg:141.17ms
step:723/1405 train_time:100661ms step_avg:141.18ms
step:724/1405 train_time:100806ms step_avg:141.19ms
step:725/1405 train_time:100952ms step_avg:141.19ms
step:726/1405 train_time:101101ms step_avg:141.20ms
step:727/1405 train_time:101246ms step_avg:141.21ms
step:728/1405 train_time:101391ms step_avg:141.21ms
step:729/1405 train_time:101536ms step_avg:141.22ms
step:730/1405 train_time:101684ms step_avg:141.23ms
step:731/1405 train_time:101829ms step_avg:141.23ms
step:732/1405 train_time:101975ms step_avg:141.24ms
step:733/1405 train_time:102124ms step_avg:141.25ms
step:734/1405 train_time:102270ms step_avg:141.26ms
step:735/1405 train_time:102419ms step_avg:141.27ms
step:736/1405 train_time:102568ms step_avg:141.28ms
step:737/1405 train_time:102713ms step_avg:141.28ms
step:738/1405 train_time:102861ms step_avg:141.29ms
step:739/1405 train_time:103009ms step_avg:141.30ms
step:740/1405 train_time:103156ms step_avg:141.31ms
step:741/1405 train_time:103305ms step_avg:141.32ms
step:742/1405 train_time:103450ms step_avg:141.32ms
step:743/1405 train_time:103597ms step_avg:141.33ms
step:744/1405 train_time:103743ms step_avg:141.34ms
step:745/1405 train_time:103892ms step_avg:141.35ms
step:746/1405 train_time:104041ms step_avg:141.36ms
step:747/1405 train_time:104188ms step_avg:141.37ms
step:748/1405 train_time:104335ms step_avg:141.38ms
step:749/1405 train_time:104484ms step_avg:141.39ms
step:750/1405 train_time:104631ms step_avg:141.39ms
step:750/1405 val_loss:3.5234 train_time:104702ms step_avg:141.49ms
step:751/1405 train_time:104779ms step_avg:141.40ms
step:752/1405 train_time:104928ms step_avg:141.41ms
step:753/1405 train_time:105077ms step_avg:141.42ms
step:754/1405 train_time:105223ms step_avg:141.43ms
step:755/1405 train_time:105369ms step_avg:141.44ms
step:756/1405 train_time:105515ms step_avg:141.44ms
step:757/1405 train_time:105666ms step_avg:141.45ms
step:758/1405 train_time:105812ms step_avg:141.46ms
step:759/1405 train_time:105958ms step_avg:141.47ms
step:760/1405 train_time:106106ms step_avg:141.47ms
step:761/1405 train_time:106290ms step_avg:141.53ms
step:762/1405 train_time:106436ms step_avg:141.54ms
step:763/1405 train_time:106583ms step_avg:141.54ms
step:764/1405 train_time:106728ms step_avg:141.55ms
step:765/1405 train_time:106873ms step_avg:141.55ms
step:766/1405 train_time:107021ms step_avg:141.56ms
step:767/1405 train_time:107170ms step_avg:141.57ms
step:768/1405 train_time:107319ms step_avg:141.58ms
step:769/1405 train_time:107467ms step_avg:141.59ms
step:770/1405 train_time:107613ms step_avg:141.60ms
step:771/1405 train_time:107761ms step_avg:141.60ms
step:772/1405 train_time:107910ms step_avg:141.61ms
step:773/1405 train_time:108055ms step_avg:141.62ms
step:774/1405 train_time:108205ms step_avg:141.63ms
step:775/1405 train_time:108352ms step_avg:141.64ms
step:776/1405 train_time:108502ms step_avg:141.65ms
step:777/1405 train_time:108650ms step_avg:141.66ms
step:778/1405 train_time:108795ms step_avg:141.66ms
step:779/1405 train_time:108942ms step_avg:141.67ms
step:780/1405 train_time:109089ms step_avg:141.67ms
step:781/1405 train_time:109236ms step_avg:141.68ms
step:782/1405 train_time:109387ms step_avg:141.69ms
step:783/1405 train_time:109532ms step_avg:141.70ms
step:784/1405 train_time:109680ms step_avg:141.71ms
step:785/1405 train_time:109826ms step_avg:141.71ms
step:786/1405 train_time:109972ms step_avg:141.72ms
step:787/1405 train_time:110119ms step_avg:141.72ms
step:788/1405 train_time:110267ms step_avg:141.73ms
step:789/1405 train_time:110413ms step_avg:141.74ms
step:790/1405 train_time:110563ms step_avg:141.75ms
step:791/1405 train_time:110710ms step_avg:141.75ms
step:792/1405 train_time:110857ms step_avg:141.76ms
step:793/1405 train_time:111004ms step_avg:141.77ms
step:794/1405 train_time:111151ms step_avg:141.77ms
step:795/1405 train_time:111299ms step_avg:141.78ms
step:796/1405 train_time:111446ms step_avg:141.79ms
step:797/1405 train_time:111594ms step_avg:141.80ms
step:798/1405 train_time:111743ms step_avg:141.81ms
step:799/1405 train_time:111891ms step_avg:141.81ms
step:800/1405 train_time:112038ms step_avg:141.82ms
step:801/1405 train_time:112186ms step_avg:141.83ms
step:802/1405 train_time:112332ms step_avg:141.83ms
step:803/1405 train_time:112479ms step_avg:141.84ms
step:804/1405 train_time:112626ms step_avg:141.85ms
step:805/1405 train_time:112774ms step_avg:141.85ms
step:806/1405 train_time:112924ms step_avg:141.86ms
step:807/1405 train_time:113071ms step_avg:141.87ms
step:808/1405 train_time:113218ms step_avg:141.88ms
step:809/1405 train_time:113365ms step_avg:141.88ms
step:810/1405 train_time:113511ms step_avg:141.89ms
step:811/1405 train_time:113660ms step_avg:141.90ms
step:812/1405 train_time:113807ms step_avg:141.90ms
step:813/1405 train_time:113953ms step_avg:141.91ms
step:814/1405 train_time:114099ms step_avg:141.91ms
step:815/1405 train_time:114247ms step_avg:141.92ms
step:816/1405 train_time:114395ms step_avg:141.93ms
step:817/1405 train_time:114543ms step_avg:141.94ms
step:818/1405 train_time:114690ms step_avg:141.94ms
step:819/1405 train_time:114837ms step_avg:141.95ms
step:820/1405 train_time:114987ms step_avg:141.96ms
step:821/1405 train_time:115133ms step_avg:141.96ms
step:822/1405 train_time:115282ms step_avg:141.97ms
step:823/1405 train_time:115429ms step_avg:141.98ms
step:824/1405 train_time:115577ms step_avg:141.99ms
step:825/1405 train_time:115724ms step_avg:141.99ms
step:826/1405 train_time:115871ms step_avg:142.00ms
step:827/1405 train_time:116016ms step_avg:142.00ms
step:828/1405 train_time:116166ms step_avg:142.01ms
step:829/1405 train_time:116314ms step_avg:142.02ms
step:830/1405 train_time:116464ms step_avg:142.03ms
step:831/1405 train_time:116610ms step_avg:142.03ms
step:832/1405 train_time:116756ms step_avg:142.04ms
step:833/1405 train_time:116904ms step_avg:142.05ms
step:834/1405 train_time:117051ms step_avg:142.05ms
step:835/1405 train_time:117203ms step_avg:142.06ms
step:836/1405 train_time:117352ms step_avg:142.07ms
step:837/1405 train_time:117500ms step_avg:142.08ms
step:838/1405 train_time:117648ms step_avg:142.09ms
step:839/1405 train_time:117795ms step_avg:142.09ms
step:840/1405 train_time:117942ms step_avg:142.10ms
step:841/1405 train_time:118090ms step_avg:142.11ms
step:842/1405 train_time:118237ms step_avg:142.11ms
step:843/1405 train_time:118387ms step_avg:142.12ms
step:844/1405 train_time:118533ms step_avg:142.13ms
step:845/1405 train_time:118681ms step_avg:142.13ms
step:846/1405 train_time:118830ms step_avg:142.14ms
step:847/1405 train_time:118979ms step_avg:142.15ms
step:848/1405 train_time:119126ms step_avg:142.16ms
step:849/1405 train_time:119273ms step_avg:142.16ms
step:850/1405 train_time:119424ms step_avg:142.17ms
step:851/1405 train_time:119574ms step_avg:142.18ms
step:852/1405 train_time:119723ms step_avg:142.19ms
step:853/1405 train_time:119869ms step_avg:142.19ms
step:854/1405 train_time:120017ms step_avg:142.20ms
step:855/1405 train_time:120166ms step_avg:142.21ms
step:856/1405 train_time:120312ms step_avg:142.21ms
step:857/1405 train_time:120462ms step_avg:142.22ms
step:858/1405 train_time:120612ms step_avg:142.23ms
step:859/1405 train_time:120762ms step_avg:142.24ms
step:860/1405 train_time:120909ms step_avg:142.25ms
step:861/1405 train_time:121057ms step_avg:142.25ms
step:862/1405 train_time:121207ms step_avg:142.26ms
step:863/1405 train_time:121353ms step_avg:142.27ms
step:864/1405 train_time:121503ms step_avg:142.28ms
step:865/1405 train_time:121650ms step_avg:142.28ms
step:866/1405 train_time:121800ms step_avg:142.29ms
step:867/1405 train_time:121948ms step_avg:142.30ms
step:868/1405 train_time:122095ms step_avg:142.30ms
step:869/1405 train_time:122244ms step_avg:142.31ms
step:870/1405 train_time:122392ms step_avg:142.32ms
step:871/1405 train_time:122542ms step_avg:142.32ms
step:872/1405 train_time:122689ms step_avg:142.33ms
step:873/1405 train_time:122837ms step_avg:142.34ms
step:874/1405 train_time:122985ms step_avg:142.34ms
step:875/1405 train_time:123130ms step_avg:142.35ms
step:875/1405 val_loss:3.4758 train_time:123203ms step_avg:142.43ms
step:876/1405 train_time:123280ms step_avg:142.36ms
step:877/1405 train_time:123429ms step_avg:142.36ms
step:878/1405 train_time:123577ms step_avg:142.37ms
step:879/1405 train_time:123724ms step_avg:142.38ms
step:880/1405 train_time:123873ms step_avg:142.38ms
step:881/1405 train_time:124019ms step_avg:142.39ms
step:882/1405 train_time:124170ms step_avg:142.40ms
step:883/1405 train_time:124318ms step_avg:142.40ms
step:884/1405 train_time:124465ms step_avg:142.41ms
step:885/1405 train_time:124614ms step_avg:142.42ms
step:886/1405 train_time:124762ms step_avg:142.42ms
step:887/1405 train_time:124908ms step_avg:142.43ms
step:888/1405 train_time:125057ms step_avg:142.43ms
step:889/1405 train_time:125202ms step_avg:142.44ms
step:890/1405 train_time:125353ms step_avg:142.45ms
step:891/1405 train_time:125500ms step_avg:142.45ms
step:892/1405 train_time:125647ms step_avg:142.46ms
step:893/1405 train_time:125795ms step_avg:142.46ms
step:894/1405 train_time:125943ms step_avg:142.47ms
step:895/1405 train_time:126093ms step_avg:142.48ms
step:896/1405 train_time:126240ms step_avg:142.48ms
step:897/1405 train_time:126388ms step_avg:142.49ms
step:898/1405 train_time:126537ms step_avg:142.50ms
step:899/1405 train_time:126683ms step_avg:142.50ms
step:900/1405 train_time:126830ms step_avg:142.51ms
step:901/1405 train_time:126979ms step_avg:142.51ms
step:902/1405 train_time:127125ms step_avg:142.52ms
step:903/1405 train_time:127273ms step_avg:142.52ms
step:904/1405 train_time:127422ms step_avg:142.53ms
step:905/1405 train_time:127571ms step_avg:142.54ms
step:906/1405 train_time:127719ms step_avg:142.54ms
step:907/1405 train_time:127871ms step_avg:142.55ms
step:908/1405 train_time:128019ms step_avg:142.56ms
step:909/1405 train_time:128166ms step_avg:142.57ms
step:910/1405 train_time:128319ms step_avg:142.58ms
step:911/1405 train_time:128466ms step_avg:142.58ms
step:912/1405 train_time:128615ms step_avg:142.59ms
step:913/1405 train_time:128764ms step_avg:142.60ms
step:914/1405 train_time:128913ms step_avg:142.60ms
step:915/1405 train_time:129060ms step_avg:142.61ms
step:916/1405 train_time:129206ms step_avg:142.61ms
step:917/1405 train_time:129356ms step_avg:142.62ms
step:918/1405 train_time:129505ms step_avg:142.63ms
step:919/1405 train_time:129655ms step_avg:142.64ms
step:920/1405 train_time:129802ms step_avg:142.64ms
step:921/1405 train_time:129952ms step_avg:142.65ms
step:922/1405 train_time:130101ms step_avg:142.65ms
step:923/1405 train_time:130250ms step_avg:142.66ms
step:924/1405 train_time:130398ms step_avg:142.67ms
step:925/1405 train_time:130546ms step_avg:142.67ms
step:926/1405 train_time:130695ms step_avg:142.68ms
step:927/1405 train_time:130842ms step_avg:142.68ms
step:928/1405 train_time:130991ms step_avg:142.69ms
step:929/1405 train_time:131139ms step_avg:142.70ms
step:930/1405 train_time:131287ms step_avg:142.70ms
step:931/1405 train_time:131436ms step_avg:142.71ms
step:932/1405 train_time:131584ms step_avg:142.72ms
step:933/1405 train_time:131732ms step_avg:142.72ms
step:934/1405 train_time:131880ms step_avg:142.73ms
step:935/1405 train_time:132027ms step_avg:142.73ms
step:936/1405 train_time:132177ms step_avg:142.74ms
step:937/1405 train_time:132325ms step_avg:142.75ms
step:938/1405 train_time:132476ms step_avg:142.75ms
step:939/1405 train_time:132627ms step_avg:142.76ms
step:940/1405 train_time:132778ms step_avg:142.77ms
step:941/1405 train_time:132924ms step_avg:142.78ms
step:942/1405 train_time:133075ms step_avg:142.78ms
step:943/1405 train_time:133225ms step_avg:142.79ms
step:944/1405 train_time:133379ms step_avg:142.80ms
step:945/1405 train_time:133528ms step_avg:142.81ms
step:946/1405 train_time:133679ms step_avg:142.82ms
step:947/1405 train_time:133829ms step_avg:142.83ms
step:948/1405 train_time:133979ms step_avg:142.83ms
step:949/1405 train_time:134128ms step_avg:142.84ms
step:950/1405 train_time:134279ms step_avg:142.85ms
step:951/1405 train_time:134466ms step_avg:142.90ms
step:952/1405 train_time:134612ms step_avg:142.90ms
step:953/1405 train_time:134762ms step_avg:142.91ms
step:954/1405 train_time:134910ms step_avg:142.91ms
step:955/1405 train_time:135058ms step_avg:142.92ms
step:956/1405 train_time:135206ms step_avg:142.92ms
step:957/1405 train_time:135358ms step_avg:142.93ms
step:958/1405 train_time:135510ms step_avg:142.94ms
step:959/1405 train_time:135661ms step_avg:142.95ms
step:960/1405 train_time:135812ms step_avg:142.96ms
step:961/1405 train_time:135960ms step_avg:142.97ms
step:962/1405 train_time:136107ms step_avg:142.97ms
step:963/1405 train_time:136260ms step_avg:142.98ms
step:964/1405 train_time:136409ms step_avg:142.99ms
step:965/1405 train_time:136560ms step_avg:142.99ms
step:966/1405 train_time:136708ms step_avg:143.00ms
step:967/1405 train_time:136858ms step_avg:143.01ms
step:968/1405 train_time:137004ms step_avg:143.01ms
step:969/1405 train_time:137155ms step_avg:143.02ms
step:970/1405 train_time:137302ms step_avg:143.02ms
step:971/1405 train_time:137454ms step_avg:143.03ms
step:972/1405 train_time:137603ms step_avg:143.04ms
step:973/1405 train_time:137753ms step_avg:143.05ms
step:974/1405 train_time:137902ms step_avg:143.05ms
step:975/1405 train_time:138054ms step_avg:143.06ms
step:976/1405 train_time:138202ms step_avg:143.07ms
step:977/1405 train_time:138353ms step_avg:143.07ms
step:978/1405 train_time:138501ms step_avg:143.08ms
step:979/1405 train_time:138651ms step_avg:143.09ms
step:980/1405 train_time:138799ms step_avg:143.09ms
step:981/1405 train_time:138946ms step_avg:143.10ms
step:982/1405 train_time:139098ms step_avg:143.10ms
step:983/1405 train_time:139246ms step_avg:143.11ms
step:984/1405 train_time:139397ms step_avg:143.12ms
step:985/1405 train_time:139545ms step_avg:143.12ms
step:986/1405 train_time:139697ms step_avg:143.13ms
step:987/1405 train_time:139844ms step_avg:143.14ms
step:988/1405 train_time:139994ms step_avg:143.14ms
step:989/1405 train_time:140142ms step_avg:143.15ms
step:990/1405 train_time:140294ms step_avg:143.16ms
step:991/1405 train_time:140443ms step_avg:143.16ms
step:992/1405 train_time:140594ms step_avg:143.17ms
step:993/1405 train_time:140748ms step_avg:143.18ms
step:994/1405 train_time:140898ms step_avg:143.19ms
step:995/1405 train_time:141046ms step_avg:143.19ms
step:996/1405 train_time:141196ms step_avg:143.20ms
step:997/1405 train_time:141344ms step_avg:143.21ms
step:998/1405 train_time:141494ms step_avg:143.21ms
step:999/1405 train_time:141642ms step_avg:143.22ms
step:1000/1405 train_time:141794ms step_avg:143.23ms
step:1000/1405 val_loss:3.4109 train_time:141867ms step_avg:143.30ms
step:1001/1405 train_time:141943ms step_avg:143.23ms
step:1002/1405 train_time:142095ms step_avg:143.24ms
step:1003/1405 train_time:142244ms step_avg:143.25ms
step:1004/1405 train_time:142396ms step_avg:143.26ms
step:1005/1405 train_time:142544ms step_avg:143.26ms
step:1006/1405 train_time:142693ms step_avg:143.27ms
step:1007/1405 train_time:142843ms step_avg:143.27ms
step:1008/1405 train_time:142994ms step_avg:143.28ms
step:1009/1405 train_time:143149ms step_avg:143.29ms
step:1010/1405 train_time:143299ms step_avg:143.30ms
step:1011/1405 train_time:143447ms step_avg:143.30ms
step:1012/1405 train_time:143596ms step_avg:143.31ms
step:1013/1405 train_time:143745ms step_avg:143.32ms
step:1014/1405 train_time:143896ms step_avg:143.32ms
step:1015/1405 train_time:144044ms step_avg:143.33ms
step:1016/1405 train_time:144194ms step_avg:143.33ms
step:1017/1405 train_time:144344ms step_avg:143.34ms
step:1018/1405 train_time:144493ms step_avg:143.35ms
step:1019/1405 train_time:144643ms step_avg:143.35ms
step:1020/1405 train_time:144794ms step_avg:143.36ms
step:1021/1405 train_time:144943ms step_avg:143.37ms
step:1022/1405 train_time:145092ms step_avg:143.37ms
step:1023/1405 train_time:145241ms step_avg:143.38ms
step:1024/1405 train_time:145389ms step_avg:143.38ms
step:1025/1405 train_time:145540ms step_avg:143.39ms
step:1026/1405 train_time:145688ms step_avg:143.39ms
step:1027/1405 train_time:145838ms step_avg:143.40ms
step:1028/1405 train_time:145987ms step_avg:143.41ms
step:1029/1405 train_time:146139ms step_avg:143.41ms
step:1030/1405 train_time:146289ms step_avg:143.42ms
step:1031/1405 train_time:146437ms step_avg:143.42ms
step:1032/1405 train_time:146585ms step_avg:143.43ms
step:1033/1405 train_time:146736ms step_avg:143.44ms
step:1034/1405 train_time:146885ms step_avg:143.44ms
step:1035/1405 train_time:147037ms step_avg:143.45ms
step:1036/1405 train_time:147187ms step_avg:143.46ms
step:1037/1405 train_time:147338ms step_avg:143.46ms
step:1038/1405 train_time:147487ms step_avg:143.47ms
step:1039/1405 train_time:147638ms step_avg:143.48ms
step:1040/1405 train_time:147786ms step_avg:143.48ms
step:1041/1405 train_time:147937ms step_avg:143.49ms
step:1042/1405 train_time:148084ms step_avg:143.49ms
step:1043/1405 train_time:148234ms step_avg:143.50ms
step:1044/1405 train_time:148385ms step_avg:143.51ms
step:1045/1405 train_time:148535ms step_avg:143.51ms
step:1046/1405 train_time:148684ms step_avg:143.52ms
step:1047/1405 train_time:148833ms step_avg:143.52ms
step:1048/1405 train_time:148984ms step_avg:143.53ms
step:1049/1405 train_time:149136ms step_avg:143.54ms
step:1050/1405 train_time:149287ms step_avg:143.55ms
step:1051/1405 train_time:149440ms step_avg:143.55ms
step:1052/1405 train_time:149589ms step_avg:143.56ms
step:1053/1405 train_time:149740ms step_avg:143.57ms
step:1054/1405 train_time:149889ms step_avg:143.57ms
step:1055/1405 train_time:150040ms step_avg:143.58ms
step:1056/1405 train_time:150189ms step_avg:143.58ms
step:1057/1405 train_time:150341ms step_avg:143.59ms
step:1058/1405 train_time:150494ms step_avg:143.60ms
step:1059/1405 train_time:150643ms step_avg:143.61ms
step:1060/1405 train_time:150793ms step_avg:143.61ms
step:1061/1405 train_time:150942ms step_avg:143.62ms
step:1062/1405 train_time:151093ms step_avg:143.62ms
step:1063/1405 train_time:151244ms step_avg:143.63ms
step:1064/1405 train_time:151395ms step_avg:143.64ms
step:1065/1405 train_time:151544ms step_avg:143.64ms
step:1066/1405 train_time:151698ms step_avg:143.65ms
step:1067/1405 train_time:151848ms step_avg:143.66ms
step:1068/1405 train_time:151999ms step_avg:143.67ms
step:1069/1405 train_time:152152ms step_avg:143.68ms
step:1070/1405 train_time:152301ms step_avg:143.68ms
step:1071/1405 train_time:152451ms step_avg:143.69ms
step:1072/1405 train_time:152601ms step_avg:143.69ms
step:1073/1405 train_time:152748ms step_avg:143.69ms
step:1074/1405 train_time:152899ms step_avg:143.70ms
step:1075/1405 train_time:153047ms step_avg:143.71ms
step:1076/1405 train_time:153198ms step_avg:143.71ms
step:1077/1405 train_time:153345ms step_avg:143.72ms
step:1078/1405 train_time:153500ms step_avg:143.73ms
step:1079/1405 train_time:153649ms step_avg:143.73ms
step:1080/1405 train_time:153801ms step_avg:143.74ms
step:1081/1405 train_time:153950ms step_avg:143.74ms
step:1082/1405 train_time:154101ms step_avg:143.75ms
step:1083/1405 train_time:154248ms step_avg:143.75ms
step:1084/1405 train_time:154401ms step_avg:143.76ms
step:1085/1405 train_time:154549ms step_avg:143.77ms
step:1086/1405 train_time:154701ms step_avg:143.77ms
step:1087/1405 train_time:154852ms step_avg:143.78ms
step:1088/1405 train_time:155004ms step_avg:143.79ms
step:1089/1405 train_time:155155ms step_avg:143.79ms
step:1090/1405 train_time:155308ms step_avg:143.80ms
step:1091/1405 train_time:155460ms step_avg:143.81ms
step:1092/1405 train_time:155608ms step_avg:143.82ms
step:1093/1405 train_time:155760ms step_avg:143.82ms
step:1094/1405 train_time:155908ms step_avg:143.83ms
step:1095/1405 train_time:156060ms step_avg:143.83ms
step:1096/1405 train_time:156211ms step_avg:143.84ms
step:1097/1405 train_time:156360ms step_avg:143.85ms
step:1098/1405 train_time:156510ms step_avg:143.85ms
step:1099/1405 train_time:156661ms step_avg:143.86ms
step:1100/1405 train_time:156812ms step_avg:143.86ms
step:1101/1405 train_time:156962ms step_avg:143.87ms
step:1102/1405 train_time:157112ms step_avg:143.88ms
step:1103/1405 train_time:157262ms step_avg:143.88ms
step:1104/1405 train_time:157413ms step_avg:143.89ms
step:1105/1405 train_time:157561ms step_avg:143.89ms
step:1106/1405 train_time:157711ms step_avg:143.90ms
step:1107/1405 train_time:157861ms step_avg:143.90ms
step:1108/1405 train_time:158010ms step_avg:143.91ms
step:1109/1405 train_time:158160ms step_avg:143.91ms
step:1110/1405 train_time:158309ms step_avg:143.92ms
step:1111/1405 train_time:158460ms step_avg:143.92ms
step:1112/1405 train_time:158610ms step_avg:143.93ms
step:1113/1405 train_time:158759ms step_avg:143.93ms
step:1114/1405 train_time:158910ms step_avg:143.94ms
step:1115/1405 train_time:159061ms step_avg:143.95ms
step:1116/1405 train_time:159210ms step_avg:143.95ms
step:1117/1405 train_time:159362ms step_avg:143.96ms
step:1118/1405 train_time:159514ms step_avg:143.97ms
step:1119/1405 train_time:159663ms step_avg:143.97ms
step:1120/1405 train_time:159813ms step_avg:143.98ms
step:1121/1405 train_time:159964ms step_avg:143.98ms
step:1122/1405 train_time:160115ms step_avg:143.99ms
step:1123/1405 train_time:160264ms step_avg:143.99ms
step:1124/1405 train_time:160417ms step_avg:144.00ms
step:1125/1405 train_time:160568ms step_avg:144.01ms
step:1125/1405 val_loss:3.3581 train_time:160643ms step_avg:144.07ms
step:1126/1405 train_time:160719ms step_avg:144.01ms
step:1127/1405 train_time:160872ms step_avg:144.02ms
step:1128/1405 train_time:161021ms step_avg:144.03ms
step:1129/1405 train_time:161173ms step_avg:144.03ms
step:1130/1405 train_time:161321ms step_avg:144.04ms
step:1131/1405 train_time:161471ms step_avg:144.04ms
step:1132/1405 train_time:161621ms step_avg:144.05ms
step:1133/1405 train_time:161771ms step_avg:144.05ms
step:1134/1405 train_time:161923ms step_avg:144.06ms
step:1135/1405 train_time:162070ms step_avg:144.06ms
step:1136/1405 train_time:162222ms step_avg:144.07ms
step:1137/1405 train_time:162370ms step_avg:144.07ms
step:1138/1405 train_time:162520ms step_avg:144.08ms
step:1139/1405 train_time:162668ms step_avg:144.08ms
step:1140/1405 train_time:162818ms step_avg:144.09ms
step:1141/1405 train_time:163007ms step_avg:144.13ms
step:1142/1405 train_time:163157ms step_avg:144.13ms
step:1143/1405 train_time:163307ms step_avg:144.14ms
step:1144/1405 train_time:163458ms step_avg:144.14ms
step:1145/1405 train_time:163605ms step_avg:144.15ms
step:1146/1405 train_time:163757ms step_avg:144.15ms
step:1147/1405 train_time:163906ms step_avg:144.16ms
step:1148/1405 train_time:164062ms step_avg:144.17ms
step:1149/1405 train_time:164215ms step_avg:144.17ms
step:1150/1405 train_time:164365ms step_avg:144.18ms
step:1151/1405 train_time:164516ms step_avg:144.19ms
step:1152/1405 train_time:164666ms step_avg:144.19ms
step:1153/1405 train_time:164819ms step_avg:144.20ms
step:1154/1405 train_time:164969ms step_avg:144.20ms
step:1155/1405 train_time:165123ms step_avg:144.21ms
step:1156/1405 train_time:165276ms step_avg:144.22ms
step:1157/1405 train_time:165429ms step_avg:144.23ms
step:1158/1405 train_time:165582ms step_avg:144.24ms
step:1159/1405 train_time:165732ms step_avg:144.24ms
step:1160/1405 train_time:165882ms step_avg:144.25ms
step:1161/1405 train_time:166034ms step_avg:144.25ms
step:1162/1405 train_time:166186ms step_avg:144.26ms
step:1163/1405 train_time:166337ms step_avg:144.26ms
step:1164/1405 train_time:166490ms step_avg:144.27ms
step:1165/1405 train_time:166641ms step_avg:144.28ms
step:1166/1405 train_time:166791ms step_avg:144.28ms
step:1167/1405 train_time:166941ms step_avg:144.29ms
step:1168/1405 train_time:167092ms step_avg:144.29ms
step:1169/1405 train_time:167243ms step_avg:144.30ms
step:1170/1405 train_time:167394ms step_avg:144.30ms
step:1171/1405 train_time:167547ms step_avg:144.31ms
step:1172/1405 train_time:167701ms step_avg:144.32ms
step:1173/1405 train_time:167852ms step_avg:144.33ms
step:1174/1405 train_time:168006ms step_avg:144.34ms
step:1175/1405 train_time:168160ms step_avg:144.34ms
step:1176/1405 train_time:168312ms step_avg:144.35ms
step:1177/1405 train_time:168467ms step_avg:144.36ms
step:1178/1405 train_time:168620ms step_avg:144.37ms
step:1179/1405 train_time:168771ms step_avg:144.37ms
step:1180/1405 train_time:168929ms step_avg:144.38ms
step:1181/1405 train_time:169081ms step_avg:144.39ms
step:1182/1405 train_time:169229ms step_avg:144.39ms
step:1183/1405 train_time:169381ms step_avg:144.40ms
step:1184/1405 train_time:169530ms step_avg:144.40ms
step:1185/1405 train_time:169685ms step_avg:144.41ms
step:1186/1405 train_time:169836ms step_avg:144.42ms
step:1187/1405 train_time:169992ms step_avg:144.43ms
step:1188/1405 train_time:170142ms step_avg:144.43ms
step:1189/1405 train_time:170294ms step_avg:144.44ms
step:1190/1405 train_time:170445ms step_avg:144.45ms
step:1191/1405 train_time:170600ms step_avg:144.45ms
step:1192/1405 train_time:170749ms step_avg:144.46ms
step:1193/1405 train_time:170902ms step_avg:144.46ms
step:1194/1405 train_time:171054ms step_avg:144.47ms
step:1195/1405 train_time:171205ms step_avg:144.48ms
step:1196/1405 train_time:171359ms step_avg:144.48ms
step:1197/1405 train_time:171510ms step_avg:144.49ms
step:1198/1405 train_time:171664ms step_avg:144.50ms
step:1199/1405 train_time:171814ms step_avg:144.50ms
step:1200/1405 train_time:171966ms step_avg:144.51ms
step:1201/1405 train_time:172117ms step_avg:144.51ms
step:1202/1405 train_time:172275ms step_avg:144.53ms
step:1203/1405 train_time:172427ms step_avg:144.53ms
step:1204/1405 train_time:172580ms step_avg:144.54ms
step:1205/1405 train_time:172730ms step_avg:144.54ms
step:1206/1405 train_time:172882ms step_avg:144.55ms
step:1207/1405 train_time:173032ms step_avg:144.55ms
step:1208/1405 train_time:173184ms step_avg:144.56ms
step:1209/1405 train_time:173335ms step_avg:144.57ms
step:1210/1405 train_time:173489ms step_avg:144.57ms
step:1211/1405 train_time:173641ms step_avg:144.58ms
step:1212/1405 train_time:173792ms step_avg:144.59ms
step:1213/1405 train_time:173943ms step_avg:144.59ms
step:1214/1405 train_time:174095ms step_avg:144.60ms
step:1215/1405 train_time:174245ms step_avg:144.60ms
step:1216/1405 train_time:174396ms step_avg:144.61ms
step:1217/1405 train_time:174545ms step_avg:144.61ms
step:1218/1405 train_time:174698ms step_avg:144.62ms
step:1219/1405 train_time:174848ms step_avg:144.62ms
step:1220/1405 train_time:175001ms step_avg:144.63ms
step:1221/1405 train_time:175152ms step_avg:144.63ms
step:1222/1405 train_time:175302ms step_avg:144.64ms
step:1223/1405 train_time:175455ms step_avg:144.65ms
step:1224/1405 train_time:175609ms step_avg:144.65ms
step:1225/1405 train_time:175761ms step_avg:144.66ms
step:1226/1405 train_time:175911ms step_avg:144.66ms
step:1227/1405 train_time:176065ms step_avg:144.67ms
step:1228/1405 train_time:176214ms step_avg:144.67ms
step:1229/1405 train_time:176364ms step_avg:144.68ms
step:1230/1405 train_time:176516ms step_avg:144.69ms
step:1231/1405 train_time:176668ms step_avg:144.69ms
step:1232/1405 train_time:176821ms step_avg:144.70ms
step:1233/1405 train_time:176971ms step_avg:144.70ms
step:1234/1405 train_time:177122ms step_avg:144.71ms
step:1235/1405 train_time:177273ms step_avg:144.71ms
step:1236/1405 train_time:177424ms step_avg:144.72ms
step:1237/1405 train_time:177575ms step_avg:144.72ms
step:1238/1405 train_time:177730ms step_avg:144.73ms
step:1239/1405 train_time:177883ms step_avg:144.74ms
step:1240/1405 train_time:178034ms step_avg:144.74ms
step:1241/1405 train_time:178188ms step_avg:144.75ms
step:1242/1405 train_time:178340ms step_avg:144.76ms
step:1243/1405 train_time:178492ms step_avg:144.76ms
step:1244/1405 train_time:178642ms step_avg:144.77ms
step:1245/1405 train_time:178793ms step_avg:144.77ms
step:1246/1405 train_time:178944ms step_avg:144.78ms
step:1247/1405 train_time:179095ms step_avg:144.78ms
step:1248/1405 train_time:179245ms step_avg:144.79ms
step:1249/1405 train_time:179396ms step_avg:144.79ms
step:1250/1405 train_time:179547ms step_avg:144.80ms
step:1250/1405 val_loss:3.3110 train_time:179625ms step_avg:144.86ms
step:1251/1405 train_time:179703ms step_avg:144.81ms
step:1252/1405 train_time:179857ms step_avg:144.81ms
step:1253/1405 train_time:180008ms step_avg:144.82ms
step:1254/1405 train_time:180157ms step_avg:144.82ms
step:1255/1405 train_time:180313ms step_avg:144.83ms
step:1256/1405 train_time:180464ms step_avg:144.83ms
step:1257/1405 train_time:180617ms step_avg:144.84ms
step:1258/1405 train_time:180771ms step_avg:144.85ms
step:1259/1405 train_time:180922ms step_avg:144.85ms
step:1260/1405 train_time:181073ms step_avg:144.86ms
step:1261/1405 train_time:181225ms step_avg:144.86ms
step:1262/1405 train_time:181378ms step_avg:144.87ms
step:1263/1405 train_time:181531ms step_avg:144.88ms
step:1264/1405 train_time:181681ms step_avg:144.88ms
step:1265/1405 train_time:181834ms step_avg:144.89ms
step:1266/1405 train_time:181985ms step_avg:144.89ms
step:1267/1405 train_time:182137ms step_avg:144.90ms
step:1268/1405 train_time:182287ms step_avg:144.90ms
step:1269/1405 train_time:182441ms step_avg:144.91ms
step:1270/1405 train_time:182594ms step_avg:144.92ms
step:1271/1405 train_time:182745ms step_avg:144.92ms
step:1272/1405 train_time:182897ms step_avg:144.93ms
step:1273/1405 train_time:183047ms step_avg:144.93ms
step:1274/1405 train_time:183198ms step_avg:144.94ms
step:1275/1405 train_time:183349ms step_avg:144.94ms
step:1276/1405 train_time:183500ms step_avg:144.95ms
step:1277/1405 train_time:183654ms step_avg:144.95ms
step:1278/1405 train_time:183803ms step_avg:144.96ms
step:1279/1405 train_time:183956ms step_avg:144.96ms
step:1280/1405 train_time:184110ms step_avg:144.97ms
step:1281/1405 train_time:184261ms step_avg:144.97ms
step:1282/1405 train_time:184413ms step_avg:144.98ms
step:1283/1405 train_time:184563ms step_avg:144.98ms
step:1284/1405 train_time:184718ms step_avg:144.99ms
step:1285/1405 train_time:184870ms step_avg:145.00ms
step:1286/1405 train_time:185021ms step_avg:145.00ms
step:1287/1405 train_time:185173ms step_avg:145.01ms
step:1288/1405 train_time:185324ms step_avg:145.01ms
step:1289/1405 train_time:185479ms step_avg:145.02ms
step:1290/1405 train_time:185633ms step_avg:145.03ms
step:1291/1405 train_time:185786ms step_avg:145.03ms
step:1292/1405 train_time:185937ms step_avg:145.04ms
step:1293/1405 train_time:186088ms step_avg:145.04ms
step:1294/1405 train_time:186239ms step_avg:145.05ms
step:1295/1405 train_time:186390ms step_avg:145.05ms
step:1296/1405 train_time:186542ms step_avg:145.06ms
step:1297/1405 train_time:186696ms step_avg:145.06ms
step:1298/1405 train_time:186848ms step_avg:145.07ms
step:1299/1405 train_time:186998ms step_avg:145.07ms
step:1300/1405 train_time:187151ms step_avg:145.08ms
step:1301/1405 train_time:187301ms step_avg:145.08ms
step:1302/1405 train_time:187456ms step_avg:145.09ms
step:1303/1405 train_time:187608ms step_avg:145.10ms
step:1304/1405 train_time:187760ms step_avg:145.10ms
step:1305/1405 train_time:187913ms step_avg:145.11ms
step:1306/1405 train_time:188065ms step_avg:145.11ms
step:1307/1405 train_time:188216ms step_avg:145.12ms
step:1308/1405 train_time:188366ms step_avg:145.12ms
step:1309/1405 train_time:188517ms step_avg:145.12ms
step:1310/1405 train_time:188667ms step_avg:145.13ms
step:1311/1405 train_time:188817ms step_avg:145.13ms
step:1312/1405 train_time:188967ms step_avg:145.14ms
step:1313/1405 train_time:189119ms step_avg:145.14ms
step:1314/1405 train_time:189272ms step_avg:145.15ms
step:1315/1405 train_time:189421ms step_avg:145.15ms
step:1316/1405 train_time:189574ms step_avg:145.16ms
step:1317/1405 train_time:189725ms step_avg:145.16ms
step:1318/1405 train_time:189879ms step_avg:145.17ms
step:1319/1405 train_time:190032ms step_avg:145.17ms
step:1320/1405 train_time:190182ms step_avg:145.18ms
step:1321/1405 train_time:190335ms step_avg:145.18ms
step:1322/1405 train_time:190488ms step_avg:145.19ms
step:1323/1405 train_time:190640ms step_avg:145.19ms
step:1324/1405 train_time:190793ms step_avg:145.20ms
step:1325/1405 train_time:190943ms step_avg:145.20ms
step:1326/1405 train_time:191096ms step_avg:145.21ms
step:1327/1405 train_time:191247ms step_avg:145.21ms
step:1328/1405 train_time:191398ms step_avg:145.22ms
step:1329/1405 train_time:191564ms step_avg:145.23ms
step:1330/1405 train_time:191717ms step_avg:145.24ms
step:1331/1405 train_time:191904ms step_avg:145.27ms
step:1332/1405 train_time:192056ms step_avg:145.28ms
step:1333/1405 train_time:192207ms step_avg:145.28ms
step:1334/1405 train_time:192357ms step_avg:145.28ms
step:1335/1405 train_time:192505ms step_avg:145.29ms
step:1336/1405 train_time:192661ms step_avg:145.30ms
step:1337/1405 train_time:192816ms step_avg:145.30ms
step:1338/1405 train_time:192969ms step_avg:145.31ms
step:1339/1405 train_time:193121ms step_avg:145.31ms
step:1340/1405 train_time:193273ms step_avg:145.32ms
step:1341/1405 train_time:193422ms step_avg:145.32ms
step:1342/1405 train_time:193575ms step_avg:145.33ms
step:1343/1405 train_time:193726ms step_avg:145.33ms
step:1344/1405 train_time:193876ms step_avg:145.33ms
step:1345/1405 train_time:194027ms step_avg:145.34ms
step:1346/1405 train_time:194179ms step_avg:145.34ms
step:1347/1405 train_time:194333ms step_avg:145.35ms
step:1348/1405 train_time:194483ms step_avg:145.35ms
step:1349/1405 train_time:194637ms step_avg:145.36ms
step:1350/1405 train_time:194785ms step_avg:145.36ms
step:1351/1405 train_time:194937ms step_avg:145.37ms
step:1352/1405 train_time:195091ms step_avg:145.37ms
step:1353/1405 train_time:195245ms step_avg:145.38ms
step:1354/1405 train_time:195398ms step_avg:145.39ms
step:1355/1405 train_time:195551ms step_avg:145.39ms
step:1356/1405 train_time:195702ms step_avg:145.40ms
step:1357/1405 train_time:195857ms step_avg:145.40ms
step:1358/1405 train_time:196010ms step_avg:145.41ms
step:1359/1405 train_time:196162ms step_avg:145.41ms
step:1360/1405 train_time:196319ms step_avg:145.42ms
step:1361/1405 train_time:196473ms step_avg:145.43ms
step:1362/1405 train_time:196624ms step_avg:145.43ms
step:1363/1405 train_time:196780ms step_avg:145.44ms
step:1364/1405 train_time:196934ms step_avg:145.45ms
step:1365/1405 train_time:197083ms step_avg:145.45ms
step:1366/1405 train_time:197238ms step_avg:145.46ms
step:1367/1405 train_time:197390ms step_avg:145.46ms
step:1368/1405 train_time:197543ms step_avg:145.47ms
step:1369/1405 train_time:197699ms step_avg:145.47ms
step:1370/1405 train_time:197853ms step_avg:145.48ms
step:1371/1405 train_time:198005ms step_avg:145.48ms
step:1372/1405 train_time:198161ms step_avg:145.49ms
step:1373/1405 train_time:198313ms step_avg:145.50ms
step:1374/1405 train_time:198466ms step_avg:145.50ms
step:1375/1405 train_time:198617ms step_avg:145.51ms
step:1375/1405 val_loss:3.2802 train_time:198692ms step_avg:145.56ms
step:1376/1405 train_time:198769ms step_avg:145.51ms
step:1377/1405 train_time:198924ms step_avg:145.52ms
step:1378/1405 train_time:199075ms step_avg:145.52ms
step:1379/1405 train_time:199227ms step_avg:145.53ms
step:1380/1405 train_time:199379ms step_avg:145.53ms
step:1381/1405 train_time:199533ms step_avg:145.54ms
step:1382/1405 train_time:199687ms step_avg:145.54ms
step:1383/1405 train_time:199843ms step_avg:145.55ms
step:1384/1405 train_time:199998ms step_avg:145.56ms
step:1385/1405 train_time:200148ms step_avg:145.56ms
step:1386/1405 train_time:200303ms step_avg:145.57ms
step:1387/1405 train_time:200459ms step_avg:145.58ms
step:1388/1405 train_time:200610ms step_avg:145.58ms
step:1389/1405 train_time:200765ms step_avg:145.59ms
step:1390/1405 train_time:200920ms step_avg:145.59ms
step:1391/1405 train_time:201072ms step_avg:145.60ms
step:1392/1405 train_time:201224ms step_avg:145.60ms
step:1393/1405 train_time:201379ms step_avg:145.61ms
step:1394/1405 train_time:201530ms step_avg:145.61ms
step:1395/1405 train_time:201684ms step_avg:145.62ms
step:1396/1405 train_time:201835ms step_avg:145.62ms
step:1397/1405 train_time:201986ms step_avg:145.63ms
step:1398/1405 train_time:202138ms step_avg:145.63ms
step:1399/1405 train_time:202290ms step_avg:145.64ms
step:1400/1405 train_time:202448ms step_avg:145.65ms
step:1401/1405 train_time:202602ms step_avg:145.65ms
step:1402/1405 train_time:202753ms step_avg:145.66ms
step:1403/1405 train_time:202908ms step_avg:145.66ms
step:1404/1405 train_time:203061ms step_avg:145.67ms
step:1405/1405 train_time:203214ms step_avg:145.67ms
step:1405/1405 val_loss:3.2776 train_time:203289ms step_avg:145.73ms
peak memory consumption: 31569 MiB

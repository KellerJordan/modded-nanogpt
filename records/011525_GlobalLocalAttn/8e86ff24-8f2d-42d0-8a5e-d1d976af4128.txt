import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: torch.Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            # indices = (~dense_mask).argsort(dim=-1, descending=False, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_masks(sliding_window_num_blocks: int):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            short_sliding_window_num_blocks = sliding_window_num_blocks // 2
            return (
                BlockMask.from_kv_blocks(
                    kv_num_blocks,
                    kv_indices,
                    full_kv_num_blocks,
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                ),
                BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(short_sliding_window_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, short_sliding_window_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                ),
            )

        # Long-short SWA block masks by @leloykun & @YouJiacheng
        long_swa_block_mask, short_swa_block_mask = create_doc_swc_block_masks(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        is_long_block_mask = [True, False, False, False, True, False]
        for i in range(self.num_encoder_layers):
            block_mask = long_swa_block_mask if is_long_block_mask[i] else short_swa_block_mask
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        is_long_block_mask = list(reversed(is_long_block_mask))
        for i in range(self.num_decoder_layers):
            block_mask = long_swa_block_mask if is_long_block_mask[i] else short_swa_block_mask
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1405 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Wed Jan 15 19:07:01 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   32C    P0             117W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   26C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1405 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1405 train_time:28251ms step_avg:nanms
step:2/1405 train_time:28332ms step_avg:nanms
step:3/1405 train_time:28519ms step_avg:nanms
step:4/1405 train_time:28651ms step_avg:nanms
step:5/1405 train_time:28784ms step_avg:nanms
step:6/1405 train_time:28915ms step_avg:nanms
step:7/1405 train_time:29047ms step_avg:nanms
step:8/1405 train_time:29180ms step_avg:nanms
step:9/1405 train_time:29313ms step_avg:nanms
step:10/1405 train_time:29452ms step_avg:nanms
step:11/1405 train_time:135ms step_avg:nanms
step:12/1405 train_time:269ms step_avg:nanms
step:13/1405 train_time:402ms step_avg:134.11ms
step:14/1405 train_time:536ms step_avg:133.96ms
step:15/1405 train_time:669ms step_avg:133.78ms
step:16/1405 train_time:802ms step_avg:133.63ms
step:17/1405 train_time:935ms step_avg:133.58ms
step:18/1405 train_time:1071ms step_avg:133.88ms
step:19/1405 train_time:1204ms step_avg:133.73ms
step:20/1405 train_time:1339ms step_avg:133.86ms
step:21/1405 train_time:1472ms step_avg:133.83ms
step:22/1405 train_time:1605ms step_avg:133.78ms
step:23/1405 train_time:1738ms step_avg:133.69ms
step:24/1405 train_time:1873ms step_avg:133.75ms
step:25/1405 train_time:2007ms step_avg:133.81ms
step:26/1405 train_time:2142ms step_avg:133.85ms
step:27/1405 train_time:2275ms step_avg:133.81ms
step:28/1405 train_time:2410ms step_avg:133.86ms
step:29/1405 train_time:2543ms step_avg:133.85ms
step:30/1405 train_time:2676ms step_avg:133.82ms
step:31/1405 train_time:2811ms step_avg:133.86ms
step:32/1405 train_time:2945ms step_avg:133.89ms
step:33/1405 train_time:3078ms step_avg:133.85ms
step:34/1405 train_time:3213ms step_avg:133.88ms
step:35/1405 train_time:3347ms step_avg:133.88ms
step:36/1405 train_time:3481ms step_avg:133.88ms
step:37/1405 train_time:3615ms step_avg:133.87ms
step:38/1405 train_time:3749ms step_avg:133.91ms
step:39/1405 train_time:3883ms step_avg:133.88ms
step:40/1405 train_time:4016ms step_avg:133.88ms
step:41/1405 train_time:4150ms step_avg:133.88ms
step:42/1405 train_time:4283ms step_avg:133.86ms
step:43/1405 train_time:4417ms step_avg:133.85ms
step:44/1405 train_time:4552ms step_avg:133.87ms
step:45/1405 train_time:4685ms step_avg:133.87ms
step:46/1405 train_time:4820ms step_avg:133.89ms
step:47/1405 train_time:4953ms step_avg:133.87ms
step:48/1405 train_time:5088ms step_avg:133.90ms
step:49/1405 train_time:5222ms step_avg:133.90ms
step:50/1405 train_time:5356ms step_avg:133.89ms
step:51/1405 train_time:5490ms step_avg:133.91ms
step:52/1405 train_time:5623ms step_avg:133.88ms
step:53/1405 train_time:5757ms step_avg:133.88ms
step:54/1405 train_time:5891ms step_avg:133.90ms
step:55/1405 train_time:6026ms step_avg:133.90ms
step:56/1405 train_time:6160ms step_avg:133.91ms
step:57/1405 train_time:6294ms step_avg:133.91ms
step:58/1405 train_time:6427ms step_avg:133.89ms
step:59/1405 train_time:6561ms step_avg:133.89ms
step:60/1405 train_time:6696ms step_avg:133.92ms
step:61/1405 train_time:6830ms step_avg:133.93ms
step:62/1405 train_time:6963ms step_avg:133.90ms
step:63/1405 train_time:7096ms step_avg:133.88ms
step:64/1405 train_time:7231ms step_avg:133.90ms
step:65/1405 train_time:7364ms step_avg:133.90ms
step:66/1405 train_time:7499ms step_avg:133.91ms
step:67/1405 train_time:7633ms step_avg:133.92ms
step:68/1405 train_time:7766ms step_avg:133.90ms
step:69/1405 train_time:7900ms step_avg:133.90ms
step:70/1405 train_time:8034ms step_avg:133.91ms
step:71/1405 train_time:8170ms step_avg:133.93ms
step:72/1405 train_time:8303ms step_avg:133.92ms
step:73/1405 train_time:8439ms step_avg:133.95ms
step:74/1405 train_time:8572ms step_avg:133.94ms
step:75/1405 train_time:8706ms step_avg:133.94ms
step:76/1405 train_time:8840ms step_avg:133.94ms
step:77/1405 train_time:8973ms step_avg:133.93ms
step:78/1405 train_time:9107ms step_avg:133.93ms
step:79/1405 train_time:9241ms step_avg:133.92ms
step:80/1405 train_time:9375ms step_avg:133.94ms
step:81/1405 train_time:9511ms step_avg:133.95ms
step:82/1405 train_time:9644ms step_avg:133.94ms
step:83/1405 train_time:9779ms step_avg:133.96ms
step:84/1405 train_time:9912ms step_avg:133.94ms
step:85/1405 train_time:10045ms step_avg:133.94ms
step:86/1405 train_time:10180ms step_avg:133.95ms
step:87/1405 train_time:10313ms step_avg:133.94ms
step:88/1405 train_time:10447ms step_avg:133.94ms
step:89/1405 train_time:10581ms step_avg:133.93ms
step:90/1405 train_time:10716ms step_avg:133.95ms
step:91/1405 train_time:10850ms step_avg:133.95ms
step:92/1405 train_time:10984ms step_avg:133.96ms
step:93/1405 train_time:11119ms step_avg:133.96ms
step:94/1405 train_time:11253ms step_avg:133.96ms
step:95/1405 train_time:11386ms step_avg:133.95ms
step:96/1405 train_time:11520ms step_avg:133.95ms
step:97/1405 train_time:11654ms step_avg:133.95ms
step:98/1405 train_time:11790ms step_avg:133.97ms
step:99/1405 train_time:11923ms step_avg:133.97ms
step:100/1405 train_time:12057ms step_avg:133.97ms
step:101/1405 train_time:12191ms step_avg:133.97ms
step:102/1405 train_time:12325ms step_avg:133.97ms
step:103/1405 train_time:12459ms step_avg:133.97ms
step:104/1405 train_time:12594ms step_avg:133.98ms
step:105/1405 train_time:12728ms step_avg:133.97ms
step:106/1405 train_time:12862ms step_avg:133.98ms
step:107/1405 train_time:12997ms step_avg:133.99ms
step:108/1405 train_time:13133ms step_avg:134.01ms
step:109/1405 train_time:13268ms step_avg:134.02ms
step:110/1405 train_time:13403ms step_avg:134.03ms
step:111/1405 train_time:13537ms step_avg:134.02ms
step:112/1405 train_time:13672ms step_avg:134.04ms
step:113/1405 train_time:13806ms step_avg:134.04ms
step:114/1405 train_time:13941ms step_avg:134.04ms
step:115/1405 train_time:14075ms step_avg:134.05ms
step:116/1405 train_time:14211ms step_avg:134.07ms
step:117/1405 train_time:14345ms step_avg:134.06ms
step:118/1405 train_time:14481ms step_avg:134.09ms
step:119/1405 train_time:14616ms step_avg:134.09ms
step:120/1405 train_time:14752ms step_avg:134.11ms
step:121/1405 train_time:14887ms step_avg:134.11ms
step:122/1405 train_time:15022ms step_avg:134.12ms
step:123/1405 train_time:15156ms step_avg:134.13ms
step:124/1405 train_time:15292ms step_avg:134.14ms
step:125/1405 train_time:15428ms step_avg:134.15ms
step:125/1405 val_loss:4.4408 train_time:15492ms step_avg:134.72ms
step:126/1405 train_time:15566ms step_avg:134.19ms
step:127/1405 train_time:15703ms step_avg:134.21ms
step:128/1405 train_time:15840ms step_avg:134.24ms
step:129/1405 train_time:15974ms step_avg:134.24ms
step:130/1405 train_time:16110ms step_avg:134.25ms
step:131/1405 train_time:16243ms step_avg:134.24ms
step:132/1405 train_time:16377ms step_avg:134.24ms
step:133/1405 train_time:16514ms step_avg:134.26ms
step:134/1405 train_time:16651ms step_avg:134.28ms
step:135/1405 train_time:16787ms step_avg:134.30ms
step:136/1405 train_time:16923ms step_avg:134.31ms
step:137/1405 train_time:17058ms step_avg:134.32ms
step:138/1405 train_time:17193ms step_avg:134.32ms
step:139/1405 train_time:17327ms step_avg:134.32ms
step:140/1405 train_time:17461ms step_avg:134.32ms
step:141/1405 train_time:17597ms step_avg:134.32ms
step:142/1405 train_time:17733ms step_avg:134.34ms
step:143/1405 train_time:17870ms step_avg:134.36ms
step:144/1405 train_time:18006ms step_avg:134.37ms
step:145/1405 train_time:18140ms step_avg:134.37ms
step:146/1405 train_time:18275ms step_avg:134.38ms
step:147/1405 train_time:18410ms step_avg:134.38ms
step:148/1405 train_time:18545ms step_avg:134.39ms
step:149/1405 train_time:18680ms step_avg:134.39ms
step:150/1405 train_time:18816ms step_avg:134.40ms
step:151/1405 train_time:18953ms step_avg:134.42ms
step:152/1405 train_time:19090ms step_avg:134.44ms
step:153/1405 train_time:19225ms step_avg:134.44ms
step:154/1405 train_time:19360ms step_avg:134.44ms
step:155/1405 train_time:19495ms step_avg:134.45ms
step:156/1405 train_time:19631ms step_avg:134.46ms
step:157/1405 train_time:19766ms step_avg:134.46ms
step:158/1405 train_time:19902ms step_avg:134.47ms
step:159/1405 train_time:20038ms step_avg:134.48ms
step:160/1405 train_time:20174ms step_avg:134.49ms
step:161/1405 train_time:20310ms step_avg:134.50ms
step:162/1405 train_time:20444ms step_avg:134.50ms
step:163/1405 train_time:20579ms step_avg:134.50ms
step:164/1405 train_time:20715ms step_avg:134.51ms
step:165/1405 train_time:20851ms step_avg:134.52ms
step:166/1405 train_time:20987ms step_avg:134.53ms
step:167/1405 train_time:21123ms step_avg:134.54ms
step:168/1405 train_time:21258ms step_avg:134.55ms
step:169/1405 train_time:21394ms step_avg:134.55ms
step:170/1405 train_time:21531ms step_avg:134.57ms
step:171/1405 train_time:21666ms step_avg:134.57ms
step:172/1405 train_time:21801ms step_avg:134.57ms
step:173/1405 train_time:21935ms step_avg:134.57ms
step:174/1405 train_time:22073ms step_avg:134.59ms
step:175/1405 train_time:22208ms step_avg:134.59ms
step:176/1405 train_time:22344ms step_avg:134.60ms
step:177/1405 train_time:22480ms step_avg:134.61ms
step:178/1405 train_time:22615ms step_avg:134.62ms
step:179/1405 train_time:22751ms step_avg:134.62ms
step:180/1405 train_time:22888ms step_avg:134.63ms
step:181/1405 train_time:23023ms step_avg:134.64ms
step:182/1405 train_time:23159ms step_avg:134.64ms
step:183/1405 train_time:23295ms step_avg:134.65ms
step:184/1405 train_time:23431ms step_avg:134.66ms
step:185/1405 train_time:23567ms step_avg:134.67ms
step:186/1405 train_time:23703ms step_avg:134.67ms
step:187/1405 train_time:23839ms step_avg:134.68ms
step:188/1405 train_time:23975ms step_avg:134.69ms
step:189/1405 train_time:24111ms step_avg:134.70ms
step:190/1405 train_time:24248ms step_avg:134.71ms
step:191/1405 train_time:24428ms step_avg:134.96ms
step:192/1405 train_time:24562ms step_avg:134.95ms
step:193/1405 train_time:24697ms step_avg:134.95ms
step:194/1405 train_time:24832ms step_avg:134.96ms
step:195/1405 train_time:24966ms step_avg:134.95ms
step:196/1405 train_time:25100ms step_avg:134.95ms
step:197/1405 train_time:25235ms step_avg:134.95ms
step:198/1405 train_time:25374ms step_avg:134.97ms
step:199/1405 train_time:25511ms step_avg:134.98ms
step:200/1405 train_time:25647ms step_avg:134.98ms
step:201/1405 train_time:25783ms step_avg:134.99ms
step:202/1405 train_time:25919ms step_avg:134.99ms
step:203/1405 train_time:26054ms step_avg:134.99ms
step:204/1405 train_time:26190ms step_avg:135.00ms
step:205/1405 train_time:26326ms step_avg:135.01ms
step:206/1405 train_time:26462ms step_avg:135.01ms
step:207/1405 train_time:26600ms step_avg:135.03ms
step:208/1405 train_time:26736ms step_avg:135.03ms
step:209/1405 train_time:26872ms step_avg:135.04ms
step:210/1405 train_time:27009ms step_avg:135.05ms
step:211/1405 train_time:27145ms step_avg:135.05ms
step:212/1405 train_time:27281ms step_avg:135.06ms
step:213/1405 train_time:27417ms step_avg:135.06ms
step:214/1405 train_time:27554ms step_avg:135.07ms
step:215/1405 train_time:27692ms step_avg:135.08ms
step:216/1405 train_time:27828ms step_avg:135.09ms
step:217/1405 train_time:27963ms step_avg:135.09ms
step:218/1405 train_time:28101ms step_avg:135.10ms
step:219/1405 train_time:28238ms step_avg:135.11ms
step:220/1405 train_time:28375ms step_avg:135.12ms
step:221/1405 train_time:28512ms step_avg:135.13ms
step:222/1405 train_time:28650ms step_avg:135.14ms
step:223/1405 train_time:28786ms step_avg:135.15ms
step:224/1405 train_time:28922ms step_avg:135.15ms
step:225/1405 train_time:29060ms step_avg:135.16ms
step:226/1405 train_time:29196ms step_avg:135.17ms
step:227/1405 train_time:29333ms step_avg:135.17ms
step:228/1405 train_time:29471ms step_avg:135.19ms
step:229/1405 train_time:29608ms step_avg:135.19ms
step:230/1405 train_time:29743ms step_avg:135.20ms
step:231/1405 train_time:29881ms step_avg:135.21ms
step:232/1405 train_time:30017ms step_avg:135.21ms
step:233/1405 train_time:30154ms step_avg:135.22ms
step:234/1405 train_time:30292ms step_avg:135.23ms
step:235/1405 train_time:30428ms step_avg:135.24ms
step:236/1405 train_time:30565ms step_avg:135.24ms
step:237/1405 train_time:30701ms step_avg:135.24ms
step:238/1405 train_time:30837ms step_avg:135.25ms
step:239/1405 train_time:30975ms step_avg:135.26ms
step:240/1405 train_time:31112ms step_avg:135.27ms
step:241/1405 train_time:31249ms step_avg:135.28ms
step:242/1405 train_time:31385ms step_avg:135.28ms
step:243/1405 train_time:31522ms step_avg:135.29ms
step:244/1405 train_time:31659ms step_avg:135.30ms
step:245/1405 train_time:31797ms step_avg:135.31ms
step:246/1405 train_time:31934ms step_avg:135.31ms
step:247/1405 train_time:32071ms step_avg:135.32ms
step:248/1405 train_time:32207ms step_avg:135.33ms
step:249/1405 train_time:32344ms step_avg:135.33ms
step:250/1405 train_time:32481ms step_avg:135.34ms
step:250/1405 val_loss:3.9840 train_time:32546ms step_avg:135.61ms
step:251/1405 train_time:32623ms step_avg:135.37ms
step:252/1405 train_time:32762ms step_avg:135.38ms
step:253/1405 train_time:32898ms step_avg:135.38ms
step:254/1405 train_time:33034ms step_avg:135.39ms
step:255/1405 train_time:33171ms step_avg:135.39ms
step:256/1405 train_time:33306ms step_avg:135.39ms
step:257/1405 train_time:33442ms step_avg:135.39ms
step:258/1405 train_time:33581ms step_avg:135.41ms
step:259/1405 train_time:33720ms step_avg:135.42ms
step:260/1405 train_time:33858ms step_avg:135.43ms
step:261/1405 train_time:33996ms step_avg:135.44ms
step:262/1405 train_time:34132ms step_avg:135.44ms
step:263/1405 train_time:34269ms step_avg:135.45ms
step:264/1405 train_time:34404ms step_avg:135.45ms
step:265/1405 train_time:34541ms step_avg:135.45ms
step:266/1405 train_time:34678ms step_avg:135.46ms
step:267/1405 train_time:34816ms step_avg:135.47ms
step:268/1405 train_time:34953ms step_avg:135.48ms
step:269/1405 train_time:35090ms step_avg:135.48ms
step:270/1405 train_time:35227ms step_avg:135.49ms
step:271/1405 train_time:35363ms step_avg:135.49ms
step:272/1405 train_time:35498ms step_avg:135.49ms
step:273/1405 train_time:35634ms step_avg:135.49ms
step:274/1405 train_time:35772ms step_avg:135.50ms
step:275/1405 train_time:35910ms step_avg:135.51ms
step:276/1405 train_time:36047ms step_avg:135.51ms
step:277/1405 train_time:36183ms step_avg:135.52ms
step:278/1405 train_time:36320ms step_avg:135.52ms
step:279/1405 train_time:36457ms step_avg:135.53ms
step:280/1405 train_time:36593ms step_avg:135.53ms
step:281/1405 train_time:36731ms step_avg:135.54ms
step:282/1405 train_time:36869ms step_avg:135.55ms
step:283/1405 train_time:37005ms step_avg:135.55ms
step:284/1405 train_time:37142ms step_avg:135.55ms
step:285/1405 train_time:37279ms step_avg:135.56ms
step:286/1405 train_time:37416ms step_avg:135.56ms
step:287/1405 train_time:37552ms step_avg:135.57ms
step:288/1405 train_time:37690ms step_avg:135.58ms
step:289/1405 train_time:37826ms step_avg:135.58ms
step:290/1405 train_time:37962ms step_avg:135.58ms
step:291/1405 train_time:38100ms step_avg:135.59ms
step:292/1405 train_time:38237ms step_avg:135.59ms
step:293/1405 train_time:38374ms step_avg:135.60ms
step:294/1405 train_time:38511ms step_avg:135.60ms
step:295/1405 train_time:38648ms step_avg:135.61ms
step:296/1405 train_time:38785ms step_avg:135.61ms
step:297/1405 train_time:38921ms step_avg:135.61ms
step:298/1405 train_time:39058ms step_avg:135.62ms
step:299/1405 train_time:39195ms step_avg:135.62ms
step:300/1405 train_time:39332ms step_avg:135.63ms
step:301/1405 train_time:39469ms step_avg:135.63ms
step:302/1405 train_time:39606ms step_avg:135.64ms
step:303/1405 train_time:39742ms step_avg:135.64ms
step:304/1405 train_time:39880ms step_avg:135.65ms
step:305/1405 train_time:40016ms step_avg:135.65ms
step:306/1405 train_time:40152ms step_avg:135.65ms
step:307/1405 train_time:40290ms step_avg:135.66ms
step:308/1405 train_time:40428ms step_avg:135.66ms
step:309/1405 train_time:40563ms step_avg:135.66ms
step:310/1405 train_time:40700ms step_avg:135.67ms
step:311/1405 train_time:40836ms step_avg:135.67ms
step:312/1405 train_time:40973ms step_avg:135.67ms
step:313/1405 train_time:41110ms step_avg:135.68ms
step:314/1405 train_time:41249ms step_avg:135.69ms
step:315/1405 train_time:41389ms step_avg:135.70ms
step:316/1405 train_time:41529ms step_avg:135.71ms
step:317/1405 train_time:41668ms step_avg:135.73ms
step:318/1405 train_time:41807ms step_avg:135.74ms
step:319/1405 train_time:41947ms step_avg:135.75ms
step:320/1405 train_time:42086ms step_avg:135.76ms
step:321/1405 train_time:42225ms step_avg:135.77ms
step:322/1405 train_time:42365ms step_avg:135.78ms
step:323/1405 train_time:42504ms step_avg:135.80ms
step:324/1405 train_time:42643ms step_avg:135.80ms
step:325/1405 train_time:42783ms step_avg:135.82ms
step:326/1405 train_time:42922ms step_avg:135.83ms
step:327/1405 train_time:43061ms step_avg:135.84ms
step:328/1405 train_time:43201ms step_avg:135.85ms
step:329/1405 train_time:43340ms step_avg:135.86ms
step:330/1405 train_time:43480ms step_avg:135.87ms
step:331/1405 train_time:43619ms step_avg:135.89ms
step:332/1405 train_time:43759ms step_avg:135.90ms
step:333/1405 train_time:43898ms step_avg:135.91ms
step:334/1405 train_time:44037ms step_avg:135.92ms
step:335/1405 train_time:44177ms step_avg:135.93ms
step:336/1405 train_time:44316ms step_avg:135.94ms
step:337/1405 train_time:44455ms step_avg:135.95ms
step:338/1405 train_time:44594ms step_avg:135.96ms
step:339/1405 train_time:44734ms step_avg:135.97ms
step:340/1405 train_time:44874ms step_avg:135.98ms
step:341/1405 train_time:45013ms step_avg:135.99ms
step:342/1405 train_time:45153ms step_avg:136.00ms
step:343/1405 train_time:45293ms step_avg:136.01ms
step:344/1405 train_time:45432ms step_avg:136.02ms
step:345/1405 train_time:45572ms step_avg:136.04ms
step:346/1405 train_time:45712ms step_avg:136.05ms
step:347/1405 train_time:45851ms step_avg:136.06ms
step:348/1405 train_time:45990ms step_avg:136.07ms
step:349/1405 train_time:46129ms step_avg:136.07ms
step:350/1405 train_time:46270ms step_avg:136.09ms
step:351/1405 train_time:46408ms step_avg:136.09ms
step:352/1405 train_time:46548ms step_avg:136.10ms
step:353/1405 train_time:46687ms step_avg:136.11ms
step:354/1405 train_time:46826ms step_avg:136.12ms
step:355/1405 train_time:46965ms step_avg:136.13ms
step:356/1405 train_time:47105ms step_avg:136.14ms
step:357/1405 train_time:47244ms step_avg:136.15ms
step:358/1405 train_time:47383ms step_avg:136.16ms
step:359/1405 train_time:47523ms step_avg:136.17ms
step:360/1405 train_time:47663ms step_avg:136.18ms
step:361/1405 train_time:47802ms step_avg:136.19ms
step:362/1405 train_time:47941ms step_avg:136.20ms
step:363/1405 train_time:48082ms step_avg:136.21ms
step:364/1405 train_time:48222ms step_avg:136.22ms
step:365/1405 train_time:48362ms step_avg:136.23ms
step:366/1405 train_time:48501ms step_avg:136.24ms
step:367/1405 train_time:48641ms step_avg:136.25ms
step:368/1405 train_time:48781ms step_avg:136.26ms
step:369/1405 train_time:48921ms step_avg:136.27ms
step:370/1405 train_time:49061ms step_avg:136.28ms
step:371/1405 train_time:49201ms step_avg:136.29ms
step:372/1405 train_time:49341ms step_avg:136.30ms
step:373/1405 train_time:49481ms step_avg:136.31ms
step:374/1405 train_time:49620ms step_avg:136.32ms
step:375/1405 train_time:49759ms step_avg:136.33ms
step:375/1405 val_loss:3.7814 train_time:49825ms step_avg:136.51ms
step:376/1405 train_time:49901ms step_avg:136.34ms
step:377/1405 train_time:50042ms step_avg:136.35ms
step:378/1405 train_time:50181ms step_avg:136.36ms
step:379/1405 train_time:50319ms step_avg:136.37ms
step:380/1405 train_time:50457ms step_avg:136.37ms
step:381/1405 train_time:50640ms step_avg:136.50ms
step:382/1405 train_time:50778ms step_avg:136.50ms
step:383/1405 train_time:50917ms step_avg:136.51ms
step:384/1405 train_time:51055ms step_avg:136.51ms
step:385/1405 train_time:51194ms step_avg:136.52ms
step:386/1405 train_time:51333ms step_avg:136.52ms
step:387/1405 train_time:51472ms step_avg:136.53ms
step:388/1405 train_time:51614ms step_avg:136.54ms
step:389/1405 train_time:51754ms step_avg:136.55ms
step:390/1405 train_time:51893ms step_avg:136.56ms
step:391/1405 train_time:52033ms step_avg:136.57ms
step:392/1405 train_time:52170ms step_avg:136.57ms
step:393/1405 train_time:52308ms step_avg:136.57ms
step:394/1405 train_time:52446ms step_avg:136.58ms
step:395/1405 train_time:52587ms step_avg:136.59ms
step:396/1405 train_time:52727ms step_avg:136.60ms
step:397/1405 train_time:52868ms step_avg:136.61ms
step:398/1405 train_time:53008ms step_avg:136.62ms
step:399/1405 train_time:53146ms step_avg:136.62ms
step:400/1405 train_time:53285ms step_avg:136.63ms
step:401/1405 train_time:53425ms step_avg:136.64ms
step:402/1405 train_time:53564ms step_avg:136.64ms
step:403/1405 train_time:53703ms step_avg:136.65ms
step:404/1405 train_time:53843ms step_avg:136.66ms
step:405/1405 train_time:53983ms step_avg:136.67ms
step:406/1405 train_time:54121ms step_avg:136.67ms
step:407/1405 train_time:54260ms step_avg:136.68ms
step:408/1405 train_time:54400ms step_avg:136.68ms
step:409/1405 train_time:54538ms step_avg:136.69ms
step:410/1405 train_time:54677ms step_avg:136.69ms
step:411/1405 train_time:54817ms step_avg:136.70ms
step:412/1405 train_time:54957ms step_avg:136.71ms
step:413/1405 train_time:55096ms step_avg:136.72ms
step:414/1405 train_time:55235ms step_avg:136.72ms
step:415/1405 train_time:55374ms step_avg:136.73ms
step:416/1405 train_time:55515ms step_avg:136.74ms
step:417/1405 train_time:55655ms step_avg:136.74ms
step:418/1405 train_time:55795ms step_avg:136.75ms
step:419/1405 train_time:55934ms step_avg:136.76ms
step:420/1405 train_time:56075ms step_avg:136.77ms
step:421/1405 train_time:56215ms step_avg:136.78ms
step:422/1405 train_time:56354ms step_avg:136.78ms
step:423/1405 train_time:56495ms step_avg:136.79ms
step:424/1405 train_time:56635ms step_avg:136.80ms
step:425/1405 train_time:56774ms step_avg:136.81ms
step:426/1405 train_time:56915ms step_avg:136.81ms
step:427/1405 train_time:57055ms step_avg:136.82ms
step:428/1405 train_time:57194ms step_avg:136.83ms
step:429/1405 train_time:57334ms step_avg:136.84ms
step:430/1405 train_time:57475ms step_avg:136.84ms
step:431/1405 train_time:57616ms step_avg:136.85ms
step:432/1405 train_time:57756ms step_avg:136.86ms
step:433/1405 train_time:57896ms step_avg:136.87ms
step:434/1405 train_time:58036ms step_avg:136.88ms
step:435/1405 train_time:58175ms step_avg:136.88ms
step:436/1405 train_time:58315ms step_avg:136.89ms
step:437/1405 train_time:58455ms step_avg:136.90ms
step:438/1405 train_time:58595ms step_avg:136.90ms
step:439/1405 train_time:58735ms step_avg:136.91ms
step:440/1405 train_time:58875ms step_avg:136.92ms
step:441/1405 train_time:59016ms step_avg:136.93ms
step:442/1405 train_time:59156ms step_avg:136.94ms
step:443/1405 train_time:59297ms step_avg:136.94ms
step:444/1405 train_time:59436ms step_avg:136.95ms
step:445/1405 train_time:59577ms step_avg:136.96ms
step:446/1405 train_time:59717ms step_avg:136.97ms
step:447/1405 train_time:59858ms step_avg:136.97ms
step:448/1405 train_time:59998ms step_avg:136.98ms
step:449/1405 train_time:60138ms step_avg:136.99ms
step:450/1405 train_time:60277ms step_avg:136.99ms
step:451/1405 train_time:60418ms step_avg:137.00ms
step:452/1405 train_time:60557ms step_avg:137.01ms
step:453/1405 train_time:60697ms step_avg:137.01ms
step:454/1405 train_time:60838ms step_avg:137.02ms
step:455/1405 train_time:60977ms step_avg:137.03ms
step:456/1405 train_time:61118ms step_avg:137.04ms
step:457/1405 train_time:61258ms step_avg:137.04ms
step:458/1405 train_time:61398ms step_avg:137.05ms
step:459/1405 train_time:61539ms step_avg:137.06ms
step:460/1405 train_time:61678ms step_avg:137.06ms
step:461/1405 train_time:61818ms step_avg:137.07ms
step:462/1405 train_time:61958ms step_avg:137.08ms
step:463/1405 train_time:62098ms step_avg:137.08ms
step:464/1405 train_time:62238ms step_avg:137.09ms
step:465/1405 train_time:62378ms step_avg:137.09ms
step:466/1405 train_time:62518ms step_avg:137.10ms
step:467/1405 train_time:62658ms step_avg:137.11ms
step:468/1405 train_time:62799ms step_avg:137.11ms
step:469/1405 train_time:62939ms step_avg:137.12ms
step:470/1405 train_time:63078ms step_avg:137.13ms
step:471/1405 train_time:63218ms step_avg:137.13ms
step:472/1405 train_time:63359ms step_avg:137.14ms
step:473/1405 train_time:63498ms step_avg:137.15ms
step:474/1405 train_time:63638ms step_avg:137.15ms
step:475/1405 train_time:63778ms step_avg:137.16ms
step:476/1405 train_time:63919ms step_avg:137.17ms
step:477/1405 train_time:64059ms step_avg:137.17ms
step:478/1405 train_time:64199ms step_avg:137.18ms
step:479/1405 train_time:64338ms step_avg:137.18ms
step:480/1405 train_time:64478ms step_avg:137.19ms
step:481/1405 train_time:64618ms step_avg:137.19ms
step:482/1405 train_time:64757ms step_avg:137.20ms
step:483/1405 train_time:64898ms step_avg:137.20ms
step:484/1405 train_time:65037ms step_avg:137.21ms
step:485/1405 train_time:65178ms step_avg:137.22ms
step:486/1405 train_time:65318ms step_avg:137.22ms
step:487/1405 train_time:65459ms step_avg:137.23ms
step:488/1405 train_time:65598ms step_avg:137.23ms
step:489/1405 train_time:65738ms step_avg:137.24ms
step:490/1405 train_time:65878ms step_avg:137.25ms
step:491/1405 train_time:66018ms step_avg:137.25ms
step:492/1405 train_time:66157ms step_avg:137.26ms
step:493/1405 train_time:66297ms step_avg:137.26ms
step:494/1405 train_time:66438ms step_avg:137.27ms
step:495/1405 train_time:66577ms step_avg:137.27ms
step:496/1405 train_time:66718ms step_avg:137.28ms
step:497/1405 train_time:66858ms step_avg:137.28ms
step:498/1405 train_time:66997ms step_avg:137.29ms
step:499/1405 train_time:67137ms step_avg:137.29ms
step:500/1405 train_time:67277ms step_avg:137.30ms
step:500/1405 val_loss:3.6617 train_time:67344ms step_avg:137.44ms
step:501/1405 train_time:67422ms step_avg:137.32ms
step:502/1405 train_time:67564ms step_avg:137.32ms
step:503/1405 train_time:67704ms step_avg:137.33ms
step:504/1405 train_time:67843ms step_avg:137.33ms
step:505/1405 train_time:67982ms step_avg:137.34ms
step:506/1405 train_time:68121ms step_avg:137.34ms
step:507/1405 train_time:68260ms step_avg:137.34ms
step:508/1405 train_time:68402ms step_avg:137.35ms
step:509/1405 train_time:68545ms step_avg:137.36ms
step:510/1405 train_time:68685ms step_avg:137.37ms
step:511/1405 train_time:68825ms step_avg:137.38ms
step:512/1405 train_time:68965ms step_avg:137.38ms
step:513/1405 train_time:69105ms step_avg:137.39ms
step:514/1405 train_time:69245ms step_avg:137.39ms
step:515/1405 train_time:69385ms step_avg:137.40ms
step:516/1405 train_time:69526ms step_avg:137.40ms
step:517/1405 train_time:69668ms step_avg:137.41ms
step:518/1405 train_time:69808ms step_avg:137.42ms
step:519/1405 train_time:69949ms step_avg:137.42ms
step:520/1405 train_time:70088ms step_avg:137.43ms
step:521/1405 train_time:70228ms step_avg:137.43ms
step:522/1405 train_time:70368ms step_avg:137.44ms
step:523/1405 train_time:70509ms step_avg:137.44ms
step:524/1405 train_time:70652ms step_avg:137.45ms
step:525/1405 train_time:70795ms step_avg:137.47ms
step:526/1405 train_time:70938ms step_avg:137.48ms
step:527/1405 train_time:71080ms step_avg:137.48ms
step:528/1405 train_time:71221ms step_avg:137.49ms
step:529/1405 train_time:71364ms step_avg:137.50ms
step:530/1405 train_time:71507ms step_avg:137.51ms
step:531/1405 train_time:71649ms step_avg:137.52ms
step:532/1405 train_time:71792ms step_avg:137.53ms
step:533/1405 train_time:71935ms step_avg:137.54ms
step:534/1405 train_time:72078ms step_avg:137.55ms
step:535/1405 train_time:72220ms step_avg:137.56ms
step:536/1405 train_time:72363ms step_avg:137.57ms
step:537/1405 train_time:72505ms step_avg:137.58ms
step:538/1405 train_time:72647ms step_avg:137.59ms
step:539/1405 train_time:72790ms step_avg:137.60ms
step:540/1405 train_time:72934ms step_avg:137.61ms
step:541/1405 train_time:73076ms step_avg:137.62ms
step:542/1405 train_time:73218ms step_avg:137.63ms
step:543/1405 train_time:73359ms step_avg:137.63ms
step:544/1405 train_time:73500ms step_avg:137.64ms
step:545/1405 train_time:73643ms step_avg:137.65ms
step:546/1405 train_time:73786ms step_avg:137.66ms
step:547/1405 train_time:73928ms step_avg:137.67ms
step:548/1405 train_time:74072ms step_avg:137.68ms
step:549/1405 train_time:74213ms step_avg:137.69ms
step:550/1405 train_time:74355ms step_avg:137.69ms
step:551/1405 train_time:74498ms step_avg:137.70ms
step:552/1405 train_time:74640ms step_avg:137.71ms
step:553/1405 train_time:74783ms step_avg:137.72ms
step:554/1405 train_time:74926ms step_avg:137.73ms
step:555/1405 train_time:75068ms step_avg:137.74ms
step:556/1405 train_time:75210ms step_avg:137.75ms
step:557/1405 train_time:75352ms step_avg:137.76ms
step:558/1405 train_time:75494ms step_avg:137.76ms
step:559/1405 train_time:75636ms step_avg:137.77ms
step:560/1405 train_time:75779ms step_avg:137.78ms
step:561/1405 train_time:75921ms step_avg:137.79ms
step:562/1405 train_time:76063ms step_avg:137.80ms
step:563/1405 train_time:76206ms step_avg:137.80ms
step:564/1405 train_time:76349ms step_avg:137.81ms
step:565/1405 train_time:76491ms step_avg:137.82ms
step:566/1405 train_time:76634ms step_avg:137.83ms
step:567/1405 train_time:76778ms step_avg:137.84ms
step:568/1405 train_time:76920ms step_avg:137.85ms
step:569/1405 train_time:77062ms step_avg:137.86ms
step:570/1405 train_time:77204ms step_avg:137.86ms
step:571/1405 train_time:77391ms step_avg:137.95ms
step:572/1405 train_time:77533ms step_avg:137.96ms
step:573/1405 train_time:77675ms step_avg:137.97ms
step:574/1405 train_time:77817ms step_avg:137.97ms
step:575/1405 train_time:77959ms step_avg:137.98ms
step:576/1405 train_time:78101ms step_avg:137.99ms
step:577/1405 train_time:78243ms step_avg:137.99ms
step:578/1405 train_time:78389ms step_avg:138.01ms
step:579/1405 train_time:78533ms step_avg:138.02ms
step:580/1405 train_time:78675ms step_avg:138.03ms
step:581/1405 train_time:78817ms step_avg:138.03ms
step:582/1405 train_time:78958ms step_avg:138.04ms
step:583/1405 train_time:79099ms step_avg:138.04ms
step:584/1405 train_time:79242ms step_avg:138.05ms
step:585/1405 train_time:79385ms step_avg:138.06ms
step:586/1405 train_time:79529ms step_avg:138.07ms
step:587/1405 train_time:79672ms step_avg:138.08ms
step:588/1405 train_time:79814ms step_avg:138.09ms
step:589/1405 train_time:79957ms step_avg:138.09ms
step:590/1405 train_time:80098ms step_avg:138.10ms
step:591/1405 train_time:80241ms step_avg:138.11ms
step:592/1405 train_time:80383ms step_avg:138.12ms
step:593/1405 train_time:80526ms step_avg:138.12ms
step:594/1405 train_time:80668ms step_avg:138.13ms
step:595/1405 train_time:80812ms step_avg:138.14ms
step:596/1405 train_time:80954ms step_avg:138.15ms
step:597/1405 train_time:81096ms step_avg:138.15ms
step:598/1405 train_time:81237ms step_avg:138.16ms
step:599/1405 train_time:81379ms step_avg:138.16ms
step:600/1405 train_time:81522ms step_avg:138.17ms
step:601/1405 train_time:81665ms step_avg:138.18ms
step:602/1405 train_time:81806ms step_avg:138.19ms
step:603/1405 train_time:81949ms step_avg:138.19ms
step:604/1405 train_time:82092ms step_avg:138.20ms
step:605/1405 train_time:82234ms step_avg:138.21ms
step:606/1405 train_time:82377ms step_avg:138.22ms
step:607/1405 train_time:82520ms step_avg:138.22ms
step:608/1405 train_time:82663ms step_avg:138.23ms
step:609/1405 train_time:82806ms step_avg:138.24ms
step:610/1405 train_time:82948ms step_avg:138.25ms
step:611/1405 train_time:83090ms step_avg:138.25ms
step:612/1405 train_time:83232ms step_avg:138.26ms
step:613/1405 train_time:83374ms step_avg:138.27ms
step:614/1405 train_time:83517ms step_avg:138.27ms
step:615/1405 train_time:83659ms step_avg:138.28ms
step:616/1405 train_time:83802ms step_avg:138.29ms
step:617/1405 train_time:83944ms step_avg:138.29ms
step:618/1405 train_time:84085ms step_avg:138.30ms
step:619/1405 train_time:84228ms step_avg:138.31ms
step:620/1405 train_time:84371ms step_avg:138.31ms
step:621/1405 train_time:84514ms step_avg:138.32ms
step:622/1405 train_time:84657ms step_avg:138.33ms
step:623/1405 train_time:84799ms step_avg:138.33ms
step:624/1405 train_time:84941ms step_avg:138.34ms
step:625/1405 train_time:85084ms step_avg:138.35ms
step:625/1405 val_loss:3.5804 train_time:85152ms step_avg:138.46ms
step:626/1405 train_time:85228ms step_avg:138.36ms
step:627/1405 train_time:85373ms step_avg:138.37ms
step:628/1405 train_time:85516ms step_avg:138.38ms
step:629/1405 train_time:85658ms step_avg:138.38ms
step:630/1405 train_time:85798ms step_avg:138.38ms
step:631/1405 train_time:85940ms step_avg:138.39ms
step:632/1405 train_time:86081ms step_avg:138.39ms
step:633/1405 train_time:86226ms step_avg:138.40ms
step:634/1405 train_time:86370ms step_avg:138.41ms
step:635/1405 train_time:86512ms step_avg:138.42ms
step:636/1405 train_time:86654ms step_avg:138.43ms
step:637/1405 train_time:86797ms step_avg:138.43ms
step:638/1405 train_time:86939ms step_avg:138.44ms
step:639/1405 train_time:87081ms step_avg:138.44ms
step:640/1405 train_time:87225ms step_avg:138.45ms
step:641/1405 train_time:87367ms step_avg:138.46ms
step:642/1405 train_time:87511ms step_avg:138.47ms
step:643/1405 train_time:87653ms step_avg:138.47ms
step:644/1405 train_time:87795ms step_avg:138.48ms
step:645/1405 train_time:87938ms step_avg:138.48ms
step:646/1405 train_time:88080ms step_avg:138.49ms
step:647/1405 train_time:88223ms step_avg:138.50ms
step:648/1405 train_time:88367ms step_avg:138.51ms
step:649/1405 train_time:88510ms step_avg:138.51ms
step:650/1405 train_time:88654ms step_avg:138.52ms
step:651/1405 train_time:88797ms step_avg:138.53ms
step:652/1405 train_time:88938ms step_avg:138.53ms
step:653/1405 train_time:89081ms step_avg:138.54ms
step:654/1405 train_time:89224ms step_avg:138.55ms
step:655/1405 train_time:89367ms step_avg:138.55ms
step:656/1405 train_time:89510ms step_avg:138.56ms
step:657/1405 train_time:89652ms step_avg:138.57ms
step:658/1405 train_time:89796ms step_avg:138.57ms
step:659/1405 train_time:89938ms step_avg:138.58ms
step:660/1405 train_time:90080ms step_avg:138.59ms
step:661/1405 train_time:90224ms step_avg:138.59ms
step:662/1405 train_time:90367ms step_avg:138.60ms
step:663/1405 train_time:90509ms step_avg:138.61ms
step:664/1405 train_time:90652ms step_avg:138.61ms
step:665/1405 train_time:90796ms step_avg:138.62ms
step:666/1405 train_time:90939ms step_avg:138.63ms
step:667/1405 train_time:91081ms step_avg:138.63ms
step:668/1405 train_time:91224ms step_avg:138.64ms
step:669/1405 train_time:91368ms step_avg:138.65ms
step:670/1405 train_time:91511ms step_avg:138.65ms
step:671/1405 train_time:91653ms step_avg:138.66ms
step:672/1405 train_time:91795ms step_avg:138.66ms
step:673/1405 train_time:91938ms step_avg:138.67ms
step:674/1405 train_time:92080ms step_avg:138.67ms
step:675/1405 train_time:92223ms step_avg:138.68ms
step:676/1405 train_time:92367ms step_avg:138.69ms
step:677/1405 train_time:92510ms step_avg:138.70ms
step:678/1405 train_time:92653ms step_avg:138.70ms
step:679/1405 train_time:92795ms step_avg:138.71ms
step:680/1405 train_time:92938ms step_avg:138.71ms
step:681/1405 train_time:93080ms step_avg:138.72ms
step:682/1405 train_time:93223ms step_avg:138.73ms
step:683/1405 train_time:93366ms step_avg:138.73ms
step:684/1405 train_time:93510ms step_avg:138.74ms
step:685/1405 train_time:93653ms step_avg:138.74ms
step:686/1405 train_time:93795ms step_avg:138.75ms
step:687/1405 train_time:93938ms step_avg:138.76ms
step:688/1405 train_time:94081ms step_avg:138.76ms
step:689/1405 train_time:94224ms step_avg:138.77ms
step:690/1405 train_time:94366ms step_avg:138.77ms
step:691/1405 train_time:94508ms step_avg:138.78ms
step:692/1405 train_time:94652ms step_avg:138.79ms
step:693/1405 train_time:94795ms step_avg:138.79ms
step:694/1405 train_time:94937ms step_avg:138.80ms
step:695/1405 train_time:95080ms step_avg:138.80ms
step:696/1405 train_time:95223ms step_avg:138.81ms
step:697/1405 train_time:95366ms step_avg:138.81ms
step:698/1405 train_time:95508ms step_avg:138.82ms
step:699/1405 train_time:95650ms step_avg:138.82ms
step:700/1405 train_time:95793ms step_avg:138.83ms
step:701/1405 train_time:95935ms step_avg:138.84ms
step:702/1405 train_time:96078ms step_avg:138.84ms
step:703/1405 train_time:96221ms step_avg:138.85ms
step:704/1405 train_time:96365ms step_avg:138.85ms
step:705/1405 train_time:96507ms step_avg:138.86ms
step:706/1405 train_time:96650ms step_avg:138.86ms
step:707/1405 train_time:96793ms step_avg:138.87ms
step:708/1405 train_time:96937ms step_avg:138.88ms
step:709/1405 train_time:97079ms step_avg:138.88ms
step:710/1405 train_time:97222ms step_avg:138.89ms
step:711/1405 train_time:97366ms step_avg:138.90ms
step:712/1405 train_time:97510ms step_avg:138.90ms
step:713/1405 train_time:97652ms step_avg:138.91ms
step:714/1405 train_time:97794ms step_avg:138.91ms
step:715/1405 train_time:97936ms step_avg:138.92ms
step:716/1405 train_time:98079ms step_avg:138.92ms
step:717/1405 train_time:98222ms step_avg:138.93ms
step:718/1405 train_time:98364ms step_avg:138.93ms
step:719/1405 train_time:98507ms step_avg:138.94ms
step:720/1405 train_time:98651ms step_avg:138.94ms
step:721/1405 train_time:98793ms step_avg:138.95ms
step:722/1405 train_time:98938ms step_avg:138.96ms
step:723/1405 train_time:99080ms step_avg:138.96ms
step:724/1405 train_time:99223ms step_avg:138.97ms
step:725/1405 train_time:99366ms step_avg:138.97ms
step:726/1405 train_time:99509ms step_avg:138.98ms
step:727/1405 train_time:99652ms step_avg:138.99ms
step:728/1405 train_time:99795ms step_avg:138.99ms
step:729/1405 train_time:99939ms step_avg:139.00ms
step:730/1405 train_time:100082ms step_avg:139.00ms
step:731/1405 train_time:100226ms step_avg:139.01ms
step:732/1405 train_time:100371ms step_avg:139.02ms
step:733/1405 train_time:100515ms step_avg:139.02ms
step:734/1405 train_time:100660ms step_avg:139.03ms
step:735/1405 train_time:100804ms step_avg:139.04ms
step:736/1405 train_time:100949ms step_avg:139.05ms
step:737/1405 train_time:101094ms step_avg:139.06ms
step:738/1405 train_time:101238ms step_avg:139.06ms
step:739/1405 train_time:101383ms step_avg:139.07ms
step:740/1405 train_time:101529ms step_avg:139.08ms
step:741/1405 train_time:101673ms step_avg:139.09ms
step:742/1405 train_time:101818ms step_avg:139.10ms
step:743/1405 train_time:101962ms step_avg:139.10ms
step:744/1405 train_time:102107ms step_avg:139.11ms
step:745/1405 train_time:102252ms step_avg:139.12ms
step:746/1405 train_time:102396ms step_avg:139.13ms
step:747/1405 train_time:102539ms step_avg:139.13ms
step:748/1405 train_time:102685ms step_avg:139.14ms
step:749/1405 train_time:102831ms step_avg:139.15ms
step:750/1405 train_time:102975ms step_avg:139.16ms
step:750/1405 val_loss:3.5279 train_time:103044ms step_avg:139.25ms
step:751/1405 train_time:103122ms step_avg:139.17ms
step:752/1405 train_time:103266ms step_avg:139.17ms
step:753/1405 train_time:103410ms step_avg:139.18ms
step:754/1405 train_time:103554ms step_avg:139.19ms
step:755/1405 train_time:103698ms step_avg:139.19ms
step:756/1405 train_time:103841ms step_avg:139.20ms
step:757/1405 train_time:103986ms step_avg:139.20ms
step:758/1405 train_time:104132ms step_avg:139.21ms
step:759/1405 train_time:104278ms step_avg:139.22ms
step:760/1405 train_time:104423ms step_avg:139.23ms
step:761/1405 train_time:104613ms step_avg:139.30ms
step:762/1405 train_time:104758ms step_avg:139.31ms
step:763/1405 train_time:104902ms step_avg:139.31ms
step:764/1405 train_time:105045ms step_avg:139.32ms
step:765/1405 train_time:105189ms step_avg:139.32ms
step:766/1405 train_time:105333ms step_avg:139.33ms
step:767/1405 train_time:105480ms step_avg:139.34ms
step:768/1405 train_time:105627ms step_avg:139.35ms
step:769/1405 train_time:105772ms step_avg:139.36ms
step:770/1405 train_time:105917ms step_avg:139.36ms
step:771/1405 train_time:106060ms step_avg:139.37ms
step:772/1405 train_time:106204ms step_avg:139.38ms
step:773/1405 train_time:106349ms step_avg:139.38ms
step:774/1405 train_time:106494ms step_avg:139.39ms
step:775/1405 train_time:106639ms step_avg:139.40ms
step:776/1405 train_time:106784ms step_avg:139.40ms
step:777/1405 train_time:106930ms step_avg:139.41ms
step:778/1405 train_time:107076ms step_avg:139.42ms
step:779/1405 train_time:107220ms step_avg:139.43ms
step:780/1405 train_time:107364ms step_avg:139.43ms
step:781/1405 train_time:107508ms step_avg:139.44ms
step:782/1405 train_time:107654ms step_avg:139.45ms
step:783/1405 train_time:107799ms step_avg:139.46ms
step:784/1405 train_time:107943ms step_avg:139.46ms
step:785/1405 train_time:108087ms step_avg:139.47ms
step:786/1405 train_time:108233ms step_avg:139.48ms
step:787/1405 train_time:108377ms step_avg:139.48ms
step:788/1405 train_time:108521ms step_avg:139.49ms
step:789/1405 train_time:108666ms step_avg:139.49ms
step:790/1405 train_time:108810ms step_avg:139.50ms
step:791/1405 train_time:108955ms step_avg:139.51ms
step:792/1405 train_time:109100ms step_avg:139.51ms
step:793/1405 train_time:109243ms step_avg:139.52ms
step:794/1405 train_time:109388ms step_avg:139.53ms
step:795/1405 train_time:109534ms step_avg:139.53ms
step:796/1405 train_time:109678ms step_avg:139.54ms
step:797/1405 train_time:109823ms step_avg:139.55ms
step:798/1405 train_time:109967ms step_avg:139.55ms
step:799/1405 train_time:110112ms step_avg:139.56ms
step:800/1405 train_time:110258ms step_avg:139.57ms
step:801/1405 train_time:110402ms step_avg:139.57ms
step:802/1405 train_time:110546ms step_avg:139.58ms
step:803/1405 train_time:110690ms step_avg:139.58ms
step:804/1405 train_time:110835ms step_avg:139.59ms
step:805/1405 train_time:110981ms step_avg:139.60ms
step:806/1405 train_time:111125ms step_avg:139.60ms
step:807/1405 train_time:111269ms step_avg:139.61ms
step:808/1405 train_time:111415ms step_avg:139.62ms
step:809/1405 train_time:111559ms step_avg:139.62ms
step:810/1405 train_time:111703ms step_avg:139.63ms
step:811/1405 train_time:111847ms step_avg:139.63ms
step:812/1405 train_time:111991ms step_avg:139.64ms
step:813/1405 train_time:112135ms step_avg:139.65ms
step:814/1405 train_time:112281ms step_avg:139.65ms
step:815/1405 train_time:112425ms step_avg:139.66ms
step:816/1405 train_time:112570ms step_avg:139.66ms
step:817/1405 train_time:112716ms step_avg:139.67ms
step:818/1405 train_time:112860ms step_avg:139.68ms
step:819/1405 train_time:113004ms step_avg:139.68ms
step:820/1405 train_time:113147ms step_avg:139.69ms
step:821/1405 train_time:113293ms step_avg:139.70ms
step:822/1405 train_time:113439ms step_avg:139.70ms
step:823/1405 train_time:113583ms step_avg:139.71ms
step:824/1405 train_time:113727ms step_avg:139.71ms
step:825/1405 train_time:113871ms step_avg:139.72ms
step:826/1405 train_time:114017ms step_avg:139.73ms
step:827/1405 train_time:114162ms step_avg:139.73ms
step:828/1405 train_time:114307ms step_avg:139.74ms
step:829/1405 train_time:114453ms step_avg:139.75ms
step:830/1405 train_time:114598ms step_avg:139.75ms
step:831/1405 train_time:114742ms step_avg:139.76ms
step:832/1405 train_time:114887ms step_avg:139.77ms
step:833/1405 train_time:115031ms step_avg:139.77ms
step:834/1405 train_time:115176ms step_avg:139.78ms
step:835/1405 train_time:115322ms step_avg:139.78ms
step:836/1405 train_time:115467ms step_avg:139.79ms
step:837/1405 train_time:115612ms step_avg:139.80ms
step:838/1405 train_time:115758ms step_avg:139.80ms
step:839/1405 train_time:115902ms step_avg:139.81ms
step:840/1405 train_time:116046ms step_avg:139.81ms
step:841/1405 train_time:116190ms step_avg:139.82ms
step:842/1405 train_time:116337ms step_avg:139.83ms
step:843/1405 train_time:116482ms step_avg:139.83ms
step:844/1405 train_time:116627ms step_avg:139.84ms
step:845/1405 train_time:116771ms step_avg:139.85ms
step:846/1405 train_time:116917ms step_avg:139.85ms
step:847/1405 train_time:117062ms step_avg:139.86ms
step:848/1405 train_time:117206ms step_avg:139.86ms
step:849/1405 train_time:117351ms step_avg:139.87ms
step:850/1405 train_time:117497ms step_avg:139.88ms
step:851/1405 train_time:117641ms step_avg:139.88ms
step:852/1405 train_time:117786ms step_avg:139.89ms
step:853/1405 train_time:117931ms step_avg:139.89ms
step:854/1405 train_time:118076ms step_avg:139.90ms
step:855/1405 train_time:118221ms step_avg:139.91ms
step:856/1405 train_time:118365ms step_avg:139.91ms
step:857/1405 train_time:118511ms step_avg:139.92ms
step:858/1405 train_time:118657ms step_avg:139.93ms
step:859/1405 train_time:118803ms step_avg:139.93ms
step:860/1405 train_time:118948ms step_avg:139.94ms
step:861/1405 train_time:119093ms step_avg:139.95ms
step:862/1405 train_time:119239ms step_avg:139.95ms
step:863/1405 train_time:119384ms step_avg:139.96ms
step:864/1405 train_time:119529ms step_avg:139.96ms
step:865/1405 train_time:119674ms step_avg:139.97ms
step:866/1405 train_time:119821ms step_avg:139.98ms
step:867/1405 train_time:119966ms step_avg:139.98ms
step:868/1405 train_time:120111ms step_avg:139.99ms
step:869/1405 train_time:120255ms step_avg:139.99ms
step:870/1405 train_time:120401ms step_avg:140.00ms
step:871/1405 train_time:120545ms step_avg:140.01ms
step:872/1405 train_time:120689ms step_avg:140.01ms
step:873/1405 train_time:120835ms step_avg:140.02ms
step:874/1405 train_time:120980ms step_avg:140.02ms
step:875/1405 train_time:121125ms step_avg:140.03ms
step:875/1405 val_loss:3.4771 train_time:121195ms step_avg:140.11ms
step:876/1405 train_time:121272ms step_avg:140.04ms
step:877/1405 train_time:121417ms step_avg:140.04ms
step:878/1405 train_time:121563ms step_avg:140.05ms
step:879/1405 train_time:121708ms step_avg:140.05ms
step:880/1405 train_time:121852ms step_avg:140.06ms
step:881/1405 train_time:121996ms step_avg:140.06ms
step:882/1405 train_time:122141ms step_avg:140.07ms
step:883/1405 train_time:122289ms step_avg:140.08ms
step:884/1405 train_time:122434ms step_avg:140.09ms
step:885/1405 train_time:122579ms step_avg:140.09ms
step:886/1405 train_time:122723ms step_avg:140.10ms
step:887/1405 train_time:122867ms step_avg:140.10ms
step:888/1405 train_time:123012ms step_avg:140.11ms
step:889/1405 train_time:123157ms step_avg:140.11ms
step:890/1405 train_time:123303ms step_avg:140.12ms
step:891/1405 train_time:123450ms step_avg:140.12ms
step:892/1405 train_time:123594ms step_avg:140.13ms
step:893/1405 train_time:123738ms step_avg:140.13ms
step:894/1405 train_time:123884ms step_avg:140.14ms
step:895/1405 train_time:124028ms step_avg:140.14ms
step:896/1405 train_time:124172ms step_avg:140.15ms
step:897/1405 train_time:124316ms step_avg:140.15ms
step:898/1405 train_time:124463ms step_avg:140.16ms
step:899/1405 train_time:124609ms step_avg:140.17ms
step:900/1405 train_time:124752ms step_avg:140.17ms
step:901/1405 train_time:124897ms step_avg:140.18ms
step:902/1405 train_time:125041ms step_avg:140.18ms
step:903/1405 train_time:125186ms step_avg:140.19ms
step:904/1405 train_time:125330ms step_avg:140.19ms
step:905/1405 train_time:125474ms step_avg:140.19ms
step:906/1405 train_time:125620ms step_avg:140.20ms
step:907/1405 train_time:125766ms step_avg:140.21ms
step:908/1405 train_time:125911ms step_avg:140.21ms
step:909/1405 train_time:126055ms step_avg:140.22ms
step:910/1405 train_time:126201ms step_avg:140.22ms
step:911/1405 train_time:126347ms step_avg:140.23ms
step:912/1405 train_time:126492ms step_avg:140.24ms
step:913/1405 train_time:126637ms step_avg:140.24ms
step:914/1405 train_time:126784ms step_avg:140.25ms
step:915/1405 train_time:126929ms step_avg:140.25ms
step:916/1405 train_time:127073ms step_avg:140.26ms
step:917/1405 train_time:127218ms step_avg:140.26ms
step:918/1405 train_time:127363ms step_avg:140.27ms
step:919/1405 train_time:127510ms step_avg:140.28ms
step:920/1405 train_time:127654ms step_avg:140.28ms
step:921/1405 train_time:127799ms step_avg:140.28ms
step:922/1405 train_time:127945ms step_avg:140.29ms
step:923/1405 train_time:128089ms step_avg:140.29ms
step:924/1405 train_time:128233ms step_avg:140.30ms
step:925/1405 train_time:128378ms step_avg:140.30ms
step:926/1405 train_time:128524ms step_avg:140.31ms
step:927/1405 train_time:128668ms step_avg:140.31ms
step:928/1405 train_time:128813ms step_avg:140.32ms
step:929/1405 train_time:128958ms step_avg:140.32ms
step:930/1405 train_time:129103ms step_avg:140.33ms
step:931/1405 train_time:129248ms step_avg:140.33ms
step:932/1405 train_time:129393ms step_avg:140.34ms
step:933/1405 train_time:129538ms step_avg:140.34ms
step:934/1405 train_time:129683ms step_avg:140.35ms
step:935/1405 train_time:129830ms step_avg:140.36ms
step:936/1405 train_time:129974ms step_avg:140.36ms
step:937/1405 train_time:130119ms step_avg:140.37ms
step:938/1405 train_time:130265ms step_avg:140.37ms
step:939/1405 train_time:130412ms step_avg:140.38ms
step:940/1405 train_time:130559ms step_avg:140.39ms
step:941/1405 train_time:130705ms step_avg:140.39ms
step:942/1405 train_time:130851ms step_avg:140.40ms
step:943/1405 train_time:130997ms step_avg:140.40ms
step:944/1405 train_time:131144ms step_avg:140.41ms
step:945/1405 train_time:131290ms step_avg:140.42ms
step:946/1405 train_time:131435ms step_avg:140.42ms
step:947/1405 train_time:131582ms step_avg:140.43ms
step:948/1405 train_time:131730ms step_avg:140.44ms
step:949/1405 train_time:131877ms step_avg:140.44ms
step:950/1405 train_time:132024ms step_avg:140.45ms
step:951/1405 train_time:132211ms step_avg:140.50ms
step:952/1405 train_time:132356ms step_avg:140.51ms
step:953/1405 train_time:132503ms step_avg:140.51ms
step:954/1405 train_time:132649ms step_avg:140.52ms
step:955/1405 train_time:132794ms step_avg:140.52ms
step:956/1405 train_time:132941ms step_avg:140.53ms
step:957/1405 train_time:133088ms step_avg:140.54ms
step:958/1405 train_time:133235ms step_avg:140.54ms
step:959/1405 train_time:133382ms step_avg:140.55ms
step:960/1405 train_time:133530ms step_avg:140.56ms
step:961/1405 train_time:133675ms step_avg:140.56ms
step:962/1405 train_time:133821ms step_avg:140.57ms
step:963/1405 train_time:133970ms step_avg:140.58ms
step:964/1405 train_time:134117ms step_avg:140.58ms
step:965/1405 train_time:134265ms step_avg:140.59ms
step:966/1405 train_time:134411ms step_avg:140.60ms
step:967/1405 train_time:134557ms step_avg:140.60ms
step:968/1405 train_time:134705ms step_avg:140.61ms
step:969/1405 train_time:134851ms step_avg:140.62ms
step:970/1405 train_time:134996ms step_avg:140.62ms
step:971/1405 train_time:135143ms step_avg:140.63ms
step:972/1405 train_time:135291ms step_avg:140.64ms
step:973/1405 train_time:135437ms step_avg:140.64ms
step:974/1405 train_time:135583ms step_avg:140.65ms
step:975/1405 train_time:135732ms step_avg:140.65ms
step:976/1405 train_time:135878ms step_avg:140.66ms
step:977/1405 train_time:136026ms step_avg:140.67ms
step:978/1405 train_time:136172ms step_avg:140.67ms
step:979/1405 train_time:136317ms step_avg:140.68ms
step:980/1405 train_time:136464ms step_avg:140.68ms
step:981/1405 train_time:136611ms step_avg:140.69ms
step:982/1405 train_time:136757ms step_avg:140.70ms
step:983/1405 train_time:136903ms step_avg:140.70ms
step:984/1405 train_time:137050ms step_avg:140.71ms
step:985/1405 train_time:137197ms step_avg:140.72ms
step:986/1405 train_time:137346ms step_avg:140.72ms
step:987/1405 train_time:137492ms step_avg:140.73ms
step:988/1405 train_time:137638ms step_avg:140.73ms
step:989/1405 train_time:137783ms step_avg:140.74ms
step:990/1405 train_time:137930ms step_avg:140.75ms
step:991/1405 train_time:138076ms step_avg:140.75ms
step:992/1405 train_time:138225ms step_avg:140.76ms
step:993/1405 train_time:138373ms step_avg:140.77ms
step:994/1405 train_time:138519ms step_avg:140.77ms
step:995/1405 train_time:138665ms step_avg:140.78ms
step:996/1405 train_time:138811ms step_avg:140.78ms
step:997/1405 train_time:138956ms step_avg:140.79ms
step:998/1405 train_time:139103ms step_avg:140.79ms
step:999/1405 train_time:139249ms step_avg:140.80ms
step:1000/1405 train_time:139395ms step_avg:140.80ms
step:1000/1405 val_loss:3.4122 train_time:139465ms step_avg:140.87ms
step:1001/1405 train_time:139542ms step_avg:140.81ms
step:1002/1405 train_time:139688ms step_avg:140.81ms
step:1003/1405 train_time:139836ms step_avg:140.82ms
step:1004/1405 train_time:139983ms step_avg:140.83ms
step:1005/1405 train_time:140129ms step_avg:140.83ms
step:1006/1405 train_time:140274ms step_avg:140.84ms
step:1007/1405 train_time:140421ms step_avg:140.84ms
step:1008/1405 train_time:140569ms step_avg:140.85ms
step:1009/1405 train_time:140718ms step_avg:140.86ms
step:1010/1405 train_time:140865ms step_avg:140.86ms
step:1011/1405 train_time:141012ms step_avg:140.87ms
step:1012/1405 train_time:141157ms step_avg:140.88ms
step:1013/1405 train_time:141304ms step_avg:140.88ms
step:1014/1405 train_time:141450ms step_avg:140.89ms
step:1015/1405 train_time:141597ms step_avg:140.89ms
step:1016/1405 train_time:141744ms step_avg:140.90ms
step:1017/1405 train_time:141890ms step_avg:140.90ms
step:1018/1405 train_time:142037ms step_avg:140.91ms
step:1019/1405 train_time:142184ms step_avg:140.92ms
step:1020/1405 train_time:142331ms step_avg:140.92ms
step:1021/1405 train_time:142476ms step_avg:140.93ms
step:1022/1405 train_time:142622ms step_avg:140.93ms
step:1023/1405 train_time:142768ms step_avg:140.94ms
step:1024/1405 train_time:142916ms step_avg:140.94ms
step:1025/1405 train_time:143064ms step_avg:140.95ms
step:1026/1405 train_time:143209ms step_avg:140.95ms
step:1027/1405 train_time:143356ms step_avg:140.96ms
step:1028/1405 train_time:143504ms step_avg:140.97ms
step:1029/1405 train_time:143650ms step_avg:140.97ms
step:1030/1405 train_time:143798ms step_avg:140.98ms
step:1031/1405 train_time:143945ms step_avg:140.98ms
step:1032/1405 train_time:144091ms step_avg:140.99ms
step:1033/1405 train_time:144238ms step_avg:140.99ms
step:1034/1405 train_time:144384ms step_avg:141.00ms
step:1035/1405 train_time:144531ms step_avg:141.01ms
step:1036/1405 train_time:144677ms step_avg:141.01ms
step:1037/1405 train_time:144825ms step_avg:141.02ms
step:1038/1405 train_time:144970ms step_avg:141.02ms
step:1039/1405 train_time:145116ms step_avg:141.03ms
step:1040/1405 train_time:145262ms step_avg:141.03ms
step:1041/1405 train_time:145408ms step_avg:141.04ms
step:1042/1405 train_time:145554ms step_avg:141.04ms
step:1043/1405 train_time:145701ms step_avg:141.05ms
step:1044/1405 train_time:145848ms step_avg:141.05ms
step:1045/1405 train_time:145996ms step_avg:141.06ms
step:1046/1405 train_time:146143ms step_avg:141.06ms
step:1047/1405 train_time:146289ms step_avg:141.07ms
step:1048/1405 train_time:146436ms step_avg:141.08ms
step:1049/1405 train_time:146585ms step_avg:141.08ms
step:1050/1405 train_time:146731ms step_avg:141.09ms
step:1051/1405 train_time:146877ms step_avg:141.09ms
step:1052/1405 train_time:147024ms step_avg:141.10ms
step:1053/1405 train_time:147171ms step_avg:141.10ms
step:1054/1405 train_time:147318ms step_avg:141.11ms
step:1055/1405 train_time:147465ms step_avg:141.12ms
step:1056/1405 train_time:147612ms step_avg:141.12ms
step:1057/1405 train_time:147759ms step_avg:141.13ms
step:1058/1405 train_time:147907ms step_avg:141.13ms
step:1059/1405 train_time:148056ms step_avg:141.14ms
step:1060/1405 train_time:148205ms step_avg:141.15ms
step:1061/1405 train_time:148350ms step_avg:141.15ms
step:1062/1405 train_time:148498ms step_avg:141.16ms
step:1063/1405 train_time:148645ms step_avg:141.16ms
step:1064/1405 train_time:148791ms step_avg:141.17ms
step:1065/1405 train_time:148938ms step_avg:141.17ms
step:1066/1405 train_time:149087ms step_avg:141.18ms
step:1067/1405 train_time:149234ms step_avg:141.19ms
step:1068/1405 train_time:149382ms step_avg:141.19ms
step:1069/1405 train_time:149529ms step_avg:141.20ms
step:1070/1405 train_time:149675ms step_avg:141.20ms
step:1071/1405 train_time:149824ms step_avg:141.21ms
step:1072/1405 train_time:149970ms step_avg:141.22ms
step:1073/1405 train_time:150115ms step_avg:141.22ms
step:1074/1405 train_time:150262ms step_avg:141.22ms
step:1075/1405 train_time:150408ms step_avg:141.23ms
step:1076/1405 train_time:150554ms step_avg:141.23ms
step:1077/1405 train_time:150700ms step_avg:141.24ms
step:1078/1405 train_time:150850ms step_avg:141.25ms
step:1079/1405 train_time:150998ms step_avg:141.25ms
step:1080/1405 train_time:151145ms step_avg:141.26ms
step:1081/1405 train_time:151291ms step_avg:141.26ms
step:1082/1405 train_time:151439ms step_avg:141.27ms
step:1083/1405 train_time:151586ms step_avg:141.27ms
step:1084/1405 train_time:151734ms step_avg:141.28ms
step:1085/1405 train_time:151881ms step_avg:141.28ms
step:1086/1405 train_time:152028ms step_avg:141.29ms
step:1087/1405 train_time:152174ms step_avg:141.29ms
step:1088/1405 train_time:152320ms step_avg:141.30ms
step:1089/1405 train_time:152468ms step_avg:141.30ms
step:1090/1405 train_time:152616ms step_avg:141.31ms
step:1091/1405 train_time:152764ms step_avg:141.32ms
step:1092/1405 train_time:152911ms step_avg:141.32ms
step:1093/1405 train_time:153057ms step_avg:141.33ms
step:1094/1405 train_time:153205ms step_avg:141.33ms
step:1095/1405 train_time:153350ms step_avg:141.34ms
step:1096/1405 train_time:153499ms step_avg:141.34ms
step:1097/1405 train_time:153646ms step_avg:141.35ms
step:1098/1405 train_time:153792ms step_avg:141.35ms
step:1099/1405 train_time:153939ms step_avg:141.36ms
step:1100/1405 train_time:154085ms step_avg:141.36ms
step:1101/1405 train_time:154234ms step_avg:141.37ms
step:1102/1405 train_time:154382ms step_avg:141.38ms
step:1103/1405 train_time:154528ms step_avg:141.38ms
step:1104/1405 train_time:154675ms step_avg:141.38ms
step:1105/1405 train_time:154824ms step_avg:141.39ms
step:1106/1405 train_time:154970ms step_avg:141.40ms
step:1107/1405 train_time:155117ms step_avg:141.40ms
step:1108/1405 train_time:155266ms step_avg:141.41ms
step:1109/1405 train_time:155412ms step_avg:141.41ms
step:1110/1405 train_time:155558ms step_avg:141.42ms
step:1111/1405 train_time:155704ms step_avg:141.42ms
step:1112/1405 train_time:155850ms step_avg:141.42ms
step:1113/1405 train_time:155996ms step_avg:141.43ms
step:1114/1405 train_time:156143ms step_avg:141.43ms
step:1115/1405 train_time:156289ms step_avg:141.44ms
step:1116/1405 train_time:156436ms step_avg:141.44ms
step:1117/1405 train_time:156585ms step_avg:141.45ms
step:1118/1405 train_time:156733ms step_avg:141.46ms
step:1119/1405 train_time:156880ms step_avg:141.46ms
step:1120/1405 train_time:157026ms step_avg:141.47ms
step:1121/1405 train_time:157172ms step_avg:141.47ms
step:1122/1405 train_time:157318ms step_avg:141.47ms
step:1123/1405 train_time:157466ms step_avg:141.48ms
step:1124/1405 train_time:157612ms step_avg:141.48ms
step:1125/1405 train_time:157759ms step_avg:141.49ms
step:1125/1405 val_loss:3.3590 train_time:157830ms step_avg:141.55ms
step:1126/1405 train_time:157907ms step_avg:141.49ms
step:1127/1405 train_time:158054ms step_avg:141.50ms
step:1128/1405 train_time:158199ms step_avg:141.50ms
step:1129/1405 train_time:158345ms step_avg:141.51ms
step:1130/1405 train_time:158492ms step_avg:141.51ms
step:1131/1405 train_time:158638ms step_avg:141.51ms
step:1132/1405 train_time:158784ms step_avg:141.52ms
step:1133/1405 train_time:158934ms step_avg:141.53ms
step:1134/1405 train_time:159081ms step_avg:141.53ms
step:1135/1405 train_time:159227ms step_avg:141.54ms
step:1136/1405 train_time:159376ms step_avg:141.54ms
step:1137/1405 train_time:159521ms step_avg:141.54ms
step:1138/1405 train_time:159668ms step_avg:141.55ms
step:1139/1405 train_time:159815ms step_avg:141.55ms
step:1140/1405 train_time:159962ms step_avg:141.56ms
step:1141/1405 train_time:160154ms step_avg:141.60ms
step:1142/1405 train_time:160300ms step_avg:141.61ms
step:1143/1405 train_time:160448ms step_avg:141.61ms
step:1144/1405 train_time:160595ms step_avg:141.62ms
step:1145/1405 train_time:160740ms step_avg:141.62ms
step:1146/1405 train_time:160887ms step_avg:141.63ms
step:1147/1405 train_time:161037ms step_avg:141.63ms
step:1148/1405 train_time:161185ms step_avg:141.64ms
step:1149/1405 train_time:161334ms step_avg:141.65ms
step:1150/1405 train_time:161481ms step_avg:141.65ms
step:1151/1405 train_time:161631ms step_avg:141.66ms
step:1152/1405 train_time:161780ms step_avg:141.66ms
step:1153/1405 train_time:161929ms step_avg:141.67ms
step:1154/1405 train_time:162079ms step_avg:141.68ms
step:1155/1405 train_time:162229ms step_avg:141.68ms
step:1156/1405 train_time:162378ms step_avg:141.69ms
step:1157/1405 train_time:162526ms step_avg:141.70ms
step:1158/1405 train_time:162675ms step_avg:141.70ms
step:1159/1405 train_time:162822ms step_avg:141.71ms
step:1160/1405 train_time:162970ms step_avg:141.71ms
step:1161/1405 train_time:163119ms step_avg:141.72ms
step:1162/1405 train_time:163268ms step_avg:141.73ms
step:1163/1405 train_time:163418ms step_avg:141.73ms
step:1164/1405 train_time:163568ms step_avg:141.74ms
step:1165/1405 train_time:163715ms step_avg:141.74ms
step:1166/1405 train_time:163862ms step_avg:141.75ms
step:1167/1405 train_time:164009ms step_avg:141.75ms
step:1168/1405 train_time:164157ms step_avg:141.76ms
step:1169/1405 train_time:164305ms step_avg:141.76ms
step:1170/1405 train_time:164455ms step_avg:141.77ms
step:1171/1405 train_time:164603ms step_avg:141.78ms
step:1172/1405 train_time:164751ms step_avg:141.78ms
step:1173/1405 train_time:164899ms step_avg:141.79ms
step:1174/1405 train_time:165050ms step_avg:141.80ms
step:1175/1405 train_time:165199ms step_avg:141.80ms
step:1176/1405 train_time:165348ms step_avg:141.81ms
step:1177/1405 train_time:165500ms step_avg:141.82ms
step:1178/1405 train_time:165648ms step_avg:141.82ms
step:1179/1405 train_time:165795ms step_avg:141.83ms
step:1180/1405 train_time:165947ms step_avg:141.83ms
step:1181/1405 train_time:166098ms step_avg:141.84ms
step:1182/1405 train_time:166245ms step_avg:141.85ms
step:1183/1405 train_time:166395ms step_avg:141.85ms
step:1184/1405 train_time:166543ms step_avg:141.86ms
step:1185/1405 train_time:166693ms step_avg:141.87ms
step:1186/1405 train_time:166841ms step_avg:141.87ms
step:1187/1405 train_time:166992ms step_avg:141.88ms
step:1188/1405 train_time:167140ms step_avg:141.88ms
step:1189/1405 train_time:167288ms step_avg:141.89ms
step:1190/1405 train_time:167436ms step_avg:141.90ms
step:1191/1405 train_time:167586ms step_avg:141.90ms
step:1192/1405 train_time:167735ms step_avg:141.91ms
step:1193/1405 train_time:167882ms step_avg:141.91ms
step:1194/1405 train_time:168031ms step_avg:141.92ms
step:1195/1405 train_time:168178ms step_avg:141.92ms
step:1196/1405 train_time:168328ms step_avg:141.93ms
step:1197/1405 train_time:168477ms step_avg:141.93ms
step:1198/1405 train_time:168627ms step_avg:141.94ms
step:1199/1405 train_time:168776ms step_avg:141.95ms
step:1200/1405 train_time:168923ms step_avg:141.95ms
step:1201/1405 train_time:169071ms step_avg:141.96ms
step:1202/1405 train_time:169225ms step_avg:141.97ms
step:1203/1405 train_time:169377ms step_avg:141.98ms
step:1204/1405 train_time:169525ms step_avg:141.98ms
step:1205/1405 train_time:169674ms step_avg:141.99ms
step:1206/1405 train_time:169822ms step_avg:141.99ms
step:1207/1405 train_time:169970ms step_avg:142.00ms
step:1208/1405 train_time:170118ms step_avg:142.00ms
step:1209/1405 train_time:170268ms step_avg:142.01ms
step:1210/1405 train_time:170417ms step_avg:142.01ms
step:1211/1405 train_time:170565ms step_avg:142.02ms
step:1212/1405 train_time:170714ms step_avg:142.02ms
step:1213/1405 train_time:170861ms step_avg:142.03ms
step:1214/1405 train_time:171013ms step_avg:142.04ms
step:1215/1405 train_time:171159ms step_avg:142.04ms
step:1216/1405 train_time:171307ms step_avg:142.05ms
step:1217/1405 train_time:171457ms step_avg:142.05ms
step:1218/1405 train_time:171604ms step_avg:142.06ms
step:1219/1405 train_time:171753ms step_avg:142.06ms
step:1220/1405 train_time:171899ms step_avg:142.07ms
step:1221/1405 train_time:172047ms step_avg:142.07ms
step:1222/1405 train_time:172196ms step_avg:142.08ms
step:1223/1405 train_time:172345ms step_avg:142.08ms
step:1224/1405 train_time:172495ms step_avg:142.09ms
step:1225/1405 train_time:172644ms step_avg:142.09ms
step:1226/1405 train_time:172793ms step_avg:142.10ms
step:1227/1405 train_time:172942ms step_avg:142.10ms
step:1228/1405 train_time:173090ms step_avg:142.11ms
step:1229/1405 train_time:173238ms step_avg:142.11ms
step:1230/1405 train_time:173388ms step_avg:142.12ms
step:1231/1405 train_time:173538ms step_avg:142.13ms
step:1232/1405 train_time:173688ms step_avg:142.13ms
step:1233/1405 train_time:173836ms step_avg:142.14ms
step:1234/1405 train_time:173983ms step_avg:142.14ms
step:1235/1405 train_time:174131ms step_avg:142.15ms
step:1236/1405 train_time:174278ms step_avg:142.15ms
step:1237/1405 train_time:174426ms step_avg:142.16ms
step:1238/1405 train_time:174577ms step_avg:142.16ms
step:1239/1405 train_time:174725ms step_avg:142.17ms
step:1240/1405 train_time:174875ms step_avg:142.17ms
step:1241/1405 train_time:175025ms step_avg:142.18ms
step:1242/1405 train_time:175174ms step_avg:142.19ms
step:1243/1405 train_time:175320ms step_avg:142.19ms
step:1244/1405 train_time:175467ms step_avg:142.19ms
step:1245/1405 train_time:175616ms step_avg:142.20ms
step:1246/1405 train_time:175763ms step_avg:142.20ms
step:1247/1405 train_time:175912ms step_avg:142.21ms
step:1248/1405 train_time:176059ms step_avg:142.21ms
step:1249/1405 train_time:176206ms step_avg:142.22ms
step:1250/1405 train_time:176356ms step_avg:142.22ms
step:1250/1405 val_loss:3.3112 train_time:176429ms step_avg:142.28ms
step:1251/1405 train_time:176509ms step_avg:142.23ms
step:1252/1405 train_time:176659ms step_avg:142.24ms
step:1253/1405 train_time:176805ms step_avg:142.24ms
step:1254/1405 train_time:176952ms step_avg:142.24ms
step:1255/1405 train_time:177103ms step_avg:142.25ms
step:1256/1405 train_time:177250ms step_avg:142.26ms
step:1257/1405 train_time:177398ms step_avg:142.26ms
step:1258/1405 train_time:177548ms step_avg:142.27ms
step:1259/1405 train_time:177699ms step_avg:142.27ms
step:1260/1405 train_time:177848ms step_avg:142.28ms
step:1261/1405 train_time:177996ms step_avg:142.28ms
step:1262/1405 train_time:178146ms step_avg:142.29ms
step:1263/1405 train_time:178295ms step_avg:142.29ms
step:1264/1405 train_time:178442ms step_avg:142.30ms
step:1265/1405 train_time:178590ms step_avg:142.30ms
step:1266/1405 train_time:178739ms step_avg:142.31ms
step:1267/1405 train_time:178887ms step_avg:142.31ms
step:1268/1405 train_time:179038ms step_avg:142.32ms
step:1269/1405 train_time:179189ms step_avg:142.33ms
step:1270/1405 train_time:179337ms step_avg:142.33ms
step:1271/1405 train_time:179485ms step_avg:142.34ms
step:1272/1405 train_time:179634ms step_avg:142.34ms
step:1273/1405 train_time:179781ms step_avg:142.34ms
step:1274/1405 train_time:179929ms step_avg:142.35ms
step:1275/1405 train_time:180078ms step_avg:142.35ms
step:1276/1405 train_time:180226ms step_avg:142.36ms
step:1277/1405 train_time:180374ms step_avg:142.36ms
step:1278/1405 train_time:180522ms step_avg:142.37ms
step:1279/1405 train_time:180671ms step_avg:142.37ms
step:1280/1405 train_time:180820ms step_avg:142.38ms
step:1281/1405 train_time:180968ms step_avg:142.38ms
step:1282/1405 train_time:181116ms step_avg:142.39ms
step:1283/1405 train_time:181264ms step_avg:142.39ms
step:1284/1405 train_time:181413ms step_avg:142.40ms
step:1285/1405 train_time:181561ms step_avg:142.40ms
step:1286/1405 train_time:181710ms step_avg:142.41ms
step:1287/1405 train_time:181858ms step_avg:142.41ms
step:1288/1405 train_time:182007ms step_avg:142.42ms
step:1289/1405 train_time:182158ms step_avg:142.42ms
step:1290/1405 train_time:182308ms step_avg:142.43ms
step:1291/1405 train_time:182457ms step_avg:142.43ms
step:1292/1405 train_time:182605ms step_avg:142.44ms
step:1293/1405 train_time:182755ms step_avg:142.44ms
step:1294/1405 train_time:182903ms step_avg:142.45ms
step:1295/1405 train_time:183051ms step_avg:142.45ms
step:1296/1405 train_time:183200ms step_avg:142.46ms
step:1297/1405 train_time:183349ms step_avg:142.46ms
step:1298/1405 train_time:183498ms step_avg:142.47ms
step:1299/1405 train_time:183646ms step_avg:142.47ms
step:1300/1405 train_time:183795ms step_avg:142.48ms
step:1301/1405 train_time:183943ms step_avg:142.48ms
step:1302/1405 train_time:184091ms step_avg:142.49ms
step:1303/1405 train_time:184241ms step_avg:142.49ms
step:1304/1405 train_time:184390ms step_avg:142.50ms
step:1305/1405 train_time:184539ms step_avg:142.50ms
step:1306/1405 train_time:184688ms step_avg:142.51ms
step:1307/1405 train_time:184836ms step_avg:142.51ms
step:1308/1405 train_time:184985ms step_avg:142.52ms
step:1309/1405 train_time:185136ms step_avg:142.52ms
step:1310/1405 train_time:185283ms step_avg:142.53ms
step:1311/1405 train_time:185432ms step_avg:142.53ms
step:1312/1405 train_time:185580ms step_avg:142.53ms
step:1313/1405 train_time:185728ms step_avg:142.54ms
step:1314/1405 train_time:185878ms step_avg:142.54ms
step:1315/1405 train_time:186026ms step_avg:142.55ms
step:1316/1405 train_time:186176ms step_avg:142.55ms
step:1317/1405 train_time:186323ms step_avg:142.56ms
step:1318/1405 train_time:186474ms step_avg:142.56ms
step:1319/1405 train_time:186623ms step_avg:142.57ms
step:1320/1405 train_time:186771ms step_avg:142.57ms
step:1321/1405 train_time:186920ms step_avg:142.58ms
step:1322/1405 train_time:187070ms step_avg:142.58ms
step:1323/1405 train_time:187219ms step_avg:142.59ms
step:1324/1405 train_time:187366ms step_avg:142.59ms
step:1325/1405 train_time:187516ms step_avg:142.60ms
step:1326/1405 train_time:187665ms step_avg:142.60ms
step:1327/1405 train_time:187814ms step_avg:142.61ms
step:1328/1405 train_time:187962ms step_avg:142.61ms
step:1329/1405 train_time:188116ms step_avg:142.62ms
step:1330/1405 train_time:188265ms step_avg:142.62ms
step:1331/1405 train_time:188460ms step_avg:142.66ms
step:1332/1405 train_time:188608ms step_avg:142.67ms
step:1333/1405 train_time:188757ms step_avg:142.67ms
step:1334/1405 train_time:188903ms step_avg:142.68ms
step:1335/1405 train_time:189050ms step_avg:142.68ms
step:1336/1405 train_time:189200ms step_avg:142.68ms
step:1337/1405 train_time:189349ms step_avg:142.69ms
step:1338/1405 train_time:189501ms step_avg:142.70ms
step:1339/1405 train_time:189649ms step_avg:142.70ms
step:1340/1405 train_time:189799ms step_avg:142.71ms
step:1341/1405 train_time:189946ms step_avg:142.71ms
step:1342/1405 train_time:190095ms step_avg:142.71ms
step:1343/1405 train_time:190242ms step_avg:142.72ms
step:1344/1405 train_time:190391ms step_avg:142.72ms
step:1345/1405 train_time:190539ms step_avg:142.73ms
step:1346/1405 train_time:190687ms step_avg:142.73ms
step:1347/1405 train_time:190836ms step_avg:142.73ms
step:1348/1405 train_time:190984ms step_avg:142.74ms
step:1349/1405 train_time:191133ms step_avg:142.74ms
step:1350/1405 train_time:191282ms step_avg:142.75ms
step:1351/1405 train_time:191431ms step_avg:142.75ms
step:1352/1405 train_time:191581ms step_avg:142.76ms
step:1353/1405 train_time:191730ms step_avg:142.76ms
step:1354/1405 train_time:191879ms step_avg:142.77ms
step:1355/1405 train_time:192027ms step_avg:142.77ms
step:1356/1405 train_time:192175ms step_avg:142.77ms
step:1357/1405 train_time:192325ms step_avg:142.78ms
step:1358/1405 train_time:192475ms step_avg:142.79ms
step:1359/1405 train_time:192623ms step_avg:142.79ms
step:1360/1405 train_time:192774ms step_avg:142.80ms
step:1361/1405 train_time:192925ms step_avg:142.80ms
step:1362/1405 train_time:193078ms step_avg:142.81ms
step:1363/1405 train_time:193229ms step_avg:142.81ms
step:1364/1405 train_time:193379ms step_avg:142.82ms
step:1365/1405 train_time:193527ms step_avg:142.82ms
step:1366/1405 train_time:193677ms step_avg:142.83ms
step:1367/1405 train_time:193827ms step_avg:142.83ms
step:1368/1405 train_time:193977ms step_avg:142.84ms
step:1369/1405 train_time:194131ms step_avg:142.85ms
step:1370/1405 train_time:194281ms step_avg:142.85ms
step:1371/1405 train_time:194431ms step_avg:142.86ms
step:1372/1405 train_time:194582ms step_avg:142.87ms
step:1373/1405 train_time:194732ms step_avg:142.87ms
step:1374/1405 train_time:194881ms step_avg:142.87ms
step:1375/1405 train_time:195029ms step_avg:142.88ms
step:1375/1405 val_loss:3.2805 train_time:195102ms step_avg:142.93ms
step:1376/1405 train_time:195181ms step_avg:142.88ms
step:1377/1405 train_time:195330ms step_avg:142.89ms
step:1378/1405 train_time:195480ms step_avg:142.89ms
step:1379/1405 train_time:195629ms step_avg:142.90ms
step:1380/1405 train_time:195780ms step_avg:142.91ms
step:1381/1405 train_time:195932ms step_avg:142.91ms
step:1382/1405 train_time:196083ms step_avg:142.92ms
step:1383/1405 train_time:196234ms step_avg:142.92ms
step:1384/1405 train_time:196385ms step_avg:142.93ms
step:1385/1405 train_time:196534ms step_avg:142.93ms
step:1386/1405 train_time:196682ms step_avg:142.94ms
step:1387/1405 train_time:196833ms step_avg:142.94ms
step:1388/1405 train_time:196982ms step_avg:142.95ms
step:1389/1405 train_time:197133ms step_avg:142.95ms
step:1390/1405 train_time:197282ms step_avg:142.96ms
step:1391/1405 train_time:197431ms step_avg:142.96ms
step:1392/1405 train_time:197579ms step_avg:142.97ms
step:1393/1405 train_time:197728ms step_avg:142.97ms
step:1394/1405 train_time:197879ms step_avg:142.98ms
step:1395/1405 train_time:198028ms step_avg:142.98ms
step:1396/1405 train_time:198179ms step_avg:142.99ms
step:1397/1405 train_time:198327ms step_avg:142.99ms
step:1398/1405 train_time:198477ms step_avg:142.99ms
step:1399/1405 train_time:198625ms step_avg:143.00ms
step:1400/1405 train_time:198778ms step_avg:143.01ms
step:1401/1405 train_time:198926ms step_avg:143.01ms
step:1402/1405 train_time:199077ms step_avg:143.02ms
step:1403/1405 train_time:199227ms step_avg:143.02ms
step:1404/1405 train_time:199377ms step_avg:143.03ms
step:1405/1405 train_time:199527ms step_avg:143.03ms
step:1405/1405 val_loss:3.2778 train_time:199600ms step_avg:143.08ms
peak memory consumption: 31567 MiB

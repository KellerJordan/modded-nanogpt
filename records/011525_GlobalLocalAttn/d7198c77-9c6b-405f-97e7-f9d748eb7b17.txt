import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: torch.Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            # indices = (~dense_mask).argsort(dim=-1, descending=False, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_masks(sliding_window_num_blocks: int):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            short_sliding_window_num_blocks = sliding_window_num_blocks // 2
            return (
                BlockMask.from_kv_blocks(
                    kv_num_blocks,
                    kv_indices,
                    full_kv_num_blocks,
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                ),
                BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(short_sliding_window_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, short_sliding_window_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                ),
            )

        # Long-short SWA block masks by @leloykun & @YouJiacheng
        long_swa_block_mask, short_swa_block_mask = create_doc_swc_block_masks(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        is_long_block_mask = [True, False, False, False, True, False]
        for i in range(self.num_encoder_layers):
            block_mask = long_swa_block_mask if is_long_block_mask[i] else short_swa_block_mask
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        is_long_block_mask = list(reversed(is_long_block_mask))
        for i in range(self.num_decoder_layers):
            block_mask = long_swa_block_mask if is_long_block_mask[i] else short_swa_block_mask
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1405 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Wed Jan 15 19:16:04 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1405 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1405 train_time:28552ms step_avg:nanms
step:2/1405 train_time:28711ms step_avg:nanms
step:3/1405 train_time:28903ms step_avg:nanms
step:4/1405 train_time:29035ms step_avg:nanms
step:5/1405 train_time:29167ms step_avg:nanms
step:6/1405 train_time:29300ms step_avg:nanms
step:7/1405 train_time:29432ms step_avg:nanms
step:8/1405 train_time:29565ms step_avg:nanms
step:9/1405 train_time:29696ms step_avg:nanms
step:10/1405 train_time:29834ms step_avg:nanms
step:11/1405 train_time:136ms step_avg:nanms
step:12/1405 train_time:271ms step_avg:nanms
step:13/1405 train_time:404ms step_avg:134.80ms
step:14/1405 train_time:538ms step_avg:134.54ms
step:15/1405 train_time:671ms step_avg:134.25ms
step:16/1405 train_time:804ms step_avg:133.97ms
step:17/1405 train_time:937ms step_avg:133.92ms
step:18/1405 train_time:1073ms step_avg:134.12ms
step:19/1405 train_time:1207ms step_avg:134.06ms
step:20/1405 train_time:1341ms step_avg:134.15ms
step:21/1405 train_time:1475ms step_avg:134.09ms
step:22/1405 train_time:1608ms step_avg:134.02ms
step:23/1405 train_time:1741ms step_avg:133.92ms
step:24/1405 train_time:1875ms step_avg:133.92ms
step:25/1405 train_time:2009ms step_avg:133.91ms
step:26/1405 train_time:2142ms step_avg:133.88ms
step:27/1405 train_time:2275ms step_avg:133.84ms
step:28/1405 train_time:2410ms step_avg:133.87ms
step:29/1405 train_time:2542ms step_avg:133.81ms
step:30/1405 train_time:2677ms step_avg:133.86ms
step:31/1405 train_time:2812ms step_avg:133.89ms
step:32/1405 train_time:2945ms step_avg:133.85ms
step:33/1405 train_time:3077ms step_avg:133.80ms
step:34/1405 train_time:3212ms step_avg:133.83ms
step:35/1405 train_time:3345ms step_avg:133.81ms
step:36/1405 train_time:3478ms step_avg:133.78ms
step:37/1405 train_time:3612ms step_avg:133.79ms
step:38/1405 train_time:3747ms step_avg:133.81ms
step:39/1405 train_time:3879ms step_avg:133.77ms
step:40/1405 train_time:4014ms step_avg:133.79ms
step:41/1405 train_time:4147ms step_avg:133.78ms
step:42/1405 train_time:4282ms step_avg:133.81ms
step:43/1405 train_time:4416ms step_avg:133.80ms
step:44/1405 train_time:4551ms step_avg:133.84ms
step:45/1405 train_time:4683ms step_avg:133.80ms
step:46/1405 train_time:4816ms step_avg:133.78ms
step:47/1405 train_time:4949ms step_avg:133.76ms
step:48/1405 train_time:5083ms step_avg:133.76ms
step:49/1405 train_time:5217ms step_avg:133.76ms
step:50/1405 train_time:5352ms step_avg:133.79ms
step:51/1405 train_time:5486ms step_avg:133.79ms
step:52/1405 train_time:5619ms step_avg:133.79ms
step:53/1405 train_time:5754ms step_avg:133.81ms
step:54/1405 train_time:5887ms step_avg:133.80ms
step:55/1405 train_time:6023ms step_avg:133.84ms
step:56/1405 train_time:6156ms step_avg:133.82ms
step:57/1405 train_time:6289ms step_avg:133.82ms
step:58/1405 train_time:6422ms step_avg:133.80ms
step:59/1405 train_time:6557ms step_avg:133.81ms
step:60/1405 train_time:6691ms step_avg:133.82ms
step:61/1405 train_time:6824ms step_avg:133.81ms
step:62/1405 train_time:6957ms step_avg:133.80ms
step:63/1405 train_time:7092ms step_avg:133.82ms
step:64/1405 train_time:7225ms step_avg:133.79ms
step:65/1405 train_time:7358ms step_avg:133.79ms
step:66/1405 train_time:7493ms step_avg:133.80ms
step:67/1405 train_time:7626ms step_avg:133.78ms
step:68/1405 train_time:7759ms step_avg:133.78ms
step:69/1405 train_time:7893ms step_avg:133.78ms
step:70/1405 train_time:8026ms step_avg:133.77ms
step:71/1405 train_time:8160ms step_avg:133.76ms
step:72/1405 train_time:8293ms step_avg:133.76ms
step:73/1405 train_time:8426ms step_avg:133.75ms
step:74/1405 train_time:8559ms step_avg:133.74ms
step:75/1405 train_time:8693ms step_avg:133.74ms
step:76/1405 train_time:8827ms step_avg:133.75ms
step:77/1405 train_time:8962ms step_avg:133.75ms
step:78/1405 train_time:9095ms step_avg:133.75ms
step:79/1405 train_time:9228ms step_avg:133.74ms
step:80/1405 train_time:9361ms step_avg:133.73ms
step:81/1405 train_time:9495ms step_avg:133.73ms
step:82/1405 train_time:9630ms step_avg:133.75ms
step:83/1405 train_time:9763ms step_avg:133.74ms
step:84/1405 train_time:9896ms step_avg:133.73ms
step:85/1405 train_time:10030ms step_avg:133.74ms
step:86/1405 train_time:10163ms step_avg:133.73ms
step:87/1405 train_time:10297ms step_avg:133.73ms
step:88/1405 train_time:10431ms step_avg:133.72ms
step:89/1405 train_time:10564ms step_avg:133.72ms
step:90/1405 train_time:10698ms step_avg:133.72ms
step:91/1405 train_time:10832ms step_avg:133.73ms
step:92/1405 train_time:10966ms step_avg:133.73ms
step:93/1405 train_time:11099ms step_avg:133.72ms
step:94/1405 train_time:11233ms step_avg:133.73ms
step:95/1405 train_time:11367ms step_avg:133.73ms
step:96/1405 train_time:11502ms step_avg:133.74ms
step:97/1405 train_time:11636ms step_avg:133.74ms
step:98/1405 train_time:11770ms step_avg:133.75ms
step:99/1405 train_time:11904ms step_avg:133.76ms
step:100/1405 train_time:12038ms step_avg:133.75ms
step:101/1405 train_time:12172ms step_avg:133.76ms
step:102/1405 train_time:12306ms step_avg:133.76ms
step:103/1405 train_time:12441ms step_avg:133.77ms
step:104/1405 train_time:12575ms step_avg:133.77ms
step:105/1405 train_time:12709ms step_avg:133.78ms
step:106/1405 train_time:12844ms step_avg:133.79ms
step:107/1405 train_time:12978ms step_avg:133.80ms
step:108/1405 train_time:13114ms step_avg:133.82ms
step:109/1405 train_time:13248ms step_avg:133.82ms
step:110/1405 train_time:13384ms step_avg:133.84ms
step:111/1405 train_time:13517ms step_avg:133.83ms
step:112/1405 train_time:13652ms step_avg:133.85ms
step:113/1405 train_time:13786ms step_avg:133.85ms
step:114/1405 train_time:13922ms step_avg:133.87ms
step:115/1405 train_time:14057ms step_avg:133.87ms
step:116/1405 train_time:14192ms step_avg:133.88ms
step:117/1405 train_time:14326ms step_avg:133.89ms
step:118/1405 train_time:14459ms step_avg:133.88ms
step:119/1405 train_time:14594ms step_avg:133.89ms
step:120/1405 train_time:14729ms step_avg:133.90ms
step:121/1405 train_time:14864ms step_avg:133.91ms
step:122/1405 train_time:15001ms step_avg:133.93ms
step:123/1405 train_time:15135ms step_avg:133.94ms
step:124/1405 train_time:15270ms step_avg:133.95ms
step:125/1405 train_time:15404ms step_avg:133.95ms
step:125/1405 val_loss:4.4363 train_time:15469ms step_avg:134.51ms
step:126/1405 train_time:15544ms step_avg:134.00ms
step:127/1405 train_time:15681ms step_avg:134.02ms
step:128/1405 train_time:15817ms step_avg:134.04ms
step:129/1405 train_time:15951ms step_avg:134.04ms
step:130/1405 train_time:16085ms step_avg:134.05ms
step:131/1405 train_time:16219ms step_avg:134.04ms
step:132/1405 train_time:16354ms step_avg:134.05ms
step:133/1405 train_time:16488ms step_avg:134.05ms
step:134/1405 train_time:16624ms step_avg:134.06ms
step:135/1405 train_time:16760ms step_avg:134.08ms
step:136/1405 train_time:16896ms step_avg:134.10ms
step:137/1405 train_time:17031ms step_avg:134.10ms
step:138/1405 train_time:17167ms step_avg:134.12ms
step:139/1405 train_time:17301ms step_avg:134.12ms
step:140/1405 train_time:17435ms step_avg:134.11ms
step:141/1405 train_time:17571ms step_avg:134.13ms
step:142/1405 train_time:17707ms step_avg:134.15ms
step:143/1405 train_time:17843ms step_avg:134.16ms
step:144/1405 train_time:17979ms step_avg:134.17ms
step:145/1405 train_time:18114ms step_avg:134.17ms
step:146/1405 train_time:18248ms step_avg:134.18ms
step:147/1405 train_time:18383ms step_avg:134.19ms
step:148/1405 train_time:18518ms step_avg:134.19ms
step:149/1405 train_time:18653ms step_avg:134.19ms
step:150/1405 train_time:18789ms step_avg:134.21ms
step:151/1405 train_time:18924ms step_avg:134.21ms
step:152/1405 train_time:19060ms step_avg:134.23ms
step:153/1405 train_time:19195ms step_avg:134.23ms
step:154/1405 train_time:19329ms step_avg:134.23ms
step:155/1405 train_time:19465ms step_avg:134.24ms
step:156/1405 train_time:19600ms step_avg:134.25ms
step:157/1405 train_time:19736ms step_avg:134.26ms
step:158/1405 train_time:19871ms step_avg:134.26ms
step:159/1405 train_time:20008ms step_avg:134.28ms
step:160/1405 train_time:20144ms step_avg:134.29ms
step:161/1405 train_time:20281ms step_avg:134.31ms
step:162/1405 train_time:20416ms step_avg:134.31ms
step:163/1405 train_time:20551ms step_avg:134.32ms
step:164/1405 train_time:20687ms step_avg:134.33ms
step:165/1405 train_time:20823ms step_avg:134.34ms
step:166/1405 train_time:20958ms step_avg:134.35ms
step:167/1405 train_time:21094ms step_avg:134.36ms
step:168/1405 train_time:21230ms step_avg:134.36ms
step:169/1405 train_time:21366ms step_avg:134.38ms
step:170/1405 train_time:21502ms step_avg:134.39ms
step:171/1405 train_time:21637ms step_avg:134.39ms
step:172/1405 train_time:21772ms step_avg:134.40ms
step:173/1405 train_time:21908ms step_avg:134.40ms
step:174/1405 train_time:22043ms step_avg:134.41ms
step:175/1405 train_time:22179ms step_avg:134.42ms
step:176/1405 train_time:22313ms step_avg:134.42ms
step:177/1405 train_time:22449ms step_avg:134.43ms
step:178/1405 train_time:22584ms step_avg:134.43ms
step:179/1405 train_time:22719ms step_avg:134.43ms
step:180/1405 train_time:22855ms step_avg:134.44ms
step:181/1405 train_time:22991ms step_avg:134.45ms
step:182/1405 train_time:23126ms step_avg:134.46ms
step:183/1405 train_time:23262ms step_avg:134.46ms
step:184/1405 train_time:23397ms step_avg:134.47ms
step:185/1405 train_time:23533ms step_avg:134.47ms
step:186/1405 train_time:23669ms step_avg:134.48ms
step:187/1405 train_time:23804ms step_avg:134.49ms
step:188/1405 train_time:23939ms step_avg:134.49ms
step:189/1405 train_time:24076ms step_avg:134.50ms
step:190/1405 train_time:24211ms step_avg:134.50ms
step:191/1405 train_time:24398ms step_avg:134.79ms
step:192/1405 train_time:24532ms step_avg:134.79ms
step:193/1405 train_time:24667ms step_avg:134.79ms
step:194/1405 train_time:24801ms step_avg:134.79ms
step:195/1405 train_time:24936ms step_avg:134.79ms
step:196/1405 train_time:25070ms step_avg:134.79ms
step:197/1405 train_time:25206ms step_avg:134.79ms
step:198/1405 train_time:25345ms step_avg:134.81ms
step:199/1405 train_time:25482ms step_avg:134.83ms
step:200/1405 train_time:25617ms step_avg:134.83ms
step:201/1405 train_time:25751ms step_avg:134.82ms
step:202/1405 train_time:25887ms step_avg:134.83ms
step:203/1405 train_time:26022ms step_avg:134.83ms
step:204/1405 train_time:26157ms step_avg:134.83ms
step:205/1405 train_time:26293ms step_avg:134.84ms
step:206/1405 train_time:26430ms step_avg:134.85ms
step:207/1405 train_time:26567ms step_avg:134.86ms
step:208/1405 train_time:26701ms step_avg:134.86ms
step:209/1405 train_time:26836ms step_avg:134.86ms
step:210/1405 train_time:26972ms step_avg:134.86ms
step:211/1405 train_time:27108ms step_avg:134.87ms
step:212/1405 train_time:27244ms step_avg:134.87ms
step:213/1405 train_time:27381ms step_avg:134.88ms
step:214/1405 train_time:27517ms step_avg:134.89ms
step:215/1405 train_time:27653ms step_avg:134.89ms
step:216/1405 train_time:27792ms step_avg:134.91ms
step:217/1405 train_time:27928ms step_avg:134.92ms
step:218/1405 train_time:28063ms step_avg:134.92ms
step:219/1405 train_time:28199ms step_avg:134.92ms
step:220/1405 train_time:28334ms step_avg:134.92ms
step:221/1405 train_time:28471ms step_avg:134.93ms
step:222/1405 train_time:28608ms step_avg:134.94ms
step:223/1405 train_time:28745ms step_avg:134.95ms
step:224/1405 train_time:28881ms step_avg:134.96ms
step:225/1405 train_time:29018ms step_avg:134.97ms
step:226/1405 train_time:29154ms step_avg:134.97ms
step:227/1405 train_time:29291ms step_avg:134.98ms
step:228/1405 train_time:29428ms step_avg:134.99ms
step:229/1405 train_time:29564ms step_avg:135.00ms
step:230/1405 train_time:29701ms step_avg:135.00ms
step:231/1405 train_time:29838ms step_avg:135.01ms
step:232/1405 train_time:29975ms step_avg:135.02ms
step:233/1405 train_time:30112ms step_avg:135.03ms
step:234/1405 train_time:30248ms step_avg:135.04ms
step:235/1405 train_time:30385ms step_avg:135.04ms
step:236/1405 train_time:30521ms step_avg:135.05ms
step:237/1405 train_time:30657ms step_avg:135.05ms
step:238/1405 train_time:30793ms step_avg:135.06ms
step:239/1405 train_time:30930ms step_avg:135.07ms
step:240/1405 train_time:31068ms step_avg:135.08ms
step:241/1405 train_time:31204ms step_avg:135.08ms
step:242/1405 train_time:31339ms step_avg:135.08ms
step:243/1405 train_time:31477ms step_avg:135.09ms
step:244/1405 train_time:31614ms step_avg:135.10ms
step:245/1405 train_time:31750ms step_avg:135.11ms
step:246/1405 train_time:31887ms step_avg:135.12ms
step:247/1405 train_time:32024ms step_avg:135.12ms
step:248/1405 train_time:32160ms step_avg:135.12ms
step:249/1405 train_time:32296ms step_avg:135.13ms
step:250/1405 train_time:32432ms step_avg:135.13ms
step:250/1405 val_loss:3.9945 train_time:32497ms step_avg:135.40ms
step:251/1405 train_time:32573ms step_avg:135.16ms
step:252/1405 train_time:32713ms step_avg:135.18ms
step:253/1405 train_time:32851ms step_avg:135.19ms
step:254/1405 train_time:32986ms step_avg:135.19ms
step:255/1405 train_time:33122ms step_avg:135.19ms
step:256/1405 train_time:33258ms step_avg:135.20ms
step:257/1405 train_time:33394ms step_avg:135.20ms
step:258/1405 train_time:33530ms step_avg:135.20ms
step:259/1405 train_time:33669ms step_avg:135.22ms
step:260/1405 train_time:33808ms step_avg:135.23ms
step:261/1405 train_time:33944ms step_avg:135.24ms
step:262/1405 train_time:34079ms step_avg:135.24ms
step:263/1405 train_time:34216ms step_avg:135.24ms
step:264/1405 train_time:34352ms step_avg:135.25ms
step:265/1405 train_time:34489ms step_avg:135.25ms
step:266/1405 train_time:34626ms step_avg:135.26ms
step:267/1405 train_time:34764ms step_avg:135.27ms
step:268/1405 train_time:34900ms step_avg:135.27ms
step:269/1405 train_time:35038ms step_avg:135.28ms
step:270/1405 train_time:35175ms step_avg:135.29ms
step:271/1405 train_time:35311ms step_avg:135.29ms
step:272/1405 train_time:35448ms step_avg:135.30ms
step:273/1405 train_time:35584ms step_avg:135.30ms
step:274/1405 train_time:35721ms step_avg:135.31ms
step:275/1405 train_time:35858ms step_avg:135.31ms
step:276/1405 train_time:35995ms step_avg:135.32ms
step:277/1405 train_time:36132ms step_avg:135.33ms
step:278/1405 train_time:36270ms step_avg:135.33ms
step:279/1405 train_time:36406ms step_avg:135.34ms
step:280/1405 train_time:36542ms step_avg:135.34ms
step:281/1405 train_time:36679ms step_avg:135.35ms
step:282/1405 train_time:36816ms step_avg:135.35ms
step:283/1405 train_time:36953ms step_avg:135.36ms
step:284/1405 train_time:37090ms step_avg:135.36ms
step:285/1405 train_time:37227ms step_avg:135.37ms
step:286/1405 train_time:37363ms step_avg:135.37ms
step:287/1405 train_time:37500ms step_avg:135.38ms
step:288/1405 train_time:37636ms step_avg:135.38ms
step:289/1405 train_time:37773ms step_avg:135.39ms
step:290/1405 train_time:37909ms step_avg:135.39ms
step:291/1405 train_time:38048ms step_avg:135.40ms
step:292/1405 train_time:38183ms step_avg:135.40ms
step:293/1405 train_time:38319ms step_avg:135.40ms
step:294/1405 train_time:38456ms step_avg:135.41ms
step:295/1405 train_time:38593ms step_avg:135.41ms
step:296/1405 train_time:38731ms step_avg:135.42ms
step:297/1405 train_time:38868ms step_avg:135.43ms
step:298/1405 train_time:39004ms step_avg:135.43ms
step:299/1405 train_time:39140ms step_avg:135.43ms
step:300/1405 train_time:39278ms step_avg:135.44ms
step:301/1405 train_time:39414ms step_avg:135.44ms
step:302/1405 train_time:39552ms step_avg:135.45ms
step:303/1405 train_time:39689ms step_avg:135.46ms
step:304/1405 train_time:39826ms step_avg:135.46ms
step:305/1405 train_time:39962ms step_avg:135.46ms
step:306/1405 train_time:40100ms step_avg:135.47ms
step:307/1405 train_time:40236ms step_avg:135.48ms
step:308/1405 train_time:40373ms step_avg:135.48ms
step:309/1405 train_time:40510ms step_avg:135.48ms
step:310/1405 train_time:40647ms step_avg:135.49ms
step:311/1405 train_time:40782ms step_avg:135.49ms
step:312/1405 train_time:40919ms step_avg:135.49ms
step:313/1405 train_time:41056ms step_avg:135.50ms
step:314/1405 train_time:41193ms step_avg:135.50ms
step:315/1405 train_time:41333ms step_avg:135.52ms
step:316/1405 train_time:41472ms step_avg:135.53ms
step:317/1405 train_time:41613ms step_avg:135.55ms
step:318/1405 train_time:41752ms step_avg:135.56ms
step:319/1405 train_time:41891ms step_avg:135.57ms
step:320/1405 train_time:42030ms step_avg:135.58ms
step:321/1405 train_time:42169ms step_avg:135.59ms
step:322/1405 train_time:42309ms step_avg:135.60ms
step:323/1405 train_time:42447ms step_avg:135.61ms
step:324/1405 train_time:42588ms step_avg:135.63ms
step:325/1405 train_time:42726ms step_avg:135.64ms
step:326/1405 train_time:42867ms step_avg:135.65ms
step:327/1405 train_time:43006ms step_avg:135.67ms
step:328/1405 train_time:43146ms step_avg:135.68ms
step:329/1405 train_time:43284ms step_avg:135.69ms
step:330/1405 train_time:43425ms step_avg:135.70ms
step:331/1405 train_time:43564ms step_avg:135.71ms
step:332/1405 train_time:43704ms step_avg:135.73ms
step:333/1405 train_time:43843ms step_avg:135.74ms
step:334/1405 train_time:43982ms step_avg:135.75ms
step:335/1405 train_time:44123ms step_avg:135.76ms
step:336/1405 train_time:44262ms step_avg:135.77ms
step:337/1405 train_time:44401ms step_avg:135.78ms
step:338/1405 train_time:44543ms step_avg:135.80ms
step:339/1405 train_time:44682ms step_avg:135.81ms
step:340/1405 train_time:44823ms step_avg:135.83ms
step:341/1405 train_time:44962ms step_avg:135.84ms
step:342/1405 train_time:45102ms step_avg:135.85ms
step:343/1405 train_time:45242ms step_avg:135.86ms
step:344/1405 train_time:45381ms step_avg:135.87ms
step:345/1405 train_time:45522ms step_avg:135.89ms
step:346/1405 train_time:45661ms step_avg:135.90ms
step:347/1405 train_time:45800ms step_avg:135.91ms
step:348/1405 train_time:45943ms step_avg:135.93ms
step:349/1405 train_time:46082ms step_avg:135.93ms
step:350/1405 train_time:46222ms step_avg:135.95ms
step:351/1405 train_time:46365ms step_avg:135.97ms
step:352/1405 train_time:46503ms step_avg:135.97ms
step:353/1405 train_time:46645ms step_avg:135.99ms
step:354/1405 train_time:46784ms step_avg:136.00ms
step:355/1405 train_time:46925ms step_avg:136.01ms
step:356/1405 train_time:47064ms step_avg:136.02ms
step:357/1405 train_time:47205ms step_avg:136.04ms
step:358/1405 train_time:47345ms step_avg:136.05ms
step:359/1405 train_time:47485ms step_avg:136.06ms
step:360/1405 train_time:47626ms step_avg:136.07ms
step:361/1405 train_time:47765ms step_avg:136.08ms
step:362/1405 train_time:47904ms step_avg:136.09ms
step:363/1405 train_time:48045ms step_avg:136.11ms
step:364/1405 train_time:48185ms step_avg:136.12ms
step:365/1405 train_time:48327ms step_avg:136.13ms
step:366/1405 train_time:48465ms step_avg:136.14ms
step:367/1405 train_time:48606ms step_avg:136.15ms
step:368/1405 train_time:48746ms step_avg:136.16ms
step:369/1405 train_time:48885ms step_avg:136.17ms
step:370/1405 train_time:49024ms step_avg:136.18ms
step:371/1405 train_time:49163ms step_avg:136.19ms
step:372/1405 train_time:49304ms step_avg:136.20ms
step:373/1405 train_time:49445ms step_avg:136.21ms
step:374/1405 train_time:49583ms step_avg:136.22ms
step:375/1405 train_time:49723ms step_avg:136.23ms
step:375/1405 val_loss:3.7841 train_time:49790ms step_avg:136.41ms
step:376/1405 train_time:49866ms step_avg:136.25ms
step:377/1405 train_time:50006ms step_avg:136.26ms
step:378/1405 train_time:50145ms step_avg:136.26ms
step:379/1405 train_time:50284ms step_avg:136.27ms
step:380/1405 train_time:50423ms step_avg:136.28ms
step:381/1405 train_time:50602ms step_avg:136.39ms
step:382/1405 train_time:50740ms step_avg:136.40ms
step:383/1405 train_time:50879ms step_avg:136.40ms
step:384/1405 train_time:51017ms step_avg:136.41ms
step:385/1405 train_time:51156ms step_avg:136.41ms
step:386/1405 train_time:51294ms step_avg:136.42ms
step:387/1405 train_time:51434ms step_avg:136.43ms
step:388/1405 train_time:51578ms step_avg:136.45ms
step:389/1405 train_time:51717ms step_avg:136.46ms
step:390/1405 train_time:51856ms step_avg:136.46ms
step:391/1405 train_time:51994ms step_avg:136.47ms
step:392/1405 train_time:52133ms step_avg:136.47ms
step:393/1405 train_time:52271ms step_avg:136.48ms
step:394/1405 train_time:52411ms step_avg:136.49ms
step:395/1405 train_time:52551ms step_avg:136.50ms
step:396/1405 train_time:52692ms step_avg:136.51ms
step:397/1405 train_time:52830ms step_avg:136.51ms
step:398/1405 train_time:52968ms step_avg:136.52ms
step:399/1405 train_time:53107ms step_avg:136.52ms
step:400/1405 train_time:53245ms step_avg:136.52ms
step:401/1405 train_time:53384ms step_avg:136.53ms
step:402/1405 train_time:53524ms step_avg:136.54ms
step:403/1405 train_time:53664ms step_avg:136.55ms
step:404/1405 train_time:53802ms step_avg:136.55ms
step:405/1405 train_time:53941ms step_avg:136.56ms
step:406/1405 train_time:54080ms step_avg:136.56ms
step:407/1405 train_time:54219ms step_avg:136.57ms
step:408/1405 train_time:54358ms step_avg:136.58ms
step:409/1405 train_time:54497ms step_avg:136.59ms
step:410/1405 train_time:54637ms step_avg:136.59ms
step:411/1405 train_time:54776ms step_avg:136.60ms
step:412/1405 train_time:54916ms step_avg:136.61ms
step:413/1405 train_time:55056ms step_avg:136.61ms
step:414/1405 train_time:55196ms step_avg:136.62ms
step:415/1405 train_time:55335ms step_avg:136.63ms
step:416/1405 train_time:55474ms step_avg:136.64ms
step:417/1405 train_time:55613ms step_avg:136.64ms
step:418/1405 train_time:55752ms step_avg:136.65ms
step:419/1405 train_time:55892ms step_avg:136.65ms
step:420/1405 train_time:56032ms step_avg:136.66ms
step:421/1405 train_time:56173ms step_avg:136.67ms
step:422/1405 train_time:56312ms step_avg:136.68ms
step:423/1405 train_time:56452ms step_avg:136.69ms
step:424/1405 train_time:56592ms step_avg:136.70ms
step:425/1405 train_time:56732ms step_avg:136.70ms
step:426/1405 train_time:56872ms step_avg:136.71ms
step:427/1405 train_time:57011ms step_avg:136.72ms
step:428/1405 train_time:57151ms step_avg:136.72ms
step:429/1405 train_time:57289ms step_avg:136.73ms
step:430/1405 train_time:57429ms step_avg:136.74ms
step:431/1405 train_time:57568ms step_avg:136.74ms
step:432/1405 train_time:57708ms step_avg:136.75ms
step:433/1405 train_time:57848ms step_avg:136.76ms
step:434/1405 train_time:57988ms step_avg:136.76ms
step:435/1405 train_time:58126ms step_avg:136.77ms
step:436/1405 train_time:58266ms step_avg:136.78ms
step:437/1405 train_time:58406ms step_avg:136.78ms
step:438/1405 train_time:58546ms step_avg:136.79ms
step:439/1405 train_time:58686ms step_avg:136.80ms
step:440/1405 train_time:58825ms step_avg:136.80ms
step:441/1405 train_time:58965ms step_avg:136.81ms
step:442/1405 train_time:59105ms step_avg:136.82ms
step:443/1405 train_time:59246ms step_avg:136.83ms
step:444/1405 train_time:59384ms step_avg:136.83ms
step:445/1405 train_time:59525ms step_avg:136.84ms
step:446/1405 train_time:59664ms step_avg:136.84ms
step:447/1405 train_time:59804ms step_avg:136.85ms
step:448/1405 train_time:59945ms step_avg:136.86ms
step:449/1405 train_time:60085ms step_avg:136.87ms
step:450/1405 train_time:60225ms step_avg:136.88ms
step:451/1405 train_time:60365ms step_avg:136.88ms
step:452/1405 train_time:60505ms step_avg:136.89ms
step:453/1405 train_time:60647ms step_avg:136.90ms
step:454/1405 train_time:60786ms step_avg:136.91ms
step:455/1405 train_time:60925ms step_avg:136.91ms
step:456/1405 train_time:61065ms step_avg:136.92ms
step:457/1405 train_time:61204ms step_avg:136.92ms
step:458/1405 train_time:61344ms step_avg:136.93ms
step:459/1405 train_time:61484ms step_avg:136.94ms
step:460/1405 train_time:61624ms step_avg:136.94ms
step:461/1405 train_time:61764ms step_avg:136.95ms
step:462/1405 train_time:61904ms step_avg:136.96ms
step:463/1405 train_time:62043ms step_avg:136.96ms
step:464/1405 train_time:62184ms step_avg:136.97ms
step:465/1405 train_time:62322ms step_avg:136.97ms
step:466/1405 train_time:62462ms step_avg:136.98ms
step:467/1405 train_time:62601ms step_avg:136.98ms
step:468/1405 train_time:62742ms step_avg:136.99ms
step:469/1405 train_time:62882ms step_avg:137.00ms
step:470/1405 train_time:63021ms step_avg:137.00ms
step:471/1405 train_time:63162ms step_avg:137.01ms
step:472/1405 train_time:63301ms step_avg:137.02ms
step:473/1405 train_time:63442ms step_avg:137.02ms
step:474/1405 train_time:63582ms step_avg:137.03ms
step:475/1405 train_time:63722ms step_avg:137.04ms
step:476/1405 train_time:63861ms step_avg:137.04ms
step:477/1405 train_time:64001ms step_avg:137.05ms
step:478/1405 train_time:64141ms step_avg:137.05ms
step:479/1405 train_time:64283ms step_avg:137.06ms
step:480/1405 train_time:64423ms step_avg:137.07ms
step:481/1405 train_time:64563ms step_avg:137.08ms
step:482/1405 train_time:64702ms step_avg:137.08ms
step:483/1405 train_time:64842ms step_avg:137.09ms
step:484/1405 train_time:64981ms step_avg:137.09ms
step:485/1405 train_time:65121ms step_avg:137.10ms
step:486/1405 train_time:65260ms step_avg:137.10ms
step:487/1405 train_time:65400ms step_avg:137.11ms
step:488/1405 train_time:65539ms step_avg:137.11ms
step:489/1405 train_time:65679ms step_avg:137.12ms
step:490/1405 train_time:65818ms step_avg:137.12ms
step:491/1405 train_time:65957ms step_avg:137.12ms
step:492/1405 train_time:66097ms step_avg:137.13ms
step:493/1405 train_time:66237ms step_avg:137.14ms
step:494/1405 train_time:66377ms step_avg:137.14ms
step:495/1405 train_time:66517ms step_avg:137.15ms
step:496/1405 train_time:66657ms step_avg:137.15ms
step:497/1405 train_time:66797ms step_avg:137.16ms
step:498/1405 train_time:66937ms step_avg:137.17ms
step:499/1405 train_time:67076ms step_avg:137.17ms
step:500/1405 train_time:67216ms step_avg:137.18ms
step:500/1405 val_loss:3.6633 train_time:67282ms step_avg:137.31ms
step:501/1405 train_time:67359ms step_avg:137.19ms
step:502/1405 train_time:67499ms step_avg:137.19ms
step:503/1405 train_time:67639ms step_avg:137.20ms
step:504/1405 train_time:67778ms step_avg:137.20ms
step:505/1405 train_time:67916ms step_avg:137.20ms
step:506/1405 train_time:68055ms step_avg:137.21ms
step:507/1405 train_time:68194ms step_avg:137.21ms
step:508/1405 train_time:68337ms step_avg:137.22ms
step:509/1405 train_time:68477ms step_avg:137.23ms
step:510/1405 train_time:68618ms step_avg:137.24ms
step:511/1405 train_time:68759ms step_avg:137.24ms
step:512/1405 train_time:68898ms step_avg:137.25ms
step:513/1405 train_time:69038ms step_avg:137.25ms
step:514/1405 train_time:69178ms step_avg:137.26ms
step:515/1405 train_time:69317ms step_avg:137.26ms
step:516/1405 train_time:69458ms step_avg:137.27ms
step:517/1405 train_time:69598ms step_avg:137.27ms
step:518/1405 train_time:69739ms step_avg:137.28ms
step:519/1405 train_time:69879ms step_avg:137.29ms
step:520/1405 train_time:70018ms step_avg:137.29ms
step:521/1405 train_time:70158ms step_avg:137.29ms
step:522/1405 train_time:70298ms step_avg:137.30ms
step:523/1405 train_time:70440ms step_avg:137.31ms
step:524/1405 train_time:70581ms step_avg:137.32ms
step:525/1405 train_time:70724ms step_avg:137.33ms
step:526/1405 train_time:70867ms step_avg:137.34ms
step:527/1405 train_time:71009ms step_avg:137.35ms
step:528/1405 train_time:71151ms step_avg:137.36ms
step:529/1405 train_time:71293ms step_avg:137.37ms
step:530/1405 train_time:71435ms step_avg:137.37ms
step:531/1405 train_time:71577ms step_avg:137.38ms
step:532/1405 train_time:71720ms step_avg:137.39ms
step:533/1405 train_time:71862ms step_avg:137.40ms
step:534/1405 train_time:72004ms step_avg:137.41ms
step:535/1405 train_time:72146ms step_avg:137.42ms
step:536/1405 train_time:72286ms step_avg:137.43ms
step:537/1405 train_time:72429ms step_avg:137.44ms
step:538/1405 train_time:72570ms step_avg:137.44ms
step:539/1405 train_time:72712ms step_avg:137.45ms
step:540/1405 train_time:72855ms step_avg:137.46ms
step:541/1405 train_time:72998ms step_avg:137.47ms
step:542/1405 train_time:73140ms step_avg:137.48ms
step:543/1405 train_time:73281ms step_avg:137.49ms
step:544/1405 train_time:73423ms step_avg:137.50ms
step:545/1405 train_time:73564ms step_avg:137.50ms
step:546/1405 train_time:73706ms step_avg:137.51ms
step:547/1405 train_time:73848ms step_avg:137.52ms
step:548/1405 train_time:73990ms step_avg:137.53ms
step:549/1405 train_time:74132ms step_avg:137.54ms
step:550/1405 train_time:74274ms step_avg:137.54ms
step:551/1405 train_time:74416ms step_avg:137.55ms
step:552/1405 train_time:74557ms step_avg:137.56ms
step:553/1405 train_time:74699ms step_avg:137.57ms
step:554/1405 train_time:74841ms step_avg:137.58ms
step:555/1405 train_time:74983ms step_avg:137.58ms
step:556/1405 train_time:75127ms step_avg:137.60ms
step:557/1405 train_time:75270ms step_avg:137.60ms
step:558/1405 train_time:75412ms step_avg:137.61ms
step:559/1405 train_time:75552ms step_avg:137.62ms
step:560/1405 train_time:75694ms step_avg:137.62ms
step:561/1405 train_time:75836ms step_avg:137.63ms
step:562/1405 train_time:75978ms step_avg:137.64ms
step:563/1405 train_time:76119ms step_avg:137.65ms
step:564/1405 train_time:76261ms step_avg:137.66ms
step:565/1405 train_time:76402ms step_avg:137.66ms
step:566/1405 train_time:76544ms step_avg:137.67ms
step:567/1405 train_time:76685ms step_avg:137.68ms
step:568/1405 train_time:76827ms step_avg:137.68ms
step:569/1405 train_time:76969ms step_avg:137.69ms
step:570/1405 train_time:77112ms step_avg:137.70ms
step:571/1405 train_time:77305ms step_avg:137.80ms
step:572/1405 train_time:77446ms step_avg:137.80ms
step:573/1405 train_time:77586ms step_avg:137.81ms
step:574/1405 train_time:77727ms step_avg:137.81ms
step:575/1405 train_time:77868ms step_avg:137.82ms
step:576/1405 train_time:78010ms step_avg:137.83ms
step:577/1405 train_time:78154ms step_avg:137.84ms
step:578/1405 train_time:78297ms step_avg:137.85ms
step:579/1405 train_time:78441ms step_avg:137.86ms
step:580/1405 train_time:78583ms step_avg:137.87ms
step:581/1405 train_time:78724ms step_avg:137.87ms
step:582/1405 train_time:78865ms step_avg:137.88ms
step:583/1405 train_time:79007ms step_avg:137.88ms
step:584/1405 train_time:79149ms step_avg:137.89ms
step:585/1405 train_time:79291ms step_avg:137.90ms
step:586/1405 train_time:79435ms step_avg:137.91ms
step:587/1405 train_time:79577ms step_avg:137.92ms
step:588/1405 train_time:79719ms step_avg:137.92ms
step:589/1405 train_time:79860ms step_avg:137.93ms
step:590/1405 train_time:80000ms step_avg:137.93ms
step:591/1405 train_time:80142ms step_avg:137.94ms
step:592/1405 train_time:80285ms step_avg:137.95ms
step:593/1405 train_time:80427ms step_avg:137.95ms
step:594/1405 train_time:80570ms step_avg:137.96ms
step:595/1405 train_time:80712ms step_avg:137.97ms
step:596/1405 train_time:80853ms step_avg:137.98ms
step:597/1405 train_time:80996ms step_avg:137.98ms
step:598/1405 train_time:81137ms step_avg:137.99ms
step:599/1405 train_time:81279ms step_avg:137.99ms
step:600/1405 train_time:81421ms step_avg:138.00ms
step:601/1405 train_time:81562ms step_avg:138.01ms
step:602/1405 train_time:81703ms step_avg:138.01ms
step:603/1405 train_time:81846ms step_avg:138.02ms
step:604/1405 train_time:81988ms step_avg:138.03ms
step:605/1405 train_time:82131ms step_avg:138.04ms
step:606/1405 train_time:82272ms step_avg:138.04ms
step:607/1405 train_time:82414ms step_avg:138.05ms
step:608/1405 train_time:82557ms step_avg:138.06ms
step:609/1405 train_time:82699ms step_avg:138.06ms
step:610/1405 train_time:82841ms step_avg:138.07ms
step:611/1405 train_time:82983ms step_avg:138.07ms
step:612/1405 train_time:83124ms step_avg:138.08ms
step:613/1405 train_time:83266ms step_avg:138.09ms
step:614/1405 train_time:83408ms step_avg:138.09ms
step:615/1405 train_time:83551ms step_avg:138.10ms
step:616/1405 train_time:83694ms step_avg:138.11ms
step:617/1405 train_time:83836ms step_avg:138.12ms
step:618/1405 train_time:83977ms step_avg:138.12ms
step:619/1405 train_time:84119ms step_avg:138.13ms
step:620/1405 train_time:84261ms step_avg:138.13ms
step:621/1405 train_time:84402ms step_avg:138.14ms
step:622/1405 train_time:84545ms step_avg:138.15ms
step:623/1405 train_time:84687ms step_avg:138.15ms
step:624/1405 train_time:84830ms step_avg:138.16ms
step:625/1405 train_time:84972ms step_avg:138.17ms
step:625/1405 val_loss:3.5822 train_time:85040ms step_avg:138.28ms
step:626/1405 train_time:85116ms step_avg:138.18ms
step:627/1405 train_time:85259ms step_avg:138.18ms
step:628/1405 train_time:85403ms step_avg:138.19ms
step:629/1405 train_time:85545ms step_avg:138.20ms
step:630/1405 train_time:85687ms step_avg:138.20ms
step:631/1405 train_time:85828ms step_avg:138.21ms
step:632/1405 train_time:85970ms step_avg:138.22ms
step:633/1405 train_time:86114ms step_avg:138.22ms
step:634/1405 train_time:86256ms step_avg:138.23ms
step:635/1405 train_time:86399ms step_avg:138.24ms
step:636/1405 train_time:86541ms step_avg:138.24ms
step:637/1405 train_time:86683ms step_avg:138.25ms
step:638/1405 train_time:86824ms step_avg:138.26ms
step:639/1405 train_time:86967ms step_avg:138.26ms
step:640/1405 train_time:87109ms step_avg:138.27ms
step:641/1405 train_time:87253ms step_avg:138.28ms
step:642/1405 train_time:87396ms step_avg:138.29ms
step:643/1405 train_time:87539ms step_avg:138.29ms
step:644/1405 train_time:87681ms step_avg:138.30ms
step:645/1405 train_time:87822ms step_avg:138.30ms
step:646/1405 train_time:87965ms step_avg:138.31ms
step:647/1405 train_time:88108ms step_avg:138.32ms
step:648/1405 train_time:88251ms step_avg:138.32ms
step:649/1405 train_time:88394ms step_avg:138.33ms
step:650/1405 train_time:88536ms step_avg:138.34ms
step:651/1405 train_time:88678ms step_avg:138.34ms
step:652/1405 train_time:88819ms step_avg:138.35ms
step:653/1405 train_time:88962ms step_avg:138.35ms
step:654/1405 train_time:89104ms step_avg:138.36ms
step:655/1405 train_time:89247ms step_avg:138.37ms
step:656/1405 train_time:89389ms step_avg:138.37ms
step:657/1405 train_time:89533ms step_avg:138.38ms
step:658/1405 train_time:89675ms step_avg:138.39ms
step:659/1405 train_time:89817ms step_avg:138.39ms
step:660/1405 train_time:89960ms step_avg:138.40ms
step:661/1405 train_time:90103ms step_avg:138.41ms
step:662/1405 train_time:90245ms step_avg:138.41ms
step:663/1405 train_time:90388ms step_avg:138.42ms
step:664/1405 train_time:90530ms step_avg:138.42ms
step:665/1405 train_time:90672ms step_avg:138.43ms
step:666/1405 train_time:90814ms step_avg:138.44ms
step:667/1405 train_time:90957ms step_avg:138.44ms
step:668/1405 train_time:91101ms step_avg:138.45ms
step:669/1405 train_time:91244ms step_avg:138.46ms
step:670/1405 train_time:91387ms step_avg:138.46ms
step:671/1405 train_time:91529ms step_avg:138.47ms
step:672/1405 train_time:91670ms step_avg:138.47ms
step:673/1405 train_time:91813ms step_avg:138.48ms
step:674/1405 train_time:91955ms step_avg:138.49ms
step:675/1405 train_time:92098ms step_avg:138.49ms
step:676/1405 train_time:92242ms step_avg:138.50ms
step:677/1405 train_time:92384ms step_avg:138.51ms
step:678/1405 train_time:92527ms step_avg:138.51ms
step:679/1405 train_time:92668ms step_avg:138.52ms
step:680/1405 train_time:92810ms step_avg:138.52ms
step:681/1405 train_time:92952ms step_avg:138.53ms
step:682/1405 train_time:93095ms step_avg:138.53ms
step:683/1405 train_time:93239ms step_avg:138.54ms
step:684/1405 train_time:93382ms step_avg:138.55ms
step:685/1405 train_time:93524ms step_avg:138.55ms
step:686/1405 train_time:93666ms step_avg:138.56ms
step:687/1405 train_time:93808ms step_avg:138.56ms
step:688/1405 train_time:93951ms step_avg:138.57ms
step:689/1405 train_time:94094ms step_avg:138.58ms
step:690/1405 train_time:94237ms step_avg:138.58ms
step:691/1405 train_time:94379ms step_avg:138.59ms
step:692/1405 train_time:94521ms step_avg:138.59ms
step:693/1405 train_time:94663ms step_avg:138.60ms
step:694/1405 train_time:94805ms step_avg:138.60ms
step:695/1405 train_time:94947ms step_avg:138.61ms
step:696/1405 train_time:95090ms step_avg:138.62ms
step:697/1405 train_time:95233ms step_avg:138.62ms
step:698/1405 train_time:95376ms step_avg:138.63ms
step:699/1405 train_time:95519ms step_avg:138.63ms
step:700/1405 train_time:95661ms step_avg:138.64ms
step:701/1405 train_time:95803ms step_avg:138.64ms
step:702/1405 train_time:95945ms step_avg:138.65ms
step:703/1405 train_time:96088ms step_avg:138.66ms
step:704/1405 train_time:96230ms step_avg:138.66ms
step:705/1405 train_time:96372ms step_avg:138.66ms
step:706/1405 train_time:96515ms step_avg:138.67ms
step:707/1405 train_time:96657ms step_avg:138.68ms
step:708/1405 train_time:96798ms step_avg:138.68ms
step:709/1405 train_time:96941ms step_avg:138.69ms
step:710/1405 train_time:97084ms step_avg:138.69ms
step:711/1405 train_time:97228ms step_avg:138.70ms
step:712/1405 train_time:97369ms step_avg:138.70ms
step:713/1405 train_time:97512ms step_avg:138.71ms
step:714/1405 train_time:97655ms step_avg:138.71ms
step:715/1405 train_time:97798ms step_avg:138.72ms
step:716/1405 train_time:97942ms step_avg:138.73ms
step:717/1405 train_time:98084ms step_avg:138.73ms
step:718/1405 train_time:98225ms step_avg:138.74ms
step:719/1405 train_time:98368ms step_avg:138.74ms
step:720/1405 train_time:98510ms step_avg:138.75ms
step:721/1405 train_time:98653ms step_avg:138.75ms
step:722/1405 train_time:98796ms step_avg:138.76ms
step:723/1405 train_time:98937ms step_avg:138.76ms
step:724/1405 train_time:99079ms step_avg:138.77ms
step:725/1405 train_time:99222ms step_avg:138.77ms
step:726/1405 train_time:99365ms step_avg:138.78ms
step:727/1405 train_time:99508ms step_avg:138.78ms
step:728/1405 train_time:99650ms step_avg:138.79ms
step:729/1405 train_time:99793ms step_avg:138.79ms
step:730/1405 train_time:99937ms step_avg:138.80ms
step:731/1405 train_time:100080ms step_avg:138.81ms
step:732/1405 train_time:100224ms step_avg:138.81ms
step:733/1405 train_time:100369ms step_avg:138.82ms
step:734/1405 train_time:100514ms step_avg:138.83ms
step:735/1405 train_time:100657ms step_avg:138.84ms
step:736/1405 train_time:100801ms step_avg:138.84ms
step:737/1405 train_time:100945ms step_avg:138.85ms
step:738/1405 train_time:101089ms step_avg:138.86ms
step:739/1405 train_time:101234ms step_avg:138.87ms
step:740/1405 train_time:101378ms step_avg:138.87ms
step:741/1405 train_time:101523ms step_avg:138.88ms
step:742/1405 train_time:101668ms step_avg:138.89ms
step:743/1405 train_time:101812ms step_avg:138.90ms
step:744/1405 train_time:101956ms step_avg:138.90ms
step:745/1405 train_time:102100ms step_avg:138.91ms
step:746/1405 train_time:102244ms step_avg:138.92ms
step:747/1405 train_time:102388ms step_avg:138.93ms
step:748/1405 train_time:102533ms step_avg:138.93ms
step:749/1405 train_time:102677ms step_avg:138.94ms
step:750/1405 train_time:102822ms step_avg:138.95ms
step:750/1405 val_loss:3.5275 train_time:102892ms step_avg:139.04ms
step:751/1405 train_time:102970ms step_avg:138.96ms
step:752/1405 train_time:103114ms step_avg:138.97ms
step:753/1405 train_time:103259ms step_avg:138.98ms
step:754/1405 train_time:103401ms step_avg:138.98ms
step:755/1405 train_time:103544ms step_avg:138.99ms
step:756/1405 train_time:103688ms step_avg:138.99ms
step:757/1405 train_time:103832ms step_avg:139.00ms
step:758/1405 train_time:103977ms step_avg:139.01ms
step:759/1405 train_time:104123ms step_avg:139.02ms
step:760/1405 train_time:104266ms step_avg:139.02ms
step:761/1405 train_time:104459ms step_avg:139.09ms
step:762/1405 train_time:104602ms step_avg:139.10ms
step:763/1405 train_time:104745ms step_avg:139.10ms
step:764/1405 train_time:104889ms step_avg:139.11ms
step:765/1405 train_time:105032ms step_avg:139.12ms
step:766/1405 train_time:105176ms step_avg:139.12ms
step:767/1405 train_time:105320ms step_avg:139.13ms
step:768/1405 train_time:105466ms step_avg:139.14ms
step:769/1405 train_time:105611ms step_avg:139.14ms
step:770/1405 train_time:105754ms step_avg:139.15ms
step:771/1405 train_time:105899ms step_avg:139.16ms
step:772/1405 train_time:106044ms step_avg:139.17ms
step:773/1405 train_time:106187ms step_avg:139.17ms
step:774/1405 train_time:106331ms step_avg:139.18ms
step:775/1405 train_time:106477ms step_avg:139.19ms
step:776/1405 train_time:106622ms step_avg:139.19ms
step:777/1405 train_time:106765ms step_avg:139.20ms
step:778/1405 train_time:106908ms step_avg:139.20ms
step:779/1405 train_time:107053ms step_avg:139.21ms
step:780/1405 train_time:107197ms step_avg:139.22ms
step:781/1405 train_time:107340ms step_avg:139.22ms
step:782/1405 train_time:107484ms step_avg:139.23ms
step:783/1405 train_time:107628ms step_avg:139.23ms
step:784/1405 train_time:107772ms step_avg:139.24ms
step:785/1405 train_time:107917ms step_avg:139.25ms
step:786/1405 train_time:108063ms step_avg:139.26ms
step:787/1405 train_time:108207ms step_avg:139.26ms
step:788/1405 train_time:108350ms step_avg:139.27ms
step:789/1405 train_time:108494ms step_avg:139.27ms
step:790/1405 train_time:108639ms step_avg:139.28ms
step:791/1405 train_time:108783ms step_avg:139.29ms
step:792/1405 train_time:108926ms step_avg:139.29ms
step:793/1405 train_time:109070ms step_avg:139.30ms
step:794/1405 train_time:109213ms step_avg:139.30ms
step:795/1405 train_time:109359ms step_avg:139.31ms
step:796/1405 train_time:109503ms step_avg:139.32ms
step:797/1405 train_time:109648ms step_avg:139.32ms
step:798/1405 train_time:109792ms step_avg:139.33ms
step:799/1405 train_time:109937ms step_avg:139.34ms
step:800/1405 train_time:110082ms step_avg:139.34ms
step:801/1405 train_time:110226ms step_avg:139.35ms
step:802/1405 train_time:110371ms step_avg:139.36ms
step:803/1405 train_time:110514ms step_avg:139.36ms
step:804/1405 train_time:110659ms step_avg:139.37ms
step:805/1405 train_time:110804ms step_avg:139.38ms
step:806/1405 train_time:110948ms step_avg:139.38ms
step:807/1405 train_time:111091ms step_avg:139.39ms
step:808/1405 train_time:111237ms step_avg:139.39ms
step:809/1405 train_time:111381ms step_avg:139.40ms
step:810/1405 train_time:111525ms step_avg:139.41ms
step:811/1405 train_time:111669ms step_avg:139.41ms
step:812/1405 train_time:111814ms step_avg:139.42ms
step:813/1405 train_time:111960ms step_avg:139.43ms
step:814/1405 train_time:112104ms step_avg:139.43ms
step:815/1405 train_time:112249ms step_avg:139.44ms
step:816/1405 train_time:112394ms step_avg:139.45ms
step:817/1405 train_time:112538ms step_avg:139.45ms
step:818/1405 train_time:112682ms step_avg:139.46ms
step:819/1405 train_time:112826ms step_avg:139.46ms
step:820/1405 train_time:112971ms step_avg:139.47ms
step:821/1405 train_time:113115ms step_avg:139.48ms
step:822/1405 train_time:113260ms step_avg:139.48ms
step:823/1405 train_time:113404ms step_avg:139.49ms
step:824/1405 train_time:113549ms step_avg:139.49ms
step:825/1405 train_time:113693ms step_avg:139.50ms
step:826/1405 train_time:113838ms step_avg:139.51ms
step:827/1405 train_time:113983ms step_avg:139.51ms
step:828/1405 train_time:114128ms step_avg:139.52ms
step:829/1405 train_time:114272ms step_avg:139.53ms
step:830/1405 train_time:114418ms step_avg:139.53ms
step:831/1405 train_time:114563ms step_avg:139.54ms
step:832/1405 train_time:114706ms step_avg:139.55ms
step:833/1405 train_time:114851ms step_avg:139.55ms
step:834/1405 train_time:114996ms step_avg:139.56ms
step:835/1405 train_time:115141ms step_avg:139.56ms
step:836/1405 train_time:115285ms step_avg:139.57ms
step:837/1405 train_time:115430ms step_avg:139.58ms
step:838/1405 train_time:115574ms step_avg:139.58ms
step:839/1405 train_time:115719ms step_avg:139.59ms
step:840/1405 train_time:115864ms step_avg:139.59ms
step:841/1405 train_time:116009ms step_avg:139.60ms
step:842/1405 train_time:116154ms step_avg:139.61ms
step:843/1405 train_time:116299ms step_avg:139.62ms
step:844/1405 train_time:116444ms step_avg:139.62ms
step:845/1405 train_time:116588ms step_avg:139.63ms
step:846/1405 train_time:116732ms step_avg:139.63ms
step:847/1405 train_time:116877ms step_avg:139.64ms
step:848/1405 train_time:117021ms step_avg:139.64ms
step:849/1405 train_time:117165ms step_avg:139.65ms
step:850/1405 train_time:117310ms step_avg:139.65ms
step:851/1405 train_time:117456ms step_avg:139.66ms
step:852/1405 train_time:117602ms step_avg:139.67ms
step:853/1405 train_time:117746ms step_avg:139.67ms
step:854/1405 train_time:117890ms step_avg:139.68ms
step:855/1405 train_time:118033ms step_avg:139.68ms
step:856/1405 train_time:118177ms step_avg:139.69ms
step:857/1405 train_time:118323ms step_avg:139.70ms
step:858/1405 train_time:118469ms step_avg:139.70ms
step:859/1405 train_time:118614ms step_avg:139.71ms
step:860/1405 train_time:118759ms step_avg:139.72ms
step:861/1405 train_time:118904ms step_avg:139.72ms
step:862/1405 train_time:119049ms step_avg:139.73ms
step:863/1405 train_time:119194ms step_avg:139.74ms
step:864/1405 train_time:119339ms step_avg:139.74ms
step:865/1405 train_time:119484ms step_avg:139.75ms
step:866/1405 train_time:119630ms step_avg:139.75ms
step:867/1405 train_time:119776ms step_avg:139.76ms
step:868/1405 train_time:119919ms step_avg:139.77ms
step:869/1405 train_time:120063ms step_avg:139.77ms
step:870/1405 train_time:120208ms step_avg:139.78ms
step:871/1405 train_time:120352ms step_avg:139.78ms
step:872/1405 train_time:120496ms step_avg:139.79ms
step:873/1405 train_time:120643ms step_avg:139.79ms
step:874/1405 train_time:120787ms step_avg:139.80ms
step:875/1405 train_time:120932ms step_avg:139.81ms
step:875/1405 val_loss:3.4765 train_time:121001ms step_avg:139.89ms
step:876/1405 train_time:121079ms step_avg:139.81ms
step:877/1405 train_time:121226ms step_avg:139.82ms
step:878/1405 train_time:121370ms step_avg:139.83ms
step:879/1405 train_time:121514ms step_avg:139.83ms
step:880/1405 train_time:121657ms step_avg:139.84ms
step:881/1405 train_time:121801ms step_avg:139.84ms
step:882/1405 train_time:121945ms step_avg:139.85ms
step:883/1405 train_time:122090ms step_avg:139.85ms
step:884/1405 train_time:122236ms step_avg:139.86ms
step:885/1405 train_time:122383ms step_avg:139.87ms
step:886/1405 train_time:122528ms step_avg:139.87ms
step:887/1405 train_time:122673ms step_avg:139.88ms
step:888/1405 train_time:122818ms step_avg:139.88ms
step:889/1405 train_time:122963ms step_avg:139.89ms
step:890/1405 train_time:123108ms step_avg:139.90ms
step:891/1405 train_time:123251ms step_avg:139.90ms
step:892/1405 train_time:123396ms step_avg:139.90ms
step:893/1405 train_time:123540ms step_avg:139.91ms
step:894/1405 train_time:123684ms step_avg:139.91ms
step:895/1405 train_time:123829ms step_avg:139.92ms
step:896/1405 train_time:123973ms step_avg:139.92ms
step:897/1405 train_time:124118ms step_avg:139.93ms
step:898/1405 train_time:124264ms step_avg:139.94ms
step:899/1405 train_time:124409ms step_avg:139.94ms
step:900/1405 train_time:124553ms step_avg:139.95ms
step:901/1405 train_time:124698ms step_avg:139.95ms
step:902/1405 train_time:124842ms step_avg:139.96ms
step:903/1405 train_time:124987ms step_avg:139.96ms
step:904/1405 train_time:125131ms step_avg:139.97ms
step:905/1405 train_time:125276ms step_avg:139.97ms
step:906/1405 train_time:125421ms step_avg:139.98ms
step:907/1405 train_time:125566ms step_avg:139.98ms
step:908/1405 train_time:125710ms step_avg:139.99ms
step:909/1405 train_time:125854ms step_avg:139.99ms
step:910/1405 train_time:126000ms step_avg:140.00ms
step:911/1405 train_time:126147ms step_avg:140.01ms
step:912/1405 train_time:126291ms step_avg:140.01ms
step:913/1405 train_time:126438ms step_avg:140.02ms
step:914/1405 train_time:126585ms step_avg:140.03ms
step:915/1405 train_time:126729ms step_avg:140.03ms
step:916/1405 train_time:126873ms step_avg:140.04ms
step:917/1405 train_time:127018ms step_avg:140.04ms
step:918/1405 train_time:127163ms step_avg:140.05ms
step:919/1405 train_time:127308ms step_avg:140.05ms
step:920/1405 train_time:127452ms step_avg:140.06ms
step:921/1405 train_time:127596ms step_avg:140.06ms
step:922/1405 train_time:127740ms step_avg:140.07ms
step:923/1405 train_time:127884ms step_avg:140.07ms
step:924/1405 train_time:128029ms step_avg:140.08ms
step:925/1405 train_time:128174ms step_avg:140.08ms
step:926/1405 train_time:128319ms step_avg:140.09ms
step:927/1405 train_time:128463ms step_avg:140.09ms
step:928/1405 train_time:128608ms step_avg:140.10ms
step:929/1405 train_time:128753ms step_avg:140.10ms
step:930/1405 train_time:128897ms step_avg:140.11ms
step:931/1405 train_time:129043ms step_avg:140.11ms
step:932/1405 train_time:129188ms step_avg:140.12ms
step:933/1405 train_time:129331ms step_avg:140.12ms
step:934/1405 train_time:129475ms step_avg:140.12ms
step:935/1405 train_time:129619ms step_avg:140.13ms
step:936/1405 train_time:129764ms step_avg:140.13ms
step:937/1405 train_time:129907ms step_avg:140.14ms
step:938/1405 train_time:130051ms step_avg:140.14ms
step:939/1405 train_time:130200ms step_avg:140.15ms
step:940/1405 train_time:130349ms step_avg:140.16ms
step:941/1405 train_time:130494ms step_avg:140.17ms
step:942/1405 train_time:130639ms step_avg:140.17ms
step:943/1405 train_time:130785ms step_avg:140.18ms
step:944/1405 train_time:130931ms step_avg:140.18ms
step:945/1405 train_time:131077ms step_avg:140.19ms
step:946/1405 train_time:131223ms step_avg:140.20ms
step:947/1405 train_time:131371ms step_avg:140.20ms
step:948/1405 train_time:131517ms step_avg:140.21ms
step:949/1405 train_time:131662ms step_avg:140.22ms
step:950/1405 train_time:131808ms step_avg:140.22ms
step:951/1405 train_time:132002ms step_avg:140.28ms
step:952/1405 train_time:132147ms step_avg:140.28ms
step:953/1405 train_time:132293ms step_avg:140.29ms
step:954/1405 train_time:132437ms step_avg:140.29ms
step:955/1405 train_time:132582ms step_avg:140.30ms
step:956/1405 train_time:132728ms step_avg:140.30ms
step:957/1405 train_time:132875ms step_avg:140.31ms
step:958/1405 train_time:133025ms step_avg:140.32ms
step:959/1405 train_time:133173ms step_avg:140.33ms
step:960/1405 train_time:133320ms step_avg:140.34ms
step:961/1405 train_time:133467ms step_avg:140.34ms
step:962/1405 train_time:133612ms step_avg:140.35ms
step:963/1405 train_time:133760ms step_avg:140.36ms
step:964/1405 train_time:133909ms step_avg:140.37ms
step:965/1405 train_time:134053ms step_avg:140.37ms
step:966/1405 train_time:134200ms step_avg:140.38ms
step:967/1405 train_time:134346ms step_avg:140.38ms
step:968/1405 train_time:134491ms step_avg:140.39ms
step:969/1405 train_time:134638ms step_avg:140.39ms
step:970/1405 train_time:134783ms step_avg:140.40ms
step:971/1405 train_time:134929ms step_avg:140.40ms
step:972/1405 train_time:135075ms step_avg:140.41ms
step:973/1405 train_time:135221ms step_avg:140.42ms
step:974/1405 train_time:135368ms step_avg:140.42ms
step:975/1405 train_time:135515ms step_avg:140.43ms
step:976/1405 train_time:135663ms step_avg:140.44ms
step:977/1405 train_time:135809ms step_avg:140.44ms
step:978/1405 train_time:135954ms step_avg:140.45ms
step:979/1405 train_time:136099ms step_avg:140.45ms
step:980/1405 train_time:136246ms step_avg:140.46ms
step:981/1405 train_time:136391ms step_avg:140.46ms
step:982/1405 train_time:136536ms step_avg:140.47ms
step:983/1405 train_time:136684ms step_avg:140.48ms
step:984/1405 train_time:136830ms step_avg:140.48ms
step:985/1405 train_time:136977ms step_avg:140.49ms
step:986/1405 train_time:137125ms step_avg:140.50ms
step:987/1405 train_time:137270ms step_avg:140.50ms
step:988/1405 train_time:137416ms step_avg:140.51ms
step:989/1405 train_time:137563ms step_avg:140.51ms
step:990/1405 train_time:137710ms step_avg:140.52ms
step:991/1405 train_time:137857ms step_avg:140.53ms
step:992/1405 train_time:138004ms step_avg:140.53ms
step:993/1405 train_time:138152ms step_avg:140.54ms
step:994/1405 train_time:138297ms step_avg:140.55ms
step:995/1405 train_time:138443ms step_avg:140.55ms
step:996/1405 train_time:138589ms step_avg:140.56ms
step:997/1405 train_time:138734ms step_avg:140.56ms
step:998/1405 train_time:138880ms step_avg:140.57ms
step:999/1405 train_time:139028ms step_avg:140.57ms
step:1000/1405 train_time:139173ms step_avg:140.58ms
step:1000/1405 val_loss:3.4125 train_time:139243ms step_avg:140.65ms
step:1001/1405 train_time:139321ms step_avg:140.59ms
step:1002/1405 train_time:139467ms step_avg:140.59ms
step:1003/1405 train_time:139613ms step_avg:140.60ms
step:1004/1405 train_time:139760ms step_avg:140.60ms
step:1005/1405 train_time:139907ms step_avg:140.61ms
step:1006/1405 train_time:140051ms step_avg:140.61ms
step:1007/1405 train_time:140198ms step_avg:140.62ms
step:1008/1405 train_time:140345ms step_avg:140.63ms
step:1009/1405 train_time:140492ms step_avg:140.63ms
step:1010/1405 train_time:140640ms step_avg:140.64ms
step:1011/1405 train_time:140786ms step_avg:140.65ms
step:1012/1405 train_time:140932ms step_avg:140.65ms
step:1013/1405 train_time:141078ms step_avg:140.66ms
step:1014/1405 train_time:141223ms step_avg:140.66ms
step:1015/1405 train_time:141369ms step_avg:140.67ms
step:1016/1405 train_time:141516ms step_avg:140.67ms
step:1017/1405 train_time:141664ms step_avg:140.68ms
step:1018/1405 train_time:141809ms step_avg:140.68ms
step:1019/1405 train_time:141955ms step_avg:140.69ms
step:1020/1405 train_time:142103ms step_avg:140.70ms
step:1021/1405 train_time:142249ms step_avg:140.70ms
step:1022/1405 train_time:142394ms step_avg:140.71ms
step:1023/1405 train_time:142541ms step_avg:140.71ms
step:1024/1405 train_time:142687ms step_avg:140.72ms
step:1025/1405 train_time:142833ms step_avg:140.72ms
step:1026/1405 train_time:142980ms step_avg:140.73ms
step:1027/1405 train_time:143125ms step_avg:140.73ms
step:1028/1405 train_time:143271ms step_avg:140.74ms
step:1029/1405 train_time:143418ms step_avg:140.74ms
step:1030/1405 train_time:143565ms step_avg:140.75ms
step:1031/1405 train_time:143709ms step_avg:140.75ms
step:1032/1405 train_time:143855ms step_avg:140.76ms
step:1033/1405 train_time:144001ms step_avg:140.76ms
step:1034/1405 train_time:144147ms step_avg:140.77ms
step:1035/1405 train_time:144293ms step_avg:140.77ms
step:1036/1405 train_time:144439ms step_avg:140.78ms
step:1037/1405 train_time:144587ms step_avg:140.79ms
step:1038/1405 train_time:144731ms step_avg:140.79ms
step:1039/1405 train_time:144877ms step_avg:140.79ms
step:1040/1405 train_time:145023ms step_avg:140.80ms
step:1041/1405 train_time:145169ms step_avg:140.80ms
step:1042/1405 train_time:145314ms step_avg:140.81ms
step:1043/1405 train_time:145460ms step_avg:140.81ms
step:1044/1405 train_time:145607ms step_avg:140.82ms
step:1045/1405 train_time:145753ms step_avg:140.82ms
step:1046/1405 train_time:145900ms step_avg:140.83ms
step:1047/1405 train_time:146047ms step_avg:140.84ms
step:1048/1405 train_time:146194ms step_avg:140.84ms
step:1049/1405 train_time:146341ms step_avg:140.85ms
step:1050/1405 train_time:146489ms step_avg:140.85ms
step:1051/1405 train_time:146636ms step_avg:140.86ms
step:1052/1405 train_time:146783ms step_avg:140.87ms
step:1053/1405 train_time:146928ms step_avg:140.87ms
step:1054/1405 train_time:147075ms step_avg:140.88ms
step:1055/1405 train_time:147221ms step_avg:140.88ms
step:1056/1405 train_time:147367ms step_avg:140.89ms
step:1057/1405 train_time:147513ms step_avg:140.89ms
step:1058/1405 train_time:147660ms step_avg:140.90ms
step:1059/1405 train_time:147809ms step_avg:140.90ms
step:1060/1405 train_time:147955ms step_avg:140.91ms
step:1061/1405 train_time:148103ms step_avg:140.92ms
step:1062/1405 train_time:148248ms step_avg:140.92ms
step:1063/1405 train_time:148393ms step_avg:140.92ms
step:1064/1405 train_time:148540ms step_avg:140.93ms
step:1065/1405 train_time:148686ms step_avg:140.94ms
step:1066/1405 train_time:148833ms step_avg:140.94ms
step:1067/1405 train_time:148982ms step_avg:140.95ms
step:1068/1405 train_time:149127ms step_avg:140.95ms
step:1069/1405 train_time:149274ms step_avg:140.96ms
step:1070/1405 train_time:149422ms step_avg:140.96ms
step:1071/1405 train_time:149568ms step_avg:140.97ms
step:1072/1405 train_time:149713ms step_avg:140.97ms
step:1073/1405 train_time:149858ms step_avg:140.98ms
step:1074/1405 train_time:150005ms step_avg:140.98ms
step:1075/1405 train_time:150151ms step_avg:140.99ms
step:1076/1405 train_time:150295ms step_avg:140.99ms
step:1077/1405 train_time:150443ms step_avg:141.00ms
step:1078/1405 train_time:150591ms step_avg:141.00ms
step:1079/1405 train_time:150738ms step_avg:141.01ms
step:1080/1405 train_time:150885ms step_avg:141.01ms
step:1081/1405 train_time:151030ms step_avg:141.02ms
step:1082/1405 train_time:151177ms step_avg:141.02ms
step:1083/1405 train_time:151324ms step_avg:141.03ms
step:1084/1405 train_time:151471ms step_avg:141.03ms
step:1085/1405 train_time:151617ms step_avg:141.04ms
step:1086/1405 train_time:151763ms step_avg:141.04ms
step:1087/1405 train_time:151908ms step_avg:141.05ms
step:1088/1405 train_time:152055ms step_avg:141.05ms
step:1089/1405 train_time:152203ms step_avg:141.06ms
step:1090/1405 train_time:152350ms step_avg:141.07ms
step:1091/1405 train_time:152498ms step_avg:141.07ms
step:1092/1405 train_time:152645ms step_avg:141.08ms
step:1093/1405 train_time:152791ms step_avg:141.08ms
step:1094/1405 train_time:152937ms step_avg:141.09ms
step:1095/1405 train_time:153084ms step_avg:141.09ms
step:1096/1405 train_time:153231ms step_avg:141.10ms
step:1097/1405 train_time:153379ms step_avg:141.10ms
step:1098/1405 train_time:153525ms step_avg:141.11ms
step:1099/1405 train_time:153670ms step_avg:141.11ms
step:1100/1405 train_time:153817ms step_avg:141.12ms
step:1101/1405 train_time:153964ms step_avg:141.12ms
step:1102/1405 train_time:154112ms step_avg:141.13ms
step:1103/1405 train_time:154260ms step_avg:141.13ms
step:1104/1405 train_time:154406ms step_avg:141.14ms
step:1105/1405 train_time:154553ms step_avg:141.14ms
step:1106/1405 train_time:154699ms step_avg:141.15ms
step:1107/1405 train_time:154845ms step_avg:141.15ms
step:1108/1405 train_time:154992ms step_avg:141.16ms
step:1109/1405 train_time:155138ms step_avg:141.16ms
step:1110/1405 train_time:155284ms step_avg:141.17ms
step:1111/1405 train_time:155430ms step_avg:141.17ms
step:1112/1405 train_time:155577ms step_avg:141.18ms
step:1113/1405 train_time:155724ms step_avg:141.18ms
step:1114/1405 train_time:155870ms step_avg:141.19ms
step:1115/1405 train_time:156016ms step_avg:141.19ms
step:1116/1405 train_time:156162ms step_avg:141.20ms
step:1117/1405 train_time:156308ms step_avg:141.20ms
step:1118/1405 train_time:156456ms step_avg:141.21ms
step:1119/1405 train_time:156603ms step_avg:141.21ms
step:1120/1405 train_time:156749ms step_avg:141.22ms
step:1121/1405 train_time:156894ms step_avg:141.22ms
step:1122/1405 train_time:157041ms step_avg:141.22ms
step:1123/1405 train_time:157187ms step_avg:141.23ms
step:1124/1405 train_time:157334ms step_avg:141.23ms
step:1125/1405 train_time:157482ms step_avg:141.24ms
step:1125/1405 val_loss:3.3596 train_time:157553ms step_avg:141.30ms
step:1126/1405 train_time:157630ms step_avg:141.25ms
step:1127/1405 train_time:157777ms step_avg:141.25ms
step:1128/1405 train_time:157922ms step_avg:141.25ms
step:1129/1405 train_time:158068ms step_avg:141.26ms
step:1130/1405 train_time:158215ms step_avg:141.26ms
step:1131/1405 train_time:158360ms step_avg:141.27ms
step:1132/1405 train_time:158507ms step_avg:141.27ms
step:1133/1405 train_time:158657ms step_avg:141.28ms
step:1134/1405 train_time:158804ms step_avg:141.28ms
step:1135/1405 train_time:158951ms step_avg:141.29ms
step:1136/1405 train_time:159098ms step_avg:141.29ms
step:1137/1405 train_time:159244ms step_avg:141.30ms
step:1138/1405 train_time:159391ms step_avg:141.30ms
step:1139/1405 train_time:159537ms step_avg:141.31ms
step:1140/1405 train_time:159686ms step_avg:141.31ms
step:1141/1405 train_time:159874ms step_avg:141.36ms
step:1142/1405 train_time:160019ms step_avg:141.36ms
step:1143/1405 train_time:160166ms step_avg:141.36ms
step:1144/1405 train_time:160311ms step_avg:141.37ms
step:1145/1405 train_time:160457ms step_avg:141.37ms
step:1146/1405 train_time:160604ms step_avg:141.38ms
step:1147/1405 train_time:160753ms step_avg:141.38ms
step:1148/1405 train_time:160900ms step_avg:141.39ms
step:1149/1405 train_time:161048ms step_avg:141.39ms
step:1150/1405 train_time:161196ms step_avg:141.40ms
step:1151/1405 train_time:161344ms step_avg:141.41ms
step:1152/1405 train_time:161492ms step_avg:141.41ms
step:1153/1405 train_time:161640ms step_avg:141.42ms
step:1154/1405 train_time:161788ms step_avg:141.42ms
step:1155/1405 train_time:161937ms step_avg:141.43ms
step:1156/1405 train_time:162087ms step_avg:141.44ms
step:1157/1405 train_time:162236ms step_avg:141.44ms
step:1158/1405 train_time:162384ms step_avg:141.45ms
step:1159/1405 train_time:162533ms step_avg:141.46ms
step:1160/1405 train_time:162680ms step_avg:141.46ms
step:1161/1405 train_time:162829ms step_avg:141.47ms
step:1162/1405 train_time:162979ms step_avg:141.47ms
step:1163/1405 train_time:163129ms step_avg:141.48ms
step:1164/1405 train_time:163278ms step_avg:141.49ms
step:1165/1405 train_time:163424ms step_avg:141.49ms
step:1166/1405 train_time:163573ms step_avg:141.50ms
step:1167/1405 train_time:163719ms step_avg:141.50ms
step:1168/1405 train_time:163868ms step_avg:141.51ms
step:1169/1405 train_time:164016ms step_avg:141.52ms
step:1170/1405 train_time:164164ms step_avg:141.52ms
step:1171/1405 train_time:164312ms step_avg:141.53ms
step:1172/1405 train_time:164460ms step_avg:141.53ms
step:1173/1405 train_time:164607ms step_avg:141.54ms
step:1174/1405 train_time:164758ms step_avg:141.54ms
step:1175/1405 train_time:164909ms step_avg:141.55ms
step:1176/1405 train_time:165058ms step_avg:141.56ms
step:1177/1405 train_time:165208ms step_avg:141.57ms
step:1178/1405 train_time:165356ms step_avg:141.57ms
step:1179/1405 train_time:165502ms step_avg:141.58ms
step:1180/1405 train_time:165654ms step_avg:141.58ms
step:1181/1405 train_time:165802ms step_avg:141.59ms
step:1182/1405 train_time:165949ms step_avg:141.59ms
step:1183/1405 train_time:166097ms step_avg:141.60ms
step:1184/1405 train_time:166245ms step_avg:141.61ms
step:1185/1405 train_time:166395ms step_avg:141.61ms
step:1186/1405 train_time:166543ms step_avg:141.62ms
step:1187/1405 train_time:166696ms step_avg:141.63ms
step:1188/1405 train_time:166843ms step_avg:141.63ms
step:1189/1405 train_time:166991ms step_avg:141.64ms
step:1190/1405 train_time:167139ms step_avg:141.64ms
step:1191/1405 train_time:167286ms step_avg:141.65ms
step:1192/1405 train_time:167435ms step_avg:141.65ms
step:1193/1405 train_time:167581ms step_avg:141.66ms
step:1194/1405 train_time:167729ms step_avg:141.66ms
step:1195/1405 train_time:167877ms step_avg:141.67ms
step:1196/1405 train_time:168027ms step_avg:141.68ms
step:1197/1405 train_time:168175ms step_avg:141.68ms
step:1198/1405 train_time:168324ms step_avg:141.69ms
step:1199/1405 train_time:168473ms step_avg:141.69ms
step:1200/1405 train_time:168620ms step_avg:141.70ms
step:1201/1405 train_time:168768ms step_avg:141.70ms
step:1202/1405 train_time:168921ms step_avg:141.71ms
step:1203/1405 train_time:169071ms step_avg:141.72ms
step:1204/1405 train_time:169219ms step_avg:141.72ms
step:1205/1405 train_time:169367ms step_avg:141.73ms
step:1206/1405 train_time:169515ms step_avg:141.74ms
step:1207/1405 train_time:169663ms step_avg:141.74ms
step:1208/1405 train_time:169813ms step_avg:141.75ms
step:1209/1405 train_time:169960ms step_avg:141.75ms
step:1210/1405 train_time:170109ms step_avg:141.76ms
step:1211/1405 train_time:170257ms step_avg:141.76ms
step:1212/1405 train_time:170405ms step_avg:141.77ms
step:1213/1405 train_time:170556ms step_avg:141.78ms
step:1214/1405 train_time:170705ms step_avg:141.78ms
step:1215/1405 train_time:170853ms step_avg:141.79ms
step:1216/1405 train_time:170999ms step_avg:141.79ms
step:1217/1405 train_time:171147ms step_avg:141.80ms
step:1218/1405 train_time:171295ms step_avg:141.80ms
step:1219/1405 train_time:171440ms step_avg:141.80ms
step:1220/1405 train_time:171588ms step_avg:141.81ms
step:1221/1405 train_time:171736ms step_avg:141.81ms
step:1222/1405 train_time:171883ms step_avg:141.82ms
step:1223/1405 train_time:172032ms step_avg:141.82ms
step:1224/1405 train_time:172182ms step_avg:141.83ms
step:1225/1405 train_time:172331ms step_avg:141.84ms
step:1226/1405 train_time:172480ms step_avg:141.84ms
step:1227/1405 train_time:172628ms step_avg:141.85ms
step:1228/1405 train_time:172778ms step_avg:141.85ms
step:1229/1405 train_time:172925ms step_avg:141.86ms
step:1230/1405 train_time:173075ms step_avg:141.87ms
step:1231/1405 train_time:173224ms step_avg:141.87ms
step:1232/1405 train_time:173375ms step_avg:141.88ms
step:1233/1405 train_time:173521ms step_avg:141.88ms
step:1234/1405 train_time:173669ms step_avg:141.89ms
step:1235/1405 train_time:173818ms step_avg:141.89ms
step:1236/1405 train_time:173967ms step_avg:141.90ms
step:1237/1405 train_time:174115ms step_avg:141.90ms
step:1238/1405 train_time:174266ms step_avg:141.91ms
step:1239/1405 train_time:174414ms step_avg:141.92ms
step:1240/1405 train_time:174563ms step_avg:141.92ms
step:1241/1405 train_time:174713ms step_avg:141.93ms
step:1242/1405 train_time:174860ms step_avg:141.93ms
step:1243/1405 train_time:175008ms step_avg:141.94ms
step:1244/1405 train_time:175155ms step_avg:141.94ms
step:1245/1405 train_time:175303ms step_avg:141.95ms
step:1246/1405 train_time:175452ms step_avg:141.95ms
step:1247/1405 train_time:175599ms step_avg:141.96ms
step:1248/1405 train_time:175746ms step_avg:141.96ms
step:1249/1405 train_time:175894ms step_avg:141.96ms
step:1250/1405 train_time:176043ms step_avg:141.97ms
step:1250/1405 val_loss:3.3120 train_time:176114ms step_avg:142.03ms
step:1251/1405 train_time:176194ms step_avg:141.98ms
step:1252/1405 train_time:176344ms step_avg:141.98ms
step:1253/1405 train_time:176491ms step_avg:141.99ms
step:1254/1405 train_time:176640ms step_avg:141.99ms
step:1255/1405 train_time:176791ms step_avg:142.00ms
step:1256/1405 train_time:176939ms step_avg:142.01ms
step:1257/1405 train_time:177085ms step_avg:142.01ms
step:1258/1405 train_time:177235ms step_avg:142.02ms
step:1259/1405 train_time:177384ms step_avg:142.02ms
step:1260/1405 train_time:177532ms step_avg:142.03ms
step:1261/1405 train_time:177681ms step_avg:142.03ms
step:1262/1405 train_time:177830ms step_avg:142.04ms
step:1263/1405 train_time:177980ms step_avg:142.04ms
step:1264/1405 train_time:178127ms step_avg:142.05ms
step:1265/1405 train_time:178276ms step_avg:142.05ms
step:1266/1405 train_time:178424ms step_avg:142.06ms
step:1267/1405 train_time:178571ms step_avg:142.06ms
step:1268/1405 train_time:178720ms step_avg:142.07ms
step:1269/1405 train_time:178869ms step_avg:142.07ms
step:1270/1405 train_time:179018ms step_avg:142.08ms
step:1271/1405 train_time:179165ms step_avg:142.08ms
step:1272/1405 train_time:179312ms step_avg:142.09ms
step:1273/1405 train_time:179461ms step_avg:142.09ms
step:1274/1405 train_time:179607ms step_avg:142.09ms
step:1275/1405 train_time:179757ms step_avg:142.10ms
step:1276/1405 train_time:179904ms step_avg:142.10ms
step:1277/1405 train_time:180052ms step_avg:142.11ms
step:1278/1405 train_time:180200ms step_avg:142.11ms
step:1279/1405 train_time:180348ms step_avg:142.12ms
step:1280/1405 train_time:180498ms step_avg:142.12ms
step:1281/1405 train_time:180645ms step_avg:142.13ms
step:1282/1405 train_time:180792ms step_avg:142.13ms
step:1283/1405 train_time:180941ms step_avg:142.14ms
step:1284/1405 train_time:181090ms step_avg:142.14ms
step:1285/1405 train_time:181239ms step_avg:142.15ms
step:1286/1405 train_time:181386ms step_avg:142.15ms
step:1287/1405 train_time:181532ms step_avg:142.16ms
step:1288/1405 train_time:181681ms step_avg:142.16ms
step:1289/1405 train_time:181832ms step_avg:142.17ms
step:1290/1405 train_time:181984ms step_avg:142.17ms
step:1291/1405 train_time:182133ms step_avg:142.18ms
step:1292/1405 train_time:182281ms step_avg:142.18ms
step:1293/1405 train_time:182431ms step_avg:142.19ms
step:1294/1405 train_time:182580ms step_avg:142.20ms
step:1295/1405 train_time:182727ms step_avg:142.20ms
step:1296/1405 train_time:182874ms step_avg:142.20ms
step:1297/1405 train_time:183023ms step_avg:142.21ms
step:1298/1405 train_time:183170ms step_avg:142.21ms
step:1299/1405 train_time:183319ms step_avg:142.22ms
step:1300/1405 train_time:183468ms step_avg:142.22ms
step:1301/1405 train_time:183616ms step_avg:142.23ms
step:1302/1405 train_time:183764ms step_avg:142.23ms
step:1303/1405 train_time:183915ms step_avg:142.24ms
step:1304/1405 train_time:184064ms step_avg:142.24ms
step:1305/1405 train_time:184212ms step_avg:142.25ms
step:1306/1405 train_time:184361ms step_avg:142.25ms
step:1307/1405 train_time:184507ms step_avg:142.26ms
step:1308/1405 train_time:184656ms step_avg:142.26ms
step:1309/1405 train_time:184805ms step_avg:142.27ms
step:1310/1405 train_time:184954ms step_avg:142.27ms
step:1311/1405 train_time:185103ms step_avg:142.28ms
step:1312/1405 train_time:185250ms step_avg:142.28ms
step:1313/1405 train_time:185400ms step_avg:142.29ms
step:1314/1405 train_time:185548ms step_avg:142.29ms
step:1315/1405 train_time:185696ms step_avg:142.30ms
step:1316/1405 train_time:185844ms step_avg:142.30ms
step:1317/1405 train_time:185991ms step_avg:142.30ms
step:1318/1405 train_time:186143ms step_avg:142.31ms
step:1319/1405 train_time:186292ms step_avg:142.32ms
step:1320/1405 train_time:186440ms step_avg:142.32ms
step:1321/1405 train_time:186587ms step_avg:142.32ms
step:1322/1405 train_time:186737ms step_avg:142.33ms
step:1323/1405 train_time:186884ms step_avg:142.33ms
step:1324/1405 train_time:187032ms step_avg:142.34ms
step:1325/1405 train_time:187181ms step_avg:142.34ms
step:1326/1405 train_time:187331ms step_avg:142.35ms
step:1327/1405 train_time:187480ms step_avg:142.35ms
step:1328/1405 train_time:187628ms step_avg:142.36ms
step:1329/1405 train_time:187783ms step_avg:142.37ms
step:1330/1405 train_time:187932ms step_avg:142.37ms
step:1331/1405 train_time:188124ms step_avg:142.41ms
step:1332/1405 train_time:188275ms step_avg:142.42ms
step:1333/1405 train_time:188423ms step_avg:142.42ms
step:1334/1405 train_time:188570ms step_avg:142.42ms
step:1335/1405 train_time:188717ms step_avg:142.43ms
step:1336/1405 train_time:188866ms step_avg:142.43ms
step:1337/1405 train_time:189015ms step_avg:142.44ms
step:1338/1405 train_time:189165ms step_avg:142.44ms
step:1339/1405 train_time:189314ms step_avg:142.45ms
step:1340/1405 train_time:189463ms step_avg:142.45ms
step:1341/1405 train_time:189610ms step_avg:142.46ms
step:1342/1405 train_time:189760ms step_avg:142.46ms
step:1343/1405 train_time:189907ms step_avg:142.47ms
step:1344/1405 train_time:190054ms step_avg:142.47ms
step:1345/1405 train_time:190202ms step_avg:142.47ms
step:1346/1405 train_time:190351ms step_avg:142.48ms
step:1347/1405 train_time:190499ms step_avg:142.48ms
step:1348/1405 train_time:190646ms step_avg:142.49ms
step:1349/1405 train_time:190795ms step_avg:142.49ms
step:1350/1405 train_time:190942ms step_avg:142.49ms
step:1351/1405 train_time:191091ms step_avg:142.50ms
step:1352/1405 train_time:191241ms step_avg:142.50ms
step:1353/1405 train_time:191389ms step_avg:142.51ms
step:1354/1405 train_time:191538ms step_avg:142.51ms
step:1355/1405 train_time:191686ms step_avg:142.52ms
step:1356/1405 train_time:191835ms step_avg:142.52ms
step:1357/1405 train_time:191985ms step_avg:142.53ms
step:1358/1405 train_time:192136ms step_avg:142.53ms
step:1359/1405 train_time:192285ms step_avg:142.54ms
step:1360/1405 train_time:192435ms step_avg:142.54ms
step:1361/1405 train_time:192586ms step_avg:142.55ms
step:1362/1405 train_time:192735ms step_avg:142.56ms
step:1363/1405 train_time:192887ms step_avg:142.56ms
step:1364/1405 train_time:193037ms step_avg:142.57ms
step:1365/1405 train_time:193185ms step_avg:142.57ms
step:1366/1405 train_time:193334ms step_avg:142.58ms
step:1367/1405 train_time:193483ms step_avg:142.58ms
step:1368/1405 train_time:193633ms step_avg:142.59ms
step:1369/1405 train_time:193786ms step_avg:142.59ms
step:1370/1405 train_time:193937ms step_avg:142.60ms
step:1371/1405 train_time:194086ms step_avg:142.61ms
step:1372/1405 train_time:194238ms step_avg:142.61ms
step:1373/1405 train_time:194387ms step_avg:142.62ms
step:1374/1405 train_time:194537ms step_avg:142.62ms
step:1375/1405 train_time:194687ms step_avg:142.63ms
step:1375/1405 val_loss:3.2810 train_time:194759ms step_avg:142.68ms
step:1376/1405 train_time:194838ms step_avg:142.63ms
step:1377/1405 train_time:194987ms step_avg:142.64ms
step:1378/1405 train_time:195135ms step_avg:142.64ms
step:1379/1405 train_time:195285ms step_avg:142.65ms
step:1380/1405 train_time:195434ms step_avg:142.65ms
step:1381/1405 train_time:195584ms step_avg:142.66ms
step:1382/1405 train_time:195736ms step_avg:142.66ms
step:1383/1405 train_time:195887ms step_avg:142.67ms
step:1384/1405 train_time:196037ms step_avg:142.68ms
step:1385/1405 train_time:196185ms step_avg:142.68ms
step:1386/1405 train_time:196335ms step_avg:142.69ms
step:1387/1405 train_time:196485ms step_avg:142.69ms
step:1388/1405 train_time:196634ms step_avg:142.70ms
step:1389/1405 train_time:196783ms step_avg:142.70ms
step:1390/1405 train_time:196933ms step_avg:142.70ms
step:1391/1405 train_time:197082ms step_avg:142.71ms
step:1392/1405 train_time:197232ms step_avg:142.71ms
step:1393/1405 train_time:197380ms step_avg:142.72ms
step:1394/1405 train_time:197530ms step_avg:142.72ms
step:1395/1405 train_time:197679ms step_avg:142.73ms
step:1396/1405 train_time:197829ms step_avg:142.73ms
step:1397/1405 train_time:197977ms step_avg:142.74ms
step:1398/1405 train_time:198126ms step_avg:142.74ms
step:1399/1405 train_time:198274ms step_avg:142.75ms
step:1400/1405 train_time:198426ms step_avg:142.75ms
step:1401/1405 train_time:198575ms step_avg:142.76ms
step:1402/1405 train_time:198724ms step_avg:142.76ms
step:1403/1405 train_time:198875ms step_avg:142.77ms
step:1404/1405 train_time:199023ms step_avg:142.77ms
step:1405/1405 train_time:199174ms step_avg:142.78ms
step:1405/1405 val_loss:3.2784 train_time:199246ms step_avg:142.83ms
peak memory consumption: 31567 MiB

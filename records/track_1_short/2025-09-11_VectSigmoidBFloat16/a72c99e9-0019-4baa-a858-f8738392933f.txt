import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(
    x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(
    g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99


def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]


@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(
        pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M
    )

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr,
    C_ptr,
    M,
    K,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr,
    C_ptr,
    M,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    alpha,
    beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (
        offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c
    )
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out


@torch.compile(
    dynamic=False, fullgraph=True
)  # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer


class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(
        self,
        params,
        lr: float = 1e-3,
        betas: tuple[float, float] = (0.9, 0.999),
        eps: float = 1e-8,
        weight_decay: float = 0.01,
    ):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(
                        grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True
                    ).get_future()
                )
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group["betas"]
            eps = group["eps"]
            wd = group["weight_decay"]
            params = group["params"]
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size : (rank + 1) * rank_size]
                lr = group["lr"] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(
                    g_slice, g_slice, value=1 - beta2
                )
                # bias corrections
                bias1 = 1 - beta1**t
                bias2 = 1 - beta2**t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(
                    dist.all_gather_into_tensor(
                        p, p_slice, async_op=True
                    ).get_future()
                )
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model


def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(
        self,
        in_features: int,
        out_features: int,
        use_fp8=False,
        x_s=1.0,
        w_s=1.0,
        grad_s=1.0,
    ):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (
            self.in_features**-0.5
        )  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3**0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(
                _x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s
            )[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    rotary_cos: torch.Tensor
    rotary_sin: torch.Tensor
    attn_scale: float


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        rotary_cos, rotary_sin = attn_args.rotary_cos, attn_args.rotary_sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = (
            attn_args.seqlens,
            attn_args.attn_scale,
            attn_args.bm_size,
        )

        q, k, v = (
            F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x))
            .view(B, T, 3 * self.num_heads, self.head_dim)
            .chunk(3, dim=-2)
        )
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = (
            rotary(q, rotary_cos, rotary_sin),
            rotary(k, rotary_cos, rotary_sin),
        )
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(
                v
            )  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = (
            args.train_max_seq_len
            if self.training
            else (args.val_batch_size // (grad_accum_steps * world_size))
        )

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(
            q[0],
            k[0],
            v[0],
            cu_seqlens_q=seqlens,
            cu_seqlens_k=seqlens,
            max_seqlen_q=max_len,
            max_seqlen_k=max_len,
            causal=True,
            softmax_scale=attn_scale,
            window_size=(bm_size, 0),
        )
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(
            self.attn_gate(x[..., : self.attn_gate.weight.size(-1)])
        ).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(
            B, T, self.num_heads * self.head_dim
        )  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x
        ).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = (
            CausalSelfAttention(dim, head_dim, num_heads)
            if layer_idx != 7
            else None
        )
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(
        self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs
    ):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model


def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        num_layers: int,
        num_heads: int,
        head_dim: int,
        model_dim: int,
        max_seq_len: int,
    ):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList(
            [nn.Embedding(vocab_size, model_dim) for _ in range(3)]
        )
        self.blocks = nn.ModuleList(
            [
                Block(model_dim, head_dim, num_heads, i)
                for i in range(num_layers)
            ]
        )
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(
            model_dim,
            vocab_size,
            use_fp8=use_fp8,
            x_s=(model_dim**0.5) / 448,
            w_s=2**-9,
            grad_s=1 / 448,
        )
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.ones(pad),
                ]
            )
        )
        self.max_seq_len = max_seq_len
        self.setup_yarn(head_dim)
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.0
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.0
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def setup_yarn(self, head_dim: int):
        # store single copy of rotary tensors
        angular_freq = (1 / 1024) ** torch.linspace(
            0, 1, steps=head_dim // 4, dtype=torch.float32
        )
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat(
            [angular_freq, angular_freq.new_zeros(head_dim // 4)]
        )
        t = torch.arange(self.max_seq_len, dtype=torch.float32)
        theta = torch.outer(t, angular_freq)
        self.rotary_cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.rotary_sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq

        # scale attention factor f in attn=softmax(f*qk) logarithmically with window size @classiclarryd
        windows = list(
            dict.fromkeys(list(args.ws_schedule) + [args.ws_validate])
        )
        scale_factors = [
            0.2 * math.log(curr / prev) + 1
            for prev, curr in zip(windows[:-1], windows[1:])
        ]
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        attn_scales = list(
            accumulate([0.1] + scale_factors, lambda acc, factor: acc * factor)
        )
        self.attn_scales = dict(zip(windows, attn_scales))

    def apply_yarn(
        self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32
    ):
        rotations = (
            args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        )
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp(
            (rotations - alpha) / (beta - alpha), 0, 1
        )
        self.angular_freq *= scaling_factor + interpolation_weight * (
            1 - scaling_factor
        )
        t = torch.arange(
            self.max_seq_len,
            dtype=torch.float32,
            device=self.angular_freq.device,
        )
        theta = torch.outer(t, self.angular_freq)
        self.rotary_cos.copy_(theta.cos())
        self.rotary_sin.copy_(theta.sin())

    def forward(
        self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int
    ):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = (
            [ve[0], ve[1], ve[2]]
            + [None] * (len(self.blocks) - 6)
            + [ve[0], ve[1], ve[2]]
        )
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        bm_sizes = [
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
        ]
        assert len(bm_sizes) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]).to(
            torch.bfloat16
        )  # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[: (len(self.blocks) // 2)]
        lambdas = self.scalars[
            1 * len(self.blocks) : 3 * len(self.blocks)
        ].view(-1, 2)
        sa_lambdas = self.scalars[
            3 * len(self.blocks) : 5 * len(self.blocks)
        ].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                rotary_cos=self.rotary_cos,
                rotary_sin=self.rotary_sin,
                attn_scale=self.attn_scales[ws],
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(
            logits.view(-1, logits.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader


def _load_data_shard(file: Path):
    header = torch.from_file(
        str(file), False, 256, dtype=torch.int32
    )  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(
            num_tokens, dtype=torch.uint16, pin_memory=True
        )  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(
            tokens.numpy()
        )  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, (
            "number of tokens read does not match header"
        )
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1):
        # Precompute BOS positions once per shard
        self.size = tokens.numel()
        self.bos_idx = (
            (tokens == BOS_ID)
            .nonzero(as_tuple=True)[0]
            .to(torch.int64)
            .cpu()
            .numpy()
        )
        self.i = 0
        self.world_size = world_size

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(
                        f"Insufficient BOS ahead of position {cur}; hit tail of shard."
                    )
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(
                    self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                    cur + max_seq_len,
                    cur + num_tokens_local - cur_len + 1,
                )
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx

        return starts, ends


def distributed_data_generator(
    filename_pattern: str,
    num_tokens: int,
    max_seq_len: int,
    grad_accum_steps: int = 1,
    align_to_bos: bool = True,
):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, (
        "Batch size must be divisible by world size"
    )
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(
            f"No files found for pattern: {filename_pattern}"
        )

    file_iter = iter(
        files
    )  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    finder = BOSFinder(tokens, world_size=world_size) if align_to_bos else None
    pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(
            num_tokens_local // 300, n=128
        )  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(
                    num_tokens_local, max_seq_len
                )
                start_idxs, end_idxs = (
                    torch.tensor(seq_starts[rank]),
                    torch.tensor(seq_ends[rank]),
                )
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens = _load_data_shard(next(file_iter))
                finder = BOSFinder(tokens, world_size=world_size)
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= (
                1  # last document was too long to account for _targets offset
            )
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(
                tokens
            ):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local : pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(
                num_tokens_local,
            )
            _targets = buf[1:].view(
                num_tokens_local,
            )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1 : len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(
                device="cuda", dtype=torch.int32, non_blocking=True
            ),
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, (
                "Num tokens must be divisible by world size"
            )
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main


@dataclass
class Hyperparameters:
    # data
    train_files: str = (
        "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    )
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1670  # number of iterations to run
    cooldown_frac: int = (
        0.5  # fraction of training spent cooling down the learning rate
    )
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = (
        125  # every how many steps to evaluate val loss? 0 for only at the end
    )
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws @classiclarryd


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = rank == 0  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(
    f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}"
)
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import

    return subprocess.run(
        ["nvidia-smi"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    ).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size)
    // (grad_accum_steps * world_size),
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [
    p
    for n, p in model.blocks.named_parameters()
    if p.ndim >= 2 and "embed" not in n
]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(
    hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)  # save the initial state
train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    ws = args.ws_schedule[
        step % len(args.ws_schedule)
    ]  # each window size is a new graph, need to warm up each
    model(inputs, targets, cum_seqlens, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws = get_ws(0)
for step in range(train_steps + 1):
    last_step = step == train_steps
    new_ws = get_ws(step)
    if new_ws != ws:
        model.apply_yarn(ws, new_ws)
        ws = new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (
        args.val_loss_every > 0 and step % args.val_loss_every == 0
    ):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(
            args.val_files,
            args.val_batch_size,
            -1,
            grad_accum_steps=grad_accum_steps,
            align_to_bos=False,
        )
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True,
        )
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(
                step=step,
                code=code,
                model=model.state_dict(),
                optimizers=[opt.state_dict() for opt in optimizers],
            )
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1)  # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (
        time.perf_counter() - t0
    )
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True,
    )

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.11 (main, Sep  2 2025, 14:20:58) [Clang 20.1.4 ]
Running PyTorch 2.9.0.dev20250718+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 11 09:40:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                  Off |
| N/A   33C    P0            120W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                  Off |
| N/A   34C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:65:00.0 Off |                  Off |
| N/A   33C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:68:00.0 Off |                  Off |
| N/A   30C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:85:00.0 Off |                  Off |
| N/A   31C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:86:00.0 Off |                  Off |
| N/A   33C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:E5:00.0 Off |                  Off |
| N/A   33C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E8:00.0 Off |                  Off |
| N/A   33C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1670 val_loss:10.8258 train_time:0ms step_avg:0.08ms
step:1/1670 train_time:297ms step_avg:296.71ms
step:2/1670 train_time:316ms step_avg:158.07ms
step:3/1670 train_time:383ms step_avg:127.80ms
step:4/1670 train_time:472ms step_avg:118.02ms
step:5/1670 train_time:562ms step_avg:112.35ms
step:6/1670 train_time:652ms step_avg:108.62ms
step:7/1670 train_time:742ms step_avg:105.94ms
step:8/1670 train_time:833ms step_avg:104.07ms
step:9/1670 train_time:922ms step_avg:102.48ms
step:10/1670 train_time:1013ms step_avg:101.26ms
step:11/1670 train_time:1103ms step_avg:100.24ms
step:12/1670 train_time:1198ms step_avg:99.79ms
step:13/1670 train_time:1292ms step_avg:99.37ms
step:14/1670 train_time:1384ms step_avg:98.87ms
step:15/1670 train_time:1475ms step_avg:98.36ms
step:16/1670 train_time:1566ms step_avg:97.89ms
step:17/1670 train_time:1658ms step_avg:97.51ms
step:18/1670 train_time:1748ms step_avg:97.12ms
step:19/1670 train_time:1838ms step_avg:96.73ms
step:20/1670 train_time:1928ms step_avg:96.40ms
step:21/1670 train_time:2018ms step_avg:96.12ms
step:22/1670 train_time:2110ms step_avg:95.90ms
step:23/1670 train_time:2202ms step_avg:95.73ms
step:24/1670 train_time:2296ms step_avg:95.65ms
step:25/1670 train_time:2388ms step_avg:95.51ms
step:26/1670 train_time:2481ms step_avg:95.43ms
step:27/1670 train_time:2571ms step_avg:95.21ms
step:28/1670 train_time:2662ms step_avg:95.07ms
step:29/1670 train_time:2752ms step_avg:94.91ms
step:30/1670 train_time:2843ms step_avg:94.76ms
step:31/1670 train_time:2934ms step_avg:94.63ms
step:32/1670 train_time:3024ms step_avg:94.51ms
step:33/1670 train_time:3115ms step_avg:94.39ms
step:34/1670 train_time:3208ms step_avg:94.36ms
step:35/1670 train_time:3300ms step_avg:94.29ms
step:36/1670 train_time:3392ms step_avg:94.24ms
step:37/1670 train_time:3484ms step_avg:94.17ms
step:38/1670 train_time:3577ms step_avg:94.13ms
step:39/1670 train_time:3668ms step_avg:94.06ms
step:40/1670 train_time:3759ms step_avg:93.98ms
step:41/1670 train_time:3850ms step_avg:93.90ms
step:42/1670 train_time:3941ms step_avg:93.84ms
step:43/1670 train_time:4032ms step_avg:93.76ms
step:44/1670 train_time:4123ms step_avg:93.71ms
step:45/1670 train_time:4214ms step_avg:93.64ms
step:46/1670 train_time:4306ms step_avg:93.61ms
step:47/1670 train_time:4398ms step_avg:93.57ms
step:48/1670 train_time:4489ms step_avg:93.53ms
step:49/1670 train_time:4580ms step_avg:93.48ms
step:50/1670 train_time:4672ms step_avg:93.44ms
step:51/1670 train_time:4768ms step_avg:93.50ms
step:52/1670 train_time:4855ms step_avg:93.36ms
step:53/1670 train_time:4945ms step_avg:93.31ms
step:54/1670 train_time:5037ms step_avg:93.28ms
step:55/1670 train_time:5128ms step_avg:93.23ms
step:56/1670 train_time:5218ms step_avg:93.18ms
step:57/1670 train_time:5310ms step_avg:93.16ms
step:58/1670 train_time:5402ms step_avg:93.15ms
step:59/1670 train_time:5494ms step_avg:93.11ms
step:60/1670 train_time:5584ms step_avg:93.07ms
step:61/1670 train_time:5676ms step_avg:93.04ms
step:62/1670 train_time:5766ms step_avg:93.00ms
step:63/1670 train_time:5858ms step_avg:92.99ms
step:64/1670 train_time:5950ms step_avg:92.96ms
step:65/1670 train_time:6041ms step_avg:92.93ms
step:66/1670 train_time:6132ms step_avg:92.90ms
step:67/1670 train_time:6222ms step_avg:92.86ms
step:68/1670 train_time:6313ms step_avg:92.84ms
step:69/1670 train_time:6404ms step_avg:92.82ms
step:70/1670 train_time:6496ms step_avg:92.80ms
step:71/1670 train_time:6586ms step_avg:92.77ms
step:72/1670 train_time:6679ms step_avg:92.76ms
step:73/1670 train_time:6771ms step_avg:92.75ms
step:74/1670 train_time:6863ms step_avg:92.74ms
step:75/1670 train_time:6954ms step_avg:92.72ms
step:76/1670 train_time:7046ms step_avg:92.71ms
step:77/1670 train_time:7138ms step_avg:92.70ms
step:78/1670 train_time:7228ms step_avg:92.67ms
step:79/1670 train_time:7319ms step_avg:92.65ms
step:80/1670 train_time:7410ms step_avg:92.62ms
step:81/1670 train_time:7502ms step_avg:92.61ms
step:82/1670 train_time:7592ms step_avg:92.58ms
step:83/1670 train_time:7684ms step_avg:92.58ms
step:84/1670 train_time:7776ms step_avg:92.58ms
step:85/1670 train_time:7867ms step_avg:92.56ms
step:86/1670 train_time:7958ms step_avg:92.54ms
step:87/1670 train_time:8049ms step_avg:92.52ms
step:88/1670 train_time:8140ms step_avg:92.50ms
step:89/1670 train_time:8231ms step_avg:92.48ms
step:90/1670 train_time:8322ms step_avg:92.47ms
step:91/1670 train_time:8414ms step_avg:92.46ms
step:92/1670 train_time:8504ms step_avg:92.44ms
step:93/1670 train_time:8595ms step_avg:92.42ms
step:94/1670 train_time:8686ms step_avg:92.40ms
step:95/1670 train_time:8776ms step_avg:92.38ms
step:96/1670 train_time:8868ms step_avg:92.38ms
step:97/1670 train_time:8961ms step_avg:92.39ms
step:98/1670 train_time:9053ms step_avg:92.37ms
step:99/1670 train_time:9143ms step_avg:92.36ms
step:100/1670 train_time:9234ms step_avg:92.34ms
step:101/1670 train_time:9325ms step_avg:92.32ms
step:102/1670 train_time:9416ms step_avg:92.31ms
step:103/1670 train_time:9507ms step_avg:92.30ms
step:104/1670 train_time:9597ms step_avg:92.28ms
step:105/1670 train_time:9688ms step_avg:92.27ms
step:106/1670 train_time:9780ms step_avg:92.26ms
step:107/1670 train_time:9873ms step_avg:92.27ms
step:108/1670 train_time:9964ms step_avg:92.26ms
step:109/1670 train_time:10055ms step_avg:92.25ms
step:110/1670 train_time:10146ms step_avg:92.23ms
step:111/1670 train_time:10237ms step_avg:92.23ms
step:112/1670 train_time:10328ms step_avg:92.21ms
step:113/1670 train_time:10420ms step_avg:92.21ms
step:114/1670 train_time:10510ms step_avg:92.20ms
step:115/1670 train_time:10601ms step_avg:92.18ms
step:116/1670 train_time:10692ms step_avg:92.18ms
step:117/1670 train_time:10784ms step_avg:92.17ms
step:118/1670 train_time:10877ms step_avg:92.17ms
step:119/1670 train_time:10967ms step_avg:92.16ms
step:120/1670 train_time:11059ms step_avg:92.16ms
step:121/1670 train_time:11150ms step_avg:92.15ms
step:122/1670 train_time:11242ms step_avg:92.15ms
step:123/1670 train_time:11334ms step_avg:92.14ms
step:124/1670 train_time:11425ms step_avg:92.13ms
step:125/1670 train_time:11516ms step_avg:92.13ms
step:125/1670 val_loss:4.3116 train_time:11606ms step_avg:92.85ms
step:126/1670 train_time:11629ms step_avg:92.30ms
step:127/1670 train_time:11698ms step_avg:92.11ms
step:128/1670 train_time:11800ms step_avg:92.18ms
step:129/1670 train_time:11893ms step_avg:92.19ms
step:130/1670 train_time:11983ms step_avg:92.18ms
step:131/1670 train_time:12073ms step_avg:92.16ms
step:132/1670 train_time:12164ms step_avg:92.15ms
step:133/1670 train_time:12253ms step_avg:92.13ms
step:134/1670 train_time:12343ms step_avg:92.11ms
step:135/1670 train_time:12434ms step_avg:92.10ms
step:136/1670 train_time:12525ms step_avg:92.09ms
step:137/1670 train_time:12616ms step_avg:92.09ms
step:138/1670 train_time:12708ms step_avg:92.09ms
step:139/1670 train_time:12801ms step_avg:92.10ms
step:140/1670 train_time:12893ms step_avg:92.10ms
step:141/1670 train_time:12985ms step_avg:92.09ms
step:142/1670 train_time:13075ms step_avg:92.08ms
step:143/1670 train_time:13166ms step_avg:92.07ms
step:144/1670 train_time:13255ms step_avg:92.05ms
step:145/1670 train_time:13345ms step_avg:92.04ms
step:146/1670 train_time:13437ms step_avg:92.03ms
step:147/1670 train_time:13528ms step_avg:92.03ms
step:148/1670 train_time:13619ms step_avg:92.02ms
step:149/1670 train_time:13712ms step_avg:92.03ms
step:150/1670 train_time:13805ms step_avg:92.03ms
step:151/1670 train_time:13897ms step_avg:92.03ms
step:152/1670 train_time:13989ms step_avg:92.03ms
step:153/1670 train_time:14079ms step_avg:92.02ms
step:154/1670 train_time:14169ms step_avg:92.01ms
step:155/1670 train_time:14259ms step_avg:91.99ms
step:156/1670 train_time:14349ms step_avg:91.98ms
step:157/1670 train_time:14440ms step_avg:91.97ms
step:158/1670 train_time:14530ms step_avg:91.96ms
step:159/1670 train_time:14622ms step_avg:91.96ms
step:160/1670 train_time:14713ms step_avg:91.96ms
step:161/1670 train_time:14805ms step_avg:91.96ms
step:162/1670 train_time:14897ms step_avg:91.96ms
step:163/1670 train_time:14988ms step_avg:91.95ms
step:164/1670 train_time:15079ms step_avg:91.94ms
step:165/1670 train_time:15169ms step_avg:91.93ms
step:166/1670 train_time:15259ms step_avg:91.92ms
step:167/1670 train_time:15349ms step_avg:91.91ms
step:168/1670 train_time:15439ms step_avg:91.90ms
step:169/1670 train_time:15529ms step_avg:91.89ms
step:170/1670 train_time:15621ms step_avg:91.89ms
step:171/1670 train_time:15712ms step_avg:91.88ms
step:172/1670 train_time:15803ms step_avg:91.88ms
step:173/1670 train_time:15895ms step_avg:91.88ms
step:174/1670 train_time:15986ms step_avg:91.88ms
step:175/1670 train_time:16077ms step_avg:91.87ms
step:176/1670 train_time:16168ms step_avg:91.86ms
step:177/1670 train_time:16258ms step_avg:91.85ms
step:178/1670 train_time:16349ms step_avg:91.85ms
step:179/1670 train_time:16439ms step_avg:91.84ms
step:180/1670 train_time:16530ms step_avg:91.83ms
step:181/1670 train_time:16620ms step_avg:91.82ms
step:182/1670 train_time:16711ms step_avg:91.82ms
step:183/1670 train_time:16801ms step_avg:91.81ms
step:184/1670 train_time:16892ms step_avg:91.81ms
step:185/1670 train_time:16983ms step_avg:91.80ms
step:186/1670 train_time:17075ms step_avg:91.80ms
step:187/1670 train_time:17167ms step_avg:91.80ms
step:188/1670 train_time:17258ms step_avg:91.80ms
step:189/1670 train_time:17347ms step_avg:91.78ms
step:190/1670 train_time:17438ms step_avg:91.78ms
step:191/1670 train_time:17528ms step_avg:91.77ms
step:192/1670 train_time:17620ms step_avg:91.77ms
step:193/1670 train_time:17711ms step_avg:91.77ms
step:194/1670 train_time:17802ms step_avg:91.76ms
step:195/1670 train_time:17893ms step_avg:91.76ms
step:196/1670 train_time:17984ms step_avg:91.75ms
step:197/1670 train_time:18075ms step_avg:91.75ms
step:198/1670 train_time:18167ms step_avg:91.75ms
step:199/1670 train_time:18258ms step_avg:91.75ms
step:200/1670 train_time:18348ms step_avg:91.74ms
step:201/1670 train_time:18439ms step_avg:91.73ms
step:202/1670 train_time:18529ms step_avg:91.73ms
step:203/1670 train_time:18620ms step_avg:91.73ms
step:204/1670 train_time:18710ms step_avg:91.72ms
step:205/1670 train_time:18801ms step_avg:91.71ms
step:206/1670 train_time:18892ms step_avg:91.71ms
step:207/1670 train_time:18984ms step_avg:91.71ms
step:208/1670 train_time:19074ms step_avg:91.70ms
step:209/1670 train_time:19166ms step_avg:91.70ms
step:210/1670 train_time:19259ms step_avg:91.71ms
step:211/1670 train_time:19350ms step_avg:91.70ms
step:212/1670 train_time:19441ms step_avg:91.70ms
step:213/1670 train_time:19690ms step_avg:92.44ms
step:214/1670 train_time:19760ms step_avg:92.34ms
step:215/1670 train_time:19850ms step_avg:92.32ms
step:216/1670 train_time:19941ms step_avg:92.32ms
step:217/1670 train_time:20030ms step_avg:92.31ms
step:218/1670 train_time:20121ms step_avg:92.30ms
step:219/1670 train_time:20210ms step_avg:92.28ms
step:220/1670 train_time:20301ms step_avg:92.28ms
step:221/1670 train_time:20391ms step_avg:92.27ms
step:222/1670 train_time:20481ms step_avg:92.26ms
step:223/1670 train_time:20574ms step_avg:92.26ms
step:224/1670 train_time:20670ms step_avg:92.28ms
step:225/1670 train_time:20764ms step_avg:92.29ms
step:226/1670 train_time:20856ms step_avg:92.28ms
step:227/1670 train_time:20946ms step_avg:92.27ms
step:228/1670 train_time:21037ms step_avg:92.27ms
step:229/1670 train_time:21127ms step_avg:92.26ms
step:230/1670 train_time:21217ms step_avg:92.25ms
step:231/1670 train_time:21306ms step_avg:92.24ms
step:232/1670 train_time:21399ms step_avg:92.24ms
step:233/1670 train_time:21487ms step_avg:92.22ms
step:234/1670 train_time:21579ms step_avg:92.22ms
step:235/1670 train_time:21674ms step_avg:92.23ms
step:236/1670 train_time:21767ms step_avg:92.23ms
step:237/1670 train_time:21859ms step_avg:92.23ms
step:238/1670 train_time:21950ms step_avg:92.23ms
step:239/1670 train_time:22042ms step_avg:92.22ms
step:240/1670 train_time:22132ms step_avg:92.22ms
step:241/1670 train_time:22223ms step_avg:92.21ms
step:242/1670 train_time:22313ms step_avg:92.20ms
step:243/1670 train_time:22403ms step_avg:92.19ms
step:244/1670 train_time:22493ms step_avg:92.19ms
step:245/1670 train_time:22585ms step_avg:92.18ms
step:246/1670 train_time:22676ms step_avg:92.18ms
step:247/1670 train_time:22769ms step_avg:92.18ms
step:248/1670 train_time:22861ms step_avg:92.18ms
step:249/1670 train_time:22952ms step_avg:92.18ms
step:250/1670 train_time:23044ms step_avg:92.18ms
step:250/1670 val_loss:3.9690 train_time:23134ms step_avg:92.54ms
step:251/1670 train_time:23154ms step_avg:92.25ms
step:252/1670 train_time:23228ms step_avg:92.17ms
step:253/1670 train_time:23319ms step_avg:92.17ms
step:254/1670 train_time:23409ms step_avg:92.16ms
step:255/1670 train_time:23500ms step_avg:92.16ms
step:256/1670 train_time:23590ms step_avg:92.15ms
step:257/1670 train_time:23680ms step_avg:92.14ms
step:258/1670 train_time:23773ms step_avg:92.14ms
step:259/1670 train_time:23864ms step_avg:92.14ms
step:260/1670 train_time:23955ms step_avg:92.14ms
step:261/1670 train_time:24045ms step_avg:92.13ms
step:262/1670 train_time:24138ms step_avg:92.13ms
step:263/1670 train_time:24229ms step_avg:92.13ms
step:264/1670 train_time:24321ms step_avg:92.12ms
step:265/1670 train_time:24412ms step_avg:92.12ms
step:266/1670 train_time:24503ms step_avg:92.12ms
step:267/1670 train_time:24594ms step_avg:92.11ms
step:268/1670 train_time:24685ms step_avg:92.11ms
step:269/1670 train_time:24776ms step_avg:92.10ms
step:270/1670 train_time:24866ms step_avg:92.10ms
step:271/1670 train_time:24957ms step_avg:92.09ms
step:272/1670 train_time:25048ms step_avg:92.09ms
step:273/1670 train_time:25140ms step_avg:92.09ms
step:274/1670 train_time:25233ms step_avg:92.09ms
step:275/1670 train_time:25324ms step_avg:92.09ms
step:276/1670 train_time:25415ms step_avg:92.08ms
step:277/1670 train_time:25506ms step_avg:92.08ms
step:278/1670 train_time:25596ms step_avg:92.07ms
step:279/1670 train_time:25687ms step_avg:92.07ms
step:280/1670 train_time:25778ms step_avg:92.06ms
step:281/1670 train_time:25869ms step_avg:92.06ms
step:282/1670 train_time:25961ms step_avg:92.06ms
step:283/1670 train_time:26052ms step_avg:92.05ms
step:284/1670 train_time:26143ms step_avg:92.05ms
step:285/1670 train_time:26235ms step_avg:92.05ms
step:286/1670 train_time:26326ms step_avg:92.05ms
step:287/1670 train_time:26417ms step_avg:92.04ms
step:288/1670 train_time:26507ms step_avg:92.04ms
step:289/1670 train_time:26599ms step_avg:92.04ms
step:290/1670 train_time:26690ms step_avg:92.03ms
step:291/1670 train_time:26780ms step_avg:92.03ms
step:292/1670 train_time:26871ms step_avg:92.02ms
step:293/1670 train_time:26963ms step_avg:92.02ms
step:294/1670 train_time:27052ms step_avg:92.01ms
step:295/1670 train_time:27143ms step_avg:92.01ms
step:296/1670 train_time:27235ms step_avg:92.01ms
step:297/1670 train_time:27326ms step_avg:92.01ms
step:298/1670 train_time:27417ms step_avg:92.00ms
step:299/1670 train_time:27508ms step_avg:92.00ms
step:300/1670 train_time:27599ms step_avg:92.00ms
step:301/1670 train_time:27689ms step_avg:91.99ms
step:302/1670 train_time:27780ms step_avg:91.99ms
step:303/1670 train_time:27871ms step_avg:91.98ms
step:304/1670 train_time:27962ms step_avg:91.98ms
step:305/1670 train_time:28053ms step_avg:91.98ms
step:306/1670 train_time:28144ms step_avg:91.97ms
step:307/1670 train_time:28236ms step_avg:91.97ms
step:308/1670 train_time:28327ms step_avg:91.97ms
step:309/1670 train_time:28418ms step_avg:91.97ms
step:310/1670 train_time:28509ms step_avg:91.96ms
step:311/1670 train_time:28600ms step_avg:91.96ms
step:312/1670 train_time:28691ms step_avg:91.96ms
step:313/1670 train_time:28781ms step_avg:91.95ms
step:314/1670 train_time:28872ms step_avg:91.95ms
step:315/1670 train_time:28963ms step_avg:91.95ms
step:316/1670 train_time:29054ms step_avg:91.94ms
step:317/1670 train_time:29145ms step_avg:91.94ms
step:318/1670 train_time:29236ms step_avg:91.94ms
step:319/1670 train_time:29327ms step_avg:91.93ms
step:320/1670 train_time:29419ms step_avg:91.93ms
step:321/1670 train_time:29509ms step_avg:91.93ms
step:322/1670 train_time:29601ms step_avg:91.93ms
step:323/1670 train_time:29692ms step_avg:91.93ms
step:324/1670 train_time:29782ms step_avg:91.92ms
step:325/1670 train_time:29874ms step_avg:91.92ms
step:326/1670 train_time:29964ms step_avg:91.91ms
step:327/1670 train_time:30054ms step_avg:91.91ms
step:328/1670 train_time:30145ms step_avg:91.91ms
step:329/1670 train_time:30237ms step_avg:91.90ms
step:330/1670 train_time:30327ms step_avg:91.90ms
step:331/1670 train_time:30419ms step_avg:91.90ms
step:332/1670 train_time:30510ms step_avg:91.90ms
step:333/1670 train_time:30601ms step_avg:91.89ms
step:334/1670 train_time:30691ms step_avg:91.89ms
step:335/1670 train_time:30781ms step_avg:91.88ms
step:336/1670 train_time:30873ms step_avg:91.88ms
step:337/1670 train_time:30964ms step_avg:91.88ms
step:338/1670 train_time:31055ms step_avg:91.88ms
step:339/1670 train_time:31145ms step_avg:91.87ms
step:340/1670 train_time:31236ms step_avg:91.87ms
step:341/1670 train_time:31327ms step_avg:91.87ms
step:342/1670 train_time:31418ms step_avg:91.87ms
step:343/1670 train_time:31509ms step_avg:91.86ms
step:344/1670 train_time:31600ms step_avg:91.86ms
step:345/1670 train_time:31690ms step_avg:91.86ms
step:346/1670 train_time:31781ms step_avg:91.85ms
step:347/1670 train_time:31873ms step_avg:91.85ms
step:348/1670 train_time:31963ms step_avg:91.85ms
step:349/1670 train_time:32055ms step_avg:91.85ms
step:350/1670 train_time:32145ms step_avg:91.84ms
step:351/1670 train_time:32237ms step_avg:91.84ms
step:352/1670 train_time:32328ms step_avg:91.84ms
step:353/1670 train_time:32420ms step_avg:91.84ms
step:354/1670 train_time:32511ms step_avg:91.84ms
step:355/1670 train_time:32604ms step_avg:91.84ms
step:356/1670 train_time:32694ms step_avg:91.84ms
step:357/1670 train_time:32784ms step_avg:91.83ms
step:358/1670 train_time:32875ms step_avg:91.83ms
step:359/1670 train_time:32966ms step_avg:91.83ms
step:360/1670 train_time:33057ms step_avg:91.82ms
step:361/1670 train_time:33147ms step_avg:91.82ms
step:362/1670 train_time:33239ms step_avg:91.82ms
step:363/1670 train_time:33330ms step_avg:91.82ms
step:364/1670 train_time:33422ms step_avg:91.82ms
step:365/1670 train_time:33513ms step_avg:91.82ms
step:366/1670 train_time:33604ms step_avg:91.81ms
step:367/1670 train_time:33695ms step_avg:91.81ms
step:368/1670 train_time:33786ms step_avg:91.81ms
step:369/1670 train_time:33876ms step_avg:91.81ms
step:370/1670 train_time:33967ms step_avg:91.80ms
step:371/1670 train_time:34058ms step_avg:91.80ms
step:372/1670 train_time:34149ms step_avg:91.80ms
step:373/1670 train_time:34240ms step_avg:91.80ms
step:374/1670 train_time:34331ms step_avg:91.79ms
step:375/1670 train_time:34423ms step_avg:91.80ms
step:375/1670 val_loss:3.8129 train_time:34514ms step_avg:92.04ms
step:376/1670 train_time:34533ms step_avg:91.84ms
step:377/1670 train_time:34608ms step_avg:91.80ms
step:378/1670 train_time:34701ms step_avg:91.80ms
step:379/1670 train_time:34792ms step_avg:91.80ms
step:380/1670 train_time:34882ms step_avg:91.79ms
step:381/1670 train_time:34972ms step_avg:91.79ms
step:382/1670 train_time:35062ms step_avg:91.79ms
step:383/1670 train_time:35153ms step_avg:91.78ms
step:384/1670 train_time:35243ms step_avg:91.78ms
step:385/1670 train_time:35335ms step_avg:91.78ms
step:386/1670 train_time:35426ms step_avg:91.78ms
step:387/1670 train_time:35518ms step_avg:91.78ms
step:388/1670 train_time:35613ms step_avg:91.79ms
step:389/1670 train_time:35706ms step_avg:91.79ms
step:390/1670 train_time:35796ms step_avg:91.78ms
step:391/1670 train_time:35886ms step_avg:91.78ms
step:392/1670 train_time:35976ms step_avg:91.78ms
step:393/1670 train_time:36067ms step_avg:91.77ms
step:394/1670 train_time:36157ms step_avg:91.77ms
step:395/1670 train_time:36248ms step_avg:91.77ms
step:396/1670 train_time:36338ms step_avg:91.76ms
step:397/1670 train_time:36431ms step_avg:91.77ms
step:398/1670 train_time:36522ms step_avg:91.76ms
step:399/1670 train_time:36616ms step_avg:91.77ms
step:400/1670 train_time:36708ms step_avg:91.77ms
step:401/1670 train_time:36799ms step_avg:91.77ms
step:402/1670 train_time:36889ms step_avg:91.76ms
step:403/1670 train_time:36979ms step_avg:91.76ms
step:404/1670 train_time:37070ms step_avg:91.76ms
step:405/1670 train_time:37160ms step_avg:91.75ms
step:406/1670 train_time:37251ms step_avg:91.75ms
step:407/1670 train_time:37341ms step_avg:91.75ms
step:408/1670 train_time:37433ms step_avg:91.75ms
step:409/1670 train_time:37524ms step_avg:91.74ms
step:410/1670 train_time:37617ms step_avg:91.75ms
step:411/1670 train_time:37708ms step_avg:91.75ms
step:412/1670 train_time:37800ms step_avg:91.75ms
step:413/1670 train_time:37890ms step_avg:91.74ms
step:414/1670 train_time:37981ms step_avg:91.74ms
step:415/1670 train_time:38071ms step_avg:91.74ms
step:416/1670 train_time:38162ms step_avg:91.74ms
step:417/1670 train_time:38252ms step_avg:91.73ms
step:418/1670 train_time:38343ms step_avg:91.73ms
step:419/1670 train_time:38434ms step_avg:91.73ms
step:420/1670 train_time:38526ms step_avg:91.73ms
step:421/1670 train_time:38618ms step_avg:91.73ms
step:422/1670 train_time:38710ms step_avg:91.73ms
step:423/1670 train_time:38802ms step_avg:91.73ms
step:424/1670 train_time:38893ms step_avg:91.73ms
step:425/1670 train_time:39144ms step_avg:92.10ms
step:426/1670 train_time:39212ms step_avg:92.05ms
step:427/1670 train_time:39302ms step_avg:92.04ms
step:428/1670 train_time:39392ms step_avg:92.04ms
step:429/1670 train_time:39482ms step_avg:92.03ms
step:430/1670 train_time:39572ms step_avg:92.03ms
step:431/1670 train_time:39662ms step_avg:92.02ms
step:432/1670 train_time:39752ms step_avg:92.02ms
step:433/1670 train_time:39843ms step_avg:92.02ms
step:434/1670 train_time:39934ms step_avg:92.01ms
step:435/1670 train_time:40027ms step_avg:92.02ms
step:436/1670 train_time:40122ms step_avg:92.02ms
step:437/1670 train_time:40215ms step_avg:92.02ms
step:438/1670 train_time:40306ms step_avg:92.02ms
step:439/1670 train_time:40397ms step_avg:92.02ms
step:440/1670 train_time:40489ms step_avg:92.02ms
step:441/1670 train_time:40579ms step_avg:92.02ms
step:442/1670 train_time:40669ms step_avg:92.01ms
step:443/1670 train_time:40759ms step_avg:92.01ms
step:444/1670 train_time:40850ms step_avg:92.00ms
step:445/1670 train_time:40940ms step_avg:92.00ms
step:446/1670 train_time:41032ms step_avg:92.00ms
step:447/1670 train_time:41124ms step_avg:92.00ms
step:448/1670 train_time:41218ms step_avg:92.00ms
step:449/1670 train_time:41310ms step_avg:92.00ms
step:450/1670 train_time:41401ms step_avg:92.00ms
step:451/1670 train_time:41492ms step_avg:92.00ms
step:452/1670 train_time:41582ms step_avg:92.00ms
step:453/1670 train_time:41673ms step_avg:91.99ms
step:454/1670 train_time:41763ms step_avg:91.99ms
step:455/1670 train_time:41853ms step_avg:91.98ms
step:456/1670 train_time:41944ms step_avg:91.98ms
step:457/1670 train_time:42036ms step_avg:91.98ms
step:458/1670 train_time:42128ms step_avg:91.98ms
step:459/1670 train_time:42220ms step_avg:91.98ms
step:460/1670 train_time:42312ms step_avg:91.98ms
step:461/1670 train_time:42402ms step_avg:91.98ms
step:462/1670 train_time:42493ms step_avg:91.98ms
step:463/1670 train_time:42584ms step_avg:91.97ms
step:464/1670 train_time:42675ms step_avg:91.97ms
step:465/1670 train_time:42765ms step_avg:91.97ms
step:466/1670 train_time:42856ms step_avg:91.97ms
step:467/1670 train_time:42947ms step_avg:91.96ms
step:468/1670 train_time:43039ms step_avg:91.96ms
step:469/1670 train_time:43130ms step_avg:91.96ms
step:470/1670 train_time:43221ms step_avg:91.96ms
step:471/1670 train_time:43313ms step_avg:91.96ms
step:472/1670 train_time:43404ms step_avg:91.96ms
step:473/1670 train_time:43496ms step_avg:91.96ms
step:474/1670 train_time:43586ms step_avg:91.95ms
step:475/1670 train_time:43676ms step_avg:91.95ms
step:476/1670 train_time:43767ms step_avg:91.95ms
step:477/1670 train_time:43857ms step_avg:91.94ms
step:478/1670 train_time:43947ms step_avg:91.94ms
step:479/1670 train_time:44038ms step_avg:91.94ms
step:480/1670 train_time:44129ms step_avg:91.94ms
step:481/1670 train_time:44220ms step_avg:91.93ms
step:482/1670 train_time:44311ms step_avg:91.93ms
step:483/1670 train_time:44402ms step_avg:91.93ms
step:484/1670 train_time:44494ms step_avg:91.93ms
step:485/1670 train_time:44585ms step_avg:91.93ms
step:486/1670 train_time:44676ms step_avg:91.93ms
step:487/1670 train_time:44767ms step_avg:91.92ms
step:488/1670 train_time:44857ms step_avg:91.92ms
step:489/1670 train_time:44948ms step_avg:91.92ms
step:490/1670 train_time:45038ms step_avg:91.91ms
step:491/1670 train_time:45129ms step_avg:91.91ms
step:492/1670 train_time:45220ms step_avg:91.91ms
step:493/1670 train_time:45312ms step_avg:91.91ms
step:494/1670 train_time:45403ms step_avg:91.91ms
step:495/1670 train_time:45494ms step_avg:91.91ms
step:496/1670 train_time:45585ms step_avg:91.91ms
step:497/1670 train_time:45677ms step_avg:91.90ms
step:498/1670 train_time:45768ms step_avg:91.90ms
step:499/1670 train_time:45858ms step_avg:91.90ms
step:500/1670 train_time:45948ms step_avg:91.90ms
step:500/1670 val_loss:3.7130 train_time:46038ms step_avg:92.08ms
step:501/1670 train_time:46058ms step_avg:91.93ms
step:502/1670 train_time:46132ms step_avg:91.90ms
step:503/1670 train_time:46224ms step_avg:91.90ms
step:504/1670 train_time:46314ms step_avg:91.89ms
step:505/1670 train_time:46404ms step_avg:91.89ms
step:506/1670 train_time:46494ms step_avg:91.88ms
step:507/1670 train_time:46584ms step_avg:91.88ms
step:508/1670 train_time:46673ms step_avg:91.88ms
step:509/1670 train_time:46764ms step_avg:91.87ms
step:510/1670 train_time:46856ms step_avg:91.87ms
step:511/1670 train_time:46947ms step_avg:91.87ms
step:512/1670 train_time:47041ms step_avg:91.88ms
step:513/1670 train_time:47132ms step_avg:91.88ms
step:514/1670 train_time:47224ms step_avg:91.88ms
step:515/1670 train_time:47318ms step_avg:91.88ms
step:516/1670 train_time:47408ms step_avg:91.88ms
step:517/1670 train_time:47498ms step_avg:91.87ms
step:518/1670 train_time:47589ms step_avg:91.87ms
step:519/1670 train_time:47680ms step_avg:91.87ms
step:520/1670 train_time:47770ms step_avg:91.87ms
step:521/1670 train_time:47861ms step_avg:91.86ms
step:522/1670 train_time:47952ms step_avg:91.86ms
step:523/1670 train_time:48044ms step_avg:91.86ms
step:524/1670 train_time:48136ms step_avg:91.86ms
step:525/1670 train_time:48227ms step_avg:91.86ms
step:526/1670 train_time:48318ms step_avg:91.86ms
step:527/1670 train_time:48409ms step_avg:91.86ms
step:528/1670 train_time:48500ms step_avg:91.86ms
step:529/1670 train_time:48590ms step_avg:91.85ms
step:530/1670 train_time:48682ms step_avg:91.85ms
step:531/1670 train_time:48772ms step_avg:91.85ms
step:532/1670 train_time:48864ms step_avg:91.85ms
step:533/1670 train_time:48955ms step_avg:91.85ms
step:534/1670 train_time:49047ms step_avg:91.85ms
step:535/1670 train_time:49139ms step_avg:91.85ms
step:536/1670 train_time:49231ms step_avg:91.85ms
step:537/1670 train_time:49322ms step_avg:91.85ms
step:538/1670 train_time:49413ms step_avg:91.85ms
step:539/1670 train_time:49504ms step_avg:91.84ms
step:540/1670 train_time:49594ms step_avg:91.84ms
step:541/1670 train_time:49684ms step_avg:91.84ms
step:542/1670 train_time:49776ms step_avg:91.84ms
step:543/1670 train_time:49868ms step_avg:91.84ms
step:544/1670 train_time:49960ms step_avg:91.84ms
step:545/1670 train_time:50050ms step_avg:91.84ms
step:546/1670 train_time:50142ms step_avg:91.83ms
step:547/1670 train_time:50233ms step_avg:91.83ms
step:548/1670 train_time:50324ms step_avg:91.83ms
step:549/1670 train_time:50416ms step_avg:91.83ms
step:550/1670 train_time:50507ms step_avg:91.83ms
step:551/1670 train_time:50598ms step_avg:91.83ms
step:552/1670 train_time:50688ms step_avg:91.83ms
step:553/1670 train_time:50778ms step_avg:91.82ms
step:554/1670 train_time:50869ms step_avg:91.82ms
step:555/1670 train_time:50961ms step_avg:91.82ms
step:556/1670 train_time:51052ms step_avg:91.82ms
step:557/1670 train_time:51144ms step_avg:91.82ms
step:558/1670 train_time:51427ms step_avg:92.16ms
step:559/1670 train_time:51500ms step_avg:92.13ms
step:560/1670 train_time:51591ms step_avg:92.13ms
step:561/1670 train_time:51683ms step_avg:92.13ms
step:562/1670 train_time:51774ms step_avg:92.12ms
step:563/1670 train_time:51865ms step_avg:92.12ms
step:564/1670 train_time:51956ms step_avg:92.12ms
step:565/1670 train_time:52047ms step_avg:92.12ms
step:566/1670 train_time:52138ms step_avg:92.12ms
step:567/1670 train_time:52229ms step_avg:92.11ms
step:568/1670 train_time:52324ms step_avg:92.12ms
step:569/1670 train_time:52420ms step_avg:92.13ms
step:570/1670 train_time:52514ms step_avg:92.13ms
step:571/1670 train_time:52606ms step_avg:92.13ms
step:572/1670 train_time:52698ms step_avg:92.13ms
step:573/1670 train_time:52789ms step_avg:92.13ms
step:574/1670 train_time:52881ms step_avg:92.13ms
step:575/1670 train_time:52972ms step_avg:92.12ms
step:576/1670 train_time:53063ms step_avg:92.12ms
step:577/1670 train_time:53156ms step_avg:92.12ms
step:578/1670 train_time:53248ms step_avg:92.13ms
step:579/1670 train_time:53342ms step_avg:92.13ms
step:580/1670 train_time:53435ms step_avg:92.13ms
step:581/1670 train_time:53528ms step_avg:92.13ms
step:582/1670 train_time:53620ms step_avg:92.13ms
step:583/1670 train_time:53712ms step_avg:92.13ms
step:584/1670 train_time:53804ms step_avg:92.13ms
step:585/1670 train_time:53896ms step_avg:92.13ms
step:586/1670 train_time:53988ms step_avg:92.13ms
step:587/1670 train_time:54080ms step_avg:92.13ms
step:588/1670 train_time:54172ms step_avg:92.13ms
step:589/1670 train_time:54265ms step_avg:92.13ms
step:590/1670 train_time:54357ms step_avg:92.13ms
step:591/1670 train_time:54450ms step_avg:92.13ms
step:592/1670 train_time:54544ms step_avg:92.13ms
step:593/1670 train_time:54636ms step_avg:92.13ms
step:594/1670 train_time:54728ms step_avg:92.13ms
step:595/1670 train_time:54819ms step_avg:92.13ms
step:596/1670 train_time:54910ms step_avg:92.13ms
step:597/1670 train_time:55002ms step_avg:92.13ms
step:598/1670 train_time:55093ms step_avg:92.13ms
step:599/1670 train_time:55185ms step_avg:92.13ms
step:600/1670 train_time:55277ms step_avg:92.13ms
step:601/1670 train_time:55369ms step_avg:92.13ms
step:602/1670 train_time:55463ms step_avg:92.13ms
step:603/1670 train_time:55556ms step_avg:92.13ms
step:604/1670 train_time:55648ms step_avg:92.13ms
step:605/1670 train_time:55741ms step_avg:92.13ms
step:606/1670 train_time:55832ms step_avg:92.13ms
step:607/1670 train_time:55927ms step_avg:92.14ms
step:608/1670 train_time:56019ms step_avg:92.14ms
step:609/1670 train_time:56110ms step_avg:92.13ms
step:610/1670 train_time:56202ms step_avg:92.14ms
step:611/1670 train_time:56294ms step_avg:92.13ms
step:612/1670 train_time:56387ms step_avg:92.14ms
step:613/1670 train_time:56481ms step_avg:92.14ms
step:614/1670 train_time:56573ms step_avg:92.14ms
step:615/1670 train_time:56667ms step_avg:92.14ms
step:616/1670 train_time:56759ms step_avg:92.14ms
step:617/1670 train_time:56851ms step_avg:92.14ms
step:618/1670 train_time:56943ms step_avg:92.14ms
step:619/1670 train_time:57034ms step_avg:92.14ms
step:620/1670 train_time:57127ms step_avg:92.14ms
step:621/1670 train_time:57219ms step_avg:92.14ms
step:622/1670 train_time:57311ms step_avg:92.14ms
step:623/1670 train_time:57404ms step_avg:92.14ms
step:624/1670 train_time:57496ms step_avg:92.14ms
step:625/1670 train_time:57589ms step_avg:92.14ms
step:625/1670 val_loss:3.6120 train_time:57683ms step_avg:92.29ms
step:626/1670 train_time:57703ms step_avg:92.18ms
step:627/1670 train_time:57783ms step_avg:92.16ms
step:628/1670 train_time:57883ms step_avg:92.17ms
step:629/1670 train_time:57976ms step_avg:92.17ms
step:630/1670 train_time:58068ms step_avg:92.17ms
step:631/1670 train_time:58159ms step_avg:92.17ms
step:632/1670 train_time:58250ms step_avg:92.17ms
step:633/1670 train_time:58341ms step_avg:92.17ms
step:634/1670 train_time:58432ms step_avg:92.16ms
step:635/1670 train_time:58523ms step_avg:92.16ms
step:636/1670 train_time:58615ms step_avg:92.16ms
step:637/1670 train_time:58707ms step_avg:92.16ms
step:638/1670 train_time:58803ms step_avg:92.17ms
step:639/1670 train_time:59038ms step_avg:92.39ms
step:640/1670 train_time:59110ms step_avg:92.36ms
step:641/1670 train_time:59201ms step_avg:92.36ms
step:642/1670 train_time:59292ms step_avg:92.36ms
step:643/1670 train_time:59383ms step_avg:92.35ms
step:644/1670 train_time:59475ms step_avg:92.35ms
step:645/1670 train_time:59566ms step_avg:92.35ms
step:646/1670 train_time:59657ms step_avg:92.35ms
step:647/1670 train_time:59747ms step_avg:92.35ms
step:648/1670 train_time:59838ms step_avg:92.34ms
step:649/1670 train_time:59936ms step_avg:92.35ms
step:650/1670 train_time:60033ms step_avg:92.36ms
step:651/1670 train_time:60126ms step_avg:92.36ms
step:652/1670 train_time:60218ms step_avg:92.36ms
step:653/1670 train_time:60310ms step_avg:92.36ms
step:654/1670 train_time:60401ms step_avg:92.36ms
step:655/1670 train_time:60493ms step_avg:92.36ms
step:656/1670 train_time:60584ms step_avg:92.35ms
step:657/1670 train_time:60675ms step_avg:92.35ms
step:658/1670 train_time:60766ms step_avg:92.35ms
step:659/1670 train_time:60859ms step_avg:92.35ms
step:660/1670 train_time:60953ms step_avg:92.35ms
step:661/1670 train_time:61048ms step_avg:92.36ms
step:662/1670 train_time:61140ms step_avg:92.36ms
step:663/1670 train_time:61233ms step_avg:92.36ms
step:664/1670 train_time:61326ms step_avg:92.36ms
step:665/1670 train_time:61417ms step_avg:92.36ms
step:666/1670 train_time:61509ms step_avg:92.36ms
step:667/1670 train_time:61602ms step_avg:92.36ms
step:668/1670 train_time:61694ms step_avg:92.36ms
step:669/1670 train_time:61785ms step_avg:92.35ms
step:670/1670 train_time:61877ms step_avg:92.35ms
step:671/1670 train_time:61971ms step_avg:92.36ms
step:672/1670 train_time:62065ms step_avg:92.36ms
step:673/1670 train_time:62158ms step_avg:92.36ms
step:674/1670 train_time:62251ms step_avg:92.36ms
step:675/1670 train_time:62344ms step_avg:92.36ms
step:676/1670 train_time:62436ms step_avg:92.36ms
step:677/1670 train_time:62528ms step_avg:92.36ms
step:678/1670 train_time:62618ms step_avg:92.36ms
step:679/1670 train_time:62710ms step_avg:92.36ms
step:680/1670 train_time:62802ms step_avg:92.36ms
step:681/1670 train_time:62894ms step_avg:92.36ms
step:682/1670 train_time:62987ms step_avg:92.36ms
step:683/1670 train_time:63080ms step_avg:92.36ms
step:684/1670 train_time:63172ms step_avg:92.36ms
step:685/1670 train_time:63265ms step_avg:92.36ms
step:686/1670 train_time:63356ms step_avg:92.36ms
step:687/1670 train_time:63449ms step_avg:92.36ms
step:688/1670 train_time:63541ms step_avg:92.36ms
step:689/1670 train_time:63632ms step_avg:92.35ms
step:690/1670 train_time:63723ms step_avg:92.35ms
step:691/1670 train_time:63816ms step_avg:92.35ms
step:692/1670 train_time:63909ms step_avg:92.35ms
step:693/1670 train_time:64003ms step_avg:92.36ms
step:694/1670 train_time:64096ms step_avg:92.36ms
step:695/1670 train_time:64188ms step_avg:92.36ms
step:696/1670 train_time:64281ms step_avg:92.36ms
step:697/1670 train_time:64375ms step_avg:92.36ms
step:698/1670 train_time:64464ms step_avg:92.36ms
step:699/1670 train_time:64555ms step_avg:92.35ms
step:700/1670 train_time:64648ms step_avg:92.35ms
step:701/1670 train_time:64739ms step_avg:92.35ms
step:702/1670 train_time:64832ms step_avg:92.35ms
step:703/1670 train_time:64924ms step_avg:92.35ms
step:704/1670 train_time:65016ms step_avg:92.35ms
step:705/1670 train_time:65109ms step_avg:92.35ms
step:706/1670 train_time:65201ms step_avg:92.35ms
step:707/1670 train_time:65294ms step_avg:92.35ms
step:708/1670 train_time:65387ms step_avg:92.35ms
step:709/1670 train_time:65479ms step_avg:92.35ms
step:710/1670 train_time:65573ms step_avg:92.36ms
step:711/1670 train_time:65666ms step_avg:92.36ms
step:712/1670 train_time:65757ms step_avg:92.36ms
step:713/1670 train_time:65849ms step_avg:92.36ms
step:714/1670 train_time:65941ms step_avg:92.35ms
step:715/1670 train_time:66033ms step_avg:92.35ms
step:716/1670 train_time:66125ms step_avg:92.35ms
step:717/1670 train_time:66218ms step_avg:92.35ms
step:718/1670 train_time:66312ms step_avg:92.36ms
step:719/1670 train_time:66404ms step_avg:92.36ms
step:720/1670 train_time:66495ms step_avg:92.35ms
step:721/1670 train_time:66588ms step_avg:92.35ms
step:722/1670 train_time:66680ms step_avg:92.35ms
step:723/1670 train_time:66772ms step_avg:92.35ms
step:724/1670 train_time:66864ms step_avg:92.35ms
step:725/1670 train_time:66956ms step_avg:92.35ms
step:726/1670 train_time:67048ms step_avg:92.35ms
step:727/1670 train_time:67140ms step_avg:92.35ms
step:728/1670 train_time:67233ms step_avg:92.35ms
step:729/1670 train_time:67325ms step_avg:92.35ms
step:730/1670 train_time:67417ms step_avg:92.35ms
step:731/1670 train_time:67509ms step_avg:92.35ms
step:732/1670 train_time:67601ms step_avg:92.35ms
step:733/1670 train_time:67693ms step_avg:92.35ms
step:734/1670 train_time:67786ms step_avg:92.35ms
step:735/1670 train_time:67878ms step_avg:92.35ms
step:736/1670 train_time:67971ms step_avg:92.35ms
step:737/1670 train_time:68063ms step_avg:92.35ms
step:738/1670 train_time:68155ms step_avg:92.35ms
step:739/1670 train_time:68247ms step_avg:92.35ms
step:740/1670 train_time:68339ms step_avg:92.35ms
step:741/1670 train_time:68432ms step_avg:92.35ms
step:742/1670 train_time:68525ms step_avg:92.35ms
step:743/1670 train_time:68617ms step_avg:92.35ms
step:744/1670 train_time:68709ms step_avg:92.35ms
step:745/1670 train_time:68801ms step_avg:92.35ms
step:746/1670 train_time:68895ms step_avg:92.35ms
step:747/1670 train_time:68988ms step_avg:92.35ms
step:748/1670 train_time:69080ms step_avg:92.35ms
step:749/1670 train_time:69172ms step_avg:92.35ms
step:750/1670 train_time:69264ms step_avg:92.35ms
step:750/1670 val_loss:3.5608 train_time:69355ms step_avg:92.47ms
step:751/1670 train_time:69375ms step_avg:92.38ms
step:752/1670 train_time:69449ms step_avg:92.35ms
step:753/1670 train_time:69542ms step_avg:92.35ms
step:754/1670 train_time:69634ms step_avg:92.35ms
step:755/1670 train_time:69726ms step_avg:92.35ms
step:756/1670 train_time:69818ms step_avg:92.35ms
step:757/1670 train_time:69909ms step_avg:92.35ms
step:758/1670 train_time:70001ms step_avg:92.35ms
step:759/1670 train_time:70094ms step_avg:92.35ms
step:760/1670 train_time:70189ms step_avg:92.35ms
step:761/1670 train_time:70281ms step_avg:92.35ms
step:762/1670 train_time:70374ms step_avg:92.35ms
step:763/1670 train_time:70467ms step_avg:92.36ms
step:764/1670 train_time:70560ms step_avg:92.36ms
step:765/1670 train_time:70652ms step_avg:92.36ms
step:766/1670 train_time:70745ms step_avg:92.36ms
step:767/1670 train_time:70836ms step_avg:92.35ms
step:768/1670 train_time:70929ms step_avg:92.36ms
step:769/1670 train_time:71021ms step_avg:92.36ms
step:770/1670 train_time:71113ms step_avg:92.35ms
step:771/1670 train_time:71205ms step_avg:92.35ms
step:772/1670 train_time:71298ms step_avg:92.36ms
step:773/1670 train_time:71391ms step_avg:92.36ms
step:774/1670 train_time:71486ms step_avg:92.36ms
step:775/1670 train_time:71577ms step_avg:92.36ms
step:776/1670 train_time:71671ms step_avg:92.36ms
step:777/1670 train_time:71763ms step_avg:92.36ms
step:778/1670 train_time:71855ms step_avg:92.36ms
step:779/1670 train_time:71949ms step_avg:92.36ms
step:780/1670 train_time:72041ms step_avg:92.36ms
step:781/1670 train_time:72133ms step_avg:92.36ms
step:782/1670 train_time:72226ms step_avg:92.36ms
step:783/1670 train_time:72318ms step_avg:92.36ms
step:784/1670 train_time:72411ms step_avg:92.36ms
step:785/1670 train_time:72505ms step_avg:92.36ms
step:786/1670 train_time:72598ms step_avg:92.36ms
step:787/1670 train_time:72691ms step_avg:92.36ms
step:788/1670 train_time:72783ms step_avg:92.36ms
step:789/1670 train_time:72875ms step_avg:92.36ms
step:790/1670 train_time:72967ms step_avg:92.36ms
step:791/1670 train_time:73060ms step_avg:92.36ms
step:792/1670 train_time:73152ms step_avg:92.36ms
step:793/1670 train_time:73245ms step_avg:92.36ms
step:794/1670 train_time:73337ms step_avg:92.36ms
step:795/1670 train_time:73430ms step_avg:92.36ms
step:796/1670 train_time:73523ms step_avg:92.37ms
step:797/1670 train_time:73615ms step_avg:92.37ms
step:798/1670 train_time:73708ms step_avg:92.37ms
step:799/1670 train_time:73799ms step_avg:92.36ms
step:800/1670 train_time:73892ms step_avg:92.36ms
step:801/1670 train_time:73984ms step_avg:92.36ms
step:802/1670 train_time:74075ms step_avg:92.36ms
step:803/1670 train_time:74168ms step_avg:92.36ms
step:804/1670 train_time:74260ms step_avg:92.36ms
step:805/1670 train_time:74352ms step_avg:92.36ms
step:806/1670 train_time:74445ms step_avg:92.36ms
step:807/1670 train_time:74537ms step_avg:92.36ms
step:808/1670 train_time:74631ms step_avg:92.37ms
step:809/1670 train_time:74724ms step_avg:92.37ms
step:810/1670 train_time:74815ms step_avg:92.36ms
step:811/1670 train_time:74907ms step_avg:92.36ms
step:812/1670 train_time:74999ms step_avg:92.36ms
step:813/1670 train_time:75092ms step_avg:92.36ms
step:814/1670 train_time:75184ms step_avg:92.36ms
step:815/1670 train_time:75276ms step_avg:92.36ms
step:816/1670 train_time:75370ms step_avg:92.37ms
step:817/1670 train_time:75463ms step_avg:92.37ms
step:818/1670 train_time:75554ms step_avg:92.36ms
step:819/1670 train_time:75648ms step_avg:92.37ms
step:820/1670 train_time:75740ms step_avg:92.37ms
step:821/1670 train_time:75832ms step_avg:92.37ms
step:822/1670 train_time:75924ms step_avg:92.37ms
step:823/1670 train_time:76016ms step_avg:92.36ms
step:824/1670 train_time:76109ms step_avg:92.37ms
step:825/1670 train_time:76202ms step_avg:92.37ms
step:826/1670 train_time:76293ms step_avg:92.36ms
step:827/1670 train_time:76385ms step_avg:92.36ms
step:828/1670 train_time:76478ms step_avg:92.36ms
step:829/1670 train_time:76571ms step_avg:92.37ms
step:830/1670 train_time:76665ms step_avg:92.37ms
step:831/1670 train_time:76757ms step_avg:92.37ms
step:832/1670 train_time:76849ms step_avg:92.37ms
step:833/1670 train_time:76941ms step_avg:92.37ms
step:834/1670 train_time:77034ms step_avg:92.37ms
step:835/1670 train_time:77125ms step_avg:92.37ms
step:836/1670 train_time:77217ms step_avg:92.37ms
step:837/1670 train_time:77311ms step_avg:92.37ms
step:838/1670 train_time:77403ms step_avg:92.37ms
step:839/1670 train_time:77495ms step_avg:92.37ms
step:840/1670 train_time:77588ms step_avg:92.37ms
step:841/1670 train_time:77681ms step_avg:92.37ms
step:842/1670 train_time:77773ms step_avg:92.37ms
step:843/1670 train_time:77865ms step_avg:92.37ms
step:844/1670 train_time:77957ms step_avg:92.37ms
step:845/1670 train_time:78049ms step_avg:92.37ms
step:846/1670 train_time:78142ms step_avg:92.37ms
step:847/1670 train_time:78234ms step_avg:92.37ms
step:848/1670 train_time:78327ms step_avg:92.37ms
step:849/1670 train_time:78420ms step_avg:92.37ms
step:850/1670 train_time:78513ms step_avg:92.37ms
step:851/1670 train_time:78767ms step_avg:92.56ms
step:852/1670 train_time:78837ms step_avg:92.53ms
step:853/1670 train_time:78928ms step_avg:92.53ms
step:854/1670 train_time:79019ms step_avg:92.53ms
step:855/1670 train_time:79110ms step_avg:92.53ms
step:856/1670 train_time:79201ms step_avg:92.52ms
step:857/1670 train_time:79292ms step_avg:92.52ms
step:858/1670 train_time:79384ms step_avg:92.52ms
step:859/1670 train_time:79474ms step_avg:92.52ms
step:860/1670 train_time:79565ms step_avg:92.52ms
step:861/1670 train_time:79661ms step_avg:92.52ms
step:862/1670 train_time:79760ms step_avg:92.53ms
step:863/1670 train_time:79853ms step_avg:92.53ms
step:864/1670 train_time:79946ms step_avg:92.53ms
step:865/1670 train_time:80037ms step_avg:92.53ms
step:866/1670 train_time:80129ms step_avg:92.53ms
step:867/1670 train_time:80221ms step_avg:92.53ms
step:868/1670 train_time:80312ms step_avg:92.53ms
step:869/1670 train_time:80403ms step_avg:92.52ms
step:870/1670 train_time:80494ms step_avg:92.52ms
step:871/1670 train_time:80587ms step_avg:92.52ms
step:872/1670 train_time:80680ms step_avg:92.52ms
step:873/1670 train_time:80775ms step_avg:92.53ms
step:874/1670 train_time:80869ms step_avg:92.53ms
step:875/1670 train_time:80961ms step_avg:92.53ms
step:875/1670 val_loss:3.5160 train_time:81052ms step_avg:92.63ms
step:876/1670 train_time:81072ms step_avg:92.55ms
step:877/1670 train_time:81145ms step_avg:92.53ms
step:878/1670 train_time:81237ms step_avg:92.53ms
step:879/1670 train_time:81329ms step_avg:92.52ms
step:880/1670 train_time:81422ms step_avg:92.52ms
step:881/1670 train_time:81512ms step_avg:92.52ms
step:882/1670 train_time:81603ms step_avg:92.52ms
step:883/1670 train_time:81695ms step_avg:92.52ms
step:884/1670 train_time:81787ms step_avg:92.52ms
step:885/1670 train_time:81880ms step_avg:92.52ms
step:886/1670 train_time:81974ms step_avg:92.52ms
step:887/1670 train_time:82068ms step_avg:92.52ms
step:888/1670 train_time:82162ms step_avg:92.52ms
step:889/1670 train_time:82254ms step_avg:92.52ms
step:890/1670 train_time:82346ms step_avg:92.52ms
step:891/1670 train_time:82437ms step_avg:92.52ms
step:892/1670 train_time:82529ms step_avg:92.52ms
step:893/1670 train_time:82621ms step_avg:92.52ms
step:894/1670 train_time:82713ms step_avg:92.52ms
step:895/1670 train_time:82805ms step_avg:92.52ms
step:896/1670 train_time:82897ms step_avg:92.52ms
step:897/1670 train_time:82990ms step_avg:92.52ms
step:898/1670 train_time:83086ms step_avg:92.52ms
step:899/1670 train_time:83178ms step_avg:92.52ms
step:900/1670 train_time:83270ms step_avg:92.52ms
step:901/1670 train_time:83363ms step_avg:92.52ms
step:902/1670 train_time:83455ms step_avg:92.52ms
step:903/1670 train_time:83546ms step_avg:92.52ms
step:904/1670 train_time:83637ms step_avg:92.52ms
step:905/1670 train_time:83730ms step_avg:92.52ms
step:906/1670 train_time:83823ms step_avg:92.52ms
step:907/1670 train_time:83914ms step_avg:92.52ms
step:908/1670 train_time:84007ms step_avg:92.52ms
step:909/1670 train_time:84100ms step_avg:92.52ms
step:910/1670 train_time:84194ms step_avg:92.52ms
step:911/1670 train_time:84286ms step_avg:92.52ms
step:912/1670 train_time:84378ms step_avg:92.52ms
step:913/1670 train_time:84470ms step_avg:92.52ms
step:914/1670 train_time:84562ms step_avg:92.52ms
step:915/1670 train_time:84654ms step_avg:92.52ms
step:916/1670 train_time:84746ms step_avg:92.52ms
step:917/1670 train_time:84838ms step_avg:92.52ms
step:918/1670 train_time:84931ms step_avg:92.52ms
step:919/1670 train_time:85024ms step_avg:92.52ms
step:920/1670 train_time:85116ms step_avg:92.52ms
step:921/1670 train_time:85209ms step_avg:92.52ms
step:922/1670 train_time:85302ms step_avg:92.52ms
step:923/1670 train_time:85395ms step_avg:92.52ms
step:924/1670 train_time:85487ms step_avg:92.52ms
step:925/1670 train_time:85579ms step_avg:92.52ms
step:926/1670 train_time:85671ms step_avg:92.52ms
step:927/1670 train_time:85763ms step_avg:92.52ms
step:928/1670 train_time:85855ms step_avg:92.52ms
step:929/1670 train_time:85948ms step_avg:92.52ms
step:930/1670 train_time:86040ms step_avg:92.52ms
step:931/1670 train_time:86132ms step_avg:92.52ms
step:932/1670 train_time:86225ms step_avg:92.52ms
step:933/1670 train_time:86317ms step_avg:92.52ms
step:934/1670 train_time:86410ms step_avg:92.52ms
step:935/1670 train_time:86503ms step_avg:92.52ms
step:936/1670 train_time:86595ms step_avg:92.52ms
step:937/1670 train_time:86687ms step_avg:92.52ms
step:938/1670 train_time:86778ms step_avg:92.51ms
step:939/1670 train_time:86872ms step_avg:92.52ms
step:940/1670 train_time:86964ms step_avg:92.52ms
step:941/1670 train_time:87056ms step_avg:92.51ms
step:942/1670 train_time:87149ms step_avg:92.51ms
step:943/1670 train_time:87241ms step_avg:92.51ms
step:944/1670 train_time:87334ms step_avg:92.51ms
step:945/1670 train_time:87427ms step_avg:92.52ms
step:946/1670 train_time:87519ms step_avg:92.52ms
step:947/1670 train_time:87612ms step_avg:92.52ms
step:948/1670 train_time:87704ms step_avg:92.51ms
step:949/1670 train_time:87797ms step_avg:92.51ms
step:950/1670 train_time:87889ms step_avg:92.51ms
step:951/1670 train_time:87981ms step_avg:92.51ms
step:952/1670 train_time:88074ms step_avg:92.51ms
step:953/1670 train_time:88166ms step_avg:92.51ms
step:954/1670 train_time:88258ms step_avg:92.51ms
step:955/1670 train_time:88351ms step_avg:92.51ms
step:956/1670 train_time:88443ms step_avg:92.51ms
step:957/1670 train_time:88535ms step_avg:92.51ms
step:958/1670 train_time:88628ms step_avg:92.51ms
step:959/1670 train_time:88720ms step_avg:92.51ms
step:960/1670 train_time:88813ms step_avg:92.51ms
step:961/1670 train_time:88905ms step_avg:92.51ms
step:962/1670 train_time:88996ms step_avg:92.51ms
step:963/1670 train_time:89089ms step_avg:92.51ms
step:964/1670 train_time:89182ms step_avg:92.51ms
step:965/1670 train_time:89274ms step_avg:92.51ms
step:966/1670 train_time:89367ms step_avg:92.51ms
step:967/1670 train_time:89459ms step_avg:92.51ms
step:968/1670 train_time:89551ms step_avg:92.51ms
step:969/1670 train_time:89644ms step_avg:92.51ms
step:970/1670 train_time:89736ms step_avg:92.51ms
step:971/1670 train_time:89828ms step_avg:92.51ms
step:972/1670 train_time:89920ms step_avg:92.51ms
step:973/1670 train_time:90013ms step_avg:92.51ms
step:974/1670 train_time:90106ms step_avg:92.51ms
step:975/1670 train_time:90198ms step_avg:92.51ms
step:976/1670 train_time:90291ms step_avg:92.51ms
step:977/1670 train_time:90383ms step_avg:92.51ms
step:978/1670 train_time:90475ms step_avg:92.51ms
step:979/1670 train_time:90568ms step_avg:92.51ms
step:980/1670 train_time:90659ms step_avg:92.51ms
step:981/1670 train_time:90752ms step_avg:92.51ms
step:982/1670 train_time:90845ms step_avg:92.51ms
step:983/1670 train_time:90937ms step_avg:92.51ms
step:984/1670 train_time:91029ms step_avg:92.51ms
step:985/1670 train_time:91121ms step_avg:92.51ms
step:986/1670 train_time:91214ms step_avg:92.51ms
step:987/1670 train_time:91307ms step_avg:92.51ms
step:988/1670 train_time:91399ms step_avg:92.51ms
step:989/1670 train_time:91493ms step_avg:92.51ms
step:990/1670 train_time:91585ms step_avg:92.51ms
step:991/1670 train_time:91676ms step_avg:92.51ms
step:992/1670 train_time:91769ms step_avg:92.51ms
step:993/1670 train_time:91861ms step_avg:92.51ms
step:994/1670 train_time:91954ms step_avg:92.51ms
step:995/1670 train_time:92045ms step_avg:92.51ms
step:996/1670 train_time:92136ms step_avg:92.51ms
step:997/1670 train_time:92230ms step_avg:92.51ms
step:998/1670 train_time:92322ms step_avg:92.51ms
step:999/1670 train_time:92415ms step_avg:92.51ms
step:1000/1670 train_time:92507ms step_avg:92.51ms
step:1000/1670 val_loss:3.4677 train_time:92599ms step_avg:92.60ms
step:1001/1670 train_time:92618ms step_avg:92.53ms
step:1002/1670 train_time:92692ms step_avg:92.51ms
step:1003/1670 train_time:92785ms step_avg:92.51ms
step:1004/1670 train_time:92877ms step_avg:92.51ms
step:1005/1670 train_time:92968ms step_avg:92.51ms
step:1006/1670 train_time:93059ms step_avg:92.50ms
step:1007/1670 train_time:93151ms step_avg:92.50ms
step:1008/1670 train_time:93242ms step_avg:92.50ms
step:1009/1670 train_time:93334ms step_avg:92.50ms
step:1010/1670 train_time:93431ms step_avg:92.51ms
step:1011/1670 train_time:93521ms step_avg:92.50ms
step:1012/1670 train_time:93614ms step_avg:92.50ms
step:1013/1670 train_time:93708ms step_avg:92.51ms
step:1014/1670 train_time:93801ms step_avg:92.51ms
step:1015/1670 train_time:93893ms step_avg:92.51ms
step:1016/1670 train_time:93986ms step_avg:92.51ms
step:1017/1670 train_time:94078ms step_avg:92.50ms
step:1018/1670 train_time:94172ms step_avg:92.51ms
step:1019/1670 train_time:94264ms step_avg:92.51ms
step:1020/1670 train_time:94357ms step_avg:92.51ms
step:1021/1670 train_time:94449ms step_avg:92.51ms
step:1022/1670 train_time:94542ms step_avg:92.51ms
step:1023/1670 train_time:94635ms step_avg:92.51ms
step:1024/1670 train_time:94728ms step_avg:92.51ms
step:1025/1670 train_time:94820ms step_avg:92.51ms
step:1026/1670 train_time:94913ms step_avg:92.51ms
step:1027/1670 train_time:95006ms step_avg:92.51ms
step:1028/1670 train_time:95099ms step_avg:92.51ms
step:1029/1670 train_time:95192ms step_avg:92.51ms
step:1030/1670 train_time:95283ms step_avg:92.51ms
step:1031/1670 train_time:95376ms step_avg:92.51ms
step:1032/1670 train_time:95469ms step_avg:92.51ms
step:1033/1670 train_time:95561ms step_avg:92.51ms
step:1034/1670 train_time:95655ms step_avg:92.51ms
step:1035/1670 train_time:95747ms step_avg:92.51ms
step:1036/1670 train_time:95839ms step_avg:92.51ms
step:1037/1670 train_time:95931ms step_avg:92.51ms
step:1038/1670 train_time:96023ms step_avg:92.51ms
step:1039/1670 train_time:96115ms step_avg:92.51ms
step:1040/1670 train_time:96208ms step_avg:92.51ms
step:1041/1670 train_time:96299ms step_avg:92.51ms
step:1042/1670 train_time:96392ms step_avg:92.51ms
step:1043/1670 train_time:96483ms step_avg:92.51ms
step:1044/1670 train_time:96577ms step_avg:92.51ms
step:1045/1670 train_time:96671ms step_avg:92.51ms
step:1046/1670 train_time:96763ms step_avg:92.51ms
step:1047/1670 train_time:96855ms step_avg:92.51ms
step:1048/1670 train_time:96947ms step_avg:92.51ms
step:1049/1670 train_time:97039ms step_avg:92.51ms
step:1050/1670 train_time:97131ms step_avg:92.51ms
step:1051/1670 train_time:97224ms step_avg:92.51ms
step:1052/1670 train_time:97316ms step_avg:92.51ms
step:1053/1670 train_time:97408ms step_avg:92.50ms
step:1054/1670 train_time:97500ms step_avg:92.50ms
step:1055/1670 train_time:97593ms step_avg:92.51ms
step:1056/1670 train_time:97686ms step_avg:92.51ms
step:1057/1670 train_time:97779ms step_avg:92.51ms
step:1058/1670 train_time:97872ms step_avg:92.51ms
step:1059/1670 train_time:97964ms step_avg:92.51ms
step:1060/1670 train_time:98057ms step_avg:92.51ms
step:1061/1670 train_time:98149ms step_avg:92.51ms
step:1062/1670 train_time:98402ms step_avg:92.66ms
step:1063/1670 train_time:98471ms step_avg:92.64ms
step:1064/1670 train_time:98561ms step_avg:92.63ms
step:1065/1670 train_time:98653ms step_avg:92.63ms
step:1066/1670 train_time:98743ms step_avg:92.63ms
step:1067/1670 train_time:98834ms step_avg:92.63ms
step:1068/1670 train_time:98926ms step_avg:92.63ms
step:1069/1670 train_time:99017ms step_avg:92.63ms
step:1070/1670 train_time:99108ms step_avg:92.62ms
step:1071/1670 train_time:99200ms step_avg:92.62ms
step:1072/1670 train_time:99296ms step_avg:92.63ms
step:1073/1670 train_time:99395ms step_avg:92.63ms
step:1074/1670 train_time:99489ms step_avg:92.63ms
step:1075/1670 train_time:99580ms step_avg:92.63ms
step:1076/1670 train_time:99673ms step_avg:92.63ms
step:1077/1670 train_time:99765ms step_avg:92.63ms
step:1078/1670 train_time:99856ms step_avg:92.63ms
step:1079/1670 train_time:99947ms step_avg:92.63ms
step:1080/1670 train_time:100038ms step_avg:92.63ms
step:1081/1670 train_time:100130ms step_avg:92.63ms
step:1082/1670 train_time:100222ms step_avg:92.63ms
step:1083/1670 train_time:100317ms step_avg:92.63ms
step:1084/1670 train_time:100414ms step_avg:92.63ms
step:1085/1670 train_time:100508ms step_avg:92.63ms
step:1086/1670 train_time:100600ms step_avg:92.63ms
step:1087/1670 train_time:100693ms step_avg:92.63ms
step:1088/1670 train_time:100785ms step_avg:92.63ms
step:1089/1670 train_time:100877ms step_avg:92.63ms
step:1090/1670 train_time:100968ms step_avg:92.63ms
step:1091/1670 train_time:101060ms step_avg:92.63ms
step:1092/1670 train_time:101152ms step_avg:92.63ms
step:1093/1670 train_time:101244ms step_avg:92.63ms
step:1094/1670 train_time:101337ms step_avg:92.63ms
step:1095/1670 train_time:101431ms step_avg:92.63ms
step:1096/1670 train_time:101523ms step_avg:92.63ms
step:1097/1670 train_time:101616ms step_avg:92.63ms
step:1098/1670 train_time:101708ms step_avg:92.63ms
step:1099/1670 train_time:101799ms step_avg:92.63ms
step:1100/1670 train_time:101891ms step_avg:92.63ms
step:1101/1670 train_time:101982ms step_avg:92.63ms
step:1102/1670 train_time:102075ms step_avg:92.63ms
step:1103/1670 train_time:102166ms step_avg:92.63ms
step:1104/1670 train_time:102259ms step_avg:92.63ms
step:1105/1670 train_time:102351ms step_avg:92.63ms
step:1106/1670 train_time:102444ms step_avg:92.63ms
step:1107/1670 train_time:102537ms step_avg:92.63ms
step:1108/1670 train_time:102630ms step_avg:92.63ms
step:1109/1670 train_time:102723ms step_avg:92.63ms
step:1110/1670 train_time:102816ms step_avg:92.63ms
step:1111/1670 train_time:102908ms step_avg:92.63ms
step:1112/1670 train_time:102999ms step_avg:92.63ms
step:1113/1670 train_time:103091ms step_avg:92.62ms
step:1114/1670 train_time:103182ms step_avg:92.62ms
step:1115/1670 train_time:103467ms step_avg:92.80ms
step:1116/1670 train_time:103544ms step_avg:92.78ms
step:1117/1670 train_time:103635ms step_avg:92.78ms
step:1118/1670 train_time:103727ms step_avg:92.78ms
step:1119/1670 train_time:103819ms step_avg:92.78ms
step:1120/1670 train_time:103911ms step_avg:92.78ms
step:1121/1670 train_time:104003ms step_avg:92.78ms
step:1122/1670 train_time:104095ms step_avg:92.78ms
step:1123/1670 train_time:104187ms step_avg:92.78ms
step:1124/1670 train_time:104278ms step_avg:92.77ms
step:1125/1670 train_time:104376ms step_avg:92.78ms
step:1125/1670 val_loss:3.4149 train_time:104475ms step_avg:92.87ms
step:1126/1670 train_time:104497ms step_avg:92.80ms
step:1127/1670 train_time:104574ms step_avg:92.79ms
step:1128/1670 train_time:104672ms step_avg:92.79ms
step:1129/1670 train_time:104765ms step_avg:92.79ms
step:1130/1670 train_time:104857ms step_avg:92.79ms
step:1131/1670 train_time:104949ms step_avg:92.79ms
step:1132/1670 train_time:105042ms step_avg:92.79ms
step:1133/1670 train_time:105134ms step_avg:92.79ms
step:1134/1670 train_time:105225ms step_avg:92.79ms
step:1135/1670 train_time:105317ms step_avg:92.79ms
step:1136/1670 train_time:105411ms step_avg:92.79ms
step:1137/1670 train_time:105508ms step_avg:92.79ms
step:1138/1670 train_time:105606ms step_avg:92.80ms
step:1139/1670 train_time:105702ms step_avg:92.80ms
step:1140/1670 train_time:105794ms step_avg:92.80ms
step:1141/1670 train_time:105887ms step_avg:92.80ms
step:1142/1670 train_time:105979ms step_avg:92.80ms
step:1143/1670 train_time:106072ms step_avg:92.80ms
step:1144/1670 train_time:106165ms step_avg:92.80ms
step:1145/1670 train_time:106256ms step_avg:92.80ms
step:1146/1670 train_time:106348ms step_avg:92.80ms
step:1147/1670 train_time:106441ms step_avg:92.80ms
step:1148/1670 train_time:106536ms step_avg:92.80ms
step:1149/1670 train_time:106630ms step_avg:92.80ms
step:1150/1670 train_time:106725ms step_avg:92.80ms
step:1151/1670 train_time:106819ms step_avg:92.81ms
step:1152/1670 train_time:106911ms step_avg:92.80ms
step:1153/1670 train_time:107004ms step_avg:92.80ms
step:1154/1670 train_time:107097ms step_avg:92.80ms
step:1155/1670 train_time:107189ms step_avg:92.80ms
step:1156/1670 train_time:107281ms step_avg:92.80ms
step:1157/1670 train_time:107373ms step_avg:92.80ms
step:1158/1670 train_time:107468ms step_avg:92.81ms
step:1159/1670 train_time:107562ms step_avg:92.81ms
step:1160/1670 train_time:107656ms step_avg:92.81ms
step:1161/1670 train_time:107749ms step_avg:92.81ms
step:1162/1670 train_time:107842ms step_avg:92.81ms
step:1163/1670 train_time:107935ms step_avg:92.81ms
step:1164/1670 train_time:108028ms step_avg:92.81ms
step:1165/1670 train_time:108121ms step_avg:92.81ms
step:1166/1670 train_time:108210ms step_avg:92.80ms
step:1167/1670 train_time:108304ms step_avg:92.81ms
step:1168/1670 train_time:108398ms step_avg:92.81ms
step:1169/1670 train_time:108491ms step_avg:92.81ms
step:1170/1670 train_time:108584ms step_avg:92.81ms
step:1171/1670 train_time:108679ms step_avg:92.81ms
step:1172/1670 train_time:108771ms step_avg:92.81ms
step:1173/1670 train_time:108864ms step_avg:92.81ms
step:1174/1670 train_time:108957ms step_avg:92.81ms
step:1175/1670 train_time:109050ms step_avg:92.81ms
step:1176/1670 train_time:109144ms step_avg:92.81ms
step:1177/1670 train_time:109237ms step_avg:92.81ms
step:1178/1670 train_time:109329ms step_avg:92.81ms
step:1179/1670 train_time:109423ms step_avg:92.81ms
step:1180/1670 train_time:109516ms step_avg:92.81ms
step:1181/1670 train_time:109610ms step_avg:92.81ms
step:1182/1670 train_time:109704ms step_avg:92.81ms
step:1183/1670 train_time:109797ms step_avg:92.81ms
step:1184/1670 train_time:109889ms step_avg:92.81ms
step:1185/1670 train_time:109982ms step_avg:92.81ms
step:1186/1670 train_time:110075ms step_avg:92.81ms
step:1187/1670 train_time:110167ms step_avg:92.81ms
step:1188/1670 train_time:110260ms step_avg:92.81ms
step:1189/1670 train_time:110353ms step_avg:92.81ms
step:1190/1670 train_time:110446ms step_avg:92.81ms
step:1191/1670 train_time:110540ms step_avg:92.81ms
step:1192/1670 train_time:110633ms step_avg:92.81ms
step:1193/1670 train_time:110727ms step_avg:92.81ms
step:1194/1670 train_time:110821ms step_avg:92.81ms
step:1195/1670 train_time:110914ms step_avg:92.81ms
step:1196/1670 train_time:111006ms step_avg:92.81ms
step:1197/1670 train_time:111099ms step_avg:92.81ms
step:1198/1670 train_time:111191ms step_avg:92.81ms
step:1199/1670 train_time:111283ms step_avg:92.81ms
step:1200/1670 train_time:111376ms step_avg:92.81ms
step:1201/1670 train_time:111469ms step_avg:92.81ms
step:1202/1670 train_time:111563ms step_avg:92.81ms
step:1203/1670 train_time:111657ms step_avg:92.82ms
step:1204/1670 train_time:111750ms step_avg:92.82ms
step:1205/1670 train_time:111843ms step_avg:92.82ms
step:1206/1670 train_time:111937ms step_avg:92.82ms
step:1207/1670 train_time:112029ms step_avg:92.82ms
step:1208/1670 train_time:112124ms step_avg:92.82ms
step:1209/1670 train_time:112217ms step_avg:92.82ms
step:1210/1670 train_time:112310ms step_avg:92.82ms
step:1211/1670 train_time:112404ms step_avg:92.82ms
step:1212/1670 train_time:112497ms step_avg:92.82ms
step:1213/1670 train_time:112589ms step_avg:92.82ms
step:1214/1670 train_time:112682ms step_avg:92.82ms
step:1215/1670 train_time:112775ms step_avg:92.82ms
step:1216/1670 train_time:112868ms step_avg:92.82ms
step:1217/1670 train_time:112962ms step_avg:92.82ms
step:1218/1670 train_time:113054ms step_avg:92.82ms
step:1219/1670 train_time:113147ms step_avg:92.82ms
step:1220/1670 train_time:113240ms step_avg:92.82ms
step:1221/1670 train_time:113333ms step_avg:92.82ms
step:1222/1670 train_time:113427ms step_avg:92.82ms
step:1223/1670 train_time:113519ms step_avg:92.82ms
step:1224/1670 train_time:113612ms step_avg:92.82ms
step:1225/1670 train_time:113705ms step_avg:92.82ms
step:1226/1670 train_time:113799ms step_avg:92.82ms
step:1227/1670 train_time:113892ms step_avg:92.82ms
step:1228/1670 train_time:113984ms step_avg:92.82ms
step:1229/1670 train_time:114078ms step_avg:92.82ms
step:1230/1670 train_time:114171ms step_avg:92.82ms
step:1231/1670 train_time:114265ms step_avg:92.82ms
step:1232/1670 train_time:114358ms step_avg:92.82ms
step:1233/1670 train_time:114451ms step_avg:92.82ms
step:1234/1670 train_time:114545ms step_avg:92.82ms
step:1235/1670 train_time:114639ms step_avg:92.82ms
step:1236/1670 train_time:114731ms step_avg:92.82ms
step:1237/1670 train_time:114825ms step_avg:92.83ms
step:1238/1670 train_time:114918ms step_avg:92.83ms
step:1239/1670 train_time:115010ms step_avg:92.82ms
step:1240/1670 train_time:115103ms step_avg:92.83ms
step:1241/1670 train_time:115197ms step_avg:92.83ms
step:1242/1670 train_time:115291ms step_avg:92.83ms
step:1243/1670 train_time:115384ms step_avg:92.83ms
step:1244/1670 train_time:115477ms step_avg:92.83ms
step:1245/1670 train_time:115570ms step_avg:92.83ms
step:1246/1670 train_time:115664ms step_avg:92.83ms
step:1247/1670 train_time:115756ms step_avg:92.83ms
step:1248/1670 train_time:115849ms step_avg:92.83ms
step:1249/1670 train_time:115941ms step_avg:92.83ms
step:1250/1670 train_time:116035ms step_avg:92.83ms
step:1250/1670 val_loss:3.3759 train_time:116127ms step_avg:92.90ms
step:1251/1670 train_time:116148ms step_avg:92.84ms
step:1252/1670 train_time:116222ms step_avg:92.83ms
step:1253/1670 train_time:116316ms step_avg:92.83ms
step:1254/1670 train_time:116408ms step_avg:92.83ms
step:1255/1670 train_time:116500ms step_avg:92.83ms
step:1256/1670 train_time:116592ms step_avg:92.83ms
step:1257/1670 train_time:116684ms step_avg:92.83ms
step:1258/1670 train_time:116777ms step_avg:92.83ms
step:1259/1670 train_time:116871ms step_avg:92.83ms
step:1260/1670 train_time:116964ms step_avg:92.83ms
step:1261/1670 train_time:117059ms step_avg:92.83ms
step:1262/1670 train_time:117153ms step_avg:92.83ms
step:1263/1670 train_time:117247ms step_avg:92.83ms
step:1264/1670 train_time:117340ms step_avg:92.83ms
step:1265/1670 train_time:117433ms step_avg:92.83ms
step:1266/1670 train_time:117525ms step_avg:92.83ms
step:1267/1670 train_time:117618ms step_avg:92.83ms
step:1268/1670 train_time:117711ms step_avg:92.83ms
step:1269/1670 train_time:117803ms step_avg:92.83ms
step:1270/1670 train_time:117897ms step_avg:92.83ms
step:1271/1670 train_time:117990ms step_avg:92.83ms
step:1272/1670 train_time:118084ms step_avg:92.83ms
step:1273/1670 train_time:118178ms step_avg:92.83ms
step:1274/1670 train_time:118419ms step_avg:92.95ms
step:1275/1670 train_time:118499ms step_avg:92.94ms
step:1276/1670 train_time:118591ms step_avg:92.94ms
step:1277/1670 train_time:118683ms step_avg:92.94ms
step:1278/1670 train_time:118775ms step_avg:92.94ms
step:1279/1670 train_time:118866ms step_avg:92.94ms
step:1280/1670 train_time:118958ms step_avg:92.94ms
step:1281/1670 train_time:119049ms step_avg:92.93ms
step:1282/1670 train_time:119141ms step_avg:92.93ms
step:1283/1670 train_time:119233ms step_avg:92.93ms
step:1284/1670 train_time:119331ms step_avg:92.94ms
step:1285/1670 train_time:119428ms step_avg:92.94ms
step:1286/1670 train_time:119522ms step_avg:92.94ms
step:1287/1670 train_time:119615ms step_avg:92.94ms
step:1288/1670 train_time:119708ms step_avg:92.94ms
step:1289/1670 train_time:119800ms step_avg:92.94ms
step:1290/1670 train_time:119893ms step_avg:92.94ms
step:1291/1670 train_time:119986ms step_avg:92.94ms
step:1292/1670 train_time:120078ms step_avg:92.94ms
step:1293/1670 train_time:120170ms step_avg:92.94ms
step:1294/1670 train_time:120264ms step_avg:92.94ms
step:1295/1670 train_time:120358ms step_avg:92.94ms
step:1296/1670 train_time:120453ms step_avg:92.94ms
step:1297/1670 train_time:120548ms step_avg:92.94ms
step:1298/1670 train_time:120640ms step_avg:92.94ms
step:1299/1670 train_time:120733ms step_avg:92.94ms
step:1300/1670 train_time:120827ms step_avg:92.94ms
step:1301/1670 train_time:120920ms step_avg:92.94ms
step:1302/1670 train_time:121012ms step_avg:92.94ms
step:1303/1670 train_time:121104ms step_avg:92.94ms
step:1304/1670 train_time:121197ms step_avg:92.94ms
step:1305/1670 train_time:121290ms step_avg:92.94ms
step:1306/1670 train_time:121383ms step_avg:92.94ms
step:1307/1670 train_time:121477ms step_avg:92.94ms
step:1308/1670 train_time:121571ms step_avg:92.94ms
step:1309/1670 train_time:121664ms step_avg:92.94ms
step:1310/1670 train_time:121758ms step_avg:92.94ms
step:1311/1670 train_time:121851ms step_avg:92.95ms
step:1312/1670 train_time:121944ms step_avg:92.95ms
step:1313/1670 train_time:122036ms step_avg:92.94ms
step:1314/1670 train_time:122129ms step_avg:92.94ms
step:1315/1670 train_time:122221ms step_avg:92.94ms
step:1316/1670 train_time:122315ms step_avg:92.94ms
step:1317/1670 train_time:122409ms step_avg:92.94ms
step:1318/1670 train_time:122502ms step_avg:92.95ms
step:1319/1670 train_time:122596ms step_avg:92.95ms
step:1320/1670 train_time:122689ms step_avg:92.95ms
step:1321/1670 train_time:122782ms step_avg:92.95ms
step:1322/1670 train_time:122877ms step_avg:92.95ms
step:1323/1670 train_time:122971ms step_avg:92.95ms
step:1324/1670 train_time:123063ms step_avg:92.95ms
step:1325/1670 train_time:123157ms step_avg:92.95ms
step:1326/1670 train_time:123251ms step_avg:92.95ms
step:1327/1670 train_time:123343ms step_avg:92.95ms
step:1328/1670 train_time:123436ms step_avg:92.95ms
step:1329/1670 train_time:123530ms step_avg:92.95ms
step:1330/1670 train_time:123623ms step_avg:92.95ms
step:1331/1670 train_time:123716ms step_avg:92.95ms
step:1332/1670 train_time:123809ms step_avg:92.95ms
step:1333/1670 train_time:123901ms step_avg:92.95ms
step:1334/1670 train_time:123994ms step_avg:92.95ms
step:1335/1670 train_time:124088ms step_avg:92.95ms
step:1336/1670 train_time:124181ms step_avg:92.95ms
step:1337/1670 train_time:124274ms step_avg:92.95ms
step:1338/1670 train_time:124367ms step_avg:92.95ms
step:1339/1670 train_time:124461ms step_avg:92.95ms
step:1340/1670 train_time:124555ms step_avg:92.95ms
step:1341/1670 train_time:124648ms step_avg:92.95ms
step:1342/1670 train_time:124741ms step_avg:92.95ms
step:1343/1670 train_time:124834ms step_avg:92.95ms
step:1344/1670 train_time:124926ms step_avg:92.95ms
step:1345/1670 train_time:125020ms step_avg:92.95ms
step:1346/1670 train_time:125113ms step_avg:92.95ms
step:1347/1670 train_time:125206ms step_avg:92.95ms
step:1348/1670 train_time:125299ms step_avg:92.95ms
step:1349/1670 train_time:125393ms step_avg:92.95ms
step:1350/1670 train_time:125486ms step_avg:92.95ms
step:1351/1670 train_time:125578ms step_avg:92.95ms
step:1352/1670 train_time:125671ms step_avg:92.95ms
step:1353/1670 train_time:125765ms step_avg:92.95ms
step:1354/1670 train_time:125858ms step_avg:92.95ms
step:1355/1670 train_time:125950ms step_avg:92.95ms
step:1356/1670 train_time:126043ms step_avg:92.95ms
step:1357/1670 train_time:126136ms step_avg:92.95ms
step:1358/1670 train_time:126229ms step_avg:92.95ms
step:1359/1670 train_time:126322ms step_avg:92.95ms
step:1360/1670 train_time:126415ms step_avg:92.95ms
step:1361/1670 train_time:126508ms step_avg:92.95ms
step:1362/1670 train_time:126600ms step_avg:92.95ms
step:1363/1670 train_time:126693ms step_avg:92.95ms
step:1364/1670 train_time:126787ms step_avg:92.95ms
step:1365/1670 train_time:126880ms step_avg:92.95ms
step:1366/1670 train_time:126974ms step_avg:92.95ms
step:1367/1670 train_time:127068ms step_avg:92.95ms
step:1368/1670 train_time:127161ms step_avg:92.95ms
step:1369/1670 train_time:127254ms step_avg:92.95ms
step:1370/1670 train_time:127347ms step_avg:92.95ms
step:1371/1670 train_time:127440ms step_avg:92.95ms
step:1372/1670 train_time:127533ms step_avg:92.95ms
step:1373/1670 train_time:127626ms step_avg:92.95ms
step:1374/1670 train_time:127719ms step_avg:92.95ms
step:1375/1670 train_time:127812ms step_avg:92.95ms
step:1375/1670 val_loss:3.3418 train_time:127904ms step_avg:93.02ms
step:1376/1670 train_time:127924ms step_avg:92.97ms
step:1377/1670 train_time:127998ms step_avg:92.95ms
step:1378/1670 train_time:128090ms step_avg:92.95ms
step:1379/1670 train_time:128183ms step_avg:92.95ms
step:1380/1670 train_time:128276ms step_avg:92.95ms
step:1381/1670 train_time:128368ms step_avg:92.95ms
step:1382/1670 train_time:128461ms step_avg:92.95ms
step:1383/1670 train_time:128553ms step_avg:92.95ms
step:1384/1670 train_time:128646ms step_avg:92.95ms
step:1385/1670 train_time:128740ms step_avg:92.95ms
step:1386/1670 train_time:128835ms step_avg:92.95ms
step:1387/1670 train_time:128930ms step_avg:92.96ms
step:1388/1670 train_time:129025ms step_avg:92.96ms
step:1389/1670 train_time:129118ms step_avg:92.96ms
step:1390/1670 train_time:129212ms step_avg:92.96ms
step:1391/1670 train_time:129306ms step_avg:92.96ms
step:1392/1670 train_time:129399ms step_avg:92.96ms
step:1393/1670 train_time:129491ms step_avg:92.96ms
step:1394/1670 train_time:129583ms step_avg:92.96ms
step:1395/1670 train_time:129675ms step_avg:92.96ms
step:1396/1670 train_time:129768ms step_avg:92.96ms
step:1397/1670 train_time:129863ms step_avg:92.96ms
step:1398/1670 train_time:129957ms step_avg:92.96ms
step:1399/1670 train_time:130050ms step_avg:92.96ms
step:1400/1670 train_time:130143ms step_avg:92.96ms
step:1401/1670 train_time:130239ms step_avg:92.96ms
step:1402/1670 train_time:130331ms step_avg:92.96ms
step:1403/1670 train_time:130425ms step_avg:92.96ms
step:1404/1670 train_time:130518ms step_avg:92.96ms
step:1405/1670 train_time:130610ms step_avg:92.96ms
step:1406/1670 train_time:130704ms step_avg:92.96ms
step:1407/1670 train_time:130797ms step_avg:92.96ms
step:1408/1670 train_time:130890ms step_avg:92.96ms
step:1409/1670 train_time:130984ms step_avg:92.96ms
step:1410/1670 train_time:131077ms step_avg:92.96ms
step:1411/1670 train_time:131170ms step_avg:92.96ms
step:1412/1670 train_time:131265ms step_avg:92.96ms
step:1413/1670 train_time:131358ms step_avg:92.96ms
step:1414/1670 train_time:131451ms step_avg:92.96ms
step:1415/1670 train_time:131544ms step_avg:92.96ms
step:1416/1670 train_time:131637ms step_avg:92.96ms
step:1417/1670 train_time:131729ms step_avg:92.96ms
step:1418/1670 train_time:131822ms step_avg:92.96ms
step:1419/1670 train_time:131915ms step_avg:92.96ms
step:1420/1670 train_time:132007ms step_avg:92.96ms
step:1421/1670 train_time:132101ms step_avg:92.96ms
step:1422/1670 train_time:132194ms step_avg:92.96ms
step:1423/1670 train_time:132287ms step_avg:92.96ms
step:1424/1670 train_time:132380ms step_avg:92.96ms
step:1425/1670 train_time:132474ms step_avg:92.96ms
step:1426/1670 train_time:132567ms step_avg:92.96ms
step:1427/1670 train_time:132660ms step_avg:92.96ms
step:1428/1670 train_time:132752ms step_avg:92.96ms
step:1429/1670 train_time:132847ms step_avg:92.96ms
step:1430/1670 train_time:132940ms step_avg:92.97ms
step:1431/1670 train_time:133033ms step_avg:92.96ms
step:1432/1670 train_time:133126ms step_avg:92.96ms
step:1433/1670 train_time:133218ms step_avg:92.96ms
step:1434/1670 train_time:133311ms step_avg:92.96ms
step:1435/1670 train_time:133403ms step_avg:92.96ms
step:1436/1670 train_time:133497ms step_avg:92.96ms
step:1437/1670 train_time:133589ms step_avg:92.96ms
step:1438/1670 train_time:133682ms step_avg:92.96ms
step:1439/1670 train_time:133775ms step_avg:92.96ms
step:1440/1670 train_time:133868ms step_avg:92.96ms
step:1441/1670 train_time:133961ms step_avg:92.96ms
step:1442/1670 train_time:134054ms step_avg:92.96ms
step:1443/1670 train_time:134148ms step_avg:92.96ms
step:1444/1670 train_time:134241ms step_avg:92.96ms
step:1445/1670 train_time:134335ms step_avg:92.97ms
step:1446/1670 train_time:134428ms step_avg:92.97ms
step:1447/1670 train_time:134522ms step_avg:92.97ms
step:1448/1670 train_time:134615ms step_avg:92.97ms
step:1449/1670 train_time:134709ms step_avg:92.97ms
step:1450/1670 train_time:134803ms step_avg:92.97ms
step:1451/1670 train_time:134897ms step_avg:92.97ms
step:1452/1670 train_time:134990ms step_avg:92.97ms
step:1453/1670 train_time:135082ms step_avg:92.97ms
step:1454/1670 train_time:135175ms step_avg:92.97ms
step:1455/1670 train_time:135269ms step_avg:92.97ms
step:1456/1670 train_time:135362ms step_avg:92.97ms
step:1457/1670 train_time:135454ms step_avg:92.97ms
step:1458/1670 train_time:135548ms step_avg:92.97ms
step:1459/1670 train_time:135641ms step_avg:92.97ms
step:1460/1670 train_time:135735ms step_avg:92.97ms
step:1461/1670 train_time:135828ms step_avg:92.97ms
step:1462/1670 train_time:135921ms step_avg:92.97ms
step:1463/1670 train_time:136014ms step_avg:92.97ms
step:1464/1670 train_time:136108ms step_avg:92.97ms
step:1465/1670 train_time:136202ms step_avg:92.97ms
step:1466/1670 train_time:136294ms step_avg:92.97ms
step:1467/1670 train_time:136387ms step_avg:92.97ms
step:1468/1670 train_time:136480ms step_avg:92.97ms
step:1469/1670 train_time:136572ms step_avg:92.97ms
step:1470/1670 train_time:136666ms step_avg:92.97ms
step:1471/1670 train_time:136759ms step_avg:92.97ms
step:1472/1670 train_time:136852ms step_avg:92.97ms
step:1473/1670 train_time:136946ms step_avg:92.97ms
step:1474/1670 train_time:137040ms step_avg:92.97ms
step:1475/1670 train_time:137133ms step_avg:92.97ms
step:1476/1670 train_time:137227ms step_avg:92.97ms
step:1477/1670 train_time:137321ms step_avg:92.97ms
step:1478/1670 train_time:137413ms step_avg:92.97ms
step:1479/1670 train_time:137506ms step_avg:92.97ms
step:1480/1670 train_time:137598ms step_avg:92.97ms
step:1481/1670 train_time:137690ms step_avg:92.97ms
step:1482/1670 train_time:137783ms step_avg:92.97ms
step:1483/1670 train_time:137877ms step_avg:92.97ms
step:1484/1670 train_time:137970ms step_avg:92.97ms
step:1485/1670 train_time:138221ms step_avg:93.08ms
step:1486/1670 train_time:138293ms step_avg:93.06ms
step:1487/1670 train_time:138385ms step_avg:93.06ms
step:1488/1670 train_time:138477ms step_avg:93.06ms
step:1489/1670 train_time:138568ms step_avg:93.06ms
step:1490/1670 train_time:138660ms step_avg:93.06ms
step:1491/1670 train_time:138752ms step_avg:93.06ms
step:1492/1670 train_time:138844ms step_avg:93.06ms
step:1493/1670 train_time:138936ms step_avg:93.06ms
step:1494/1670 train_time:139028ms step_avg:93.06ms
step:1495/1670 train_time:139125ms step_avg:93.06ms
step:1496/1670 train_time:139224ms step_avg:93.06ms
step:1497/1670 train_time:139319ms step_avg:93.07ms
step:1498/1670 train_time:139412ms step_avg:93.07ms
step:1499/1670 train_time:139505ms step_avg:93.07ms
step:1500/1670 train_time:139597ms step_avg:93.06ms
step:1500/1670 val_loss:3.3115 train_time:139690ms step_avg:93.13ms
step:1501/1670 train_time:139710ms step_avg:93.08ms
step:1502/1670 train_time:139784ms step_avg:93.06ms
step:1503/1670 train_time:139876ms step_avg:93.06ms
step:1504/1670 train_time:139968ms step_avg:93.06ms
step:1505/1670 train_time:140060ms step_avg:93.06ms
step:1506/1670 train_time:140152ms step_avg:93.06ms
step:1507/1670 train_time:140246ms step_avg:93.06ms
step:1508/1670 train_time:140342ms step_avg:93.07ms
step:1509/1670 train_time:140436ms step_avg:93.07ms
step:1510/1670 train_time:140528ms step_avg:93.06ms
step:1511/1670 train_time:140622ms step_avg:93.07ms
step:1512/1670 train_time:140716ms step_avg:93.07ms
step:1513/1670 train_time:140809ms step_avg:93.07ms
step:1514/1670 train_time:140901ms step_avg:93.07ms
step:1515/1670 train_time:140993ms step_avg:93.06ms
step:1516/1670 train_time:141087ms step_avg:93.07ms
step:1517/1670 train_time:141180ms step_avg:93.06ms
step:1518/1670 train_time:141273ms step_avg:93.07ms
step:1519/1670 train_time:141368ms step_avg:93.07ms
step:1520/1670 train_time:141462ms step_avg:93.07ms
step:1521/1670 train_time:141555ms step_avg:93.07ms
step:1522/1670 train_time:141650ms step_avg:93.07ms
step:1523/1670 train_time:141744ms step_avg:93.07ms
step:1524/1670 train_time:141837ms step_avg:93.07ms
step:1525/1670 train_time:141930ms step_avg:93.07ms
step:1526/1670 train_time:142022ms step_avg:93.07ms
step:1527/1670 train_time:142114ms step_avg:93.07ms
step:1528/1670 train_time:142208ms step_avg:93.07ms
step:1529/1670 train_time:142301ms step_avg:93.07ms
step:1530/1670 train_time:142394ms step_avg:93.07ms
step:1531/1670 train_time:142489ms step_avg:93.07ms
step:1532/1670 train_time:142583ms step_avg:93.07ms
step:1533/1670 train_time:142677ms step_avg:93.07ms
step:1534/1670 train_time:142770ms step_avg:93.07ms
step:1535/1670 train_time:142863ms step_avg:93.07ms
step:1536/1670 train_time:142955ms step_avg:93.07ms
step:1537/1670 train_time:143047ms step_avg:93.07ms
step:1538/1670 train_time:143140ms step_avg:93.07ms
step:1539/1670 train_time:143233ms step_avg:93.07ms
step:1540/1670 train_time:143326ms step_avg:93.07ms
step:1541/1670 train_time:143421ms step_avg:93.07ms
step:1542/1670 train_time:143514ms step_avg:93.07ms
step:1543/1670 train_time:143607ms step_avg:93.07ms
step:1544/1670 train_time:143701ms step_avg:93.07ms
step:1545/1670 train_time:143793ms step_avg:93.07ms
step:1546/1670 train_time:143887ms step_avg:93.07ms
step:1547/1670 train_time:143979ms step_avg:93.07ms
step:1548/1670 train_time:144072ms step_avg:93.07ms
step:1549/1670 train_time:144164ms step_avg:93.07ms
step:1550/1670 train_time:144257ms step_avg:93.07ms
step:1551/1670 train_time:144351ms step_avg:93.07ms
step:1552/1670 train_time:144445ms step_avg:93.07ms
step:1553/1670 train_time:144538ms step_avg:93.07ms
step:1554/1670 train_time:144631ms step_avg:93.07ms
step:1555/1670 train_time:144723ms step_avg:93.07ms
step:1556/1670 train_time:144817ms step_avg:93.07ms
step:1557/1670 train_time:144911ms step_avg:93.07ms
step:1558/1670 train_time:145003ms step_avg:93.07ms
step:1559/1670 train_time:145095ms step_avg:93.07ms
step:1560/1670 train_time:145190ms step_avg:93.07ms
step:1561/1670 train_time:145283ms step_avg:93.07ms
step:1562/1670 train_time:145376ms step_avg:93.07ms
step:1563/1670 train_time:145471ms step_avg:93.07ms
step:1564/1670 train_time:145563ms step_avg:93.07ms
step:1565/1670 train_time:145656ms step_avg:93.07ms
step:1566/1670 train_time:145749ms step_avg:93.07ms
step:1567/1670 train_time:145842ms step_avg:93.07ms
step:1568/1670 train_time:145936ms step_avg:93.07ms
step:1569/1670 train_time:146029ms step_avg:93.07ms
step:1570/1670 train_time:146121ms step_avg:93.07ms
step:1571/1670 train_time:146215ms step_avg:93.07ms
step:1572/1670 train_time:146308ms step_avg:93.07ms
step:1573/1670 train_time:146402ms step_avg:93.07ms
step:1574/1670 train_time:146495ms step_avg:93.07ms
step:1575/1670 train_time:146589ms step_avg:93.07ms
step:1576/1670 train_time:146683ms step_avg:93.07ms
step:1577/1670 train_time:146776ms step_avg:93.07ms
step:1578/1670 train_time:146870ms step_avg:93.07ms
step:1579/1670 train_time:146965ms step_avg:93.07ms
step:1580/1670 train_time:147058ms step_avg:93.07ms
step:1581/1670 train_time:147151ms step_avg:93.07ms
step:1582/1670 train_time:147244ms step_avg:93.07ms
step:1583/1670 train_time:147338ms step_avg:93.08ms
step:1584/1670 train_time:147431ms step_avg:93.08ms
step:1585/1670 train_time:147524ms step_avg:93.07ms
step:1586/1670 train_time:147617ms step_avg:93.08ms
step:1587/1670 train_time:147710ms step_avg:93.08ms
step:1588/1670 train_time:147802ms step_avg:93.07ms
step:1589/1670 train_time:147895ms step_avg:93.07ms
step:1590/1670 train_time:147989ms step_avg:93.07ms
step:1591/1670 train_time:148083ms step_avg:93.08ms
step:1592/1670 train_time:148175ms step_avg:93.07ms
step:1593/1670 train_time:148269ms step_avg:93.08ms
step:1594/1670 train_time:148362ms step_avg:93.08ms
step:1595/1670 train_time:148455ms step_avg:93.08ms
step:1596/1670 train_time:148549ms step_avg:93.08ms
step:1597/1670 train_time:148643ms step_avg:93.08ms
step:1598/1670 train_time:148734ms step_avg:93.08ms
step:1599/1670 train_time:148827ms step_avg:93.07ms
step:1600/1670 train_time:148920ms step_avg:93.07ms
step:1601/1670 train_time:149013ms step_avg:93.07ms
step:1602/1670 train_time:149106ms step_avg:93.08ms
step:1603/1670 train_time:149199ms step_avg:93.07ms
step:1604/1670 train_time:149292ms step_avg:93.07ms
step:1605/1670 train_time:149386ms step_avg:93.08ms
step:1606/1670 train_time:149479ms step_avg:93.08ms
step:1607/1670 train_time:149572ms step_avg:93.08ms
step:1608/1670 train_time:149666ms step_avg:93.08ms
step:1609/1670 train_time:149759ms step_avg:93.08ms
step:1610/1670 train_time:149853ms step_avg:93.08ms
step:1611/1670 train_time:149946ms step_avg:93.08ms
step:1612/1670 train_time:150039ms step_avg:93.08ms
step:1613/1670 train_time:150132ms step_avg:93.08ms
step:1614/1670 train_time:150225ms step_avg:93.08ms
step:1615/1670 train_time:150319ms step_avg:93.08ms
step:1616/1670 train_time:150412ms step_avg:93.08ms
step:1617/1670 train_time:150505ms step_avg:93.08ms
step:1618/1670 train_time:150597ms step_avg:93.08ms
step:1619/1670 train_time:150692ms step_avg:93.08ms
step:1620/1670 train_time:150785ms step_avg:93.08ms
step:1621/1670 train_time:150878ms step_avg:93.08ms
step:1622/1670 train_time:150971ms step_avg:93.08ms
step:1623/1670 train_time:151064ms step_avg:93.08ms
step:1624/1670 train_time:151156ms step_avg:93.08ms
step:1625/1670 train_time:151251ms step_avg:93.08ms
step:1625/1670 val_loss:3.2866 train_time:151345ms step_avg:93.14ms
step:1626/1670 train_time:151370ms step_avg:93.09ms
step:1627/1670 train_time:151440ms step_avg:93.08ms
step:1628/1670 train_time:151535ms step_avg:93.08ms
step:1629/1670 train_time:151628ms step_avg:93.08ms
step:1630/1670 train_time:151720ms step_avg:93.08ms
step:1631/1670 train_time:151814ms step_avg:93.08ms
step:1632/1670 train_time:151908ms step_avg:93.08ms
step:1633/1670 train_time:152000ms step_avg:93.08ms
step:1634/1670 train_time:152093ms step_avg:93.08ms
step:1635/1670 train_time:152186ms step_avg:93.08ms
step:1636/1670 train_time:152281ms step_avg:93.08ms
step:1637/1670 train_time:152374ms step_avg:93.08ms
step:1638/1670 train_time:152468ms step_avg:93.08ms
step:1639/1670 train_time:152561ms step_avg:93.08ms
step:1640/1670 train_time:152656ms step_avg:93.08ms
step:1641/1670 train_time:152749ms step_avg:93.08ms
step:1642/1670 train_time:152842ms step_avg:93.08ms
step:1643/1670 train_time:152939ms step_avg:93.09ms
step:1644/1670 train_time:153031ms step_avg:93.08ms
step:1645/1670 train_time:153124ms step_avg:93.08ms
step:1646/1670 train_time:153217ms step_avg:93.08ms
step:1647/1670 train_time:153311ms step_avg:93.08ms
step:1648/1670 train_time:153404ms step_avg:93.09ms
step:1649/1670 train_time:153498ms step_avg:93.09ms
step:1650/1670 train_time:153592ms step_avg:93.09ms
step:1651/1670 train_time:153685ms step_avg:93.09ms
step:1652/1670 train_time:153778ms step_avg:93.09ms
step:1653/1670 train_time:153870ms step_avg:93.09ms
step:1654/1670 train_time:153963ms step_avg:93.09ms
step:1655/1670 train_time:154058ms step_avg:93.09ms
step:1656/1670 train_time:154151ms step_avg:93.09ms
step:1657/1670 train_time:154244ms step_avg:93.09ms
step:1658/1670 train_time:154336ms step_avg:93.09ms
step:1659/1670 train_time:154429ms step_avg:93.09ms
step:1660/1670 train_time:154522ms step_avg:93.09ms
step:1661/1670 train_time:154615ms step_avg:93.09ms
step:1662/1670 train_time:154708ms step_avg:93.09ms
step:1663/1670 train_time:154801ms step_avg:93.09ms
step:1664/1670 train_time:154896ms step_avg:93.09ms
step:1665/1670 train_time:154988ms step_avg:93.09ms
step:1666/1670 train_time:155082ms step_avg:93.09ms
step:1667/1670 train_time:155175ms step_avg:93.09ms
step:1668/1670 train_time:155268ms step_avg:93.09ms
step:1669/1670 train_time:155361ms step_avg:93.09ms
step:1670/1670 train_time:155455ms step_avg:93.09ms
step:1670/1670 val_loss:3.2781 train_time:155717ms step_avg:93.24ms
peak memory allocated: 32002 MiB reserved: 46556 MiB

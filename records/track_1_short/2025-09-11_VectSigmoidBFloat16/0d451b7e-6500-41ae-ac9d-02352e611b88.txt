import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(
    x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(
    g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99


def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]


@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(
        pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M
    )

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr,
    C_ptr,
    M,
    K,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr,
    C_ptr,
    M,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    alpha,
    beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (
        offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c
    )
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out


@torch.compile(
    dynamic=False, fullgraph=True
)  # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer


class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(
        self,
        params,
        lr: float = 1e-3,
        betas: tuple[float, float] = (0.9, 0.999),
        eps: float = 1e-8,
        weight_decay: float = 0.01,
    ):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(
                        grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True
                    ).get_future()
                )
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group["betas"]
            eps = group["eps"]
            wd = group["weight_decay"]
            params = group["params"]
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size : (rank + 1) * rank_size]
                lr = group["lr"] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(
                    g_slice, g_slice, value=1 - beta2
                )
                # bias corrections
                bias1 = 1 - beta1**t
                bias2 = 1 - beta2**t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(
                    dist.all_gather_into_tensor(
                        p, p_slice, async_op=True
                    ).get_future()
                )
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model


def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(
        self,
        in_features: int,
        out_features: int,
        use_fp8=False,
        x_s=1.0,
        w_s=1.0,
        grad_s=1.0,
    ):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (
            self.in_features**-0.5
        )  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3**0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(
                _x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s
            )[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    rotary_cos: torch.Tensor
    rotary_sin: torch.Tensor
    attn_scale: float


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        rotary_cos, rotary_sin = attn_args.rotary_cos, attn_args.rotary_sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = (
            attn_args.seqlens,
            attn_args.attn_scale,
            attn_args.bm_size,
        )

        q, k, v = (
            F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x))
            .view(B, T, 3 * self.num_heads, self.head_dim)
            .chunk(3, dim=-2)
        )
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = (
            rotary(q, rotary_cos, rotary_sin),
            rotary(k, rotary_cos, rotary_sin),
        )
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(
                v
            )  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = (
            args.train_max_seq_len
            if self.training
            else (args.val_batch_size // (grad_accum_steps * world_size))
        )

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(
            q[0],
            k[0],
            v[0],
            cu_seqlens_q=seqlens,
            cu_seqlens_k=seqlens,
            max_seqlen_q=max_len,
            max_seqlen_k=max_len,
            causal=True,
            softmax_scale=attn_scale,
            window_size=(bm_size, 0),
        )
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(
            self.attn_gate(x[..., : self.attn_gate.weight.size(-1)])
        ).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(
            B, T, self.num_heads * self.head_dim
        )  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x
        ).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = (
            CausalSelfAttention(dim, head_dim, num_heads)
            if layer_idx != 7
            else None
        )
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(
        self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs
    ):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model


def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        num_layers: int,
        num_heads: int,
        head_dim: int,
        model_dim: int,
        max_seq_len: int,
    ):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList(
            [nn.Embedding(vocab_size, model_dim) for _ in range(3)]
        )
        self.blocks = nn.ModuleList(
            [
                Block(model_dim, head_dim, num_heads, i)
                for i in range(num_layers)
            ]
        )
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(
            model_dim,
            vocab_size,
            use_fp8=use_fp8,
            x_s=(model_dim**0.5) / 448,
            w_s=2**-9,
            grad_s=1 / 448,
        )
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.ones(pad),
                ]
            )
        )
        self.max_seq_len = max_seq_len
        self.setup_yarn(head_dim)
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.0
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.0
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def setup_yarn(self, head_dim: int):
        # store single copy of rotary tensors
        angular_freq = (1 / 1024) ** torch.linspace(
            0, 1, steps=head_dim // 4, dtype=torch.float32
        )
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat(
            [angular_freq, angular_freq.new_zeros(head_dim // 4)]
        )
        t = torch.arange(self.max_seq_len, dtype=torch.float32)
        theta = torch.outer(t, angular_freq)
        self.rotary_cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.rotary_sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq

        # scale attention factor f in attn=softmax(f*qk) logarithmically with window size @classiclarryd
        windows = list(
            dict.fromkeys(list(args.ws_schedule) + [args.ws_validate])
        )
        scale_factors = [
            0.2 * math.log(curr / prev) + 1
            for prev, curr in zip(windows[:-1], windows[1:])
        ]
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        attn_scales = list(
            accumulate([0.1] + scale_factors, lambda acc, factor: acc * factor)
        )
        self.attn_scales = dict(zip(windows, attn_scales))

    def apply_yarn(
        self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32
    ):
        rotations = (
            args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        )
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp(
            (rotations - alpha) / (beta - alpha), 0, 1
        )
        self.angular_freq *= scaling_factor + interpolation_weight * (
            1 - scaling_factor
        )
        t = torch.arange(
            self.max_seq_len,
            dtype=torch.float32,
            device=self.angular_freq.device,
        )
        theta = torch.outer(t, self.angular_freq)
        self.rotary_cos.copy_(theta.cos())
        self.rotary_sin.copy_(theta.sin())

    def forward(
        self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int
    ):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = (
            [ve[0], ve[1], ve[2]]
            + [None] * (len(self.blocks) - 6)
            + [ve[0], ve[1], ve[2]]
        )
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        bm_sizes = [
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
        ]
        assert len(bm_sizes) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]).to(
            torch.bfloat16
        )  # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[: (len(self.blocks) // 2)]
        lambdas = self.scalars[
            1 * len(self.blocks) : 3 * len(self.blocks)
        ].view(-1, 2)
        sa_lambdas = self.scalars[
            3 * len(self.blocks) : 5 * len(self.blocks)
        ].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                rotary_cos=self.rotary_cos,
                rotary_sin=self.rotary_sin,
                attn_scale=self.attn_scales[ws],
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(
            logits.view(-1, logits.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader


def _load_data_shard(file: Path):
    header = torch.from_file(
        str(file), False, 256, dtype=torch.int32
    )  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(
            num_tokens, dtype=torch.uint16, pin_memory=True
        )  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(
            tokens.numpy()
        )  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, (
            "number of tokens read does not match header"
        )
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1):
        # Precompute BOS positions once per shard
        self.size = tokens.numel()
        self.bos_idx = (
            (tokens == BOS_ID)
            .nonzero(as_tuple=True)[0]
            .to(torch.int64)
            .cpu()
            .numpy()
        )
        self.i = 0
        self.world_size = world_size

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(
                        f"Insufficient BOS ahead of position {cur}; hit tail of shard."
                    )
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(
                    self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                    cur + max_seq_len,
                    cur + num_tokens_local - cur_len + 1,
                )
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx

        return starts, ends


def distributed_data_generator(
    filename_pattern: str,
    num_tokens: int,
    max_seq_len: int,
    grad_accum_steps: int = 1,
    align_to_bos: bool = True,
):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, (
        "Batch size must be divisible by world size"
    )
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(
            f"No files found for pattern: {filename_pattern}"
        )

    file_iter = iter(
        files
    )  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    finder = BOSFinder(tokens, world_size=world_size) if align_to_bos else None
    pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(
            num_tokens_local // 300, n=128
        )  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(
                    num_tokens_local, max_seq_len
                )
                start_idxs, end_idxs = (
                    torch.tensor(seq_starts[rank]),
                    torch.tensor(seq_ends[rank]),
                )
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens = _load_data_shard(next(file_iter))
                finder = BOSFinder(tokens, world_size=world_size)
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= (
                1  # last document was too long to account for _targets offset
            )
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(
                tokens
            ):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local : pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(
                num_tokens_local,
            )
            _targets = buf[1:].view(
                num_tokens_local,
            )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1 : len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(
                device="cuda", dtype=torch.int32, non_blocking=True
            ),
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, (
                "Num tokens must be divisible by world size"
            )
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main


@dataclass
class Hyperparameters:
    # data
    train_files: str = (
        "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    )
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1670  # number of iterations to run
    cooldown_frac: int = (
        0.5  # fraction of training spent cooling down the learning rate
    )
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = (
        125  # every how many steps to evaluate val loss? 0 for only at the end
    )
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws @classiclarryd


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = rank == 0  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(
    f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}"
)
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import

    return subprocess.run(
        ["nvidia-smi"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    ).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size)
    // (grad_accum_steps * world_size),
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [
    p
    for n, p in model.blocks.named_parameters()
    if p.ndim >= 2 and "embed" not in n
]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(
    hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)  # save the initial state
train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    ws = args.ws_schedule[
        step % len(args.ws_schedule)
    ]  # each window size is a new graph, need to warm up each
    model(inputs, targets, cum_seqlens, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws = get_ws(0)
for step in range(train_steps + 1):
    last_step = step == train_steps
    new_ws = get_ws(step)
    if new_ws != ws:
        model.apply_yarn(ws, new_ws)
        ws = new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (
        args.val_loss_every > 0 and step % args.val_loss_every == 0
    ):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(
            args.val_files,
            args.val_batch_size,
            -1,
            grad_accum_steps=grad_accum_steps,
            align_to_bos=False,
        )
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True,
        )
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(
                step=step,
                code=code,
                model=model.state_dict(),
                optimizers=[opt.state_dict() for opt in optimizers],
            )
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1)  # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (
        time.perf_counter() - t0
    )
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True,
    )

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.11 (main, Sep  2 2025, 14:20:58) [Clang 20.1.4 ]
Running PyTorch 2.9.0.dev20250718+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 11 09:47:47 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                  Off |
| N/A   40C    P0            126W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                  Off |
| N/A   45C    P0            135W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:65:00.0 Off |                  Off |
| N/A   46C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:68:00.0 Off |                  Off |
| N/A   37C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:85:00.0 Off |                  Off |
| N/A   38C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:86:00.0 Off |                  Off |
| N/A   46C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:E5:00.0 Off |                  Off |
| N/A   45C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E8:00.0 Off |                  Off |
| N/A   41C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1670 val_loss:10.8258 train_time:0ms step_avg:0.07ms
step:1/1670 train_time:296ms step_avg:295.61ms
step:2/1670 train_time:314ms step_avg:157.20ms
step:3/1670 train_time:382ms step_avg:127.43ms
step:4/1670 train_time:471ms step_avg:117.84ms
step:5/1670 train_time:562ms step_avg:112.38ms
step:6/1670 train_time:652ms step_avg:108.64ms
step:7/1670 train_time:743ms step_avg:106.07ms
step:8/1670 train_time:833ms step_avg:104.16ms
step:9/1670 train_time:924ms step_avg:102.68ms
step:10/1670 train_time:1015ms step_avg:101.46ms
step:11/1670 train_time:1106ms step_avg:100.51ms
step:12/1670 train_time:1199ms step_avg:99.91ms
step:13/1670 train_time:1294ms step_avg:99.50ms
step:14/1670 train_time:1386ms step_avg:99.02ms
step:15/1670 train_time:1478ms step_avg:98.50ms
step:16/1670 train_time:1570ms step_avg:98.10ms
step:17/1670 train_time:1662ms step_avg:97.75ms
step:18/1670 train_time:1753ms step_avg:97.36ms
step:19/1670 train_time:1843ms step_avg:97.02ms
step:20/1670 train_time:1934ms step_avg:96.69ms
step:21/1670 train_time:2026ms step_avg:96.48ms
step:22/1670 train_time:2118ms step_avg:96.26ms
step:23/1670 train_time:2211ms step_avg:96.12ms
step:24/1670 train_time:2304ms step_avg:96.01ms
step:25/1670 train_time:2396ms step_avg:95.86ms
step:26/1670 train_time:2488ms step_avg:95.69ms
step:27/1670 train_time:2579ms step_avg:95.53ms
step:28/1670 train_time:2671ms step_avg:95.39ms
step:29/1670 train_time:2763ms step_avg:95.29ms
step:30/1670 train_time:2854ms step_avg:95.13ms
step:31/1670 train_time:2945ms step_avg:95.01ms
step:32/1670 train_time:3036ms step_avg:94.87ms
step:33/1670 train_time:3128ms step_avg:94.78ms
step:34/1670 train_time:3220ms step_avg:94.69ms
step:35/1670 train_time:3312ms step_avg:94.62ms
step:36/1670 train_time:3405ms step_avg:94.57ms
step:37/1670 train_time:3496ms step_avg:94.48ms
step:38/1670 train_time:3587ms step_avg:94.40ms
step:39/1670 train_time:3679ms step_avg:94.34ms
step:40/1670 train_time:3772ms step_avg:94.30ms
step:41/1670 train_time:3864ms step_avg:94.25ms
step:42/1670 train_time:3955ms step_avg:94.18ms
step:43/1670 train_time:4048ms step_avg:94.13ms
step:44/1670 train_time:4140ms step_avg:94.09ms
step:45/1670 train_time:4231ms step_avg:94.02ms
step:46/1670 train_time:4322ms step_avg:93.97ms
step:47/1670 train_time:4414ms step_avg:93.91ms
step:48/1670 train_time:4505ms step_avg:93.86ms
step:49/1670 train_time:4597ms step_avg:93.82ms
step:50/1670 train_time:4689ms step_avg:93.78ms
step:51/1670 train_time:4780ms step_avg:93.73ms
step:52/1670 train_time:4872ms step_avg:93.69ms
step:53/1670 train_time:4964ms step_avg:93.66ms
step:54/1670 train_time:5055ms step_avg:93.62ms
step:55/1670 train_time:5149ms step_avg:93.61ms
step:56/1670 train_time:5241ms step_avg:93.59ms
step:57/1670 train_time:5333ms step_avg:93.56ms
step:58/1670 train_time:5425ms step_avg:93.54ms
step:59/1670 train_time:5516ms step_avg:93.50ms
step:60/1670 train_time:5608ms step_avg:93.46ms
step:61/1670 train_time:5699ms step_avg:93.42ms
step:62/1670 train_time:5790ms step_avg:93.39ms
step:63/1670 train_time:5881ms step_avg:93.35ms
step:64/1670 train_time:5972ms step_avg:93.32ms
step:65/1670 train_time:6064ms step_avg:93.29ms
step:66/1670 train_time:6155ms step_avg:93.26ms
step:67/1670 train_time:6248ms step_avg:93.25ms
step:68/1670 train_time:6339ms step_avg:93.23ms
step:69/1670 train_time:6432ms step_avg:93.21ms
step:70/1670 train_time:6524ms step_avg:93.20ms
step:71/1670 train_time:6615ms step_avg:93.18ms
step:72/1670 train_time:6706ms step_avg:93.14ms
step:73/1670 train_time:6798ms step_avg:93.12ms
step:74/1670 train_time:6890ms step_avg:93.11ms
step:75/1670 train_time:6982ms step_avg:93.09ms
step:76/1670 train_time:7073ms step_avg:93.07ms
step:77/1670 train_time:7166ms step_avg:93.07ms
step:78/1670 train_time:7258ms step_avg:93.05ms
step:79/1670 train_time:7350ms step_avg:93.04ms
step:80/1670 train_time:7443ms step_avg:93.03ms
step:81/1670 train_time:7534ms step_avg:93.01ms
step:82/1670 train_time:7626ms step_avg:93.00ms
step:83/1670 train_time:7717ms step_avg:92.98ms
step:84/1670 train_time:7809ms step_avg:92.97ms
step:85/1670 train_time:7900ms step_avg:92.94ms
step:86/1670 train_time:7991ms step_avg:92.92ms
step:87/1670 train_time:8083ms step_avg:92.91ms
step:88/1670 train_time:8174ms step_avg:92.88ms
step:89/1670 train_time:8267ms step_avg:92.89ms
step:90/1670 train_time:8360ms step_avg:92.89ms
step:91/1670 train_time:8452ms step_avg:92.88ms
step:92/1670 train_time:8544ms step_avg:92.87ms
step:93/1670 train_time:8635ms step_avg:92.85ms
step:94/1670 train_time:8727ms step_avg:92.84ms
step:95/1670 train_time:8820ms step_avg:92.84ms
step:96/1670 train_time:8911ms step_avg:92.82ms
step:97/1670 train_time:9002ms step_avg:92.81ms
step:98/1670 train_time:9093ms step_avg:92.79ms
step:99/1670 train_time:9185ms step_avg:92.78ms
step:100/1670 train_time:9277ms step_avg:92.77ms
step:101/1670 train_time:9370ms step_avg:92.77ms
step:102/1670 train_time:9462ms step_avg:92.76ms
step:103/1670 train_time:9552ms step_avg:92.74ms
step:104/1670 train_time:9644ms step_avg:92.73ms
step:105/1670 train_time:9734ms step_avg:92.71ms
step:106/1670 train_time:9826ms step_avg:92.70ms
step:107/1670 train_time:9917ms step_avg:92.68ms
step:108/1670 train_time:10009ms step_avg:92.67ms
step:109/1670 train_time:10100ms step_avg:92.66ms
step:110/1670 train_time:10192ms step_avg:92.65ms
step:111/1670 train_time:10283ms step_avg:92.64ms
step:112/1670 train_time:10373ms step_avg:92.62ms
step:113/1670 train_time:10466ms step_avg:92.62ms
step:114/1670 train_time:10558ms step_avg:92.61ms
step:115/1670 train_time:10650ms step_avg:92.61ms
step:116/1670 train_time:10741ms step_avg:92.60ms
step:117/1670 train_time:10832ms step_avg:92.58ms
step:118/1670 train_time:10924ms step_avg:92.57ms
step:119/1670 train_time:11014ms step_avg:92.56ms
step:120/1670 train_time:11106ms step_avg:92.55ms
step:121/1670 train_time:11198ms step_avg:92.54ms
step:122/1670 train_time:11289ms step_avg:92.53ms
step:123/1670 train_time:11381ms step_avg:92.52ms
step:124/1670 train_time:11472ms step_avg:92.51ms
step:125/1670 train_time:11563ms step_avg:92.50ms
step:125/1670 val_loss:4.3109 train_time:11654ms step_avg:93.23ms
step:126/1670 train_time:11674ms step_avg:92.65ms
step:127/1670 train_time:11750ms step_avg:92.52ms
step:128/1670 train_time:11851ms step_avg:92.59ms
step:129/1670 train_time:11945ms step_avg:92.59ms
step:130/1670 train_time:12036ms step_avg:92.58ms
step:131/1670 train_time:12127ms step_avg:92.57ms
step:132/1670 train_time:12216ms step_avg:92.55ms
step:133/1670 train_time:12307ms step_avg:92.54ms
step:134/1670 train_time:12397ms step_avg:92.51ms
step:135/1670 train_time:12487ms step_avg:92.50ms
step:136/1670 train_time:12578ms step_avg:92.49ms
step:137/1670 train_time:12670ms step_avg:92.48ms
step:138/1670 train_time:12763ms step_avg:92.48ms
step:139/1670 train_time:12857ms step_avg:92.50ms
step:140/1670 train_time:12950ms step_avg:92.50ms
step:141/1670 train_time:13041ms step_avg:92.49ms
step:142/1670 train_time:13132ms step_avg:92.48ms
step:143/1670 train_time:13224ms step_avg:92.47ms
step:144/1670 train_time:13314ms step_avg:92.46ms
step:145/1670 train_time:13404ms step_avg:92.44ms
step:146/1670 train_time:13496ms step_avg:92.44ms
step:147/1670 train_time:13586ms step_avg:92.42ms
step:148/1670 train_time:13677ms step_avg:92.41ms
step:149/1670 train_time:13769ms step_avg:92.41ms
step:150/1670 train_time:13861ms step_avg:92.41ms
step:151/1670 train_time:13955ms step_avg:92.42ms
step:152/1670 train_time:14048ms step_avg:92.42ms
step:153/1670 train_time:14138ms step_avg:92.41ms
step:154/1670 train_time:14229ms step_avg:92.40ms
step:155/1670 train_time:14320ms step_avg:92.39ms
step:156/1670 train_time:14411ms step_avg:92.38ms
step:157/1670 train_time:14501ms step_avg:92.36ms
step:158/1670 train_time:14591ms step_avg:92.35ms
step:159/1670 train_time:14682ms step_avg:92.34ms
step:160/1670 train_time:14774ms step_avg:92.34ms
step:161/1670 train_time:14866ms step_avg:92.33ms
step:162/1670 train_time:14957ms step_avg:92.33ms
step:163/1670 train_time:15050ms step_avg:92.33ms
step:164/1670 train_time:15142ms step_avg:92.33ms
step:165/1670 train_time:15234ms step_avg:92.32ms
step:166/1670 train_time:15324ms step_avg:92.31ms
step:167/1670 train_time:15414ms step_avg:92.30ms
step:168/1670 train_time:15506ms step_avg:92.30ms
step:169/1670 train_time:15598ms step_avg:92.29ms
step:170/1670 train_time:15688ms step_avg:92.28ms
step:171/1670 train_time:15779ms step_avg:92.28ms
step:172/1670 train_time:15872ms step_avg:92.28ms
step:173/1670 train_time:15963ms step_avg:92.27ms
step:174/1670 train_time:16055ms step_avg:92.27ms
step:175/1670 train_time:16148ms step_avg:92.27ms
step:176/1670 train_time:16239ms step_avg:92.26ms
step:177/1670 train_time:16329ms step_avg:92.26ms
step:178/1670 train_time:16420ms step_avg:92.25ms
step:179/1670 train_time:16512ms step_avg:92.25ms
step:180/1670 train_time:16603ms step_avg:92.24ms
step:181/1670 train_time:16693ms step_avg:92.23ms
step:182/1670 train_time:16784ms step_avg:92.22ms
step:183/1670 train_time:16875ms step_avg:92.22ms
step:184/1670 train_time:16967ms step_avg:92.21ms
step:185/1670 train_time:17058ms step_avg:92.21ms
step:186/1670 train_time:17152ms step_avg:92.21ms
step:187/1670 train_time:17243ms step_avg:92.21ms
step:188/1670 train_time:17334ms step_avg:92.20ms
step:189/1670 train_time:17425ms step_avg:92.20ms
step:190/1670 train_time:17516ms step_avg:92.19ms
step:191/1670 train_time:17608ms step_avg:92.19ms
step:192/1670 train_time:17698ms step_avg:92.18ms
step:193/1670 train_time:17789ms step_avg:92.17ms
step:194/1670 train_time:17881ms step_avg:92.17ms
step:195/1670 train_time:17972ms step_avg:92.16ms
step:196/1670 train_time:18063ms step_avg:92.16ms
step:197/1670 train_time:18155ms step_avg:92.16ms
step:198/1670 train_time:18246ms step_avg:92.15ms
step:199/1670 train_time:18337ms step_avg:92.15ms
step:200/1670 train_time:18429ms step_avg:92.14ms
step:201/1670 train_time:18519ms step_avg:92.13ms
step:202/1670 train_time:18611ms step_avg:92.13ms
step:203/1670 train_time:18701ms step_avg:92.12ms
step:204/1670 train_time:18792ms step_avg:92.12ms
step:205/1670 train_time:18883ms step_avg:92.11ms
step:206/1670 train_time:18974ms step_avg:92.11ms
step:207/1670 train_time:19065ms step_avg:92.10ms
step:208/1670 train_time:19156ms step_avg:92.10ms
step:209/1670 train_time:19249ms step_avg:92.10ms
step:210/1670 train_time:19339ms step_avg:92.09ms
step:211/1670 train_time:19431ms step_avg:92.09ms
step:212/1670 train_time:19522ms step_avg:92.09ms
step:213/1670 train_time:19773ms step_avg:92.83ms
step:214/1670 train_time:19842ms step_avg:92.72ms
step:215/1670 train_time:19932ms step_avg:92.71ms
step:216/1670 train_time:20022ms step_avg:92.69ms
step:217/1670 train_time:20112ms step_avg:92.68ms
step:218/1670 train_time:20202ms step_avg:92.67ms
step:219/1670 train_time:20292ms step_avg:92.66ms
step:220/1670 train_time:20382ms step_avg:92.64ms
step:221/1670 train_time:20471ms step_avg:92.63ms
step:222/1670 train_time:20561ms step_avg:92.62ms
step:223/1670 train_time:20654ms step_avg:92.62ms
step:224/1670 train_time:20751ms step_avg:92.64ms
step:225/1670 train_time:20846ms step_avg:92.65ms
step:226/1670 train_time:20938ms step_avg:92.65ms
step:227/1670 train_time:21029ms step_avg:92.64ms
step:228/1670 train_time:21119ms step_avg:92.63ms
step:229/1670 train_time:21209ms step_avg:92.62ms
step:230/1670 train_time:21299ms step_avg:92.61ms
step:231/1670 train_time:21389ms step_avg:92.59ms
step:232/1670 train_time:21480ms step_avg:92.58ms
step:233/1670 train_time:21570ms step_avg:92.58ms
step:234/1670 train_time:21663ms step_avg:92.58ms
step:235/1670 train_time:21757ms step_avg:92.58ms
step:236/1670 train_time:21850ms step_avg:92.59ms
step:237/1670 train_time:21941ms step_avg:92.58ms
step:238/1670 train_time:22034ms step_avg:92.58ms
step:239/1670 train_time:22124ms step_avg:92.57ms
step:240/1670 train_time:22215ms step_avg:92.56ms
step:241/1670 train_time:22305ms step_avg:92.55ms
step:242/1670 train_time:22395ms step_avg:92.54ms
step:243/1670 train_time:22485ms step_avg:92.53ms
step:244/1670 train_time:22576ms step_avg:92.52ms
step:245/1670 train_time:22667ms step_avg:92.52ms
step:246/1670 train_time:22759ms step_avg:92.52ms
step:247/1670 train_time:22853ms step_avg:92.52ms
step:248/1670 train_time:22945ms step_avg:92.52ms
step:249/1670 train_time:23037ms step_avg:92.52ms
step:250/1670 train_time:23129ms step_avg:92.51ms
step:250/1670 val_loss:3.9614 train_time:23218ms step_avg:92.87ms
step:251/1670 train_time:23238ms step_avg:92.58ms
step:252/1670 train_time:23312ms step_avg:92.51ms
step:253/1670 train_time:23404ms step_avg:92.50ms
step:254/1670 train_time:23494ms step_avg:92.50ms
step:255/1670 train_time:23585ms step_avg:92.49ms
step:256/1670 train_time:23676ms step_avg:92.48ms
step:257/1670 train_time:23766ms step_avg:92.47ms
step:258/1670 train_time:23856ms step_avg:92.47ms
step:259/1670 train_time:23946ms step_avg:92.46ms
step:260/1670 train_time:24037ms step_avg:92.45ms
step:261/1670 train_time:24130ms step_avg:92.45ms
step:262/1670 train_time:24223ms step_avg:92.46ms
step:263/1670 train_time:24316ms step_avg:92.46ms
step:264/1670 train_time:24408ms step_avg:92.46ms
step:265/1670 train_time:24500ms step_avg:92.45ms
step:266/1670 train_time:24590ms step_avg:92.44ms
step:267/1670 train_time:24681ms step_avg:92.44ms
step:268/1670 train_time:24771ms step_avg:92.43ms
step:269/1670 train_time:24862ms step_avg:92.42ms
step:270/1670 train_time:24952ms step_avg:92.42ms
step:271/1670 train_time:25043ms step_avg:92.41ms
step:272/1670 train_time:25134ms step_avg:92.41ms
step:273/1670 train_time:25227ms step_avg:92.41ms
step:274/1670 train_time:25320ms step_avg:92.41ms
step:275/1670 train_time:25411ms step_avg:92.40ms
step:276/1670 train_time:25502ms step_avg:92.40ms
step:277/1670 train_time:25593ms step_avg:92.39ms
step:278/1670 train_time:25684ms step_avg:92.39ms
step:279/1670 train_time:25775ms step_avg:92.38ms
step:280/1670 train_time:25866ms step_avg:92.38ms
step:281/1670 train_time:25956ms step_avg:92.37ms
step:282/1670 train_time:26047ms step_avg:92.37ms
step:283/1670 train_time:26139ms step_avg:92.37ms
step:284/1670 train_time:26231ms step_avg:92.36ms
step:285/1670 train_time:26323ms step_avg:92.36ms
step:286/1670 train_time:26414ms step_avg:92.36ms
step:287/1670 train_time:26506ms step_avg:92.35ms
step:288/1670 train_time:26597ms step_avg:92.35ms
step:289/1670 train_time:26688ms step_avg:92.35ms
step:290/1670 train_time:26778ms step_avg:92.34ms
step:291/1670 train_time:26869ms step_avg:92.33ms
step:292/1670 train_time:26960ms step_avg:92.33ms
step:293/1670 train_time:27050ms step_avg:92.32ms
step:294/1670 train_time:27142ms step_avg:92.32ms
step:295/1670 train_time:27233ms step_avg:92.32ms
step:296/1670 train_time:27325ms step_avg:92.32ms
step:297/1670 train_time:27416ms step_avg:92.31ms
step:298/1670 train_time:27510ms step_avg:92.32ms
step:299/1670 train_time:27601ms step_avg:92.31ms
step:300/1670 train_time:27692ms step_avg:92.31ms
step:301/1670 train_time:27784ms step_avg:92.30ms
step:302/1670 train_time:27874ms step_avg:92.30ms
step:303/1670 train_time:27965ms step_avg:92.29ms
step:304/1670 train_time:28056ms step_avg:92.29ms
step:305/1670 train_time:28147ms step_avg:92.28ms
step:306/1670 train_time:28238ms step_avg:92.28ms
step:307/1670 train_time:28329ms step_avg:92.28ms
step:308/1670 train_time:28422ms step_avg:92.28ms
step:309/1670 train_time:28513ms step_avg:92.27ms
step:310/1670 train_time:28605ms step_avg:92.27ms
step:311/1670 train_time:28696ms step_avg:92.27ms
step:312/1670 train_time:28788ms step_avg:92.27ms
step:313/1670 train_time:28879ms step_avg:92.27ms
step:314/1670 train_time:28970ms step_avg:92.26ms
step:315/1670 train_time:29061ms step_avg:92.26ms
step:316/1670 train_time:29152ms step_avg:92.25ms
step:317/1670 train_time:29242ms step_avg:92.25ms
step:318/1670 train_time:29334ms step_avg:92.25ms
step:319/1670 train_time:29426ms step_avg:92.24ms
step:320/1670 train_time:29517ms step_avg:92.24ms
step:321/1670 train_time:29609ms step_avg:92.24ms
step:322/1670 train_time:29701ms step_avg:92.24ms
step:323/1670 train_time:29792ms step_avg:92.23ms
step:324/1670 train_time:29883ms step_avg:92.23ms
step:325/1670 train_time:29974ms step_avg:92.23ms
step:326/1670 train_time:30066ms step_avg:92.23ms
step:327/1670 train_time:30156ms step_avg:92.22ms
step:328/1670 train_time:30248ms step_avg:92.22ms
step:329/1670 train_time:30339ms step_avg:92.22ms
step:330/1670 train_time:30430ms step_avg:92.21ms
step:331/1670 train_time:30522ms step_avg:92.21ms
step:332/1670 train_time:30614ms step_avg:92.21ms
step:333/1670 train_time:30706ms step_avg:92.21ms
step:334/1670 train_time:30797ms step_avg:92.21ms
step:335/1670 train_time:30889ms step_avg:92.21ms
step:336/1670 train_time:30981ms step_avg:92.20ms
step:337/1670 train_time:31071ms step_avg:92.20ms
step:338/1670 train_time:31163ms step_avg:92.20ms
step:339/1670 train_time:31254ms step_avg:92.19ms
step:340/1670 train_time:31345ms step_avg:92.19ms
step:341/1670 train_time:31436ms step_avg:92.19ms
step:342/1670 train_time:31527ms step_avg:92.18ms
step:343/1670 train_time:31618ms step_avg:92.18ms
step:344/1670 train_time:31710ms step_avg:92.18ms
step:345/1670 train_time:31802ms step_avg:92.18ms
step:346/1670 train_time:31892ms step_avg:92.17ms
step:347/1670 train_time:31984ms step_avg:92.17ms
step:348/1670 train_time:32075ms step_avg:92.17ms
step:349/1670 train_time:32166ms step_avg:92.17ms
step:350/1670 train_time:32257ms step_avg:92.16ms
step:351/1670 train_time:32350ms step_avg:92.17ms
step:352/1670 train_time:32441ms step_avg:92.16ms
step:353/1670 train_time:32532ms step_avg:92.16ms
step:354/1670 train_time:32624ms step_avg:92.16ms
step:355/1670 train_time:32715ms step_avg:92.16ms
step:356/1670 train_time:32808ms step_avg:92.16ms
step:357/1670 train_time:32898ms step_avg:92.15ms
step:358/1670 train_time:32989ms step_avg:92.15ms
step:359/1670 train_time:33081ms step_avg:92.15ms
step:360/1670 train_time:33172ms step_avg:92.14ms
step:361/1670 train_time:33263ms step_avg:92.14ms
step:362/1670 train_time:33354ms step_avg:92.14ms
step:363/1670 train_time:33445ms step_avg:92.13ms
step:364/1670 train_time:33535ms step_avg:92.13ms
step:365/1670 train_time:33628ms step_avg:92.13ms
step:366/1670 train_time:33721ms step_avg:92.13ms
step:367/1670 train_time:33812ms step_avg:92.13ms
step:368/1670 train_time:33903ms step_avg:92.13ms
step:369/1670 train_time:33994ms step_avg:92.12ms
step:370/1670 train_time:34086ms step_avg:92.12ms
step:371/1670 train_time:34177ms step_avg:92.12ms
step:372/1670 train_time:34268ms step_avg:92.12ms
step:373/1670 train_time:34358ms step_avg:92.11ms
step:374/1670 train_time:34449ms step_avg:92.11ms
step:375/1670 train_time:34540ms step_avg:92.11ms
step:375/1670 val_loss:3.8131 train_time:34630ms step_avg:92.35ms
step:376/1670 train_time:34650ms step_avg:92.15ms
step:377/1670 train_time:34723ms step_avg:92.10ms
step:378/1670 train_time:34815ms step_avg:92.10ms
step:379/1670 train_time:34906ms step_avg:92.10ms
step:380/1670 train_time:34997ms step_avg:92.10ms
step:381/1670 train_time:35089ms step_avg:92.10ms
step:382/1670 train_time:35179ms step_avg:92.09ms
step:383/1670 train_time:35269ms step_avg:92.09ms
step:384/1670 train_time:35360ms step_avg:92.08ms
step:385/1670 train_time:35452ms step_avg:92.08ms
step:386/1670 train_time:35543ms step_avg:92.08ms
step:387/1670 train_time:35635ms step_avg:92.08ms
step:388/1670 train_time:35728ms step_avg:92.08ms
step:389/1670 train_time:35819ms step_avg:92.08ms
step:390/1670 train_time:35910ms step_avg:92.08ms
step:391/1670 train_time:36000ms step_avg:92.07ms
step:392/1670 train_time:36092ms step_avg:92.07ms
step:393/1670 train_time:36182ms step_avg:92.07ms
step:394/1670 train_time:36273ms step_avg:92.06ms
step:395/1670 train_time:36364ms step_avg:92.06ms
step:396/1670 train_time:36455ms step_avg:92.06ms
step:397/1670 train_time:36547ms step_avg:92.06ms
step:398/1670 train_time:36640ms step_avg:92.06ms
step:399/1670 train_time:36732ms step_avg:92.06ms
step:400/1670 train_time:36823ms step_avg:92.06ms
step:401/1670 train_time:36914ms step_avg:92.06ms
step:402/1670 train_time:37005ms step_avg:92.05ms
step:403/1670 train_time:37097ms step_avg:92.05ms
step:404/1670 train_time:37188ms step_avg:92.05ms
step:405/1670 train_time:37279ms step_avg:92.05ms
step:406/1670 train_time:37371ms step_avg:92.05ms
step:407/1670 train_time:37461ms step_avg:92.04ms
step:408/1670 train_time:37553ms step_avg:92.04ms
step:409/1670 train_time:37644ms step_avg:92.04ms
step:410/1670 train_time:37736ms step_avg:92.04ms
step:411/1670 train_time:37827ms step_avg:92.04ms
step:412/1670 train_time:37918ms step_avg:92.03ms
step:413/1670 train_time:38008ms step_avg:92.03ms
step:414/1670 train_time:38100ms step_avg:92.03ms
step:415/1670 train_time:38192ms step_avg:92.03ms
step:416/1670 train_time:38282ms step_avg:92.02ms
step:417/1670 train_time:38375ms step_avg:92.03ms
step:418/1670 train_time:38466ms step_avg:92.02ms
step:419/1670 train_time:38558ms step_avg:92.02ms
step:420/1670 train_time:38650ms step_avg:92.02ms
step:421/1670 train_time:38741ms step_avg:92.02ms
step:422/1670 train_time:38832ms step_avg:92.02ms
step:423/1670 train_time:38922ms step_avg:92.01ms
step:424/1670 train_time:39013ms step_avg:92.01ms
step:425/1670 train_time:39270ms step_avg:92.40ms
step:426/1670 train_time:39338ms step_avg:92.34ms
step:427/1670 train_time:39428ms step_avg:92.34ms
step:428/1670 train_time:39518ms step_avg:92.33ms
step:429/1670 train_time:39608ms step_avg:92.33ms
step:430/1670 train_time:39698ms step_avg:92.32ms
step:431/1670 train_time:39788ms step_avg:92.32ms
step:432/1670 train_time:39877ms step_avg:92.31ms
step:433/1670 train_time:39968ms step_avg:92.31ms
step:434/1670 train_time:40059ms step_avg:92.30ms
step:435/1670 train_time:40153ms step_avg:92.31ms
step:436/1670 train_time:40248ms step_avg:92.31ms
step:437/1670 train_time:40340ms step_avg:92.31ms
step:438/1670 train_time:40431ms step_avg:92.31ms
step:439/1670 train_time:40522ms step_avg:92.31ms
step:440/1670 train_time:40613ms step_avg:92.30ms
step:441/1670 train_time:40704ms step_avg:92.30ms
step:442/1670 train_time:40795ms step_avg:92.30ms
step:443/1670 train_time:40885ms step_avg:92.29ms
step:444/1670 train_time:40976ms step_avg:92.29ms
step:445/1670 train_time:41067ms step_avg:92.28ms
step:446/1670 train_time:41159ms step_avg:92.29ms
step:447/1670 train_time:41253ms step_avg:92.29ms
step:448/1670 train_time:41344ms step_avg:92.29ms
step:449/1670 train_time:41436ms step_avg:92.29ms
step:450/1670 train_time:41528ms step_avg:92.28ms
step:451/1670 train_time:41618ms step_avg:92.28ms
step:452/1670 train_time:41709ms step_avg:92.28ms
step:453/1670 train_time:41799ms step_avg:92.27ms
step:454/1670 train_time:41889ms step_avg:92.27ms
step:455/1670 train_time:41980ms step_avg:92.26ms
step:456/1670 train_time:42072ms step_avg:92.26ms
step:457/1670 train_time:42164ms step_avg:92.26ms
step:458/1670 train_time:42258ms step_avg:92.27ms
step:459/1670 train_time:42350ms step_avg:92.27ms
step:460/1670 train_time:42441ms step_avg:92.26ms
step:461/1670 train_time:42533ms step_avg:92.26ms
step:462/1670 train_time:42624ms step_avg:92.26ms
step:463/1670 train_time:42715ms step_avg:92.26ms
step:464/1670 train_time:42806ms step_avg:92.25ms
step:465/1670 train_time:42896ms step_avg:92.25ms
step:466/1670 train_time:42987ms step_avg:92.25ms
step:467/1670 train_time:43080ms step_avg:92.25ms
step:468/1670 train_time:43172ms step_avg:92.25ms
step:469/1670 train_time:43262ms step_avg:92.24ms
step:470/1670 train_time:43355ms step_avg:92.24ms
step:471/1670 train_time:43446ms step_avg:92.24ms
step:472/1670 train_time:43538ms step_avg:92.24ms
step:473/1670 train_time:43630ms step_avg:92.24ms
step:474/1670 train_time:43720ms step_avg:92.24ms
step:475/1670 train_time:43811ms step_avg:92.23ms
step:476/1670 train_time:43902ms step_avg:92.23ms
step:477/1670 train_time:43994ms step_avg:92.23ms
step:478/1670 train_time:44084ms step_avg:92.23ms
step:479/1670 train_time:44176ms step_avg:92.23ms
step:480/1670 train_time:44267ms step_avg:92.22ms
step:481/1670 train_time:44359ms step_avg:92.22ms
step:482/1670 train_time:44451ms step_avg:92.22ms
step:483/1670 train_time:44542ms step_avg:92.22ms
step:484/1670 train_time:44634ms step_avg:92.22ms
step:485/1670 train_time:44725ms step_avg:92.22ms
step:486/1670 train_time:44817ms step_avg:92.22ms
step:487/1670 train_time:44908ms step_avg:92.21ms
step:488/1670 train_time:45000ms step_avg:92.21ms
step:489/1670 train_time:45091ms step_avg:92.21ms
step:490/1670 train_time:45182ms step_avg:92.21ms
step:491/1670 train_time:45274ms step_avg:92.21ms
step:492/1670 train_time:45365ms step_avg:92.20ms
step:493/1670 train_time:45457ms step_avg:92.20ms
step:494/1670 train_time:45548ms step_avg:92.20ms
step:495/1670 train_time:45639ms step_avg:92.20ms
step:496/1670 train_time:45731ms step_avg:92.20ms
step:497/1670 train_time:45821ms step_avg:92.20ms
step:498/1670 train_time:45912ms step_avg:92.19ms
step:499/1670 train_time:46003ms step_avg:92.19ms
step:500/1670 train_time:46096ms step_avg:92.19ms
step:500/1670 val_loss:3.7135 train_time:46186ms step_avg:92.37ms
step:501/1670 train_time:46206ms step_avg:92.23ms
step:502/1670 train_time:46279ms step_avg:92.19ms
step:503/1670 train_time:46371ms step_avg:92.19ms
step:504/1670 train_time:46462ms step_avg:92.19ms
step:505/1670 train_time:46552ms step_avg:92.18ms
step:506/1670 train_time:46642ms step_avg:92.18ms
step:507/1670 train_time:46732ms step_avg:92.17ms
step:508/1670 train_time:46824ms step_avg:92.17ms
step:509/1670 train_time:46914ms step_avg:92.17ms
step:510/1670 train_time:47006ms step_avg:92.17ms
step:511/1670 train_time:47099ms step_avg:92.17ms
step:512/1670 train_time:47192ms step_avg:92.17ms
step:513/1670 train_time:47284ms step_avg:92.17ms
step:514/1670 train_time:47376ms step_avg:92.17ms
step:515/1670 train_time:47468ms step_avg:92.17ms
step:516/1670 train_time:47559ms step_avg:92.17ms
step:517/1670 train_time:47649ms step_avg:92.16ms
step:518/1670 train_time:47740ms step_avg:92.16ms
step:519/1670 train_time:47831ms step_avg:92.16ms
step:520/1670 train_time:47922ms step_avg:92.16ms
step:521/1670 train_time:48012ms step_avg:92.15ms
step:522/1670 train_time:48104ms step_avg:92.15ms
step:523/1670 train_time:48196ms step_avg:92.15ms
step:524/1670 train_time:48287ms step_avg:92.15ms
step:525/1670 train_time:48378ms step_avg:92.15ms
step:526/1670 train_time:48470ms step_avg:92.15ms
step:527/1670 train_time:48562ms step_avg:92.15ms
step:528/1670 train_time:48652ms step_avg:92.14ms
step:529/1670 train_time:48743ms step_avg:92.14ms
step:530/1670 train_time:48834ms step_avg:92.14ms
step:531/1670 train_time:48926ms step_avg:92.14ms
step:532/1670 train_time:49017ms step_avg:92.14ms
step:533/1670 train_time:49108ms step_avg:92.14ms
step:534/1670 train_time:49200ms step_avg:92.13ms
step:535/1670 train_time:49291ms step_avg:92.13ms
step:536/1670 train_time:49381ms step_avg:92.13ms
step:537/1670 train_time:49472ms step_avg:92.13ms
step:538/1670 train_time:49564ms step_avg:92.13ms
step:539/1670 train_time:49655ms step_avg:92.12ms
step:540/1670 train_time:49746ms step_avg:92.12ms
step:541/1670 train_time:49838ms step_avg:92.12ms
step:542/1670 train_time:49930ms step_avg:92.12ms
step:543/1670 train_time:50021ms step_avg:92.12ms
step:544/1670 train_time:50112ms step_avg:92.12ms
step:545/1670 train_time:50204ms step_avg:92.12ms
step:546/1670 train_time:50295ms step_avg:92.11ms
step:547/1670 train_time:50386ms step_avg:92.11ms
step:548/1670 train_time:50478ms step_avg:92.11ms
step:549/1670 train_time:50570ms step_avg:92.11ms
step:550/1670 train_time:50661ms step_avg:92.11ms
step:551/1670 train_time:50753ms step_avg:92.11ms
step:552/1670 train_time:50845ms step_avg:92.11ms
step:553/1670 train_time:50936ms step_avg:92.11ms
step:554/1670 train_time:51029ms step_avg:92.11ms
step:555/1670 train_time:51121ms step_avg:92.11ms
step:556/1670 train_time:51212ms step_avg:92.11ms
step:557/1670 train_time:51303ms step_avg:92.11ms
step:558/1670 train_time:51577ms step_avg:92.43ms
step:559/1670 train_time:51658ms step_avg:92.41ms
step:560/1670 train_time:51748ms step_avg:92.41ms
step:561/1670 train_time:51840ms step_avg:92.41ms
step:562/1670 train_time:51932ms step_avg:92.40ms
step:563/1670 train_time:52023ms step_avg:92.40ms
step:564/1670 train_time:52114ms step_avg:92.40ms
step:565/1670 train_time:52205ms step_avg:92.40ms
step:566/1670 train_time:52296ms step_avg:92.40ms
step:567/1670 train_time:52389ms step_avg:92.40ms
step:568/1670 train_time:52483ms step_avg:92.40ms
step:569/1670 train_time:52580ms step_avg:92.41ms
step:570/1670 train_time:52673ms step_avg:92.41ms
step:571/1670 train_time:52767ms step_avg:92.41ms
step:572/1670 train_time:52859ms step_avg:92.41ms
step:573/1670 train_time:52951ms step_avg:92.41ms
step:574/1670 train_time:53042ms step_avg:92.41ms
step:575/1670 train_time:53134ms step_avg:92.41ms
step:576/1670 train_time:53226ms step_avg:92.41ms
step:577/1670 train_time:53317ms step_avg:92.40ms
step:578/1670 train_time:53410ms step_avg:92.40ms
step:579/1670 train_time:53506ms step_avg:92.41ms
step:580/1670 train_time:53600ms step_avg:92.41ms
step:581/1670 train_time:53693ms step_avg:92.42ms
step:582/1670 train_time:53789ms step_avg:92.42ms
step:583/1670 train_time:53881ms step_avg:92.42ms
step:584/1670 train_time:53973ms step_avg:92.42ms
step:585/1670 train_time:54066ms step_avg:92.42ms
step:586/1670 train_time:54157ms step_avg:92.42ms
step:587/1670 train_time:54249ms step_avg:92.42ms
step:588/1670 train_time:54341ms step_avg:92.42ms
step:589/1670 train_time:54433ms step_avg:92.42ms
step:590/1670 train_time:54527ms step_avg:92.42ms
step:591/1670 train_time:54620ms step_avg:92.42ms
step:592/1670 train_time:54713ms step_avg:92.42ms
step:593/1670 train_time:54806ms step_avg:92.42ms
step:594/1670 train_time:54899ms step_avg:92.42ms
step:595/1670 train_time:54992ms step_avg:92.42ms
step:596/1670 train_time:55085ms step_avg:92.42ms
step:597/1670 train_time:55176ms step_avg:92.42ms
step:598/1670 train_time:55268ms step_avg:92.42ms
step:599/1670 train_time:55360ms step_avg:92.42ms
step:600/1670 train_time:55454ms step_avg:92.42ms
step:601/1670 train_time:55546ms step_avg:92.42ms
step:602/1670 train_time:55639ms step_avg:92.42ms
step:603/1670 train_time:55733ms step_avg:92.43ms
step:604/1670 train_time:55826ms step_avg:92.43ms
step:605/1670 train_time:55918ms step_avg:92.43ms
step:606/1670 train_time:56009ms step_avg:92.42ms
step:607/1670 train_time:56101ms step_avg:92.42ms
step:608/1670 train_time:56194ms step_avg:92.42ms
step:609/1670 train_time:56285ms step_avg:92.42ms
step:610/1670 train_time:56378ms step_avg:92.42ms
step:611/1670 train_time:56471ms step_avg:92.42ms
step:612/1670 train_time:56564ms step_avg:92.42ms
step:613/1670 train_time:56656ms step_avg:92.42ms
step:614/1670 train_time:56750ms step_avg:92.43ms
step:615/1670 train_time:56842ms step_avg:92.43ms
step:616/1670 train_time:56935ms step_avg:92.43ms
step:617/1670 train_time:57027ms step_avg:92.43ms
step:618/1670 train_time:57119ms step_avg:92.43ms
step:619/1670 train_time:57212ms step_avg:92.43ms
step:620/1670 train_time:57305ms step_avg:92.43ms
step:621/1670 train_time:57397ms step_avg:92.43ms
step:622/1670 train_time:57490ms step_avg:92.43ms
step:623/1670 train_time:57582ms step_avg:92.43ms
step:624/1670 train_time:57675ms step_avg:92.43ms
step:625/1670 train_time:57769ms step_avg:92.43ms
step:625/1670 val_loss:3.6141 train_time:57861ms step_avg:92.58ms
step:626/1670 train_time:57881ms step_avg:92.46ms
step:627/1670 train_time:57959ms step_avg:92.44ms
step:628/1670 train_time:58064ms step_avg:92.46ms
step:629/1670 train_time:58159ms step_avg:92.46ms
step:630/1670 train_time:58251ms step_avg:92.46ms
step:631/1670 train_time:58341ms step_avg:92.46ms
step:632/1670 train_time:58432ms step_avg:92.46ms
step:633/1670 train_time:58524ms step_avg:92.45ms
step:634/1670 train_time:58615ms step_avg:92.45ms
step:635/1670 train_time:58707ms step_avg:92.45ms
step:636/1670 train_time:58798ms step_avg:92.45ms
step:637/1670 train_time:58889ms step_avg:92.45ms
step:638/1670 train_time:58984ms step_avg:92.45ms
step:639/1670 train_time:59222ms step_avg:92.68ms
step:640/1670 train_time:59292ms step_avg:92.64ms
step:641/1670 train_time:59383ms step_avg:92.64ms
step:642/1670 train_time:59474ms step_avg:92.64ms
step:643/1670 train_time:59565ms step_avg:92.64ms
step:644/1670 train_time:59656ms step_avg:92.63ms
step:645/1670 train_time:59748ms step_avg:92.63ms
step:646/1670 train_time:59838ms step_avg:92.63ms
step:647/1670 train_time:59929ms step_avg:92.63ms
step:648/1670 train_time:60021ms step_avg:92.62ms
step:649/1670 train_time:60117ms step_avg:92.63ms
step:650/1670 train_time:60215ms step_avg:92.64ms
step:651/1670 train_time:60308ms step_avg:92.64ms
step:652/1670 train_time:60400ms step_avg:92.64ms
step:653/1670 train_time:60493ms step_avg:92.64ms
step:654/1670 train_time:60585ms step_avg:92.64ms
step:655/1670 train_time:60677ms step_avg:92.64ms
step:656/1670 train_time:60770ms step_avg:92.64ms
step:657/1670 train_time:60861ms step_avg:92.63ms
step:658/1670 train_time:60952ms step_avg:92.63ms
step:659/1670 train_time:61044ms step_avg:92.63ms
step:660/1670 train_time:61138ms step_avg:92.63ms
step:661/1670 train_time:61232ms step_avg:92.64ms
step:662/1670 train_time:61325ms step_avg:92.64ms
step:663/1670 train_time:61418ms step_avg:92.64ms
step:664/1670 train_time:61511ms step_avg:92.64ms
step:665/1670 train_time:61603ms step_avg:92.64ms
step:666/1670 train_time:61695ms step_avg:92.64ms
step:667/1670 train_time:61788ms step_avg:92.64ms
step:668/1670 train_time:61879ms step_avg:92.63ms
step:669/1670 train_time:61971ms step_avg:92.63ms
step:670/1670 train_time:62064ms step_avg:92.63ms
step:671/1670 train_time:62158ms step_avg:92.64ms
step:672/1670 train_time:62253ms step_avg:92.64ms
step:673/1670 train_time:62346ms step_avg:92.64ms
step:674/1670 train_time:62439ms step_avg:92.64ms
step:675/1670 train_time:62531ms step_avg:92.64ms
step:676/1670 train_time:62624ms step_avg:92.64ms
step:677/1670 train_time:62717ms step_avg:92.64ms
step:678/1670 train_time:62809ms step_avg:92.64ms
step:679/1670 train_time:62901ms step_avg:92.64ms
step:680/1670 train_time:62993ms step_avg:92.64ms
step:681/1670 train_time:63085ms step_avg:92.64ms
step:682/1670 train_time:63179ms step_avg:92.64ms
step:683/1670 train_time:63273ms step_avg:92.64ms
step:684/1670 train_time:63365ms step_avg:92.64ms
step:685/1670 train_time:63458ms step_avg:92.64ms
step:686/1670 train_time:63550ms step_avg:92.64ms
step:687/1670 train_time:63642ms step_avg:92.64ms
step:688/1670 train_time:63735ms step_avg:92.64ms
step:689/1670 train_time:63826ms step_avg:92.64ms
step:690/1670 train_time:63919ms step_avg:92.64ms
step:691/1670 train_time:64011ms step_avg:92.64ms
step:692/1670 train_time:64104ms step_avg:92.64ms
step:693/1670 train_time:64198ms step_avg:92.64ms
step:694/1670 train_time:64291ms step_avg:92.64ms
step:695/1670 train_time:64383ms step_avg:92.64ms
step:696/1670 train_time:64477ms step_avg:92.64ms
step:697/1670 train_time:64569ms step_avg:92.64ms
step:698/1670 train_time:64661ms step_avg:92.64ms
step:699/1670 train_time:64753ms step_avg:92.64ms
step:700/1670 train_time:64846ms step_avg:92.64ms
step:701/1670 train_time:64939ms step_avg:92.64ms
step:702/1670 train_time:65032ms step_avg:92.64ms
step:703/1670 train_time:65125ms step_avg:92.64ms
step:704/1670 train_time:65218ms step_avg:92.64ms
step:705/1670 train_time:65310ms step_avg:92.64ms
step:706/1670 train_time:65402ms step_avg:92.64ms
step:707/1670 train_time:65496ms step_avg:92.64ms
step:708/1670 train_time:65588ms step_avg:92.64ms
step:709/1670 train_time:65681ms step_avg:92.64ms
step:710/1670 train_time:65774ms step_avg:92.64ms
step:711/1670 train_time:65866ms step_avg:92.64ms
step:712/1670 train_time:65959ms step_avg:92.64ms
step:713/1670 train_time:66052ms step_avg:92.64ms
step:714/1670 train_time:66145ms step_avg:92.64ms
step:715/1670 train_time:66238ms step_avg:92.64ms
step:716/1670 train_time:66330ms step_avg:92.64ms
step:717/1670 train_time:66423ms step_avg:92.64ms
step:718/1670 train_time:66515ms step_avg:92.64ms
step:719/1670 train_time:66607ms step_avg:92.64ms
step:720/1670 train_time:66701ms step_avg:92.64ms
step:721/1670 train_time:66794ms step_avg:92.64ms
step:722/1670 train_time:66886ms step_avg:92.64ms
step:723/1670 train_time:66979ms step_avg:92.64ms
step:724/1670 train_time:67072ms step_avg:92.64ms
step:725/1670 train_time:67164ms step_avg:92.64ms
step:726/1670 train_time:67257ms step_avg:92.64ms
step:727/1670 train_time:67349ms step_avg:92.64ms
step:728/1670 train_time:67441ms step_avg:92.64ms
step:729/1670 train_time:67533ms step_avg:92.64ms
step:730/1670 train_time:67625ms step_avg:92.64ms
step:731/1670 train_time:67718ms step_avg:92.64ms
step:732/1670 train_time:67811ms step_avg:92.64ms
step:733/1670 train_time:67902ms step_avg:92.64ms
step:734/1670 train_time:67996ms step_avg:92.64ms
step:735/1670 train_time:68090ms step_avg:92.64ms
step:736/1670 train_time:68182ms step_avg:92.64ms
step:737/1670 train_time:68275ms step_avg:92.64ms
step:738/1670 train_time:68368ms step_avg:92.64ms
step:739/1670 train_time:68461ms step_avg:92.64ms
step:740/1670 train_time:68552ms step_avg:92.64ms
step:741/1670 train_time:68645ms step_avg:92.64ms
step:742/1670 train_time:68738ms step_avg:92.64ms
step:743/1670 train_time:68831ms step_avg:92.64ms
step:744/1670 train_time:68924ms step_avg:92.64ms
step:745/1670 train_time:69017ms step_avg:92.64ms
step:746/1670 train_time:69109ms step_avg:92.64ms
step:747/1670 train_time:69201ms step_avg:92.64ms
step:748/1670 train_time:69294ms step_avg:92.64ms
step:749/1670 train_time:69386ms step_avg:92.64ms
step:750/1670 train_time:69479ms step_avg:92.64ms
step:750/1670 val_loss:3.5619 train_time:69571ms step_avg:92.76ms
step:751/1670 train_time:69591ms step_avg:92.66ms
step:752/1670 train_time:69665ms step_avg:92.64ms
step:753/1670 train_time:69757ms step_avg:92.64ms
step:754/1670 train_time:69849ms step_avg:92.64ms
step:755/1670 train_time:69940ms step_avg:92.64ms
step:756/1670 train_time:70033ms step_avg:92.64ms
step:757/1670 train_time:70126ms step_avg:92.64ms
step:758/1670 train_time:70218ms step_avg:92.64ms
step:759/1670 train_time:70310ms step_avg:92.64ms
step:760/1670 train_time:70403ms step_avg:92.64ms
step:761/1670 train_time:70497ms step_avg:92.64ms
step:762/1670 train_time:70590ms step_avg:92.64ms
step:763/1670 train_time:70683ms step_avg:92.64ms
step:764/1670 train_time:70776ms step_avg:92.64ms
step:765/1670 train_time:70867ms step_avg:92.64ms
step:766/1670 train_time:70959ms step_avg:92.64ms
step:767/1670 train_time:71052ms step_avg:92.64ms
step:768/1670 train_time:71144ms step_avg:92.64ms
step:769/1670 train_time:71237ms step_avg:92.64ms
step:770/1670 train_time:71330ms step_avg:92.64ms
step:771/1670 train_time:71423ms step_avg:92.64ms
step:772/1670 train_time:71516ms step_avg:92.64ms
step:773/1670 train_time:71610ms step_avg:92.64ms
step:774/1670 train_time:71703ms step_avg:92.64ms
step:775/1670 train_time:71795ms step_avg:92.64ms
step:776/1670 train_time:71887ms step_avg:92.64ms
step:777/1670 train_time:71979ms step_avg:92.64ms
step:778/1670 train_time:72071ms step_avg:92.64ms
step:779/1670 train_time:72163ms step_avg:92.64ms
step:780/1670 train_time:72256ms step_avg:92.64ms
step:781/1670 train_time:72349ms step_avg:92.64ms
step:782/1670 train_time:72441ms step_avg:92.64ms
step:783/1670 train_time:72536ms step_avg:92.64ms
step:784/1670 train_time:72629ms step_avg:92.64ms
step:785/1670 train_time:72721ms step_avg:92.64ms
step:786/1670 train_time:72814ms step_avg:92.64ms
step:787/1670 train_time:72906ms step_avg:92.64ms
step:788/1670 train_time:72998ms step_avg:92.64ms
step:789/1670 train_time:73091ms step_avg:92.64ms
step:790/1670 train_time:73183ms step_avg:92.64ms
step:791/1670 train_time:73276ms step_avg:92.64ms
step:792/1670 train_time:73368ms step_avg:92.64ms
step:793/1670 train_time:73460ms step_avg:92.64ms
step:794/1670 train_time:73554ms step_avg:92.64ms
step:795/1670 train_time:73648ms step_avg:92.64ms
step:796/1670 train_time:73740ms step_avg:92.64ms
step:797/1670 train_time:73834ms step_avg:92.64ms
step:798/1670 train_time:73926ms step_avg:92.64ms
step:799/1670 train_time:74018ms step_avg:92.64ms
step:800/1670 train_time:74111ms step_avg:92.64ms
step:801/1670 train_time:74203ms step_avg:92.64ms
step:802/1670 train_time:74295ms step_avg:92.64ms
step:803/1670 train_time:74388ms step_avg:92.64ms
step:804/1670 train_time:74481ms step_avg:92.64ms
step:805/1670 train_time:74573ms step_avg:92.64ms
step:806/1670 train_time:74666ms step_avg:92.64ms
step:807/1670 train_time:74759ms step_avg:92.64ms
step:808/1670 train_time:74853ms step_avg:92.64ms
step:809/1670 train_time:74945ms step_avg:92.64ms
step:810/1670 train_time:75037ms step_avg:92.64ms
step:811/1670 train_time:75130ms step_avg:92.64ms
step:812/1670 train_time:75223ms step_avg:92.64ms
step:813/1670 train_time:75317ms step_avg:92.64ms
step:814/1670 train_time:75409ms step_avg:92.64ms
step:815/1670 train_time:75502ms step_avg:92.64ms
step:816/1670 train_time:75595ms step_avg:92.64ms
step:817/1670 train_time:75687ms step_avg:92.64ms
step:818/1670 train_time:75781ms step_avg:92.64ms
step:819/1670 train_time:75874ms step_avg:92.64ms
step:820/1670 train_time:75965ms step_avg:92.64ms
step:821/1670 train_time:76059ms step_avg:92.64ms
step:822/1670 train_time:76153ms step_avg:92.64ms
step:823/1670 train_time:76246ms step_avg:92.64ms
step:824/1670 train_time:76339ms step_avg:92.64ms
step:825/1670 train_time:76430ms step_avg:92.64ms
step:826/1670 train_time:76523ms step_avg:92.64ms
step:827/1670 train_time:76616ms step_avg:92.64ms
step:828/1670 train_time:76708ms step_avg:92.64ms
step:829/1670 train_time:76800ms step_avg:92.64ms
step:830/1670 train_time:76893ms step_avg:92.64ms
step:831/1670 train_time:76986ms step_avg:92.64ms
step:832/1670 train_time:77078ms step_avg:92.64ms
step:833/1670 train_time:77171ms step_avg:92.64ms
step:834/1670 train_time:77262ms step_avg:92.64ms
step:835/1670 train_time:77356ms step_avg:92.64ms
step:836/1670 train_time:77449ms step_avg:92.64ms
step:837/1670 train_time:77541ms step_avg:92.64ms
step:838/1670 train_time:77634ms step_avg:92.64ms
step:839/1670 train_time:77727ms step_avg:92.64ms
step:840/1670 train_time:77818ms step_avg:92.64ms
step:841/1670 train_time:77911ms step_avg:92.64ms
step:842/1670 train_time:78004ms step_avg:92.64ms
step:843/1670 train_time:78096ms step_avg:92.64ms
step:844/1670 train_time:78189ms step_avg:92.64ms
step:845/1670 train_time:78281ms step_avg:92.64ms
step:846/1670 train_time:78374ms step_avg:92.64ms
step:847/1670 train_time:78466ms step_avg:92.64ms
step:848/1670 train_time:78558ms step_avg:92.64ms
step:849/1670 train_time:78651ms step_avg:92.64ms
step:850/1670 train_time:78743ms step_avg:92.64ms
step:851/1670 train_time:78995ms step_avg:92.83ms
step:852/1670 train_time:79066ms step_avg:92.80ms
step:853/1670 train_time:79157ms step_avg:92.80ms
step:854/1670 train_time:79248ms step_avg:92.80ms
step:855/1670 train_time:79338ms step_avg:92.79ms
step:856/1670 train_time:79430ms step_avg:92.79ms
step:857/1670 train_time:79521ms step_avg:92.79ms
step:858/1670 train_time:79613ms step_avg:92.79ms
step:859/1670 train_time:79705ms step_avg:92.79ms
step:860/1670 train_time:79796ms step_avg:92.79ms
step:861/1670 train_time:79892ms step_avg:92.79ms
step:862/1670 train_time:79988ms step_avg:92.79ms
step:863/1670 train_time:80082ms step_avg:92.79ms
step:864/1670 train_time:80175ms step_avg:92.79ms
step:865/1670 train_time:80267ms step_avg:92.79ms
step:866/1670 train_time:80358ms step_avg:92.79ms
step:867/1670 train_time:80450ms step_avg:92.79ms
step:868/1670 train_time:80541ms step_avg:92.79ms
step:869/1670 train_time:80633ms step_avg:92.79ms
step:870/1670 train_time:80724ms step_avg:92.79ms
step:871/1670 train_time:80817ms step_avg:92.79ms
step:872/1670 train_time:80913ms step_avg:92.79ms
step:873/1670 train_time:81007ms step_avg:92.79ms
step:874/1670 train_time:81100ms step_avg:92.79ms
step:875/1670 train_time:81193ms step_avg:92.79ms
step:875/1670 val_loss:3.5187 train_time:81286ms step_avg:92.90ms
step:876/1670 train_time:81306ms step_avg:92.81ms
step:877/1670 train_time:81383ms step_avg:92.80ms
step:878/1670 train_time:81479ms step_avg:92.80ms
step:879/1670 train_time:81571ms step_avg:92.80ms
step:880/1670 train_time:81661ms step_avg:92.80ms
step:881/1670 train_time:81753ms step_avg:92.80ms
step:882/1670 train_time:81844ms step_avg:92.79ms
step:883/1670 train_time:81936ms step_avg:92.79ms
step:884/1670 train_time:82028ms step_avg:92.79ms
step:885/1670 train_time:82121ms step_avg:92.79ms
step:886/1670 train_time:82214ms step_avg:92.79ms
step:887/1670 train_time:82308ms step_avg:92.79ms
step:888/1670 train_time:82402ms step_avg:92.80ms
step:889/1670 train_time:82496ms step_avg:92.80ms
step:890/1670 train_time:82588ms step_avg:92.79ms
step:891/1670 train_time:82680ms step_avg:92.79ms
step:892/1670 train_time:82772ms step_avg:92.79ms
step:893/1670 train_time:82864ms step_avg:92.79ms
step:894/1670 train_time:82956ms step_avg:92.79ms
step:895/1670 train_time:83048ms step_avg:92.79ms
step:896/1670 train_time:83141ms step_avg:92.79ms
step:897/1670 train_time:83234ms step_avg:92.79ms
step:898/1670 train_time:83327ms step_avg:92.79ms
step:899/1670 train_time:83420ms step_avg:92.79ms
step:900/1670 train_time:83514ms step_avg:92.79ms
step:901/1670 train_time:83606ms step_avg:92.79ms
step:902/1670 train_time:83698ms step_avg:92.79ms
step:903/1670 train_time:83790ms step_avg:92.79ms
step:904/1670 train_time:83882ms step_avg:92.79ms
step:905/1670 train_time:83975ms step_avg:92.79ms
step:906/1670 train_time:84067ms step_avg:92.79ms
step:907/1670 train_time:84159ms step_avg:92.79ms
step:908/1670 train_time:84252ms step_avg:92.79ms
step:909/1670 train_time:84345ms step_avg:92.79ms
step:910/1670 train_time:84439ms step_avg:92.79ms
step:911/1670 train_time:84531ms step_avg:92.79ms
step:912/1670 train_time:84623ms step_avg:92.79ms
step:913/1670 train_time:84716ms step_avg:92.79ms
step:914/1670 train_time:84808ms step_avg:92.79ms
step:915/1670 train_time:84900ms step_avg:92.79ms
step:916/1670 train_time:84992ms step_avg:92.79ms
step:917/1670 train_time:85085ms step_avg:92.79ms
step:918/1670 train_time:85177ms step_avg:92.79ms
step:919/1670 train_time:85270ms step_avg:92.79ms
step:920/1670 train_time:85363ms step_avg:92.79ms
step:921/1670 train_time:85457ms step_avg:92.79ms
step:922/1670 train_time:85550ms step_avg:92.79ms
step:923/1670 train_time:85643ms step_avg:92.79ms
step:924/1670 train_time:85735ms step_avg:92.79ms
step:925/1670 train_time:85827ms step_avg:92.79ms
step:926/1670 train_time:85920ms step_avg:92.79ms
step:927/1670 train_time:86012ms step_avg:92.79ms
step:928/1670 train_time:86104ms step_avg:92.79ms
step:929/1670 train_time:86197ms step_avg:92.79ms
step:930/1670 train_time:86291ms step_avg:92.79ms
step:931/1670 train_time:86384ms step_avg:92.79ms
step:932/1670 train_time:86477ms step_avg:92.79ms
step:933/1670 train_time:86568ms step_avg:92.78ms
step:934/1670 train_time:86662ms step_avg:92.79ms
step:935/1670 train_time:86754ms step_avg:92.79ms
step:936/1670 train_time:86846ms step_avg:92.78ms
step:937/1670 train_time:86940ms step_avg:92.78ms
step:938/1670 train_time:87032ms step_avg:92.78ms
step:939/1670 train_time:87123ms step_avg:92.78ms
step:940/1670 train_time:87217ms step_avg:92.78ms
step:941/1670 train_time:87310ms step_avg:92.78ms
step:942/1670 train_time:87403ms step_avg:92.78ms
step:943/1670 train_time:87495ms step_avg:92.78ms
step:944/1670 train_time:87587ms step_avg:92.78ms
step:945/1670 train_time:87681ms step_avg:92.78ms
step:946/1670 train_time:87773ms step_avg:92.78ms
step:947/1670 train_time:87866ms step_avg:92.78ms
step:948/1670 train_time:87958ms step_avg:92.78ms
step:949/1670 train_time:88050ms step_avg:92.78ms
step:950/1670 train_time:88143ms step_avg:92.78ms
step:951/1670 train_time:88236ms step_avg:92.78ms
step:952/1670 train_time:88327ms step_avg:92.78ms
step:953/1670 train_time:88421ms step_avg:92.78ms
step:954/1670 train_time:88513ms step_avg:92.78ms
step:955/1670 train_time:88605ms step_avg:92.78ms
step:956/1670 train_time:88698ms step_avg:92.78ms
step:957/1670 train_time:88791ms step_avg:92.78ms
step:958/1670 train_time:88883ms step_avg:92.78ms
step:959/1670 train_time:88976ms step_avg:92.78ms
step:960/1670 train_time:89068ms step_avg:92.78ms
step:961/1670 train_time:89160ms step_avg:92.78ms
step:962/1670 train_time:89253ms step_avg:92.78ms
step:963/1670 train_time:89345ms step_avg:92.78ms
step:964/1670 train_time:89438ms step_avg:92.78ms
step:965/1670 train_time:89530ms step_avg:92.78ms
step:966/1670 train_time:89623ms step_avg:92.78ms
step:967/1670 train_time:89717ms step_avg:92.78ms
step:968/1670 train_time:89809ms step_avg:92.78ms
step:969/1670 train_time:89903ms step_avg:92.78ms
step:970/1670 train_time:89996ms step_avg:92.78ms
step:971/1670 train_time:90088ms step_avg:92.78ms
step:972/1670 train_time:90181ms step_avg:92.78ms
step:973/1670 train_time:90273ms step_avg:92.78ms
step:974/1670 train_time:90365ms step_avg:92.78ms
step:975/1670 train_time:90457ms step_avg:92.78ms
step:976/1670 train_time:90549ms step_avg:92.78ms
step:977/1670 train_time:90642ms step_avg:92.78ms
step:978/1670 train_time:90734ms step_avg:92.78ms
step:979/1670 train_time:90826ms step_avg:92.77ms
step:980/1670 train_time:90920ms step_avg:92.78ms
step:981/1670 train_time:91012ms step_avg:92.77ms
step:982/1670 train_time:91104ms step_avg:92.77ms
step:983/1670 train_time:91197ms step_avg:92.77ms
step:984/1670 train_time:91288ms step_avg:92.77ms
step:985/1670 train_time:91382ms step_avg:92.77ms
step:986/1670 train_time:91474ms step_avg:92.77ms
step:987/1670 train_time:91566ms step_avg:92.77ms
step:988/1670 train_time:91659ms step_avg:92.77ms
step:989/1670 train_time:91751ms step_avg:92.77ms
step:990/1670 train_time:91844ms step_avg:92.77ms
step:991/1670 train_time:91938ms step_avg:92.77ms
step:992/1670 train_time:92030ms step_avg:92.77ms
step:993/1670 train_time:92123ms step_avg:92.77ms
step:994/1670 train_time:92216ms step_avg:92.77ms
step:995/1670 train_time:92308ms step_avg:92.77ms
step:996/1670 train_time:92400ms step_avg:92.77ms
step:997/1670 train_time:92493ms step_avg:92.77ms
step:998/1670 train_time:92585ms step_avg:92.77ms
step:999/1670 train_time:92677ms step_avg:92.77ms
step:1000/1670 train_time:92770ms step_avg:92.77ms
step:1000/1670 val_loss:3.4700 train_time:92862ms step_avg:92.86ms
step:1001/1670 train_time:92882ms step_avg:92.79ms
step:1002/1670 train_time:92957ms step_avg:92.77ms
step:1003/1670 train_time:93050ms step_avg:92.77ms
step:1004/1670 train_time:93141ms step_avg:92.77ms
step:1005/1670 train_time:93233ms step_avg:92.77ms
step:1006/1670 train_time:93324ms step_avg:92.77ms
step:1007/1670 train_time:93416ms step_avg:92.77ms
step:1008/1670 train_time:93509ms step_avg:92.77ms
step:1009/1670 train_time:93601ms step_avg:92.77ms
step:1010/1670 train_time:93693ms step_avg:92.77ms
step:1011/1670 train_time:93786ms step_avg:92.77ms
step:1012/1670 train_time:93880ms step_avg:92.77ms
step:1013/1670 train_time:93974ms step_avg:92.77ms
step:1014/1670 train_time:94068ms step_avg:92.77ms
step:1015/1670 train_time:94160ms step_avg:92.77ms
step:1016/1670 train_time:94252ms step_avg:92.77ms
step:1017/1670 train_time:94343ms step_avg:92.77ms
step:1018/1670 train_time:94437ms step_avg:92.77ms
step:1019/1670 train_time:94529ms step_avg:92.77ms
step:1020/1670 train_time:94621ms step_avg:92.77ms
step:1021/1670 train_time:94713ms step_avg:92.76ms
step:1022/1670 train_time:94806ms step_avg:92.77ms
step:1023/1670 train_time:94899ms step_avg:92.77ms
step:1024/1670 train_time:94993ms step_avg:92.77ms
step:1025/1670 train_time:95085ms step_avg:92.77ms
step:1026/1670 train_time:95178ms step_avg:92.77ms
step:1027/1670 train_time:95271ms step_avg:92.77ms
step:1028/1670 train_time:95363ms step_avg:92.77ms
step:1029/1670 train_time:95455ms step_avg:92.77ms
step:1030/1670 train_time:95548ms step_avg:92.77ms
step:1031/1670 train_time:95641ms step_avg:92.77ms
step:1032/1670 train_time:95734ms step_avg:92.77ms
step:1033/1670 train_time:95827ms step_avg:92.77ms
step:1034/1670 train_time:95919ms step_avg:92.77ms
step:1035/1670 train_time:96013ms step_avg:92.77ms
step:1036/1670 train_time:96105ms step_avg:92.77ms
step:1037/1670 train_time:96198ms step_avg:92.77ms
step:1038/1670 train_time:96290ms step_avg:92.76ms
step:1039/1670 train_time:96382ms step_avg:92.76ms
step:1040/1670 train_time:96475ms step_avg:92.76ms
step:1041/1670 train_time:96567ms step_avg:92.76ms
step:1042/1670 train_time:96659ms step_avg:92.76ms
step:1043/1670 train_time:96751ms step_avg:92.76ms
step:1044/1670 train_time:96844ms step_avg:92.76ms
step:1045/1670 train_time:96937ms step_avg:92.76ms
step:1046/1670 train_time:97030ms step_avg:92.76ms
step:1047/1670 train_time:97122ms step_avg:92.76ms
step:1048/1670 train_time:97214ms step_avg:92.76ms
step:1049/1670 train_time:97307ms step_avg:92.76ms
step:1050/1670 train_time:97399ms step_avg:92.76ms
step:1051/1670 train_time:97491ms step_avg:92.76ms
step:1052/1670 train_time:97582ms step_avg:92.76ms
step:1053/1670 train_time:97676ms step_avg:92.76ms
step:1054/1670 train_time:97769ms step_avg:92.76ms
step:1055/1670 train_time:97861ms step_avg:92.76ms
step:1056/1670 train_time:97954ms step_avg:92.76ms
step:1057/1670 train_time:98047ms step_avg:92.76ms
step:1058/1670 train_time:98139ms step_avg:92.76ms
step:1059/1670 train_time:98232ms step_avg:92.76ms
step:1060/1670 train_time:98325ms step_avg:92.76ms
step:1061/1670 train_time:98417ms step_avg:92.76ms
step:1062/1670 train_time:98653ms step_avg:92.89ms
step:1063/1670 train_time:98739ms step_avg:92.89ms
step:1064/1670 train_time:98830ms step_avg:92.88ms
step:1065/1670 train_time:98921ms step_avg:92.88ms
step:1066/1670 train_time:99012ms step_avg:92.88ms
step:1067/1670 train_time:99103ms step_avg:92.88ms
step:1068/1670 train_time:99195ms step_avg:92.88ms
step:1069/1670 train_time:99286ms step_avg:92.88ms
step:1070/1670 train_time:99377ms step_avg:92.88ms
step:1071/1670 train_time:99469ms step_avg:92.87ms
step:1072/1670 train_time:99568ms step_avg:92.88ms
step:1073/1670 train_time:99665ms step_avg:92.88ms
step:1074/1670 train_time:99758ms step_avg:92.88ms
step:1075/1670 train_time:99851ms step_avg:92.89ms
step:1076/1670 train_time:99943ms step_avg:92.88ms
step:1077/1670 train_time:100034ms step_avg:92.88ms
step:1078/1670 train_time:100126ms step_avg:92.88ms
step:1079/1670 train_time:100217ms step_avg:92.88ms
step:1080/1670 train_time:100309ms step_avg:92.88ms
step:1081/1670 train_time:100400ms step_avg:92.88ms
step:1082/1670 train_time:100494ms step_avg:92.88ms
step:1083/1670 train_time:100587ms step_avg:92.88ms
step:1084/1670 train_time:100681ms step_avg:92.88ms
step:1085/1670 train_time:100776ms step_avg:92.88ms
step:1086/1670 train_time:100870ms step_avg:92.88ms
step:1087/1670 train_time:100961ms step_avg:92.88ms
step:1088/1670 train_time:101054ms step_avg:92.88ms
step:1089/1670 train_time:101146ms step_avg:92.88ms
step:1090/1670 train_time:101239ms step_avg:92.88ms
step:1091/1670 train_time:101330ms step_avg:92.88ms
step:1092/1670 train_time:101423ms step_avg:92.88ms
step:1093/1670 train_time:101516ms step_avg:92.88ms
step:1094/1670 train_time:101611ms step_avg:92.88ms
step:1095/1670 train_time:101703ms step_avg:92.88ms
step:1096/1670 train_time:101797ms step_avg:92.88ms
step:1097/1670 train_time:101889ms step_avg:92.88ms
step:1098/1670 train_time:101981ms step_avg:92.88ms
step:1099/1670 train_time:102073ms step_avg:92.88ms
step:1100/1670 train_time:102167ms step_avg:92.88ms
step:1101/1670 train_time:102260ms step_avg:92.88ms
step:1102/1670 train_time:102352ms step_avg:92.88ms
step:1103/1670 train_time:102445ms step_avg:92.88ms
step:1104/1670 train_time:102538ms step_avg:92.88ms
step:1105/1670 train_time:102632ms step_avg:92.88ms
step:1106/1670 train_time:102724ms step_avg:92.88ms
step:1107/1670 train_time:102818ms step_avg:92.88ms
step:1108/1670 train_time:102912ms step_avg:92.88ms
step:1109/1670 train_time:103003ms step_avg:92.88ms
step:1110/1670 train_time:103096ms step_avg:92.88ms
step:1111/1670 train_time:103189ms step_avg:92.88ms
step:1112/1670 train_time:103283ms step_avg:92.88ms
step:1113/1670 train_time:103372ms step_avg:92.88ms
step:1114/1670 train_time:103465ms step_avg:92.88ms
step:1115/1670 train_time:103755ms step_avg:93.05ms
step:1116/1670 train_time:103827ms step_avg:93.04ms
step:1117/1670 train_time:103919ms step_avg:93.03ms
step:1118/1670 train_time:104011ms step_avg:93.03ms
step:1119/1670 train_time:104102ms step_avg:93.03ms
step:1120/1670 train_time:104194ms step_avg:93.03ms
step:1121/1670 train_time:104286ms step_avg:93.03ms
step:1122/1670 train_time:104378ms step_avg:93.03ms
step:1123/1670 train_time:104470ms step_avg:93.03ms
step:1124/1670 train_time:104562ms step_avg:93.03ms
step:1125/1670 train_time:104661ms step_avg:93.03ms
step:1125/1670 val_loss:3.4172 train_time:104761ms step_avg:93.12ms
step:1126/1670 train_time:104781ms step_avg:93.06ms
step:1127/1670 train_time:104857ms step_avg:93.04ms
step:1128/1670 train_time:104957ms step_avg:93.05ms
step:1129/1670 train_time:105053ms step_avg:93.05ms
step:1130/1670 train_time:105147ms step_avg:93.05ms
step:1131/1670 train_time:105238ms step_avg:93.05ms
step:1132/1670 train_time:105331ms step_avg:93.05ms
step:1133/1670 train_time:105423ms step_avg:93.05ms
step:1134/1670 train_time:105515ms step_avg:93.05ms
step:1135/1670 train_time:105607ms step_avg:93.05ms
step:1136/1670 train_time:105701ms step_avg:93.05ms
step:1137/1670 train_time:105795ms step_avg:93.05ms
step:1138/1670 train_time:105892ms step_avg:93.05ms
step:1139/1670 train_time:105989ms step_avg:93.05ms
step:1140/1670 train_time:106082ms step_avg:93.05ms
step:1141/1670 train_time:106175ms step_avg:93.05ms
step:1142/1670 train_time:106269ms step_avg:93.05ms
step:1143/1670 train_time:106361ms step_avg:93.05ms
step:1144/1670 train_time:106453ms step_avg:93.05ms
step:1145/1670 train_time:106545ms step_avg:93.05ms
step:1146/1670 train_time:106636ms step_avg:93.05ms
step:1147/1670 train_time:106729ms step_avg:93.05ms
step:1148/1670 train_time:106823ms step_avg:93.05ms
step:1149/1670 train_time:106918ms step_avg:93.05ms
step:1150/1670 train_time:107012ms step_avg:93.05ms
step:1151/1670 train_time:107105ms step_avg:93.05ms
step:1152/1670 train_time:107198ms step_avg:93.05ms
step:1153/1670 train_time:107293ms step_avg:93.06ms
step:1154/1670 train_time:107386ms step_avg:93.06ms
step:1155/1670 train_time:107478ms step_avg:93.05ms
step:1156/1670 train_time:107570ms step_avg:93.05ms
step:1157/1670 train_time:107663ms step_avg:93.05ms
step:1158/1670 train_time:107756ms step_avg:93.05ms
step:1159/1670 train_time:107851ms step_avg:93.05ms
step:1160/1670 train_time:107945ms step_avg:93.06ms
step:1161/1670 train_time:108041ms step_avg:93.06ms
step:1162/1670 train_time:108134ms step_avg:93.06ms
step:1163/1670 train_time:108229ms step_avg:93.06ms
step:1164/1670 train_time:108322ms step_avg:93.06ms
step:1165/1670 train_time:108415ms step_avg:93.06ms
step:1166/1670 train_time:108508ms step_avg:93.06ms
step:1167/1670 train_time:108600ms step_avg:93.06ms
step:1168/1670 train_time:108694ms step_avg:93.06ms
step:1169/1670 train_time:108788ms step_avg:93.06ms
step:1170/1670 train_time:108881ms step_avg:93.06ms
step:1171/1670 train_time:108974ms step_avg:93.06ms
step:1172/1670 train_time:109068ms step_avg:93.06ms
step:1173/1670 train_time:109161ms step_avg:93.06ms
step:1174/1670 train_time:109255ms step_avg:93.06ms
step:1175/1670 train_time:109348ms step_avg:93.06ms
step:1176/1670 train_time:109441ms step_avg:93.06ms
step:1177/1670 train_time:109535ms step_avg:93.06ms
step:1178/1670 train_time:109628ms step_avg:93.06ms
step:1179/1670 train_time:109720ms step_avg:93.06ms
step:1180/1670 train_time:109815ms step_avg:93.06ms
step:1181/1670 train_time:109909ms step_avg:93.06ms
step:1182/1670 train_time:110002ms step_avg:93.06ms
step:1183/1670 train_time:110096ms step_avg:93.06ms
step:1184/1670 train_time:110190ms step_avg:93.07ms
step:1185/1670 train_time:110283ms step_avg:93.07ms
step:1186/1670 train_time:110376ms step_avg:93.07ms
step:1187/1670 train_time:110468ms step_avg:93.07ms
step:1188/1670 train_time:110561ms step_avg:93.06ms
step:1189/1670 train_time:110654ms step_avg:93.06ms
step:1190/1670 train_time:110747ms step_avg:93.06ms
step:1191/1670 train_time:110839ms step_avg:93.06ms
step:1192/1670 train_time:110934ms step_avg:93.07ms
step:1193/1670 train_time:111027ms step_avg:93.07ms
step:1194/1670 train_time:111120ms step_avg:93.07ms
step:1195/1670 train_time:111213ms step_avg:93.07ms
step:1196/1670 train_time:111307ms step_avg:93.07ms
step:1197/1670 train_time:111400ms step_avg:93.07ms
step:1198/1670 train_time:111493ms step_avg:93.07ms
step:1199/1670 train_time:111586ms step_avg:93.07ms
step:1200/1670 train_time:111679ms step_avg:93.07ms
step:1201/1670 train_time:111771ms step_avg:93.07ms
step:1202/1670 train_time:111865ms step_avg:93.07ms
step:1203/1670 train_time:111958ms step_avg:93.07ms
step:1204/1670 train_time:112052ms step_avg:93.07ms
step:1205/1670 train_time:112145ms step_avg:93.07ms
step:1206/1670 train_time:112238ms step_avg:93.07ms
step:1207/1670 train_time:112334ms step_avg:93.07ms
step:1208/1670 train_time:112428ms step_avg:93.07ms
step:1209/1670 train_time:112521ms step_avg:93.07ms
step:1210/1670 train_time:112615ms step_avg:93.07ms
step:1211/1670 train_time:112708ms step_avg:93.07ms
step:1212/1670 train_time:112801ms step_avg:93.07ms
step:1213/1670 train_time:112894ms step_avg:93.07ms
step:1214/1670 train_time:112987ms step_avg:93.07ms
step:1215/1670 train_time:113080ms step_avg:93.07ms
step:1216/1670 train_time:113173ms step_avg:93.07ms
step:1217/1670 train_time:113268ms step_avg:93.07ms
step:1218/1670 train_time:113362ms step_avg:93.07ms
step:1219/1670 train_time:113456ms step_avg:93.07ms
step:1220/1670 train_time:113548ms step_avg:93.07ms
step:1221/1670 train_time:113642ms step_avg:93.07ms
step:1222/1670 train_time:113736ms step_avg:93.07ms
step:1223/1670 train_time:113829ms step_avg:93.07ms
step:1224/1670 train_time:113922ms step_avg:93.07ms
step:1225/1670 train_time:114015ms step_avg:93.07ms
step:1226/1670 train_time:114108ms step_avg:93.07ms
step:1227/1670 train_time:114202ms step_avg:93.07ms
step:1228/1670 train_time:114295ms step_avg:93.07ms
step:1229/1670 train_time:114388ms step_avg:93.07ms
step:1230/1670 train_time:114482ms step_avg:93.07ms
step:1231/1670 train_time:114575ms step_avg:93.07ms
step:1232/1670 train_time:114668ms step_avg:93.07ms
step:1233/1670 train_time:114762ms step_avg:93.08ms
step:1234/1670 train_time:114855ms step_avg:93.08ms
step:1235/1670 train_time:114948ms step_avg:93.08ms
step:1236/1670 train_time:115040ms step_avg:93.07ms
step:1237/1670 train_time:115134ms step_avg:93.08ms
step:1238/1670 train_time:115228ms step_avg:93.08ms
step:1239/1670 train_time:115321ms step_avg:93.08ms
step:1240/1670 train_time:115415ms step_avg:93.08ms
step:1241/1670 train_time:115508ms step_avg:93.08ms
step:1242/1670 train_time:115601ms step_avg:93.08ms
step:1243/1670 train_time:115694ms step_avg:93.08ms
step:1244/1670 train_time:115787ms step_avg:93.08ms
step:1245/1670 train_time:115880ms step_avg:93.08ms
step:1246/1670 train_time:115973ms step_avg:93.08ms
step:1247/1670 train_time:116068ms step_avg:93.08ms
step:1248/1670 train_time:116161ms step_avg:93.08ms
step:1249/1670 train_time:116254ms step_avg:93.08ms
step:1250/1670 train_time:116346ms step_avg:93.08ms
step:1250/1670 val_loss:3.3784 train_time:116438ms step_avg:93.15ms
step:1251/1670 train_time:116458ms step_avg:93.09ms
step:1252/1670 train_time:116533ms step_avg:93.08ms
step:1253/1670 train_time:116626ms step_avg:93.08ms
step:1254/1670 train_time:116719ms step_avg:93.08ms
step:1255/1670 train_time:116812ms step_avg:93.08ms
step:1256/1670 train_time:116904ms step_avg:93.08ms
step:1257/1670 train_time:116996ms step_avg:93.08ms
step:1258/1670 train_time:117089ms step_avg:93.08ms
step:1259/1670 train_time:117182ms step_avg:93.08ms
step:1260/1670 train_time:117276ms step_avg:93.08ms
step:1261/1670 train_time:117370ms step_avg:93.08ms
step:1262/1670 train_time:117465ms step_avg:93.08ms
step:1263/1670 train_time:117559ms step_avg:93.08ms
step:1264/1670 train_time:117651ms step_avg:93.08ms
step:1265/1670 train_time:117743ms step_avg:93.08ms
step:1266/1670 train_time:117836ms step_avg:93.08ms
step:1267/1670 train_time:117930ms step_avg:93.08ms
step:1268/1670 train_time:118025ms step_avg:93.08ms
step:1269/1670 train_time:118117ms step_avg:93.08ms
step:1270/1670 train_time:118210ms step_avg:93.08ms
step:1271/1670 train_time:118303ms step_avg:93.08ms
step:1272/1670 train_time:118398ms step_avg:93.08ms
step:1273/1670 train_time:118492ms step_avg:93.08ms
step:1274/1670 train_time:118735ms step_avg:93.20ms
step:1275/1670 train_time:118809ms step_avg:93.18ms
step:1276/1670 train_time:118901ms step_avg:93.18ms
step:1277/1670 train_time:118993ms step_avg:93.18ms
step:1278/1670 train_time:119084ms step_avg:93.18ms
step:1279/1670 train_time:119176ms step_avg:93.18ms
step:1280/1670 train_time:119269ms step_avg:93.18ms
step:1281/1670 train_time:119360ms step_avg:93.18ms
step:1282/1670 train_time:119453ms step_avg:93.18ms
step:1283/1670 train_time:119545ms step_avg:93.18ms
step:1284/1670 train_time:119646ms step_avg:93.18ms
step:1285/1670 train_time:119743ms step_avg:93.19ms
step:1286/1670 train_time:119836ms step_avg:93.19ms
step:1287/1670 train_time:119930ms step_avg:93.19ms
step:1288/1670 train_time:120023ms step_avg:93.19ms
step:1289/1670 train_time:120115ms step_avg:93.18ms
step:1290/1670 train_time:120207ms step_avg:93.18ms
step:1291/1670 train_time:120299ms step_avg:93.18ms
step:1292/1670 train_time:120391ms step_avg:93.18ms
step:1293/1670 train_time:120483ms step_avg:93.18ms
step:1294/1670 train_time:120579ms step_avg:93.18ms
step:1295/1670 train_time:120679ms step_avg:93.19ms
step:1296/1670 train_time:120774ms step_avg:93.19ms
step:1297/1670 train_time:120867ms step_avg:93.19ms
step:1298/1670 train_time:120960ms step_avg:93.19ms
step:1299/1670 train_time:121052ms step_avg:93.19ms
step:1300/1670 train_time:121144ms step_avg:93.19ms
step:1301/1670 train_time:121237ms step_avg:93.19ms
step:1302/1670 train_time:121330ms step_avg:93.19ms
step:1303/1670 train_time:121421ms step_avg:93.19ms
step:1304/1670 train_time:121515ms step_avg:93.19ms
step:1305/1670 train_time:121610ms step_avg:93.19ms
step:1306/1670 train_time:121705ms step_avg:93.19ms
step:1307/1670 train_time:121798ms step_avg:93.19ms
step:1308/1670 train_time:121891ms step_avg:93.19ms
step:1309/1670 train_time:121984ms step_avg:93.19ms
step:1310/1670 train_time:122077ms step_avg:93.19ms
step:1311/1670 train_time:122170ms step_avg:93.19ms
step:1312/1670 train_time:122263ms step_avg:93.19ms
step:1313/1670 train_time:122356ms step_avg:93.19ms
step:1314/1670 train_time:122448ms step_avg:93.19ms
step:1315/1670 train_time:122541ms step_avg:93.19ms
step:1316/1670 train_time:122637ms step_avg:93.19ms
step:1317/1670 train_time:122732ms step_avg:93.19ms
step:1318/1670 train_time:122825ms step_avg:93.19ms
step:1319/1670 train_time:122918ms step_avg:93.19ms
step:1320/1670 train_time:123011ms step_avg:93.19ms
step:1321/1670 train_time:123103ms step_avg:93.19ms
step:1322/1670 train_time:123196ms step_avg:93.19ms
step:1323/1670 train_time:123288ms step_avg:93.19ms
step:1324/1670 train_time:123381ms step_avg:93.19ms
step:1325/1670 train_time:123474ms step_avg:93.19ms
step:1326/1670 train_time:123568ms step_avg:93.19ms
step:1327/1670 train_time:123661ms step_avg:93.19ms
step:1328/1670 train_time:123756ms step_avg:93.19ms
step:1329/1670 train_time:123850ms step_avg:93.19ms
step:1330/1670 train_time:123943ms step_avg:93.19ms
step:1331/1670 train_time:124036ms step_avg:93.19ms
step:1332/1670 train_time:124129ms step_avg:93.19ms
step:1333/1670 train_time:124222ms step_avg:93.19ms
step:1334/1670 train_time:124314ms step_avg:93.19ms
step:1335/1670 train_time:124407ms step_avg:93.19ms
step:1336/1670 train_time:124500ms step_avg:93.19ms
step:1337/1670 train_time:124595ms step_avg:93.19ms
step:1338/1670 train_time:124688ms step_avg:93.19ms
step:1339/1670 train_time:124782ms step_avg:93.19ms
step:1340/1670 train_time:124877ms step_avg:93.19ms
step:1341/1670 train_time:124970ms step_avg:93.19ms
step:1342/1670 train_time:125064ms step_avg:93.19ms
step:1343/1670 train_time:125157ms step_avg:93.19ms
step:1344/1670 train_time:125250ms step_avg:93.19ms
step:1345/1670 train_time:125343ms step_avg:93.19ms
step:1346/1670 train_time:125435ms step_avg:93.19ms
step:1347/1670 train_time:125529ms step_avg:93.19ms
step:1348/1670 train_time:125621ms step_avg:93.19ms
step:1349/1670 train_time:125715ms step_avg:93.19ms
step:1350/1670 train_time:125808ms step_avg:93.19ms
step:1351/1670 train_time:125903ms step_avg:93.19ms
step:1352/1670 train_time:125997ms step_avg:93.19ms
step:1353/1670 train_time:126090ms step_avg:93.19ms
step:1354/1670 train_time:126183ms step_avg:93.19ms
step:1355/1670 train_time:126277ms step_avg:93.19ms
step:1356/1670 train_time:126371ms step_avg:93.19ms
step:1357/1670 train_time:126464ms step_avg:93.19ms
step:1358/1670 train_time:126557ms step_avg:93.19ms
step:1359/1670 train_time:126650ms step_avg:93.19ms
step:1360/1670 train_time:126742ms step_avg:93.19ms
step:1361/1670 train_time:126837ms step_avg:93.19ms
step:1362/1670 train_time:126931ms step_avg:93.19ms
step:1363/1670 train_time:127024ms step_avg:93.19ms
step:1364/1670 train_time:127117ms step_avg:93.19ms
step:1365/1670 train_time:127210ms step_avg:93.19ms
step:1366/1670 train_time:127303ms step_avg:93.19ms
step:1367/1670 train_time:127398ms step_avg:93.20ms
step:1368/1670 train_time:127491ms step_avg:93.19ms
step:1369/1670 train_time:127583ms step_avg:93.19ms
step:1370/1670 train_time:127678ms step_avg:93.20ms
step:1371/1670 train_time:127772ms step_avg:93.20ms
step:1372/1670 train_time:127865ms step_avg:93.20ms
step:1373/1670 train_time:127959ms step_avg:93.20ms
step:1374/1670 train_time:128053ms step_avg:93.20ms
step:1375/1670 train_time:128145ms step_avg:93.20ms
step:1375/1670 val_loss:3.3439 train_time:128238ms step_avg:93.26ms
step:1376/1670 train_time:128258ms step_avg:93.21ms
step:1377/1670 train_time:128331ms step_avg:93.20ms
step:1378/1670 train_time:128423ms step_avg:93.20ms
step:1379/1670 train_time:128516ms step_avg:93.20ms
step:1380/1670 train_time:128609ms step_avg:93.20ms
step:1381/1670 train_time:128702ms step_avg:93.19ms
step:1382/1670 train_time:128795ms step_avg:93.19ms
step:1383/1670 train_time:128889ms step_avg:93.20ms
step:1384/1670 train_time:128982ms step_avg:93.20ms
step:1385/1670 train_time:129077ms step_avg:93.20ms
step:1386/1670 train_time:129172ms step_avg:93.20ms
step:1387/1670 train_time:129267ms step_avg:93.20ms
step:1388/1670 train_time:129360ms step_avg:93.20ms
step:1389/1670 train_time:129453ms step_avg:93.20ms
step:1390/1670 train_time:129546ms step_avg:93.20ms
step:1391/1670 train_time:129639ms step_avg:93.20ms
step:1392/1670 train_time:129730ms step_avg:93.20ms
step:1393/1670 train_time:129823ms step_avg:93.20ms
step:1394/1670 train_time:129917ms step_avg:93.20ms
step:1395/1670 train_time:130012ms step_avg:93.20ms
step:1396/1670 train_time:130106ms step_avg:93.20ms
step:1397/1670 train_time:130200ms step_avg:93.20ms
step:1398/1670 train_time:130294ms step_avg:93.20ms
step:1399/1670 train_time:130386ms step_avg:93.20ms
step:1400/1670 train_time:130480ms step_avg:93.20ms
step:1401/1670 train_time:130572ms step_avg:93.20ms
step:1402/1670 train_time:130665ms step_avg:93.20ms
step:1403/1670 train_time:130757ms step_avg:93.20ms
step:1404/1670 train_time:130851ms step_avg:93.20ms
step:1405/1670 train_time:130943ms step_avg:93.20ms
step:1406/1670 train_time:131038ms step_avg:93.20ms
step:1407/1670 train_time:131131ms step_avg:93.20ms
step:1408/1670 train_time:131224ms step_avg:93.20ms
step:1409/1670 train_time:131319ms step_avg:93.20ms
step:1410/1670 train_time:131413ms step_avg:93.20ms
step:1411/1670 train_time:131505ms step_avg:93.20ms
step:1412/1670 train_time:131600ms step_avg:93.20ms
step:1413/1670 train_time:131692ms step_avg:93.20ms
step:1414/1670 train_time:131785ms step_avg:93.20ms
step:1415/1670 train_time:131880ms step_avg:93.20ms
step:1416/1670 train_time:131973ms step_avg:93.20ms
step:1417/1670 train_time:132066ms step_avg:93.20ms
step:1418/1670 train_time:132160ms step_avg:93.20ms
step:1419/1670 train_time:132254ms step_avg:93.20ms
step:1420/1670 train_time:132348ms step_avg:93.20ms
step:1421/1670 train_time:132441ms step_avg:93.20ms
step:1422/1670 train_time:132534ms step_avg:93.20ms
step:1423/1670 train_time:132627ms step_avg:93.20ms
step:1424/1670 train_time:132721ms step_avg:93.20ms
step:1425/1670 train_time:132814ms step_avg:93.20ms
step:1426/1670 train_time:132906ms step_avg:93.20ms
step:1427/1670 train_time:133000ms step_avg:93.20ms
step:1428/1670 train_time:133093ms step_avg:93.20ms
step:1429/1670 train_time:133187ms step_avg:93.20ms
step:1430/1670 train_time:133280ms step_avg:93.20ms
step:1431/1670 train_time:133373ms step_avg:93.20ms
step:1432/1670 train_time:133466ms step_avg:93.20ms
step:1433/1670 train_time:133560ms step_avg:93.20ms
step:1434/1670 train_time:133653ms step_avg:93.20ms
step:1435/1670 train_time:133746ms step_avg:93.20ms
step:1436/1670 train_time:133840ms step_avg:93.20ms
step:1437/1670 train_time:133933ms step_avg:93.20ms
step:1438/1670 train_time:134026ms step_avg:93.20ms
step:1439/1670 train_time:134121ms step_avg:93.20ms
step:1440/1670 train_time:134214ms step_avg:93.20ms
step:1441/1670 train_time:134307ms step_avg:93.20ms
step:1442/1670 train_time:134400ms step_avg:93.20ms
step:1443/1670 train_time:134493ms step_avg:93.20ms
step:1444/1670 train_time:134587ms step_avg:93.20ms
step:1445/1670 train_time:134680ms step_avg:93.20ms
step:1446/1670 train_time:134774ms step_avg:93.20ms
step:1447/1670 train_time:134867ms step_avg:93.20ms
step:1448/1670 train_time:134960ms step_avg:93.20ms
step:1449/1670 train_time:135054ms step_avg:93.20ms
step:1450/1670 train_time:135148ms step_avg:93.21ms
step:1451/1670 train_time:135241ms step_avg:93.21ms
step:1452/1670 train_time:135335ms step_avg:93.21ms
step:1453/1670 train_time:135427ms step_avg:93.21ms
step:1454/1670 train_time:135521ms step_avg:93.21ms
step:1455/1670 train_time:135615ms step_avg:93.21ms
step:1456/1670 train_time:135707ms step_avg:93.21ms
step:1457/1670 train_time:135800ms step_avg:93.21ms
step:1458/1670 train_time:135894ms step_avg:93.21ms
step:1459/1670 train_time:135987ms step_avg:93.21ms
step:1460/1670 train_time:136081ms step_avg:93.21ms
step:1461/1670 train_time:136175ms step_avg:93.21ms
step:1462/1670 train_time:136268ms step_avg:93.21ms
step:1463/1670 train_time:136361ms step_avg:93.21ms
step:1464/1670 train_time:136454ms step_avg:93.21ms
step:1465/1670 train_time:136547ms step_avg:93.21ms
step:1466/1670 train_time:136640ms step_avg:93.21ms
step:1467/1670 train_time:136734ms step_avg:93.21ms
step:1468/1670 train_time:136827ms step_avg:93.21ms
step:1469/1670 train_time:136920ms step_avg:93.21ms
step:1470/1670 train_time:137013ms step_avg:93.21ms
step:1471/1670 train_time:137106ms step_avg:93.21ms
step:1472/1670 train_time:137200ms step_avg:93.21ms
step:1473/1670 train_time:137293ms step_avg:93.21ms
step:1474/1670 train_time:137386ms step_avg:93.21ms
step:1475/1670 train_time:137480ms step_avg:93.21ms
step:1476/1670 train_time:137572ms step_avg:93.21ms
step:1477/1670 train_time:137666ms step_avg:93.21ms
step:1478/1670 train_time:137759ms step_avg:93.21ms
step:1479/1670 train_time:137852ms step_avg:93.21ms
step:1480/1670 train_time:137946ms step_avg:93.21ms
step:1481/1670 train_time:138040ms step_avg:93.21ms
step:1482/1670 train_time:138131ms step_avg:93.21ms
step:1483/1670 train_time:138226ms step_avg:93.21ms
step:1484/1670 train_time:138319ms step_avg:93.21ms
step:1485/1670 train_time:138567ms step_avg:93.31ms
step:1486/1670 train_time:138641ms step_avg:93.30ms
step:1487/1670 train_time:138732ms step_avg:93.30ms
step:1488/1670 train_time:138824ms step_avg:93.30ms
step:1489/1670 train_time:138916ms step_avg:93.29ms
step:1490/1670 train_time:139008ms step_avg:93.29ms
step:1491/1670 train_time:139099ms step_avg:93.29ms
step:1492/1670 train_time:139191ms step_avg:93.29ms
step:1493/1670 train_time:139283ms step_avg:93.29ms
step:1494/1670 train_time:139376ms step_avg:93.29ms
step:1495/1670 train_time:139473ms step_avg:93.29ms
step:1496/1670 train_time:139571ms step_avg:93.30ms
step:1497/1670 train_time:139666ms step_avg:93.30ms
step:1498/1670 train_time:139759ms step_avg:93.30ms
step:1499/1670 train_time:139853ms step_avg:93.30ms
step:1500/1670 train_time:139945ms step_avg:93.30ms
step:1500/1670 val_loss:3.3137 train_time:140039ms step_avg:93.36ms
step:1501/1670 train_time:140059ms step_avg:93.31ms
step:1502/1670 train_time:140133ms step_avg:93.30ms
step:1503/1670 train_time:140225ms step_avg:93.30ms
step:1504/1670 train_time:140319ms step_avg:93.30ms
step:1505/1670 train_time:140412ms step_avg:93.30ms
step:1506/1670 train_time:140504ms step_avg:93.30ms
step:1507/1670 train_time:140598ms step_avg:93.30ms
step:1508/1670 train_time:140693ms step_avg:93.30ms
step:1509/1670 train_time:140786ms step_avg:93.30ms
step:1510/1670 train_time:140879ms step_avg:93.30ms
step:1511/1670 train_time:140973ms step_avg:93.30ms
step:1512/1670 train_time:141068ms step_avg:93.30ms
step:1513/1670 train_time:141161ms step_avg:93.30ms
step:1514/1670 train_time:141254ms step_avg:93.30ms
step:1515/1670 train_time:141346ms step_avg:93.30ms
step:1516/1670 train_time:141439ms step_avg:93.30ms
step:1517/1670 train_time:141532ms step_avg:93.30ms
step:1518/1670 train_time:141625ms step_avg:93.30ms
step:1519/1670 train_time:141719ms step_avg:93.30ms
step:1520/1670 train_time:141812ms step_avg:93.30ms
step:1521/1670 train_time:141905ms step_avg:93.30ms
step:1522/1670 train_time:142000ms step_avg:93.30ms
step:1523/1670 train_time:142093ms step_avg:93.30ms
step:1524/1670 train_time:142186ms step_avg:93.30ms
step:1525/1670 train_time:142282ms step_avg:93.30ms
step:1526/1670 train_time:142375ms step_avg:93.30ms
step:1527/1670 train_time:142467ms step_avg:93.30ms
step:1528/1670 train_time:142561ms step_avg:93.30ms
step:1529/1670 train_time:142655ms step_avg:93.30ms
step:1530/1670 train_time:142747ms step_avg:93.30ms
step:1531/1670 train_time:142841ms step_avg:93.30ms
step:1532/1670 train_time:142934ms step_avg:93.30ms
step:1533/1670 train_time:143028ms step_avg:93.30ms
step:1534/1670 train_time:143121ms step_avg:93.30ms
step:1535/1670 train_time:143214ms step_avg:93.30ms
step:1536/1670 train_time:143308ms step_avg:93.30ms
step:1537/1670 train_time:143401ms step_avg:93.30ms
step:1538/1670 train_time:143493ms step_avg:93.30ms
step:1539/1670 train_time:143586ms step_avg:93.30ms
step:1540/1670 train_time:143680ms step_avg:93.30ms
step:1541/1670 train_time:143774ms step_avg:93.30ms
step:1542/1670 train_time:143867ms step_avg:93.30ms
step:1543/1670 train_time:143962ms step_avg:93.30ms
step:1544/1670 train_time:144055ms step_avg:93.30ms
step:1545/1670 train_time:144148ms step_avg:93.30ms
step:1546/1670 train_time:144241ms step_avg:93.30ms
step:1547/1670 train_time:144335ms step_avg:93.30ms
step:1548/1670 train_time:144428ms step_avg:93.30ms
step:1549/1670 train_time:144522ms step_avg:93.30ms
step:1550/1670 train_time:144615ms step_avg:93.30ms
step:1551/1670 train_time:144708ms step_avg:93.30ms
step:1552/1670 train_time:144801ms step_avg:93.30ms
step:1553/1670 train_time:144894ms step_avg:93.30ms
step:1554/1670 train_time:144988ms step_avg:93.30ms
step:1555/1670 train_time:145080ms step_avg:93.30ms
step:1556/1670 train_time:145173ms step_avg:93.30ms
step:1557/1670 train_time:145266ms step_avg:93.30ms
step:1558/1670 train_time:145360ms step_avg:93.30ms
step:1559/1670 train_time:145454ms step_avg:93.30ms
step:1560/1670 train_time:145547ms step_avg:93.30ms
step:1561/1670 train_time:145641ms step_avg:93.30ms
step:1562/1670 train_time:145733ms step_avg:93.30ms
step:1563/1670 train_time:145827ms step_avg:93.30ms
step:1564/1670 train_time:145921ms step_avg:93.30ms
step:1565/1670 train_time:146015ms step_avg:93.30ms
step:1566/1670 train_time:146108ms step_avg:93.30ms
step:1567/1670 train_time:146201ms step_avg:93.30ms
step:1568/1670 train_time:146294ms step_avg:93.30ms
step:1569/1670 train_time:146387ms step_avg:93.30ms
step:1570/1670 train_time:146481ms step_avg:93.30ms
step:1571/1670 train_time:146574ms step_avg:93.30ms
step:1572/1670 train_time:146666ms step_avg:93.30ms
step:1573/1670 train_time:146761ms step_avg:93.30ms
step:1574/1670 train_time:146855ms step_avg:93.30ms
step:1575/1670 train_time:146948ms step_avg:93.30ms
step:1576/1670 train_time:147042ms step_avg:93.30ms
step:1577/1670 train_time:147135ms step_avg:93.30ms
step:1578/1670 train_time:147227ms step_avg:93.30ms
step:1579/1670 train_time:147322ms step_avg:93.30ms
step:1580/1670 train_time:147415ms step_avg:93.30ms
step:1581/1670 train_time:147508ms step_avg:93.30ms
step:1582/1670 train_time:147601ms step_avg:93.30ms
step:1583/1670 train_time:147695ms step_avg:93.30ms
step:1584/1670 train_time:147787ms step_avg:93.30ms
step:1585/1670 train_time:147880ms step_avg:93.30ms
step:1586/1670 train_time:147973ms step_avg:93.30ms
step:1587/1670 train_time:148066ms step_avg:93.30ms
step:1588/1670 train_time:148161ms step_avg:93.30ms
step:1589/1670 train_time:148255ms step_avg:93.30ms
step:1590/1670 train_time:148347ms step_avg:93.30ms
step:1591/1670 train_time:148441ms step_avg:93.30ms
step:1592/1670 train_time:148535ms step_avg:93.30ms
step:1593/1670 train_time:148628ms step_avg:93.30ms
step:1594/1670 train_time:148721ms step_avg:93.30ms
step:1595/1670 train_time:148814ms step_avg:93.30ms
step:1596/1670 train_time:148907ms step_avg:93.30ms
step:1597/1670 train_time:149001ms step_avg:93.30ms
step:1598/1670 train_time:149094ms step_avg:93.30ms
step:1599/1670 train_time:149188ms step_avg:93.30ms
step:1600/1670 train_time:149283ms step_avg:93.30ms
step:1601/1670 train_time:149376ms step_avg:93.30ms
step:1602/1670 train_time:149468ms step_avg:93.30ms
step:1603/1670 train_time:149561ms step_avg:93.30ms
step:1604/1670 train_time:149655ms step_avg:93.30ms
step:1605/1670 train_time:149748ms step_avg:93.30ms
step:1606/1670 train_time:149841ms step_avg:93.30ms
step:1607/1670 train_time:149935ms step_avg:93.30ms
step:1608/1670 train_time:150028ms step_avg:93.30ms
step:1609/1670 train_time:150122ms step_avg:93.30ms
step:1610/1670 train_time:150216ms step_avg:93.30ms
step:1611/1670 train_time:150309ms step_avg:93.30ms
step:1612/1670 train_time:150402ms step_avg:93.30ms
step:1613/1670 train_time:150496ms step_avg:93.30ms
step:1614/1670 train_time:150591ms step_avg:93.30ms
step:1615/1670 train_time:150684ms step_avg:93.30ms
step:1616/1670 train_time:150777ms step_avg:93.30ms
step:1617/1670 train_time:150870ms step_avg:93.30ms
step:1618/1670 train_time:150963ms step_avg:93.30ms
step:1619/1670 train_time:151057ms step_avg:93.30ms
step:1620/1670 train_time:151150ms step_avg:93.30ms
step:1621/1670 train_time:151243ms step_avg:93.30ms
step:1622/1670 train_time:151336ms step_avg:93.30ms
step:1623/1670 train_time:151430ms step_avg:93.30ms
step:1624/1670 train_time:151523ms step_avg:93.30ms
step:1625/1670 train_time:151617ms step_avg:93.30ms
step:1625/1670 val_loss:3.2887 train_time:151709ms step_avg:93.36ms
step:1626/1670 train_time:151730ms step_avg:93.32ms
step:1627/1670 train_time:151804ms step_avg:93.30ms
step:1628/1670 train_time:151898ms step_avg:93.30ms
step:1629/1670 train_time:151990ms step_avg:93.30ms
step:1630/1670 train_time:152082ms step_avg:93.30ms
step:1631/1670 train_time:152175ms step_avg:93.30ms
step:1632/1670 train_time:152268ms step_avg:93.30ms
step:1633/1670 train_time:152362ms step_avg:93.30ms
step:1634/1670 train_time:152455ms step_avg:93.30ms
step:1635/1670 train_time:152547ms step_avg:93.30ms
step:1636/1670 train_time:152642ms step_avg:93.30ms
step:1637/1670 train_time:152737ms step_avg:93.30ms
step:1638/1670 train_time:152831ms step_avg:93.30ms
step:1639/1670 train_time:152926ms step_avg:93.30ms
step:1640/1670 train_time:153019ms step_avg:93.30ms
step:1641/1670 train_time:153111ms step_avg:93.30ms
step:1642/1670 train_time:153204ms step_avg:93.30ms
step:1643/1670 train_time:153297ms step_avg:93.30ms
step:1644/1670 train_time:153390ms step_avg:93.30ms
step:1645/1670 train_time:153483ms step_avg:93.30ms
step:1646/1670 train_time:153577ms step_avg:93.30ms
step:1647/1670 train_time:153671ms step_avg:93.30ms
step:1648/1670 train_time:153766ms step_avg:93.30ms
step:1649/1670 train_time:153860ms step_avg:93.31ms
step:1650/1670 train_time:153953ms step_avg:93.31ms
step:1651/1670 train_time:154046ms step_avg:93.30ms
step:1652/1670 train_time:154140ms step_avg:93.30ms
step:1653/1670 train_time:154231ms step_avg:93.30ms
step:1654/1670 train_time:154324ms step_avg:93.30ms
step:1655/1670 train_time:154417ms step_avg:93.30ms
step:1656/1670 train_time:154510ms step_avg:93.30ms
step:1657/1670 train_time:154603ms step_avg:93.30ms
step:1658/1670 train_time:154697ms step_avg:93.30ms
step:1659/1670 train_time:154790ms step_avg:93.30ms
step:1660/1670 train_time:154884ms step_avg:93.30ms
step:1661/1670 train_time:154977ms step_avg:93.30ms
step:1662/1670 train_time:155070ms step_avg:93.30ms
step:1663/1670 train_time:155164ms step_avg:93.30ms
step:1664/1670 train_time:155257ms step_avg:93.30ms
step:1665/1670 train_time:155350ms step_avg:93.30ms
step:1666/1670 train_time:155443ms step_avg:93.30ms
step:1667/1670 train_time:155536ms step_avg:93.30ms
step:1668/1670 train_time:155629ms step_avg:93.30ms
step:1669/1670 train_time:155723ms step_avg:93.30ms
step:1670/1670 train_time:155816ms step_avg:93.30ms
step:1670/1670 val_loss:3.2802 train_time:156077ms step_avg:93.46ms
peak memory allocated: 32002 MiB reserved: 46834 MiB

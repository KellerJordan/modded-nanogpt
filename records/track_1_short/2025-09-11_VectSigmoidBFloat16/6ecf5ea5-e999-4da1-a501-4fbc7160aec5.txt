import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(
    x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(
    g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99


def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]


@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(
        pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M
    )

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr,
    C_ptr,
    M,
    K,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr,
    C_ptr,
    M,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    alpha,
    beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (
        offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c
    )
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out


@torch.compile(
    dynamic=False, fullgraph=True
)  # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer


class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(
        self,
        params,
        lr: float = 1e-3,
        betas: tuple[float, float] = (0.9, 0.999),
        eps: float = 1e-8,
        weight_decay: float = 0.01,
    ):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(
                        grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True
                    ).get_future()
                )
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group["betas"]
            eps = group["eps"]
            wd = group["weight_decay"]
            params = group["params"]
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size : (rank + 1) * rank_size]
                lr = group["lr"] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(
                    g_slice, g_slice, value=1 - beta2
                )
                # bias corrections
                bias1 = 1 - beta1**t
                bias2 = 1 - beta2**t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(
                    dist.all_gather_into_tensor(
                        p, p_slice, async_op=True
                    ).get_future()
                )
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model


def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(
        self,
        in_features: int,
        out_features: int,
        use_fp8=False,
        x_s=1.0,
        w_s=1.0,
        grad_s=1.0,
    ):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (
            self.in_features**-0.5
        )  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3**0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(
                _x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s
            )[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    rotary_cos: torch.Tensor
    rotary_sin: torch.Tensor
    attn_scale: float


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        rotary_cos, rotary_sin = attn_args.rotary_cos, attn_args.rotary_sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = (
            attn_args.seqlens,
            attn_args.attn_scale,
            attn_args.bm_size,
        )

        q, k, v = (
            F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x))
            .view(B, T, 3 * self.num_heads, self.head_dim)
            .chunk(3, dim=-2)
        )
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = (
            rotary(q, rotary_cos, rotary_sin),
            rotary(k, rotary_cos, rotary_sin),
        )
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(
                v
            )  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = (
            args.train_max_seq_len
            if self.training
            else (args.val_batch_size // (grad_accum_steps * world_size))
        )

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(
            q[0],
            k[0],
            v[0],
            cu_seqlens_q=seqlens,
            cu_seqlens_k=seqlens,
            max_seqlen_q=max_len,
            max_seqlen_k=max_len,
            causal=True,
            softmax_scale=attn_scale,
            window_size=(bm_size, 0),
        )
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(
            self.attn_gate(x[..., : self.attn_gate.weight.size(-1)])
        ).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(
            B, T, self.num_heads * self.head_dim
        )  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x
        ).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = (
            CausalSelfAttention(dim, head_dim, num_heads)
            if layer_idx != 7
            else None
        )
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(
        self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs
    ):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model


def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        num_layers: int,
        num_heads: int,
        head_dim: int,
        model_dim: int,
        max_seq_len: int,
    ):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList(
            [nn.Embedding(vocab_size, model_dim) for _ in range(3)]
        )
        self.blocks = nn.ModuleList(
            [
                Block(model_dim, head_dim, num_heads, i)
                for i in range(num_layers)
            ]
        )
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(
            model_dim,
            vocab_size,
            use_fp8=use_fp8,
            x_s=(model_dim**0.5) / 448,
            w_s=2**-9,
            grad_s=1 / 448,
        )
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.ones(pad),
                ]
            )
        )
        self.max_seq_len = max_seq_len
        self.setup_yarn(head_dim)
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.0
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.0
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def setup_yarn(self, head_dim: int):
        # store single copy of rotary tensors
        angular_freq = (1 / 1024) ** torch.linspace(
            0, 1, steps=head_dim // 4, dtype=torch.float32
        )
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat(
            [angular_freq, angular_freq.new_zeros(head_dim // 4)]
        )
        t = torch.arange(self.max_seq_len, dtype=torch.float32)
        theta = torch.outer(t, angular_freq)
        self.rotary_cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.rotary_sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq

        # scale attention factor f in attn=softmax(f*qk) logarithmically with window size @classiclarryd
        windows = list(
            dict.fromkeys(list(args.ws_schedule) + [args.ws_validate])
        )
        scale_factors = [
            0.2 * math.log(curr / prev) + 1
            for prev, curr in zip(windows[:-1], windows[1:])
        ]
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        attn_scales = list(
            accumulate([0.1] + scale_factors, lambda acc, factor: acc * factor)
        )
        self.attn_scales = dict(zip(windows, attn_scales))

    def apply_yarn(
        self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32
    ):
        rotations = (
            args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        )
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp(
            (rotations - alpha) / (beta - alpha), 0, 1
        )
        self.angular_freq *= scaling_factor + interpolation_weight * (
            1 - scaling_factor
        )
        t = torch.arange(
            self.max_seq_len,
            dtype=torch.float32,
            device=self.angular_freq.device,
        )
        theta = torch.outer(t, self.angular_freq)
        self.rotary_cos.copy_(theta.cos())
        self.rotary_sin.copy_(theta.sin())

    def forward(
        self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int
    ):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = (
            [ve[0], ve[1], ve[2]]
            + [None] * (len(self.blocks) - 6)
            + [ve[0], ve[1], ve[2]]
        )
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        bm_sizes = [
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
        ]
        assert len(bm_sizes) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]).to(
            torch.bfloat16
        )  # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[: (len(self.blocks) // 2)]
        lambdas = self.scalars[
            1 * len(self.blocks) : 3 * len(self.blocks)
        ].view(-1, 2)
        sa_lambdas = self.scalars[
            3 * len(self.blocks) : 5 * len(self.blocks)
        ].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                rotary_cos=self.rotary_cos,
                rotary_sin=self.rotary_sin,
                attn_scale=self.attn_scales[ws],
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(
            logits.view(-1, logits.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader


def _load_data_shard(file: Path):
    header = torch.from_file(
        str(file), False, 256, dtype=torch.int32
    )  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(
            num_tokens, dtype=torch.uint16, pin_memory=True
        )  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(
            tokens.numpy()
        )  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, (
            "number of tokens read does not match header"
        )
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1):
        # Precompute BOS positions once per shard
        self.size = tokens.numel()
        self.bos_idx = (
            (tokens == BOS_ID)
            .nonzero(as_tuple=True)[0]
            .to(torch.int64)
            .cpu()
            .numpy()
        )
        self.i = 0
        self.world_size = world_size

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(
                        f"Insufficient BOS ahead of position {cur}; hit tail of shard."
                    )
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(
                    self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                    cur + max_seq_len,
                    cur + num_tokens_local - cur_len + 1,
                )
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx

        return starts, ends


def distributed_data_generator(
    filename_pattern: str,
    num_tokens: int,
    max_seq_len: int,
    grad_accum_steps: int = 1,
    align_to_bos: bool = True,
):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, (
        "Batch size must be divisible by world size"
    )
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(
            f"No files found for pattern: {filename_pattern}"
        )

    file_iter = iter(
        files
    )  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    finder = BOSFinder(tokens, world_size=world_size) if align_to_bos else None
    pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(
            num_tokens_local // 300, n=128
        )  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(
                    num_tokens_local, max_seq_len
                )
                start_idxs, end_idxs = (
                    torch.tensor(seq_starts[rank]),
                    torch.tensor(seq_ends[rank]),
                )
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens = _load_data_shard(next(file_iter))
                finder = BOSFinder(tokens, world_size=world_size)
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= (
                1  # last document was too long to account for _targets offset
            )
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(
                tokens
            ):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local : pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(
                num_tokens_local,
            )
            _targets = buf[1:].view(
                num_tokens_local,
            )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1 : len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(
                device="cuda", dtype=torch.int32, non_blocking=True
            ),
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, (
                "Num tokens must be divisible by world size"
            )
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main


@dataclass
class Hyperparameters:
    # data
    train_files: str = (
        "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    )
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1670  # number of iterations to run
    cooldown_frac: int = (
        0.5  # fraction of training spent cooling down the learning rate
    )
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = (
        125  # every how many steps to evaluate val loss? 0 for only at the end
    )
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws @classiclarryd


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = rank == 0  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(
    f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}"
)
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import

    return subprocess.run(
        ["nvidia-smi"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    ).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size)
    // (grad_accum_steps * world_size),
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [
    p
    for n, p in model.blocks.named_parameters()
    if p.ndim >= 2 and "embed" not in n
]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(
    hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)  # save the initial state
train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    ws = args.ws_schedule[
        step % len(args.ws_schedule)
    ]  # each window size is a new graph, need to warm up each
    model(inputs, targets, cum_seqlens, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws = get_ws(0)
for step in range(train_steps + 1):
    last_step = step == train_steps
    new_ws = get_ws(step)
    if new_ws != ws:
        model.apply_yarn(ws, new_ws)
        ws = new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (
        args.val_loss_every > 0 and step % args.val_loss_every == 0
    ):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(
            args.val_files,
            args.val_batch_size,
            -1,
            grad_accum_steps=grad_accum_steps,
            align_to_bos=False,
        )
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True,
        )
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(
                step=step,
                code=code,
                model=model.state_dict(),
                optimizers=[opt.state_dict() for opt in optimizers],
            )
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1)  # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (
        time.perf_counter() - t0
    )
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True,
    )

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.11 (main, Sep  2 2025, 14:20:58) [Clang 20.1.4 ]
Running PyTorch 2.9.0.dev20250718+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 11 09:24:28 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                  Off |
| N/A   40C    P0            126W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                  Off |
| N/A   45C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:65:00.0 Off |                  Off |
| N/A   45C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:68:00.0 Off |                  Off |
| N/A   37C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:85:00.0 Off |                  Off |
| N/A   38C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:86:00.0 Off |                  Off |
| N/A   45C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:E5:00.0 Off |                  Off |
| N/A   45C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E8:00.0 Off |                  Off |
| N/A   41C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1670 val_loss:10.8258 train_time:0ms step_avg:0.09ms
step:1/1670 train_time:297ms step_avg:297.03ms
step:2/1670 train_time:315ms step_avg:157.52ms
step:3/1670 train_time:384ms step_avg:127.86ms
step:4/1670 train_time:473ms step_avg:118.16ms
step:5/1670 train_time:563ms step_avg:112.68ms
step:6/1670 train_time:654ms step_avg:108.95ms
step:7/1670 train_time:744ms step_avg:106.26ms
step:8/1670 train_time:834ms step_avg:104.28ms
step:9/1670 train_time:925ms step_avg:102.75ms
step:10/1670 train_time:1016ms step_avg:101.56ms
step:11/1670 train_time:1109ms step_avg:100.78ms
step:12/1670 train_time:1203ms step_avg:100.21ms
step:13/1670 train_time:1298ms step_avg:99.81ms
step:14/1670 train_time:1390ms step_avg:99.32ms
step:15/1670 train_time:1481ms step_avg:98.74ms
step:16/1670 train_time:1573ms step_avg:98.28ms
step:17/1670 train_time:1664ms step_avg:97.91ms
step:18/1670 train_time:1755ms step_avg:97.49ms
step:19/1670 train_time:1845ms step_avg:97.12ms
step:20/1670 train_time:1936ms step_avg:96.79ms
step:21/1670 train_time:2029ms step_avg:96.62ms
step:22/1670 train_time:2121ms step_avg:96.40ms
step:23/1670 train_time:2215ms step_avg:96.31ms
step:24/1670 train_time:2308ms step_avg:96.18ms
step:25/1670 train_time:2401ms step_avg:96.03ms
step:26/1670 train_time:2492ms step_avg:95.86ms
step:27/1670 train_time:2584ms step_avg:95.71ms
step:28/1670 train_time:2675ms step_avg:95.54ms
step:29/1670 train_time:2767ms step_avg:95.42ms
step:30/1670 train_time:2858ms step_avg:95.25ms
step:31/1670 train_time:2949ms step_avg:95.13ms
step:32/1670 train_time:3040ms step_avg:94.99ms
step:33/1670 train_time:3132ms step_avg:94.90ms
step:34/1670 train_time:3224ms step_avg:94.82ms
step:35/1670 train_time:3316ms step_avg:94.75ms
step:36/1670 train_time:3408ms step_avg:94.68ms
step:37/1670 train_time:3501ms step_avg:94.62ms
step:38/1670 train_time:3593ms step_avg:94.54ms
step:39/1670 train_time:3684ms step_avg:94.46ms
step:40/1670 train_time:3775ms step_avg:94.37ms
step:41/1670 train_time:3866ms step_avg:94.28ms
step:42/1670 train_time:3956ms step_avg:94.20ms
step:43/1670 train_time:4048ms step_avg:94.14ms
step:44/1670 train_time:4139ms step_avg:94.08ms
step:45/1670 train_time:4234ms step_avg:94.08ms
step:46/1670 train_time:4327ms step_avg:94.07ms
step:47/1670 train_time:4419ms step_avg:94.03ms
step:48/1670 train_time:4510ms step_avg:93.96ms
step:49/1670 train_time:4602ms step_avg:93.91ms
step:50/1670 train_time:4693ms step_avg:93.85ms
step:51/1670 train_time:4785ms step_avg:93.82ms
step:52/1670 train_time:4876ms step_avg:93.76ms
step:53/1670 train_time:4967ms step_avg:93.72ms
step:54/1670 train_time:5058ms step_avg:93.67ms
step:55/1670 train_time:5150ms step_avg:93.65ms
step:56/1670 train_time:5243ms step_avg:93.63ms
step:57/1670 train_time:5336ms step_avg:93.61ms
step:58/1670 train_time:5428ms step_avg:93.58ms
step:59/1670 train_time:5519ms step_avg:93.55ms
step:60/1670 train_time:5611ms step_avg:93.52ms
step:61/1670 train_time:5703ms step_avg:93.50ms
step:62/1670 train_time:5793ms step_avg:93.44ms
step:63/1670 train_time:5885ms step_avg:93.41ms
step:64/1670 train_time:5975ms step_avg:93.36ms
step:65/1670 train_time:6067ms step_avg:93.34ms
step:66/1670 train_time:6158ms step_avg:93.31ms
step:67/1670 train_time:6253ms step_avg:93.32ms
step:68/1670 train_time:6345ms step_avg:93.30ms
step:69/1670 train_time:6437ms step_avg:93.28ms
step:70/1670 train_time:6528ms step_avg:93.25ms
step:71/1670 train_time:6619ms step_avg:93.23ms
step:72/1670 train_time:6710ms step_avg:93.19ms
step:73/1670 train_time:6800ms step_avg:93.15ms
step:74/1670 train_time:6892ms step_avg:93.13ms
step:75/1670 train_time:6983ms step_avg:93.11ms
step:76/1670 train_time:7076ms step_avg:93.11ms
step:77/1670 train_time:7168ms step_avg:93.09ms
step:78/1670 train_time:7259ms step_avg:93.06ms
step:79/1670 train_time:7353ms step_avg:93.08ms
step:80/1670 train_time:7447ms step_avg:93.09ms
step:81/1670 train_time:7539ms step_avg:93.08ms
step:82/1670 train_time:7632ms step_avg:93.07ms
step:83/1670 train_time:7723ms step_avg:93.05ms
step:84/1670 train_time:7814ms step_avg:93.03ms
step:85/1670 train_time:7905ms step_avg:93.00ms
step:86/1670 train_time:7996ms step_avg:92.98ms
step:87/1670 train_time:8087ms step_avg:92.96ms
step:88/1670 train_time:8178ms step_avg:92.93ms
step:89/1670 train_time:8270ms step_avg:92.92ms
step:90/1670 train_time:8361ms step_avg:92.90ms
step:91/1670 train_time:8454ms step_avg:92.90ms
step:92/1670 train_time:8546ms step_avg:92.90ms
step:93/1670 train_time:8638ms step_avg:92.88ms
step:94/1670 train_time:8729ms step_avg:92.86ms
step:95/1670 train_time:8821ms step_avg:92.85ms
step:96/1670 train_time:8912ms step_avg:92.83ms
step:97/1670 train_time:9003ms step_avg:92.82ms
step:98/1670 train_time:9095ms step_avg:92.80ms
step:99/1670 train_time:9186ms step_avg:92.79ms
step:100/1670 train_time:9277ms step_avg:92.77ms
step:101/1670 train_time:9369ms step_avg:92.76ms
step:102/1670 train_time:9460ms step_avg:92.74ms
step:103/1670 train_time:9552ms step_avg:92.74ms
step:104/1670 train_time:9645ms step_avg:92.74ms
step:105/1670 train_time:9736ms step_avg:92.72ms
step:106/1670 train_time:9827ms step_avg:92.71ms
step:107/1670 train_time:9918ms step_avg:92.69ms
step:108/1670 train_time:10009ms step_avg:92.68ms
step:109/1670 train_time:10102ms step_avg:92.68ms
step:110/1670 train_time:10192ms step_avg:92.66ms
step:111/1670 train_time:10285ms step_avg:92.65ms
step:112/1670 train_time:10376ms step_avg:92.65ms
step:113/1670 train_time:10467ms step_avg:92.63ms
step:114/1670 train_time:10558ms step_avg:92.62ms
step:115/1670 train_time:10651ms step_avg:92.61ms
step:116/1670 train_time:10741ms step_avg:92.60ms
step:117/1670 train_time:10834ms step_avg:92.60ms
step:118/1670 train_time:10926ms step_avg:92.60ms
step:119/1670 train_time:11018ms step_avg:92.59ms
step:120/1670 train_time:11109ms step_avg:92.58ms
step:121/1670 train_time:11200ms step_avg:92.56ms
step:122/1670 train_time:11292ms step_avg:92.55ms
step:123/1670 train_time:11382ms step_avg:92.54ms
step:124/1670 train_time:11474ms step_avg:92.53ms
step:125/1670 train_time:11566ms step_avg:92.53ms
step:125/1670 val_loss:4.2978 train_time:11657ms step_avg:93.26ms
step:126/1670 train_time:11676ms step_avg:92.67ms
step:127/1670 train_time:11750ms step_avg:92.52ms
step:128/1670 train_time:11849ms step_avg:92.57ms
step:129/1670 train_time:11944ms step_avg:92.59ms
step:130/1670 train_time:12037ms step_avg:92.59ms
step:131/1670 train_time:12128ms step_avg:92.58ms
step:132/1670 train_time:12218ms step_avg:92.56ms
step:133/1670 train_time:12308ms step_avg:92.54ms
step:134/1670 train_time:12397ms step_avg:92.52ms
step:135/1670 train_time:12487ms step_avg:92.50ms
step:136/1670 train_time:12578ms step_avg:92.49ms
step:137/1670 train_time:12670ms step_avg:92.48ms
step:138/1670 train_time:12763ms step_avg:92.49ms
step:139/1670 train_time:12857ms step_avg:92.50ms
step:140/1670 train_time:12950ms step_avg:92.50ms
step:141/1670 train_time:13041ms step_avg:92.49ms
step:142/1670 train_time:13132ms step_avg:92.48ms
step:143/1670 train_time:13222ms step_avg:92.46ms
step:144/1670 train_time:13313ms step_avg:92.45ms
step:145/1670 train_time:13403ms step_avg:92.44ms
step:146/1670 train_time:13494ms step_avg:92.43ms
step:147/1670 train_time:13585ms step_avg:92.42ms
step:148/1670 train_time:13677ms step_avg:92.41ms
step:149/1670 train_time:13769ms step_avg:92.41ms
step:150/1670 train_time:13861ms step_avg:92.41ms
step:151/1670 train_time:13954ms step_avg:92.41ms
step:152/1670 train_time:14046ms step_avg:92.41ms
step:153/1670 train_time:14137ms step_avg:92.40ms
step:154/1670 train_time:14229ms step_avg:92.40ms
step:155/1670 train_time:14320ms step_avg:92.39ms
step:156/1670 train_time:14410ms step_avg:92.37ms
step:157/1670 train_time:14501ms step_avg:92.36ms
step:158/1670 train_time:14591ms step_avg:92.35ms
step:159/1670 train_time:14681ms step_avg:92.34ms
step:160/1670 train_time:14772ms step_avg:92.33ms
step:161/1670 train_time:14864ms step_avg:92.32ms
step:162/1670 train_time:14956ms step_avg:92.32ms
step:163/1670 train_time:15047ms step_avg:92.31ms
step:164/1670 train_time:15139ms step_avg:92.31ms
step:165/1670 train_time:15231ms step_avg:92.31ms
step:166/1670 train_time:15322ms step_avg:92.30ms
step:167/1670 train_time:15413ms step_avg:92.30ms
step:168/1670 train_time:15504ms step_avg:92.29ms
step:169/1670 train_time:15595ms step_avg:92.28ms
step:170/1670 train_time:15686ms step_avg:92.27ms
step:171/1670 train_time:15776ms step_avg:92.26ms
step:172/1670 train_time:15868ms step_avg:92.25ms
step:173/1670 train_time:15959ms step_avg:92.25ms
step:174/1670 train_time:16051ms step_avg:92.25ms
step:175/1670 train_time:16141ms step_avg:92.24ms
step:176/1670 train_time:16233ms step_avg:92.24ms
step:177/1670 train_time:16325ms step_avg:92.23ms
step:178/1670 train_time:16417ms step_avg:92.23ms
step:179/1670 train_time:16509ms step_avg:92.23ms
step:180/1670 train_time:16600ms step_avg:92.22ms
step:181/1670 train_time:16692ms step_avg:92.22ms
step:182/1670 train_time:16783ms step_avg:92.21ms
step:183/1670 train_time:16875ms step_avg:92.21ms
step:184/1670 train_time:16965ms step_avg:92.20ms
step:185/1670 train_time:17056ms step_avg:92.20ms
step:186/1670 train_time:17148ms step_avg:92.19ms
step:187/1670 train_time:17239ms step_avg:92.19ms
step:188/1670 train_time:17330ms step_avg:92.18ms
step:189/1670 train_time:17421ms step_avg:92.17ms
step:190/1670 train_time:17512ms step_avg:92.17ms
step:191/1670 train_time:17603ms step_avg:92.16ms
step:192/1670 train_time:17695ms step_avg:92.16ms
step:193/1670 train_time:17787ms step_avg:92.16ms
step:194/1670 train_time:17878ms step_avg:92.16ms
step:195/1670 train_time:17969ms step_avg:92.15ms
step:196/1670 train_time:18060ms step_avg:92.14ms
step:197/1670 train_time:18151ms step_avg:92.14ms
step:198/1670 train_time:18241ms step_avg:92.13ms
step:199/1670 train_time:18332ms step_avg:92.12ms
step:200/1670 train_time:18423ms step_avg:92.11ms
step:201/1670 train_time:18515ms step_avg:92.11ms
step:202/1670 train_time:18605ms step_avg:92.11ms
step:203/1670 train_time:18697ms step_avg:92.10ms
step:204/1670 train_time:18788ms step_avg:92.10ms
step:205/1670 train_time:18879ms step_avg:92.09ms
step:206/1670 train_time:18970ms step_avg:92.09ms
step:207/1670 train_time:19060ms step_avg:92.08ms
step:208/1670 train_time:19151ms step_avg:92.07ms
step:209/1670 train_time:19241ms step_avg:92.06ms
step:210/1670 train_time:19332ms step_avg:92.06ms
step:211/1670 train_time:19423ms step_avg:92.05ms
step:212/1670 train_time:19515ms step_avg:92.05ms
step:213/1670 train_time:19762ms step_avg:92.78ms
step:214/1670 train_time:19834ms step_avg:92.68ms
step:215/1670 train_time:19923ms step_avg:92.67ms
step:216/1670 train_time:20013ms step_avg:92.65ms
step:217/1670 train_time:20103ms step_avg:92.64ms
step:218/1670 train_time:20193ms step_avg:92.63ms
step:219/1670 train_time:20284ms step_avg:92.62ms
step:220/1670 train_time:20374ms step_avg:92.61ms
step:221/1670 train_time:20464ms step_avg:92.60ms
step:222/1670 train_time:20554ms step_avg:92.59ms
step:223/1670 train_time:20648ms step_avg:92.59ms
step:224/1670 train_time:20743ms step_avg:92.60ms
step:225/1670 train_time:20837ms step_avg:92.61ms
step:226/1670 train_time:20929ms step_avg:92.61ms
step:227/1670 train_time:21019ms step_avg:92.59ms
step:228/1670 train_time:21110ms step_avg:92.59ms
step:229/1670 train_time:21200ms step_avg:92.58ms
step:230/1670 train_time:21290ms step_avg:92.57ms
step:231/1670 train_time:21380ms step_avg:92.55ms
step:232/1670 train_time:21470ms step_avg:92.54ms
step:233/1670 train_time:21561ms step_avg:92.53ms
step:234/1670 train_time:21655ms step_avg:92.54ms
step:235/1670 train_time:21748ms step_avg:92.54ms
step:236/1670 train_time:21841ms step_avg:92.55ms
step:237/1670 train_time:21934ms step_avg:92.55ms
step:238/1670 train_time:22024ms step_avg:92.54ms
step:239/1670 train_time:22115ms step_avg:92.53ms
step:240/1670 train_time:22206ms step_avg:92.53ms
step:241/1670 train_time:22297ms step_avg:92.52ms
step:242/1670 train_time:22388ms step_avg:92.51ms
step:243/1670 train_time:22478ms step_avg:92.50ms
step:244/1670 train_time:22570ms step_avg:92.50ms
step:245/1670 train_time:22662ms step_avg:92.50ms
step:246/1670 train_time:22756ms step_avg:92.50ms
step:247/1670 train_time:22847ms step_avg:92.50ms
step:248/1670 train_time:22939ms step_avg:92.50ms
step:249/1670 train_time:23030ms step_avg:92.49ms
step:250/1670 train_time:23121ms step_avg:92.49ms
step:250/1670 val_loss:3.9621 train_time:23212ms step_avg:92.85ms
step:251/1670 train_time:23231ms step_avg:92.55ms
step:252/1670 train_time:23304ms step_avg:92.48ms
step:253/1670 train_time:23396ms step_avg:92.48ms
step:254/1670 train_time:23488ms step_avg:92.47ms
step:255/1670 train_time:23578ms step_avg:92.46ms
step:256/1670 train_time:23669ms step_avg:92.46ms
step:257/1670 train_time:23759ms step_avg:92.45ms
step:258/1670 train_time:23849ms step_avg:92.44ms
step:259/1670 train_time:23940ms step_avg:92.43ms
step:260/1670 train_time:24031ms step_avg:92.43ms
step:261/1670 train_time:24123ms step_avg:92.42ms
step:262/1670 train_time:24216ms step_avg:92.43ms
step:263/1670 train_time:24309ms step_avg:92.43ms
step:264/1670 train_time:24400ms step_avg:92.42ms
step:265/1670 train_time:24492ms step_avg:92.42ms
step:266/1670 train_time:24584ms step_avg:92.42ms
step:267/1670 train_time:24675ms step_avg:92.41ms
step:268/1670 train_time:24765ms step_avg:92.41ms
step:269/1670 train_time:24854ms step_avg:92.40ms
step:270/1670 train_time:24945ms step_avg:92.39ms
step:271/1670 train_time:25036ms step_avg:92.38ms
step:272/1670 train_time:25129ms step_avg:92.39ms
step:273/1670 train_time:25221ms step_avg:92.39ms
step:274/1670 train_time:25316ms step_avg:92.39ms
step:275/1670 train_time:25408ms step_avg:92.39ms
step:276/1670 train_time:25498ms step_avg:92.38ms
step:277/1670 train_time:25590ms step_avg:92.38ms
step:278/1670 train_time:25681ms step_avg:92.38ms
step:279/1670 train_time:25772ms step_avg:92.37ms
step:280/1670 train_time:25863ms step_avg:92.37ms
step:281/1670 train_time:25954ms step_avg:92.36ms
step:282/1670 train_time:26045ms step_avg:92.36ms
step:283/1670 train_time:26136ms step_avg:92.35ms
step:284/1670 train_time:26228ms step_avg:92.35ms
step:285/1670 train_time:26319ms step_avg:92.35ms
step:286/1670 train_time:26412ms step_avg:92.35ms
step:287/1670 train_time:26503ms step_avg:92.35ms
step:288/1670 train_time:26595ms step_avg:92.34ms
step:289/1670 train_time:26685ms step_avg:92.34ms
step:290/1670 train_time:26776ms step_avg:92.33ms
step:291/1670 train_time:26867ms step_avg:92.33ms
step:292/1670 train_time:26957ms step_avg:92.32ms
step:293/1670 train_time:27048ms step_avg:92.31ms
step:294/1670 train_time:27139ms step_avg:92.31ms
step:295/1670 train_time:27230ms step_avg:92.31ms
step:296/1670 train_time:27322ms step_avg:92.30ms
step:297/1670 train_time:27414ms step_avg:92.30ms
step:298/1670 train_time:27506ms step_avg:92.30ms
step:299/1670 train_time:27597ms step_avg:92.30ms
step:300/1670 train_time:27689ms step_avg:92.30ms
step:301/1670 train_time:27780ms step_avg:92.29ms
step:302/1670 train_time:27872ms step_avg:92.29ms
step:303/1670 train_time:27963ms step_avg:92.29ms
step:304/1670 train_time:28053ms step_avg:92.28ms
step:305/1670 train_time:28145ms step_avg:92.28ms
step:306/1670 train_time:28236ms step_avg:92.27ms
step:307/1670 train_time:28328ms step_avg:92.28ms
step:308/1670 train_time:28419ms step_avg:92.27ms
step:309/1670 train_time:28511ms step_avg:92.27ms
step:310/1670 train_time:28602ms step_avg:92.26ms
step:311/1670 train_time:28695ms step_avg:92.27ms
step:312/1670 train_time:28786ms step_avg:92.26ms
step:313/1670 train_time:28877ms step_avg:92.26ms
step:314/1670 train_time:28968ms step_avg:92.25ms
step:315/1670 train_time:29058ms step_avg:92.25ms
step:316/1670 train_time:29149ms step_avg:92.24ms
step:317/1670 train_time:29240ms step_avg:92.24ms
step:318/1670 train_time:29331ms step_avg:92.24ms
step:319/1670 train_time:29423ms step_avg:92.23ms
step:320/1670 train_time:29514ms step_avg:92.23ms
step:321/1670 train_time:29605ms step_avg:92.23ms
step:322/1670 train_time:29696ms step_avg:92.22ms
step:323/1670 train_time:29789ms step_avg:92.23ms
step:324/1670 train_time:29880ms step_avg:92.22ms
step:325/1670 train_time:29970ms step_avg:92.22ms
step:326/1670 train_time:30060ms step_avg:92.21ms
step:327/1670 train_time:30152ms step_avg:92.21ms
step:328/1670 train_time:30243ms step_avg:92.20ms
step:329/1670 train_time:30334ms step_avg:92.20ms
step:330/1670 train_time:30426ms step_avg:92.20ms
step:331/1670 train_time:30517ms step_avg:92.20ms
step:332/1670 train_time:30610ms step_avg:92.20ms
step:333/1670 train_time:30700ms step_avg:92.19ms
step:334/1670 train_time:30792ms step_avg:92.19ms
step:335/1670 train_time:30883ms step_avg:92.19ms
step:336/1670 train_time:30974ms step_avg:92.19ms
step:337/1670 train_time:31065ms step_avg:92.18ms
step:338/1670 train_time:31156ms step_avg:92.18ms
step:339/1670 train_time:31247ms step_avg:92.17ms
step:340/1670 train_time:31338ms step_avg:92.17ms
step:341/1670 train_time:31430ms step_avg:92.17ms
step:342/1670 train_time:31520ms step_avg:92.16ms
step:343/1670 train_time:31613ms step_avg:92.17ms
step:344/1670 train_time:31704ms step_avg:92.16ms
step:345/1670 train_time:31796ms step_avg:92.16ms
step:346/1670 train_time:31887ms step_avg:92.16ms
step:347/1670 train_time:31977ms step_avg:92.15ms
step:348/1670 train_time:32069ms step_avg:92.15ms
step:349/1670 train_time:32159ms step_avg:92.15ms
step:350/1670 train_time:32250ms step_avg:92.14ms
step:351/1670 train_time:32341ms step_avg:92.14ms
step:352/1670 train_time:32432ms step_avg:92.14ms
step:353/1670 train_time:32523ms step_avg:92.13ms
step:354/1670 train_time:32615ms step_avg:92.13ms
step:355/1670 train_time:32707ms step_avg:92.13ms
step:356/1670 train_time:32797ms step_avg:92.13ms
step:357/1670 train_time:32888ms step_avg:92.12ms
step:358/1670 train_time:32978ms step_avg:92.12ms
step:359/1670 train_time:33069ms step_avg:92.11ms
step:360/1670 train_time:33161ms step_avg:92.11ms
step:361/1670 train_time:33252ms step_avg:92.11ms
step:362/1670 train_time:33344ms step_avg:92.11ms
step:363/1670 train_time:33434ms step_avg:92.11ms
step:364/1670 train_time:33525ms step_avg:92.10ms
step:365/1670 train_time:33617ms step_avg:92.10ms
step:366/1670 train_time:33709ms step_avg:92.10ms
step:367/1670 train_time:33800ms step_avg:92.10ms
step:368/1670 train_time:33891ms step_avg:92.10ms
step:369/1670 train_time:33982ms step_avg:92.09ms
step:370/1670 train_time:34074ms step_avg:92.09ms
step:371/1670 train_time:34166ms step_avg:92.09ms
step:372/1670 train_time:34257ms step_avg:92.09ms
step:373/1670 train_time:34348ms step_avg:92.09ms
step:374/1670 train_time:34440ms step_avg:92.08ms
step:375/1670 train_time:34531ms step_avg:92.08ms
step:375/1670 val_loss:3.8126 train_time:34621ms step_avg:92.32ms
step:376/1670 train_time:34642ms step_avg:92.13ms
step:377/1670 train_time:34716ms step_avg:92.08ms
step:378/1670 train_time:34807ms step_avg:92.08ms
step:379/1670 train_time:34898ms step_avg:92.08ms
step:380/1670 train_time:34989ms step_avg:92.08ms
step:381/1670 train_time:35081ms step_avg:92.08ms
step:382/1670 train_time:35172ms step_avg:92.07ms
step:383/1670 train_time:35263ms step_avg:92.07ms
step:384/1670 train_time:35355ms step_avg:92.07ms
step:385/1670 train_time:35446ms step_avg:92.07ms
step:386/1670 train_time:35537ms step_avg:92.07ms
step:387/1670 train_time:35630ms step_avg:92.07ms
step:388/1670 train_time:35723ms step_avg:92.07ms
step:389/1670 train_time:35815ms step_avg:92.07ms
step:390/1670 train_time:35904ms step_avg:92.06ms
step:391/1670 train_time:35995ms step_avg:92.06ms
step:392/1670 train_time:36086ms step_avg:92.06ms
step:393/1670 train_time:36177ms step_avg:92.05ms
step:394/1670 train_time:36267ms step_avg:92.05ms
step:395/1670 train_time:36360ms step_avg:92.05ms
step:396/1670 train_time:36450ms step_avg:92.05ms
step:397/1670 train_time:36543ms step_avg:92.05ms
step:398/1670 train_time:36635ms step_avg:92.05ms
step:399/1670 train_time:36726ms step_avg:92.04ms
step:400/1670 train_time:36817ms step_avg:92.04ms
step:401/1670 train_time:36908ms step_avg:92.04ms
step:402/1670 train_time:36999ms step_avg:92.04ms
step:403/1670 train_time:37089ms step_avg:92.03ms
step:404/1670 train_time:37183ms step_avg:92.04ms
step:405/1670 train_time:37273ms step_avg:92.03ms
step:406/1670 train_time:37364ms step_avg:92.03ms
step:407/1670 train_time:37456ms step_avg:92.03ms
step:408/1670 train_time:37548ms step_avg:92.03ms
step:409/1670 train_time:37639ms step_avg:92.03ms
step:410/1670 train_time:37730ms step_avg:92.03ms
step:411/1670 train_time:37821ms step_avg:92.02ms
step:412/1670 train_time:37912ms step_avg:92.02ms
step:413/1670 train_time:38003ms step_avg:92.02ms
step:414/1670 train_time:38095ms step_avg:92.02ms
step:415/1670 train_time:38187ms step_avg:92.02ms
step:416/1670 train_time:38279ms step_avg:92.02ms
step:417/1670 train_time:38369ms step_avg:92.01ms
step:418/1670 train_time:38461ms step_avg:92.01ms
step:419/1670 train_time:38552ms step_avg:92.01ms
step:420/1670 train_time:38645ms step_avg:92.01ms
step:421/1670 train_time:38737ms step_avg:92.01ms
step:422/1670 train_time:38827ms step_avg:92.01ms
step:423/1670 train_time:38918ms step_avg:92.00ms
step:424/1670 train_time:39008ms step_avg:92.00ms
step:425/1670 train_time:39260ms step_avg:92.38ms
step:426/1670 train_time:39329ms step_avg:92.32ms
step:427/1670 train_time:39419ms step_avg:92.32ms
step:428/1670 train_time:39509ms step_avg:92.31ms
step:429/1670 train_time:39599ms step_avg:92.30ms
step:430/1670 train_time:39688ms step_avg:92.30ms
step:431/1670 train_time:39778ms step_avg:92.29ms
step:432/1670 train_time:39868ms step_avg:92.29ms
step:433/1670 train_time:39958ms step_avg:92.28ms
step:434/1670 train_time:40048ms step_avg:92.28ms
step:435/1670 train_time:40142ms step_avg:92.28ms
step:436/1670 train_time:40238ms step_avg:92.29ms
step:437/1670 train_time:40331ms step_avg:92.29ms
step:438/1670 train_time:40422ms step_avg:92.29ms
step:439/1670 train_time:40513ms step_avg:92.28ms
step:440/1670 train_time:40603ms step_avg:92.28ms
step:441/1670 train_time:40693ms step_avg:92.27ms
step:442/1670 train_time:40783ms step_avg:92.27ms
step:443/1670 train_time:40874ms step_avg:92.27ms
step:444/1670 train_time:40964ms step_avg:92.26ms
step:445/1670 train_time:41055ms step_avg:92.26ms
step:446/1670 train_time:41148ms step_avg:92.26ms
step:447/1670 train_time:41242ms step_avg:92.26ms
step:448/1670 train_time:41335ms step_avg:92.26ms
step:449/1670 train_time:41426ms step_avg:92.26ms
step:450/1670 train_time:41517ms step_avg:92.26ms
step:451/1670 train_time:41607ms step_avg:92.26ms
step:452/1670 train_time:41698ms step_avg:92.25ms
step:453/1670 train_time:41788ms step_avg:92.25ms
step:454/1670 train_time:41880ms step_avg:92.25ms
step:455/1670 train_time:41970ms step_avg:92.24ms
step:456/1670 train_time:42061ms step_avg:92.24ms
step:457/1670 train_time:42153ms step_avg:92.24ms
step:458/1670 train_time:42247ms step_avg:92.24ms
step:459/1670 train_time:42340ms step_avg:92.24ms
step:460/1670 train_time:42431ms step_avg:92.24ms
step:461/1670 train_time:42522ms step_avg:92.24ms
step:462/1670 train_time:42613ms step_avg:92.24ms
step:463/1670 train_time:42704ms step_avg:92.23ms
step:464/1670 train_time:42795ms step_avg:92.23ms
step:465/1670 train_time:42886ms step_avg:92.23ms
step:466/1670 train_time:42977ms step_avg:92.23ms
step:467/1670 train_time:43069ms step_avg:92.22ms
step:468/1670 train_time:43162ms step_avg:92.23ms
step:469/1670 train_time:43254ms step_avg:92.23ms
step:470/1670 train_time:43347ms step_avg:92.23ms
step:471/1670 train_time:43439ms step_avg:92.23ms
step:472/1670 train_time:43529ms step_avg:92.22ms
step:473/1670 train_time:43621ms step_avg:92.22ms
step:474/1670 train_time:43712ms step_avg:92.22ms
step:475/1670 train_time:43802ms step_avg:92.22ms
step:476/1670 train_time:43892ms step_avg:92.21ms
step:477/1670 train_time:43984ms step_avg:92.21ms
step:478/1670 train_time:44076ms step_avg:92.21ms
step:479/1670 train_time:44167ms step_avg:92.21ms
step:480/1670 train_time:44259ms step_avg:92.21ms
step:481/1670 train_time:44351ms step_avg:92.20ms
step:482/1670 train_time:44442ms step_avg:92.20ms
step:483/1670 train_time:44533ms step_avg:92.20ms
step:484/1670 train_time:44624ms step_avg:92.20ms
step:485/1670 train_time:44715ms step_avg:92.20ms
step:486/1670 train_time:44806ms step_avg:92.19ms
step:487/1670 train_time:44896ms step_avg:92.19ms
step:488/1670 train_time:44987ms step_avg:92.19ms
step:489/1670 train_time:45078ms step_avg:92.18ms
step:490/1670 train_time:45169ms step_avg:92.18ms
step:491/1670 train_time:45260ms step_avg:92.18ms
step:492/1670 train_time:45351ms step_avg:92.18ms
step:493/1670 train_time:45443ms step_avg:92.18ms
step:494/1670 train_time:45534ms step_avg:92.17ms
step:495/1670 train_time:45625ms step_avg:92.17ms
step:496/1670 train_time:45716ms step_avg:92.17ms
step:497/1670 train_time:45806ms step_avg:92.17ms
step:498/1670 train_time:45897ms step_avg:92.16ms
step:499/1670 train_time:45988ms step_avg:92.16ms
step:500/1670 train_time:46079ms step_avg:92.16ms
step:500/1670 val_loss:3.7158 train_time:46170ms step_avg:92.34ms
step:501/1670 train_time:46190ms step_avg:92.20ms
step:502/1670 train_time:46263ms step_avg:92.16ms
step:503/1670 train_time:46356ms step_avg:92.16ms
step:504/1670 train_time:46448ms step_avg:92.16ms
step:505/1670 train_time:46539ms step_avg:92.16ms
step:506/1670 train_time:46630ms step_avg:92.15ms
step:507/1670 train_time:46721ms step_avg:92.15ms
step:508/1670 train_time:46813ms step_avg:92.15ms
step:509/1670 train_time:46904ms step_avg:92.15ms
step:510/1670 train_time:46995ms step_avg:92.15ms
step:511/1670 train_time:47086ms step_avg:92.14ms
step:512/1670 train_time:47178ms step_avg:92.14ms
step:513/1670 train_time:47269ms step_avg:92.14ms
step:514/1670 train_time:47362ms step_avg:92.14ms
step:515/1670 train_time:47454ms step_avg:92.14ms
step:516/1670 train_time:47545ms step_avg:92.14ms
step:517/1670 train_time:47636ms step_avg:92.14ms
step:518/1670 train_time:47727ms step_avg:92.14ms
step:519/1670 train_time:47819ms step_avg:92.14ms
step:520/1670 train_time:47909ms step_avg:92.13ms
step:521/1670 train_time:48000ms step_avg:92.13ms
step:522/1670 train_time:48092ms step_avg:92.13ms
step:523/1670 train_time:48184ms step_avg:92.13ms
step:524/1670 train_time:48275ms step_avg:92.13ms
step:525/1670 train_time:48367ms step_avg:92.13ms
step:526/1670 train_time:48460ms step_avg:92.13ms
step:527/1670 train_time:48550ms step_avg:92.13ms
step:528/1670 train_time:48641ms step_avg:92.12ms
step:529/1670 train_time:48732ms step_avg:92.12ms
step:530/1670 train_time:48823ms step_avg:92.12ms
step:531/1670 train_time:48914ms step_avg:92.12ms
step:532/1670 train_time:49004ms step_avg:92.11ms
step:533/1670 train_time:49095ms step_avg:92.11ms
step:534/1670 train_time:49185ms step_avg:92.11ms
step:535/1670 train_time:49277ms step_avg:92.11ms
step:536/1670 train_time:49368ms step_avg:92.10ms
step:537/1670 train_time:49460ms step_avg:92.10ms
step:538/1670 train_time:49551ms step_avg:92.10ms
step:539/1670 train_time:49642ms step_avg:92.10ms
step:540/1670 train_time:49732ms step_avg:92.10ms
step:541/1670 train_time:49824ms step_avg:92.10ms
step:542/1670 train_time:49916ms step_avg:92.10ms
step:543/1670 train_time:50007ms step_avg:92.09ms
step:544/1670 train_time:50098ms step_avg:92.09ms
step:545/1670 train_time:50188ms step_avg:92.09ms
step:546/1670 train_time:50279ms step_avg:92.09ms
step:547/1670 train_time:50371ms step_avg:92.09ms
step:548/1670 train_time:50462ms step_avg:92.08ms
step:549/1670 train_time:50553ms step_avg:92.08ms
step:550/1670 train_time:50645ms step_avg:92.08ms
step:551/1670 train_time:50737ms step_avg:92.08ms
step:552/1670 train_time:50828ms step_avg:92.08ms
step:553/1670 train_time:50918ms step_avg:92.08ms
step:554/1670 train_time:51009ms step_avg:92.07ms
step:555/1670 train_time:51099ms step_avg:92.07ms
step:556/1670 train_time:51190ms step_avg:92.07ms
step:557/1670 train_time:51281ms step_avg:92.07ms
step:558/1670 train_time:51569ms step_avg:92.42ms
step:559/1670 train_time:51642ms step_avg:92.38ms
step:560/1670 train_time:51734ms step_avg:92.38ms
step:561/1670 train_time:51825ms step_avg:92.38ms
step:562/1670 train_time:51917ms step_avg:92.38ms
step:563/1670 train_time:52007ms step_avg:92.38ms
step:564/1670 train_time:52099ms step_avg:92.37ms
step:565/1670 train_time:52190ms step_avg:92.37ms
step:566/1670 train_time:52281ms step_avg:92.37ms
step:567/1670 train_time:52373ms step_avg:92.37ms
step:568/1670 train_time:52473ms step_avg:92.38ms
step:569/1670 train_time:52569ms step_avg:92.39ms
step:570/1670 train_time:52663ms step_avg:92.39ms
step:571/1670 train_time:52755ms step_avg:92.39ms
step:572/1670 train_time:52847ms step_avg:92.39ms
step:573/1670 train_time:52939ms step_avg:92.39ms
step:574/1670 train_time:53030ms step_avg:92.39ms
step:575/1670 train_time:53122ms step_avg:92.39ms
step:576/1670 train_time:53213ms step_avg:92.38ms
step:577/1670 train_time:53304ms step_avg:92.38ms
step:578/1670 train_time:53399ms step_avg:92.39ms
step:579/1670 train_time:53493ms step_avg:92.39ms
step:580/1670 train_time:53587ms step_avg:92.39ms
step:581/1670 train_time:53680ms step_avg:92.39ms
step:582/1670 train_time:53772ms step_avg:92.39ms
step:583/1670 train_time:53866ms step_avg:92.40ms
step:584/1670 train_time:53959ms step_avg:92.40ms
step:585/1670 train_time:54050ms step_avg:92.39ms
step:586/1670 train_time:54141ms step_avg:92.39ms
step:587/1670 train_time:54232ms step_avg:92.39ms
step:588/1670 train_time:54325ms step_avg:92.39ms
step:589/1670 train_time:54417ms step_avg:92.39ms
step:590/1670 train_time:54510ms step_avg:92.39ms
step:591/1670 train_time:54604ms step_avg:92.39ms
step:592/1670 train_time:54698ms step_avg:92.40ms
step:593/1670 train_time:54791ms step_avg:92.40ms
step:594/1670 train_time:54883ms step_avg:92.40ms
step:595/1670 train_time:54976ms step_avg:92.40ms
step:596/1670 train_time:55068ms step_avg:92.40ms
step:597/1670 train_time:55160ms step_avg:92.40ms
step:598/1670 train_time:55252ms step_avg:92.39ms
step:599/1670 train_time:55345ms step_avg:92.40ms
step:600/1670 train_time:55439ms step_avg:92.40ms
step:601/1670 train_time:55531ms step_avg:92.40ms
step:602/1670 train_time:55624ms step_avg:92.40ms
step:603/1670 train_time:55717ms step_avg:92.40ms
step:604/1670 train_time:55810ms step_avg:92.40ms
step:605/1670 train_time:55902ms step_avg:92.40ms
step:606/1670 train_time:55995ms step_avg:92.40ms
step:607/1670 train_time:56087ms step_avg:92.40ms
step:608/1670 train_time:56178ms step_avg:92.40ms
step:609/1670 train_time:56270ms step_avg:92.40ms
step:610/1670 train_time:56364ms step_avg:92.40ms
step:611/1670 train_time:56457ms step_avg:92.40ms
step:612/1670 train_time:56549ms step_avg:92.40ms
step:613/1670 train_time:56642ms step_avg:92.40ms
step:614/1670 train_time:56734ms step_avg:92.40ms
step:615/1670 train_time:56827ms step_avg:92.40ms
step:616/1670 train_time:56920ms step_avg:92.40ms
step:617/1670 train_time:57011ms step_avg:92.40ms
step:618/1670 train_time:57103ms step_avg:92.40ms
step:619/1670 train_time:57196ms step_avg:92.40ms
step:620/1670 train_time:57288ms step_avg:92.40ms
step:621/1670 train_time:57381ms step_avg:92.40ms
step:622/1670 train_time:57474ms step_avg:92.40ms
step:623/1670 train_time:57567ms step_avg:92.40ms
step:624/1670 train_time:57660ms step_avg:92.40ms
step:625/1670 train_time:57751ms step_avg:92.40ms
step:625/1670 val_loss:3.6125 train_time:57844ms step_avg:92.55ms
step:626/1670 train_time:57865ms step_avg:92.44ms
step:627/1670 train_time:57943ms step_avg:92.41ms
step:628/1670 train_time:58044ms step_avg:92.43ms
step:629/1670 train_time:58138ms step_avg:92.43ms
step:630/1670 train_time:58230ms step_avg:92.43ms
step:631/1670 train_time:58321ms step_avg:92.43ms
step:632/1670 train_time:58412ms step_avg:92.42ms
step:633/1670 train_time:58503ms step_avg:92.42ms
step:634/1670 train_time:58595ms step_avg:92.42ms
step:635/1670 train_time:58686ms step_avg:92.42ms
step:636/1670 train_time:58777ms step_avg:92.42ms
step:637/1670 train_time:58869ms step_avg:92.42ms
step:638/1670 train_time:58966ms step_avg:92.42ms
step:639/1670 train_time:59204ms step_avg:92.65ms
step:640/1670 train_time:59275ms step_avg:92.62ms
step:641/1670 train_time:59366ms step_avg:92.61ms
step:642/1670 train_time:59457ms step_avg:92.61ms
step:643/1670 train_time:59548ms step_avg:92.61ms
step:644/1670 train_time:59640ms step_avg:92.61ms
step:645/1670 train_time:59731ms step_avg:92.61ms
step:646/1670 train_time:59823ms step_avg:92.60ms
step:647/1670 train_time:59914ms step_avg:92.60ms
step:648/1670 train_time:60005ms step_avg:92.60ms
step:649/1670 train_time:60103ms step_avg:92.61ms
step:650/1670 train_time:60200ms step_avg:92.62ms
step:651/1670 train_time:60294ms step_avg:92.62ms
step:652/1670 train_time:60386ms step_avg:92.62ms
step:653/1670 train_time:60478ms step_avg:92.61ms
step:654/1670 train_time:60569ms step_avg:92.61ms
step:655/1670 train_time:60663ms step_avg:92.62ms
step:656/1670 train_time:60754ms step_avg:92.61ms
step:657/1670 train_time:60845ms step_avg:92.61ms
step:658/1670 train_time:60936ms step_avg:92.61ms
step:659/1670 train_time:61029ms step_avg:92.61ms
step:660/1670 train_time:61124ms step_avg:92.61ms
step:661/1670 train_time:61218ms step_avg:92.61ms
step:662/1670 train_time:61311ms step_avg:92.61ms
step:663/1670 train_time:61405ms step_avg:92.62ms
step:664/1670 train_time:61497ms step_avg:92.62ms
step:665/1670 train_time:61588ms step_avg:92.61ms
step:666/1670 train_time:61681ms step_avg:92.61ms
step:667/1670 train_time:61772ms step_avg:92.61ms
step:668/1670 train_time:61864ms step_avg:92.61ms
step:669/1670 train_time:61956ms step_avg:92.61ms
step:670/1670 train_time:62048ms step_avg:92.61ms
step:671/1670 train_time:62142ms step_avg:92.61ms
step:672/1670 train_time:62236ms step_avg:92.61ms
step:673/1670 train_time:62329ms step_avg:92.61ms
step:674/1670 train_time:62422ms step_avg:92.61ms
step:675/1670 train_time:62514ms step_avg:92.61ms
step:676/1670 train_time:62606ms step_avg:92.61ms
step:677/1670 train_time:62699ms step_avg:92.61ms
step:678/1670 train_time:62790ms step_avg:92.61ms
step:679/1670 train_time:62882ms step_avg:92.61ms
step:680/1670 train_time:62974ms step_avg:92.61ms
step:681/1670 train_time:63066ms step_avg:92.61ms
step:682/1670 train_time:63159ms step_avg:92.61ms
step:683/1670 train_time:63252ms step_avg:92.61ms
step:684/1670 train_time:63345ms step_avg:92.61ms
step:685/1670 train_time:63438ms step_avg:92.61ms
step:686/1670 train_time:63529ms step_avg:92.61ms
step:687/1670 train_time:63622ms step_avg:92.61ms
step:688/1670 train_time:63714ms step_avg:92.61ms
step:689/1670 train_time:63806ms step_avg:92.61ms
step:690/1670 train_time:63898ms step_avg:92.61ms
step:691/1670 train_time:63991ms step_avg:92.61ms
step:692/1670 train_time:64084ms step_avg:92.61ms
step:693/1670 train_time:64176ms step_avg:92.61ms
step:694/1670 train_time:64269ms step_avg:92.61ms
step:695/1670 train_time:64363ms step_avg:92.61ms
step:696/1670 train_time:64455ms step_avg:92.61ms
step:697/1670 train_time:64546ms step_avg:92.61ms
step:698/1670 train_time:64639ms step_avg:92.61ms
step:699/1670 train_time:64731ms step_avg:92.60ms
step:700/1670 train_time:64823ms step_avg:92.60ms
step:701/1670 train_time:64916ms step_avg:92.60ms
step:702/1670 train_time:65008ms step_avg:92.60ms
step:703/1670 train_time:65101ms step_avg:92.60ms
step:704/1670 train_time:65194ms step_avg:92.60ms
step:705/1670 train_time:65287ms step_avg:92.61ms
step:706/1670 train_time:65380ms step_avg:92.61ms
step:707/1670 train_time:65474ms step_avg:92.61ms
step:708/1670 train_time:65567ms step_avg:92.61ms
step:709/1670 train_time:65660ms step_avg:92.61ms
step:710/1670 train_time:65752ms step_avg:92.61ms
step:711/1670 train_time:65845ms step_avg:92.61ms
step:712/1670 train_time:65938ms step_avg:92.61ms
step:713/1670 train_time:66030ms step_avg:92.61ms
step:714/1670 train_time:66123ms step_avg:92.61ms
step:715/1670 train_time:66215ms step_avg:92.61ms
step:716/1670 train_time:66308ms step_avg:92.61ms
step:717/1670 train_time:66400ms step_avg:92.61ms
step:718/1670 train_time:66492ms step_avg:92.61ms
step:719/1670 train_time:66586ms step_avg:92.61ms
step:720/1670 train_time:66679ms step_avg:92.61ms
step:721/1670 train_time:66771ms step_avg:92.61ms
step:722/1670 train_time:66863ms step_avg:92.61ms
step:723/1670 train_time:66956ms step_avg:92.61ms
step:724/1670 train_time:67047ms step_avg:92.61ms
step:725/1670 train_time:67140ms step_avg:92.61ms
step:726/1670 train_time:67232ms step_avg:92.61ms
step:727/1670 train_time:67326ms step_avg:92.61ms
step:728/1670 train_time:67418ms step_avg:92.61ms
step:729/1670 train_time:67510ms step_avg:92.61ms
step:730/1670 train_time:67603ms step_avg:92.61ms
step:731/1670 train_time:67694ms step_avg:92.61ms
step:732/1670 train_time:67787ms step_avg:92.61ms
step:733/1670 train_time:67881ms step_avg:92.61ms
step:734/1670 train_time:67973ms step_avg:92.61ms
step:735/1670 train_time:68066ms step_avg:92.61ms
step:736/1670 train_time:68158ms step_avg:92.61ms
step:737/1670 train_time:68250ms step_avg:92.61ms
step:738/1670 train_time:68343ms step_avg:92.61ms
step:739/1670 train_time:68436ms step_avg:92.61ms
step:740/1670 train_time:68528ms step_avg:92.61ms
step:741/1670 train_time:68620ms step_avg:92.61ms
step:742/1670 train_time:68712ms step_avg:92.60ms
step:743/1670 train_time:68805ms step_avg:92.60ms
step:744/1670 train_time:68898ms step_avg:92.60ms
step:745/1670 train_time:68990ms step_avg:92.60ms
step:746/1670 train_time:69083ms step_avg:92.60ms
step:747/1670 train_time:69175ms step_avg:92.60ms
step:748/1670 train_time:69267ms step_avg:92.60ms
step:749/1670 train_time:69360ms step_avg:92.60ms
step:750/1670 train_time:69452ms step_avg:92.60ms
step:750/1670 val_loss:3.5618 train_time:69544ms step_avg:92.73ms
step:751/1670 train_time:69563ms step_avg:92.63ms
step:752/1670 train_time:69640ms step_avg:92.61ms
step:753/1670 train_time:69732ms step_avg:92.61ms
step:754/1670 train_time:69824ms step_avg:92.60ms
step:755/1670 train_time:69916ms step_avg:92.60ms
step:756/1670 train_time:70007ms step_avg:92.60ms
step:757/1670 train_time:70099ms step_avg:92.60ms
step:758/1670 train_time:70193ms step_avg:92.60ms
step:759/1670 train_time:70284ms step_avg:92.60ms
step:760/1670 train_time:70376ms step_avg:92.60ms
step:761/1670 train_time:70469ms step_avg:92.60ms
step:762/1670 train_time:70563ms step_avg:92.60ms
step:763/1670 train_time:70659ms step_avg:92.61ms
step:764/1670 train_time:70752ms step_avg:92.61ms
step:765/1670 train_time:70844ms step_avg:92.61ms
step:766/1670 train_time:70936ms step_avg:92.61ms
step:767/1670 train_time:71028ms step_avg:92.61ms
step:768/1670 train_time:71120ms step_avg:92.60ms
step:769/1670 train_time:71212ms step_avg:92.60ms
step:770/1670 train_time:71303ms step_avg:92.60ms
step:771/1670 train_time:71395ms step_avg:92.60ms
step:772/1670 train_time:71488ms step_avg:92.60ms
step:773/1670 train_time:71582ms step_avg:92.60ms
step:774/1670 train_time:71677ms step_avg:92.61ms
step:775/1670 train_time:71770ms step_avg:92.61ms
step:776/1670 train_time:71863ms step_avg:92.61ms
step:777/1670 train_time:71956ms step_avg:92.61ms
step:778/1670 train_time:72048ms step_avg:92.61ms
step:779/1670 train_time:72141ms step_avg:92.61ms
step:780/1670 train_time:72233ms step_avg:92.61ms
step:781/1670 train_time:72325ms step_avg:92.61ms
step:782/1670 train_time:72416ms step_avg:92.60ms
step:783/1670 train_time:72510ms step_avg:92.61ms
step:784/1670 train_time:72603ms step_avg:92.61ms
step:785/1670 train_time:72696ms step_avg:92.61ms
step:786/1670 train_time:72788ms step_avg:92.61ms
step:787/1670 train_time:72882ms step_avg:92.61ms
step:788/1670 train_time:72974ms step_avg:92.61ms
step:789/1670 train_time:73066ms step_avg:92.61ms
step:790/1670 train_time:73158ms step_avg:92.61ms
step:791/1670 train_time:73251ms step_avg:92.61ms
step:792/1670 train_time:73343ms step_avg:92.60ms
step:793/1670 train_time:73436ms step_avg:92.61ms
step:794/1670 train_time:73529ms step_avg:92.61ms
step:795/1670 train_time:73621ms step_avg:92.61ms
step:796/1670 train_time:73714ms step_avg:92.61ms
step:797/1670 train_time:73806ms step_avg:92.60ms
step:798/1670 train_time:73899ms step_avg:92.60ms
step:799/1670 train_time:73991ms step_avg:92.60ms
step:800/1670 train_time:74083ms step_avg:92.60ms
step:801/1670 train_time:74175ms step_avg:92.60ms
step:802/1670 train_time:74267ms step_avg:92.60ms
step:803/1670 train_time:74359ms step_avg:92.60ms
step:804/1670 train_time:74452ms step_avg:92.60ms
step:805/1670 train_time:74545ms step_avg:92.60ms
step:806/1670 train_time:74638ms step_avg:92.60ms
step:807/1670 train_time:74730ms step_avg:92.60ms
step:808/1670 train_time:74822ms step_avg:92.60ms
step:809/1670 train_time:74914ms step_avg:92.60ms
step:810/1670 train_time:75006ms step_avg:92.60ms
step:811/1670 train_time:75099ms step_avg:92.60ms
step:812/1670 train_time:75191ms step_avg:92.60ms
step:813/1670 train_time:75282ms step_avg:92.60ms
step:814/1670 train_time:75374ms step_avg:92.60ms
step:815/1670 train_time:75466ms step_avg:92.60ms
step:816/1670 train_time:75561ms step_avg:92.60ms
step:817/1670 train_time:75654ms step_avg:92.60ms
step:818/1670 train_time:75745ms step_avg:92.60ms
step:819/1670 train_time:75838ms step_avg:92.60ms
step:820/1670 train_time:75930ms step_avg:92.60ms
step:821/1670 train_time:76022ms step_avg:92.60ms
step:822/1670 train_time:76116ms step_avg:92.60ms
step:823/1670 train_time:76208ms step_avg:92.60ms
step:824/1670 train_time:76301ms step_avg:92.60ms
step:825/1670 train_time:76394ms step_avg:92.60ms
step:826/1670 train_time:76486ms step_avg:92.60ms
step:827/1670 train_time:76580ms step_avg:92.60ms
step:828/1670 train_time:76673ms step_avg:92.60ms
step:829/1670 train_time:76764ms step_avg:92.60ms
step:830/1670 train_time:76857ms step_avg:92.60ms
step:831/1670 train_time:76950ms step_avg:92.60ms
step:832/1670 train_time:77043ms step_avg:92.60ms
step:833/1670 train_time:77135ms step_avg:92.60ms
step:834/1670 train_time:77227ms step_avg:92.60ms
step:835/1670 train_time:77320ms step_avg:92.60ms
step:836/1670 train_time:77413ms step_avg:92.60ms
step:837/1670 train_time:77505ms step_avg:92.60ms
step:838/1670 train_time:77599ms step_avg:92.60ms
step:839/1670 train_time:77692ms step_avg:92.60ms
step:840/1670 train_time:77784ms step_avg:92.60ms
step:841/1670 train_time:77877ms step_avg:92.60ms
step:842/1670 train_time:77969ms step_avg:92.60ms
step:843/1670 train_time:78063ms step_avg:92.60ms
step:844/1670 train_time:78156ms step_avg:92.60ms
step:845/1670 train_time:78247ms step_avg:92.60ms
step:846/1670 train_time:78341ms step_avg:92.60ms
step:847/1670 train_time:78434ms step_avg:92.60ms
step:848/1670 train_time:78525ms step_avg:92.60ms
step:849/1670 train_time:78620ms step_avg:92.60ms
step:850/1670 train_time:78713ms step_avg:92.60ms
step:851/1670 train_time:78964ms step_avg:92.79ms
step:852/1670 train_time:79035ms step_avg:92.76ms
step:853/1670 train_time:79125ms step_avg:92.76ms
step:854/1670 train_time:79217ms step_avg:92.76ms
step:855/1670 train_time:79308ms step_avg:92.76ms
step:856/1670 train_time:79399ms step_avg:92.76ms
step:857/1670 train_time:79491ms step_avg:92.75ms
step:858/1670 train_time:79582ms step_avg:92.75ms
step:859/1670 train_time:79673ms step_avg:92.75ms
step:860/1670 train_time:79764ms step_avg:92.75ms
step:861/1670 train_time:79867ms step_avg:92.76ms
step:862/1670 train_time:79964ms step_avg:92.77ms
step:863/1670 train_time:80057ms step_avg:92.77ms
step:864/1670 train_time:80149ms step_avg:92.76ms
step:865/1670 train_time:80240ms step_avg:92.76ms
step:866/1670 train_time:80331ms step_avg:92.76ms
step:867/1670 train_time:80422ms step_avg:92.76ms
step:868/1670 train_time:80513ms step_avg:92.76ms
step:869/1670 train_time:80604ms step_avg:92.75ms
step:870/1670 train_time:80695ms step_avg:92.75ms
step:871/1670 train_time:80790ms step_avg:92.76ms
step:872/1670 train_time:80885ms step_avg:92.76ms
step:873/1670 train_time:80981ms step_avg:92.76ms
step:874/1670 train_time:81075ms step_avg:92.76ms
step:875/1670 train_time:81167ms step_avg:92.76ms
step:875/1670 val_loss:3.5199 train_time:81260ms step_avg:92.87ms
step:876/1670 train_time:81278ms step_avg:92.78ms
step:877/1670 train_time:81353ms step_avg:92.76ms
step:878/1670 train_time:81446ms step_avg:92.76ms
step:879/1670 train_time:81538ms step_avg:92.76ms
step:880/1670 train_time:81629ms step_avg:92.76ms
step:881/1670 train_time:81721ms step_avg:92.76ms
step:882/1670 train_time:81812ms step_avg:92.76ms
step:883/1670 train_time:81904ms step_avg:92.76ms
step:884/1670 train_time:81997ms step_avg:92.76ms
step:885/1670 train_time:82091ms step_avg:92.76ms
step:886/1670 train_time:82184ms step_avg:92.76ms
step:887/1670 train_time:82278ms step_avg:92.76ms
step:888/1670 train_time:82372ms step_avg:92.76ms
step:889/1670 train_time:82464ms step_avg:92.76ms
step:890/1670 train_time:82556ms step_avg:92.76ms
step:891/1670 train_time:82647ms step_avg:92.76ms
step:892/1670 train_time:82741ms step_avg:92.76ms
step:893/1670 train_time:82833ms step_avg:92.76ms
step:894/1670 train_time:82925ms step_avg:92.76ms
step:895/1670 train_time:83018ms step_avg:92.76ms
step:896/1670 train_time:83110ms step_avg:92.76ms
step:897/1670 train_time:83204ms step_avg:92.76ms
step:898/1670 train_time:83297ms step_avg:92.76ms
step:899/1670 train_time:83390ms step_avg:92.76ms
step:900/1670 train_time:83483ms step_avg:92.76ms
step:901/1670 train_time:83576ms step_avg:92.76ms
step:902/1670 train_time:83668ms step_avg:92.76ms
step:903/1670 train_time:83760ms step_avg:92.76ms
step:904/1670 train_time:83851ms step_avg:92.76ms
step:905/1670 train_time:83944ms step_avg:92.76ms
step:906/1670 train_time:84036ms step_avg:92.75ms
step:907/1670 train_time:84127ms step_avg:92.75ms
step:908/1670 train_time:84222ms step_avg:92.76ms
step:909/1670 train_time:84315ms step_avg:92.76ms
step:910/1670 train_time:84406ms step_avg:92.75ms
step:911/1670 train_time:84500ms step_avg:92.75ms
step:912/1670 train_time:84592ms step_avg:92.75ms
step:913/1670 train_time:84684ms step_avg:92.75ms
step:914/1670 train_time:84776ms step_avg:92.75ms
step:915/1670 train_time:84868ms step_avg:92.75ms
step:916/1670 train_time:84960ms step_avg:92.75ms
step:917/1670 train_time:85053ms step_avg:92.75ms
step:918/1670 train_time:85145ms step_avg:92.75ms
step:919/1670 train_time:85237ms step_avg:92.75ms
step:920/1670 train_time:85329ms step_avg:92.75ms
step:921/1670 train_time:85422ms step_avg:92.75ms
step:922/1670 train_time:85515ms step_avg:92.75ms
step:923/1670 train_time:85608ms step_avg:92.75ms
step:924/1670 train_time:85701ms step_avg:92.75ms
step:925/1670 train_time:85793ms step_avg:92.75ms
step:926/1670 train_time:85885ms step_avg:92.75ms
step:927/1670 train_time:85977ms step_avg:92.75ms
step:928/1670 train_time:86069ms step_avg:92.75ms
step:929/1670 train_time:86162ms step_avg:92.75ms
step:930/1670 train_time:86255ms step_avg:92.75ms
step:931/1670 train_time:86347ms step_avg:92.75ms
step:932/1670 train_time:86440ms step_avg:92.75ms
step:933/1670 train_time:86532ms step_avg:92.75ms
step:934/1670 train_time:86626ms step_avg:92.75ms
step:935/1670 train_time:86719ms step_avg:92.75ms
step:936/1670 train_time:86810ms step_avg:92.75ms
step:937/1670 train_time:86903ms step_avg:92.75ms
step:938/1670 train_time:86995ms step_avg:92.75ms
step:939/1670 train_time:87088ms step_avg:92.75ms
step:940/1670 train_time:87181ms step_avg:92.75ms
step:941/1670 train_time:87274ms step_avg:92.75ms
step:942/1670 train_time:87366ms step_avg:92.75ms
step:943/1670 train_time:87458ms step_avg:92.74ms
step:944/1670 train_time:87551ms step_avg:92.75ms
step:945/1670 train_time:87644ms step_avg:92.75ms
step:946/1670 train_time:87737ms step_avg:92.74ms
step:947/1670 train_time:87828ms step_avg:92.74ms
step:948/1670 train_time:87921ms step_avg:92.74ms
step:949/1670 train_time:88014ms step_avg:92.74ms
step:950/1670 train_time:88106ms step_avg:92.74ms
step:951/1670 train_time:88199ms step_avg:92.74ms
step:952/1670 train_time:88291ms step_avg:92.74ms
step:953/1670 train_time:88383ms step_avg:92.74ms
step:954/1670 train_time:88476ms step_avg:92.74ms
step:955/1670 train_time:88568ms step_avg:92.74ms
step:956/1670 train_time:88661ms step_avg:92.74ms
step:957/1670 train_time:88755ms step_avg:92.74ms
step:958/1670 train_time:88847ms step_avg:92.74ms
step:959/1670 train_time:88940ms step_avg:92.74ms
step:960/1670 train_time:89032ms step_avg:92.74ms
step:961/1670 train_time:89125ms step_avg:92.74ms
step:962/1670 train_time:89218ms step_avg:92.74ms
step:963/1670 train_time:89310ms step_avg:92.74ms
step:964/1670 train_time:89403ms step_avg:92.74ms
step:965/1670 train_time:89496ms step_avg:92.74ms
step:966/1670 train_time:89588ms step_avg:92.74ms
step:967/1670 train_time:89681ms step_avg:92.74ms
step:968/1670 train_time:89773ms step_avg:92.74ms
step:969/1670 train_time:89865ms step_avg:92.74ms
step:970/1670 train_time:89958ms step_avg:92.74ms
step:971/1670 train_time:90050ms step_avg:92.74ms
step:972/1670 train_time:90144ms step_avg:92.74ms
step:973/1670 train_time:90236ms step_avg:92.74ms
step:974/1670 train_time:90328ms step_avg:92.74ms
step:975/1670 train_time:90421ms step_avg:92.74ms
step:976/1670 train_time:90513ms step_avg:92.74ms
step:977/1670 train_time:90605ms step_avg:92.74ms
step:978/1670 train_time:90698ms step_avg:92.74ms
step:979/1670 train_time:90791ms step_avg:92.74ms
step:980/1670 train_time:90883ms step_avg:92.74ms
step:981/1670 train_time:90976ms step_avg:92.74ms
step:982/1670 train_time:91068ms step_avg:92.74ms
step:983/1670 train_time:91162ms step_avg:92.74ms
step:984/1670 train_time:91254ms step_avg:92.74ms
step:985/1670 train_time:91346ms step_avg:92.74ms
step:986/1670 train_time:91438ms step_avg:92.74ms
step:987/1670 train_time:91529ms step_avg:92.74ms
step:988/1670 train_time:91623ms step_avg:92.74ms
step:989/1670 train_time:91717ms step_avg:92.74ms
step:990/1670 train_time:91809ms step_avg:92.74ms
step:991/1670 train_time:91902ms step_avg:92.74ms
step:992/1670 train_time:91995ms step_avg:92.74ms
step:993/1670 train_time:92088ms step_avg:92.74ms
step:994/1670 train_time:92181ms step_avg:92.74ms
step:995/1670 train_time:92273ms step_avg:92.74ms
step:996/1670 train_time:92365ms step_avg:92.74ms
step:997/1670 train_time:92457ms step_avg:92.74ms
step:998/1670 train_time:92550ms step_avg:92.74ms
step:999/1670 train_time:92643ms step_avg:92.74ms
step:1000/1670 train_time:92736ms step_avg:92.74ms
step:1000/1670 val_loss:3.4695 train_time:92828ms step_avg:92.83ms
step:1001/1670 train_time:92846ms step_avg:92.75ms
step:1002/1670 train_time:92922ms step_avg:92.74ms
step:1003/1670 train_time:93014ms step_avg:92.74ms
step:1004/1670 train_time:93105ms step_avg:92.73ms
step:1005/1670 train_time:93196ms step_avg:92.73ms
step:1006/1670 train_time:93288ms step_avg:92.73ms
step:1007/1670 train_time:93380ms step_avg:92.73ms
step:1008/1670 train_time:93472ms step_avg:92.73ms
step:1009/1670 train_time:93564ms step_avg:92.73ms
step:1010/1670 train_time:93657ms step_avg:92.73ms
step:1011/1670 train_time:93750ms step_avg:92.73ms
step:1012/1670 train_time:93844ms step_avg:92.73ms
step:1013/1670 train_time:93939ms step_avg:92.73ms
step:1014/1670 train_time:94032ms step_avg:92.73ms
step:1015/1670 train_time:94124ms step_avg:92.73ms
step:1016/1670 train_time:94216ms step_avg:92.73ms
step:1017/1670 train_time:94307ms step_avg:92.73ms
step:1018/1670 train_time:94401ms step_avg:92.73ms
step:1019/1670 train_time:94492ms step_avg:92.73ms
step:1020/1670 train_time:94584ms step_avg:92.73ms
step:1021/1670 train_time:94676ms step_avg:92.73ms
step:1022/1670 train_time:94769ms step_avg:92.73ms
step:1023/1670 train_time:94864ms step_avg:92.73ms
step:1024/1670 train_time:94958ms step_avg:92.73ms
step:1025/1670 train_time:95050ms step_avg:92.73ms
step:1026/1670 train_time:95143ms step_avg:92.73ms
step:1027/1670 train_time:95235ms step_avg:92.73ms
step:1028/1670 train_time:95326ms step_avg:92.73ms
step:1029/1670 train_time:95419ms step_avg:92.73ms
step:1030/1670 train_time:95511ms step_avg:92.73ms
step:1031/1670 train_time:95603ms step_avg:92.73ms
step:1032/1670 train_time:95696ms step_avg:92.73ms
step:1033/1670 train_time:95788ms step_avg:92.73ms
step:1034/1670 train_time:95881ms step_avg:92.73ms
step:1035/1670 train_time:95975ms step_avg:92.73ms
step:1036/1670 train_time:96067ms step_avg:92.73ms
step:1037/1670 train_time:96160ms step_avg:92.73ms
step:1038/1670 train_time:96252ms step_avg:92.73ms
step:1039/1670 train_time:96345ms step_avg:92.73ms
step:1040/1670 train_time:96438ms step_avg:92.73ms
step:1041/1670 train_time:96529ms step_avg:92.73ms
step:1042/1670 train_time:96621ms step_avg:92.73ms
step:1043/1670 train_time:96714ms step_avg:92.73ms
step:1044/1670 train_time:96807ms step_avg:92.73ms
step:1045/1670 train_time:96901ms step_avg:92.73ms
step:1046/1670 train_time:96994ms step_avg:92.73ms
step:1047/1670 train_time:97085ms step_avg:92.73ms
step:1048/1670 train_time:97178ms step_avg:92.73ms
step:1049/1670 train_time:97270ms step_avg:92.73ms
step:1050/1670 train_time:97363ms step_avg:92.73ms
step:1051/1670 train_time:97457ms step_avg:92.73ms
step:1052/1670 train_time:97548ms step_avg:92.73ms
step:1053/1670 train_time:97641ms step_avg:92.73ms
step:1054/1670 train_time:97733ms step_avg:92.73ms
step:1055/1670 train_time:97826ms step_avg:92.73ms
step:1056/1670 train_time:97919ms step_avg:92.73ms
step:1057/1670 train_time:98011ms step_avg:92.73ms
step:1058/1670 train_time:98104ms step_avg:92.73ms
step:1059/1670 train_time:98196ms step_avg:92.73ms
step:1060/1670 train_time:98288ms step_avg:92.72ms
step:1061/1670 train_time:98381ms step_avg:92.72ms
step:1062/1670 train_time:98630ms step_avg:92.87ms
step:1063/1670 train_time:98701ms step_avg:92.85ms
step:1064/1670 train_time:98791ms step_avg:92.85ms
step:1065/1670 train_time:98883ms step_avg:92.85ms
step:1066/1670 train_time:98974ms step_avg:92.85ms
step:1067/1670 train_time:99065ms step_avg:92.84ms
step:1068/1670 train_time:99156ms step_avg:92.84ms
step:1069/1670 train_time:99248ms step_avg:92.84ms
step:1070/1670 train_time:99339ms step_avg:92.84ms
step:1071/1670 train_time:99430ms step_avg:92.84ms
step:1072/1670 train_time:99526ms step_avg:92.84ms
step:1073/1670 train_time:99623ms step_avg:92.85ms
step:1074/1670 train_time:99717ms step_avg:92.85ms
step:1075/1670 train_time:99809ms step_avg:92.85ms
step:1076/1670 train_time:99901ms step_avg:92.85ms
step:1077/1670 train_time:99993ms step_avg:92.84ms
step:1078/1670 train_time:100084ms step_avg:92.84ms
step:1079/1670 train_time:100176ms step_avg:92.84ms
step:1080/1670 train_time:100268ms step_avg:92.84ms
step:1081/1670 train_time:100360ms step_avg:92.84ms
step:1082/1670 train_time:100452ms step_avg:92.84ms
step:1083/1670 train_time:100546ms step_avg:92.84ms
step:1084/1670 train_time:100642ms step_avg:92.84ms
step:1085/1670 train_time:100735ms step_avg:92.84ms
step:1086/1670 train_time:100827ms step_avg:92.84ms
step:1087/1670 train_time:100918ms step_avg:92.84ms
step:1088/1670 train_time:101010ms step_avg:92.84ms
step:1089/1670 train_time:101102ms step_avg:92.84ms
step:1090/1670 train_time:101194ms step_avg:92.84ms
step:1091/1670 train_time:101285ms step_avg:92.84ms
step:1092/1670 train_time:101378ms step_avg:92.84ms
step:1093/1670 train_time:101470ms step_avg:92.84ms
step:1094/1670 train_time:101565ms step_avg:92.84ms
step:1095/1670 train_time:101659ms step_avg:92.84ms
step:1096/1670 train_time:101752ms step_avg:92.84ms
step:1097/1670 train_time:101843ms step_avg:92.84ms
step:1098/1670 train_time:101936ms step_avg:92.84ms
step:1099/1670 train_time:102027ms step_avg:92.84ms
step:1100/1670 train_time:102119ms step_avg:92.84ms
step:1101/1670 train_time:102211ms step_avg:92.84ms
step:1102/1670 train_time:102304ms step_avg:92.83ms
step:1103/1670 train_time:102396ms step_avg:92.83ms
step:1104/1670 train_time:102488ms step_avg:92.83ms
step:1105/1670 train_time:102581ms step_avg:92.83ms
step:1106/1670 train_time:102675ms step_avg:92.83ms
step:1107/1670 train_time:102768ms step_avg:92.83ms
step:1108/1670 train_time:102862ms step_avg:92.84ms
step:1109/1670 train_time:102954ms step_avg:92.84ms
step:1110/1670 train_time:103045ms step_avg:92.83ms
step:1111/1670 train_time:103137ms step_avg:92.83ms
step:1112/1670 train_time:103229ms step_avg:92.83ms
step:1113/1670 train_time:103321ms step_avg:92.83ms
step:1114/1670 train_time:103413ms step_avg:92.83ms
step:1115/1670 train_time:103680ms step_avg:92.99ms
step:1116/1670 train_time:103772ms step_avg:92.99ms
step:1117/1670 train_time:103863ms step_avg:92.98ms
step:1118/1670 train_time:103955ms step_avg:92.98ms
step:1119/1670 train_time:104047ms step_avg:92.98ms
step:1120/1670 train_time:104139ms step_avg:92.98ms
step:1121/1670 train_time:104231ms step_avg:92.98ms
step:1122/1670 train_time:104322ms step_avg:92.98ms
step:1123/1670 train_time:104414ms step_avg:92.98ms
step:1124/1670 train_time:104506ms step_avg:92.98ms
step:1125/1670 train_time:104603ms step_avg:92.98ms
step:1125/1670 val_loss:3.4163 train_time:104704ms step_avg:93.07ms
step:1126/1670 train_time:104724ms step_avg:93.01ms
step:1127/1670 train_time:104804ms step_avg:92.99ms
step:1128/1670 train_time:104902ms step_avg:93.00ms
step:1129/1670 train_time:104996ms step_avg:93.00ms
step:1130/1670 train_time:105088ms step_avg:93.00ms
step:1131/1670 train_time:105180ms step_avg:93.00ms
step:1132/1670 train_time:105272ms step_avg:93.00ms
step:1133/1670 train_time:105364ms step_avg:93.00ms
step:1134/1670 train_time:105457ms step_avg:93.00ms
step:1135/1670 train_time:105550ms step_avg:93.00ms
step:1136/1670 train_time:105644ms step_avg:93.00ms
step:1137/1670 train_time:105741ms step_avg:93.00ms
step:1138/1670 train_time:105838ms step_avg:93.00ms
step:1139/1670 train_time:105931ms step_avg:93.00ms
step:1140/1670 train_time:106025ms step_avg:93.00ms
step:1141/1670 train_time:106117ms step_avg:93.00ms
step:1142/1670 train_time:106210ms step_avg:93.00ms
step:1143/1670 train_time:106301ms step_avg:93.00ms
step:1144/1670 train_time:106393ms step_avg:93.00ms
step:1145/1670 train_time:106485ms step_avg:93.00ms
step:1146/1670 train_time:106580ms step_avg:93.00ms
step:1147/1670 train_time:106674ms step_avg:93.00ms
step:1148/1670 train_time:106768ms step_avg:93.00ms
step:1149/1670 train_time:106864ms step_avg:93.01ms
step:1150/1670 train_time:106960ms step_avg:93.01ms
step:1151/1670 train_time:107053ms step_avg:93.01ms
step:1152/1670 train_time:107145ms step_avg:93.01ms
step:1153/1670 train_time:107237ms step_avg:93.01ms
step:1154/1670 train_time:107330ms step_avg:93.01ms
step:1155/1670 train_time:107423ms step_avg:93.01ms
step:1156/1670 train_time:107517ms step_avg:93.01ms
step:1157/1670 train_time:107610ms step_avg:93.01ms
step:1158/1670 train_time:107703ms step_avg:93.01ms
step:1159/1670 train_time:107797ms step_avg:93.01ms
step:1160/1670 train_time:107892ms step_avg:93.01ms
step:1161/1670 train_time:107985ms step_avg:93.01ms
step:1162/1670 train_time:108079ms step_avg:93.01ms
step:1163/1670 train_time:108172ms step_avg:93.01ms
step:1164/1670 train_time:108264ms step_avg:93.01ms
step:1165/1670 train_time:108358ms step_avg:93.01ms
step:1166/1670 train_time:108451ms step_avg:93.01ms
step:1167/1670 train_time:108543ms step_avg:93.01ms
step:1168/1670 train_time:108638ms step_avg:93.01ms
step:1169/1670 train_time:108732ms step_avg:93.01ms
step:1170/1670 train_time:108825ms step_avg:93.01ms
step:1171/1670 train_time:108920ms step_avg:93.01ms
step:1172/1670 train_time:109013ms step_avg:93.01ms
step:1173/1670 train_time:109106ms step_avg:93.01ms
step:1174/1670 train_time:109198ms step_avg:93.01ms
step:1175/1670 train_time:109292ms step_avg:93.01ms
step:1176/1670 train_time:109385ms step_avg:93.01ms
step:1177/1670 train_time:109479ms step_avg:93.01ms
step:1178/1670 train_time:109571ms step_avg:93.01ms
step:1179/1670 train_time:109665ms step_avg:93.01ms
step:1180/1670 train_time:109759ms step_avg:93.02ms
step:1181/1670 train_time:109853ms step_avg:93.02ms
step:1182/1670 train_time:109946ms step_avg:93.02ms
step:1183/1670 train_time:110040ms step_avg:93.02ms
step:1184/1670 train_time:110133ms step_avg:93.02ms
step:1185/1670 train_time:110225ms step_avg:93.02ms
step:1186/1670 train_time:110318ms step_avg:93.02ms
step:1187/1670 train_time:110410ms step_avg:93.02ms
step:1188/1670 train_time:110503ms step_avg:93.02ms
step:1189/1670 train_time:110599ms step_avg:93.02ms
step:1190/1670 train_time:110690ms step_avg:93.02ms
step:1191/1670 train_time:110784ms step_avg:93.02ms
step:1192/1670 train_time:110877ms step_avg:93.02ms
step:1193/1670 train_time:110970ms step_avg:93.02ms
step:1194/1670 train_time:111063ms step_avg:93.02ms
step:1195/1670 train_time:111156ms step_avg:93.02ms
step:1196/1670 train_time:111248ms step_avg:93.02ms
step:1197/1670 train_time:111341ms step_avg:93.02ms
step:1198/1670 train_time:111434ms step_avg:93.02ms
step:1199/1670 train_time:111526ms step_avg:93.02ms
step:1200/1670 train_time:111619ms step_avg:93.02ms
step:1201/1670 train_time:111713ms step_avg:93.02ms
step:1202/1670 train_time:111807ms step_avg:93.02ms
step:1203/1670 train_time:111901ms step_avg:93.02ms
step:1204/1670 train_time:111995ms step_avg:93.02ms
step:1205/1670 train_time:112088ms step_avg:93.02ms
step:1206/1670 train_time:112181ms step_avg:93.02ms
step:1207/1670 train_time:112275ms step_avg:93.02ms
step:1208/1670 train_time:112368ms step_avg:93.02ms
step:1209/1670 train_time:112462ms step_avg:93.02ms
step:1210/1670 train_time:112556ms step_avg:93.02ms
step:1211/1670 train_time:112649ms step_avg:93.02ms
step:1212/1670 train_time:112742ms step_avg:93.02ms
step:1213/1670 train_time:112835ms step_avg:93.02ms
step:1214/1670 train_time:112928ms step_avg:93.02ms
step:1215/1670 train_time:113022ms step_avg:93.02ms
step:1216/1670 train_time:113115ms step_avg:93.02ms
step:1217/1670 train_time:113208ms step_avg:93.02ms
step:1218/1670 train_time:113301ms step_avg:93.02ms
step:1219/1670 train_time:113394ms step_avg:93.02ms
step:1220/1670 train_time:113487ms step_avg:93.02ms
step:1221/1670 train_time:113581ms step_avg:93.02ms
step:1222/1670 train_time:113675ms step_avg:93.02ms
step:1223/1670 train_time:113767ms step_avg:93.02ms
step:1224/1670 train_time:113861ms step_avg:93.02ms
step:1225/1670 train_time:113954ms step_avg:93.02ms
step:1226/1670 train_time:114047ms step_avg:93.02ms
step:1227/1670 train_time:114141ms step_avg:93.02ms
step:1228/1670 train_time:114234ms step_avg:93.02ms
step:1229/1670 train_time:114327ms step_avg:93.02ms
step:1230/1670 train_time:114422ms step_avg:93.03ms
step:1231/1670 train_time:114513ms step_avg:93.02ms
step:1232/1670 train_time:114606ms step_avg:93.02ms
step:1233/1670 train_time:114700ms step_avg:93.02ms
step:1234/1670 train_time:114793ms step_avg:93.03ms
step:1235/1670 train_time:114886ms step_avg:93.02ms
step:1236/1670 train_time:114979ms step_avg:93.03ms
step:1237/1670 train_time:115072ms step_avg:93.02ms
step:1238/1670 train_time:115165ms step_avg:93.02ms
step:1239/1670 train_time:115258ms step_avg:93.03ms
step:1240/1670 train_time:115353ms step_avg:93.03ms
step:1241/1670 train_time:115446ms step_avg:93.03ms
step:1242/1670 train_time:115538ms step_avg:93.03ms
step:1243/1670 train_time:115632ms step_avg:93.03ms
step:1244/1670 train_time:115725ms step_avg:93.03ms
step:1245/1670 train_time:115820ms step_avg:93.03ms
step:1246/1670 train_time:115913ms step_avg:93.03ms
step:1247/1670 train_time:116005ms step_avg:93.03ms
step:1248/1670 train_time:116098ms step_avg:93.03ms
step:1249/1670 train_time:116191ms step_avg:93.03ms
step:1250/1670 train_time:116284ms step_avg:93.03ms
step:1250/1670 val_loss:3.3780 train_time:116376ms step_avg:93.10ms
step:1251/1670 train_time:116396ms step_avg:93.04ms
step:1252/1670 train_time:116471ms step_avg:93.03ms
step:1253/1670 train_time:116564ms step_avg:93.03ms
step:1254/1670 train_time:116656ms step_avg:93.03ms
step:1255/1670 train_time:116748ms step_avg:93.03ms
step:1256/1670 train_time:116841ms step_avg:93.03ms
step:1257/1670 train_time:116933ms step_avg:93.03ms
step:1258/1670 train_time:117027ms step_avg:93.03ms
step:1259/1670 train_time:117120ms step_avg:93.03ms
step:1260/1670 train_time:117213ms step_avg:93.03ms
step:1261/1670 train_time:117308ms step_avg:93.03ms
step:1262/1670 train_time:117402ms step_avg:93.03ms
step:1263/1670 train_time:117496ms step_avg:93.03ms
step:1264/1670 train_time:117588ms step_avg:93.03ms
step:1265/1670 train_time:117681ms step_avg:93.03ms
step:1266/1670 train_time:117773ms step_avg:93.03ms
step:1267/1670 train_time:117868ms step_avg:93.03ms
step:1268/1670 train_time:117961ms step_avg:93.03ms
step:1269/1670 train_time:118054ms step_avg:93.03ms
step:1270/1670 train_time:118146ms step_avg:93.03ms
step:1271/1670 train_time:118240ms step_avg:93.03ms
step:1272/1670 train_time:118333ms step_avg:93.03ms
step:1273/1670 train_time:118429ms step_avg:93.03ms
step:1274/1670 train_time:118667ms step_avg:93.15ms
step:1275/1670 train_time:118748ms step_avg:93.14ms
step:1276/1670 train_time:118840ms step_avg:93.13ms
step:1277/1670 train_time:118932ms step_avg:93.13ms
step:1278/1670 train_time:119024ms step_avg:93.13ms
step:1279/1670 train_time:119115ms step_avg:93.13ms
step:1280/1670 train_time:119208ms step_avg:93.13ms
step:1281/1670 train_time:119299ms step_avg:93.13ms
step:1282/1670 train_time:119391ms step_avg:93.13ms
step:1283/1670 train_time:119483ms step_avg:93.13ms
step:1284/1670 train_time:119581ms step_avg:93.13ms
step:1285/1670 train_time:119680ms step_avg:93.14ms
step:1286/1670 train_time:119774ms step_avg:93.14ms
step:1287/1670 train_time:119867ms step_avg:93.14ms
step:1288/1670 train_time:119959ms step_avg:93.14ms
step:1289/1670 train_time:120051ms step_avg:93.13ms
step:1290/1670 train_time:120143ms step_avg:93.13ms
step:1291/1670 train_time:120235ms step_avg:93.13ms
step:1292/1670 train_time:120327ms step_avg:93.13ms
step:1293/1670 train_time:120419ms step_avg:93.13ms
step:1294/1670 train_time:120512ms step_avg:93.13ms
step:1295/1670 train_time:120609ms step_avg:93.13ms
step:1296/1670 train_time:120704ms step_avg:93.14ms
step:1297/1670 train_time:120798ms step_avg:93.14ms
step:1298/1670 train_time:120891ms step_avg:93.14ms
step:1299/1670 train_time:120983ms step_avg:93.14ms
step:1300/1670 train_time:121076ms step_avg:93.14ms
step:1301/1670 train_time:121169ms step_avg:93.14ms
step:1302/1670 train_time:121261ms step_avg:93.13ms
step:1303/1670 train_time:121353ms step_avg:93.13ms
step:1304/1670 train_time:121446ms step_avg:93.13ms
step:1305/1670 train_time:121540ms step_avg:93.13ms
step:1306/1670 train_time:121634ms step_avg:93.13ms
step:1307/1670 train_time:121729ms step_avg:93.14ms
step:1308/1670 train_time:121823ms step_avg:93.14ms
step:1309/1670 train_time:121916ms step_avg:93.14ms
step:1310/1670 train_time:122009ms step_avg:93.14ms
step:1311/1670 train_time:122102ms step_avg:93.14ms
step:1312/1670 train_time:122194ms step_avg:93.14ms
step:1313/1670 train_time:122287ms step_avg:93.14ms
step:1314/1670 train_time:122380ms step_avg:93.14ms
step:1315/1670 train_time:122473ms step_avg:93.14ms
step:1316/1670 train_time:122568ms step_avg:93.14ms
step:1317/1670 train_time:122662ms step_avg:93.14ms
step:1318/1670 train_time:122756ms step_avg:93.14ms
step:1319/1670 train_time:122849ms step_avg:93.14ms
step:1320/1670 train_time:122942ms step_avg:93.14ms
step:1321/1670 train_time:123035ms step_avg:93.14ms
step:1322/1670 train_time:123129ms step_avg:93.14ms
step:1323/1670 train_time:123221ms step_avg:93.14ms
step:1324/1670 train_time:123313ms step_avg:93.14ms
step:1325/1670 train_time:123408ms step_avg:93.14ms
step:1326/1670 train_time:123502ms step_avg:93.14ms
step:1327/1670 train_time:123594ms step_avg:93.14ms
step:1328/1670 train_time:123688ms step_avg:93.14ms
step:1329/1670 train_time:123781ms step_avg:93.14ms
step:1330/1670 train_time:123874ms step_avg:93.14ms
step:1331/1670 train_time:123967ms step_avg:93.14ms
step:1332/1670 train_time:124060ms step_avg:93.14ms
step:1333/1670 train_time:124154ms step_avg:93.14ms
step:1334/1670 train_time:124246ms step_avg:93.14ms
step:1335/1670 train_time:124340ms step_avg:93.14ms
step:1336/1670 train_time:124434ms step_avg:93.14ms
step:1337/1670 train_time:124528ms step_avg:93.14ms
step:1338/1670 train_time:124621ms step_avg:93.14ms
step:1339/1670 train_time:124714ms step_avg:93.14ms
step:1340/1670 train_time:124808ms step_avg:93.14ms
step:1341/1670 train_time:124901ms step_avg:93.14ms
step:1342/1670 train_time:124993ms step_avg:93.14ms
step:1343/1670 train_time:125087ms step_avg:93.14ms
step:1344/1670 train_time:125180ms step_avg:93.14ms
step:1345/1670 train_time:125273ms step_avg:93.14ms
step:1346/1670 train_time:125366ms step_avg:93.14ms
step:1347/1670 train_time:125459ms step_avg:93.14ms
step:1348/1670 train_time:125553ms step_avg:93.14ms
step:1349/1670 train_time:125647ms step_avg:93.14ms
step:1350/1670 train_time:125740ms step_avg:93.14ms
step:1351/1670 train_time:125834ms step_avg:93.14ms
step:1352/1670 train_time:125928ms step_avg:93.14ms
step:1353/1670 train_time:126021ms step_avg:93.14ms
step:1354/1670 train_time:126113ms step_avg:93.14ms
step:1355/1670 train_time:126207ms step_avg:93.14ms
step:1356/1670 train_time:126300ms step_avg:93.14ms
step:1357/1670 train_time:126393ms step_avg:93.14ms
step:1358/1670 train_time:126486ms step_avg:93.14ms
step:1359/1670 train_time:126578ms step_avg:93.14ms
step:1360/1670 train_time:126672ms step_avg:93.14ms
step:1361/1670 train_time:126765ms step_avg:93.14ms
step:1362/1670 train_time:126858ms step_avg:93.14ms
step:1363/1670 train_time:126951ms step_avg:93.14ms
step:1364/1670 train_time:127048ms step_avg:93.14ms
step:1365/1670 train_time:127139ms step_avg:93.14ms
step:1366/1670 train_time:127232ms step_avg:93.14ms
step:1367/1670 train_time:127326ms step_avg:93.14ms
step:1368/1670 train_time:127418ms step_avg:93.14ms
step:1369/1670 train_time:127511ms step_avg:93.14ms
step:1370/1670 train_time:127604ms step_avg:93.14ms
step:1371/1670 train_time:127697ms step_avg:93.14ms
step:1372/1670 train_time:127791ms step_avg:93.14ms
step:1373/1670 train_time:127886ms step_avg:93.14ms
step:1374/1670 train_time:127979ms step_avg:93.14ms
step:1375/1670 train_time:128073ms step_avg:93.14ms
step:1375/1670 val_loss:3.3443 train_time:128166ms step_avg:93.21ms
step:1376/1670 train_time:128185ms step_avg:93.16ms
step:1377/1670 train_time:128261ms step_avg:93.15ms
step:1378/1670 train_time:128354ms step_avg:93.15ms
step:1379/1670 train_time:128447ms step_avg:93.15ms
step:1380/1670 train_time:128540ms step_avg:93.14ms
step:1381/1670 train_time:128632ms step_avg:93.14ms
step:1382/1670 train_time:128724ms step_avg:93.14ms
step:1383/1670 train_time:128819ms step_avg:93.14ms
step:1384/1670 train_time:128913ms step_avg:93.15ms
step:1385/1670 train_time:129005ms step_avg:93.14ms
step:1386/1670 train_time:129099ms step_avg:93.15ms
step:1387/1670 train_time:129194ms step_avg:93.15ms
step:1388/1670 train_time:129287ms step_avg:93.15ms
step:1389/1670 train_time:129380ms step_avg:93.15ms
step:1390/1670 train_time:129473ms step_avg:93.15ms
step:1391/1670 train_time:129565ms step_avg:93.15ms
step:1392/1670 train_time:129658ms step_avg:93.15ms
step:1393/1670 train_time:129752ms step_avg:93.15ms
step:1394/1670 train_time:129844ms step_avg:93.15ms
step:1395/1670 train_time:129938ms step_avg:93.15ms
step:1396/1670 train_time:130032ms step_avg:93.15ms
step:1397/1670 train_time:130125ms step_avg:93.15ms
step:1398/1670 train_time:130220ms step_avg:93.15ms
step:1399/1670 train_time:130316ms step_avg:93.15ms
step:1400/1670 train_time:130410ms step_avg:93.15ms
step:1401/1670 train_time:130503ms step_avg:93.15ms
step:1402/1670 train_time:130596ms step_avg:93.15ms
step:1403/1670 train_time:130690ms step_avg:93.15ms
step:1404/1670 train_time:130783ms step_avg:93.15ms
step:1405/1670 train_time:130876ms step_avg:93.15ms
step:1406/1670 train_time:130970ms step_avg:93.15ms
step:1407/1670 train_time:131063ms step_avg:93.15ms
step:1408/1670 train_time:131157ms step_avg:93.15ms
step:1409/1670 train_time:131250ms step_avg:93.15ms
step:1410/1670 train_time:131344ms step_avg:93.15ms
step:1411/1670 train_time:131437ms step_avg:93.15ms
step:1412/1670 train_time:131529ms step_avg:93.15ms
step:1413/1670 train_time:131623ms step_avg:93.15ms
step:1414/1670 train_time:131717ms step_avg:93.15ms
step:1415/1670 train_time:131810ms step_avg:93.15ms
step:1416/1670 train_time:131903ms step_avg:93.15ms
step:1417/1670 train_time:131996ms step_avg:93.15ms
step:1418/1670 train_time:132089ms step_avg:93.15ms
step:1419/1670 train_time:132183ms step_avg:93.15ms
step:1420/1670 train_time:132276ms step_avg:93.15ms
step:1421/1670 train_time:132370ms step_avg:93.15ms
step:1422/1670 train_time:132463ms step_avg:93.15ms
step:1423/1670 train_time:132557ms step_avg:93.15ms
step:1424/1670 train_time:132650ms step_avg:93.15ms
step:1425/1670 train_time:132743ms step_avg:93.15ms
step:1426/1670 train_time:132836ms step_avg:93.15ms
step:1427/1670 train_time:132929ms step_avg:93.15ms
step:1428/1670 train_time:133023ms step_avg:93.15ms
step:1429/1670 train_time:133117ms step_avg:93.15ms
step:1430/1670 train_time:133210ms step_avg:93.15ms
step:1431/1670 train_time:133302ms step_avg:93.15ms
step:1432/1670 train_time:133395ms step_avg:93.15ms
step:1433/1670 train_time:133489ms step_avg:93.15ms
step:1434/1670 train_time:133582ms step_avg:93.15ms
step:1435/1670 train_time:133675ms step_avg:93.15ms
step:1436/1670 train_time:133770ms step_avg:93.15ms
step:1437/1670 train_time:133861ms step_avg:93.15ms
step:1438/1670 train_time:133955ms step_avg:93.15ms
step:1439/1670 train_time:134048ms step_avg:93.15ms
step:1440/1670 train_time:134141ms step_avg:93.15ms
step:1441/1670 train_time:134234ms step_avg:93.15ms
step:1442/1670 train_time:134326ms step_avg:93.15ms
step:1443/1670 train_time:134420ms step_avg:93.15ms
step:1444/1670 train_time:134513ms step_avg:93.15ms
step:1445/1670 train_time:134606ms step_avg:93.15ms
step:1446/1670 train_time:134699ms step_avg:93.15ms
step:1447/1670 train_time:134793ms step_avg:93.15ms
step:1448/1670 train_time:134885ms step_avg:93.15ms
step:1449/1670 train_time:134980ms step_avg:93.15ms
step:1450/1670 train_time:135074ms step_avg:93.15ms
step:1451/1670 train_time:135167ms step_avg:93.15ms
step:1452/1670 train_time:135261ms step_avg:93.15ms
step:1453/1670 train_time:135354ms step_avg:93.16ms
step:1454/1670 train_time:135448ms step_avg:93.16ms
step:1455/1670 train_time:135542ms step_avg:93.16ms
step:1456/1670 train_time:135634ms step_avg:93.16ms
step:1457/1670 train_time:135727ms step_avg:93.16ms
step:1458/1670 train_time:135821ms step_avg:93.16ms
step:1459/1670 train_time:135914ms step_avg:93.16ms
step:1460/1670 train_time:136007ms step_avg:93.16ms
step:1461/1670 train_time:136101ms step_avg:93.16ms
step:1462/1670 train_time:136195ms step_avg:93.16ms
step:1463/1670 train_time:136288ms step_avg:93.16ms
step:1464/1670 train_time:136382ms step_avg:93.16ms
step:1465/1670 train_time:136475ms step_avg:93.16ms
step:1466/1670 train_time:136568ms step_avg:93.16ms
step:1467/1670 train_time:136663ms step_avg:93.16ms
step:1468/1670 train_time:136756ms step_avg:93.16ms
step:1469/1670 train_time:136849ms step_avg:93.16ms
step:1470/1670 train_time:136942ms step_avg:93.16ms
step:1471/1670 train_time:137034ms step_avg:93.16ms
step:1472/1670 train_time:137128ms step_avg:93.16ms
step:1473/1670 train_time:137221ms step_avg:93.16ms
step:1474/1670 train_time:137314ms step_avg:93.16ms
step:1475/1670 train_time:137407ms step_avg:93.16ms
step:1476/1670 train_time:137501ms step_avg:93.16ms
step:1477/1670 train_time:137595ms step_avg:93.16ms
step:1478/1670 train_time:137688ms step_avg:93.16ms
step:1479/1670 train_time:137782ms step_avg:93.16ms
step:1480/1670 train_time:137876ms step_avg:93.16ms
step:1481/1670 train_time:137968ms step_avg:93.16ms
step:1482/1670 train_time:138063ms step_avg:93.16ms
step:1483/1670 train_time:138157ms step_avg:93.16ms
step:1484/1670 train_time:138250ms step_avg:93.16ms
step:1485/1670 train_time:138500ms step_avg:93.27ms
step:1486/1670 train_time:138571ms step_avg:93.25ms
step:1487/1670 train_time:138663ms step_avg:93.25ms
step:1488/1670 train_time:138754ms step_avg:93.25ms
step:1489/1670 train_time:138847ms step_avg:93.25ms
step:1490/1670 train_time:138939ms step_avg:93.25ms
step:1491/1670 train_time:139030ms step_avg:93.25ms
step:1492/1670 train_time:139122ms step_avg:93.25ms
step:1493/1670 train_time:139215ms step_avg:93.25ms
step:1494/1670 train_time:139307ms step_avg:93.24ms
step:1495/1670 train_time:139405ms step_avg:93.25ms
step:1496/1670 train_time:139503ms step_avg:93.25ms
step:1497/1670 train_time:139597ms step_avg:93.25ms
step:1498/1670 train_time:139692ms step_avg:93.25ms
step:1499/1670 train_time:139784ms step_avg:93.25ms
step:1500/1670 train_time:139876ms step_avg:93.25ms
step:1500/1670 val_loss:3.3143 train_time:139969ms step_avg:93.31ms
step:1501/1670 train_time:139988ms step_avg:93.26ms
step:1502/1670 train_time:140063ms step_avg:93.25ms
step:1503/1670 train_time:140156ms step_avg:93.25ms
step:1504/1670 train_time:140248ms step_avg:93.25ms
step:1505/1670 train_time:140341ms step_avg:93.25ms
step:1506/1670 train_time:140433ms step_avg:93.25ms
step:1507/1670 train_time:140526ms step_avg:93.25ms
step:1508/1670 train_time:140618ms step_avg:93.25ms
step:1509/1670 train_time:140711ms step_avg:93.25ms
step:1510/1670 train_time:140805ms step_avg:93.25ms
step:1511/1670 train_time:140899ms step_avg:93.25ms
step:1512/1670 train_time:140993ms step_avg:93.25ms
step:1513/1670 train_time:141087ms step_avg:93.25ms
step:1514/1670 train_time:141181ms step_avg:93.25ms
step:1515/1670 train_time:141273ms step_avg:93.25ms
step:1516/1670 train_time:141367ms step_avg:93.25ms
step:1517/1670 train_time:141461ms step_avg:93.25ms
step:1518/1670 train_time:141553ms step_avg:93.25ms
step:1519/1670 train_time:141646ms step_avg:93.25ms
step:1520/1670 train_time:141739ms step_avg:93.25ms
step:1521/1670 train_time:141833ms step_avg:93.25ms
step:1522/1670 train_time:141927ms step_avg:93.25ms
step:1523/1670 train_time:142021ms step_avg:93.25ms
step:1524/1670 train_time:142113ms step_avg:93.25ms
step:1525/1670 train_time:142208ms step_avg:93.25ms
step:1526/1670 train_time:142302ms step_avg:93.25ms
step:1527/1670 train_time:142395ms step_avg:93.25ms
step:1528/1670 train_time:142488ms step_avg:93.25ms
step:1529/1670 train_time:142580ms step_avg:93.25ms
step:1530/1670 train_time:142673ms step_avg:93.25ms
step:1531/1670 train_time:142768ms step_avg:93.25ms
step:1532/1670 train_time:142861ms step_avg:93.25ms
step:1533/1670 train_time:142955ms step_avg:93.25ms
step:1534/1670 train_time:143048ms step_avg:93.25ms
step:1535/1670 train_time:143141ms step_avg:93.25ms
step:1536/1670 train_time:143234ms step_avg:93.25ms
step:1537/1670 train_time:143327ms step_avg:93.25ms
step:1538/1670 train_time:143420ms step_avg:93.25ms
step:1539/1670 train_time:143513ms step_avg:93.25ms
step:1540/1670 train_time:143607ms step_avg:93.25ms
step:1541/1670 train_time:143700ms step_avg:93.25ms
step:1542/1670 train_time:143794ms step_avg:93.25ms
step:1543/1670 train_time:143887ms step_avg:93.25ms
step:1544/1670 train_time:143980ms step_avg:93.25ms
step:1545/1670 train_time:144074ms step_avg:93.25ms
step:1546/1670 train_time:144167ms step_avg:93.25ms
step:1547/1670 train_time:144260ms step_avg:93.25ms
step:1548/1670 train_time:144354ms step_avg:93.25ms
step:1549/1670 train_time:144446ms step_avg:93.25ms
step:1550/1670 train_time:144539ms step_avg:93.25ms
step:1551/1670 train_time:144632ms step_avg:93.25ms
step:1552/1670 train_time:144728ms step_avg:93.25ms
step:1553/1670 train_time:144822ms step_avg:93.25ms
step:1554/1670 train_time:144914ms step_avg:93.25ms
step:1555/1670 train_time:145009ms step_avg:93.25ms
step:1556/1670 train_time:145102ms step_avg:93.25ms
step:1557/1670 train_time:145196ms step_avg:93.25ms
step:1558/1670 train_time:145290ms step_avg:93.25ms
step:1559/1670 train_time:145382ms step_avg:93.25ms
step:1560/1670 train_time:145475ms step_avg:93.25ms
step:1561/1670 train_time:145569ms step_avg:93.25ms
step:1562/1670 train_time:145663ms step_avg:93.25ms
step:1563/1670 train_time:145756ms step_avg:93.25ms
step:1564/1670 train_time:145849ms step_avg:93.25ms
step:1565/1670 train_time:145942ms step_avg:93.25ms
step:1566/1670 train_time:146034ms step_avg:93.25ms
step:1567/1670 train_time:146128ms step_avg:93.25ms
step:1568/1670 train_time:146221ms step_avg:93.25ms
step:1569/1670 train_time:146314ms step_avg:93.25ms
step:1570/1670 train_time:146407ms step_avg:93.25ms
step:1571/1670 train_time:146500ms step_avg:93.25ms
step:1572/1670 train_time:146593ms step_avg:93.25ms
step:1573/1670 train_time:146687ms step_avg:93.25ms
step:1574/1670 train_time:146780ms step_avg:93.25ms
step:1575/1670 train_time:146873ms step_avg:93.25ms
step:1576/1670 train_time:146966ms step_avg:93.25ms
step:1577/1670 train_time:147059ms step_avg:93.25ms
step:1578/1670 train_time:147152ms step_avg:93.25ms
step:1579/1670 train_time:147245ms step_avg:93.25ms
step:1580/1670 train_time:147338ms step_avg:93.25ms
step:1581/1670 train_time:147431ms step_avg:93.25ms
step:1582/1670 train_time:147525ms step_avg:93.25ms
step:1583/1670 train_time:147619ms step_avg:93.25ms
step:1584/1670 train_time:147712ms step_avg:93.25ms
step:1585/1670 train_time:147806ms step_avg:93.25ms
step:1586/1670 train_time:147900ms step_avg:93.25ms
step:1587/1670 train_time:147992ms step_avg:93.25ms
step:1588/1670 train_time:148085ms step_avg:93.25ms
step:1589/1670 train_time:148178ms step_avg:93.25ms
step:1590/1670 train_time:148271ms step_avg:93.25ms
step:1591/1670 train_time:148364ms step_avg:93.25ms
step:1592/1670 train_time:148457ms step_avg:93.25ms
step:1593/1670 train_time:148550ms step_avg:93.25ms
step:1594/1670 train_time:148643ms step_avg:93.25ms
step:1595/1670 train_time:148736ms step_avg:93.25ms
step:1596/1670 train_time:148830ms step_avg:93.25ms
step:1597/1670 train_time:148923ms step_avg:93.25ms
step:1598/1670 train_time:149016ms step_avg:93.25ms
step:1599/1670 train_time:149110ms step_avg:93.25ms
step:1600/1670 train_time:149204ms step_avg:93.25ms
step:1601/1670 train_time:149297ms step_avg:93.25ms
step:1602/1670 train_time:149389ms step_avg:93.25ms
step:1603/1670 train_time:149482ms step_avg:93.25ms
step:1604/1670 train_time:149575ms step_avg:93.25ms
step:1605/1670 train_time:149669ms step_avg:93.25ms
step:1606/1670 train_time:149763ms step_avg:93.25ms
step:1607/1670 train_time:149856ms step_avg:93.25ms
step:1608/1670 train_time:149949ms step_avg:93.25ms
step:1609/1670 train_time:150042ms step_avg:93.25ms
step:1610/1670 train_time:150136ms step_avg:93.25ms
step:1611/1670 train_time:150229ms step_avg:93.25ms
step:1612/1670 train_time:150322ms step_avg:93.25ms
step:1613/1670 train_time:150414ms step_avg:93.25ms
step:1614/1670 train_time:150509ms step_avg:93.25ms
step:1615/1670 train_time:150603ms step_avg:93.25ms
step:1616/1670 train_time:150696ms step_avg:93.25ms
step:1617/1670 train_time:150789ms step_avg:93.25ms
step:1618/1670 train_time:150882ms step_avg:93.25ms
step:1619/1670 train_time:150975ms step_avg:93.25ms
step:1620/1670 train_time:151070ms step_avg:93.25ms
step:1621/1670 train_time:151163ms step_avg:93.25ms
step:1622/1670 train_time:151255ms step_avg:93.25ms
step:1623/1670 train_time:151348ms step_avg:93.25ms
step:1624/1670 train_time:151441ms step_avg:93.25ms
step:1625/1670 train_time:151534ms step_avg:93.25ms
step:1625/1670 val_loss:3.2889 train_time:151627ms step_avg:93.31ms
step:1626/1670 train_time:151646ms step_avg:93.26ms
step:1627/1670 train_time:151721ms step_avg:93.25ms
step:1628/1670 train_time:151814ms step_avg:93.25ms
step:1629/1670 train_time:151907ms step_avg:93.25ms
step:1630/1670 train_time:151999ms step_avg:93.25ms
step:1631/1670 train_time:152092ms step_avg:93.25ms
step:1632/1670 train_time:152185ms step_avg:93.25ms
step:1633/1670 train_time:152278ms step_avg:93.25ms
step:1634/1670 train_time:152372ms step_avg:93.25ms
step:1635/1670 train_time:152464ms step_avg:93.25ms
step:1636/1670 train_time:152559ms step_avg:93.25ms
step:1637/1670 train_time:152655ms step_avg:93.25ms
step:1638/1670 train_time:152748ms step_avg:93.25ms
step:1639/1670 train_time:152841ms step_avg:93.25ms
step:1640/1670 train_time:152934ms step_avg:93.25ms
step:1641/1670 train_time:153026ms step_avg:93.25ms
step:1642/1670 train_time:153119ms step_avg:93.25ms
step:1643/1670 train_time:153213ms step_avg:93.25ms
step:1644/1670 train_time:153306ms step_avg:93.25ms
step:1645/1670 train_time:153398ms step_avg:93.25ms
step:1646/1670 train_time:153492ms step_avg:93.25ms
step:1647/1670 train_time:153586ms step_avg:93.25ms
step:1648/1670 train_time:153679ms step_avg:93.25ms
step:1649/1670 train_time:153773ms step_avg:93.25ms
step:1650/1670 train_time:153865ms step_avg:93.25ms
step:1651/1670 train_time:153958ms step_avg:93.25ms
step:1652/1670 train_time:154053ms step_avg:93.25ms
step:1653/1670 train_time:154145ms step_avg:93.25ms
step:1654/1670 train_time:154238ms step_avg:93.25ms
step:1655/1670 train_time:154332ms step_avg:93.25ms
step:1656/1670 train_time:154425ms step_avg:93.25ms
step:1657/1670 train_time:154519ms step_avg:93.25ms
step:1658/1670 train_time:154613ms step_avg:93.25ms
step:1659/1670 train_time:154706ms step_avg:93.25ms
step:1660/1670 train_time:154799ms step_avg:93.25ms
step:1661/1670 train_time:154891ms step_avg:93.25ms
step:1662/1670 train_time:154985ms step_avg:93.25ms
step:1663/1670 train_time:155078ms step_avg:93.25ms
step:1664/1670 train_time:155171ms step_avg:93.25ms
step:1665/1670 train_time:155264ms step_avg:93.25ms
step:1666/1670 train_time:155358ms step_avg:93.25ms
step:1667/1670 train_time:155451ms step_avg:93.25ms
step:1668/1670 train_time:155544ms step_avg:93.25ms
step:1669/1670 train_time:155639ms step_avg:93.25ms
step:1670/1670 train_time:155732ms step_avg:93.25ms
step:1670/1670 val_loss:3.2805 train_time:155980ms step_avg:93.40ms
peak memory allocated: 32002 MiB reserved: 46616 MiB

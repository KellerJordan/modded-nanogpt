import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(
    x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(
    g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99


def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]


@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(
        pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M
    )

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr,
    C_ptr,
    M,
    K,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr,
    C_ptr,
    M,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    alpha,
    beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (
        offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c
    )
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out


@torch.compile(
    dynamic=False, fullgraph=True
)  # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer


class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(
        self,
        params,
        lr: float = 1e-3,
        betas: tuple[float, float] = (0.9, 0.999),
        eps: float = 1e-8,
        weight_decay: float = 0.01,
    ):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(
                        grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True
                    ).get_future()
                )
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group["betas"]
            eps = group["eps"]
            wd = group["weight_decay"]
            params = group["params"]
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size : (rank + 1) * rank_size]
                lr = group["lr"] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(
                    g_slice, g_slice, value=1 - beta2
                )
                # bias corrections
                bias1 = 1 - beta1**t
                bias2 = 1 - beta2**t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(
                    dist.all_gather_into_tensor(
                        p, p_slice, async_op=True
                    ).get_future()
                )
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model


def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(
        self,
        in_features: int,
        out_features: int,
        use_fp8=False,
        x_s=1.0,
        w_s=1.0,
        grad_s=1.0,
    ):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (
            self.in_features**-0.5
        )  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3**0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(
                _x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s
            )[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    rotary_cos: torch.Tensor
    rotary_sin: torch.Tensor
    attn_scale: float


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        rotary_cos, rotary_sin = attn_args.rotary_cos, attn_args.rotary_sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = (
            attn_args.seqlens,
            attn_args.attn_scale,
            attn_args.bm_size,
        )

        q, k, v = (
            F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x))
            .view(B, T, 3 * self.num_heads, self.head_dim)
            .chunk(3, dim=-2)
        )
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = (
            rotary(q, rotary_cos, rotary_sin),
            rotary(k, rotary_cos, rotary_sin),
        )
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(
                v
            )  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = (
            args.train_max_seq_len
            if self.training
            else (args.val_batch_size // (grad_accum_steps * world_size))
        )

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(
            q[0],
            k[0],
            v[0],
            cu_seqlens_q=seqlens,
            cu_seqlens_k=seqlens,
            max_seqlen_q=max_len,
            max_seqlen_k=max_len,
            causal=True,
            softmax_scale=attn_scale,
            window_size=(bm_size, 0),
        )
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(
            self.attn_gate(x[..., : self.attn_gate.weight.size(-1)])
        ).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(
            B, T, self.num_heads * self.head_dim
        )  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x
        ).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = (
            CausalSelfAttention(dim, head_dim, num_heads)
            if layer_idx != 7
            else None
        )
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(
        self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs
    ):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model


def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        num_layers: int,
        num_heads: int,
        head_dim: int,
        model_dim: int,
        max_seq_len: int,
    ):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList(
            [nn.Embedding(vocab_size, model_dim) for _ in range(3)]
        )
        self.blocks = nn.ModuleList(
            [
                Block(model_dim, head_dim, num_heads, i)
                for i in range(num_layers)
            ]
        )
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(
            model_dim,
            vocab_size,
            use_fp8=use_fp8,
            x_s=(model_dim**0.5) / 448,
            w_s=2**-9,
            grad_s=1 / 448,
        )
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.ones(pad),
                ]
            )
        )
        self.max_seq_len = max_seq_len
        self.setup_yarn(head_dim)
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.0
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.0
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def setup_yarn(self, head_dim: int):
        # store single copy of rotary tensors
        angular_freq = (1 / 1024) ** torch.linspace(
            0, 1, steps=head_dim // 4, dtype=torch.float32
        )
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat(
            [angular_freq, angular_freq.new_zeros(head_dim // 4)]
        )
        t = torch.arange(self.max_seq_len, dtype=torch.float32)
        theta = torch.outer(t, angular_freq)
        self.rotary_cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.rotary_sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq

        # scale attention factor f in attn=softmax(f*qk) logarithmically with window size @classiclarryd
        windows = list(
            dict.fromkeys(list(args.ws_schedule) + [args.ws_validate])
        )
        scale_factors = [
            0.2 * math.log(curr / prev) + 1
            for prev, curr in zip(windows[:-1], windows[1:])
        ]
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        attn_scales = list(
            accumulate([0.1] + scale_factors, lambda acc, factor: acc * factor)
        )
        self.attn_scales = dict(zip(windows, attn_scales))

    def apply_yarn(
        self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32
    ):
        rotations = (
            args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        )
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp(
            (rotations - alpha) / (beta - alpha), 0, 1
        )
        self.angular_freq *= scaling_factor + interpolation_weight * (
            1 - scaling_factor
        )
        t = torch.arange(
            self.max_seq_len,
            dtype=torch.float32,
            device=self.angular_freq.device,
        )
        theta = torch.outer(t, self.angular_freq)
        self.rotary_cos.copy_(theta.cos())
        self.rotary_sin.copy_(theta.sin())

    def forward(
        self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int
    ):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = (
            [ve[0], ve[1], ve[2]]
            + [None] * (len(self.blocks) - 6)
            + [ve[0], ve[1], ve[2]]
        )
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        bm_sizes = [
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
        ]
        assert len(bm_sizes) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]).to(
            torch.bfloat16
        )  # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[: (len(self.blocks) // 2)]
        lambdas = self.scalars[
            1 * len(self.blocks) : 3 * len(self.blocks)
        ].view(-1, 2)
        sa_lambdas = self.scalars[
            3 * len(self.blocks) : 5 * len(self.blocks)
        ].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                rotary_cos=self.rotary_cos,
                rotary_sin=self.rotary_sin,
                attn_scale=self.attn_scales[ws],
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(
            logits.view(-1, logits.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader


def _load_data_shard(file: Path):
    header = torch.from_file(
        str(file), False, 256, dtype=torch.int32
    )  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(
            num_tokens, dtype=torch.uint16, pin_memory=True
        )  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(
            tokens.numpy()
        )  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, (
            "number of tokens read does not match header"
        )
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1):
        # Precompute BOS positions once per shard
        self.size = tokens.numel()
        self.bos_idx = (
            (tokens == BOS_ID)
            .nonzero(as_tuple=True)[0]
            .to(torch.int64)
            .cpu()
            .numpy()
        )
        self.i = 0
        self.world_size = world_size

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(
                        f"Insufficient BOS ahead of position {cur}; hit tail of shard."
                    )
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(
                    self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                    cur + max_seq_len,
                    cur + num_tokens_local - cur_len + 1,
                )
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx

        return starts, ends


def distributed_data_generator(
    filename_pattern: str,
    num_tokens: int,
    max_seq_len: int,
    grad_accum_steps: int = 1,
    align_to_bos: bool = True,
):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, (
        "Batch size must be divisible by world size"
    )
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(
            f"No files found for pattern: {filename_pattern}"
        )

    file_iter = iter(
        files
    )  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    finder = BOSFinder(tokens, world_size=world_size) if align_to_bos else None
    pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(
            num_tokens_local // 300, n=128
        )  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(
                    num_tokens_local, max_seq_len
                )
                start_idxs, end_idxs = (
                    torch.tensor(seq_starts[rank]),
                    torch.tensor(seq_ends[rank]),
                )
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens = _load_data_shard(next(file_iter))
                finder = BOSFinder(tokens, world_size=world_size)
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= (
                1  # last document was too long to account for _targets offset
            )
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(
                tokens
            ):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local : pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(
                num_tokens_local,
            )
            _targets = buf[1:].view(
                num_tokens_local,
            )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1 : len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(
                device="cuda", dtype=torch.int32, non_blocking=True
            ),
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, (
                "Num tokens must be divisible by world size"
            )
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main


@dataclass
class Hyperparameters:
    # data
    train_files: str = (
        "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    )
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1670  # number of iterations to run
    cooldown_frac: int = (
        0.5  # fraction of training spent cooling down the learning rate
    )
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = (
        125  # every how many steps to evaluate val loss? 0 for only at the end
    )
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws @classiclarryd


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = rank == 0  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(
    f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}"
)
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import

    return subprocess.run(
        ["nvidia-smi"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    ).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size)
    // (grad_accum_steps * world_size),
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [
    p
    for n, p in model.blocks.named_parameters()
    if p.ndim >= 2 and "embed" not in n
]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(
    hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)  # save the initial state
train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    ws = args.ws_schedule[
        step % len(args.ws_schedule)
    ]  # each window size is a new graph, need to warm up each
    model(inputs, targets, cum_seqlens, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws = get_ws(0)
for step in range(train_steps + 1):
    last_step = step == train_steps
    new_ws = get_ws(step)
    if new_ws != ws:
        model.apply_yarn(ws, new_ws)
        ws = new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (
        args.val_loss_every > 0 and step % args.val_loss_every == 0
    ):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(
            args.val_files,
            args.val_batch_size,
            -1,
            grad_accum_steps=grad_accum_steps,
            align_to_bos=False,
        )
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True,
        )
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(
                step=step,
                code=code,
                model=model.state_dict(),
                optimizers=[opt.state_dict() for opt in optimizers],
            )
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1)  # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (
        time.perf_counter() - t0
    )
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True,
    )

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.11 (main, Sep  2 2025, 14:20:58) [Clang 20.1.4 ]
Running PyTorch 2.9.0.dev20250718+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 11 09:59:20 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                  Off |
| N/A   40C    P0            125W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                  Off |
| N/A   45C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:65:00.0 Off |                  Off |
| N/A   46C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:68:00.0 Off |                  Off |
| N/A   37C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:85:00.0 Off |                  Off |
| N/A   38C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:86:00.0 Off |                  Off |
| N/A   45C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:E5:00.0 Off |                  Off |
| N/A   45C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E8:00.0 Off |                  Off |
| N/A   41C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1670 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/1670 train_time:293ms step_avg:293.10ms
step:2/1670 train_time:311ms step_avg:155.46ms
step:3/1670 train_time:380ms step_avg:126.68ms
step:4/1670 train_time:469ms step_avg:117.30ms
step:5/1670 train_time:560ms step_avg:111.94ms
step:6/1670 train_time:651ms step_avg:108.52ms
step:7/1670 train_time:741ms step_avg:105.91ms
step:8/1670 train_time:832ms step_avg:103.98ms
step:9/1670 train_time:922ms step_avg:102.50ms
step:10/1670 train_time:1013ms step_avg:101.30ms
step:11/1670 train_time:1103ms step_avg:100.29ms
step:12/1670 train_time:1198ms step_avg:99.86ms
step:13/1670 train_time:1292ms step_avg:99.42ms
step:14/1670 train_time:1386ms step_avg:98.97ms
step:15/1670 train_time:1478ms step_avg:98.51ms
step:16/1670 train_time:1569ms step_avg:98.09ms
step:17/1670 train_time:1661ms step_avg:97.68ms
step:18/1670 train_time:1752ms step_avg:97.36ms
step:19/1670 train_time:1844ms step_avg:97.03ms
step:20/1670 train_time:1934ms step_avg:96.72ms
step:21/1670 train_time:2026ms step_avg:96.46ms
step:22/1670 train_time:2118ms step_avg:96.28ms
step:23/1670 train_time:2212ms step_avg:96.15ms
step:24/1670 train_time:2303ms step_avg:95.97ms
step:25/1670 train_time:2398ms step_avg:95.90ms
step:26/1670 train_time:2492ms step_avg:95.84ms
step:27/1670 train_time:2584ms step_avg:95.69ms
step:28/1670 train_time:2675ms step_avg:95.54ms
step:29/1670 train_time:2766ms step_avg:95.37ms
step:30/1670 train_time:2857ms step_avg:95.22ms
step:31/1670 train_time:2948ms step_avg:95.11ms
step:32/1670 train_time:3039ms step_avg:94.97ms
step:33/1670 train_time:3132ms step_avg:94.90ms
step:34/1670 train_time:3223ms step_avg:94.80ms
step:35/1670 train_time:3316ms step_avg:94.73ms
step:36/1670 train_time:3407ms step_avg:94.65ms
step:37/1670 train_time:3501ms step_avg:94.62ms
step:38/1670 train_time:3594ms step_avg:94.57ms
step:39/1670 train_time:3686ms step_avg:94.52ms
step:40/1670 train_time:3779ms step_avg:94.48ms
step:41/1670 train_time:3872ms step_avg:94.43ms
step:42/1670 train_time:3963ms step_avg:94.35ms
step:43/1670 train_time:4055ms step_avg:94.30ms
step:44/1670 train_time:4145ms step_avg:94.21ms
step:45/1670 train_time:4237ms step_avg:94.15ms
step:46/1670 train_time:4329ms step_avg:94.11ms
step:47/1670 train_time:4422ms step_avg:94.08ms
step:48/1670 train_time:4514ms step_avg:94.05ms
step:49/1670 train_time:4605ms step_avg:93.98ms
step:50/1670 train_time:4698ms step_avg:93.97ms
step:51/1670 train_time:4791ms step_avg:93.95ms
step:52/1670 train_time:4883ms step_avg:93.89ms
step:53/1670 train_time:4975ms step_avg:93.87ms
step:54/1670 train_time:5066ms step_avg:93.81ms
step:55/1670 train_time:5157ms step_avg:93.76ms
step:56/1670 train_time:5249ms step_avg:93.73ms
step:57/1670 train_time:5341ms step_avg:93.69ms
step:58/1670 train_time:5433ms step_avg:93.67ms
step:59/1670 train_time:5524ms step_avg:93.62ms
step:60/1670 train_time:5615ms step_avg:93.59ms
step:61/1670 train_time:5707ms step_avg:93.56ms
step:62/1670 train_time:5799ms step_avg:93.54ms
step:63/1670 train_time:5892ms step_avg:93.52ms
step:64/1670 train_time:5983ms step_avg:93.48ms
step:65/1670 train_time:6075ms step_avg:93.46ms
step:66/1670 train_time:6166ms step_avg:93.42ms
step:67/1670 train_time:6258ms step_avg:93.40ms
step:68/1670 train_time:6349ms step_avg:93.36ms
step:69/1670 train_time:6441ms step_avg:93.34ms
step:70/1670 train_time:6532ms step_avg:93.32ms
step:71/1670 train_time:6623ms step_avg:93.28ms
step:72/1670 train_time:6714ms step_avg:93.25ms
step:73/1670 train_time:6805ms step_avg:93.22ms
step:74/1670 train_time:6898ms step_avg:93.21ms
step:75/1670 train_time:6990ms step_avg:93.20ms
step:76/1670 train_time:7082ms step_avg:93.18ms
step:77/1670 train_time:7173ms step_avg:93.16ms
step:78/1670 train_time:7265ms step_avg:93.14ms
step:79/1670 train_time:7357ms step_avg:93.13ms
step:80/1670 train_time:7450ms step_avg:93.12ms
step:81/1670 train_time:7539ms step_avg:93.08ms
step:82/1670 train_time:7630ms step_avg:93.05ms
step:83/1670 train_time:7721ms step_avg:93.03ms
step:84/1670 train_time:7813ms step_avg:93.01ms
step:85/1670 train_time:7904ms step_avg:92.98ms
step:86/1670 train_time:7996ms step_avg:92.98ms
step:87/1670 train_time:8088ms step_avg:92.96ms
step:88/1670 train_time:8179ms step_avg:92.94ms
step:89/1670 train_time:8270ms step_avg:92.92ms
step:90/1670 train_time:8361ms step_avg:92.90ms
step:91/1670 train_time:8456ms step_avg:92.92ms
step:92/1670 train_time:8548ms step_avg:92.92ms
step:93/1670 train_time:8640ms step_avg:92.91ms
step:94/1670 train_time:8732ms step_avg:92.89ms
step:95/1670 train_time:8822ms step_avg:92.87ms
step:96/1670 train_time:8915ms step_avg:92.86ms
step:97/1670 train_time:9006ms step_avg:92.85ms
step:98/1670 train_time:9098ms step_avg:92.84ms
step:99/1670 train_time:9191ms step_avg:92.84ms
step:100/1670 train_time:9282ms step_avg:92.82ms
step:101/1670 train_time:9373ms step_avg:92.81ms
step:102/1670 train_time:9465ms step_avg:92.79ms
step:103/1670 train_time:9556ms step_avg:92.78ms
step:104/1670 train_time:9647ms step_avg:92.76ms
step:105/1670 train_time:9739ms step_avg:92.75ms
step:106/1670 train_time:9830ms step_avg:92.73ms
step:107/1670 train_time:9921ms step_avg:92.72ms
step:108/1670 train_time:10013ms step_avg:92.71ms
step:109/1670 train_time:10104ms step_avg:92.70ms
step:110/1670 train_time:10197ms step_avg:92.70ms
step:111/1670 train_time:10289ms step_avg:92.69ms
step:112/1670 train_time:10380ms step_avg:92.68ms
step:113/1670 train_time:10472ms step_avg:92.68ms
step:114/1670 train_time:10563ms step_avg:92.66ms
step:115/1670 train_time:10655ms step_avg:92.65ms
step:116/1670 train_time:10746ms step_avg:92.64ms
step:117/1670 train_time:10837ms step_avg:92.62ms
step:118/1670 train_time:10929ms step_avg:92.62ms
step:119/1670 train_time:11020ms step_avg:92.60ms
step:120/1670 train_time:11112ms step_avg:92.60ms
step:121/1670 train_time:11202ms step_avg:92.58ms
step:122/1670 train_time:11294ms step_avg:92.57ms
step:123/1670 train_time:11385ms step_avg:92.56ms
step:124/1670 train_time:11478ms step_avg:92.57ms
step:125/1670 train_time:11570ms step_avg:92.56ms
step:125/1670 val_loss:4.2895 train_time:11661ms step_avg:93.29ms
step:126/1670 train_time:11680ms step_avg:92.70ms
step:127/1670 train_time:11755ms step_avg:92.56ms
step:128/1670 train_time:11855ms step_avg:92.62ms
step:129/1670 train_time:11950ms step_avg:92.64ms
step:130/1670 train_time:12041ms step_avg:92.63ms
step:131/1670 train_time:12132ms step_avg:92.61ms
step:132/1670 train_time:12222ms step_avg:92.59ms
step:133/1670 train_time:12312ms step_avg:92.57ms
step:134/1670 train_time:12402ms step_avg:92.55ms
step:135/1670 train_time:12492ms step_avg:92.54ms
step:136/1670 train_time:12583ms step_avg:92.52ms
step:137/1670 train_time:12673ms step_avg:92.50ms
step:138/1670 train_time:12766ms step_avg:92.51ms
step:139/1670 train_time:12860ms step_avg:92.52ms
step:140/1670 train_time:12955ms step_avg:92.53ms
step:141/1670 train_time:13046ms step_avg:92.53ms
step:142/1670 train_time:13139ms step_avg:92.53ms
step:143/1670 train_time:13230ms step_avg:92.52ms
step:144/1670 train_time:13320ms step_avg:92.50ms
step:145/1670 train_time:13411ms step_avg:92.49ms
step:146/1670 train_time:13501ms step_avg:92.47ms
step:147/1670 train_time:13590ms step_avg:92.45ms
step:148/1670 train_time:13681ms step_avg:92.44ms
step:149/1670 train_time:13773ms step_avg:92.44ms
step:150/1670 train_time:13865ms step_avg:92.43ms
step:151/1670 train_time:13958ms step_avg:92.44ms
step:152/1670 train_time:14049ms step_avg:92.43ms
step:153/1670 train_time:14141ms step_avg:92.42ms
step:154/1670 train_time:14233ms step_avg:92.42ms
step:155/1670 train_time:14323ms step_avg:92.40ms
step:156/1670 train_time:14413ms step_avg:92.39ms
step:157/1670 train_time:14503ms step_avg:92.38ms
step:158/1670 train_time:14594ms step_avg:92.36ms
step:159/1670 train_time:14684ms step_avg:92.36ms
step:160/1670 train_time:14778ms step_avg:92.36ms
step:161/1670 train_time:14869ms step_avg:92.36ms
step:162/1670 train_time:14962ms step_avg:92.36ms
step:163/1670 train_time:15054ms step_avg:92.36ms
step:164/1670 train_time:15146ms step_avg:92.35ms
step:165/1670 train_time:15237ms step_avg:92.34ms
step:166/1670 train_time:15327ms step_avg:92.33ms
step:167/1670 train_time:15418ms step_avg:92.32ms
step:168/1670 train_time:15508ms step_avg:92.31ms
step:169/1670 train_time:15599ms step_avg:92.30ms
step:170/1670 train_time:15690ms step_avg:92.30ms
step:171/1670 train_time:15783ms step_avg:92.30ms
step:172/1670 train_time:15875ms step_avg:92.30ms
step:173/1670 train_time:15966ms step_avg:92.29ms
step:174/1670 train_time:16058ms step_avg:92.29ms
step:175/1670 train_time:16150ms step_avg:92.29ms
step:176/1670 train_time:16242ms step_avg:92.28ms
step:177/1670 train_time:16333ms step_avg:92.28ms
step:178/1670 train_time:16423ms step_avg:92.26ms
step:179/1670 train_time:16514ms step_avg:92.25ms
step:180/1670 train_time:16604ms step_avg:92.24ms
step:181/1670 train_time:16696ms step_avg:92.24ms
step:182/1670 train_time:16786ms step_avg:92.23ms
step:183/1670 train_time:16879ms step_avg:92.24ms
step:184/1670 train_time:16971ms step_avg:92.24ms
step:185/1670 train_time:17063ms step_avg:92.23ms
step:186/1670 train_time:17155ms step_avg:92.23ms
step:187/1670 train_time:17247ms step_avg:92.23ms
step:188/1670 train_time:17339ms step_avg:92.23ms
step:189/1670 train_time:17430ms step_avg:92.22ms
step:190/1670 train_time:17520ms step_avg:92.21ms
step:191/1670 train_time:17611ms step_avg:92.20ms
step:192/1670 train_time:17703ms step_avg:92.20ms
step:193/1670 train_time:17795ms step_avg:92.20ms
step:194/1670 train_time:17885ms step_avg:92.19ms
step:195/1670 train_time:17977ms step_avg:92.19ms
step:196/1670 train_time:18069ms step_avg:92.19ms
step:197/1670 train_time:18161ms step_avg:92.19ms
step:198/1670 train_time:18253ms step_avg:92.19ms
step:199/1670 train_time:18345ms step_avg:92.18ms
step:200/1670 train_time:18438ms step_avg:92.19ms
step:201/1670 train_time:18527ms step_avg:92.17ms
step:202/1670 train_time:18619ms step_avg:92.17ms
step:203/1670 train_time:18709ms step_avg:92.16ms
step:204/1670 train_time:18801ms step_avg:92.16ms
step:205/1670 train_time:18891ms step_avg:92.15ms
step:206/1670 train_time:18982ms step_avg:92.15ms
step:207/1670 train_time:19073ms step_avg:92.14ms
step:208/1670 train_time:19164ms step_avg:92.14ms
step:209/1670 train_time:19257ms step_avg:92.14ms
step:210/1670 train_time:19347ms step_avg:92.13ms
step:211/1670 train_time:19438ms step_avg:92.12ms
step:212/1670 train_time:19529ms step_avg:92.12ms
step:213/1670 train_time:19782ms step_avg:92.87ms
step:214/1670 train_time:19851ms step_avg:92.76ms
step:215/1670 train_time:19941ms step_avg:92.75ms
step:216/1670 train_time:20031ms step_avg:92.74ms
step:217/1670 train_time:20121ms step_avg:92.73ms
step:218/1670 train_time:20211ms step_avg:92.71ms
step:219/1670 train_time:20301ms step_avg:92.70ms
step:220/1670 train_time:20391ms step_avg:92.69ms
step:221/1670 train_time:20481ms step_avg:92.67ms
step:222/1670 train_time:20572ms step_avg:92.67ms
step:223/1670 train_time:20666ms step_avg:92.67ms
step:224/1670 train_time:20761ms step_avg:92.68ms
step:225/1670 train_time:20854ms step_avg:92.69ms
step:226/1670 train_time:20946ms step_avg:92.68ms
step:227/1670 train_time:21037ms step_avg:92.67ms
step:228/1670 train_time:21127ms step_avg:92.66ms
step:229/1670 train_time:21217ms step_avg:92.65ms
step:230/1670 train_time:21308ms step_avg:92.64ms
step:231/1670 train_time:21400ms step_avg:92.64ms
step:232/1670 train_time:21491ms step_avg:92.63ms
step:233/1670 train_time:21583ms step_avg:92.63ms
step:234/1670 train_time:21676ms step_avg:92.63ms
step:235/1670 train_time:21769ms step_avg:92.63ms
step:236/1670 train_time:21862ms step_avg:92.64ms
step:237/1670 train_time:21955ms step_avg:92.64ms
step:238/1670 train_time:22045ms step_avg:92.63ms
step:239/1670 train_time:22136ms step_avg:92.62ms
step:240/1670 train_time:22226ms step_avg:92.61ms
step:241/1670 train_time:22316ms step_avg:92.60ms
step:242/1670 train_time:22406ms step_avg:92.59ms
step:243/1670 train_time:22498ms step_avg:92.58ms
step:244/1670 train_time:22589ms step_avg:92.58ms
step:245/1670 train_time:22683ms step_avg:92.58ms
step:246/1670 train_time:22775ms step_avg:92.58ms
step:247/1670 train_time:22867ms step_avg:92.58ms
step:248/1670 train_time:22961ms step_avg:92.58ms
step:249/1670 train_time:23052ms step_avg:92.58ms
step:250/1670 train_time:23144ms step_avg:92.57ms
step:250/1670 val_loss:3.9615 train_time:23234ms step_avg:92.94ms
step:251/1670 train_time:23252ms step_avg:92.64ms
step:252/1670 train_time:23327ms step_avg:92.57ms
step:253/1670 train_time:23419ms step_avg:92.57ms
step:254/1670 train_time:23516ms step_avg:92.58ms
step:255/1670 train_time:23608ms step_avg:92.58ms
step:256/1670 train_time:23699ms step_avg:92.57ms
step:257/1670 train_time:23789ms step_avg:92.56ms
step:258/1670 train_time:23880ms step_avg:92.56ms
step:259/1670 train_time:23969ms step_avg:92.55ms
step:260/1670 train_time:24060ms step_avg:92.54ms
step:261/1670 train_time:24153ms step_avg:92.54ms
step:262/1670 train_time:24248ms step_avg:92.55ms
step:263/1670 train_time:24340ms step_avg:92.55ms
step:264/1670 train_time:24432ms step_avg:92.55ms
step:265/1670 train_time:24525ms step_avg:92.55ms
step:266/1670 train_time:24618ms step_avg:92.55ms
step:267/1670 train_time:24709ms step_avg:92.54ms
step:268/1670 train_time:24800ms step_avg:92.54ms
step:269/1670 train_time:24890ms step_avg:92.53ms
step:270/1670 train_time:24981ms step_avg:92.52ms
step:271/1670 train_time:25071ms step_avg:92.51ms
step:272/1670 train_time:25162ms step_avg:92.51ms
step:273/1670 train_time:25253ms step_avg:92.50ms
step:274/1670 train_time:25347ms step_avg:92.51ms
step:275/1670 train_time:25439ms step_avg:92.50ms
step:276/1670 train_time:25530ms step_avg:92.50ms
step:277/1670 train_time:25624ms step_avg:92.50ms
step:278/1670 train_time:25717ms step_avg:92.51ms
step:279/1670 train_time:25807ms step_avg:92.50ms
step:280/1670 train_time:25898ms step_avg:92.49ms
step:281/1670 train_time:25989ms step_avg:92.49ms
step:282/1670 train_time:26080ms step_avg:92.48ms
step:283/1670 train_time:26170ms step_avg:92.47ms
step:284/1670 train_time:26261ms step_avg:92.47ms
step:285/1670 train_time:26353ms step_avg:92.47ms
step:286/1670 train_time:26444ms step_avg:92.46ms
step:287/1670 train_time:26536ms step_avg:92.46ms
step:288/1670 train_time:26628ms step_avg:92.46ms
step:289/1670 train_time:26720ms step_avg:92.46ms
step:290/1670 train_time:26811ms step_avg:92.45ms
step:291/1670 train_time:26902ms step_avg:92.45ms
step:292/1670 train_time:26993ms step_avg:92.44ms
step:293/1670 train_time:27085ms step_avg:92.44ms
step:294/1670 train_time:27175ms step_avg:92.43ms
step:295/1670 train_time:27266ms step_avg:92.43ms
step:296/1670 train_time:27358ms step_avg:92.42ms
step:297/1670 train_time:27449ms step_avg:92.42ms
step:298/1670 train_time:27541ms step_avg:92.42ms
step:299/1670 train_time:27634ms step_avg:92.42ms
step:300/1670 train_time:27725ms step_avg:92.42ms
step:301/1670 train_time:27817ms step_avg:92.41ms
step:302/1670 train_time:27908ms step_avg:92.41ms
step:303/1670 train_time:27999ms step_avg:92.41ms
step:304/1670 train_time:28090ms step_avg:92.40ms
step:305/1670 train_time:28181ms step_avg:92.40ms
step:306/1670 train_time:28273ms step_avg:92.40ms
step:307/1670 train_time:28365ms step_avg:92.39ms
step:308/1670 train_time:28456ms step_avg:92.39ms
step:309/1670 train_time:28547ms step_avg:92.38ms
step:310/1670 train_time:28639ms step_avg:92.38ms
step:311/1670 train_time:28729ms step_avg:92.38ms
step:312/1670 train_time:28822ms step_avg:92.38ms
step:313/1670 train_time:28914ms step_avg:92.38ms
step:314/1670 train_time:29006ms step_avg:92.37ms
step:315/1670 train_time:29096ms step_avg:92.37ms
step:316/1670 train_time:29187ms step_avg:92.36ms
step:317/1670 train_time:29278ms step_avg:92.36ms
step:318/1670 train_time:29369ms step_avg:92.36ms
step:319/1670 train_time:29460ms step_avg:92.35ms
step:320/1670 train_time:29551ms step_avg:92.35ms
step:321/1670 train_time:29642ms step_avg:92.34ms
step:322/1670 train_time:29733ms step_avg:92.34ms
step:323/1670 train_time:29824ms step_avg:92.33ms
step:324/1670 train_time:29916ms step_avg:92.33ms
step:325/1670 train_time:30007ms step_avg:92.33ms
step:326/1670 train_time:30098ms step_avg:92.33ms
step:327/1670 train_time:30189ms step_avg:92.32ms
step:328/1670 train_time:30281ms step_avg:92.32ms
step:329/1670 train_time:30372ms step_avg:92.32ms
step:330/1670 train_time:30464ms step_avg:92.31ms
step:331/1670 train_time:30554ms step_avg:92.31ms
step:332/1670 train_time:30645ms step_avg:92.30ms
step:333/1670 train_time:30735ms step_avg:92.30ms
step:334/1670 train_time:30826ms step_avg:92.29ms
step:335/1670 train_time:30918ms step_avg:92.29ms
step:336/1670 train_time:31008ms step_avg:92.29ms
step:337/1670 train_time:31101ms step_avg:92.29ms
step:338/1670 train_time:31193ms step_avg:92.29ms
step:339/1670 train_time:31284ms step_avg:92.28ms
step:340/1670 train_time:31375ms step_avg:92.28ms
step:341/1670 train_time:31466ms step_avg:92.28ms
step:342/1670 train_time:31558ms step_avg:92.28ms
step:343/1670 train_time:31649ms step_avg:92.27ms
step:344/1670 train_time:31740ms step_avg:92.27ms
step:345/1670 train_time:31831ms step_avg:92.26ms
step:346/1670 train_time:31923ms step_avg:92.26ms
step:347/1670 train_time:32014ms step_avg:92.26ms
step:348/1670 train_time:32106ms step_avg:92.26ms
step:349/1670 train_time:32198ms step_avg:92.26ms
step:350/1670 train_time:32289ms step_avg:92.25ms
step:351/1670 train_time:32381ms step_avg:92.25ms
step:352/1670 train_time:32472ms step_avg:92.25ms
step:353/1670 train_time:32564ms step_avg:92.25ms
step:354/1670 train_time:32656ms step_avg:92.25ms
step:355/1670 train_time:32747ms step_avg:92.25ms
step:356/1670 train_time:32838ms step_avg:92.24ms
step:357/1670 train_time:32929ms step_avg:92.24ms
step:358/1670 train_time:33021ms step_avg:92.24ms
step:359/1670 train_time:33112ms step_avg:92.23ms
step:360/1670 train_time:33205ms step_avg:92.24ms
step:361/1670 train_time:33296ms step_avg:92.23ms
step:362/1670 train_time:33387ms step_avg:92.23ms
step:363/1670 train_time:33479ms step_avg:92.23ms
step:364/1670 train_time:33569ms step_avg:92.22ms
step:365/1670 train_time:33661ms step_avg:92.22ms
step:366/1670 train_time:33752ms step_avg:92.22ms
step:367/1670 train_time:33844ms step_avg:92.22ms
step:368/1670 train_time:33936ms step_avg:92.22ms
step:369/1670 train_time:34026ms step_avg:92.21ms
step:370/1670 train_time:34117ms step_avg:92.21ms
step:371/1670 train_time:34208ms step_avg:92.20ms
step:372/1670 train_time:34300ms step_avg:92.20ms
step:373/1670 train_time:34391ms step_avg:92.20ms
step:374/1670 train_time:34482ms step_avg:92.20ms
step:375/1670 train_time:34573ms step_avg:92.20ms
step:375/1670 val_loss:3.8098 train_time:34664ms step_avg:92.44ms
step:376/1670 train_time:34681ms step_avg:92.24ms
step:377/1670 train_time:34759ms step_avg:92.20ms
step:378/1670 train_time:34852ms step_avg:92.20ms
step:379/1670 train_time:34942ms step_avg:92.20ms
step:380/1670 train_time:35033ms step_avg:92.19ms
step:381/1670 train_time:35124ms step_avg:92.19ms
step:382/1670 train_time:35215ms step_avg:92.19ms
step:383/1670 train_time:35306ms step_avg:92.18ms
step:384/1670 train_time:35397ms step_avg:92.18ms
step:385/1670 train_time:35488ms step_avg:92.18ms
step:386/1670 train_time:35579ms step_avg:92.17ms
step:387/1670 train_time:35671ms step_avg:92.17ms
step:388/1670 train_time:35764ms step_avg:92.18ms
step:389/1670 train_time:35857ms step_avg:92.18ms
step:390/1670 train_time:35948ms step_avg:92.17ms
step:391/1670 train_time:36039ms step_avg:92.17ms
step:392/1670 train_time:36130ms step_avg:92.17ms
step:393/1670 train_time:36220ms step_avg:92.16ms
step:394/1670 train_time:36313ms step_avg:92.16ms
step:395/1670 train_time:36403ms step_avg:92.16ms
step:396/1670 train_time:36495ms step_avg:92.16ms
step:397/1670 train_time:36586ms step_avg:92.16ms
step:398/1670 train_time:36679ms step_avg:92.16ms
step:399/1670 train_time:36771ms step_avg:92.16ms
step:400/1670 train_time:36862ms step_avg:92.16ms
step:401/1670 train_time:36954ms step_avg:92.15ms
step:402/1670 train_time:37045ms step_avg:92.15ms
step:403/1670 train_time:37136ms step_avg:92.15ms
step:404/1670 train_time:37227ms step_avg:92.15ms
step:405/1670 train_time:37317ms step_avg:92.14ms
step:406/1670 train_time:37407ms step_avg:92.14ms
step:407/1670 train_time:37497ms step_avg:92.13ms
step:408/1670 train_time:37588ms step_avg:92.13ms
step:409/1670 train_time:37679ms step_avg:92.12ms
step:410/1670 train_time:37771ms step_avg:92.12ms
step:411/1670 train_time:37862ms step_avg:92.12ms
step:412/1670 train_time:37955ms step_avg:92.12ms
step:413/1670 train_time:38046ms step_avg:92.12ms
step:414/1670 train_time:38137ms step_avg:92.12ms
step:415/1670 train_time:38228ms step_avg:92.11ms
step:416/1670 train_time:38318ms step_avg:92.11ms
step:417/1670 train_time:38408ms step_avg:92.11ms
step:418/1670 train_time:38499ms step_avg:92.10ms
step:419/1670 train_time:38590ms step_avg:92.10ms
step:420/1670 train_time:38681ms step_avg:92.10ms
step:421/1670 train_time:38773ms step_avg:92.10ms
step:422/1670 train_time:38864ms step_avg:92.10ms
step:423/1670 train_time:38956ms step_avg:92.10ms
step:424/1670 train_time:39048ms step_avg:92.09ms
step:425/1670 train_time:39295ms step_avg:92.46ms
step:426/1670 train_time:39366ms step_avg:92.41ms
step:427/1670 train_time:39456ms step_avg:92.40ms
step:428/1670 train_time:39546ms step_avg:92.40ms
step:429/1670 train_time:39636ms step_avg:92.39ms
step:430/1670 train_time:39727ms step_avg:92.39ms
step:431/1670 train_time:39817ms step_avg:92.38ms
step:432/1670 train_time:39907ms step_avg:92.38ms
step:433/1670 train_time:39997ms step_avg:92.37ms
step:434/1670 train_time:40087ms step_avg:92.37ms
step:435/1670 train_time:40183ms step_avg:92.37ms
step:436/1670 train_time:40281ms step_avg:92.39ms
step:437/1670 train_time:40375ms step_avg:92.39ms
step:438/1670 train_time:40466ms step_avg:92.39ms
step:439/1670 train_time:40556ms step_avg:92.38ms
step:440/1670 train_time:40646ms step_avg:92.38ms
step:441/1670 train_time:40736ms step_avg:92.37ms
step:442/1670 train_time:40827ms step_avg:92.37ms
step:443/1670 train_time:40917ms step_avg:92.36ms
step:444/1670 train_time:41007ms step_avg:92.36ms
step:445/1670 train_time:41098ms step_avg:92.36ms
step:446/1670 train_time:41193ms step_avg:92.36ms
step:447/1670 train_time:41285ms step_avg:92.36ms
step:448/1670 train_time:41377ms step_avg:92.36ms
step:449/1670 train_time:41469ms step_avg:92.36ms
step:450/1670 train_time:41560ms step_avg:92.35ms
step:451/1670 train_time:41650ms step_avg:92.35ms
step:452/1670 train_time:41741ms step_avg:92.35ms
step:453/1670 train_time:41831ms step_avg:92.34ms
step:454/1670 train_time:41921ms step_avg:92.34ms
step:455/1670 train_time:42012ms step_avg:92.33ms
step:456/1670 train_time:42104ms step_avg:92.33ms
step:457/1670 train_time:42197ms step_avg:92.34ms
step:458/1670 train_time:42291ms step_avg:92.34ms
step:459/1670 train_time:42381ms step_avg:92.33ms
step:460/1670 train_time:42474ms step_avg:92.33ms
step:461/1670 train_time:42564ms step_avg:92.33ms
step:462/1670 train_time:42656ms step_avg:92.33ms
step:463/1670 train_time:42747ms step_avg:92.33ms
step:464/1670 train_time:42837ms step_avg:92.32ms
step:465/1670 train_time:42928ms step_avg:92.32ms
step:466/1670 train_time:43019ms step_avg:92.32ms
step:467/1670 train_time:43110ms step_avg:92.31ms
step:468/1670 train_time:43202ms step_avg:92.31ms
step:469/1670 train_time:43295ms step_avg:92.31ms
step:470/1670 train_time:43388ms step_avg:92.32ms
step:471/1670 train_time:43480ms step_avg:92.31ms
step:472/1670 train_time:43571ms step_avg:92.31ms
step:473/1670 train_time:43662ms step_avg:92.31ms
step:474/1670 train_time:43754ms step_avg:92.31ms
step:475/1670 train_time:43845ms step_avg:92.31ms
step:476/1670 train_time:43935ms step_avg:92.30ms
step:477/1670 train_time:44026ms step_avg:92.30ms
step:478/1670 train_time:44117ms step_avg:92.29ms
step:479/1670 train_time:44208ms step_avg:92.29ms
step:480/1670 train_time:44300ms step_avg:92.29ms
step:481/1670 train_time:44392ms step_avg:92.29ms
step:482/1670 train_time:44483ms step_avg:92.29ms
step:483/1670 train_time:44574ms step_avg:92.29ms
step:484/1670 train_time:44665ms step_avg:92.28ms
step:485/1670 train_time:44757ms step_avg:92.28ms
step:486/1670 train_time:44848ms step_avg:92.28ms
step:487/1670 train_time:44939ms step_avg:92.28ms
step:488/1670 train_time:45030ms step_avg:92.28ms
step:489/1670 train_time:45121ms step_avg:92.27ms
step:490/1670 train_time:45213ms step_avg:92.27ms
step:491/1670 train_time:45304ms step_avg:92.27ms
step:492/1670 train_time:45397ms step_avg:92.27ms
step:493/1670 train_time:45488ms step_avg:92.27ms
step:494/1670 train_time:45580ms step_avg:92.27ms
step:495/1670 train_time:45670ms step_avg:92.26ms
step:496/1670 train_time:45760ms step_avg:92.26ms
step:497/1670 train_time:45851ms step_avg:92.26ms
step:498/1670 train_time:45943ms step_avg:92.26ms
step:499/1670 train_time:46034ms step_avg:92.25ms
step:500/1670 train_time:46126ms step_avg:92.25ms
step:500/1670 val_loss:3.7097 train_time:46217ms step_avg:92.43ms
step:501/1670 train_time:46234ms step_avg:92.28ms
step:502/1670 train_time:46311ms step_avg:92.25ms
step:503/1670 train_time:46403ms step_avg:92.25ms
step:504/1670 train_time:46495ms step_avg:92.25ms
step:505/1670 train_time:46585ms step_avg:92.25ms
step:506/1670 train_time:46677ms step_avg:92.25ms
step:507/1670 train_time:46768ms step_avg:92.24ms
step:508/1670 train_time:46859ms step_avg:92.24ms
step:509/1670 train_time:46949ms step_avg:92.24ms
step:510/1670 train_time:47041ms step_avg:92.24ms
step:511/1670 train_time:47132ms step_avg:92.24ms
step:512/1670 train_time:47227ms step_avg:92.24ms
step:513/1670 train_time:47318ms step_avg:92.24ms
step:514/1670 train_time:47410ms step_avg:92.24ms
step:515/1670 train_time:47501ms step_avg:92.23ms
step:516/1670 train_time:47591ms step_avg:92.23ms
step:517/1670 train_time:47682ms step_avg:92.23ms
step:518/1670 train_time:47774ms step_avg:92.23ms
step:519/1670 train_time:47865ms step_avg:92.23ms
step:520/1670 train_time:47956ms step_avg:92.22ms
step:521/1670 train_time:48047ms step_avg:92.22ms
step:522/1670 train_time:48138ms step_avg:92.22ms
step:523/1670 train_time:48230ms step_avg:92.22ms
step:524/1670 train_time:48322ms step_avg:92.22ms
step:525/1670 train_time:48413ms step_avg:92.22ms
step:526/1670 train_time:48506ms step_avg:92.22ms
step:527/1670 train_time:48597ms step_avg:92.21ms
step:528/1670 train_time:48688ms step_avg:92.21ms
step:529/1670 train_time:48779ms step_avg:92.21ms
step:530/1670 train_time:48869ms step_avg:92.21ms
step:531/1670 train_time:48960ms step_avg:92.20ms
step:532/1670 train_time:49050ms step_avg:92.20ms
step:533/1670 train_time:49142ms step_avg:92.20ms
step:534/1670 train_time:49234ms step_avg:92.20ms
step:535/1670 train_time:49327ms step_avg:92.20ms
step:536/1670 train_time:49418ms step_avg:92.20ms
step:537/1670 train_time:49509ms step_avg:92.20ms
step:538/1670 train_time:49600ms step_avg:92.19ms
step:539/1670 train_time:49691ms step_avg:92.19ms
step:540/1670 train_time:49782ms step_avg:92.19ms
step:541/1670 train_time:49872ms step_avg:92.18ms
step:542/1670 train_time:49963ms step_avg:92.18ms
step:543/1670 train_time:50055ms step_avg:92.18ms
step:544/1670 train_time:50148ms step_avg:92.18ms
step:545/1670 train_time:50239ms step_avg:92.18ms
step:546/1670 train_time:50330ms step_avg:92.18ms
step:547/1670 train_time:50423ms step_avg:92.18ms
step:548/1670 train_time:50513ms step_avg:92.18ms
step:549/1670 train_time:50605ms step_avg:92.18ms
step:550/1670 train_time:50695ms step_avg:92.17ms
step:551/1670 train_time:50786ms step_avg:92.17ms
step:552/1670 train_time:50877ms step_avg:92.17ms
step:553/1670 train_time:50969ms step_avg:92.17ms
step:554/1670 train_time:51060ms step_avg:92.17ms
step:555/1670 train_time:51151ms step_avg:92.16ms
step:556/1670 train_time:51243ms step_avg:92.16ms
step:557/1670 train_time:51334ms step_avg:92.16ms
step:558/1670 train_time:51619ms step_avg:92.51ms
step:559/1670 train_time:51697ms step_avg:92.48ms
step:560/1670 train_time:51787ms step_avg:92.48ms
step:561/1670 train_time:51878ms step_avg:92.47ms
step:562/1670 train_time:51969ms step_avg:92.47ms
step:563/1670 train_time:52061ms step_avg:92.47ms
step:564/1670 train_time:52152ms step_avg:92.47ms
step:565/1670 train_time:52243ms step_avg:92.47ms
step:566/1670 train_time:52334ms step_avg:92.46ms
step:567/1670 train_time:52426ms step_avg:92.46ms
step:568/1670 train_time:52523ms step_avg:92.47ms
step:569/1670 train_time:52620ms step_avg:92.48ms
step:570/1670 train_time:52714ms step_avg:92.48ms
step:571/1670 train_time:52807ms step_avg:92.48ms
step:572/1670 train_time:52899ms step_avg:92.48ms
step:573/1670 train_time:52990ms step_avg:92.48ms
step:574/1670 train_time:53082ms step_avg:92.48ms
step:575/1670 train_time:53173ms step_avg:92.47ms
step:576/1670 train_time:53265ms step_avg:92.47ms
step:577/1670 train_time:53357ms step_avg:92.47ms
step:578/1670 train_time:53450ms step_avg:92.47ms
step:579/1670 train_time:53545ms step_avg:92.48ms
step:580/1670 train_time:53640ms step_avg:92.48ms
step:581/1670 train_time:53732ms step_avg:92.48ms
step:582/1670 train_time:53826ms step_avg:92.48ms
step:583/1670 train_time:53918ms step_avg:92.48ms
step:584/1670 train_time:54010ms step_avg:92.48ms
step:585/1670 train_time:54103ms step_avg:92.48ms
step:586/1670 train_time:54195ms step_avg:92.48ms
step:587/1670 train_time:54287ms step_avg:92.48ms
step:588/1670 train_time:54379ms step_avg:92.48ms
step:589/1670 train_time:54471ms step_avg:92.48ms
step:590/1670 train_time:54565ms step_avg:92.48ms
step:591/1670 train_time:54660ms step_avg:92.49ms
step:592/1670 train_time:54753ms step_avg:92.49ms
step:593/1670 train_time:54847ms step_avg:92.49ms
step:594/1670 train_time:54939ms step_avg:92.49ms
step:595/1670 train_time:55031ms step_avg:92.49ms
step:596/1670 train_time:55124ms step_avg:92.49ms
step:597/1670 train_time:55215ms step_avg:92.49ms
step:598/1670 train_time:55307ms step_avg:92.49ms
step:599/1670 train_time:55399ms step_avg:92.49ms
step:600/1670 train_time:55492ms step_avg:92.49ms
step:601/1670 train_time:55586ms step_avg:92.49ms
step:602/1670 train_time:55679ms step_avg:92.49ms
step:603/1670 train_time:55771ms step_avg:92.49ms
step:604/1670 train_time:55865ms step_avg:92.49ms
step:605/1670 train_time:55956ms step_avg:92.49ms
step:606/1670 train_time:56049ms step_avg:92.49ms
step:607/1670 train_time:56141ms step_avg:92.49ms
step:608/1670 train_time:56233ms step_avg:92.49ms
step:609/1670 train_time:56325ms step_avg:92.49ms
step:610/1670 train_time:56418ms step_avg:92.49ms
step:611/1670 train_time:56510ms step_avg:92.49ms
step:612/1670 train_time:56603ms step_avg:92.49ms
step:613/1670 train_time:56695ms step_avg:92.49ms
step:614/1670 train_time:56788ms step_avg:92.49ms
step:615/1670 train_time:56881ms step_avg:92.49ms
step:616/1670 train_time:56973ms step_avg:92.49ms
step:617/1670 train_time:57066ms step_avg:92.49ms
step:618/1670 train_time:57158ms step_avg:92.49ms
step:619/1670 train_time:57250ms step_avg:92.49ms
step:620/1670 train_time:57343ms step_avg:92.49ms
step:621/1670 train_time:57435ms step_avg:92.49ms
step:622/1670 train_time:57528ms step_avg:92.49ms
step:623/1670 train_time:57621ms step_avg:92.49ms
step:624/1670 train_time:57714ms step_avg:92.49ms
step:625/1670 train_time:57807ms step_avg:92.49ms
step:625/1670 val_loss:3.6092 train_time:57899ms step_avg:92.64ms
step:626/1670 train_time:57917ms step_avg:92.52ms
step:627/1670 train_time:58000ms step_avg:92.50ms
step:628/1670 train_time:58099ms step_avg:92.51ms
step:629/1670 train_time:58193ms step_avg:92.52ms
step:630/1670 train_time:58286ms step_avg:92.52ms
step:631/1670 train_time:58376ms step_avg:92.51ms
step:632/1670 train_time:58467ms step_avg:92.51ms
step:633/1670 train_time:58558ms step_avg:92.51ms
step:634/1670 train_time:58650ms step_avg:92.51ms
step:635/1670 train_time:58741ms step_avg:92.51ms
step:636/1670 train_time:58832ms step_avg:92.50ms
step:637/1670 train_time:58926ms step_avg:92.51ms
step:638/1670 train_time:59021ms step_avg:92.51ms
step:639/1670 train_time:59256ms step_avg:92.73ms
step:640/1670 train_time:59331ms step_avg:92.70ms
step:641/1670 train_time:59422ms step_avg:92.70ms
step:642/1670 train_time:59513ms step_avg:92.70ms
step:643/1670 train_time:59604ms step_avg:92.70ms
step:644/1670 train_time:59695ms step_avg:92.69ms
step:645/1670 train_time:59787ms step_avg:92.69ms
step:646/1670 train_time:59878ms step_avg:92.69ms
step:647/1670 train_time:59969ms step_avg:92.69ms
step:648/1670 train_time:60060ms step_avg:92.69ms
step:649/1670 train_time:60161ms step_avg:92.70ms
step:650/1670 train_time:60258ms step_avg:92.70ms
step:651/1670 train_time:60353ms step_avg:92.71ms
step:652/1670 train_time:60446ms step_avg:92.71ms
step:653/1670 train_time:60537ms step_avg:92.71ms
step:654/1670 train_time:60628ms step_avg:92.70ms
step:655/1670 train_time:60720ms step_avg:92.70ms
step:656/1670 train_time:60811ms step_avg:92.70ms
step:657/1670 train_time:60902ms step_avg:92.70ms
step:658/1670 train_time:60994ms step_avg:92.70ms
step:659/1670 train_time:61089ms step_avg:92.70ms
step:660/1670 train_time:61183ms step_avg:92.70ms
step:661/1670 train_time:61277ms step_avg:92.70ms
step:662/1670 train_time:61371ms step_avg:92.71ms
step:663/1670 train_time:61464ms step_avg:92.71ms
step:664/1670 train_time:61557ms step_avg:92.71ms
step:665/1670 train_time:61648ms step_avg:92.70ms
step:666/1670 train_time:61740ms step_avg:92.70ms
step:667/1670 train_time:61831ms step_avg:92.70ms
step:668/1670 train_time:61923ms step_avg:92.70ms
step:669/1670 train_time:62014ms step_avg:92.70ms
step:670/1670 train_time:62108ms step_avg:92.70ms
step:671/1670 train_time:62201ms step_avg:92.70ms
step:672/1670 train_time:62296ms step_avg:92.70ms
step:673/1670 train_time:62390ms step_avg:92.70ms
step:674/1670 train_time:62482ms step_avg:92.70ms
step:675/1670 train_time:62576ms step_avg:92.70ms
step:676/1670 train_time:62668ms step_avg:92.70ms
step:677/1670 train_time:62760ms step_avg:92.70ms
step:678/1670 train_time:62851ms step_avg:92.70ms
step:679/1670 train_time:62943ms step_avg:92.70ms
step:680/1670 train_time:63034ms step_avg:92.70ms
step:681/1670 train_time:63127ms step_avg:92.70ms
step:682/1670 train_time:63220ms step_avg:92.70ms
step:683/1670 train_time:63313ms step_avg:92.70ms
step:684/1670 train_time:63407ms step_avg:92.70ms
step:685/1670 train_time:63499ms step_avg:92.70ms
step:686/1670 train_time:63593ms step_avg:92.70ms
step:687/1670 train_time:63686ms step_avg:92.70ms
step:688/1670 train_time:63777ms step_avg:92.70ms
step:689/1670 train_time:63870ms step_avg:92.70ms
step:690/1670 train_time:63961ms step_avg:92.70ms
step:691/1670 train_time:64053ms step_avg:92.70ms
step:692/1670 train_time:64146ms step_avg:92.70ms
step:693/1670 train_time:64239ms step_avg:92.70ms
step:694/1670 train_time:64332ms step_avg:92.70ms
step:695/1670 train_time:64425ms step_avg:92.70ms
step:696/1670 train_time:64518ms step_avg:92.70ms
step:697/1670 train_time:64612ms step_avg:92.70ms
step:698/1670 train_time:64704ms step_avg:92.70ms
step:699/1670 train_time:64797ms step_avg:92.70ms
step:700/1670 train_time:64889ms step_avg:92.70ms
step:701/1670 train_time:64981ms step_avg:92.70ms
step:702/1670 train_time:65074ms step_avg:92.70ms
step:703/1670 train_time:65167ms step_avg:92.70ms
step:704/1670 train_time:65260ms step_avg:92.70ms
step:705/1670 train_time:65352ms step_avg:92.70ms
step:706/1670 train_time:65445ms step_avg:92.70ms
step:707/1670 train_time:65537ms step_avg:92.70ms
step:708/1670 train_time:65629ms step_avg:92.70ms
step:709/1670 train_time:65723ms step_avg:92.70ms
step:710/1670 train_time:65815ms step_avg:92.70ms
step:711/1670 train_time:65908ms step_avg:92.70ms
step:712/1670 train_time:65999ms step_avg:92.70ms
step:713/1670 train_time:66092ms step_avg:92.70ms
step:714/1670 train_time:66185ms step_avg:92.70ms
step:715/1670 train_time:66278ms step_avg:92.70ms
step:716/1670 train_time:66371ms step_avg:92.70ms
step:717/1670 train_time:66463ms step_avg:92.70ms
step:718/1670 train_time:66557ms step_avg:92.70ms
step:719/1670 train_time:66649ms step_avg:92.70ms
step:720/1670 train_time:66742ms step_avg:92.70ms
step:721/1670 train_time:66834ms step_avg:92.70ms
step:722/1670 train_time:66926ms step_avg:92.70ms
step:723/1670 train_time:67018ms step_avg:92.69ms
step:724/1670 train_time:67111ms step_avg:92.69ms
step:725/1670 train_time:67202ms step_avg:92.69ms
step:726/1670 train_time:67297ms step_avg:92.70ms
step:727/1670 train_time:67391ms step_avg:92.70ms
step:728/1670 train_time:67483ms step_avg:92.70ms
step:729/1670 train_time:67575ms step_avg:92.70ms
step:730/1670 train_time:67668ms step_avg:92.70ms
step:731/1670 train_time:67760ms step_avg:92.69ms
step:732/1670 train_time:67853ms step_avg:92.69ms
step:733/1670 train_time:67945ms step_avg:92.69ms
step:734/1670 train_time:68037ms step_avg:92.69ms
step:735/1670 train_time:68129ms step_avg:92.69ms
step:736/1670 train_time:68221ms step_avg:92.69ms
step:737/1670 train_time:68316ms step_avg:92.69ms
step:738/1670 train_time:68409ms step_avg:92.70ms
step:739/1670 train_time:68501ms step_avg:92.69ms
step:740/1670 train_time:68595ms step_avg:92.70ms
step:741/1670 train_time:68687ms step_avg:92.70ms
step:742/1670 train_time:68779ms step_avg:92.69ms
step:743/1670 train_time:68872ms step_avg:92.69ms
step:744/1670 train_time:68963ms step_avg:92.69ms
step:745/1670 train_time:69056ms step_avg:92.69ms
step:746/1670 train_time:69149ms step_avg:92.69ms
step:747/1670 train_time:69242ms step_avg:92.69ms
step:748/1670 train_time:69335ms step_avg:92.69ms
step:749/1670 train_time:69428ms step_avg:92.69ms
step:750/1670 train_time:69520ms step_avg:92.69ms
step:750/1670 val_loss:3.5584 train_time:69612ms step_avg:92.82ms
step:751/1670 train_time:69629ms step_avg:92.72ms
step:752/1670 train_time:69707ms step_avg:92.70ms
step:753/1670 train_time:69800ms step_avg:92.70ms
step:754/1670 train_time:69892ms step_avg:92.69ms
step:755/1670 train_time:69983ms step_avg:92.69ms
step:756/1670 train_time:70075ms step_avg:92.69ms
step:757/1670 train_time:70167ms step_avg:92.69ms
step:758/1670 train_time:70259ms step_avg:92.69ms
step:759/1670 train_time:70352ms step_avg:92.69ms
step:760/1670 train_time:70445ms step_avg:92.69ms
step:761/1670 train_time:70538ms step_avg:92.69ms
step:762/1670 train_time:70633ms step_avg:92.69ms
step:763/1670 train_time:70726ms step_avg:92.69ms
step:764/1670 train_time:70819ms step_avg:92.70ms
step:765/1670 train_time:70912ms step_avg:92.70ms
step:766/1670 train_time:71004ms step_avg:92.69ms
step:767/1670 train_time:71096ms step_avg:92.69ms
step:768/1670 train_time:71187ms step_avg:92.69ms
step:769/1670 train_time:71279ms step_avg:92.69ms
step:770/1670 train_time:71372ms step_avg:92.69ms
step:771/1670 train_time:71465ms step_avg:92.69ms
step:772/1670 train_time:71559ms step_avg:92.69ms
step:773/1670 train_time:71653ms step_avg:92.69ms
step:774/1670 train_time:71745ms step_avg:92.69ms
step:775/1670 train_time:71837ms step_avg:92.69ms
step:776/1670 train_time:71931ms step_avg:92.69ms
step:777/1670 train_time:72023ms step_avg:92.69ms
step:778/1670 train_time:72115ms step_avg:92.69ms
step:779/1670 train_time:72207ms step_avg:92.69ms
step:780/1670 train_time:72299ms step_avg:92.69ms
step:781/1670 train_time:72391ms step_avg:92.69ms
step:782/1670 train_time:72484ms step_avg:92.69ms
step:783/1670 train_time:72577ms step_avg:92.69ms
step:784/1670 train_time:72670ms step_avg:92.69ms
step:785/1670 train_time:72763ms step_avg:92.69ms
step:786/1670 train_time:72856ms step_avg:92.69ms
step:787/1670 train_time:72949ms step_avg:92.69ms
step:788/1670 train_time:73042ms step_avg:92.69ms
step:789/1670 train_time:73135ms step_avg:92.69ms
step:790/1670 train_time:73226ms step_avg:92.69ms
step:791/1670 train_time:73318ms step_avg:92.69ms
step:792/1670 train_time:73411ms step_avg:92.69ms
step:793/1670 train_time:73504ms step_avg:92.69ms
step:794/1670 train_time:73597ms step_avg:92.69ms
step:795/1670 train_time:73691ms step_avg:92.69ms
step:796/1670 train_time:73784ms step_avg:92.69ms
step:797/1670 train_time:73877ms step_avg:92.69ms
step:798/1670 train_time:73972ms step_avg:92.70ms
step:799/1670 train_time:74065ms step_avg:92.70ms
step:800/1670 train_time:74156ms step_avg:92.69ms
step:801/1670 train_time:74250ms step_avg:92.70ms
step:802/1670 train_time:74343ms step_avg:92.70ms
step:803/1670 train_time:74435ms step_avg:92.70ms
step:804/1670 train_time:74527ms step_avg:92.69ms
step:805/1670 train_time:74619ms step_avg:92.69ms
step:806/1670 train_time:74711ms step_avg:92.69ms
step:807/1670 train_time:74804ms step_avg:92.69ms
step:808/1670 train_time:74896ms step_avg:92.69ms
step:809/1670 train_time:74989ms step_avg:92.69ms
step:810/1670 train_time:75082ms step_avg:92.69ms
step:811/1670 train_time:75174ms step_avg:92.69ms
step:812/1670 train_time:75267ms step_avg:92.69ms
step:813/1670 train_time:75359ms step_avg:92.69ms
step:814/1670 train_time:75452ms step_avg:92.69ms
step:815/1670 train_time:75544ms step_avg:92.69ms
step:816/1670 train_time:75637ms step_avg:92.69ms
step:817/1670 train_time:75730ms step_avg:92.69ms
step:818/1670 train_time:75822ms step_avg:92.69ms
step:819/1670 train_time:75915ms step_avg:92.69ms
step:820/1670 train_time:76009ms step_avg:92.69ms
step:821/1670 train_time:76102ms step_avg:92.69ms
step:822/1670 train_time:76193ms step_avg:92.69ms
step:823/1670 train_time:76286ms step_avg:92.69ms
step:824/1670 train_time:76378ms step_avg:92.69ms
step:825/1670 train_time:76472ms step_avg:92.69ms
step:826/1670 train_time:76564ms step_avg:92.69ms
step:827/1670 train_time:76656ms step_avg:92.69ms
step:828/1670 train_time:76749ms step_avg:92.69ms
step:829/1670 train_time:76840ms step_avg:92.69ms
step:830/1670 train_time:76933ms step_avg:92.69ms
step:831/1670 train_time:77025ms step_avg:92.69ms
step:832/1670 train_time:77117ms step_avg:92.69ms
step:833/1670 train_time:77211ms step_avg:92.69ms
step:834/1670 train_time:77304ms step_avg:92.69ms
step:835/1670 train_time:77396ms step_avg:92.69ms
step:836/1670 train_time:77489ms step_avg:92.69ms
step:837/1670 train_time:77581ms step_avg:92.69ms
step:838/1670 train_time:77674ms step_avg:92.69ms
step:839/1670 train_time:77766ms step_avg:92.69ms
step:840/1670 train_time:77858ms step_avg:92.69ms
step:841/1670 train_time:77952ms step_avg:92.69ms
step:842/1670 train_time:78044ms step_avg:92.69ms
step:843/1670 train_time:78136ms step_avg:92.69ms
step:844/1670 train_time:78230ms step_avg:92.69ms
step:845/1670 train_time:78321ms step_avg:92.69ms
step:846/1670 train_time:78414ms step_avg:92.69ms
step:847/1670 train_time:78508ms step_avg:92.69ms
step:848/1670 train_time:78600ms step_avg:92.69ms
step:849/1670 train_time:78693ms step_avg:92.69ms
step:850/1670 train_time:78785ms step_avg:92.69ms
step:851/1670 train_time:79032ms step_avg:92.87ms
step:852/1670 train_time:79108ms step_avg:92.85ms
step:853/1670 train_time:79199ms step_avg:92.85ms
step:854/1670 train_time:79290ms step_avg:92.85ms
step:855/1670 train_time:79381ms step_avg:92.84ms
step:856/1670 train_time:79472ms step_avg:92.84ms
step:857/1670 train_time:79564ms step_avg:92.84ms
step:858/1670 train_time:79655ms step_avg:92.84ms
step:859/1670 train_time:79746ms step_avg:92.84ms
step:860/1670 train_time:79837ms step_avg:92.83ms
step:861/1670 train_time:79935ms step_avg:92.84ms
step:862/1670 train_time:80034ms step_avg:92.85ms
step:863/1670 train_time:80128ms step_avg:92.85ms
step:864/1670 train_time:80220ms step_avg:92.85ms
step:865/1670 train_time:80312ms step_avg:92.85ms
step:866/1670 train_time:80404ms step_avg:92.84ms
step:867/1670 train_time:80495ms step_avg:92.84ms
step:868/1670 train_time:80588ms step_avg:92.84ms
step:869/1670 train_time:80680ms step_avg:92.84ms
step:870/1670 train_time:80771ms step_avg:92.84ms
step:871/1670 train_time:80864ms step_avg:92.84ms
step:872/1670 train_time:80958ms step_avg:92.84ms
step:873/1670 train_time:81054ms step_avg:92.85ms
step:874/1670 train_time:81149ms step_avg:92.85ms
step:875/1670 train_time:81241ms step_avg:92.85ms
step:875/1670 val_loss:3.5154 train_time:81333ms step_avg:92.95ms
step:876/1670 train_time:81350ms step_avg:92.87ms
step:877/1670 train_time:81428ms step_avg:92.85ms
step:878/1670 train_time:81521ms step_avg:92.85ms
step:879/1670 train_time:81612ms step_avg:92.85ms
step:880/1670 train_time:81705ms step_avg:92.85ms
step:881/1670 train_time:81796ms step_avg:92.84ms
step:882/1670 train_time:81887ms step_avg:92.84ms
step:883/1670 train_time:81979ms step_avg:92.84ms
step:884/1670 train_time:82071ms step_avg:92.84ms
step:885/1670 train_time:82165ms step_avg:92.84ms
step:886/1670 train_time:82258ms step_avg:92.84ms
step:887/1670 train_time:82353ms step_avg:92.84ms
step:888/1670 train_time:82447ms step_avg:92.85ms
step:889/1670 train_time:82540ms step_avg:92.85ms
step:890/1670 train_time:82632ms step_avg:92.84ms
step:891/1670 train_time:82724ms step_avg:92.84ms
step:892/1670 train_time:82815ms step_avg:92.84ms
step:893/1670 train_time:82907ms step_avg:92.84ms
step:894/1670 train_time:83000ms step_avg:92.84ms
step:895/1670 train_time:83091ms step_avg:92.84ms
step:896/1670 train_time:83185ms step_avg:92.84ms
step:897/1670 train_time:83280ms step_avg:92.84ms
step:898/1670 train_time:83373ms step_avg:92.84ms
step:899/1670 train_time:83466ms step_avg:92.84ms
step:900/1670 train_time:83560ms step_avg:92.84ms
step:901/1670 train_time:83652ms step_avg:92.84ms
step:902/1670 train_time:83744ms step_avg:92.84ms
step:903/1670 train_time:83836ms step_avg:92.84ms
step:904/1670 train_time:83928ms step_avg:92.84ms
step:905/1670 train_time:84020ms step_avg:92.84ms
step:906/1670 train_time:84111ms step_avg:92.84ms
step:907/1670 train_time:84205ms step_avg:92.84ms
step:908/1670 train_time:84298ms step_avg:92.84ms
step:909/1670 train_time:84392ms step_avg:92.84ms
step:910/1670 train_time:84485ms step_avg:92.84ms
step:911/1670 train_time:84579ms step_avg:92.84ms
step:912/1670 train_time:84671ms step_avg:92.84ms
step:913/1670 train_time:84764ms step_avg:92.84ms
step:914/1670 train_time:84856ms step_avg:92.84ms
step:915/1670 train_time:84948ms step_avg:92.84ms
step:916/1670 train_time:85040ms step_avg:92.84ms
step:917/1670 train_time:85132ms step_avg:92.84ms
step:918/1670 train_time:85227ms step_avg:92.84ms
step:919/1670 train_time:85318ms step_avg:92.84ms
step:920/1670 train_time:85411ms step_avg:92.84ms
step:921/1670 train_time:85504ms step_avg:92.84ms
step:922/1670 train_time:85596ms step_avg:92.84ms
step:923/1670 train_time:85689ms step_avg:92.84ms
step:924/1670 train_time:85781ms step_avg:92.84ms
step:925/1670 train_time:85874ms step_avg:92.84ms
step:926/1670 train_time:85966ms step_avg:92.84ms
step:927/1670 train_time:86059ms step_avg:92.84ms
step:928/1670 train_time:86151ms step_avg:92.84ms
step:929/1670 train_time:86244ms step_avg:92.84ms
step:930/1670 train_time:86336ms step_avg:92.83ms
step:931/1670 train_time:86429ms step_avg:92.83ms
step:932/1670 train_time:86522ms step_avg:92.83ms
step:933/1670 train_time:86614ms step_avg:92.83ms
step:934/1670 train_time:86707ms step_avg:92.83ms
step:935/1670 train_time:86800ms step_avg:92.83ms
step:936/1670 train_time:86892ms step_avg:92.83ms
step:937/1670 train_time:86985ms step_avg:92.83ms
step:938/1670 train_time:87078ms step_avg:92.83ms
step:939/1670 train_time:87170ms step_avg:92.83ms
step:940/1670 train_time:87263ms step_avg:92.83ms
step:941/1670 train_time:87355ms step_avg:92.83ms
step:942/1670 train_time:87448ms step_avg:92.83ms
step:943/1670 train_time:87541ms step_avg:92.83ms
step:944/1670 train_time:87633ms step_avg:92.83ms
step:945/1670 train_time:87728ms step_avg:92.83ms
step:946/1670 train_time:87821ms step_avg:92.83ms
step:947/1670 train_time:87912ms step_avg:92.83ms
step:948/1670 train_time:88006ms step_avg:92.83ms
step:949/1670 train_time:88099ms step_avg:92.83ms
step:950/1670 train_time:88190ms step_avg:92.83ms
step:951/1670 train_time:88284ms step_avg:92.83ms
step:952/1670 train_time:88377ms step_avg:92.83ms
step:953/1670 train_time:88470ms step_avg:92.83ms
step:954/1670 train_time:88563ms step_avg:92.83ms
step:955/1670 train_time:88655ms step_avg:92.83ms
step:956/1670 train_time:88748ms step_avg:92.83ms
step:957/1670 train_time:88841ms step_avg:92.83ms
step:958/1670 train_time:88933ms step_avg:92.83ms
step:959/1670 train_time:89025ms step_avg:92.83ms
step:960/1670 train_time:89117ms step_avg:92.83ms
step:961/1670 train_time:89209ms step_avg:92.83ms
step:962/1670 train_time:89302ms step_avg:92.83ms
step:963/1670 train_time:89394ms step_avg:92.83ms
step:964/1670 train_time:89487ms step_avg:92.83ms
step:965/1670 train_time:89581ms step_avg:92.83ms
step:966/1670 train_time:89673ms step_avg:92.83ms
step:967/1670 train_time:89766ms step_avg:92.83ms
step:968/1670 train_time:89859ms step_avg:92.83ms
step:969/1670 train_time:89950ms step_avg:92.83ms
step:970/1670 train_time:90043ms step_avg:92.83ms
step:971/1670 train_time:90135ms step_avg:92.83ms
step:972/1670 train_time:90227ms step_avg:92.83ms
step:973/1670 train_time:90318ms step_avg:92.82ms
step:974/1670 train_time:90410ms step_avg:92.82ms
step:975/1670 train_time:90504ms step_avg:92.82ms
step:976/1670 train_time:90597ms step_avg:92.83ms
step:977/1670 train_time:90690ms step_avg:92.83ms
step:978/1670 train_time:90784ms step_avg:92.83ms
step:979/1670 train_time:90876ms step_avg:92.83ms
step:980/1670 train_time:90969ms step_avg:92.83ms
step:981/1670 train_time:91061ms step_avg:92.82ms
step:982/1670 train_time:91153ms step_avg:92.82ms
step:983/1670 train_time:91246ms step_avg:92.82ms
step:984/1670 train_time:91339ms step_avg:92.82ms
step:985/1670 train_time:91431ms step_avg:92.82ms
step:986/1670 train_time:91524ms step_avg:92.82ms
step:987/1670 train_time:91617ms step_avg:92.82ms
step:988/1670 train_time:91709ms step_avg:92.82ms
step:989/1670 train_time:91800ms step_avg:92.82ms
step:990/1670 train_time:91893ms step_avg:92.82ms
step:991/1670 train_time:91987ms step_avg:92.82ms
step:992/1670 train_time:92079ms step_avg:92.82ms
step:993/1670 train_time:92171ms step_avg:92.82ms
step:994/1670 train_time:92264ms step_avg:92.82ms
step:995/1670 train_time:92356ms step_avg:92.82ms
step:996/1670 train_time:92448ms step_avg:92.82ms
step:997/1670 train_time:92540ms step_avg:92.82ms
step:998/1670 train_time:92632ms step_avg:92.82ms
step:999/1670 train_time:92725ms step_avg:92.82ms
step:1000/1670 train_time:92817ms step_avg:92.82ms
step:1000/1670 val_loss:3.4664 train_time:92911ms step_avg:92.91ms
step:1001/1670 train_time:92928ms step_avg:92.84ms
step:1002/1670 train_time:93005ms step_avg:92.82ms
step:1003/1670 train_time:93098ms step_avg:92.82ms
step:1004/1670 train_time:93189ms step_avg:92.82ms
step:1005/1670 train_time:93281ms step_avg:92.82ms
step:1006/1670 train_time:93373ms step_avg:92.82ms
step:1007/1670 train_time:93464ms step_avg:92.81ms
step:1008/1670 train_time:93557ms step_avg:92.81ms
step:1009/1670 train_time:93650ms step_avg:92.81ms
step:1010/1670 train_time:93742ms step_avg:92.81ms
step:1011/1670 train_time:93835ms step_avg:92.81ms
step:1012/1670 train_time:93929ms step_avg:92.82ms
step:1013/1670 train_time:94023ms step_avg:92.82ms
step:1014/1670 train_time:94116ms step_avg:92.82ms
step:1015/1670 train_time:94208ms step_avg:92.82ms
step:1016/1670 train_time:94301ms step_avg:92.82ms
step:1017/1670 train_time:94392ms step_avg:92.81ms
step:1018/1670 train_time:94484ms step_avg:92.81ms
step:1019/1670 train_time:94576ms step_avg:92.81ms
step:1020/1670 train_time:94668ms step_avg:92.81ms
step:1021/1670 train_time:94761ms step_avg:92.81ms
step:1022/1670 train_time:94856ms step_avg:92.81ms
step:1023/1670 train_time:94949ms step_avg:92.81ms
step:1024/1670 train_time:95042ms step_avg:92.81ms
step:1025/1670 train_time:95135ms step_avg:92.81ms
step:1026/1670 train_time:95227ms step_avg:92.81ms
step:1027/1670 train_time:95320ms step_avg:92.81ms
step:1028/1670 train_time:95412ms step_avg:92.81ms
step:1029/1670 train_time:95504ms step_avg:92.81ms
step:1030/1670 train_time:95598ms step_avg:92.81ms
step:1031/1670 train_time:95690ms step_avg:92.81ms
step:1032/1670 train_time:95783ms step_avg:92.81ms
step:1033/1670 train_time:95875ms step_avg:92.81ms
step:1034/1670 train_time:95969ms step_avg:92.81ms
step:1035/1670 train_time:96062ms step_avg:92.81ms
step:1036/1670 train_time:96154ms step_avg:92.81ms
step:1037/1670 train_time:96246ms step_avg:92.81ms
step:1038/1670 train_time:96338ms step_avg:92.81ms
step:1039/1670 train_time:96431ms step_avg:92.81ms
step:1040/1670 train_time:96523ms step_avg:92.81ms
step:1041/1670 train_time:96615ms step_avg:92.81ms
step:1042/1670 train_time:96707ms step_avg:92.81ms
step:1043/1670 train_time:96800ms step_avg:92.81ms
step:1044/1670 train_time:96892ms step_avg:92.81ms
step:1045/1670 train_time:96985ms step_avg:92.81ms
step:1046/1670 train_time:97079ms step_avg:92.81ms
step:1047/1670 train_time:97170ms step_avg:92.81ms
step:1048/1670 train_time:97263ms step_avg:92.81ms
step:1049/1670 train_time:97356ms step_avg:92.81ms
step:1050/1670 train_time:97449ms step_avg:92.81ms
step:1051/1670 train_time:97542ms step_avg:92.81ms
step:1052/1670 train_time:97634ms step_avg:92.81ms
step:1053/1670 train_time:97726ms step_avg:92.81ms
step:1054/1670 train_time:97820ms step_avg:92.81ms
step:1055/1670 train_time:97913ms step_avg:92.81ms
step:1056/1670 train_time:98006ms step_avg:92.81ms
step:1057/1670 train_time:98098ms step_avg:92.81ms
step:1058/1670 train_time:98190ms step_avg:92.81ms
step:1059/1670 train_time:98283ms step_avg:92.81ms
step:1060/1670 train_time:98375ms step_avg:92.81ms
step:1061/1670 train_time:98467ms step_avg:92.81ms
step:1062/1670 train_time:98716ms step_avg:92.95ms
step:1063/1670 train_time:98788ms step_avg:92.93ms
step:1064/1670 train_time:98878ms step_avg:92.93ms
step:1065/1670 train_time:98969ms step_avg:92.93ms
step:1066/1670 train_time:99060ms step_avg:92.93ms
step:1067/1670 train_time:99151ms step_avg:92.93ms
step:1068/1670 train_time:99242ms step_avg:92.92ms
step:1069/1670 train_time:99333ms step_avg:92.92ms
step:1070/1670 train_time:99425ms step_avg:92.92ms
step:1071/1670 train_time:99516ms step_avg:92.92ms
step:1072/1670 train_time:99612ms step_avg:92.92ms
step:1073/1670 train_time:99709ms step_avg:92.93ms
step:1074/1670 train_time:99803ms step_avg:92.93ms
step:1075/1670 train_time:99895ms step_avg:92.93ms
step:1076/1670 train_time:99987ms step_avg:92.92ms
step:1077/1670 train_time:100078ms step_avg:92.92ms
step:1078/1670 train_time:100169ms step_avg:92.92ms
step:1079/1670 train_time:100260ms step_avg:92.92ms
step:1080/1670 train_time:100354ms step_avg:92.92ms
step:1081/1670 train_time:100445ms step_avg:92.92ms
step:1082/1670 train_time:100540ms step_avg:92.92ms
step:1083/1670 train_time:100636ms step_avg:92.92ms
step:1084/1670 train_time:100730ms step_avg:92.92ms
step:1085/1670 train_time:100824ms step_avg:92.93ms
step:1086/1670 train_time:100917ms step_avg:92.93ms
step:1087/1670 train_time:101009ms step_avg:92.92ms
step:1088/1670 train_time:101100ms step_avg:92.92ms
step:1089/1670 train_time:101192ms step_avg:92.92ms
step:1090/1670 train_time:101283ms step_avg:92.92ms
step:1091/1670 train_time:101375ms step_avg:92.92ms
step:1092/1670 train_time:101467ms step_avg:92.92ms
step:1093/1670 train_time:101561ms step_avg:92.92ms
step:1094/1670 train_time:101655ms step_avg:92.92ms
step:1095/1670 train_time:101747ms step_avg:92.92ms
step:1096/1670 train_time:101843ms step_avg:92.92ms
step:1097/1670 train_time:101936ms step_avg:92.92ms
step:1098/1670 train_time:102028ms step_avg:92.92ms
step:1099/1670 train_time:102121ms step_avg:92.92ms
step:1100/1670 train_time:102212ms step_avg:92.92ms
step:1101/1670 train_time:102303ms step_avg:92.92ms
step:1102/1670 train_time:102396ms step_avg:92.92ms
step:1103/1670 train_time:102488ms step_avg:92.92ms
step:1104/1670 train_time:102583ms step_avg:92.92ms
step:1105/1670 train_time:102676ms step_avg:92.92ms
step:1106/1670 train_time:102768ms step_avg:92.92ms
step:1107/1670 train_time:102863ms step_avg:92.92ms
step:1108/1670 train_time:102957ms step_avg:92.92ms
step:1109/1670 train_time:103049ms step_avg:92.92ms
step:1110/1670 train_time:103141ms step_avg:92.92ms
step:1111/1670 train_time:103232ms step_avg:92.92ms
step:1112/1670 train_time:103324ms step_avg:92.92ms
step:1113/1670 train_time:103416ms step_avg:92.92ms
step:1114/1670 train_time:103509ms step_avg:92.92ms
step:1115/1670 train_time:103793ms step_avg:93.09ms
step:1116/1670 train_time:103870ms step_avg:93.07ms
step:1117/1670 train_time:103962ms step_avg:93.07ms
step:1118/1670 train_time:104054ms step_avg:93.07ms
step:1119/1670 train_time:104146ms step_avg:93.07ms
step:1120/1670 train_time:104237ms step_avg:93.07ms
step:1121/1670 train_time:104329ms step_avg:93.07ms
step:1122/1670 train_time:104421ms step_avg:93.07ms
step:1123/1670 train_time:104513ms step_avg:93.07ms
step:1124/1670 train_time:104605ms step_avg:93.07ms
step:1125/1670 train_time:104705ms step_avg:93.07ms
step:1125/1670 val_loss:3.4137 train_time:104806ms step_avg:93.16ms
step:1126/1670 train_time:104823ms step_avg:93.09ms
step:1127/1670 train_time:104906ms step_avg:93.08ms
step:1128/1670 train_time:105006ms step_avg:93.09ms
step:1129/1670 train_time:105099ms step_avg:93.09ms
step:1130/1670 train_time:105192ms step_avg:93.09ms
step:1131/1670 train_time:105285ms step_avg:93.09ms
step:1132/1670 train_time:105377ms step_avg:93.09ms
step:1133/1670 train_time:105469ms step_avg:93.09ms
step:1134/1670 train_time:105561ms step_avg:93.09ms
step:1135/1670 train_time:105654ms step_avg:93.09ms
step:1136/1670 train_time:105747ms step_avg:93.09ms
step:1137/1670 train_time:105842ms step_avg:93.09ms
step:1138/1670 train_time:105937ms step_avg:93.09ms
step:1139/1670 train_time:106033ms step_avg:93.09ms
step:1140/1670 train_time:106127ms step_avg:93.09ms
step:1141/1670 train_time:106220ms step_avg:93.09ms
step:1142/1670 train_time:106313ms step_avg:93.09ms
step:1143/1670 train_time:106406ms step_avg:93.09ms
step:1144/1670 train_time:106497ms step_avg:93.09ms
step:1145/1670 train_time:106589ms step_avg:93.09ms
step:1146/1670 train_time:106683ms step_avg:93.09ms
step:1147/1670 train_time:106777ms step_avg:93.09ms
step:1148/1670 train_time:106871ms step_avg:93.09ms
step:1149/1670 train_time:106966ms step_avg:93.09ms
step:1150/1670 train_time:107061ms step_avg:93.10ms
step:1151/1670 train_time:107154ms step_avg:93.10ms
step:1152/1670 train_time:107248ms step_avg:93.10ms
step:1153/1670 train_time:107340ms step_avg:93.10ms
step:1154/1670 train_time:107432ms step_avg:93.10ms
step:1155/1670 train_time:107525ms step_avg:93.10ms
step:1156/1670 train_time:107618ms step_avg:93.10ms
step:1157/1670 train_time:107711ms step_avg:93.09ms
step:1158/1670 train_time:107804ms step_avg:93.10ms
step:1159/1670 train_time:107898ms step_avg:93.10ms
step:1160/1670 train_time:107993ms step_avg:93.10ms
step:1161/1670 train_time:108087ms step_avg:93.10ms
step:1162/1670 train_time:108181ms step_avg:93.10ms
step:1163/1670 train_time:108274ms step_avg:93.10ms
step:1164/1670 train_time:108367ms step_avg:93.10ms
step:1165/1670 train_time:108460ms step_avg:93.10ms
step:1166/1670 train_time:108553ms step_avg:93.10ms
step:1167/1670 train_time:108645ms step_avg:93.10ms
step:1168/1670 train_time:108737ms step_avg:93.10ms
step:1169/1670 train_time:108832ms step_avg:93.10ms
step:1170/1670 train_time:108926ms step_avg:93.10ms
step:1171/1670 train_time:109019ms step_avg:93.10ms
step:1172/1670 train_time:109113ms step_avg:93.10ms
step:1173/1670 train_time:109207ms step_avg:93.10ms
step:1174/1670 train_time:109300ms step_avg:93.10ms
step:1175/1670 train_time:109393ms step_avg:93.10ms
step:1176/1670 train_time:109488ms step_avg:93.10ms
step:1177/1670 train_time:109581ms step_avg:93.10ms
step:1178/1670 train_time:109673ms step_avg:93.10ms
step:1179/1670 train_time:109766ms step_avg:93.10ms
step:1180/1670 train_time:109859ms step_avg:93.10ms
step:1181/1670 train_time:109953ms step_avg:93.10ms
step:1182/1670 train_time:110046ms step_avg:93.10ms
step:1183/1670 train_time:110139ms step_avg:93.10ms
step:1184/1670 train_time:110232ms step_avg:93.10ms
step:1185/1670 train_time:110326ms step_avg:93.10ms
step:1186/1670 train_time:110419ms step_avg:93.10ms
step:1187/1670 train_time:110512ms step_avg:93.10ms
step:1188/1670 train_time:110604ms step_avg:93.10ms
step:1189/1670 train_time:110698ms step_avg:93.10ms
step:1190/1670 train_time:110791ms step_avg:93.10ms
step:1191/1670 train_time:110885ms step_avg:93.10ms
step:1192/1670 train_time:110978ms step_avg:93.10ms
step:1193/1670 train_time:111071ms step_avg:93.10ms
step:1194/1670 train_time:111164ms step_avg:93.10ms
step:1195/1670 train_time:111257ms step_avg:93.10ms
step:1196/1670 train_time:111350ms step_avg:93.10ms
step:1197/1670 train_time:111443ms step_avg:93.10ms
step:1198/1670 train_time:111536ms step_avg:93.10ms
step:1199/1670 train_time:111630ms step_avg:93.10ms
step:1200/1670 train_time:111723ms step_avg:93.10ms
step:1201/1670 train_time:111815ms step_avg:93.10ms
step:1202/1670 train_time:111911ms step_avg:93.10ms
step:1203/1670 train_time:112004ms step_avg:93.10ms
step:1204/1670 train_time:112097ms step_avg:93.10ms
step:1205/1670 train_time:112190ms step_avg:93.10ms
step:1206/1670 train_time:112284ms step_avg:93.10ms
step:1207/1670 train_time:112377ms step_avg:93.10ms
step:1208/1670 train_time:112471ms step_avg:93.11ms
step:1209/1670 train_time:112564ms step_avg:93.11ms
step:1210/1670 train_time:112657ms step_avg:93.10ms
step:1211/1670 train_time:112751ms step_avg:93.11ms
step:1212/1670 train_time:112844ms step_avg:93.11ms
step:1213/1670 train_time:112937ms step_avg:93.11ms
step:1214/1670 train_time:113030ms step_avg:93.11ms
step:1215/1670 train_time:113124ms step_avg:93.11ms
step:1216/1670 train_time:113216ms step_avg:93.11ms
step:1217/1670 train_time:113310ms step_avg:93.11ms
step:1218/1670 train_time:113403ms step_avg:93.11ms
step:1219/1670 train_time:113497ms step_avg:93.11ms
step:1220/1670 train_time:113590ms step_avg:93.11ms
step:1221/1670 train_time:113683ms step_avg:93.11ms
step:1222/1670 train_time:113776ms step_avg:93.11ms
step:1223/1670 train_time:113869ms step_avg:93.11ms
step:1224/1670 train_time:113962ms step_avg:93.11ms
step:1225/1670 train_time:114055ms step_avg:93.11ms
step:1226/1670 train_time:114149ms step_avg:93.11ms
step:1227/1670 train_time:114242ms step_avg:93.11ms
step:1228/1670 train_time:114334ms step_avg:93.11ms
step:1229/1670 train_time:114428ms step_avg:93.11ms
step:1230/1670 train_time:114521ms step_avg:93.11ms
step:1231/1670 train_time:114614ms step_avg:93.11ms
step:1232/1670 train_time:114706ms step_avg:93.11ms
step:1233/1670 train_time:114799ms step_avg:93.11ms
step:1234/1670 train_time:114892ms step_avg:93.11ms
step:1235/1670 train_time:114987ms step_avg:93.11ms
step:1236/1670 train_time:115080ms step_avg:93.11ms
step:1237/1670 train_time:115173ms step_avg:93.11ms
step:1238/1670 train_time:115267ms step_avg:93.11ms
step:1239/1670 train_time:115359ms step_avg:93.11ms
step:1240/1670 train_time:115452ms step_avg:93.11ms
step:1241/1670 train_time:115546ms step_avg:93.11ms
step:1242/1670 train_time:115639ms step_avg:93.11ms
step:1243/1670 train_time:115732ms step_avg:93.11ms
step:1244/1670 train_time:115827ms step_avg:93.11ms
step:1245/1670 train_time:115920ms step_avg:93.11ms
step:1246/1670 train_time:116013ms step_avg:93.11ms
step:1247/1670 train_time:116106ms step_avg:93.11ms
step:1248/1670 train_time:116199ms step_avg:93.11ms
step:1249/1670 train_time:116292ms step_avg:93.11ms
step:1250/1670 train_time:116386ms step_avg:93.11ms
step:1250/1670 val_loss:3.3755 train_time:116479ms step_avg:93.18ms
step:1251/1670 train_time:116496ms step_avg:93.12ms
step:1252/1670 train_time:116574ms step_avg:93.11ms
step:1253/1670 train_time:116668ms step_avg:93.11ms
step:1254/1670 train_time:116761ms step_avg:93.11ms
step:1255/1670 train_time:116854ms step_avg:93.11ms
step:1256/1670 train_time:116946ms step_avg:93.11ms
step:1257/1670 train_time:117039ms step_avg:93.11ms
step:1258/1670 train_time:117132ms step_avg:93.11ms
step:1259/1670 train_time:117224ms step_avg:93.11ms
step:1260/1670 train_time:117318ms step_avg:93.11ms
step:1261/1670 train_time:117412ms step_avg:93.11ms
step:1262/1670 train_time:117506ms step_avg:93.11ms
step:1263/1670 train_time:117601ms step_avg:93.11ms
step:1264/1670 train_time:117694ms step_avg:93.11ms
step:1265/1670 train_time:117786ms step_avg:93.11ms
step:1266/1670 train_time:117879ms step_avg:93.11ms
step:1267/1670 train_time:117973ms step_avg:93.11ms
step:1268/1670 train_time:118066ms step_avg:93.11ms
step:1269/1670 train_time:118159ms step_avg:93.11ms
step:1270/1670 train_time:118251ms step_avg:93.11ms
step:1271/1670 train_time:118344ms step_avg:93.11ms
step:1272/1670 train_time:118438ms step_avg:93.11ms
step:1273/1670 train_time:118532ms step_avg:93.11ms
step:1274/1670 train_time:118769ms step_avg:93.23ms
step:1275/1670 train_time:118853ms step_avg:93.22ms
step:1276/1670 train_time:118944ms step_avg:93.22ms
step:1277/1670 train_time:119036ms step_avg:93.22ms
step:1278/1670 train_time:119129ms step_avg:93.21ms
step:1279/1670 train_time:119220ms step_avg:93.21ms
step:1280/1670 train_time:119312ms step_avg:93.21ms
step:1281/1670 train_time:119404ms step_avg:93.21ms
step:1282/1670 train_time:119497ms step_avg:93.21ms
step:1283/1670 train_time:119588ms step_avg:93.21ms
step:1284/1670 train_time:119686ms step_avg:93.21ms
step:1285/1670 train_time:119785ms step_avg:93.22ms
step:1286/1670 train_time:119879ms step_avg:93.22ms
step:1287/1670 train_time:119972ms step_avg:93.22ms
step:1288/1670 train_time:120064ms step_avg:93.22ms
step:1289/1670 train_time:120156ms step_avg:93.22ms
step:1290/1670 train_time:120249ms step_avg:93.22ms
step:1291/1670 train_time:120341ms step_avg:93.22ms
step:1292/1670 train_time:120433ms step_avg:93.21ms
step:1293/1670 train_time:120525ms step_avg:93.21ms
step:1294/1670 train_time:120619ms step_avg:93.21ms
step:1295/1670 train_time:120715ms step_avg:93.22ms
step:1296/1670 train_time:120810ms step_avg:93.22ms
step:1297/1670 train_time:120903ms step_avg:93.22ms
step:1298/1670 train_time:120997ms step_avg:93.22ms
step:1299/1670 train_time:121090ms step_avg:93.22ms
step:1300/1670 train_time:121182ms step_avg:93.22ms
step:1301/1670 train_time:121278ms step_avg:93.22ms
step:1302/1670 train_time:121371ms step_avg:93.22ms
step:1303/1670 train_time:121462ms step_avg:93.22ms
step:1304/1670 train_time:121555ms step_avg:93.22ms
step:1305/1670 train_time:121648ms step_avg:93.22ms
step:1306/1670 train_time:121743ms step_avg:93.22ms
step:1307/1670 train_time:121838ms step_avg:93.22ms
step:1308/1670 train_time:121931ms step_avg:93.22ms
step:1309/1670 train_time:122024ms step_avg:93.22ms
step:1310/1670 train_time:122117ms step_avg:93.22ms
step:1311/1670 train_time:122211ms step_avg:93.22ms
step:1312/1670 train_time:122304ms step_avg:93.22ms
step:1313/1670 train_time:122397ms step_avg:93.22ms
step:1314/1670 train_time:122490ms step_avg:93.22ms
step:1315/1670 train_time:122583ms step_avg:93.22ms
step:1316/1670 train_time:122677ms step_avg:93.22ms
step:1317/1670 train_time:122771ms step_avg:93.22ms
step:1318/1670 train_time:122865ms step_avg:93.22ms
step:1319/1670 train_time:122958ms step_avg:93.22ms
step:1320/1670 train_time:123050ms step_avg:93.22ms
step:1321/1670 train_time:123144ms step_avg:93.22ms
step:1322/1670 train_time:123238ms step_avg:93.22ms
step:1323/1670 train_time:123331ms step_avg:93.22ms
step:1324/1670 train_time:123423ms step_avg:93.22ms
step:1325/1670 train_time:123516ms step_avg:93.22ms
step:1326/1670 train_time:123610ms step_avg:93.22ms
step:1327/1670 train_time:123704ms step_avg:93.22ms
step:1328/1670 train_time:123798ms step_avg:93.22ms
step:1329/1670 train_time:123891ms step_avg:93.22ms
step:1330/1670 train_time:123984ms step_avg:93.22ms
step:1331/1670 train_time:124078ms step_avg:93.22ms
step:1332/1670 train_time:124171ms step_avg:93.22ms
step:1333/1670 train_time:124263ms step_avg:93.22ms
step:1334/1670 train_time:124356ms step_avg:93.22ms
step:1335/1670 train_time:124448ms step_avg:93.22ms
step:1336/1670 train_time:124542ms step_avg:93.22ms
step:1337/1670 train_time:124636ms step_avg:93.22ms
step:1338/1670 train_time:124730ms step_avg:93.22ms
step:1339/1670 train_time:124824ms step_avg:93.22ms
step:1340/1670 train_time:124918ms step_avg:93.22ms
step:1341/1670 train_time:125010ms step_avg:93.22ms
step:1342/1670 train_time:125105ms step_avg:93.22ms
step:1343/1670 train_time:125198ms step_avg:93.22ms
step:1344/1670 train_time:125291ms step_avg:93.22ms
step:1345/1670 train_time:125383ms step_avg:93.22ms
step:1346/1670 train_time:125477ms step_avg:93.22ms
step:1347/1670 train_time:125570ms step_avg:93.22ms
step:1348/1670 train_time:125663ms step_avg:93.22ms
step:1349/1670 train_time:125756ms step_avg:93.22ms
step:1350/1670 train_time:125849ms step_avg:93.22ms
step:1351/1670 train_time:125942ms step_avg:93.22ms
step:1352/1670 train_time:126037ms step_avg:93.22ms
step:1353/1670 train_time:126131ms step_avg:93.22ms
step:1354/1670 train_time:126224ms step_avg:93.22ms
step:1355/1670 train_time:126317ms step_avg:93.22ms
step:1356/1670 train_time:126410ms step_avg:93.22ms
step:1357/1670 train_time:126503ms step_avg:93.22ms
step:1358/1670 train_time:126597ms step_avg:93.22ms
step:1359/1670 train_time:126690ms step_avg:93.22ms
step:1360/1670 train_time:126783ms step_avg:93.22ms
step:1361/1670 train_time:126878ms step_avg:93.22ms
step:1362/1670 train_time:126971ms step_avg:93.22ms
step:1363/1670 train_time:127064ms step_avg:93.22ms
step:1364/1670 train_time:127157ms step_avg:93.22ms
step:1365/1670 train_time:127250ms step_avg:93.22ms
step:1366/1670 train_time:127343ms step_avg:93.22ms
step:1367/1670 train_time:127437ms step_avg:93.22ms
step:1368/1670 train_time:127530ms step_avg:93.22ms
step:1369/1670 train_time:127623ms step_avg:93.22ms
step:1370/1670 train_time:127715ms step_avg:93.22ms
step:1371/1670 train_time:127808ms step_avg:93.22ms
step:1372/1670 train_time:127903ms step_avg:93.22ms
step:1373/1670 train_time:127997ms step_avg:93.22ms
step:1374/1670 train_time:128090ms step_avg:93.22ms
step:1375/1670 train_time:128184ms step_avg:93.22ms
step:1375/1670 val_loss:3.3405 train_time:128278ms step_avg:93.29ms
step:1376/1670 train_time:128295ms step_avg:93.24ms
step:1377/1670 train_time:128373ms step_avg:93.23ms
step:1378/1670 train_time:128466ms step_avg:93.23ms
step:1379/1670 train_time:128559ms step_avg:93.23ms
step:1380/1670 train_time:128651ms step_avg:93.23ms
step:1381/1670 train_time:128744ms step_avg:93.23ms
step:1382/1670 train_time:128836ms step_avg:93.22ms
step:1383/1670 train_time:128930ms step_avg:93.23ms
step:1384/1670 train_time:129023ms step_avg:93.22ms
step:1385/1670 train_time:129116ms step_avg:93.22ms
step:1386/1670 train_time:129211ms step_avg:93.23ms
step:1387/1670 train_time:129305ms step_avg:93.23ms
step:1388/1670 train_time:129400ms step_avg:93.23ms
step:1389/1670 train_time:129494ms step_avg:93.23ms
step:1390/1670 train_time:129587ms step_avg:93.23ms
step:1391/1670 train_time:129680ms step_avg:93.23ms
step:1392/1670 train_time:129772ms step_avg:93.23ms
step:1393/1670 train_time:129865ms step_avg:93.23ms
step:1394/1670 train_time:129958ms step_avg:93.23ms
step:1395/1670 train_time:130051ms step_avg:93.23ms
step:1396/1670 train_time:130145ms step_avg:93.23ms
step:1397/1670 train_time:130238ms step_avg:93.23ms
step:1398/1670 train_time:130334ms step_avg:93.23ms
step:1399/1670 train_time:130429ms step_avg:93.23ms
step:1400/1670 train_time:130522ms step_avg:93.23ms
step:1401/1670 train_time:130615ms step_avg:93.23ms
step:1402/1670 train_time:130709ms step_avg:93.23ms
step:1403/1670 train_time:130801ms step_avg:93.23ms
step:1404/1670 train_time:130894ms step_avg:93.23ms
step:1405/1670 train_time:130987ms step_avg:93.23ms
step:1406/1670 train_time:131079ms step_avg:93.23ms
step:1407/1670 train_time:131172ms step_avg:93.23ms
step:1408/1670 train_time:131266ms step_avg:93.23ms
step:1409/1670 train_time:131360ms step_avg:93.23ms
step:1410/1670 train_time:131453ms step_avg:93.23ms
step:1411/1670 train_time:131547ms step_avg:93.23ms
step:1412/1670 train_time:131641ms step_avg:93.23ms
step:1413/1670 train_time:131734ms step_avg:93.23ms
step:1414/1670 train_time:131828ms step_avg:93.23ms
step:1415/1670 train_time:131922ms step_avg:93.23ms
step:1416/1670 train_time:132014ms step_avg:93.23ms
step:1417/1670 train_time:132108ms step_avg:93.23ms
step:1418/1670 train_time:132202ms step_avg:93.23ms
step:1419/1670 train_time:132295ms step_avg:93.23ms
step:1420/1670 train_time:132388ms step_avg:93.23ms
step:1421/1670 train_time:132480ms step_avg:93.23ms
step:1422/1670 train_time:132574ms step_avg:93.23ms
step:1423/1670 train_time:132668ms step_avg:93.23ms
step:1424/1670 train_time:132762ms step_avg:93.23ms
step:1425/1670 train_time:132855ms step_avg:93.23ms
step:1426/1670 train_time:132948ms step_avg:93.23ms
step:1427/1670 train_time:133041ms step_avg:93.23ms
step:1428/1670 train_time:133134ms step_avg:93.23ms
step:1429/1670 train_time:133229ms step_avg:93.23ms
step:1430/1670 train_time:133323ms step_avg:93.23ms
step:1431/1670 train_time:133416ms step_avg:93.23ms
step:1432/1670 train_time:133509ms step_avg:93.23ms
step:1433/1670 train_time:133602ms step_avg:93.23ms
step:1434/1670 train_time:133695ms step_avg:93.23ms
step:1435/1670 train_time:133789ms step_avg:93.23ms
step:1436/1670 train_time:133882ms step_avg:93.23ms
step:1437/1670 train_time:133974ms step_avg:93.23ms
step:1438/1670 train_time:134068ms step_avg:93.23ms
step:1439/1670 train_time:134161ms step_avg:93.23ms
step:1440/1670 train_time:134256ms step_avg:93.23ms
step:1441/1670 train_time:134350ms step_avg:93.23ms
step:1442/1670 train_time:134444ms step_avg:93.23ms
step:1443/1670 train_time:134537ms step_avg:93.23ms
step:1444/1670 train_time:134630ms step_avg:93.23ms
step:1445/1670 train_time:134724ms step_avg:93.23ms
step:1446/1670 train_time:134817ms step_avg:93.23ms
step:1447/1670 train_time:134911ms step_avg:93.23ms
step:1448/1670 train_time:135004ms step_avg:93.23ms
step:1449/1670 train_time:135097ms step_avg:93.23ms
step:1450/1670 train_time:135191ms step_avg:93.24ms
step:1451/1670 train_time:135286ms step_avg:93.24ms
step:1452/1670 train_time:135379ms step_avg:93.24ms
step:1453/1670 train_time:135472ms step_avg:93.24ms
step:1454/1670 train_time:135565ms step_avg:93.24ms
step:1455/1670 train_time:135659ms step_avg:93.24ms
step:1456/1670 train_time:135752ms step_avg:93.24ms
step:1457/1670 train_time:135844ms step_avg:93.24ms
step:1458/1670 train_time:135937ms step_avg:93.24ms
step:1459/1670 train_time:136032ms step_avg:93.24ms
step:1460/1670 train_time:136126ms step_avg:93.24ms
step:1461/1670 train_time:136220ms step_avg:93.24ms
step:1462/1670 train_time:136313ms step_avg:93.24ms
step:1463/1670 train_time:136406ms step_avg:93.24ms
step:1464/1670 train_time:136499ms step_avg:93.24ms
step:1465/1670 train_time:136593ms step_avg:93.24ms
step:1466/1670 train_time:136688ms step_avg:93.24ms
step:1467/1670 train_time:136780ms step_avg:93.24ms
step:1468/1670 train_time:136873ms step_avg:93.24ms
step:1469/1670 train_time:136966ms step_avg:93.24ms
step:1470/1670 train_time:137060ms step_avg:93.24ms
step:1471/1670 train_time:137153ms step_avg:93.24ms
step:1472/1670 train_time:137248ms step_avg:93.24ms
step:1473/1670 train_time:137341ms step_avg:93.24ms
step:1474/1670 train_time:137434ms step_avg:93.24ms
step:1475/1670 train_time:137528ms step_avg:93.24ms
step:1476/1670 train_time:137621ms step_avg:93.24ms
step:1477/1670 train_time:137715ms step_avg:93.24ms
step:1478/1670 train_time:137808ms step_avg:93.24ms
step:1479/1670 train_time:137900ms step_avg:93.24ms
step:1480/1670 train_time:137993ms step_avg:93.24ms
step:1481/1670 train_time:138086ms step_avg:93.24ms
step:1482/1670 train_time:138180ms step_avg:93.24ms
step:1483/1670 train_time:138273ms step_avg:93.24ms
step:1484/1670 train_time:138366ms step_avg:93.24ms
step:1485/1670 train_time:138616ms step_avg:93.34ms
step:1486/1670 train_time:138689ms step_avg:93.33ms
step:1487/1670 train_time:138782ms step_avg:93.33ms
step:1488/1670 train_time:138873ms step_avg:93.33ms
step:1489/1670 train_time:138965ms step_avg:93.33ms
step:1490/1670 train_time:139057ms step_avg:93.33ms
step:1491/1670 train_time:139149ms step_avg:93.33ms
step:1492/1670 train_time:139241ms step_avg:93.32ms
step:1493/1670 train_time:139332ms step_avg:93.32ms
step:1494/1670 train_time:139425ms step_avg:93.32ms
step:1495/1670 train_time:139523ms step_avg:93.33ms
step:1496/1670 train_time:139620ms step_avg:93.33ms
step:1497/1670 train_time:139714ms step_avg:93.33ms
step:1498/1670 train_time:139807ms step_avg:93.33ms
step:1499/1670 train_time:139900ms step_avg:93.33ms
step:1500/1670 train_time:139992ms step_avg:93.33ms
step:1500/1670 val_loss:3.3106 train_time:140086ms step_avg:93.39ms
step:1501/1670 train_time:140103ms step_avg:93.34ms
step:1502/1670 train_time:140180ms step_avg:93.33ms
step:1503/1670 train_time:140272ms step_avg:93.33ms
step:1504/1670 train_time:140364ms step_avg:93.33ms
step:1505/1670 train_time:140457ms step_avg:93.33ms
step:1506/1670 train_time:140549ms step_avg:93.33ms
step:1507/1670 train_time:140642ms step_avg:93.33ms
step:1508/1670 train_time:140738ms step_avg:93.33ms
step:1509/1670 train_time:140831ms step_avg:93.33ms
step:1510/1670 train_time:140924ms step_avg:93.33ms
step:1511/1670 train_time:141018ms step_avg:93.33ms
step:1512/1670 train_time:141112ms step_avg:93.33ms
step:1513/1670 train_time:141206ms step_avg:93.33ms
step:1514/1670 train_time:141299ms step_avg:93.33ms
step:1515/1670 train_time:141391ms step_avg:93.33ms
step:1516/1670 train_time:141483ms step_avg:93.33ms
step:1517/1670 train_time:141577ms step_avg:93.33ms
step:1518/1670 train_time:141670ms step_avg:93.33ms
step:1519/1670 train_time:141764ms step_avg:93.33ms
step:1520/1670 train_time:141858ms step_avg:93.33ms
step:1521/1670 train_time:141951ms step_avg:93.33ms
step:1522/1670 train_time:142045ms step_avg:93.33ms
step:1523/1670 train_time:142140ms step_avg:93.33ms
step:1524/1670 train_time:142233ms step_avg:93.33ms
step:1525/1670 train_time:142326ms step_avg:93.33ms
step:1526/1670 train_time:142420ms step_avg:93.33ms
step:1527/1670 train_time:142512ms step_avg:93.33ms
step:1528/1670 train_time:142605ms step_avg:93.33ms
step:1529/1670 train_time:142699ms step_avg:93.33ms
step:1530/1670 train_time:142792ms step_avg:93.33ms
step:1531/1670 train_time:142885ms step_avg:93.33ms
step:1532/1670 train_time:142980ms step_avg:93.33ms
step:1533/1670 train_time:143074ms step_avg:93.33ms
step:1534/1670 train_time:143167ms step_avg:93.33ms
step:1535/1670 train_time:143260ms step_avg:93.33ms
step:1536/1670 train_time:143353ms step_avg:93.33ms
step:1537/1670 train_time:143447ms step_avg:93.33ms
step:1538/1670 train_time:143541ms step_avg:93.33ms
step:1539/1670 train_time:143634ms step_avg:93.33ms
step:1540/1670 train_time:143727ms step_avg:93.33ms
step:1541/1670 train_time:143821ms step_avg:93.33ms
step:1542/1670 train_time:143916ms step_avg:93.33ms
step:1543/1670 train_time:144010ms step_avg:93.33ms
step:1544/1670 train_time:144103ms step_avg:93.33ms
step:1545/1670 train_time:144196ms step_avg:93.33ms
step:1546/1670 train_time:144289ms step_avg:93.33ms
step:1547/1670 train_time:144382ms step_avg:93.33ms
step:1548/1670 train_time:144476ms step_avg:93.33ms
step:1549/1670 train_time:144569ms step_avg:93.33ms
step:1550/1670 train_time:144662ms step_avg:93.33ms
step:1551/1670 train_time:144755ms step_avg:93.33ms
step:1552/1670 train_time:144849ms step_avg:93.33ms
step:1553/1670 train_time:144942ms step_avg:93.33ms
step:1554/1670 train_time:145035ms step_avg:93.33ms
step:1555/1670 train_time:145128ms step_avg:93.33ms
step:1556/1670 train_time:145221ms step_avg:93.33ms
step:1557/1670 train_time:145313ms step_avg:93.33ms
step:1558/1670 train_time:145406ms step_avg:93.33ms
step:1559/1670 train_time:145499ms step_avg:93.33ms
step:1560/1670 train_time:145592ms step_avg:93.33ms
step:1561/1670 train_time:145685ms step_avg:93.33ms
step:1562/1670 train_time:145780ms step_avg:93.33ms
step:1563/1670 train_time:145875ms step_avg:93.33ms
step:1564/1670 train_time:145966ms step_avg:93.33ms
step:1565/1670 train_time:146059ms step_avg:93.33ms
step:1566/1670 train_time:146152ms step_avg:93.33ms
step:1567/1670 train_time:146245ms step_avg:93.33ms
step:1568/1670 train_time:146338ms step_avg:93.33ms
step:1569/1670 train_time:146431ms step_avg:93.33ms
step:1570/1670 train_time:146524ms step_avg:93.33ms
step:1571/1670 train_time:146618ms step_avg:93.33ms
step:1572/1670 train_time:146711ms step_avg:93.33ms
step:1573/1670 train_time:146804ms step_avg:93.33ms
step:1574/1670 train_time:146898ms step_avg:93.33ms
step:1575/1670 train_time:146992ms step_avg:93.33ms
step:1576/1670 train_time:147084ms step_avg:93.33ms
step:1577/1670 train_time:147180ms step_avg:93.33ms
step:1578/1670 train_time:147273ms step_avg:93.33ms
step:1579/1670 train_time:147365ms step_avg:93.33ms
step:1580/1670 train_time:147459ms step_avg:93.33ms
step:1581/1670 train_time:147553ms step_avg:93.33ms
step:1582/1670 train_time:147646ms step_avg:93.33ms
step:1583/1670 train_time:147740ms step_avg:93.33ms
step:1584/1670 train_time:147833ms step_avg:93.33ms
step:1585/1670 train_time:147927ms step_avg:93.33ms
step:1586/1670 train_time:148020ms step_avg:93.33ms
step:1587/1670 train_time:148113ms step_avg:93.33ms
step:1588/1670 train_time:148206ms step_avg:93.33ms
step:1589/1670 train_time:148300ms step_avg:93.33ms
step:1590/1670 train_time:148393ms step_avg:93.33ms
step:1591/1670 train_time:148486ms step_avg:93.33ms
step:1592/1670 train_time:148579ms step_avg:93.33ms
step:1593/1670 train_time:148673ms step_avg:93.33ms
step:1594/1670 train_time:148766ms step_avg:93.33ms
step:1595/1670 train_time:148860ms step_avg:93.33ms
step:1596/1670 train_time:148952ms step_avg:93.33ms
step:1597/1670 train_time:149046ms step_avg:93.33ms
step:1598/1670 train_time:149140ms step_avg:93.33ms
step:1599/1670 train_time:149233ms step_avg:93.33ms
step:1600/1670 train_time:149326ms step_avg:93.33ms
step:1601/1670 train_time:149419ms step_avg:93.33ms
step:1602/1670 train_time:149511ms step_avg:93.33ms
step:1603/1670 train_time:149604ms step_avg:93.33ms
step:1604/1670 train_time:149699ms step_avg:93.33ms
step:1605/1670 train_time:149792ms step_avg:93.33ms
step:1606/1670 train_time:149884ms step_avg:93.33ms
step:1607/1670 train_time:149979ms step_avg:93.33ms
step:1608/1670 train_time:150073ms step_avg:93.33ms
step:1609/1670 train_time:150166ms step_avg:93.33ms
step:1610/1670 train_time:150261ms step_avg:93.33ms
step:1611/1670 train_time:150354ms step_avg:93.33ms
step:1612/1670 train_time:150447ms step_avg:93.33ms
step:1613/1670 train_time:150540ms step_avg:93.33ms
step:1614/1670 train_time:150635ms step_avg:93.33ms
step:1615/1670 train_time:150729ms step_avg:93.33ms
step:1616/1670 train_time:150821ms step_avg:93.33ms
step:1617/1670 train_time:150914ms step_avg:93.33ms
step:1618/1670 train_time:151008ms step_avg:93.33ms
step:1619/1670 train_time:151101ms step_avg:93.33ms
step:1620/1670 train_time:151194ms step_avg:93.33ms
step:1621/1670 train_time:151287ms step_avg:93.33ms
step:1622/1670 train_time:151381ms step_avg:93.33ms
step:1623/1670 train_time:151474ms step_avg:93.33ms
step:1624/1670 train_time:151567ms step_avg:93.33ms
step:1625/1670 train_time:151662ms step_avg:93.33ms
step:1625/1670 val_loss:3.2855 train_time:151756ms step_avg:93.39ms
step:1626/1670 train_time:151773ms step_avg:93.34ms
step:1627/1670 train_time:151850ms step_avg:93.33ms
step:1628/1670 train_time:151944ms step_avg:93.33ms
step:1629/1670 train_time:152038ms step_avg:93.33ms
step:1630/1670 train_time:152130ms step_avg:93.33ms
step:1631/1670 train_time:152224ms step_avg:93.33ms
step:1632/1670 train_time:152317ms step_avg:93.33ms
step:1633/1670 train_time:152410ms step_avg:93.33ms
step:1634/1670 train_time:152503ms step_avg:93.33ms
step:1635/1670 train_time:152597ms step_avg:93.33ms
step:1636/1670 train_time:152690ms step_avg:93.33ms
step:1637/1670 train_time:152785ms step_avg:93.33ms
step:1638/1670 train_time:152879ms step_avg:93.33ms
step:1639/1670 train_time:152972ms step_avg:93.33ms
step:1640/1670 train_time:153066ms step_avg:93.33ms
step:1641/1670 train_time:153160ms step_avg:93.33ms
step:1642/1670 train_time:153252ms step_avg:93.33ms
step:1643/1670 train_time:153346ms step_avg:93.33ms
step:1644/1670 train_time:153439ms step_avg:93.33ms
step:1645/1670 train_time:153532ms step_avg:93.33ms
step:1646/1670 train_time:153627ms step_avg:93.33ms
step:1647/1670 train_time:153722ms step_avg:93.33ms
step:1648/1670 train_time:153816ms step_avg:93.34ms
step:1649/1670 train_time:153910ms step_avg:93.34ms
step:1650/1670 train_time:154004ms step_avg:93.34ms
step:1651/1670 train_time:154096ms step_avg:93.34ms
step:1652/1670 train_time:154190ms step_avg:93.34ms
step:1653/1670 train_time:154283ms step_avg:93.34ms
step:1654/1670 train_time:154375ms step_avg:93.33ms
step:1655/1670 train_time:154469ms step_avg:93.33ms
step:1656/1670 train_time:154562ms step_avg:93.33ms
step:1657/1670 train_time:154655ms step_avg:93.33ms
step:1658/1670 train_time:154749ms step_avg:93.33ms
step:1659/1670 train_time:154842ms step_avg:93.33ms
step:1660/1670 train_time:154936ms step_avg:93.33ms
step:1661/1670 train_time:155029ms step_avg:93.33ms
step:1662/1670 train_time:155123ms step_avg:93.33ms
step:1663/1670 train_time:155216ms step_avg:93.33ms
step:1664/1670 train_time:155309ms step_avg:93.33ms
step:1665/1670 train_time:155402ms step_avg:93.33ms
step:1666/1670 train_time:155496ms step_avg:93.33ms
step:1667/1670 train_time:155589ms step_avg:93.33ms
step:1668/1670 train_time:155683ms step_avg:93.33ms
step:1669/1670 train_time:155775ms step_avg:93.33ms
step:1670/1670 train_time:155868ms step_avg:93.33ms
step:1670/1670 val_loss:3.2769 train_time:156129ms step_avg:93.49ms
peak memory allocated: 31587 MiB reserved: 47114 MiB

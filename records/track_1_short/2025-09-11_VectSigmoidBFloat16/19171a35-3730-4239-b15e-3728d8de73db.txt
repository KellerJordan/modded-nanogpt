import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(
    x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(
    g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99


def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]


@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(
        pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M
    )

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr,
    C_ptr,
    M,
    K,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr,
    C_ptr,
    M,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    alpha,
    beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (
        offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c
    )
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out


@torch.compile(
    dynamic=False, fullgraph=True
)  # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer


class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(
        self,
        params,
        lr: float = 1e-3,
        betas: tuple[float, float] = (0.9, 0.999),
        eps: float = 1e-8,
        weight_decay: float = 0.01,
    ):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(
                        grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True
                    ).get_future()
                )
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group["betas"]
            eps = group["eps"]
            wd = group["weight_decay"]
            params = group["params"]
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size : (rank + 1) * rank_size]
                lr = group["lr"] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(
                    g_slice, g_slice, value=1 - beta2
                )
                # bias corrections
                bias1 = 1 - beta1**t
                bias2 = 1 - beta2**t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(
                    dist.all_gather_into_tensor(
                        p, p_slice, async_op=True
                    ).get_future()
                )
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model


def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(
        self,
        in_features: int,
        out_features: int,
        use_fp8=False,
        x_s=1.0,
        w_s=1.0,
        grad_s=1.0,
    ):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (
            self.in_features**-0.5
        )  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3**0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(
                _x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s
            )[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    rotary_cos: torch.Tensor
    rotary_sin: torch.Tensor
    attn_scale: float


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        rotary_cos, rotary_sin = attn_args.rotary_cos, attn_args.rotary_sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = (
            attn_args.seqlens,
            attn_args.attn_scale,
            attn_args.bm_size,
        )

        q, k, v = (
            F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x))
            .view(B, T, 3 * self.num_heads, self.head_dim)
            .chunk(3, dim=-2)
        )
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = (
            rotary(q, rotary_cos, rotary_sin),
            rotary(k, rotary_cos, rotary_sin),
        )
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(
                v
            )  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = (
            args.train_max_seq_len
            if self.training
            else (args.val_batch_size // (grad_accum_steps * world_size))
        )

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(
            q[0],
            k[0],
            v[0],
            cu_seqlens_q=seqlens,
            cu_seqlens_k=seqlens,
            max_seqlen_q=max_len,
            max_seqlen_k=max_len,
            causal=True,
            softmax_scale=attn_scale,
            window_size=(bm_size, 0),
        )
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(
            self.attn_gate(x[..., : self.attn_gate.weight.size(-1)])
        ).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(
            B, T, self.num_heads * self.head_dim
        )  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x
        ).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = (
            CausalSelfAttention(dim, head_dim, num_heads)
            if layer_idx != 7
            else None
        )
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(
        self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs
    ):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model


def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        num_layers: int,
        num_heads: int,
        head_dim: int,
        model_dim: int,
        max_seq_len: int,
    ):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList(
            [nn.Embedding(vocab_size, model_dim) for _ in range(3)]
        )
        self.blocks = nn.ModuleList(
            [
                Block(model_dim, head_dim, num_heads, i)
                for i in range(num_layers)
            ]
        )
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(
            model_dim,
            vocab_size,
            use_fp8=use_fp8,
            x_s=(model_dim**0.5) / 448,
            w_s=2**-9,
            grad_s=1 / 448,
        )
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.ones(pad),
                ]
            )
        )
        self.max_seq_len = max_seq_len
        self.setup_yarn(head_dim)
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.0
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.0
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def setup_yarn(self, head_dim: int):
        # store single copy of rotary tensors
        angular_freq = (1 / 1024) ** torch.linspace(
            0, 1, steps=head_dim // 4, dtype=torch.float32
        )
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat(
            [angular_freq, angular_freq.new_zeros(head_dim // 4)]
        )
        t = torch.arange(self.max_seq_len, dtype=torch.float32)
        theta = torch.outer(t, angular_freq)
        self.rotary_cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.rotary_sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq

        # scale attention factor f in attn=softmax(f*qk) logarithmically with window size @classiclarryd
        windows = list(
            dict.fromkeys(list(args.ws_schedule) + [args.ws_validate])
        )
        scale_factors = [
            0.2 * math.log(curr / prev) + 1
            for prev, curr in zip(windows[:-1], windows[1:])
        ]
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        attn_scales = list(
            accumulate([0.1] + scale_factors, lambda acc, factor: acc * factor)
        )
        self.attn_scales = dict(zip(windows, attn_scales))

    def apply_yarn(
        self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32
    ):
        rotations = (
            args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        )
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp(
            (rotations - alpha) / (beta - alpha), 0, 1
        )
        self.angular_freq *= scaling_factor + interpolation_weight * (
            1 - scaling_factor
        )
        t = torch.arange(
            self.max_seq_len,
            dtype=torch.float32,
            device=self.angular_freq.device,
        )
        theta = torch.outer(t, self.angular_freq)
        self.rotary_cos.copy_(theta.cos())
        self.rotary_sin.copy_(theta.sin())

    def forward(
        self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int
    ):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = (
            [ve[0], ve[1], ve[2]]
            + [None] * (len(self.blocks) - 6)
            + [ve[0], ve[1], ve[2]]
        )
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        bm_sizes = [
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
        ]
        assert len(bm_sizes) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]).to(
            torch.bfloat16
        )  # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[: (len(self.blocks) // 2)]
        lambdas = self.scalars[
            1 * len(self.blocks) : 3 * len(self.blocks)
        ].view(-1, 2)
        sa_lambdas = self.scalars[
            3 * len(self.blocks) : 5 * len(self.blocks)
        ].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                rotary_cos=self.rotary_cos,
                rotary_sin=self.rotary_sin,
                attn_scale=self.attn_scales[ws],
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(
            logits.view(-1, logits.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader


def _load_data_shard(file: Path):
    header = torch.from_file(
        str(file), False, 256, dtype=torch.int32
    )  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(
            num_tokens, dtype=torch.uint16, pin_memory=True
        )  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(
            tokens.numpy()
        )  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, (
            "number of tokens read does not match header"
        )
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1):
        # Precompute BOS positions once per shard
        self.size = tokens.numel()
        self.bos_idx = (
            (tokens == BOS_ID)
            .nonzero(as_tuple=True)[0]
            .to(torch.int64)
            .cpu()
            .numpy()
        )
        self.i = 0
        self.world_size = world_size

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(
                        f"Insufficient BOS ahead of position {cur}; hit tail of shard."
                    )
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(
                    self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                    cur + max_seq_len,
                    cur + num_tokens_local - cur_len + 1,
                )
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx

        return starts, ends


def distributed_data_generator(
    filename_pattern: str,
    num_tokens: int,
    max_seq_len: int,
    grad_accum_steps: int = 1,
    align_to_bos: bool = True,
):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, (
        "Batch size must be divisible by world size"
    )
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(
            f"No files found for pattern: {filename_pattern}"
        )

    file_iter = iter(
        files
    )  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    finder = BOSFinder(tokens, world_size=world_size) if align_to_bos else None
    pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(
            num_tokens_local // 300, n=128
        )  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(
                    num_tokens_local, max_seq_len
                )
                start_idxs, end_idxs = (
                    torch.tensor(seq_starts[rank]),
                    torch.tensor(seq_ends[rank]),
                )
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens = _load_data_shard(next(file_iter))
                finder = BOSFinder(tokens, world_size=world_size)
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= (
                1  # last document was too long to account for _targets offset
            )
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(
                tokens
            ):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local : pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(
                num_tokens_local,
            )
            _targets = buf[1:].view(
                num_tokens_local,
            )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1 : len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(
                device="cuda", dtype=torch.int32, non_blocking=True
            ),
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, (
                "Num tokens must be divisible by world size"
            )
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main


@dataclass
class Hyperparameters:
    # data
    train_files: str = (
        "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    )
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1670  # number of iterations to run
    cooldown_frac: int = (
        0.5  # fraction of training spent cooling down the learning rate
    )
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = (
        125  # every how many steps to evaluate val loss? 0 for only at the end
    )
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws @classiclarryd


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = rank == 0  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(
    f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}"
)
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import

    return subprocess.run(
        ["nvidia-smi"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    ).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size)
    // (grad_accum_steps * world_size),
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [
    p
    for n, p in model.blocks.named_parameters()
    if p.ndim >= 2 and "embed" not in n
]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(
    hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)  # save the initial state
train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    ws = args.ws_schedule[
        step % len(args.ws_schedule)
    ]  # each window size is a new graph, need to warm up each
    model(inputs, targets, cum_seqlens, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws = get_ws(0)
for step in range(train_steps + 1):
    last_step = step == train_steps
    new_ws = get_ws(step)
    if new_ws != ws:
        model.apply_yarn(ws, new_ws)
        ws = new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (
        args.val_loss_every > 0 and step % args.val_loss_every == 0
    ):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(
            args.val_files,
            args.val_batch_size,
            -1,
            grad_accum_steps=grad_accum_steps,
            align_to_bos=False,
        )
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True,
        )
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(
                step=step,
                code=code,
                model=model.state_dict(),
                optimizers=[opt.state_dict() for opt in optimizers],
            )
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1)  # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (
        time.perf_counter() - t0
    )
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True,
    )

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.11 (main, Sep  2 2025, 14:20:58) [Clang 20.1.4 ]
Running PyTorch 2.9.0.dev20250718+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 11 09:12:55 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                  Off |
| N/A   40C    P0            126W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                  Off |
| N/A   45C    P0            134W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:65:00.0 Off |                  Off |
| N/A   45C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:68:00.0 Off |                  Off |
| N/A   37C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:85:00.0 Off |                  Off |
| N/A   37C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:86:00.0 Off |                  Off |
| N/A   45C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:E5:00.0 Off |                  Off |
| N/A   45C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E8:00.0 Off |                  Off |
| N/A   40C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1670 val_loss:10.8258 train_time:0ms step_avg:0.07ms
step:1/1670 train_time:293ms step_avg:292.89ms
step:2/1670 train_time:312ms step_avg:156.04ms
step:3/1670 train_time:382ms step_avg:127.20ms
step:4/1670 train_time:471ms step_avg:117.85ms
step:5/1670 train_time:562ms step_avg:112.31ms
step:6/1670 train_time:652ms step_avg:108.62ms
step:7/1670 train_time:742ms step_avg:105.98ms
step:8/1670 train_time:832ms step_avg:104.01ms
step:9/1670 train_time:922ms step_avg:102.44ms
step:10/1670 train_time:1013ms step_avg:101.29ms
step:11/1670 train_time:1104ms step_avg:100.39ms
step:12/1670 train_time:1199ms step_avg:99.88ms
step:13/1670 train_time:1293ms step_avg:99.45ms
step:14/1670 train_time:1386ms step_avg:98.99ms
step:15/1670 train_time:1478ms step_avg:98.50ms
step:16/1670 train_time:1569ms step_avg:98.04ms
step:17/1670 train_time:1660ms step_avg:97.63ms
step:18/1670 train_time:1750ms step_avg:97.24ms
step:19/1670 train_time:1842ms step_avg:96.97ms
step:20/1670 train_time:1934ms step_avg:96.70ms
step:21/1670 train_time:2024ms step_avg:96.40ms
step:22/1670 train_time:2117ms step_avg:96.21ms
step:23/1670 train_time:2210ms step_avg:96.07ms
step:24/1670 train_time:2302ms step_avg:95.94ms
step:25/1670 train_time:2396ms step_avg:95.83ms
step:26/1670 train_time:2488ms step_avg:95.69ms
step:27/1670 train_time:2580ms step_avg:95.55ms
step:28/1670 train_time:2672ms step_avg:95.43ms
step:29/1670 train_time:2763ms step_avg:95.28ms
step:30/1670 train_time:2854ms step_avg:95.13ms
step:31/1670 train_time:2944ms step_avg:94.98ms
step:32/1670 train_time:3036ms step_avg:94.87ms
step:33/1670 train_time:3126ms step_avg:94.73ms
step:34/1670 train_time:3220ms step_avg:94.71ms
step:35/1670 train_time:3312ms step_avg:94.64ms
step:36/1670 train_time:3404ms step_avg:94.55ms
step:37/1670 train_time:3498ms step_avg:94.53ms
step:38/1670 train_time:3590ms step_avg:94.47ms
step:39/1670 train_time:3682ms step_avg:94.41ms
step:40/1670 train_time:3775ms step_avg:94.37ms
step:41/1670 train_time:3866ms step_avg:94.28ms
step:42/1670 train_time:3957ms step_avg:94.23ms
step:43/1670 train_time:4048ms step_avg:94.15ms
step:44/1670 train_time:4139ms step_avg:94.08ms
step:45/1670 train_time:4231ms step_avg:94.03ms
step:46/1670 train_time:4324ms step_avg:93.99ms
step:47/1670 train_time:4417ms step_avg:93.97ms
step:48/1670 train_time:4509ms step_avg:93.94ms
step:49/1670 train_time:4601ms step_avg:93.90ms
step:50/1670 train_time:4695ms step_avg:93.89ms
step:51/1670 train_time:4786ms step_avg:93.85ms
step:52/1670 train_time:4878ms step_avg:93.82ms
step:53/1670 train_time:4970ms step_avg:93.77ms
step:54/1670 train_time:5061ms step_avg:93.72ms
step:55/1670 train_time:5152ms step_avg:93.68ms
step:56/1670 train_time:5243ms step_avg:93.62ms
step:57/1670 train_time:5334ms step_avg:93.58ms
step:58/1670 train_time:5426ms step_avg:93.55ms
step:59/1670 train_time:5520ms step_avg:93.55ms
step:60/1670 train_time:5613ms step_avg:93.54ms
step:61/1670 train_time:5704ms step_avg:93.51ms
step:62/1670 train_time:5798ms step_avg:93.51ms
step:63/1670 train_time:5890ms step_avg:93.49ms
step:64/1670 train_time:5981ms step_avg:93.46ms
step:65/1670 train_time:6073ms step_avg:93.43ms
step:66/1670 train_time:6164ms step_avg:93.39ms
step:67/1670 train_time:6256ms step_avg:93.37ms
step:68/1670 train_time:6346ms step_avg:93.33ms
step:69/1670 train_time:6439ms step_avg:93.32ms
step:70/1670 train_time:6531ms step_avg:93.30ms
step:71/1670 train_time:6623ms step_avg:93.28ms
step:72/1670 train_time:6716ms step_avg:93.28ms
step:73/1670 train_time:6808ms step_avg:93.27ms
step:74/1670 train_time:6900ms step_avg:93.25ms
step:75/1670 train_time:6991ms step_avg:93.21ms
step:76/1670 train_time:7082ms step_avg:93.18ms
step:77/1670 train_time:7174ms step_avg:93.17ms
step:78/1670 train_time:7263ms step_avg:93.12ms
step:79/1670 train_time:7355ms step_avg:93.10ms
step:80/1670 train_time:7446ms step_avg:93.07ms
step:81/1670 train_time:7537ms step_avg:93.05ms
step:82/1670 train_time:7628ms step_avg:93.03ms
step:83/1670 train_time:7720ms step_avg:93.01ms
step:84/1670 train_time:7812ms step_avg:93.01ms
step:85/1670 train_time:7904ms step_avg:92.99ms
step:86/1670 train_time:7996ms step_avg:92.98ms
step:87/1670 train_time:8089ms step_avg:92.97ms
step:88/1670 train_time:8180ms step_avg:92.95ms
step:89/1670 train_time:8271ms step_avg:92.93ms
step:90/1670 train_time:8362ms step_avg:92.91ms
step:91/1670 train_time:8454ms step_avg:92.90ms
step:92/1670 train_time:8545ms step_avg:92.88ms
step:93/1670 train_time:8637ms step_avg:92.87ms
step:94/1670 train_time:8728ms step_avg:92.85ms
step:95/1670 train_time:8820ms step_avg:92.84ms
step:96/1670 train_time:8911ms step_avg:92.83ms
step:97/1670 train_time:9003ms step_avg:92.81ms
step:98/1670 train_time:9093ms step_avg:92.79ms
step:99/1670 train_time:9184ms step_avg:92.77ms
step:100/1670 train_time:9276ms step_avg:92.76ms
step:101/1670 train_time:9371ms step_avg:92.78ms
step:102/1670 train_time:9462ms step_avg:92.76ms
step:103/1670 train_time:9554ms step_avg:92.75ms
step:104/1670 train_time:9644ms step_avg:92.73ms
step:105/1670 train_time:9735ms step_avg:92.71ms
step:106/1670 train_time:9826ms step_avg:92.70ms
step:107/1670 train_time:9917ms step_avg:92.68ms
step:108/1670 train_time:10008ms step_avg:92.67ms
step:109/1670 train_time:10100ms step_avg:92.66ms
step:110/1670 train_time:10191ms step_avg:92.64ms
step:111/1670 train_time:10282ms step_avg:92.63ms
step:112/1670 train_time:10374ms step_avg:92.62ms
step:113/1670 train_time:10464ms step_avg:92.60ms
step:114/1670 train_time:10555ms step_avg:92.59ms
step:115/1670 train_time:10646ms step_avg:92.57ms
step:116/1670 train_time:10738ms step_avg:92.57ms
step:117/1670 train_time:10829ms step_avg:92.56ms
step:118/1670 train_time:10920ms step_avg:92.55ms
step:119/1670 train_time:11011ms step_avg:92.53ms
step:120/1670 train_time:11102ms step_avg:92.52ms
step:121/1670 train_time:11193ms step_avg:92.50ms
step:122/1670 train_time:11284ms step_avg:92.49ms
step:123/1670 train_time:11375ms step_avg:92.48ms
step:124/1670 train_time:11467ms step_avg:92.47ms
step:125/1670 train_time:11558ms step_avg:92.46ms
step:125/1670 val_loss:4.3115 train_time:11648ms step_avg:93.19ms
step:126/1670 train_time:11668ms step_avg:92.61ms
step:127/1670 train_time:11743ms step_avg:92.47ms
step:128/1670 train_time:11845ms step_avg:92.54ms
step:129/1670 train_time:11938ms step_avg:92.54ms
step:130/1670 train_time:12028ms step_avg:92.52ms
step:131/1670 train_time:12118ms step_avg:92.50ms
step:132/1670 train_time:12208ms step_avg:92.48ms
step:133/1670 train_time:12299ms step_avg:92.47ms
step:134/1670 train_time:12388ms step_avg:92.45ms
step:135/1670 train_time:12478ms step_avg:92.43ms
step:136/1670 train_time:12568ms step_avg:92.41ms
step:137/1670 train_time:12659ms step_avg:92.40ms
step:138/1670 train_time:12753ms step_avg:92.41ms
step:139/1670 train_time:12846ms step_avg:92.42ms
step:140/1670 train_time:12939ms step_avg:92.42ms
step:141/1670 train_time:13030ms step_avg:92.41ms
step:142/1670 train_time:13121ms step_avg:92.40ms
step:143/1670 train_time:13210ms step_avg:92.38ms
step:144/1670 train_time:13300ms step_avg:92.36ms
step:145/1670 train_time:13391ms step_avg:92.35ms
step:146/1670 train_time:13482ms step_avg:92.34ms
step:147/1670 train_time:13572ms step_avg:92.33ms
step:148/1670 train_time:13666ms step_avg:92.34ms
step:149/1670 train_time:13758ms step_avg:92.33ms
step:150/1670 train_time:13850ms step_avg:92.33ms
step:151/1670 train_time:13945ms step_avg:92.35ms
step:152/1670 train_time:14036ms step_avg:92.34ms
step:153/1670 train_time:14126ms step_avg:92.33ms
step:154/1670 train_time:14217ms step_avg:92.32ms
step:155/1670 train_time:14307ms step_avg:92.30ms
step:156/1670 train_time:14398ms step_avg:92.29ms
step:157/1670 train_time:14488ms step_avg:92.28ms
step:158/1670 train_time:14579ms step_avg:92.27ms
step:159/1670 train_time:14669ms step_avg:92.26ms
step:160/1670 train_time:14763ms step_avg:92.27ms
step:161/1670 train_time:14856ms step_avg:92.27ms
step:162/1670 train_time:14948ms step_avg:92.27ms
step:163/1670 train_time:15040ms step_avg:92.27ms
step:164/1670 train_time:15131ms step_avg:92.26ms
step:165/1670 train_time:15222ms step_avg:92.25ms
step:166/1670 train_time:15311ms step_avg:92.24ms
step:167/1670 train_time:15404ms step_avg:92.24ms
step:168/1670 train_time:15494ms step_avg:92.23ms
step:169/1670 train_time:15583ms step_avg:92.21ms
step:170/1670 train_time:15675ms step_avg:92.21ms
step:171/1670 train_time:15767ms step_avg:92.21ms
step:172/1670 train_time:15859ms step_avg:92.20ms
step:173/1670 train_time:15951ms step_avg:92.20ms
step:174/1670 train_time:16043ms step_avg:92.20ms
step:175/1670 train_time:16133ms step_avg:92.19ms
step:176/1670 train_time:16224ms step_avg:92.18ms
step:177/1670 train_time:16315ms step_avg:92.17ms
step:178/1670 train_time:16406ms step_avg:92.17ms
step:179/1670 train_time:16498ms step_avg:92.17ms
step:180/1670 train_time:16588ms step_avg:92.16ms
step:181/1670 train_time:16680ms step_avg:92.15ms
step:182/1670 train_time:16770ms step_avg:92.15ms
step:183/1670 train_time:16862ms step_avg:92.14ms
step:184/1670 train_time:16953ms step_avg:92.13ms
step:185/1670 train_time:17044ms step_avg:92.13ms
step:186/1670 train_time:17135ms step_avg:92.12ms
step:187/1670 train_time:17226ms step_avg:92.12ms
step:188/1670 train_time:17316ms step_avg:92.11ms
step:189/1670 train_time:17407ms step_avg:92.10ms
step:190/1670 train_time:17498ms step_avg:92.10ms
step:191/1670 train_time:17589ms step_avg:92.09ms
step:192/1670 train_time:17680ms step_avg:92.09ms
step:193/1670 train_time:17771ms step_avg:92.08ms
step:194/1670 train_time:17864ms step_avg:92.08ms
step:195/1670 train_time:17955ms step_avg:92.08ms
step:196/1670 train_time:18046ms step_avg:92.07ms
step:197/1670 train_time:18138ms step_avg:92.07ms
step:198/1670 train_time:18229ms step_avg:92.06ms
step:199/1670 train_time:18319ms step_avg:92.06ms
step:200/1670 train_time:18409ms step_avg:92.05ms
step:201/1670 train_time:18500ms step_avg:92.04ms
step:202/1670 train_time:18593ms step_avg:92.04ms
step:203/1670 train_time:18685ms step_avg:92.04ms
step:204/1670 train_time:18776ms step_avg:92.04ms
step:205/1670 train_time:18868ms step_avg:92.04ms
step:206/1670 train_time:18959ms step_avg:92.03ms
step:207/1670 train_time:19050ms step_avg:92.03ms
step:208/1670 train_time:19141ms step_avg:92.03ms
step:209/1670 train_time:19232ms step_avg:92.02ms
step:210/1670 train_time:19323ms step_avg:92.01ms
step:211/1670 train_time:19414ms step_avg:92.01ms
step:212/1670 train_time:19505ms step_avg:92.00ms
step:213/1670 train_time:19752ms step_avg:92.73ms
step:214/1670 train_time:19831ms step_avg:92.67ms
step:215/1670 train_time:19920ms step_avg:92.65ms
step:216/1670 train_time:20010ms step_avg:92.64ms
step:217/1670 train_time:20101ms step_avg:92.63ms
step:218/1670 train_time:20191ms step_avg:92.62ms
step:219/1670 train_time:20281ms step_avg:92.61ms
step:220/1670 train_time:20370ms step_avg:92.59ms
step:221/1670 train_time:20461ms step_avg:92.58ms
step:222/1670 train_time:20550ms step_avg:92.57ms
step:223/1670 train_time:20644ms step_avg:92.58ms
step:224/1670 train_time:20742ms step_avg:92.60ms
step:225/1670 train_time:20835ms step_avg:92.60ms
step:226/1670 train_time:20926ms step_avg:92.59ms
step:227/1670 train_time:21017ms step_avg:92.58ms
step:228/1670 train_time:21107ms step_avg:92.57ms
step:229/1670 train_time:21199ms step_avg:92.57ms
step:230/1670 train_time:21289ms step_avg:92.56ms
step:231/1670 train_time:21379ms step_avg:92.55ms
step:232/1670 train_time:21470ms step_avg:92.54ms
step:233/1670 train_time:21560ms step_avg:92.53ms
step:234/1670 train_time:21652ms step_avg:92.53ms
step:235/1670 train_time:21745ms step_avg:92.53ms
step:236/1670 train_time:21837ms step_avg:92.53ms
step:237/1670 train_time:21928ms step_avg:92.53ms
step:238/1670 train_time:22019ms step_avg:92.52ms
step:239/1670 train_time:22110ms step_avg:92.51ms
step:240/1670 train_time:22201ms step_avg:92.50ms
step:241/1670 train_time:22290ms step_avg:92.49ms
step:242/1670 train_time:22381ms step_avg:92.48ms
step:243/1670 train_time:22471ms step_avg:92.47ms
step:244/1670 train_time:22561ms step_avg:92.46ms
step:245/1670 train_time:22653ms step_avg:92.46ms
step:246/1670 train_time:22745ms step_avg:92.46ms
step:247/1670 train_time:22837ms step_avg:92.46ms
step:248/1670 train_time:22928ms step_avg:92.45ms
step:249/1670 train_time:23020ms step_avg:92.45ms
step:250/1670 train_time:23111ms step_avg:92.44ms
step:250/1670 val_loss:3.9716 train_time:23200ms step_avg:92.80ms
step:251/1670 train_time:23220ms step_avg:92.51ms
step:252/1670 train_time:23292ms step_avg:92.43ms
step:253/1670 train_time:23383ms step_avg:92.42ms
step:254/1670 train_time:23475ms step_avg:92.42ms
step:255/1670 train_time:23565ms step_avg:92.41ms
step:256/1670 train_time:23657ms step_avg:92.41ms
step:257/1670 train_time:23747ms step_avg:92.40ms
step:258/1670 train_time:23839ms step_avg:92.40ms
step:259/1670 train_time:23929ms step_avg:92.39ms
step:260/1670 train_time:24020ms step_avg:92.39ms
step:261/1670 train_time:24112ms step_avg:92.38ms
step:262/1670 train_time:24206ms step_avg:92.39ms
step:263/1670 train_time:24297ms step_avg:92.38ms
step:264/1670 train_time:24388ms step_avg:92.38ms
step:265/1670 train_time:24479ms step_avg:92.37ms
step:266/1670 train_time:24571ms step_avg:92.37ms
step:267/1670 train_time:24661ms step_avg:92.36ms
step:268/1670 train_time:24752ms step_avg:92.36ms
step:269/1670 train_time:24843ms step_avg:92.35ms
step:270/1670 train_time:24934ms step_avg:92.35ms
step:271/1670 train_time:25025ms step_avg:92.34ms
step:272/1670 train_time:25117ms step_avg:92.34ms
step:273/1670 train_time:25209ms step_avg:92.34ms
step:274/1670 train_time:25301ms step_avg:92.34ms
step:275/1670 train_time:25393ms step_avg:92.34ms
step:276/1670 train_time:25483ms step_avg:92.33ms
step:277/1670 train_time:25577ms step_avg:92.34ms
step:278/1670 train_time:25667ms step_avg:92.33ms
step:279/1670 train_time:25759ms step_avg:92.33ms
step:280/1670 train_time:25849ms step_avg:92.32ms
step:281/1670 train_time:25941ms step_avg:92.32ms
step:282/1670 train_time:26031ms step_avg:92.31ms
step:283/1670 train_time:26123ms step_avg:92.31ms
step:284/1670 train_time:26214ms step_avg:92.30ms
step:285/1670 train_time:26305ms step_avg:92.30ms
step:286/1670 train_time:26396ms step_avg:92.29ms
step:287/1670 train_time:26487ms step_avg:92.29ms
step:288/1670 train_time:26579ms step_avg:92.29ms
step:289/1670 train_time:26671ms step_avg:92.29ms
step:290/1670 train_time:26762ms step_avg:92.28ms
step:291/1670 train_time:26853ms step_avg:92.28ms
step:292/1670 train_time:26943ms step_avg:92.27ms
step:293/1670 train_time:27035ms step_avg:92.27ms
step:294/1670 train_time:27125ms step_avg:92.26ms
step:295/1670 train_time:27217ms step_avg:92.26ms
step:296/1670 train_time:27308ms step_avg:92.26ms
step:297/1670 train_time:27400ms step_avg:92.25ms
step:298/1670 train_time:27491ms step_avg:92.25ms
step:299/1670 train_time:27582ms step_avg:92.25ms
step:300/1670 train_time:27673ms step_avg:92.24ms
step:301/1670 train_time:27764ms step_avg:92.24ms
step:302/1670 train_time:27855ms step_avg:92.24ms
step:303/1670 train_time:27946ms step_avg:92.23ms
step:304/1670 train_time:28038ms step_avg:92.23ms
step:305/1670 train_time:28129ms step_avg:92.22ms
step:306/1670 train_time:28220ms step_avg:92.22ms
step:307/1670 train_time:28311ms step_avg:92.22ms
step:308/1670 train_time:28402ms step_avg:92.21ms
step:309/1670 train_time:28494ms step_avg:92.21ms
step:310/1670 train_time:28584ms step_avg:92.21ms
step:311/1670 train_time:28675ms step_avg:92.20ms
step:312/1670 train_time:28765ms step_avg:92.20ms
step:313/1670 train_time:28857ms step_avg:92.19ms
step:314/1670 train_time:28948ms step_avg:92.19ms
step:315/1670 train_time:29040ms step_avg:92.19ms
step:316/1670 train_time:29131ms step_avg:92.19ms
step:317/1670 train_time:29222ms step_avg:92.18ms
step:318/1670 train_time:29313ms step_avg:92.18ms
step:319/1670 train_time:29403ms step_avg:92.17ms
step:320/1670 train_time:29494ms step_avg:92.17ms
step:321/1670 train_time:29584ms step_avg:92.16ms
step:322/1670 train_time:29675ms step_avg:92.16ms
step:323/1670 train_time:29766ms step_avg:92.16ms
step:324/1670 train_time:29859ms step_avg:92.16ms
step:325/1670 train_time:29951ms step_avg:92.16ms
step:326/1670 train_time:30043ms step_avg:92.16ms
step:327/1670 train_time:30134ms step_avg:92.15ms
step:328/1670 train_time:30225ms step_avg:92.15ms
step:329/1670 train_time:30316ms step_avg:92.15ms
step:330/1670 train_time:30406ms step_avg:92.14ms
step:331/1670 train_time:30497ms step_avg:92.14ms
step:332/1670 train_time:30588ms step_avg:92.13ms
step:333/1670 train_time:30680ms step_avg:92.13ms
step:334/1670 train_time:30772ms step_avg:92.13ms
step:335/1670 train_time:30863ms step_avg:92.13ms
step:336/1670 train_time:30954ms step_avg:92.13ms
step:337/1670 train_time:31045ms step_avg:92.12ms
step:338/1670 train_time:31137ms step_avg:92.12ms
step:339/1670 train_time:31228ms step_avg:92.12ms
step:340/1670 train_time:31319ms step_avg:92.11ms
step:341/1670 train_time:31410ms step_avg:92.11ms
step:342/1670 train_time:31500ms step_avg:92.11ms
step:343/1670 train_time:31590ms step_avg:92.10ms
step:344/1670 train_time:31681ms step_avg:92.10ms
step:345/1670 train_time:31772ms step_avg:92.09ms
step:346/1670 train_time:31863ms step_avg:92.09ms
step:347/1670 train_time:31955ms step_avg:92.09ms
step:348/1670 train_time:32046ms step_avg:92.09ms
step:349/1670 train_time:32138ms step_avg:92.09ms
step:350/1670 train_time:32229ms step_avg:92.08ms
step:351/1670 train_time:32320ms step_avg:92.08ms
step:352/1670 train_time:32412ms step_avg:92.08ms
step:353/1670 train_time:32502ms step_avg:92.08ms
step:354/1670 train_time:32593ms step_avg:92.07ms
step:355/1670 train_time:32683ms step_avg:92.07ms
step:356/1670 train_time:32776ms step_avg:92.07ms
step:357/1670 train_time:32866ms step_avg:92.06ms
step:358/1670 train_time:32958ms step_avg:92.06ms
step:359/1670 train_time:33050ms step_avg:92.06ms
step:360/1670 train_time:33141ms step_avg:92.06ms
step:361/1670 train_time:33232ms step_avg:92.06ms
step:362/1670 train_time:33323ms step_avg:92.05ms
step:363/1670 train_time:33414ms step_avg:92.05ms
step:364/1670 train_time:33505ms step_avg:92.05ms
step:365/1670 train_time:33596ms step_avg:92.04ms
step:366/1670 train_time:33687ms step_avg:92.04ms
step:367/1670 train_time:33779ms step_avg:92.04ms
step:368/1670 train_time:33871ms step_avg:92.04ms
step:369/1670 train_time:33962ms step_avg:92.04ms
step:370/1670 train_time:34054ms step_avg:92.04ms
step:371/1670 train_time:34144ms step_avg:92.03ms
step:372/1670 train_time:34235ms step_avg:92.03ms
step:373/1670 train_time:34327ms step_avg:92.03ms
step:374/1670 train_time:34418ms step_avg:92.03ms
step:375/1670 train_time:34510ms step_avg:92.03ms
step:375/1670 val_loss:3.8163 train_time:34600ms step_avg:92.27ms
step:376/1670 train_time:34620ms step_avg:92.07ms
step:377/1670 train_time:34692ms step_avg:92.02ms
step:378/1670 train_time:34784ms step_avg:92.02ms
step:379/1670 train_time:34874ms step_avg:92.02ms
step:380/1670 train_time:34964ms step_avg:92.01ms
step:381/1670 train_time:35054ms step_avg:92.01ms
step:382/1670 train_time:35145ms step_avg:92.00ms
step:383/1670 train_time:35235ms step_avg:92.00ms
step:384/1670 train_time:35326ms step_avg:92.00ms
step:385/1670 train_time:35419ms step_avg:92.00ms
step:386/1670 train_time:35510ms step_avg:92.00ms
step:387/1670 train_time:35603ms step_avg:92.00ms
step:388/1670 train_time:35696ms step_avg:92.00ms
step:389/1670 train_time:35788ms step_avg:92.00ms
step:390/1670 train_time:35878ms step_avg:92.00ms
step:391/1670 train_time:35969ms step_avg:91.99ms
step:392/1670 train_time:36060ms step_avg:91.99ms
step:393/1670 train_time:36150ms step_avg:91.99ms
step:394/1670 train_time:36241ms step_avg:91.98ms
step:395/1670 train_time:36332ms step_avg:91.98ms
step:396/1670 train_time:36423ms step_avg:91.98ms
step:397/1670 train_time:36514ms step_avg:91.98ms
step:398/1670 train_time:36608ms step_avg:91.98ms
step:399/1670 train_time:36699ms step_avg:91.98ms
step:400/1670 train_time:36790ms step_avg:91.98ms
step:401/1670 train_time:36881ms step_avg:91.97ms
step:402/1670 train_time:36972ms step_avg:91.97ms
step:403/1670 train_time:37063ms step_avg:91.97ms
step:404/1670 train_time:37153ms step_avg:91.96ms
step:405/1670 train_time:37245ms step_avg:91.96ms
step:406/1670 train_time:37335ms step_avg:91.96ms
step:407/1670 train_time:37427ms step_avg:91.96ms
step:408/1670 train_time:37518ms step_avg:91.95ms
step:409/1670 train_time:37609ms step_avg:91.95ms
step:410/1670 train_time:37701ms step_avg:91.95ms
step:411/1670 train_time:37792ms step_avg:91.95ms
step:412/1670 train_time:37884ms step_avg:91.95ms
step:413/1670 train_time:37974ms step_avg:91.95ms
step:414/1670 train_time:38064ms step_avg:91.94ms
step:415/1670 train_time:38155ms step_avg:91.94ms
step:416/1670 train_time:38248ms step_avg:91.94ms
step:417/1670 train_time:38338ms step_avg:91.94ms
step:418/1670 train_time:38430ms step_avg:91.94ms
step:419/1670 train_time:38522ms step_avg:91.94ms
step:420/1670 train_time:38613ms step_avg:91.94ms
step:421/1670 train_time:38706ms step_avg:91.94ms
step:422/1670 train_time:38797ms step_avg:91.94ms
step:423/1670 train_time:38888ms step_avg:91.93ms
step:424/1670 train_time:38978ms step_avg:91.93ms
step:425/1670 train_time:39227ms step_avg:92.30ms
step:426/1670 train_time:39298ms step_avg:92.25ms
step:427/1670 train_time:39387ms step_avg:92.24ms
step:428/1670 train_time:39477ms step_avg:92.24ms
step:429/1670 train_time:39567ms step_avg:92.23ms
step:430/1670 train_time:39657ms step_avg:92.23ms
step:431/1670 train_time:39747ms step_avg:92.22ms
step:432/1670 train_time:39837ms step_avg:92.22ms
step:433/1670 train_time:39928ms step_avg:92.21ms
step:434/1670 train_time:40019ms step_avg:92.21ms
step:435/1670 train_time:40115ms step_avg:92.22ms
step:436/1670 train_time:40214ms step_avg:92.23ms
step:437/1670 train_time:40307ms step_avg:92.24ms
step:438/1670 train_time:40398ms step_avg:92.23ms
step:439/1670 train_time:40488ms step_avg:92.23ms
step:440/1670 train_time:40579ms step_avg:92.22ms
step:441/1670 train_time:40669ms step_avg:92.22ms
step:442/1670 train_time:40759ms step_avg:92.21ms
step:443/1670 train_time:40849ms step_avg:92.21ms
step:444/1670 train_time:40939ms step_avg:92.21ms
step:445/1670 train_time:41031ms step_avg:92.20ms
step:446/1670 train_time:41123ms step_avg:92.20ms
step:447/1670 train_time:41215ms step_avg:92.20ms
step:448/1670 train_time:41309ms step_avg:92.21ms
step:449/1670 train_time:41400ms step_avg:92.21ms
step:450/1670 train_time:41491ms step_avg:92.20ms
step:451/1670 train_time:41582ms step_avg:92.20ms
step:452/1670 train_time:41672ms step_avg:92.20ms
step:453/1670 train_time:41763ms step_avg:92.19ms
step:454/1670 train_time:41853ms step_avg:92.19ms
step:455/1670 train_time:41944ms step_avg:92.18ms
step:456/1670 train_time:42035ms step_avg:92.18ms
step:457/1670 train_time:42128ms step_avg:92.18ms
step:458/1670 train_time:42222ms step_avg:92.19ms
step:459/1670 train_time:42314ms step_avg:92.19ms
step:460/1670 train_time:42406ms step_avg:92.19ms
step:461/1670 train_time:42497ms step_avg:92.19ms
step:462/1670 train_time:42588ms step_avg:92.18ms
step:463/1670 train_time:42679ms step_avg:92.18ms
step:464/1670 train_time:42769ms step_avg:92.17ms
step:465/1670 train_time:42859ms step_avg:92.17ms
step:466/1670 train_time:42950ms step_avg:92.17ms
step:467/1670 train_time:43041ms step_avg:92.17ms
step:468/1670 train_time:43132ms step_avg:92.16ms
step:469/1670 train_time:43225ms step_avg:92.16ms
step:470/1670 train_time:43316ms step_avg:92.16ms
step:471/1670 train_time:43408ms step_avg:92.16ms
step:472/1670 train_time:43499ms step_avg:92.16ms
step:473/1670 train_time:43589ms step_avg:92.16ms
step:474/1670 train_time:43680ms step_avg:92.15ms
step:475/1670 train_time:43770ms step_avg:92.15ms
step:476/1670 train_time:43860ms step_avg:92.14ms
step:477/1670 train_time:43951ms step_avg:92.14ms
step:478/1670 train_time:44042ms step_avg:92.14ms
step:479/1670 train_time:44133ms step_avg:92.14ms
step:480/1670 train_time:44225ms step_avg:92.13ms
step:481/1670 train_time:44316ms step_avg:92.13ms
step:482/1670 train_time:44409ms step_avg:92.13ms
step:483/1670 train_time:44500ms step_avg:92.13ms
step:484/1670 train_time:44591ms step_avg:92.13ms
step:485/1670 train_time:44682ms step_avg:92.13ms
step:486/1670 train_time:44773ms step_avg:92.12ms
step:487/1670 train_time:44864ms step_avg:92.12ms
step:488/1670 train_time:44954ms step_avg:92.12ms
step:489/1670 train_time:45047ms step_avg:92.12ms
step:490/1670 train_time:45138ms step_avg:92.12ms
step:491/1670 train_time:45230ms step_avg:92.12ms
step:492/1670 train_time:45322ms step_avg:92.12ms
step:493/1670 train_time:45412ms step_avg:92.11ms
step:494/1670 train_time:45504ms step_avg:92.11ms
step:495/1670 train_time:45595ms step_avg:92.11ms
step:496/1670 train_time:45686ms step_avg:92.11ms
step:497/1670 train_time:45777ms step_avg:92.11ms
step:498/1670 train_time:45868ms step_avg:92.11ms
step:499/1670 train_time:45960ms step_avg:92.10ms
step:500/1670 train_time:46052ms step_avg:92.10ms
step:500/1670 val_loss:3.7143 train_time:46144ms step_avg:92.29ms
step:501/1670 train_time:46164ms step_avg:92.14ms
step:502/1670 train_time:46236ms step_avg:92.10ms
step:503/1670 train_time:46329ms step_avg:92.10ms
step:504/1670 train_time:46420ms step_avg:92.10ms
step:505/1670 train_time:46509ms step_avg:92.10ms
step:506/1670 train_time:46600ms step_avg:92.09ms
step:507/1670 train_time:46690ms step_avg:92.09ms
step:508/1670 train_time:46783ms step_avg:92.09ms
step:509/1670 train_time:46874ms step_avg:92.09ms
step:510/1670 train_time:46964ms step_avg:92.09ms
step:511/1670 train_time:47055ms step_avg:92.08ms
step:512/1670 train_time:47147ms step_avg:92.08ms
step:513/1670 train_time:47239ms step_avg:92.08ms
step:514/1670 train_time:47331ms step_avg:92.08ms
step:515/1670 train_time:47423ms step_avg:92.08ms
step:516/1670 train_time:47514ms step_avg:92.08ms
step:517/1670 train_time:47605ms step_avg:92.08ms
step:518/1670 train_time:47698ms step_avg:92.08ms
step:519/1670 train_time:47789ms step_avg:92.08ms
step:520/1670 train_time:47880ms step_avg:92.08ms
step:521/1670 train_time:47970ms step_avg:92.07ms
step:522/1670 train_time:48061ms step_avg:92.07ms
step:523/1670 train_time:48152ms step_avg:92.07ms
step:524/1670 train_time:48244ms step_avg:92.07ms
step:525/1670 train_time:48335ms step_avg:92.07ms
step:526/1670 train_time:48427ms step_avg:92.07ms
step:527/1670 train_time:48520ms step_avg:92.07ms
step:528/1670 train_time:48610ms step_avg:92.06ms
step:529/1670 train_time:48701ms step_avg:92.06ms
step:530/1670 train_time:48791ms step_avg:92.06ms
step:531/1670 train_time:48882ms step_avg:92.06ms
step:532/1670 train_time:48974ms step_avg:92.06ms
step:533/1670 train_time:49065ms step_avg:92.05ms
step:534/1670 train_time:49157ms step_avg:92.05ms
step:535/1670 train_time:49248ms step_avg:92.05ms
step:536/1670 train_time:49340ms step_avg:92.05ms
step:537/1670 train_time:49430ms step_avg:92.05ms
step:538/1670 train_time:49523ms step_avg:92.05ms
step:539/1670 train_time:49614ms step_avg:92.05ms
step:540/1670 train_time:49707ms step_avg:92.05ms
step:541/1670 train_time:49797ms step_avg:92.05ms
step:542/1670 train_time:49888ms step_avg:92.04ms
step:543/1670 train_time:49979ms step_avg:92.04ms
step:544/1670 train_time:50070ms step_avg:92.04ms
step:545/1670 train_time:50161ms step_avg:92.04ms
step:546/1670 train_time:50252ms step_avg:92.04ms
step:547/1670 train_time:50344ms step_avg:92.04ms
step:548/1670 train_time:50434ms step_avg:92.03ms
step:549/1670 train_time:50527ms step_avg:92.04ms
step:550/1670 train_time:50619ms step_avg:92.04ms
step:551/1670 train_time:50710ms step_avg:92.03ms
step:552/1670 train_time:50801ms step_avg:92.03ms
step:553/1670 train_time:50892ms step_avg:92.03ms
step:554/1670 train_time:50983ms step_avg:92.03ms
step:555/1670 train_time:51074ms step_avg:92.02ms
step:556/1670 train_time:51165ms step_avg:92.02ms
step:557/1670 train_time:51257ms step_avg:92.02ms
step:558/1670 train_time:51539ms step_avg:92.36ms
step:559/1670 train_time:51615ms step_avg:92.33ms
step:560/1670 train_time:51706ms step_avg:92.33ms
step:561/1670 train_time:51797ms step_avg:92.33ms
step:562/1670 train_time:51888ms step_avg:92.33ms
step:563/1670 train_time:51980ms step_avg:92.33ms
step:564/1670 train_time:52071ms step_avg:92.32ms
step:565/1670 train_time:52162ms step_avg:92.32ms
step:566/1670 train_time:52253ms step_avg:92.32ms
step:567/1670 train_time:52345ms step_avg:92.32ms
step:568/1670 train_time:52440ms step_avg:92.32ms
step:569/1670 train_time:52538ms step_avg:92.33ms
step:570/1670 train_time:52630ms step_avg:92.33ms
step:571/1670 train_time:52723ms step_avg:92.33ms
step:572/1670 train_time:52815ms step_avg:92.33ms
step:573/1670 train_time:52907ms step_avg:92.33ms
step:574/1670 train_time:52998ms step_avg:92.33ms
step:575/1670 train_time:53089ms step_avg:92.33ms
step:576/1670 train_time:53180ms step_avg:92.33ms
step:577/1670 train_time:53272ms step_avg:92.33ms
step:578/1670 train_time:53364ms step_avg:92.32ms
step:579/1670 train_time:53458ms step_avg:92.33ms
step:580/1670 train_time:53552ms step_avg:92.33ms
step:581/1670 train_time:53645ms step_avg:92.33ms
step:582/1670 train_time:53739ms step_avg:92.33ms
step:583/1670 train_time:53830ms step_avg:92.33ms
step:584/1670 train_time:53923ms step_avg:92.33ms
step:585/1670 train_time:54015ms step_avg:92.33ms
step:586/1670 train_time:54107ms step_avg:92.33ms
step:587/1670 train_time:54199ms step_avg:92.33ms
step:588/1670 train_time:54290ms step_avg:92.33ms
step:589/1670 train_time:54383ms step_avg:92.33ms
step:590/1670 train_time:54476ms step_avg:92.33ms
step:591/1670 train_time:54569ms step_avg:92.33ms
step:592/1670 train_time:54662ms step_avg:92.33ms
step:593/1670 train_time:54755ms step_avg:92.34ms
step:594/1670 train_time:54847ms step_avg:92.34ms
step:595/1670 train_time:54940ms step_avg:92.34ms
step:596/1670 train_time:55031ms step_avg:92.33ms
step:597/1670 train_time:55125ms step_avg:92.34ms
step:598/1670 train_time:55217ms step_avg:92.34ms
step:599/1670 train_time:55308ms step_avg:92.33ms
step:600/1670 train_time:55401ms step_avg:92.34ms
step:601/1670 train_time:55494ms step_avg:92.34ms
step:602/1670 train_time:55587ms step_avg:92.34ms
step:603/1670 train_time:55680ms step_avg:92.34ms
step:604/1670 train_time:55772ms step_avg:92.34ms
step:605/1670 train_time:55864ms step_avg:92.34ms
step:606/1670 train_time:55957ms step_avg:92.34ms
step:607/1670 train_time:56049ms step_avg:92.34ms
step:608/1670 train_time:56141ms step_avg:92.34ms
step:609/1670 train_time:56233ms step_avg:92.34ms
step:610/1670 train_time:56325ms step_avg:92.34ms
step:611/1670 train_time:56417ms step_avg:92.34ms
step:612/1670 train_time:56509ms step_avg:92.33ms
step:613/1670 train_time:56602ms step_avg:92.34ms
step:614/1670 train_time:56694ms step_avg:92.34ms
step:615/1670 train_time:56787ms step_avg:92.34ms
step:616/1670 train_time:56879ms step_avg:92.34ms
step:617/1670 train_time:56971ms step_avg:92.34ms
step:618/1670 train_time:57063ms step_avg:92.33ms
step:619/1670 train_time:57156ms step_avg:92.34ms
step:620/1670 train_time:57248ms step_avg:92.34ms
step:621/1670 train_time:57340ms step_avg:92.34ms
step:622/1670 train_time:57432ms step_avg:92.33ms
step:623/1670 train_time:57525ms step_avg:92.34ms
step:624/1670 train_time:57618ms step_avg:92.34ms
step:625/1670 train_time:57710ms step_avg:92.34ms
step:625/1670 val_loss:3.6125 train_time:57804ms step_avg:92.49ms
step:626/1670 train_time:57824ms step_avg:92.37ms
step:627/1670 train_time:57901ms step_avg:92.35ms
step:628/1670 train_time:58004ms step_avg:92.36ms
step:629/1670 train_time:58098ms step_avg:92.37ms
step:630/1670 train_time:58190ms step_avg:92.36ms
step:631/1670 train_time:58281ms step_avg:92.36ms
step:632/1670 train_time:58372ms step_avg:92.36ms
step:633/1670 train_time:58463ms step_avg:92.36ms
step:634/1670 train_time:58554ms step_avg:92.36ms
step:635/1670 train_time:58646ms step_avg:92.36ms
step:636/1670 train_time:58738ms step_avg:92.35ms
step:637/1670 train_time:58829ms step_avg:92.35ms
step:638/1670 train_time:58925ms step_avg:92.36ms
step:639/1670 train_time:59161ms step_avg:92.58ms
step:640/1670 train_time:59233ms step_avg:92.55ms
step:641/1670 train_time:59324ms step_avg:92.55ms
step:642/1670 train_time:59415ms step_avg:92.55ms
step:643/1670 train_time:59506ms step_avg:92.54ms
step:644/1670 train_time:59597ms step_avg:92.54ms
step:645/1670 train_time:59688ms step_avg:92.54ms
step:646/1670 train_time:59780ms step_avg:92.54ms
step:647/1670 train_time:59872ms step_avg:92.54ms
step:648/1670 train_time:59963ms step_avg:92.54ms
step:649/1670 train_time:60059ms step_avg:92.54ms
step:650/1670 train_time:60156ms step_avg:92.55ms
step:651/1670 train_time:60249ms step_avg:92.55ms
step:652/1670 train_time:60342ms step_avg:92.55ms
step:653/1670 train_time:60434ms step_avg:92.55ms
step:654/1670 train_time:60525ms step_avg:92.55ms
step:655/1670 train_time:60617ms step_avg:92.55ms
step:656/1670 train_time:60708ms step_avg:92.54ms
step:657/1670 train_time:60800ms step_avg:92.54ms
step:658/1670 train_time:60891ms step_avg:92.54ms
step:659/1670 train_time:60985ms step_avg:92.54ms
step:660/1670 train_time:61081ms step_avg:92.55ms
step:661/1670 train_time:61175ms step_avg:92.55ms
step:662/1670 train_time:61268ms step_avg:92.55ms
step:663/1670 train_time:61362ms step_avg:92.55ms
step:664/1670 train_time:61454ms step_avg:92.55ms
step:665/1670 train_time:61545ms step_avg:92.55ms
step:666/1670 train_time:61637ms step_avg:92.55ms
step:667/1670 train_time:61726ms step_avg:92.54ms
step:668/1670 train_time:61817ms step_avg:92.54ms
step:669/1670 train_time:61909ms step_avg:92.54ms
step:670/1670 train_time:62002ms step_avg:92.54ms
step:671/1670 train_time:62096ms step_avg:92.54ms
step:672/1670 train_time:62188ms step_avg:92.54ms
step:673/1670 train_time:62282ms step_avg:92.54ms
step:674/1670 train_time:62375ms step_avg:92.55ms
step:675/1670 train_time:62467ms step_avg:92.54ms
step:676/1670 train_time:62559ms step_avg:92.54ms
step:677/1670 train_time:62652ms step_avg:92.54ms
step:678/1670 train_time:62744ms step_avg:92.54ms
step:679/1670 train_time:62835ms step_avg:92.54ms
step:680/1670 train_time:62928ms step_avg:92.54ms
step:681/1670 train_time:63021ms step_avg:92.54ms
step:682/1670 train_time:63114ms step_avg:92.54ms
step:683/1670 train_time:63206ms step_avg:92.54ms
step:684/1670 train_time:63302ms step_avg:92.55ms
step:685/1670 train_time:63394ms step_avg:92.55ms
step:686/1670 train_time:63486ms step_avg:92.55ms
step:687/1670 train_time:63579ms step_avg:92.55ms
step:688/1670 train_time:63671ms step_avg:92.55ms
step:689/1670 train_time:63764ms step_avg:92.55ms
step:690/1670 train_time:63856ms step_avg:92.55ms
step:691/1670 train_time:63948ms step_avg:92.54ms
step:692/1670 train_time:64041ms step_avg:92.55ms
step:693/1670 train_time:64134ms step_avg:92.55ms
step:694/1670 train_time:64227ms step_avg:92.55ms
step:695/1670 train_time:64321ms step_avg:92.55ms
step:696/1670 train_time:64414ms step_avg:92.55ms
step:697/1670 train_time:64506ms step_avg:92.55ms
step:698/1670 train_time:64598ms step_avg:92.55ms
step:699/1670 train_time:64689ms step_avg:92.55ms
step:700/1670 train_time:64782ms step_avg:92.55ms
step:701/1670 train_time:64875ms step_avg:92.55ms
step:702/1670 train_time:64966ms step_avg:92.54ms
step:703/1670 train_time:65060ms step_avg:92.55ms
step:704/1670 train_time:65152ms step_avg:92.55ms
step:705/1670 train_time:65245ms step_avg:92.55ms
step:706/1670 train_time:65337ms step_avg:92.55ms
step:707/1670 train_time:65430ms step_avg:92.55ms
step:708/1670 train_time:65523ms step_avg:92.55ms
step:709/1670 train_time:65617ms step_avg:92.55ms
step:710/1670 train_time:65708ms step_avg:92.55ms
step:711/1670 train_time:65802ms step_avg:92.55ms
step:712/1670 train_time:65895ms step_avg:92.55ms
step:713/1670 train_time:65987ms step_avg:92.55ms
step:714/1670 train_time:66080ms step_avg:92.55ms
step:715/1670 train_time:66172ms step_avg:92.55ms
step:716/1670 train_time:66265ms step_avg:92.55ms
step:717/1670 train_time:66357ms step_avg:92.55ms
step:718/1670 train_time:66450ms step_avg:92.55ms
step:719/1670 train_time:66543ms step_avg:92.55ms
step:720/1670 train_time:66635ms step_avg:92.55ms
step:721/1670 train_time:66727ms step_avg:92.55ms
step:722/1670 train_time:66821ms step_avg:92.55ms
step:723/1670 train_time:66913ms step_avg:92.55ms
step:724/1670 train_time:67005ms step_avg:92.55ms
step:725/1670 train_time:67097ms step_avg:92.55ms
step:726/1670 train_time:67189ms step_avg:92.55ms
step:727/1670 train_time:67283ms step_avg:92.55ms
step:728/1670 train_time:67375ms step_avg:92.55ms
step:729/1670 train_time:67467ms step_avg:92.55ms
step:730/1670 train_time:67561ms step_avg:92.55ms
step:731/1670 train_time:67653ms step_avg:92.55ms
step:732/1670 train_time:67746ms step_avg:92.55ms
step:733/1670 train_time:67838ms step_avg:92.55ms
step:734/1670 train_time:67931ms step_avg:92.55ms
step:735/1670 train_time:68024ms step_avg:92.55ms
step:736/1670 train_time:68117ms step_avg:92.55ms
step:737/1670 train_time:68209ms step_avg:92.55ms
step:738/1670 train_time:68302ms step_avg:92.55ms
step:739/1670 train_time:68394ms step_avg:92.55ms
step:740/1670 train_time:68486ms step_avg:92.55ms
step:741/1670 train_time:68580ms step_avg:92.55ms
step:742/1670 train_time:68672ms step_avg:92.55ms
step:743/1670 train_time:68764ms step_avg:92.55ms
step:744/1670 train_time:68857ms step_avg:92.55ms
step:745/1670 train_time:68949ms step_avg:92.55ms
step:746/1670 train_time:69042ms step_avg:92.55ms
step:747/1670 train_time:69135ms step_avg:92.55ms
step:748/1670 train_time:69227ms step_avg:92.55ms
step:749/1670 train_time:69320ms step_avg:92.55ms
step:750/1670 train_time:69412ms step_avg:92.55ms
step:750/1670 val_loss:3.5629 train_time:69505ms step_avg:92.67ms
step:751/1670 train_time:69525ms step_avg:92.58ms
step:752/1670 train_time:69600ms step_avg:92.55ms
step:753/1670 train_time:69693ms step_avg:92.55ms
step:754/1670 train_time:69784ms step_avg:92.55ms
step:755/1670 train_time:69875ms step_avg:92.55ms
step:756/1670 train_time:69966ms step_avg:92.55ms
step:757/1670 train_time:70058ms step_avg:92.55ms
step:758/1670 train_time:70151ms step_avg:92.55ms
step:759/1670 train_time:70243ms step_avg:92.55ms
step:760/1670 train_time:70336ms step_avg:92.55ms
step:761/1670 train_time:70428ms step_avg:92.55ms
step:762/1670 train_time:70524ms step_avg:92.55ms
step:763/1670 train_time:70617ms step_avg:92.55ms
step:764/1670 train_time:70710ms step_avg:92.55ms
step:765/1670 train_time:70803ms step_avg:92.55ms
step:766/1670 train_time:70896ms step_avg:92.55ms
step:767/1670 train_time:70987ms step_avg:92.55ms
step:768/1670 train_time:71079ms step_avg:92.55ms
step:769/1670 train_time:71171ms step_avg:92.55ms
step:770/1670 train_time:71263ms step_avg:92.55ms
step:771/1670 train_time:71356ms step_avg:92.55ms
step:772/1670 train_time:71448ms step_avg:92.55ms
step:773/1670 train_time:71541ms step_avg:92.55ms
step:774/1670 train_time:71634ms step_avg:92.55ms
step:775/1670 train_time:71726ms step_avg:92.55ms
step:776/1670 train_time:71821ms step_avg:92.55ms
step:777/1670 train_time:71913ms step_avg:92.55ms
step:778/1670 train_time:72005ms step_avg:92.55ms
step:779/1670 train_time:72098ms step_avg:92.55ms
step:780/1670 train_time:72189ms step_avg:92.55ms
step:781/1670 train_time:72282ms step_avg:92.55ms
step:782/1670 train_time:72375ms step_avg:92.55ms
step:783/1670 train_time:72468ms step_avg:92.55ms
step:784/1670 train_time:72561ms step_avg:92.55ms
step:785/1670 train_time:72654ms step_avg:92.55ms
step:786/1670 train_time:72746ms step_avg:92.55ms
step:787/1670 train_time:72841ms step_avg:92.56ms
step:788/1670 train_time:72934ms step_avg:92.56ms
step:789/1670 train_time:73025ms step_avg:92.55ms
step:790/1670 train_time:73118ms step_avg:92.55ms
step:791/1670 train_time:73211ms step_avg:92.56ms
step:792/1670 train_time:73304ms step_avg:92.56ms
step:793/1670 train_time:73396ms step_avg:92.55ms
step:794/1670 train_time:73489ms step_avg:92.55ms
step:795/1670 train_time:73581ms step_avg:92.55ms
step:796/1670 train_time:73674ms step_avg:92.55ms
step:797/1670 train_time:73766ms step_avg:92.55ms
step:798/1670 train_time:73858ms step_avg:92.55ms
step:799/1670 train_time:73952ms step_avg:92.56ms
step:800/1670 train_time:74044ms step_avg:92.55ms
step:801/1670 train_time:74136ms step_avg:92.55ms
step:802/1670 train_time:74228ms step_avg:92.55ms
step:803/1670 train_time:74322ms step_avg:92.56ms
step:804/1670 train_time:74415ms step_avg:92.56ms
step:805/1670 train_time:74507ms step_avg:92.56ms
step:806/1670 train_time:74600ms step_avg:92.56ms
step:807/1670 train_time:74693ms step_avg:92.56ms
step:808/1670 train_time:74786ms step_avg:92.56ms
step:809/1670 train_time:74878ms step_avg:92.56ms
step:810/1670 train_time:74971ms step_avg:92.56ms
step:811/1670 train_time:75063ms step_avg:92.56ms
step:812/1670 train_time:75155ms step_avg:92.56ms
step:813/1670 train_time:75247ms step_avg:92.55ms
step:814/1670 train_time:75340ms step_avg:92.56ms
step:815/1670 train_time:75432ms step_avg:92.55ms
step:816/1670 train_time:75524ms step_avg:92.55ms
step:817/1670 train_time:75617ms step_avg:92.55ms
step:818/1670 train_time:75710ms step_avg:92.55ms
step:819/1670 train_time:75803ms step_avg:92.56ms
step:820/1670 train_time:75896ms step_avg:92.56ms
step:821/1670 train_time:75988ms step_avg:92.55ms
step:822/1670 train_time:76080ms step_avg:92.56ms
step:823/1670 train_time:76173ms step_avg:92.56ms
step:824/1670 train_time:76265ms step_avg:92.55ms
step:825/1670 train_time:76357ms step_avg:92.55ms
step:826/1670 train_time:76449ms step_avg:92.55ms
step:827/1670 train_time:76542ms step_avg:92.55ms
step:828/1670 train_time:76633ms step_avg:92.55ms
step:829/1670 train_time:76726ms step_avg:92.55ms
step:830/1670 train_time:76819ms step_avg:92.55ms
step:831/1670 train_time:76911ms step_avg:92.55ms
step:832/1670 train_time:77003ms step_avg:92.55ms
step:833/1670 train_time:77096ms step_avg:92.55ms
step:834/1670 train_time:77189ms step_avg:92.55ms
step:835/1670 train_time:77280ms step_avg:92.55ms
step:836/1670 train_time:77373ms step_avg:92.55ms
step:837/1670 train_time:77465ms step_avg:92.55ms
step:838/1670 train_time:77559ms step_avg:92.55ms
step:839/1670 train_time:77652ms step_avg:92.55ms
step:840/1670 train_time:77744ms step_avg:92.55ms
step:841/1670 train_time:77837ms step_avg:92.55ms
step:842/1670 train_time:77929ms step_avg:92.55ms
step:843/1670 train_time:78023ms step_avg:92.55ms
step:844/1670 train_time:78116ms step_avg:92.55ms
step:845/1670 train_time:78207ms step_avg:92.55ms
step:846/1670 train_time:78300ms step_avg:92.55ms
step:847/1670 train_time:78393ms step_avg:92.55ms
step:848/1670 train_time:78486ms step_avg:92.55ms
step:849/1670 train_time:78579ms step_avg:92.56ms
step:850/1670 train_time:78672ms step_avg:92.56ms
step:851/1670 train_time:78921ms step_avg:92.74ms
step:852/1670 train_time:78992ms step_avg:92.71ms
step:853/1670 train_time:79082ms step_avg:92.71ms
step:854/1670 train_time:79173ms step_avg:92.71ms
step:855/1670 train_time:79263ms step_avg:92.71ms
step:856/1670 train_time:79354ms step_avg:92.70ms
step:857/1670 train_time:79445ms step_avg:92.70ms
step:858/1670 train_time:79537ms step_avg:92.70ms
step:859/1670 train_time:79628ms step_avg:92.70ms
step:860/1670 train_time:79720ms step_avg:92.70ms
step:861/1670 train_time:79815ms step_avg:92.70ms
step:862/1670 train_time:79912ms step_avg:92.71ms
step:863/1670 train_time:80007ms step_avg:92.71ms
step:864/1670 train_time:80100ms step_avg:92.71ms
step:865/1670 train_time:80192ms step_avg:92.71ms
step:866/1670 train_time:80284ms step_avg:92.71ms
step:867/1670 train_time:80375ms step_avg:92.71ms
step:868/1670 train_time:80466ms step_avg:92.70ms
step:869/1670 train_time:80558ms step_avg:92.70ms
step:870/1670 train_time:80649ms step_avg:92.70ms
step:871/1670 train_time:80742ms step_avg:92.70ms
step:872/1670 train_time:80837ms step_avg:92.70ms
step:873/1670 train_time:80932ms step_avg:92.71ms
step:874/1670 train_time:81026ms step_avg:92.71ms
step:875/1670 train_time:81119ms step_avg:92.71ms
step:875/1670 val_loss:3.5185 train_time:81211ms step_avg:92.81ms
step:876/1670 train_time:81231ms step_avg:92.73ms
step:877/1670 train_time:81306ms step_avg:92.71ms
step:878/1670 train_time:81399ms step_avg:92.71ms
step:879/1670 train_time:81490ms step_avg:92.71ms
step:880/1670 train_time:81582ms step_avg:92.71ms
step:881/1670 train_time:81673ms step_avg:92.71ms
step:882/1670 train_time:81764ms step_avg:92.70ms
step:883/1670 train_time:81857ms step_avg:92.70ms
step:884/1670 train_time:81949ms step_avg:92.70ms
step:885/1670 train_time:82042ms step_avg:92.70ms
step:886/1670 train_time:82135ms step_avg:92.70ms
step:887/1670 train_time:82230ms step_avg:92.71ms
step:888/1670 train_time:82323ms step_avg:92.71ms
step:889/1670 train_time:82416ms step_avg:92.71ms
step:890/1670 train_time:82508ms step_avg:92.71ms
step:891/1670 train_time:82601ms step_avg:92.71ms
step:892/1670 train_time:82693ms step_avg:92.71ms
step:893/1670 train_time:82784ms step_avg:92.70ms
step:894/1670 train_time:82877ms step_avg:92.70ms
step:895/1670 train_time:82969ms step_avg:92.70ms
step:896/1670 train_time:83063ms step_avg:92.70ms
step:897/1670 train_time:83157ms step_avg:92.71ms
step:898/1670 train_time:83250ms step_avg:92.71ms
step:899/1670 train_time:83342ms step_avg:92.71ms
step:900/1670 train_time:83435ms step_avg:92.71ms
step:901/1670 train_time:83528ms step_avg:92.71ms
step:902/1670 train_time:83620ms step_avg:92.70ms
step:903/1670 train_time:83712ms step_avg:92.70ms
step:904/1670 train_time:83805ms step_avg:92.70ms
step:905/1670 train_time:83897ms step_avg:92.70ms
step:906/1670 train_time:83989ms step_avg:92.70ms
step:907/1670 train_time:84084ms step_avg:92.71ms
step:908/1670 train_time:84177ms step_avg:92.71ms
step:909/1670 train_time:84269ms step_avg:92.71ms
step:910/1670 train_time:84363ms step_avg:92.71ms
step:911/1670 train_time:84456ms step_avg:92.71ms
step:912/1670 train_time:84548ms step_avg:92.71ms
step:913/1670 train_time:84640ms step_avg:92.71ms
step:914/1670 train_time:84733ms step_avg:92.71ms
step:915/1670 train_time:84825ms step_avg:92.70ms
step:916/1670 train_time:84916ms step_avg:92.70ms
step:917/1670 train_time:85009ms step_avg:92.70ms
step:918/1670 train_time:85102ms step_avg:92.70ms
step:919/1670 train_time:85194ms step_avg:92.70ms
step:920/1670 train_time:85287ms step_avg:92.70ms
step:921/1670 train_time:85379ms step_avg:92.70ms
step:922/1670 train_time:85471ms step_avg:92.70ms
step:923/1670 train_time:85563ms step_avg:92.70ms
step:924/1670 train_time:85656ms step_avg:92.70ms
step:925/1670 train_time:85748ms step_avg:92.70ms
step:926/1670 train_time:85842ms step_avg:92.70ms
step:927/1670 train_time:85934ms step_avg:92.70ms
step:928/1670 train_time:86026ms step_avg:92.70ms
step:929/1670 train_time:86119ms step_avg:92.70ms
step:930/1670 train_time:86212ms step_avg:92.70ms
step:931/1670 train_time:86304ms step_avg:92.70ms
step:932/1670 train_time:86397ms step_avg:92.70ms
step:933/1670 train_time:86489ms step_avg:92.70ms
step:934/1670 train_time:86582ms step_avg:92.70ms
step:935/1670 train_time:86675ms step_avg:92.70ms
step:936/1670 train_time:86767ms step_avg:92.70ms
step:937/1670 train_time:86860ms step_avg:92.70ms
step:938/1670 train_time:86952ms step_avg:92.70ms
step:939/1670 train_time:87045ms step_avg:92.70ms
step:940/1670 train_time:87138ms step_avg:92.70ms
step:941/1670 train_time:87230ms step_avg:92.70ms
step:942/1670 train_time:87323ms step_avg:92.70ms
step:943/1670 train_time:87416ms step_avg:92.70ms
step:944/1670 train_time:87507ms step_avg:92.70ms
step:945/1670 train_time:87600ms step_avg:92.70ms
step:946/1670 train_time:87692ms step_avg:92.70ms
step:947/1670 train_time:87784ms step_avg:92.70ms
step:948/1670 train_time:87878ms step_avg:92.70ms
step:949/1670 train_time:87970ms step_avg:92.70ms
step:950/1670 train_time:88063ms step_avg:92.70ms
step:951/1670 train_time:88156ms step_avg:92.70ms
step:952/1670 train_time:88248ms step_avg:92.70ms
step:953/1670 train_time:88340ms step_avg:92.70ms
step:954/1670 train_time:88433ms step_avg:92.70ms
step:955/1670 train_time:88525ms step_avg:92.70ms
step:956/1670 train_time:88617ms step_avg:92.70ms
step:957/1670 train_time:88709ms step_avg:92.69ms
step:958/1670 train_time:88802ms step_avg:92.69ms
step:959/1670 train_time:88894ms step_avg:92.69ms
step:960/1670 train_time:88986ms step_avg:92.69ms
step:961/1670 train_time:89080ms step_avg:92.69ms
step:962/1670 train_time:89172ms step_avg:92.69ms
step:963/1670 train_time:89265ms step_avg:92.69ms
step:964/1670 train_time:89358ms step_avg:92.70ms
step:965/1670 train_time:89450ms step_avg:92.69ms
step:966/1670 train_time:89543ms step_avg:92.69ms
step:967/1670 train_time:89635ms step_avg:92.69ms
step:968/1670 train_time:89727ms step_avg:92.69ms
step:969/1670 train_time:89819ms step_avg:92.69ms
step:970/1670 train_time:89912ms step_avg:92.69ms
step:971/1670 train_time:90004ms step_avg:92.69ms
step:972/1670 train_time:90096ms step_avg:92.69ms
step:973/1670 train_time:90188ms step_avg:92.69ms
step:974/1670 train_time:90282ms step_avg:92.69ms
step:975/1670 train_time:90375ms step_avg:92.69ms
step:976/1670 train_time:90467ms step_avg:92.69ms
step:977/1670 train_time:90559ms step_avg:92.69ms
step:978/1670 train_time:90651ms step_avg:92.69ms
step:979/1670 train_time:90743ms step_avg:92.69ms
step:980/1670 train_time:90836ms step_avg:92.69ms
step:981/1670 train_time:90928ms step_avg:92.69ms
step:982/1670 train_time:91020ms step_avg:92.69ms
step:983/1670 train_time:91114ms step_avg:92.69ms
step:984/1670 train_time:91206ms step_avg:92.69ms
step:985/1670 train_time:91298ms step_avg:92.69ms
step:986/1670 train_time:91389ms step_avg:92.69ms
step:987/1670 train_time:91483ms step_avg:92.69ms
step:988/1670 train_time:91576ms step_avg:92.69ms
step:989/1670 train_time:91667ms step_avg:92.69ms
step:990/1670 train_time:91759ms step_avg:92.69ms
step:991/1670 train_time:91852ms step_avg:92.69ms
step:992/1670 train_time:91944ms step_avg:92.69ms
step:993/1670 train_time:92037ms step_avg:92.69ms
step:994/1670 train_time:92129ms step_avg:92.69ms
step:995/1670 train_time:92222ms step_avg:92.69ms
step:996/1670 train_time:92314ms step_avg:92.68ms
step:997/1670 train_time:92406ms step_avg:92.68ms
step:998/1670 train_time:92499ms step_avg:92.68ms
step:999/1670 train_time:92591ms step_avg:92.68ms
step:1000/1670 train_time:92684ms step_avg:92.68ms
step:1000/1670 val_loss:3.4675 train_time:92777ms step_avg:92.78ms
step:1001/1670 train_time:92797ms step_avg:92.70ms
step:1002/1670 train_time:92871ms step_avg:92.69ms
step:1003/1670 train_time:92963ms step_avg:92.69ms
step:1004/1670 train_time:93055ms step_avg:92.68ms
step:1005/1670 train_time:93146ms step_avg:92.68ms
step:1006/1670 train_time:93238ms step_avg:92.68ms
step:1007/1670 train_time:93330ms step_avg:92.68ms
step:1008/1670 train_time:93422ms step_avg:92.68ms
step:1009/1670 train_time:93514ms step_avg:92.68ms
step:1010/1670 train_time:93607ms step_avg:92.68ms
step:1011/1670 train_time:93701ms step_avg:92.68ms
step:1012/1670 train_time:93795ms step_avg:92.68ms
step:1013/1670 train_time:93889ms step_avg:92.68ms
step:1014/1670 train_time:93981ms step_avg:92.68ms
step:1015/1670 train_time:94075ms step_avg:92.68ms
step:1016/1670 train_time:94166ms step_avg:92.68ms
step:1017/1670 train_time:94258ms step_avg:92.68ms
step:1018/1670 train_time:94350ms step_avg:92.68ms
step:1019/1670 train_time:94442ms step_avg:92.68ms
step:1020/1670 train_time:94534ms step_avg:92.68ms
step:1021/1670 train_time:94625ms step_avg:92.68ms
step:1022/1670 train_time:94720ms step_avg:92.68ms
step:1023/1670 train_time:94814ms step_avg:92.68ms
step:1024/1670 train_time:94907ms step_avg:92.68ms
step:1025/1670 train_time:95001ms step_avg:92.68ms
step:1026/1670 train_time:95093ms step_avg:92.68ms
step:1027/1670 train_time:95185ms step_avg:92.68ms
step:1028/1670 train_time:95278ms step_avg:92.68ms
step:1029/1670 train_time:95370ms step_avg:92.68ms
step:1030/1670 train_time:95462ms step_avg:92.68ms
step:1031/1670 train_time:95554ms step_avg:92.68ms
step:1032/1670 train_time:95646ms step_avg:92.68ms
step:1033/1670 train_time:95739ms step_avg:92.68ms
step:1034/1670 train_time:95832ms step_avg:92.68ms
step:1035/1670 train_time:95925ms step_avg:92.68ms
step:1036/1670 train_time:96019ms step_avg:92.68ms
step:1037/1670 train_time:96112ms step_avg:92.68ms
step:1038/1670 train_time:96205ms step_avg:92.68ms
step:1039/1670 train_time:96297ms step_avg:92.68ms
step:1040/1670 train_time:96389ms step_avg:92.68ms
step:1041/1670 train_time:96481ms step_avg:92.68ms
step:1042/1670 train_time:96573ms step_avg:92.68ms
step:1043/1670 train_time:96665ms step_avg:92.68ms
step:1044/1670 train_time:96758ms step_avg:92.68ms
step:1045/1670 train_time:96850ms step_avg:92.68ms
step:1046/1670 train_time:96944ms step_avg:92.68ms
step:1047/1670 train_time:97037ms step_avg:92.68ms
step:1048/1670 train_time:97130ms step_avg:92.68ms
step:1049/1670 train_time:97223ms step_avg:92.68ms
step:1050/1670 train_time:97315ms step_avg:92.68ms
step:1051/1670 train_time:97407ms step_avg:92.68ms
step:1052/1670 train_time:97499ms step_avg:92.68ms
step:1053/1670 train_time:97591ms step_avg:92.68ms
step:1054/1670 train_time:97683ms step_avg:92.68ms
step:1055/1670 train_time:97776ms step_avg:92.68ms
step:1056/1670 train_time:97869ms step_avg:92.68ms
step:1057/1670 train_time:97962ms step_avg:92.68ms
step:1058/1670 train_time:98055ms step_avg:92.68ms
step:1059/1670 train_time:98148ms step_avg:92.68ms
step:1060/1670 train_time:98242ms step_avg:92.68ms
step:1061/1670 train_time:98336ms step_avg:92.68ms
step:1062/1670 train_time:98584ms step_avg:92.83ms
step:1063/1670 train_time:98653ms step_avg:92.81ms
step:1064/1670 train_time:98743ms step_avg:92.80ms
step:1065/1670 train_time:98835ms step_avg:92.80ms
step:1066/1670 train_time:98925ms step_avg:92.80ms
step:1067/1670 train_time:99017ms step_avg:92.80ms
step:1068/1670 train_time:99108ms step_avg:92.80ms
step:1069/1670 train_time:99199ms step_avg:92.80ms
step:1070/1670 train_time:99291ms step_avg:92.80ms
step:1071/1670 train_time:99382ms step_avg:92.79ms
step:1072/1670 train_time:99479ms step_avg:92.80ms
step:1073/1670 train_time:99575ms step_avg:92.80ms
step:1074/1670 train_time:99669ms step_avg:92.80ms
step:1075/1670 train_time:99762ms step_avg:92.80ms
step:1076/1670 train_time:99854ms step_avg:92.80ms
step:1077/1670 train_time:99944ms step_avg:92.80ms
step:1078/1670 train_time:100036ms step_avg:92.80ms
step:1079/1670 train_time:100127ms step_avg:92.80ms
step:1080/1670 train_time:100220ms step_avg:92.80ms
step:1081/1670 train_time:100311ms step_avg:92.79ms
step:1082/1670 train_time:100403ms step_avg:92.79ms
step:1083/1670 train_time:100500ms step_avg:92.80ms
step:1084/1670 train_time:100594ms step_avg:92.80ms
step:1085/1670 train_time:100687ms step_avg:92.80ms
step:1086/1670 train_time:100780ms step_avg:92.80ms
step:1087/1670 train_time:100873ms step_avg:92.80ms
step:1088/1670 train_time:100965ms step_avg:92.80ms
step:1089/1670 train_time:101056ms step_avg:92.80ms
step:1090/1670 train_time:101147ms step_avg:92.80ms
step:1091/1670 train_time:101239ms step_avg:92.79ms
step:1092/1670 train_time:101330ms step_avg:92.79ms
step:1093/1670 train_time:101424ms step_avg:92.79ms
step:1094/1670 train_time:101518ms step_avg:92.80ms
step:1095/1670 train_time:101611ms step_avg:92.80ms
step:1096/1670 train_time:101705ms step_avg:92.80ms
step:1097/1670 train_time:101798ms step_avg:92.80ms
step:1098/1670 train_time:101890ms step_avg:92.80ms
step:1099/1670 train_time:101982ms step_avg:92.80ms
step:1100/1670 train_time:102073ms step_avg:92.79ms
step:1101/1670 train_time:102165ms step_avg:92.79ms
step:1102/1670 train_time:102256ms step_avg:92.79ms
step:1103/1670 train_time:102348ms step_avg:92.79ms
step:1104/1670 train_time:102443ms step_avg:92.79ms
step:1105/1670 train_time:102537ms step_avg:92.79ms
step:1106/1670 train_time:102628ms step_avg:92.79ms
step:1107/1670 train_time:102723ms step_avg:92.79ms
step:1108/1670 train_time:102816ms step_avg:92.79ms
step:1109/1670 train_time:102909ms step_avg:92.79ms
step:1110/1670 train_time:103002ms step_avg:92.79ms
step:1111/1670 train_time:103095ms step_avg:92.79ms
step:1112/1670 train_time:103186ms step_avg:92.79ms
step:1113/1670 train_time:103277ms step_avg:92.79ms
step:1114/1670 train_time:103370ms step_avg:92.79ms
step:1115/1670 train_time:103652ms step_avg:92.96ms
step:1116/1670 train_time:103725ms step_avg:92.94ms
step:1117/1670 train_time:103816ms step_avg:92.94ms
step:1118/1670 train_time:103908ms step_avg:92.94ms
step:1119/1670 train_time:104000ms step_avg:92.94ms
step:1120/1670 train_time:104092ms step_avg:92.94ms
step:1121/1670 train_time:104183ms step_avg:92.94ms
step:1122/1670 train_time:104275ms step_avg:92.94ms
step:1123/1670 train_time:104367ms step_avg:92.94ms
step:1124/1670 train_time:104459ms step_avg:92.93ms
step:1125/1670 train_time:104555ms step_avg:92.94ms
step:1125/1670 val_loss:3.4152 train_time:104654ms step_avg:93.03ms
step:1126/1670 train_time:104673ms step_avg:92.96ms
step:1127/1670 train_time:104751ms step_avg:92.95ms
step:1128/1670 train_time:104852ms step_avg:92.95ms
step:1129/1670 train_time:104948ms step_avg:92.96ms
step:1130/1670 train_time:105041ms step_avg:92.96ms
step:1131/1670 train_time:105132ms step_avg:92.95ms
step:1132/1670 train_time:105224ms step_avg:92.95ms
step:1133/1670 train_time:105316ms step_avg:92.95ms
step:1134/1670 train_time:105408ms step_avg:92.95ms
step:1135/1670 train_time:105500ms step_avg:92.95ms
step:1136/1670 train_time:105592ms step_avg:92.95ms
step:1137/1670 train_time:105688ms step_avg:92.95ms
step:1138/1670 train_time:105784ms step_avg:92.96ms
step:1139/1670 train_time:105880ms step_avg:92.96ms
step:1140/1670 train_time:105973ms step_avg:92.96ms
step:1141/1670 train_time:106066ms step_avg:92.96ms
step:1142/1670 train_time:106159ms step_avg:92.96ms
step:1143/1670 train_time:106251ms step_avg:92.96ms
step:1144/1670 train_time:106343ms step_avg:92.96ms
step:1145/1670 train_time:106435ms step_avg:92.96ms
step:1146/1670 train_time:106528ms step_avg:92.96ms
step:1147/1670 train_time:106621ms step_avg:92.96ms
step:1148/1670 train_time:106715ms step_avg:92.96ms
step:1149/1670 train_time:106811ms step_avg:92.96ms
step:1150/1670 train_time:106906ms step_avg:92.96ms
step:1151/1670 train_time:107000ms step_avg:92.96ms
step:1152/1670 train_time:107092ms step_avg:92.96ms
step:1153/1670 train_time:107185ms step_avg:92.96ms
step:1154/1670 train_time:107277ms step_avg:92.96ms
step:1155/1670 train_time:107370ms step_avg:92.96ms
step:1156/1670 train_time:107462ms step_avg:92.96ms
step:1157/1670 train_time:107554ms step_avg:92.96ms
step:1158/1670 train_time:107647ms step_avg:92.96ms
step:1159/1670 train_time:107741ms step_avg:92.96ms
step:1160/1670 train_time:107836ms step_avg:92.96ms
step:1161/1670 train_time:107930ms step_avg:92.96ms
step:1162/1670 train_time:108025ms step_avg:92.96ms
step:1163/1670 train_time:108119ms step_avg:92.97ms
step:1164/1670 train_time:108211ms step_avg:92.96ms
step:1165/1670 train_time:108303ms step_avg:92.96ms
step:1166/1670 train_time:108395ms step_avg:92.96ms
step:1167/1670 train_time:108488ms step_avg:92.96ms
step:1168/1670 train_time:108582ms step_avg:92.96ms
step:1169/1670 train_time:108674ms step_avg:92.96ms
step:1170/1670 train_time:108768ms step_avg:92.96ms
step:1171/1670 train_time:108861ms step_avg:92.96ms
step:1172/1670 train_time:108954ms step_avg:92.96ms
step:1173/1670 train_time:109048ms step_avg:92.96ms
step:1174/1670 train_time:109142ms step_avg:92.97ms
step:1175/1670 train_time:109235ms step_avg:92.97ms
step:1176/1670 train_time:109329ms step_avg:92.97ms
step:1177/1670 train_time:109421ms step_avg:92.97ms
step:1178/1670 train_time:109513ms step_avg:92.97ms
step:1179/1670 train_time:109607ms step_avg:92.97ms
step:1180/1670 train_time:109701ms step_avg:92.97ms
step:1181/1670 train_time:109794ms step_avg:92.97ms
step:1182/1670 train_time:109888ms step_avg:92.97ms
step:1183/1670 train_time:109982ms step_avg:92.97ms
step:1184/1670 train_time:110074ms step_avg:92.97ms
step:1185/1670 train_time:110168ms step_avg:92.97ms
step:1186/1670 train_time:110261ms step_avg:92.97ms
step:1187/1670 train_time:110353ms step_avg:92.97ms
step:1188/1670 train_time:110446ms step_avg:92.97ms
step:1189/1670 train_time:110539ms step_avg:92.97ms
step:1190/1670 train_time:110632ms step_avg:92.97ms
step:1191/1670 train_time:110727ms step_avg:92.97ms
step:1192/1670 train_time:110819ms step_avg:92.97ms
step:1193/1670 train_time:110913ms step_avg:92.97ms
step:1194/1670 train_time:111008ms step_avg:92.97ms
step:1195/1670 train_time:111102ms step_avg:92.97ms
step:1196/1670 train_time:111194ms step_avg:92.97ms
step:1197/1670 train_time:111288ms step_avg:92.97ms
step:1198/1670 train_time:111381ms step_avg:92.97ms
step:1199/1670 train_time:111473ms step_avg:92.97ms
step:1200/1670 train_time:111567ms step_avg:92.97ms
step:1201/1670 train_time:111659ms step_avg:92.97ms
step:1202/1670 train_time:111752ms step_avg:92.97ms
step:1203/1670 train_time:111846ms step_avg:92.97ms
step:1204/1670 train_time:111939ms step_avg:92.97ms
step:1205/1670 train_time:112032ms step_avg:92.97ms
step:1206/1670 train_time:112126ms step_avg:92.97ms
step:1207/1670 train_time:112219ms step_avg:92.97ms
step:1208/1670 train_time:112312ms step_avg:92.97ms
step:1209/1670 train_time:112406ms step_avg:92.97ms
step:1210/1670 train_time:112500ms step_avg:92.98ms
step:1211/1670 train_time:112593ms step_avg:92.97ms
step:1212/1670 train_time:112686ms step_avg:92.98ms
step:1213/1670 train_time:112778ms step_avg:92.97ms
step:1214/1670 train_time:112871ms step_avg:92.97ms
step:1215/1670 train_time:112964ms step_avg:92.97ms
step:1216/1670 train_time:113057ms step_avg:92.97ms
step:1217/1670 train_time:113150ms step_avg:92.97ms
step:1218/1670 train_time:113243ms step_avg:92.97ms
step:1219/1670 train_time:113336ms step_avg:92.97ms
step:1220/1670 train_time:113429ms step_avg:92.97ms
step:1221/1670 train_time:113522ms step_avg:92.97ms
step:1222/1670 train_time:113614ms step_avg:92.97ms
step:1223/1670 train_time:113708ms step_avg:92.97ms
step:1224/1670 train_time:113801ms step_avg:92.97ms
step:1225/1670 train_time:113893ms step_avg:92.97ms
step:1226/1670 train_time:113988ms step_avg:92.98ms
step:1227/1670 train_time:114081ms step_avg:92.98ms
step:1228/1670 train_time:114174ms step_avg:92.98ms
step:1229/1670 train_time:114269ms step_avg:92.98ms
step:1230/1670 train_time:114361ms step_avg:92.98ms
step:1231/1670 train_time:114454ms step_avg:92.98ms
step:1232/1670 train_time:114548ms step_avg:92.98ms
step:1233/1670 train_time:114640ms step_avg:92.98ms
step:1234/1670 train_time:114733ms step_avg:92.98ms
step:1235/1670 train_time:114826ms step_avg:92.98ms
step:1236/1670 train_time:114919ms step_avg:92.98ms
step:1237/1670 train_time:115012ms step_avg:92.98ms
step:1238/1670 train_time:115106ms step_avg:92.98ms
step:1239/1670 train_time:115199ms step_avg:92.98ms
step:1240/1670 train_time:115292ms step_avg:92.98ms
step:1241/1670 train_time:115386ms step_avg:92.98ms
step:1242/1670 train_time:115479ms step_avg:92.98ms
step:1243/1670 train_time:115573ms step_avg:92.98ms
step:1244/1670 train_time:115668ms step_avg:92.98ms
step:1245/1670 train_time:115761ms step_avg:92.98ms
step:1246/1670 train_time:115853ms step_avg:92.98ms
step:1247/1670 train_time:115947ms step_avg:92.98ms
step:1248/1670 train_time:116040ms step_avg:92.98ms
step:1249/1670 train_time:116132ms step_avg:92.98ms
step:1250/1670 train_time:116225ms step_avg:92.98ms
step:1250/1670 val_loss:3.3763 train_time:116317ms step_avg:93.05ms
step:1251/1670 train_time:116337ms step_avg:93.00ms
step:1252/1670 train_time:116411ms step_avg:92.98ms
step:1253/1670 train_time:116503ms step_avg:92.98ms
step:1254/1670 train_time:116596ms step_avg:92.98ms
step:1255/1670 train_time:116690ms step_avg:92.98ms
step:1256/1670 train_time:116783ms step_avg:92.98ms
step:1257/1670 train_time:116875ms step_avg:92.98ms
step:1258/1670 train_time:116968ms step_avg:92.98ms
step:1259/1670 train_time:117060ms step_avg:92.98ms
step:1260/1670 train_time:117153ms step_avg:92.98ms
step:1261/1670 train_time:117247ms step_avg:92.98ms
step:1262/1670 train_time:117341ms step_avg:92.98ms
step:1263/1670 train_time:117435ms step_avg:92.98ms
step:1264/1670 train_time:117529ms step_avg:92.98ms
step:1265/1670 train_time:117621ms step_avg:92.98ms
step:1266/1670 train_time:117714ms step_avg:92.98ms
step:1267/1670 train_time:117809ms step_avg:92.98ms
step:1268/1670 train_time:117901ms step_avg:92.98ms
step:1269/1670 train_time:117993ms step_avg:92.98ms
step:1270/1670 train_time:118086ms step_avg:92.98ms
step:1271/1670 train_time:118179ms step_avg:92.98ms
step:1272/1670 train_time:118274ms step_avg:92.98ms
step:1273/1670 train_time:118367ms step_avg:92.98ms
step:1274/1670 train_time:118617ms step_avg:93.11ms
step:1275/1670 train_time:118686ms step_avg:93.09ms
step:1276/1670 train_time:118777ms step_avg:93.09ms
step:1277/1670 train_time:118870ms step_avg:93.09ms
step:1278/1670 train_time:118961ms step_avg:93.08ms
step:1279/1670 train_time:119052ms step_avg:93.08ms
step:1280/1670 train_time:119145ms step_avg:93.08ms
step:1281/1670 train_time:119237ms step_avg:93.08ms
step:1282/1670 train_time:119329ms step_avg:93.08ms
step:1283/1670 train_time:119420ms step_avg:93.08ms
step:1284/1670 train_time:119520ms step_avg:93.08ms
step:1285/1670 train_time:119619ms step_avg:93.09ms
step:1286/1670 train_time:119715ms step_avg:93.09ms
step:1287/1670 train_time:119807ms step_avg:93.09ms
step:1288/1670 train_time:119899ms step_avg:93.09ms
step:1289/1670 train_time:119991ms step_avg:93.09ms
step:1290/1670 train_time:120084ms step_avg:93.09ms
step:1291/1670 train_time:120176ms step_avg:93.09ms
step:1292/1670 train_time:120267ms step_avg:93.09ms
step:1293/1670 train_time:120360ms step_avg:93.09ms
step:1294/1670 train_time:120455ms step_avg:93.09ms
step:1295/1670 train_time:120551ms step_avg:93.09ms
step:1296/1670 train_time:120646ms step_avg:93.09ms
step:1297/1670 train_time:120740ms step_avg:93.09ms
step:1298/1670 train_time:120834ms step_avg:93.09ms
step:1299/1670 train_time:120927ms step_avg:93.09ms
step:1300/1670 train_time:121019ms step_avg:93.09ms
step:1301/1670 train_time:121111ms step_avg:93.09ms
step:1302/1670 train_time:121203ms step_avg:93.09ms
step:1303/1670 train_time:121295ms step_avg:93.09ms
step:1304/1670 train_time:121388ms step_avg:93.09ms
step:1305/1670 train_time:121481ms step_avg:93.09ms
step:1306/1670 train_time:121576ms step_avg:93.09ms
step:1307/1670 train_time:121670ms step_avg:93.09ms
step:1308/1670 train_time:121763ms step_avg:93.09ms
step:1309/1670 train_time:121857ms step_avg:93.09ms
step:1310/1670 train_time:121951ms step_avg:93.09ms
step:1311/1670 train_time:122043ms step_avg:93.09ms
step:1312/1670 train_time:122136ms step_avg:93.09ms
step:1313/1670 train_time:122228ms step_avg:93.09ms
step:1314/1670 train_time:122321ms step_avg:93.09ms
step:1315/1670 train_time:122415ms step_avg:93.09ms
step:1316/1670 train_time:122508ms step_avg:93.09ms
step:1317/1670 train_time:122602ms step_avg:93.09ms
step:1318/1670 train_time:122698ms step_avg:93.09ms
step:1319/1670 train_time:122792ms step_avg:93.09ms
step:1320/1670 train_time:122885ms step_avg:93.09ms
step:1321/1670 train_time:122978ms step_avg:93.09ms
step:1322/1670 train_time:123071ms step_avg:93.09ms
step:1323/1670 train_time:123164ms step_avg:93.09ms
step:1324/1670 train_time:123256ms step_avg:93.09ms
step:1325/1670 train_time:123349ms step_avg:93.09ms
step:1326/1670 train_time:123443ms step_avg:93.09ms
step:1327/1670 train_time:123537ms step_avg:93.09ms
step:1328/1670 train_time:123630ms step_avg:93.10ms
step:1329/1670 train_time:123724ms step_avg:93.10ms
step:1330/1670 train_time:123818ms step_avg:93.10ms
step:1331/1670 train_time:123913ms step_avg:93.10ms
step:1332/1670 train_time:124005ms step_avg:93.10ms
step:1333/1670 train_time:124097ms step_avg:93.10ms
step:1334/1670 train_time:124190ms step_avg:93.10ms
step:1335/1670 train_time:124283ms step_avg:93.10ms
step:1336/1670 train_time:124376ms step_avg:93.10ms
step:1337/1670 train_time:124469ms step_avg:93.10ms
step:1338/1670 train_time:124562ms step_avg:93.10ms
step:1339/1670 train_time:124657ms step_avg:93.10ms
step:1340/1670 train_time:124751ms step_avg:93.10ms
step:1341/1670 train_time:124843ms step_avg:93.10ms
step:1342/1670 train_time:124937ms step_avg:93.10ms
step:1343/1670 train_time:125031ms step_avg:93.10ms
step:1344/1670 train_time:125123ms step_avg:93.10ms
step:1345/1670 train_time:125217ms step_avg:93.10ms
step:1346/1670 train_time:125309ms step_avg:93.10ms
step:1347/1670 train_time:125402ms step_avg:93.10ms
step:1348/1670 train_time:125496ms step_avg:93.10ms
step:1349/1670 train_time:125589ms step_avg:93.10ms
step:1350/1670 train_time:125682ms step_avg:93.10ms
step:1351/1670 train_time:125777ms step_avg:93.10ms
step:1352/1670 train_time:125870ms step_avg:93.10ms
step:1353/1670 train_time:125963ms step_avg:93.10ms
step:1354/1670 train_time:126057ms step_avg:93.10ms
step:1355/1670 train_time:126151ms step_avg:93.10ms
step:1356/1670 train_time:126243ms step_avg:93.10ms
step:1357/1670 train_time:126337ms step_avg:93.10ms
step:1358/1670 train_time:126429ms step_avg:93.10ms
step:1359/1670 train_time:126522ms step_avg:93.10ms
step:1360/1670 train_time:126617ms step_avg:93.10ms
step:1361/1670 train_time:126709ms step_avg:93.10ms
step:1362/1670 train_time:126803ms step_avg:93.10ms
step:1363/1670 train_time:126896ms step_avg:93.10ms
step:1364/1670 train_time:126990ms step_avg:93.10ms
step:1365/1670 train_time:127082ms step_avg:93.10ms
step:1366/1670 train_time:127176ms step_avg:93.10ms
step:1367/1670 train_time:127269ms step_avg:93.10ms
step:1368/1670 train_time:127361ms step_avg:93.10ms
step:1369/1670 train_time:127455ms step_avg:93.10ms
step:1370/1670 train_time:127549ms step_avg:93.10ms
step:1371/1670 train_time:127642ms step_avg:93.10ms
step:1372/1670 train_time:127737ms step_avg:93.10ms
step:1373/1670 train_time:127830ms step_avg:93.10ms
step:1374/1670 train_time:127923ms step_avg:93.10ms
step:1375/1670 train_time:128017ms step_avg:93.10ms
step:1375/1670 val_loss:3.3410 train_time:128110ms step_avg:93.17ms
step:1376/1670 train_time:128130ms step_avg:93.12ms
step:1377/1670 train_time:128207ms step_avg:93.11ms
step:1378/1670 train_time:128300ms step_avg:93.11ms
step:1379/1670 train_time:128393ms step_avg:93.11ms
step:1380/1670 train_time:128485ms step_avg:93.10ms
step:1381/1670 train_time:128578ms step_avg:93.10ms
step:1382/1670 train_time:128670ms step_avg:93.10ms
step:1383/1670 train_time:128764ms step_avg:93.11ms
step:1384/1670 train_time:128858ms step_avg:93.11ms
step:1385/1670 train_time:128950ms step_avg:93.11ms
step:1386/1670 train_time:129045ms step_avg:93.11ms
step:1387/1670 train_time:129139ms step_avg:93.11ms
step:1388/1670 train_time:129234ms step_avg:93.11ms
step:1389/1670 train_time:129327ms step_avg:93.11ms
step:1390/1670 train_time:129419ms step_avg:93.11ms
step:1391/1670 train_time:129511ms step_avg:93.11ms
step:1392/1670 train_time:129605ms step_avg:93.11ms
step:1393/1670 train_time:129699ms step_avg:93.11ms
step:1394/1670 train_time:129791ms step_avg:93.11ms
step:1395/1670 train_time:129886ms step_avg:93.11ms
step:1396/1670 train_time:129979ms step_avg:93.11ms
step:1397/1670 train_time:130072ms step_avg:93.11ms
step:1398/1670 train_time:130166ms step_avg:93.11ms
step:1399/1670 train_time:130260ms step_avg:93.11ms
step:1400/1670 train_time:130352ms step_avg:93.11ms
step:1401/1670 train_time:130445ms step_avg:93.11ms
step:1402/1670 train_time:130538ms step_avg:93.11ms
step:1403/1670 train_time:130631ms step_avg:93.11ms
step:1404/1670 train_time:130723ms step_avg:93.11ms
step:1405/1670 train_time:130816ms step_avg:93.11ms
step:1406/1670 train_time:130910ms step_avg:93.11ms
step:1407/1670 train_time:131004ms step_avg:93.11ms
step:1408/1670 train_time:131097ms step_avg:93.11ms
step:1409/1670 train_time:131190ms step_avg:93.11ms
step:1410/1670 train_time:131285ms step_avg:93.11ms
step:1411/1670 train_time:131378ms step_avg:93.11ms
step:1412/1670 train_time:131471ms step_avg:93.11ms
step:1413/1670 train_time:131564ms step_avg:93.11ms
step:1414/1670 train_time:131658ms step_avg:93.11ms
step:1415/1670 train_time:131750ms step_avg:93.11ms
step:1416/1670 train_time:131843ms step_avg:93.11ms
step:1417/1670 train_time:131936ms step_avg:93.11ms
step:1418/1670 train_time:132030ms step_avg:93.11ms
step:1419/1670 train_time:132123ms step_avg:93.11ms
step:1420/1670 train_time:132216ms step_avg:93.11ms
step:1421/1670 train_time:132309ms step_avg:93.11ms
step:1422/1670 train_time:132403ms step_avg:93.11ms
step:1423/1670 train_time:132497ms step_avg:93.11ms
step:1424/1670 train_time:132589ms step_avg:93.11ms
step:1425/1670 train_time:132683ms step_avg:93.11ms
step:1426/1670 train_time:132776ms step_avg:93.11ms
step:1427/1670 train_time:132870ms step_avg:93.11ms
step:1428/1670 train_time:132964ms step_avg:93.11ms
step:1429/1670 train_time:133058ms step_avg:93.11ms
step:1430/1670 train_time:133151ms step_avg:93.11ms
step:1431/1670 train_time:133245ms step_avg:93.11ms
step:1432/1670 train_time:133337ms step_avg:93.11ms
step:1433/1670 train_time:133431ms step_avg:93.11ms
step:1434/1670 train_time:133524ms step_avg:93.11ms
step:1435/1670 train_time:133616ms step_avg:93.11ms
step:1436/1670 train_time:133710ms step_avg:93.11ms
step:1437/1670 train_time:133802ms step_avg:93.11ms
step:1438/1670 train_time:133895ms step_avg:93.11ms
step:1439/1670 train_time:133988ms step_avg:93.11ms
step:1440/1670 train_time:134082ms step_avg:93.11ms
step:1441/1670 train_time:134175ms step_avg:93.11ms
step:1442/1670 train_time:134269ms step_avg:93.11ms
step:1443/1670 train_time:134363ms step_avg:93.11ms
step:1444/1670 train_time:134457ms step_avg:93.11ms
step:1445/1670 train_time:134550ms step_avg:93.11ms
step:1446/1670 train_time:134643ms step_avg:93.11ms
step:1447/1670 train_time:134736ms step_avg:93.11ms
step:1448/1670 train_time:134829ms step_avg:93.11ms
step:1449/1670 train_time:134922ms step_avg:93.11ms
step:1450/1670 train_time:135016ms step_avg:93.11ms
step:1451/1670 train_time:135109ms step_avg:93.11ms
step:1452/1670 train_time:135203ms step_avg:93.12ms
step:1453/1670 train_time:135297ms step_avg:93.12ms
step:1454/1670 train_time:135390ms step_avg:93.12ms
step:1455/1670 train_time:135484ms step_avg:93.12ms
step:1456/1670 train_time:135577ms step_avg:93.12ms
step:1457/1670 train_time:135670ms step_avg:93.12ms
step:1458/1670 train_time:135765ms step_avg:93.12ms
step:1459/1670 train_time:135858ms step_avg:93.12ms
step:1460/1670 train_time:135951ms step_avg:93.12ms
step:1461/1670 train_time:136044ms step_avg:93.12ms
step:1462/1670 train_time:136138ms step_avg:93.12ms
step:1463/1670 train_time:136230ms step_avg:93.12ms
step:1464/1670 train_time:136324ms step_avg:93.12ms
step:1465/1670 train_time:136417ms step_avg:93.12ms
step:1466/1670 train_time:136511ms step_avg:93.12ms
step:1467/1670 train_time:136605ms step_avg:93.12ms
step:1468/1670 train_time:136699ms step_avg:93.12ms
step:1469/1670 train_time:136791ms step_avg:93.12ms
step:1470/1670 train_time:136884ms step_avg:93.12ms
step:1471/1670 train_time:136977ms step_avg:93.12ms
step:1472/1670 train_time:137071ms step_avg:93.12ms
step:1473/1670 train_time:137164ms step_avg:93.12ms
step:1474/1670 train_time:137257ms step_avg:93.12ms
step:1475/1670 train_time:137350ms step_avg:93.12ms
step:1476/1670 train_time:137444ms step_avg:93.12ms
step:1477/1670 train_time:137537ms step_avg:93.12ms
step:1478/1670 train_time:137630ms step_avg:93.12ms
step:1479/1670 train_time:137725ms step_avg:93.12ms
step:1480/1670 train_time:137818ms step_avg:93.12ms
step:1481/1670 train_time:137911ms step_avg:93.12ms
step:1482/1670 train_time:138005ms step_avg:93.12ms
step:1483/1670 train_time:138098ms step_avg:93.12ms
step:1484/1670 train_time:138190ms step_avg:93.12ms
step:1485/1670 train_time:138439ms step_avg:93.23ms
step:1486/1670 train_time:138513ms step_avg:93.21ms
step:1487/1670 train_time:138606ms step_avg:93.21ms
step:1488/1670 train_time:138698ms step_avg:93.21ms
step:1489/1670 train_time:138789ms step_avg:93.21ms
step:1490/1670 train_time:138882ms step_avg:93.21ms
step:1491/1670 train_time:138973ms step_avg:93.21ms
step:1492/1670 train_time:139065ms step_avg:93.21ms
step:1493/1670 train_time:139157ms step_avg:93.21ms
step:1494/1670 train_time:139249ms step_avg:93.21ms
step:1495/1670 train_time:139347ms step_avg:93.21ms
step:1496/1670 train_time:139447ms step_avg:93.21ms
step:1497/1670 train_time:139542ms step_avg:93.21ms
step:1498/1670 train_time:139634ms step_avg:93.21ms
step:1499/1670 train_time:139726ms step_avg:93.21ms
step:1500/1670 train_time:139818ms step_avg:93.21ms
step:1500/1670 val_loss:3.3110 train_time:139910ms step_avg:93.27ms
step:1501/1670 train_time:139930ms step_avg:93.22ms
step:1502/1670 train_time:140004ms step_avg:93.21ms
step:1503/1670 train_time:140098ms step_avg:93.21ms
step:1504/1670 train_time:140190ms step_avg:93.21ms
step:1505/1670 train_time:140282ms step_avg:93.21ms
step:1506/1670 train_time:140375ms step_avg:93.21ms
step:1507/1670 train_time:140468ms step_avg:93.21ms
step:1508/1670 train_time:140561ms step_avg:93.21ms
step:1509/1670 train_time:140655ms step_avg:93.21ms
step:1510/1670 train_time:140749ms step_avg:93.21ms
step:1511/1670 train_time:140842ms step_avg:93.21ms
step:1512/1670 train_time:140937ms step_avg:93.21ms
step:1513/1670 train_time:141031ms step_avg:93.21ms
step:1514/1670 train_time:141124ms step_avg:93.21ms
step:1515/1670 train_time:141217ms step_avg:93.21ms
step:1516/1670 train_time:141310ms step_avg:93.21ms
step:1517/1670 train_time:141402ms step_avg:93.21ms
step:1518/1670 train_time:141495ms step_avg:93.21ms
step:1519/1670 train_time:141588ms step_avg:93.21ms
step:1520/1670 train_time:141680ms step_avg:93.21ms
step:1521/1670 train_time:141775ms step_avg:93.21ms
step:1522/1670 train_time:141869ms step_avg:93.21ms
step:1523/1670 train_time:141961ms step_avg:93.21ms
step:1524/1670 train_time:142054ms step_avg:93.21ms
step:1525/1670 train_time:142147ms step_avg:93.21ms
step:1526/1670 train_time:142239ms step_avg:93.21ms
step:1527/1670 train_time:142332ms step_avg:93.21ms
step:1528/1670 train_time:142425ms step_avg:93.21ms
step:1529/1670 train_time:142519ms step_avg:93.21ms
step:1530/1670 train_time:142612ms step_avg:93.21ms
step:1531/1670 train_time:142707ms step_avg:93.21ms
step:1532/1670 train_time:142800ms step_avg:93.21ms
step:1533/1670 train_time:142895ms step_avg:93.21ms
step:1534/1670 train_time:142989ms step_avg:93.21ms
step:1535/1670 train_time:143082ms step_avg:93.21ms
step:1536/1670 train_time:143176ms step_avg:93.21ms
step:1537/1670 train_time:143268ms step_avg:93.21ms
step:1538/1670 train_time:143361ms step_avg:93.21ms
step:1539/1670 train_time:143455ms step_avg:93.21ms
step:1540/1670 train_time:143548ms step_avg:93.21ms
step:1541/1670 train_time:143641ms step_avg:93.21ms
step:1542/1670 train_time:143734ms step_avg:93.21ms
step:1543/1670 train_time:143827ms step_avg:93.21ms
step:1544/1670 train_time:143922ms step_avg:93.21ms
step:1545/1670 train_time:144016ms step_avg:93.21ms
step:1546/1670 train_time:144108ms step_avg:93.21ms
step:1547/1670 train_time:144202ms step_avg:93.21ms
step:1548/1670 train_time:144295ms step_avg:93.21ms
step:1549/1670 train_time:144388ms step_avg:93.21ms
step:1550/1670 train_time:144481ms step_avg:93.21ms
step:1551/1670 train_time:144574ms step_avg:93.21ms
step:1552/1670 train_time:144666ms step_avg:93.21ms
step:1553/1670 train_time:144763ms step_avg:93.21ms
step:1554/1670 train_time:144854ms step_avg:93.21ms
step:1555/1670 train_time:144947ms step_avg:93.21ms
step:1556/1670 train_time:145040ms step_avg:93.21ms
step:1557/1670 train_time:145134ms step_avg:93.21ms
step:1558/1670 train_time:145227ms step_avg:93.21ms
step:1559/1670 train_time:145320ms step_avg:93.21ms
step:1560/1670 train_time:145413ms step_avg:93.21ms
step:1561/1670 train_time:145506ms step_avg:93.21ms
step:1562/1670 train_time:145599ms step_avg:93.21ms
step:1563/1670 train_time:145693ms step_avg:93.21ms
step:1564/1670 train_time:145785ms step_avg:93.21ms
step:1565/1670 train_time:145879ms step_avg:93.21ms
step:1566/1670 train_time:145974ms step_avg:93.21ms
step:1567/1670 train_time:146066ms step_avg:93.21ms
step:1568/1670 train_time:146159ms step_avg:93.21ms
step:1569/1670 train_time:146253ms step_avg:93.21ms
step:1570/1670 train_time:146346ms step_avg:93.21ms
step:1571/1670 train_time:146440ms step_avg:93.21ms
step:1572/1670 train_time:146532ms step_avg:93.21ms
step:1573/1670 train_time:146626ms step_avg:93.21ms
step:1574/1670 train_time:146720ms step_avg:93.21ms
step:1575/1670 train_time:146814ms step_avg:93.22ms
step:1576/1670 train_time:146907ms step_avg:93.21ms
step:1577/1670 train_time:147000ms step_avg:93.21ms
step:1578/1670 train_time:147093ms step_avg:93.22ms
step:1579/1670 train_time:147186ms step_avg:93.21ms
step:1580/1670 train_time:147280ms step_avg:93.21ms
step:1581/1670 train_time:147373ms step_avg:93.22ms
step:1582/1670 train_time:147466ms step_avg:93.21ms
step:1583/1670 train_time:147560ms step_avg:93.22ms
step:1584/1670 train_time:147655ms step_avg:93.22ms
step:1585/1670 train_time:147747ms step_avg:93.22ms
step:1586/1670 train_time:147840ms step_avg:93.22ms
step:1587/1670 train_time:147933ms step_avg:93.22ms
step:1588/1670 train_time:148027ms step_avg:93.22ms
step:1589/1670 train_time:148121ms step_avg:93.22ms
step:1590/1670 train_time:148215ms step_avg:93.22ms
step:1591/1670 train_time:148308ms step_avg:93.22ms
step:1592/1670 train_time:148401ms step_avg:93.22ms
step:1593/1670 train_time:148494ms step_avg:93.22ms
step:1594/1670 train_time:148588ms step_avg:93.22ms
step:1595/1670 train_time:148681ms step_avg:93.22ms
step:1596/1670 train_time:148774ms step_avg:93.22ms
step:1597/1670 train_time:148866ms step_avg:93.22ms
step:1598/1670 train_time:148960ms step_avg:93.22ms
step:1599/1670 train_time:149053ms step_avg:93.22ms
step:1600/1670 train_time:149146ms step_avg:93.22ms
step:1601/1670 train_time:149241ms step_avg:93.22ms
step:1602/1670 train_time:149334ms step_avg:93.22ms
step:1603/1670 train_time:149427ms step_avg:93.22ms
step:1604/1670 train_time:149522ms step_avg:93.22ms
step:1605/1670 train_time:149615ms step_avg:93.22ms
step:1606/1670 train_time:149708ms step_avg:93.22ms
step:1607/1670 train_time:149801ms step_avg:93.22ms
step:1608/1670 train_time:149896ms step_avg:93.22ms
step:1609/1670 train_time:149990ms step_avg:93.22ms
step:1610/1670 train_time:150084ms step_avg:93.22ms
step:1611/1670 train_time:150177ms step_avg:93.22ms
step:1612/1670 train_time:150269ms step_avg:93.22ms
step:1613/1670 train_time:150362ms step_avg:93.22ms
step:1614/1670 train_time:150456ms step_avg:93.22ms
step:1615/1670 train_time:150550ms step_avg:93.22ms
step:1616/1670 train_time:150642ms step_avg:93.22ms
step:1617/1670 train_time:150735ms step_avg:93.22ms
step:1618/1670 train_time:150829ms step_avg:93.22ms
step:1619/1670 train_time:150922ms step_avg:93.22ms
step:1620/1670 train_time:151017ms step_avg:93.22ms
step:1621/1670 train_time:151111ms step_avg:93.22ms
step:1622/1670 train_time:151204ms step_avg:93.22ms
step:1623/1670 train_time:151297ms step_avg:93.22ms
step:1624/1670 train_time:151390ms step_avg:93.22ms
step:1625/1670 train_time:151484ms step_avg:93.22ms
step:1625/1670 val_loss:3.2859 train_time:151578ms step_avg:93.28ms
step:1626/1670 train_time:151597ms step_avg:93.23ms
step:1627/1670 train_time:151674ms step_avg:93.22ms
step:1628/1670 train_time:151767ms step_avg:93.22ms
step:1629/1670 train_time:151860ms step_avg:93.22ms
step:1630/1670 train_time:151952ms step_avg:93.22ms
step:1631/1670 train_time:152044ms step_avg:93.22ms
step:1632/1670 train_time:152137ms step_avg:93.22ms
step:1633/1670 train_time:152230ms step_avg:93.22ms
step:1634/1670 train_time:152323ms step_avg:93.22ms
step:1635/1670 train_time:152416ms step_avg:93.22ms
step:1636/1670 train_time:152510ms step_avg:93.22ms
step:1637/1670 train_time:152608ms step_avg:93.22ms
step:1638/1670 train_time:152703ms step_avg:93.23ms
step:1639/1670 train_time:152797ms step_avg:93.23ms
step:1640/1670 train_time:152889ms step_avg:93.22ms
step:1641/1670 train_time:152982ms step_avg:93.22ms
step:1642/1670 train_time:153075ms step_avg:93.22ms
step:1643/1670 train_time:153170ms step_avg:93.23ms
step:1644/1670 train_time:153264ms step_avg:93.23ms
step:1645/1670 train_time:153358ms step_avg:93.23ms
step:1646/1670 train_time:153450ms step_avg:93.23ms
step:1647/1670 train_time:153545ms step_avg:93.23ms
step:1648/1670 train_time:153640ms step_avg:93.23ms
step:1649/1670 train_time:153733ms step_avg:93.23ms
step:1650/1670 train_time:153827ms step_avg:93.23ms
step:1651/1670 train_time:153920ms step_avg:93.23ms
step:1652/1670 train_time:154012ms step_avg:93.23ms
step:1653/1670 train_time:154106ms step_avg:93.23ms
step:1654/1670 train_time:154200ms step_avg:93.23ms
step:1655/1670 train_time:154295ms step_avg:93.23ms
step:1656/1670 train_time:154387ms step_avg:93.23ms
step:1657/1670 train_time:154480ms step_avg:93.23ms
step:1658/1670 train_time:154573ms step_avg:93.23ms
step:1659/1670 train_time:154667ms step_avg:93.23ms
step:1660/1670 train_time:154760ms step_avg:93.23ms
step:1661/1670 train_time:154853ms step_avg:93.23ms
step:1662/1670 train_time:154948ms step_avg:93.23ms
step:1663/1670 train_time:155040ms step_avg:93.23ms
step:1664/1670 train_time:155133ms step_avg:93.23ms
step:1665/1670 train_time:155226ms step_avg:93.23ms
step:1666/1670 train_time:155318ms step_avg:93.23ms
step:1667/1670 train_time:155411ms step_avg:93.23ms
step:1668/1670 train_time:155506ms step_avg:93.23ms
step:1669/1670 train_time:155599ms step_avg:93.23ms
step:1670/1670 train_time:155693ms step_avg:93.23ms
step:1670/1670 val_loss:3.2772 train_time:155956ms step_avg:93.39ms
peak memory allocated: 32002 MiB reserved: 47034 MiB

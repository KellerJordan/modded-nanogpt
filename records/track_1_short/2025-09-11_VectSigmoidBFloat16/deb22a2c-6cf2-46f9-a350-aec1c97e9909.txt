import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(
    x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(
    g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99


def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]


@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(
        pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M
    )

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr,
    C_ptr,
    M,
    K,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr,
    C_ptr,
    M,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    alpha,
    beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (
        offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c
    )
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out


@torch.compile(
    dynamic=False, fullgraph=True
)  # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer


class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(
        self,
        params,
        lr: float = 1e-3,
        betas: tuple[float, float] = (0.9, 0.999),
        eps: float = 1e-8,
        weight_decay: float = 0.01,
    ):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(
                        grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True
                    ).get_future()
                )
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group["betas"]
            eps = group["eps"]
            wd = group["weight_decay"]
            params = group["params"]
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size : (rank + 1) * rank_size]
                lr = group["lr"] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(
                    g_slice, g_slice, value=1 - beta2
                )
                # bias corrections
                bias1 = 1 - beta1**t
                bias2 = 1 - beta2**t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(
                    dist.all_gather_into_tensor(
                        p, p_slice, async_op=True
                    ).get_future()
                )
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model


def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(
        self,
        in_features: int,
        out_features: int,
        use_fp8=False,
        x_s=1.0,
        w_s=1.0,
        grad_s=1.0,
    ):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (
            self.in_features**-0.5
        )  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3**0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(
                _x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s
            )[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    rotary_cos: torch.Tensor
    rotary_sin: torch.Tensor
    attn_scale: float


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        rotary_cos, rotary_sin = attn_args.rotary_cos, attn_args.rotary_sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = (
            attn_args.seqlens,
            attn_args.attn_scale,
            attn_args.bm_size,
        )

        q, k, v = (
            F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x))
            .view(B, T, 3 * self.num_heads, self.head_dim)
            .chunk(3, dim=-2)
        )
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = (
            rotary(q, rotary_cos, rotary_sin),
            rotary(k, rotary_cos, rotary_sin),
        )
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(
                v
            )  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = (
            args.train_max_seq_len
            if self.training
            else (args.val_batch_size // (grad_accum_steps * world_size))
        )

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(
            q[0],
            k[0],
            v[0],
            cu_seqlens_q=seqlens,
            cu_seqlens_k=seqlens,
            max_seqlen_q=max_len,
            max_seqlen_k=max_len,
            causal=True,
            softmax_scale=attn_scale,
            window_size=(bm_size, 0),
        )
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(
            self.attn_gate(x[..., : self.attn_gate.weight.size(-1)])
        ).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(
            B, T, self.num_heads * self.head_dim
        )  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x
        ).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = (
            CausalSelfAttention(dim, head_dim, num_heads)
            if layer_idx != 7
            else None
        )
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(
        self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs
    ):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model


def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        num_layers: int,
        num_heads: int,
        head_dim: int,
        model_dim: int,
        max_seq_len: int,
    ):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList(
            [nn.Embedding(vocab_size, model_dim) for _ in range(3)]
        )
        self.blocks = nn.ModuleList(
            [
                Block(model_dim, head_dim, num_heads, i)
                for i in range(num_layers)
            ]
        )
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(
            model_dim,
            vocab_size,
            use_fp8=use_fp8,
            x_s=(model_dim**0.5) / 448,
            w_s=2**-9,
            grad_s=1 / 448,
        )
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.ones(pad),
                ]
            )
        )
        self.max_seq_len = max_seq_len
        self.setup_yarn(head_dim)
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.0
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.0
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def setup_yarn(self, head_dim: int):
        # store single copy of rotary tensors
        angular_freq = (1 / 1024) ** torch.linspace(
            0, 1, steps=head_dim // 4, dtype=torch.float32
        )
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat(
            [angular_freq, angular_freq.new_zeros(head_dim // 4)]
        )
        t = torch.arange(self.max_seq_len, dtype=torch.float32)
        theta = torch.outer(t, angular_freq)
        self.rotary_cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.rotary_sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq

        # scale attention factor f in attn=softmax(f*qk) logarithmically with window size @classiclarryd
        windows = list(
            dict.fromkeys(list(args.ws_schedule) + [args.ws_validate])
        )
        scale_factors = [
            0.2 * math.log(curr / prev) + 1
            for prev, curr in zip(windows[:-1], windows[1:])
        ]
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        attn_scales = list(
            accumulate([0.1] + scale_factors, lambda acc, factor: acc * factor)
        )
        self.attn_scales = dict(zip(windows, attn_scales))

    def apply_yarn(
        self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32
    ):
        rotations = (
            args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        )
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp(
            (rotations - alpha) / (beta - alpha), 0, 1
        )
        self.angular_freq *= scaling_factor + interpolation_weight * (
            1 - scaling_factor
        )
        t = torch.arange(
            self.max_seq_len,
            dtype=torch.float32,
            device=self.angular_freq.device,
        )
        theta = torch.outer(t, self.angular_freq)
        self.rotary_cos.copy_(theta.cos())
        self.rotary_sin.copy_(theta.sin())

    def forward(
        self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int
    ):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = (
            [ve[0], ve[1], ve[2]]
            + [None] * (len(self.blocks) - 6)
            + [ve[0], ve[1], ve[2]]
        )
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        bm_sizes = [
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
        ]
        assert len(bm_sizes) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]).to(
            torch.bfloat16
        )  # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[: (len(self.blocks) // 2)]
        lambdas = self.scalars[
            1 * len(self.blocks) : 3 * len(self.blocks)
        ].view(-1, 2)
        sa_lambdas = self.scalars[
            3 * len(self.blocks) : 5 * len(self.blocks)
        ].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                rotary_cos=self.rotary_cos,
                rotary_sin=self.rotary_sin,
                attn_scale=self.attn_scales[ws],
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(
            logits.view(-1, logits.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader


def _load_data_shard(file: Path):
    header = torch.from_file(
        str(file), False, 256, dtype=torch.int32
    )  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(
            num_tokens, dtype=torch.uint16, pin_memory=True
        )  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(
            tokens.numpy()
        )  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, (
            "number of tokens read does not match header"
        )
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1):
        # Precompute BOS positions once per shard
        self.size = tokens.numel()
        self.bos_idx = (
            (tokens == BOS_ID)
            .nonzero(as_tuple=True)[0]
            .to(torch.int64)
            .cpu()
            .numpy()
        )
        self.i = 0
        self.world_size = world_size

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(
                        f"Insufficient BOS ahead of position {cur}; hit tail of shard."
                    )
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(
                    self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                    cur + max_seq_len,
                    cur + num_tokens_local - cur_len + 1,
                )
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx

        return starts, ends


def distributed_data_generator(
    filename_pattern: str,
    num_tokens: int,
    max_seq_len: int,
    grad_accum_steps: int = 1,
    align_to_bos: bool = True,
):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, (
        "Batch size must be divisible by world size"
    )
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(
            f"No files found for pattern: {filename_pattern}"
        )

    file_iter = iter(
        files
    )  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    finder = BOSFinder(tokens, world_size=world_size) if align_to_bos else None
    pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(
            num_tokens_local // 300, n=128
        )  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(
                    num_tokens_local, max_seq_len
                )
                start_idxs, end_idxs = (
                    torch.tensor(seq_starts[rank]),
                    torch.tensor(seq_ends[rank]),
                )
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens = _load_data_shard(next(file_iter))
                finder = BOSFinder(tokens, world_size=world_size)
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= (
                1  # last document was too long to account for _targets offset
            )
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(
                tokens
            ):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local : pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(
                num_tokens_local,
            )
            _targets = buf[1:].view(
                num_tokens_local,
            )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1 : len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(
                device="cuda", dtype=torch.int32, non_blocking=True
            ),
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, (
                "Num tokens must be divisible by world size"
            )
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main


@dataclass
class Hyperparameters:
    # data
    train_files: str = (
        "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    )
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1670  # number of iterations to run
    cooldown_frac: int = (
        0.5  # fraction of training spent cooling down the learning rate
    )
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = (
        125  # every how many steps to evaluate val loss? 0 for only at the end
    )
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws @classiclarryd


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = rank == 0  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(
    f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}"
)
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import

    return subprocess.run(
        ["nvidia-smi"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    ).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size)
    // (grad_accum_steps * world_size),
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [
    p
    for n, p in model.blocks.named_parameters()
    if p.ndim >= 2 and "embed" not in n
]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(
    hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)  # save the initial state
train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    ws = args.ws_schedule[
        step % len(args.ws_schedule)
    ]  # each window size is a new graph, need to warm up each
    model(inputs, targets, cum_seqlens, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws = get_ws(0)
for step in range(train_steps + 1):
    last_step = step == train_steps
    new_ws = get_ws(step)
    if new_ws != ws:
        model.apply_yarn(ws, new_ws)
        ws = new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (
        args.val_loss_every > 0 and step % args.val_loss_every == 0
    ):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(
            args.val_files,
            args.val_batch_size,
            -1,
            grad_accum_steps=grad_accum_steps,
            align_to_bos=False,
        )
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True,
        )
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(
                step=step,
                code=code,
                model=model.state_dict(),
                optimizers=[opt.state_dict() for opt in optimizers],
            )
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1)  # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (
        time.perf_counter() - t0
    )
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True,
    )

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.11 (main, Sep  2 2025, 14:20:58) [Clang 20.1.4 ]
Running PyTorch 2.9.0.dev20250718+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 11 09:43:56 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                  Off |
| N/A   40C    P0            125W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                  Off |
| N/A   45C    P0            134W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:65:00.0 Off |                  Off |
| N/A   45C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:68:00.0 Off |                  Off |
| N/A   37C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:85:00.0 Off |                  Off |
| N/A   37C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:86:00.0 Off |                  Off |
| N/A   45C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:E5:00.0 Off |                  Off |
| N/A   44C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E8:00.0 Off |                  Off |
| N/A   40C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1670 val_loss:10.8258 train_time:0ms step_avg:0.07ms
step:1/1670 train_time:294ms step_avg:293.96ms
step:2/1670 train_time:312ms step_avg:155.93ms
step:3/1670 train_time:381ms step_avg:127.08ms
step:4/1670 train_time:471ms step_avg:117.71ms
step:5/1670 train_time:561ms step_avg:112.17ms
step:6/1670 train_time:652ms step_avg:108.66ms
step:7/1670 train_time:742ms step_avg:106.04ms
step:8/1670 train_time:833ms step_avg:104.11ms
step:9/1670 train_time:923ms step_avg:102.55ms
step:10/1670 train_time:1014ms step_avg:101.38ms
step:11/1670 train_time:1104ms step_avg:100.35ms
step:12/1670 train_time:1197ms step_avg:99.77ms
step:13/1670 train_time:1292ms step_avg:99.36ms
step:14/1670 train_time:1384ms step_avg:98.84ms
step:15/1670 train_time:1476ms step_avg:98.42ms
step:16/1670 train_time:1568ms step_avg:97.98ms
step:17/1670 train_time:1659ms step_avg:97.62ms
step:18/1670 train_time:1751ms step_avg:97.29ms
step:19/1670 train_time:1842ms step_avg:96.95ms
step:20/1670 train_time:1933ms step_avg:96.63ms
step:21/1670 train_time:2024ms step_avg:96.39ms
step:22/1670 train_time:2116ms step_avg:96.18ms
step:23/1670 train_time:2208ms step_avg:95.98ms
step:24/1670 train_time:2301ms step_avg:95.87ms
step:25/1670 train_time:2393ms step_avg:95.73ms
step:26/1670 train_time:2486ms step_avg:95.63ms
step:27/1670 train_time:2578ms step_avg:95.47ms
step:28/1670 train_time:2666ms step_avg:95.23ms
step:29/1670 train_time:2757ms step_avg:95.08ms
step:30/1670 train_time:2849ms step_avg:94.95ms
step:31/1670 train_time:2940ms step_avg:94.83ms
step:32/1670 train_time:3032ms step_avg:94.74ms
step:33/1670 train_time:3123ms step_avg:94.64ms
step:34/1670 train_time:3216ms step_avg:94.59ms
step:35/1670 train_time:3307ms step_avg:94.50ms
step:36/1670 train_time:3400ms step_avg:94.44ms
step:37/1670 train_time:3493ms step_avg:94.40ms
step:38/1670 train_time:3585ms step_avg:94.33ms
step:39/1670 train_time:3677ms step_avg:94.28ms
step:40/1670 train_time:3768ms step_avg:94.19ms
step:41/1670 train_time:3859ms step_avg:94.12ms
step:42/1670 train_time:3950ms step_avg:94.04ms
step:43/1670 train_time:4041ms step_avg:93.97ms
step:44/1670 train_time:4133ms step_avg:93.93ms
step:45/1670 train_time:4224ms step_avg:93.87ms
step:46/1670 train_time:4318ms step_avg:93.86ms
step:47/1670 train_time:4411ms step_avg:93.85ms
step:48/1670 train_time:4504ms step_avg:93.83ms
step:49/1670 train_time:4596ms step_avg:93.79ms
step:50/1670 train_time:4688ms step_avg:93.76ms
step:51/1670 train_time:4780ms step_avg:93.73ms
step:52/1670 train_time:4872ms step_avg:93.69ms
step:53/1670 train_time:4963ms step_avg:93.63ms
step:54/1670 train_time:5053ms step_avg:93.58ms
step:55/1670 train_time:5144ms step_avg:93.53ms
step:56/1670 train_time:5235ms step_avg:93.48ms
step:57/1670 train_time:5327ms step_avg:93.45ms
step:58/1670 train_time:5420ms step_avg:93.46ms
step:59/1670 train_time:5513ms step_avg:93.44ms
step:60/1670 train_time:5604ms step_avg:93.41ms
step:61/1670 train_time:5697ms step_avg:93.39ms
step:62/1670 train_time:5788ms step_avg:93.36ms
step:63/1670 train_time:5881ms step_avg:93.35ms
step:64/1670 train_time:5972ms step_avg:93.31ms
step:65/1670 train_time:6062ms step_avg:93.27ms
step:66/1670 train_time:6153ms step_avg:93.23ms
step:67/1670 train_time:6244ms step_avg:93.19ms
step:68/1670 train_time:6335ms step_avg:93.16ms
step:69/1670 train_time:6426ms step_avg:93.13ms
step:70/1670 train_time:6518ms step_avg:93.11ms
step:71/1670 train_time:6611ms step_avg:93.11ms
step:72/1670 train_time:6703ms step_avg:93.09ms
step:73/1670 train_time:6795ms step_avg:93.08ms
step:74/1670 train_time:6886ms step_avg:93.05ms
step:75/1670 train_time:6979ms step_avg:93.06ms
step:76/1670 train_time:7070ms step_avg:93.03ms
step:77/1670 train_time:7160ms step_avg:92.99ms
step:78/1670 train_time:7253ms step_avg:92.99ms
step:79/1670 train_time:7345ms step_avg:92.97ms
step:80/1670 train_time:7436ms step_avg:92.95ms
step:81/1670 train_time:7527ms step_avg:92.93ms
step:82/1670 train_time:7620ms step_avg:92.93ms
step:83/1670 train_time:7712ms step_avg:92.92ms
step:84/1670 train_time:7804ms step_avg:92.90ms
step:85/1670 train_time:7895ms step_avg:92.89ms
step:86/1670 train_time:7986ms step_avg:92.86ms
step:87/1670 train_time:8078ms step_avg:92.85ms
step:88/1670 train_time:8169ms step_avg:92.83ms
step:89/1670 train_time:8260ms step_avg:92.81ms
step:90/1670 train_time:8352ms step_avg:92.80ms
step:91/1670 train_time:8443ms step_avg:92.78ms
step:92/1670 train_time:8535ms step_avg:92.77ms
step:93/1670 train_time:8626ms step_avg:92.75ms
step:94/1670 train_time:8719ms step_avg:92.76ms
step:95/1670 train_time:8812ms step_avg:92.76ms
step:96/1670 train_time:8904ms step_avg:92.75ms
step:97/1670 train_time:8996ms step_avg:92.74ms
step:98/1670 train_time:9087ms step_avg:92.72ms
step:99/1670 train_time:9178ms step_avg:92.70ms
step:100/1670 train_time:9269ms step_avg:92.69ms
step:101/1670 train_time:9360ms step_avg:92.67ms
step:102/1670 train_time:9451ms step_avg:92.66ms
step:103/1670 train_time:9543ms step_avg:92.65ms
step:104/1670 train_time:9634ms step_avg:92.64ms
step:105/1670 train_time:9725ms step_avg:92.62ms
step:106/1670 train_time:9819ms step_avg:92.63ms
step:107/1670 train_time:9910ms step_avg:92.62ms
step:108/1670 train_time:10002ms step_avg:92.61ms
step:109/1670 train_time:10094ms step_avg:92.61ms
step:110/1670 train_time:10185ms step_avg:92.59ms
step:111/1670 train_time:10276ms step_avg:92.58ms
step:112/1670 train_time:10366ms step_avg:92.56ms
step:113/1670 train_time:10458ms step_avg:92.55ms
step:114/1670 train_time:10549ms step_avg:92.54ms
step:115/1670 train_time:10641ms step_avg:92.53ms
step:116/1670 train_time:10733ms step_avg:92.52ms
step:117/1670 train_time:10824ms step_avg:92.51ms
step:118/1670 train_time:10918ms step_avg:92.52ms
step:119/1670 train_time:11009ms step_avg:92.51ms
step:120/1670 train_time:11101ms step_avg:92.51ms
step:121/1670 train_time:11192ms step_avg:92.50ms
step:122/1670 train_time:11283ms step_avg:92.48ms
step:123/1670 train_time:11374ms step_avg:92.48ms
step:124/1670 train_time:11466ms step_avg:92.47ms
step:125/1670 train_time:11559ms step_avg:92.47ms
step:125/1670 val_loss:4.3014 train_time:11649ms step_avg:93.19ms
step:126/1670 train_time:11667ms step_avg:92.59ms
step:127/1670 train_time:11744ms step_avg:92.47ms
step:128/1670 train_time:11844ms step_avg:92.53ms
step:129/1670 train_time:11937ms step_avg:92.53ms
step:130/1670 train_time:12026ms step_avg:92.51ms
step:131/1670 train_time:12117ms step_avg:92.50ms
step:132/1670 train_time:12208ms step_avg:92.48ms
step:133/1670 train_time:12298ms step_avg:92.47ms
step:134/1670 train_time:12388ms step_avg:92.45ms
step:135/1670 train_time:12479ms step_avg:92.43ms
step:136/1670 train_time:12569ms step_avg:92.42ms
step:137/1670 train_time:12661ms step_avg:92.42ms
step:138/1670 train_time:12756ms step_avg:92.43ms
step:139/1670 train_time:12852ms step_avg:92.46ms
step:140/1670 train_time:12945ms step_avg:92.47ms
step:141/1670 train_time:13037ms step_avg:92.46ms
step:142/1670 train_time:13128ms step_avg:92.45ms
step:143/1670 train_time:13218ms step_avg:92.43ms
step:144/1670 train_time:13309ms step_avg:92.42ms
step:145/1670 train_time:13400ms step_avg:92.41ms
step:146/1670 train_time:13490ms step_avg:92.40ms
step:147/1670 train_time:13583ms step_avg:92.40ms
step:148/1670 train_time:13674ms step_avg:92.39ms
step:149/1670 train_time:13768ms step_avg:92.40ms
step:150/1670 train_time:13861ms step_avg:92.41ms
step:151/1670 train_time:13953ms step_avg:92.40ms
step:152/1670 train_time:14046ms step_avg:92.41ms
step:153/1670 train_time:14137ms step_avg:92.40ms
step:154/1670 train_time:14228ms step_avg:92.39ms
step:155/1670 train_time:14318ms step_avg:92.37ms
step:156/1670 train_time:14409ms step_avg:92.36ms
step:157/1670 train_time:14500ms step_avg:92.36ms
step:158/1670 train_time:14591ms step_avg:92.35ms
step:159/1670 train_time:14682ms step_avg:92.34ms
step:160/1670 train_time:14775ms step_avg:92.34ms
step:161/1670 train_time:14868ms step_avg:92.35ms
step:162/1670 train_time:14960ms step_avg:92.35ms
step:163/1670 train_time:15052ms step_avg:92.34ms
step:164/1670 train_time:15143ms step_avg:92.34ms
step:165/1670 train_time:15233ms step_avg:92.32ms
step:166/1670 train_time:15324ms step_avg:92.31ms
step:167/1670 train_time:15415ms step_avg:92.30ms
step:168/1670 train_time:15507ms step_avg:92.31ms
step:169/1670 train_time:15598ms step_avg:92.30ms
step:170/1670 train_time:15690ms step_avg:92.30ms
step:171/1670 train_time:15783ms step_avg:92.30ms
step:172/1670 train_time:15876ms step_avg:92.30ms
step:173/1670 train_time:15969ms step_avg:92.30ms
step:174/1670 train_time:16061ms step_avg:92.30ms
step:175/1670 train_time:16152ms step_avg:92.30ms
step:176/1670 train_time:16242ms step_avg:92.28ms
step:177/1670 train_time:16333ms step_avg:92.28ms
step:178/1670 train_time:16424ms step_avg:92.27ms
step:179/1670 train_time:16514ms step_avg:92.26ms
step:180/1670 train_time:16605ms step_avg:92.25ms
step:181/1670 train_time:16696ms step_avg:92.24ms
step:182/1670 train_time:16789ms step_avg:92.25ms
step:183/1670 train_time:16882ms step_avg:92.25ms
step:184/1670 train_time:16973ms step_avg:92.25ms
step:185/1670 train_time:17066ms step_avg:92.25ms
step:186/1670 train_time:17157ms step_avg:92.24ms
step:187/1670 train_time:17249ms step_avg:92.24ms
step:188/1670 train_time:17340ms step_avg:92.23ms
step:189/1670 train_time:17431ms step_avg:92.23ms
step:190/1670 train_time:17521ms step_avg:92.21ms
step:191/1670 train_time:17612ms step_avg:92.21ms
step:192/1670 train_time:17704ms step_avg:92.21ms
step:193/1670 train_time:17795ms step_avg:92.20ms
step:194/1670 train_time:17887ms step_avg:92.20ms
step:195/1670 train_time:17979ms step_avg:92.20ms
step:196/1670 train_time:18072ms step_avg:92.21ms
step:197/1670 train_time:18164ms step_avg:92.20ms
step:198/1670 train_time:18255ms step_avg:92.20ms
step:199/1670 train_time:18346ms step_avg:92.19ms
step:200/1670 train_time:18438ms step_avg:92.19ms
step:201/1670 train_time:18529ms step_avg:92.19ms
step:202/1670 train_time:18620ms step_avg:92.18ms
step:203/1670 train_time:18711ms step_avg:92.17ms
step:204/1670 train_time:18803ms step_avg:92.17ms
step:205/1670 train_time:18894ms step_avg:92.17ms
step:206/1670 train_time:18986ms step_avg:92.16ms
step:207/1670 train_time:19077ms step_avg:92.16ms
step:208/1670 train_time:19171ms step_avg:92.17ms
step:209/1670 train_time:19263ms step_avg:92.17ms
step:210/1670 train_time:19353ms step_avg:92.16ms
step:211/1670 train_time:19444ms step_avg:92.15ms
step:212/1670 train_time:19534ms step_avg:92.14ms
step:213/1670 train_time:19782ms step_avg:92.87ms
step:214/1670 train_time:19857ms step_avg:92.79ms
step:215/1670 train_time:19948ms step_avg:92.78ms
step:216/1670 train_time:20038ms step_avg:92.77ms
step:217/1670 train_time:20129ms step_avg:92.76ms
step:218/1670 train_time:20219ms step_avg:92.75ms
step:219/1670 train_time:20309ms step_avg:92.74ms
step:220/1670 train_time:20399ms step_avg:92.72ms
step:221/1670 train_time:20489ms step_avg:92.71ms
step:222/1670 train_time:20579ms step_avg:92.70ms
step:223/1670 train_time:20674ms step_avg:92.71ms
step:224/1670 train_time:20769ms step_avg:92.72ms
step:225/1670 train_time:20862ms step_avg:92.72ms
step:226/1670 train_time:20952ms step_avg:92.71ms
step:227/1670 train_time:21042ms step_avg:92.70ms
step:228/1670 train_time:21133ms step_avg:92.69ms
step:229/1670 train_time:21224ms step_avg:92.68ms
step:230/1670 train_time:21315ms step_avg:92.67ms
step:231/1670 train_time:21406ms step_avg:92.67ms
step:232/1670 train_time:21496ms step_avg:92.66ms
step:233/1670 train_time:21590ms step_avg:92.66ms
step:234/1670 train_time:21683ms step_avg:92.66ms
step:235/1670 train_time:21775ms step_avg:92.66ms
step:236/1670 train_time:21868ms step_avg:92.66ms
step:237/1670 train_time:21960ms step_avg:92.66ms
step:238/1670 train_time:22051ms step_avg:92.65ms
step:239/1670 train_time:22142ms step_avg:92.65ms
step:240/1670 train_time:22232ms step_avg:92.64ms
step:241/1670 train_time:22323ms step_avg:92.63ms
step:242/1670 train_time:22414ms step_avg:92.62ms
step:243/1670 train_time:22505ms step_avg:92.61ms
step:244/1670 train_time:22595ms step_avg:92.60ms
step:245/1670 train_time:22688ms step_avg:92.60ms
step:246/1670 train_time:22781ms step_avg:92.60ms
step:247/1670 train_time:22873ms step_avg:92.60ms
step:248/1670 train_time:22964ms step_avg:92.60ms
step:249/1670 train_time:23055ms step_avg:92.59ms
step:250/1670 train_time:23146ms step_avg:92.58ms
step:250/1670 val_loss:3.9754 train_time:23237ms step_avg:92.95ms
step:251/1670 train_time:23254ms step_avg:92.65ms
step:252/1670 train_time:23329ms step_avg:92.58ms
step:253/1670 train_time:23422ms step_avg:92.58ms
step:254/1670 train_time:23512ms step_avg:92.57ms
step:255/1670 train_time:23602ms step_avg:92.56ms
step:256/1670 train_time:23692ms step_avg:92.55ms
step:257/1670 train_time:23782ms step_avg:92.54ms
step:258/1670 train_time:23872ms step_avg:92.53ms
step:259/1670 train_time:23962ms step_avg:92.52ms
step:260/1670 train_time:24054ms step_avg:92.52ms
step:261/1670 train_time:24145ms step_avg:92.51ms
step:262/1670 train_time:24238ms step_avg:92.51ms
step:263/1670 train_time:24331ms step_avg:92.51ms
step:264/1670 train_time:24423ms step_avg:92.51ms
step:265/1670 train_time:24515ms step_avg:92.51ms
step:266/1670 train_time:24605ms step_avg:92.50ms
step:267/1670 train_time:24696ms step_avg:92.49ms
step:268/1670 train_time:24786ms step_avg:92.48ms
step:269/1670 train_time:24877ms step_avg:92.48ms
step:270/1670 train_time:24968ms step_avg:92.47ms
step:271/1670 train_time:25060ms step_avg:92.47ms
step:272/1670 train_time:25151ms step_avg:92.47ms
step:273/1670 train_time:25244ms step_avg:92.47ms
step:274/1670 train_time:25336ms step_avg:92.47ms
step:275/1670 train_time:25428ms step_avg:92.46ms
step:276/1670 train_time:25518ms step_avg:92.46ms
step:277/1670 train_time:25609ms step_avg:92.45ms
step:278/1670 train_time:25699ms step_avg:92.44ms
step:279/1670 train_time:25790ms step_avg:92.44ms
step:280/1670 train_time:25881ms step_avg:92.43ms
step:281/1670 train_time:25972ms step_avg:92.43ms
step:282/1670 train_time:26065ms step_avg:92.43ms
step:283/1670 train_time:26157ms step_avg:92.43ms
step:284/1670 train_time:26248ms step_avg:92.42ms
step:285/1670 train_time:26340ms step_avg:92.42ms
step:286/1670 train_time:26431ms step_avg:92.42ms
step:287/1670 train_time:26521ms step_avg:92.41ms
step:288/1670 train_time:26613ms step_avg:92.40ms
step:289/1670 train_time:26704ms step_avg:92.40ms
step:290/1670 train_time:26795ms step_avg:92.39ms
step:291/1670 train_time:26886ms step_avg:92.39ms
step:292/1670 train_time:26977ms step_avg:92.39ms
step:293/1670 train_time:27068ms step_avg:92.38ms
step:294/1670 train_time:27159ms step_avg:92.38ms
step:295/1670 train_time:27249ms step_avg:92.37ms
step:296/1670 train_time:27342ms step_avg:92.37ms
step:297/1670 train_time:27433ms step_avg:92.37ms
step:298/1670 train_time:27525ms step_avg:92.37ms
step:299/1670 train_time:27615ms step_avg:92.36ms
step:300/1670 train_time:27705ms step_avg:92.35ms
step:301/1670 train_time:27796ms step_avg:92.35ms
step:302/1670 train_time:27887ms step_avg:92.34ms
step:303/1670 train_time:27979ms step_avg:92.34ms
step:304/1670 train_time:28070ms step_avg:92.34ms
step:305/1670 train_time:28162ms step_avg:92.33ms
step:306/1670 train_time:28252ms step_avg:92.33ms
step:307/1670 train_time:28345ms step_avg:92.33ms
step:308/1670 train_time:28437ms step_avg:92.33ms
step:309/1670 train_time:28528ms step_avg:92.32ms
step:310/1670 train_time:28619ms step_avg:92.32ms
step:311/1670 train_time:28709ms step_avg:92.31ms
step:312/1670 train_time:28799ms step_avg:92.31ms
step:313/1670 train_time:28890ms step_avg:92.30ms
step:314/1670 train_time:28981ms step_avg:92.30ms
step:315/1670 train_time:29072ms step_avg:92.29ms
step:316/1670 train_time:29164ms step_avg:92.29ms
step:317/1670 train_time:29256ms step_avg:92.29ms
step:318/1670 train_time:29348ms step_avg:92.29ms
step:319/1670 train_time:29441ms step_avg:92.29ms
step:320/1670 train_time:29531ms step_avg:92.29ms
step:321/1670 train_time:29622ms step_avg:92.28ms
step:322/1670 train_time:29713ms step_avg:92.28ms
step:323/1670 train_time:29804ms step_avg:92.27ms
step:324/1670 train_time:29895ms step_avg:92.27ms
step:325/1670 train_time:29986ms step_avg:92.26ms
step:326/1670 train_time:30077ms step_avg:92.26ms
step:327/1670 train_time:30167ms step_avg:92.25ms
step:328/1670 train_time:30259ms step_avg:92.25ms
step:329/1670 train_time:30350ms step_avg:92.25ms
step:330/1670 train_time:30442ms step_avg:92.25ms
step:331/1670 train_time:30533ms step_avg:92.24ms
step:332/1670 train_time:30624ms step_avg:92.24ms
step:333/1670 train_time:30715ms step_avg:92.24ms
step:334/1670 train_time:30806ms step_avg:92.23ms
step:335/1670 train_time:30897ms step_avg:92.23ms
step:336/1670 train_time:30987ms step_avg:92.22ms
step:337/1670 train_time:31079ms step_avg:92.22ms
step:338/1670 train_time:31170ms step_avg:92.22ms
step:339/1670 train_time:31262ms step_avg:92.22ms
step:340/1670 train_time:31354ms step_avg:92.22ms
step:341/1670 train_time:31446ms step_avg:92.22ms
step:342/1670 train_time:31537ms step_avg:92.21ms
step:343/1670 train_time:31627ms step_avg:92.21ms
step:344/1670 train_time:31719ms step_avg:92.21ms
step:345/1670 train_time:31810ms step_avg:92.20ms
step:346/1670 train_time:31901ms step_avg:92.20ms
step:347/1670 train_time:31992ms step_avg:92.20ms
step:348/1670 train_time:32085ms step_avg:92.20ms
step:349/1670 train_time:32176ms step_avg:92.19ms
step:350/1670 train_time:32266ms step_avg:92.19ms
step:351/1670 train_time:32359ms step_avg:92.19ms
step:352/1670 train_time:32450ms step_avg:92.19ms
step:353/1670 train_time:32541ms step_avg:92.18ms
step:354/1670 train_time:32632ms step_avg:92.18ms
step:355/1670 train_time:32723ms step_avg:92.18ms
step:356/1670 train_time:32814ms step_avg:92.17ms
step:357/1670 train_time:32905ms step_avg:92.17ms
step:358/1670 train_time:32996ms step_avg:92.17ms
step:359/1670 train_time:33087ms step_avg:92.17ms
step:360/1670 train_time:33178ms step_avg:92.16ms
step:361/1670 train_time:33269ms step_avg:92.16ms
step:362/1670 train_time:33361ms step_avg:92.16ms
step:363/1670 train_time:33453ms step_avg:92.16ms
step:364/1670 train_time:33544ms step_avg:92.15ms
step:365/1670 train_time:33635ms step_avg:92.15ms
step:366/1670 train_time:33727ms step_avg:92.15ms
step:367/1670 train_time:33818ms step_avg:92.15ms
step:368/1670 train_time:33908ms step_avg:92.14ms
step:369/1670 train_time:33999ms step_avg:92.14ms
step:370/1670 train_time:34090ms step_avg:92.13ms
step:371/1670 train_time:34181ms step_avg:92.13ms
step:372/1670 train_time:34273ms step_avg:92.13ms
step:373/1670 train_time:34366ms step_avg:92.13ms
step:374/1670 train_time:34457ms step_avg:92.13ms
step:375/1670 train_time:34548ms step_avg:92.13ms
step:375/1670 val_loss:3.8165 train_time:34639ms step_avg:92.37ms
step:376/1670 train_time:34656ms step_avg:92.17ms
step:377/1670 train_time:34732ms step_avg:92.13ms
step:378/1670 train_time:34824ms step_avg:92.13ms
step:379/1670 train_time:34916ms step_avg:92.13ms
step:380/1670 train_time:35006ms step_avg:92.12ms
step:381/1670 train_time:35096ms step_avg:92.12ms
step:382/1670 train_time:35186ms step_avg:92.11ms
step:383/1670 train_time:35277ms step_avg:92.11ms
step:384/1670 train_time:35368ms step_avg:92.10ms
step:385/1670 train_time:35459ms step_avg:92.10ms
step:386/1670 train_time:35552ms step_avg:92.10ms
step:387/1670 train_time:35644ms step_avg:92.10ms
step:388/1670 train_time:35737ms step_avg:92.11ms
step:389/1670 train_time:35829ms step_avg:92.11ms
step:390/1670 train_time:35920ms step_avg:92.10ms
step:391/1670 train_time:36009ms step_avg:92.10ms
step:392/1670 train_time:36099ms step_avg:92.09ms
step:393/1670 train_time:36190ms step_avg:92.09ms
step:394/1670 train_time:36280ms step_avg:92.08ms
step:395/1670 train_time:36373ms step_avg:92.08ms
step:396/1670 train_time:36463ms step_avg:92.08ms
step:397/1670 train_time:36555ms step_avg:92.08ms
step:398/1670 train_time:36647ms step_avg:92.08ms
step:399/1670 train_time:36739ms step_avg:92.08ms
step:400/1670 train_time:36832ms step_avg:92.08ms
step:401/1670 train_time:36923ms step_avg:92.08ms
step:402/1670 train_time:37014ms step_avg:92.07ms
step:403/1670 train_time:37104ms step_avg:92.07ms
step:404/1670 train_time:37195ms step_avg:92.07ms
step:405/1670 train_time:37286ms step_avg:92.06ms
step:406/1670 train_time:37377ms step_avg:92.06ms
step:407/1670 train_time:37467ms step_avg:92.06ms
step:408/1670 train_time:37559ms step_avg:92.06ms
step:409/1670 train_time:37651ms step_avg:92.06ms
step:410/1670 train_time:37742ms step_avg:92.05ms
step:411/1670 train_time:37835ms step_avg:92.06ms
step:412/1670 train_time:37926ms step_avg:92.05ms
step:413/1670 train_time:38017ms step_avg:92.05ms
step:414/1670 train_time:38108ms step_avg:92.05ms
step:415/1670 train_time:38198ms step_avg:92.04ms
step:416/1670 train_time:38290ms step_avg:92.04ms
step:417/1670 train_time:38380ms step_avg:92.04ms
step:418/1670 train_time:38470ms step_avg:92.03ms
step:419/1670 train_time:38561ms step_avg:92.03ms
step:420/1670 train_time:38652ms step_avg:92.03ms
step:421/1670 train_time:38743ms step_avg:92.03ms
step:422/1670 train_time:38836ms step_avg:92.03ms
step:423/1670 train_time:38928ms step_avg:92.03ms
step:424/1670 train_time:39020ms step_avg:92.03ms
step:425/1670 train_time:39267ms step_avg:92.39ms
step:426/1670 train_time:39346ms step_avg:92.36ms
step:427/1670 train_time:39435ms step_avg:92.35ms
step:428/1670 train_time:39526ms step_avg:92.35ms
step:429/1670 train_time:39616ms step_avg:92.35ms
step:430/1670 train_time:39706ms step_avg:92.34ms
step:431/1670 train_time:39796ms step_avg:92.33ms
step:432/1670 train_time:39886ms step_avg:92.33ms
step:433/1670 train_time:39976ms step_avg:92.32ms
step:434/1670 train_time:40065ms step_avg:92.32ms
step:435/1670 train_time:40158ms step_avg:92.32ms
step:436/1670 train_time:40255ms step_avg:92.33ms
step:437/1670 train_time:40349ms step_avg:92.33ms
step:438/1670 train_time:40440ms step_avg:92.33ms
step:439/1670 train_time:40532ms step_avg:92.33ms
step:440/1670 train_time:40620ms step_avg:92.32ms
step:441/1670 train_time:40712ms step_avg:92.32ms
step:442/1670 train_time:40801ms step_avg:92.31ms
step:443/1670 train_time:40891ms step_avg:92.31ms
step:444/1670 train_time:40981ms step_avg:92.30ms
step:445/1670 train_time:41074ms step_avg:92.30ms
step:446/1670 train_time:41166ms step_avg:92.30ms
step:447/1670 train_time:41261ms step_avg:92.31ms
step:448/1670 train_time:41355ms step_avg:92.31ms
step:449/1670 train_time:41446ms step_avg:92.31ms
step:450/1670 train_time:41538ms step_avg:92.31ms
step:451/1670 train_time:41629ms step_avg:92.30ms
step:452/1670 train_time:41719ms step_avg:92.30ms
step:453/1670 train_time:41809ms step_avg:92.29ms
step:454/1670 train_time:41899ms step_avg:92.29ms
step:455/1670 train_time:41990ms step_avg:92.29ms
step:456/1670 train_time:42080ms step_avg:92.28ms
step:457/1670 train_time:42172ms step_avg:92.28ms
step:458/1670 train_time:42264ms step_avg:92.28ms
step:459/1670 train_time:42357ms step_avg:92.28ms
step:460/1670 train_time:42449ms step_avg:92.28ms
step:461/1670 train_time:42540ms step_avg:92.28ms
step:462/1670 train_time:42631ms step_avg:92.27ms
step:463/1670 train_time:42721ms step_avg:92.27ms
step:464/1670 train_time:42812ms step_avg:92.27ms
step:465/1670 train_time:42902ms step_avg:92.26ms
step:466/1670 train_time:42992ms step_avg:92.26ms
step:467/1670 train_time:43083ms step_avg:92.25ms
step:468/1670 train_time:43174ms step_avg:92.25ms
step:469/1670 train_time:43266ms step_avg:92.25ms
step:470/1670 train_time:43358ms step_avg:92.25ms
step:471/1670 train_time:43451ms step_avg:92.25ms
step:472/1670 train_time:43541ms step_avg:92.25ms
step:473/1670 train_time:43632ms step_avg:92.25ms
step:474/1670 train_time:43723ms step_avg:92.24ms
step:475/1670 train_time:43814ms step_avg:92.24ms
step:476/1670 train_time:43904ms step_avg:92.24ms
step:477/1670 train_time:43994ms step_avg:92.23ms
step:478/1670 train_time:44086ms step_avg:92.23ms
step:479/1670 train_time:44178ms step_avg:92.23ms
step:480/1670 train_time:44269ms step_avg:92.23ms
step:481/1670 train_time:44361ms step_avg:92.23ms
step:482/1670 train_time:44452ms step_avg:92.22ms
step:483/1670 train_time:44544ms step_avg:92.22ms
step:484/1670 train_time:44635ms step_avg:92.22ms
step:485/1670 train_time:44728ms step_avg:92.22ms
step:486/1670 train_time:44819ms step_avg:92.22ms
step:487/1670 train_time:44910ms step_avg:92.22ms
step:488/1670 train_time:45000ms step_avg:92.21ms
step:489/1670 train_time:45092ms step_avg:92.21ms
step:490/1670 train_time:45183ms step_avg:92.21ms
step:491/1670 train_time:45275ms step_avg:92.21ms
step:492/1670 train_time:45365ms step_avg:92.21ms
step:493/1670 train_time:45458ms step_avg:92.21ms
step:494/1670 train_time:45548ms step_avg:92.20ms
step:495/1670 train_time:45639ms step_avg:92.20ms
step:496/1670 train_time:45731ms step_avg:92.20ms
step:497/1670 train_time:45822ms step_avg:92.20ms
step:498/1670 train_time:45913ms step_avg:92.20ms
step:499/1670 train_time:46004ms step_avg:92.19ms
step:500/1670 train_time:46095ms step_avg:92.19ms
step:500/1670 val_loss:3.7140 train_time:46186ms step_avg:92.37ms
step:501/1670 train_time:46203ms step_avg:92.22ms
step:502/1670 train_time:46279ms step_avg:92.19ms
step:503/1670 train_time:46369ms step_avg:92.19ms
step:504/1670 train_time:46460ms step_avg:92.18ms
step:505/1670 train_time:46550ms step_avg:92.18ms
step:506/1670 train_time:46640ms step_avg:92.17ms
step:507/1670 train_time:46730ms step_avg:92.17ms
step:508/1670 train_time:46822ms step_avg:92.17ms
step:509/1670 train_time:46913ms step_avg:92.17ms
step:510/1670 train_time:47005ms step_avg:92.17ms
step:511/1670 train_time:47096ms step_avg:92.16ms
step:512/1670 train_time:47189ms step_avg:92.17ms
step:513/1670 train_time:47281ms step_avg:92.17ms
step:514/1670 train_time:47373ms step_avg:92.16ms
step:515/1670 train_time:47466ms step_avg:92.17ms
step:516/1670 train_time:47557ms step_avg:92.16ms
step:517/1670 train_time:47647ms step_avg:92.16ms
step:518/1670 train_time:47738ms step_avg:92.16ms
step:519/1670 train_time:47829ms step_avg:92.16ms
step:520/1670 train_time:47921ms step_avg:92.16ms
step:521/1670 train_time:48011ms step_avg:92.15ms
step:522/1670 train_time:48103ms step_avg:92.15ms
step:523/1670 train_time:48194ms step_avg:92.15ms
step:524/1670 train_time:48285ms step_avg:92.15ms
step:525/1670 train_time:48376ms step_avg:92.15ms
step:526/1670 train_time:48467ms step_avg:92.14ms
step:527/1670 train_time:48558ms step_avg:92.14ms
step:528/1670 train_time:48648ms step_avg:92.14ms
step:529/1670 train_time:48739ms step_avg:92.13ms
step:530/1670 train_time:48830ms step_avg:92.13ms
step:531/1670 train_time:48922ms step_avg:92.13ms
step:532/1670 train_time:49013ms step_avg:92.13ms
step:533/1670 train_time:49105ms step_avg:92.13ms
step:534/1670 train_time:49197ms step_avg:92.13ms
step:535/1670 train_time:49287ms step_avg:92.13ms
step:536/1670 train_time:49378ms step_avg:92.12ms
step:537/1670 train_time:49469ms step_avg:92.12ms
step:538/1670 train_time:49560ms step_avg:92.12ms
step:539/1670 train_time:49650ms step_avg:92.12ms
step:540/1670 train_time:49742ms step_avg:92.11ms
step:541/1670 train_time:49832ms step_avg:92.11ms
step:542/1670 train_time:49923ms step_avg:92.11ms
step:543/1670 train_time:50014ms step_avg:92.11ms
step:544/1670 train_time:50106ms step_avg:92.11ms
step:545/1670 train_time:50199ms step_avg:92.11ms
step:546/1670 train_time:50290ms step_avg:92.11ms
step:547/1670 train_time:50381ms step_avg:92.11ms
step:548/1670 train_time:50472ms step_avg:92.10ms
step:549/1670 train_time:50563ms step_avg:92.10ms
step:550/1670 train_time:50654ms step_avg:92.10ms
step:551/1670 train_time:50746ms step_avg:92.10ms
step:552/1670 train_time:50837ms step_avg:92.10ms
step:553/1670 train_time:50928ms step_avg:92.09ms
step:554/1670 train_time:51020ms step_avg:92.09ms
step:555/1670 train_time:51110ms step_avg:92.09ms
step:556/1670 train_time:51202ms step_avg:92.09ms
step:557/1670 train_time:51293ms step_avg:92.09ms
step:558/1670 train_time:51577ms step_avg:92.43ms
step:559/1670 train_time:51648ms step_avg:92.39ms
step:560/1670 train_time:51739ms step_avg:92.39ms
step:561/1670 train_time:51830ms step_avg:92.39ms
step:562/1670 train_time:51921ms step_avg:92.39ms
step:563/1670 train_time:52012ms step_avg:92.38ms
step:564/1670 train_time:52103ms step_avg:92.38ms
step:565/1670 train_time:52195ms step_avg:92.38ms
step:566/1670 train_time:52286ms step_avg:92.38ms
step:567/1670 train_time:52378ms step_avg:92.38ms
step:568/1670 train_time:52474ms step_avg:92.38ms
step:569/1670 train_time:52570ms step_avg:92.39ms
step:570/1670 train_time:52665ms step_avg:92.39ms
step:571/1670 train_time:52758ms step_avg:92.40ms
step:572/1670 train_time:52850ms step_avg:92.39ms
step:573/1670 train_time:52942ms step_avg:92.39ms
step:574/1670 train_time:53032ms step_avg:92.39ms
step:575/1670 train_time:53125ms step_avg:92.39ms
step:576/1670 train_time:53216ms step_avg:92.39ms
step:577/1670 train_time:53308ms step_avg:92.39ms
step:578/1670 train_time:53400ms step_avg:92.39ms
step:579/1670 train_time:53494ms step_avg:92.39ms
step:580/1670 train_time:53589ms step_avg:92.39ms
step:581/1670 train_time:53683ms step_avg:92.40ms
step:582/1670 train_time:53775ms step_avg:92.40ms
step:583/1670 train_time:53867ms step_avg:92.40ms
step:584/1670 train_time:53959ms step_avg:92.39ms
step:585/1670 train_time:54050ms step_avg:92.39ms
step:586/1670 train_time:54141ms step_avg:92.39ms
step:587/1670 train_time:54232ms step_avg:92.39ms
step:588/1670 train_time:54325ms step_avg:92.39ms
step:589/1670 train_time:54419ms step_avg:92.39ms
step:590/1670 train_time:54511ms step_avg:92.39ms
step:591/1670 train_time:54604ms step_avg:92.39ms
step:592/1670 train_time:54698ms step_avg:92.39ms
step:593/1670 train_time:54790ms step_avg:92.39ms
step:594/1670 train_time:54883ms step_avg:92.40ms
step:595/1670 train_time:54975ms step_avg:92.40ms
step:596/1670 train_time:55067ms step_avg:92.39ms
step:597/1670 train_time:55158ms step_avg:92.39ms
step:598/1670 train_time:55250ms step_avg:92.39ms
step:599/1670 train_time:55342ms step_avg:92.39ms
step:600/1670 train_time:55435ms step_avg:92.39ms
step:601/1670 train_time:55528ms step_avg:92.39ms
step:602/1670 train_time:55622ms step_avg:92.39ms
step:603/1670 train_time:55715ms step_avg:92.40ms
step:604/1670 train_time:55808ms step_avg:92.40ms
step:605/1670 train_time:55901ms step_avg:92.40ms
step:606/1670 train_time:55993ms step_avg:92.40ms
step:607/1670 train_time:56087ms step_avg:92.40ms
step:608/1670 train_time:56179ms step_avg:92.40ms
step:609/1670 train_time:56270ms step_avg:92.40ms
step:610/1670 train_time:56363ms step_avg:92.40ms
step:611/1670 train_time:56455ms step_avg:92.40ms
step:612/1670 train_time:56547ms step_avg:92.40ms
step:613/1670 train_time:56639ms step_avg:92.40ms
step:614/1670 train_time:56731ms step_avg:92.40ms
step:615/1670 train_time:56824ms step_avg:92.40ms
step:616/1670 train_time:56918ms step_avg:92.40ms
step:617/1670 train_time:57010ms step_avg:92.40ms
step:618/1670 train_time:57103ms step_avg:92.40ms
step:619/1670 train_time:57194ms step_avg:92.40ms
step:620/1670 train_time:57287ms step_avg:92.40ms
step:621/1670 train_time:57379ms step_avg:92.40ms
step:622/1670 train_time:57473ms step_avg:92.40ms
step:623/1670 train_time:57566ms step_avg:92.40ms
step:624/1670 train_time:57659ms step_avg:92.40ms
step:625/1670 train_time:57751ms step_avg:92.40ms
step:625/1670 val_loss:3.6159 train_time:57844ms step_avg:92.55ms
step:626/1670 train_time:57861ms step_avg:92.43ms
step:627/1670 train_time:57937ms step_avg:92.40ms
step:628/1670 train_time:58041ms step_avg:92.42ms
step:629/1670 train_time:58136ms step_avg:92.43ms
step:630/1670 train_time:58229ms step_avg:92.43ms
step:631/1670 train_time:58320ms step_avg:92.43ms
step:632/1670 train_time:58412ms step_avg:92.42ms
step:633/1670 train_time:58502ms step_avg:92.42ms
step:634/1670 train_time:58593ms step_avg:92.42ms
step:635/1670 train_time:58684ms step_avg:92.42ms
step:636/1670 train_time:58776ms step_avg:92.41ms
step:637/1670 train_time:58869ms step_avg:92.42ms
step:638/1670 train_time:58962ms step_avg:92.42ms
step:639/1670 train_time:59199ms step_avg:92.64ms
step:640/1670 train_time:59274ms step_avg:92.62ms
step:641/1670 train_time:59364ms step_avg:92.61ms
step:642/1670 train_time:59455ms step_avg:92.61ms
step:643/1670 train_time:59546ms step_avg:92.61ms
step:644/1670 train_time:59637ms step_avg:92.60ms
step:645/1670 train_time:59728ms step_avg:92.60ms
step:646/1670 train_time:59819ms step_avg:92.60ms
step:647/1670 train_time:59911ms step_avg:92.60ms
step:648/1670 train_time:60002ms step_avg:92.60ms
step:649/1670 train_time:60098ms step_avg:92.60ms
step:650/1670 train_time:60195ms step_avg:92.61ms
step:651/1670 train_time:60290ms step_avg:92.61ms
step:652/1670 train_time:60382ms step_avg:92.61ms
step:653/1670 train_time:60474ms step_avg:92.61ms
step:654/1670 train_time:60565ms step_avg:92.61ms
step:655/1670 train_time:60658ms step_avg:92.61ms
step:656/1670 train_time:60750ms step_avg:92.61ms
step:657/1670 train_time:60840ms step_avg:92.60ms
step:658/1670 train_time:60932ms step_avg:92.60ms
step:659/1670 train_time:61025ms step_avg:92.60ms
step:660/1670 train_time:61119ms step_avg:92.60ms
step:661/1670 train_time:61215ms step_avg:92.61ms
step:662/1670 train_time:61309ms step_avg:92.61ms
step:663/1670 train_time:61401ms step_avg:92.61ms
step:664/1670 train_time:61493ms step_avg:92.61ms
step:665/1670 train_time:61585ms step_avg:92.61ms
step:666/1670 train_time:61676ms step_avg:92.61ms
step:667/1670 train_time:61768ms step_avg:92.61ms
step:668/1670 train_time:61859ms step_avg:92.60ms
step:669/1670 train_time:61951ms step_avg:92.60ms
step:670/1670 train_time:62044ms step_avg:92.60ms
step:671/1670 train_time:62137ms step_avg:92.60ms
step:672/1670 train_time:62230ms step_avg:92.60ms
step:673/1670 train_time:62323ms step_avg:92.61ms
step:674/1670 train_time:62415ms step_avg:92.60ms
step:675/1670 train_time:62508ms step_avg:92.60ms
step:676/1670 train_time:62599ms step_avg:92.60ms
step:677/1670 train_time:62692ms step_avg:92.60ms
step:678/1670 train_time:62783ms step_avg:92.60ms
step:679/1670 train_time:62875ms step_avg:92.60ms
step:680/1670 train_time:62966ms step_avg:92.60ms
step:681/1670 train_time:63058ms step_avg:92.60ms
step:682/1670 train_time:63153ms step_avg:92.60ms
step:683/1670 train_time:63247ms step_avg:92.60ms
step:684/1670 train_time:63339ms step_avg:92.60ms
step:685/1670 train_time:63432ms step_avg:92.60ms
step:686/1670 train_time:63524ms step_avg:92.60ms
step:687/1670 train_time:63615ms step_avg:92.60ms
step:688/1670 train_time:63707ms step_avg:92.60ms
step:689/1670 train_time:63799ms step_avg:92.60ms
step:690/1670 train_time:63891ms step_avg:92.60ms
step:691/1670 train_time:63984ms step_avg:92.60ms
step:692/1670 train_time:64076ms step_avg:92.59ms
step:693/1670 train_time:64170ms step_avg:92.60ms
step:694/1670 train_time:64262ms step_avg:92.60ms
step:695/1670 train_time:64355ms step_avg:92.60ms
step:696/1670 train_time:64448ms step_avg:92.60ms
step:697/1670 train_time:64540ms step_avg:92.60ms
step:698/1670 train_time:64633ms step_avg:92.60ms
step:699/1670 train_time:64725ms step_avg:92.60ms
step:700/1670 train_time:64818ms step_avg:92.60ms
step:701/1670 train_time:64911ms step_avg:92.60ms
step:702/1670 train_time:65003ms step_avg:92.60ms
step:703/1670 train_time:65095ms step_avg:92.60ms
step:704/1670 train_time:65188ms step_avg:92.60ms
step:705/1670 train_time:65280ms step_avg:92.60ms
step:706/1670 train_time:65373ms step_avg:92.60ms
step:707/1670 train_time:65466ms step_avg:92.60ms
step:708/1670 train_time:65558ms step_avg:92.60ms
step:709/1670 train_time:65651ms step_avg:92.60ms
step:710/1670 train_time:65743ms step_avg:92.60ms
step:711/1670 train_time:65836ms step_avg:92.60ms
step:712/1670 train_time:65928ms step_avg:92.60ms
step:713/1670 train_time:66020ms step_avg:92.59ms
step:714/1670 train_time:66112ms step_avg:92.59ms
step:715/1670 train_time:66205ms step_avg:92.59ms
step:716/1670 train_time:66297ms step_avg:92.59ms
step:717/1670 train_time:66389ms step_avg:92.59ms
step:718/1670 train_time:66482ms step_avg:92.59ms
step:719/1670 train_time:66574ms step_avg:92.59ms
step:720/1670 train_time:66668ms step_avg:92.59ms
step:721/1670 train_time:66759ms step_avg:92.59ms
step:722/1670 train_time:66853ms step_avg:92.59ms
step:723/1670 train_time:66946ms step_avg:92.60ms
step:724/1670 train_time:67038ms step_avg:92.59ms
step:725/1670 train_time:67131ms step_avg:92.59ms
step:726/1670 train_time:67223ms step_avg:92.59ms
step:727/1670 train_time:67315ms step_avg:92.59ms
step:728/1670 train_time:67409ms step_avg:92.59ms
step:729/1670 train_time:67502ms step_avg:92.60ms
step:730/1670 train_time:67595ms step_avg:92.60ms
step:731/1670 train_time:67687ms step_avg:92.60ms
step:732/1670 train_time:67780ms step_avg:92.59ms
step:733/1670 train_time:67873ms step_avg:92.60ms
step:734/1670 train_time:67964ms step_avg:92.59ms
step:735/1670 train_time:68056ms step_avg:92.59ms
step:736/1670 train_time:68150ms step_avg:92.59ms
step:737/1670 train_time:68241ms step_avg:92.59ms
step:738/1670 train_time:68334ms step_avg:92.59ms
step:739/1670 train_time:68427ms step_avg:92.59ms
step:740/1670 train_time:68519ms step_avg:92.59ms
step:741/1670 train_time:68613ms step_avg:92.60ms
step:742/1670 train_time:68706ms step_avg:92.60ms
step:743/1670 train_time:68798ms step_avg:92.59ms
step:744/1670 train_time:68891ms step_avg:92.59ms
step:745/1670 train_time:68982ms step_avg:92.59ms
step:746/1670 train_time:69075ms step_avg:92.59ms
step:747/1670 train_time:69167ms step_avg:92.59ms
step:748/1670 train_time:69259ms step_avg:92.59ms
step:749/1670 train_time:69352ms step_avg:92.59ms
step:750/1670 train_time:69444ms step_avg:92.59ms
step:750/1670 val_loss:3.5620 train_time:69537ms step_avg:92.72ms
step:751/1670 train_time:69555ms step_avg:92.62ms
step:752/1670 train_time:69632ms step_avg:92.60ms
step:753/1670 train_time:69725ms step_avg:92.60ms
step:754/1670 train_time:69816ms step_avg:92.59ms
step:755/1670 train_time:69908ms step_avg:92.59ms
step:756/1670 train_time:70000ms step_avg:92.59ms
step:757/1670 train_time:70092ms step_avg:92.59ms
step:758/1670 train_time:70184ms step_avg:92.59ms
step:759/1670 train_time:70276ms step_avg:92.59ms
step:760/1670 train_time:70369ms step_avg:92.59ms
step:761/1670 train_time:70463ms step_avg:92.59ms
step:762/1670 train_time:70556ms step_avg:92.59ms
step:763/1670 train_time:70649ms step_avg:92.59ms
step:764/1670 train_time:70743ms step_avg:92.60ms
step:765/1670 train_time:70835ms step_avg:92.59ms
step:766/1670 train_time:70926ms step_avg:92.59ms
step:767/1670 train_time:71019ms step_avg:92.59ms
step:768/1670 train_time:71111ms step_avg:92.59ms
step:769/1670 train_time:71204ms step_avg:92.59ms
step:770/1670 train_time:71295ms step_avg:92.59ms
step:771/1670 train_time:71388ms step_avg:92.59ms
step:772/1670 train_time:71481ms step_avg:92.59ms
step:773/1670 train_time:71574ms step_avg:92.59ms
step:774/1670 train_time:71667ms step_avg:92.59ms
step:775/1670 train_time:71760ms step_avg:92.59ms
step:776/1670 train_time:71854ms step_avg:92.59ms
step:777/1670 train_time:71946ms step_avg:92.59ms
step:778/1670 train_time:72037ms step_avg:92.59ms
step:779/1670 train_time:72129ms step_avg:92.59ms
step:780/1670 train_time:72221ms step_avg:92.59ms
step:781/1670 train_time:72314ms step_avg:92.59ms
step:782/1670 train_time:72407ms step_avg:92.59ms
step:783/1670 train_time:72500ms step_avg:92.59ms
step:784/1670 train_time:72595ms step_avg:92.60ms
step:785/1670 train_time:72688ms step_avg:92.60ms
step:786/1670 train_time:72780ms step_avg:92.60ms
step:787/1670 train_time:72874ms step_avg:92.60ms
step:788/1670 train_time:72967ms step_avg:92.60ms
step:789/1670 train_time:73058ms step_avg:92.60ms
step:790/1670 train_time:73149ms step_avg:92.59ms
step:791/1670 train_time:73241ms step_avg:92.59ms
step:792/1670 train_time:73335ms step_avg:92.59ms
step:793/1670 train_time:73429ms step_avg:92.60ms
step:794/1670 train_time:73521ms step_avg:92.60ms
step:795/1670 train_time:73615ms step_avg:92.60ms
step:796/1670 train_time:73708ms step_avg:92.60ms
step:797/1670 train_time:73800ms step_avg:92.60ms
step:798/1670 train_time:73894ms step_avg:92.60ms
step:799/1670 train_time:73987ms step_avg:92.60ms
step:800/1670 train_time:74079ms step_avg:92.60ms
step:801/1670 train_time:74171ms step_avg:92.60ms
step:802/1670 train_time:74263ms step_avg:92.60ms
step:803/1670 train_time:74356ms step_avg:92.60ms
step:804/1670 train_time:74448ms step_avg:92.60ms
step:805/1670 train_time:74540ms step_avg:92.60ms
step:806/1670 train_time:74634ms step_avg:92.60ms
step:807/1670 train_time:74727ms step_avg:92.60ms
step:808/1670 train_time:74819ms step_avg:92.60ms
step:809/1670 train_time:74913ms step_avg:92.60ms
step:810/1670 train_time:75005ms step_avg:92.60ms
step:811/1670 train_time:75097ms step_avg:92.60ms
step:812/1670 train_time:75190ms step_avg:92.60ms
step:813/1670 train_time:75282ms step_avg:92.60ms
step:814/1670 train_time:75374ms step_avg:92.60ms
step:815/1670 train_time:75467ms step_avg:92.60ms
step:816/1670 train_time:75559ms step_avg:92.60ms
step:817/1670 train_time:75651ms step_avg:92.60ms
step:818/1670 train_time:75743ms step_avg:92.60ms
step:819/1670 train_time:75836ms step_avg:92.60ms
step:820/1670 train_time:75928ms step_avg:92.60ms
step:821/1670 train_time:76020ms step_avg:92.59ms
step:822/1670 train_time:76114ms step_avg:92.60ms
step:823/1670 train_time:76207ms step_avg:92.60ms
step:824/1670 train_time:76298ms step_avg:92.59ms
step:825/1670 train_time:76391ms step_avg:92.60ms
step:826/1670 train_time:76484ms step_avg:92.60ms
step:827/1670 train_time:76576ms step_avg:92.59ms
step:828/1670 train_time:76669ms step_avg:92.59ms
step:829/1670 train_time:76761ms step_avg:92.59ms
step:830/1670 train_time:76853ms step_avg:92.59ms
step:831/1670 train_time:76947ms step_avg:92.60ms
step:832/1670 train_time:77038ms step_avg:92.59ms
step:833/1670 train_time:77131ms step_avg:92.59ms
step:834/1670 train_time:77223ms step_avg:92.59ms
step:835/1670 train_time:77316ms step_avg:92.59ms
step:836/1670 train_time:77409ms step_avg:92.59ms
step:837/1670 train_time:77500ms step_avg:92.59ms
step:838/1670 train_time:77594ms step_avg:92.59ms
step:839/1670 train_time:77687ms step_avg:92.59ms
step:840/1670 train_time:77779ms step_avg:92.59ms
step:841/1670 train_time:77872ms step_avg:92.59ms
step:842/1670 train_time:77965ms step_avg:92.59ms
step:843/1670 train_time:78057ms step_avg:92.59ms
step:844/1670 train_time:78149ms step_avg:92.59ms
step:845/1670 train_time:78241ms step_avg:92.59ms
step:846/1670 train_time:78334ms step_avg:92.59ms
step:847/1670 train_time:78427ms step_avg:92.59ms
step:848/1670 train_time:78518ms step_avg:92.59ms
step:849/1670 train_time:78612ms step_avg:92.59ms
step:850/1670 train_time:78705ms step_avg:92.59ms
step:851/1670 train_time:78955ms step_avg:92.78ms
step:852/1670 train_time:79029ms step_avg:92.76ms
step:853/1670 train_time:79120ms step_avg:92.75ms
step:854/1670 train_time:79211ms step_avg:92.75ms
step:855/1670 train_time:79302ms step_avg:92.75ms
step:856/1670 train_time:79393ms step_avg:92.75ms
step:857/1670 train_time:79484ms step_avg:92.75ms
step:858/1670 train_time:79576ms step_avg:92.75ms
step:859/1670 train_time:79667ms step_avg:92.74ms
step:860/1670 train_time:79758ms step_avg:92.74ms
step:861/1670 train_time:79854ms step_avg:92.75ms
step:862/1670 train_time:79953ms step_avg:92.75ms
step:863/1670 train_time:80047ms step_avg:92.75ms
step:864/1670 train_time:80140ms step_avg:92.75ms
step:865/1670 train_time:80232ms step_avg:92.75ms
step:866/1670 train_time:80323ms step_avg:92.75ms
step:867/1670 train_time:80415ms step_avg:92.75ms
step:868/1670 train_time:80508ms step_avg:92.75ms
step:869/1670 train_time:80599ms step_avg:92.75ms
step:870/1670 train_time:80690ms step_avg:92.75ms
step:871/1670 train_time:80782ms step_avg:92.75ms
step:872/1670 train_time:80877ms step_avg:92.75ms
step:873/1670 train_time:80972ms step_avg:92.75ms
step:874/1670 train_time:81066ms step_avg:92.75ms
step:875/1670 train_time:81158ms step_avg:92.75ms
step:875/1670 val_loss:3.5192 train_time:81250ms step_avg:92.86ms
step:876/1670 train_time:81268ms step_avg:92.77ms
step:877/1670 train_time:81345ms step_avg:92.75ms
step:878/1670 train_time:81438ms step_avg:92.75ms
step:879/1670 train_time:81530ms step_avg:92.75ms
step:880/1670 train_time:81621ms step_avg:92.75ms
step:881/1670 train_time:81712ms step_avg:92.75ms
step:882/1670 train_time:81803ms step_avg:92.75ms
step:883/1670 train_time:81894ms step_avg:92.75ms
step:884/1670 train_time:81986ms step_avg:92.74ms
step:885/1670 train_time:82077ms step_avg:92.74ms
step:886/1670 train_time:82171ms step_avg:92.74ms
step:887/1670 train_time:82267ms step_avg:92.75ms
step:888/1670 train_time:82361ms step_avg:92.75ms
step:889/1670 train_time:82453ms step_avg:92.75ms
step:890/1670 train_time:82547ms step_avg:92.75ms
step:891/1670 train_time:82639ms step_avg:92.75ms
step:892/1670 train_time:82731ms step_avg:92.75ms
step:893/1670 train_time:82823ms step_avg:92.75ms
step:894/1670 train_time:82914ms step_avg:92.75ms
step:895/1670 train_time:83005ms step_avg:92.74ms
step:896/1670 train_time:83097ms step_avg:92.74ms
step:897/1670 train_time:83191ms step_avg:92.74ms
step:898/1670 train_time:83285ms step_avg:92.74ms
step:899/1670 train_time:83377ms step_avg:92.74ms
step:900/1670 train_time:83471ms step_avg:92.75ms
step:901/1670 train_time:83564ms step_avg:92.75ms
step:902/1670 train_time:83656ms step_avg:92.75ms
step:903/1670 train_time:83749ms step_avg:92.75ms
step:904/1670 train_time:83841ms step_avg:92.74ms
step:905/1670 train_time:83933ms step_avg:92.74ms
step:906/1670 train_time:84025ms step_avg:92.74ms
step:907/1670 train_time:84118ms step_avg:92.74ms
step:908/1670 train_time:84212ms step_avg:92.74ms
step:909/1670 train_time:84306ms step_avg:92.75ms
step:910/1670 train_time:84399ms step_avg:92.75ms
step:911/1670 train_time:84493ms step_avg:92.75ms
step:912/1670 train_time:84585ms step_avg:92.75ms
step:913/1670 train_time:84677ms step_avg:92.75ms
step:914/1670 train_time:84770ms step_avg:92.75ms
step:915/1670 train_time:84862ms step_avg:92.75ms
step:916/1670 train_time:84953ms step_avg:92.74ms
step:917/1670 train_time:85046ms step_avg:92.74ms
step:918/1670 train_time:85139ms step_avg:92.74ms
step:919/1670 train_time:85232ms step_avg:92.74ms
step:920/1670 train_time:85325ms step_avg:92.74ms
step:921/1670 train_time:85418ms step_avg:92.74ms
step:922/1670 train_time:85511ms step_avg:92.75ms
step:923/1670 train_time:85604ms step_avg:92.75ms
step:924/1670 train_time:85696ms step_avg:92.74ms
step:925/1670 train_time:85789ms step_avg:92.74ms
step:926/1670 train_time:85881ms step_avg:92.74ms
step:927/1670 train_time:85974ms step_avg:92.74ms
step:928/1670 train_time:86067ms step_avg:92.74ms
step:929/1670 train_time:86159ms step_avg:92.74ms
step:930/1670 train_time:86252ms step_avg:92.74ms
step:931/1670 train_time:86344ms step_avg:92.74ms
step:932/1670 train_time:86437ms step_avg:92.74ms
step:933/1670 train_time:86530ms step_avg:92.74ms
step:934/1670 train_time:86624ms step_avg:92.74ms
step:935/1670 train_time:86716ms step_avg:92.74ms
step:936/1670 train_time:86808ms step_avg:92.74ms
step:937/1670 train_time:86900ms step_avg:92.74ms
step:938/1670 train_time:86992ms step_avg:92.74ms
step:939/1670 train_time:87085ms step_avg:92.74ms
step:940/1670 train_time:87177ms step_avg:92.74ms
step:941/1670 train_time:87269ms step_avg:92.74ms
step:942/1670 train_time:87362ms step_avg:92.74ms
step:943/1670 train_time:87455ms step_avg:92.74ms
step:944/1670 train_time:87548ms step_avg:92.74ms
step:945/1670 train_time:87641ms step_avg:92.74ms
step:946/1670 train_time:87734ms step_avg:92.74ms
step:947/1670 train_time:87827ms step_avg:92.74ms
step:948/1670 train_time:87919ms step_avg:92.74ms
step:949/1670 train_time:88012ms step_avg:92.74ms
step:950/1670 train_time:88105ms step_avg:92.74ms
step:951/1670 train_time:88197ms step_avg:92.74ms
step:952/1670 train_time:88290ms step_avg:92.74ms
step:953/1670 train_time:88382ms step_avg:92.74ms
step:954/1670 train_time:88475ms step_avg:92.74ms
step:955/1670 train_time:88567ms step_avg:92.74ms
step:956/1670 train_time:88660ms step_avg:92.74ms
step:957/1670 train_time:88753ms step_avg:92.74ms
step:958/1670 train_time:88845ms step_avg:92.74ms
step:959/1670 train_time:88937ms step_avg:92.74ms
step:960/1670 train_time:89029ms step_avg:92.74ms
step:961/1670 train_time:89121ms step_avg:92.74ms
step:962/1670 train_time:89213ms step_avg:92.74ms
step:963/1670 train_time:89306ms step_avg:92.74ms
step:964/1670 train_time:89398ms step_avg:92.74ms
step:965/1670 train_time:89492ms step_avg:92.74ms
step:966/1670 train_time:89584ms step_avg:92.74ms
step:967/1670 train_time:89676ms step_avg:92.74ms
step:968/1670 train_time:89771ms step_avg:92.74ms
step:969/1670 train_time:89864ms step_avg:92.74ms
step:970/1670 train_time:89956ms step_avg:92.74ms
step:971/1670 train_time:90049ms step_avg:92.74ms
step:972/1670 train_time:90141ms step_avg:92.74ms
step:973/1670 train_time:90234ms step_avg:92.74ms
step:974/1670 train_time:90326ms step_avg:92.74ms
step:975/1670 train_time:90419ms step_avg:92.74ms
step:976/1670 train_time:90511ms step_avg:92.74ms
step:977/1670 train_time:90604ms step_avg:92.74ms
step:978/1670 train_time:90697ms step_avg:92.74ms
step:979/1670 train_time:90790ms step_avg:92.74ms
step:980/1670 train_time:90882ms step_avg:92.74ms
step:981/1670 train_time:90974ms step_avg:92.74ms
step:982/1670 train_time:91068ms step_avg:92.74ms
step:983/1670 train_time:91160ms step_avg:92.74ms
step:984/1670 train_time:91253ms step_avg:92.74ms
step:985/1670 train_time:91346ms step_avg:92.74ms
step:986/1670 train_time:91438ms step_avg:92.74ms
step:987/1670 train_time:91532ms step_avg:92.74ms
step:988/1670 train_time:91625ms step_avg:92.74ms
step:989/1670 train_time:91717ms step_avg:92.74ms
step:990/1670 train_time:91810ms step_avg:92.74ms
step:991/1670 train_time:91902ms step_avg:92.74ms
step:992/1670 train_time:91995ms step_avg:92.74ms
step:993/1670 train_time:92088ms step_avg:92.74ms
step:994/1670 train_time:92180ms step_avg:92.74ms
step:995/1670 train_time:92273ms step_avg:92.74ms
step:996/1670 train_time:92366ms step_avg:92.74ms
step:997/1670 train_time:92458ms step_avg:92.74ms
step:998/1670 train_time:92552ms step_avg:92.74ms
step:999/1670 train_time:92645ms step_avg:92.74ms
step:1000/1670 train_time:92737ms step_avg:92.74ms
step:1000/1670 val_loss:3.4692 train_time:92831ms step_avg:92.83ms
step:1001/1670 train_time:92848ms step_avg:92.76ms
step:1002/1670 train_time:92926ms step_avg:92.74ms
step:1003/1670 train_time:93018ms step_avg:92.74ms
step:1004/1670 train_time:93111ms step_avg:92.74ms
step:1005/1670 train_time:93203ms step_avg:92.74ms
step:1006/1670 train_time:93296ms step_avg:92.74ms
step:1007/1670 train_time:93388ms step_avg:92.74ms
step:1008/1670 train_time:93479ms step_avg:92.74ms
step:1009/1670 train_time:93572ms step_avg:92.74ms
step:1010/1670 train_time:93664ms step_avg:92.74ms
step:1011/1670 train_time:93756ms step_avg:92.74ms
step:1012/1670 train_time:93850ms step_avg:92.74ms
step:1013/1670 train_time:93944ms step_avg:92.74ms
step:1014/1670 train_time:94036ms step_avg:92.74ms
step:1015/1670 train_time:94129ms step_avg:92.74ms
step:1016/1670 train_time:94221ms step_avg:92.74ms
step:1017/1670 train_time:94313ms step_avg:92.74ms
step:1018/1670 train_time:94406ms step_avg:92.74ms
step:1019/1670 train_time:94498ms step_avg:92.74ms
step:1020/1670 train_time:94590ms step_avg:92.74ms
step:1021/1670 train_time:94684ms step_avg:92.74ms
step:1022/1670 train_time:94776ms step_avg:92.74ms
step:1023/1670 train_time:94870ms step_avg:92.74ms
step:1024/1670 train_time:94963ms step_avg:92.74ms
step:1025/1670 train_time:95055ms step_avg:92.74ms
step:1026/1670 train_time:95147ms step_avg:92.74ms
step:1027/1670 train_time:95239ms step_avg:92.74ms
step:1028/1670 train_time:95333ms step_avg:92.74ms
step:1029/1670 train_time:95425ms step_avg:92.74ms
step:1030/1670 train_time:95517ms step_avg:92.74ms
step:1031/1670 train_time:95610ms step_avg:92.74ms
step:1032/1670 train_time:95702ms step_avg:92.73ms
step:1033/1670 train_time:95794ms step_avg:92.73ms
step:1034/1670 train_time:95887ms step_avg:92.73ms
step:1035/1670 train_time:95980ms step_avg:92.73ms
step:1036/1670 train_time:96073ms step_avg:92.73ms
step:1037/1670 train_time:96165ms step_avg:92.73ms
step:1038/1670 train_time:96257ms step_avg:92.73ms
step:1039/1670 train_time:96349ms step_avg:92.73ms
step:1040/1670 train_time:96441ms step_avg:92.73ms
step:1041/1670 train_time:96535ms step_avg:92.73ms
step:1042/1670 train_time:96627ms step_avg:92.73ms
step:1043/1670 train_time:96719ms step_avg:92.73ms
step:1044/1670 train_time:96813ms step_avg:92.73ms
step:1045/1670 train_time:96906ms step_avg:92.73ms
step:1046/1670 train_time:96998ms step_avg:92.73ms
step:1047/1670 train_time:97091ms step_avg:92.73ms
step:1048/1670 train_time:97183ms step_avg:92.73ms
step:1049/1670 train_time:97275ms step_avg:92.73ms
step:1050/1670 train_time:97368ms step_avg:92.73ms
step:1051/1670 train_time:97461ms step_avg:92.73ms
step:1052/1670 train_time:97553ms step_avg:92.73ms
step:1053/1670 train_time:97646ms step_avg:92.73ms
step:1054/1670 train_time:97738ms step_avg:92.73ms
step:1055/1670 train_time:97831ms step_avg:92.73ms
step:1056/1670 train_time:97924ms step_avg:92.73ms
step:1057/1670 train_time:98017ms step_avg:92.73ms
step:1058/1670 train_time:98109ms step_avg:92.73ms
step:1059/1670 train_time:98201ms step_avg:92.73ms
step:1060/1670 train_time:98294ms step_avg:92.73ms
step:1061/1670 train_time:98386ms step_avg:92.73ms
step:1062/1670 train_time:98634ms step_avg:92.88ms
step:1063/1670 train_time:98714ms step_avg:92.86ms
step:1064/1670 train_time:98805ms step_avg:92.86ms
step:1065/1670 train_time:98896ms step_avg:92.86ms
step:1066/1670 train_time:98987ms step_avg:92.86ms
step:1067/1670 train_time:99078ms step_avg:92.86ms
step:1068/1670 train_time:99169ms step_avg:92.85ms
step:1069/1670 train_time:99260ms step_avg:92.85ms
step:1070/1670 train_time:99352ms step_avg:92.85ms
step:1071/1670 train_time:99443ms step_avg:92.85ms
step:1072/1670 train_time:99542ms step_avg:92.86ms
step:1073/1670 train_time:99638ms step_avg:92.86ms
step:1074/1670 train_time:99733ms step_avg:92.86ms
step:1075/1670 train_time:99826ms step_avg:92.86ms
step:1076/1670 train_time:99917ms step_avg:92.86ms
step:1077/1670 train_time:100009ms step_avg:92.86ms
step:1078/1670 train_time:100100ms step_avg:92.86ms
step:1079/1670 train_time:100192ms step_avg:92.86ms
step:1080/1670 train_time:100283ms step_avg:92.85ms
step:1081/1670 train_time:100374ms step_avg:92.85ms
step:1082/1670 train_time:100469ms step_avg:92.85ms
step:1083/1670 train_time:100563ms step_avg:92.86ms
step:1084/1670 train_time:100658ms step_avg:92.86ms
step:1085/1670 train_time:100752ms step_avg:92.86ms
step:1086/1670 train_time:100844ms step_avg:92.86ms
step:1087/1670 train_time:100936ms step_avg:92.86ms
step:1088/1670 train_time:101027ms step_avg:92.86ms
step:1089/1670 train_time:101119ms step_avg:92.86ms
step:1090/1670 train_time:101211ms step_avg:92.85ms
step:1091/1670 train_time:101302ms step_avg:92.85ms
step:1092/1670 train_time:101395ms step_avg:92.85ms
step:1093/1670 train_time:101488ms step_avg:92.85ms
step:1094/1670 train_time:101580ms step_avg:92.85ms
step:1095/1670 train_time:101674ms step_avg:92.85ms
step:1096/1670 train_time:101768ms step_avg:92.85ms
step:1097/1670 train_time:101861ms step_avg:92.85ms
step:1098/1670 train_time:101954ms step_avg:92.85ms
step:1099/1670 train_time:102045ms step_avg:92.85ms
step:1100/1670 train_time:102137ms step_avg:92.85ms
step:1101/1670 train_time:102228ms step_avg:92.85ms
step:1102/1670 train_time:102320ms step_avg:92.85ms
step:1103/1670 train_time:102412ms step_avg:92.85ms
step:1104/1670 train_time:102505ms step_avg:92.85ms
step:1105/1670 train_time:102598ms step_avg:92.85ms
step:1106/1670 train_time:102692ms step_avg:92.85ms
step:1107/1670 train_time:102784ms step_avg:92.85ms
step:1108/1670 train_time:102877ms step_avg:92.85ms
step:1109/1670 train_time:102970ms step_avg:92.85ms
step:1110/1670 train_time:103062ms step_avg:92.85ms
step:1111/1670 train_time:103154ms step_avg:92.85ms
step:1112/1670 train_time:103246ms step_avg:92.85ms
step:1113/1670 train_time:103336ms step_avg:92.84ms
step:1114/1670 train_time:103429ms step_avg:92.84ms
step:1115/1670 train_time:103715ms step_avg:93.02ms
step:1116/1670 train_time:103788ms step_avg:93.00ms
step:1117/1670 train_time:103879ms step_avg:93.00ms
step:1118/1670 train_time:103971ms step_avg:93.00ms
step:1119/1670 train_time:104063ms step_avg:93.00ms
step:1120/1670 train_time:104154ms step_avg:93.00ms
step:1121/1670 train_time:104246ms step_avg:92.99ms
step:1122/1670 train_time:104338ms step_avg:92.99ms
step:1123/1670 train_time:104430ms step_avg:92.99ms
step:1124/1670 train_time:104522ms step_avg:92.99ms
step:1125/1670 train_time:104622ms step_avg:93.00ms
step:1125/1670 val_loss:3.4164 train_time:104722ms step_avg:93.09ms
step:1126/1670 train_time:104740ms step_avg:93.02ms
step:1127/1670 train_time:104821ms step_avg:93.01ms
step:1128/1670 train_time:104921ms step_avg:93.01ms
step:1129/1670 train_time:105015ms step_avg:93.02ms
step:1130/1670 train_time:105108ms step_avg:93.02ms
step:1131/1670 train_time:105199ms step_avg:93.01ms
step:1132/1670 train_time:105292ms step_avg:93.01ms
step:1133/1670 train_time:105383ms step_avg:93.01ms
step:1134/1670 train_time:105475ms step_avg:93.01ms
step:1135/1670 train_time:105567ms step_avg:93.01ms
step:1136/1670 train_time:105662ms step_avg:93.01ms
step:1137/1670 train_time:105758ms step_avg:93.01ms
step:1138/1670 train_time:105855ms step_avg:93.02ms
step:1139/1670 train_time:105950ms step_avg:93.02ms
step:1140/1670 train_time:106044ms step_avg:93.02ms
step:1141/1670 train_time:106136ms step_avg:93.02ms
step:1142/1670 train_time:106228ms step_avg:93.02ms
step:1143/1670 train_time:106320ms step_avg:93.02ms
step:1144/1670 train_time:106413ms step_avg:93.02ms
step:1145/1670 train_time:106504ms step_avg:93.02ms
step:1146/1670 train_time:106597ms step_avg:93.02ms
step:1147/1670 train_time:106691ms step_avg:93.02ms
step:1148/1670 train_time:106785ms step_avg:93.02ms
step:1149/1670 train_time:106882ms step_avg:93.02ms
step:1150/1670 train_time:106976ms step_avg:93.02ms
step:1151/1670 train_time:107070ms step_avg:93.02ms
step:1152/1670 train_time:107162ms step_avg:93.02ms
step:1153/1670 train_time:107254ms step_avg:93.02ms
step:1154/1670 train_time:107348ms step_avg:93.02ms
step:1155/1670 train_time:107440ms step_avg:93.02ms
step:1156/1670 train_time:107532ms step_avg:93.02ms
step:1157/1670 train_time:107625ms step_avg:93.02ms
step:1158/1670 train_time:107718ms step_avg:93.02ms
step:1159/1670 train_time:107816ms step_avg:93.02ms
step:1160/1670 train_time:107911ms step_avg:93.03ms
step:1161/1670 train_time:108004ms step_avg:93.03ms
step:1162/1670 train_time:108098ms step_avg:93.03ms
step:1163/1670 train_time:108191ms step_avg:93.03ms
step:1164/1670 train_time:108283ms step_avg:93.03ms
step:1165/1670 train_time:108375ms step_avg:93.03ms
step:1166/1670 train_time:108468ms step_avg:93.03ms
step:1167/1670 train_time:108560ms step_avg:93.03ms
step:1168/1670 train_time:108654ms step_avg:93.03ms
step:1169/1670 train_time:108749ms step_avg:93.03ms
step:1170/1670 train_time:108843ms step_avg:93.03ms
step:1171/1670 train_time:108937ms step_avg:93.03ms
step:1172/1670 train_time:109032ms step_avg:93.03ms
step:1173/1670 train_time:109124ms step_avg:93.03ms
step:1174/1670 train_time:109218ms step_avg:93.03ms
step:1175/1670 train_time:109311ms step_avg:93.03ms
step:1176/1670 train_time:109403ms step_avg:93.03ms
step:1177/1670 train_time:109496ms step_avg:93.03ms
step:1178/1670 train_time:109588ms step_avg:93.03ms
step:1179/1670 train_time:109682ms step_avg:93.03ms
step:1180/1670 train_time:109775ms step_avg:93.03ms
step:1181/1670 train_time:109868ms step_avg:93.03ms
step:1182/1670 train_time:109962ms step_avg:93.03ms
step:1183/1670 train_time:110056ms step_avg:93.03ms
step:1184/1670 train_time:110149ms step_avg:93.03ms
step:1185/1670 train_time:110242ms step_avg:93.03ms
step:1186/1670 train_time:110335ms step_avg:93.03ms
step:1187/1670 train_time:110427ms step_avg:93.03ms
step:1188/1670 train_time:110520ms step_avg:93.03ms
step:1189/1670 train_time:110614ms step_avg:93.03ms
step:1190/1670 train_time:110707ms step_avg:93.03ms
step:1191/1670 train_time:110799ms step_avg:93.03ms
step:1192/1670 train_time:110893ms step_avg:93.03ms
step:1193/1670 train_time:110987ms step_avg:93.03ms
step:1194/1670 train_time:111081ms step_avg:93.03ms
step:1195/1670 train_time:111174ms step_avg:93.03ms
step:1196/1670 train_time:111267ms step_avg:93.03ms
step:1197/1670 train_time:111359ms step_avg:93.03ms
step:1198/1670 train_time:111452ms step_avg:93.03ms
step:1199/1670 train_time:111544ms step_avg:93.03ms
step:1200/1670 train_time:111637ms step_avg:93.03ms
step:1201/1670 train_time:111731ms step_avg:93.03ms
step:1202/1670 train_time:111824ms step_avg:93.03ms
step:1203/1670 train_time:111918ms step_avg:93.03ms
step:1204/1670 train_time:112012ms step_avg:93.03ms
step:1205/1670 train_time:112104ms step_avg:93.03ms
step:1206/1670 train_time:112198ms step_avg:93.03ms
step:1207/1670 train_time:112292ms step_avg:93.03ms
step:1208/1670 train_time:112385ms step_avg:93.03ms
step:1209/1670 train_time:112477ms step_avg:93.03ms
step:1210/1670 train_time:112570ms step_avg:93.03ms
step:1211/1670 train_time:112664ms step_avg:93.03ms
step:1212/1670 train_time:112757ms step_avg:93.03ms
step:1213/1670 train_time:112850ms step_avg:93.03ms
step:1214/1670 train_time:112944ms step_avg:93.03ms
step:1215/1670 train_time:113037ms step_avg:93.03ms
step:1216/1670 train_time:113131ms step_avg:93.04ms
step:1217/1670 train_time:113224ms step_avg:93.04ms
step:1218/1670 train_time:113317ms step_avg:93.04ms
step:1219/1670 train_time:113410ms step_avg:93.04ms
step:1220/1670 train_time:113503ms step_avg:93.04ms
step:1221/1670 train_time:113596ms step_avg:93.04ms
step:1222/1670 train_time:113689ms step_avg:93.03ms
step:1223/1670 train_time:113782ms step_avg:93.04ms
step:1224/1670 train_time:113875ms step_avg:93.04ms
step:1225/1670 train_time:113968ms step_avg:93.04ms
step:1226/1670 train_time:114061ms step_avg:93.04ms
step:1227/1670 train_time:114155ms step_avg:93.04ms
step:1228/1670 train_time:114248ms step_avg:93.04ms
step:1229/1670 train_time:114341ms step_avg:93.04ms
step:1230/1670 train_time:114434ms step_avg:93.04ms
step:1231/1670 train_time:114527ms step_avg:93.04ms
step:1232/1670 train_time:114620ms step_avg:93.04ms
step:1233/1670 train_time:114713ms step_avg:93.04ms
step:1234/1670 train_time:114807ms step_avg:93.04ms
step:1235/1670 train_time:114900ms step_avg:93.04ms
step:1236/1670 train_time:114993ms step_avg:93.04ms
step:1237/1670 train_time:115086ms step_avg:93.04ms
step:1238/1670 train_time:115179ms step_avg:93.04ms
step:1239/1670 train_time:115272ms step_avg:93.04ms
step:1240/1670 train_time:115365ms step_avg:93.04ms
step:1241/1670 train_time:115457ms step_avg:93.04ms
step:1242/1670 train_time:115550ms step_avg:93.04ms
step:1243/1670 train_time:115643ms step_avg:93.04ms
step:1244/1670 train_time:115737ms step_avg:93.04ms
step:1245/1670 train_time:115831ms step_avg:93.04ms
step:1246/1670 train_time:115924ms step_avg:93.04ms
step:1247/1670 train_time:116018ms step_avg:93.04ms
step:1248/1670 train_time:116111ms step_avg:93.04ms
step:1249/1670 train_time:116203ms step_avg:93.04ms
step:1250/1670 train_time:116297ms step_avg:93.04ms
step:1250/1670 val_loss:3.3777 train_time:116390ms step_avg:93.11ms
step:1251/1670 train_time:116407ms step_avg:93.05ms
step:1252/1670 train_time:116484ms step_avg:93.04ms
step:1253/1670 train_time:116578ms step_avg:93.04ms
step:1254/1670 train_time:116671ms step_avg:93.04ms
step:1255/1670 train_time:116763ms step_avg:93.04ms
step:1256/1670 train_time:116855ms step_avg:93.04ms
step:1257/1670 train_time:116948ms step_avg:93.04ms
step:1258/1670 train_time:117042ms step_avg:93.04ms
step:1259/1670 train_time:117135ms step_avg:93.04ms
step:1260/1670 train_time:117229ms step_avg:93.04ms
step:1261/1670 train_time:117322ms step_avg:93.04ms
step:1262/1670 train_time:117416ms step_avg:93.04ms
step:1263/1670 train_time:117510ms step_avg:93.04ms
step:1264/1670 train_time:117603ms step_avg:93.04ms
step:1265/1670 train_time:117696ms step_avg:93.04ms
step:1266/1670 train_time:117788ms step_avg:93.04ms
step:1267/1670 train_time:117881ms step_avg:93.04ms
step:1268/1670 train_time:117974ms step_avg:93.04ms
step:1269/1670 train_time:118066ms step_avg:93.04ms
step:1270/1670 train_time:118160ms step_avg:93.04ms
step:1271/1670 train_time:118253ms step_avg:93.04ms
step:1272/1670 train_time:118347ms step_avg:93.04ms
step:1273/1670 train_time:118441ms step_avg:93.04ms
step:1274/1670 train_time:118678ms step_avg:93.15ms
step:1275/1670 train_time:118770ms step_avg:93.15ms
step:1276/1670 train_time:118862ms step_avg:93.15ms
step:1277/1670 train_time:118954ms step_avg:93.15ms
step:1278/1670 train_time:119045ms step_avg:93.15ms
step:1279/1670 train_time:119137ms step_avg:93.15ms
step:1280/1670 train_time:119229ms step_avg:93.15ms
step:1281/1670 train_time:119321ms step_avg:93.15ms
step:1282/1670 train_time:119413ms step_avg:93.15ms
step:1283/1670 train_time:119505ms step_avg:93.14ms
step:1284/1670 train_time:119602ms step_avg:93.15ms
step:1285/1670 train_time:119699ms step_avg:93.15ms
step:1286/1670 train_time:119793ms step_avg:93.15ms
step:1287/1670 train_time:119886ms step_avg:93.15ms
step:1288/1670 train_time:119979ms step_avg:93.15ms
step:1289/1670 train_time:120071ms step_avg:93.15ms
step:1290/1670 train_time:120163ms step_avg:93.15ms
step:1291/1670 train_time:120256ms step_avg:93.15ms
step:1292/1670 train_time:120347ms step_avg:93.15ms
step:1293/1670 train_time:120441ms step_avg:93.15ms
step:1294/1670 train_time:120535ms step_avg:93.15ms
step:1295/1670 train_time:120630ms step_avg:93.15ms
step:1296/1670 train_time:120726ms step_avg:93.15ms
step:1297/1670 train_time:120820ms step_avg:93.15ms
step:1298/1670 train_time:120912ms step_avg:93.15ms
step:1299/1670 train_time:121006ms step_avg:93.15ms
step:1300/1670 train_time:121099ms step_avg:93.15ms
step:1301/1670 train_time:121191ms step_avg:93.15ms
step:1302/1670 train_time:121283ms step_avg:93.15ms
step:1303/1670 train_time:121375ms step_avg:93.15ms
step:1304/1670 train_time:121467ms step_avg:93.15ms
step:1305/1670 train_time:121561ms step_avg:93.15ms
step:1306/1670 train_time:121655ms step_avg:93.15ms
step:1307/1670 train_time:121749ms step_avg:93.15ms
step:1308/1670 train_time:121842ms step_avg:93.15ms
step:1309/1670 train_time:121936ms step_avg:93.15ms
step:1310/1670 train_time:122029ms step_avg:93.15ms
step:1311/1670 train_time:122122ms step_avg:93.15ms
step:1312/1670 train_time:122214ms step_avg:93.15ms
step:1313/1670 train_time:122306ms step_avg:93.15ms
step:1314/1670 train_time:122398ms step_avg:93.15ms
step:1315/1670 train_time:122491ms step_avg:93.15ms
step:1316/1670 train_time:122586ms step_avg:93.15ms
step:1317/1670 train_time:122680ms step_avg:93.15ms
step:1318/1670 train_time:122774ms step_avg:93.15ms
step:1319/1670 train_time:122868ms step_avg:93.15ms
step:1320/1670 train_time:122962ms step_avg:93.15ms
step:1321/1670 train_time:123055ms step_avg:93.15ms
step:1322/1670 train_time:123147ms step_avg:93.15ms
step:1323/1670 train_time:123240ms step_avg:93.15ms
step:1324/1670 train_time:123333ms step_avg:93.15ms
step:1325/1670 train_time:123426ms step_avg:93.15ms
step:1326/1670 train_time:123520ms step_avg:93.15ms
step:1327/1670 train_time:123614ms step_avg:93.15ms
step:1328/1670 train_time:123707ms step_avg:93.15ms
step:1329/1670 train_time:123801ms step_avg:93.15ms
step:1330/1670 train_time:123895ms step_avg:93.15ms
step:1331/1670 train_time:123988ms step_avg:93.15ms
step:1332/1670 train_time:124080ms step_avg:93.15ms
step:1333/1670 train_time:124173ms step_avg:93.15ms
step:1334/1670 train_time:124267ms step_avg:93.15ms
step:1335/1670 train_time:124360ms step_avg:93.15ms
step:1336/1670 train_time:124453ms step_avg:93.15ms
step:1337/1670 train_time:124546ms step_avg:93.15ms
step:1338/1670 train_time:124639ms step_avg:93.15ms
step:1339/1670 train_time:124733ms step_avg:93.15ms
step:1340/1670 train_time:124828ms step_avg:93.15ms
step:1341/1670 train_time:124921ms step_avg:93.16ms
step:1342/1670 train_time:125014ms step_avg:93.16ms
step:1343/1670 train_time:125107ms step_avg:93.15ms
step:1344/1670 train_time:125199ms step_avg:93.15ms
step:1345/1670 train_time:125291ms step_avg:93.15ms
step:1346/1670 train_time:125384ms step_avg:93.15ms
step:1347/1670 train_time:125478ms step_avg:93.15ms
step:1348/1670 train_time:125571ms step_avg:93.15ms
step:1349/1670 train_time:125664ms step_avg:93.15ms
step:1350/1670 train_time:125757ms step_avg:93.15ms
step:1351/1670 train_time:125850ms step_avg:93.15ms
step:1352/1670 train_time:125945ms step_avg:93.15ms
step:1353/1670 train_time:126038ms step_avg:93.15ms
step:1354/1670 train_time:126132ms step_avg:93.15ms
step:1355/1670 train_time:126224ms step_avg:93.15ms
step:1356/1670 train_time:126317ms step_avg:93.15ms
step:1357/1670 train_time:126409ms step_avg:93.15ms
step:1358/1670 train_time:126503ms step_avg:93.15ms
step:1359/1670 train_time:126597ms step_avg:93.15ms
step:1360/1670 train_time:126689ms step_avg:93.15ms
step:1361/1670 train_time:126782ms step_avg:93.15ms
step:1362/1670 train_time:126876ms step_avg:93.15ms
step:1363/1670 train_time:126969ms step_avg:93.15ms
step:1364/1670 train_time:127064ms step_avg:93.16ms
step:1365/1670 train_time:127158ms step_avg:93.16ms
step:1366/1670 train_time:127250ms step_avg:93.15ms
step:1367/1670 train_time:127343ms step_avg:93.16ms
step:1368/1670 train_time:127436ms step_avg:93.15ms
step:1369/1670 train_time:127529ms step_avg:93.15ms
step:1370/1670 train_time:127623ms step_avg:93.16ms
step:1371/1670 train_time:127716ms step_avg:93.16ms
step:1372/1670 train_time:127809ms step_avg:93.16ms
step:1373/1670 train_time:127903ms step_avg:93.16ms
step:1374/1670 train_time:127997ms step_avg:93.16ms
step:1375/1670 train_time:128090ms step_avg:93.16ms
step:1375/1670 val_loss:3.3432 train_time:128184ms step_avg:93.22ms
step:1376/1670 train_time:128201ms step_avg:93.17ms
step:1377/1670 train_time:128280ms step_avg:93.16ms
step:1378/1670 train_time:128375ms step_avg:93.16ms
step:1379/1670 train_time:128468ms step_avg:93.16ms
step:1380/1670 train_time:128559ms step_avg:93.16ms
step:1381/1670 train_time:128652ms step_avg:93.16ms
step:1382/1670 train_time:128745ms step_avg:93.16ms
step:1383/1670 train_time:128838ms step_avg:93.16ms
step:1384/1670 train_time:128931ms step_avg:93.16ms
step:1385/1670 train_time:129024ms step_avg:93.16ms
step:1386/1670 train_time:129118ms step_avg:93.16ms
step:1387/1670 train_time:129214ms step_avg:93.16ms
step:1388/1670 train_time:129309ms step_avg:93.16ms
step:1389/1670 train_time:129402ms step_avg:93.16ms
step:1390/1670 train_time:129495ms step_avg:93.16ms
step:1391/1670 train_time:129588ms step_avg:93.16ms
step:1392/1670 train_time:129680ms step_avg:93.16ms
step:1393/1670 train_time:129774ms step_avg:93.16ms
step:1394/1670 train_time:129867ms step_avg:93.16ms
step:1395/1670 train_time:129959ms step_avg:93.16ms
step:1396/1670 train_time:130052ms step_avg:93.16ms
step:1397/1670 train_time:130147ms step_avg:93.16ms
step:1398/1670 train_time:130240ms step_avg:93.16ms
step:1399/1670 train_time:130335ms step_avg:93.16ms
step:1400/1670 train_time:130428ms step_avg:93.16ms
step:1401/1670 train_time:130520ms step_avg:93.16ms
step:1402/1670 train_time:130615ms step_avg:93.16ms
step:1403/1670 train_time:130709ms step_avg:93.16ms
step:1404/1670 train_time:130801ms step_avg:93.16ms
step:1405/1670 train_time:130893ms step_avg:93.16ms
step:1406/1670 train_time:130986ms step_avg:93.16ms
step:1407/1670 train_time:131079ms step_avg:93.16ms
step:1408/1670 train_time:131174ms step_avg:93.16ms
step:1409/1670 train_time:131269ms step_avg:93.16ms
step:1410/1670 train_time:131361ms step_avg:93.16ms
step:1411/1670 train_time:131455ms step_avg:93.16ms
step:1412/1670 train_time:131549ms step_avg:93.16ms
step:1413/1670 train_time:131642ms step_avg:93.16ms
step:1414/1670 train_time:131737ms step_avg:93.17ms
step:1415/1670 train_time:131829ms step_avg:93.17ms
step:1416/1670 train_time:131921ms step_avg:93.16ms
step:1417/1670 train_time:132015ms step_avg:93.17ms
step:1418/1670 train_time:132108ms step_avg:93.17ms
step:1419/1670 train_time:132201ms step_avg:93.17ms
step:1420/1670 train_time:132294ms step_avg:93.16ms
step:1421/1670 train_time:132386ms step_avg:93.16ms
step:1422/1670 train_time:132479ms step_avg:93.16ms
step:1423/1670 train_time:132572ms step_avg:93.16ms
step:1424/1670 train_time:132666ms step_avg:93.16ms
step:1425/1670 train_time:132759ms step_avg:93.16ms
step:1426/1670 train_time:132852ms step_avg:93.16ms
step:1427/1670 train_time:132945ms step_avg:93.16ms
step:1428/1670 train_time:133038ms step_avg:93.16ms
step:1429/1670 train_time:133132ms step_avg:93.16ms
step:1430/1670 train_time:133224ms step_avg:93.16ms
step:1431/1670 train_time:133318ms step_avg:93.16ms
step:1432/1670 train_time:133411ms step_avg:93.16ms
step:1433/1670 train_time:133504ms step_avg:93.16ms
step:1434/1670 train_time:133598ms step_avg:93.16ms
step:1435/1670 train_time:133691ms step_avg:93.16ms
step:1436/1670 train_time:133784ms step_avg:93.16ms
step:1437/1670 train_time:133877ms step_avg:93.16ms
step:1438/1670 train_time:133969ms step_avg:93.16ms
step:1439/1670 train_time:134062ms step_avg:93.16ms
step:1440/1670 train_time:134155ms step_avg:93.16ms
step:1441/1670 train_time:134248ms step_avg:93.16ms
step:1442/1670 train_time:134341ms step_avg:93.16ms
step:1443/1670 train_time:134436ms step_avg:93.16ms
step:1444/1670 train_time:134530ms step_avg:93.16ms
step:1445/1670 train_time:134623ms step_avg:93.16ms
step:1446/1670 train_time:134717ms step_avg:93.17ms
step:1447/1670 train_time:134810ms step_avg:93.17ms
step:1448/1670 train_time:134903ms step_avg:93.16ms
step:1449/1670 train_time:134996ms step_avg:93.16ms
step:1450/1670 train_time:135089ms step_avg:93.16ms
step:1451/1670 train_time:135181ms step_avg:93.16ms
step:1452/1670 train_time:135274ms step_avg:93.16ms
step:1453/1670 train_time:135367ms step_avg:93.16ms
step:1454/1670 train_time:135460ms step_avg:93.16ms
step:1455/1670 train_time:135554ms step_avg:93.16ms
step:1456/1670 train_time:135647ms step_avg:93.16ms
step:1457/1670 train_time:135741ms step_avg:93.16ms
step:1458/1670 train_time:135836ms step_avg:93.17ms
step:1459/1670 train_time:135928ms step_avg:93.17ms
step:1460/1670 train_time:136022ms step_avg:93.17ms
step:1461/1670 train_time:136117ms step_avg:93.17ms
step:1462/1670 train_time:136211ms step_avg:93.17ms
step:1463/1670 train_time:136304ms step_avg:93.17ms
step:1464/1670 train_time:136397ms step_avg:93.17ms
step:1465/1670 train_time:136490ms step_avg:93.17ms
step:1466/1670 train_time:136583ms step_avg:93.17ms
step:1467/1670 train_time:136676ms step_avg:93.17ms
step:1468/1670 train_time:136769ms step_avg:93.17ms
step:1469/1670 train_time:136862ms step_avg:93.17ms
step:1470/1670 train_time:136955ms step_avg:93.17ms
step:1471/1670 train_time:137047ms step_avg:93.17ms
step:1472/1670 train_time:137140ms step_avg:93.17ms
step:1473/1670 train_time:137234ms step_avg:93.17ms
step:1474/1670 train_time:137327ms step_avg:93.17ms
step:1475/1670 train_time:137421ms step_avg:93.17ms
step:1476/1670 train_time:137516ms step_avg:93.17ms
step:1477/1670 train_time:137610ms step_avg:93.17ms
step:1478/1670 train_time:137702ms step_avg:93.17ms
step:1479/1670 train_time:137796ms step_avg:93.17ms
step:1480/1670 train_time:137889ms step_avg:93.17ms
step:1481/1670 train_time:137982ms step_avg:93.17ms
step:1482/1670 train_time:138076ms step_avg:93.17ms
step:1483/1670 train_time:138169ms step_avg:93.17ms
step:1484/1670 train_time:138261ms step_avg:93.17ms
step:1485/1670 train_time:138512ms step_avg:93.27ms
step:1486/1670 train_time:138583ms step_avg:93.26ms
step:1487/1670 train_time:138676ms step_avg:93.26ms
step:1488/1670 train_time:138769ms step_avg:93.26ms
step:1489/1670 train_time:138860ms step_avg:93.26ms
step:1490/1670 train_time:138952ms step_avg:93.26ms
step:1491/1670 train_time:139044ms step_avg:93.26ms
step:1492/1670 train_time:139136ms step_avg:93.25ms
step:1493/1670 train_time:139228ms step_avg:93.25ms
step:1494/1670 train_time:139319ms step_avg:93.25ms
step:1495/1670 train_time:139418ms step_avg:93.26ms
step:1496/1670 train_time:139516ms step_avg:93.26ms
step:1497/1670 train_time:139610ms step_avg:93.26ms
step:1498/1670 train_time:139703ms step_avg:93.26ms
step:1499/1670 train_time:139796ms step_avg:93.26ms
step:1500/1670 train_time:139888ms step_avg:93.26ms
step:1500/1670 val_loss:3.3132 train_time:139982ms step_avg:93.32ms
step:1501/1670 train_time:140000ms step_avg:93.27ms
step:1502/1670 train_time:140078ms step_avg:93.26ms
step:1503/1670 train_time:140171ms step_avg:93.26ms
step:1504/1670 train_time:140262ms step_avg:93.26ms
step:1505/1670 train_time:140355ms step_avg:93.26ms
step:1506/1670 train_time:140446ms step_avg:93.26ms
step:1507/1670 train_time:140540ms step_avg:93.26ms
step:1508/1670 train_time:140634ms step_avg:93.26ms
step:1509/1670 train_time:140727ms step_avg:93.26ms
step:1510/1670 train_time:140821ms step_avg:93.26ms
step:1511/1670 train_time:140917ms step_avg:93.26ms
step:1512/1670 train_time:141011ms step_avg:93.26ms
step:1513/1670 train_time:141104ms step_avg:93.26ms
step:1514/1670 train_time:141197ms step_avg:93.26ms
step:1515/1670 train_time:141289ms step_avg:93.26ms
step:1516/1670 train_time:141381ms step_avg:93.26ms
step:1517/1670 train_time:141473ms step_avg:93.26ms
step:1518/1670 train_time:141567ms step_avg:93.26ms
step:1519/1670 train_time:141660ms step_avg:93.26ms
step:1520/1670 train_time:141754ms step_avg:93.26ms
step:1521/1670 train_time:141847ms step_avg:93.26ms
step:1522/1670 train_time:141942ms step_avg:93.26ms
step:1523/1670 train_time:142036ms step_avg:93.26ms
step:1524/1670 train_time:142130ms step_avg:93.26ms
step:1525/1670 train_time:142223ms step_avg:93.26ms
step:1526/1670 train_time:142317ms step_avg:93.26ms
step:1527/1670 train_time:142409ms step_avg:93.26ms
step:1528/1670 train_time:142501ms step_avg:93.26ms
step:1529/1670 train_time:142593ms step_avg:93.26ms
step:1530/1670 train_time:142686ms step_avg:93.26ms
step:1531/1670 train_time:142780ms step_avg:93.26ms
step:1532/1670 train_time:142874ms step_avg:93.26ms
step:1533/1670 train_time:142968ms step_avg:93.26ms
step:1534/1670 train_time:143062ms step_avg:93.26ms
step:1535/1670 train_time:143156ms step_avg:93.26ms
step:1536/1670 train_time:143248ms step_avg:93.26ms
step:1537/1670 train_time:143342ms step_avg:93.26ms
step:1538/1670 train_time:143435ms step_avg:93.26ms
step:1539/1670 train_time:143528ms step_avg:93.26ms
step:1540/1670 train_time:143621ms step_avg:93.26ms
step:1541/1670 train_time:143716ms step_avg:93.26ms
step:1542/1670 train_time:143809ms step_avg:93.26ms
step:1543/1670 train_time:143903ms step_avg:93.26ms
step:1544/1670 train_time:143997ms step_avg:93.26ms
step:1545/1670 train_time:144091ms step_avg:93.26ms
step:1546/1670 train_time:144183ms step_avg:93.26ms
step:1547/1670 train_time:144277ms step_avg:93.26ms
step:1548/1670 train_time:144369ms step_avg:93.26ms
step:1549/1670 train_time:144462ms step_avg:93.26ms
step:1550/1670 train_time:144555ms step_avg:93.26ms
step:1551/1670 train_time:144649ms step_avg:93.26ms
step:1552/1670 train_time:144742ms step_avg:93.26ms
step:1553/1670 train_time:144836ms step_avg:93.26ms
step:1554/1670 train_time:144931ms step_avg:93.26ms
step:1555/1670 train_time:145024ms step_avg:93.26ms
step:1556/1670 train_time:145116ms step_avg:93.26ms
step:1557/1670 train_time:145210ms step_avg:93.26ms
step:1558/1670 train_time:145302ms step_avg:93.26ms
step:1559/1670 train_time:145395ms step_avg:93.26ms
step:1560/1670 train_time:145488ms step_avg:93.26ms
step:1561/1670 train_time:145582ms step_avg:93.26ms
step:1562/1670 train_time:145675ms step_avg:93.26ms
step:1563/1670 train_time:145768ms step_avg:93.26ms
step:1564/1670 train_time:145862ms step_avg:93.26ms
step:1565/1670 train_time:145955ms step_avg:93.26ms
step:1566/1670 train_time:146049ms step_avg:93.26ms
step:1567/1670 train_time:146142ms step_avg:93.26ms
step:1568/1670 train_time:146236ms step_avg:93.26ms
step:1569/1670 train_time:146328ms step_avg:93.26ms
step:1570/1670 train_time:146423ms step_avg:93.26ms
step:1571/1670 train_time:146516ms step_avg:93.26ms
step:1572/1670 train_time:146609ms step_avg:93.26ms
step:1573/1670 train_time:146702ms step_avg:93.26ms
step:1574/1670 train_time:146796ms step_avg:93.26ms
step:1575/1670 train_time:146889ms step_avg:93.26ms
step:1576/1670 train_time:146983ms step_avg:93.26ms
step:1577/1670 train_time:147076ms step_avg:93.26ms
step:1578/1670 train_time:147168ms step_avg:93.26ms
step:1579/1670 train_time:147263ms step_avg:93.26ms
step:1580/1670 train_time:147357ms step_avg:93.26ms
step:1581/1670 train_time:147450ms step_avg:93.26ms
step:1582/1670 train_time:147543ms step_avg:93.26ms
step:1583/1670 train_time:147637ms step_avg:93.26ms
step:1584/1670 train_time:147730ms step_avg:93.26ms
step:1585/1670 train_time:147822ms step_avg:93.26ms
step:1586/1670 train_time:147916ms step_avg:93.26ms
step:1587/1670 train_time:148010ms step_avg:93.26ms
step:1588/1670 train_time:148102ms step_avg:93.26ms
step:1589/1670 train_time:148196ms step_avg:93.26ms
step:1590/1670 train_time:148288ms step_avg:93.26ms
step:1591/1670 train_time:148383ms step_avg:93.26ms
step:1592/1670 train_time:148476ms step_avg:93.26ms
step:1593/1670 train_time:148569ms step_avg:93.26ms
step:1594/1670 train_time:148663ms step_avg:93.26ms
step:1595/1670 train_time:148757ms step_avg:93.26ms
step:1596/1670 train_time:148849ms step_avg:93.26ms
step:1597/1670 train_time:148943ms step_avg:93.26ms
step:1598/1670 train_time:149036ms step_avg:93.26ms
step:1599/1670 train_time:149129ms step_avg:93.26ms
step:1600/1670 train_time:149222ms step_avg:93.26ms
step:1601/1670 train_time:149316ms step_avg:93.26ms
step:1602/1670 train_time:149408ms step_avg:93.26ms
step:1603/1670 train_time:149501ms step_avg:93.26ms
step:1604/1670 train_time:149595ms step_avg:93.26ms
step:1605/1670 train_time:149687ms step_avg:93.26ms
step:1606/1670 train_time:149780ms step_avg:93.26ms
step:1607/1670 train_time:149873ms step_avg:93.26ms
step:1608/1670 train_time:149966ms step_avg:93.26ms
step:1609/1670 train_time:150060ms step_avg:93.26ms
step:1610/1670 train_time:150154ms step_avg:93.26ms
step:1611/1670 train_time:150247ms step_avg:93.26ms
step:1612/1670 train_time:150341ms step_avg:93.26ms
step:1613/1670 train_time:150434ms step_avg:93.26ms
step:1614/1670 train_time:150528ms step_avg:93.26ms
step:1615/1670 train_time:150622ms step_avg:93.26ms
step:1616/1670 train_time:150716ms step_avg:93.26ms
step:1617/1670 train_time:150808ms step_avg:93.26ms
step:1618/1670 train_time:150901ms step_avg:93.26ms
step:1619/1670 train_time:150994ms step_avg:93.26ms
step:1620/1670 train_time:151087ms step_avg:93.26ms
step:1621/1670 train_time:151180ms step_avg:93.26ms
step:1622/1670 train_time:151273ms step_avg:93.26ms
step:1623/1670 train_time:151366ms step_avg:93.26ms
step:1624/1670 train_time:151459ms step_avg:93.26ms
step:1625/1670 train_time:151553ms step_avg:93.26ms
step:1625/1670 val_loss:3.2883 train_time:151647ms step_avg:93.32ms
step:1626/1670 train_time:151664ms step_avg:93.27ms
step:1627/1670 train_time:151739ms step_avg:93.26ms
step:1628/1670 train_time:151831ms step_avg:93.26ms
step:1629/1670 train_time:151925ms step_avg:93.26ms
step:1630/1670 train_time:152017ms step_avg:93.26ms
step:1631/1670 train_time:152110ms step_avg:93.26ms
step:1632/1670 train_time:152203ms step_avg:93.26ms
step:1633/1670 train_time:152296ms step_avg:93.26ms
step:1634/1670 train_time:152390ms step_avg:93.26ms
step:1635/1670 train_time:152483ms step_avg:93.26ms
step:1636/1670 train_time:152578ms step_avg:93.26ms
step:1637/1670 train_time:152674ms step_avg:93.26ms
step:1638/1670 train_time:152767ms step_avg:93.26ms
step:1639/1670 train_time:152860ms step_avg:93.26ms
step:1640/1670 train_time:152952ms step_avg:93.26ms
step:1641/1670 train_time:153045ms step_avg:93.26ms
step:1642/1670 train_time:153138ms step_avg:93.26ms
step:1643/1670 train_time:153231ms step_avg:93.26ms
step:1644/1670 train_time:153326ms step_avg:93.26ms
step:1645/1670 train_time:153420ms step_avg:93.26ms
step:1646/1670 train_time:153513ms step_avg:93.26ms
step:1647/1670 train_time:153607ms step_avg:93.26ms
step:1648/1670 train_time:153701ms step_avg:93.26ms
step:1649/1670 train_time:153794ms step_avg:93.27ms
step:1650/1670 train_time:153890ms step_avg:93.27ms
step:1651/1670 train_time:153982ms step_avg:93.27ms
step:1652/1670 train_time:154075ms step_avg:93.27ms
step:1653/1670 train_time:154168ms step_avg:93.27ms
step:1654/1670 train_time:154261ms step_avg:93.27ms
step:1655/1670 train_time:154354ms step_avg:93.27ms
step:1656/1670 train_time:154448ms step_avg:93.27ms
step:1657/1670 train_time:154542ms step_avg:93.27ms
step:1658/1670 train_time:154635ms step_avg:93.27ms
step:1659/1670 train_time:154729ms step_avg:93.27ms
step:1660/1670 train_time:154821ms step_avg:93.27ms
step:1661/1670 train_time:154914ms step_avg:93.27ms
step:1662/1670 train_time:155008ms step_avg:93.27ms
step:1663/1670 train_time:155101ms step_avg:93.27ms
step:1664/1670 train_time:155194ms step_avg:93.27ms
step:1665/1670 train_time:155288ms step_avg:93.27ms
step:1666/1670 train_time:155382ms step_avg:93.27ms
step:1667/1670 train_time:155474ms step_avg:93.27ms
step:1668/1670 train_time:155569ms step_avg:93.27ms
step:1669/1670 train_time:155662ms step_avg:93.27ms
step:1670/1670 train_time:155755ms step_avg:93.27ms
step:1670/1670 val_loss:3.2797 train_time:156019ms step_avg:93.42ms
peak memory allocated: 32002 MiB reserved: 46294 MiB

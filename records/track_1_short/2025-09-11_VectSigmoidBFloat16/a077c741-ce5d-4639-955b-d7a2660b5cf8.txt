import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(
    x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(
    g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99


def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]


@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(
        pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M
    )

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr,
    C_ptr,
    M,
    K,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr,
    C_ptr,
    M,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    alpha,
    beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (
        offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c
    )
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out


@torch.compile(
    dynamic=False, fullgraph=True
)  # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer


class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(
        self,
        params,
        lr: float = 1e-3,
        betas: tuple[float, float] = (0.9, 0.999),
        eps: float = 1e-8,
        weight_decay: float = 0.01,
    ):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(
                        grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True
                    ).get_future()
                )
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group["betas"]
            eps = group["eps"]
            wd = group["weight_decay"]
            params = group["params"]
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size : (rank + 1) * rank_size]
                lr = group["lr"] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(
                    g_slice, g_slice, value=1 - beta2
                )
                # bias corrections
                bias1 = 1 - beta1**t
                bias2 = 1 - beta2**t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(
                    dist.all_gather_into_tensor(
                        p, p_slice, async_op=True
                    ).get_future()
                )
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model


def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(
        self,
        in_features: int,
        out_features: int,
        use_fp8=False,
        x_s=1.0,
        w_s=1.0,
        grad_s=1.0,
    ):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (
            self.in_features**-0.5
        )  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3**0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(
                _x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s
            )[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    rotary_cos: torch.Tensor
    rotary_sin: torch.Tensor
    attn_scale: float


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        rotary_cos, rotary_sin = attn_args.rotary_cos, attn_args.rotary_sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = (
            attn_args.seqlens,
            attn_args.attn_scale,
            attn_args.bm_size,
        )

        q, k, v = (
            F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x))
            .view(B, T, 3 * self.num_heads, self.head_dim)
            .chunk(3, dim=-2)
        )
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = (
            rotary(q, rotary_cos, rotary_sin),
            rotary(k, rotary_cos, rotary_sin),
        )
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(
                v
            )  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = (
            args.train_max_seq_len
            if self.training
            else (args.val_batch_size // (grad_accum_steps * world_size))
        )

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(
            q[0],
            k[0],
            v[0],
            cu_seqlens_q=seqlens,
            cu_seqlens_k=seqlens,
            max_seqlen_q=max_len,
            max_seqlen_k=max_len,
            causal=True,
            softmax_scale=attn_scale,
            window_size=(bm_size, 0),
        )
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(
            self.attn_gate(x[..., : self.attn_gate.weight.size(-1)])
        ).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(
            B, T, self.num_heads * self.head_dim
        )  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x
        ).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = (
            CausalSelfAttention(dim, head_dim, num_heads)
            if layer_idx != 7
            else None
        )
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(
        self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs
    ):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model


def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        num_layers: int,
        num_heads: int,
        head_dim: int,
        model_dim: int,
        max_seq_len: int,
    ):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList(
            [nn.Embedding(vocab_size, model_dim) for _ in range(3)]
        )
        self.blocks = nn.ModuleList(
            [
                Block(model_dim, head_dim, num_heads, i)
                for i in range(num_layers)
            ]
        )
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(
            model_dim,
            vocab_size,
            use_fp8=use_fp8,
            x_s=(model_dim**0.5) / 448,
            w_s=2**-9,
            grad_s=1 / 448,
        )
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.ones(pad),
                ]
            )
        )
        self.max_seq_len = max_seq_len
        self.setup_yarn(head_dim)
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.0
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.0
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def setup_yarn(self, head_dim: int):
        # store single copy of rotary tensors
        angular_freq = (1 / 1024) ** torch.linspace(
            0, 1, steps=head_dim // 4, dtype=torch.float32
        )
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat(
            [angular_freq, angular_freq.new_zeros(head_dim // 4)]
        )
        t = torch.arange(self.max_seq_len, dtype=torch.float32)
        theta = torch.outer(t, angular_freq)
        self.rotary_cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.rotary_sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq

        # scale attention factor f in attn=softmax(f*qk) logarithmically with window size @classiclarryd
        windows = list(
            dict.fromkeys(list(args.ws_schedule) + [args.ws_validate])
        )
        scale_factors = [
            0.2 * math.log(curr / prev) + 1
            for prev, curr in zip(windows[:-1], windows[1:])
        ]
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        attn_scales = list(
            accumulate([0.1] + scale_factors, lambda acc, factor: acc * factor)
        )
        self.attn_scales = dict(zip(windows, attn_scales))

    def apply_yarn(
        self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32
    ):
        rotations = (
            args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        )
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp(
            (rotations - alpha) / (beta - alpha), 0, 1
        )
        self.angular_freq *= scaling_factor + interpolation_weight * (
            1 - scaling_factor
        )
        t = torch.arange(
            self.max_seq_len,
            dtype=torch.float32,
            device=self.angular_freq.device,
        )
        theta = torch.outer(t, self.angular_freq)
        self.rotary_cos.copy_(theta.cos())
        self.rotary_sin.copy_(theta.sin())

    def forward(
        self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int
    ):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = (
            [ve[0], ve[1], ve[2]]
            + [None] * (len(self.blocks) - 6)
            + [ve[0], ve[1], ve[2]]
        )
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        bm_sizes = [
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
        ]
        assert len(bm_sizes) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]).to(
            torch.bfloat16
        )  # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[: (len(self.blocks) // 2)]
        lambdas = self.scalars[
            1 * len(self.blocks) : 3 * len(self.blocks)
        ].view(-1, 2)
        sa_lambdas = self.scalars[
            3 * len(self.blocks) : 5 * len(self.blocks)
        ].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                rotary_cos=self.rotary_cos,
                rotary_sin=self.rotary_sin,
                attn_scale=self.attn_scales[ws],
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(
            logits.view(-1, logits.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader


def _load_data_shard(file: Path):
    header = torch.from_file(
        str(file), False, 256, dtype=torch.int32
    )  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(
            num_tokens, dtype=torch.uint16, pin_memory=True
        )  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(
            tokens.numpy()
        )  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, (
            "number of tokens read does not match header"
        )
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1):
        # Precompute BOS positions once per shard
        self.size = tokens.numel()
        self.bos_idx = (
            (tokens == BOS_ID)
            .nonzero(as_tuple=True)[0]
            .to(torch.int64)
            .cpu()
            .numpy()
        )
        self.i = 0
        self.world_size = world_size

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(
                        f"Insufficient BOS ahead of position {cur}; hit tail of shard."
                    )
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(
                    self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                    cur + max_seq_len,
                    cur + num_tokens_local - cur_len + 1,
                )
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx

        return starts, ends


def distributed_data_generator(
    filename_pattern: str,
    num_tokens: int,
    max_seq_len: int,
    grad_accum_steps: int = 1,
    align_to_bos: bool = True,
):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, (
        "Batch size must be divisible by world size"
    )
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(
            f"No files found for pattern: {filename_pattern}"
        )

    file_iter = iter(
        files
    )  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    finder = BOSFinder(tokens, world_size=world_size) if align_to_bos else None
    pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(
            num_tokens_local // 300, n=128
        )  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(
                    num_tokens_local, max_seq_len
                )
                start_idxs, end_idxs = (
                    torch.tensor(seq_starts[rank]),
                    torch.tensor(seq_ends[rank]),
                )
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens = _load_data_shard(next(file_iter))
                finder = BOSFinder(tokens, world_size=world_size)
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= (
                1  # last document was too long to account for _targets offset
            )
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(
                tokens
            ):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local : pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(
                num_tokens_local,
            )
            _targets = buf[1:].view(
                num_tokens_local,
            )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1 : len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(
                device="cuda", dtype=torch.int32, non_blocking=True
            ),
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, (
                "Num tokens must be divisible by world size"
            )
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main


@dataclass
class Hyperparameters:
    # data
    train_files: str = (
        "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    )
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1670  # number of iterations to run
    cooldown_frac: int = (
        0.5  # fraction of training spent cooling down the learning rate
    )
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = (
        125  # every how many steps to evaluate val loss? 0 for only at the end
    )
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws @classiclarryd


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = rank == 0  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(
    f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}"
)
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import

    return subprocess.run(
        ["nvidia-smi"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    ).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size)
    // (grad_accum_steps * world_size),
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [
    p
    for n, p in model.blocks.named_parameters()
    if p.ndim >= 2 and "embed" not in n
]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(
    hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)  # save the initial state
train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    ws = args.ws_schedule[
        step % len(args.ws_schedule)
    ]  # each window size is a new graph, need to warm up each
    model(inputs, targets, cum_seqlens, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws = get_ws(0)
for step in range(train_steps + 1):
    last_step = step == train_steps
    new_ws = get_ws(step)
    if new_ws != ws:
        model.apply_yarn(ws, new_ws)
        ws = new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (
        args.val_loss_every > 0 and step % args.val_loss_every == 0
    ):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(
            args.val_files,
            args.val_batch_size,
            -1,
            grad_accum_steps=grad_accum_steps,
            align_to_bos=False,
        )
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True,
        )
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(
                step=step,
                code=code,
                model=model.state_dict(),
                optimizers=[opt.state_dict() for opt in optimizers],
            )
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1)  # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (
        time.perf_counter() - t0
    )
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True,
    )

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.11 (main, Sep  2 2025, 14:20:58) [Clang 20.1.4 ]
Running PyTorch 2.9.0.dev20250718+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 11 09:51:37 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                  Off |
| N/A   40C    P0            125W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                  Off |
| N/A   45C    P0            134W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:65:00.0 Off |                  Off |
| N/A   46C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:68:00.0 Off |                  Off |
| N/A   37C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:85:00.0 Off |                  Off |
| N/A   37C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:86:00.0 Off |                  Off |
| N/A   45C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:E5:00.0 Off |                  Off |
| N/A   45C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E8:00.0 Off |                  Off |
| N/A   41C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1670 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/1670 train_time:294ms step_avg:294.48ms
step:2/1670 train_time:313ms step_avg:156.32ms
step:3/1670 train_time:382ms step_avg:127.30ms
step:4/1670 train_time:471ms step_avg:117.80ms
step:5/1670 train_time:561ms step_avg:112.24ms
step:6/1670 train_time:652ms step_avg:108.63ms
step:7/1670 train_time:742ms step_avg:106.00ms
step:8/1670 train_time:833ms step_avg:104.12ms
step:9/1670 train_time:923ms step_avg:102.59ms
step:10/1670 train_time:1015ms step_avg:101.48ms
step:11/1670 train_time:1106ms step_avg:100.52ms
step:12/1670 train_time:1200ms step_avg:99.98ms
step:13/1670 train_time:1295ms step_avg:99.59ms
step:14/1670 train_time:1387ms step_avg:99.06ms
step:15/1670 train_time:1479ms step_avg:98.57ms
step:16/1670 train_time:1571ms step_avg:98.17ms
step:17/1670 train_time:1661ms step_avg:97.73ms
step:18/1670 train_time:1754ms step_avg:97.44ms
step:19/1670 train_time:1844ms step_avg:97.07ms
step:20/1670 train_time:1936ms step_avg:96.82ms
step:21/1670 train_time:2028ms step_avg:96.59ms
step:22/1670 train_time:2121ms step_avg:96.40ms
step:23/1670 train_time:2215ms step_avg:96.30ms
step:24/1670 train_time:2308ms step_avg:96.16ms
step:25/1670 train_time:2400ms step_avg:96.00ms
step:26/1670 train_time:2491ms step_avg:95.82ms
step:27/1670 train_time:2582ms step_avg:95.64ms
step:28/1670 train_time:2676ms step_avg:95.58ms
step:29/1670 train_time:2767ms step_avg:95.43ms
step:30/1670 train_time:2859ms step_avg:95.31ms
step:31/1670 train_time:2950ms step_avg:95.16ms
step:32/1670 train_time:3041ms step_avg:95.04ms
step:33/1670 train_time:3133ms step_avg:94.94ms
step:34/1670 train_time:3225ms step_avg:94.85ms
step:35/1670 train_time:3318ms step_avg:94.81ms
step:36/1670 train_time:3410ms step_avg:94.72ms
step:37/1670 train_time:3501ms step_avg:94.63ms
step:38/1670 train_time:3592ms step_avg:94.54ms
step:39/1670 train_time:3684ms step_avg:94.46ms
step:40/1670 train_time:3775ms step_avg:94.37ms
step:41/1670 train_time:3866ms step_avg:94.30ms
step:42/1670 train_time:3958ms step_avg:94.25ms
step:43/1670 train_time:4051ms step_avg:94.20ms
step:44/1670 train_time:4143ms step_avg:94.15ms
step:45/1670 train_time:4235ms step_avg:94.12ms
step:46/1670 train_time:4327ms step_avg:94.06ms
step:47/1670 train_time:4419ms step_avg:94.02ms
step:48/1670 train_time:4512ms step_avg:93.99ms
step:49/1670 train_time:4603ms step_avg:93.93ms
step:50/1670 train_time:4694ms step_avg:93.88ms
step:51/1670 train_time:4786ms step_avg:93.84ms
step:52/1670 train_time:4877ms step_avg:93.80ms
step:53/1670 train_time:4968ms step_avg:93.74ms
step:54/1670 train_time:5060ms step_avg:93.71ms
step:55/1670 train_time:5152ms step_avg:93.68ms
step:56/1670 train_time:5243ms step_avg:93.63ms
step:57/1670 train_time:5335ms step_avg:93.60ms
step:58/1670 train_time:5426ms step_avg:93.56ms
step:59/1670 train_time:5519ms step_avg:93.54ms
step:60/1670 train_time:5611ms step_avg:93.52ms
step:61/1670 train_time:5702ms step_avg:93.48ms
step:62/1670 train_time:5794ms step_avg:93.45ms
step:63/1670 train_time:5885ms step_avg:93.42ms
step:64/1670 train_time:5977ms step_avg:93.39ms
step:65/1670 train_time:6068ms step_avg:93.35ms
step:66/1670 train_time:6159ms step_avg:93.32ms
step:67/1670 train_time:6251ms step_avg:93.30ms
step:68/1670 train_time:6342ms step_avg:93.26ms
step:69/1670 train_time:6435ms step_avg:93.25ms
step:70/1670 train_time:6527ms step_avg:93.24ms
step:71/1670 train_time:6618ms step_avg:93.22ms
step:72/1670 train_time:6710ms step_avg:93.19ms
step:73/1670 train_time:6801ms step_avg:93.17ms
step:74/1670 train_time:6894ms step_avg:93.16ms
step:75/1670 train_time:6985ms step_avg:93.14ms
step:76/1670 train_time:7077ms step_avg:93.12ms
step:77/1670 train_time:7169ms step_avg:93.10ms
step:78/1670 train_time:7261ms step_avg:93.09ms
step:79/1670 train_time:7352ms step_avg:93.06ms
step:80/1670 train_time:7444ms step_avg:93.05ms
step:81/1670 train_time:7538ms step_avg:93.06ms
step:82/1670 train_time:7631ms step_avg:93.06ms
step:83/1670 train_time:7722ms step_avg:93.04ms
step:84/1670 train_time:7814ms step_avg:93.03ms
step:85/1670 train_time:7905ms step_avg:93.00ms
step:86/1670 train_time:7996ms step_avg:92.97ms
step:87/1670 train_time:8087ms step_avg:92.95ms
step:88/1670 train_time:8178ms step_avg:92.93ms
step:89/1670 train_time:8269ms step_avg:92.91ms
step:90/1670 train_time:8361ms step_avg:92.90ms
step:91/1670 train_time:8454ms step_avg:92.90ms
step:92/1670 train_time:8545ms step_avg:92.88ms
step:93/1670 train_time:8638ms step_avg:92.88ms
step:94/1670 train_time:8730ms step_avg:92.87ms
step:95/1670 train_time:8822ms step_avg:92.86ms
step:96/1670 train_time:8914ms step_avg:92.85ms
step:97/1670 train_time:9005ms step_avg:92.84ms
step:98/1670 train_time:9097ms step_avg:92.82ms
step:99/1670 train_time:9188ms step_avg:92.81ms
step:100/1670 train_time:9279ms step_avg:92.79ms
step:101/1670 train_time:9370ms step_avg:92.77ms
step:102/1670 train_time:9462ms step_avg:92.76ms
step:103/1670 train_time:9555ms step_avg:92.76ms
step:104/1670 train_time:9647ms step_avg:92.76ms
step:105/1670 train_time:9739ms step_avg:92.76ms
step:106/1670 train_time:9831ms step_avg:92.75ms
step:107/1670 train_time:9922ms step_avg:92.73ms
step:108/1670 train_time:10014ms step_avg:92.72ms
step:109/1670 train_time:10105ms step_avg:92.71ms
step:110/1670 train_time:10196ms step_avg:92.69ms
step:111/1670 train_time:10287ms step_avg:92.68ms
step:112/1670 train_time:10379ms step_avg:92.67ms
step:113/1670 train_time:10470ms step_avg:92.65ms
step:114/1670 train_time:10561ms step_avg:92.64ms
step:115/1670 train_time:10653ms step_avg:92.64ms
step:116/1670 train_time:10745ms step_avg:92.63ms
step:117/1670 train_time:10837ms step_avg:92.63ms
step:118/1670 train_time:10929ms step_avg:92.62ms
step:119/1670 train_time:11020ms step_avg:92.60ms
step:120/1670 train_time:11111ms step_avg:92.59ms
step:121/1670 train_time:11202ms step_avg:92.58ms
step:122/1670 train_time:11292ms step_avg:92.56ms
step:123/1670 train_time:11384ms step_avg:92.55ms
step:124/1670 train_time:11474ms step_avg:92.53ms
step:125/1670 train_time:11565ms step_avg:92.52ms
step:125/1670 val_loss:4.3026 train_time:11657ms step_avg:93.25ms
step:126/1670 train_time:11676ms step_avg:92.66ms
step:127/1670 train_time:11752ms step_avg:92.53ms
step:128/1670 train_time:11852ms step_avg:92.59ms
step:129/1670 train_time:11945ms step_avg:92.60ms
step:130/1670 train_time:12038ms step_avg:92.60ms
step:131/1670 train_time:12128ms step_avg:92.58ms
step:132/1670 train_time:12218ms step_avg:92.56ms
step:133/1670 train_time:12308ms step_avg:92.54ms
step:134/1670 train_time:12399ms step_avg:92.53ms
step:135/1670 train_time:12489ms step_avg:92.51ms
step:136/1670 train_time:12580ms step_avg:92.50ms
step:137/1670 train_time:12671ms step_avg:92.49ms
step:138/1670 train_time:12763ms step_avg:92.49ms
step:139/1670 train_time:12857ms step_avg:92.50ms
step:140/1670 train_time:12949ms step_avg:92.49ms
step:141/1670 train_time:13040ms step_avg:92.49ms
step:142/1670 train_time:13132ms step_avg:92.48ms
step:143/1670 train_time:13222ms step_avg:92.46ms
step:144/1670 train_time:13313ms step_avg:92.45ms
step:145/1670 train_time:13403ms step_avg:92.43ms
step:146/1670 train_time:13496ms step_avg:92.43ms
step:147/1670 train_time:13586ms step_avg:92.42ms
step:148/1670 train_time:13677ms step_avg:92.41ms
step:149/1670 train_time:13769ms step_avg:92.41ms
step:150/1670 train_time:13861ms step_avg:92.40ms
step:151/1670 train_time:13953ms step_avg:92.40ms
step:152/1670 train_time:14044ms step_avg:92.39ms
step:153/1670 train_time:14136ms step_avg:92.39ms
step:154/1670 train_time:14227ms step_avg:92.38ms
step:155/1670 train_time:14318ms step_avg:92.38ms
step:156/1670 train_time:14409ms step_avg:92.37ms
step:157/1670 train_time:14500ms step_avg:92.36ms
step:158/1670 train_time:14591ms step_avg:92.35ms
step:159/1670 train_time:14681ms step_avg:92.34ms
step:160/1670 train_time:14772ms step_avg:92.33ms
step:161/1670 train_time:14864ms step_avg:92.32ms
step:162/1670 train_time:14955ms step_avg:92.32ms
step:163/1670 train_time:15046ms step_avg:92.31ms
step:164/1670 train_time:15139ms step_avg:92.31ms
step:165/1670 train_time:15231ms step_avg:92.31ms
step:166/1670 train_time:15322ms step_avg:92.30ms
step:167/1670 train_time:15413ms step_avg:92.30ms
step:168/1670 train_time:15503ms step_avg:92.28ms
step:169/1670 train_time:15594ms step_avg:92.27ms
step:170/1670 train_time:15686ms step_avg:92.27ms
step:171/1670 train_time:15776ms step_avg:92.26ms
step:172/1670 train_time:15867ms step_avg:92.25ms
step:173/1670 train_time:15958ms step_avg:92.25ms
step:174/1670 train_time:16050ms step_avg:92.24ms
step:175/1670 train_time:16141ms step_avg:92.24ms
step:176/1670 train_time:16232ms step_avg:92.23ms
step:177/1670 train_time:16323ms step_avg:92.22ms
step:178/1670 train_time:16415ms step_avg:92.22ms
step:179/1670 train_time:16505ms step_avg:92.21ms
step:180/1670 train_time:16597ms step_avg:92.21ms
step:181/1670 train_time:16688ms step_avg:92.20ms
step:182/1670 train_time:16779ms step_avg:92.19ms
step:183/1670 train_time:16870ms step_avg:92.18ms
step:184/1670 train_time:16961ms step_avg:92.18ms
step:185/1670 train_time:17052ms step_avg:92.18ms
step:186/1670 train_time:17143ms step_avg:92.17ms
step:187/1670 train_time:17235ms step_avg:92.17ms
step:188/1670 train_time:17326ms step_avg:92.16ms
step:189/1670 train_time:17419ms step_avg:92.16ms
step:190/1670 train_time:17511ms step_avg:92.16ms
step:191/1670 train_time:17602ms step_avg:92.16ms
step:192/1670 train_time:17695ms step_avg:92.16ms
step:193/1670 train_time:17785ms step_avg:92.15ms
step:194/1670 train_time:17876ms step_avg:92.14ms
step:195/1670 train_time:17967ms step_avg:92.14ms
step:196/1670 train_time:18058ms step_avg:92.13ms
step:197/1670 train_time:18149ms step_avg:92.13ms
step:198/1670 train_time:18240ms step_avg:92.12ms
step:199/1670 train_time:18332ms step_avg:92.12ms
step:200/1670 train_time:18422ms step_avg:92.11ms
step:201/1670 train_time:18514ms step_avg:92.11ms
step:202/1670 train_time:18604ms step_avg:92.10ms
step:203/1670 train_time:18696ms step_avg:92.10ms
step:204/1670 train_time:18787ms step_avg:92.09ms
step:205/1670 train_time:18878ms step_avg:92.09ms
step:206/1670 train_time:18969ms step_avg:92.08ms
step:207/1670 train_time:19061ms step_avg:92.08ms
step:208/1670 train_time:19151ms step_avg:92.07ms
step:209/1670 train_time:19242ms step_avg:92.07ms
step:210/1670 train_time:19333ms step_avg:92.06ms
step:211/1670 train_time:19424ms step_avg:92.06ms
step:212/1670 train_time:19515ms step_avg:92.05ms
step:213/1670 train_time:19765ms step_avg:92.79ms
step:214/1670 train_time:19833ms step_avg:92.68ms
step:215/1670 train_time:19922ms step_avg:92.66ms
step:216/1670 train_time:20013ms step_avg:92.65ms
step:217/1670 train_time:20102ms step_avg:92.64ms
step:218/1670 train_time:20193ms step_avg:92.63ms
step:219/1670 train_time:20283ms step_avg:92.62ms
step:220/1670 train_time:20374ms step_avg:92.61ms
step:221/1670 train_time:20464ms step_avg:92.60ms
step:222/1670 train_time:20554ms step_avg:92.59ms
step:223/1670 train_time:20647ms step_avg:92.59ms
step:224/1670 train_time:20742ms step_avg:92.60ms
step:225/1670 train_time:20838ms step_avg:92.62ms
step:226/1670 train_time:20930ms step_avg:92.61ms
step:227/1670 train_time:21021ms step_avg:92.60ms
step:228/1670 train_time:21112ms step_avg:92.59ms
step:229/1670 train_time:21202ms step_avg:92.58ms
step:230/1670 train_time:21292ms step_avg:92.57ms
step:231/1670 train_time:21382ms step_avg:92.56ms
step:232/1670 train_time:21472ms step_avg:92.55ms
step:233/1670 train_time:21563ms step_avg:92.55ms
step:234/1670 train_time:21656ms step_avg:92.55ms
step:235/1670 train_time:21749ms step_avg:92.55ms
step:236/1670 train_time:21842ms step_avg:92.55ms
step:237/1670 train_time:21935ms step_avg:92.55ms
step:238/1670 train_time:22025ms step_avg:92.54ms
step:239/1670 train_time:22116ms step_avg:92.54ms
step:240/1670 train_time:22207ms step_avg:92.53ms
step:241/1670 train_time:22298ms step_avg:92.52ms
step:242/1670 train_time:22388ms step_avg:92.51ms
step:243/1670 train_time:22479ms step_avg:92.50ms
step:244/1670 train_time:22569ms step_avg:92.50ms
step:245/1670 train_time:22661ms step_avg:92.49ms
step:246/1670 train_time:22752ms step_avg:92.49ms
step:247/1670 train_time:22844ms step_avg:92.49ms
step:248/1670 train_time:22936ms step_avg:92.48ms
step:249/1670 train_time:23027ms step_avg:92.48ms
step:250/1670 train_time:23118ms step_avg:92.47ms
step:250/1670 val_loss:3.9615 train_time:23208ms step_avg:92.83ms
step:251/1670 train_time:23228ms step_avg:92.54ms
step:252/1670 train_time:23301ms step_avg:92.46ms
step:253/1670 train_time:23393ms step_avg:92.46ms
step:254/1670 train_time:23484ms step_avg:92.46ms
step:255/1670 train_time:23574ms step_avg:92.45ms
step:256/1670 train_time:23664ms step_avg:92.44ms
step:257/1670 train_time:23754ms step_avg:92.43ms
step:258/1670 train_time:23845ms step_avg:92.42ms
step:259/1670 train_time:23936ms step_avg:92.42ms
step:260/1670 train_time:24026ms step_avg:92.41ms
step:261/1670 train_time:24119ms step_avg:92.41ms
step:262/1670 train_time:24213ms step_avg:92.42ms
step:263/1670 train_time:24305ms step_avg:92.42ms
step:264/1670 train_time:24398ms step_avg:92.42ms
step:265/1670 train_time:24491ms step_avg:92.42ms
step:266/1670 train_time:24581ms step_avg:92.41ms
step:267/1670 train_time:24671ms step_avg:92.40ms
step:268/1670 train_time:24761ms step_avg:92.39ms
step:269/1670 train_time:24852ms step_avg:92.39ms
step:270/1670 train_time:24942ms step_avg:92.38ms
step:271/1670 train_time:25033ms step_avg:92.37ms
step:272/1670 train_time:25125ms step_avg:92.37ms
step:273/1670 train_time:25217ms step_avg:92.37ms
step:274/1670 train_time:25309ms step_avg:92.37ms
step:275/1670 train_time:25402ms step_avg:92.37ms
step:276/1670 train_time:25493ms step_avg:92.37ms
step:277/1670 train_time:25584ms step_avg:92.36ms
step:278/1670 train_time:25676ms step_avg:92.36ms
step:279/1670 train_time:25765ms step_avg:92.35ms
step:280/1670 train_time:25856ms step_avg:92.34ms
step:281/1670 train_time:25947ms step_avg:92.34ms
step:282/1670 train_time:26039ms step_avg:92.34ms
step:283/1670 train_time:26131ms step_avg:92.33ms
step:284/1670 train_time:26223ms step_avg:92.33ms
step:285/1670 train_time:26315ms step_avg:92.33ms
step:286/1670 train_time:26405ms step_avg:92.33ms
step:287/1670 train_time:26498ms step_avg:92.33ms
step:288/1670 train_time:26590ms step_avg:92.33ms
step:289/1670 train_time:26682ms step_avg:92.33ms
step:290/1670 train_time:26774ms step_avg:92.32ms
step:291/1670 train_time:26865ms step_avg:92.32ms
step:292/1670 train_time:26956ms step_avg:92.31ms
step:293/1670 train_time:27046ms step_avg:92.31ms
step:294/1670 train_time:27138ms step_avg:92.31ms
step:295/1670 train_time:27229ms step_avg:92.30ms
step:296/1670 train_time:27320ms step_avg:92.30ms
step:297/1670 train_time:27412ms step_avg:92.30ms
step:298/1670 train_time:27504ms step_avg:92.30ms
step:299/1670 train_time:27594ms step_avg:92.29ms
step:300/1670 train_time:27687ms step_avg:92.29ms
step:301/1670 train_time:27777ms step_avg:92.28ms
step:302/1670 train_time:27869ms step_avg:92.28ms
step:303/1670 train_time:27959ms step_avg:92.27ms
step:304/1670 train_time:28049ms step_avg:92.27ms
step:305/1670 train_time:28141ms step_avg:92.27ms
step:306/1670 train_time:28231ms step_avg:92.26ms
step:307/1670 train_time:28323ms step_avg:92.26ms
step:308/1670 train_time:28414ms step_avg:92.25ms
step:309/1670 train_time:28506ms step_avg:92.25ms
step:310/1670 train_time:28597ms step_avg:92.25ms
step:311/1670 train_time:28689ms step_avg:92.25ms
step:312/1670 train_time:28779ms step_avg:92.24ms
step:313/1670 train_time:28872ms step_avg:92.24ms
step:314/1670 train_time:28962ms step_avg:92.23ms
step:315/1670 train_time:29053ms step_avg:92.23ms
step:316/1670 train_time:29144ms step_avg:92.23ms
step:317/1670 train_time:29234ms step_avg:92.22ms
step:318/1670 train_time:29326ms step_avg:92.22ms
step:319/1670 train_time:29417ms step_avg:92.22ms
step:320/1670 train_time:29508ms step_avg:92.21ms
step:321/1670 train_time:29600ms step_avg:92.21ms
step:322/1670 train_time:29693ms step_avg:92.21ms
step:323/1670 train_time:29785ms step_avg:92.21ms
step:324/1670 train_time:29876ms step_avg:92.21ms
step:325/1670 train_time:29967ms step_avg:92.20ms
step:326/1670 train_time:30057ms step_avg:92.20ms
step:327/1670 train_time:30149ms step_avg:92.20ms
step:328/1670 train_time:30240ms step_avg:92.20ms
step:329/1670 train_time:30332ms step_avg:92.19ms
step:330/1670 train_time:30423ms step_avg:92.19ms
step:331/1670 train_time:30514ms step_avg:92.19ms
step:332/1670 train_time:30606ms step_avg:92.19ms
step:333/1670 train_time:30697ms step_avg:92.18ms
step:334/1670 train_time:30789ms step_avg:92.18ms
step:335/1670 train_time:30880ms step_avg:92.18ms
step:336/1670 train_time:30972ms step_avg:92.18ms
step:337/1670 train_time:31063ms step_avg:92.17ms
step:338/1670 train_time:31153ms step_avg:92.17ms
step:339/1670 train_time:31244ms step_avg:92.17ms
step:340/1670 train_time:31336ms step_avg:92.16ms
step:341/1670 train_time:31425ms step_avg:92.16ms
step:342/1670 train_time:31516ms step_avg:92.15ms
step:343/1670 train_time:31607ms step_avg:92.15ms
step:344/1670 train_time:31698ms step_avg:92.14ms
step:345/1670 train_time:31790ms step_avg:92.15ms
step:346/1670 train_time:31883ms step_avg:92.15ms
step:347/1670 train_time:31974ms step_avg:92.14ms
step:348/1670 train_time:32065ms step_avg:92.14ms
step:349/1670 train_time:32155ms step_avg:92.14ms
step:350/1670 train_time:32246ms step_avg:92.13ms
step:351/1670 train_time:32338ms step_avg:92.13ms
step:352/1670 train_time:32429ms step_avg:92.13ms
step:353/1670 train_time:32520ms step_avg:92.12ms
step:354/1670 train_time:32611ms step_avg:92.12ms
step:355/1670 train_time:32702ms step_avg:92.12ms
step:356/1670 train_time:32795ms step_avg:92.12ms
step:357/1670 train_time:32887ms step_avg:92.12ms
step:358/1670 train_time:32978ms step_avg:92.12ms
step:359/1670 train_time:33070ms step_avg:92.12ms
step:360/1670 train_time:33161ms step_avg:92.11ms
step:361/1670 train_time:33252ms step_avg:92.11ms
step:362/1670 train_time:33343ms step_avg:92.11ms
step:363/1670 train_time:33434ms step_avg:92.10ms
step:364/1670 train_time:33525ms step_avg:92.10ms
step:365/1670 train_time:33615ms step_avg:92.10ms
step:366/1670 train_time:33706ms step_avg:92.09ms
step:367/1670 train_time:33798ms step_avg:92.09ms
step:368/1670 train_time:33889ms step_avg:92.09ms
step:369/1670 train_time:33980ms step_avg:92.09ms
step:370/1670 train_time:34073ms step_avg:92.09ms
step:371/1670 train_time:34164ms step_avg:92.09ms
step:372/1670 train_time:34255ms step_avg:92.08ms
step:373/1670 train_time:34346ms step_avg:92.08ms
step:374/1670 train_time:34436ms step_avg:92.08ms
step:375/1670 train_time:34528ms step_avg:92.07ms
step:375/1670 val_loss:3.8087 train_time:34618ms step_avg:92.32ms
step:376/1670 train_time:34638ms step_avg:92.12ms
step:377/1670 train_time:34712ms step_avg:92.07ms
step:378/1670 train_time:34803ms step_avg:92.07ms
step:379/1670 train_time:34894ms step_avg:92.07ms
step:380/1670 train_time:34984ms step_avg:92.06ms
step:381/1670 train_time:35075ms step_avg:92.06ms
step:382/1670 train_time:35164ms step_avg:92.05ms
step:383/1670 train_time:35256ms step_avg:92.05ms
step:384/1670 train_time:35347ms step_avg:92.05ms
step:385/1670 train_time:35440ms step_avg:92.05ms
step:386/1670 train_time:35531ms step_avg:92.05ms
step:387/1670 train_time:35623ms step_avg:92.05ms
step:388/1670 train_time:35716ms step_avg:92.05ms
step:389/1670 train_time:35809ms step_avg:92.05ms
step:390/1670 train_time:35903ms step_avg:92.06ms
step:391/1670 train_time:35992ms step_avg:92.05ms
step:392/1670 train_time:36083ms step_avg:92.05ms
step:393/1670 train_time:36174ms step_avg:92.05ms
step:394/1670 train_time:36264ms step_avg:92.04ms
step:395/1670 train_time:36355ms step_avg:92.04ms
step:396/1670 train_time:36446ms step_avg:92.04ms
step:397/1670 train_time:36538ms step_avg:92.03ms
step:398/1670 train_time:36630ms step_avg:92.04ms
step:399/1670 train_time:36722ms step_avg:92.03ms
step:400/1670 train_time:36813ms step_avg:92.03ms
step:401/1670 train_time:36905ms step_avg:92.03ms
step:402/1670 train_time:36994ms step_avg:92.03ms
step:403/1670 train_time:37085ms step_avg:92.02ms
step:404/1670 train_time:37176ms step_avg:92.02ms
step:405/1670 train_time:37266ms step_avg:92.02ms
step:406/1670 train_time:37358ms step_avg:92.01ms
step:407/1670 train_time:37449ms step_avg:92.01ms
step:408/1670 train_time:37541ms step_avg:92.01ms
step:409/1670 train_time:37634ms step_avg:92.01ms
step:410/1670 train_time:37724ms step_avg:92.01ms
step:411/1670 train_time:37816ms step_avg:92.01ms
step:412/1670 train_time:37907ms step_avg:92.01ms
step:413/1670 train_time:37999ms step_avg:92.01ms
step:414/1670 train_time:38090ms step_avg:92.00ms
step:415/1670 train_time:38182ms step_avg:92.00ms
step:416/1670 train_time:38273ms step_avg:92.00ms
step:417/1670 train_time:38363ms step_avg:92.00ms
step:418/1670 train_time:38454ms step_avg:91.99ms
step:419/1670 train_time:38545ms step_avg:91.99ms
step:420/1670 train_time:38637ms step_avg:91.99ms
step:421/1670 train_time:38729ms step_avg:91.99ms
step:422/1670 train_time:38820ms step_avg:91.99ms
step:423/1670 train_time:38912ms step_avg:91.99ms
step:424/1670 train_time:39002ms step_avg:91.99ms
step:425/1670 train_time:39252ms step_avg:92.36ms
step:426/1670 train_time:39328ms step_avg:92.32ms
step:427/1670 train_time:39418ms step_avg:92.31ms
step:428/1670 train_time:39508ms step_avg:92.31ms
step:429/1670 train_time:39598ms step_avg:92.30ms
step:430/1670 train_time:39688ms step_avg:92.30ms
step:431/1670 train_time:39778ms step_avg:92.29ms
step:432/1670 train_time:39868ms step_avg:92.29ms
step:433/1670 train_time:39958ms step_avg:92.28ms
step:434/1670 train_time:40049ms step_avg:92.28ms
step:435/1670 train_time:40144ms step_avg:92.28ms
step:436/1670 train_time:40243ms step_avg:92.30ms
step:437/1670 train_time:40336ms step_avg:92.30ms
step:438/1670 train_time:40427ms step_avg:92.30ms
step:439/1670 train_time:40518ms step_avg:92.30ms
step:440/1670 train_time:40608ms step_avg:92.29ms
step:441/1670 train_time:40699ms step_avg:92.29ms
step:442/1670 train_time:40789ms step_avg:92.28ms
step:443/1670 train_time:40879ms step_avg:92.28ms
step:444/1670 train_time:40969ms step_avg:92.27ms
step:445/1670 train_time:41060ms step_avg:92.27ms
step:446/1670 train_time:41153ms step_avg:92.27ms
step:447/1670 train_time:41246ms step_avg:92.27ms
step:448/1670 train_time:41340ms step_avg:92.28ms
step:449/1670 train_time:41431ms step_avg:92.27ms
step:450/1670 train_time:41521ms step_avg:92.27ms
step:451/1670 train_time:41613ms step_avg:92.27ms
step:452/1670 train_time:41703ms step_avg:92.26ms
step:453/1670 train_time:41793ms step_avg:92.26ms
step:454/1670 train_time:41883ms step_avg:92.25ms
step:455/1670 train_time:41974ms step_avg:92.25ms
step:456/1670 train_time:42066ms step_avg:92.25ms
step:457/1670 train_time:42158ms step_avg:92.25ms
step:458/1670 train_time:42250ms step_avg:92.25ms
step:459/1670 train_time:42343ms step_avg:92.25ms
step:460/1670 train_time:42434ms step_avg:92.25ms
step:461/1670 train_time:42525ms step_avg:92.25ms
step:462/1670 train_time:42617ms step_avg:92.24ms
step:463/1670 train_time:42708ms step_avg:92.24ms
step:464/1670 train_time:42799ms step_avg:92.24ms
step:465/1670 train_time:42889ms step_avg:92.24ms
step:466/1670 train_time:42981ms step_avg:92.23ms
step:467/1670 train_time:43072ms step_avg:92.23ms
step:468/1670 train_time:43164ms step_avg:92.23ms
step:469/1670 train_time:43255ms step_avg:92.23ms
step:470/1670 train_time:43347ms step_avg:92.23ms
step:471/1670 train_time:43439ms step_avg:92.23ms
step:472/1670 train_time:43530ms step_avg:92.23ms
step:473/1670 train_time:43621ms step_avg:92.22ms
step:474/1670 train_time:43711ms step_avg:92.22ms
step:475/1670 train_time:43802ms step_avg:92.21ms
step:476/1670 train_time:43892ms step_avg:92.21ms
step:477/1670 train_time:43983ms step_avg:92.21ms
step:478/1670 train_time:44075ms step_avg:92.21ms
step:479/1670 train_time:44165ms step_avg:92.20ms
step:480/1670 train_time:44257ms step_avg:92.20ms
step:481/1670 train_time:44348ms step_avg:92.20ms
step:482/1670 train_time:44441ms step_avg:92.20ms
step:483/1670 train_time:44533ms step_avg:92.20ms
step:484/1670 train_time:44624ms step_avg:92.20ms
step:485/1670 train_time:44715ms step_avg:92.20ms
step:486/1670 train_time:44806ms step_avg:92.19ms
step:487/1670 train_time:44897ms step_avg:92.19ms
step:488/1670 train_time:44987ms step_avg:92.19ms
step:489/1670 train_time:45078ms step_avg:92.18ms
step:490/1670 train_time:45170ms step_avg:92.18ms
step:491/1670 train_time:45262ms step_avg:92.18ms
step:492/1670 train_time:45354ms step_avg:92.18ms
step:493/1670 train_time:45444ms step_avg:92.18ms
step:494/1670 train_time:45536ms step_avg:92.18ms
step:495/1670 train_time:45628ms step_avg:92.18ms
step:496/1670 train_time:45719ms step_avg:92.18ms
step:497/1670 train_time:45810ms step_avg:92.17ms
step:498/1670 train_time:45901ms step_avg:92.17ms
step:499/1670 train_time:45993ms step_avg:92.17ms
step:500/1670 train_time:46083ms step_avg:92.17ms
step:500/1670 val_loss:3.7114 train_time:46173ms step_avg:92.35ms
step:501/1670 train_time:46193ms step_avg:92.20ms
step:502/1670 train_time:46266ms step_avg:92.16ms
step:503/1670 train_time:46359ms step_avg:92.16ms
step:504/1670 train_time:46452ms step_avg:92.17ms
step:505/1670 train_time:46542ms step_avg:92.16ms
step:506/1670 train_time:46632ms step_avg:92.16ms
step:507/1670 train_time:46722ms step_avg:92.15ms
step:508/1670 train_time:46814ms step_avg:92.15ms
step:509/1670 train_time:46904ms step_avg:92.15ms
step:510/1670 train_time:46995ms step_avg:92.15ms
step:511/1670 train_time:47086ms step_avg:92.14ms
step:512/1670 train_time:47178ms step_avg:92.14ms
step:513/1670 train_time:47270ms step_avg:92.14ms
step:514/1670 train_time:47362ms step_avg:92.14ms
step:515/1670 train_time:47455ms step_avg:92.15ms
step:516/1670 train_time:47546ms step_avg:92.14ms
step:517/1670 train_time:47637ms step_avg:92.14ms
step:518/1670 train_time:47728ms step_avg:92.14ms
step:519/1670 train_time:47819ms step_avg:92.14ms
step:520/1670 train_time:47909ms step_avg:92.13ms
step:521/1670 train_time:48001ms step_avg:92.13ms
step:522/1670 train_time:48092ms step_avg:92.13ms
step:523/1670 train_time:48184ms step_avg:92.13ms
step:524/1670 train_time:48275ms step_avg:92.13ms
step:525/1670 train_time:48366ms step_avg:92.13ms
step:526/1670 train_time:48457ms step_avg:92.12ms
step:527/1670 train_time:48549ms step_avg:92.12ms
step:528/1670 train_time:48639ms step_avg:92.12ms
step:529/1670 train_time:48730ms step_avg:92.12ms
step:530/1670 train_time:48821ms step_avg:92.11ms
step:531/1670 train_time:48914ms step_avg:92.12ms
step:532/1670 train_time:49005ms step_avg:92.11ms
step:533/1670 train_time:49096ms step_avg:92.11ms
step:534/1670 train_time:49187ms step_avg:92.11ms
step:535/1670 train_time:49279ms step_avg:92.11ms
step:536/1670 train_time:49370ms step_avg:92.11ms
step:537/1670 train_time:49462ms step_avg:92.11ms
step:538/1670 train_time:49554ms step_avg:92.11ms
step:539/1670 train_time:49645ms step_avg:92.11ms
step:540/1670 train_time:49737ms step_avg:92.11ms
step:541/1670 train_time:49829ms step_avg:92.11ms
step:542/1670 train_time:49920ms step_avg:92.10ms
step:543/1670 train_time:50012ms step_avg:92.10ms
step:544/1670 train_time:50103ms step_avg:92.10ms
step:545/1670 train_time:50194ms step_avg:92.10ms
step:546/1670 train_time:50286ms step_avg:92.10ms
step:547/1670 train_time:50378ms step_avg:92.10ms
step:548/1670 train_time:50469ms step_avg:92.10ms
step:549/1670 train_time:50560ms step_avg:92.09ms
step:550/1670 train_time:50651ms step_avg:92.09ms
step:551/1670 train_time:50742ms step_avg:92.09ms
step:552/1670 train_time:50835ms step_avg:92.09ms
step:553/1670 train_time:50926ms step_avg:92.09ms
step:554/1670 train_time:51017ms step_avg:92.09ms
step:555/1670 train_time:51108ms step_avg:92.09ms
step:556/1670 train_time:51199ms step_avg:92.08ms
step:557/1670 train_time:51289ms step_avg:92.08ms
step:558/1670 train_time:51574ms step_avg:92.43ms
step:559/1670 train_time:51647ms step_avg:92.39ms
step:560/1670 train_time:51738ms step_avg:92.39ms
step:561/1670 train_time:51829ms step_avg:92.39ms
step:562/1670 train_time:51920ms step_avg:92.38ms
step:563/1670 train_time:52011ms step_avg:92.38ms
step:564/1670 train_time:52103ms step_avg:92.38ms
step:565/1670 train_time:52194ms step_avg:92.38ms
step:566/1670 train_time:52285ms step_avg:92.38ms
step:567/1670 train_time:52376ms step_avg:92.37ms
step:568/1670 train_time:52472ms step_avg:92.38ms
step:569/1670 train_time:52568ms step_avg:92.39ms
step:570/1670 train_time:52661ms step_avg:92.39ms
step:571/1670 train_time:52755ms step_avg:92.39ms
step:572/1670 train_time:52847ms step_avg:92.39ms
step:573/1670 train_time:52940ms step_avg:92.39ms
step:574/1670 train_time:53032ms step_avg:92.39ms
step:575/1670 train_time:53123ms step_avg:92.39ms
step:576/1670 train_time:53215ms step_avg:92.39ms
step:577/1670 train_time:53307ms step_avg:92.39ms
step:578/1670 train_time:53399ms step_avg:92.39ms
step:579/1670 train_time:53493ms step_avg:92.39ms
step:580/1670 train_time:53586ms step_avg:92.39ms
step:581/1670 train_time:53680ms step_avg:92.39ms
step:582/1670 train_time:53774ms step_avg:92.40ms
step:583/1670 train_time:53866ms step_avg:92.39ms
step:584/1670 train_time:53958ms step_avg:92.39ms
step:585/1670 train_time:54051ms step_avg:92.39ms
step:586/1670 train_time:54142ms step_avg:92.39ms
step:587/1670 train_time:54233ms step_avg:92.39ms
step:588/1670 train_time:54325ms step_avg:92.39ms
step:589/1670 train_time:54418ms step_avg:92.39ms
step:590/1670 train_time:54511ms step_avg:92.39ms
step:591/1670 train_time:54604ms step_avg:92.39ms
step:592/1670 train_time:54697ms step_avg:92.39ms
step:593/1670 train_time:54791ms step_avg:92.40ms
step:594/1670 train_time:54884ms step_avg:92.40ms
step:595/1670 train_time:54977ms step_avg:92.40ms
step:596/1670 train_time:55069ms step_avg:92.40ms
step:597/1670 train_time:55161ms step_avg:92.40ms
step:598/1670 train_time:55253ms step_avg:92.40ms
step:599/1670 train_time:55345ms step_avg:92.40ms
step:600/1670 train_time:55438ms step_avg:92.40ms
step:601/1670 train_time:55531ms step_avg:92.40ms
step:602/1670 train_time:55624ms step_avg:92.40ms
step:603/1670 train_time:55718ms step_avg:92.40ms
step:604/1670 train_time:55811ms step_avg:92.40ms
step:605/1670 train_time:55903ms step_avg:92.40ms
step:606/1670 train_time:55995ms step_avg:92.40ms
step:607/1670 train_time:56087ms step_avg:92.40ms
step:608/1670 train_time:56179ms step_avg:92.40ms
step:609/1670 train_time:56271ms step_avg:92.40ms
step:610/1670 train_time:56362ms step_avg:92.40ms
step:611/1670 train_time:56456ms step_avg:92.40ms
step:612/1670 train_time:56549ms step_avg:92.40ms
step:613/1670 train_time:56641ms step_avg:92.40ms
step:614/1670 train_time:56735ms step_avg:92.40ms
step:615/1670 train_time:56827ms step_avg:92.40ms
step:616/1670 train_time:56920ms step_avg:92.40ms
step:617/1670 train_time:57012ms step_avg:92.40ms
step:618/1670 train_time:57104ms step_avg:92.40ms
step:619/1670 train_time:57196ms step_avg:92.40ms
step:620/1670 train_time:57288ms step_avg:92.40ms
step:621/1670 train_time:57380ms step_avg:92.40ms
step:622/1670 train_time:57472ms step_avg:92.40ms
step:623/1670 train_time:57564ms step_avg:92.40ms
step:624/1670 train_time:57659ms step_avg:92.40ms
step:625/1670 train_time:57752ms step_avg:92.40ms
step:625/1670 val_loss:3.6108 train_time:57844ms step_avg:92.55ms
step:626/1670 train_time:57864ms step_avg:92.44ms
step:627/1670 train_time:57943ms step_avg:92.41ms
step:628/1670 train_time:58042ms step_avg:92.42ms
step:629/1670 train_time:58137ms step_avg:92.43ms
step:630/1670 train_time:58229ms step_avg:92.43ms
step:631/1670 train_time:58320ms step_avg:92.42ms
step:632/1670 train_time:58411ms step_avg:92.42ms
step:633/1670 train_time:58502ms step_avg:92.42ms
step:634/1670 train_time:58593ms step_avg:92.42ms
step:635/1670 train_time:58684ms step_avg:92.42ms
step:636/1670 train_time:58776ms step_avg:92.41ms
step:637/1670 train_time:58868ms step_avg:92.42ms
step:638/1670 train_time:58966ms step_avg:92.42ms
step:639/1670 train_time:59188ms step_avg:92.63ms
step:640/1670 train_time:59276ms step_avg:92.62ms
step:641/1670 train_time:59367ms step_avg:92.62ms
step:642/1670 train_time:59458ms step_avg:92.61ms
step:643/1670 train_time:59549ms step_avg:92.61ms
step:644/1670 train_time:59641ms step_avg:92.61ms
step:645/1670 train_time:59732ms step_avg:92.61ms
step:646/1670 train_time:59824ms step_avg:92.61ms
step:647/1670 train_time:59915ms step_avg:92.60ms
step:648/1670 train_time:60006ms step_avg:92.60ms
step:649/1670 train_time:60102ms step_avg:92.61ms
step:650/1670 train_time:60198ms step_avg:92.61ms
step:651/1670 train_time:60292ms step_avg:92.61ms
step:652/1670 train_time:60385ms step_avg:92.61ms
step:653/1670 train_time:60477ms step_avg:92.61ms
step:654/1670 train_time:60568ms step_avg:92.61ms
step:655/1670 train_time:60660ms step_avg:92.61ms
step:656/1670 train_time:60752ms step_avg:92.61ms
step:657/1670 train_time:60844ms step_avg:92.61ms
step:658/1670 train_time:60936ms step_avg:92.61ms
step:659/1670 train_time:61029ms step_avg:92.61ms
step:660/1670 train_time:61125ms step_avg:92.61ms
step:661/1670 train_time:61220ms step_avg:92.62ms
step:662/1670 train_time:61313ms step_avg:92.62ms
step:663/1670 train_time:61406ms step_avg:92.62ms
step:664/1670 train_time:61499ms step_avg:92.62ms
step:665/1670 train_time:61591ms step_avg:92.62ms
step:666/1670 train_time:61683ms step_avg:92.62ms
step:667/1670 train_time:61774ms step_avg:92.61ms
step:668/1670 train_time:61865ms step_avg:92.61ms
step:669/1670 train_time:61958ms step_avg:92.61ms
step:670/1670 train_time:62051ms step_avg:92.61ms
step:671/1670 train_time:62146ms step_avg:92.62ms
step:672/1670 train_time:62240ms step_avg:92.62ms
step:673/1670 train_time:62332ms step_avg:92.62ms
step:674/1670 train_time:62425ms step_avg:92.62ms
step:675/1670 train_time:62518ms step_avg:92.62ms
step:676/1670 train_time:62610ms step_avg:92.62ms
step:677/1670 train_time:62703ms step_avg:92.62ms
step:678/1670 train_time:62794ms step_avg:92.62ms
step:679/1670 train_time:62886ms step_avg:92.62ms
step:680/1670 train_time:62978ms step_avg:92.61ms
step:681/1670 train_time:63070ms step_avg:92.61ms
step:682/1670 train_time:63164ms step_avg:92.62ms
step:683/1670 train_time:63257ms step_avg:92.62ms
step:684/1670 train_time:63349ms step_avg:92.62ms
step:685/1670 train_time:63443ms step_avg:92.62ms
step:686/1670 train_time:63535ms step_avg:92.62ms
step:687/1670 train_time:63627ms step_avg:92.62ms
step:688/1670 train_time:63720ms step_avg:92.62ms
step:689/1670 train_time:63812ms step_avg:92.62ms
step:690/1670 train_time:63904ms step_avg:92.61ms
step:691/1670 train_time:63995ms step_avg:92.61ms
step:692/1670 train_time:64087ms step_avg:92.61ms
step:693/1670 train_time:64181ms step_avg:92.61ms
step:694/1670 train_time:64274ms step_avg:92.61ms
step:695/1670 train_time:64367ms step_avg:92.61ms
step:696/1670 train_time:64459ms step_avg:92.61ms
step:697/1670 train_time:64552ms step_avg:92.61ms
step:698/1670 train_time:64645ms step_avg:92.61ms
step:699/1670 train_time:64737ms step_avg:92.61ms
step:700/1670 train_time:64829ms step_avg:92.61ms
step:701/1670 train_time:64921ms step_avg:92.61ms
step:702/1670 train_time:65014ms step_avg:92.61ms
step:703/1670 train_time:65105ms step_avg:92.61ms
step:704/1670 train_time:65198ms step_avg:92.61ms
step:705/1670 train_time:65291ms step_avg:92.61ms
step:706/1670 train_time:65385ms step_avg:92.61ms
step:707/1670 train_time:65478ms step_avg:92.61ms
step:708/1670 train_time:65570ms step_avg:92.61ms
step:709/1670 train_time:65663ms step_avg:92.61ms
step:710/1670 train_time:65756ms step_avg:92.61ms
step:711/1670 train_time:65848ms step_avg:92.61ms
step:712/1670 train_time:65940ms step_avg:92.61ms
step:713/1670 train_time:66032ms step_avg:92.61ms
step:714/1670 train_time:66125ms step_avg:92.61ms
step:715/1670 train_time:66218ms step_avg:92.61ms
step:716/1670 train_time:66310ms step_avg:92.61ms
step:717/1670 train_time:66403ms step_avg:92.61ms
step:718/1670 train_time:66495ms step_avg:92.61ms
step:719/1670 train_time:66587ms step_avg:92.61ms
step:720/1670 train_time:66680ms step_avg:92.61ms
step:721/1670 train_time:66772ms step_avg:92.61ms
step:722/1670 train_time:66865ms step_avg:92.61ms
step:723/1670 train_time:66958ms step_avg:92.61ms
step:724/1670 train_time:67050ms step_avg:92.61ms
step:725/1670 train_time:67143ms step_avg:92.61ms
step:726/1670 train_time:67235ms step_avg:92.61ms
step:727/1670 train_time:67328ms step_avg:92.61ms
step:728/1670 train_time:67420ms step_avg:92.61ms
step:729/1670 train_time:67512ms step_avg:92.61ms
step:730/1670 train_time:67605ms step_avg:92.61ms
step:731/1670 train_time:67697ms step_avg:92.61ms
step:732/1670 train_time:67789ms step_avg:92.61ms
step:733/1670 train_time:67881ms step_avg:92.61ms
step:734/1670 train_time:67974ms step_avg:92.61ms
step:735/1670 train_time:68066ms step_avg:92.61ms
step:736/1670 train_time:68159ms step_avg:92.61ms
step:737/1670 train_time:68252ms step_avg:92.61ms
step:738/1670 train_time:68345ms step_avg:92.61ms
step:739/1670 train_time:68438ms step_avg:92.61ms
step:740/1670 train_time:68529ms step_avg:92.61ms
step:741/1670 train_time:68623ms step_avg:92.61ms
step:742/1670 train_time:68714ms step_avg:92.61ms
step:743/1670 train_time:68806ms step_avg:92.61ms
step:744/1670 train_time:68899ms step_avg:92.61ms
step:745/1670 train_time:68991ms step_avg:92.61ms
step:746/1670 train_time:69084ms step_avg:92.61ms
step:747/1670 train_time:69177ms step_avg:92.61ms
step:748/1670 train_time:69269ms step_avg:92.61ms
step:749/1670 train_time:69362ms step_avg:92.61ms
step:750/1670 train_time:69454ms step_avg:92.61ms
step:750/1670 val_loss:3.5593 train_time:69547ms step_avg:92.73ms
step:751/1670 train_time:69566ms step_avg:92.63ms
step:752/1670 train_time:69642ms step_avg:92.61ms
step:753/1670 train_time:69735ms step_avg:92.61ms
step:754/1670 train_time:69827ms step_avg:92.61ms
step:755/1670 train_time:69919ms step_avg:92.61ms
step:756/1670 train_time:70010ms step_avg:92.61ms
step:757/1670 train_time:70102ms step_avg:92.61ms
step:758/1670 train_time:70195ms step_avg:92.61ms
step:759/1670 train_time:70287ms step_avg:92.61ms
step:760/1670 train_time:70381ms step_avg:92.61ms
step:761/1670 train_time:70474ms step_avg:92.61ms
step:762/1670 train_time:70567ms step_avg:92.61ms
step:763/1670 train_time:70661ms step_avg:92.61ms
step:764/1670 train_time:70754ms step_avg:92.61ms
step:765/1670 train_time:70846ms step_avg:92.61ms
step:766/1670 train_time:70938ms step_avg:92.61ms
step:767/1670 train_time:71029ms step_avg:92.61ms
step:768/1670 train_time:71122ms step_avg:92.61ms
step:769/1670 train_time:71214ms step_avg:92.61ms
step:770/1670 train_time:71307ms step_avg:92.61ms
step:771/1670 train_time:71400ms step_avg:92.61ms
step:772/1670 train_time:71493ms step_avg:92.61ms
step:773/1670 train_time:71586ms step_avg:92.61ms
step:774/1670 train_time:71679ms step_avg:92.61ms
step:775/1670 train_time:71771ms step_avg:92.61ms
step:776/1670 train_time:71864ms step_avg:92.61ms
step:777/1670 train_time:71955ms step_avg:92.61ms
step:778/1670 train_time:72047ms step_avg:92.61ms
step:779/1670 train_time:72142ms step_avg:92.61ms
step:780/1670 train_time:72236ms step_avg:92.61ms
step:781/1670 train_time:72328ms step_avg:92.61ms
step:782/1670 train_time:72421ms step_avg:92.61ms
step:783/1670 train_time:72514ms step_avg:92.61ms
step:784/1670 train_time:72606ms step_avg:92.61ms
step:785/1670 train_time:72699ms step_avg:92.61ms
step:786/1670 train_time:72791ms step_avg:92.61ms
step:787/1670 train_time:72883ms step_avg:92.61ms
step:788/1670 train_time:72976ms step_avg:92.61ms
step:789/1670 train_time:73068ms step_avg:92.61ms
step:790/1670 train_time:73160ms step_avg:92.61ms
step:791/1670 train_time:73252ms step_avg:92.61ms
step:792/1670 train_time:73346ms step_avg:92.61ms
step:793/1670 train_time:73438ms step_avg:92.61ms
step:794/1670 train_time:73530ms step_avg:92.61ms
step:795/1670 train_time:73623ms step_avg:92.61ms
step:796/1670 train_time:73716ms step_avg:92.61ms
step:797/1670 train_time:73808ms step_avg:92.61ms
step:798/1670 train_time:73902ms step_avg:92.61ms
step:799/1670 train_time:73994ms step_avg:92.61ms
step:800/1670 train_time:74086ms step_avg:92.61ms
step:801/1670 train_time:74179ms step_avg:92.61ms
step:802/1670 train_time:74272ms step_avg:92.61ms
step:803/1670 train_time:74365ms step_avg:92.61ms
step:804/1670 train_time:74457ms step_avg:92.61ms
step:805/1670 train_time:74548ms step_avg:92.61ms
step:806/1670 train_time:74642ms step_avg:92.61ms
step:807/1670 train_time:74735ms step_avg:92.61ms
step:808/1670 train_time:74827ms step_avg:92.61ms
step:809/1670 train_time:74920ms step_avg:92.61ms
step:810/1670 train_time:75012ms step_avg:92.61ms
step:811/1670 train_time:75105ms step_avg:92.61ms
step:812/1670 train_time:75197ms step_avg:92.61ms
step:813/1670 train_time:75290ms step_avg:92.61ms
step:814/1670 train_time:75383ms step_avg:92.61ms
step:815/1670 train_time:75476ms step_avg:92.61ms
step:816/1670 train_time:75568ms step_avg:92.61ms
step:817/1670 train_time:75661ms step_avg:92.61ms
step:818/1670 train_time:75753ms step_avg:92.61ms
step:819/1670 train_time:75845ms step_avg:92.61ms
step:820/1670 train_time:75938ms step_avg:92.61ms
step:821/1670 train_time:76030ms step_avg:92.61ms
step:822/1670 train_time:76123ms step_avg:92.61ms
step:823/1670 train_time:76216ms step_avg:92.61ms
step:824/1670 train_time:76308ms step_avg:92.61ms
step:825/1670 train_time:76401ms step_avg:92.61ms
step:826/1670 train_time:76493ms step_avg:92.61ms
step:827/1670 train_time:76585ms step_avg:92.61ms
step:828/1670 train_time:76678ms step_avg:92.61ms
step:829/1670 train_time:76770ms step_avg:92.61ms
step:830/1670 train_time:76862ms step_avg:92.61ms
step:831/1670 train_time:76955ms step_avg:92.61ms
step:832/1670 train_time:77047ms step_avg:92.60ms
step:833/1670 train_time:77140ms step_avg:92.60ms
step:834/1670 train_time:77232ms step_avg:92.60ms
step:835/1670 train_time:77325ms step_avg:92.61ms
step:836/1670 train_time:77418ms step_avg:92.61ms
step:837/1670 train_time:77510ms step_avg:92.60ms
step:838/1670 train_time:77603ms step_avg:92.61ms
step:839/1670 train_time:77696ms step_avg:92.61ms
step:840/1670 train_time:77788ms step_avg:92.60ms
step:841/1670 train_time:77881ms step_avg:92.61ms
step:842/1670 train_time:77973ms step_avg:92.60ms
step:843/1670 train_time:78066ms step_avg:92.60ms
step:844/1670 train_time:78158ms step_avg:92.60ms
step:845/1670 train_time:78251ms step_avg:92.61ms
step:846/1670 train_time:78344ms step_avg:92.61ms
step:847/1670 train_time:78437ms step_avg:92.61ms
step:848/1670 train_time:78530ms step_avg:92.61ms
step:849/1670 train_time:78623ms step_avg:92.61ms
step:850/1670 train_time:78716ms step_avg:92.61ms
step:851/1670 train_time:78965ms step_avg:92.79ms
step:852/1670 train_time:79037ms step_avg:92.77ms
step:853/1670 train_time:79128ms step_avg:92.76ms
step:854/1670 train_time:79220ms step_avg:92.76ms
step:855/1670 train_time:79311ms step_avg:92.76ms
step:856/1670 train_time:79402ms step_avg:92.76ms
step:857/1670 train_time:79493ms step_avg:92.76ms
step:858/1670 train_time:79585ms step_avg:92.76ms
step:859/1670 train_time:79676ms step_avg:92.75ms
step:860/1670 train_time:79767ms step_avg:92.75ms
step:861/1670 train_time:79867ms step_avg:92.76ms
step:862/1670 train_time:79965ms step_avg:92.77ms
step:863/1670 train_time:80058ms step_avg:92.77ms
step:864/1670 train_time:80150ms step_avg:92.77ms
step:865/1670 train_time:80242ms step_avg:92.77ms
step:866/1670 train_time:80334ms step_avg:92.76ms
step:867/1670 train_time:80426ms step_avg:92.76ms
step:868/1670 train_time:80517ms step_avg:92.76ms
step:869/1670 train_time:80608ms step_avg:92.76ms
step:870/1670 train_time:80699ms step_avg:92.76ms
step:871/1670 train_time:80793ms step_avg:92.76ms
step:872/1670 train_time:80888ms step_avg:92.76ms
step:873/1670 train_time:80983ms step_avg:92.76ms
step:874/1670 train_time:81076ms step_avg:92.76ms
step:875/1670 train_time:81169ms step_avg:92.76ms
step:875/1670 val_loss:3.5164 train_time:81263ms step_avg:92.87ms
step:876/1670 train_time:81283ms step_avg:92.79ms
step:877/1670 train_time:81357ms step_avg:92.77ms
step:878/1670 train_time:81450ms step_avg:92.77ms
step:879/1670 train_time:81542ms step_avg:92.77ms
step:880/1670 train_time:81633ms step_avg:92.77ms
step:881/1670 train_time:81725ms step_avg:92.76ms
step:882/1670 train_time:81816ms step_avg:92.76ms
step:883/1670 train_time:81909ms step_avg:92.76ms
step:884/1670 train_time:82001ms step_avg:92.76ms
step:885/1670 train_time:82094ms step_avg:92.76ms
step:886/1670 train_time:82188ms step_avg:92.76ms
step:887/1670 train_time:82281ms step_avg:92.76ms
step:888/1670 train_time:82376ms step_avg:92.77ms
step:889/1670 train_time:82468ms step_avg:92.76ms
step:890/1670 train_time:82560ms step_avg:92.76ms
step:891/1670 train_time:82651ms step_avg:92.76ms
step:892/1670 train_time:82743ms step_avg:92.76ms
step:893/1670 train_time:82834ms step_avg:92.76ms
step:894/1670 train_time:82927ms step_avg:92.76ms
step:895/1670 train_time:83020ms step_avg:92.76ms
step:896/1670 train_time:83112ms step_avg:92.76ms
step:897/1670 train_time:83206ms step_avg:92.76ms
step:898/1670 train_time:83301ms step_avg:92.76ms
step:899/1670 train_time:83395ms step_avg:92.76ms
step:900/1670 train_time:83487ms step_avg:92.76ms
step:901/1670 train_time:83580ms step_avg:92.76ms
step:902/1670 train_time:83672ms step_avg:92.76ms
step:903/1670 train_time:83764ms step_avg:92.76ms
step:904/1670 train_time:83856ms step_avg:92.76ms
step:905/1670 train_time:83948ms step_avg:92.76ms
step:906/1670 train_time:84040ms step_avg:92.76ms
step:907/1670 train_time:84133ms step_avg:92.76ms
step:908/1670 train_time:84226ms step_avg:92.76ms
step:909/1670 train_time:84321ms step_avg:92.76ms
step:910/1670 train_time:84414ms step_avg:92.76ms
step:911/1670 train_time:84506ms step_avg:92.76ms
step:912/1670 train_time:84599ms step_avg:92.76ms
step:913/1670 train_time:84691ms step_avg:92.76ms
step:914/1670 train_time:84783ms step_avg:92.76ms
step:915/1670 train_time:84875ms step_avg:92.76ms
step:916/1670 train_time:84967ms step_avg:92.76ms
step:917/1670 train_time:85060ms step_avg:92.76ms
step:918/1670 train_time:85152ms step_avg:92.76ms
step:919/1670 train_time:85245ms step_avg:92.76ms
step:920/1670 train_time:85338ms step_avg:92.76ms
step:921/1670 train_time:85430ms step_avg:92.76ms
step:922/1670 train_time:85523ms step_avg:92.76ms
step:923/1670 train_time:85617ms step_avg:92.76ms
step:924/1670 train_time:85709ms step_avg:92.76ms
step:925/1670 train_time:85803ms step_avg:92.76ms
step:926/1670 train_time:85895ms step_avg:92.76ms
step:927/1670 train_time:85987ms step_avg:92.76ms
step:928/1670 train_time:86081ms step_avg:92.76ms
step:929/1670 train_time:86174ms step_avg:92.76ms
step:930/1670 train_time:86267ms step_avg:92.76ms
step:931/1670 train_time:86361ms step_avg:92.76ms
step:932/1670 train_time:86453ms step_avg:92.76ms
step:933/1670 train_time:86546ms step_avg:92.76ms
step:934/1670 train_time:86639ms step_avg:92.76ms
step:935/1670 train_time:86731ms step_avg:92.76ms
step:936/1670 train_time:86822ms step_avg:92.76ms
step:937/1670 train_time:86914ms step_avg:92.76ms
step:938/1670 train_time:87006ms step_avg:92.76ms
step:939/1670 train_time:87099ms step_avg:92.76ms
step:940/1670 train_time:87192ms step_avg:92.76ms
step:941/1670 train_time:87284ms step_avg:92.76ms
step:942/1670 train_time:87378ms step_avg:92.76ms
step:943/1670 train_time:87470ms step_avg:92.76ms
step:944/1670 train_time:87563ms step_avg:92.76ms
step:945/1670 train_time:87655ms step_avg:92.76ms
step:946/1670 train_time:87748ms step_avg:92.76ms
step:947/1670 train_time:87840ms step_avg:92.76ms
step:948/1670 train_time:87933ms step_avg:92.76ms
step:949/1670 train_time:88026ms step_avg:92.76ms
step:950/1670 train_time:88118ms step_avg:92.76ms
step:951/1670 train_time:88210ms step_avg:92.76ms
step:952/1670 train_time:88303ms step_avg:92.75ms
step:953/1670 train_time:88394ms step_avg:92.75ms
step:954/1670 train_time:88487ms step_avg:92.75ms
step:955/1670 train_time:88580ms step_avg:92.75ms
step:956/1670 train_time:88673ms step_avg:92.75ms
step:957/1670 train_time:88766ms step_avg:92.75ms
step:958/1670 train_time:88859ms step_avg:92.75ms
step:959/1670 train_time:88952ms step_avg:92.76ms
step:960/1670 train_time:89045ms step_avg:92.75ms
step:961/1670 train_time:89137ms step_avg:92.75ms
step:962/1670 train_time:89229ms step_avg:92.75ms
step:963/1670 train_time:89322ms step_avg:92.75ms
step:964/1670 train_time:89414ms step_avg:92.75ms
step:965/1670 train_time:89507ms step_avg:92.75ms
step:966/1670 train_time:89601ms step_avg:92.75ms
step:967/1670 train_time:89694ms step_avg:92.75ms
step:968/1670 train_time:89787ms step_avg:92.75ms
step:969/1670 train_time:89881ms step_avg:92.76ms
step:970/1670 train_time:89973ms step_avg:92.76ms
step:971/1670 train_time:90065ms step_avg:92.75ms
step:972/1670 train_time:90157ms step_avg:92.75ms
step:973/1670 train_time:90248ms step_avg:92.75ms
step:974/1670 train_time:90341ms step_avg:92.75ms
step:975/1670 train_time:90433ms step_avg:92.75ms
step:976/1670 train_time:90526ms step_avg:92.75ms
step:977/1670 train_time:90618ms step_avg:92.75ms
step:978/1670 train_time:90710ms step_avg:92.75ms
step:979/1670 train_time:90803ms step_avg:92.75ms
step:980/1670 train_time:90897ms step_avg:92.75ms
step:981/1670 train_time:90989ms step_avg:92.75ms
step:982/1670 train_time:91081ms step_avg:92.75ms
step:983/1670 train_time:91174ms step_avg:92.75ms
step:984/1670 train_time:91267ms step_avg:92.75ms
step:985/1670 train_time:91360ms step_avg:92.75ms
step:986/1670 train_time:91452ms step_avg:92.75ms
step:987/1670 train_time:91545ms step_avg:92.75ms
step:988/1670 train_time:91638ms step_avg:92.75ms
step:989/1670 train_time:91731ms step_avg:92.75ms
step:990/1670 train_time:91824ms step_avg:92.75ms
step:991/1670 train_time:91916ms step_avg:92.75ms
step:992/1670 train_time:92009ms step_avg:92.75ms
step:993/1670 train_time:92102ms step_avg:92.75ms
step:994/1670 train_time:92194ms step_avg:92.75ms
step:995/1670 train_time:92287ms step_avg:92.75ms
step:996/1670 train_time:92380ms step_avg:92.75ms
step:997/1670 train_time:92473ms step_avg:92.75ms
step:998/1670 train_time:92565ms step_avg:92.75ms
step:999/1670 train_time:92658ms step_avg:92.75ms
step:1000/1670 train_time:92751ms step_avg:92.75ms
step:1000/1670 val_loss:3.4669 train_time:92843ms step_avg:92.84ms
step:1001/1670 train_time:92863ms step_avg:92.77ms
step:1002/1670 train_time:92941ms step_avg:92.76ms
step:1003/1670 train_time:93033ms step_avg:92.75ms
step:1004/1670 train_time:93124ms step_avg:92.75ms
step:1005/1670 train_time:93216ms step_avg:92.75ms
step:1006/1670 train_time:93307ms step_avg:92.75ms
step:1007/1670 train_time:93399ms step_avg:92.75ms
step:1008/1670 train_time:93493ms step_avg:92.75ms
step:1009/1670 train_time:93585ms step_avg:92.75ms
step:1010/1670 train_time:93678ms step_avg:92.75ms
step:1011/1670 train_time:93771ms step_avg:92.75ms
step:1012/1670 train_time:93865ms step_avg:92.75ms
step:1013/1670 train_time:93959ms step_avg:92.75ms
step:1014/1670 train_time:94052ms step_avg:92.75ms
step:1015/1670 train_time:94144ms step_avg:92.75ms
step:1016/1670 train_time:94236ms step_avg:92.75ms
step:1017/1670 train_time:94328ms step_avg:92.75ms
step:1018/1670 train_time:94421ms step_avg:92.75ms
step:1019/1670 train_time:94513ms step_avg:92.75ms
step:1020/1670 train_time:94604ms step_avg:92.75ms
step:1021/1670 train_time:94697ms step_avg:92.75ms
step:1022/1670 train_time:94791ms step_avg:92.75ms
step:1023/1670 train_time:94884ms step_avg:92.75ms
step:1024/1670 train_time:94978ms step_avg:92.75ms
step:1025/1670 train_time:95070ms step_avg:92.75ms
step:1026/1670 train_time:95162ms step_avg:92.75ms
step:1027/1670 train_time:95255ms step_avg:92.75ms
step:1028/1670 train_time:95347ms step_avg:92.75ms
step:1029/1670 train_time:95439ms step_avg:92.75ms
step:1030/1670 train_time:95532ms step_avg:92.75ms
step:1031/1670 train_time:95625ms step_avg:92.75ms
step:1032/1670 train_time:95718ms step_avg:92.75ms
step:1033/1670 train_time:95811ms step_avg:92.75ms
step:1034/1670 train_time:95903ms step_avg:92.75ms
step:1035/1670 train_time:95996ms step_avg:92.75ms
step:1036/1670 train_time:96088ms step_avg:92.75ms
step:1037/1670 train_time:96181ms step_avg:92.75ms
step:1038/1670 train_time:96273ms step_avg:92.75ms
step:1039/1670 train_time:96365ms step_avg:92.75ms
step:1040/1670 train_time:96458ms step_avg:92.75ms
step:1041/1670 train_time:96552ms step_avg:92.75ms
step:1042/1670 train_time:96645ms step_avg:92.75ms
step:1043/1670 train_time:96738ms step_avg:92.75ms
step:1044/1670 train_time:96832ms step_avg:92.75ms
step:1045/1670 train_time:96924ms step_avg:92.75ms
step:1046/1670 train_time:97016ms step_avg:92.75ms
step:1047/1670 train_time:97109ms step_avg:92.75ms
step:1048/1670 train_time:97200ms step_avg:92.75ms
step:1049/1670 train_time:97292ms step_avg:92.75ms
step:1050/1670 train_time:97385ms step_avg:92.75ms
step:1051/1670 train_time:97476ms step_avg:92.75ms
step:1052/1670 train_time:97568ms step_avg:92.75ms
step:1053/1670 train_time:97662ms step_avg:92.75ms
step:1054/1670 train_time:97757ms step_avg:92.75ms
step:1055/1670 train_time:97849ms step_avg:92.75ms
step:1056/1670 train_time:97942ms step_avg:92.75ms
step:1057/1670 train_time:98036ms step_avg:92.75ms
step:1058/1670 train_time:98128ms step_avg:92.75ms
step:1059/1670 train_time:98221ms step_avg:92.75ms
step:1060/1670 train_time:98313ms step_avg:92.75ms
step:1061/1670 train_time:98405ms step_avg:92.75ms
step:1062/1670 train_time:98656ms step_avg:92.90ms
step:1063/1670 train_time:98727ms step_avg:92.88ms
step:1064/1670 train_time:98818ms step_avg:92.87ms
step:1065/1670 train_time:98909ms step_avg:92.87ms
step:1066/1670 train_time:99000ms step_avg:92.87ms
step:1067/1670 train_time:99091ms step_avg:92.87ms
step:1068/1670 train_time:99183ms step_avg:92.87ms
step:1069/1670 train_time:99274ms step_avg:92.87ms
step:1070/1670 train_time:99365ms step_avg:92.86ms
step:1071/1670 train_time:99457ms step_avg:92.86ms
step:1072/1670 train_time:99555ms step_avg:92.87ms
step:1073/1670 train_time:99654ms step_avg:92.87ms
step:1074/1670 train_time:99747ms step_avg:92.87ms
step:1075/1670 train_time:99839ms step_avg:92.87ms
step:1076/1670 train_time:99931ms step_avg:92.87ms
step:1077/1670 train_time:100022ms step_avg:92.87ms
step:1078/1670 train_time:100113ms step_avg:92.87ms
step:1079/1670 train_time:100205ms step_avg:92.87ms
step:1080/1670 train_time:100296ms step_avg:92.87ms
step:1081/1670 train_time:100387ms step_avg:92.87ms
step:1082/1670 train_time:100482ms step_avg:92.87ms
step:1083/1670 train_time:100577ms step_avg:92.87ms
step:1084/1670 train_time:100670ms step_avg:92.87ms
step:1085/1670 train_time:100764ms step_avg:92.87ms
step:1086/1670 train_time:100858ms step_avg:92.87ms
step:1087/1670 train_time:100949ms step_avg:92.87ms
step:1088/1670 train_time:101044ms step_avg:92.87ms
step:1089/1670 train_time:101135ms step_avg:92.87ms
step:1090/1670 train_time:101226ms step_avg:92.87ms
step:1091/1670 train_time:101318ms step_avg:92.87ms
step:1092/1670 train_time:101410ms step_avg:92.87ms
step:1093/1670 train_time:101502ms step_avg:92.87ms
step:1094/1670 train_time:101596ms step_avg:92.87ms
step:1095/1670 train_time:101689ms step_avg:92.87ms
step:1096/1670 train_time:101783ms step_avg:92.87ms
step:1097/1670 train_time:101876ms step_avg:92.87ms
step:1098/1670 train_time:101968ms step_avg:92.87ms
step:1099/1670 train_time:102060ms step_avg:92.87ms
step:1100/1670 train_time:102153ms step_avg:92.87ms
step:1101/1670 train_time:102245ms step_avg:92.87ms
step:1102/1670 train_time:102336ms step_avg:92.86ms
step:1103/1670 train_time:102429ms step_avg:92.86ms
step:1104/1670 train_time:102522ms step_avg:92.86ms
step:1105/1670 train_time:102615ms step_avg:92.86ms
step:1106/1670 train_time:102708ms step_avg:92.86ms
step:1107/1670 train_time:102801ms step_avg:92.86ms
step:1108/1670 train_time:102894ms step_avg:92.87ms
step:1109/1670 train_time:102987ms step_avg:92.86ms
step:1110/1670 train_time:103079ms step_avg:92.86ms
step:1111/1670 train_time:103171ms step_avg:92.86ms
step:1112/1670 train_time:103263ms step_avg:92.86ms
step:1113/1670 train_time:103356ms step_avg:92.86ms
step:1114/1670 train_time:103448ms step_avg:92.86ms
step:1115/1670 train_time:103729ms step_avg:93.03ms
step:1116/1670 train_time:103806ms step_avg:93.02ms
step:1117/1670 train_time:103897ms step_avg:93.01ms
step:1118/1670 train_time:103989ms step_avg:93.01ms
step:1119/1670 train_time:104080ms step_avg:93.01ms
step:1120/1670 train_time:104172ms step_avg:93.01ms
step:1121/1670 train_time:104264ms step_avg:93.01ms
step:1122/1670 train_time:104356ms step_avg:93.01ms
step:1123/1670 train_time:104447ms step_avg:93.01ms
step:1124/1670 train_time:104540ms step_avg:93.01ms
step:1125/1670 train_time:104638ms step_avg:93.01ms
step:1125/1670 val_loss:3.4137 train_time:104738ms step_avg:93.10ms
step:1126/1670 train_time:104760ms step_avg:93.04ms
step:1127/1670 train_time:104836ms step_avg:93.02ms
step:1128/1670 train_time:104938ms step_avg:93.03ms
step:1129/1670 train_time:105033ms step_avg:93.03ms
step:1130/1670 train_time:105126ms step_avg:93.03ms
step:1131/1670 train_time:105218ms step_avg:93.03ms
step:1132/1670 train_time:105310ms step_avg:93.03ms
step:1133/1670 train_time:105403ms step_avg:93.03ms
step:1134/1670 train_time:105495ms step_avg:93.03ms
step:1135/1670 train_time:105587ms step_avg:93.03ms
step:1136/1670 train_time:105680ms step_avg:93.03ms
step:1137/1670 train_time:105773ms step_avg:93.03ms
step:1138/1670 train_time:105869ms step_avg:93.03ms
step:1139/1670 train_time:105966ms step_avg:93.03ms
step:1140/1670 train_time:106060ms step_avg:93.04ms
step:1141/1670 train_time:106153ms step_avg:93.03ms
step:1142/1670 train_time:106246ms step_avg:93.03ms
step:1143/1670 train_time:106338ms step_avg:93.03ms
step:1144/1670 train_time:106430ms step_avg:93.03ms
step:1145/1670 train_time:106523ms step_avg:93.03ms
step:1146/1670 train_time:106617ms step_avg:93.03ms
step:1147/1670 train_time:106709ms step_avg:93.03ms
step:1148/1670 train_time:106802ms step_avg:93.03ms
step:1149/1670 train_time:106897ms step_avg:93.03ms
step:1150/1670 train_time:106992ms step_avg:93.04ms
step:1151/1670 train_time:107086ms step_avg:93.04ms
step:1152/1670 train_time:107179ms step_avg:93.04ms
step:1153/1670 train_time:107271ms step_avg:93.04ms
step:1154/1670 train_time:107364ms step_avg:93.04ms
step:1155/1670 train_time:107458ms step_avg:93.04ms
step:1156/1670 train_time:107550ms step_avg:93.04ms
step:1157/1670 train_time:107642ms step_avg:93.04ms
step:1158/1670 train_time:107735ms step_avg:93.04ms
step:1159/1670 train_time:107830ms step_avg:93.04ms
step:1160/1670 train_time:107926ms step_avg:93.04ms
step:1161/1670 train_time:108020ms step_avg:93.04ms
step:1162/1670 train_time:108114ms step_avg:93.04ms
step:1163/1670 train_time:108208ms step_avg:93.04ms
step:1164/1670 train_time:108301ms step_avg:93.04ms
step:1165/1670 train_time:108393ms step_avg:93.04ms
step:1166/1670 train_time:108487ms step_avg:93.04ms
step:1167/1670 train_time:108580ms step_avg:93.04ms
step:1168/1670 train_time:108672ms step_avg:93.04ms
step:1169/1670 train_time:108765ms step_avg:93.04ms
step:1170/1670 train_time:108859ms step_avg:93.04ms
step:1171/1670 train_time:108952ms step_avg:93.04ms
step:1172/1670 train_time:109047ms step_avg:93.04ms
step:1173/1670 train_time:109140ms step_avg:93.04ms
step:1174/1670 train_time:109233ms step_avg:93.04ms
step:1175/1670 train_time:109328ms step_avg:93.04ms
step:1176/1670 train_time:109421ms step_avg:93.05ms
step:1177/1670 train_time:109514ms step_avg:93.04ms
step:1178/1670 train_time:109607ms step_avg:93.04ms
step:1179/1670 train_time:109700ms step_avg:93.05ms
step:1180/1670 train_time:109794ms step_avg:93.05ms
step:1181/1670 train_time:109888ms step_avg:93.05ms
step:1182/1670 train_time:109981ms step_avg:93.05ms
step:1183/1670 train_time:110075ms step_avg:93.05ms
step:1184/1670 train_time:110168ms step_avg:93.05ms
step:1185/1670 train_time:110261ms step_avg:93.05ms
step:1186/1670 train_time:110354ms step_avg:93.05ms
step:1187/1670 train_time:110447ms step_avg:93.05ms
step:1188/1670 train_time:110540ms step_avg:93.05ms
step:1189/1670 train_time:110632ms step_avg:93.05ms
step:1190/1670 train_time:110726ms step_avg:93.05ms
step:1191/1670 train_time:110819ms step_avg:93.05ms
step:1192/1670 train_time:110912ms step_avg:93.05ms
step:1193/1670 train_time:111006ms step_avg:93.05ms
step:1194/1670 train_time:111099ms step_avg:93.05ms
step:1195/1670 train_time:111193ms step_avg:93.05ms
step:1196/1670 train_time:111286ms step_avg:93.05ms
step:1197/1670 train_time:111379ms step_avg:93.05ms
step:1198/1670 train_time:111472ms step_avg:93.05ms
step:1199/1670 train_time:111565ms step_avg:93.05ms
step:1200/1670 train_time:111658ms step_avg:93.05ms
step:1201/1670 train_time:111751ms step_avg:93.05ms
step:1202/1670 train_time:111846ms step_avg:93.05ms
step:1203/1670 train_time:111939ms step_avg:93.05ms
step:1204/1670 train_time:112032ms step_avg:93.05ms
step:1205/1670 train_time:112127ms step_avg:93.05ms
step:1206/1670 train_time:112221ms step_avg:93.05ms
step:1207/1670 train_time:112315ms step_avg:93.05ms
step:1208/1670 train_time:112408ms step_avg:93.05ms
step:1209/1670 train_time:112501ms step_avg:93.05ms
step:1210/1670 train_time:112593ms step_avg:93.05ms
step:1211/1670 train_time:112688ms step_avg:93.05ms
step:1212/1670 train_time:112782ms step_avg:93.05ms
step:1213/1670 train_time:112874ms step_avg:93.05ms
step:1214/1670 train_time:112966ms step_avg:93.05ms
step:1215/1670 train_time:113060ms step_avg:93.05ms
step:1216/1670 train_time:113153ms step_avg:93.05ms
step:1217/1670 train_time:113248ms step_avg:93.05ms
step:1218/1670 train_time:113341ms step_avg:93.06ms
step:1219/1670 train_time:113434ms step_avg:93.05ms
step:1220/1670 train_time:113528ms step_avg:93.06ms
step:1221/1670 train_time:113622ms step_avg:93.06ms
step:1222/1670 train_time:113714ms step_avg:93.06ms
step:1223/1670 train_time:113807ms step_avg:93.06ms
step:1224/1670 train_time:113900ms step_avg:93.06ms
step:1225/1670 train_time:113993ms step_avg:93.06ms
step:1226/1670 train_time:114090ms step_avg:93.06ms
step:1227/1670 train_time:114181ms step_avg:93.06ms
step:1228/1670 train_time:114274ms step_avg:93.06ms
step:1229/1670 train_time:114367ms step_avg:93.06ms
step:1230/1670 train_time:114460ms step_avg:93.06ms
step:1231/1670 train_time:114553ms step_avg:93.06ms
step:1232/1670 train_time:114647ms step_avg:93.06ms
step:1233/1670 train_time:114739ms step_avg:93.06ms
step:1234/1670 train_time:114831ms step_avg:93.06ms
step:1235/1670 train_time:114925ms step_avg:93.06ms
step:1236/1670 train_time:115019ms step_avg:93.06ms
step:1237/1670 train_time:115112ms step_avg:93.06ms
step:1238/1670 train_time:115207ms step_avg:93.06ms
step:1239/1670 train_time:115299ms step_avg:93.06ms
step:1240/1670 train_time:115391ms step_avg:93.06ms
step:1241/1670 train_time:115485ms step_avg:93.06ms
step:1242/1670 train_time:115578ms step_avg:93.06ms
step:1243/1670 train_time:115670ms step_avg:93.06ms
step:1244/1670 train_time:115764ms step_avg:93.06ms
step:1245/1670 train_time:115856ms step_avg:93.06ms
step:1246/1670 train_time:115950ms step_avg:93.06ms
step:1247/1670 train_time:116044ms step_avg:93.06ms
step:1248/1670 train_time:116137ms step_avg:93.06ms
step:1249/1670 train_time:116230ms step_avg:93.06ms
step:1250/1670 train_time:116325ms step_avg:93.06ms
step:1250/1670 val_loss:3.3755 train_time:116419ms step_avg:93.13ms
step:1251/1670 train_time:116438ms step_avg:93.08ms
step:1252/1670 train_time:116514ms step_avg:93.06ms
step:1253/1670 train_time:116608ms step_avg:93.06ms
step:1254/1670 train_time:116700ms step_avg:93.06ms
step:1255/1670 train_time:116794ms step_avg:93.06ms
step:1256/1670 train_time:116887ms step_avg:93.06ms
step:1257/1670 train_time:116979ms step_avg:93.06ms
step:1258/1670 train_time:117072ms step_avg:93.06ms
step:1259/1670 train_time:117165ms step_avg:93.06ms
step:1260/1670 train_time:117258ms step_avg:93.06ms
step:1261/1670 train_time:117353ms step_avg:93.06ms
step:1262/1670 train_time:117449ms step_avg:93.07ms
step:1263/1670 train_time:117542ms step_avg:93.07ms
step:1264/1670 train_time:117636ms step_avg:93.07ms
step:1265/1670 train_time:117728ms step_avg:93.07ms
step:1266/1670 train_time:117821ms step_avg:93.07ms
step:1267/1670 train_time:117914ms step_avg:93.07ms
step:1268/1670 train_time:118007ms step_avg:93.07ms
step:1269/1670 train_time:118109ms step_avg:93.07ms
step:1270/1670 train_time:118196ms step_avg:93.07ms
step:1271/1670 train_time:118289ms step_avg:93.07ms
step:1272/1670 train_time:118383ms step_avg:93.07ms
step:1273/1670 train_time:118478ms step_avg:93.07ms
step:1274/1670 train_time:118727ms step_avg:93.19ms
step:1275/1670 train_time:118797ms step_avg:93.17ms
step:1276/1670 train_time:118888ms step_avg:93.17ms
step:1277/1670 train_time:118980ms step_avg:93.17ms
step:1278/1670 train_time:119072ms step_avg:93.17ms
step:1279/1670 train_time:119164ms step_avg:93.17ms
step:1280/1670 train_time:119256ms step_avg:93.17ms
step:1281/1670 train_time:119348ms step_avg:93.17ms
step:1282/1670 train_time:119440ms step_avg:93.17ms
step:1283/1670 train_time:119533ms step_avg:93.17ms
step:1284/1670 train_time:119631ms step_avg:93.17ms
step:1285/1670 train_time:119727ms step_avg:93.17ms
step:1286/1670 train_time:119823ms step_avg:93.18ms
step:1287/1670 train_time:119916ms step_avg:93.17ms
step:1288/1670 train_time:120008ms step_avg:93.17ms
step:1289/1670 train_time:120100ms step_avg:93.17ms
step:1290/1670 train_time:120193ms step_avg:93.17ms
step:1291/1670 train_time:120285ms step_avg:93.17ms
step:1292/1670 train_time:120378ms step_avg:93.17ms
step:1293/1670 train_time:120470ms step_avg:93.17ms
step:1294/1670 train_time:120564ms step_avg:93.17ms
step:1295/1670 train_time:120660ms step_avg:93.17ms
step:1296/1670 train_time:120756ms step_avg:93.18ms
step:1297/1670 train_time:120850ms step_avg:93.18ms
step:1298/1670 train_time:120943ms step_avg:93.18ms
step:1299/1670 train_time:121036ms step_avg:93.18ms
step:1300/1670 train_time:121128ms step_avg:93.18ms
step:1301/1670 train_time:121221ms step_avg:93.17ms
step:1302/1670 train_time:121313ms step_avg:93.17ms
step:1303/1670 train_time:121404ms step_avg:93.17ms
step:1304/1670 train_time:121497ms step_avg:93.17ms
step:1305/1670 train_time:121590ms step_avg:93.17ms
step:1306/1670 train_time:121684ms step_avg:93.17ms
step:1307/1670 train_time:121778ms step_avg:93.17ms
step:1308/1670 train_time:121872ms step_avg:93.17ms
step:1309/1670 train_time:121966ms step_avg:93.17ms
step:1310/1670 train_time:122058ms step_avg:93.17ms
step:1311/1670 train_time:122151ms step_avg:93.17ms
step:1312/1670 train_time:122244ms step_avg:93.17ms
step:1313/1670 train_time:122337ms step_avg:93.17ms
step:1314/1670 train_time:122430ms step_avg:93.17ms
step:1315/1670 train_time:122522ms step_avg:93.17ms
step:1316/1670 train_time:122616ms step_avg:93.17ms
step:1317/1670 train_time:122710ms step_avg:93.17ms
step:1318/1670 train_time:122805ms step_avg:93.18ms
step:1319/1670 train_time:122898ms step_avg:93.18ms
step:1320/1670 train_time:122991ms step_avg:93.18ms
step:1321/1670 train_time:123084ms step_avg:93.18ms
step:1322/1670 train_time:123177ms step_avg:93.17ms
step:1323/1670 train_time:123270ms step_avg:93.17ms
step:1324/1670 train_time:123362ms step_avg:93.17ms
step:1325/1670 train_time:123455ms step_avg:93.17ms
step:1326/1670 train_time:123548ms step_avg:93.17ms
step:1327/1670 train_time:123642ms step_avg:93.17ms
step:1328/1670 train_time:123736ms step_avg:93.17ms
step:1329/1670 train_time:123830ms step_avg:93.18ms
step:1330/1670 train_time:123923ms step_avg:93.18ms
step:1331/1670 train_time:124017ms step_avg:93.18ms
step:1332/1670 train_time:124111ms step_avg:93.18ms
step:1333/1670 train_time:124203ms step_avg:93.18ms
step:1334/1670 train_time:124296ms step_avg:93.18ms
step:1335/1670 train_time:124389ms step_avg:93.18ms
step:1336/1670 train_time:124482ms step_avg:93.18ms
step:1337/1670 train_time:124576ms step_avg:93.18ms
step:1338/1670 train_time:124669ms step_avg:93.18ms
step:1339/1670 train_time:124762ms step_avg:93.18ms
step:1340/1670 train_time:124856ms step_avg:93.18ms
step:1341/1670 train_time:124950ms step_avg:93.18ms
step:1342/1670 train_time:125043ms step_avg:93.18ms
step:1343/1670 train_time:125137ms step_avg:93.18ms
step:1344/1670 train_time:125230ms step_avg:93.18ms
step:1345/1670 train_time:125322ms step_avg:93.18ms
step:1346/1670 train_time:125415ms step_avg:93.18ms
step:1347/1670 train_time:125508ms step_avg:93.18ms
step:1348/1670 train_time:125600ms step_avg:93.18ms
step:1349/1670 train_time:125693ms step_avg:93.18ms
step:1350/1670 train_time:125786ms step_avg:93.17ms
step:1351/1670 train_time:125879ms step_avg:93.17ms
step:1352/1670 train_time:125973ms step_avg:93.17ms
step:1353/1670 train_time:126065ms step_avg:93.17ms
step:1354/1670 train_time:126158ms step_avg:93.17ms
step:1355/1670 train_time:126253ms step_avg:93.18ms
step:1356/1670 train_time:126346ms step_avg:93.18ms
step:1357/1670 train_time:126439ms step_avg:93.18ms
step:1358/1670 train_time:126532ms step_avg:93.17ms
step:1359/1670 train_time:126624ms step_avg:93.17ms
step:1360/1670 train_time:126717ms step_avg:93.17ms
step:1361/1670 train_time:126811ms step_avg:93.17ms
step:1362/1670 train_time:126904ms step_avg:93.17ms
step:1363/1670 train_time:126997ms step_avg:93.17ms
step:1364/1670 train_time:127090ms step_avg:93.17ms
step:1365/1670 train_time:127183ms step_avg:93.17ms
step:1366/1670 train_time:127276ms step_avg:93.17ms
step:1367/1670 train_time:127371ms step_avg:93.18ms
step:1368/1670 train_time:127465ms step_avg:93.18ms
step:1369/1670 train_time:127557ms step_avg:93.18ms
step:1370/1670 train_time:127651ms step_avg:93.18ms
step:1371/1670 train_time:127744ms step_avg:93.18ms
step:1372/1670 train_time:127838ms step_avg:93.18ms
step:1373/1670 train_time:127931ms step_avg:93.18ms
step:1374/1670 train_time:128024ms step_avg:93.18ms
step:1375/1670 train_time:128117ms step_avg:93.18ms
step:1375/1670 val_loss:3.3410 train_time:128210ms step_avg:93.24ms
step:1376/1670 train_time:128230ms step_avg:93.19ms
step:1377/1670 train_time:128304ms step_avg:93.18ms
step:1378/1670 train_time:128398ms step_avg:93.18ms
step:1379/1670 train_time:128490ms step_avg:93.18ms
step:1380/1670 train_time:128584ms step_avg:93.18ms
step:1381/1670 train_time:128678ms step_avg:93.18ms
step:1382/1670 train_time:128771ms step_avg:93.18ms
step:1383/1670 train_time:128864ms step_avg:93.18ms
step:1384/1670 train_time:128957ms step_avg:93.18ms
step:1385/1670 train_time:129050ms step_avg:93.18ms
step:1386/1670 train_time:129144ms step_avg:93.18ms
step:1387/1670 train_time:129239ms step_avg:93.18ms
step:1388/1670 train_time:129333ms step_avg:93.18ms
step:1389/1670 train_time:129426ms step_avg:93.18ms
step:1390/1670 train_time:129519ms step_avg:93.18ms
step:1391/1670 train_time:129611ms step_avg:93.18ms
step:1392/1670 train_time:129705ms step_avg:93.18ms
step:1393/1670 train_time:129800ms step_avg:93.18ms
step:1394/1670 train_time:129893ms step_avg:93.18ms
step:1395/1670 train_time:129985ms step_avg:93.18ms
step:1396/1670 train_time:130079ms step_avg:93.18ms
step:1397/1670 train_time:130173ms step_avg:93.18ms
step:1398/1670 train_time:130267ms step_avg:93.18ms
step:1399/1670 train_time:130361ms step_avg:93.18ms
step:1400/1670 train_time:130453ms step_avg:93.18ms
step:1401/1670 train_time:130545ms step_avg:93.18ms
step:1402/1670 train_time:130639ms step_avg:93.18ms
step:1403/1670 train_time:130731ms step_avg:93.18ms
step:1404/1670 train_time:130824ms step_avg:93.18ms
step:1405/1670 train_time:130917ms step_avg:93.18ms
step:1406/1670 train_time:131010ms step_avg:93.18ms
step:1407/1670 train_time:131104ms step_avg:93.18ms
step:1408/1670 train_time:131199ms step_avg:93.18ms
step:1409/1670 train_time:131293ms step_avg:93.18ms
step:1410/1670 train_time:131387ms step_avg:93.18ms
step:1411/1670 train_time:131481ms step_avg:93.18ms
step:1412/1670 train_time:131575ms step_avg:93.18ms
step:1413/1670 train_time:131667ms step_avg:93.18ms
step:1414/1670 train_time:131760ms step_avg:93.18ms
step:1415/1670 train_time:131853ms step_avg:93.18ms
step:1416/1670 train_time:131945ms step_avg:93.18ms
step:1417/1670 train_time:132039ms step_avg:93.18ms
step:1418/1670 train_time:132132ms step_avg:93.18ms
step:1419/1670 train_time:132226ms step_avg:93.18ms
step:1420/1670 train_time:132320ms step_avg:93.18ms
step:1421/1670 train_time:132414ms step_avg:93.18ms
step:1422/1670 train_time:132508ms step_avg:93.18ms
step:1423/1670 train_time:132602ms step_avg:93.18ms
step:1424/1670 train_time:132696ms step_avg:93.19ms
step:1425/1670 train_time:132789ms step_avg:93.19ms
step:1426/1670 train_time:132882ms step_avg:93.19ms
step:1427/1670 train_time:132975ms step_avg:93.18ms
step:1428/1670 train_time:133068ms step_avg:93.18ms
step:1429/1670 train_time:133160ms step_avg:93.18ms
step:1430/1670 train_time:133254ms step_avg:93.18ms
step:1431/1670 train_time:133347ms step_avg:93.18ms
step:1432/1670 train_time:133440ms step_avg:93.18ms
step:1433/1670 train_time:133533ms step_avg:93.18ms
step:1434/1670 train_time:133627ms step_avg:93.18ms
step:1435/1670 train_time:133721ms step_avg:93.19ms
step:1436/1670 train_time:133815ms step_avg:93.19ms
step:1437/1670 train_time:133907ms step_avg:93.19ms
step:1438/1670 train_time:134000ms step_avg:93.19ms
step:1439/1670 train_time:134093ms step_avg:93.19ms
step:1440/1670 train_time:134187ms step_avg:93.19ms
step:1441/1670 train_time:134281ms step_avg:93.19ms
step:1442/1670 train_time:134373ms step_avg:93.19ms
step:1443/1670 train_time:134466ms step_avg:93.19ms
step:1444/1670 train_time:134560ms step_avg:93.19ms
step:1445/1670 train_time:134653ms step_avg:93.19ms
step:1446/1670 train_time:134747ms step_avg:93.19ms
step:1447/1670 train_time:134841ms step_avg:93.19ms
step:1448/1670 train_time:134934ms step_avg:93.19ms
step:1449/1670 train_time:135027ms step_avg:93.19ms
step:1450/1670 train_time:135122ms step_avg:93.19ms
step:1451/1670 train_time:135215ms step_avg:93.19ms
step:1452/1670 train_time:135308ms step_avg:93.19ms
step:1453/1670 train_time:135401ms step_avg:93.19ms
step:1454/1670 train_time:135494ms step_avg:93.19ms
step:1455/1670 train_time:135588ms step_avg:93.19ms
step:1456/1670 train_time:135681ms step_avg:93.19ms
step:1457/1670 train_time:135774ms step_avg:93.19ms
step:1458/1670 train_time:135867ms step_avg:93.19ms
step:1459/1670 train_time:135961ms step_avg:93.19ms
step:1460/1670 train_time:136054ms step_avg:93.19ms
step:1461/1670 train_time:136148ms step_avg:93.19ms
step:1462/1670 train_time:136242ms step_avg:93.19ms
step:1463/1670 train_time:136335ms step_avg:93.19ms
step:1464/1670 train_time:136428ms step_avg:93.19ms
step:1465/1670 train_time:136523ms step_avg:93.19ms
step:1466/1670 train_time:136617ms step_avg:93.19ms
step:1467/1670 train_time:136710ms step_avg:93.19ms
step:1468/1670 train_time:136803ms step_avg:93.19ms
step:1469/1670 train_time:136896ms step_avg:93.19ms
step:1470/1670 train_time:136988ms step_avg:93.19ms
step:1471/1670 train_time:137082ms step_avg:93.19ms
step:1472/1670 train_time:137175ms step_avg:93.19ms
step:1473/1670 train_time:137267ms step_avg:93.19ms
step:1474/1670 train_time:137361ms step_avg:93.19ms
step:1475/1670 train_time:137456ms step_avg:93.19ms
step:1476/1670 train_time:137549ms step_avg:93.19ms
step:1477/1670 train_time:137644ms step_avg:93.19ms
step:1478/1670 train_time:137738ms step_avg:93.19ms
step:1479/1670 train_time:137831ms step_avg:93.19ms
step:1480/1670 train_time:137924ms step_avg:93.19ms
step:1481/1670 train_time:138017ms step_avg:93.19ms
step:1482/1670 train_time:138110ms step_avg:93.19ms
step:1483/1670 train_time:138204ms step_avg:93.19ms
step:1484/1670 train_time:138297ms step_avg:93.19ms
step:1485/1670 train_time:138548ms step_avg:93.30ms
step:1486/1670 train_time:138619ms step_avg:93.28ms
step:1487/1670 train_time:138711ms step_avg:93.28ms
step:1488/1670 train_time:138802ms step_avg:93.28ms
step:1489/1670 train_time:138894ms step_avg:93.28ms
step:1490/1670 train_time:138986ms step_avg:93.28ms
step:1491/1670 train_time:139078ms step_avg:93.28ms
step:1492/1670 train_time:139170ms step_avg:93.28ms
step:1493/1670 train_time:139261ms step_avg:93.28ms
step:1494/1670 train_time:139354ms step_avg:93.28ms
step:1495/1670 train_time:139453ms step_avg:93.28ms
step:1496/1670 train_time:139551ms step_avg:93.28ms
step:1497/1670 train_time:139644ms step_avg:93.28ms
step:1498/1670 train_time:139738ms step_avg:93.28ms
step:1499/1670 train_time:139830ms step_avg:93.28ms
step:1500/1670 train_time:139922ms step_avg:93.28ms
step:1500/1670 val_loss:3.3107 train_time:140016ms step_avg:93.34ms
step:1501/1670 train_time:140036ms step_avg:93.30ms
step:1502/1670 train_time:140112ms step_avg:93.28ms
step:1503/1670 train_time:140207ms step_avg:93.28ms
step:1504/1670 train_time:140299ms step_avg:93.28ms
step:1505/1670 train_time:140392ms step_avg:93.28ms
step:1506/1670 train_time:140484ms step_avg:93.28ms
step:1507/1670 train_time:140577ms step_avg:93.28ms
step:1508/1670 train_time:140673ms step_avg:93.28ms
step:1509/1670 train_time:140766ms step_avg:93.28ms
step:1510/1670 train_time:140859ms step_avg:93.28ms
step:1511/1670 train_time:140953ms step_avg:93.28ms
step:1512/1670 train_time:141047ms step_avg:93.29ms
step:1513/1670 train_time:141142ms step_avg:93.29ms
step:1514/1670 train_time:141236ms step_avg:93.29ms
step:1515/1670 train_time:141330ms step_avg:93.29ms
step:1516/1670 train_time:141422ms step_avg:93.29ms
step:1517/1670 train_time:141515ms step_avg:93.29ms
step:1518/1670 train_time:141610ms step_avg:93.29ms
step:1519/1670 train_time:141703ms step_avg:93.29ms
step:1520/1670 train_time:141796ms step_avg:93.29ms
step:1521/1670 train_time:141889ms step_avg:93.29ms
step:1522/1670 train_time:141983ms step_avg:93.29ms
step:1523/1670 train_time:142077ms step_avg:93.29ms
step:1524/1670 train_time:142170ms step_avg:93.29ms
step:1525/1670 train_time:142263ms step_avg:93.29ms
step:1526/1670 train_time:142356ms step_avg:93.29ms
step:1527/1670 train_time:142448ms step_avg:93.29ms
step:1528/1670 train_time:142540ms step_avg:93.29ms
step:1529/1670 train_time:142634ms step_avg:93.29ms
step:1530/1670 train_time:142728ms step_avg:93.29ms
step:1531/1670 train_time:142820ms step_avg:93.29ms
step:1532/1670 train_time:142914ms step_avg:93.29ms
step:1533/1670 train_time:143007ms step_avg:93.29ms
step:1534/1670 train_time:143101ms step_avg:93.29ms
step:1535/1670 train_time:143195ms step_avg:93.29ms
step:1536/1670 train_time:143288ms step_avg:93.29ms
step:1537/1670 train_time:143381ms step_avg:93.29ms
step:1538/1670 train_time:143475ms step_avg:93.29ms
step:1539/1670 train_time:143568ms step_avg:93.29ms
step:1540/1670 train_time:143661ms step_avg:93.29ms
step:1541/1670 train_time:143754ms step_avg:93.29ms
step:1542/1670 train_time:143847ms step_avg:93.29ms
step:1543/1670 train_time:143941ms step_avg:93.29ms
step:1544/1670 train_time:144036ms step_avg:93.29ms
step:1545/1670 train_time:144129ms step_avg:93.29ms
step:1546/1670 train_time:144221ms step_avg:93.29ms
step:1547/1670 train_time:144316ms step_avg:93.29ms
step:1548/1670 train_time:144409ms step_avg:93.29ms
step:1549/1670 train_time:144501ms step_avg:93.29ms
step:1550/1670 train_time:144595ms step_avg:93.29ms
step:1551/1670 train_time:144688ms step_avg:93.29ms
step:1552/1670 train_time:144780ms step_avg:93.29ms
step:1553/1670 train_time:144875ms step_avg:93.29ms
step:1554/1670 train_time:144969ms step_avg:93.29ms
step:1555/1670 train_time:145062ms step_avg:93.29ms
step:1556/1670 train_time:145155ms step_avg:93.29ms
step:1557/1670 train_time:145248ms step_avg:93.29ms
step:1558/1670 train_time:145341ms step_avg:93.29ms
step:1559/1670 train_time:145435ms step_avg:93.29ms
step:1560/1670 train_time:145527ms step_avg:93.29ms
step:1561/1670 train_time:145620ms step_avg:93.29ms
step:1562/1670 train_time:145713ms step_avg:93.29ms
step:1563/1670 train_time:145807ms step_avg:93.29ms
step:1564/1670 train_time:145900ms step_avg:93.29ms
step:1565/1670 train_time:145993ms step_avg:93.29ms
step:1566/1670 train_time:146087ms step_avg:93.29ms
step:1567/1670 train_time:146180ms step_avg:93.29ms
step:1568/1670 train_time:146275ms step_avg:93.29ms
step:1569/1670 train_time:146369ms step_avg:93.29ms
step:1570/1670 train_time:146462ms step_avg:93.29ms
step:1571/1670 train_time:146554ms step_avg:93.29ms
step:1572/1670 train_time:146647ms step_avg:93.29ms
step:1573/1670 train_time:146741ms step_avg:93.29ms
step:1574/1670 train_time:146836ms step_avg:93.29ms
step:1575/1670 train_time:146930ms step_avg:93.29ms
step:1576/1670 train_time:147023ms step_avg:93.29ms
step:1577/1670 train_time:147116ms step_avg:93.29ms
step:1578/1670 train_time:147209ms step_avg:93.29ms
step:1579/1670 train_time:147303ms step_avg:93.29ms
step:1580/1670 train_time:147398ms step_avg:93.29ms
step:1581/1670 train_time:147491ms step_avg:93.29ms
step:1582/1670 train_time:147584ms step_avg:93.29ms
step:1583/1670 train_time:147678ms step_avg:93.29ms
step:1584/1670 train_time:147771ms step_avg:93.29ms
step:1585/1670 train_time:147864ms step_avg:93.29ms
step:1586/1670 train_time:147957ms step_avg:93.29ms
step:1587/1670 train_time:148050ms step_avg:93.29ms
step:1588/1670 train_time:148143ms step_avg:93.29ms
step:1589/1670 train_time:148237ms step_avg:93.29ms
step:1590/1670 train_time:148330ms step_avg:93.29ms
step:1591/1670 train_time:148423ms step_avg:93.29ms
step:1592/1670 train_time:148516ms step_avg:93.29ms
step:1593/1670 train_time:148610ms step_avg:93.29ms
step:1594/1670 train_time:148703ms step_avg:93.29ms
step:1595/1670 train_time:148797ms step_avg:93.29ms
step:1596/1670 train_time:148891ms step_avg:93.29ms
step:1597/1670 train_time:148983ms step_avg:93.29ms
step:1598/1670 train_time:149077ms step_avg:93.29ms
step:1599/1670 train_time:149170ms step_avg:93.29ms
step:1600/1670 train_time:149264ms step_avg:93.29ms
step:1601/1670 train_time:149358ms step_avg:93.29ms
step:1602/1670 train_time:149450ms step_avg:93.29ms
step:1603/1670 train_time:149542ms step_avg:93.29ms
step:1604/1670 train_time:149637ms step_avg:93.29ms
step:1605/1670 train_time:149731ms step_avg:93.29ms
step:1606/1670 train_time:149824ms step_avg:93.29ms
step:1607/1670 train_time:149916ms step_avg:93.29ms
step:1608/1670 train_time:150010ms step_avg:93.29ms
step:1609/1670 train_time:150103ms step_avg:93.29ms
step:1610/1670 train_time:150198ms step_avg:93.29ms
step:1611/1670 train_time:150291ms step_avg:93.29ms
step:1612/1670 train_time:150384ms step_avg:93.29ms
step:1613/1670 train_time:150477ms step_avg:93.29ms
step:1614/1670 train_time:150571ms step_avg:93.29ms
step:1615/1670 train_time:150664ms step_avg:93.29ms
step:1616/1670 train_time:150757ms step_avg:93.29ms
step:1617/1670 train_time:150850ms step_avg:93.29ms
step:1618/1670 train_time:150943ms step_avg:93.29ms
step:1619/1670 train_time:151037ms step_avg:93.29ms
step:1620/1670 train_time:151132ms step_avg:93.29ms
step:1621/1670 train_time:151225ms step_avg:93.29ms
step:1622/1670 train_time:151318ms step_avg:93.29ms
step:1623/1670 train_time:151413ms step_avg:93.29ms
step:1624/1670 train_time:151506ms step_avg:93.29ms
step:1625/1670 train_time:151599ms step_avg:93.29ms
step:1625/1670 val_loss:3.2859 train_time:151692ms step_avg:93.35ms
step:1626/1670 train_time:151712ms step_avg:93.30ms
step:1627/1670 train_time:151786ms step_avg:93.29ms
step:1628/1670 train_time:151879ms step_avg:93.29ms
step:1629/1670 train_time:151972ms step_avg:93.29ms
step:1630/1670 train_time:152064ms step_avg:93.29ms
step:1631/1670 train_time:152159ms step_avg:93.29ms
step:1632/1670 train_time:152252ms step_avg:93.29ms
step:1633/1670 train_time:152345ms step_avg:93.29ms
step:1634/1670 train_time:152439ms step_avg:93.29ms
step:1635/1670 train_time:152531ms step_avg:93.29ms
step:1636/1670 train_time:152626ms step_avg:93.29ms
step:1637/1670 train_time:152724ms step_avg:93.29ms
step:1638/1670 train_time:152818ms step_avg:93.30ms
step:1639/1670 train_time:152910ms step_avg:93.29ms
step:1640/1670 train_time:153003ms step_avg:93.29ms
step:1641/1670 train_time:153096ms step_avg:93.29ms
step:1642/1670 train_time:153189ms step_avg:93.29ms
step:1643/1670 train_time:153283ms step_avg:93.29ms
step:1644/1670 train_time:153377ms step_avg:93.29ms
step:1645/1670 train_time:153469ms step_avg:93.29ms
step:1646/1670 train_time:153563ms step_avg:93.29ms
step:1647/1670 train_time:153658ms step_avg:93.30ms
step:1648/1670 train_time:153752ms step_avg:93.30ms
step:1649/1670 train_time:153845ms step_avg:93.30ms
step:1650/1670 train_time:153939ms step_avg:93.30ms
step:1651/1670 train_time:154031ms step_avg:93.30ms
step:1652/1670 train_time:154125ms step_avg:93.30ms
step:1653/1670 train_time:154217ms step_avg:93.30ms
step:1654/1670 train_time:154310ms step_avg:93.29ms
step:1655/1670 train_time:154404ms step_avg:93.30ms
step:1656/1670 train_time:154497ms step_avg:93.30ms
step:1657/1670 train_time:154591ms step_avg:93.30ms
step:1658/1670 train_time:154685ms step_avg:93.30ms
step:1659/1670 train_time:154779ms step_avg:93.30ms
step:1660/1670 train_time:154872ms step_avg:93.30ms
step:1661/1670 train_time:154964ms step_avg:93.30ms
step:1662/1670 train_time:155057ms step_avg:93.30ms
step:1663/1670 train_time:155150ms step_avg:93.30ms
step:1664/1670 train_time:155243ms step_avg:93.29ms
step:1665/1670 train_time:155335ms step_avg:93.29ms
step:1666/1670 train_time:155428ms step_avg:93.29ms
step:1667/1670 train_time:155521ms step_avg:93.29ms
step:1668/1670 train_time:155615ms step_avg:93.29ms
step:1669/1670 train_time:155708ms step_avg:93.29ms
step:1670/1670 train_time:155801ms step_avg:93.29ms
step:1670/1670 val_loss:3.2774 train_time:156064ms step_avg:93.45ms
peak memory allocated: 32002 MiB reserved: 47054 MiB

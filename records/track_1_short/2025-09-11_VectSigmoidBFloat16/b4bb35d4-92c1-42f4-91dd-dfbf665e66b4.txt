import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(
    x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(
    g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99


def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]


@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(
        pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M
    )

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr,
    C_ptr,
    M,
    K,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr,
    C_ptr,
    M,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    alpha,
    beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (
        offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c
    )
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out


@torch.compile(
    dynamic=False, fullgraph=True
)  # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer


class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(
        self,
        params,
        lr: float = 1e-3,
        betas: tuple[float, float] = (0.9, 0.999),
        eps: float = 1e-8,
        weight_decay: float = 0.01,
    ):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(
                        grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True
                    ).get_future()
                )
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group["betas"]
            eps = group["eps"]
            wd = group["weight_decay"]
            params = group["params"]
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size : (rank + 1) * rank_size]
                lr = group["lr"] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(
                    g_slice, g_slice, value=1 - beta2
                )
                # bias corrections
                bias1 = 1 - beta1**t
                bias2 = 1 - beta2**t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(
                    dist.all_gather_into_tensor(
                        p, p_slice, async_op=True
                    ).get_future()
                )
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model


def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(
        self,
        in_features: int,
        out_features: int,
        use_fp8=False,
        x_s=1.0,
        w_s=1.0,
        grad_s=1.0,
    ):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (
            self.in_features**-0.5
        )  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3**0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(
                _x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s
            )[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    rotary_cos: torch.Tensor
    rotary_sin: torch.Tensor
    attn_scale: float


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        rotary_cos, rotary_sin = attn_args.rotary_cos, attn_args.rotary_sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = (
            attn_args.seqlens,
            attn_args.attn_scale,
            attn_args.bm_size,
        )

        q, k, v = (
            F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x))
            .view(B, T, 3 * self.num_heads, self.head_dim)
            .chunk(3, dim=-2)
        )
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = (
            rotary(q, rotary_cos, rotary_sin),
            rotary(k, rotary_cos, rotary_sin),
        )
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(
                v
            )  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = (
            args.train_max_seq_len
            if self.training
            else (args.val_batch_size // (grad_accum_steps * world_size))
        )

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(
            q[0],
            k[0],
            v[0],
            cu_seqlens_q=seqlens,
            cu_seqlens_k=seqlens,
            max_seqlen_q=max_len,
            max_seqlen_k=max_len,
            causal=True,
            softmax_scale=attn_scale,
            window_size=(bm_size, 0),
        )
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(
            self.attn_gate(x[..., : self.attn_gate.weight.size(-1)])
        ).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(
            B, T, self.num_heads * self.head_dim
        )  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x
        ).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = (
            CausalSelfAttention(dim, head_dim, num_heads)
            if layer_idx != 7
            else None
        )
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(
        self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs
    ):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model


def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        num_layers: int,
        num_heads: int,
        head_dim: int,
        model_dim: int,
        max_seq_len: int,
    ):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList(
            [nn.Embedding(vocab_size, model_dim) for _ in range(3)]
        )
        self.blocks = nn.ModuleList(
            [
                Block(model_dim, head_dim, num_heads, i)
                for i in range(num_layers)
            ]
        )
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(
            model_dim,
            vocab_size,
            use_fp8=use_fp8,
            x_s=(model_dim**0.5) / 448,
            w_s=2**-9,
            grad_s=1 / 448,
        )
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.ones(pad),
                ]
            )
        )
        self.max_seq_len = max_seq_len
        self.setup_yarn(head_dim)
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.0
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.0
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def setup_yarn(self, head_dim: int):
        # store single copy of rotary tensors
        angular_freq = (1 / 1024) ** torch.linspace(
            0, 1, steps=head_dim // 4, dtype=torch.float32
        )
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat(
            [angular_freq, angular_freq.new_zeros(head_dim // 4)]
        )
        t = torch.arange(self.max_seq_len, dtype=torch.float32)
        theta = torch.outer(t, angular_freq)
        self.rotary_cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.rotary_sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq

        # scale attention factor f in attn=softmax(f*qk) logarithmically with window size @classiclarryd
        windows = list(
            dict.fromkeys(list(args.ws_schedule) + [args.ws_validate])
        )
        scale_factors = [
            0.2 * math.log(curr / prev) + 1
            for prev, curr in zip(windows[:-1], windows[1:])
        ]
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        attn_scales = list(
            accumulate([0.1] + scale_factors, lambda acc, factor: acc * factor)
        )
        self.attn_scales = dict(zip(windows, attn_scales))

    def apply_yarn(
        self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32
    ):
        rotations = (
            args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        )
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp(
            (rotations - alpha) / (beta - alpha), 0, 1
        )
        self.angular_freq *= scaling_factor + interpolation_weight * (
            1 - scaling_factor
        )
        t = torch.arange(
            self.max_seq_len,
            dtype=torch.float32,
            device=self.angular_freq.device,
        )
        theta = torch.outer(t, self.angular_freq)
        self.rotary_cos.copy_(theta.cos())
        self.rotary_sin.copy_(theta.sin())

    def forward(
        self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int
    ):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = (
            [ve[0], ve[1], ve[2]]
            + [None] * (len(self.blocks) - 6)
            + [ve[0], ve[1], ve[2]]
        )
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        bm_sizes = [
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
        ]
        assert len(bm_sizes) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]).to(
            torch.bfloat16
        )  # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[: (len(self.blocks) // 2)]
        lambdas = self.scalars[
            1 * len(self.blocks) : 3 * len(self.blocks)
        ].view(-1, 2)
        sa_lambdas = self.scalars[
            3 * len(self.blocks) : 5 * len(self.blocks)
        ].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                rotary_cos=self.rotary_cos,
                rotary_sin=self.rotary_sin,
                attn_scale=self.attn_scales[ws],
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(
            logits.view(-1, logits.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader


def _load_data_shard(file: Path):
    header = torch.from_file(
        str(file), False, 256, dtype=torch.int32
    )  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(
            num_tokens, dtype=torch.uint16, pin_memory=True
        )  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(
            tokens.numpy()
        )  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, (
            "number of tokens read does not match header"
        )
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1):
        # Precompute BOS positions once per shard
        self.size = tokens.numel()
        self.bos_idx = (
            (tokens == BOS_ID)
            .nonzero(as_tuple=True)[0]
            .to(torch.int64)
            .cpu()
            .numpy()
        )
        self.i = 0
        self.world_size = world_size

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(
                        f"Insufficient BOS ahead of position {cur}; hit tail of shard."
                    )
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(
                    self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                    cur + max_seq_len,
                    cur + num_tokens_local - cur_len + 1,
                )
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx

        return starts, ends


def distributed_data_generator(
    filename_pattern: str,
    num_tokens: int,
    max_seq_len: int,
    grad_accum_steps: int = 1,
    align_to_bos: bool = True,
):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, (
        "Batch size must be divisible by world size"
    )
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(
            f"No files found for pattern: {filename_pattern}"
        )

    file_iter = iter(
        files
    )  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    finder = BOSFinder(tokens, world_size=world_size) if align_to_bos else None
    pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(
            num_tokens_local // 300, n=128
        )  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(
                    num_tokens_local, max_seq_len
                )
                start_idxs, end_idxs = (
                    torch.tensor(seq_starts[rank]),
                    torch.tensor(seq_ends[rank]),
                )
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens = _load_data_shard(next(file_iter))
                finder = BOSFinder(tokens, world_size=world_size)
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= (
                1  # last document was too long to account for _targets offset
            )
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(
                tokens
            ):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local : pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(
                num_tokens_local,
            )
            _targets = buf[1:].view(
                num_tokens_local,
            )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1 : len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(
                device="cuda", dtype=torch.int32, non_blocking=True
            ),
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, (
                "Num tokens must be divisible by world size"
            )
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main


@dataclass
class Hyperparameters:
    # data
    train_files: str = (
        "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    )
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1670  # number of iterations to run
    cooldown_frac: int = (
        0.5  # fraction of training spent cooling down the learning rate
    )
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = (
        125  # every how many steps to evaluate val loss? 0 for only at the end
    )
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws @classiclarryd


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = rank == 0  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(
    f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}"
)
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import

    return subprocess.run(
        ["nvidia-smi"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    ).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size)
    // (grad_accum_steps * world_size),
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [
    p
    for n, p in model.blocks.named_parameters()
    if p.ndim >= 2 and "embed" not in n
]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(
    hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)  # save the initial state
train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    ws = args.ws_schedule[
        step % len(args.ws_schedule)
    ]  # each window size is a new graph, need to warm up each
    model(inputs, targets, cum_seqlens, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws = get_ws(0)
for step in range(train_steps + 1):
    last_step = step == train_steps
    new_ws = get_ws(step)
    if new_ws != ws:
        model.apply_yarn(ws, new_ws)
        ws = new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (
        args.val_loss_every > 0 and step % args.val_loss_every == 0
    ):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(
            args.val_files,
            args.val_batch_size,
            -1,
            grad_accum_steps=grad_accum_steps,
            align_to_bos=False,
        )
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True,
        )
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(
                step=step,
                code=code,
                model=model.state_dict(),
                optimizers=[opt.state_dict() for opt in optimizers],
            )
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1)  # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (
        time.perf_counter() - t0
    )
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True,
    )

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.11 (main, Sep  2 2025, 14:20:58) [Clang 20.1.4 ]
Running PyTorch 2.9.0.dev20250718+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 11 09:20:37 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                  Off |
| N/A   40C    P0            127W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                  Off |
| N/A   45C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:65:00.0 Off |                  Off |
| N/A   46C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:68:00.0 Off |                  Off |
| N/A   37C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:85:00.0 Off |                  Off |
| N/A   38C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:86:00.0 Off |                  Off |
| N/A   46C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:E5:00.0 Off |                  Off |
| N/A   45C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E8:00.0 Off |                  Off |
| N/A   41C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1670 val_loss:10.8258 train_time:0ms step_avg:0.07ms
step:1/1670 train_time:292ms step_avg:291.66ms
step:2/1670 train_time:311ms step_avg:155.28ms
step:3/1670 train_time:378ms step_avg:126.07ms
step:4/1670 train_time:468ms step_avg:116.89ms
step:5/1670 train_time:558ms step_avg:111.51ms
step:6/1670 train_time:649ms step_avg:108.12ms
step:7/1670 train_time:739ms step_avg:105.56ms
step:8/1670 train_time:830ms step_avg:103.75ms
step:9/1670 train_time:920ms step_avg:102.26ms
step:10/1670 train_time:1011ms step_avg:101.11ms
step:11/1670 train_time:1101ms step_avg:100.12ms
step:12/1670 train_time:1195ms step_avg:99.57ms
step:13/1670 train_time:1289ms step_avg:99.13ms
step:14/1670 train_time:1381ms step_avg:98.67ms
step:15/1670 train_time:1474ms step_avg:98.25ms
step:16/1670 train_time:1565ms step_avg:97.84ms
step:17/1670 train_time:1656ms step_avg:97.44ms
step:18/1670 train_time:1748ms step_avg:97.09ms
step:19/1670 train_time:1839ms step_avg:96.79ms
step:20/1670 train_time:1930ms step_avg:96.50ms
step:21/1670 train_time:2020ms step_avg:96.20ms
step:22/1670 train_time:2111ms step_avg:95.97ms
step:23/1670 train_time:2205ms step_avg:95.86ms
step:24/1670 train_time:2299ms step_avg:95.78ms
step:25/1670 train_time:2391ms step_avg:95.65ms
step:26/1670 train_time:2483ms step_avg:95.50ms
step:27/1670 train_time:2575ms step_avg:95.37ms
step:28/1670 train_time:2668ms step_avg:95.28ms
step:29/1670 train_time:2759ms step_avg:95.15ms
step:30/1670 train_time:2851ms step_avg:95.03ms
step:31/1670 train_time:2942ms step_avg:94.90ms
step:32/1670 train_time:3034ms step_avg:94.82ms
step:33/1670 train_time:3125ms step_avg:94.70ms
step:34/1670 train_time:3218ms step_avg:94.65ms
step:35/1670 train_time:3311ms step_avg:94.60ms
step:36/1670 train_time:3403ms step_avg:94.51ms
step:37/1670 train_time:3494ms step_avg:94.44ms
step:38/1670 train_time:3586ms step_avg:94.36ms
step:39/1670 train_time:3679ms step_avg:94.34ms
step:40/1670 train_time:3772ms step_avg:94.31ms
step:41/1670 train_time:3864ms step_avg:94.26ms
step:42/1670 train_time:3956ms step_avg:94.20ms
step:43/1670 train_time:4047ms step_avg:94.12ms
step:44/1670 train_time:4139ms step_avg:94.07ms
step:45/1670 train_time:4232ms step_avg:94.05ms
step:46/1670 train_time:4325ms step_avg:94.01ms
step:47/1670 train_time:4418ms step_avg:94.00ms
step:48/1670 train_time:4511ms step_avg:93.97ms
step:49/1670 train_time:4602ms step_avg:93.92ms
step:50/1670 train_time:4694ms step_avg:93.89ms
step:51/1670 train_time:4786ms step_avg:93.84ms
step:52/1670 train_time:4878ms step_avg:93.81ms
step:53/1670 train_time:4970ms step_avg:93.78ms
step:54/1670 train_time:5062ms step_avg:93.74ms
step:55/1670 train_time:5154ms step_avg:93.71ms
step:56/1670 train_time:5245ms step_avg:93.66ms
step:57/1670 train_time:5338ms step_avg:93.65ms
step:58/1670 train_time:5431ms step_avg:93.65ms
step:59/1670 train_time:5523ms step_avg:93.61ms
step:60/1670 train_time:5615ms step_avg:93.59ms
step:61/1670 train_time:5707ms step_avg:93.55ms
step:62/1670 train_time:5799ms step_avg:93.53ms
step:63/1670 train_time:5890ms step_avg:93.49ms
step:64/1670 train_time:5981ms step_avg:93.45ms
step:65/1670 train_time:6073ms step_avg:93.44ms
step:66/1670 train_time:6164ms step_avg:93.39ms
step:67/1670 train_time:6256ms step_avg:93.37ms
step:68/1670 train_time:6348ms step_avg:93.35ms
step:69/1670 train_time:6441ms step_avg:93.34ms
step:70/1670 train_time:6534ms step_avg:93.34ms
step:71/1670 train_time:6625ms step_avg:93.31ms
step:72/1670 train_time:6718ms step_avg:93.30ms
step:73/1670 train_time:6810ms step_avg:93.29ms
step:74/1670 train_time:6901ms step_avg:93.26ms
step:75/1670 train_time:6993ms step_avg:93.24ms
step:76/1670 train_time:7085ms step_avg:93.23ms
step:77/1670 train_time:7177ms step_avg:93.20ms
step:78/1670 train_time:7268ms step_avg:93.17ms
step:79/1670 train_time:7359ms step_avg:93.15ms
step:80/1670 train_time:7450ms step_avg:93.13ms
step:81/1670 train_time:7542ms step_avg:93.11ms
step:82/1670 train_time:7634ms step_avg:93.09ms
step:83/1670 train_time:7725ms step_avg:93.07ms
step:84/1670 train_time:7817ms step_avg:93.06ms
step:85/1670 train_time:7910ms step_avg:93.06ms
step:86/1670 train_time:8001ms step_avg:93.04ms
step:87/1670 train_time:8092ms step_avg:93.01ms
step:88/1670 train_time:8182ms step_avg:92.98ms
step:89/1670 train_time:8274ms step_avg:92.96ms
step:90/1670 train_time:8365ms step_avg:92.94ms
step:91/1670 train_time:8456ms step_avg:92.92ms
step:92/1670 train_time:8547ms step_avg:92.90ms
step:93/1670 train_time:8639ms step_avg:92.89ms
step:94/1670 train_time:8731ms step_avg:92.88ms
step:95/1670 train_time:8822ms step_avg:92.86ms
step:96/1670 train_time:8914ms step_avg:92.85ms
step:97/1670 train_time:9006ms step_avg:92.84ms
step:98/1670 train_time:9099ms step_avg:92.84ms
step:99/1670 train_time:9190ms step_avg:92.83ms
step:100/1670 train_time:9281ms step_avg:92.81ms
step:101/1670 train_time:9372ms step_avg:92.79ms
step:102/1670 train_time:9463ms step_avg:92.78ms
step:103/1670 train_time:9554ms step_avg:92.76ms
step:104/1670 train_time:9645ms step_avg:92.74ms
step:105/1670 train_time:9738ms step_avg:92.75ms
step:106/1670 train_time:9829ms step_avg:92.73ms
step:107/1670 train_time:9921ms step_avg:92.72ms
step:108/1670 train_time:10013ms step_avg:92.71ms
step:109/1670 train_time:10105ms step_avg:92.71ms
step:110/1670 train_time:10197ms step_avg:92.70ms
step:111/1670 train_time:10288ms step_avg:92.68ms
step:112/1670 train_time:10380ms step_avg:92.68ms
step:113/1670 train_time:10472ms step_avg:92.67ms
step:114/1670 train_time:10563ms step_avg:92.66ms
step:115/1670 train_time:10656ms step_avg:92.66ms
step:116/1670 train_time:10748ms step_avg:92.65ms
step:117/1670 train_time:10840ms step_avg:92.65ms
step:118/1670 train_time:10932ms step_avg:92.64ms
step:119/1670 train_time:11022ms step_avg:92.63ms
step:120/1670 train_time:11115ms step_avg:92.62ms
step:121/1670 train_time:11206ms step_avg:92.62ms
step:122/1670 train_time:11299ms step_avg:92.61ms
step:123/1670 train_time:11390ms step_avg:92.60ms
step:124/1670 train_time:11482ms step_avg:92.59ms
step:125/1670 train_time:11573ms step_avg:92.59ms
step:125/1670 val_loss:4.3115 train_time:11664ms step_avg:93.31ms
step:126/1670 train_time:11684ms step_avg:92.73ms
step:127/1670 train_time:11760ms step_avg:92.60ms
step:128/1670 train_time:11861ms step_avg:92.67ms
step:129/1670 train_time:11954ms step_avg:92.67ms
step:130/1670 train_time:12045ms step_avg:92.66ms
step:131/1670 train_time:12136ms step_avg:92.64ms
step:132/1670 train_time:12225ms step_avg:92.62ms
step:133/1670 train_time:12315ms step_avg:92.60ms
step:134/1670 train_time:12405ms step_avg:92.57ms
step:135/1670 train_time:12496ms step_avg:92.56ms
step:136/1670 train_time:12586ms step_avg:92.55ms
step:137/1670 train_time:12678ms step_avg:92.54ms
step:138/1670 train_time:12771ms step_avg:92.54ms
step:139/1670 train_time:12866ms step_avg:92.56ms
step:140/1670 train_time:12960ms step_avg:92.57ms
step:141/1670 train_time:13053ms step_avg:92.57ms
step:142/1670 train_time:13143ms step_avg:92.56ms
step:143/1670 train_time:13234ms step_avg:92.54ms
step:144/1670 train_time:13325ms step_avg:92.53ms
step:145/1670 train_time:13415ms step_avg:92.52ms
step:146/1670 train_time:13505ms step_avg:92.50ms
step:147/1670 train_time:13597ms step_avg:92.50ms
step:148/1670 train_time:13688ms step_avg:92.49ms
step:149/1670 train_time:13781ms step_avg:92.49ms
step:150/1670 train_time:13873ms step_avg:92.49ms
step:151/1670 train_time:13966ms step_avg:92.49ms
step:152/1670 train_time:14061ms step_avg:92.51ms
step:153/1670 train_time:14152ms step_avg:92.49ms
step:154/1670 train_time:14243ms step_avg:92.49ms
step:155/1670 train_time:14333ms step_avg:92.47ms
step:156/1670 train_time:14425ms step_avg:92.47ms
step:157/1670 train_time:14516ms step_avg:92.46ms
step:158/1670 train_time:14606ms step_avg:92.44ms
step:159/1670 train_time:14697ms step_avg:92.43ms
step:160/1670 train_time:14787ms step_avg:92.42ms
step:161/1670 train_time:14880ms step_avg:92.42ms
step:162/1670 train_time:14971ms step_avg:92.41ms
step:163/1670 train_time:15063ms step_avg:92.41ms
step:164/1670 train_time:15155ms step_avg:92.41ms
step:165/1670 train_time:15247ms step_avg:92.40ms
step:166/1670 train_time:15338ms step_avg:92.40ms
step:167/1670 train_time:15428ms step_avg:92.38ms
step:168/1670 train_time:15520ms step_avg:92.38ms
step:169/1670 train_time:15610ms step_avg:92.37ms
step:170/1670 train_time:15702ms step_avg:92.36ms
step:171/1670 train_time:15794ms step_avg:92.36ms
step:172/1670 train_time:15885ms step_avg:92.36ms
step:173/1670 train_time:15977ms step_avg:92.35ms
step:174/1670 train_time:16068ms step_avg:92.35ms
step:175/1670 train_time:16159ms step_avg:92.34ms
step:176/1670 train_time:16250ms step_avg:92.33ms
step:177/1670 train_time:16341ms step_avg:92.32ms
step:178/1670 train_time:16431ms step_avg:92.31ms
step:179/1670 train_time:16524ms step_avg:92.31ms
step:180/1670 train_time:16616ms step_avg:92.31ms
step:181/1670 train_time:16707ms step_avg:92.30ms
step:182/1670 train_time:16798ms step_avg:92.30ms
step:183/1670 train_time:16889ms step_avg:92.29ms
step:184/1670 train_time:16980ms step_avg:92.28ms
step:185/1670 train_time:17072ms step_avg:92.28ms
step:186/1670 train_time:17164ms step_avg:92.28ms
step:187/1670 train_time:17255ms step_avg:92.27ms
step:188/1670 train_time:17347ms step_avg:92.27ms
step:189/1670 train_time:17438ms step_avg:92.26ms
step:190/1670 train_time:17528ms step_avg:92.25ms
step:191/1670 train_time:17621ms step_avg:92.26ms
step:192/1670 train_time:17712ms step_avg:92.25ms
step:193/1670 train_time:17804ms step_avg:92.25ms
step:194/1670 train_time:17896ms step_avg:92.25ms
step:195/1670 train_time:17987ms step_avg:92.24ms
step:196/1670 train_time:18078ms step_avg:92.24ms
step:197/1670 train_time:18169ms step_avg:92.23ms
step:198/1670 train_time:18260ms step_avg:92.22ms
step:199/1670 train_time:18351ms step_avg:92.22ms
step:200/1670 train_time:18442ms step_avg:92.21ms
step:201/1670 train_time:18533ms step_avg:92.20ms
step:202/1670 train_time:18625ms step_avg:92.20ms
step:203/1670 train_time:18717ms step_avg:92.20ms
step:204/1670 train_time:18808ms step_avg:92.20ms
step:205/1670 train_time:18901ms step_avg:92.20ms
step:206/1670 train_time:18993ms step_avg:92.20ms
step:207/1670 train_time:19085ms step_avg:92.20ms
step:208/1670 train_time:19176ms step_avg:92.19ms
step:209/1670 train_time:19267ms step_avg:92.19ms
step:210/1670 train_time:19359ms step_avg:92.18ms
step:211/1670 train_time:19449ms step_avg:92.18ms
step:212/1670 train_time:19540ms step_avg:92.17ms
step:213/1670 train_time:19789ms step_avg:92.91ms
step:214/1670 train_time:19858ms step_avg:92.79ms
step:215/1670 train_time:19947ms step_avg:92.78ms
step:216/1670 train_time:20037ms step_avg:92.77ms
step:217/1670 train_time:20127ms step_avg:92.75ms
step:218/1670 train_time:20217ms step_avg:92.74ms
step:219/1670 train_time:20307ms step_avg:92.72ms
step:220/1670 train_time:20397ms step_avg:92.71ms
step:221/1670 train_time:20486ms step_avg:92.70ms
step:222/1670 train_time:20577ms step_avg:92.69ms
step:223/1670 train_time:20672ms step_avg:92.70ms
step:224/1670 train_time:20768ms step_avg:92.71ms
step:225/1670 train_time:20861ms step_avg:92.71ms
step:226/1670 train_time:20951ms step_avg:92.70ms
step:227/1670 train_time:21042ms step_avg:92.70ms
step:228/1670 train_time:21133ms step_avg:92.69ms
step:229/1670 train_time:21223ms step_avg:92.68ms
step:230/1670 train_time:21314ms step_avg:92.67ms
step:231/1670 train_time:21404ms step_avg:92.66ms
step:232/1670 train_time:21496ms step_avg:92.65ms
step:233/1670 train_time:21587ms step_avg:92.65ms
step:234/1670 train_time:21681ms step_avg:92.66ms
step:235/1670 train_time:21774ms step_avg:92.66ms
step:236/1670 train_time:21867ms step_avg:92.66ms
step:237/1670 train_time:21958ms step_avg:92.65ms
step:238/1670 train_time:22048ms step_avg:92.64ms
step:239/1670 train_time:22139ms step_avg:92.63ms
step:240/1670 train_time:22229ms step_avg:92.62ms
step:241/1670 train_time:22319ms step_avg:92.61ms
step:242/1670 train_time:22409ms step_avg:92.60ms
step:243/1670 train_time:22500ms step_avg:92.59ms
step:244/1670 train_time:22591ms step_avg:92.59ms
step:245/1670 train_time:22684ms step_avg:92.59ms
step:246/1670 train_time:22777ms step_avg:92.59ms
step:247/1670 train_time:22869ms step_avg:92.59ms
step:248/1670 train_time:22961ms step_avg:92.58ms
step:249/1670 train_time:23052ms step_avg:92.58ms
step:250/1670 train_time:23142ms step_avg:92.57ms
step:250/1670 val_loss:3.9705 train_time:23232ms step_avg:92.93ms
step:251/1670 train_time:23253ms step_avg:92.64ms
step:252/1670 train_time:23325ms step_avg:92.56ms
step:253/1670 train_time:23416ms step_avg:92.55ms
step:254/1670 train_time:23506ms step_avg:92.54ms
step:255/1670 train_time:23596ms step_avg:92.53ms
step:256/1670 train_time:23687ms step_avg:92.53ms
step:257/1670 train_time:23777ms step_avg:92.52ms
step:258/1670 train_time:23867ms step_avg:92.51ms
step:259/1670 train_time:23958ms step_avg:92.50ms
step:260/1670 train_time:24052ms step_avg:92.51ms
step:261/1670 train_time:24142ms step_avg:92.50ms
step:262/1670 train_time:24235ms step_avg:92.50ms
step:263/1670 train_time:24328ms step_avg:92.50ms
step:264/1670 train_time:24418ms step_avg:92.49ms
step:265/1670 train_time:24509ms step_avg:92.49ms
step:266/1670 train_time:24599ms step_avg:92.48ms
step:267/1670 train_time:24690ms step_avg:92.47ms
step:268/1670 train_time:24780ms step_avg:92.46ms
step:269/1670 train_time:24871ms step_avg:92.46ms
step:270/1670 train_time:24961ms step_avg:92.45ms
step:271/1670 train_time:25054ms step_avg:92.45ms
step:272/1670 train_time:25145ms step_avg:92.44ms
step:273/1670 train_time:25236ms step_avg:92.44ms
step:274/1670 train_time:25328ms step_avg:92.44ms
step:275/1670 train_time:25419ms step_avg:92.43ms
step:276/1670 train_time:25512ms step_avg:92.43ms
step:277/1670 train_time:25602ms step_avg:92.43ms
step:278/1670 train_time:25694ms step_avg:92.42ms
step:279/1670 train_time:25785ms step_avg:92.42ms
step:280/1670 train_time:25876ms step_avg:92.41ms
step:281/1670 train_time:25966ms step_avg:92.41ms
step:282/1670 train_time:26058ms step_avg:92.40ms
step:283/1670 train_time:26149ms step_avg:92.40ms
step:284/1670 train_time:26240ms step_avg:92.39ms
step:285/1670 train_time:26332ms step_avg:92.39ms
step:286/1670 train_time:26423ms step_avg:92.39ms
step:287/1670 train_time:26514ms step_avg:92.38ms
step:288/1670 train_time:26605ms step_avg:92.38ms
step:289/1670 train_time:26697ms step_avg:92.38ms
step:290/1670 train_time:26789ms step_avg:92.38ms
step:291/1670 train_time:26880ms step_avg:92.37ms
step:292/1670 train_time:26971ms step_avg:92.37ms
step:293/1670 train_time:27061ms step_avg:92.36ms
step:294/1670 train_time:27152ms step_avg:92.35ms
step:295/1670 train_time:27242ms step_avg:92.35ms
step:296/1670 train_time:27333ms step_avg:92.34ms
step:297/1670 train_time:27424ms step_avg:92.34ms
step:298/1670 train_time:27515ms step_avg:92.33ms
step:299/1670 train_time:27605ms step_avg:92.33ms
step:300/1670 train_time:27697ms step_avg:92.32ms
step:301/1670 train_time:27790ms step_avg:92.33ms
step:302/1670 train_time:27881ms step_avg:92.32ms
step:303/1670 train_time:27972ms step_avg:92.32ms
step:304/1670 train_time:28062ms step_avg:92.31ms
step:305/1670 train_time:28153ms step_avg:92.30ms
step:306/1670 train_time:28243ms step_avg:92.30ms
step:307/1670 train_time:28336ms step_avg:92.30ms
step:308/1670 train_time:28427ms step_avg:92.29ms
step:309/1670 train_time:28517ms step_avg:92.29ms
step:310/1670 train_time:28608ms step_avg:92.28ms
step:311/1670 train_time:28699ms step_avg:92.28ms
step:312/1670 train_time:28790ms step_avg:92.28ms
step:313/1670 train_time:28882ms step_avg:92.27ms
step:314/1670 train_time:28973ms step_avg:92.27ms
step:315/1670 train_time:29063ms step_avg:92.26ms
step:316/1670 train_time:29155ms step_avg:92.26ms
step:317/1670 train_time:29246ms step_avg:92.26ms
step:318/1670 train_time:29337ms step_avg:92.25ms
step:319/1670 train_time:29428ms step_avg:92.25ms
step:320/1670 train_time:29518ms step_avg:92.24ms
step:321/1670 train_time:29610ms step_avg:92.24ms
step:322/1670 train_time:29700ms step_avg:92.24ms
step:323/1670 train_time:29793ms step_avg:92.24ms
step:324/1670 train_time:29883ms step_avg:92.23ms
step:325/1670 train_time:29975ms step_avg:92.23ms
step:326/1670 train_time:30066ms step_avg:92.23ms
step:327/1670 train_time:30157ms step_avg:92.22ms
step:328/1670 train_time:30248ms step_avg:92.22ms
step:329/1670 train_time:30338ms step_avg:92.21ms
step:330/1670 train_time:30429ms step_avg:92.21ms
step:331/1670 train_time:30520ms step_avg:92.20ms
step:332/1670 train_time:30610ms step_avg:92.20ms
step:333/1670 train_time:30701ms step_avg:92.19ms
step:334/1670 train_time:30793ms step_avg:92.19ms
step:335/1670 train_time:30884ms step_avg:92.19ms
step:336/1670 train_time:30975ms step_avg:92.19ms
step:337/1670 train_time:31068ms step_avg:92.19ms
step:338/1670 train_time:31159ms step_avg:92.19ms
step:339/1670 train_time:31249ms step_avg:92.18ms
step:340/1670 train_time:31340ms step_avg:92.18ms
step:341/1670 train_time:31430ms step_avg:92.17ms
step:342/1670 train_time:31520ms step_avg:92.17ms
step:343/1670 train_time:31611ms step_avg:92.16ms
step:344/1670 train_time:31702ms step_avg:92.16ms
step:345/1670 train_time:31794ms step_avg:92.16ms
step:346/1670 train_time:31885ms step_avg:92.15ms
step:347/1670 train_time:31976ms step_avg:92.15ms
step:348/1670 train_time:32068ms step_avg:92.15ms
step:349/1670 train_time:32159ms step_avg:92.15ms
step:350/1670 train_time:32251ms step_avg:92.15ms
step:351/1670 train_time:32341ms step_avg:92.14ms
step:352/1670 train_time:32432ms step_avg:92.14ms
step:353/1670 train_time:32522ms step_avg:92.13ms
step:354/1670 train_time:32613ms step_avg:92.13ms
step:355/1670 train_time:32703ms step_avg:92.12ms
step:356/1670 train_time:32796ms step_avg:92.12ms
step:357/1670 train_time:32886ms step_avg:92.12ms
step:358/1670 train_time:32977ms step_avg:92.11ms
step:359/1670 train_time:33068ms step_avg:92.11ms
step:360/1670 train_time:33159ms step_avg:92.11ms
step:361/1670 train_time:33251ms step_avg:92.11ms
step:362/1670 train_time:33342ms step_avg:92.10ms
step:363/1670 train_time:33433ms step_avg:92.10ms
step:364/1670 train_time:33525ms step_avg:92.10ms
step:365/1670 train_time:33617ms step_avg:92.10ms
step:366/1670 train_time:33708ms step_avg:92.10ms
step:367/1670 train_time:33799ms step_avg:92.10ms
step:368/1670 train_time:33889ms step_avg:92.09ms
step:369/1670 train_time:33980ms step_avg:92.09ms
step:370/1670 train_time:34071ms step_avg:92.08ms
step:371/1670 train_time:34162ms step_avg:92.08ms
step:372/1670 train_time:34254ms step_avg:92.08ms
step:373/1670 train_time:34345ms step_avg:92.08ms
step:374/1670 train_time:34435ms step_avg:92.07ms
step:375/1670 train_time:34526ms step_avg:92.07ms
step:375/1670 val_loss:3.8117 train_time:34617ms step_avg:92.31ms
step:376/1670 train_time:34637ms step_avg:92.12ms
step:377/1670 train_time:34710ms step_avg:92.07ms
step:378/1670 train_time:34802ms step_avg:92.07ms
step:379/1670 train_time:34892ms step_avg:92.06ms
step:380/1670 train_time:34984ms step_avg:92.06ms
step:381/1670 train_time:35074ms step_avg:92.06ms
step:382/1670 train_time:35165ms step_avg:92.06ms
step:383/1670 train_time:35256ms step_avg:92.05ms
step:384/1670 train_time:35347ms step_avg:92.05ms
step:385/1670 train_time:35437ms step_avg:92.05ms
step:386/1670 train_time:35529ms step_avg:92.04ms
step:387/1670 train_time:35623ms step_avg:92.05ms
step:388/1670 train_time:35715ms step_avg:92.05ms
step:389/1670 train_time:35807ms step_avg:92.05ms
step:390/1670 train_time:35898ms step_avg:92.05ms
step:391/1670 train_time:35988ms step_avg:92.04ms
step:392/1670 train_time:36079ms step_avg:92.04ms
step:393/1670 train_time:36170ms step_avg:92.03ms
step:394/1670 train_time:36260ms step_avg:92.03ms
step:395/1670 train_time:36351ms step_avg:92.03ms
step:396/1670 train_time:36444ms step_avg:92.03ms
step:397/1670 train_time:36535ms step_avg:92.03ms
step:398/1670 train_time:36628ms step_avg:92.03ms
step:399/1670 train_time:36719ms step_avg:92.03ms
step:400/1670 train_time:36810ms step_avg:92.03ms
step:401/1670 train_time:36902ms step_avg:92.03ms
step:402/1670 train_time:36993ms step_avg:92.02ms
step:403/1670 train_time:37084ms step_avg:92.02ms
step:404/1670 train_time:37174ms step_avg:92.01ms
step:405/1670 train_time:37265ms step_avg:92.01ms
step:406/1670 train_time:37356ms step_avg:92.01ms
step:407/1670 train_time:37447ms step_avg:92.01ms
step:408/1670 train_time:37539ms step_avg:92.01ms
step:409/1670 train_time:37630ms step_avg:92.00ms
step:410/1670 train_time:37721ms step_avg:92.00ms
step:411/1670 train_time:37812ms step_avg:92.00ms
step:412/1670 train_time:37904ms step_avg:92.00ms
step:413/1670 train_time:37995ms step_avg:92.00ms
step:414/1670 train_time:38087ms step_avg:92.00ms
step:415/1670 train_time:38177ms step_avg:91.99ms
step:416/1670 train_time:38269ms step_avg:91.99ms
step:417/1670 train_time:38360ms step_avg:91.99ms
step:418/1670 train_time:38451ms step_avg:91.99ms
step:419/1670 train_time:38543ms step_avg:91.99ms
step:420/1670 train_time:38634ms step_avg:91.99ms
step:421/1670 train_time:38726ms step_avg:91.99ms
step:422/1670 train_time:38817ms step_avg:91.98ms
step:423/1670 train_time:38908ms step_avg:91.98ms
step:424/1670 train_time:39000ms step_avg:91.98ms
step:425/1670 train_time:39250ms step_avg:92.35ms
step:426/1670 train_time:39322ms step_avg:92.30ms
step:427/1670 train_time:39411ms step_avg:92.30ms
step:428/1670 train_time:39501ms step_avg:92.29ms
step:429/1670 train_time:39592ms step_avg:92.29ms
step:430/1670 train_time:39682ms step_avg:92.28ms
step:431/1670 train_time:39772ms step_avg:92.28ms
step:432/1670 train_time:39861ms step_avg:92.27ms
step:433/1670 train_time:39951ms step_avg:92.27ms
step:434/1670 train_time:40042ms step_avg:92.26ms
step:435/1670 train_time:40133ms step_avg:92.26ms
step:436/1670 train_time:40230ms step_avg:92.27ms
step:437/1670 train_time:40324ms step_avg:92.27ms
step:438/1670 train_time:40415ms step_avg:92.27ms
step:439/1670 train_time:40506ms step_avg:92.27ms
step:440/1670 train_time:40598ms step_avg:92.27ms
step:441/1670 train_time:40688ms step_avg:92.26ms
step:442/1670 train_time:40778ms step_avg:92.26ms
step:443/1670 train_time:40868ms step_avg:92.25ms
step:444/1670 train_time:40960ms step_avg:92.25ms
step:445/1670 train_time:41050ms step_avg:92.25ms
step:446/1670 train_time:41141ms step_avg:92.24ms
step:447/1670 train_time:41234ms step_avg:92.25ms
step:448/1670 train_time:41326ms step_avg:92.25ms
step:449/1670 train_time:41418ms step_avg:92.25ms
step:450/1670 train_time:41509ms step_avg:92.24ms
step:451/1670 train_time:41601ms step_avg:92.24ms
step:452/1670 train_time:41691ms step_avg:92.24ms
step:453/1670 train_time:41782ms step_avg:92.23ms
step:454/1670 train_time:41873ms step_avg:92.23ms
step:455/1670 train_time:41963ms step_avg:92.23ms
step:456/1670 train_time:42053ms step_avg:92.22ms
step:457/1670 train_time:42145ms step_avg:92.22ms
step:458/1670 train_time:42238ms step_avg:92.22ms
step:459/1670 train_time:42329ms step_avg:92.22ms
step:460/1670 train_time:42422ms step_avg:92.22ms
step:461/1670 train_time:42512ms step_avg:92.22ms
step:462/1670 train_time:42603ms step_avg:92.22ms
step:463/1670 train_time:42694ms step_avg:92.21ms
step:464/1670 train_time:42785ms step_avg:92.21ms
step:465/1670 train_time:42876ms step_avg:92.21ms
step:466/1670 train_time:42968ms step_avg:92.21ms
step:467/1670 train_time:43059ms step_avg:92.20ms
step:468/1670 train_time:43151ms step_avg:92.20ms
step:469/1670 train_time:43243ms step_avg:92.20ms
step:470/1670 train_time:43334ms step_avg:92.20ms
step:471/1670 train_time:43426ms step_avg:92.20ms
step:472/1670 train_time:43517ms step_avg:92.20ms
step:473/1670 train_time:43609ms step_avg:92.20ms
step:474/1670 train_time:43700ms step_avg:92.19ms
step:475/1670 train_time:43790ms step_avg:92.19ms
step:476/1670 train_time:43881ms step_avg:92.19ms
step:477/1670 train_time:43972ms step_avg:92.18ms
step:478/1670 train_time:44064ms step_avg:92.18ms
step:479/1670 train_time:44155ms step_avg:92.18ms
step:480/1670 train_time:44247ms step_avg:92.18ms
step:481/1670 train_time:44339ms step_avg:92.18ms
step:482/1670 train_time:44431ms step_avg:92.18ms
step:483/1670 train_time:44523ms step_avg:92.18ms
step:484/1670 train_time:44613ms step_avg:92.18ms
step:485/1670 train_time:44704ms step_avg:92.17ms
step:486/1670 train_time:44794ms step_avg:92.17ms
step:487/1670 train_time:44886ms step_avg:92.17ms
step:488/1670 train_time:44977ms step_avg:92.17ms
step:489/1670 train_time:45068ms step_avg:92.16ms
step:490/1670 train_time:45160ms step_avg:92.16ms
step:491/1670 train_time:45250ms step_avg:92.16ms
step:492/1670 train_time:45342ms step_avg:92.16ms
step:493/1670 train_time:45433ms step_avg:92.16ms
step:494/1670 train_time:45525ms step_avg:92.16ms
step:495/1670 train_time:45616ms step_avg:92.15ms
step:496/1670 train_time:45707ms step_avg:92.15ms
step:497/1670 train_time:45798ms step_avg:92.15ms
step:498/1670 train_time:45889ms step_avg:92.15ms
step:499/1670 train_time:45980ms step_avg:92.14ms
step:500/1670 train_time:46071ms step_avg:92.14ms
step:500/1670 val_loss:3.7132 train_time:46162ms step_avg:92.32ms
step:501/1670 train_time:46183ms step_avg:92.18ms
step:502/1670 train_time:46255ms step_avg:92.14ms
step:503/1670 train_time:46346ms step_avg:92.14ms
step:504/1670 train_time:46438ms step_avg:92.14ms
step:505/1670 train_time:46529ms step_avg:92.14ms
step:506/1670 train_time:46619ms step_avg:92.13ms
step:507/1670 train_time:46710ms step_avg:92.13ms
step:508/1670 train_time:46800ms step_avg:92.13ms
step:509/1670 train_time:46892ms step_avg:92.13ms
step:510/1670 train_time:46983ms step_avg:92.12ms
step:511/1670 train_time:47075ms step_avg:92.12ms
step:512/1670 train_time:47168ms step_avg:92.12ms
step:513/1670 train_time:47260ms step_avg:92.12ms
step:514/1670 train_time:47352ms step_avg:92.12ms
step:515/1670 train_time:47442ms step_avg:92.12ms
step:516/1670 train_time:47533ms step_avg:92.12ms
step:517/1670 train_time:47623ms step_avg:92.11ms
step:518/1670 train_time:47715ms step_avg:92.11ms
step:519/1670 train_time:47807ms step_avg:92.11ms
step:520/1670 train_time:47898ms step_avg:92.11ms
step:521/1670 train_time:47989ms step_avg:92.11ms
step:522/1670 train_time:48080ms step_avg:92.11ms
step:523/1670 train_time:48173ms step_avg:92.11ms
step:524/1670 train_time:48264ms step_avg:92.11ms
step:525/1670 train_time:48356ms step_avg:92.11ms
step:526/1670 train_time:48448ms step_avg:92.11ms
step:527/1670 train_time:48539ms step_avg:92.10ms
step:528/1670 train_time:48629ms step_avg:92.10ms
step:529/1670 train_time:48720ms step_avg:92.10ms
step:530/1670 train_time:48810ms step_avg:92.10ms
step:531/1670 train_time:48902ms step_avg:92.09ms
step:532/1670 train_time:48993ms step_avg:92.09ms
step:533/1670 train_time:49084ms step_avg:92.09ms
step:534/1670 train_time:49176ms step_avg:92.09ms
step:535/1670 train_time:49268ms step_avg:92.09ms
step:536/1670 train_time:49361ms step_avg:92.09ms
step:537/1670 train_time:49452ms step_avg:92.09ms
step:538/1670 train_time:49542ms step_avg:92.09ms
step:539/1670 train_time:49633ms step_avg:92.08ms
step:540/1670 train_time:49724ms step_avg:92.08ms
step:541/1670 train_time:49816ms step_avg:92.08ms
step:542/1670 train_time:49907ms step_avg:92.08ms
step:543/1670 train_time:49998ms step_avg:92.08ms
step:544/1670 train_time:50089ms step_avg:92.07ms
step:545/1670 train_time:50180ms step_avg:92.07ms
step:546/1670 train_time:50271ms step_avg:92.07ms
step:547/1670 train_time:50363ms step_avg:92.07ms
step:548/1670 train_time:50454ms step_avg:92.07ms
step:549/1670 train_time:50545ms step_avg:92.07ms
step:550/1670 train_time:50637ms step_avg:92.07ms
step:551/1670 train_time:50728ms step_avg:92.07ms
step:552/1670 train_time:50819ms step_avg:92.06ms
step:553/1670 train_time:50910ms step_avg:92.06ms
step:554/1670 train_time:51001ms step_avg:92.06ms
step:555/1670 train_time:51092ms step_avg:92.06ms
step:556/1670 train_time:51182ms step_avg:92.05ms
step:557/1670 train_time:51274ms step_avg:92.05ms
step:558/1670 train_time:51565ms step_avg:92.41ms
step:559/1670 train_time:51634ms step_avg:92.37ms
step:560/1670 train_time:51725ms step_avg:92.37ms
step:561/1670 train_time:51816ms step_avg:92.36ms
step:562/1670 train_time:51907ms step_avg:92.36ms
step:563/1670 train_time:51998ms step_avg:92.36ms
step:564/1670 train_time:52090ms step_avg:92.36ms
step:565/1670 train_time:52184ms step_avg:92.36ms
step:566/1670 train_time:52272ms step_avg:92.35ms
step:567/1670 train_time:52363ms step_avg:92.35ms
step:568/1670 train_time:52460ms step_avg:92.36ms
step:569/1670 train_time:52558ms step_avg:92.37ms
step:570/1670 train_time:52651ms step_avg:92.37ms
step:571/1670 train_time:52742ms step_avg:92.37ms
step:572/1670 train_time:52835ms step_avg:92.37ms
step:573/1670 train_time:52927ms step_avg:92.37ms
step:574/1670 train_time:53018ms step_avg:92.37ms
step:575/1670 train_time:53110ms step_avg:92.36ms
step:576/1670 train_time:53201ms step_avg:92.36ms
step:577/1670 train_time:53294ms step_avg:92.36ms
step:578/1670 train_time:53388ms step_avg:92.37ms
step:579/1670 train_time:53482ms step_avg:92.37ms
step:580/1670 train_time:53577ms step_avg:92.37ms
step:581/1670 train_time:53670ms step_avg:92.38ms
step:582/1670 train_time:53762ms step_avg:92.38ms
step:583/1670 train_time:53856ms step_avg:92.38ms
step:584/1670 train_time:53948ms step_avg:92.38ms
step:585/1670 train_time:54039ms step_avg:92.37ms
step:586/1670 train_time:54131ms step_avg:92.37ms
step:587/1670 train_time:54225ms step_avg:92.38ms
step:588/1670 train_time:54319ms step_avg:92.38ms
step:589/1670 train_time:54410ms step_avg:92.38ms
step:590/1670 train_time:54504ms step_avg:92.38ms
step:591/1670 train_time:54598ms step_avg:92.38ms
step:592/1670 train_time:54692ms step_avg:92.38ms
step:593/1670 train_time:54784ms step_avg:92.38ms
step:594/1670 train_time:54876ms step_avg:92.38ms
step:595/1670 train_time:54969ms step_avg:92.38ms
step:596/1670 train_time:55060ms step_avg:92.38ms
step:597/1670 train_time:55152ms step_avg:92.38ms
step:598/1670 train_time:55244ms step_avg:92.38ms
step:599/1670 train_time:55336ms step_avg:92.38ms
step:600/1670 train_time:55429ms step_avg:92.38ms
step:601/1670 train_time:55521ms step_avg:92.38ms
step:602/1670 train_time:55615ms step_avg:92.38ms
step:603/1670 train_time:55708ms step_avg:92.38ms
step:604/1670 train_time:55800ms step_avg:92.38ms
step:605/1670 train_time:55893ms step_avg:92.38ms
step:606/1670 train_time:55984ms step_avg:92.38ms
step:607/1670 train_time:56076ms step_avg:92.38ms
step:608/1670 train_time:56168ms step_avg:92.38ms
step:609/1670 train_time:56260ms step_avg:92.38ms
step:610/1670 train_time:56353ms step_avg:92.38ms
step:611/1670 train_time:56445ms step_avg:92.38ms
step:612/1670 train_time:56539ms step_avg:92.38ms
step:613/1670 train_time:56632ms step_avg:92.39ms
step:614/1670 train_time:56725ms step_avg:92.39ms
step:615/1670 train_time:56819ms step_avg:92.39ms
step:616/1670 train_time:56912ms step_avg:92.39ms
step:617/1670 train_time:57003ms step_avg:92.39ms
step:618/1670 train_time:57096ms step_avg:92.39ms
step:619/1670 train_time:57188ms step_avg:92.39ms
step:620/1670 train_time:57280ms step_avg:92.39ms
step:621/1670 train_time:57373ms step_avg:92.39ms
step:622/1670 train_time:57465ms step_avg:92.39ms
step:623/1670 train_time:57560ms step_avg:92.39ms
step:624/1670 train_time:57653ms step_avg:92.39ms
step:625/1670 train_time:57744ms step_avg:92.39ms
step:625/1670 val_loss:3.6122 train_time:57838ms step_avg:92.54ms
step:626/1670 train_time:57861ms step_avg:92.43ms
step:627/1670 train_time:57936ms step_avg:92.40ms
step:628/1670 train_time:58035ms step_avg:92.41ms
step:629/1670 train_time:58128ms step_avg:92.41ms
step:630/1670 train_time:58220ms step_avg:92.41ms
step:631/1670 train_time:58311ms step_avg:92.41ms
step:632/1670 train_time:58402ms step_avg:92.41ms
step:633/1670 train_time:58493ms step_avg:92.41ms
step:634/1670 train_time:58584ms step_avg:92.40ms
step:635/1670 train_time:58675ms step_avg:92.40ms
step:636/1670 train_time:58766ms step_avg:92.40ms
step:637/1670 train_time:58858ms step_avg:92.40ms
step:638/1670 train_time:58954ms step_avg:92.40ms
step:639/1670 train_time:59190ms step_avg:92.63ms
step:640/1670 train_time:59264ms step_avg:92.60ms
step:641/1670 train_time:59355ms step_avg:92.60ms
step:642/1670 train_time:59447ms step_avg:92.60ms
step:643/1670 train_time:59537ms step_avg:92.59ms
step:644/1670 train_time:59629ms step_avg:92.59ms
step:645/1670 train_time:59720ms step_avg:92.59ms
step:646/1670 train_time:59811ms step_avg:92.59ms
step:647/1670 train_time:59902ms step_avg:92.58ms
step:648/1670 train_time:59993ms step_avg:92.58ms
step:649/1670 train_time:60089ms step_avg:92.59ms
step:650/1670 train_time:60185ms step_avg:92.59ms
step:651/1670 train_time:60278ms step_avg:92.59ms
step:652/1670 train_time:60371ms step_avg:92.59ms
step:653/1670 train_time:60463ms step_avg:92.59ms
step:654/1670 train_time:60554ms step_avg:92.59ms
step:655/1670 train_time:60647ms step_avg:92.59ms
step:656/1670 train_time:60738ms step_avg:92.59ms
step:657/1670 train_time:60830ms step_avg:92.59ms
step:658/1670 train_time:60921ms step_avg:92.59ms
step:659/1670 train_time:61014ms step_avg:92.59ms
step:660/1670 train_time:61109ms step_avg:92.59ms
step:661/1670 train_time:61203ms step_avg:92.59ms
step:662/1670 train_time:61296ms step_avg:92.59ms
step:663/1670 train_time:61390ms step_avg:92.59ms
step:664/1670 train_time:61484ms step_avg:92.60ms
step:665/1670 train_time:61576ms step_avg:92.60ms
step:666/1670 train_time:61668ms step_avg:92.59ms
step:667/1670 train_time:61759ms step_avg:92.59ms
step:668/1670 train_time:61851ms step_avg:92.59ms
step:669/1670 train_time:61942ms step_avg:92.59ms
step:670/1670 train_time:62037ms step_avg:92.59ms
step:671/1670 train_time:62131ms step_avg:92.59ms
step:672/1670 train_time:62224ms step_avg:92.60ms
step:673/1670 train_time:62316ms step_avg:92.59ms
step:674/1670 train_time:62408ms step_avg:92.59ms
step:675/1670 train_time:62500ms step_avg:92.59ms
step:676/1670 train_time:62595ms step_avg:92.60ms
step:677/1670 train_time:62685ms step_avg:92.59ms
step:678/1670 train_time:62777ms step_avg:92.59ms
step:679/1670 train_time:62870ms step_avg:92.59ms
step:680/1670 train_time:62962ms step_avg:92.59ms
step:681/1670 train_time:63055ms step_avg:92.59ms
step:682/1670 train_time:63149ms step_avg:92.59ms
step:683/1670 train_time:63241ms step_avg:92.59ms
step:684/1670 train_time:63337ms step_avg:92.60ms
step:685/1670 train_time:63428ms step_avg:92.60ms
step:686/1670 train_time:63520ms step_avg:92.59ms
step:687/1670 train_time:63613ms step_avg:92.60ms
step:688/1670 train_time:63704ms step_avg:92.59ms
step:689/1670 train_time:63796ms step_avg:92.59ms
step:690/1670 train_time:63889ms step_avg:92.59ms
step:691/1670 train_time:63980ms step_avg:92.59ms
step:692/1670 train_time:64073ms step_avg:92.59ms
step:693/1670 train_time:64168ms step_avg:92.59ms
step:694/1670 train_time:64259ms step_avg:92.59ms
step:695/1670 train_time:64352ms step_avg:92.59ms
step:696/1670 train_time:64446ms step_avg:92.59ms
step:697/1670 train_time:64537ms step_avg:92.59ms
step:698/1670 train_time:64630ms step_avg:92.59ms
step:699/1670 train_time:64722ms step_avg:92.59ms
step:700/1670 train_time:64814ms step_avg:92.59ms
step:701/1670 train_time:64907ms step_avg:92.59ms
step:702/1670 train_time:65000ms step_avg:92.59ms
step:703/1670 train_time:65092ms step_avg:92.59ms
step:704/1670 train_time:65185ms step_avg:92.59ms
step:705/1670 train_time:65278ms step_avg:92.59ms
step:706/1670 train_time:65371ms step_avg:92.59ms
step:707/1670 train_time:65464ms step_avg:92.59ms
step:708/1670 train_time:65556ms step_avg:92.59ms
step:709/1670 train_time:65647ms step_avg:92.59ms
step:710/1670 train_time:65739ms step_avg:92.59ms
step:711/1670 train_time:65832ms step_avg:92.59ms
step:712/1670 train_time:65925ms step_avg:92.59ms
step:713/1670 train_time:66016ms step_avg:92.59ms
step:714/1670 train_time:66108ms step_avg:92.59ms
step:715/1670 train_time:66201ms step_avg:92.59ms
step:716/1670 train_time:66294ms step_avg:92.59ms
step:717/1670 train_time:66386ms step_avg:92.59ms
step:718/1670 train_time:66478ms step_avg:92.59ms
step:719/1670 train_time:66572ms step_avg:92.59ms
step:720/1670 train_time:66664ms step_avg:92.59ms
step:721/1670 train_time:66756ms step_avg:92.59ms
step:722/1670 train_time:66849ms step_avg:92.59ms
step:723/1670 train_time:66939ms step_avg:92.59ms
step:724/1670 train_time:67032ms step_avg:92.59ms
step:725/1670 train_time:67124ms step_avg:92.58ms
step:726/1670 train_time:67215ms step_avg:92.58ms
step:727/1670 train_time:67309ms step_avg:92.58ms
step:728/1670 train_time:67401ms step_avg:92.58ms
step:729/1670 train_time:67494ms step_avg:92.58ms
step:730/1670 train_time:67587ms step_avg:92.58ms
step:731/1670 train_time:67678ms step_avg:92.58ms
step:732/1670 train_time:67771ms step_avg:92.58ms
step:733/1670 train_time:67863ms step_avg:92.58ms
step:734/1670 train_time:67955ms step_avg:92.58ms
step:735/1670 train_time:68047ms step_avg:92.58ms
step:736/1670 train_time:68139ms step_avg:92.58ms
step:737/1670 train_time:68233ms step_avg:92.58ms
step:738/1670 train_time:68325ms step_avg:92.58ms
step:739/1670 train_time:68417ms step_avg:92.58ms
step:740/1670 train_time:68511ms step_avg:92.58ms
step:741/1670 train_time:68606ms step_avg:92.59ms
step:742/1670 train_time:68696ms step_avg:92.58ms
step:743/1670 train_time:68788ms step_avg:92.58ms
step:744/1670 train_time:68880ms step_avg:92.58ms
step:745/1670 train_time:68973ms step_avg:92.58ms
step:746/1670 train_time:69067ms step_avg:92.58ms
step:747/1670 train_time:69158ms step_avg:92.58ms
step:748/1670 train_time:69251ms step_avg:92.58ms
step:749/1670 train_time:69343ms step_avg:92.58ms
step:750/1670 train_time:69436ms step_avg:92.58ms
step:750/1670 val_loss:3.5597 train_time:69529ms step_avg:92.70ms
step:751/1670 train_time:69549ms step_avg:92.61ms
step:752/1670 train_time:69622ms step_avg:92.58ms
step:753/1670 train_time:69714ms step_avg:92.58ms
step:754/1670 train_time:69808ms step_avg:92.58ms
step:755/1670 train_time:69900ms step_avg:92.58ms
step:756/1670 train_time:69991ms step_avg:92.58ms
step:757/1670 train_time:70084ms step_avg:92.58ms
step:758/1670 train_time:70175ms step_avg:92.58ms
step:759/1670 train_time:70268ms step_avg:92.58ms
step:760/1670 train_time:70360ms step_avg:92.58ms
step:761/1670 train_time:70453ms step_avg:92.58ms
step:762/1670 train_time:70548ms step_avg:92.58ms
step:763/1670 train_time:70641ms step_avg:92.58ms
step:764/1670 train_time:70734ms step_avg:92.58ms
step:765/1670 train_time:70827ms step_avg:92.58ms
step:766/1670 train_time:70920ms step_avg:92.59ms
step:767/1670 train_time:71013ms step_avg:92.58ms
step:768/1670 train_time:71105ms step_avg:92.58ms
step:769/1670 train_time:71197ms step_avg:92.58ms
step:770/1670 train_time:71290ms step_avg:92.58ms
step:771/1670 train_time:71382ms step_avg:92.58ms
step:772/1670 train_time:71475ms step_avg:92.58ms
step:773/1670 train_time:71567ms step_avg:92.58ms
step:774/1670 train_time:71660ms step_avg:92.58ms
step:775/1670 train_time:71753ms step_avg:92.58ms
step:776/1670 train_time:71845ms step_avg:92.58ms
step:777/1670 train_time:71936ms step_avg:92.58ms
step:778/1670 train_time:72029ms step_avg:92.58ms
step:779/1670 train_time:72121ms step_avg:92.58ms
step:780/1670 train_time:72213ms step_avg:92.58ms
step:781/1670 train_time:72305ms step_avg:92.58ms
step:782/1670 train_time:72397ms step_avg:92.58ms
step:783/1670 train_time:72491ms step_avg:92.58ms
step:784/1670 train_time:72584ms step_avg:92.58ms
step:785/1670 train_time:72676ms step_avg:92.58ms
step:786/1670 train_time:72768ms step_avg:92.58ms
step:787/1670 train_time:72860ms step_avg:92.58ms
step:788/1670 train_time:72952ms step_avg:92.58ms
step:789/1670 train_time:73045ms step_avg:92.58ms
step:790/1670 train_time:73137ms step_avg:92.58ms
step:791/1670 train_time:73229ms step_avg:92.58ms
step:792/1670 train_time:73322ms step_avg:92.58ms
step:793/1670 train_time:73415ms step_avg:92.58ms
step:794/1670 train_time:73509ms step_avg:92.58ms
step:795/1670 train_time:73603ms step_avg:92.58ms
step:796/1670 train_time:73695ms step_avg:92.58ms
step:797/1670 train_time:73787ms step_avg:92.58ms
step:798/1670 train_time:73880ms step_avg:92.58ms
step:799/1670 train_time:73972ms step_avg:92.58ms
step:800/1670 train_time:74065ms step_avg:92.58ms
step:801/1670 train_time:74157ms step_avg:92.58ms
step:802/1670 train_time:74250ms step_avg:92.58ms
step:803/1670 train_time:74342ms step_avg:92.58ms
step:804/1670 train_time:74434ms step_avg:92.58ms
step:805/1670 train_time:74527ms step_avg:92.58ms
step:806/1670 train_time:74620ms step_avg:92.58ms
step:807/1670 train_time:74713ms step_avg:92.58ms
step:808/1670 train_time:74806ms step_avg:92.58ms
step:809/1670 train_time:74898ms step_avg:92.58ms
step:810/1670 train_time:74991ms step_avg:92.58ms
step:811/1670 train_time:75083ms step_avg:92.58ms
step:812/1670 train_time:75175ms step_avg:92.58ms
step:813/1670 train_time:75268ms step_avg:92.58ms
step:814/1670 train_time:75361ms step_avg:92.58ms
step:815/1670 train_time:75453ms step_avg:92.58ms
step:816/1670 train_time:75546ms step_avg:92.58ms
step:817/1670 train_time:75637ms step_avg:92.58ms
step:818/1670 train_time:75731ms step_avg:92.58ms
step:819/1670 train_time:75823ms step_avg:92.58ms
step:820/1670 train_time:75915ms step_avg:92.58ms
step:821/1670 train_time:76008ms step_avg:92.58ms
step:822/1670 train_time:76100ms step_avg:92.58ms
step:823/1670 train_time:76193ms step_avg:92.58ms
step:824/1670 train_time:76286ms step_avg:92.58ms
step:825/1670 train_time:76380ms step_avg:92.58ms
step:826/1670 train_time:76473ms step_avg:92.58ms
step:827/1670 train_time:76566ms step_avg:92.58ms
step:828/1670 train_time:76658ms step_avg:92.58ms
step:829/1670 train_time:76751ms step_avg:92.58ms
step:830/1670 train_time:76844ms step_avg:92.58ms
step:831/1670 train_time:76935ms step_avg:92.58ms
step:832/1670 train_time:77028ms step_avg:92.58ms
step:833/1670 train_time:77120ms step_avg:92.58ms
step:834/1670 train_time:77213ms step_avg:92.58ms
step:835/1670 train_time:77306ms step_avg:92.58ms
step:836/1670 train_time:77398ms step_avg:92.58ms
step:837/1670 train_time:77491ms step_avg:92.58ms
step:838/1670 train_time:77584ms step_avg:92.58ms
step:839/1670 train_time:77676ms step_avg:92.58ms
step:840/1670 train_time:77769ms step_avg:92.58ms
step:841/1670 train_time:77862ms step_avg:92.58ms
step:842/1670 train_time:77953ms step_avg:92.58ms
step:843/1670 train_time:78046ms step_avg:92.58ms
step:844/1670 train_time:78138ms step_avg:92.58ms
step:845/1670 train_time:78232ms step_avg:92.58ms
step:846/1670 train_time:78325ms step_avg:92.58ms
step:847/1670 train_time:78416ms step_avg:92.58ms
step:848/1670 train_time:78510ms step_avg:92.58ms
step:849/1670 train_time:78602ms step_avg:92.58ms
step:850/1670 train_time:78694ms step_avg:92.58ms
step:851/1670 train_time:78946ms step_avg:92.77ms
step:852/1670 train_time:79015ms step_avg:92.74ms
step:853/1670 train_time:79106ms step_avg:92.74ms
step:854/1670 train_time:79198ms step_avg:92.74ms
step:855/1670 train_time:79290ms step_avg:92.74ms
step:856/1670 train_time:79381ms step_avg:92.73ms
step:857/1670 train_time:79472ms step_avg:92.73ms
step:858/1670 train_time:79564ms step_avg:92.73ms
step:859/1670 train_time:79655ms step_avg:92.73ms
step:860/1670 train_time:79746ms step_avg:92.73ms
step:861/1670 train_time:79842ms step_avg:92.73ms
step:862/1670 train_time:79939ms step_avg:92.74ms
step:863/1670 train_time:80033ms step_avg:92.74ms
step:864/1670 train_time:80125ms step_avg:92.74ms
step:865/1670 train_time:80216ms step_avg:92.74ms
step:866/1670 train_time:80309ms step_avg:92.74ms
step:867/1670 train_time:80400ms step_avg:92.73ms
step:868/1670 train_time:80492ms step_avg:92.73ms
step:869/1670 train_time:80583ms step_avg:92.73ms
step:870/1670 train_time:80673ms step_avg:92.73ms
step:871/1670 train_time:80766ms step_avg:92.73ms
step:872/1670 train_time:80860ms step_avg:92.73ms
step:873/1670 train_time:80954ms step_avg:92.73ms
step:874/1670 train_time:81049ms step_avg:92.73ms
step:875/1670 train_time:81141ms step_avg:92.73ms
step:875/1670 val_loss:3.5163 train_time:81234ms step_avg:92.84ms
step:876/1670 train_time:81254ms step_avg:92.76ms
step:877/1670 train_time:81331ms step_avg:92.74ms
step:878/1670 train_time:81423ms step_avg:92.74ms
step:879/1670 train_time:81514ms step_avg:92.73ms
step:880/1670 train_time:81605ms step_avg:92.73ms
step:881/1670 train_time:81696ms step_avg:92.73ms
step:882/1670 train_time:81788ms step_avg:92.73ms
step:883/1670 train_time:81879ms step_avg:92.73ms
step:884/1670 train_time:81972ms step_avg:92.73ms
step:885/1670 train_time:82067ms step_avg:92.73ms
step:886/1670 train_time:82160ms step_avg:92.73ms
step:887/1670 train_time:82255ms step_avg:92.73ms
step:888/1670 train_time:82349ms step_avg:92.74ms
step:889/1670 train_time:82442ms step_avg:92.74ms
step:890/1670 train_time:82534ms step_avg:92.73ms
step:891/1670 train_time:82626ms step_avg:92.73ms
step:892/1670 train_time:82717ms step_avg:92.73ms
step:893/1670 train_time:82809ms step_avg:92.73ms
step:894/1670 train_time:82900ms step_avg:92.73ms
step:895/1670 train_time:82992ms step_avg:92.73ms
step:896/1670 train_time:83086ms step_avg:92.73ms
step:897/1670 train_time:83179ms step_avg:92.73ms
step:898/1670 train_time:83273ms step_avg:92.73ms
step:899/1670 train_time:83366ms step_avg:92.73ms
step:900/1670 train_time:83459ms step_avg:92.73ms
step:901/1670 train_time:83551ms step_avg:92.73ms
step:902/1670 train_time:83645ms step_avg:92.73ms
step:903/1670 train_time:83736ms step_avg:92.73ms
step:904/1670 train_time:83828ms step_avg:92.73ms
step:905/1670 train_time:83920ms step_avg:92.73ms
step:906/1670 train_time:84013ms step_avg:92.73ms
step:907/1670 train_time:84105ms step_avg:92.73ms
step:908/1670 train_time:84198ms step_avg:92.73ms
step:909/1670 train_time:84292ms step_avg:92.73ms
step:910/1670 train_time:84385ms step_avg:92.73ms
step:911/1670 train_time:84476ms step_avg:92.73ms
step:912/1670 train_time:84569ms step_avg:92.73ms
step:913/1670 train_time:84661ms step_avg:92.73ms
step:914/1670 train_time:84753ms step_avg:92.73ms
step:915/1670 train_time:84845ms step_avg:92.73ms
step:916/1670 train_time:84936ms step_avg:92.73ms
step:917/1670 train_time:85028ms step_avg:92.72ms
step:918/1670 train_time:85120ms step_avg:92.72ms
step:919/1670 train_time:85213ms step_avg:92.72ms
step:920/1670 train_time:85306ms step_avg:92.72ms
step:921/1670 train_time:85398ms step_avg:92.72ms
step:922/1670 train_time:85493ms step_avg:92.73ms
step:923/1670 train_time:85585ms step_avg:92.73ms
step:924/1670 train_time:85678ms step_avg:92.73ms
step:925/1670 train_time:85770ms step_avg:92.72ms
step:926/1670 train_time:85863ms step_avg:92.72ms
step:927/1670 train_time:85955ms step_avg:92.72ms
step:928/1670 train_time:86047ms step_avg:92.72ms
step:929/1670 train_time:86140ms step_avg:92.72ms
step:930/1670 train_time:86233ms step_avg:92.72ms
step:931/1670 train_time:86326ms step_avg:92.72ms
step:932/1670 train_time:86418ms step_avg:92.72ms
step:933/1670 train_time:86511ms step_avg:92.72ms
step:934/1670 train_time:86604ms step_avg:92.72ms
step:935/1670 train_time:86697ms step_avg:92.72ms
step:936/1670 train_time:86790ms step_avg:92.72ms
step:937/1670 train_time:86883ms step_avg:92.72ms
step:938/1670 train_time:86976ms step_avg:92.72ms
step:939/1670 train_time:87069ms step_avg:92.72ms
step:940/1670 train_time:87161ms step_avg:92.72ms
step:941/1670 train_time:87254ms step_avg:92.72ms
step:942/1670 train_time:87346ms step_avg:92.72ms
step:943/1670 train_time:87438ms step_avg:92.72ms
step:944/1670 train_time:87530ms step_avg:92.72ms
step:945/1670 train_time:87623ms step_avg:92.72ms
step:946/1670 train_time:87715ms step_avg:92.72ms
step:947/1670 train_time:87807ms step_avg:92.72ms
step:948/1670 train_time:87899ms step_avg:92.72ms
step:949/1670 train_time:87992ms step_avg:92.72ms
step:950/1670 train_time:88085ms step_avg:92.72ms
step:951/1670 train_time:88178ms step_avg:92.72ms
step:952/1670 train_time:88271ms step_avg:92.72ms
step:953/1670 train_time:88363ms step_avg:92.72ms
step:954/1670 train_time:88456ms step_avg:92.72ms
step:955/1670 train_time:88548ms step_avg:92.72ms
step:956/1670 train_time:88640ms step_avg:92.72ms
step:957/1670 train_time:88733ms step_avg:92.72ms
step:958/1670 train_time:88825ms step_avg:92.72ms
step:959/1670 train_time:88917ms step_avg:92.72ms
step:960/1670 train_time:89010ms step_avg:92.72ms
step:961/1670 train_time:89103ms step_avg:92.72ms
step:962/1670 train_time:89195ms step_avg:92.72ms
step:963/1670 train_time:89288ms step_avg:92.72ms
step:964/1670 train_time:89380ms step_avg:92.72ms
step:965/1670 train_time:89473ms step_avg:92.72ms
step:966/1670 train_time:89566ms step_avg:92.72ms
step:967/1670 train_time:89658ms step_avg:92.72ms
step:968/1670 train_time:89751ms step_avg:92.72ms
step:969/1670 train_time:89843ms step_avg:92.72ms
step:970/1670 train_time:89935ms step_avg:92.72ms
step:971/1670 train_time:90028ms step_avg:92.72ms
step:972/1670 train_time:90120ms step_avg:92.72ms
step:973/1670 train_time:90213ms step_avg:92.72ms
step:974/1670 train_time:90305ms step_avg:92.72ms
step:975/1670 train_time:90397ms step_avg:92.71ms
step:976/1670 train_time:90490ms step_avg:92.71ms
step:977/1670 train_time:90582ms step_avg:92.71ms
step:978/1670 train_time:90675ms step_avg:92.72ms
step:979/1670 train_time:90768ms step_avg:92.71ms
step:980/1670 train_time:90860ms step_avg:92.71ms
step:981/1670 train_time:90953ms step_avg:92.71ms
step:982/1670 train_time:91046ms step_avg:92.71ms
step:983/1670 train_time:91138ms step_avg:92.71ms
step:984/1670 train_time:91231ms step_avg:92.71ms
step:985/1670 train_time:91323ms step_avg:92.71ms
step:986/1670 train_time:91415ms step_avg:92.71ms
step:987/1670 train_time:91507ms step_avg:92.71ms
step:988/1670 train_time:91599ms step_avg:92.71ms
step:989/1670 train_time:91692ms step_avg:92.71ms
step:990/1670 train_time:91784ms step_avg:92.71ms
step:991/1670 train_time:91876ms step_avg:92.71ms
step:992/1670 train_time:91970ms step_avg:92.71ms
step:993/1670 train_time:92062ms step_avg:92.71ms
step:994/1670 train_time:92155ms step_avg:92.71ms
step:995/1670 train_time:92247ms step_avg:92.71ms
step:996/1670 train_time:92339ms step_avg:92.71ms
step:997/1670 train_time:92431ms step_avg:92.71ms
step:998/1670 train_time:92523ms step_avg:92.71ms
step:999/1670 train_time:92616ms step_avg:92.71ms
step:1000/1670 train_time:92708ms step_avg:92.71ms
step:1000/1670 val_loss:3.4665 train_time:92799ms step_avg:92.80ms
step:1001/1670 train_time:92819ms step_avg:92.73ms
step:1002/1670 train_time:92892ms step_avg:92.71ms
step:1003/1670 train_time:92985ms step_avg:92.71ms
step:1004/1670 train_time:93077ms step_avg:92.71ms
step:1005/1670 train_time:93170ms step_avg:92.71ms
step:1006/1670 train_time:93261ms step_avg:92.71ms
step:1007/1670 train_time:93353ms step_avg:92.70ms
step:1008/1670 train_time:93445ms step_avg:92.70ms
step:1009/1670 train_time:93537ms step_avg:92.70ms
step:1010/1670 train_time:93630ms step_avg:92.70ms
step:1011/1670 train_time:93723ms step_avg:92.70ms
step:1012/1670 train_time:93817ms step_avg:92.70ms
step:1013/1670 train_time:93910ms step_avg:92.70ms
step:1014/1670 train_time:94002ms step_avg:92.70ms
step:1015/1670 train_time:94095ms step_avg:92.70ms
step:1016/1670 train_time:94189ms step_avg:92.71ms
step:1017/1670 train_time:94281ms step_avg:92.70ms
step:1018/1670 train_time:94373ms step_avg:92.70ms
step:1019/1670 train_time:94464ms step_avg:92.70ms
step:1020/1670 train_time:94556ms step_avg:92.70ms
step:1021/1670 train_time:94649ms step_avg:92.70ms
step:1022/1670 train_time:94742ms step_avg:92.70ms
step:1023/1670 train_time:94835ms step_avg:92.70ms
step:1024/1670 train_time:94928ms step_avg:92.70ms
step:1025/1670 train_time:95021ms step_avg:92.70ms
step:1026/1670 train_time:95115ms step_avg:92.70ms
step:1027/1670 train_time:95208ms step_avg:92.70ms
step:1028/1670 train_time:95299ms step_avg:92.70ms
step:1029/1670 train_time:95391ms step_avg:92.70ms
step:1030/1670 train_time:95483ms step_avg:92.70ms
step:1031/1670 train_time:95576ms step_avg:92.70ms
step:1032/1670 train_time:95669ms step_avg:92.70ms
step:1033/1670 train_time:95761ms step_avg:92.70ms
step:1034/1670 train_time:95855ms step_avg:92.70ms
step:1035/1670 train_time:95948ms step_avg:92.70ms
step:1036/1670 train_time:96041ms step_avg:92.70ms
step:1037/1670 train_time:96134ms step_avg:92.70ms
step:1038/1670 train_time:96227ms step_avg:92.70ms
step:1039/1670 train_time:96319ms step_avg:92.70ms
step:1040/1670 train_time:96411ms step_avg:92.70ms
step:1041/1670 train_time:96503ms step_avg:92.70ms
step:1042/1670 train_time:96596ms step_avg:92.70ms
step:1043/1670 train_time:96688ms step_avg:92.70ms
step:1044/1670 train_time:96780ms step_avg:92.70ms
step:1045/1670 train_time:96873ms step_avg:92.70ms
step:1046/1670 train_time:96965ms step_avg:92.70ms
step:1047/1670 train_time:97059ms step_avg:92.70ms
step:1048/1670 train_time:97152ms step_avg:92.70ms
step:1049/1670 train_time:97244ms step_avg:92.70ms
step:1050/1670 train_time:97336ms step_avg:92.70ms
step:1051/1670 train_time:97429ms step_avg:92.70ms
step:1052/1670 train_time:97521ms step_avg:92.70ms
step:1053/1670 train_time:97614ms step_avg:92.70ms
step:1054/1670 train_time:97706ms step_avg:92.70ms
step:1055/1670 train_time:97798ms step_avg:92.70ms
step:1056/1670 train_time:97890ms step_avg:92.70ms
step:1057/1670 train_time:97983ms step_avg:92.70ms
step:1058/1670 train_time:98078ms step_avg:92.70ms
step:1059/1670 train_time:98169ms step_avg:92.70ms
step:1060/1670 train_time:98261ms step_avg:92.70ms
step:1061/1670 train_time:98354ms step_avg:92.70ms
step:1062/1670 train_time:98605ms step_avg:92.85ms
step:1063/1670 train_time:98676ms step_avg:92.83ms
step:1064/1670 train_time:98768ms step_avg:92.83ms
step:1065/1670 train_time:98858ms step_avg:92.82ms
step:1066/1670 train_time:98950ms step_avg:92.82ms
step:1067/1670 train_time:99041ms step_avg:92.82ms
step:1068/1670 train_time:99133ms step_avg:92.82ms
step:1069/1670 train_time:99225ms step_avg:92.82ms
step:1070/1670 train_time:99316ms step_avg:92.82ms
step:1071/1670 train_time:99408ms step_avg:92.82ms
step:1072/1670 train_time:99505ms step_avg:92.82ms
step:1073/1670 train_time:99601ms step_avg:92.83ms
step:1074/1670 train_time:99695ms step_avg:92.83ms
step:1075/1670 train_time:99787ms step_avg:92.82ms
step:1076/1670 train_time:99878ms step_avg:92.82ms
step:1077/1670 train_time:99970ms step_avg:92.82ms
step:1078/1670 train_time:100060ms step_avg:92.82ms
step:1079/1670 train_time:100153ms step_avg:92.82ms
step:1080/1670 train_time:100245ms step_avg:92.82ms
step:1081/1670 train_time:100336ms step_avg:92.82ms
step:1082/1670 train_time:100430ms step_avg:92.82ms
step:1083/1670 train_time:100524ms step_avg:92.82ms
step:1084/1670 train_time:100618ms step_avg:92.82ms
step:1085/1670 train_time:100713ms step_avg:92.82ms
step:1086/1670 train_time:100807ms step_avg:92.82ms
step:1087/1670 train_time:100898ms step_avg:92.82ms
step:1088/1670 train_time:100990ms step_avg:92.82ms
step:1089/1670 train_time:101081ms step_avg:92.82ms
step:1090/1670 train_time:101173ms step_avg:92.82ms
step:1091/1670 train_time:101264ms step_avg:92.82ms
step:1092/1670 train_time:101357ms step_avg:92.82ms
step:1093/1670 train_time:101451ms step_avg:92.82ms
step:1094/1670 train_time:101543ms step_avg:92.82ms
step:1095/1670 train_time:101638ms step_avg:92.82ms
step:1096/1670 train_time:101731ms step_avg:92.82ms
step:1097/1670 train_time:101823ms step_avg:92.82ms
step:1098/1670 train_time:101916ms step_avg:92.82ms
step:1099/1670 train_time:102007ms step_avg:92.82ms
step:1100/1670 train_time:102099ms step_avg:92.82ms
step:1101/1670 train_time:102191ms step_avg:92.82ms
step:1102/1670 train_time:102282ms step_avg:92.81ms
step:1103/1670 train_time:102375ms step_avg:92.81ms
step:1104/1670 train_time:102467ms step_avg:92.81ms
step:1105/1670 train_time:102560ms step_avg:92.81ms
step:1106/1670 train_time:102655ms step_avg:92.82ms
step:1107/1670 train_time:102749ms step_avg:92.82ms
step:1108/1670 train_time:102842ms step_avg:92.82ms
step:1109/1670 train_time:102935ms step_avg:92.82ms
step:1110/1670 train_time:103028ms step_avg:92.82ms
step:1111/1670 train_time:103121ms step_avg:92.82ms
step:1112/1670 train_time:103213ms step_avg:92.82ms
step:1113/1670 train_time:103304ms step_avg:92.82ms
step:1114/1670 train_time:103397ms step_avg:92.82ms
step:1115/1670 train_time:103685ms step_avg:92.99ms
step:1116/1670 train_time:103754ms step_avg:92.97ms
step:1117/1670 train_time:103845ms step_avg:92.97ms
step:1118/1670 train_time:103936ms step_avg:92.97ms
step:1119/1670 train_time:104028ms step_avg:92.97ms
step:1120/1670 train_time:104120ms step_avg:92.96ms
step:1121/1670 train_time:104212ms step_avg:92.96ms
step:1122/1670 train_time:104304ms step_avg:92.96ms
step:1123/1670 train_time:104395ms step_avg:92.96ms
step:1124/1670 train_time:104487ms step_avg:92.96ms
step:1125/1670 train_time:104584ms step_avg:92.96ms
step:1125/1670 val_loss:3.4132 train_time:104682ms step_avg:93.05ms
step:1126/1670 train_time:104704ms step_avg:92.99ms
step:1127/1670 train_time:104782ms step_avg:92.97ms
step:1128/1670 train_time:104882ms step_avg:92.98ms
step:1129/1670 train_time:104977ms step_avg:92.98ms
step:1130/1670 train_time:105071ms step_avg:92.98ms
step:1131/1670 train_time:105163ms step_avg:92.98ms
step:1132/1670 train_time:105255ms step_avg:92.98ms
step:1133/1670 train_time:105347ms step_avg:92.98ms
step:1134/1670 train_time:105438ms step_avg:92.98ms
step:1135/1670 train_time:105530ms step_avg:92.98ms
step:1136/1670 train_time:105622ms step_avg:92.98ms
step:1137/1670 train_time:105717ms step_avg:92.98ms
step:1138/1670 train_time:105812ms step_avg:92.98ms
step:1139/1670 train_time:105908ms step_avg:92.98ms
step:1140/1670 train_time:106002ms step_avg:92.98ms
step:1141/1670 train_time:106095ms step_avg:92.98ms
step:1142/1670 train_time:106187ms step_avg:92.98ms
step:1143/1670 train_time:106279ms step_avg:92.98ms
step:1144/1670 train_time:106372ms step_avg:92.98ms
step:1145/1670 train_time:106465ms step_avg:92.98ms
step:1146/1670 train_time:106557ms step_avg:92.98ms
step:1147/1670 train_time:106649ms step_avg:92.98ms
step:1148/1670 train_time:106744ms step_avg:92.98ms
step:1149/1670 train_time:106839ms step_avg:92.98ms
step:1150/1670 train_time:106933ms step_avg:92.99ms
step:1151/1670 train_time:107027ms step_avg:92.99ms
step:1152/1670 train_time:107120ms step_avg:92.99ms
step:1153/1670 train_time:107212ms step_avg:92.99ms
step:1154/1670 train_time:107305ms step_avg:92.99ms
step:1155/1670 train_time:107398ms step_avg:92.98ms
step:1156/1670 train_time:107489ms step_avg:92.98ms
step:1157/1670 train_time:107582ms step_avg:92.98ms
step:1158/1670 train_time:107676ms step_avg:92.98ms
step:1159/1670 train_time:107771ms step_avg:92.99ms
step:1160/1670 train_time:107866ms step_avg:92.99ms
step:1161/1670 train_time:107959ms step_avg:92.99ms
step:1162/1670 train_time:108052ms step_avg:92.99ms
step:1163/1670 train_time:108146ms step_avg:92.99ms
step:1164/1670 train_time:108239ms step_avg:92.99ms
step:1165/1670 train_time:108331ms step_avg:92.99ms
step:1166/1670 train_time:108424ms step_avg:92.99ms
step:1167/1670 train_time:108517ms step_avg:92.99ms
step:1168/1670 train_time:108610ms step_avg:92.99ms
step:1169/1670 train_time:108704ms step_avg:92.99ms
step:1170/1670 train_time:108798ms step_avg:92.99ms
step:1171/1670 train_time:108891ms step_avg:92.99ms
step:1172/1670 train_time:108985ms step_avg:92.99ms
step:1173/1670 train_time:109080ms step_avg:92.99ms
step:1174/1670 train_time:109172ms step_avg:92.99ms
step:1175/1670 train_time:109266ms step_avg:92.99ms
step:1176/1670 train_time:109359ms step_avg:92.99ms
step:1177/1670 train_time:109451ms step_avg:92.99ms
step:1178/1670 train_time:109543ms step_avg:92.99ms
step:1179/1670 train_time:109636ms step_avg:92.99ms
step:1180/1670 train_time:109731ms step_avg:92.99ms
step:1181/1670 train_time:109826ms step_avg:92.99ms
step:1182/1670 train_time:109920ms step_avg:92.99ms
step:1183/1670 train_time:110013ms step_avg:93.00ms
step:1184/1670 train_time:110108ms step_avg:93.00ms
step:1185/1670 train_time:110201ms step_avg:93.00ms
step:1186/1670 train_time:110293ms step_avg:93.00ms
step:1187/1670 train_time:110386ms step_avg:93.00ms
step:1188/1670 train_time:110479ms step_avg:93.00ms
step:1189/1670 train_time:110572ms step_avg:93.00ms
step:1190/1670 train_time:110666ms step_avg:93.00ms
step:1191/1670 train_time:110759ms step_avg:93.00ms
step:1192/1670 train_time:110852ms step_avg:93.00ms
step:1193/1670 train_time:110947ms step_avg:93.00ms
step:1194/1670 train_time:111040ms step_avg:93.00ms
step:1195/1670 train_time:111133ms step_avg:93.00ms
step:1196/1670 train_time:111225ms step_avg:93.00ms
step:1197/1670 train_time:111319ms step_avg:93.00ms
step:1198/1670 train_time:111411ms step_avg:93.00ms
step:1199/1670 train_time:111504ms step_avg:93.00ms
step:1200/1670 train_time:111597ms step_avg:93.00ms
step:1201/1670 train_time:111690ms step_avg:93.00ms
step:1202/1670 train_time:111785ms step_avg:93.00ms
step:1203/1670 train_time:111879ms step_avg:93.00ms
step:1204/1670 train_time:111972ms step_avg:93.00ms
step:1205/1670 train_time:112067ms step_avg:93.00ms
step:1206/1670 train_time:112160ms step_avg:93.00ms
step:1207/1670 train_time:112253ms step_avg:93.00ms
step:1208/1670 train_time:112348ms step_avg:93.00ms
step:1209/1670 train_time:112441ms step_avg:93.00ms
step:1210/1670 train_time:112533ms step_avg:93.00ms
step:1211/1670 train_time:112626ms step_avg:93.00ms
step:1212/1670 train_time:112719ms step_avg:93.00ms
step:1213/1670 train_time:112812ms step_avg:93.00ms
step:1214/1670 train_time:112906ms step_avg:93.00ms
step:1215/1670 train_time:112999ms step_avg:93.00ms
step:1216/1670 train_time:113092ms step_avg:93.00ms
step:1217/1670 train_time:113186ms step_avg:93.00ms
step:1218/1670 train_time:113279ms step_avg:93.00ms
step:1219/1670 train_time:113373ms step_avg:93.00ms
step:1220/1670 train_time:113466ms step_avg:93.01ms
step:1221/1670 train_time:113560ms step_avg:93.01ms
step:1222/1670 train_time:113652ms step_avg:93.01ms
step:1223/1670 train_time:113746ms step_avg:93.01ms
step:1224/1670 train_time:113840ms step_avg:93.01ms
step:1225/1670 train_time:113932ms step_avg:93.01ms
step:1226/1670 train_time:114027ms step_avg:93.01ms
step:1227/1670 train_time:114120ms step_avg:93.01ms
step:1228/1670 train_time:114212ms step_avg:93.01ms
step:1229/1670 train_time:114305ms step_avg:93.01ms
step:1230/1670 train_time:114398ms step_avg:93.01ms
step:1231/1670 train_time:114491ms step_avg:93.01ms
step:1232/1670 train_time:114585ms step_avg:93.01ms
step:1233/1670 train_time:114677ms step_avg:93.01ms
step:1234/1670 train_time:114772ms step_avg:93.01ms
step:1235/1670 train_time:114865ms step_avg:93.01ms
step:1236/1670 train_time:114959ms step_avg:93.01ms
step:1237/1670 train_time:115052ms step_avg:93.01ms
step:1238/1670 train_time:115146ms step_avg:93.01ms
step:1239/1670 train_time:115238ms step_avg:93.01ms
step:1240/1670 train_time:115331ms step_avg:93.01ms
step:1241/1670 train_time:115424ms step_avg:93.01ms
step:1242/1670 train_time:115517ms step_avg:93.01ms
step:1243/1670 train_time:115611ms step_avg:93.01ms
step:1244/1670 train_time:115705ms step_avg:93.01ms
step:1245/1670 train_time:115798ms step_avg:93.01ms
step:1246/1670 train_time:115891ms step_avg:93.01ms
step:1247/1670 train_time:115984ms step_avg:93.01ms
step:1248/1670 train_time:116077ms step_avg:93.01ms
step:1249/1670 train_time:116170ms step_avg:93.01ms
step:1250/1670 train_time:116263ms step_avg:93.01ms
step:1250/1670 val_loss:3.3746 train_time:116355ms step_avg:93.08ms
step:1251/1670 train_time:116375ms step_avg:93.03ms
step:1252/1670 train_time:116450ms step_avg:93.01ms
step:1253/1670 train_time:116544ms step_avg:93.01ms
step:1254/1670 train_time:116635ms step_avg:93.01ms
step:1255/1670 train_time:116728ms step_avg:93.01ms
step:1256/1670 train_time:116820ms step_avg:93.01ms
step:1257/1670 train_time:116913ms step_avg:93.01ms
step:1258/1670 train_time:117005ms step_avg:93.01ms
step:1259/1670 train_time:117097ms step_avg:93.01ms
step:1260/1670 train_time:117191ms step_avg:93.01ms
step:1261/1670 train_time:117288ms step_avg:93.01ms
step:1262/1670 train_time:117383ms step_avg:93.01ms
step:1263/1670 train_time:117477ms step_avg:93.01ms
step:1264/1670 train_time:117570ms step_avg:93.01ms
step:1265/1670 train_time:117664ms step_avg:93.01ms
step:1266/1670 train_time:117756ms step_avg:93.01ms
step:1267/1670 train_time:117851ms step_avg:93.02ms
step:1268/1670 train_time:117944ms step_avg:93.02ms
step:1269/1670 train_time:118036ms step_avg:93.01ms
step:1270/1670 train_time:118128ms step_avg:93.01ms
step:1271/1670 train_time:118222ms step_avg:93.02ms
step:1272/1670 train_time:118316ms step_avg:93.02ms
step:1273/1670 train_time:118410ms step_avg:93.02ms
step:1274/1670 train_time:118649ms step_avg:93.13ms
step:1275/1670 train_time:118728ms step_avg:93.12ms
step:1276/1670 train_time:118819ms step_avg:93.12ms
step:1277/1670 train_time:118911ms step_avg:93.12ms
step:1278/1670 train_time:119002ms step_avg:93.12ms
step:1279/1670 train_time:119094ms step_avg:93.11ms
step:1280/1670 train_time:119186ms step_avg:93.11ms
step:1281/1670 train_time:119278ms step_avg:93.11ms
step:1282/1670 train_time:119371ms step_avg:93.11ms
step:1283/1670 train_time:119463ms step_avg:93.11ms
step:1284/1670 train_time:119561ms step_avg:93.12ms
step:1285/1670 train_time:119658ms step_avg:93.12ms
step:1286/1670 train_time:119753ms step_avg:93.12ms
step:1287/1670 train_time:119846ms step_avg:93.12ms
step:1288/1670 train_time:119939ms step_avg:93.12ms
step:1289/1670 train_time:120033ms step_avg:93.12ms
step:1290/1670 train_time:120126ms step_avg:93.12ms
step:1291/1670 train_time:120218ms step_avg:93.12ms
step:1292/1670 train_time:120310ms step_avg:93.12ms
step:1293/1670 train_time:120402ms step_avg:93.12ms
step:1294/1670 train_time:120498ms step_avg:93.12ms
step:1295/1670 train_time:120593ms step_avg:93.12ms
step:1296/1670 train_time:120688ms step_avg:93.12ms
step:1297/1670 train_time:120781ms step_avg:93.12ms
step:1298/1670 train_time:120875ms step_avg:93.12ms
step:1299/1670 train_time:120968ms step_avg:93.12ms
step:1300/1670 train_time:121060ms step_avg:93.12ms
step:1301/1670 train_time:121153ms step_avg:93.12ms
step:1302/1670 train_time:121245ms step_avg:93.12ms
step:1303/1670 train_time:121336ms step_avg:93.12ms
step:1304/1670 train_time:121430ms step_avg:93.12ms
step:1305/1670 train_time:121523ms step_avg:93.12ms
step:1306/1670 train_time:121617ms step_avg:93.12ms
step:1307/1670 train_time:121711ms step_avg:93.12ms
step:1308/1670 train_time:121804ms step_avg:93.12ms
step:1309/1670 train_time:121898ms step_avg:93.12ms
step:1310/1670 train_time:121991ms step_avg:93.12ms
step:1311/1670 train_time:122084ms step_avg:93.12ms
step:1312/1670 train_time:122177ms step_avg:93.12ms
step:1313/1670 train_time:122270ms step_avg:93.12ms
step:1314/1670 train_time:122362ms step_avg:93.12ms
step:1315/1670 train_time:122456ms step_avg:93.12ms
step:1316/1670 train_time:122551ms step_avg:93.12ms
step:1317/1670 train_time:122644ms step_avg:93.12ms
step:1318/1670 train_time:122738ms step_avg:93.12ms
step:1319/1670 train_time:122832ms step_avg:93.12ms
step:1320/1670 train_time:122924ms step_avg:93.12ms
step:1321/1670 train_time:123017ms step_avg:93.12ms
step:1322/1670 train_time:123109ms step_avg:93.12ms
step:1323/1670 train_time:123202ms step_avg:93.12ms
step:1324/1670 train_time:123295ms step_avg:93.12ms
step:1325/1670 train_time:123389ms step_avg:93.12ms
step:1326/1670 train_time:123481ms step_avg:93.12ms
step:1327/1670 train_time:123576ms step_avg:93.12ms
step:1328/1670 train_time:123670ms step_avg:93.12ms
step:1329/1670 train_time:123763ms step_avg:93.12ms
step:1330/1670 train_time:123857ms step_avg:93.13ms
step:1331/1670 train_time:123952ms step_avg:93.13ms
step:1332/1670 train_time:124044ms step_avg:93.13ms
step:1333/1670 train_time:124136ms step_avg:93.13ms
step:1334/1670 train_time:124229ms step_avg:93.12ms
step:1335/1670 train_time:124321ms step_avg:93.12ms
step:1336/1670 train_time:124415ms step_avg:93.12ms
step:1337/1670 train_time:124508ms step_avg:93.13ms
step:1338/1670 train_time:124601ms step_avg:93.12ms
step:1339/1670 train_time:124695ms step_avg:93.13ms
step:1340/1670 train_time:124789ms step_avg:93.13ms
step:1341/1670 train_time:124881ms step_avg:93.13ms
step:1342/1670 train_time:124975ms step_avg:93.13ms
step:1343/1670 train_time:125068ms step_avg:93.13ms
step:1344/1670 train_time:125160ms step_avg:93.13ms
step:1345/1670 train_time:125254ms step_avg:93.13ms
step:1346/1670 train_time:125347ms step_avg:93.13ms
step:1347/1670 train_time:125441ms step_avg:93.13ms
step:1348/1670 train_time:125533ms step_avg:93.13ms
step:1349/1670 train_time:125626ms step_avg:93.13ms
step:1350/1670 train_time:125719ms step_avg:93.13ms
step:1351/1670 train_time:125813ms step_avg:93.13ms
step:1352/1670 train_time:125906ms step_avg:93.13ms
step:1353/1670 train_time:125999ms step_avg:93.13ms
step:1354/1670 train_time:126093ms step_avg:93.13ms
step:1355/1670 train_time:126186ms step_avg:93.13ms
step:1356/1670 train_time:126278ms step_avg:93.13ms
step:1357/1670 train_time:126372ms step_avg:93.13ms
step:1358/1670 train_time:126465ms step_avg:93.13ms
step:1359/1670 train_time:126558ms step_avg:93.13ms
step:1360/1670 train_time:126652ms step_avg:93.13ms
step:1361/1670 train_time:126746ms step_avg:93.13ms
step:1362/1670 train_time:126839ms step_avg:93.13ms
step:1363/1670 train_time:126933ms step_avg:93.13ms
step:1364/1670 train_time:127027ms step_avg:93.13ms
step:1365/1670 train_time:127121ms step_avg:93.13ms
step:1366/1670 train_time:127215ms step_avg:93.13ms
step:1367/1670 train_time:127308ms step_avg:93.13ms
step:1368/1670 train_time:127400ms step_avg:93.13ms
step:1369/1670 train_time:127494ms step_avg:93.13ms
step:1370/1670 train_time:127588ms step_avg:93.13ms
step:1371/1670 train_time:127681ms step_avg:93.13ms
step:1372/1670 train_time:127776ms step_avg:93.13ms
step:1373/1670 train_time:127869ms step_avg:93.13ms
step:1374/1670 train_time:127962ms step_avg:93.13ms
step:1375/1670 train_time:128056ms step_avg:93.13ms
step:1375/1670 val_loss:3.3403 train_time:128149ms step_avg:93.20ms
step:1376/1670 train_time:128169ms step_avg:93.15ms
step:1377/1670 train_time:128244ms step_avg:93.13ms
step:1378/1670 train_time:128337ms step_avg:93.13ms
step:1379/1670 train_time:128430ms step_avg:93.13ms
step:1380/1670 train_time:128522ms step_avg:93.13ms
step:1381/1670 train_time:128615ms step_avg:93.13ms
step:1382/1670 train_time:128708ms step_avg:93.13ms
step:1383/1670 train_time:128804ms step_avg:93.13ms
step:1384/1670 train_time:128898ms step_avg:93.13ms
step:1385/1670 train_time:128991ms step_avg:93.13ms
step:1386/1670 train_time:129085ms step_avg:93.14ms
step:1387/1670 train_time:129181ms step_avg:93.14ms
step:1388/1670 train_time:129274ms step_avg:93.14ms
step:1389/1670 train_time:129367ms step_avg:93.14ms
step:1390/1670 train_time:129459ms step_avg:93.14ms
step:1391/1670 train_time:129552ms step_avg:93.14ms
step:1392/1670 train_time:129644ms step_avg:93.14ms
step:1393/1670 train_time:129738ms step_avg:93.14ms
step:1394/1670 train_time:129830ms step_avg:93.13ms
step:1395/1670 train_time:129923ms step_avg:93.13ms
step:1396/1670 train_time:130017ms step_avg:93.14ms
step:1397/1670 train_time:130111ms step_avg:93.14ms
step:1398/1670 train_time:130205ms step_avg:93.14ms
step:1399/1670 train_time:130300ms step_avg:93.14ms
step:1400/1670 train_time:130392ms step_avg:93.14ms
step:1401/1670 train_time:130485ms step_avg:93.14ms
step:1402/1670 train_time:130578ms step_avg:93.14ms
step:1403/1670 train_time:130671ms step_avg:93.14ms
step:1404/1670 train_time:130764ms step_avg:93.14ms
step:1405/1670 train_time:130857ms step_avg:93.14ms
step:1406/1670 train_time:130949ms step_avg:93.14ms
step:1407/1670 train_time:131043ms step_avg:93.14ms
step:1408/1670 train_time:131137ms step_avg:93.14ms
step:1409/1670 train_time:131232ms step_avg:93.14ms
step:1410/1670 train_time:131325ms step_avg:93.14ms
step:1411/1670 train_time:131419ms step_avg:93.14ms
step:1412/1670 train_time:131513ms step_avg:93.14ms
step:1413/1670 train_time:131606ms step_avg:93.14ms
step:1414/1670 train_time:131700ms step_avg:93.14ms
step:1415/1670 train_time:131793ms step_avg:93.14ms
step:1416/1670 train_time:131886ms step_avg:93.14ms
step:1417/1670 train_time:131979ms step_avg:93.14ms
step:1418/1670 train_time:132071ms step_avg:93.14ms
step:1419/1670 train_time:132165ms step_avg:93.14ms
step:1420/1670 train_time:132258ms step_avg:93.14ms
step:1421/1670 train_time:132351ms step_avg:93.14ms
step:1422/1670 train_time:132444ms step_avg:93.14ms
step:1423/1670 train_time:132538ms step_avg:93.14ms
step:1424/1670 train_time:132630ms step_avg:93.14ms
step:1425/1670 train_time:132724ms step_avg:93.14ms
step:1426/1670 train_time:132818ms step_avg:93.14ms
step:1427/1670 train_time:132910ms step_avg:93.14ms
step:1428/1670 train_time:133004ms step_avg:93.14ms
step:1429/1670 train_time:133097ms step_avg:93.14ms
step:1430/1670 train_time:133190ms step_avg:93.14ms
step:1431/1670 train_time:133283ms step_avg:93.14ms
step:1432/1670 train_time:133376ms step_avg:93.14ms
step:1433/1670 train_time:133469ms step_avg:93.14ms
step:1434/1670 train_time:133562ms step_avg:93.14ms
step:1435/1670 train_time:133655ms step_avg:93.14ms
step:1436/1670 train_time:133747ms step_avg:93.14ms
step:1437/1670 train_time:133841ms step_avg:93.14ms
step:1438/1670 train_time:133934ms step_avg:93.14ms
step:1439/1670 train_time:134027ms step_avg:93.14ms
step:1440/1670 train_time:134121ms step_avg:93.14ms
step:1441/1670 train_time:134214ms step_avg:93.14ms
step:1442/1670 train_time:134308ms step_avg:93.14ms
step:1443/1670 train_time:134403ms step_avg:93.14ms
step:1444/1670 train_time:134496ms step_avg:93.14ms
step:1445/1670 train_time:134588ms step_avg:93.14ms
step:1446/1670 train_time:134681ms step_avg:93.14ms
step:1447/1670 train_time:134775ms step_avg:93.14ms
step:1448/1670 train_time:134867ms step_avg:93.14ms
step:1449/1670 train_time:134961ms step_avg:93.14ms
step:1450/1670 train_time:135055ms step_avg:93.14ms
step:1451/1670 train_time:135148ms step_avg:93.14ms
step:1452/1670 train_time:135242ms step_avg:93.14ms
step:1453/1670 train_time:135336ms step_avg:93.14ms
step:1454/1670 train_time:135429ms step_avg:93.14ms
step:1455/1670 train_time:135523ms step_avg:93.14ms
step:1456/1670 train_time:135615ms step_avg:93.14ms
step:1457/1670 train_time:135708ms step_avg:93.14ms
step:1458/1670 train_time:135803ms step_avg:93.14ms
step:1459/1670 train_time:135896ms step_avg:93.14ms
step:1460/1670 train_time:135989ms step_avg:93.14ms
step:1461/1670 train_time:136083ms step_avg:93.14ms
step:1462/1670 train_time:136176ms step_avg:93.14ms
step:1463/1670 train_time:136269ms step_avg:93.14ms
step:1464/1670 train_time:136364ms step_avg:93.14ms
step:1465/1670 train_time:136457ms step_avg:93.14ms
step:1466/1670 train_time:136550ms step_avg:93.14ms
step:1467/1670 train_time:136644ms step_avg:93.14ms
step:1468/1670 train_time:136737ms step_avg:93.15ms
step:1469/1670 train_time:136830ms step_avg:93.15ms
step:1470/1670 train_time:136923ms step_avg:93.14ms
step:1471/1670 train_time:137015ms step_avg:93.14ms
step:1472/1670 train_time:137109ms step_avg:93.14ms
step:1473/1670 train_time:137202ms step_avg:93.14ms
step:1474/1670 train_time:137297ms step_avg:93.15ms
step:1475/1670 train_time:137389ms step_avg:93.15ms
step:1476/1670 train_time:137482ms step_avg:93.15ms
step:1477/1670 train_time:137575ms step_avg:93.15ms
step:1478/1670 train_time:137668ms step_avg:93.14ms
step:1479/1670 train_time:137761ms step_avg:93.14ms
step:1480/1670 train_time:137854ms step_avg:93.14ms
step:1481/1670 train_time:137947ms step_avg:93.14ms
step:1482/1670 train_time:138040ms step_avg:93.14ms
step:1483/1670 train_time:138133ms step_avg:93.14ms
step:1484/1670 train_time:138226ms step_avg:93.14ms
step:1485/1670 train_time:138476ms step_avg:93.25ms
step:1486/1670 train_time:138546ms step_avg:93.23ms
step:1487/1670 train_time:138637ms step_avg:93.23ms
step:1488/1670 train_time:138729ms step_avg:93.23ms
step:1489/1670 train_time:138820ms step_avg:93.23ms
step:1490/1670 train_time:138912ms step_avg:93.23ms
step:1491/1670 train_time:139004ms step_avg:93.23ms
step:1492/1670 train_time:139096ms step_avg:93.23ms
step:1493/1670 train_time:139188ms step_avg:93.23ms
step:1494/1670 train_time:139280ms step_avg:93.23ms
step:1495/1670 train_time:139377ms step_avg:93.23ms
step:1496/1670 train_time:139474ms step_avg:93.23ms
step:1497/1670 train_time:139572ms step_avg:93.23ms
step:1498/1670 train_time:139665ms step_avg:93.23ms
step:1499/1670 train_time:139758ms step_avg:93.23ms
step:1500/1670 train_time:139849ms step_avg:93.23ms
step:1500/1670 val_loss:3.3104 train_time:139944ms step_avg:93.30ms
step:1501/1670 train_time:139964ms step_avg:93.25ms
step:1502/1670 train_time:140037ms step_avg:93.23ms
step:1503/1670 train_time:140130ms step_avg:93.23ms
step:1504/1670 train_time:140222ms step_avg:93.23ms
step:1505/1670 train_time:140315ms step_avg:93.23ms
step:1506/1670 train_time:140408ms step_avg:93.23ms
step:1507/1670 train_time:140501ms step_avg:93.23ms
step:1508/1670 train_time:140594ms step_avg:93.23ms
step:1509/1670 train_time:140689ms step_avg:93.23ms
step:1510/1670 train_time:140784ms step_avg:93.23ms
step:1511/1670 train_time:140877ms step_avg:93.23ms
step:1512/1670 train_time:140972ms step_avg:93.24ms
step:1513/1670 train_time:141066ms step_avg:93.24ms
step:1514/1670 train_time:141159ms step_avg:93.24ms
step:1515/1670 train_time:141252ms step_avg:93.24ms
step:1516/1670 train_time:141345ms step_avg:93.24ms
step:1517/1670 train_time:141437ms step_avg:93.23ms
step:1518/1670 train_time:141529ms step_avg:93.23ms
step:1519/1670 train_time:141623ms step_avg:93.23ms
step:1520/1670 train_time:141716ms step_avg:93.23ms
step:1521/1670 train_time:141811ms step_avg:93.24ms
step:1522/1670 train_time:141905ms step_avg:93.24ms
step:1523/1670 train_time:141998ms step_avg:93.24ms
step:1524/1670 train_time:142091ms step_avg:93.24ms
step:1525/1670 train_time:142185ms step_avg:93.24ms
step:1526/1670 train_time:142279ms step_avg:93.24ms
step:1527/1670 train_time:142371ms step_avg:93.24ms
step:1528/1670 train_time:142464ms step_avg:93.24ms
step:1529/1670 train_time:142557ms step_avg:93.24ms
step:1530/1670 train_time:142651ms step_avg:93.24ms
step:1531/1670 train_time:142745ms step_avg:93.24ms
step:1532/1670 train_time:142839ms step_avg:93.24ms
step:1533/1670 train_time:142932ms step_avg:93.24ms
step:1534/1670 train_time:143026ms step_avg:93.24ms
step:1535/1670 train_time:143120ms step_avg:93.24ms
step:1536/1670 train_time:143212ms step_avg:93.24ms
step:1537/1670 train_time:143306ms step_avg:93.24ms
step:1538/1670 train_time:143399ms step_avg:93.24ms
step:1539/1670 train_time:143492ms step_avg:93.24ms
step:1540/1670 train_time:143585ms step_avg:93.24ms
step:1541/1670 train_time:143678ms step_avg:93.24ms
step:1542/1670 train_time:143772ms step_avg:93.24ms
step:1543/1670 train_time:143865ms step_avg:93.24ms
step:1544/1670 train_time:143958ms step_avg:93.24ms
step:1545/1670 train_time:144052ms step_avg:93.24ms
step:1546/1670 train_time:144146ms step_avg:93.24ms
step:1547/1670 train_time:144240ms step_avg:93.24ms
step:1548/1670 train_time:144333ms step_avg:93.24ms
step:1549/1670 train_time:144427ms step_avg:93.24ms
step:1550/1670 train_time:144519ms step_avg:93.24ms
step:1551/1670 train_time:144612ms step_avg:93.24ms
step:1552/1670 train_time:144706ms step_avg:93.24ms
step:1553/1670 train_time:144799ms step_avg:93.24ms
step:1554/1670 train_time:144892ms step_avg:93.24ms
step:1555/1670 train_time:144986ms step_avg:93.24ms
step:1556/1670 train_time:145079ms step_avg:93.24ms
step:1557/1670 train_time:145172ms step_avg:93.24ms
step:1558/1670 train_time:145265ms step_avg:93.24ms
step:1559/1670 train_time:145358ms step_avg:93.24ms
step:1560/1670 train_time:145451ms step_avg:93.24ms
step:1561/1670 train_time:145544ms step_avg:93.24ms
step:1562/1670 train_time:145638ms step_avg:93.24ms
step:1563/1670 train_time:145732ms step_avg:93.24ms
step:1564/1670 train_time:145825ms step_avg:93.24ms
step:1565/1670 train_time:145918ms step_avg:93.24ms
step:1566/1670 train_time:146012ms step_avg:93.24ms
step:1567/1670 train_time:146106ms step_avg:93.24ms
step:1568/1670 train_time:146198ms step_avg:93.24ms
step:1569/1670 train_time:146291ms step_avg:93.24ms
step:1570/1670 train_time:146385ms step_avg:93.24ms
step:1571/1670 train_time:146478ms step_avg:93.24ms
step:1572/1670 train_time:146571ms step_avg:93.24ms
step:1573/1670 train_time:146664ms step_avg:93.24ms
step:1574/1670 train_time:146758ms step_avg:93.24ms
step:1575/1670 train_time:146852ms step_avg:93.24ms
step:1576/1670 train_time:146946ms step_avg:93.24ms
step:1577/1670 train_time:147039ms step_avg:93.24ms
step:1578/1670 train_time:147132ms step_avg:93.24ms
step:1579/1670 train_time:147227ms step_avg:93.24ms
step:1580/1670 train_time:147321ms step_avg:93.24ms
step:1581/1670 train_time:147414ms step_avg:93.24ms
step:1582/1670 train_time:147507ms step_avg:93.24ms
step:1583/1670 train_time:147600ms step_avg:93.24ms
step:1584/1670 train_time:147693ms step_avg:93.24ms
step:1585/1670 train_time:147786ms step_avg:93.24ms
step:1586/1670 train_time:147880ms step_avg:93.24ms
step:1587/1670 train_time:147973ms step_avg:93.24ms
step:1588/1670 train_time:148066ms step_avg:93.24ms
step:1589/1670 train_time:148160ms step_avg:93.24ms
step:1590/1670 train_time:148253ms step_avg:93.24ms
step:1591/1670 train_time:148348ms step_avg:93.24ms
step:1592/1670 train_time:148441ms step_avg:93.24ms
step:1593/1670 train_time:148534ms step_avg:93.24ms
step:1594/1670 train_time:148628ms step_avg:93.24ms
step:1595/1670 train_time:148721ms step_avg:93.24ms
step:1596/1670 train_time:148814ms step_avg:93.24ms
step:1597/1670 train_time:148907ms step_avg:93.24ms
step:1598/1670 train_time:149001ms step_avg:93.24ms
step:1599/1670 train_time:149093ms step_avg:93.24ms
step:1600/1670 train_time:149187ms step_avg:93.24ms
step:1601/1670 train_time:149281ms step_avg:93.24ms
step:1602/1670 train_time:149373ms step_avg:93.24ms
step:1603/1670 train_time:149466ms step_avg:93.24ms
step:1604/1670 train_time:149559ms step_avg:93.24ms
step:1605/1670 train_time:149652ms step_avg:93.24ms
step:1606/1670 train_time:149745ms step_avg:93.24ms
step:1607/1670 train_time:149838ms step_avg:93.24ms
step:1608/1670 train_time:149931ms step_avg:93.24ms
step:1609/1670 train_time:150025ms step_avg:93.24ms
step:1610/1670 train_time:150119ms step_avg:93.24ms
step:1611/1670 train_time:150211ms step_avg:93.24ms
step:1612/1670 train_time:150305ms step_avg:93.24ms
step:1613/1670 train_time:150398ms step_avg:93.24ms
step:1614/1670 train_time:150493ms step_avg:93.24ms
step:1615/1670 train_time:150588ms step_avg:93.24ms
step:1616/1670 train_time:150681ms step_avg:93.24ms
step:1617/1670 train_time:150773ms step_avg:93.24ms
step:1618/1670 train_time:150865ms step_avg:93.24ms
step:1619/1670 train_time:150958ms step_avg:93.24ms
step:1620/1670 train_time:151052ms step_avg:93.24ms
step:1621/1670 train_time:151146ms step_avg:93.24ms
step:1622/1670 train_time:151239ms step_avg:93.24ms
step:1623/1670 train_time:151332ms step_avg:93.24ms
step:1624/1670 train_time:151426ms step_avg:93.24ms
step:1625/1670 train_time:151519ms step_avg:93.24ms
step:1625/1670 val_loss:3.2852 train_time:151612ms step_avg:93.30ms
step:1626/1670 train_time:151632ms step_avg:93.25ms
step:1627/1670 train_time:151707ms step_avg:93.24ms
step:1628/1670 train_time:151800ms step_avg:93.24ms
step:1629/1670 train_time:151893ms step_avg:93.24ms
step:1630/1670 train_time:151985ms step_avg:93.24ms
step:1631/1670 train_time:152078ms step_avg:93.24ms
step:1632/1670 train_time:152171ms step_avg:93.24ms
step:1633/1670 train_time:152265ms step_avg:93.24ms
step:1634/1670 train_time:152357ms step_avg:93.24ms
step:1635/1670 train_time:152450ms step_avg:93.24ms
step:1636/1670 train_time:152544ms step_avg:93.24ms
step:1637/1670 train_time:152639ms step_avg:93.24ms
step:1638/1670 train_time:152734ms step_avg:93.24ms
step:1639/1670 train_time:152827ms step_avg:93.24ms
step:1640/1670 train_time:152920ms step_avg:93.24ms
step:1641/1670 train_time:153013ms step_avg:93.24ms
step:1642/1670 train_time:153105ms step_avg:93.24ms
step:1643/1670 train_time:153198ms step_avg:93.24ms
step:1644/1670 train_time:153292ms step_avg:93.24ms
step:1645/1670 train_time:153387ms step_avg:93.24ms
step:1646/1670 train_time:153481ms step_avg:93.24ms
step:1647/1670 train_time:153575ms step_avg:93.25ms
step:1648/1670 train_time:153670ms step_avg:93.25ms
step:1649/1670 train_time:153764ms step_avg:93.25ms
step:1650/1670 train_time:153857ms step_avg:93.25ms
step:1651/1670 train_time:153951ms step_avg:93.25ms
step:1652/1670 train_time:154044ms step_avg:93.25ms
step:1653/1670 train_time:154137ms step_avg:93.25ms
step:1654/1670 train_time:154230ms step_avg:93.25ms
step:1655/1670 train_time:154323ms step_avg:93.25ms
step:1656/1670 train_time:154416ms step_avg:93.25ms
step:1657/1670 train_time:154509ms step_avg:93.25ms
step:1658/1670 train_time:154603ms step_avg:93.25ms
step:1659/1670 train_time:154697ms step_avg:93.25ms
step:1660/1670 train_time:154790ms step_avg:93.25ms
step:1661/1670 train_time:154884ms step_avg:93.25ms
step:1662/1670 train_time:154977ms step_avg:93.25ms
step:1663/1670 train_time:155069ms step_avg:93.25ms
step:1664/1670 train_time:155162ms step_avg:93.25ms
step:1665/1670 train_time:155255ms step_avg:93.25ms
step:1666/1670 train_time:155349ms step_avg:93.25ms
step:1667/1670 train_time:155441ms step_avg:93.25ms
step:1668/1670 train_time:155536ms step_avg:93.25ms
step:1669/1670 train_time:155630ms step_avg:93.25ms
step:1670/1670 train_time:155723ms step_avg:93.25ms
step:1670/1670 val_loss:3.2767 train_time:155987ms step_avg:93.41ms
peak memory allocated: 31753 MiB reserved: 46816 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(
    x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)


@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)


@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(
    g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float
) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)


@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)


def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None


def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)


mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99


def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]


@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(
        pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M
    )

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr,
    C_ptr,
    M,
    K,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out


@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr,
    C_ptr,
    M,
    a_stride_b,
    a_stride_r,
    a_stride_c,
    c_stride_b,
    c_stride_r,
    c_stride_c,
    alpha,
    beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (
        n_idx + BLOCK_SIZE_N <= m_idx
    )
    skip_block_above_diag = (LOWER_UPPER != 0) and (
        m_idx + BLOCK_SIZE_M <= n_idx
    )
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (
        offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c
    )
    at_ptrs = A_ptr + (
        offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r
    )

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(
            a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0
        )
        at = tl.load(
            at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0
        )
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (
        offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c
    )
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (
        offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c
    )
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (
        offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c
    )
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)


def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size
        * triton.cdiv(M, meta["BLOCK_SIZE_M"])
        * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out


@torch.compile(
    dynamic=False, fullgraph=True
)  # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Muon optimizer


class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """

    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(
        self,
        params,
        lr: float = 1e-3,
        betas: tuple[float, float] = (0.9, 0.999),
        eps: float = 1e-8,
        weight_decay: float = 0.01,
    ):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(
                    dist.reduce_scatter_tensor(
                        grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True
                    ).get_future()
                )
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group["betas"]
            eps = group["eps"]
            wd = group["weight_decay"]
            params = group["params"]
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size : (rank + 1) * rank_size]
                lr = group["lr"] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(
                    g_slice, g_slice, value=1 - beta2
                )
                # bias corrections
                bias1 = 1 - beta1**t
                bias2 = 1 - beta2**t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(
                    dist.all_gather_into_tensor(
                        p, p_slice, async_op=True
                    ).get_future()
                )
        torch.futures.collect_all(all_gather_futures).wait()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model


def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))


class CastedLinear(nn.Linear):
    def __init__(
        self,
        in_features: int,
        out_features: int,
        use_fp8=False,
        x_s=1.0,
        w_s=1.0,
        grad_s=1.0,
    ):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (
            self.in_features**-0.5
        )  # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3**0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(
                _x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s
            )[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)


@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    rotary_cos: torch.Tensor
    rotary_sin: torch.Tensor
    attn_scale: float


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[3].zero_()  # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1)  # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        rotary_cos, rotary_sin = attn_args.rotary_cos, attn_args.rotary_sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = (
            attn_args.seqlens,
            attn_args.attn_scale,
            attn_args.bm_size,
        )

        q, k, v = (
            F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x))
            .view(B, T, 3 * self.num_heads, self.head_dim)
            .chunk(3, dim=-2)
        )
        q, k = norm(q), norm(k)  # QK norm @Grad62304977
        q, k = (
            rotary(q, rotary_cos, rotary_sin),
            rotary(k, rotary_cos, rotary_sin),
        )
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(
                v
            )  # @ KoszarskyB & @Grad62304977
        else:  # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = (
            args.train_max_seq_len
            if self.training
            else (args.val_batch_size // (grad_accum_steps * world_size))
        )

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(
            q[0],
            k[0],
            v[0],
            cu_seqlens_q=seqlens,
            cu_seqlens_k=seqlens,
            max_seqlen_q=max_len,
            max_seqlen_k=max_len,
            causal=True,
            softmax_scale=attn_scale,
            window_size=(bm_size, 0),
        )
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(
            self.attn_gate(x[..., : self.attn_gate.weight.size(-1)])
        ).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(
            B, T, self.num_heads * self.head_dim
        )  # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim**-0.5)
        bound = (3**0.5) * std  # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_()  # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(
            x
        ).square()  # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = (
            CausalSelfAttention(dim, head_dim, num_heads)
            if layer_idx != 7
            else None
        )
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(
        self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs
    ):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x


# -----------------------------------------------------------------------------
# The main model


def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)


class GPT(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        num_layers: int,
        num_heads: int,
        head_dim: int,
        model_dim: int,
        max_seq_len: int,
    ):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList(
            [nn.Embedding(vocab_size, model_dim) for _ in range(3)]
        )
        self.blocks = nn.ModuleList(
            [
                Block(model_dim, head_dim, num_heads, i)
                for i in range(num_layers)
            ]
        )
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(
            model_dim,
            vocab_size,
            use_fp8=use_fp8,
            x_s=(model_dim**0.5) / 448,
            w_s=2**-9,
            grad_s=1 / 448,
        )
        self.lm_head.weight.detach().zero_()  # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.ones(pad),
                ]
            )
        )
        self.max_seq_len = max_seq_len
        self.setup_yarn(head_dim)
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.0
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.0
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def setup_yarn(self, head_dim: int):
        # store single copy of rotary tensors
        angular_freq = (1 / 1024) ** torch.linspace(
            0, 1, steps=head_dim // 4, dtype=torch.float32
        )
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat(
            [angular_freq, angular_freq.new_zeros(head_dim // 4)]
        )
        t = torch.arange(self.max_seq_len, dtype=torch.float32)
        theta = torch.outer(t, angular_freq)
        self.rotary_cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.rotary_sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq

        # scale attention factor f in attn=softmax(f*qk) logarithmically with window size @classiclarryd
        windows = list(
            dict.fromkeys(list(args.ws_schedule) + [args.ws_validate])
        )
        scale_factors = [
            0.2 * math.log(curr / prev) + 1
            for prev, curr in zip(windows[:-1], windows[1:])
        ]
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        attn_scales = list(
            accumulate([0.1] + scale_factors, lambda acc, factor: acc * factor)
        )
        self.attn_scales = dict(zip(windows, attn_scales))

    def apply_yarn(
        self, old_window: int, new_window: int, alpha: int = 1, beta: int = 32
    ):
        rotations = (
            args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        )
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp(
            (rotations - alpha) / (beta - alpha), 0, 1
        )
        self.angular_freq *= scaling_factor + interpolation_weight * (
            1 - scaling_factor
        )
        t = torch.arange(
            self.max_seq_len,
            dtype=torch.float32,
            device=self.angular_freq.device,
        )
        theta = torch.outer(t, self.angular_freq)
        self.rotary_cos.copy_(theta.cos())
        self.rotary_sin.copy_(theta.sin())

    def forward(
        self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int
    ):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = (
            [ve[0], ve[1], ve[2]]
            + [None] * (len(self.blocks) - 6)
            + [ve[0], ve[1], ve[2]]
        )
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        bm_sizes = [
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            long_bm,
            short_bm,
            short_bm,
            short_bm,
            long_bm,
        ]
        assert len(bm_sizes) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]).to(
            torch.bfloat16
        )  # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[: (len(self.blocks) // 2)]
        lambdas = self.scalars[
            1 * len(self.blocks) : 3 * len(self.blocks)
        ].view(-1, 2)
        sa_lambdas = self.scalars[
            3 * len(self.blocks) : 5 * len(self.blocks)
        ].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                rotary_cos=self.rotary_cos,
                rotary_sin=self.rotary_sin,
                attn_scale=self.attn_scales[ws],
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(
            logits.view(-1, logits.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss


# -----------------------------------------------------------------------------
# Distributed data loader


def _load_data_shard(file: Path):
    header = torch.from_file(
        str(file), False, 256, dtype=torch.int32
    )  # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2])  # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(
            num_tokens, dtype=torch.uint16, pin_memory=True
        )  # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(
            tokens.numpy()
        )  # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, (
            "number of tokens read does not match header"
        )
    return tokens


BOS_ID = 50256


class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1):
        # Precompute BOS positions once per shard
        self.size = tokens.numel()
        self.bos_idx = (
            (tokens == BOS_ID)
            .nonzero(as_tuple=True)[0]
            .to(torch.int64)
            .cpu()
            .numpy()
        )
        self.i = 0
        self.world_size = world_size

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(
                        f"Insufficient BOS ahead of position {cur}; hit tail of shard."
                    )
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(
                    self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                    cur + max_seq_len,
                    cur + num_tokens_local - cur_len + 1,
                )
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx

        return starts, ends


def distributed_data_generator(
    filename_pattern: str,
    num_tokens: int,
    max_seq_len: int,
    grad_accum_steps: int = 1,
    align_to_bos: bool = True,
):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, (
        "Batch size must be divisible by world size"
    )
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(
            f"No files found for pattern: {filename_pattern}"
        )

    file_iter = iter(
        files
    )  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    finder = BOSFinder(tokens, world_size=world_size) if align_to_bos else None
    pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(
            num_tokens_local // 300, n=128
        )  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(
                    num_tokens_local, max_seq_len
                )
                start_idxs, end_idxs = (
                    torch.tensor(seq_starts[rank]),
                    torch.tensor(seq_ends[rank]),
                )
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens = _load_data_shard(next(file_iter))
                finder = BOSFinder(tokens, world_size=world_size)
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= (
                1  # last document was too long to account for _targets offset
            )
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(
                tokens
            ):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local : pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(
                num_tokens_local,
            )
            _targets = buf[1:].view(
                num_tokens_local,
            )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens

        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1 : len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(
                device="cuda", dtype=torch.int32, non_blocking=True
            ),
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, (
                "Num tokens must be divisible by world size"
            )
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main


@dataclass
class Hyperparameters:
    # data
    train_files: str = (
        "data/fineweb10B/fineweb_train_*.bin"  # input .bin to train on
    )
    val_files: str = "data/fineweb10B/fineweb_val_*.bin"  # input .bin to eval validation loss on
    val_tokens: int = 10485760  # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1670  # number of iterations to run
    cooldown_frac: int = (
        0.5  # fraction of training spent cooling down the learning rate
    )
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = (
        125  # every how many steps to evaluate val loss? 0 for only at the end
    )
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13  # increase final validation ws @classiclarryd


args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = rank == 0  # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)


def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)


# begin by printing this file (the Python code)
print0(code)
print0("=" * 100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(
    f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}"
)
print0(f"Running Triton version {triton.__version__}")


def nvidia_smi():
    import subprocess  # avoid top level import

    return subprocess.run(
        ["nvidia-smi"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    ).stdout


print0(nvidia_smi())
print0("=" * 100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size)
    // (grad_accum_steps * world_size),
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [
    p
    for n, p in model.blocks.named_parameters()
    if p.ndim >= 2 and "embed" not in n
]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(
    hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]


# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr


def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]


model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(
    model=copy.deepcopy(model.state_dict()),
    optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers],
)  # save the initial state
train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    ws = args.ws_schedule[
        step % len(args.ws_schedule)
    ]  # each window size is a new graph, need to warm up each
    model(inputs, targets, cum_seqlens, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(
    args.train_files,
    args.train_batch_size,
    args.train_max_seq_len,
    grad_accum_steps=grad_accum_steps,
)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws = get_ws(0)
for step in range(train_steps + 1):
    last_step = step == train_steps
    new_ws = get_ws(step)
    if new_ws != ws:
        model.apply_yarn(ws, new_ws)
        ws = new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (
        args.val_loss_every > 0 and step % args.val_loss_every == 0
    ):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(
            args.val_files,
            args.val_batch_size,
            -1,
            grad_accum_steps=grad_accum_steps,
            align_to_bos=False,
        )
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(
            f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms / max(step, 1):.2f}ms",
            console=True,
        )
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(
                step=step,
                code=code,
                model=model.state_dict(),
                optimizers=[opt.state_dict() for opt in optimizers],
            )
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1)  # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (
        time.perf_counter() - t0
    )
    print0(
        f"step:{step + 1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms / (step + 1):.2f}ms",
        console=True,
    )

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.11 (main, Sep  2 2025, 14:20:58) [Clang 20.1.4 ]
Running PyTorch 2.9.0.dev20250718+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Thu Sep 11 09:16:45 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                  Off |
| N/A   40C    P0            126W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:06:00.0 Off |                  Off |
| N/A   45C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:65:00.0 Off |                  Off |
| N/A   45C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:68:00.0 Off |                  Off |
| N/A   37C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:85:00.0 Off |                  Off |
| N/A   38C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:86:00.0 Off |                  Off |
| N/A   45C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:E5:00.0 Off |                  Off |
| N/A   45C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E8:00.0 Off |                  Off |
| N/A   41C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1670 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/1670 train_time:296ms step_avg:296.32ms
step:2/1670 train_time:313ms step_avg:156.66ms
step:3/1670 train_time:383ms step_avg:127.83ms
step:4/1670 train_time:472ms step_avg:118.07ms
step:5/1670 train_time:563ms step_avg:112.54ms
step:6/1670 train_time:653ms step_avg:108.86ms
step:7/1670 train_time:744ms step_avg:106.32ms
step:8/1670 train_time:835ms step_avg:104.32ms
step:9/1670 train_time:925ms step_avg:102.77ms
step:10/1670 train_time:1016ms step_avg:101.58ms
step:11/1670 train_time:1108ms step_avg:100.69ms
step:12/1670 train_time:1203ms step_avg:100.28ms
step:13/1670 train_time:1299ms step_avg:99.90ms
step:14/1670 train_time:1392ms step_avg:99.46ms
step:15/1670 train_time:1484ms step_avg:98.91ms
step:16/1670 train_time:1574ms step_avg:98.39ms
step:17/1670 train_time:1666ms step_avg:97.98ms
step:18/1670 train_time:1756ms step_avg:97.55ms
step:19/1670 train_time:1846ms step_avg:97.18ms
step:20/1670 train_time:1938ms step_avg:96.88ms
step:21/1670 train_time:2029ms step_avg:96.61ms
step:22/1670 train_time:2120ms step_avg:96.37ms
step:23/1670 train_time:2213ms step_avg:96.20ms
step:24/1670 train_time:2307ms step_avg:96.13ms
step:25/1670 train_time:2400ms step_avg:96.01ms
step:26/1670 train_time:2491ms step_avg:95.81ms
step:27/1670 train_time:2583ms step_avg:95.66ms
step:28/1670 train_time:2674ms step_avg:95.50ms
step:29/1670 train_time:2766ms step_avg:95.38ms
step:30/1670 train_time:2857ms step_avg:95.24ms
step:31/1670 train_time:2949ms step_avg:95.12ms
step:32/1670 train_time:3040ms step_avg:95.00ms
step:33/1670 train_time:3131ms step_avg:94.88ms
step:34/1670 train_time:3223ms step_avg:94.80ms
step:35/1670 train_time:3315ms step_avg:94.72ms
step:36/1670 train_time:3408ms step_avg:94.66ms
step:37/1670 train_time:3501ms step_avg:94.61ms
step:38/1670 train_time:3591ms step_avg:94.50ms
step:39/1670 train_time:3683ms step_avg:94.43ms
step:40/1670 train_time:3773ms step_avg:94.34ms
step:41/1670 train_time:3866ms step_avg:94.28ms
step:42/1670 train_time:3957ms step_avg:94.22ms
step:43/1670 train_time:4049ms step_avg:94.16ms
step:44/1670 train_time:4141ms step_avg:94.12ms
step:45/1670 train_time:4233ms step_avg:94.06ms
step:46/1670 train_time:4326ms step_avg:94.05ms
step:47/1670 train_time:4418ms step_avg:93.99ms
step:48/1670 train_time:4509ms step_avg:93.93ms
step:49/1670 train_time:4601ms step_avg:93.90ms
step:50/1670 train_time:4692ms step_avg:93.85ms
step:51/1670 train_time:4783ms step_avg:93.79ms
step:52/1670 train_time:4875ms step_avg:93.75ms
step:53/1670 train_time:4967ms step_avg:93.72ms
step:54/1670 train_time:5059ms step_avg:93.68ms
step:55/1670 train_time:5150ms step_avg:93.63ms
step:56/1670 train_time:5244ms step_avg:93.64ms
step:57/1670 train_time:5336ms step_avg:93.61ms
step:58/1670 train_time:5428ms step_avg:93.58ms
step:59/1670 train_time:5520ms step_avg:93.55ms
step:60/1670 train_time:5611ms step_avg:93.51ms
step:61/1670 train_time:5702ms step_avg:93.48ms
step:62/1670 train_time:5794ms step_avg:93.45ms
step:63/1670 train_time:5885ms step_avg:93.42ms
step:64/1670 train_time:5977ms step_avg:93.39ms
step:65/1670 train_time:6068ms step_avg:93.35ms
step:66/1670 train_time:6160ms step_avg:93.34ms
step:67/1670 train_time:6252ms step_avg:93.31ms
step:68/1670 train_time:6345ms step_avg:93.31ms
step:69/1670 train_time:6438ms step_avg:93.31ms
step:70/1670 train_time:6531ms step_avg:93.29ms
step:71/1670 train_time:6623ms step_avg:93.28ms
step:72/1670 train_time:6715ms step_avg:93.26ms
step:73/1670 train_time:6806ms step_avg:93.24ms
step:74/1670 train_time:6899ms step_avg:93.23ms
step:75/1670 train_time:6989ms step_avg:93.19ms
step:76/1670 train_time:7081ms step_avg:93.17ms
step:77/1670 train_time:7172ms step_avg:93.14ms
step:78/1670 train_time:7264ms step_avg:93.12ms
step:79/1670 train_time:7355ms step_avg:93.10ms
step:80/1670 train_time:7448ms step_avg:93.10ms
step:81/1670 train_time:7540ms step_avg:93.09ms
step:82/1670 train_time:7632ms step_avg:93.07ms
step:83/1670 train_time:7724ms step_avg:93.06ms
step:84/1670 train_time:7815ms step_avg:93.04ms
step:85/1670 train_time:7907ms step_avg:93.02ms
step:86/1670 train_time:7998ms step_avg:93.01ms
step:87/1670 train_time:8089ms step_avg:92.98ms
step:88/1670 train_time:8180ms step_avg:92.95ms
step:89/1670 train_time:8270ms step_avg:92.92ms
step:90/1670 train_time:8361ms step_avg:92.90ms
step:91/1670 train_time:8453ms step_avg:92.89ms
step:92/1670 train_time:8546ms step_avg:92.89ms
step:93/1670 train_time:8637ms step_avg:92.87ms
step:94/1670 train_time:8729ms step_avg:92.86ms
step:95/1670 train_time:8822ms step_avg:92.86ms
step:96/1670 train_time:8912ms step_avg:92.83ms
step:97/1670 train_time:9004ms step_avg:92.82ms
step:98/1670 train_time:9095ms step_avg:92.80ms
step:99/1670 train_time:9186ms step_avg:92.79ms
step:100/1670 train_time:9279ms step_avg:92.79ms
step:101/1670 train_time:9369ms step_avg:92.76ms
step:102/1670 train_time:9460ms step_avg:92.75ms
step:103/1670 train_time:9551ms step_avg:92.73ms
step:104/1670 train_time:9642ms step_avg:92.72ms
step:105/1670 train_time:9733ms step_avg:92.70ms
step:106/1670 train_time:9826ms step_avg:92.69ms
step:107/1670 train_time:9917ms step_avg:92.68ms
step:108/1670 train_time:10009ms step_avg:92.68ms
step:109/1670 train_time:10101ms step_avg:92.67ms
step:110/1670 train_time:10191ms step_avg:92.65ms
step:111/1670 train_time:10283ms step_avg:92.64ms
step:112/1670 train_time:10374ms step_avg:92.62ms
step:113/1670 train_time:10465ms step_avg:92.61ms
step:114/1670 train_time:10555ms step_avg:92.59ms
step:115/1670 train_time:10648ms step_avg:92.59ms
step:116/1670 train_time:10740ms step_avg:92.59ms
step:117/1670 train_time:10831ms step_avg:92.58ms
step:118/1670 train_time:10923ms step_avg:92.57ms
step:119/1670 train_time:11015ms step_avg:92.56ms
step:120/1670 train_time:11107ms step_avg:92.56ms
step:121/1670 train_time:11199ms step_avg:92.55ms
step:122/1670 train_time:11290ms step_avg:92.54ms
step:123/1670 train_time:11381ms step_avg:92.53ms
step:124/1670 train_time:11471ms step_avg:92.51ms
step:125/1670 train_time:11562ms step_avg:92.49ms
step:125/1670 val_loss:4.3072 train_time:11653ms step_avg:93.22ms
step:126/1670 train_time:11671ms step_avg:92.63ms
step:127/1670 train_time:11748ms step_avg:92.51ms
step:128/1670 train_time:11848ms step_avg:92.57ms
step:129/1670 train_time:11940ms step_avg:92.56ms
step:130/1670 train_time:12031ms step_avg:92.55ms
step:131/1670 train_time:12122ms step_avg:92.54ms
step:132/1670 train_time:12212ms step_avg:92.52ms
step:133/1670 train_time:12303ms step_avg:92.50ms
step:134/1670 train_time:12393ms step_avg:92.48ms
step:135/1670 train_time:12483ms step_avg:92.47ms
step:136/1670 train_time:12573ms step_avg:92.45ms
step:137/1670 train_time:12663ms step_avg:92.43ms
step:138/1670 train_time:12756ms step_avg:92.43ms
step:139/1670 train_time:12849ms step_avg:92.44ms
step:140/1670 train_time:12941ms step_avg:92.44ms
step:141/1670 train_time:13033ms step_avg:92.43ms
step:142/1670 train_time:13124ms step_avg:92.42ms
step:143/1670 train_time:13214ms step_avg:92.41ms
step:144/1670 train_time:13305ms step_avg:92.39ms
step:145/1670 train_time:13395ms step_avg:92.38ms
step:146/1670 train_time:13485ms step_avg:92.36ms
step:147/1670 train_time:13575ms step_avg:92.35ms
step:148/1670 train_time:13667ms step_avg:92.35ms
step:149/1670 train_time:13759ms step_avg:92.34ms
step:150/1670 train_time:13851ms step_avg:92.34ms
step:151/1670 train_time:13943ms step_avg:92.33ms
step:152/1670 train_time:14035ms step_avg:92.33ms
step:153/1670 train_time:14126ms step_avg:92.33ms
step:154/1670 train_time:14216ms step_avg:92.31ms
step:155/1670 train_time:14306ms step_avg:92.30ms
step:156/1670 train_time:14397ms step_avg:92.29ms
step:157/1670 train_time:14488ms step_avg:92.28ms
step:158/1670 train_time:14578ms step_avg:92.26ms
step:159/1670 train_time:14669ms step_avg:92.26ms
step:160/1670 train_time:14761ms step_avg:92.26ms
step:161/1670 train_time:14852ms step_avg:92.25ms
step:162/1670 train_time:14944ms step_avg:92.25ms
step:163/1670 train_time:15036ms step_avg:92.24ms
step:164/1670 train_time:15129ms step_avg:92.25ms
step:165/1670 train_time:15219ms step_avg:92.24ms
step:166/1670 train_time:15310ms step_avg:92.23ms
step:167/1670 train_time:15401ms step_avg:92.22ms
step:168/1670 train_time:15492ms step_avg:92.21ms
step:169/1670 train_time:15583ms step_avg:92.21ms
step:170/1670 train_time:15674ms step_avg:92.20ms
step:171/1670 train_time:15765ms step_avg:92.19ms
step:172/1670 train_time:15856ms step_avg:92.19ms
step:173/1670 train_time:15947ms step_avg:92.18ms
step:174/1670 train_time:16039ms step_avg:92.18ms
step:175/1670 train_time:16132ms step_avg:92.18ms
step:176/1670 train_time:16224ms step_avg:92.18ms
step:177/1670 train_time:16315ms step_avg:92.18ms
step:178/1670 train_time:16406ms step_avg:92.17ms
step:179/1670 train_time:16496ms step_avg:92.16ms
step:180/1670 train_time:16588ms step_avg:92.16ms
step:181/1670 train_time:16680ms step_avg:92.15ms
step:182/1670 train_time:16771ms step_avg:92.15ms
step:183/1670 train_time:16861ms step_avg:92.14ms
step:184/1670 train_time:16952ms step_avg:92.13ms
step:185/1670 train_time:17043ms step_avg:92.13ms
step:186/1670 train_time:17135ms step_avg:92.12ms
step:187/1670 train_time:17226ms step_avg:92.12ms
step:188/1670 train_time:17316ms step_avg:92.11ms
step:189/1670 train_time:17408ms step_avg:92.11ms
step:190/1670 train_time:17499ms step_avg:92.10ms
step:191/1670 train_time:17590ms step_avg:92.10ms
step:192/1670 train_time:17681ms step_avg:92.09ms
step:193/1670 train_time:17772ms step_avg:92.08ms
step:194/1670 train_time:17863ms step_avg:92.08ms
step:195/1670 train_time:17956ms step_avg:92.08ms
step:196/1670 train_time:18046ms step_avg:92.07ms
step:197/1670 train_time:18136ms step_avg:92.06ms
step:198/1670 train_time:18227ms step_avg:92.06ms
step:199/1670 train_time:18319ms step_avg:92.06ms
step:200/1670 train_time:18411ms step_avg:92.06ms
step:201/1670 train_time:18503ms step_avg:92.05ms
step:202/1670 train_time:18594ms step_avg:92.05ms
step:203/1670 train_time:18686ms step_avg:92.05ms
step:204/1670 train_time:18777ms step_avg:92.04ms
step:205/1670 train_time:18869ms step_avg:92.04ms
step:206/1670 train_time:18959ms step_avg:92.03ms
step:207/1670 train_time:19051ms step_avg:92.03ms
step:208/1670 train_time:19142ms step_avg:92.03ms
step:209/1670 train_time:19233ms step_avg:92.03ms
step:210/1670 train_time:19324ms step_avg:92.02ms
step:211/1670 train_time:19415ms step_avg:92.02ms
step:212/1670 train_time:19507ms step_avg:92.01ms
step:213/1670 train_time:19759ms step_avg:92.76ms
step:214/1670 train_time:19829ms step_avg:92.66ms
step:215/1670 train_time:19919ms step_avg:92.64ms
step:216/1670 train_time:20009ms step_avg:92.63ms
step:217/1670 train_time:20100ms step_avg:92.63ms
step:218/1670 train_time:20190ms step_avg:92.62ms
step:219/1670 train_time:20280ms step_avg:92.60ms
step:220/1670 train_time:20371ms step_avg:92.59ms
step:221/1670 train_time:20461ms step_avg:92.58ms
step:222/1670 train_time:20550ms step_avg:92.57ms
step:223/1670 train_time:20642ms step_avg:92.57ms
step:224/1670 train_time:20738ms step_avg:92.58ms
step:225/1670 train_time:20832ms step_avg:92.58ms
step:226/1670 train_time:20925ms step_avg:92.59ms
step:227/1670 train_time:21015ms step_avg:92.58ms
step:228/1670 train_time:21105ms step_avg:92.57ms
step:229/1670 train_time:21196ms step_avg:92.56ms
step:230/1670 train_time:21287ms step_avg:92.55ms
step:231/1670 train_time:21377ms step_avg:92.54ms
step:232/1670 train_time:21467ms step_avg:92.53ms
step:233/1670 train_time:21558ms step_avg:92.53ms
step:234/1670 train_time:21652ms step_avg:92.53ms
step:235/1670 train_time:21744ms step_avg:92.53ms
step:236/1670 train_time:21836ms step_avg:92.52ms
step:237/1670 train_time:21927ms step_avg:92.52ms
step:238/1670 train_time:22018ms step_avg:92.51ms
step:239/1670 train_time:22109ms step_avg:92.51ms
step:240/1670 train_time:22199ms step_avg:92.50ms
step:241/1670 train_time:22291ms step_avg:92.49ms
step:242/1670 train_time:22381ms step_avg:92.48ms
step:243/1670 train_time:22472ms step_avg:92.48ms
step:244/1670 train_time:22563ms step_avg:92.47ms
step:245/1670 train_time:22654ms step_avg:92.47ms
step:246/1670 train_time:22745ms step_avg:92.46ms
step:247/1670 train_time:22837ms step_avg:92.46ms
step:248/1670 train_time:22929ms step_avg:92.46ms
step:249/1670 train_time:23021ms step_avg:92.45ms
step:250/1670 train_time:23113ms step_avg:92.45ms
step:250/1670 val_loss:3.9688 train_time:23204ms step_avg:92.82ms
step:251/1670 train_time:23221ms step_avg:92.51ms
step:252/1670 train_time:23296ms step_avg:92.44ms
step:253/1670 train_time:23387ms step_avg:92.44ms
step:254/1670 train_time:23479ms step_avg:92.44ms
step:255/1670 train_time:23569ms step_avg:92.43ms
step:256/1670 train_time:23659ms step_avg:92.42ms
step:257/1670 train_time:23749ms step_avg:92.41ms
step:258/1670 train_time:23840ms step_avg:92.40ms
step:259/1670 train_time:23932ms step_avg:92.40ms
step:260/1670 train_time:24023ms step_avg:92.40ms
step:261/1670 train_time:24114ms step_avg:92.39ms
step:262/1670 train_time:24208ms step_avg:92.40ms
step:263/1670 train_time:24301ms step_avg:92.40ms
step:264/1670 train_time:24392ms step_avg:92.39ms
step:265/1670 train_time:24484ms step_avg:92.39ms
step:266/1670 train_time:24575ms step_avg:92.39ms
step:267/1670 train_time:24666ms step_avg:92.38ms
step:268/1670 train_time:24756ms step_avg:92.37ms
step:269/1670 train_time:24848ms step_avg:92.37ms
step:270/1670 train_time:24939ms step_avg:92.37ms
step:271/1670 train_time:25030ms step_avg:92.36ms
step:272/1670 train_time:25120ms step_avg:92.35ms
step:273/1670 train_time:25212ms step_avg:92.35ms
step:274/1670 train_time:25305ms step_avg:92.35ms
step:275/1670 train_time:25397ms step_avg:92.35ms
step:276/1670 train_time:25489ms step_avg:92.35ms
step:277/1670 train_time:25580ms step_avg:92.35ms
step:278/1670 train_time:25672ms step_avg:92.34ms
step:279/1670 train_time:25763ms step_avg:92.34ms
step:280/1670 train_time:25853ms step_avg:92.33ms
step:281/1670 train_time:25944ms step_avg:92.33ms
step:282/1670 train_time:26034ms step_avg:92.32ms
step:283/1670 train_time:26126ms step_avg:92.32ms
step:284/1670 train_time:26217ms step_avg:92.31ms
step:285/1670 train_time:26309ms step_avg:92.31ms
step:286/1670 train_time:26401ms step_avg:92.31ms
step:287/1670 train_time:26491ms step_avg:92.30ms
step:288/1670 train_time:26584ms step_avg:92.31ms
step:289/1670 train_time:26676ms step_avg:92.30ms
step:290/1670 train_time:26768ms step_avg:92.30ms
step:291/1670 train_time:26860ms step_avg:92.30ms
step:292/1670 train_time:26950ms step_avg:92.30ms
step:293/1670 train_time:27040ms step_avg:92.29ms
step:294/1670 train_time:27131ms step_avg:92.28ms
step:295/1670 train_time:27222ms step_avg:92.28ms
step:296/1670 train_time:27313ms step_avg:92.27ms
step:297/1670 train_time:27406ms step_avg:92.28ms
step:298/1670 train_time:27498ms step_avg:92.27ms
step:299/1670 train_time:27589ms step_avg:92.27ms
step:300/1670 train_time:27680ms step_avg:92.27ms
step:301/1670 train_time:27771ms step_avg:92.26ms
step:302/1670 train_time:27862ms step_avg:92.26ms
step:303/1670 train_time:27953ms step_avg:92.25ms
step:304/1670 train_time:28044ms step_avg:92.25ms
step:305/1670 train_time:28134ms step_avg:92.24ms
step:306/1670 train_time:28226ms step_avg:92.24ms
step:307/1670 train_time:28317ms step_avg:92.24ms
step:308/1670 train_time:28408ms step_avg:92.23ms
step:309/1670 train_time:28499ms step_avg:92.23ms
step:310/1670 train_time:28591ms step_avg:92.23ms
step:311/1670 train_time:28684ms step_avg:92.23ms
step:312/1670 train_time:28775ms step_avg:92.23ms
step:313/1670 train_time:28867ms step_avg:92.23ms
step:314/1670 train_time:28957ms step_avg:92.22ms
step:315/1670 train_time:29048ms step_avg:92.21ms
step:316/1670 train_time:29137ms step_avg:92.21ms
step:317/1670 train_time:29229ms step_avg:92.20ms
step:318/1670 train_time:29319ms step_avg:92.20ms
step:319/1670 train_time:29410ms step_avg:92.20ms
step:320/1670 train_time:29502ms step_avg:92.19ms
step:321/1670 train_time:29594ms step_avg:92.19ms
step:322/1670 train_time:29686ms step_avg:92.19ms
step:323/1670 train_time:29778ms step_avg:92.19ms
step:324/1670 train_time:29870ms step_avg:92.19ms
step:325/1670 train_time:29961ms step_avg:92.19ms
step:326/1670 train_time:30051ms step_avg:92.18ms
step:327/1670 train_time:30141ms step_avg:92.18ms
step:328/1670 train_time:30232ms step_avg:92.17ms
step:329/1670 train_time:30323ms step_avg:92.17ms
step:330/1670 train_time:30414ms step_avg:92.16ms
step:331/1670 train_time:30506ms step_avg:92.16ms
step:332/1670 train_time:30599ms step_avg:92.17ms
step:333/1670 train_time:30690ms step_avg:92.16ms
step:334/1670 train_time:30782ms step_avg:92.16ms
step:335/1670 train_time:30873ms step_avg:92.16ms
step:336/1670 train_time:30966ms step_avg:92.16ms
step:337/1670 train_time:31057ms step_avg:92.16ms
step:338/1670 train_time:31148ms step_avg:92.15ms
step:339/1670 train_time:31239ms step_avg:92.15ms
step:340/1670 train_time:31329ms step_avg:92.14ms
step:341/1670 train_time:31420ms step_avg:92.14ms
step:342/1670 train_time:31510ms step_avg:92.13ms
step:343/1670 train_time:31603ms step_avg:92.14ms
step:344/1670 train_time:31694ms step_avg:92.13ms
step:345/1670 train_time:31787ms step_avg:92.14ms
step:346/1670 train_time:31880ms step_avg:92.14ms
step:347/1670 train_time:31971ms step_avg:92.13ms
step:348/1670 train_time:32063ms step_avg:92.14ms
step:349/1670 train_time:32154ms step_avg:92.13ms
step:350/1670 train_time:32246ms step_avg:92.13ms
step:351/1670 train_time:32337ms step_avg:92.13ms
step:352/1670 train_time:32427ms step_avg:92.12ms
step:353/1670 train_time:32518ms step_avg:92.12ms
step:354/1670 train_time:32609ms step_avg:92.11ms
step:355/1670 train_time:32701ms step_avg:92.12ms
step:356/1670 train_time:32792ms step_avg:92.11ms
step:357/1670 train_time:32884ms step_avg:92.11ms
step:358/1670 train_time:32975ms step_avg:92.11ms
step:359/1670 train_time:33067ms step_avg:92.11ms
step:360/1670 train_time:33158ms step_avg:92.11ms
step:361/1670 train_time:33248ms step_avg:92.10ms
step:362/1670 train_time:33339ms step_avg:92.10ms
step:363/1670 train_time:33429ms step_avg:92.09ms
step:364/1670 train_time:33519ms step_avg:92.08ms
step:365/1670 train_time:33610ms step_avg:92.08ms
step:366/1670 train_time:33702ms step_avg:92.08ms
step:367/1670 train_time:33793ms step_avg:92.08ms
step:368/1670 train_time:33886ms step_avg:92.08ms
step:369/1670 train_time:33978ms step_avg:92.08ms
step:370/1670 train_time:34070ms step_avg:92.08ms
step:371/1670 train_time:34162ms step_avg:92.08ms
step:372/1670 train_time:34252ms step_avg:92.08ms
step:373/1670 train_time:34344ms step_avg:92.07ms
step:374/1670 train_time:34435ms step_avg:92.07ms
step:375/1670 train_time:34526ms step_avg:92.07ms
step:375/1670 val_loss:3.8157 train_time:34617ms step_avg:92.31ms
step:376/1670 train_time:34633ms step_avg:92.11ms
step:377/1670 train_time:34709ms step_avg:92.07ms
step:378/1670 train_time:34800ms step_avg:92.06ms
step:379/1670 train_time:34891ms step_avg:92.06ms
step:380/1670 train_time:34982ms step_avg:92.06ms
step:381/1670 train_time:35072ms step_avg:92.05ms
step:382/1670 train_time:35162ms step_avg:92.05ms
step:383/1670 train_time:35255ms step_avg:92.05ms
step:384/1670 train_time:35346ms step_avg:92.05ms
step:385/1670 train_time:35437ms step_avg:92.04ms
step:386/1670 train_time:35529ms step_avg:92.04ms
step:387/1670 train_time:35621ms step_avg:92.04ms
step:388/1670 train_time:35713ms step_avg:92.04ms
step:389/1670 train_time:35804ms step_avg:92.04ms
step:390/1670 train_time:35896ms step_avg:92.04ms
step:391/1670 train_time:35987ms step_avg:92.04ms
step:392/1670 train_time:36078ms step_avg:92.03ms
step:393/1670 train_time:36170ms step_avg:92.03ms
step:394/1670 train_time:36260ms step_avg:92.03ms
step:395/1670 train_time:36351ms step_avg:92.03ms
step:396/1670 train_time:36442ms step_avg:92.02ms
step:397/1670 train_time:36534ms step_avg:92.02ms
step:398/1670 train_time:36625ms step_avg:92.02ms
step:399/1670 train_time:36718ms step_avg:92.02ms
step:400/1670 train_time:36809ms step_avg:92.02ms
step:401/1670 train_time:36900ms step_avg:92.02ms
step:402/1670 train_time:36991ms step_avg:92.02ms
step:403/1670 train_time:37082ms step_avg:92.01ms
step:404/1670 train_time:37173ms step_avg:92.01ms
step:405/1670 train_time:37264ms step_avg:92.01ms
step:406/1670 train_time:37354ms step_avg:92.01ms
step:407/1670 train_time:37445ms step_avg:92.00ms
step:408/1670 train_time:37537ms step_avg:92.00ms
step:409/1670 train_time:37628ms step_avg:92.00ms
step:410/1670 train_time:37719ms step_avg:92.00ms
step:411/1670 train_time:37811ms step_avg:92.00ms
step:412/1670 train_time:37902ms step_avg:91.99ms
step:413/1670 train_time:37993ms step_avg:91.99ms
step:414/1670 train_time:38084ms step_avg:91.99ms
step:415/1670 train_time:38175ms step_avg:91.99ms
step:416/1670 train_time:38266ms step_avg:91.98ms
step:417/1670 train_time:38357ms step_avg:91.98ms
step:418/1670 train_time:38448ms step_avg:91.98ms
step:419/1670 train_time:38541ms step_avg:91.98ms
step:420/1670 train_time:38631ms step_avg:91.98ms
step:421/1670 train_time:38722ms step_avg:91.98ms
step:422/1670 train_time:38814ms step_avg:91.98ms
step:423/1670 train_time:38904ms step_avg:91.97ms
step:424/1670 train_time:38995ms step_avg:91.97ms
step:425/1670 train_time:39248ms step_avg:92.35ms
step:426/1670 train_time:39319ms step_avg:92.30ms
step:427/1670 train_time:39409ms step_avg:92.29ms
step:428/1670 train_time:39499ms step_avg:92.29ms
step:429/1670 train_time:39589ms step_avg:92.28ms
step:430/1670 train_time:39679ms step_avg:92.28ms
step:431/1670 train_time:39769ms step_avg:92.27ms
step:432/1670 train_time:39859ms step_avg:92.27ms
step:433/1670 train_time:39950ms step_avg:92.26ms
step:434/1670 train_time:40040ms step_avg:92.26ms
step:435/1670 train_time:40136ms step_avg:92.27ms
step:436/1670 train_time:40232ms step_avg:92.28ms
step:437/1670 train_time:40325ms step_avg:92.28ms
step:438/1670 train_time:40417ms step_avg:92.28ms
step:439/1670 train_time:40507ms step_avg:92.27ms
step:440/1670 train_time:40598ms step_avg:92.27ms
step:441/1670 train_time:40688ms step_avg:92.26ms
step:442/1670 train_time:40779ms step_avg:92.26ms
step:443/1670 train_time:40869ms step_avg:92.26ms
step:444/1670 train_time:40959ms step_avg:92.25ms
step:445/1670 train_time:41051ms step_avg:92.25ms
step:446/1670 train_time:41144ms step_avg:92.25ms
step:447/1670 train_time:41238ms step_avg:92.25ms
step:448/1670 train_time:41330ms step_avg:92.25ms
step:449/1670 train_time:41421ms step_avg:92.25ms
step:450/1670 train_time:41512ms step_avg:92.25ms
step:451/1670 train_time:41602ms step_avg:92.24ms
step:452/1670 train_time:41693ms step_avg:92.24ms
step:453/1670 train_time:41783ms step_avg:92.24ms
step:454/1670 train_time:41874ms step_avg:92.23ms
step:455/1670 train_time:41964ms step_avg:92.23ms
step:456/1670 train_time:42055ms step_avg:92.23ms
step:457/1670 train_time:42146ms step_avg:92.22ms
step:458/1670 train_time:42240ms step_avg:92.23ms
step:459/1670 train_time:42333ms step_avg:92.23ms
step:460/1670 train_time:42424ms step_avg:92.23ms
step:461/1670 train_time:42515ms step_avg:92.22ms
step:462/1670 train_time:42606ms step_avg:92.22ms
step:463/1670 train_time:42698ms step_avg:92.22ms
step:464/1670 train_time:42786ms step_avg:92.21ms
step:465/1670 train_time:42878ms step_avg:92.21ms
step:466/1670 train_time:42967ms step_avg:92.20ms
step:467/1670 train_time:43059ms step_avg:92.20ms
step:468/1670 train_time:43151ms step_avg:92.20ms
step:469/1670 train_time:43243ms step_avg:92.20ms
step:470/1670 train_time:43335ms step_avg:92.20ms
step:471/1670 train_time:43426ms step_avg:92.20ms
step:472/1670 train_time:43517ms step_avg:92.20ms
step:473/1670 train_time:43608ms step_avg:92.19ms
step:474/1670 train_time:43699ms step_avg:92.19ms
step:475/1670 train_time:43791ms step_avg:92.19ms
step:476/1670 train_time:43881ms step_avg:92.19ms
step:477/1670 train_time:43971ms step_avg:92.18ms
step:478/1670 train_time:44062ms step_avg:92.18ms
step:479/1670 train_time:44154ms step_avg:92.18ms
step:480/1670 train_time:44245ms step_avg:92.18ms
step:481/1670 train_time:44338ms step_avg:92.18ms
step:482/1670 train_time:44430ms step_avg:92.18ms
step:483/1670 train_time:44522ms step_avg:92.18ms
step:484/1670 train_time:44612ms step_avg:92.17ms
step:485/1670 train_time:44703ms step_avg:92.17ms
step:486/1670 train_time:44794ms step_avg:92.17ms
step:487/1670 train_time:44885ms step_avg:92.17ms
step:488/1670 train_time:44976ms step_avg:92.16ms
step:489/1670 train_time:45066ms step_avg:92.16ms
step:490/1670 train_time:45158ms step_avg:92.16ms
step:491/1670 train_time:45250ms step_avg:92.16ms
step:492/1670 train_time:45341ms step_avg:92.16ms
step:493/1670 train_time:45433ms step_avg:92.16ms
step:494/1670 train_time:45523ms step_avg:92.15ms
step:495/1670 train_time:45615ms step_avg:92.15ms
step:496/1670 train_time:45706ms step_avg:92.15ms
step:497/1670 train_time:45796ms step_avg:92.15ms
step:498/1670 train_time:45887ms step_avg:92.14ms
step:499/1670 train_time:45978ms step_avg:92.14ms
step:500/1670 train_time:46069ms step_avg:92.14ms
step:500/1670 val_loss:3.7139 train_time:46161ms step_avg:92.32ms
step:501/1670 train_time:46178ms step_avg:92.17ms
step:502/1670 train_time:46253ms step_avg:92.14ms
step:503/1670 train_time:46345ms step_avg:92.14ms
step:504/1670 train_time:46435ms step_avg:92.13ms
step:505/1670 train_time:46526ms step_avg:92.13ms
step:506/1670 train_time:46616ms step_avg:92.13ms
step:507/1670 train_time:46707ms step_avg:92.12ms
step:508/1670 train_time:46798ms step_avg:92.12ms
step:509/1670 train_time:46888ms step_avg:92.12ms
step:510/1670 train_time:46979ms step_avg:92.12ms
step:511/1670 train_time:47070ms step_avg:92.11ms
step:512/1670 train_time:47163ms step_avg:92.11ms
step:513/1670 train_time:47254ms step_avg:92.11ms
step:514/1670 train_time:47347ms step_avg:92.11ms
step:515/1670 train_time:47438ms step_avg:92.11ms
step:516/1670 train_time:47529ms step_avg:92.11ms
step:517/1670 train_time:47620ms step_avg:92.11ms
step:518/1670 train_time:47710ms step_avg:92.11ms
step:519/1670 train_time:47801ms step_avg:92.10ms
step:520/1670 train_time:47891ms step_avg:92.10ms
step:521/1670 train_time:47983ms step_avg:92.10ms
step:522/1670 train_time:48074ms step_avg:92.09ms
step:523/1670 train_time:48165ms step_avg:92.09ms
step:524/1670 train_time:48256ms step_avg:92.09ms
step:525/1670 train_time:48350ms step_avg:92.09ms
step:526/1670 train_time:48441ms step_avg:92.09ms
step:527/1670 train_time:48533ms step_avg:92.09ms
step:528/1670 train_time:48624ms step_avg:92.09ms
step:529/1670 train_time:48715ms step_avg:92.09ms
step:530/1670 train_time:48806ms step_avg:92.09ms
step:531/1670 train_time:48896ms step_avg:92.08ms
step:532/1670 train_time:48988ms step_avg:92.08ms
step:533/1670 train_time:49079ms step_avg:92.08ms
step:534/1670 train_time:49170ms step_avg:92.08ms
step:535/1670 train_time:49262ms step_avg:92.08ms
step:536/1670 train_time:49354ms step_avg:92.08ms
step:537/1670 train_time:49446ms step_avg:92.08ms
step:538/1670 train_time:49537ms step_avg:92.08ms
step:539/1670 train_time:49627ms step_avg:92.07ms
step:540/1670 train_time:49718ms step_avg:92.07ms
step:541/1670 train_time:49809ms step_avg:92.07ms
step:542/1670 train_time:49899ms step_avg:92.06ms
step:543/1670 train_time:49990ms step_avg:92.06ms
step:544/1670 train_time:50081ms step_avg:92.06ms
step:545/1670 train_time:50172ms step_avg:92.06ms
step:546/1670 train_time:50264ms step_avg:92.06ms
step:547/1670 train_time:50355ms step_avg:92.06ms
step:548/1670 train_time:50447ms step_avg:92.06ms
step:549/1670 train_time:50538ms step_avg:92.05ms
step:550/1670 train_time:50628ms step_avg:92.05ms
step:551/1670 train_time:50719ms step_avg:92.05ms
step:552/1670 train_time:50812ms step_avg:92.05ms
step:553/1670 train_time:50903ms step_avg:92.05ms
step:554/1670 train_time:50994ms step_avg:92.05ms
step:555/1670 train_time:51085ms step_avg:92.04ms
step:556/1670 train_time:51175ms step_avg:92.04ms
step:557/1670 train_time:51266ms step_avg:92.04ms
step:558/1670 train_time:51556ms step_avg:92.39ms
step:559/1670 train_time:51626ms step_avg:92.35ms
step:560/1670 train_time:51716ms step_avg:92.35ms
step:561/1670 train_time:51807ms step_avg:92.35ms
step:562/1670 train_time:51898ms step_avg:92.35ms
step:563/1670 train_time:51989ms step_avg:92.34ms
step:564/1670 train_time:52081ms step_avg:92.34ms
step:565/1670 train_time:52172ms step_avg:92.34ms
step:566/1670 train_time:52263ms step_avg:92.34ms
step:567/1670 train_time:52354ms step_avg:92.34ms
step:568/1670 train_time:52456ms step_avg:92.35ms
step:569/1670 train_time:52555ms step_avg:92.36ms
step:570/1670 train_time:52648ms step_avg:92.37ms
step:571/1670 train_time:52740ms step_avg:92.36ms
step:572/1670 train_time:52832ms step_avg:92.36ms
step:573/1670 train_time:52923ms step_avg:92.36ms
step:574/1670 train_time:53014ms step_avg:92.36ms
step:575/1670 train_time:53105ms step_avg:92.36ms
step:576/1670 train_time:53197ms step_avg:92.36ms
step:577/1670 train_time:53290ms step_avg:92.36ms
step:578/1670 train_time:53382ms step_avg:92.36ms
step:579/1670 train_time:53477ms step_avg:92.36ms
step:580/1670 train_time:53572ms step_avg:92.37ms
step:581/1670 train_time:53665ms step_avg:92.37ms
step:582/1670 train_time:53757ms step_avg:92.37ms
step:583/1670 train_time:53851ms step_avg:92.37ms
step:584/1670 train_time:53943ms step_avg:92.37ms
step:585/1670 train_time:54034ms step_avg:92.37ms
step:586/1670 train_time:54126ms step_avg:92.36ms
step:587/1670 train_time:54217ms step_avg:92.36ms
step:588/1670 train_time:54310ms step_avg:92.36ms
step:589/1670 train_time:54403ms step_avg:92.37ms
step:590/1670 train_time:54496ms step_avg:92.37ms
step:591/1670 train_time:54591ms step_avg:92.37ms
step:592/1670 train_time:54683ms step_avg:92.37ms
step:593/1670 train_time:54775ms step_avg:92.37ms
step:594/1670 train_time:54868ms step_avg:92.37ms
step:595/1670 train_time:54959ms step_avg:92.37ms
step:596/1670 train_time:55052ms step_avg:92.37ms
step:597/1670 train_time:55144ms step_avg:92.37ms
step:598/1670 train_time:55235ms step_avg:92.37ms
step:599/1670 train_time:55329ms step_avg:92.37ms
step:600/1670 train_time:55421ms step_avg:92.37ms
step:601/1670 train_time:55514ms step_avg:92.37ms
step:602/1670 train_time:55608ms step_avg:92.37ms
step:603/1670 train_time:55701ms step_avg:92.37ms
step:604/1670 train_time:55793ms step_avg:92.37ms
step:605/1670 train_time:55885ms step_avg:92.37ms
step:606/1670 train_time:55977ms step_avg:92.37ms
step:607/1670 train_time:56069ms step_avg:92.37ms
step:608/1670 train_time:56160ms step_avg:92.37ms
step:609/1670 train_time:56253ms step_avg:92.37ms
step:610/1670 train_time:56346ms step_avg:92.37ms
step:611/1670 train_time:56438ms step_avg:92.37ms
step:612/1670 train_time:56531ms step_avg:92.37ms
step:613/1670 train_time:56624ms step_avg:92.37ms
step:614/1670 train_time:56716ms step_avg:92.37ms
step:615/1670 train_time:56809ms step_avg:92.37ms
step:616/1670 train_time:56901ms step_avg:92.37ms
step:617/1670 train_time:56992ms step_avg:92.37ms
step:618/1670 train_time:57086ms step_avg:92.37ms
step:619/1670 train_time:57178ms step_avg:92.37ms
step:620/1670 train_time:57271ms step_avg:92.37ms
step:621/1670 train_time:57364ms step_avg:92.37ms
step:622/1670 train_time:57456ms step_avg:92.37ms
step:623/1670 train_time:57550ms step_avg:92.38ms
step:624/1670 train_time:57642ms step_avg:92.38ms
step:625/1670 train_time:57734ms step_avg:92.37ms
step:625/1670 val_loss:3.6124 train_time:57827ms step_avg:92.52ms
step:626/1670 train_time:57844ms step_avg:92.40ms
step:627/1670 train_time:57923ms step_avg:92.38ms
step:628/1670 train_time:58025ms step_avg:92.40ms
step:629/1670 train_time:58118ms step_avg:92.40ms
step:630/1670 train_time:58211ms step_avg:92.40ms
step:631/1670 train_time:58302ms step_avg:92.40ms
step:632/1670 train_time:58394ms step_avg:92.40ms
step:633/1670 train_time:58484ms step_avg:92.39ms
step:634/1670 train_time:58575ms step_avg:92.39ms
step:635/1670 train_time:58666ms step_avg:92.39ms
step:636/1670 train_time:58758ms step_avg:92.39ms
step:637/1670 train_time:58852ms step_avg:92.39ms
step:638/1670 train_time:58949ms step_avg:92.40ms
step:639/1670 train_time:59184ms step_avg:92.62ms
step:640/1670 train_time:59256ms step_avg:92.59ms
step:641/1670 train_time:59347ms step_avg:92.58ms
step:642/1670 train_time:59439ms step_avg:92.58ms
step:643/1670 train_time:59530ms step_avg:92.58ms
step:644/1670 train_time:59621ms step_avg:92.58ms
step:645/1670 train_time:59712ms step_avg:92.58ms
step:646/1670 train_time:59804ms step_avg:92.58ms
step:647/1670 train_time:59895ms step_avg:92.57ms
step:648/1670 train_time:59986ms step_avg:92.57ms
step:649/1670 train_time:60086ms step_avg:92.58ms
step:650/1670 train_time:60184ms step_avg:92.59ms
step:651/1670 train_time:60277ms step_avg:92.59ms
step:652/1670 train_time:60369ms step_avg:92.59ms
step:653/1670 train_time:60461ms step_avg:92.59ms
step:654/1670 train_time:60554ms step_avg:92.59ms
step:655/1670 train_time:60646ms step_avg:92.59ms
step:656/1670 train_time:60737ms step_avg:92.59ms
step:657/1670 train_time:60827ms step_avg:92.58ms
step:658/1670 train_time:60919ms step_avg:92.58ms
step:659/1670 train_time:61011ms step_avg:92.58ms
step:660/1670 train_time:61105ms step_avg:92.58ms
step:661/1670 train_time:61199ms step_avg:92.59ms
step:662/1670 train_time:61292ms step_avg:92.59ms
step:663/1670 train_time:61384ms step_avg:92.59ms
step:664/1670 train_time:61477ms step_avg:92.59ms
step:665/1670 train_time:61568ms step_avg:92.58ms
step:666/1670 train_time:61660ms step_avg:92.58ms
step:667/1670 train_time:61752ms step_avg:92.58ms
step:668/1670 train_time:61843ms step_avg:92.58ms
step:669/1670 train_time:61935ms step_avg:92.58ms
step:670/1670 train_time:62027ms step_avg:92.58ms
step:671/1670 train_time:62121ms step_avg:92.58ms
step:672/1670 train_time:62214ms step_avg:92.58ms
step:673/1670 train_time:62307ms step_avg:92.58ms
step:674/1670 train_time:62400ms step_avg:92.58ms
step:675/1670 train_time:62492ms step_avg:92.58ms
step:676/1670 train_time:62584ms step_avg:92.58ms
step:677/1670 train_time:62677ms step_avg:92.58ms
step:678/1670 train_time:62768ms step_avg:92.58ms
step:679/1670 train_time:62860ms step_avg:92.58ms
step:680/1670 train_time:62952ms step_avg:92.58ms
step:681/1670 train_time:63044ms step_avg:92.58ms
step:682/1670 train_time:63137ms step_avg:92.58ms
step:683/1670 train_time:63230ms step_avg:92.58ms
step:684/1670 train_time:63323ms step_avg:92.58ms
step:685/1670 train_time:63416ms step_avg:92.58ms
step:686/1670 train_time:63509ms step_avg:92.58ms
step:687/1670 train_time:63602ms step_avg:92.58ms
step:688/1670 train_time:63694ms step_avg:92.58ms
step:689/1670 train_time:63785ms step_avg:92.58ms
step:690/1670 train_time:63877ms step_avg:92.58ms
step:691/1670 train_time:63970ms step_avg:92.58ms
step:692/1670 train_time:64062ms step_avg:92.58ms
step:693/1670 train_time:64156ms step_avg:92.58ms
step:694/1670 train_time:64247ms step_avg:92.58ms
step:695/1670 train_time:64341ms step_avg:92.58ms
step:696/1670 train_time:64435ms step_avg:92.58ms
step:697/1670 train_time:64527ms step_avg:92.58ms
step:698/1670 train_time:64621ms step_avg:92.58ms
step:699/1670 train_time:64713ms step_avg:92.58ms
step:700/1670 train_time:64805ms step_avg:92.58ms
step:701/1670 train_time:64897ms step_avg:92.58ms
step:702/1670 train_time:64990ms step_avg:92.58ms
step:703/1670 train_time:65083ms step_avg:92.58ms
step:704/1670 train_time:65175ms step_avg:92.58ms
step:705/1670 train_time:65267ms step_avg:92.58ms
step:706/1670 train_time:65360ms step_avg:92.58ms
step:707/1670 train_time:65453ms step_avg:92.58ms
step:708/1670 train_time:65545ms step_avg:92.58ms
step:709/1670 train_time:65638ms step_avg:92.58ms
step:710/1670 train_time:65729ms step_avg:92.58ms
step:711/1670 train_time:65822ms step_avg:92.58ms
step:712/1670 train_time:65915ms step_avg:92.58ms
step:713/1670 train_time:66007ms step_avg:92.58ms
step:714/1670 train_time:66100ms step_avg:92.58ms
step:715/1670 train_time:66192ms step_avg:92.58ms
step:716/1670 train_time:66285ms step_avg:92.58ms
step:717/1670 train_time:66377ms step_avg:92.58ms
step:718/1670 train_time:66470ms step_avg:92.58ms
step:719/1670 train_time:66563ms step_avg:92.58ms
step:720/1670 train_time:66656ms step_avg:92.58ms
step:721/1670 train_time:66748ms step_avg:92.58ms
step:722/1670 train_time:66840ms step_avg:92.58ms
step:723/1670 train_time:66933ms step_avg:92.58ms
step:724/1670 train_time:67025ms step_avg:92.58ms
step:725/1670 train_time:67118ms step_avg:92.58ms
step:726/1670 train_time:67210ms step_avg:92.58ms
step:727/1670 train_time:67303ms step_avg:92.58ms
step:728/1670 train_time:67396ms step_avg:92.58ms
step:729/1670 train_time:67488ms step_avg:92.58ms
step:730/1670 train_time:67581ms step_avg:92.58ms
step:731/1670 train_time:67674ms step_avg:92.58ms
step:732/1670 train_time:67766ms step_avg:92.58ms
step:733/1670 train_time:67859ms step_avg:92.58ms
step:734/1670 train_time:67951ms step_avg:92.58ms
step:735/1670 train_time:68044ms step_avg:92.58ms
step:736/1670 train_time:68136ms step_avg:92.58ms
step:737/1670 train_time:68228ms step_avg:92.58ms
step:738/1670 train_time:68323ms step_avg:92.58ms
step:739/1670 train_time:68416ms step_avg:92.58ms
step:740/1670 train_time:68507ms step_avg:92.58ms
step:741/1670 train_time:68601ms step_avg:92.58ms
step:742/1670 train_time:68692ms step_avg:92.58ms
step:743/1670 train_time:68785ms step_avg:92.58ms
step:744/1670 train_time:68878ms step_avg:92.58ms
step:745/1670 train_time:68969ms step_avg:92.58ms
step:746/1670 train_time:69063ms step_avg:92.58ms
step:747/1670 train_time:69156ms step_avg:92.58ms
step:748/1670 train_time:69248ms step_avg:92.58ms
step:749/1670 train_time:69341ms step_avg:92.58ms
step:750/1670 train_time:69433ms step_avg:92.58ms
step:750/1670 val_loss:3.5594 train_time:69525ms step_avg:92.70ms
step:751/1670 train_time:69542ms step_avg:92.60ms
step:752/1670 train_time:69618ms step_avg:92.58ms
step:753/1670 train_time:69711ms step_avg:92.58ms
step:754/1670 train_time:69803ms step_avg:92.58ms
step:755/1670 train_time:69895ms step_avg:92.58ms
step:756/1670 train_time:69987ms step_avg:92.58ms
step:757/1670 train_time:70078ms step_avg:92.57ms
step:758/1670 train_time:70171ms step_avg:92.57ms
step:759/1670 train_time:70263ms step_avg:92.57ms
step:760/1670 train_time:70355ms step_avg:92.57ms
step:761/1670 train_time:70449ms step_avg:92.57ms
step:762/1670 train_time:70543ms step_avg:92.58ms
step:763/1670 train_time:70636ms step_avg:92.58ms
step:764/1670 train_time:70728ms step_avg:92.58ms
step:765/1670 train_time:70820ms step_avg:92.58ms
step:766/1670 train_time:70913ms step_avg:92.58ms
step:767/1670 train_time:71005ms step_avg:92.58ms
step:768/1670 train_time:71097ms step_avg:92.57ms
step:769/1670 train_time:71190ms step_avg:92.57ms
step:770/1670 train_time:71281ms step_avg:92.57ms
step:771/1670 train_time:71375ms step_avg:92.57ms
step:772/1670 train_time:71468ms step_avg:92.58ms
step:773/1670 train_time:71563ms step_avg:92.58ms
step:774/1670 train_time:71656ms step_avg:92.58ms
step:775/1670 train_time:71748ms step_avg:92.58ms
step:776/1670 train_time:71841ms step_avg:92.58ms
step:777/1670 train_time:71933ms step_avg:92.58ms
step:778/1670 train_time:72025ms step_avg:92.58ms
step:779/1670 train_time:72118ms step_avg:92.58ms
step:780/1670 train_time:72210ms step_avg:92.58ms
step:781/1670 train_time:72302ms step_avg:92.58ms
step:782/1670 train_time:72396ms step_avg:92.58ms
step:783/1670 train_time:72490ms step_avg:92.58ms
step:784/1670 train_time:72582ms step_avg:92.58ms
step:785/1670 train_time:72675ms step_avg:92.58ms
step:786/1670 train_time:72767ms step_avg:92.58ms
step:787/1670 train_time:72859ms step_avg:92.58ms
step:788/1670 train_time:72951ms step_avg:92.58ms
step:789/1670 train_time:73043ms step_avg:92.58ms
step:790/1670 train_time:73136ms step_avg:92.58ms
step:791/1670 train_time:73229ms step_avg:92.58ms
step:792/1670 train_time:73320ms step_avg:92.58ms
step:793/1670 train_time:73414ms step_avg:92.58ms
step:794/1670 train_time:73508ms step_avg:92.58ms
step:795/1670 train_time:73600ms step_avg:92.58ms
step:796/1670 train_time:73693ms step_avg:92.58ms
step:797/1670 train_time:73786ms step_avg:92.58ms
step:798/1670 train_time:73878ms step_avg:92.58ms
step:799/1670 train_time:73971ms step_avg:92.58ms
step:800/1670 train_time:74063ms step_avg:92.58ms
step:801/1670 train_time:74156ms step_avg:92.58ms
step:802/1670 train_time:74248ms step_avg:92.58ms
step:803/1670 train_time:74340ms step_avg:92.58ms
step:804/1670 train_time:74433ms step_avg:92.58ms
step:805/1670 train_time:74526ms step_avg:92.58ms
step:806/1670 train_time:74618ms step_avg:92.58ms
step:807/1670 train_time:74711ms step_avg:92.58ms
step:808/1670 train_time:74803ms step_avg:92.58ms
step:809/1670 train_time:74896ms step_avg:92.58ms
step:810/1670 train_time:74988ms step_avg:92.58ms
step:811/1670 train_time:75080ms step_avg:92.58ms
step:812/1670 train_time:75174ms step_avg:92.58ms
step:813/1670 train_time:75267ms step_avg:92.58ms
step:814/1670 train_time:75359ms step_avg:92.58ms
step:815/1670 train_time:75452ms step_avg:92.58ms
step:816/1670 train_time:75544ms step_avg:92.58ms
step:817/1670 train_time:75638ms step_avg:92.58ms
step:818/1670 train_time:75731ms step_avg:92.58ms
step:819/1670 train_time:75823ms step_avg:92.58ms
step:820/1670 train_time:75917ms step_avg:92.58ms
step:821/1670 train_time:76009ms step_avg:92.58ms
step:822/1670 train_time:76101ms step_avg:92.58ms
step:823/1670 train_time:76193ms step_avg:92.58ms
step:824/1670 train_time:76286ms step_avg:92.58ms
step:825/1670 train_time:76378ms step_avg:92.58ms
step:826/1670 train_time:76470ms step_avg:92.58ms
step:827/1670 train_time:76563ms step_avg:92.58ms
step:828/1670 train_time:76654ms step_avg:92.58ms
step:829/1670 train_time:76747ms step_avg:92.58ms
step:830/1670 train_time:76839ms step_avg:92.58ms
step:831/1670 train_time:76933ms step_avg:92.58ms
step:832/1670 train_time:77026ms step_avg:92.58ms
step:833/1670 train_time:77119ms step_avg:92.58ms
step:834/1670 train_time:77212ms step_avg:92.58ms
step:835/1670 train_time:77305ms step_avg:92.58ms
step:836/1670 train_time:77397ms step_avg:92.58ms
step:837/1670 train_time:77490ms step_avg:92.58ms
step:838/1670 train_time:77581ms step_avg:92.58ms
step:839/1670 train_time:77675ms step_avg:92.58ms
step:840/1670 train_time:77768ms step_avg:92.58ms
step:841/1670 train_time:77860ms step_avg:92.58ms
step:842/1670 train_time:77952ms step_avg:92.58ms
step:843/1670 train_time:78045ms step_avg:92.58ms
step:844/1670 train_time:78138ms step_avg:92.58ms
step:845/1670 train_time:78232ms step_avg:92.58ms
step:846/1670 train_time:78323ms step_avg:92.58ms
step:847/1670 train_time:78418ms step_avg:92.58ms
step:848/1670 train_time:78509ms step_avg:92.58ms
step:849/1670 train_time:78602ms step_avg:92.58ms
step:850/1670 train_time:78695ms step_avg:92.58ms
step:851/1670 train_time:78943ms step_avg:92.76ms
step:852/1670 train_time:79017ms step_avg:92.74ms
step:853/1670 train_time:79109ms step_avg:92.74ms
step:854/1670 train_time:79200ms step_avg:92.74ms
step:855/1670 train_time:79291ms step_avg:92.74ms
step:856/1670 train_time:79383ms step_avg:92.74ms
step:857/1670 train_time:79475ms step_avg:92.74ms
step:858/1670 train_time:79566ms step_avg:92.73ms
step:859/1670 train_time:79657ms step_avg:92.73ms
step:860/1670 train_time:79748ms step_avg:92.73ms
step:861/1670 train_time:79846ms step_avg:92.74ms
step:862/1670 train_time:79945ms step_avg:92.74ms
step:863/1670 train_time:80039ms step_avg:92.74ms
step:864/1670 train_time:80131ms step_avg:92.74ms
step:865/1670 train_time:80223ms step_avg:92.74ms
step:866/1670 train_time:80316ms step_avg:92.74ms
step:867/1670 train_time:80408ms step_avg:92.74ms
step:868/1670 train_time:80499ms step_avg:92.74ms
step:869/1670 train_time:80590ms step_avg:92.74ms
step:870/1670 train_time:80681ms step_avg:92.74ms
step:871/1670 train_time:80775ms step_avg:92.74ms
step:872/1670 train_time:80870ms step_avg:92.74ms
step:873/1670 train_time:80964ms step_avg:92.74ms
step:874/1670 train_time:81058ms step_avg:92.74ms
step:875/1670 train_time:81150ms step_avg:92.74ms
step:875/1670 val_loss:3.5154 train_time:81241ms step_avg:92.85ms
step:876/1670 train_time:81258ms step_avg:92.76ms
step:877/1670 train_time:81334ms step_avg:92.74ms
step:878/1670 train_time:81428ms step_avg:92.74ms
step:879/1670 train_time:81519ms step_avg:92.74ms
step:880/1670 train_time:81610ms step_avg:92.74ms
step:881/1670 train_time:81701ms step_avg:92.74ms
step:882/1670 train_time:81793ms step_avg:92.74ms
step:883/1670 train_time:81884ms step_avg:92.73ms
step:884/1670 train_time:81976ms step_avg:92.73ms
step:885/1670 train_time:82070ms step_avg:92.73ms
step:886/1670 train_time:82164ms step_avg:92.74ms
step:887/1670 train_time:82259ms step_avg:92.74ms
step:888/1670 train_time:82353ms step_avg:92.74ms
step:889/1670 train_time:82446ms step_avg:92.74ms
step:890/1670 train_time:82538ms step_avg:92.74ms
step:891/1670 train_time:82630ms step_avg:92.74ms
step:892/1670 train_time:82721ms step_avg:92.74ms
step:893/1670 train_time:82813ms step_avg:92.74ms
step:894/1670 train_time:82905ms step_avg:92.73ms
step:895/1670 train_time:82999ms step_avg:92.74ms
step:896/1670 train_time:83094ms step_avg:92.74ms
step:897/1670 train_time:83187ms step_avg:92.74ms
step:898/1670 train_time:83280ms step_avg:92.74ms
step:899/1670 train_time:83373ms step_avg:92.74ms
step:900/1670 train_time:83466ms step_avg:92.74ms
step:901/1670 train_time:83558ms step_avg:92.74ms
step:902/1670 train_time:83652ms step_avg:92.74ms
step:903/1670 train_time:83743ms step_avg:92.74ms
step:904/1670 train_time:83835ms step_avg:92.74ms
step:905/1670 train_time:83927ms step_avg:92.74ms
step:906/1670 train_time:84020ms step_avg:92.74ms
step:907/1670 train_time:84113ms step_avg:92.74ms
step:908/1670 train_time:84205ms step_avg:92.74ms
step:909/1670 train_time:84300ms step_avg:92.74ms
step:910/1670 train_time:84393ms step_avg:92.74ms
step:911/1670 train_time:84485ms step_avg:92.74ms
step:912/1670 train_time:84577ms step_avg:92.74ms
step:913/1670 train_time:84670ms step_avg:92.74ms
step:914/1670 train_time:84762ms step_avg:92.74ms
step:915/1670 train_time:84854ms step_avg:92.74ms
step:916/1670 train_time:84946ms step_avg:92.74ms
step:917/1670 train_time:85038ms step_avg:92.73ms
step:918/1670 train_time:85131ms step_avg:92.74ms
step:919/1670 train_time:85223ms step_avg:92.74ms
step:920/1670 train_time:85317ms step_avg:92.74ms
step:921/1670 train_time:85410ms step_avg:92.74ms
step:922/1670 train_time:85502ms step_avg:92.73ms
step:923/1670 train_time:85595ms step_avg:92.74ms
step:924/1670 train_time:85687ms step_avg:92.74ms
step:925/1670 train_time:85780ms step_avg:92.73ms
step:926/1670 train_time:85872ms step_avg:92.73ms
step:927/1670 train_time:85963ms step_avg:92.73ms
step:928/1670 train_time:86057ms step_avg:92.73ms
step:929/1670 train_time:86149ms step_avg:92.73ms
step:930/1670 train_time:86241ms step_avg:92.73ms
step:931/1670 train_time:86334ms step_avg:92.73ms
step:932/1670 train_time:86426ms step_avg:92.73ms
step:933/1670 train_time:86519ms step_avg:92.73ms
step:934/1670 train_time:86612ms step_avg:92.73ms
step:935/1670 train_time:86704ms step_avg:92.73ms
step:936/1670 train_time:86797ms step_avg:92.73ms
step:937/1670 train_time:86889ms step_avg:92.73ms
step:938/1670 train_time:86981ms step_avg:92.73ms
step:939/1670 train_time:87074ms step_avg:92.73ms
step:940/1670 train_time:87167ms step_avg:92.73ms
step:941/1670 train_time:87260ms step_avg:92.73ms
step:942/1670 train_time:87352ms step_avg:92.73ms
step:943/1670 train_time:87445ms step_avg:92.73ms
step:944/1670 train_time:87537ms step_avg:92.73ms
step:945/1670 train_time:87630ms step_avg:92.73ms
step:946/1670 train_time:87723ms step_avg:92.73ms
step:947/1670 train_time:87817ms step_avg:92.73ms
step:948/1670 train_time:87909ms step_avg:92.73ms
step:949/1670 train_time:88001ms step_avg:92.73ms
step:950/1670 train_time:88095ms step_avg:92.73ms
step:951/1670 train_time:88188ms step_avg:92.73ms
step:952/1670 train_time:88280ms step_avg:92.73ms
step:953/1670 train_time:88373ms step_avg:92.73ms
step:954/1670 train_time:88466ms step_avg:92.73ms
step:955/1670 train_time:88558ms step_avg:92.73ms
step:956/1670 train_time:88651ms step_avg:92.73ms
step:957/1670 train_time:88743ms step_avg:92.73ms
step:958/1670 train_time:88835ms step_avg:92.73ms
step:959/1670 train_time:88928ms step_avg:92.73ms
step:960/1670 train_time:89020ms step_avg:92.73ms
step:961/1670 train_time:89113ms step_avg:92.73ms
step:962/1670 train_time:89205ms step_avg:92.73ms
step:963/1670 train_time:89299ms step_avg:92.73ms
step:964/1670 train_time:89392ms step_avg:92.73ms
step:965/1670 train_time:89484ms step_avg:92.73ms
step:966/1670 train_time:89577ms step_avg:92.73ms
step:967/1670 train_time:89670ms step_avg:92.73ms
step:968/1670 train_time:89762ms step_avg:92.73ms
step:969/1670 train_time:89855ms step_avg:92.73ms
step:970/1670 train_time:89947ms step_avg:92.73ms
step:971/1670 train_time:90039ms step_avg:92.73ms
step:972/1670 train_time:90132ms step_avg:92.73ms
step:973/1670 train_time:90224ms step_avg:92.73ms
step:974/1670 train_time:90317ms step_avg:92.73ms
step:975/1670 train_time:90410ms step_avg:92.73ms
step:976/1670 train_time:90502ms step_avg:92.73ms
step:977/1670 train_time:90595ms step_avg:92.73ms
step:978/1670 train_time:90687ms step_avg:92.73ms
step:979/1670 train_time:90779ms step_avg:92.73ms
step:980/1670 train_time:90872ms step_avg:92.73ms
step:981/1670 train_time:90963ms step_avg:92.72ms
step:982/1670 train_time:91057ms step_avg:92.73ms
step:983/1670 train_time:91149ms step_avg:92.73ms
step:984/1670 train_time:91242ms step_avg:92.73ms
step:985/1670 train_time:91334ms step_avg:92.73ms
step:986/1670 train_time:91426ms step_avg:92.72ms
step:987/1670 train_time:91519ms step_avg:92.72ms
step:988/1670 train_time:91611ms step_avg:92.72ms
step:989/1670 train_time:91702ms step_avg:92.72ms
step:990/1670 train_time:91796ms step_avg:92.72ms
step:991/1670 train_time:91890ms step_avg:92.72ms
step:992/1670 train_time:91982ms step_avg:92.72ms
step:993/1670 train_time:92075ms step_avg:92.72ms
step:994/1670 train_time:92167ms step_avg:92.72ms
step:995/1670 train_time:92260ms step_avg:92.72ms
step:996/1670 train_time:92353ms step_avg:92.72ms
step:997/1670 train_time:92445ms step_avg:92.72ms
step:998/1670 train_time:92537ms step_avg:92.72ms
step:999/1670 train_time:92630ms step_avg:92.72ms
step:1000/1670 train_time:92723ms step_avg:92.72ms
step:1000/1670 val_loss:3.4673 train_time:92816ms step_avg:92.82ms
step:1001/1670 train_time:92834ms step_avg:92.74ms
step:1002/1670 train_time:92911ms step_avg:92.73ms
step:1003/1670 train_time:93002ms step_avg:92.72ms
step:1004/1670 train_time:93094ms step_avg:92.72ms
step:1005/1670 train_time:93186ms step_avg:92.72ms
step:1006/1670 train_time:93278ms step_avg:92.72ms
step:1007/1670 train_time:93370ms step_avg:92.72ms
step:1008/1670 train_time:93462ms step_avg:92.72ms
step:1009/1670 train_time:93555ms step_avg:92.72ms
step:1010/1670 train_time:93647ms step_avg:92.72ms
step:1011/1670 train_time:93740ms step_avg:92.72ms
step:1012/1670 train_time:93835ms step_avg:92.72ms
step:1013/1670 train_time:93930ms step_avg:92.72ms
step:1014/1670 train_time:94022ms step_avg:92.72ms
step:1015/1670 train_time:94114ms step_avg:92.72ms
step:1016/1670 train_time:94207ms step_avg:92.72ms
step:1017/1670 train_time:94298ms step_avg:92.72ms
step:1018/1670 train_time:94391ms step_avg:92.72ms
step:1019/1670 train_time:94483ms step_avg:92.72ms
step:1020/1670 train_time:94576ms step_avg:92.72ms
step:1021/1670 train_time:94668ms step_avg:92.72ms
step:1022/1670 train_time:94761ms step_avg:92.72ms
step:1023/1670 train_time:94855ms step_avg:92.72ms
step:1024/1670 train_time:94948ms step_avg:92.72ms
step:1025/1670 train_time:95040ms step_avg:92.72ms
step:1026/1670 train_time:95134ms step_avg:92.72ms
step:1027/1670 train_time:95227ms step_avg:92.72ms
step:1028/1670 train_time:95319ms step_avg:92.72ms
step:1029/1670 train_time:95411ms step_avg:92.72ms
step:1030/1670 train_time:95503ms step_avg:92.72ms
step:1031/1670 train_time:95596ms step_avg:92.72ms
step:1032/1670 train_time:95688ms step_avg:92.72ms
step:1033/1670 train_time:95780ms step_avg:92.72ms
step:1034/1670 train_time:95874ms step_avg:92.72ms
step:1035/1670 train_time:95968ms step_avg:92.72ms
step:1036/1670 train_time:96060ms step_avg:92.72ms
step:1037/1670 train_time:96154ms step_avg:92.72ms
step:1038/1670 train_time:96247ms step_avg:92.72ms
step:1039/1670 train_time:96339ms step_avg:92.72ms
step:1040/1670 train_time:96430ms step_avg:92.72ms
step:1041/1670 train_time:96523ms step_avg:92.72ms
step:1042/1670 train_time:96616ms step_avg:92.72ms
step:1043/1670 train_time:96708ms step_avg:92.72ms
step:1044/1670 train_time:96800ms step_avg:92.72ms
step:1045/1670 train_time:96893ms step_avg:92.72ms
step:1046/1670 train_time:96985ms step_avg:92.72ms
step:1047/1670 train_time:97078ms step_avg:92.72ms
step:1048/1670 train_time:97170ms step_avg:92.72ms
step:1049/1670 train_time:97262ms step_avg:92.72ms
step:1050/1670 train_time:97356ms step_avg:92.72ms
step:1051/1670 train_time:97448ms step_avg:92.72ms
step:1052/1670 train_time:97541ms step_avg:92.72ms
step:1053/1670 train_time:97635ms step_avg:92.72ms
step:1054/1670 train_time:97728ms step_avg:92.72ms
step:1055/1670 train_time:97820ms step_avg:92.72ms
step:1056/1670 train_time:97912ms step_avg:92.72ms
step:1057/1670 train_time:98005ms step_avg:92.72ms
step:1058/1670 train_time:98097ms step_avg:92.72ms
step:1059/1670 train_time:98190ms step_avg:92.72ms
step:1060/1670 train_time:98282ms step_avg:92.72ms
step:1061/1670 train_time:98375ms step_avg:92.72ms
step:1062/1670 train_time:98624ms step_avg:92.87ms
step:1063/1670 train_time:98697ms step_avg:92.85ms
step:1064/1670 train_time:98788ms step_avg:92.85ms
step:1065/1670 train_time:98879ms step_avg:92.84ms
step:1066/1670 train_time:98970ms step_avg:92.84ms
step:1067/1670 train_time:99061ms step_avg:92.84ms
step:1068/1670 train_time:99153ms step_avg:92.84ms
step:1069/1670 train_time:99244ms step_avg:92.84ms
step:1070/1670 train_time:99335ms step_avg:92.84ms
step:1071/1670 train_time:99426ms step_avg:92.83ms
step:1072/1670 train_time:99523ms step_avg:92.84ms
step:1073/1670 train_time:99619ms step_avg:92.84ms
step:1074/1670 train_time:99714ms step_avg:92.84ms
step:1075/1670 train_time:99807ms step_avg:92.84ms
step:1076/1670 train_time:99898ms step_avg:92.84ms
step:1077/1670 train_time:99991ms step_avg:92.84ms
step:1078/1670 train_time:100082ms step_avg:92.84ms
step:1079/1670 train_time:100174ms step_avg:92.84ms
step:1080/1670 train_time:100265ms step_avg:92.84ms
step:1081/1670 train_time:100356ms step_avg:92.84ms
step:1082/1670 train_time:100452ms step_avg:92.84ms
step:1083/1670 train_time:100546ms step_avg:92.84ms
step:1084/1670 train_time:100640ms step_avg:92.84ms
step:1085/1670 train_time:100734ms step_avg:92.84ms
step:1086/1670 train_time:100827ms step_avg:92.84ms
step:1087/1670 train_time:100919ms step_avg:92.84ms
step:1088/1670 train_time:101012ms step_avg:92.84ms
step:1089/1670 train_time:101103ms step_avg:92.84ms
step:1090/1670 train_time:101194ms step_avg:92.84ms
step:1091/1670 train_time:101286ms step_avg:92.84ms
step:1092/1670 train_time:101378ms step_avg:92.84ms
step:1093/1670 train_time:101470ms step_avg:92.84ms
step:1094/1670 train_time:101563ms step_avg:92.84ms
step:1095/1670 train_time:101657ms step_avg:92.84ms
step:1096/1670 train_time:101751ms step_avg:92.84ms
step:1097/1670 train_time:101843ms step_avg:92.84ms
step:1098/1670 train_time:101936ms step_avg:92.84ms
step:1099/1670 train_time:102029ms step_avg:92.84ms
step:1100/1670 train_time:102120ms step_avg:92.84ms
step:1101/1670 train_time:102212ms step_avg:92.84ms
step:1102/1670 train_time:102304ms step_avg:92.83ms
step:1103/1670 train_time:102396ms step_avg:92.83ms
step:1104/1670 train_time:102489ms step_avg:92.83ms
step:1105/1670 train_time:102581ms step_avg:92.83ms
step:1106/1670 train_time:102675ms step_avg:92.83ms
step:1107/1670 train_time:102768ms step_avg:92.83ms
step:1108/1670 train_time:102860ms step_avg:92.83ms
step:1109/1670 train_time:102955ms step_avg:92.84ms
step:1110/1670 train_time:103047ms step_avg:92.83ms
step:1111/1670 train_time:103139ms step_avg:92.83ms
step:1112/1670 train_time:103231ms step_avg:92.83ms
step:1113/1670 train_time:103323ms step_avg:92.83ms
step:1114/1670 train_time:103415ms step_avg:92.83ms
step:1115/1670 train_time:103701ms step_avg:93.01ms
step:1116/1670 train_time:103778ms step_avg:92.99ms
step:1117/1670 train_time:103871ms step_avg:92.99ms
step:1118/1670 train_time:103962ms step_avg:92.99ms
step:1119/1670 train_time:104054ms step_avg:92.99ms
step:1120/1670 train_time:104146ms step_avg:92.99ms
step:1121/1670 train_time:104237ms step_avg:92.99ms
step:1122/1670 train_time:104329ms step_avg:92.99ms
step:1123/1670 train_time:104421ms step_avg:92.98ms
step:1124/1670 train_time:104514ms step_avg:92.98ms
step:1125/1670 train_time:104614ms step_avg:92.99ms
step:1125/1670 val_loss:3.4139 train_time:104716ms step_avg:93.08ms
step:1126/1670 train_time:104734ms step_avg:93.01ms
step:1127/1670 train_time:104816ms step_avg:93.00ms
step:1128/1670 train_time:104919ms step_avg:93.01ms
step:1129/1670 train_time:105014ms step_avg:93.01ms
step:1130/1670 train_time:105105ms step_avg:93.01ms
step:1131/1670 train_time:105197ms step_avg:93.01ms
step:1132/1670 train_time:105289ms step_avg:93.01ms
step:1133/1670 train_time:105381ms step_avg:93.01ms
step:1134/1670 train_time:105473ms step_avg:93.01ms
step:1135/1670 train_time:105564ms step_avg:93.01ms
step:1136/1670 train_time:105657ms step_avg:93.01ms
step:1137/1670 train_time:105751ms step_avg:93.01ms
step:1138/1670 train_time:105847ms step_avg:93.01ms
step:1139/1670 train_time:105945ms step_avg:93.02ms
step:1140/1670 train_time:106038ms step_avg:93.02ms
step:1141/1670 train_time:106131ms step_avg:93.02ms
step:1142/1670 train_time:106224ms step_avg:93.02ms
step:1143/1670 train_time:106316ms step_avg:93.01ms
step:1144/1670 train_time:106408ms step_avg:93.01ms
step:1145/1670 train_time:106501ms step_avg:93.01ms
step:1146/1670 train_time:106593ms step_avg:93.01ms
step:1147/1670 train_time:106685ms step_avg:93.01ms
step:1148/1670 train_time:106781ms step_avg:93.01ms
step:1149/1670 train_time:106878ms step_avg:93.02ms
step:1150/1670 train_time:106972ms step_avg:93.02ms
step:1151/1670 train_time:107065ms step_avg:93.02ms
step:1152/1670 train_time:107159ms step_avg:93.02ms
step:1153/1670 train_time:107251ms step_avg:93.02ms
step:1154/1670 train_time:107344ms step_avg:93.02ms
step:1155/1670 train_time:107436ms step_avg:93.02ms
step:1156/1670 train_time:107528ms step_avg:93.02ms
step:1157/1670 train_time:107621ms step_avg:93.02ms
step:1158/1670 train_time:107715ms step_avg:93.02ms
step:1159/1670 train_time:107809ms step_avg:93.02ms
step:1160/1670 train_time:107904ms step_avg:93.02ms
step:1161/1670 train_time:107998ms step_avg:93.02ms
step:1162/1670 train_time:108090ms step_avg:93.02ms
step:1163/1670 train_time:108184ms step_avg:93.02ms
step:1164/1670 train_time:108277ms step_avg:93.02ms
step:1165/1670 train_time:108369ms step_avg:93.02ms
step:1166/1670 train_time:108462ms step_avg:93.02ms
step:1167/1670 train_time:108555ms step_avg:93.02ms
step:1168/1670 train_time:108648ms step_avg:93.02ms
step:1169/1670 train_time:108743ms step_avg:93.02ms
step:1170/1670 train_time:108838ms step_avg:93.02ms
step:1171/1670 train_time:108932ms step_avg:93.02ms
step:1172/1670 train_time:109026ms step_avg:93.03ms
step:1173/1670 train_time:109118ms step_avg:93.02ms
step:1174/1670 train_time:109211ms step_avg:93.03ms
step:1175/1670 train_time:109305ms step_avg:93.03ms
step:1176/1670 train_time:109398ms step_avg:93.03ms
step:1177/1670 train_time:109490ms step_avg:93.02ms
step:1178/1670 train_time:109582ms step_avg:93.02ms
step:1179/1670 train_time:109675ms step_avg:93.02ms
step:1180/1670 train_time:109768ms step_avg:93.02ms
step:1181/1670 train_time:109863ms step_avg:93.03ms
step:1182/1670 train_time:109956ms step_avg:93.03ms
step:1183/1670 train_time:110049ms step_avg:93.03ms
step:1184/1670 train_time:110144ms step_avg:93.03ms
step:1185/1670 train_time:110237ms step_avg:93.03ms
step:1186/1670 train_time:110330ms step_avg:93.03ms
step:1187/1670 train_time:110423ms step_avg:93.03ms
step:1188/1670 train_time:110515ms step_avg:93.03ms
step:1189/1670 train_time:110608ms step_avg:93.03ms
step:1190/1670 train_time:110702ms step_avg:93.03ms
step:1191/1670 train_time:110796ms step_avg:93.03ms
step:1192/1670 train_time:110889ms step_avg:93.03ms
step:1193/1670 train_time:110982ms step_avg:93.03ms
step:1194/1670 train_time:111076ms step_avg:93.03ms
step:1195/1670 train_time:111169ms step_avg:93.03ms
step:1196/1670 train_time:111262ms step_avg:93.03ms
step:1197/1670 train_time:111355ms step_avg:93.03ms
step:1198/1670 train_time:111448ms step_avg:93.03ms
step:1199/1670 train_time:111541ms step_avg:93.03ms
step:1200/1670 train_time:111634ms step_avg:93.03ms
step:1201/1670 train_time:111726ms step_avg:93.03ms
step:1202/1670 train_time:111820ms step_avg:93.03ms
step:1203/1670 train_time:111913ms step_avg:93.03ms
step:1204/1670 train_time:112006ms step_avg:93.03ms
step:1205/1670 train_time:112100ms step_avg:93.03ms
step:1206/1670 train_time:112194ms step_avg:93.03ms
step:1207/1670 train_time:112286ms step_avg:93.03ms
step:1208/1670 train_time:112381ms step_avg:93.03ms
step:1209/1670 train_time:112473ms step_avg:93.03ms
step:1210/1670 train_time:112567ms step_avg:93.03ms
step:1211/1670 train_time:112661ms step_avg:93.03ms
step:1212/1670 train_time:112754ms step_avg:93.03ms
step:1213/1670 train_time:112847ms step_avg:93.03ms
step:1214/1670 train_time:112939ms step_avg:93.03ms
step:1215/1670 train_time:113033ms step_avg:93.03ms
step:1216/1670 train_time:113127ms step_avg:93.03ms
step:1217/1670 train_time:113221ms step_avg:93.03ms
step:1218/1670 train_time:113315ms step_avg:93.03ms
step:1219/1670 train_time:113408ms step_avg:93.03ms
step:1220/1670 train_time:113502ms step_avg:93.03ms
step:1221/1670 train_time:113595ms step_avg:93.03ms
step:1222/1670 train_time:113687ms step_avg:93.03ms
step:1223/1670 train_time:113781ms step_avg:93.03ms
step:1224/1670 train_time:113874ms step_avg:93.03ms
step:1225/1670 train_time:113966ms step_avg:93.03ms
step:1226/1670 train_time:114060ms step_avg:93.03ms
step:1227/1670 train_time:114152ms step_avg:93.03ms
step:1228/1670 train_time:114246ms step_avg:93.03ms
step:1229/1670 train_time:114341ms step_avg:93.04ms
step:1230/1670 train_time:114435ms step_avg:93.04ms
step:1231/1670 train_time:114528ms step_avg:93.04ms
step:1232/1670 train_time:114623ms step_avg:93.04ms
step:1233/1670 train_time:114715ms step_avg:93.04ms
step:1234/1670 train_time:114807ms step_avg:93.04ms
step:1235/1670 train_time:114901ms step_avg:93.04ms
step:1236/1670 train_time:114994ms step_avg:93.04ms
step:1237/1670 train_time:115086ms step_avg:93.04ms
step:1238/1670 train_time:115179ms step_avg:93.04ms
step:1239/1670 train_time:115273ms step_avg:93.04ms
step:1240/1670 train_time:115366ms step_avg:93.04ms
step:1241/1670 train_time:115460ms step_avg:93.04ms
step:1242/1670 train_time:115553ms step_avg:93.04ms
step:1243/1670 train_time:115646ms step_avg:93.04ms
step:1244/1670 train_time:115741ms step_avg:93.04ms
step:1245/1670 train_time:115835ms step_avg:93.04ms
step:1246/1670 train_time:115927ms step_avg:93.04ms
step:1247/1670 train_time:116020ms step_avg:93.04ms
step:1248/1670 train_time:116114ms step_avg:93.04ms
step:1249/1670 train_time:116207ms step_avg:93.04ms
step:1250/1670 train_time:116300ms step_avg:93.04ms
step:1250/1670 val_loss:3.3762 train_time:116392ms step_avg:93.11ms
step:1251/1670 train_time:116411ms step_avg:93.05ms
step:1252/1670 train_time:116487ms step_avg:93.04ms
step:1253/1670 train_time:116579ms step_avg:93.04ms
step:1254/1670 train_time:116671ms step_avg:93.04ms
step:1255/1670 train_time:116764ms step_avg:93.04ms
step:1256/1670 train_time:116857ms step_avg:93.04ms
step:1257/1670 train_time:116949ms step_avg:93.04ms
step:1258/1670 train_time:117044ms step_avg:93.04ms
step:1259/1670 train_time:117137ms step_avg:93.04ms
step:1260/1670 train_time:117229ms step_avg:93.04ms
step:1261/1670 train_time:117324ms step_avg:93.04ms
step:1262/1670 train_time:117419ms step_avg:93.04ms
step:1263/1670 train_time:117512ms step_avg:93.04ms
step:1264/1670 train_time:117605ms step_avg:93.04ms
step:1265/1670 train_time:117698ms step_avg:93.04ms
step:1266/1670 train_time:117790ms step_avg:93.04ms
step:1267/1670 train_time:117885ms step_avg:93.04ms
step:1268/1670 train_time:117979ms step_avg:93.04ms
step:1269/1670 train_time:118071ms step_avg:93.04ms
step:1270/1670 train_time:118165ms step_avg:93.04ms
step:1271/1670 train_time:118258ms step_avg:93.04ms
step:1272/1670 train_time:118352ms step_avg:93.04ms
step:1273/1670 train_time:118446ms step_avg:93.04ms
step:1274/1670 train_time:118685ms step_avg:93.16ms
step:1275/1670 train_time:118773ms step_avg:93.16ms
step:1276/1670 train_time:118864ms step_avg:93.15ms
step:1277/1670 train_time:118956ms step_avg:93.15ms
step:1278/1670 train_time:119048ms step_avg:93.15ms
step:1279/1670 train_time:119140ms step_avg:93.15ms
step:1280/1670 train_time:119231ms step_avg:93.15ms
step:1281/1670 train_time:119323ms step_avg:93.15ms
step:1282/1670 train_time:119416ms step_avg:93.15ms
step:1283/1670 train_time:119507ms step_avg:93.15ms
step:1284/1670 train_time:119605ms step_avg:93.15ms
step:1285/1670 train_time:119703ms step_avg:93.15ms
step:1286/1670 train_time:119797ms step_avg:93.15ms
step:1287/1670 train_time:119890ms step_avg:93.15ms
step:1288/1670 train_time:119984ms step_avg:93.16ms
step:1289/1670 train_time:120076ms step_avg:93.15ms
step:1290/1670 train_time:120168ms step_avg:93.15ms
step:1291/1670 train_time:120260ms step_avg:93.15ms
step:1292/1670 train_time:120352ms step_avg:93.15ms
step:1293/1670 train_time:120445ms step_avg:93.15ms
step:1294/1670 train_time:120538ms step_avg:93.15ms
step:1295/1670 train_time:120633ms step_avg:93.15ms
step:1296/1670 train_time:120728ms step_avg:93.15ms
step:1297/1670 train_time:120821ms step_avg:93.15ms
step:1298/1670 train_time:120914ms step_avg:93.15ms
step:1299/1670 train_time:121008ms step_avg:93.15ms
step:1300/1670 train_time:121102ms step_avg:93.16ms
step:1301/1670 train_time:121195ms step_avg:93.16ms
step:1302/1670 train_time:121288ms step_avg:93.15ms
step:1303/1670 train_time:121380ms step_avg:93.15ms
step:1304/1670 train_time:121472ms step_avg:93.15ms
step:1305/1670 train_time:121565ms step_avg:93.15ms
step:1306/1670 train_time:121659ms step_avg:93.15ms
step:1307/1670 train_time:121752ms step_avg:93.15ms
step:1308/1670 train_time:121846ms step_avg:93.15ms
step:1309/1670 train_time:121940ms step_avg:93.16ms
step:1310/1670 train_time:122033ms step_avg:93.16ms
step:1311/1670 train_time:122126ms step_avg:93.16ms
step:1312/1670 train_time:122219ms step_avg:93.15ms
step:1313/1670 train_time:122311ms step_avg:93.15ms
step:1314/1670 train_time:122406ms step_avg:93.15ms
step:1315/1670 train_time:122498ms step_avg:93.15ms
step:1316/1670 train_time:122590ms step_avg:93.15ms
step:1317/1670 train_time:122687ms step_avg:93.16ms
step:1318/1670 train_time:122782ms step_avg:93.16ms
step:1319/1670 train_time:122875ms step_avg:93.16ms
step:1320/1670 train_time:122968ms step_avg:93.16ms
step:1321/1670 train_time:123061ms step_avg:93.16ms
step:1322/1670 train_time:123154ms step_avg:93.16ms
step:1323/1670 train_time:123247ms step_avg:93.16ms
step:1324/1670 train_time:123340ms step_avg:93.16ms
step:1325/1670 train_time:123433ms step_avg:93.16ms
step:1326/1670 train_time:123527ms step_avg:93.16ms
step:1327/1670 train_time:123620ms step_avg:93.16ms
step:1328/1670 train_time:123714ms step_avg:93.16ms
step:1329/1670 train_time:123808ms step_avg:93.16ms
step:1330/1670 train_time:123902ms step_avg:93.16ms
step:1331/1670 train_time:123995ms step_avg:93.16ms
step:1332/1670 train_time:124090ms step_avg:93.16ms
step:1333/1670 train_time:124183ms step_avg:93.16ms
step:1334/1670 train_time:124274ms step_avg:93.16ms
step:1335/1670 train_time:124368ms step_avg:93.16ms
step:1336/1670 train_time:124461ms step_avg:93.16ms
step:1337/1670 train_time:124553ms step_avg:93.16ms
step:1338/1670 train_time:124647ms step_avg:93.16ms
step:1339/1670 train_time:124742ms step_avg:93.16ms
step:1340/1670 train_time:124835ms step_avg:93.16ms
step:1341/1670 train_time:124928ms step_avg:93.16ms
step:1342/1670 train_time:125021ms step_avg:93.16ms
step:1343/1670 train_time:125114ms step_avg:93.16ms
step:1344/1670 train_time:125207ms step_avg:93.16ms
step:1345/1670 train_time:125301ms step_avg:93.16ms
step:1346/1670 train_time:125392ms step_avg:93.16ms
step:1347/1670 train_time:125488ms step_avg:93.16ms
step:1348/1670 train_time:125581ms step_avg:93.16ms
step:1349/1670 train_time:125673ms step_avg:93.16ms
step:1350/1670 train_time:125767ms step_avg:93.16ms
step:1351/1670 train_time:125860ms step_avg:93.16ms
step:1352/1670 train_time:125953ms step_avg:93.16ms
step:1353/1670 train_time:126047ms step_avg:93.16ms
step:1354/1670 train_time:126140ms step_avg:93.16ms
step:1355/1670 train_time:126232ms step_avg:93.16ms
step:1356/1670 train_time:126326ms step_avg:93.16ms
step:1357/1670 train_time:126418ms step_avg:93.16ms
step:1358/1670 train_time:126512ms step_avg:93.16ms
step:1359/1670 train_time:126606ms step_avg:93.16ms
step:1360/1670 train_time:126700ms step_avg:93.16ms
step:1361/1670 train_time:126793ms step_avg:93.16ms
step:1362/1670 train_time:126886ms step_avg:93.16ms
step:1363/1670 train_time:126977ms step_avg:93.16ms
step:1364/1670 train_time:127072ms step_avg:93.16ms
step:1365/1670 train_time:127165ms step_avg:93.16ms
step:1366/1670 train_time:127258ms step_avg:93.16ms
step:1367/1670 train_time:127351ms step_avg:93.16ms
step:1368/1670 train_time:127444ms step_avg:93.16ms
step:1369/1670 train_time:127538ms step_avg:93.16ms
step:1370/1670 train_time:127631ms step_avg:93.16ms
step:1371/1670 train_time:127724ms step_avg:93.16ms
step:1372/1670 train_time:127818ms step_avg:93.16ms
step:1373/1670 train_time:127911ms step_avg:93.16ms
step:1374/1670 train_time:128004ms step_avg:93.16ms
step:1375/1670 train_time:128097ms step_avg:93.16ms
step:1375/1670 val_loss:3.3416 train_time:128190ms step_avg:93.23ms
step:1376/1670 train_time:128209ms step_avg:93.17ms
step:1377/1670 train_time:128284ms step_avg:93.16ms
step:1378/1670 train_time:128377ms step_avg:93.16ms
step:1379/1670 train_time:128470ms step_avg:93.16ms
step:1380/1670 train_time:128562ms step_avg:93.16ms
step:1381/1670 train_time:128654ms step_avg:93.16ms
step:1382/1670 train_time:128746ms step_avg:93.16ms
step:1383/1670 train_time:128839ms step_avg:93.16ms
step:1384/1670 train_time:128934ms step_avg:93.16ms
step:1385/1670 train_time:129026ms step_avg:93.16ms
step:1386/1670 train_time:129120ms step_avg:93.16ms
step:1387/1670 train_time:129216ms step_avg:93.16ms
step:1388/1670 train_time:129310ms step_avg:93.16ms
step:1389/1670 train_time:129403ms step_avg:93.16ms
step:1390/1670 train_time:129497ms step_avg:93.16ms
step:1391/1670 train_time:129589ms step_avg:93.16ms
step:1392/1670 train_time:129681ms step_avg:93.16ms
step:1393/1670 train_time:129775ms step_avg:93.16ms
step:1394/1670 train_time:129868ms step_avg:93.16ms
step:1395/1670 train_time:129961ms step_avg:93.16ms
step:1396/1670 train_time:130055ms step_avg:93.16ms
step:1397/1670 train_time:130148ms step_avg:93.16ms
step:1398/1670 train_time:130241ms step_avg:93.16ms
step:1399/1670 train_time:130336ms step_avg:93.16ms
step:1400/1670 train_time:130430ms step_avg:93.16ms
step:1401/1670 train_time:130523ms step_avg:93.16ms
step:1402/1670 train_time:130617ms step_avg:93.16ms
step:1403/1670 train_time:130710ms step_avg:93.16ms
step:1404/1670 train_time:130803ms step_avg:93.16ms
step:1405/1670 train_time:130896ms step_avg:93.16ms
step:1406/1670 train_time:130991ms step_avg:93.17ms
step:1407/1670 train_time:131083ms step_avg:93.17ms
step:1408/1670 train_time:131177ms step_avg:93.17ms
step:1409/1670 train_time:131271ms step_avg:93.17ms
step:1410/1670 train_time:131364ms step_avg:93.17ms
step:1411/1670 train_time:131458ms step_avg:93.17ms
step:1412/1670 train_time:131551ms step_avg:93.17ms
step:1413/1670 train_time:131644ms step_avg:93.17ms
step:1414/1670 train_time:131736ms step_avg:93.17ms
step:1415/1670 train_time:131829ms step_avg:93.17ms
step:1416/1670 train_time:131922ms step_avg:93.17ms
step:1417/1670 train_time:132015ms step_avg:93.17ms
step:1418/1670 train_time:132109ms step_avg:93.17ms
step:1419/1670 train_time:132203ms step_avg:93.17ms
step:1420/1670 train_time:132297ms step_avg:93.17ms
step:1421/1670 train_time:132391ms step_avg:93.17ms
step:1422/1670 train_time:132485ms step_avg:93.17ms
step:1423/1670 train_time:132578ms step_avg:93.17ms
step:1424/1670 train_time:132672ms step_avg:93.17ms
step:1425/1670 train_time:132766ms step_avg:93.17ms
step:1426/1670 train_time:132858ms step_avg:93.17ms
step:1427/1670 train_time:132952ms step_avg:93.17ms
step:1428/1670 train_time:133045ms step_avg:93.17ms
step:1429/1670 train_time:133139ms step_avg:93.17ms
step:1430/1670 train_time:133234ms step_avg:93.17ms
step:1431/1670 train_time:133328ms step_avg:93.17ms
step:1432/1670 train_time:133421ms step_avg:93.17ms
step:1433/1670 train_time:133514ms step_avg:93.17ms
step:1434/1670 train_time:133607ms step_avg:93.17ms
step:1435/1670 train_time:133700ms step_avg:93.17ms
step:1436/1670 train_time:133795ms step_avg:93.17ms
step:1437/1670 train_time:133887ms step_avg:93.17ms
step:1438/1670 train_time:133979ms step_avg:93.17ms
step:1439/1670 train_time:134073ms step_avg:93.17ms
step:1440/1670 train_time:134167ms step_avg:93.17ms
step:1441/1670 train_time:134260ms step_avg:93.17ms
step:1442/1670 train_time:134355ms step_avg:93.17ms
step:1443/1670 train_time:134448ms step_avg:93.17ms
step:1444/1670 train_time:134540ms step_avg:93.17ms
step:1445/1670 train_time:134634ms step_avg:93.17ms
step:1446/1670 train_time:134727ms step_avg:93.17ms
step:1447/1670 train_time:134820ms step_avg:93.17ms
step:1448/1670 train_time:134914ms step_avg:93.17ms
step:1449/1670 train_time:135006ms step_avg:93.17ms
step:1450/1670 train_time:135099ms step_avg:93.17ms
step:1451/1670 train_time:135195ms step_avg:93.17ms
step:1452/1670 train_time:135289ms step_avg:93.17ms
step:1453/1670 train_time:135382ms step_avg:93.17ms
step:1454/1670 train_time:135476ms step_avg:93.17ms
step:1455/1670 train_time:135569ms step_avg:93.17ms
step:1456/1670 train_time:135663ms step_avg:93.17ms
step:1457/1670 train_time:135756ms step_avg:93.17ms
step:1458/1670 train_time:135849ms step_avg:93.17ms
step:1459/1670 train_time:135941ms step_avg:93.17ms
step:1460/1670 train_time:136036ms step_avg:93.18ms
step:1461/1670 train_time:136130ms step_avg:93.18ms
step:1462/1670 train_time:136223ms step_avg:93.18ms
step:1463/1670 train_time:136316ms step_avg:93.18ms
step:1464/1670 train_time:136410ms step_avg:93.18ms
step:1465/1670 train_time:136503ms step_avg:93.18ms
step:1466/1670 train_time:136597ms step_avg:93.18ms
step:1467/1670 train_time:136691ms step_avg:93.18ms
step:1468/1670 train_time:136785ms step_avg:93.18ms
step:1469/1670 train_time:136878ms step_avg:93.18ms
step:1470/1670 train_time:136970ms step_avg:93.18ms
step:1471/1670 train_time:137063ms step_avg:93.18ms
step:1472/1670 train_time:137157ms step_avg:93.18ms
step:1473/1670 train_time:137250ms step_avg:93.18ms
step:1474/1670 train_time:137343ms step_avg:93.18ms
step:1475/1670 train_time:137438ms step_avg:93.18ms
step:1476/1670 train_time:137531ms step_avg:93.18ms
step:1477/1670 train_time:137625ms step_avg:93.18ms
step:1478/1670 train_time:137718ms step_avg:93.18ms
step:1479/1670 train_time:137811ms step_avg:93.18ms
step:1480/1670 train_time:137905ms step_avg:93.18ms
step:1481/1670 train_time:137999ms step_avg:93.18ms
step:1482/1670 train_time:138091ms step_avg:93.18ms
step:1483/1670 train_time:138185ms step_avg:93.18ms
step:1484/1670 train_time:138277ms step_avg:93.18ms
step:1485/1670 train_time:138529ms step_avg:93.29ms
step:1486/1670 train_time:138601ms step_avg:93.27ms
step:1487/1670 train_time:138693ms step_avg:93.27ms
step:1488/1670 train_time:138785ms step_avg:93.27ms
step:1489/1670 train_time:138876ms step_avg:93.27ms
step:1490/1670 train_time:138968ms step_avg:93.27ms
step:1491/1670 train_time:139060ms step_avg:93.27ms
step:1492/1670 train_time:139152ms step_avg:93.27ms
step:1493/1670 train_time:139244ms step_avg:93.26ms
step:1494/1670 train_time:139336ms step_avg:93.26ms
step:1495/1670 train_time:139437ms step_avg:93.27ms
step:1496/1670 train_time:139535ms step_avg:93.27ms
step:1497/1670 train_time:139629ms step_avg:93.27ms
step:1498/1670 train_time:139721ms step_avg:93.27ms
step:1499/1670 train_time:139814ms step_avg:93.27ms
step:1500/1670 train_time:139906ms step_avg:93.27ms
step:1500/1670 val_loss:3.3117 train_time:139999ms step_avg:93.33ms
step:1501/1670 train_time:140017ms step_avg:93.28ms
step:1502/1670 train_time:140095ms step_avg:93.27ms
step:1503/1670 train_time:140188ms step_avg:93.27ms
step:1504/1670 train_time:140280ms step_avg:93.27ms
step:1505/1670 train_time:140372ms step_avg:93.27ms
step:1506/1670 train_time:140464ms step_avg:93.27ms
step:1507/1670 train_time:140558ms step_avg:93.27ms
step:1508/1670 train_time:140652ms step_avg:93.27ms
step:1509/1670 train_time:140745ms step_avg:93.27ms
step:1510/1670 train_time:140839ms step_avg:93.27ms
step:1511/1670 train_time:140933ms step_avg:93.27ms
step:1512/1670 train_time:141027ms step_avg:93.27ms
step:1513/1670 train_time:141122ms step_avg:93.27ms
step:1514/1670 train_time:141215ms step_avg:93.27ms
step:1515/1670 train_time:141307ms step_avg:93.27ms
step:1516/1670 train_time:141400ms step_avg:93.27ms
step:1517/1670 train_time:141492ms step_avg:93.27ms
step:1518/1670 train_time:141585ms step_avg:93.27ms
step:1519/1670 train_time:141679ms step_avg:93.27ms
step:1520/1670 train_time:141771ms step_avg:93.27ms
step:1521/1670 train_time:141865ms step_avg:93.27ms
step:1522/1670 train_time:141959ms step_avg:93.27ms
step:1523/1670 train_time:142053ms step_avg:93.27ms
step:1524/1670 train_time:142146ms step_avg:93.27ms
step:1525/1670 train_time:142241ms step_avg:93.27ms
step:1526/1670 train_time:142333ms step_avg:93.27ms
step:1527/1670 train_time:142425ms step_avg:93.27ms
step:1528/1670 train_time:142518ms step_avg:93.27ms
step:1529/1670 train_time:142611ms step_avg:93.27ms
step:1530/1670 train_time:142704ms step_avg:93.27ms
step:1531/1670 train_time:142798ms step_avg:93.27ms
step:1532/1670 train_time:142891ms step_avg:93.27ms
step:1533/1670 train_time:142985ms step_avg:93.27ms
step:1534/1670 train_time:143081ms step_avg:93.27ms
step:1535/1670 train_time:143175ms step_avg:93.27ms
step:1536/1670 train_time:143269ms step_avg:93.27ms
step:1537/1670 train_time:143362ms step_avg:93.27ms
step:1538/1670 train_time:143456ms step_avg:93.27ms
step:1539/1670 train_time:143549ms step_avg:93.27ms
step:1540/1670 train_time:143642ms step_avg:93.27ms
step:1541/1670 train_time:143736ms step_avg:93.27ms
step:1542/1670 train_time:143830ms step_avg:93.27ms
step:1543/1670 train_time:143923ms step_avg:93.27ms
step:1544/1670 train_time:144015ms step_avg:93.27ms
step:1545/1670 train_time:144109ms step_avg:93.27ms
step:1546/1670 train_time:144202ms step_avg:93.27ms
step:1547/1670 train_time:144297ms step_avg:93.28ms
step:1548/1670 train_time:144389ms step_avg:93.27ms
step:1549/1670 train_time:144483ms step_avg:93.28ms
step:1550/1670 train_time:144577ms step_avg:93.28ms
step:1551/1670 train_time:144669ms step_avg:93.27ms
step:1552/1670 train_time:144764ms step_avg:93.28ms
step:1553/1670 train_time:144857ms step_avg:93.28ms
step:1554/1670 train_time:144949ms step_avg:93.28ms
step:1555/1670 train_time:145044ms step_avg:93.28ms
step:1556/1670 train_time:145136ms step_avg:93.28ms
step:1557/1670 train_time:145229ms step_avg:93.27ms
step:1558/1670 train_time:145322ms step_avg:93.27ms
step:1559/1670 train_time:145415ms step_avg:93.27ms
step:1560/1670 train_time:145508ms step_avg:93.27ms
step:1561/1670 train_time:145602ms step_avg:93.27ms
step:1562/1670 train_time:145695ms step_avg:93.27ms
step:1563/1670 train_time:145788ms step_avg:93.27ms
step:1564/1670 train_time:145881ms step_avg:93.27ms
step:1565/1670 train_time:145975ms step_avg:93.27ms
step:1566/1670 train_time:146069ms step_avg:93.28ms
step:1567/1670 train_time:146162ms step_avg:93.28ms
step:1568/1670 train_time:146256ms step_avg:93.28ms
step:1569/1670 train_time:146350ms step_avg:93.28ms
step:1570/1670 train_time:146444ms step_avg:93.28ms
step:1571/1670 train_time:146537ms step_avg:93.28ms
step:1572/1670 train_time:146629ms step_avg:93.28ms
step:1573/1670 train_time:146723ms step_avg:93.28ms
step:1574/1670 train_time:146816ms step_avg:93.28ms
step:1575/1670 train_time:146909ms step_avg:93.28ms
step:1576/1670 train_time:147003ms step_avg:93.28ms
step:1577/1670 train_time:147096ms step_avg:93.28ms
step:1578/1670 train_time:147189ms step_avg:93.28ms
step:1579/1670 train_time:147283ms step_avg:93.28ms
step:1580/1670 train_time:147376ms step_avg:93.28ms
step:1581/1670 train_time:147468ms step_avg:93.28ms
step:1582/1670 train_time:147562ms step_avg:93.28ms
step:1583/1670 train_time:147654ms step_avg:93.27ms
step:1584/1670 train_time:147747ms step_avg:93.27ms
step:1585/1670 train_time:147839ms step_avg:93.27ms
step:1586/1670 train_time:147932ms step_avg:93.27ms
step:1587/1670 train_time:148025ms step_avg:93.27ms
step:1588/1670 train_time:148118ms step_avg:93.27ms
step:1589/1670 train_time:148212ms step_avg:93.27ms
step:1590/1670 train_time:148305ms step_avg:93.27ms
step:1591/1670 train_time:148398ms step_avg:93.27ms
step:1592/1670 train_time:148491ms step_avg:93.27ms
step:1593/1670 train_time:148584ms step_avg:93.27ms
step:1594/1670 train_time:148678ms step_avg:93.27ms
step:1595/1670 train_time:148772ms step_avg:93.27ms
step:1596/1670 train_time:148866ms step_avg:93.27ms
step:1597/1670 train_time:148959ms step_avg:93.27ms
step:1598/1670 train_time:149051ms step_avg:93.27ms
step:1599/1670 train_time:149145ms step_avg:93.27ms
step:1600/1670 train_time:149239ms step_avg:93.27ms
step:1601/1670 train_time:149333ms step_avg:93.27ms
step:1602/1670 train_time:149425ms step_avg:93.27ms
step:1603/1670 train_time:149518ms step_avg:93.27ms
step:1604/1670 train_time:149611ms step_avg:93.27ms
step:1605/1670 train_time:149704ms step_avg:93.27ms
step:1606/1670 train_time:149798ms step_avg:93.27ms
step:1607/1670 train_time:149890ms step_avg:93.27ms
step:1608/1670 train_time:149984ms step_avg:93.27ms
step:1609/1670 train_time:150078ms step_avg:93.27ms
step:1610/1670 train_time:150172ms step_avg:93.27ms
step:1611/1670 train_time:150265ms step_avg:93.27ms
step:1612/1670 train_time:150359ms step_avg:93.27ms
step:1613/1670 train_time:150451ms step_avg:93.27ms
step:1614/1670 train_time:150545ms step_avg:93.27ms
step:1615/1670 train_time:150640ms step_avg:93.28ms
step:1616/1670 train_time:150732ms step_avg:93.27ms
step:1617/1670 train_time:150825ms step_avg:93.27ms
step:1618/1670 train_time:150918ms step_avg:93.27ms
step:1619/1670 train_time:151011ms step_avg:93.27ms
step:1620/1670 train_time:151105ms step_avg:93.27ms
step:1621/1670 train_time:151199ms step_avg:93.27ms
step:1622/1670 train_time:151292ms step_avg:93.27ms
step:1623/1670 train_time:151385ms step_avg:93.27ms
step:1624/1670 train_time:151479ms step_avg:93.28ms
step:1625/1670 train_time:151572ms step_avg:93.28ms
step:1625/1670 val_loss:3.2864 train_time:151665ms step_avg:93.33ms
step:1626/1670 train_time:151683ms step_avg:93.29ms
step:1627/1670 train_time:151759ms step_avg:93.28ms
step:1628/1670 train_time:151853ms step_avg:93.28ms
step:1629/1670 train_time:151946ms step_avg:93.28ms
step:1630/1670 train_time:152038ms step_avg:93.27ms
step:1631/1670 train_time:152131ms step_avg:93.27ms
step:1632/1670 train_time:152224ms step_avg:93.27ms
step:1633/1670 train_time:152317ms step_avg:93.27ms
step:1634/1670 train_time:152411ms step_avg:93.27ms
step:1635/1670 train_time:152505ms step_avg:93.28ms
step:1636/1670 train_time:152599ms step_avg:93.28ms
step:1637/1670 train_time:152695ms step_avg:93.28ms
step:1638/1670 train_time:152788ms step_avg:93.28ms
step:1639/1670 train_time:152881ms step_avg:93.28ms
step:1640/1670 train_time:152973ms step_avg:93.28ms
step:1641/1670 train_time:153066ms step_avg:93.28ms
step:1642/1670 train_time:153160ms step_avg:93.28ms
step:1643/1670 train_time:153253ms step_avg:93.28ms
step:1644/1670 train_time:153347ms step_avg:93.28ms
step:1645/1670 train_time:153440ms step_avg:93.28ms
step:1646/1670 train_time:153534ms step_avg:93.28ms
step:1647/1670 train_time:153629ms step_avg:93.28ms
step:1648/1670 train_time:153722ms step_avg:93.28ms
step:1649/1670 train_time:153815ms step_avg:93.28ms
step:1650/1670 train_time:153908ms step_avg:93.28ms
step:1651/1670 train_time:154001ms step_avg:93.28ms
step:1652/1670 train_time:154094ms step_avg:93.28ms
step:1653/1670 train_time:154187ms step_avg:93.28ms
step:1654/1670 train_time:154279ms step_avg:93.28ms
step:1655/1670 train_time:154372ms step_avg:93.28ms
step:1656/1670 train_time:154467ms step_avg:93.28ms
step:1657/1670 train_time:154560ms step_avg:93.28ms
step:1658/1670 train_time:154653ms step_avg:93.28ms
step:1659/1670 train_time:154747ms step_avg:93.28ms
step:1660/1670 train_time:154841ms step_avg:93.28ms
step:1661/1670 train_time:154934ms step_avg:93.28ms
step:1662/1670 train_time:155028ms step_avg:93.28ms
step:1663/1670 train_time:155121ms step_avg:93.28ms
step:1664/1670 train_time:155214ms step_avg:93.28ms
step:1665/1670 train_time:155306ms step_avg:93.28ms
step:1666/1670 train_time:155400ms step_avg:93.28ms
step:1667/1670 train_time:155493ms step_avg:93.28ms
step:1668/1670 train_time:155587ms step_avg:93.28ms
step:1669/1670 train_time:155680ms step_avg:93.28ms
step:1670/1670 train_time:155773ms step_avg:93.28ms
step:1670/1670 val_loss:3.2778 train_time:156043ms step_avg:93.44ms
peak memory allocated: 32002 MiB reserved: 46354 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
coeffs_list = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in coeffs_list:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            # x_out = self.blocks[i](x, x0, lambdas[i], attn_args)
            # x_backout += backout_lambdas[i] * (x_out-x)
            # x = x_out
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i==8:
                x_backout=x

        # backout contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda*x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2290  # number of iterations to run
    iteration_extension = 40  # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.45  # fraction of training spent cooling down the learning rate
    momentum_cd_steps = 50  # number of iterations for muon momentum cooldown
    # evaluation and logging
    run_id: str = f"new/{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

def update_optimizer_params(step, optimizer1, optimizer2):
    # Update lr
    for group in optimizer1.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)

    # Warmup phase: gradually increase momentum from 0.85 to 0.95
    if step < 300:
        frac = step / 300
        momentum = 0.85 + frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum

    # Cooldown phase: gradually decrease momentum
    momentum_cd_start = args.num_iterations + args.iteration_extension - args.momentum_cd_steps
    if step > momentum_cd_start:
        frac = (step - momentum_cd_start) / args.momentum_cd_steps

        # Decay momentum from 0.95 to 0.85
        momentum = 0.95 - frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    update_optimizer_params(step, optimizer1, optimizer2)
    # only step Adam every other step
    if step%2==0:
        optimizer2.step()
        optimizer2.zero_grad(set_to_none=True)
    else:
        for opt in optimizers:
            opt.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
    
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Sat Oct  4 05:56:52 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   28C    P0            116W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   26C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   26C    P0            112W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   26C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   27C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   27C    P0            112W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   37C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   25C    P0            112W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          265209      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          265210      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          265211      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          265212      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          265213      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          265214      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          265215      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          265216      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          265210      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          265211      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          265212      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          265213      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          265214      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          265215      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          265216      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:93ms step_avg:93.35ms
step:2/2330 train_time:187ms step_avg:93.46ms
step:3/2330 train_time:208ms step_avg:69.40ms
step:4/2330 train_time:242ms step_avg:60.62ms
step:5/2330 train_time:299ms step_avg:59.85ms
step:6/2330 train_time:359ms step_avg:59.90ms
step:7/2330 train_time:417ms step_avg:59.60ms
step:8/2330 train_time:477ms step_avg:59.67ms
step:9/2330 train_time:536ms step_avg:59.51ms
step:10/2330 train_time:596ms step_avg:59.58ms
step:11/2330 train_time:654ms step_avg:59.46ms
step:12/2330 train_time:714ms step_avg:59.52ms
step:13/2330 train_time:772ms step_avg:59.38ms
step:14/2330 train_time:832ms step_avg:59.42ms
step:15/2330 train_time:890ms step_avg:59.34ms
step:16/2330 train_time:951ms step_avg:59.42ms
step:17/2330 train_time:1010ms step_avg:59.41ms
step:18/2330 train_time:1072ms step_avg:59.57ms
step:19/2330 train_time:1134ms step_avg:59.68ms
step:20/2330 train_time:1197ms step_avg:59.85ms
step:21/2330 train_time:1257ms step_avg:59.84ms
step:22/2330 train_time:1317ms step_avg:59.89ms
step:23/2330 train_time:1376ms step_avg:59.83ms
step:24/2330 train_time:1437ms step_avg:59.86ms
step:25/2330 train_time:1495ms step_avg:59.80ms
step:26/2330 train_time:1556ms step_avg:59.84ms
step:27/2330 train_time:1614ms step_avg:59.77ms
step:28/2330 train_time:1674ms step_avg:59.78ms
step:29/2330 train_time:1733ms step_avg:59.74ms
step:30/2330 train_time:1793ms step_avg:59.76ms
step:31/2330 train_time:1851ms step_avg:59.71ms
step:32/2330 train_time:1912ms step_avg:59.74ms
step:33/2330 train_time:1971ms step_avg:59.72ms
step:34/2330 train_time:2031ms step_avg:59.75ms
step:35/2330 train_time:2092ms step_avg:59.77ms
step:36/2330 train_time:2155ms step_avg:59.85ms
step:37/2330 train_time:2214ms step_avg:59.83ms
step:38/2330 train_time:2275ms step_avg:59.86ms
step:39/2330 train_time:2334ms step_avg:59.85ms
step:40/2330 train_time:2395ms step_avg:59.88ms
step:41/2330 train_time:2455ms step_avg:59.87ms
step:42/2330 train_time:2516ms step_avg:59.90ms
step:43/2330 train_time:2574ms step_avg:59.86ms
step:44/2330 train_time:2634ms step_avg:59.87ms
step:45/2330 train_time:2693ms step_avg:59.84ms
step:46/2330 train_time:2753ms step_avg:59.85ms
step:47/2330 train_time:2812ms step_avg:59.82ms
step:48/2330 train_time:2872ms step_avg:59.83ms
step:49/2330 train_time:2930ms step_avg:59.80ms
step:50/2330 train_time:2991ms step_avg:59.82ms
step:51/2330 train_time:3050ms step_avg:59.80ms
step:52/2330 train_time:3111ms step_avg:59.83ms
step:53/2330 train_time:3171ms step_avg:59.83ms
step:54/2330 train_time:3232ms step_avg:59.85ms
step:55/2330 train_time:3291ms step_avg:59.84ms
step:56/2330 train_time:3353ms step_avg:59.87ms
step:57/2330 train_time:3412ms step_avg:59.86ms
step:58/2330 train_time:3473ms step_avg:59.88ms
step:59/2330 train_time:3532ms step_avg:59.86ms
step:60/2330 train_time:3593ms step_avg:59.88ms
step:61/2330 train_time:3651ms step_avg:59.86ms
step:62/2330 train_time:3712ms step_avg:59.87ms
step:63/2330 train_time:3770ms step_avg:59.84ms
step:64/2330 train_time:3830ms step_avg:59.84ms
step:65/2330 train_time:3888ms step_avg:59.82ms
step:66/2330 train_time:3949ms step_avg:59.83ms
step:67/2330 train_time:4007ms step_avg:59.81ms
step:68/2330 train_time:4068ms step_avg:59.82ms
step:69/2330 train_time:4127ms step_avg:59.81ms
step:70/2330 train_time:4188ms step_avg:59.84ms
step:71/2330 train_time:4247ms step_avg:59.82ms
step:72/2330 train_time:4309ms step_avg:59.84ms
step:73/2330 train_time:4368ms step_avg:59.83ms
step:74/2330 train_time:4428ms step_avg:59.84ms
step:75/2330 train_time:4488ms step_avg:59.84ms
step:76/2330 train_time:4550ms step_avg:59.86ms
step:77/2330 train_time:4609ms step_avg:59.86ms
step:78/2330 train_time:4670ms step_avg:59.87ms
step:79/2330 train_time:4728ms step_avg:59.85ms
step:80/2330 train_time:4789ms step_avg:59.86ms
step:81/2330 train_time:4847ms step_avg:59.84ms
step:82/2330 train_time:4908ms step_avg:59.85ms
step:83/2330 train_time:4966ms step_avg:59.84ms
step:84/2330 train_time:5027ms step_avg:59.84ms
step:85/2330 train_time:5086ms step_avg:59.83ms
step:86/2330 train_time:5147ms step_avg:59.85ms
step:87/2330 train_time:5206ms step_avg:59.84ms
step:88/2330 train_time:5267ms step_avg:59.85ms
step:89/2330 train_time:5327ms step_avg:59.85ms
step:90/2330 train_time:5388ms step_avg:59.87ms
step:91/2330 train_time:5447ms step_avg:59.85ms
step:92/2330 train_time:5507ms step_avg:59.86ms
step:93/2330 train_time:5567ms step_avg:59.86ms
step:94/2330 train_time:5628ms step_avg:59.87ms
step:95/2330 train_time:5687ms step_avg:59.86ms
step:96/2330 train_time:5747ms step_avg:59.86ms
step:97/2330 train_time:5805ms step_avg:59.85ms
step:98/2330 train_time:5866ms step_avg:59.85ms
step:99/2330 train_time:5924ms step_avg:59.84ms
step:100/2330 train_time:5985ms step_avg:59.85ms
step:101/2330 train_time:6043ms step_avg:59.83ms
step:102/2330 train_time:6103ms step_avg:59.84ms
step:103/2330 train_time:6161ms step_avg:59.82ms
step:104/2330 train_time:6223ms step_avg:59.83ms
step:105/2330 train_time:6282ms step_avg:59.83ms
step:106/2330 train_time:6344ms step_avg:59.85ms
step:107/2330 train_time:6403ms step_avg:59.84ms
step:108/2330 train_time:6464ms step_avg:59.85ms
step:109/2330 train_time:6523ms step_avg:59.84ms
step:110/2330 train_time:6584ms step_avg:59.86ms
step:111/2330 train_time:6643ms step_avg:59.85ms
step:112/2330 train_time:6704ms step_avg:59.86ms
step:113/2330 train_time:6763ms step_avg:59.85ms
step:114/2330 train_time:6823ms step_avg:59.85ms
step:115/2330 train_time:6882ms step_avg:59.84ms
step:116/2330 train_time:6942ms step_avg:59.85ms
step:117/2330 train_time:7001ms step_avg:59.84ms
step:118/2330 train_time:7061ms step_avg:59.84ms
step:119/2330 train_time:7120ms step_avg:59.83ms
step:120/2330 train_time:7180ms step_avg:59.84ms
step:121/2330 train_time:7239ms step_avg:59.83ms
step:122/2330 train_time:7300ms step_avg:59.84ms
step:123/2330 train_time:7359ms step_avg:59.83ms
step:124/2330 train_time:7420ms step_avg:59.84ms
step:125/2330 train_time:7479ms step_avg:59.83ms
step:126/2330 train_time:7539ms step_avg:59.84ms
step:127/2330 train_time:7599ms step_avg:59.83ms
step:128/2330 train_time:7659ms step_avg:59.84ms
step:129/2330 train_time:7719ms step_avg:59.84ms
step:130/2330 train_time:7779ms step_avg:59.84ms
step:131/2330 train_time:7838ms step_avg:59.83ms
step:132/2330 train_time:7899ms step_avg:59.84ms
step:133/2330 train_time:7958ms step_avg:59.83ms
step:134/2330 train_time:8019ms step_avg:59.84ms
step:135/2330 train_time:8077ms step_avg:59.83ms
step:136/2330 train_time:8138ms step_avg:59.84ms
step:137/2330 train_time:8197ms step_avg:59.83ms
step:138/2330 train_time:8257ms step_avg:59.83ms
step:139/2330 train_time:8316ms step_avg:59.82ms
step:140/2330 train_time:8376ms step_avg:59.83ms
step:141/2330 train_time:8435ms step_avg:59.82ms
step:142/2330 train_time:8496ms step_avg:59.83ms
step:143/2330 train_time:8555ms step_avg:59.83ms
step:144/2330 train_time:8616ms step_avg:59.84ms
step:145/2330 train_time:8674ms step_avg:59.82ms
step:146/2330 train_time:8735ms step_avg:59.83ms
step:147/2330 train_time:8794ms step_avg:59.82ms
step:148/2330 train_time:8855ms step_avg:59.83ms
step:149/2330 train_time:8913ms step_avg:59.82ms
step:150/2330 train_time:8974ms step_avg:59.83ms
step:151/2330 train_time:9032ms step_avg:59.81ms
step:152/2330 train_time:9092ms step_avg:59.82ms
step:153/2330 train_time:9151ms step_avg:59.81ms
step:154/2330 train_time:9212ms step_avg:59.82ms
step:155/2330 train_time:9270ms step_avg:59.81ms
step:156/2330 train_time:9331ms step_avg:59.82ms
step:157/2330 train_time:9389ms step_avg:59.81ms
step:158/2330 train_time:9450ms step_avg:59.81ms
step:159/2330 train_time:9509ms step_avg:59.81ms
step:160/2330 train_time:9570ms step_avg:59.81ms
step:161/2330 train_time:9628ms step_avg:59.80ms
step:162/2330 train_time:9689ms step_avg:59.81ms
step:163/2330 train_time:9747ms step_avg:59.80ms
step:164/2330 train_time:9808ms step_avg:59.81ms
step:165/2330 train_time:9866ms step_avg:59.80ms
step:166/2330 train_time:9927ms step_avg:59.80ms
step:167/2330 train_time:9985ms step_avg:59.79ms
step:168/2330 train_time:10046ms step_avg:59.80ms
step:169/2330 train_time:10105ms step_avg:59.79ms
step:170/2330 train_time:10165ms step_avg:59.80ms
step:171/2330 train_time:10224ms step_avg:59.79ms
step:172/2330 train_time:10285ms step_avg:59.80ms
step:173/2330 train_time:10345ms step_avg:59.80ms
step:174/2330 train_time:10404ms step_avg:59.79ms
step:175/2330 train_time:10463ms step_avg:59.79ms
step:176/2330 train_time:10524ms step_avg:59.79ms
step:177/2330 train_time:10582ms step_avg:59.79ms
step:178/2330 train_time:10643ms step_avg:59.79ms
step:179/2330 train_time:10702ms step_avg:59.79ms
step:180/2330 train_time:10763ms step_avg:59.79ms
step:181/2330 train_time:10821ms step_avg:59.79ms
step:182/2330 train_time:10882ms step_avg:59.79ms
step:183/2330 train_time:10941ms step_avg:59.79ms
step:184/2330 train_time:11002ms step_avg:59.79ms
step:185/2330 train_time:11060ms step_avg:59.78ms
step:186/2330 train_time:11121ms step_avg:59.79ms
step:187/2330 train_time:11180ms step_avg:59.78ms
step:188/2330 train_time:11240ms step_avg:59.79ms
step:189/2330 train_time:11299ms step_avg:59.78ms
step:190/2330 train_time:11359ms step_avg:59.79ms
step:191/2330 train_time:11418ms step_avg:59.78ms
step:192/2330 train_time:11478ms step_avg:59.78ms
step:193/2330 train_time:11537ms step_avg:59.78ms
step:194/2330 train_time:11598ms step_avg:59.78ms
step:195/2330 train_time:11657ms step_avg:59.78ms
step:196/2330 train_time:11718ms step_avg:59.79ms
step:197/2330 train_time:11776ms step_avg:59.78ms
step:198/2330 train_time:11837ms step_avg:59.78ms
step:199/2330 train_time:11896ms step_avg:59.78ms
step:200/2330 train_time:11956ms step_avg:59.78ms
step:201/2330 train_time:12015ms step_avg:59.78ms
step:202/2330 train_time:12075ms step_avg:59.78ms
step:203/2330 train_time:12133ms step_avg:59.77ms
step:204/2330 train_time:12193ms step_avg:59.77ms
step:205/2330 train_time:12251ms step_avg:59.76ms
step:206/2330 train_time:12312ms step_avg:59.77ms
step:207/2330 train_time:12370ms step_avg:59.76ms
step:208/2330 train_time:12431ms step_avg:59.76ms
step:209/2330 train_time:12490ms step_avg:59.76ms
step:210/2330 train_time:12551ms step_avg:59.77ms
step:211/2330 train_time:12610ms step_avg:59.76ms
step:212/2330 train_time:12670ms step_avg:59.77ms
step:213/2330 train_time:12729ms step_avg:59.76ms
step:214/2330 train_time:12789ms step_avg:59.76ms
step:215/2330 train_time:12848ms step_avg:59.76ms
step:216/2330 train_time:12909ms step_avg:59.76ms
step:217/2330 train_time:12967ms step_avg:59.76ms
step:218/2330 train_time:13028ms step_avg:59.76ms
step:219/2330 train_time:13087ms step_avg:59.76ms
step:220/2330 train_time:13148ms step_avg:59.76ms
step:221/2330 train_time:13206ms step_avg:59.76ms
step:222/2330 train_time:13267ms step_avg:59.76ms
step:223/2330 train_time:13325ms step_avg:59.75ms
step:224/2330 train_time:13387ms step_avg:59.76ms
step:225/2330 train_time:13445ms step_avg:59.75ms
step:226/2330 train_time:13505ms step_avg:59.76ms
step:227/2330 train_time:13564ms step_avg:59.75ms
step:228/2330 train_time:13624ms step_avg:59.76ms
step:229/2330 train_time:13683ms step_avg:59.75ms
step:230/2330 train_time:13744ms step_avg:59.75ms
step:231/2330 train_time:13802ms step_avg:59.75ms
step:232/2330 train_time:13863ms step_avg:59.75ms
step:233/2330 train_time:13922ms step_avg:59.75ms
step:234/2330 train_time:13984ms step_avg:59.76ms
step:235/2330 train_time:14042ms step_avg:59.75ms
step:236/2330 train_time:14103ms step_avg:59.76ms
step:237/2330 train_time:14161ms step_avg:59.75ms
step:238/2330 train_time:14222ms step_avg:59.76ms
step:239/2330 train_time:14281ms step_avg:59.75ms
step:240/2330 train_time:14342ms step_avg:59.76ms
step:241/2330 train_time:14400ms step_avg:59.75ms
step:242/2330 train_time:14461ms step_avg:59.76ms
step:243/2330 train_time:14519ms step_avg:59.75ms
step:244/2330 train_time:14579ms step_avg:59.75ms
step:245/2330 train_time:14638ms step_avg:59.75ms
step:246/2330 train_time:14699ms step_avg:59.75ms
step:247/2330 train_time:14756ms step_avg:59.74ms
step:248/2330 train_time:14817ms step_avg:59.75ms
step:249/2330 train_time:14875ms step_avg:59.74ms
step:250/2330 train_time:14936ms step_avg:59.74ms
step:250/2330 val_loss:4.0916 train_time:14999ms step_avg:60.00ms
step:251/2330 train_time:15020ms step_avg:59.84ms
step:252/2330 train_time:15058ms step_avg:59.75ms
step:253/2330 train_time:15122ms step_avg:59.77ms
step:254/2330 train_time:15187ms step_avg:59.79ms
step:255/2330 train_time:15248ms step_avg:59.79ms
step:256/2330 train_time:15309ms step_avg:59.80ms
step:257/2330 train_time:15368ms step_avg:59.80ms
step:258/2330 train_time:15427ms step_avg:59.80ms
step:259/2330 train_time:15485ms step_avg:59.79ms
step:260/2330 train_time:15545ms step_avg:59.79ms
step:261/2330 train_time:15603ms step_avg:59.78ms
step:262/2330 train_time:15663ms step_avg:59.78ms
step:263/2330 train_time:15721ms step_avg:59.77ms
step:264/2330 train_time:15781ms step_avg:59.78ms
step:265/2330 train_time:15838ms step_avg:59.77ms
step:266/2330 train_time:15898ms step_avg:59.77ms
step:267/2330 train_time:15956ms step_avg:59.76ms
step:268/2330 train_time:16017ms step_avg:59.76ms
step:269/2330 train_time:16077ms step_avg:59.76ms
step:270/2330 train_time:16139ms step_avg:59.77ms
step:271/2330 train_time:16198ms step_avg:59.77ms
step:272/2330 train_time:16260ms step_avg:59.78ms
step:273/2330 train_time:16318ms step_avg:59.77ms
step:274/2330 train_time:16379ms step_avg:59.78ms
step:275/2330 train_time:16437ms step_avg:59.77ms
step:276/2330 train_time:16498ms step_avg:59.78ms
step:277/2330 train_time:16557ms step_avg:59.77ms
step:278/2330 train_time:16617ms step_avg:59.77ms
step:279/2330 train_time:16675ms step_avg:59.77ms
step:280/2330 train_time:16735ms step_avg:59.77ms
step:281/2330 train_time:16793ms step_avg:59.76ms
step:282/2330 train_time:16852ms step_avg:59.76ms
step:283/2330 train_time:16910ms step_avg:59.75ms
step:284/2330 train_time:16971ms step_avg:59.76ms
step:285/2330 train_time:17028ms step_avg:59.75ms
step:286/2330 train_time:17089ms step_avg:59.75ms
step:287/2330 train_time:17148ms step_avg:59.75ms
step:288/2330 train_time:17209ms step_avg:59.75ms
step:289/2330 train_time:17269ms step_avg:59.75ms
step:290/2330 train_time:17330ms step_avg:59.76ms
step:291/2330 train_time:17388ms step_avg:59.75ms
step:292/2330 train_time:17449ms step_avg:59.76ms
step:293/2330 train_time:17508ms step_avg:59.75ms
step:294/2330 train_time:17568ms step_avg:59.76ms
step:295/2330 train_time:17627ms step_avg:59.75ms
step:296/2330 train_time:17687ms step_avg:59.75ms
step:297/2330 train_time:17745ms step_avg:59.75ms
step:298/2330 train_time:17807ms step_avg:59.75ms
step:299/2330 train_time:17865ms step_avg:59.75ms
step:300/2330 train_time:17925ms step_avg:59.75ms
step:301/2330 train_time:17984ms step_avg:59.75ms
step:302/2330 train_time:18045ms step_avg:59.75ms
step:303/2330 train_time:18104ms step_avg:59.75ms
step:304/2330 train_time:18164ms step_avg:59.75ms
step:305/2330 train_time:18223ms step_avg:59.75ms
step:306/2330 train_time:18284ms step_avg:59.75ms
step:307/2330 train_time:18344ms step_avg:59.75ms
step:308/2330 train_time:18405ms step_avg:59.76ms
step:309/2330 train_time:18463ms step_avg:59.75ms
step:310/2330 train_time:18524ms step_avg:59.76ms
step:311/2330 train_time:18582ms step_avg:59.75ms
step:312/2330 train_time:18644ms step_avg:59.76ms
step:313/2330 train_time:18703ms step_avg:59.75ms
step:314/2330 train_time:18764ms step_avg:59.76ms
step:315/2330 train_time:18822ms step_avg:59.75ms
step:316/2330 train_time:18883ms step_avg:59.76ms
step:317/2330 train_time:18941ms step_avg:59.75ms
step:318/2330 train_time:19002ms step_avg:59.76ms
step:319/2330 train_time:19060ms step_avg:59.75ms
step:320/2330 train_time:19120ms step_avg:59.75ms
step:321/2330 train_time:19179ms step_avg:59.75ms
step:322/2330 train_time:19240ms step_avg:59.75ms
step:323/2330 train_time:19298ms step_avg:59.75ms
step:324/2330 train_time:19359ms step_avg:59.75ms
step:325/2330 train_time:19417ms step_avg:59.74ms
step:326/2330 train_time:19477ms step_avg:59.75ms
step:327/2330 train_time:19535ms step_avg:59.74ms
step:328/2330 train_time:19597ms step_avg:59.75ms
step:329/2330 train_time:19655ms step_avg:59.74ms
step:330/2330 train_time:19715ms step_avg:59.74ms
step:331/2330 train_time:19773ms step_avg:59.74ms
step:332/2330 train_time:19834ms step_avg:59.74ms
step:333/2330 train_time:19893ms step_avg:59.74ms
step:334/2330 train_time:19953ms step_avg:59.74ms
step:335/2330 train_time:20012ms step_avg:59.74ms
step:336/2330 train_time:20072ms step_avg:59.74ms
step:337/2330 train_time:20131ms step_avg:59.74ms
step:338/2330 train_time:20191ms step_avg:59.74ms
step:339/2330 train_time:20249ms step_avg:59.73ms
step:340/2330 train_time:20309ms step_avg:59.73ms
step:341/2330 train_time:20368ms step_avg:59.73ms
step:342/2330 train_time:20428ms step_avg:59.73ms
step:343/2330 train_time:20487ms step_avg:59.73ms
step:344/2330 train_time:20547ms step_avg:59.73ms
step:345/2330 train_time:20606ms step_avg:59.73ms
step:346/2330 train_time:20667ms step_avg:59.73ms
step:347/2330 train_time:20726ms step_avg:59.73ms
step:348/2330 train_time:20786ms step_avg:59.73ms
step:349/2330 train_time:20845ms step_avg:59.73ms
step:350/2330 train_time:20906ms step_avg:59.73ms
step:351/2330 train_time:20965ms step_avg:59.73ms
step:352/2330 train_time:21025ms step_avg:59.73ms
step:353/2330 train_time:21084ms step_avg:59.73ms
step:354/2330 train_time:21145ms step_avg:59.73ms
step:355/2330 train_time:21203ms step_avg:59.73ms
step:356/2330 train_time:21264ms step_avg:59.73ms
step:357/2330 train_time:21322ms step_avg:59.72ms
step:358/2330 train_time:21382ms step_avg:59.73ms
step:359/2330 train_time:21441ms step_avg:59.72ms
step:360/2330 train_time:21501ms step_avg:59.72ms
step:361/2330 train_time:21559ms step_avg:59.72ms
step:362/2330 train_time:21619ms step_avg:59.72ms
step:363/2330 train_time:21678ms step_avg:59.72ms
step:364/2330 train_time:21739ms step_avg:59.72ms
step:365/2330 train_time:21798ms step_avg:59.72ms
step:366/2330 train_time:21859ms step_avg:59.72ms
step:367/2330 train_time:21917ms step_avg:59.72ms
step:368/2330 train_time:21978ms step_avg:59.72ms
step:369/2330 train_time:22036ms step_avg:59.72ms
step:370/2330 train_time:22097ms step_avg:59.72ms
step:371/2330 train_time:22156ms step_avg:59.72ms
step:372/2330 train_time:22216ms step_avg:59.72ms
step:373/2330 train_time:22274ms step_avg:59.72ms
step:374/2330 train_time:22335ms step_avg:59.72ms
step:375/2330 train_time:22393ms step_avg:59.71ms
step:376/2330 train_time:22453ms step_avg:59.72ms
step:377/2330 train_time:22512ms step_avg:59.71ms
step:378/2330 train_time:22572ms step_avg:59.71ms
step:379/2330 train_time:22630ms step_avg:59.71ms
step:380/2330 train_time:22690ms step_avg:59.71ms
step:381/2330 train_time:22750ms step_avg:59.71ms
step:382/2330 train_time:22810ms step_avg:59.71ms
step:383/2330 train_time:22869ms step_avg:59.71ms
step:384/2330 train_time:22930ms step_avg:59.71ms
step:385/2330 train_time:22988ms step_avg:59.71ms
step:386/2330 train_time:23049ms step_avg:59.71ms
step:387/2330 train_time:23107ms step_avg:59.71ms
step:388/2330 train_time:23168ms step_avg:59.71ms
step:389/2330 train_time:23227ms step_avg:59.71ms
step:390/2330 train_time:23288ms step_avg:59.71ms
step:391/2330 train_time:23346ms step_avg:59.71ms
step:392/2330 train_time:23407ms step_avg:59.71ms
step:393/2330 train_time:23465ms step_avg:59.71ms
step:394/2330 train_time:23526ms step_avg:59.71ms
step:395/2330 train_time:23585ms step_avg:59.71ms
step:396/2330 train_time:23646ms step_avg:59.71ms
step:397/2330 train_time:23704ms step_avg:59.71ms
step:398/2330 train_time:23765ms step_avg:59.71ms
step:399/2330 train_time:23823ms step_avg:59.71ms
step:400/2330 train_time:23884ms step_avg:59.71ms
step:401/2330 train_time:23943ms step_avg:59.71ms
step:402/2330 train_time:24004ms step_avg:59.71ms
step:403/2330 train_time:24062ms step_avg:59.71ms
step:404/2330 train_time:24122ms step_avg:59.71ms
step:405/2330 train_time:24181ms step_avg:59.71ms
step:406/2330 train_time:24242ms step_avg:59.71ms
step:407/2330 train_time:24301ms step_avg:59.71ms
step:408/2330 train_time:24362ms step_avg:59.71ms
step:409/2330 train_time:24420ms step_avg:59.71ms
step:410/2330 train_time:24481ms step_avg:59.71ms
step:411/2330 train_time:24540ms step_avg:59.71ms
step:412/2330 train_time:24601ms step_avg:59.71ms
step:413/2330 train_time:24659ms step_avg:59.71ms
step:414/2330 train_time:24719ms step_avg:59.71ms
step:415/2330 train_time:24778ms step_avg:59.70ms
step:416/2330 train_time:24838ms step_avg:59.71ms
step:417/2330 train_time:24896ms step_avg:59.70ms
step:418/2330 train_time:24957ms step_avg:59.71ms
step:419/2330 train_time:25015ms step_avg:59.70ms
step:420/2330 train_time:25075ms step_avg:59.70ms
step:421/2330 train_time:25134ms step_avg:59.70ms
step:422/2330 train_time:25195ms step_avg:59.70ms
step:423/2330 train_time:25254ms step_avg:59.70ms
step:424/2330 train_time:25315ms step_avg:59.71ms
step:425/2330 train_time:25374ms step_avg:59.70ms
step:426/2330 train_time:25435ms step_avg:59.71ms
step:427/2330 train_time:25493ms step_avg:59.70ms
step:428/2330 train_time:25554ms step_avg:59.70ms
step:429/2330 train_time:25612ms step_avg:59.70ms
step:430/2330 train_time:25672ms step_avg:59.70ms
step:431/2330 train_time:25731ms step_avg:59.70ms
step:432/2330 train_time:25792ms step_avg:59.70ms
step:433/2330 train_time:25850ms step_avg:59.70ms
step:434/2330 train_time:25910ms step_avg:59.70ms
step:435/2330 train_time:25969ms step_avg:59.70ms
step:436/2330 train_time:26029ms step_avg:59.70ms
step:437/2330 train_time:26088ms step_avg:59.70ms
step:438/2330 train_time:26149ms step_avg:59.70ms
step:439/2330 train_time:26207ms step_avg:59.70ms
step:440/2330 train_time:26268ms step_avg:59.70ms
step:441/2330 train_time:26326ms step_avg:59.70ms
step:442/2330 train_time:26387ms step_avg:59.70ms
step:443/2330 train_time:26446ms step_avg:59.70ms
step:444/2330 train_time:26506ms step_avg:59.70ms
step:445/2330 train_time:26565ms step_avg:59.70ms
step:446/2330 train_time:26625ms step_avg:59.70ms
step:447/2330 train_time:26684ms step_avg:59.70ms
step:448/2330 train_time:26745ms step_avg:59.70ms
step:449/2330 train_time:26804ms step_avg:59.70ms
step:450/2330 train_time:26864ms step_avg:59.70ms
step:451/2330 train_time:26923ms step_avg:59.70ms
step:452/2330 train_time:26984ms step_avg:59.70ms
step:453/2330 train_time:27043ms step_avg:59.70ms
step:454/2330 train_time:27104ms step_avg:59.70ms
step:455/2330 train_time:27162ms step_avg:59.70ms
step:456/2330 train_time:27223ms step_avg:59.70ms
step:457/2330 train_time:27282ms step_avg:59.70ms
step:458/2330 train_time:27342ms step_avg:59.70ms
step:459/2330 train_time:27401ms step_avg:59.70ms
step:460/2330 train_time:27462ms step_avg:59.70ms
step:461/2330 train_time:27522ms step_avg:59.70ms
step:462/2330 train_time:27582ms step_avg:59.70ms
step:463/2330 train_time:27641ms step_avg:59.70ms
step:464/2330 train_time:27701ms step_avg:59.70ms
step:465/2330 train_time:27760ms step_avg:59.70ms
step:466/2330 train_time:27820ms step_avg:59.70ms
step:467/2330 train_time:27879ms step_avg:59.70ms
step:468/2330 train_time:27940ms step_avg:59.70ms
step:469/2330 train_time:27998ms step_avg:59.70ms
step:470/2330 train_time:28059ms step_avg:59.70ms
step:471/2330 train_time:28117ms step_avg:59.70ms
step:472/2330 train_time:28177ms step_avg:59.70ms
step:473/2330 train_time:28235ms step_avg:59.69ms
step:474/2330 train_time:28296ms step_avg:59.70ms
step:475/2330 train_time:28355ms step_avg:59.69ms
step:476/2330 train_time:28417ms step_avg:59.70ms
step:477/2330 train_time:28475ms step_avg:59.70ms
step:478/2330 train_time:28535ms step_avg:59.70ms
step:479/2330 train_time:28594ms step_avg:59.70ms
step:480/2330 train_time:28655ms step_avg:59.70ms
step:481/2330 train_time:28713ms step_avg:59.69ms
step:482/2330 train_time:28773ms step_avg:59.69ms
step:483/2330 train_time:28832ms step_avg:59.69ms
step:484/2330 train_time:28892ms step_avg:59.70ms
step:485/2330 train_time:28950ms step_avg:59.69ms
step:486/2330 train_time:29011ms step_avg:59.69ms
step:487/2330 train_time:29069ms step_avg:59.69ms
step:488/2330 train_time:29129ms step_avg:59.69ms
step:489/2330 train_time:29188ms step_avg:59.69ms
step:490/2330 train_time:29248ms step_avg:59.69ms
step:491/2330 train_time:29306ms step_avg:59.69ms
step:492/2330 train_time:29368ms step_avg:59.69ms
step:493/2330 train_time:29427ms step_avg:59.69ms
step:494/2330 train_time:29487ms step_avg:59.69ms
step:495/2330 train_time:29546ms step_avg:59.69ms
step:496/2330 train_time:29607ms step_avg:59.69ms
step:497/2330 train_time:29665ms step_avg:59.69ms
step:498/2330 train_time:29725ms step_avg:59.69ms
step:499/2330 train_time:29784ms step_avg:59.69ms
step:500/2330 train_time:29845ms step_avg:59.69ms
step:500/2330 val_loss:3.8239 train_time:29907ms step_avg:59.81ms
step:501/2330 train_time:29928ms step_avg:59.74ms
step:502/2330 train_time:29966ms step_avg:59.69ms
step:503/2330 train_time:30027ms step_avg:59.70ms
step:504/2330 train_time:30092ms step_avg:59.71ms
step:505/2330 train_time:30152ms step_avg:59.71ms
step:506/2330 train_time:30212ms step_avg:59.71ms
step:507/2330 train_time:30270ms step_avg:59.71ms
step:508/2330 train_time:30330ms step_avg:59.71ms
step:509/2330 train_time:30388ms step_avg:59.70ms
step:510/2330 train_time:30449ms step_avg:59.70ms
step:511/2330 train_time:30507ms step_avg:59.70ms
step:512/2330 train_time:30568ms step_avg:59.70ms
step:513/2330 train_time:30625ms step_avg:59.70ms
step:514/2330 train_time:30686ms step_avg:59.70ms
step:515/2330 train_time:30744ms step_avg:59.70ms
step:516/2330 train_time:30804ms step_avg:59.70ms
step:517/2330 train_time:30862ms step_avg:59.69ms
step:518/2330 train_time:30923ms step_avg:59.70ms
step:519/2330 train_time:30983ms step_avg:59.70ms
step:520/2330 train_time:31044ms step_avg:59.70ms
step:521/2330 train_time:31104ms step_avg:59.70ms
step:522/2330 train_time:31165ms step_avg:59.70ms
step:523/2330 train_time:31223ms step_avg:59.70ms
step:524/2330 train_time:31284ms step_avg:59.70ms
step:525/2330 train_time:31342ms step_avg:59.70ms
step:526/2330 train_time:31403ms step_avg:59.70ms
step:527/2330 train_time:31461ms step_avg:59.70ms
step:528/2330 train_time:31521ms step_avg:59.70ms
step:529/2330 train_time:31580ms step_avg:59.70ms
step:530/2330 train_time:31640ms step_avg:59.70ms
step:531/2330 train_time:31698ms step_avg:59.70ms
step:532/2330 train_time:31759ms step_avg:59.70ms
step:533/2330 train_time:31818ms step_avg:59.70ms
step:534/2330 train_time:31878ms step_avg:59.70ms
step:535/2330 train_time:31936ms step_avg:59.69ms
step:536/2330 train_time:31997ms step_avg:59.70ms
step:537/2330 train_time:32056ms step_avg:59.69ms
step:538/2330 train_time:32116ms step_avg:59.70ms
step:539/2330 train_time:32176ms step_avg:59.69ms
step:540/2330 train_time:32236ms step_avg:59.70ms
step:541/2330 train_time:32295ms step_avg:59.70ms
step:542/2330 train_time:32356ms step_avg:59.70ms
step:543/2330 train_time:32414ms step_avg:59.70ms
step:544/2330 train_time:32475ms step_avg:59.70ms
step:545/2330 train_time:32533ms step_avg:59.69ms
step:546/2330 train_time:32594ms step_avg:59.70ms
step:547/2330 train_time:32652ms step_avg:59.69ms
step:548/2330 train_time:32712ms step_avg:59.69ms
step:549/2330 train_time:32771ms step_avg:59.69ms
step:550/2330 train_time:32831ms step_avg:59.69ms
step:551/2330 train_time:32890ms step_avg:59.69ms
step:552/2330 train_time:32950ms step_avg:59.69ms
step:553/2330 train_time:33009ms step_avg:59.69ms
step:554/2330 train_time:33070ms step_avg:59.69ms
step:555/2330 train_time:33129ms step_avg:59.69ms
step:556/2330 train_time:33191ms step_avg:59.70ms
step:557/2330 train_time:33250ms step_avg:59.69ms
step:558/2330 train_time:33311ms step_avg:59.70ms
step:559/2330 train_time:33369ms step_avg:59.69ms
step:560/2330 train_time:33430ms step_avg:59.70ms
step:561/2330 train_time:33489ms step_avg:59.70ms
step:562/2330 train_time:33550ms step_avg:59.70ms
step:563/2330 train_time:33608ms step_avg:59.70ms
step:564/2330 train_time:33669ms step_avg:59.70ms
step:565/2330 train_time:33728ms step_avg:59.69ms
step:566/2330 train_time:33789ms step_avg:59.70ms
step:567/2330 train_time:33847ms step_avg:59.70ms
step:568/2330 train_time:33908ms step_avg:59.70ms
step:569/2330 train_time:33967ms step_avg:59.70ms
step:570/2330 train_time:34027ms step_avg:59.70ms
step:571/2330 train_time:34086ms step_avg:59.70ms
step:572/2330 train_time:34147ms step_avg:59.70ms
step:573/2330 train_time:34206ms step_avg:59.70ms
step:574/2330 train_time:34267ms step_avg:59.70ms
step:575/2330 train_time:34326ms step_avg:59.70ms
step:576/2330 train_time:34387ms step_avg:59.70ms
step:577/2330 train_time:34445ms step_avg:59.70ms
step:578/2330 train_time:34506ms step_avg:59.70ms
step:579/2330 train_time:34564ms step_avg:59.70ms
step:580/2330 train_time:34624ms step_avg:59.70ms
step:581/2330 train_time:34682ms step_avg:59.69ms
step:582/2330 train_time:34743ms step_avg:59.70ms
step:583/2330 train_time:34801ms step_avg:59.69ms
step:584/2330 train_time:34861ms step_avg:59.69ms
step:585/2330 train_time:34920ms step_avg:59.69ms
step:586/2330 train_time:34981ms step_avg:59.69ms
step:587/2330 train_time:35039ms step_avg:59.69ms
step:588/2330 train_time:35100ms step_avg:59.69ms
step:589/2330 train_time:35158ms step_avg:59.69ms
step:590/2330 train_time:35219ms step_avg:59.69ms
step:591/2330 train_time:35277ms step_avg:59.69ms
step:592/2330 train_time:35338ms step_avg:59.69ms
step:593/2330 train_time:35396ms step_avg:59.69ms
step:594/2330 train_time:35457ms step_avg:59.69ms
step:595/2330 train_time:35515ms step_avg:59.69ms
step:596/2330 train_time:35575ms step_avg:59.69ms
step:597/2330 train_time:35634ms step_avg:59.69ms
step:598/2330 train_time:35695ms step_avg:59.69ms
step:599/2330 train_time:35753ms step_avg:59.69ms
step:600/2330 train_time:35813ms step_avg:59.69ms
step:601/2330 train_time:35872ms step_avg:59.69ms
step:602/2330 train_time:35934ms step_avg:59.69ms
step:603/2330 train_time:35992ms step_avg:59.69ms
step:604/2330 train_time:36053ms step_avg:59.69ms
step:605/2330 train_time:36112ms step_avg:59.69ms
step:606/2330 train_time:36173ms step_avg:59.69ms
step:607/2330 train_time:36232ms step_avg:59.69ms
step:608/2330 train_time:36292ms step_avg:59.69ms
step:609/2330 train_time:36350ms step_avg:59.69ms
step:610/2330 train_time:36411ms step_avg:59.69ms
step:611/2330 train_time:36470ms step_avg:59.69ms
step:612/2330 train_time:36531ms step_avg:59.69ms
step:613/2330 train_time:36589ms step_avg:59.69ms
step:614/2330 train_time:36650ms step_avg:59.69ms
step:615/2330 train_time:36709ms step_avg:59.69ms
step:616/2330 train_time:36770ms step_avg:59.69ms
step:617/2330 train_time:36828ms step_avg:59.69ms
step:618/2330 train_time:36889ms step_avg:59.69ms
step:619/2330 train_time:36947ms step_avg:59.69ms
step:620/2330 train_time:37008ms step_avg:59.69ms
step:621/2330 train_time:37067ms step_avg:59.69ms
step:622/2330 train_time:37128ms step_avg:59.69ms
step:623/2330 train_time:37187ms step_avg:59.69ms
step:624/2330 train_time:37247ms step_avg:59.69ms
step:625/2330 train_time:37306ms step_avg:59.69ms
step:626/2330 train_time:37366ms step_avg:59.69ms
step:627/2330 train_time:37425ms step_avg:59.69ms
step:628/2330 train_time:37487ms step_avg:59.69ms
step:629/2330 train_time:37546ms step_avg:59.69ms
step:630/2330 train_time:37606ms step_avg:59.69ms
step:631/2330 train_time:37665ms step_avg:59.69ms
step:632/2330 train_time:37725ms step_avg:59.69ms
step:633/2330 train_time:37784ms step_avg:59.69ms
step:634/2330 train_time:37845ms step_avg:59.69ms
step:635/2330 train_time:37903ms step_avg:59.69ms
step:636/2330 train_time:37964ms step_avg:59.69ms
step:637/2330 train_time:38023ms step_avg:59.69ms
step:638/2330 train_time:38083ms step_avg:59.69ms
step:639/2330 train_time:38142ms step_avg:59.69ms
step:640/2330 train_time:38203ms step_avg:59.69ms
step:641/2330 train_time:38261ms step_avg:59.69ms
step:642/2330 train_time:38322ms step_avg:59.69ms
step:643/2330 train_time:38380ms step_avg:59.69ms
step:644/2330 train_time:38441ms step_avg:59.69ms
step:645/2330 train_time:38500ms step_avg:59.69ms
step:646/2330 train_time:38560ms step_avg:59.69ms
step:647/2330 train_time:38619ms step_avg:59.69ms
step:648/2330 train_time:38679ms step_avg:59.69ms
step:649/2330 train_time:38738ms step_avg:59.69ms
step:650/2330 train_time:38799ms step_avg:59.69ms
step:651/2330 train_time:38857ms step_avg:59.69ms
step:652/2330 train_time:38917ms step_avg:59.69ms
step:653/2330 train_time:38977ms step_avg:59.69ms
step:654/2330 train_time:39037ms step_avg:59.69ms
step:655/2330 train_time:39096ms step_avg:59.69ms
step:656/2330 train_time:39156ms step_avg:59.69ms
step:657/2330 train_time:39215ms step_avg:59.69ms
step:658/2330 train_time:39276ms step_avg:59.69ms
step:659/2330 train_time:39334ms step_avg:59.69ms
step:660/2330 train_time:39395ms step_avg:59.69ms
step:661/2330 train_time:39453ms step_avg:59.69ms
step:662/2330 train_time:39515ms step_avg:59.69ms
step:663/2330 train_time:39573ms step_avg:59.69ms
step:664/2330 train_time:39634ms step_avg:59.69ms
step:665/2330 train_time:39692ms step_avg:59.69ms
step:666/2330 train_time:39753ms step_avg:59.69ms
step:667/2330 train_time:39812ms step_avg:59.69ms
step:668/2330 train_time:39874ms step_avg:59.69ms
step:669/2330 train_time:39932ms step_avg:59.69ms
step:670/2330 train_time:39993ms step_avg:59.69ms
step:671/2330 train_time:40052ms step_avg:59.69ms
step:672/2330 train_time:40114ms step_avg:59.69ms
step:673/2330 train_time:40172ms step_avg:59.69ms
step:674/2330 train_time:40233ms step_avg:59.69ms
step:675/2330 train_time:40291ms step_avg:59.69ms
step:676/2330 train_time:40351ms step_avg:59.69ms
step:677/2330 train_time:40410ms step_avg:59.69ms
step:678/2330 train_time:40471ms step_avg:59.69ms
step:679/2330 train_time:40529ms step_avg:59.69ms
step:680/2330 train_time:40590ms step_avg:59.69ms
step:681/2330 train_time:40648ms step_avg:59.69ms
step:682/2330 train_time:40709ms step_avg:59.69ms
step:683/2330 train_time:40768ms step_avg:59.69ms
step:684/2330 train_time:40829ms step_avg:59.69ms
step:685/2330 train_time:40889ms step_avg:59.69ms
step:686/2330 train_time:40950ms step_avg:59.69ms
step:687/2330 train_time:41008ms step_avg:59.69ms
step:688/2330 train_time:41068ms step_avg:59.69ms
step:689/2330 train_time:41127ms step_avg:59.69ms
step:690/2330 train_time:41189ms step_avg:59.69ms
step:691/2330 train_time:41247ms step_avg:59.69ms
step:692/2330 train_time:41307ms step_avg:59.69ms
step:693/2330 train_time:41366ms step_avg:59.69ms
step:694/2330 train_time:41427ms step_avg:59.69ms
step:695/2330 train_time:41487ms step_avg:59.69ms
step:696/2330 train_time:41547ms step_avg:59.69ms
step:697/2330 train_time:41606ms step_avg:59.69ms
step:698/2330 train_time:41666ms step_avg:59.69ms
step:699/2330 train_time:41725ms step_avg:59.69ms
step:700/2330 train_time:41786ms step_avg:59.69ms
step:701/2330 train_time:41845ms step_avg:59.69ms
step:702/2330 train_time:41906ms step_avg:59.70ms
step:703/2330 train_time:41964ms step_avg:59.69ms
step:704/2330 train_time:42025ms step_avg:59.69ms
step:705/2330 train_time:42084ms step_avg:59.69ms
step:706/2330 train_time:42145ms step_avg:59.70ms
step:707/2330 train_time:42203ms step_avg:59.69ms
step:708/2330 train_time:42264ms step_avg:59.69ms
step:709/2330 train_time:42323ms step_avg:59.69ms
step:710/2330 train_time:42384ms step_avg:59.70ms
step:711/2330 train_time:42442ms step_avg:59.69ms
step:712/2330 train_time:42503ms step_avg:59.69ms
step:713/2330 train_time:42561ms step_avg:59.69ms
step:714/2330 train_time:42622ms step_avg:59.69ms
step:715/2330 train_time:42680ms step_avg:59.69ms
step:716/2330 train_time:42741ms step_avg:59.69ms
step:717/2330 train_time:42799ms step_avg:59.69ms
step:718/2330 train_time:42859ms step_avg:59.69ms
step:719/2330 train_time:42917ms step_avg:59.69ms
step:720/2330 train_time:42978ms step_avg:59.69ms
step:721/2330 train_time:43037ms step_avg:59.69ms
step:722/2330 train_time:43098ms step_avg:59.69ms
step:723/2330 train_time:43157ms step_avg:59.69ms
step:724/2330 train_time:43217ms step_avg:59.69ms
step:725/2330 train_time:43276ms step_avg:59.69ms
step:726/2330 train_time:43337ms step_avg:59.69ms
step:727/2330 train_time:43395ms step_avg:59.69ms
step:728/2330 train_time:43456ms step_avg:59.69ms
step:729/2330 train_time:43515ms step_avg:59.69ms
step:730/2330 train_time:43575ms step_avg:59.69ms
step:731/2330 train_time:43634ms step_avg:59.69ms
step:732/2330 train_time:43695ms step_avg:59.69ms
step:733/2330 train_time:43753ms step_avg:59.69ms
step:734/2330 train_time:43814ms step_avg:59.69ms
step:735/2330 train_time:43872ms step_avg:59.69ms
step:736/2330 train_time:43933ms step_avg:59.69ms
step:737/2330 train_time:43992ms step_avg:59.69ms
step:738/2330 train_time:44052ms step_avg:59.69ms
step:739/2330 train_time:44111ms step_avg:59.69ms
step:740/2330 train_time:44171ms step_avg:59.69ms
step:741/2330 train_time:44230ms step_avg:59.69ms
step:742/2330 train_time:44291ms step_avg:59.69ms
step:743/2330 train_time:44351ms step_avg:59.69ms
step:744/2330 train_time:44412ms step_avg:59.69ms
step:745/2330 train_time:44470ms step_avg:59.69ms
step:746/2330 train_time:44531ms step_avg:59.69ms
step:747/2330 train_time:44590ms step_avg:59.69ms
step:748/2330 train_time:44651ms step_avg:59.69ms
step:749/2330 train_time:44709ms step_avg:59.69ms
step:750/2330 train_time:44770ms step_avg:59.69ms
step:750/2330 val_loss:3.6902 train_time:44833ms step_avg:59.78ms
step:751/2330 train_time:44854ms step_avg:59.73ms
step:752/2330 train_time:44892ms step_avg:59.70ms
step:753/2330 train_time:44955ms step_avg:59.70ms
step:754/2330 train_time:45019ms step_avg:59.71ms
step:755/2330 train_time:45078ms step_avg:59.71ms
step:756/2330 train_time:45140ms step_avg:59.71ms
step:757/2330 train_time:45198ms step_avg:59.71ms
step:758/2330 train_time:45258ms step_avg:59.71ms
step:759/2330 train_time:45317ms step_avg:59.71ms
step:760/2330 train_time:45378ms step_avg:59.71ms
step:761/2330 train_time:45436ms step_avg:59.71ms
step:762/2330 train_time:45496ms step_avg:59.71ms
step:763/2330 train_time:45554ms step_avg:59.70ms
step:764/2330 train_time:45614ms step_avg:59.70ms
step:765/2330 train_time:45673ms step_avg:59.70ms
step:766/2330 train_time:45733ms step_avg:59.70ms
step:767/2330 train_time:45792ms step_avg:59.70ms
step:768/2330 train_time:45854ms step_avg:59.71ms
step:769/2330 train_time:45914ms step_avg:59.71ms
step:770/2330 train_time:45977ms step_avg:59.71ms
step:771/2330 train_time:46037ms step_avg:59.71ms
step:772/2330 train_time:46100ms step_avg:59.72ms
step:773/2330 train_time:46160ms step_avg:59.72ms
step:774/2330 train_time:46222ms step_avg:59.72ms
step:775/2330 train_time:46280ms step_avg:59.72ms
step:776/2330 train_time:46341ms step_avg:59.72ms
step:777/2330 train_time:46401ms step_avg:59.72ms
step:778/2330 train_time:46461ms step_avg:59.72ms
step:779/2330 train_time:46520ms step_avg:59.72ms
step:780/2330 train_time:46581ms step_avg:59.72ms
step:781/2330 train_time:46640ms step_avg:59.72ms
step:782/2330 train_time:46701ms step_avg:59.72ms
step:783/2330 train_time:46761ms step_avg:59.72ms
step:784/2330 train_time:46822ms step_avg:59.72ms
step:785/2330 train_time:46883ms step_avg:59.72ms
step:786/2330 train_time:46944ms step_avg:59.73ms
step:787/2330 train_time:47004ms step_avg:59.73ms
step:788/2330 train_time:47066ms step_avg:59.73ms
step:789/2330 train_time:47125ms step_avg:59.73ms
step:790/2330 train_time:47187ms step_avg:59.73ms
step:791/2330 train_time:47246ms step_avg:59.73ms
step:792/2330 train_time:47307ms step_avg:59.73ms
step:793/2330 train_time:47367ms step_avg:59.73ms
step:794/2330 train_time:47428ms step_avg:59.73ms
step:795/2330 train_time:47487ms step_avg:59.73ms
step:796/2330 train_time:47548ms step_avg:59.73ms
step:797/2330 train_time:47608ms step_avg:59.73ms
step:798/2330 train_time:47669ms step_avg:59.74ms
step:799/2330 train_time:47728ms step_avg:59.73ms
step:800/2330 train_time:47789ms step_avg:59.74ms
step:801/2330 train_time:47848ms step_avg:59.74ms
step:802/2330 train_time:47909ms step_avg:59.74ms
step:803/2330 train_time:47968ms step_avg:59.74ms
step:804/2330 train_time:48029ms step_avg:59.74ms
step:805/2330 train_time:48089ms step_avg:59.74ms
step:806/2330 train_time:48150ms step_avg:59.74ms
step:807/2330 train_time:48209ms step_avg:59.74ms
step:808/2330 train_time:48271ms step_avg:59.74ms
step:809/2330 train_time:48330ms step_avg:59.74ms
step:810/2330 train_time:48392ms step_avg:59.74ms
step:811/2330 train_time:48451ms step_avg:59.74ms
step:812/2330 train_time:48513ms step_avg:59.74ms
step:813/2330 train_time:48572ms step_avg:59.74ms
step:814/2330 train_time:48633ms step_avg:59.75ms
step:815/2330 train_time:48692ms step_avg:59.74ms
step:816/2330 train_time:48753ms step_avg:59.75ms
step:817/2330 train_time:48812ms step_avg:59.75ms
step:818/2330 train_time:48874ms step_avg:59.75ms
step:819/2330 train_time:48933ms step_avg:59.75ms
step:820/2330 train_time:48995ms step_avg:59.75ms
step:821/2330 train_time:49055ms step_avg:59.75ms
step:822/2330 train_time:49117ms step_avg:59.75ms
step:823/2330 train_time:49176ms step_avg:59.75ms
step:824/2330 train_time:49238ms step_avg:59.76ms
step:825/2330 train_time:49298ms step_avg:59.75ms
step:826/2330 train_time:49360ms step_avg:59.76ms
step:827/2330 train_time:49419ms step_avg:59.76ms
step:828/2330 train_time:49481ms step_avg:59.76ms
step:829/2330 train_time:49540ms step_avg:59.76ms
step:830/2330 train_time:49602ms step_avg:59.76ms
step:831/2330 train_time:49661ms step_avg:59.76ms
step:832/2330 train_time:49723ms step_avg:59.76ms
step:833/2330 train_time:49782ms step_avg:59.76ms
step:834/2330 train_time:49843ms step_avg:59.76ms
step:835/2330 train_time:49902ms step_avg:59.76ms
step:836/2330 train_time:49963ms step_avg:59.76ms
step:837/2330 train_time:50023ms step_avg:59.76ms
step:838/2330 train_time:50084ms step_avg:59.77ms
step:839/2330 train_time:50144ms step_avg:59.77ms
step:840/2330 train_time:50205ms step_avg:59.77ms
step:841/2330 train_time:50265ms step_avg:59.77ms
step:842/2330 train_time:50326ms step_avg:59.77ms
step:843/2330 train_time:50385ms step_avg:59.77ms
step:844/2330 train_time:50446ms step_avg:59.77ms
step:845/2330 train_time:50505ms step_avg:59.77ms
step:846/2330 train_time:50567ms step_avg:59.77ms
step:847/2330 train_time:50626ms step_avg:59.77ms
step:848/2330 train_time:50688ms step_avg:59.77ms
step:849/2330 train_time:50747ms step_avg:59.77ms
step:850/2330 train_time:50808ms step_avg:59.77ms
step:851/2330 train_time:50867ms step_avg:59.77ms
step:852/2330 train_time:50928ms step_avg:59.77ms
step:853/2330 train_time:50987ms step_avg:59.77ms
step:854/2330 train_time:51048ms step_avg:59.78ms
step:855/2330 train_time:51108ms step_avg:59.78ms
step:856/2330 train_time:51169ms step_avg:59.78ms
step:857/2330 train_time:51229ms step_avg:59.78ms
step:858/2330 train_time:51290ms step_avg:59.78ms
step:859/2330 train_time:51349ms step_avg:59.78ms
step:860/2330 train_time:51411ms step_avg:59.78ms
step:861/2330 train_time:51469ms step_avg:59.78ms
step:862/2330 train_time:51531ms step_avg:59.78ms
step:863/2330 train_time:51590ms step_avg:59.78ms
step:864/2330 train_time:51651ms step_avg:59.78ms
step:865/2330 train_time:51710ms step_avg:59.78ms
step:866/2330 train_time:51772ms step_avg:59.78ms
step:867/2330 train_time:51831ms step_avg:59.78ms
step:868/2330 train_time:51893ms step_avg:59.78ms
step:869/2330 train_time:51952ms step_avg:59.78ms
step:870/2330 train_time:52013ms step_avg:59.79ms
step:871/2330 train_time:52072ms step_avg:59.78ms
step:872/2330 train_time:52134ms step_avg:59.79ms
step:873/2330 train_time:52193ms step_avg:59.79ms
step:874/2330 train_time:52255ms step_avg:59.79ms
step:875/2330 train_time:52313ms step_avg:59.79ms
step:876/2330 train_time:52375ms step_avg:59.79ms
step:877/2330 train_time:52434ms step_avg:59.79ms
step:878/2330 train_time:52496ms step_avg:59.79ms
step:879/2330 train_time:52555ms step_avg:59.79ms
step:880/2330 train_time:52617ms step_avg:59.79ms
step:881/2330 train_time:52676ms step_avg:59.79ms
step:882/2330 train_time:52738ms step_avg:59.79ms
step:883/2330 train_time:52797ms step_avg:59.79ms
step:884/2330 train_time:52859ms step_avg:59.79ms
step:885/2330 train_time:52918ms step_avg:59.79ms
step:886/2330 train_time:52980ms step_avg:59.80ms
step:887/2330 train_time:53040ms step_avg:59.80ms
step:888/2330 train_time:53101ms step_avg:59.80ms
step:889/2330 train_time:53161ms step_avg:59.80ms
step:890/2330 train_time:53222ms step_avg:59.80ms
step:891/2330 train_time:53281ms step_avg:59.80ms
step:892/2330 train_time:53343ms step_avg:59.80ms
step:893/2330 train_time:53402ms step_avg:59.80ms
step:894/2330 train_time:53463ms step_avg:59.80ms
step:895/2330 train_time:53522ms step_avg:59.80ms
step:896/2330 train_time:53584ms step_avg:59.80ms
step:897/2330 train_time:53643ms step_avg:59.80ms
step:898/2330 train_time:53705ms step_avg:59.80ms
step:899/2330 train_time:53764ms step_avg:59.80ms
step:900/2330 train_time:53825ms step_avg:59.81ms
step:901/2330 train_time:53885ms step_avg:59.81ms
step:902/2330 train_time:53946ms step_avg:59.81ms
step:903/2330 train_time:54005ms step_avg:59.81ms
step:904/2330 train_time:54067ms step_avg:59.81ms
step:905/2330 train_time:54126ms step_avg:59.81ms
step:906/2330 train_time:54187ms step_avg:59.81ms
step:907/2330 train_time:54246ms step_avg:59.81ms
step:908/2330 train_time:54307ms step_avg:59.81ms
step:909/2330 train_time:54366ms step_avg:59.81ms
step:910/2330 train_time:54427ms step_avg:59.81ms
step:911/2330 train_time:54486ms step_avg:59.81ms
step:912/2330 train_time:54547ms step_avg:59.81ms
step:913/2330 train_time:54607ms step_avg:59.81ms
step:914/2330 train_time:54668ms step_avg:59.81ms
step:915/2330 train_time:54728ms step_avg:59.81ms
step:916/2330 train_time:54789ms step_avg:59.81ms
step:917/2330 train_time:54848ms step_avg:59.81ms
step:918/2330 train_time:54909ms step_avg:59.81ms
step:919/2330 train_time:54968ms step_avg:59.81ms
step:920/2330 train_time:55029ms step_avg:59.81ms
step:921/2330 train_time:55089ms step_avg:59.81ms
step:922/2330 train_time:55150ms step_avg:59.82ms
step:923/2330 train_time:55209ms step_avg:59.81ms
step:924/2330 train_time:55270ms step_avg:59.82ms
step:925/2330 train_time:55329ms step_avg:59.82ms
step:926/2330 train_time:55391ms step_avg:59.82ms
step:927/2330 train_time:55450ms step_avg:59.82ms
step:928/2330 train_time:55511ms step_avg:59.82ms
step:929/2330 train_time:55570ms step_avg:59.82ms
step:930/2330 train_time:55632ms step_avg:59.82ms
step:931/2330 train_time:55691ms step_avg:59.82ms
step:932/2330 train_time:55752ms step_avg:59.82ms
step:933/2330 train_time:55812ms step_avg:59.82ms
step:934/2330 train_time:55874ms step_avg:59.82ms
step:935/2330 train_time:55932ms step_avg:59.82ms
step:936/2330 train_time:55994ms step_avg:59.82ms
step:937/2330 train_time:56053ms step_avg:59.82ms
step:938/2330 train_time:56114ms step_avg:59.82ms
step:939/2330 train_time:56175ms step_avg:59.82ms
step:940/2330 train_time:56237ms step_avg:59.83ms
step:941/2330 train_time:56296ms step_avg:59.83ms
step:942/2330 train_time:56358ms step_avg:59.83ms
step:943/2330 train_time:56417ms step_avg:59.83ms
step:944/2330 train_time:56479ms step_avg:59.83ms
step:945/2330 train_time:56539ms step_avg:59.83ms
step:946/2330 train_time:56600ms step_avg:59.83ms
step:947/2330 train_time:56660ms step_avg:59.83ms
step:948/2330 train_time:56721ms step_avg:59.83ms
step:949/2330 train_time:56780ms step_avg:59.83ms
step:950/2330 train_time:56841ms step_avg:59.83ms
step:951/2330 train_time:56900ms step_avg:59.83ms
step:952/2330 train_time:56962ms step_avg:59.83ms
step:953/2330 train_time:57022ms step_avg:59.83ms
step:954/2330 train_time:57084ms step_avg:59.84ms
step:955/2330 train_time:57143ms step_avg:59.84ms
step:956/2330 train_time:57204ms step_avg:59.84ms
step:957/2330 train_time:57263ms step_avg:59.84ms
step:958/2330 train_time:57324ms step_avg:59.84ms
step:959/2330 train_time:57383ms step_avg:59.84ms
step:960/2330 train_time:57445ms step_avg:59.84ms
step:961/2330 train_time:57504ms step_avg:59.84ms
step:962/2330 train_time:57565ms step_avg:59.84ms
step:963/2330 train_time:57625ms step_avg:59.84ms
step:964/2330 train_time:57687ms step_avg:59.84ms
step:965/2330 train_time:57746ms step_avg:59.84ms
step:966/2330 train_time:57807ms step_avg:59.84ms
step:967/2330 train_time:57867ms step_avg:59.84ms
step:968/2330 train_time:57929ms step_avg:59.84ms
step:969/2330 train_time:57988ms step_avg:59.84ms
step:970/2330 train_time:58049ms step_avg:59.84ms
step:971/2330 train_time:58109ms step_avg:59.84ms
step:972/2330 train_time:58169ms step_avg:59.85ms
step:973/2330 train_time:58228ms step_avg:59.84ms
step:974/2330 train_time:58290ms step_avg:59.85ms
step:975/2330 train_time:58349ms step_avg:59.85ms
step:976/2330 train_time:58410ms step_avg:59.85ms
step:977/2330 train_time:58470ms step_avg:59.85ms
step:978/2330 train_time:58531ms step_avg:59.85ms
step:979/2330 train_time:58591ms step_avg:59.85ms
step:980/2330 train_time:58652ms step_avg:59.85ms
step:981/2330 train_time:58711ms step_avg:59.85ms
step:982/2330 train_time:58772ms step_avg:59.85ms
step:983/2330 train_time:58831ms step_avg:59.85ms
step:984/2330 train_time:58892ms step_avg:59.85ms
step:985/2330 train_time:58951ms step_avg:59.85ms
step:986/2330 train_time:59013ms step_avg:59.85ms
step:987/2330 train_time:59072ms step_avg:59.85ms
step:988/2330 train_time:59133ms step_avg:59.85ms
step:989/2330 train_time:59193ms step_avg:59.85ms
step:990/2330 train_time:59255ms step_avg:59.85ms
step:991/2330 train_time:59314ms step_avg:59.85ms
step:992/2330 train_time:59375ms step_avg:59.85ms
step:993/2330 train_time:59435ms step_avg:59.85ms
step:994/2330 train_time:59497ms step_avg:59.86ms
step:995/2330 train_time:59556ms step_avg:59.86ms
step:996/2330 train_time:59618ms step_avg:59.86ms
step:997/2330 train_time:59678ms step_avg:59.86ms
step:998/2330 train_time:59739ms step_avg:59.86ms
step:999/2330 train_time:59799ms step_avg:59.86ms
step:1000/2330 train_time:59860ms step_avg:59.86ms
step:1000/2330 val_loss:3.5745 train_time:59924ms step_avg:59.92ms
step:1001/2330 train_time:59945ms step_avg:59.88ms
step:1002/2330 train_time:59984ms step_avg:59.86ms
step:1003/2330 train_time:60043ms step_avg:59.86ms
step:1004/2330 train_time:60106ms step_avg:59.87ms
step:1005/2330 train_time:60166ms step_avg:59.87ms
step:1006/2330 train_time:60227ms step_avg:59.87ms
step:1007/2330 train_time:60286ms step_avg:59.87ms
step:1008/2330 train_time:60347ms step_avg:59.87ms
step:1009/2330 train_time:60405ms step_avg:59.87ms
step:1010/2330 train_time:60466ms step_avg:59.87ms
step:1011/2330 train_time:60524ms step_avg:59.87ms
step:1012/2330 train_time:60585ms step_avg:59.87ms
step:1013/2330 train_time:60643ms step_avg:59.87ms
step:1014/2330 train_time:60704ms step_avg:59.87ms
step:1015/2330 train_time:60762ms step_avg:59.86ms
step:1016/2330 train_time:60824ms step_avg:59.87ms
step:1017/2330 train_time:60888ms step_avg:59.87ms
step:1018/2330 train_time:60951ms step_avg:59.87ms
step:1019/2330 train_time:61011ms step_avg:59.87ms
step:1020/2330 train_time:61072ms step_avg:59.87ms
step:1021/2330 train_time:61132ms step_avg:59.87ms
step:1022/2330 train_time:61193ms step_avg:59.88ms
step:1023/2330 train_time:61253ms step_avg:59.88ms
step:1024/2330 train_time:61314ms step_avg:59.88ms
step:1025/2330 train_time:61373ms step_avg:59.88ms
step:1026/2330 train_time:61434ms step_avg:59.88ms
step:1027/2330 train_time:61493ms step_avg:59.88ms
step:1028/2330 train_time:61554ms step_avg:59.88ms
step:1029/2330 train_time:61612ms step_avg:59.88ms
step:1030/2330 train_time:61674ms step_avg:59.88ms
step:1031/2330 train_time:61733ms step_avg:59.88ms
step:1032/2330 train_time:61795ms step_avg:59.88ms
step:1033/2330 train_time:61855ms step_avg:59.88ms
step:1034/2330 train_time:61917ms step_avg:59.88ms
step:1035/2330 train_time:61976ms step_avg:59.88ms
step:1036/2330 train_time:62038ms step_avg:59.88ms
step:1037/2330 train_time:62099ms step_avg:59.88ms
step:1038/2330 train_time:62160ms step_avg:59.88ms
step:1039/2330 train_time:62219ms step_avg:59.88ms
step:1040/2330 train_time:62281ms step_avg:59.89ms
step:1041/2330 train_time:62340ms step_avg:59.88ms
step:1042/2330 train_time:62401ms step_avg:59.89ms
step:1043/2330 train_time:62460ms step_avg:59.89ms
step:1044/2330 train_time:62522ms step_avg:59.89ms
step:1045/2330 train_time:62581ms step_avg:59.89ms
step:1046/2330 train_time:62642ms step_avg:59.89ms
step:1047/2330 train_time:62702ms step_avg:59.89ms
step:1048/2330 train_time:62763ms step_avg:59.89ms
step:1049/2330 train_time:62822ms step_avg:59.89ms
step:1050/2330 train_time:62884ms step_avg:59.89ms
step:1051/2330 train_time:62944ms step_avg:59.89ms
step:1052/2330 train_time:63005ms step_avg:59.89ms
step:1053/2330 train_time:63065ms step_avg:59.89ms
step:1054/2330 train_time:63126ms step_avg:59.89ms
step:1055/2330 train_time:63185ms step_avg:59.89ms
step:1056/2330 train_time:63247ms step_avg:59.89ms
step:1057/2330 train_time:63307ms step_avg:59.89ms
step:1058/2330 train_time:63368ms step_avg:59.89ms
step:1059/2330 train_time:63427ms step_avg:59.89ms
step:1060/2330 train_time:63488ms step_avg:59.89ms
step:1061/2330 train_time:63547ms step_avg:59.89ms
step:1062/2330 train_time:63608ms step_avg:59.89ms
step:1063/2330 train_time:63668ms step_avg:59.89ms
step:1064/2330 train_time:63729ms step_avg:59.90ms
step:1065/2330 train_time:63788ms step_avg:59.90ms
step:1066/2330 train_time:63850ms step_avg:59.90ms
step:1067/2330 train_time:63909ms step_avg:59.90ms
step:1068/2330 train_time:63970ms step_avg:59.90ms
step:1069/2330 train_time:64029ms step_avg:59.90ms
step:1070/2330 train_time:64091ms step_avg:59.90ms
step:1071/2330 train_time:64150ms step_avg:59.90ms
step:1072/2330 train_time:64211ms step_avg:59.90ms
step:1073/2330 train_time:64270ms step_avg:59.90ms
step:1074/2330 train_time:64331ms step_avg:59.90ms
step:1075/2330 train_time:64391ms step_avg:59.90ms
step:1076/2330 train_time:64452ms step_avg:59.90ms
step:1077/2330 train_time:64512ms step_avg:59.90ms
step:1078/2330 train_time:64573ms step_avg:59.90ms
step:1079/2330 train_time:64633ms step_avg:59.90ms
step:1080/2330 train_time:64694ms step_avg:59.90ms
step:1081/2330 train_time:64753ms step_avg:59.90ms
step:1082/2330 train_time:64814ms step_avg:59.90ms
step:1083/2330 train_time:64874ms step_avg:59.90ms
step:1084/2330 train_time:64935ms step_avg:59.90ms
step:1085/2330 train_time:64994ms step_avg:59.90ms
step:1086/2330 train_time:65055ms step_avg:59.90ms
step:1087/2330 train_time:65115ms step_avg:59.90ms
step:1088/2330 train_time:65176ms step_avg:59.90ms
step:1089/2330 train_time:65235ms step_avg:59.90ms
step:1090/2330 train_time:65296ms step_avg:59.90ms
step:1091/2330 train_time:65355ms step_avg:59.90ms
step:1092/2330 train_time:65417ms step_avg:59.91ms
step:1093/2330 train_time:65476ms step_avg:59.91ms
step:1094/2330 train_time:65538ms step_avg:59.91ms
step:1095/2330 train_time:65598ms step_avg:59.91ms
step:1096/2330 train_time:65659ms step_avg:59.91ms
step:1097/2330 train_time:65719ms step_avg:59.91ms
step:1098/2330 train_time:65781ms step_avg:59.91ms
step:1099/2330 train_time:65841ms step_avg:59.91ms
step:1100/2330 train_time:65903ms step_avg:59.91ms
step:1101/2330 train_time:65962ms step_avg:59.91ms
step:1102/2330 train_time:66023ms step_avg:59.91ms
step:1103/2330 train_time:66083ms step_avg:59.91ms
step:1104/2330 train_time:66144ms step_avg:59.91ms
step:1105/2330 train_time:66204ms step_avg:59.91ms
step:1106/2330 train_time:66265ms step_avg:59.91ms
step:1107/2330 train_time:66324ms step_avg:59.91ms
step:1108/2330 train_time:66385ms step_avg:59.91ms
step:1109/2330 train_time:66445ms step_avg:59.91ms
step:1110/2330 train_time:66507ms step_avg:59.92ms
step:1111/2330 train_time:66566ms step_avg:59.92ms
step:1112/2330 train_time:66627ms step_avg:59.92ms
step:1113/2330 train_time:66686ms step_avg:59.92ms
step:1114/2330 train_time:66748ms step_avg:59.92ms
step:1115/2330 train_time:66808ms step_avg:59.92ms
step:1116/2330 train_time:66869ms step_avg:59.92ms
step:1117/2330 train_time:66929ms step_avg:59.92ms
step:1118/2330 train_time:66990ms step_avg:59.92ms
step:1119/2330 train_time:67050ms step_avg:59.92ms
step:1120/2330 train_time:67112ms step_avg:59.92ms
step:1121/2330 train_time:67171ms step_avg:59.92ms
step:1122/2330 train_time:67233ms step_avg:59.92ms
step:1123/2330 train_time:67292ms step_avg:59.92ms
step:1124/2330 train_time:67353ms step_avg:59.92ms
step:1125/2330 train_time:67412ms step_avg:59.92ms
step:1126/2330 train_time:67474ms step_avg:59.92ms
step:1127/2330 train_time:67533ms step_avg:59.92ms
step:1128/2330 train_time:67594ms step_avg:59.92ms
step:1129/2330 train_time:67654ms step_avg:59.92ms
step:1130/2330 train_time:67715ms step_avg:59.93ms
step:1131/2330 train_time:67775ms step_avg:59.93ms
step:1132/2330 train_time:67837ms step_avg:59.93ms
step:1133/2330 train_time:67896ms step_avg:59.93ms
step:1134/2330 train_time:67958ms step_avg:59.93ms
step:1135/2330 train_time:68017ms step_avg:59.93ms
step:1136/2330 train_time:68079ms step_avg:59.93ms
step:1137/2330 train_time:68139ms step_avg:59.93ms
step:1138/2330 train_time:68200ms step_avg:59.93ms
step:1139/2330 train_time:68260ms step_avg:59.93ms
step:1140/2330 train_time:68322ms step_avg:59.93ms
step:1141/2330 train_time:68382ms step_avg:59.93ms
step:1142/2330 train_time:68443ms step_avg:59.93ms
step:1143/2330 train_time:68503ms step_avg:59.93ms
step:1144/2330 train_time:68565ms step_avg:59.93ms
step:1145/2330 train_time:68624ms step_avg:59.93ms
step:1146/2330 train_time:68685ms step_avg:59.93ms
step:1147/2330 train_time:68744ms step_avg:59.93ms
step:1148/2330 train_time:68806ms step_avg:59.94ms
step:1149/2330 train_time:68865ms step_avg:59.93ms
step:1150/2330 train_time:68926ms step_avg:59.94ms
step:1151/2330 train_time:68986ms step_avg:59.94ms
step:1152/2330 train_time:69047ms step_avg:59.94ms
step:1153/2330 train_time:69106ms step_avg:59.94ms
step:1154/2330 train_time:69167ms step_avg:59.94ms
step:1155/2330 train_time:69226ms step_avg:59.94ms
step:1156/2330 train_time:69288ms step_avg:59.94ms
step:1157/2330 train_time:69347ms step_avg:59.94ms
step:1158/2330 train_time:69409ms step_avg:59.94ms
step:1159/2330 train_time:69467ms step_avg:59.94ms
step:1160/2330 train_time:69528ms step_avg:59.94ms
step:1161/2330 train_time:69587ms step_avg:59.94ms
step:1162/2330 train_time:69648ms step_avg:59.94ms
step:1163/2330 train_time:69707ms step_avg:59.94ms
step:1164/2330 train_time:69769ms step_avg:59.94ms
step:1165/2330 train_time:69828ms step_avg:59.94ms
step:1166/2330 train_time:69889ms step_avg:59.94ms
step:1167/2330 train_time:69949ms step_avg:59.94ms
step:1168/2330 train_time:70011ms step_avg:59.94ms
step:1169/2330 train_time:70070ms step_avg:59.94ms
step:1170/2330 train_time:70131ms step_avg:59.94ms
step:1171/2330 train_time:70191ms step_avg:59.94ms
step:1172/2330 train_time:70253ms step_avg:59.94ms
step:1173/2330 train_time:70312ms step_avg:59.94ms
step:1174/2330 train_time:70373ms step_avg:59.94ms
step:1175/2330 train_time:70432ms step_avg:59.94ms
step:1176/2330 train_time:70494ms step_avg:59.94ms
step:1177/2330 train_time:70554ms step_avg:59.94ms
step:1178/2330 train_time:70616ms step_avg:59.95ms
step:1179/2330 train_time:70675ms step_avg:59.94ms
step:1180/2330 train_time:70737ms step_avg:59.95ms
step:1181/2330 train_time:70796ms step_avg:59.95ms
step:1182/2330 train_time:70858ms step_avg:59.95ms
step:1183/2330 train_time:70918ms step_avg:59.95ms
step:1184/2330 train_time:70980ms step_avg:59.95ms
step:1185/2330 train_time:71039ms step_avg:59.95ms
step:1186/2330 train_time:71101ms step_avg:59.95ms
step:1187/2330 train_time:71161ms step_avg:59.95ms
step:1188/2330 train_time:71223ms step_avg:59.95ms
step:1189/2330 train_time:71282ms step_avg:59.95ms
step:1190/2330 train_time:71343ms step_avg:59.95ms
step:1191/2330 train_time:71402ms step_avg:59.95ms
step:1192/2330 train_time:71464ms step_avg:59.95ms
step:1193/2330 train_time:71524ms step_avg:59.95ms
step:1194/2330 train_time:71585ms step_avg:59.95ms
step:1195/2330 train_time:71644ms step_avg:59.95ms
step:1196/2330 train_time:71705ms step_avg:59.95ms
step:1197/2330 train_time:71764ms step_avg:59.95ms
step:1198/2330 train_time:71824ms step_avg:59.95ms
step:1199/2330 train_time:71884ms step_avg:59.95ms
step:1200/2330 train_time:71945ms step_avg:59.95ms
step:1201/2330 train_time:72005ms step_avg:59.95ms
step:1202/2330 train_time:72067ms step_avg:59.96ms
step:1203/2330 train_time:72126ms step_avg:59.96ms
step:1204/2330 train_time:72187ms step_avg:59.96ms
step:1205/2330 train_time:72246ms step_avg:59.96ms
step:1206/2330 train_time:72307ms step_avg:59.96ms
step:1207/2330 train_time:72367ms step_avg:59.96ms
step:1208/2330 train_time:72428ms step_avg:59.96ms
step:1209/2330 train_time:72487ms step_avg:59.96ms
step:1210/2330 train_time:72548ms step_avg:59.96ms
step:1211/2330 train_time:72607ms step_avg:59.96ms
step:1212/2330 train_time:72668ms step_avg:59.96ms
step:1213/2330 train_time:72726ms step_avg:59.96ms
step:1214/2330 train_time:72787ms step_avg:59.96ms
step:1215/2330 train_time:72846ms step_avg:59.96ms
step:1216/2330 train_time:72908ms step_avg:59.96ms
step:1217/2330 train_time:72967ms step_avg:59.96ms
step:1218/2330 train_time:73028ms step_avg:59.96ms
step:1219/2330 train_time:73088ms step_avg:59.96ms
step:1220/2330 train_time:73149ms step_avg:59.96ms
step:1221/2330 train_time:73210ms step_avg:59.96ms
step:1222/2330 train_time:73270ms step_avg:59.96ms
step:1223/2330 train_time:73329ms step_avg:59.96ms
step:1224/2330 train_time:73391ms step_avg:59.96ms
step:1225/2330 train_time:73451ms step_avg:59.96ms
step:1226/2330 train_time:73512ms step_avg:59.96ms
step:1227/2330 train_time:73571ms step_avg:59.96ms
step:1228/2330 train_time:73631ms step_avg:59.96ms
step:1229/2330 train_time:73691ms step_avg:59.96ms
step:1230/2330 train_time:73752ms step_avg:59.96ms
step:1231/2330 train_time:73810ms step_avg:59.96ms
step:1232/2330 train_time:73871ms step_avg:59.96ms
step:1233/2330 train_time:73930ms step_avg:59.96ms
step:1234/2330 train_time:73991ms step_avg:59.96ms
step:1235/2330 train_time:74051ms step_avg:59.96ms
step:1236/2330 train_time:74112ms step_avg:59.96ms
step:1237/2330 train_time:74171ms step_avg:59.96ms
step:1238/2330 train_time:74232ms step_avg:59.96ms
step:1239/2330 train_time:74292ms step_avg:59.96ms
step:1240/2330 train_time:74353ms step_avg:59.96ms
step:1241/2330 train_time:74413ms step_avg:59.96ms
step:1242/2330 train_time:74473ms step_avg:59.96ms
step:1243/2330 train_time:74532ms step_avg:59.96ms
step:1244/2330 train_time:74594ms step_avg:59.96ms
step:1245/2330 train_time:74653ms step_avg:59.96ms
step:1246/2330 train_time:74714ms step_avg:59.96ms
step:1247/2330 train_time:74773ms step_avg:59.96ms
step:1248/2330 train_time:74834ms step_avg:59.96ms
step:1249/2330 train_time:74894ms step_avg:59.96ms
step:1250/2330 train_time:74955ms step_avg:59.96ms
step:1250/2330 val_loss:3.5191 train_time:75018ms step_avg:60.01ms
step:1251/2330 train_time:75039ms step_avg:59.98ms
step:1252/2330 train_time:75077ms step_avg:59.97ms
step:1253/2330 train_time:75139ms step_avg:59.97ms
step:1254/2330 train_time:75205ms step_avg:59.97ms
step:1255/2330 train_time:75267ms step_avg:59.97ms
step:1256/2330 train_time:75328ms step_avg:59.97ms
step:1257/2330 train_time:75387ms step_avg:59.97ms
step:1258/2330 train_time:75448ms step_avg:59.97ms
step:1259/2330 train_time:75506ms step_avg:59.97ms
step:1260/2330 train_time:75567ms step_avg:59.97ms
step:1261/2330 train_time:75625ms step_avg:59.97ms
step:1262/2330 train_time:75686ms step_avg:59.97ms
step:1263/2330 train_time:75744ms step_avg:59.97ms
step:1264/2330 train_time:75805ms step_avg:59.97ms
step:1265/2330 train_time:75863ms step_avg:59.97ms
step:1266/2330 train_time:75924ms step_avg:59.97ms
step:1267/2330 train_time:75984ms step_avg:59.97ms
step:1268/2330 train_time:76047ms step_avg:59.97ms
step:1269/2330 train_time:76108ms step_avg:59.98ms
step:1270/2330 train_time:76172ms step_avg:59.98ms
step:1271/2330 train_time:76231ms step_avg:59.98ms
step:1272/2330 train_time:76293ms step_avg:59.98ms
step:1273/2330 train_time:76352ms step_avg:59.98ms
step:1274/2330 train_time:76413ms step_avg:59.98ms
step:1275/2330 train_time:76473ms step_avg:59.98ms
step:1276/2330 train_time:76534ms step_avg:59.98ms
step:1277/2330 train_time:76593ms step_avg:59.98ms
step:1278/2330 train_time:76654ms step_avg:59.98ms
step:1279/2330 train_time:76713ms step_avg:59.98ms
step:1280/2330 train_time:76774ms step_avg:59.98ms
step:1281/2330 train_time:76834ms step_avg:59.98ms
step:1282/2330 train_time:76895ms step_avg:59.98ms
step:1283/2330 train_time:76954ms step_avg:59.98ms
step:1284/2330 train_time:77016ms step_avg:59.98ms
step:1285/2330 train_time:77075ms step_avg:59.98ms
step:1286/2330 train_time:77137ms step_avg:59.98ms
step:1287/2330 train_time:77198ms step_avg:59.98ms
step:1288/2330 train_time:77260ms step_avg:59.98ms
step:1289/2330 train_time:77320ms step_avg:59.98ms
step:1290/2330 train_time:77381ms step_avg:59.99ms
step:1291/2330 train_time:77441ms step_avg:59.99ms
step:1292/2330 train_time:77502ms step_avg:59.99ms
step:1293/2330 train_time:77561ms step_avg:59.99ms
step:1294/2330 train_time:77623ms step_avg:59.99ms
step:1295/2330 train_time:77682ms step_avg:59.99ms
step:1296/2330 train_time:77743ms step_avg:59.99ms
step:1297/2330 train_time:77802ms step_avg:59.99ms
step:1298/2330 train_time:77862ms step_avg:59.99ms
step:1299/2330 train_time:77922ms step_avg:59.99ms
step:1300/2330 train_time:77983ms step_avg:59.99ms
step:1301/2330 train_time:78042ms step_avg:59.99ms
step:1302/2330 train_time:78104ms step_avg:59.99ms
step:1303/2330 train_time:78164ms step_avg:59.99ms
step:1304/2330 train_time:78226ms step_avg:59.99ms
step:1305/2330 train_time:78286ms step_avg:59.99ms
step:1306/2330 train_time:78347ms step_avg:59.99ms
step:1307/2330 train_time:78407ms step_avg:59.99ms
step:1308/2330 train_time:78468ms step_avg:59.99ms
step:1309/2330 train_time:78526ms step_avg:59.99ms
step:1310/2330 train_time:78588ms step_avg:59.99ms
step:1311/2330 train_time:78647ms step_avg:59.99ms
step:1312/2330 train_time:78707ms step_avg:59.99ms
step:1313/2330 train_time:78766ms step_avg:59.99ms
step:1314/2330 train_time:78827ms step_avg:59.99ms
step:1315/2330 train_time:78886ms step_avg:59.99ms
step:1316/2330 train_time:78947ms step_avg:59.99ms
step:1317/2330 train_time:79006ms step_avg:59.99ms
step:1318/2330 train_time:79067ms step_avg:59.99ms
step:1319/2330 train_time:79127ms step_avg:59.99ms
step:1320/2330 train_time:79188ms step_avg:59.99ms
step:1321/2330 train_time:79248ms step_avg:59.99ms
step:1322/2330 train_time:79309ms step_avg:59.99ms
step:1323/2330 train_time:79368ms step_avg:59.99ms
step:1324/2330 train_time:79429ms step_avg:59.99ms
step:1325/2330 train_time:79489ms step_avg:59.99ms
step:1326/2330 train_time:79549ms step_avg:59.99ms
step:1327/2330 train_time:79608ms step_avg:59.99ms
step:1328/2330 train_time:79669ms step_avg:59.99ms
step:1329/2330 train_time:79728ms step_avg:59.99ms
step:1330/2330 train_time:79790ms step_avg:59.99ms
step:1331/2330 train_time:79849ms step_avg:59.99ms
step:1332/2330 train_time:79910ms step_avg:59.99ms
step:1333/2330 train_time:79969ms step_avg:59.99ms
step:1334/2330 train_time:80031ms step_avg:59.99ms
step:1335/2330 train_time:80090ms step_avg:59.99ms
step:1336/2330 train_time:80151ms step_avg:59.99ms
step:1337/2330 train_time:80210ms step_avg:59.99ms
step:1338/2330 train_time:80272ms step_avg:59.99ms
step:1339/2330 train_time:80331ms step_avg:59.99ms
step:1340/2330 train_time:80392ms step_avg:59.99ms
step:1341/2330 train_time:80451ms step_avg:59.99ms
step:1342/2330 train_time:80513ms step_avg:59.99ms
step:1343/2330 train_time:80572ms step_avg:59.99ms
step:1344/2330 train_time:80633ms step_avg:60.00ms
step:1345/2330 train_time:80693ms step_avg:59.99ms
step:1346/2330 train_time:80754ms step_avg:60.00ms
step:1347/2330 train_time:80813ms step_avg:60.00ms
step:1348/2330 train_time:80874ms step_avg:60.00ms
step:1349/2330 train_time:80934ms step_avg:60.00ms
step:1350/2330 train_time:80996ms step_avg:60.00ms
step:1351/2330 train_time:81056ms step_avg:60.00ms
step:1352/2330 train_time:81117ms step_avg:60.00ms
step:1353/2330 train_time:81176ms step_avg:60.00ms
step:1354/2330 train_time:81238ms step_avg:60.00ms
step:1355/2330 train_time:81298ms step_avg:60.00ms
step:1356/2330 train_time:81360ms step_avg:60.00ms
step:1357/2330 train_time:81420ms step_avg:60.00ms
step:1358/2330 train_time:81481ms step_avg:60.00ms
step:1359/2330 train_time:81541ms step_avg:60.00ms
step:1360/2330 train_time:81602ms step_avg:60.00ms
step:1361/2330 train_time:81662ms step_avg:60.00ms
step:1362/2330 train_time:81723ms step_avg:60.00ms
step:1363/2330 train_time:81782ms step_avg:60.00ms
step:1364/2330 train_time:81843ms step_avg:60.00ms
step:1365/2330 train_time:81903ms step_avg:60.00ms
step:1366/2330 train_time:81963ms step_avg:60.00ms
step:1367/2330 train_time:82023ms step_avg:60.00ms
step:1368/2330 train_time:82084ms step_avg:60.00ms
step:1369/2330 train_time:82143ms step_avg:60.00ms
step:1370/2330 train_time:82205ms step_avg:60.00ms
step:1371/2330 train_time:82264ms step_avg:60.00ms
step:1372/2330 train_time:82326ms step_avg:60.00ms
step:1373/2330 train_time:82385ms step_avg:60.00ms
step:1374/2330 train_time:82446ms step_avg:60.00ms
step:1375/2330 train_time:82506ms step_avg:60.00ms
step:1376/2330 train_time:82566ms step_avg:60.00ms
step:1377/2330 train_time:82626ms step_avg:60.00ms
step:1378/2330 train_time:82687ms step_avg:60.01ms
step:1379/2330 train_time:82746ms step_avg:60.00ms
step:1380/2330 train_time:82808ms step_avg:60.01ms
step:1381/2330 train_time:82867ms step_avg:60.00ms
step:1382/2330 train_time:82927ms step_avg:60.01ms
step:1383/2330 train_time:82986ms step_avg:60.00ms
step:1384/2330 train_time:83048ms step_avg:60.01ms
step:1385/2330 train_time:83108ms step_avg:60.01ms
step:1386/2330 train_time:83169ms step_avg:60.01ms
step:1387/2330 train_time:83228ms step_avg:60.01ms
step:1388/2330 train_time:83289ms step_avg:60.01ms
step:1389/2330 train_time:83349ms step_avg:60.01ms
step:1390/2330 train_time:83410ms step_avg:60.01ms
step:1391/2330 train_time:83469ms step_avg:60.01ms
step:1392/2330 train_time:83530ms step_avg:60.01ms
step:1393/2330 train_time:83590ms step_avg:60.01ms
step:1394/2330 train_time:83651ms step_avg:60.01ms
step:1395/2330 train_time:83710ms step_avg:60.01ms
step:1396/2330 train_time:83771ms step_avg:60.01ms
step:1397/2330 train_time:83831ms step_avg:60.01ms
step:1398/2330 train_time:83892ms step_avg:60.01ms
step:1399/2330 train_time:83951ms step_avg:60.01ms
step:1400/2330 train_time:84013ms step_avg:60.01ms
step:1401/2330 train_time:84072ms step_avg:60.01ms
step:1402/2330 train_time:84134ms step_avg:60.01ms
step:1403/2330 train_time:84194ms step_avg:60.01ms
step:1404/2330 train_time:84255ms step_avg:60.01ms
step:1405/2330 train_time:84315ms step_avg:60.01ms
step:1406/2330 train_time:84376ms step_avg:60.01ms
step:1407/2330 train_time:84435ms step_avg:60.01ms
step:1408/2330 train_time:84497ms step_avg:60.01ms
step:1409/2330 train_time:84556ms step_avg:60.01ms
step:1410/2330 train_time:84618ms step_avg:60.01ms
step:1411/2330 train_time:84678ms step_avg:60.01ms
step:1412/2330 train_time:84740ms step_avg:60.01ms
step:1413/2330 train_time:84799ms step_avg:60.01ms
step:1414/2330 train_time:84861ms step_avg:60.01ms
step:1415/2330 train_time:84921ms step_avg:60.01ms
step:1416/2330 train_time:84982ms step_avg:60.02ms
step:1417/2330 train_time:85042ms step_avg:60.02ms
step:1418/2330 train_time:85103ms step_avg:60.02ms
step:1419/2330 train_time:85162ms step_avg:60.02ms
step:1420/2330 train_time:85223ms step_avg:60.02ms
step:1421/2330 train_time:85282ms step_avg:60.02ms
step:1422/2330 train_time:85344ms step_avg:60.02ms
step:1423/2330 train_time:85403ms step_avg:60.02ms
step:1424/2330 train_time:85465ms step_avg:60.02ms
step:1425/2330 train_time:85524ms step_avg:60.02ms
step:1426/2330 train_time:85586ms step_avg:60.02ms
step:1427/2330 train_time:85645ms step_avg:60.02ms
step:1428/2330 train_time:85707ms step_avg:60.02ms
step:1429/2330 train_time:85766ms step_avg:60.02ms
step:1430/2330 train_time:85828ms step_avg:60.02ms
step:1431/2330 train_time:85887ms step_avg:60.02ms
step:1432/2330 train_time:85949ms step_avg:60.02ms
step:1433/2330 train_time:86008ms step_avg:60.02ms
step:1434/2330 train_time:86069ms step_avg:60.02ms
step:1435/2330 train_time:86128ms step_avg:60.02ms
step:1436/2330 train_time:86189ms step_avg:60.02ms
step:1437/2330 train_time:86248ms step_avg:60.02ms
step:1438/2330 train_time:86309ms step_avg:60.02ms
step:1439/2330 train_time:86368ms step_avg:60.02ms
step:1440/2330 train_time:86430ms step_avg:60.02ms
step:1441/2330 train_time:86489ms step_avg:60.02ms
step:1442/2330 train_time:86550ms step_avg:60.02ms
step:1443/2330 train_time:86609ms step_avg:60.02ms
step:1444/2330 train_time:86670ms step_avg:60.02ms
step:1445/2330 train_time:86729ms step_avg:60.02ms
step:1446/2330 train_time:86790ms step_avg:60.02ms
step:1447/2330 train_time:86849ms step_avg:60.02ms
step:1448/2330 train_time:86910ms step_avg:60.02ms
step:1449/2330 train_time:86970ms step_avg:60.02ms
step:1450/2330 train_time:87031ms step_avg:60.02ms
step:1451/2330 train_time:87090ms step_avg:60.02ms
step:1452/2330 train_time:87152ms step_avg:60.02ms
step:1453/2330 train_time:87211ms step_avg:60.02ms
step:1454/2330 train_time:87272ms step_avg:60.02ms
step:1455/2330 train_time:87331ms step_avg:60.02ms
step:1456/2330 train_time:87392ms step_avg:60.02ms
step:1457/2330 train_time:87451ms step_avg:60.02ms
step:1458/2330 train_time:87513ms step_avg:60.02ms
step:1459/2330 train_time:87572ms step_avg:60.02ms
step:1460/2330 train_time:87633ms step_avg:60.02ms
step:1461/2330 train_time:87692ms step_avg:60.02ms
step:1462/2330 train_time:87754ms step_avg:60.02ms
step:1463/2330 train_time:87814ms step_avg:60.02ms
step:1464/2330 train_time:87875ms step_avg:60.02ms
step:1465/2330 train_time:87934ms step_avg:60.02ms
step:1466/2330 train_time:87996ms step_avg:60.02ms
step:1467/2330 train_time:88055ms step_avg:60.02ms
step:1468/2330 train_time:88117ms step_avg:60.02ms
step:1469/2330 train_time:88176ms step_avg:60.02ms
step:1470/2330 train_time:88238ms step_avg:60.03ms
step:1471/2330 train_time:88298ms step_avg:60.03ms
step:1472/2330 train_time:88360ms step_avg:60.03ms
step:1473/2330 train_time:88420ms step_avg:60.03ms
step:1474/2330 train_time:88481ms step_avg:60.03ms
step:1475/2330 train_time:88541ms step_avg:60.03ms
step:1476/2330 train_time:88602ms step_avg:60.03ms
step:1477/2330 train_time:88661ms step_avg:60.03ms
step:1478/2330 train_time:88723ms step_avg:60.03ms
step:1479/2330 train_time:88782ms step_avg:60.03ms
step:1480/2330 train_time:88843ms step_avg:60.03ms
step:1481/2330 train_time:88902ms step_avg:60.03ms
step:1482/2330 train_time:88963ms step_avg:60.03ms
step:1483/2330 train_time:89022ms step_avg:60.03ms
step:1484/2330 train_time:89083ms step_avg:60.03ms
step:1485/2330 train_time:89142ms step_avg:60.03ms
step:1486/2330 train_time:89204ms step_avg:60.03ms
step:1487/2330 train_time:89263ms step_avg:60.03ms
step:1488/2330 train_time:89324ms step_avg:60.03ms
step:1489/2330 train_time:89384ms step_avg:60.03ms
step:1490/2330 train_time:89445ms step_avg:60.03ms
step:1491/2330 train_time:89504ms step_avg:60.03ms
step:1492/2330 train_time:89565ms step_avg:60.03ms
step:1493/2330 train_time:89625ms step_avg:60.03ms
step:1494/2330 train_time:89687ms step_avg:60.03ms
step:1495/2330 train_time:89745ms step_avg:60.03ms
step:1496/2330 train_time:89806ms step_avg:60.03ms
step:1497/2330 train_time:89865ms step_avg:60.03ms
step:1498/2330 train_time:89926ms step_avg:60.03ms
step:1499/2330 train_time:89985ms step_avg:60.03ms
step:1500/2330 train_time:90046ms step_avg:60.03ms
step:1500/2330 val_loss:3.4510 train_time:90110ms step_avg:60.07ms
step:1501/2330 train_time:90130ms step_avg:60.05ms
step:1502/2330 train_time:90171ms step_avg:60.03ms
step:1503/2330 train_time:90234ms step_avg:60.04ms
step:1504/2330 train_time:90299ms step_avg:60.04ms
step:1505/2330 train_time:90359ms step_avg:60.04ms
step:1506/2330 train_time:90421ms step_avg:60.04ms
step:1507/2330 train_time:90479ms step_avg:60.04ms
step:1508/2330 train_time:90540ms step_avg:60.04ms
step:1509/2330 train_time:90598ms step_avg:60.04ms
step:1510/2330 train_time:90658ms step_avg:60.04ms
step:1511/2330 train_time:90717ms step_avg:60.04ms
step:1512/2330 train_time:90778ms step_avg:60.04ms
step:1513/2330 train_time:90836ms step_avg:60.04ms
step:1514/2330 train_time:90896ms step_avg:60.04ms
step:1515/2330 train_time:90955ms step_avg:60.04ms
step:1516/2330 train_time:91016ms step_avg:60.04ms
step:1517/2330 train_time:91076ms step_avg:60.04ms
step:1518/2330 train_time:91138ms step_avg:60.04ms
step:1519/2330 train_time:91199ms step_avg:60.04ms
step:1520/2330 train_time:91262ms step_avg:60.04ms
step:1521/2330 train_time:91322ms step_avg:60.04ms
step:1522/2330 train_time:91384ms step_avg:60.04ms
step:1523/2330 train_time:91443ms step_avg:60.04ms
step:1524/2330 train_time:91504ms step_avg:60.04ms
step:1525/2330 train_time:91563ms step_avg:60.04ms
step:1526/2330 train_time:91624ms step_avg:60.04ms
step:1527/2330 train_time:91682ms step_avg:60.04ms
step:1528/2330 train_time:91743ms step_avg:60.04ms
step:1529/2330 train_time:91802ms step_avg:60.04ms
step:1530/2330 train_time:91863ms step_avg:60.04ms
step:1531/2330 train_time:91923ms step_avg:60.04ms
step:1532/2330 train_time:91984ms step_avg:60.04ms
step:1533/2330 train_time:92043ms step_avg:60.04ms
step:1534/2330 train_time:92105ms step_avg:60.04ms
step:1535/2330 train_time:92165ms step_avg:60.04ms
step:1536/2330 train_time:92227ms step_avg:60.04ms
step:1537/2330 train_time:92287ms step_avg:60.04ms
step:1538/2330 train_time:92350ms step_avg:60.05ms
step:1539/2330 train_time:92410ms step_avg:60.05ms
step:1540/2330 train_time:92472ms step_avg:60.05ms
step:1541/2330 train_time:92533ms step_avg:60.05ms
step:1542/2330 train_time:92594ms step_avg:60.05ms
step:1543/2330 train_time:92654ms step_avg:60.05ms
step:1544/2330 train_time:92716ms step_avg:60.05ms
step:1545/2330 train_time:92776ms step_avg:60.05ms
step:1546/2330 train_time:92837ms step_avg:60.05ms
step:1547/2330 train_time:92897ms step_avg:60.05ms
step:1548/2330 train_time:92958ms step_avg:60.05ms
step:1549/2330 train_time:93017ms step_avg:60.05ms
step:1550/2330 train_time:93079ms step_avg:60.05ms
step:1551/2330 train_time:93139ms step_avg:60.05ms
step:1552/2330 train_time:93201ms step_avg:60.05ms
step:1553/2330 train_time:93261ms step_avg:60.05ms
step:1554/2330 train_time:93324ms step_avg:60.05ms
step:1555/2330 train_time:93385ms step_avg:60.05ms
step:1556/2330 train_time:93447ms step_avg:60.06ms
step:1557/2330 train_time:93507ms step_avg:60.06ms
step:1558/2330 train_time:93569ms step_avg:60.06ms
step:1559/2330 train_time:93629ms step_avg:60.06ms
step:1560/2330 train_time:93691ms step_avg:60.06ms
step:1561/2330 train_time:93751ms step_avg:60.06ms
step:1562/2330 train_time:93812ms step_avg:60.06ms
step:1563/2330 train_time:93873ms step_avg:60.06ms
step:1564/2330 train_time:93935ms step_avg:60.06ms
step:1565/2330 train_time:93995ms step_avg:60.06ms
step:1566/2330 train_time:94057ms step_avg:60.06ms
step:1567/2330 train_time:94117ms step_avg:60.06ms
step:1568/2330 train_time:94179ms step_avg:60.06ms
step:1569/2330 train_time:94238ms step_avg:60.06ms
step:1570/2330 train_time:94300ms step_avg:60.06ms
step:1571/2330 train_time:94360ms step_avg:60.06ms
step:1572/2330 train_time:94423ms step_avg:60.07ms
step:1573/2330 train_time:94483ms step_avg:60.07ms
step:1574/2330 train_time:94545ms step_avg:60.07ms
step:1575/2330 train_time:94605ms step_avg:60.07ms
step:1576/2330 train_time:94667ms step_avg:60.07ms
step:1577/2330 train_time:94726ms step_avg:60.07ms
step:1578/2330 train_time:94788ms step_avg:60.07ms
step:1579/2330 train_time:94847ms step_avg:60.07ms
step:1580/2330 train_time:94909ms step_avg:60.07ms
step:1581/2330 train_time:94970ms step_avg:60.07ms
step:1582/2330 train_time:95033ms step_avg:60.07ms
step:1583/2330 train_time:95093ms step_avg:60.07ms
step:1584/2330 train_time:95155ms step_avg:60.07ms
step:1585/2330 train_time:95215ms step_avg:60.07ms
step:1586/2330 train_time:95277ms step_avg:60.07ms
step:1587/2330 train_time:95337ms step_avg:60.07ms
step:1588/2330 train_time:95399ms step_avg:60.07ms
step:1589/2330 train_time:95459ms step_avg:60.07ms
step:1590/2330 train_time:95520ms step_avg:60.08ms
step:1591/2330 train_time:95580ms step_avg:60.08ms
step:1592/2330 train_time:95642ms step_avg:60.08ms
step:1593/2330 train_time:95702ms step_avg:60.08ms
step:1594/2330 train_time:95764ms step_avg:60.08ms
step:1595/2330 train_time:95824ms step_avg:60.08ms
step:1596/2330 train_time:95886ms step_avg:60.08ms
step:1597/2330 train_time:95946ms step_avg:60.08ms
step:1598/2330 train_time:96008ms step_avg:60.08ms
step:1599/2330 train_time:96068ms step_avg:60.08ms
step:1600/2330 train_time:96130ms step_avg:60.08ms
step:1601/2330 train_time:96190ms step_avg:60.08ms
step:1602/2330 train_time:96252ms step_avg:60.08ms
step:1603/2330 train_time:96312ms step_avg:60.08ms
step:1604/2330 train_time:96375ms step_avg:60.08ms
step:1605/2330 train_time:96436ms step_avg:60.08ms
step:1606/2330 train_time:96497ms step_avg:60.09ms
step:1607/2330 train_time:96558ms step_avg:60.09ms
step:1608/2330 train_time:96619ms step_avg:60.09ms
step:1609/2330 train_time:96678ms step_avg:60.09ms
step:1610/2330 train_time:96740ms step_avg:60.09ms
step:1611/2330 train_time:96800ms step_avg:60.09ms
step:1612/2330 train_time:96862ms step_avg:60.09ms
step:1613/2330 train_time:96922ms step_avg:60.09ms
step:1614/2330 train_time:96984ms step_avg:60.09ms
step:1615/2330 train_time:97044ms step_avg:60.09ms
step:1616/2330 train_time:97106ms step_avg:60.09ms
step:1617/2330 train_time:97166ms step_avg:60.09ms
step:1618/2330 train_time:97228ms step_avg:60.09ms
step:1619/2330 train_time:97287ms step_avg:60.09ms
step:1620/2330 train_time:97349ms step_avg:60.09ms
step:1621/2330 train_time:97408ms step_avg:60.09ms
step:1622/2330 train_time:97471ms step_avg:60.09ms
step:1623/2330 train_time:97530ms step_avg:60.09ms
step:1624/2330 train_time:97593ms step_avg:60.09ms
step:1625/2330 train_time:97653ms step_avg:60.09ms
step:1626/2330 train_time:97716ms step_avg:60.10ms
step:1627/2330 train_time:97776ms step_avg:60.10ms
step:1628/2330 train_time:97837ms step_avg:60.10ms
step:1629/2330 train_time:97897ms step_avg:60.10ms
step:1630/2330 train_time:97959ms step_avg:60.10ms
step:1631/2330 train_time:98019ms step_avg:60.10ms
step:1632/2330 train_time:98081ms step_avg:60.10ms
step:1633/2330 train_time:98141ms step_avg:60.10ms
step:1634/2330 train_time:98203ms step_avg:60.10ms
step:1635/2330 train_time:98263ms step_avg:60.10ms
step:1636/2330 train_time:98325ms step_avg:60.10ms
step:1637/2330 train_time:98384ms step_avg:60.10ms
step:1638/2330 train_time:98446ms step_avg:60.10ms
step:1639/2330 train_time:98505ms step_avg:60.10ms
step:1640/2330 train_time:98567ms step_avg:60.10ms
step:1641/2330 train_time:98627ms step_avg:60.10ms
step:1642/2330 train_time:98689ms step_avg:60.10ms
step:1643/2330 train_time:98748ms step_avg:60.10ms
step:1644/2330 train_time:98811ms step_avg:60.10ms
step:1645/2330 train_time:98871ms step_avg:60.10ms
step:1646/2330 train_time:98934ms step_avg:60.11ms
step:1647/2330 train_time:98994ms step_avg:60.11ms
step:1648/2330 train_time:99056ms step_avg:60.11ms
step:1649/2330 train_time:99117ms step_avg:60.11ms
step:1650/2330 train_time:99178ms step_avg:60.11ms
step:1651/2330 train_time:99238ms step_avg:60.11ms
step:1652/2330 train_time:99299ms step_avg:60.11ms
step:1653/2330 train_time:99359ms step_avg:60.11ms
step:1654/2330 train_time:99421ms step_avg:60.11ms
step:1655/2330 train_time:99481ms step_avg:60.11ms
step:1656/2330 train_time:99543ms step_avg:60.11ms
step:1657/2330 train_time:99603ms step_avg:60.11ms
step:1658/2330 train_time:99665ms step_avg:60.11ms
step:1659/2330 train_time:99724ms step_avg:60.11ms
step:1660/2330 train_time:99786ms step_avg:60.11ms
step:1661/2330 train_time:99846ms step_avg:60.11ms
step:1662/2330 train_time:99907ms step_avg:60.11ms
step:1663/2330 train_time:99967ms step_avg:60.11ms
step:1664/2330 train_time:100030ms step_avg:60.11ms
step:1665/2330 train_time:100090ms step_avg:60.11ms
step:1666/2330 train_time:100152ms step_avg:60.12ms
step:1667/2330 train_time:100212ms step_avg:60.12ms
step:1668/2330 train_time:100274ms step_avg:60.12ms
step:1669/2330 train_time:100334ms step_avg:60.12ms
step:1670/2330 train_time:100396ms step_avg:60.12ms
step:1671/2330 train_time:100457ms step_avg:60.12ms
step:1672/2330 train_time:100518ms step_avg:60.12ms
step:1673/2330 train_time:100578ms step_avg:60.12ms
step:1674/2330 train_time:100640ms step_avg:60.12ms
step:1675/2330 train_time:100699ms step_avg:60.12ms
step:1676/2330 train_time:100761ms step_avg:60.12ms
step:1677/2330 train_time:100821ms step_avg:60.12ms
step:1678/2330 train_time:100883ms step_avg:60.12ms
step:1679/2330 train_time:100943ms step_avg:60.12ms
step:1680/2330 train_time:101005ms step_avg:60.12ms
step:1681/2330 train_time:101065ms step_avg:60.12ms
step:1682/2330 train_time:101127ms step_avg:60.12ms
step:1683/2330 train_time:101186ms step_avg:60.12ms
step:1684/2330 train_time:101248ms step_avg:60.12ms
step:1685/2330 train_time:101308ms step_avg:60.12ms
step:1686/2330 train_time:101370ms step_avg:60.12ms
step:1687/2330 train_time:101431ms step_avg:60.12ms
step:1688/2330 train_time:101493ms step_avg:60.13ms
step:1689/2330 train_time:101553ms step_avg:60.13ms
step:1690/2330 train_time:101615ms step_avg:60.13ms
step:1691/2330 train_time:101675ms step_avg:60.13ms
step:1692/2330 train_time:101737ms step_avg:60.13ms
step:1693/2330 train_time:101797ms step_avg:60.13ms
step:1694/2330 train_time:101859ms step_avg:60.13ms
step:1695/2330 train_time:101919ms step_avg:60.13ms
step:1696/2330 train_time:101981ms step_avg:60.13ms
step:1697/2330 train_time:102041ms step_avg:60.13ms
step:1698/2330 train_time:102103ms step_avg:60.13ms
step:1699/2330 train_time:102163ms step_avg:60.13ms
step:1700/2330 train_time:102225ms step_avg:60.13ms
step:1701/2330 train_time:102285ms step_avg:60.13ms
step:1702/2330 train_time:102346ms step_avg:60.13ms
step:1703/2330 train_time:102406ms step_avg:60.13ms
step:1704/2330 train_time:102467ms step_avg:60.13ms
step:1705/2330 train_time:102527ms step_avg:60.13ms
step:1706/2330 train_time:102589ms step_avg:60.13ms
step:1707/2330 train_time:102649ms step_avg:60.13ms
step:1708/2330 train_time:102711ms step_avg:60.14ms
step:1709/2330 train_time:102772ms step_avg:60.14ms
step:1710/2330 train_time:102834ms step_avg:60.14ms
step:1711/2330 train_time:102895ms step_avg:60.14ms
step:1712/2330 train_time:102957ms step_avg:60.14ms
step:1713/2330 train_time:103017ms step_avg:60.14ms
step:1714/2330 train_time:103079ms step_avg:60.14ms
step:1715/2330 train_time:103138ms step_avg:60.14ms
step:1716/2330 train_time:103200ms step_avg:60.14ms
step:1717/2330 train_time:103260ms step_avg:60.14ms
step:1718/2330 train_time:103322ms step_avg:60.14ms
step:1719/2330 train_time:103382ms step_avg:60.14ms
step:1720/2330 train_time:103444ms step_avg:60.14ms
step:1721/2330 train_time:103504ms step_avg:60.14ms
step:1722/2330 train_time:103566ms step_avg:60.14ms
step:1723/2330 train_time:103625ms step_avg:60.14ms
step:1724/2330 train_time:103687ms step_avg:60.14ms
step:1725/2330 train_time:103747ms step_avg:60.14ms
step:1726/2330 train_time:103809ms step_avg:60.14ms
step:1727/2330 train_time:103869ms step_avg:60.14ms
step:1728/2330 train_time:103932ms step_avg:60.15ms
step:1729/2330 train_time:103992ms step_avg:60.15ms
step:1730/2330 train_time:104054ms step_avg:60.15ms
step:1731/2330 train_time:104113ms step_avg:60.15ms
step:1732/2330 train_time:104176ms step_avg:60.15ms
step:1733/2330 train_time:104235ms step_avg:60.15ms
step:1734/2330 train_time:104297ms step_avg:60.15ms
step:1735/2330 train_time:104358ms step_avg:60.15ms
step:1736/2330 train_time:104419ms step_avg:60.15ms
step:1737/2330 train_time:104479ms step_avg:60.15ms
step:1738/2330 train_time:104541ms step_avg:60.15ms
step:1739/2330 train_time:104601ms step_avg:60.15ms
step:1740/2330 train_time:104663ms step_avg:60.15ms
step:1741/2330 train_time:104724ms step_avg:60.15ms
step:1742/2330 train_time:104786ms step_avg:60.15ms
step:1743/2330 train_time:104846ms step_avg:60.15ms
step:1744/2330 train_time:104908ms step_avg:60.15ms
step:1745/2330 train_time:104968ms step_avg:60.15ms
step:1746/2330 train_time:105029ms step_avg:60.15ms
step:1747/2330 train_time:105089ms step_avg:60.15ms
step:1748/2330 train_time:105151ms step_avg:60.15ms
step:1749/2330 train_time:105212ms step_avg:60.16ms
step:1750/2330 train_time:105274ms step_avg:60.16ms
step:1750/2330 val_loss:3.3818 train_time:105338ms step_avg:60.19ms
step:1751/2330 train_time:105359ms step_avg:60.17ms
step:1752/2330 train_time:105399ms step_avg:60.16ms
step:1753/2330 train_time:105459ms step_avg:60.16ms
step:1754/2330 train_time:105523ms step_avg:60.16ms
step:1755/2330 train_time:105585ms step_avg:60.16ms
step:1756/2330 train_time:105648ms step_avg:60.16ms
step:1757/2330 train_time:105707ms step_avg:60.16ms
step:1758/2330 train_time:105768ms step_avg:60.16ms
step:1759/2330 train_time:105827ms step_avg:60.16ms
step:1760/2330 train_time:105888ms step_avg:60.16ms
step:1761/2330 train_time:105946ms step_avg:60.16ms
step:1762/2330 train_time:106008ms step_avg:60.16ms
step:1763/2330 train_time:106067ms step_avg:60.16ms
step:1764/2330 train_time:106128ms step_avg:60.16ms
step:1765/2330 train_time:106188ms step_avg:60.16ms
step:1766/2330 train_time:106251ms step_avg:60.16ms
step:1767/2330 train_time:106316ms step_avg:60.17ms
step:1768/2330 train_time:106379ms step_avg:60.17ms
step:1769/2330 train_time:106439ms step_avg:60.17ms
step:1770/2330 train_time:106501ms step_avg:60.17ms
step:1771/2330 train_time:106561ms step_avg:60.17ms
step:1772/2330 train_time:106623ms step_avg:60.17ms
step:1773/2330 train_time:106683ms step_avg:60.17ms
step:1774/2330 train_time:106745ms step_avg:60.17ms
step:1775/2330 train_time:106805ms step_avg:60.17ms
step:1776/2330 train_time:106867ms step_avg:60.17ms
step:1777/2330 train_time:106926ms step_avg:60.17ms
step:1778/2330 train_time:106988ms step_avg:60.17ms
step:1779/2330 train_time:107047ms step_avg:60.17ms
step:1780/2330 train_time:107108ms step_avg:60.17ms
step:1781/2330 train_time:107168ms step_avg:60.17ms
step:1782/2330 train_time:107231ms step_avg:60.17ms
step:1783/2330 train_time:107293ms step_avg:60.18ms
step:1784/2330 train_time:107355ms step_avg:60.18ms
step:1785/2330 train_time:107415ms step_avg:60.18ms
step:1786/2330 train_time:107477ms step_avg:60.18ms
step:1787/2330 train_time:107536ms step_avg:60.18ms
step:1788/2330 train_time:107598ms step_avg:60.18ms
step:1789/2330 train_time:107659ms step_avg:60.18ms
step:1790/2330 train_time:107720ms step_avg:60.18ms
step:1791/2330 train_time:107780ms step_avg:60.18ms
step:1792/2330 train_time:107841ms step_avg:60.18ms
step:1793/2330 train_time:107901ms step_avg:60.18ms
step:1794/2330 train_time:107963ms step_avg:60.18ms
step:1795/2330 train_time:108023ms step_avg:60.18ms
step:1796/2330 train_time:108084ms step_avg:60.18ms
step:1797/2330 train_time:108144ms step_avg:60.18ms
step:1798/2330 train_time:108206ms step_avg:60.18ms
step:1799/2330 train_time:108267ms step_avg:60.18ms
step:1800/2330 train_time:108330ms step_avg:60.18ms
step:1801/2330 train_time:108392ms step_avg:60.18ms
step:1802/2330 train_time:108454ms step_avg:60.19ms
step:1803/2330 train_time:108513ms step_avg:60.18ms
step:1804/2330 train_time:108575ms step_avg:60.19ms
step:1805/2330 train_time:108634ms step_avg:60.19ms
step:1806/2330 train_time:108696ms step_avg:60.19ms
step:1807/2330 train_time:108755ms step_avg:60.19ms
step:1808/2330 train_time:108817ms step_avg:60.19ms
step:1809/2330 train_time:108876ms step_avg:60.19ms
step:1810/2330 train_time:108938ms step_avg:60.19ms
step:1811/2330 train_time:108998ms step_avg:60.19ms
step:1812/2330 train_time:109060ms step_avg:60.19ms
step:1813/2330 train_time:109120ms step_avg:60.19ms
step:1814/2330 train_time:109182ms step_avg:60.19ms
step:1815/2330 train_time:109242ms step_avg:60.19ms
step:1816/2330 train_time:109305ms step_avg:60.19ms
step:1817/2330 train_time:109365ms step_avg:60.19ms
step:1818/2330 train_time:109428ms step_avg:60.19ms
step:1819/2330 train_time:109489ms step_avg:60.19ms
step:1820/2330 train_time:109551ms step_avg:60.19ms
step:1821/2330 train_time:109611ms step_avg:60.19ms
step:1822/2330 train_time:109672ms step_avg:60.19ms
step:1823/2330 train_time:109732ms step_avg:60.19ms
step:1824/2330 train_time:109793ms step_avg:60.19ms
step:1825/2330 train_time:109852ms step_avg:60.19ms
step:1826/2330 train_time:109914ms step_avg:60.19ms
step:1827/2330 train_time:109973ms step_avg:60.19ms
step:1828/2330 train_time:110036ms step_avg:60.19ms
step:1829/2330 train_time:110096ms step_avg:60.19ms
step:1830/2330 train_time:110159ms step_avg:60.20ms
step:1831/2330 train_time:110219ms step_avg:60.20ms
step:1832/2330 train_time:110281ms step_avg:60.20ms
step:1833/2330 train_time:110340ms step_avg:60.20ms
step:1834/2330 train_time:110401ms step_avg:60.20ms
step:1835/2330 train_time:110461ms step_avg:60.20ms
step:1836/2330 train_time:110524ms step_avg:60.20ms
step:1837/2330 train_time:110584ms step_avg:60.20ms
step:1838/2330 train_time:110646ms step_avg:60.20ms
step:1839/2330 train_time:110706ms step_avg:60.20ms
step:1840/2330 train_time:110768ms step_avg:60.20ms
step:1841/2330 train_time:110828ms step_avg:60.20ms
step:1842/2330 train_time:110890ms step_avg:60.20ms
step:1843/2330 train_time:110950ms step_avg:60.20ms
step:1844/2330 train_time:111012ms step_avg:60.20ms
step:1845/2330 train_time:111072ms step_avg:60.20ms
step:1846/2330 train_time:111133ms step_avg:60.20ms
step:1847/2330 train_time:111192ms step_avg:60.20ms
step:1848/2330 train_time:111255ms step_avg:60.20ms
step:1849/2330 train_time:111315ms step_avg:60.20ms
step:1850/2330 train_time:111377ms step_avg:60.20ms
step:1851/2330 train_time:111437ms step_avg:60.20ms
step:1852/2330 train_time:111499ms step_avg:60.20ms
step:1853/2330 train_time:111558ms step_avg:60.20ms
step:1854/2330 train_time:111620ms step_avg:60.21ms
step:1855/2330 train_time:111681ms step_avg:60.21ms
step:1856/2330 train_time:111742ms step_avg:60.21ms
step:1857/2330 train_time:111802ms step_avg:60.21ms
step:1858/2330 train_time:111864ms step_avg:60.21ms
step:1859/2330 train_time:111924ms step_avg:60.21ms
step:1860/2330 train_time:111986ms step_avg:60.21ms
step:1861/2330 train_time:112046ms step_avg:60.21ms
step:1862/2330 train_time:112109ms step_avg:60.21ms
step:1863/2330 train_time:112169ms step_avg:60.21ms
step:1864/2330 train_time:112231ms step_avg:60.21ms
step:1865/2330 train_time:112291ms step_avg:60.21ms
step:1866/2330 train_time:112353ms step_avg:60.21ms
step:1867/2330 train_time:112412ms step_avg:60.21ms
step:1868/2330 train_time:112474ms step_avg:60.21ms
step:1869/2330 train_time:112534ms step_avg:60.21ms
step:1870/2330 train_time:112596ms step_avg:60.21ms
step:1871/2330 train_time:112656ms step_avg:60.21ms
step:1872/2330 train_time:112718ms step_avg:60.21ms
step:1873/2330 train_time:112778ms step_avg:60.21ms
step:1874/2330 train_time:112840ms step_avg:60.21ms
step:1875/2330 train_time:112899ms step_avg:60.21ms
step:1876/2330 train_time:112961ms step_avg:60.21ms
step:1877/2330 train_time:113021ms step_avg:60.21ms
step:1878/2330 train_time:113082ms step_avg:60.21ms
step:1879/2330 train_time:113143ms step_avg:60.21ms
step:1880/2330 train_time:113206ms step_avg:60.22ms
step:1881/2330 train_time:113266ms step_avg:60.22ms
step:1882/2330 train_time:113328ms step_avg:60.22ms
step:1883/2330 train_time:113389ms step_avg:60.22ms
step:1884/2330 train_time:113450ms step_avg:60.22ms
step:1885/2330 train_time:113510ms step_avg:60.22ms
step:1886/2330 train_time:113572ms step_avg:60.22ms
step:1887/2330 train_time:113631ms step_avg:60.22ms
step:1888/2330 train_time:113693ms step_avg:60.22ms
step:1889/2330 train_time:113753ms step_avg:60.22ms
step:1890/2330 train_time:113814ms step_avg:60.22ms
step:1891/2330 train_time:113874ms step_avg:60.22ms
step:1892/2330 train_time:113936ms step_avg:60.22ms
step:1893/2330 train_time:113996ms step_avg:60.22ms
step:1894/2330 train_time:114058ms step_avg:60.22ms
step:1895/2330 train_time:114118ms step_avg:60.22ms
step:1896/2330 train_time:114180ms step_avg:60.22ms
step:1897/2330 train_time:114240ms step_avg:60.22ms
step:1898/2330 train_time:114302ms step_avg:60.22ms
step:1899/2330 train_time:114362ms step_avg:60.22ms
step:1900/2330 train_time:114423ms step_avg:60.22ms
step:1901/2330 train_time:114484ms step_avg:60.22ms
step:1902/2330 train_time:114546ms step_avg:60.22ms
step:1903/2330 train_time:114606ms step_avg:60.22ms
step:1904/2330 train_time:114669ms step_avg:60.23ms
step:1905/2330 train_time:114729ms step_avg:60.23ms
step:1906/2330 train_time:114792ms step_avg:60.23ms
step:1907/2330 train_time:114851ms step_avg:60.23ms
step:1908/2330 train_time:114913ms step_avg:60.23ms
step:1909/2330 train_time:114972ms step_avg:60.23ms
step:1910/2330 train_time:115033ms step_avg:60.23ms
step:1911/2330 train_time:115093ms step_avg:60.23ms
step:1912/2330 train_time:115156ms step_avg:60.23ms
step:1913/2330 train_time:115216ms step_avg:60.23ms
step:1914/2330 train_time:115278ms step_avg:60.23ms
step:1915/2330 train_time:115338ms step_avg:60.23ms
step:1916/2330 train_time:115399ms step_avg:60.23ms
step:1917/2330 train_time:115459ms step_avg:60.23ms
step:1918/2330 train_time:115521ms step_avg:60.23ms
step:1919/2330 train_time:115580ms step_avg:60.23ms
step:1920/2330 train_time:115643ms step_avg:60.23ms
step:1921/2330 train_time:115703ms step_avg:60.23ms
step:1922/2330 train_time:115766ms step_avg:60.23ms
step:1923/2330 train_time:115826ms step_avg:60.23ms
step:1924/2330 train_time:115888ms step_avg:60.23ms
step:1925/2330 train_time:115948ms step_avg:60.23ms
step:1926/2330 train_time:116010ms step_avg:60.23ms
step:1927/2330 train_time:116070ms step_avg:60.23ms
step:1928/2330 train_time:116132ms step_avg:60.23ms
step:1929/2330 train_time:116192ms step_avg:60.23ms
step:1930/2330 train_time:116254ms step_avg:60.24ms
step:1931/2330 train_time:116314ms step_avg:60.24ms
step:1932/2330 train_time:116375ms step_avg:60.24ms
step:1933/2330 train_time:116436ms step_avg:60.24ms
step:1934/2330 train_time:116498ms step_avg:60.24ms
step:1935/2330 train_time:116558ms step_avg:60.24ms
step:1936/2330 train_time:116620ms step_avg:60.24ms
step:1937/2330 train_time:116680ms step_avg:60.24ms
step:1938/2330 train_time:116741ms step_avg:60.24ms
step:1939/2330 train_time:116802ms step_avg:60.24ms
step:1940/2330 train_time:116863ms step_avg:60.24ms
step:1941/2330 train_time:116922ms step_avg:60.24ms
step:1942/2330 train_time:116985ms step_avg:60.24ms
step:1943/2330 train_time:117045ms step_avg:60.24ms
step:1944/2330 train_time:117108ms step_avg:60.24ms
step:1945/2330 train_time:117168ms step_avg:60.24ms
step:1946/2330 train_time:117230ms step_avg:60.24ms
step:1947/2330 train_time:117290ms step_avg:60.24ms
step:1948/2330 train_time:117352ms step_avg:60.24ms
step:1949/2330 train_time:117411ms step_avg:60.24ms
step:1950/2330 train_time:117474ms step_avg:60.24ms
step:1951/2330 train_time:117532ms step_avg:60.24ms
step:1952/2330 train_time:117595ms step_avg:60.24ms
step:1953/2330 train_time:117654ms step_avg:60.24ms
step:1954/2330 train_time:117717ms step_avg:60.24ms
step:1955/2330 train_time:117777ms step_avg:60.24ms
step:1956/2330 train_time:117838ms step_avg:60.24ms
step:1957/2330 train_time:117898ms step_avg:60.24ms
step:1958/2330 train_time:117960ms step_avg:60.25ms
step:1959/2330 train_time:118020ms step_avg:60.25ms
step:1960/2330 train_time:118082ms step_avg:60.25ms
step:1961/2330 train_time:118142ms step_avg:60.25ms
step:1962/2330 train_time:118204ms step_avg:60.25ms
step:1963/2330 train_time:118264ms step_avg:60.25ms
step:1964/2330 train_time:118327ms step_avg:60.25ms
step:1965/2330 train_time:118387ms step_avg:60.25ms
step:1966/2330 train_time:118449ms step_avg:60.25ms
step:1967/2330 train_time:118510ms step_avg:60.25ms
step:1968/2330 train_time:118572ms step_avg:60.25ms
step:1969/2330 train_time:118632ms step_avg:60.25ms
step:1970/2330 train_time:118694ms step_avg:60.25ms
step:1971/2330 train_time:118753ms step_avg:60.25ms
step:1972/2330 train_time:118816ms step_avg:60.25ms
step:1973/2330 train_time:118875ms step_avg:60.25ms
step:1974/2330 train_time:118937ms step_avg:60.25ms
step:1975/2330 train_time:118996ms step_avg:60.25ms
step:1976/2330 train_time:119059ms step_avg:60.25ms
step:1977/2330 train_time:119119ms step_avg:60.25ms
step:1978/2330 train_time:119181ms step_avg:60.25ms
step:1979/2330 train_time:119240ms step_avg:60.25ms
step:1980/2330 train_time:119302ms step_avg:60.25ms
step:1981/2330 train_time:119361ms step_avg:60.25ms
step:1982/2330 train_time:119424ms step_avg:60.25ms
step:1983/2330 train_time:119484ms step_avg:60.25ms
step:1984/2330 train_time:119547ms step_avg:60.26ms
step:1985/2330 train_time:119608ms step_avg:60.26ms
step:1986/2330 train_time:119670ms step_avg:60.26ms
step:1987/2330 train_time:119730ms step_avg:60.26ms
step:1988/2330 train_time:119792ms step_avg:60.26ms
step:1989/2330 train_time:119851ms step_avg:60.26ms
step:1990/2330 train_time:119913ms step_avg:60.26ms
step:1991/2330 train_time:119973ms step_avg:60.26ms
step:1992/2330 train_time:120034ms step_avg:60.26ms
step:1993/2330 train_time:120094ms step_avg:60.26ms
step:1994/2330 train_time:120157ms step_avg:60.26ms
step:1995/2330 train_time:120217ms step_avg:60.26ms
step:1996/2330 train_time:120279ms step_avg:60.26ms
step:1997/2330 train_time:120339ms step_avg:60.26ms
step:1998/2330 train_time:120401ms step_avg:60.26ms
step:1999/2330 train_time:120461ms step_avg:60.26ms
step:2000/2330 train_time:120522ms step_avg:60.26ms
step:2000/2330 val_loss:3.3309 train_time:120586ms step_avg:60.29ms
step:2001/2330 train_time:120607ms step_avg:60.27ms
step:2002/2330 train_time:120647ms step_avg:60.26ms
step:2003/2330 train_time:120712ms step_avg:60.27ms
step:2004/2330 train_time:120776ms step_avg:60.27ms
step:2005/2330 train_time:120836ms step_avg:60.27ms
step:2006/2330 train_time:120898ms step_avg:60.27ms
step:2007/2330 train_time:120957ms step_avg:60.27ms
step:2008/2330 train_time:121018ms step_avg:60.27ms
step:2009/2330 train_time:121076ms step_avg:60.27ms
step:2010/2330 train_time:121138ms step_avg:60.27ms
step:2011/2330 train_time:121197ms step_avg:60.27ms
step:2012/2330 train_time:121258ms step_avg:60.27ms
step:2013/2330 train_time:121316ms step_avg:60.27ms
step:2014/2330 train_time:121378ms step_avg:60.27ms
step:2015/2330 train_time:121437ms step_avg:60.27ms
step:2016/2330 train_time:121498ms step_avg:60.27ms
step:2017/2330 train_time:121559ms step_avg:60.27ms
step:2018/2330 train_time:121623ms step_avg:60.27ms
step:2019/2330 train_time:121686ms step_avg:60.27ms
step:2020/2330 train_time:121748ms step_avg:60.27ms
step:2021/2330 train_time:121809ms step_avg:60.27ms
step:2022/2330 train_time:121871ms step_avg:60.27ms
step:2023/2330 train_time:121931ms step_avg:60.27ms
step:2024/2330 train_time:121992ms step_avg:60.27ms
step:2025/2330 train_time:122052ms step_avg:60.27ms
step:2026/2330 train_time:122114ms step_avg:60.27ms
step:2027/2330 train_time:122173ms step_avg:60.27ms
step:2028/2330 train_time:122234ms step_avg:60.27ms
step:2029/2330 train_time:122294ms step_avg:60.27ms
step:2030/2330 train_time:122355ms step_avg:60.27ms
step:2031/2330 train_time:122414ms step_avg:60.27ms
step:2032/2330 train_time:122476ms step_avg:60.27ms
step:2033/2330 train_time:122536ms step_avg:60.27ms
step:2034/2330 train_time:122599ms step_avg:60.27ms
step:2035/2330 train_time:122659ms step_avg:60.27ms
step:2036/2330 train_time:122722ms step_avg:60.28ms
step:2037/2330 train_time:122782ms step_avg:60.28ms
step:2038/2330 train_time:122844ms step_avg:60.28ms
step:2039/2330 train_time:122904ms step_avg:60.28ms
step:2040/2330 train_time:122966ms step_avg:60.28ms
step:2041/2330 train_time:123026ms step_avg:60.28ms
step:2042/2330 train_time:123087ms step_avg:60.28ms
step:2043/2330 train_time:123147ms step_avg:60.28ms
step:2044/2330 train_time:123208ms step_avg:60.28ms
step:2045/2330 train_time:123267ms step_avg:60.28ms
step:2046/2330 train_time:123329ms step_avg:60.28ms
step:2047/2330 train_time:123388ms step_avg:60.28ms
step:2048/2330 train_time:123451ms step_avg:60.28ms
step:2049/2330 train_time:123511ms step_avg:60.28ms
step:2050/2330 train_time:123573ms step_avg:60.28ms
step:2051/2330 train_time:123635ms step_avg:60.28ms
step:2052/2330 train_time:123697ms step_avg:60.28ms
step:2053/2330 train_time:123757ms step_avg:60.28ms
step:2054/2330 train_time:123820ms step_avg:60.28ms
step:2055/2330 train_time:123879ms step_avg:60.28ms
step:2056/2330 train_time:123941ms step_avg:60.28ms
step:2057/2330 train_time:124000ms step_avg:60.28ms
step:2058/2330 train_time:124062ms step_avg:60.28ms
step:2059/2330 train_time:124122ms step_avg:60.28ms
step:2060/2330 train_time:124184ms step_avg:60.28ms
step:2061/2330 train_time:124243ms step_avg:60.28ms
step:2062/2330 train_time:124305ms step_avg:60.28ms
step:2063/2330 train_time:124364ms step_avg:60.28ms
step:2064/2330 train_time:124426ms step_avg:60.28ms
step:2065/2330 train_time:124486ms step_avg:60.28ms
step:2066/2330 train_time:124548ms step_avg:60.28ms
step:2067/2330 train_time:124607ms step_avg:60.28ms
step:2068/2330 train_time:124670ms step_avg:60.29ms
step:2069/2330 train_time:124730ms step_avg:60.29ms
step:2070/2330 train_time:124792ms step_avg:60.29ms
step:2071/2330 train_time:124853ms step_avg:60.29ms
step:2072/2330 train_time:124916ms step_avg:60.29ms
step:2073/2330 train_time:124976ms step_avg:60.29ms
step:2074/2330 train_time:125039ms step_avg:60.29ms
step:2075/2330 train_time:125099ms step_avg:60.29ms
step:2076/2330 train_time:125161ms step_avg:60.29ms
step:2077/2330 train_time:125220ms step_avg:60.29ms
step:2078/2330 train_time:125282ms step_avg:60.29ms
step:2079/2330 train_time:125342ms step_avg:60.29ms
step:2080/2330 train_time:125403ms step_avg:60.29ms
step:2081/2330 train_time:125462ms step_avg:60.29ms
step:2082/2330 train_time:125525ms step_avg:60.29ms
step:2083/2330 train_time:125585ms step_avg:60.29ms
step:2084/2330 train_time:125647ms step_avg:60.29ms
step:2085/2330 train_time:125706ms step_avg:60.29ms
step:2086/2330 train_time:125768ms step_avg:60.29ms
step:2087/2330 train_time:125828ms step_avg:60.29ms
step:2088/2330 train_time:125891ms step_avg:60.29ms
step:2089/2330 train_time:125951ms step_avg:60.29ms
step:2090/2330 train_time:126013ms step_avg:60.29ms
step:2091/2330 train_time:126074ms step_avg:60.29ms
step:2092/2330 train_time:126136ms step_avg:60.29ms
step:2093/2330 train_time:126196ms step_avg:60.29ms
step:2094/2330 train_time:126259ms step_avg:60.30ms
step:2095/2330 train_time:126319ms step_avg:60.30ms
step:2096/2330 train_time:126380ms step_avg:60.30ms
step:2097/2330 train_time:126439ms step_avg:60.30ms
step:2098/2330 train_time:126501ms step_avg:60.30ms
step:2099/2330 train_time:126560ms step_avg:60.30ms
step:2100/2330 train_time:126622ms step_avg:60.30ms
step:2101/2330 train_time:126681ms step_avg:60.30ms
step:2102/2330 train_time:126743ms step_avg:60.30ms
step:2103/2330 train_time:126804ms step_avg:60.30ms
step:2104/2330 train_time:126865ms step_avg:60.30ms
step:2105/2330 train_time:126925ms step_avg:60.30ms
step:2106/2330 train_time:126987ms step_avg:60.30ms
step:2107/2330 train_time:127047ms step_avg:60.30ms
step:2108/2330 train_time:127109ms step_avg:60.30ms
step:2109/2330 train_time:127169ms step_avg:60.30ms
step:2110/2330 train_time:127231ms step_avg:60.30ms
step:2111/2330 train_time:127292ms step_avg:60.30ms
step:2112/2330 train_time:127354ms step_avg:60.30ms
step:2113/2330 train_time:127414ms step_avg:60.30ms
step:2114/2330 train_time:127476ms step_avg:60.30ms
step:2115/2330 train_time:127536ms step_avg:60.30ms
step:2116/2330 train_time:127598ms step_avg:60.30ms
step:2117/2330 train_time:127658ms step_avg:60.30ms
step:2118/2330 train_time:127720ms step_avg:60.30ms
step:2119/2330 train_time:127780ms step_avg:60.30ms
step:2120/2330 train_time:127841ms step_avg:60.30ms
step:2121/2330 train_time:127901ms step_avg:60.30ms
step:2122/2330 train_time:127963ms step_avg:60.30ms
step:2123/2330 train_time:128023ms step_avg:60.30ms
step:2124/2330 train_time:128085ms step_avg:60.30ms
step:2125/2330 train_time:128145ms step_avg:60.30ms
step:2126/2330 train_time:128206ms step_avg:60.30ms
step:2127/2330 train_time:128266ms step_avg:60.30ms
step:2128/2330 train_time:128327ms step_avg:60.30ms
step:2129/2330 train_time:128387ms step_avg:60.30ms
step:2130/2330 train_time:128449ms step_avg:60.30ms
step:2131/2330 train_time:128509ms step_avg:60.30ms
step:2132/2330 train_time:128571ms step_avg:60.31ms
step:2133/2330 train_time:128632ms step_avg:60.31ms
step:2134/2330 train_time:128694ms step_avg:60.31ms
step:2135/2330 train_time:128754ms step_avg:60.31ms
step:2136/2330 train_time:128816ms step_avg:60.31ms
step:2137/2330 train_time:128877ms step_avg:60.31ms
step:2138/2330 train_time:128938ms step_avg:60.31ms
step:2139/2330 train_time:128998ms step_avg:60.31ms
step:2140/2330 train_time:129060ms step_avg:60.31ms
step:2141/2330 train_time:129120ms step_avg:60.31ms
step:2142/2330 train_time:129182ms step_avg:60.31ms
step:2143/2330 train_time:129242ms step_avg:60.31ms
step:2144/2330 train_time:129304ms step_avg:60.31ms
step:2145/2330 train_time:129364ms step_avg:60.31ms
step:2146/2330 train_time:129426ms step_avg:60.31ms
step:2147/2330 train_time:129486ms step_avg:60.31ms
step:2148/2330 train_time:129548ms step_avg:60.31ms
step:2149/2330 train_time:129607ms step_avg:60.31ms
step:2150/2330 train_time:129669ms step_avg:60.31ms
step:2151/2330 train_time:129729ms step_avg:60.31ms
step:2152/2330 train_time:129792ms step_avg:60.31ms
step:2153/2330 train_time:129852ms step_avg:60.31ms
step:2154/2330 train_time:129915ms step_avg:60.31ms
step:2155/2330 train_time:129975ms step_avg:60.31ms
step:2156/2330 train_time:130037ms step_avg:60.31ms
step:2157/2330 train_time:130097ms step_avg:60.31ms
step:2158/2330 train_time:130158ms step_avg:60.31ms
step:2159/2330 train_time:130218ms step_avg:60.31ms
step:2160/2330 train_time:130280ms step_avg:60.31ms
step:2161/2330 train_time:130339ms step_avg:60.31ms
step:2162/2330 train_time:130401ms step_avg:60.32ms
step:2163/2330 train_time:130461ms step_avg:60.31ms
step:2164/2330 train_time:130522ms step_avg:60.32ms
step:2165/2330 train_time:130582ms step_avg:60.31ms
step:2166/2330 train_time:130644ms step_avg:60.32ms
step:2167/2330 train_time:130704ms step_avg:60.32ms
step:2168/2330 train_time:130766ms step_avg:60.32ms
step:2169/2330 train_time:130826ms step_avg:60.32ms
step:2170/2330 train_time:130888ms step_avg:60.32ms
step:2171/2330 train_time:130948ms step_avg:60.32ms
step:2172/2330 train_time:131010ms step_avg:60.32ms
step:2173/2330 train_time:131070ms step_avg:60.32ms
step:2174/2330 train_time:131132ms step_avg:60.32ms
step:2175/2330 train_time:131192ms step_avg:60.32ms
step:2176/2330 train_time:131254ms step_avg:60.32ms
step:2177/2330 train_time:131313ms step_avg:60.32ms
step:2178/2330 train_time:131375ms step_avg:60.32ms
step:2179/2330 train_time:131436ms step_avg:60.32ms
step:2180/2330 train_time:131497ms step_avg:60.32ms
step:2181/2330 train_time:131558ms step_avg:60.32ms
step:2182/2330 train_time:131620ms step_avg:60.32ms
step:2183/2330 train_time:131679ms step_avg:60.32ms
step:2184/2330 train_time:131741ms step_avg:60.32ms
step:2185/2330 train_time:131801ms step_avg:60.32ms
step:2186/2330 train_time:131863ms step_avg:60.32ms
step:2187/2330 train_time:131923ms step_avg:60.32ms
step:2188/2330 train_time:131985ms step_avg:60.32ms
step:2189/2330 train_time:132046ms step_avg:60.32ms
step:2190/2330 train_time:132108ms step_avg:60.32ms
step:2191/2330 train_time:132167ms step_avg:60.32ms
step:2192/2330 train_time:132229ms step_avg:60.32ms
step:2193/2330 train_time:132288ms step_avg:60.32ms
step:2194/2330 train_time:132350ms step_avg:60.32ms
step:2195/2330 train_time:132410ms step_avg:60.32ms
step:2196/2330 train_time:132472ms step_avg:60.32ms
step:2197/2330 train_time:132533ms step_avg:60.32ms
step:2198/2330 train_time:132595ms step_avg:60.33ms
step:2199/2330 train_time:132656ms step_avg:60.33ms
step:2200/2330 train_time:132718ms step_avg:60.33ms
step:2201/2330 train_time:132778ms step_avg:60.33ms
step:2202/2330 train_time:132840ms step_avg:60.33ms
step:2203/2330 train_time:132900ms step_avg:60.33ms
step:2204/2330 train_time:132961ms step_avg:60.33ms
step:2205/2330 train_time:133021ms step_avg:60.33ms
step:2206/2330 train_time:133083ms step_avg:60.33ms
step:2207/2330 train_time:133143ms step_avg:60.33ms
step:2208/2330 train_time:133206ms step_avg:60.33ms
step:2209/2330 train_time:133266ms step_avg:60.33ms
step:2210/2330 train_time:133327ms step_avg:60.33ms
step:2211/2330 train_time:133386ms step_avg:60.33ms
step:2212/2330 train_time:133448ms step_avg:60.33ms
step:2213/2330 train_time:133508ms step_avg:60.33ms
step:2214/2330 train_time:133570ms step_avg:60.33ms
step:2215/2330 train_time:133630ms step_avg:60.33ms
step:2216/2330 train_time:133693ms step_avg:60.33ms
step:2217/2330 train_time:133753ms step_avg:60.33ms
step:2218/2330 train_time:133816ms step_avg:60.33ms
step:2219/2330 train_time:133875ms step_avg:60.33ms
step:2220/2330 train_time:133937ms step_avg:60.33ms
step:2221/2330 train_time:133997ms step_avg:60.33ms
step:2222/2330 train_time:134059ms step_avg:60.33ms
step:2223/2330 train_time:134119ms step_avg:60.33ms
step:2224/2330 train_time:134181ms step_avg:60.33ms
step:2225/2330 train_time:134240ms step_avg:60.33ms
step:2226/2330 train_time:134302ms step_avg:60.33ms
step:2227/2330 train_time:134362ms step_avg:60.33ms
step:2228/2330 train_time:134424ms step_avg:60.33ms
step:2229/2330 train_time:134484ms step_avg:60.33ms
step:2230/2330 train_time:134546ms step_avg:60.33ms
step:2231/2330 train_time:134606ms step_avg:60.33ms
step:2232/2330 train_time:134668ms step_avg:60.33ms
step:2233/2330 train_time:134728ms step_avg:60.33ms
step:2234/2330 train_time:134790ms step_avg:60.34ms
step:2235/2330 train_time:134850ms step_avg:60.34ms
step:2236/2330 train_time:134912ms step_avg:60.34ms
step:2237/2330 train_time:134973ms step_avg:60.34ms
step:2238/2330 train_time:135035ms step_avg:60.34ms
step:2239/2330 train_time:135096ms step_avg:60.34ms
step:2240/2330 train_time:135158ms step_avg:60.34ms
step:2241/2330 train_time:135218ms step_avg:60.34ms
step:2242/2330 train_time:135280ms step_avg:60.34ms
step:2243/2330 train_time:135339ms step_avg:60.34ms
step:2244/2330 train_time:135401ms step_avg:60.34ms
step:2245/2330 train_time:135461ms step_avg:60.34ms
step:2246/2330 train_time:135523ms step_avg:60.34ms
step:2247/2330 train_time:135583ms step_avg:60.34ms
step:2248/2330 train_time:135645ms step_avg:60.34ms
step:2249/2330 train_time:135705ms step_avg:60.34ms
step:2250/2330 train_time:135767ms step_avg:60.34ms
step:2250/2330 val_loss:3.2912 train_time:135831ms step_avg:60.37ms
step:2251/2330 train_time:135852ms step_avg:60.35ms
step:2252/2330 train_time:135893ms step_avg:60.34ms
step:2253/2330 train_time:135957ms step_avg:60.35ms
step:2254/2330 train_time:136021ms step_avg:60.35ms
step:2255/2330 train_time:136081ms step_avg:60.35ms
step:2256/2330 train_time:136144ms step_avg:60.35ms
step:2257/2330 train_time:136204ms step_avg:60.35ms
step:2258/2330 train_time:136266ms step_avg:60.35ms
step:2259/2330 train_time:136325ms step_avg:60.35ms
step:2260/2330 train_time:136386ms step_avg:60.35ms
step:2261/2330 train_time:136445ms step_avg:60.35ms
step:2262/2330 train_time:136506ms step_avg:60.35ms
step:2263/2330 train_time:136565ms step_avg:60.35ms
step:2264/2330 train_time:136626ms step_avg:60.35ms
step:2265/2330 train_time:136685ms step_avg:60.35ms
step:2266/2330 train_time:136747ms step_avg:60.35ms
step:2267/2330 train_time:136809ms step_avg:60.35ms
step:2268/2330 train_time:136873ms step_avg:60.35ms
step:2269/2330 train_time:136934ms step_avg:60.35ms
step:2270/2330 train_time:136997ms step_avg:60.35ms
step:2271/2330 train_time:137057ms step_avg:60.35ms
step:2272/2330 train_time:137119ms step_avg:60.35ms
step:2273/2330 train_time:137179ms step_avg:60.35ms
step:2274/2330 train_time:137240ms step_avg:60.35ms
step:2275/2330 train_time:137300ms step_avg:60.35ms
step:2276/2330 train_time:137363ms step_avg:60.35ms
step:2277/2330 train_time:137422ms step_avg:60.35ms
step:2278/2330 train_time:137483ms step_avg:60.35ms
step:2279/2330 train_time:137542ms step_avg:60.35ms
step:2280/2330 train_time:137604ms step_avg:60.35ms
step:2281/2330 train_time:137663ms step_avg:60.35ms
step:2282/2330 train_time:137725ms step_avg:60.35ms
step:2283/2330 train_time:137786ms step_avg:60.35ms
step:2284/2330 train_time:137849ms step_avg:60.35ms
step:2285/2330 train_time:137909ms step_avg:60.35ms
step:2286/2330 train_time:137971ms step_avg:60.35ms
step:2287/2330 train_time:138031ms step_avg:60.35ms
step:2288/2330 train_time:138093ms step_avg:60.36ms
step:2289/2330 train_time:138153ms step_avg:60.36ms
step:2290/2330 train_time:138216ms step_avg:60.36ms
step:2291/2330 train_time:138275ms step_avg:60.36ms
step:2292/2330 train_time:138336ms step_avg:60.36ms
step:2293/2330 train_time:138396ms step_avg:60.36ms
step:2294/2330 train_time:138458ms step_avg:60.36ms
step:2295/2330 train_time:138517ms step_avg:60.36ms
step:2296/2330 train_time:138579ms step_avg:60.36ms
step:2297/2330 train_time:138638ms step_avg:60.36ms
step:2298/2330 train_time:138700ms step_avg:60.36ms
step:2299/2330 train_time:138760ms step_avg:60.36ms
step:2300/2330 train_time:138823ms step_avg:60.36ms
step:2301/2330 train_time:138885ms step_avg:60.36ms
step:2302/2330 train_time:138947ms step_avg:60.36ms
step:2303/2330 train_time:139008ms step_avg:60.36ms
step:2304/2330 train_time:139069ms step_avg:60.36ms
step:2305/2330 train_time:139129ms step_avg:60.36ms
step:2306/2330 train_time:139191ms step_avg:60.36ms
step:2307/2330 train_time:139251ms step_avg:60.36ms
step:2308/2330 train_time:139313ms step_avg:60.36ms
step:2309/2330 train_time:139373ms step_avg:60.36ms
step:2310/2330 train_time:139434ms step_avg:60.36ms
step:2311/2330 train_time:139494ms step_avg:60.36ms
step:2312/2330 train_time:139555ms step_avg:60.36ms
step:2313/2330 train_time:139615ms step_avg:60.36ms
step:2314/2330 train_time:139677ms step_avg:60.36ms
step:2315/2330 train_time:139736ms step_avg:60.36ms
step:2316/2330 train_time:139798ms step_avg:60.36ms
step:2317/2330 train_time:139859ms step_avg:60.36ms
step:2318/2330 train_time:139922ms step_avg:60.36ms
step:2319/2330 train_time:139983ms step_avg:60.36ms
step:2320/2330 train_time:140046ms step_avg:60.36ms
step:2321/2330 train_time:140107ms step_avg:60.36ms
step:2322/2330 train_time:140169ms step_avg:60.37ms
step:2323/2330 train_time:140228ms step_avg:60.37ms
step:2324/2330 train_time:140289ms step_avg:60.37ms
step:2325/2330 train_time:140349ms step_avg:60.37ms
step:2326/2330 train_time:140410ms step_avg:60.37ms
step:2327/2330 train_time:140470ms step_avg:60.37ms
step:2328/2330 train_time:140532ms step_avg:60.37ms
step:2329/2330 train_time:140591ms step_avg:60.37ms
step:2330/2330 train_time:140654ms step_avg:60.37ms
step:2330/2330 val_loss:3.2783 train_time:140718ms step_avg:60.39ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
coeffs_list = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in coeffs_list:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            # x_out = self.blocks[i](x, x0, lambdas[i], attn_args)
            # x_backout += backout_lambdas[i] * (x_out-x)
            # x = x_out
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i==8:
                x_backout=x

        # backout contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda*x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2290  # number of iterations to run
    iteration_extension = 40  # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.45  # fraction of training spent cooling down the learning rate
    momentum_cd_steps = 50  # number of iterations for muon momentum cooldown
    # evaluation and logging
    run_id: str = f"new/{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

def update_optimizer_params(step, optimizer1, optimizer2):
    # Update lr
    for group in optimizer1.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)

    # Warmup phase: gradually increase momentum from 0.85 to 0.95
    if step < 300:
        frac = step / 300
        momentum = 0.85 + frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum

    # Cooldown phase: gradually decrease momentum
    momentum_cd_start = args.num_iterations + args.iteration_extension - args.momentum_cd_steps
    if step > momentum_cd_start:
        frac = (step - momentum_cd_start) / args.momentum_cd_steps

        # Decay momentum from 0.95 to 0.85
        momentum = 0.95 - frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    update_optimizer_params(step, optimizer1, optimizer2)
    # only step Adam every other step
    if step%2==0:
        optimizer2.step()
        optimizer2.zero_grad(set_to_none=True)
    else:
        for opt in optimizers:
            opt.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
    
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Sat Oct  4 06:07:53 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   28C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   28C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   29C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   40C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   28C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          271255      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          271256      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          271257      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          271258      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          271259      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          271260      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          271261      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          271262      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          271256      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          271257      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          271258      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          271259      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          271260      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          271261      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          271262      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:86ms step_avg:85.99ms
step:2/2330 train_time:175ms step_avg:87.26ms
step:3/2330 train_time:198ms step_avg:65.86ms
step:4/2330 train_time:231ms step_avg:57.68ms
step:5/2330 train_time:287ms step_avg:57.47ms
step:6/2330 train_time:354ms step_avg:58.99ms
step:7/2330 train_time:405ms step_avg:57.82ms
step:8/2330 train_time:465ms step_avg:58.12ms
step:9/2330 train_time:523ms step_avg:58.11ms
step:10/2330 train_time:584ms step_avg:58.35ms
step:11/2330 train_time:642ms step_avg:58.33ms
step:12/2330 train_time:702ms step_avg:58.53ms
step:13/2330 train_time:760ms step_avg:58.50ms
step:14/2330 train_time:821ms step_avg:58.65ms
step:15/2330 train_time:879ms step_avg:58.63ms
step:16/2330 train_time:940ms step_avg:58.75ms
step:17/2330 train_time:1000ms step_avg:58.81ms
step:18/2330 train_time:1063ms step_avg:59.04ms
step:19/2330 train_time:1124ms step_avg:59.15ms
step:20/2330 train_time:1187ms step_avg:59.34ms
step:21/2330 train_time:1246ms step_avg:59.35ms
step:22/2330 train_time:1307ms step_avg:59.43ms
step:23/2330 train_time:1367ms step_avg:59.41ms
step:24/2330 train_time:1428ms step_avg:59.49ms
step:25/2330 train_time:1486ms step_avg:59.44ms
step:26/2330 train_time:1546ms step_avg:59.48ms
step:27/2330 train_time:1605ms step_avg:59.43ms
step:28/2330 train_time:1665ms step_avg:59.46ms
step:29/2330 train_time:1723ms step_avg:59.43ms
step:30/2330 train_time:1784ms step_avg:59.47ms
step:31/2330 train_time:1843ms step_avg:59.45ms
step:32/2330 train_time:1904ms step_avg:59.49ms
step:33/2330 train_time:1963ms step_avg:59.49ms
step:34/2330 train_time:2025ms step_avg:59.55ms
step:35/2330 train_time:2085ms step_avg:59.57ms
step:36/2330 train_time:2147ms step_avg:59.63ms
step:37/2330 train_time:2206ms step_avg:59.63ms
step:38/2330 train_time:2268ms step_avg:59.69ms
step:39/2330 train_time:2328ms step_avg:59.70ms
step:40/2330 train_time:2389ms step_avg:59.73ms
step:41/2330 train_time:2448ms step_avg:59.71ms
step:42/2330 train_time:2509ms step_avg:59.73ms
step:43/2330 train_time:2567ms step_avg:59.70ms
step:44/2330 train_time:2628ms step_avg:59.72ms
step:45/2330 train_time:2686ms step_avg:59.69ms
step:46/2330 train_time:2747ms step_avg:59.72ms
step:47/2330 train_time:2805ms step_avg:59.69ms
step:48/2330 train_time:2866ms step_avg:59.71ms
step:49/2330 train_time:2925ms step_avg:59.70ms
step:50/2330 train_time:2986ms step_avg:59.73ms
step:51/2330 train_time:3045ms step_avg:59.71ms
step:52/2330 train_time:3107ms step_avg:59.74ms
step:53/2330 train_time:3166ms step_avg:59.73ms
step:54/2330 train_time:3227ms step_avg:59.77ms
step:55/2330 train_time:3287ms step_avg:59.76ms
step:56/2330 train_time:3348ms step_avg:59.78ms
step:57/2330 train_time:3406ms step_avg:59.76ms
step:58/2330 train_time:3467ms step_avg:59.78ms
step:59/2330 train_time:3525ms step_avg:59.75ms
step:60/2330 train_time:3586ms step_avg:59.77ms
step:61/2330 train_time:3644ms step_avg:59.74ms
step:62/2330 train_time:3705ms step_avg:59.76ms
step:63/2330 train_time:3763ms step_avg:59.73ms
step:64/2330 train_time:3824ms step_avg:59.75ms
step:65/2330 train_time:3883ms step_avg:59.73ms
step:66/2330 train_time:3943ms step_avg:59.75ms
step:67/2330 train_time:4002ms step_avg:59.73ms
step:68/2330 train_time:4063ms step_avg:59.75ms
step:69/2330 train_time:4122ms step_avg:59.74ms
step:70/2330 train_time:4183ms step_avg:59.76ms
step:71/2330 train_time:4242ms step_avg:59.74ms
step:72/2330 train_time:4302ms step_avg:59.76ms
step:73/2330 train_time:4362ms step_avg:59.75ms
step:74/2330 train_time:4423ms step_avg:59.77ms
step:75/2330 train_time:4482ms step_avg:59.75ms
step:76/2330 train_time:4542ms step_avg:59.76ms
step:77/2330 train_time:4600ms step_avg:59.75ms
step:78/2330 train_time:4661ms step_avg:59.76ms
step:79/2330 train_time:4720ms step_avg:59.74ms
step:80/2330 train_time:4780ms step_avg:59.75ms
step:81/2330 train_time:4838ms step_avg:59.73ms
step:82/2330 train_time:4899ms step_avg:59.75ms
step:83/2330 train_time:4959ms step_avg:59.74ms
step:84/2330 train_time:5020ms step_avg:59.76ms
step:85/2330 train_time:5079ms step_avg:59.75ms
step:86/2330 train_time:5140ms step_avg:59.76ms
step:87/2330 train_time:5199ms step_avg:59.75ms
step:88/2330 train_time:5260ms step_avg:59.77ms
step:89/2330 train_time:5319ms step_avg:59.76ms
step:90/2330 train_time:5381ms step_avg:59.78ms
step:91/2330 train_time:5439ms step_avg:59.77ms
step:92/2330 train_time:5500ms step_avg:59.79ms
step:93/2330 train_time:5559ms step_avg:59.78ms
step:94/2330 train_time:5621ms step_avg:59.79ms
step:95/2330 train_time:5679ms step_avg:59.78ms
step:96/2330 train_time:5740ms step_avg:59.79ms
step:97/2330 train_time:5798ms step_avg:59.78ms
step:98/2330 train_time:5859ms step_avg:59.79ms
step:99/2330 train_time:5918ms step_avg:59.78ms
step:100/2330 train_time:5979ms step_avg:59.79ms
step:101/2330 train_time:6038ms step_avg:59.78ms
step:102/2330 train_time:6099ms step_avg:59.79ms
step:103/2330 train_time:6158ms step_avg:59.78ms
step:104/2330 train_time:6220ms step_avg:59.80ms
step:105/2330 train_time:6279ms step_avg:59.80ms
step:106/2330 train_time:6339ms step_avg:59.81ms
step:107/2330 train_time:6399ms step_avg:59.80ms
step:108/2330 train_time:6460ms step_avg:59.82ms
step:109/2330 train_time:6519ms step_avg:59.81ms
step:110/2330 train_time:6580ms step_avg:59.82ms
step:111/2330 train_time:6639ms step_avg:59.81ms
step:112/2330 train_time:6699ms step_avg:59.82ms
step:113/2330 train_time:6758ms step_avg:59.80ms
step:114/2330 train_time:6819ms step_avg:59.81ms
step:115/2330 train_time:6877ms step_avg:59.80ms
step:116/2330 train_time:6938ms step_avg:59.81ms
step:117/2330 train_time:6997ms step_avg:59.80ms
step:118/2330 train_time:7058ms step_avg:59.81ms
step:119/2330 train_time:7117ms step_avg:59.81ms
step:120/2330 train_time:7179ms step_avg:59.82ms
step:121/2330 train_time:7237ms step_avg:59.81ms
step:122/2330 train_time:7298ms step_avg:59.82ms
step:123/2330 train_time:7357ms step_avg:59.81ms
step:124/2330 train_time:7418ms step_avg:59.82ms
step:125/2330 train_time:7477ms step_avg:59.81ms
step:126/2330 train_time:7538ms step_avg:59.82ms
step:127/2330 train_time:7596ms step_avg:59.81ms
step:128/2330 train_time:7657ms step_avg:59.82ms
step:129/2330 train_time:7715ms step_avg:59.81ms
step:130/2330 train_time:7776ms step_avg:59.82ms
step:131/2330 train_time:7834ms step_avg:59.81ms
step:132/2330 train_time:7895ms step_avg:59.81ms
step:133/2330 train_time:7954ms step_avg:59.81ms
step:134/2330 train_time:8015ms step_avg:59.81ms
step:135/2330 train_time:8073ms step_avg:59.80ms
step:136/2330 train_time:8134ms step_avg:59.81ms
step:137/2330 train_time:8193ms step_avg:59.80ms
step:138/2330 train_time:8253ms step_avg:59.81ms
step:139/2330 train_time:8312ms step_avg:59.80ms
step:140/2330 train_time:8373ms step_avg:59.81ms
step:141/2330 train_time:8432ms step_avg:59.80ms
step:142/2330 train_time:8492ms step_avg:59.80ms
step:143/2330 train_time:8551ms step_avg:59.80ms
step:144/2330 train_time:8611ms step_avg:59.80ms
step:145/2330 train_time:8670ms step_avg:59.79ms
step:146/2330 train_time:8731ms step_avg:59.80ms
step:147/2330 train_time:8789ms step_avg:59.79ms
step:148/2330 train_time:8850ms step_avg:59.80ms
step:149/2330 train_time:8908ms step_avg:59.79ms
step:150/2330 train_time:8970ms step_avg:59.80ms
step:151/2330 train_time:9028ms step_avg:59.79ms
step:152/2330 train_time:9089ms step_avg:59.80ms
step:153/2330 train_time:9147ms step_avg:59.79ms
step:154/2330 train_time:9208ms step_avg:59.79ms
step:155/2330 train_time:9266ms step_avg:59.78ms
step:156/2330 train_time:9327ms step_avg:59.79ms
step:157/2330 train_time:9385ms step_avg:59.78ms
step:158/2330 train_time:9446ms step_avg:59.78ms
step:159/2330 train_time:9504ms step_avg:59.77ms
step:160/2330 train_time:9564ms step_avg:59.78ms
step:161/2330 train_time:9623ms step_avg:59.77ms
step:162/2330 train_time:9684ms step_avg:59.78ms
step:163/2330 train_time:9742ms step_avg:59.77ms
step:164/2330 train_time:9802ms step_avg:59.77ms
step:165/2330 train_time:9860ms step_avg:59.76ms
step:166/2330 train_time:9921ms step_avg:59.77ms
step:167/2330 train_time:9980ms step_avg:59.76ms
step:168/2330 train_time:10041ms step_avg:59.77ms
step:169/2330 train_time:10100ms step_avg:59.76ms
step:170/2330 train_time:10160ms step_avg:59.77ms
step:171/2330 train_time:10219ms step_avg:59.76ms
step:172/2330 train_time:10281ms step_avg:59.77ms
step:173/2330 train_time:10339ms step_avg:59.77ms
step:174/2330 train_time:10400ms step_avg:59.77ms
step:175/2330 train_time:10459ms step_avg:59.77ms
step:176/2330 train_time:10521ms step_avg:59.78ms
step:177/2330 train_time:10580ms step_avg:59.78ms
step:178/2330 train_time:10641ms step_avg:59.78ms
step:179/2330 train_time:10700ms step_avg:59.77ms
step:180/2330 train_time:10760ms step_avg:59.78ms
step:181/2330 train_time:10819ms step_avg:59.77ms
step:182/2330 train_time:10879ms step_avg:59.78ms
step:183/2330 train_time:10938ms step_avg:59.77ms
step:184/2330 train_time:10999ms step_avg:59.78ms
step:185/2330 train_time:11058ms step_avg:59.77ms
step:186/2330 train_time:11119ms step_avg:59.78ms
step:187/2330 train_time:11177ms step_avg:59.77ms
step:188/2330 train_time:11238ms step_avg:59.78ms
step:189/2330 train_time:11297ms step_avg:59.77ms
step:190/2330 train_time:11358ms step_avg:59.78ms
step:191/2330 train_time:11417ms step_avg:59.78ms
step:192/2330 train_time:11478ms step_avg:59.78ms
step:193/2330 train_time:11536ms step_avg:59.77ms
step:194/2330 train_time:11598ms step_avg:59.78ms
step:195/2330 train_time:11656ms step_avg:59.78ms
step:196/2330 train_time:11718ms step_avg:59.78ms
step:197/2330 train_time:11777ms step_avg:59.78ms
step:198/2330 train_time:11837ms step_avg:59.78ms
step:199/2330 train_time:11896ms step_avg:59.78ms
step:200/2330 train_time:11956ms step_avg:59.78ms
step:201/2330 train_time:12015ms step_avg:59.78ms
step:202/2330 train_time:12076ms step_avg:59.78ms
step:203/2330 train_time:12135ms step_avg:59.78ms
step:204/2330 train_time:12196ms step_avg:59.78ms
step:205/2330 train_time:12255ms step_avg:59.78ms
step:206/2330 train_time:12316ms step_avg:59.79ms
step:207/2330 train_time:12374ms step_avg:59.78ms
step:208/2330 train_time:12435ms step_avg:59.79ms
step:209/2330 train_time:12494ms step_avg:59.78ms
step:210/2330 train_time:12555ms step_avg:59.78ms
step:211/2330 train_time:12613ms step_avg:59.78ms
step:212/2330 train_time:12674ms step_avg:59.78ms
step:213/2330 train_time:12733ms step_avg:59.78ms
step:214/2330 train_time:12793ms step_avg:59.78ms
step:215/2330 train_time:12852ms step_avg:59.78ms
step:216/2330 train_time:12912ms step_avg:59.78ms
step:217/2330 train_time:12971ms step_avg:59.78ms
step:218/2330 train_time:13032ms step_avg:59.78ms
step:219/2330 train_time:13090ms step_avg:59.77ms
step:220/2330 train_time:13150ms step_avg:59.77ms
step:221/2330 train_time:13209ms step_avg:59.77ms
step:222/2330 train_time:13271ms step_avg:59.78ms
step:223/2330 train_time:13329ms step_avg:59.77ms
step:224/2330 train_time:13390ms step_avg:59.78ms
step:225/2330 train_time:13448ms step_avg:59.77ms
step:226/2330 train_time:13509ms step_avg:59.77ms
step:227/2330 train_time:13568ms step_avg:59.77ms
step:228/2330 train_time:13628ms step_avg:59.77ms
step:229/2330 train_time:13687ms step_avg:59.77ms
step:230/2330 train_time:13747ms step_avg:59.77ms
step:231/2330 train_time:13806ms step_avg:59.77ms
step:232/2330 train_time:13867ms step_avg:59.77ms
step:233/2330 train_time:13925ms step_avg:59.77ms
step:234/2330 train_time:13985ms step_avg:59.77ms
step:235/2330 train_time:14043ms step_avg:59.76ms
step:236/2330 train_time:14103ms step_avg:59.76ms
step:237/2330 train_time:14162ms step_avg:59.75ms
step:238/2330 train_time:14222ms step_avg:59.76ms
step:239/2330 train_time:14281ms step_avg:59.75ms
step:240/2330 train_time:14342ms step_avg:59.76ms
step:241/2330 train_time:14401ms step_avg:59.75ms
step:242/2330 train_time:14461ms step_avg:59.76ms
step:243/2330 train_time:14520ms step_avg:59.75ms
step:244/2330 train_time:14580ms step_avg:59.76ms
step:245/2330 train_time:14640ms step_avg:59.75ms
step:246/2330 train_time:14701ms step_avg:59.76ms
step:247/2330 train_time:14760ms step_avg:59.76ms
step:248/2330 train_time:14821ms step_avg:59.76ms
step:249/2330 train_time:14879ms step_avg:59.76ms
step:250/2330 train_time:14940ms step_avg:59.76ms
step:250/2330 val_loss:4.0802 train_time:15002ms step_avg:60.01ms
step:251/2330 train_time:15024ms step_avg:59.86ms
step:252/2330 train_time:15062ms step_avg:59.77ms
step:253/2330 train_time:15122ms step_avg:59.77ms
step:254/2330 train_time:15189ms step_avg:59.80ms
step:255/2330 train_time:15250ms step_avg:59.80ms
step:256/2330 train_time:15310ms step_avg:59.81ms
step:257/2330 train_time:15368ms step_avg:59.80ms
step:258/2330 train_time:15428ms step_avg:59.80ms
step:259/2330 train_time:15486ms step_avg:59.79ms
step:260/2330 train_time:15546ms step_avg:59.79ms
step:261/2330 train_time:15604ms step_avg:59.79ms
step:262/2330 train_time:15664ms step_avg:59.79ms
step:263/2330 train_time:15722ms step_avg:59.78ms
step:264/2330 train_time:15782ms step_avg:59.78ms
step:265/2330 train_time:15839ms step_avg:59.77ms
step:266/2330 train_time:15899ms step_avg:59.77ms
step:267/2330 train_time:15958ms step_avg:59.77ms
step:268/2330 train_time:16018ms step_avg:59.77ms
step:269/2330 train_time:16078ms step_avg:59.77ms
step:270/2330 train_time:16140ms step_avg:59.78ms
step:271/2330 train_time:16200ms step_avg:59.78ms
step:272/2330 train_time:16263ms step_avg:59.79ms
step:273/2330 train_time:16322ms step_avg:59.79ms
step:274/2330 train_time:16384ms step_avg:59.79ms
step:275/2330 train_time:16442ms step_avg:59.79ms
step:276/2330 train_time:16503ms step_avg:59.79ms
step:277/2330 train_time:16561ms step_avg:59.79ms
step:278/2330 train_time:16622ms step_avg:59.79ms
step:279/2330 train_time:16679ms step_avg:59.78ms
step:280/2330 train_time:16739ms step_avg:59.78ms
step:281/2330 train_time:16797ms step_avg:59.77ms
step:282/2330 train_time:16857ms step_avg:59.78ms
step:283/2330 train_time:16915ms step_avg:59.77ms
step:284/2330 train_time:16975ms step_avg:59.77ms
step:285/2330 train_time:17033ms step_avg:59.77ms
step:286/2330 train_time:17094ms step_avg:59.77ms
step:287/2330 train_time:17154ms step_avg:59.77ms
step:288/2330 train_time:17215ms step_avg:59.77ms
step:289/2330 train_time:17274ms step_avg:59.77ms
step:290/2330 train_time:17335ms step_avg:59.77ms
step:291/2330 train_time:17393ms step_avg:59.77ms
step:292/2330 train_time:17454ms step_avg:59.77ms
step:293/2330 train_time:17513ms step_avg:59.77ms
step:294/2330 train_time:17574ms step_avg:59.78ms
step:295/2330 train_time:17633ms step_avg:59.77ms
step:296/2330 train_time:17693ms step_avg:59.77ms
step:297/2330 train_time:17751ms step_avg:59.77ms
step:298/2330 train_time:17812ms step_avg:59.77ms
step:299/2330 train_time:17870ms step_avg:59.77ms
step:300/2330 train_time:17931ms step_avg:59.77ms
step:301/2330 train_time:17989ms step_avg:59.76ms
step:302/2330 train_time:18049ms step_avg:59.77ms
step:303/2330 train_time:18108ms step_avg:59.76ms
step:304/2330 train_time:18169ms step_avg:59.77ms
step:305/2330 train_time:18228ms step_avg:59.77ms
step:306/2330 train_time:18289ms step_avg:59.77ms
step:307/2330 train_time:18348ms step_avg:59.77ms
step:308/2330 train_time:18409ms step_avg:59.77ms
step:309/2330 train_time:18468ms step_avg:59.77ms
step:310/2330 train_time:18529ms step_avg:59.77ms
step:311/2330 train_time:18588ms step_avg:59.77ms
step:312/2330 train_time:18648ms step_avg:59.77ms
step:313/2330 train_time:18707ms step_avg:59.77ms
step:314/2330 train_time:18767ms step_avg:59.77ms
step:315/2330 train_time:18826ms step_avg:59.76ms
step:316/2330 train_time:18886ms step_avg:59.77ms
step:317/2330 train_time:18945ms step_avg:59.76ms
step:318/2330 train_time:19005ms step_avg:59.76ms
step:319/2330 train_time:19063ms step_avg:59.76ms
step:320/2330 train_time:19124ms step_avg:59.76ms
step:321/2330 train_time:19182ms step_avg:59.76ms
step:322/2330 train_time:19242ms step_avg:59.76ms
step:323/2330 train_time:19301ms step_avg:59.76ms
step:324/2330 train_time:19362ms step_avg:59.76ms
step:325/2330 train_time:19421ms step_avg:59.76ms
step:326/2330 train_time:19481ms step_avg:59.76ms
step:327/2330 train_time:19540ms step_avg:59.75ms
step:328/2330 train_time:19600ms step_avg:59.76ms
step:329/2330 train_time:19658ms step_avg:59.75ms
step:330/2330 train_time:19719ms step_avg:59.75ms
step:331/2330 train_time:19778ms step_avg:59.75ms
step:332/2330 train_time:19838ms step_avg:59.75ms
step:333/2330 train_time:19896ms step_avg:59.75ms
step:334/2330 train_time:19957ms step_avg:59.75ms
step:335/2330 train_time:20015ms step_avg:59.75ms
step:336/2330 train_time:20075ms step_avg:59.75ms
step:337/2330 train_time:20134ms step_avg:59.74ms
step:338/2330 train_time:20194ms step_avg:59.75ms
step:339/2330 train_time:20253ms step_avg:59.74ms
step:340/2330 train_time:20314ms step_avg:59.75ms
step:341/2330 train_time:20374ms step_avg:59.75ms
step:342/2330 train_time:20435ms step_avg:59.75ms
step:343/2330 train_time:20494ms step_avg:59.75ms
step:344/2330 train_time:20554ms step_avg:59.75ms
step:345/2330 train_time:20613ms step_avg:59.75ms
step:346/2330 train_time:20675ms step_avg:59.75ms
step:347/2330 train_time:20733ms step_avg:59.75ms
step:348/2330 train_time:20792ms step_avg:59.75ms
step:349/2330 train_time:20850ms step_avg:59.74ms
step:350/2330 train_time:20911ms step_avg:59.75ms
step:351/2330 train_time:20969ms step_avg:59.74ms
step:352/2330 train_time:21030ms step_avg:59.74ms
step:353/2330 train_time:21088ms step_avg:59.74ms
step:354/2330 train_time:21149ms step_avg:59.74ms
step:355/2330 train_time:21207ms step_avg:59.74ms
step:356/2330 train_time:21268ms step_avg:59.74ms
step:357/2330 train_time:21327ms step_avg:59.74ms
step:358/2330 train_time:21387ms step_avg:59.74ms
step:359/2330 train_time:21446ms step_avg:59.74ms
step:360/2330 train_time:21506ms step_avg:59.74ms
step:361/2330 train_time:21565ms step_avg:59.74ms
step:362/2330 train_time:21626ms step_avg:59.74ms
step:363/2330 train_time:21685ms step_avg:59.74ms
step:364/2330 train_time:21745ms step_avg:59.74ms
step:365/2330 train_time:21803ms step_avg:59.73ms
step:366/2330 train_time:21864ms step_avg:59.74ms
step:367/2330 train_time:21922ms step_avg:59.73ms
step:368/2330 train_time:21982ms step_avg:59.73ms
step:369/2330 train_time:22041ms step_avg:59.73ms
step:370/2330 train_time:22101ms step_avg:59.73ms
step:371/2330 train_time:22159ms step_avg:59.73ms
step:372/2330 train_time:22220ms step_avg:59.73ms
step:373/2330 train_time:22278ms step_avg:59.73ms
step:374/2330 train_time:22339ms step_avg:59.73ms
step:375/2330 train_time:22397ms step_avg:59.73ms
step:376/2330 train_time:22458ms step_avg:59.73ms
step:377/2330 train_time:22516ms step_avg:59.72ms
step:378/2330 train_time:22577ms step_avg:59.73ms
step:379/2330 train_time:22635ms step_avg:59.72ms
step:380/2330 train_time:22696ms step_avg:59.73ms
step:381/2330 train_time:22754ms step_avg:59.72ms
step:382/2330 train_time:22814ms step_avg:59.72ms
step:383/2330 train_time:22873ms step_avg:59.72ms
step:384/2330 train_time:22933ms step_avg:59.72ms
step:385/2330 train_time:22992ms step_avg:59.72ms
step:386/2330 train_time:23054ms step_avg:59.73ms
step:387/2330 train_time:23112ms step_avg:59.72ms
step:388/2330 train_time:23173ms step_avg:59.72ms
step:389/2330 train_time:23231ms step_avg:59.72ms
step:390/2330 train_time:23292ms step_avg:59.72ms
step:391/2330 train_time:23351ms step_avg:59.72ms
step:392/2330 train_time:23411ms step_avg:59.72ms
step:393/2330 train_time:23470ms step_avg:59.72ms
step:394/2330 train_time:23531ms step_avg:59.72ms
step:395/2330 train_time:23589ms step_avg:59.72ms
step:396/2330 train_time:23650ms step_avg:59.72ms
step:397/2330 train_time:23708ms step_avg:59.72ms
step:398/2330 train_time:23769ms step_avg:59.72ms
step:399/2330 train_time:23827ms step_avg:59.72ms
step:400/2330 train_time:23888ms step_avg:59.72ms
step:401/2330 train_time:23947ms step_avg:59.72ms
step:402/2330 train_time:24007ms step_avg:59.72ms
step:403/2330 train_time:24066ms step_avg:59.72ms
step:404/2330 train_time:24127ms step_avg:59.72ms
step:405/2330 train_time:24185ms step_avg:59.72ms
step:406/2330 train_time:24245ms step_avg:59.72ms
step:407/2330 train_time:24304ms step_avg:59.72ms
step:408/2330 train_time:24365ms step_avg:59.72ms
step:409/2330 train_time:24423ms step_avg:59.71ms
step:410/2330 train_time:24483ms step_avg:59.72ms
step:411/2330 train_time:24542ms step_avg:59.71ms
step:412/2330 train_time:24603ms step_avg:59.72ms
step:413/2330 train_time:24661ms step_avg:59.71ms
step:414/2330 train_time:24721ms step_avg:59.71ms
step:415/2330 train_time:24780ms step_avg:59.71ms
step:416/2330 train_time:24840ms step_avg:59.71ms
step:417/2330 train_time:24898ms step_avg:59.71ms
step:418/2330 train_time:24959ms step_avg:59.71ms
step:419/2330 train_time:25017ms step_avg:59.71ms
step:420/2330 train_time:25077ms step_avg:59.71ms
step:421/2330 train_time:25136ms step_avg:59.70ms
step:422/2330 train_time:25196ms step_avg:59.71ms
step:423/2330 train_time:25254ms step_avg:59.70ms
step:424/2330 train_time:25315ms step_avg:59.70ms
step:425/2330 train_time:25373ms step_avg:59.70ms
step:426/2330 train_time:25434ms step_avg:59.70ms
step:427/2330 train_time:25494ms step_avg:59.70ms
step:428/2330 train_time:25555ms step_avg:59.71ms
step:429/2330 train_time:25613ms step_avg:59.70ms
step:430/2330 train_time:25674ms step_avg:59.71ms
step:431/2330 train_time:25733ms step_avg:59.71ms
step:432/2330 train_time:25794ms step_avg:59.71ms
step:433/2330 train_time:25853ms step_avg:59.71ms
step:434/2330 train_time:25914ms step_avg:59.71ms
step:435/2330 train_time:25972ms step_avg:59.71ms
step:436/2330 train_time:26033ms step_avg:59.71ms
step:437/2330 train_time:26092ms step_avg:59.71ms
step:438/2330 train_time:26153ms step_avg:59.71ms
step:439/2330 train_time:26211ms step_avg:59.71ms
step:440/2330 train_time:26272ms step_avg:59.71ms
step:441/2330 train_time:26330ms step_avg:59.70ms
step:442/2330 train_time:26390ms step_avg:59.71ms
step:443/2330 train_time:26449ms step_avg:59.70ms
step:444/2330 train_time:26510ms step_avg:59.71ms
step:445/2330 train_time:26569ms step_avg:59.70ms
step:446/2330 train_time:26630ms step_avg:59.71ms
step:447/2330 train_time:26689ms step_avg:59.71ms
step:448/2330 train_time:26750ms step_avg:59.71ms
step:449/2330 train_time:26808ms step_avg:59.71ms
step:450/2330 train_time:26869ms step_avg:59.71ms
step:451/2330 train_time:26927ms step_avg:59.71ms
step:452/2330 train_time:26988ms step_avg:59.71ms
step:453/2330 train_time:27046ms step_avg:59.70ms
step:454/2330 train_time:27107ms step_avg:59.71ms
step:455/2330 train_time:27165ms step_avg:59.70ms
step:456/2330 train_time:27225ms step_avg:59.70ms
step:457/2330 train_time:27284ms step_avg:59.70ms
step:458/2330 train_time:27344ms step_avg:59.70ms
step:459/2330 train_time:27402ms step_avg:59.70ms
step:460/2330 train_time:27463ms step_avg:59.70ms
step:461/2330 train_time:27521ms step_avg:59.70ms
step:462/2330 train_time:27582ms step_avg:59.70ms
step:463/2330 train_time:27640ms step_avg:59.70ms
step:464/2330 train_time:27701ms step_avg:59.70ms
step:465/2330 train_time:27760ms step_avg:59.70ms
step:466/2330 train_time:27821ms step_avg:59.70ms
step:467/2330 train_time:27880ms step_avg:59.70ms
step:468/2330 train_time:27940ms step_avg:59.70ms
step:469/2330 train_time:27998ms step_avg:59.70ms
step:470/2330 train_time:28059ms step_avg:59.70ms
step:471/2330 train_time:28117ms step_avg:59.70ms
step:472/2330 train_time:28177ms step_avg:59.70ms
step:473/2330 train_time:28235ms step_avg:59.69ms
step:474/2330 train_time:28295ms step_avg:59.69ms
step:475/2330 train_time:28354ms step_avg:59.69ms
step:476/2330 train_time:28414ms step_avg:59.69ms
step:477/2330 train_time:28473ms step_avg:59.69ms
step:478/2330 train_time:28534ms step_avg:59.69ms
step:479/2330 train_time:28592ms step_avg:59.69ms
step:480/2330 train_time:28654ms step_avg:59.70ms
step:481/2330 train_time:28712ms step_avg:59.69ms
step:482/2330 train_time:28774ms step_avg:59.70ms
step:483/2330 train_time:28833ms step_avg:59.70ms
step:484/2330 train_time:28893ms step_avg:59.70ms
step:485/2330 train_time:28952ms step_avg:59.69ms
step:486/2330 train_time:29012ms step_avg:59.70ms
step:487/2330 train_time:29071ms step_avg:59.69ms
step:488/2330 train_time:29131ms step_avg:59.70ms
step:489/2330 train_time:29190ms step_avg:59.69ms
step:490/2330 train_time:29251ms step_avg:59.70ms
step:491/2330 train_time:29310ms step_avg:59.69ms
step:492/2330 train_time:29371ms step_avg:59.70ms
step:493/2330 train_time:29430ms step_avg:59.70ms
step:494/2330 train_time:29490ms step_avg:59.70ms
step:495/2330 train_time:29549ms step_avg:59.70ms
step:496/2330 train_time:29610ms step_avg:59.70ms
step:497/2330 train_time:29668ms step_avg:59.70ms
step:498/2330 train_time:29729ms step_avg:59.70ms
step:499/2330 train_time:29788ms step_avg:59.70ms
step:500/2330 train_time:29849ms step_avg:59.70ms
step:500/2330 val_loss:3.8176 train_time:29912ms step_avg:59.82ms
step:501/2330 train_time:29934ms step_avg:59.75ms
step:502/2330 train_time:29971ms step_avg:59.70ms
step:503/2330 train_time:30034ms step_avg:59.71ms
step:504/2330 train_time:30100ms step_avg:59.72ms
step:505/2330 train_time:30160ms step_avg:59.72ms
step:506/2330 train_time:30221ms step_avg:59.72ms
step:507/2330 train_time:30279ms step_avg:59.72ms
step:508/2330 train_time:30340ms step_avg:59.72ms
step:509/2330 train_time:30399ms step_avg:59.72ms
step:510/2330 train_time:30459ms step_avg:59.72ms
step:511/2330 train_time:30517ms step_avg:59.72ms
step:512/2330 train_time:30577ms step_avg:59.72ms
step:513/2330 train_time:30635ms step_avg:59.72ms
step:514/2330 train_time:30695ms step_avg:59.72ms
step:515/2330 train_time:30753ms step_avg:59.72ms
step:516/2330 train_time:30813ms step_avg:59.72ms
step:517/2330 train_time:30872ms step_avg:59.71ms
step:518/2330 train_time:30933ms step_avg:59.72ms
step:519/2330 train_time:30993ms step_avg:59.72ms
step:520/2330 train_time:31055ms step_avg:59.72ms
step:521/2330 train_time:31115ms step_avg:59.72ms
step:522/2330 train_time:31177ms step_avg:59.73ms
step:523/2330 train_time:31236ms step_avg:59.72ms
step:524/2330 train_time:31297ms step_avg:59.73ms
step:525/2330 train_time:31355ms step_avg:59.72ms
step:526/2330 train_time:31416ms step_avg:59.73ms
step:527/2330 train_time:31475ms step_avg:59.73ms
step:528/2330 train_time:31535ms step_avg:59.73ms
step:529/2330 train_time:31593ms step_avg:59.72ms
step:530/2330 train_time:31653ms step_avg:59.72ms
step:531/2330 train_time:31711ms step_avg:59.72ms
step:532/2330 train_time:31771ms step_avg:59.72ms
step:533/2330 train_time:31830ms step_avg:59.72ms
step:534/2330 train_time:31890ms step_avg:59.72ms
step:535/2330 train_time:31948ms step_avg:59.72ms
step:536/2330 train_time:32010ms step_avg:59.72ms
step:537/2330 train_time:32069ms step_avg:59.72ms
step:538/2330 train_time:32130ms step_avg:59.72ms
step:539/2330 train_time:32189ms step_avg:59.72ms
step:540/2330 train_time:32250ms step_avg:59.72ms
step:541/2330 train_time:32309ms step_avg:59.72ms
step:542/2330 train_time:32371ms step_avg:59.72ms
step:543/2330 train_time:32429ms step_avg:59.72ms
step:544/2330 train_time:32490ms step_avg:59.72ms
step:545/2330 train_time:32548ms step_avg:59.72ms
step:546/2330 train_time:32609ms step_avg:59.72ms
step:547/2330 train_time:32666ms step_avg:59.72ms
step:548/2330 train_time:32727ms step_avg:59.72ms
step:549/2330 train_time:32785ms step_avg:59.72ms
step:550/2330 train_time:32845ms step_avg:59.72ms
step:551/2330 train_time:32903ms step_avg:59.71ms
step:552/2330 train_time:32963ms step_avg:59.72ms
step:553/2330 train_time:33021ms step_avg:59.71ms
step:554/2330 train_time:33082ms step_avg:59.71ms
step:555/2330 train_time:33141ms step_avg:59.71ms
step:556/2330 train_time:33202ms step_avg:59.72ms
step:557/2330 train_time:33261ms step_avg:59.71ms
step:558/2330 train_time:33322ms step_avg:59.72ms
step:559/2330 train_time:33380ms step_avg:59.71ms
step:560/2330 train_time:33441ms step_avg:59.72ms
step:561/2330 train_time:33500ms step_avg:59.71ms
step:562/2330 train_time:33560ms step_avg:59.72ms
step:563/2330 train_time:33619ms step_avg:59.71ms
step:564/2330 train_time:33680ms step_avg:59.72ms
step:565/2330 train_time:33738ms step_avg:59.71ms
step:566/2330 train_time:33798ms step_avg:59.71ms
step:567/2330 train_time:33857ms step_avg:59.71ms
step:568/2330 train_time:33918ms step_avg:59.71ms
step:569/2330 train_time:33976ms step_avg:59.71ms
step:570/2330 train_time:34037ms step_avg:59.71ms
step:571/2330 train_time:34096ms step_avg:59.71ms
step:572/2330 train_time:34157ms step_avg:59.72ms
step:573/2330 train_time:34216ms step_avg:59.71ms
step:574/2330 train_time:34277ms step_avg:59.72ms
step:575/2330 train_time:34336ms step_avg:59.71ms
step:576/2330 train_time:34396ms step_avg:59.72ms
step:577/2330 train_time:34455ms step_avg:59.71ms
step:578/2330 train_time:34516ms step_avg:59.72ms
step:579/2330 train_time:34575ms step_avg:59.72ms
step:580/2330 train_time:34636ms step_avg:59.72ms
step:581/2330 train_time:34694ms step_avg:59.71ms
step:582/2330 train_time:34755ms step_avg:59.72ms
step:583/2330 train_time:34813ms step_avg:59.71ms
step:584/2330 train_time:34873ms step_avg:59.71ms
step:585/2330 train_time:34932ms step_avg:59.71ms
step:586/2330 train_time:34992ms step_avg:59.71ms
step:587/2330 train_time:35051ms step_avg:59.71ms
step:588/2330 train_time:35112ms step_avg:59.71ms
step:589/2330 train_time:35171ms step_avg:59.71ms
step:590/2330 train_time:35232ms step_avg:59.71ms
step:591/2330 train_time:35290ms step_avg:59.71ms
step:592/2330 train_time:35351ms step_avg:59.71ms
step:593/2330 train_time:35409ms step_avg:59.71ms
step:594/2330 train_time:35471ms step_avg:59.71ms
step:595/2330 train_time:35529ms step_avg:59.71ms
step:596/2330 train_time:35590ms step_avg:59.71ms
step:597/2330 train_time:35648ms step_avg:59.71ms
step:598/2330 train_time:35708ms step_avg:59.71ms
step:599/2330 train_time:35767ms step_avg:59.71ms
step:600/2330 train_time:35827ms step_avg:59.71ms
step:601/2330 train_time:35885ms step_avg:59.71ms
step:602/2330 train_time:35945ms step_avg:59.71ms
step:603/2330 train_time:36004ms step_avg:59.71ms
step:604/2330 train_time:36065ms step_avg:59.71ms
step:605/2330 train_time:36124ms step_avg:59.71ms
step:606/2330 train_time:36184ms step_avg:59.71ms
step:607/2330 train_time:36242ms step_avg:59.71ms
step:608/2330 train_time:36303ms step_avg:59.71ms
step:609/2330 train_time:36361ms step_avg:59.71ms
step:610/2330 train_time:36421ms step_avg:59.71ms
step:611/2330 train_time:36481ms step_avg:59.71ms
step:612/2330 train_time:36541ms step_avg:59.71ms
step:613/2330 train_time:36600ms step_avg:59.71ms
step:614/2330 train_time:36660ms step_avg:59.71ms
step:615/2330 train_time:36719ms step_avg:59.71ms
step:616/2330 train_time:36780ms step_avg:59.71ms
step:617/2330 train_time:36839ms step_avg:59.71ms
step:618/2330 train_time:36899ms step_avg:59.71ms
step:619/2330 train_time:36958ms step_avg:59.71ms
step:620/2330 train_time:37019ms step_avg:59.71ms
step:621/2330 train_time:37078ms step_avg:59.71ms
step:622/2330 train_time:37139ms step_avg:59.71ms
step:623/2330 train_time:37198ms step_avg:59.71ms
step:624/2330 train_time:37258ms step_avg:59.71ms
step:625/2330 train_time:37317ms step_avg:59.71ms
step:626/2330 train_time:37378ms step_avg:59.71ms
step:627/2330 train_time:37437ms step_avg:59.71ms
step:628/2330 train_time:37499ms step_avg:59.71ms
step:629/2330 train_time:37558ms step_avg:59.71ms
step:630/2330 train_time:37619ms step_avg:59.71ms
step:631/2330 train_time:37678ms step_avg:59.71ms
step:632/2330 train_time:37739ms step_avg:59.71ms
step:633/2330 train_time:37797ms step_avg:59.71ms
step:634/2330 train_time:37859ms step_avg:59.71ms
step:635/2330 train_time:37917ms step_avg:59.71ms
step:636/2330 train_time:37978ms step_avg:59.71ms
step:637/2330 train_time:38037ms step_avg:59.71ms
step:638/2330 train_time:38098ms step_avg:59.71ms
step:639/2330 train_time:38156ms step_avg:59.71ms
step:640/2330 train_time:38217ms step_avg:59.71ms
step:641/2330 train_time:38275ms step_avg:59.71ms
step:642/2330 train_time:38335ms step_avg:59.71ms
step:643/2330 train_time:38393ms step_avg:59.71ms
step:644/2330 train_time:38454ms step_avg:59.71ms
step:645/2330 train_time:38513ms step_avg:59.71ms
step:646/2330 train_time:38574ms step_avg:59.71ms
step:647/2330 train_time:38633ms step_avg:59.71ms
step:648/2330 train_time:38694ms step_avg:59.71ms
step:649/2330 train_time:38752ms step_avg:59.71ms
step:650/2330 train_time:38813ms step_avg:59.71ms
step:651/2330 train_time:38871ms step_avg:59.71ms
step:652/2330 train_time:38932ms step_avg:59.71ms
step:653/2330 train_time:38991ms step_avg:59.71ms
step:654/2330 train_time:39051ms step_avg:59.71ms
step:655/2330 train_time:39110ms step_avg:59.71ms
step:656/2330 train_time:39171ms step_avg:59.71ms
step:657/2330 train_time:39230ms step_avg:59.71ms
step:658/2330 train_time:39290ms step_avg:59.71ms
step:659/2330 train_time:39347ms step_avg:59.71ms
step:660/2330 train_time:39408ms step_avg:59.71ms
step:661/2330 train_time:39466ms step_avg:59.71ms
step:662/2330 train_time:39526ms step_avg:59.71ms
step:663/2330 train_time:39585ms step_avg:59.71ms
step:664/2330 train_time:39645ms step_avg:59.71ms
step:665/2330 train_time:39704ms step_avg:59.71ms
step:666/2330 train_time:39764ms step_avg:59.71ms
step:667/2330 train_time:39822ms step_avg:59.70ms
step:668/2330 train_time:39883ms step_avg:59.70ms
step:669/2330 train_time:39941ms step_avg:59.70ms
step:670/2330 train_time:40002ms step_avg:59.70ms
step:671/2330 train_time:40060ms step_avg:59.70ms
step:672/2330 train_time:40121ms step_avg:59.70ms
step:673/2330 train_time:40179ms step_avg:59.70ms
step:674/2330 train_time:40240ms step_avg:59.70ms
step:675/2330 train_time:40299ms step_avg:59.70ms
step:676/2330 train_time:40360ms step_avg:59.70ms
step:677/2330 train_time:40419ms step_avg:59.70ms
step:678/2330 train_time:40479ms step_avg:59.70ms
step:679/2330 train_time:40539ms step_avg:59.70ms
step:680/2330 train_time:40599ms step_avg:59.70ms
step:681/2330 train_time:40658ms step_avg:59.70ms
step:682/2330 train_time:40719ms step_avg:59.71ms
step:683/2330 train_time:40777ms step_avg:59.70ms
step:684/2330 train_time:40837ms step_avg:59.70ms
step:685/2330 train_time:40896ms step_avg:59.70ms
step:686/2330 train_time:40956ms step_avg:59.70ms
step:687/2330 train_time:41015ms step_avg:59.70ms
step:688/2330 train_time:41076ms step_avg:59.70ms
step:689/2330 train_time:41134ms step_avg:59.70ms
step:690/2330 train_time:41195ms step_avg:59.70ms
step:691/2330 train_time:41253ms step_avg:59.70ms
step:692/2330 train_time:41314ms step_avg:59.70ms
step:693/2330 train_time:41372ms step_avg:59.70ms
step:694/2330 train_time:41433ms step_avg:59.70ms
step:695/2330 train_time:41492ms step_avg:59.70ms
step:696/2330 train_time:41552ms step_avg:59.70ms
step:697/2330 train_time:41612ms step_avg:59.70ms
step:698/2330 train_time:41672ms step_avg:59.70ms
step:699/2330 train_time:41730ms step_avg:59.70ms
step:700/2330 train_time:41790ms step_avg:59.70ms
step:701/2330 train_time:41849ms step_avg:59.70ms
step:702/2330 train_time:41909ms step_avg:59.70ms
step:703/2330 train_time:41967ms step_avg:59.70ms
step:704/2330 train_time:42027ms step_avg:59.70ms
step:705/2330 train_time:42086ms step_avg:59.70ms
step:706/2330 train_time:42146ms step_avg:59.70ms
step:707/2330 train_time:42205ms step_avg:59.70ms
step:708/2330 train_time:42265ms step_avg:59.70ms
step:709/2330 train_time:42324ms step_avg:59.70ms
step:710/2330 train_time:42384ms step_avg:59.70ms
step:711/2330 train_time:42442ms step_avg:59.69ms
step:712/2330 train_time:42503ms step_avg:59.69ms
step:713/2330 train_time:42561ms step_avg:59.69ms
step:714/2330 train_time:42622ms step_avg:59.69ms
step:715/2330 train_time:42680ms step_avg:59.69ms
step:716/2330 train_time:42741ms step_avg:59.69ms
step:717/2330 train_time:42800ms step_avg:59.69ms
step:718/2330 train_time:42861ms step_avg:59.70ms
step:719/2330 train_time:42920ms step_avg:59.69ms
step:720/2330 train_time:42981ms step_avg:59.70ms
step:721/2330 train_time:43039ms step_avg:59.69ms
step:722/2330 train_time:43100ms step_avg:59.70ms
step:723/2330 train_time:43159ms step_avg:59.69ms
step:724/2330 train_time:43220ms step_avg:59.70ms
step:725/2330 train_time:43279ms step_avg:59.70ms
step:726/2330 train_time:43340ms step_avg:59.70ms
step:727/2330 train_time:43399ms step_avg:59.70ms
step:728/2330 train_time:43460ms step_avg:59.70ms
step:729/2330 train_time:43519ms step_avg:59.70ms
step:730/2330 train_time:43580ms step_avg:59.70ms
step:731/2330 train_time:43638ms step_avg:59.70ms
step:732/2330 train_time:43699ms step_avg:59.70ms
step:733/2330 train_time:43758ms step_avg:59.70ms
step:734/2330 train_time:43818ms step_avg:59.70ms
step:735/2330 train_time:43877ms step_avg:59.70ms
step:736/2330 train_time:43938ms step_avg:59.70ms
step:737/2330 train_time:43997ms step_avg:59.70ms
step:738/2330 train_time:44058ms step_avg:59.70ms
step:739/2330 train_time:44118ms step_avg:59.70ms
step:740/2330 train_time:44179ms step_avg:59.70ms
step:741/2330 train_time:44238ms step_avg:59.70ms
step:742/2330 train_time:44298ms step_avg:59.70ms
step:743/2330 train_time:44357ms step_avg:59.70ms
step:744/2330 train_time:44418ms step_avg:59.70ms
step:745/2330 train_time:44476ms step_avg:59.70ms
step:746/2330 train_time:44537ms step_avg:59.70ms
step:747/2330 train_time:44596ms step_avg:59.70ms
step:748/2330 train_time:44657ms step_avg:59.70ms
step:749/2330 train_time:44715ms step_avg:59.70ms
step:750/2330 train_time:44776ms step_avg:59.70ms
step:750/2330 val_loss:3.6897 train_time:44838ms step_avg:59.78ms
step:751/2330 train_time:44860ms step_avg:59.73ms
step:752/2330 train_time:44899ms step_avg:59.71ms
step:753/2330 train_time:44960ms step_avg:59.71ms
step:754/2330 train_time:45028ms step_avg:59.72ms
step:755/2330 train_time:45087ms step_avg:59.72ms
step:756/2330 train_time:45148ms step_avg:59.72ms
step:757/2330 train_time:45207ms step_avg:59.72ms
step:758/2330 train_time:45266ms step_avg:59.72ms
step:759/2330 train_time:45326ms step_avg:59.72ms
step:760/2330 train_time:45386ms step_avg:59.72ms
step:761/2330 train_time:45444ms step_avg:59.72ms
step:762/2330 train_time:45504ms step_avg:59.72ms
step:763/2330 train_time:45561ms step_avg:59.71ms
step:764/2330 train_time:45622ms step_avg:59.71ms
step:765/2330 train_time:45680ms step_avg:59.71ms
step:766/2330 train_time:45741ms step_avg:59.71ms
step:767/2330 train_time:45800ms step_avg:59.71ms
step:768/2330 train_time:45862ms step_avg:59.72ms
step:769/2330 train_time:45922ms step_avg:59.72ms
step:770/2330 train_time:45984ms step_avg:59.72ms
step:771/2330 train_time:46044ms step_avg:59.72ms
step:772/2330 train_time:46106ms step_avg:59.72ms
step:773/2330 train_time:46166ms step_avg:59.72ms
step:774/2330 train_time:46228ms step_avg:59.73ms
step:775/2330 train_time:46287ms step_avg:59.73ms
step:776/2330 train_time:46348ms step_avg:59.73ms
step:777/2330 train_time:46408ms step_avg:59.73ms
step:778/2330 train_time:46469ms step_avg:59.73ms
step:779/2330 train_time:46528ms step_avg:59.73ms
step:780/2330 train_time:46589ms step_avg:59.73ms
step:781/2330 train_time:46647ms step_avg:59.73ms
step:782/2330 train_time:46708ms step_avg:59.73ms
step:783/2330 train_time:46768ms step_avg:59.73ms
step:784/2330 train_time:46830ms step_avg:59.73ms
step:785/2330 train_time:46890ms step_avg:59.73ms
step:786/2330 train_time:46952ms step_avg:59.74ms
step:787/2330 train_time:47012ms step_avg:59.74ms
step:788/2330 train_time:47073ms step_avg:59.74ms
step:789/2330 train_time:47133ms step_avg:59.74ms
step:790/2330 train_time:47194ms step_avg:59.74ms
step:791/2330 train_time:47253ms step_avg:59.74ms
step:792/2330 train_time:47314ms step_avg:59.74ms
step:793/2330 train_time:47374ms step_avg:59.74ms
step:794/2330 train_time:47435ms step_avg:59.74ms
step:795/2330 train_time:47494ms step_avg:59.74ms
step:796/2330 train_time:47555ms step_avg:59.74ms
step:797/2330 train_time:47614ms step_avg:59.74ms
step:798/2330 train_time:47675ms step_avg:59.74ms
step:799/2330 train_time:47734ms step_avg:59.74ms
step:800/2330 train_time:47795ms step_avg:59.74ms
step:801/2330 train_time:47854ms step_avg:59.74ms
step:802/2330 train_time:47915ms step_avg:59.74ms
step:803/2330 train_time:47974ms step_avg:59.74ms
step:804/2330 train_time:48036ms step_avg:59.75ms
step:805/2330 train_time:48095ms step_avg:59.75ms
step:806/2330 train_time:48156ms step_avg:59.75ms
step:807/2330 train_time:48216ms step_avg:59.75ms
step:808/2330 train_time:48277ms step_avg:59.75ms
step:809/2330 train_time:48336ms step_avg:59.75ms
step:810/2330 train_time:48397ms step_avg:59.75ms
step:811/2330 train_time:48457ms step_avg:59.75ms
step:812/2330 train_time:48518ms step_avg:59.75ms
step:813/2330 train_time:48577ms step_avg:59.75ms
step:814/2330 train_time:48639ms step_avg:59.75ms
step:815/2330 train_time:48697ms step_avg:59.75ms
step:816/2330 train_time:48758ms step_avg:59.75ms
step:817/2330 train_time:48818ms step_avg:59.75ms
step:818/2330 train_time:48879ms step_avg:59.75ms
step:819/2330 train_time:48938ms step_avg:59.75ms
step:820/2330 train_time:49000ms step_avg:59.76ms
step:821/2330 train_time:49059ms step_avg:59.76ms
step:822/2330 train_time:49121ms step_avg:59.76ms
step:823/2330 train_time:49180ms step_avg:59.76ms
step:824/2330 train_time:49241ms step_avg:59.76ms
step:825/2330 train_time:49300ms step_avg:59.76ms
step:826/2330 train_time:49362ms step_avg:59.76ms
step:827/2330 train_time:49421ms step_avg:59.76ms
step:828/2330 train_time:49482ms step_avg:59.76ms
step:829/2330 train_time:49540ms step_avg:59.76ms
step:830/2330 train_time:49601ms step_avg:59.76ms
step:831/2330 train_time:49660ms step_avg:59.76ms
step:832/2330 train_time:49722ms step_avg:59.76ms
step:833/2330 train_time:49781ms step_avg:59.76ms
step:834/2330 train_time:49843ms step_avg:59.76ms
step:835/2330 train_time:49902ms step_avg:59.76ms
step:836/2330 train_time:49963ms step_avg:59.76ms
step:837/2330 train_time:50022ms step_avg:59.76ms
step:838/2330 train_time:50084ms step_avg:59.77ms
step:839/2330 train_time:50143ms step_avg:59.77ms
step:840/2330 train_time:50205ms step_avg:59.77ms
step:841/2330 train_time:50264ms step_avg:59.77ms
step:842/2330 train_time:50325ms step_avg:59.77ms
step:843/2330 train_time:50385ms step_avg:59.77ms
step:844/2330 train_time:50446ms step_avg:59.77ms
step:845/2330 train_time:50505ms step_avg:59.77ms
step:846/2330 train_time:50566ms step_avg:59.77ms
step:847/2330 train_time:50626ms step_avg:59.77ms
step:848/2330 train_time:50688ms step_avg:59.77ms
step:849/2330 train_time:50747ms step_avg:59.77ms
step:850/2330 train_time:50809ms step_avg:59.78ms
step:851/2330 train_time:50869ms step_avg:59.78ms
step:852/2330 train_time:50931ms step_avg:59.78ms
step:853/2330 train_time:50990ms step_avg:59.78ms
step:854/2330 train_time:51051ms step_avg:59.78ms
step:855/2330 train_time:51111ms step_avg:59.78ms
step:856/2330 train_time:51173ms step_avg:59.78ms
step:857/2330 train_time:51232ms step_avg:59.78ms
step:858/2330 train_time:51293ms step_avg:59.78ms
step:859/2330 train_time:51352ms step_avg:59.78ms
step:860/2330 train_time:51413ms step_avg:59.78ms
step:861/2330 train_time:51472ms step_avg:59.78ms
step:862/2330 train_time:51533ms step_avg:59.78ms
step:863/2330 train_time:51592ms step_avg:59.78ms
step:864/2330 train_time:51653ms step_avg:59.78ms
step:865/2330 train_time:51713ms step_avg:59.78ms
step:866/2330 train_time:51774ms step_avg:59.79ms
step:867/2330 train_time:51833ms step_avg:59.78ms
step:868/2330 train_time:51895ms step_avg:59.79ms
step:869/2330 train_time:51954ms step_avg:59.79ms
step:870/2330 train_time:52016ms step_avg:59.79ms
step:871/2330 train_time:52075ms step_avg:59.79ms
step:872/2330 train_time:52136ms step_avg:59.79ms
step:873/2330 train_time:52195ms step_avg:59.79ms
step:874/2330 train_time:52256ms step_avg:59.79ms
step:875/2330 train_time:52316ms step_avg:59.79ms
step:876/2330 train_time:52377ms step_avg:59.79ms
step:877/2330 train_time:52436ms step_avg:59.79ms
step:878/2330 train_time:52497ms step_avg:59.79ms
step:879/2330 train_time:52557ms step_avg:59.79ms
step:880/2330 train_time:52618ms step_avg:59.79ms
step:881/2330 train_time:52678ms step_avg:59.79ms
step:882/2330 train_time:52739ms step_avg:59.79ms
step:883/2330 train_time:52798ms step_avg:59.79ms
step:884/2330 train_time:52860ms step_avg:59.80ms
step:885/2330 train_time:52920ms step_avg:59.80ms
step:886/2330 train_time:52981ms step_avg:59.80ms
step:887/2330 train_time:53040ms step_avg:59.80ms
step:888/2330 train_time:53101ms step_avg:59.80ms
step:889/2330 train_time:53160ms step_avg:59.80ms
step:890/2330 train_time:53222ms step_avg:59.80ms
step:891/2330 train_time:53281ms step_avg:59.80ms
step:892/2330 train_time:53342ms step_avg:59.80ms
step:893/2330 train_time:53400ms step_avg:59.80ms
step:894/2330 train_time:53461ms step_avg:59.80ms
step:895/2330 train_time:53520ms step_avg:59.80ms
step:896/2330 train_time:53582ms step_avg:59.80ms
step:897/2330 train_time:53641ms step_avg:59.80ms
step:898/2330 train_time:53702ms step_avg:59.80ms
step:899/2330 train_time:53762ms step_avg:59.80ms
step:900/2330 train_time:53823ms step_avg:59.80ms
step:901/2330 train_time:53883ms step_avg:59.80ms
step:902/2330 train_time:53944ms step_avg:59.80ms
step:903/2330 train_time:54003ms step_avg:59.80ms
step:904/2330 train_time:54064ms step_avg:59.81ms
step:905/2330 train_time:54123ms step_avg:59.80ms
step:906/2330 train_time:54185ms step_avg:59.81ms
step:907/2330 train_time:54244ms step_avg:59.81ms
step:908/2330 train_time:54305ms step_avg:59.81ms
step:909/2330 train_time:54364ms step_avg:59.81ms
step:910/2330 train_time:54425ms step_avg:59.81ms
step:911/2330 train_time:54485ms step_avg:59.81ms
step:912/2330 train_time:54546ms step_avg:59.81ms
step:913/2330 train_time:54606ms step_avg:59.81ms
step:914/2330 train_time:54668ms step_avg:59.81ms
step:915/2330 train_time:54728ms step_avg:59.81ms
step:916/2330 train_time:54789ms step_avg:59.81ms
step:917/2330 train_time:54848ms step_avg:59.81ms
step:918/2330 train_time:54910ms step_avg:59.81ms
step:919/2330 train_time:54969ms step_avg:59.81ms
step:920/2330 train_time:55031ms step_avg:59.82ms
step:921/2330 train_time:55090ms step_avg:59.82ms
step:922/2330 train_time:55151ms step_avg:59.82ms
step:923/2330 train_time:55211ms step_avg:59.82ms
step:924/2330 train_time:55272ms step_avg:59.82ms
step:925/2330 train_time:55331ms step_avg:59.82ms
step:926/2330 train_time:55393ms step_avg:59.82ms
step:927/2330 train_time:55452ms step_avg:59.82ms
step:928/2330 train_time:55513ms step_avg:59.82ms
step:929/2330 train_time:55573ms step_avg:59.82ms
step:930/2330 train_time:55634ms step_avg:59.82ms
step:931/2330 train_time:55693ms step_avg:59.82ms
step:932/2330 train_time:55754ms step_avg:59.82ms
step:933/2330 train_time:55813ms step_avg:59.82ms
step:934/2330 train_time:55875ms step_avg:59.82ms
step:935/2330 train_time:55934ms step_avg:59.82ms
step:936/2330 train_time:55995ms step_avg:59.82ms
step:937/2330 train_time:56055ms step_avg:59.82ms
step:938/2330 train_time:56116ms step_avg:59.83ms
step:939/2330 train_time:56176ms step_avg:59.82ms
step:940/2330 train_time:56237ms step_avg:59.83ms
step:941/2330 train_time:56296ms step_avg:59.83ms
step:942/2330 train_time:56357ms step_avg:59.83ms
step:943/2330 train_time:56416ms step_avg:59.83ms
step:944/2330 train_time:56478ms step_avg:59.83ms
step:945/2330 train_time:56537ms step_avg:59.83ms
step:946/2330 train_time:56598ms step_avg:59.83ms
step:947/2330 train_time:56658ms step_avg:59.83ms
step:948/2330 train_time:56720ms step_avg:59.83ms
step:949/2330 train_time:56779ms step_avg:59.83ms
step:950/2330 train_time:56840ms step_avg:59.83ms
step:951/2330 train_time:56899ms step_avg:59.83ms
step:952/2330 train_time:56962ms step_avg:59.83ms
step:953/2330 train_time:57021ms step_avg:59.83ms
step:954/2330 train_time:57082ms step_avg:59.83ms
step:955/2330 train_time:57141ms step_avg:59.83ms
step:956/2330 train_time:57202ms step_avg:59.83ms
step:957/2330 train_time:57261ms step_avg:59.83ms
step:958/2330 train_time:57323ms step_avg:59.84ms
step:959/2330 train_time:57382ms step_avg:59.83ms
step:960/2330 train_time:57443ms step_avg:59.84ms
step:961/2330 train_time:57502ms step_avg:59.84ms
step:962/2330 train_time:57563ms step_avg:59.84ms
step:963/2330 train_time:57622ms step_avg:59.84ms
step:964/2330 train_time:57684ms step_avg:59.84ms
step:965/2330 train_time:57743ms step_avg:59.84ms
step:966/2330 train_time:57804ms step_avg:59.84ms
step:967/2330 train_time:57863ms step_avg:59.84ms
step:968/2330 train_time:57925ms step_avg:59.84ms
step:969/2330 train_time:57985ms step_avg:59.84ms
step:970/2330 train_time:58046ms step_avg:59.84ms
step:971/2330 train_time:58105ms step_avg:59.84ms
step:972/2330 train_time:58167ms step_avg:59.84ms
step:973/2330 train_time:58226ms step_avg:59.84ms
step:974/2330 train_time:58287ms step_avg:59.84ms
step:975/2330 train_time:58347ms step_avg:59.84ms
step:976/2330 train_time:58408ms step_avg:59.84ms
step:977/2330 train_time:58467ms step_avg:59.84ms
step:978/2330 train_time:58529ms step_avg:59.85ms
step:979/2330 train_time:58589ms step_avg:59.85ms
step:980/2330 train_time:58651ms step_avg:59.85ms
step:981/2330 train_time:58710ms step_avg:59.85ms
step:982/2330 train_time:58772ms step_avg:59.85ms
step:983/2330 train_time:58831ms step_avg:59.85ms
step:984/2330 train_time:58893ms step_avg:59.85ms
step:985/2330 train_time:58951ms step_avg:59.85ms
step:986/2330 train_time:59013ms step_avg:59.85ms
step:987/2330 train_time:59072ms step_avg:59.85ms
step:988/2330 train_time:59133ms step_avg:59.85ms
step:989/2330 train_time:59191ms step_avg:59.85ms
step:990/2330 train_time:59252ms step_avg:59.85ms
step:991/2330 train_time:59311ms step_avg:59.85ms
step:992/2330 train_time:59373ms step_avg:59.85ms
step:993/2330 train_time:59432ms step_avg:59.85ms
step:994/2330 train_time:59493ms step_avg:59.85ms
step:995/2330 train_time:59552ms step_avg:59.85ms
step:996/2330 train_time:59613ms step_avg:59.85ms
step:997/2330 train_time:59672ms step_avg:59.85ms
step:998/2330 train_time:59733ms step_avg:59.85ms
step:999/2330 train_time:59792ms step_avg:59.85ms
step:1000/2330 train_time:59853ms step_avg:59.85ms
step:1000/2330 val_loss:3.5759 train_time:59917ms step_avg:59.92ms
step:1001/2330 train_time:59938ms step_avg:59.88ms
step:1002/2330 train_time:59977ms step_avg:59.86ms
step:1003/2330 train_time:60036ms step_avg:59.86ms
step:1004/2330 train_time:60098ms step_avg:59.86ms
step:1005/2330 train_time:60159ms step_avg:59.86ms
step:1006/2330 train_time:60221ms step_avg:59.86ms
step:1007/2330 train_time:60280ms step_avg:59.86ms
step:1008/2330 train_time:60340ms step_avg:59.86ms
step:1009/2330 train_time:60399ms step_avg:59.86ms
step:1010/2330 train_time:60459ms step_avg:59.86ms
step:1011/2330 train_time:60518ms step_avg:59.86ms
step:1012/2330 train_time:60578ms step_avg:59.86ms
step:1013/2330 train_time:60637ms step_avg:59.86ms
step:1014/2330 train_time:60698ms step_avg:59.86ms
step:1015/2330 train_time:60756ms step_avg:59.86ms
step:1016/2330 train_time:60819ms step_avg:59.86ms
step:1017/2330 train_time:60882ms step_avg:59.86ms
step:1018/2330 train_time:60945ms step_avg:59.87ms
step:1019/2330 train_time:61004ms step_avg:59.87ms
step:1020/2330 train_time:61066ms step_avg:59.87ms
step:1021/2330 train_time:61126ms step_avg:59.87ms
step:1022/2330 train_time:61187ms step_avg:59.87ms
step:1023/2330 train_time:61246ms step_avg:59.87ms
step:1024/2330 train_time:61307ms step_avg:59.87ms
step:1025/2330 train_time:61366ms step_avg:59.87ms
step:1026/2330 train_time:61426ms step_avg:59.87ms
step:1027/2330 train_time:61485ms step_avg:59.87ms
step:1028/2330 train_time:61546ms step_avg:59.87ms
step:1029/2330 train_time:61605ms step_avg:59.87ms
step:1030/2330 train_time:61666ms step_avg:59.87ms
step:1031/2330 train_time:61725ms step_avg:59.87ms
step:1032/2330 train_time:61788ms step_avg:59.87ms
step:1033/2330 train_time:61847ms step_avg:59.87ms
step:1034/2330 train_time:61909ms step_avg:59.87ms
step:1035/2330 train_time:61968ms step_avg:59.87ms
step:1036/2330 train_time:62030ms step_avg:59.87ms
step:1037/2330 train_time:62090ms step_avg:59.87ms
step:1038/2330 train_time:62152ms step_avg:59.88ms
step:1039/2330 train_time:62212ms step_avg:59.88ms
step:1040/2330 train_time:62273ms step_avg:59.88ms
step:1041/2330 train_time:62332ms step_avg:59.88ms
step:1042/2330 train_time:62394ms step_avg:59.88ms
step:1043/2330 train_time:62454ms step_avg:59.88ms
step:1044/2330 train_time:62515ms step_avg:59.88ms
step:1045/2330 train_time:62574ms step_avg:59.88ms
step:1046/2330 train_time:62636ms step_avg:59.88ms
step:1047/2330 train_time:62696ms step_avg:59.88ms
step:1048/2330 train_time:62757ms step_avg:59.88ms
step:1049/2330 train_time:62817ms step_avg:59.88ms
step:1050/2330 train_time:62879ms step_avg:59.88ms
step:1051/2330 train_time:62938ms step_avg:59.88ms
step:1052/2330 train_time:62999ms step_avg:59.89ms
step:1053/2330 train_time:63059ms step_avg:59.88ms
step:1054/2330 train_time:63119ms step_avg:59.89ms
step:1055/2330 train_time:63179ms step_avg:59.89ms
step:1056/2330 train_time:63240ms step_avg:59.89ms
step:1057/2330 train_time:63299ms step_avg:59.89ms
step:1058/2330 train_time:63360ms step_avg:59.89ms
step:1059/2330 train_time:63420ms step_avg:59.89ms
step:1060/2330 train_time:63481ms step_avg:59.89ms
step:1061/2330 train_time:63541ms step_avg:59.89ms
step:1062/2330 train_time:63602ms step_avg:59.89ms
step:1063/2330 train_time:63662ms step_avg:59.89ms
step:1064/2330 train_time:63723ms step_avg:59.89ms
step:1065/2330 train_time:63782ms step_avg:59.89ms
step:1066/2330 train_time:63844ms step_avg:59.89ms
step:1067/2330 train_time:63902ms step_avg:59.89ms
step:1068/2330 train_time:63964ms step_avg:59.89ms
step:1069/2330 train_time:64023ms step_avg:59.89ms
step:1070/2330 train_time:64085ms step_avg:59.89ms
step:1071/2330 train_time:64144ms step_avg:59.89ms
step:1072/2330 train_time:64206ms step_avg:59.89ms
step:1073/2330 train_time:64265ms step_avg:59.89ms
step:1074/2330 train_time:64326ms step_avg:59.89ms
step:1075/2330 train_time:64386ms step_avg:59.89ms
step:1076/2330 train_time:64447ms step_avg:59.90ms
step:1077/2330 train_time:64506ms step_avg:59.89ms
step:1078/2330 train_time:64567ms step_avg:59.90ms
step:1079/2330 train_time:64626ms step_avg:59.89ms
step:1080/2330 train_time:64688ms step_avg:59.90ms
step:1081/2330 train_time:64746ms step_avg:59.89ms
step:1082/2330 train_time:64808ms step_avg:59.90ms
step:1083/2330 train_time:64867ms step_avg:59.90ms
step:1084/2330 train_time:64928ms step_avg:59.90ms
step:1085/2330 train_time:64987ms step_avg:59.90ms
step:1086/2330 train_time:65048ms step_avg:59.90ms
step:1087/2330 train_time:65108ms step_avg:59.90ms
step:1088/2330 train_time:65169ms step_avg:59.90ms
step:1089/2330 train_time:65228ms step_avg:59.90ms
step:1090/2330 train_time:65289ms step_avg:59.90ms
step:1091/2330 train_time:65349ms step_avg:59.90ms
step:1092/2330 train_time:65410ms step_avg:59.90ms
step:1093/2330 train_time:65469ms step_avg:59.90ms
step:1094/2330 train_time:65530ms step_avg:59.90ms
step:1095/2330 train_time:65589ms step_avg:59.90ms
step:1096/2330 train_time:65651ms step_avg:59.90ms
step:1097/2330 train_time:65712ms step_avg:59.90ms
step:1098/2330 train_time:65773ms step_avg:59.90ms
step:1099/2330 train_time:65833ms step_avg:59.90ms
step:1100/2330 train_time:65895ms step_avg:59.90ms
step:1101/2330 train_time:65954ms step_avg:59.90ms
step:1102/2330 train_time:66016ms step_avg:59.91ms
step:1103/2330 train_time:66076ms step_avg:59.91ms
step:1104/2330 train_time:66137ms step_avg:59.91ms
step:1105/2330 train_time:66198ms step_avg:59.91ms
step:1106/2330 train_time:66259ms step_avg:59.91ms
step:1107/2330 train_time:66318ms step_avg:59.91ms
step:1108/2330 train_time:66379ms step_avg:59.91ms
step:1109/2330 train_time:66438ms step_avg:59.91ms
step:1110/2330 train_time:66499ms step_avg:59.91ms
step:1111/2330 train_time:66558ms step_avg:59.91ms
step:1112/2330 train_time:66619ms step_avg:59.91ms
step:1113/2330 train_time:66678ms step_avg:59.91ms
step:1114/2330 train_time:66740ms step_avg:59.91ms
step:1115/2330 train_time:66799ms step_avg:59.91ms
step:1116/2330 train_time:66860ms step_avg:59.91ms
step:1117/2330 train_time:66919ms step_avg:59.91ms
step:1118/2330 train_time:66981ms step_avg:59.91ms
step:1119/2330 train_time:67040ms step_avg:59.91ms
step:1120/2330 train_time:67102ms step_avg:59.91ms
step:1121/2330 train_time:67161ms step_avg:59.91ms
step:1122/2330 train_time:67223ms step_avg:59.91ms
step:1123/2330 train_time:67283ms step_avg:59.91ms
step:1124/2330 train_time:67344ms step_avg:59.91ms
step:1125/2330 train_time:67403ms step_avg:59.91ms
step:1126/2330 train_time:67464ms step_avg:59.91ms
step:1127/2330 train_time:67523ms step_avg:59.91ms
step:1128/2330 train_time:67584ms step_avg:59.92ms
step:1129/2330 train_time:67644ms step_avg:59.91ms
step:1130/2330 train_time:67705ms step_avg:59.92ms
step:1131/2330 train_time:67765ms step_avg:59.92ms
step:1132/2330 train_time:67826ms step_avg:59.92ms
step:1133/2330 train_time:67885ms step_avg:59.92ms
step:1134/2330 train_time:67946ms step_avg:59.92ms
step:1135/2330 train_time:68006ms step_avg:59.92ms
step:1136/2330 train_time:68067ms step_avg:59.92ms
step:1137/2330 train_time:68127ms step_avg:59.92ms
step:1138/2330 train_time:68188ms step_avg:59.92ms
step:1139/2330 train_time:68247ms step_avg:59.92ms
step:1140/2330 train_time:68308ms step_avg:59.92ms
step:1141/2330 train_time:68367ms step_avg:59.92ms
step:1142/2330 train_time:68428ms step_avg:59.92ms
step:1143/2330 train_time:68488ms step_avg:59.92ms
step:1144/2330 train_time:68549ms step_avg:59.92ms
step:1145/2330 train_time:68608ms step_avg:59.92ms
step:1146/2330 train_time:68669ms step_avg:59.92ms
step:1147/2330 train_time:68729ms step_avg:59.92ms
step:1148/2330 train_time:68790ms step_avg:59.92ms
step:1149/2330 train_time:68851ms step_avg:59.92ms
step:1150/2330 train_time:68913ms step_avg:59.92ms
step:1151/2330 train_time:68972ms step_avg:59.92ms
step:1152/2330 train_time:69034ms step_avg:59.93ms
step:1153/2330 train_time:69094ms step_avg:59.93ms
step:1154/2330 train_time:69155ms step_avg:59.93ms
step:1155/2330 train_time:69215ms step_avg:59.93ms
step:1156/2330 train_time:69276ms step_avg:59.93ms
step:1157/2330 train_time:69336ms step_avg:59.93ms
step:1158/2330 train_time:69397ms step_avg:59.93ms
step:1159/2330 train_time:69457ms step_avg:59.93ms
step:1160/2330 train_time:69517ms step_avg:59.93ms
step:1161/2330 train_time:69577ms step_avg:59.93ms
step:1162/2330 train_time:69639ms step_avg:59.93ms
step:1163/2330 train_time:69698ms step_avg:59.93ms
step:1164/2330 train_time:69759ms step_avg:59.93ms
step:1165/2330 train_time:69818ms step_avg:59.93ms
step:1166/2330 train_time:69879ms step_avg:59.93ms
step:1167/2330 train_time:69938ms step_avg:59.93ms
step:1168/2330 train_time:69999ms step_avg:59.93ms
step:1169/2330 train_time:70058ms step_avg:59.93ms
step:1170/2330 train_time:70120ms step_avg:59.93ms
step:1171/2330 train_time:70179ms step_avg:59.93ms
step:1172/2330 train_time:70241ms step_avg:59.93ms
step:1173/2330 train_time:70300ms step_avg:59.93ms
step:1174/2330 train_time:70361ms step_avg:59.93ms
step:1175/2330 train_time:70420ms step_avg:59.93ms
step:1176/2330 train_time:70482ms step_avg:59.93ms
step:1177/2330 train_time:70540ms step_avg:59.93ms
step:1178/2330 train_time:70602ms step_avg:59.93ms
step:1179/2330 train_time:70661ms step_avg:59.93ms
step:1180/2330 train_time:70722ms step_avg:59.93ms
step:1181/2330 train_time:70781ms step_avg:59.93ms
step:1182/2330 train_time:70842ms step_avg:59.93ms
step:1183/2330 train_time:70902ms step_avg:59.93ms
step:1184/2330 train_time:70963ms step_avg:59.94ms
step:1185/2330 train_time:71023ms step_avg:59.93ms
step:1186/2330 train_time:71085ms step_avg:59.94ms
step:1187/2330 train_time:71144ms step_avg:59.94ms
step:1188/2330 train_time:71206ms step_avg:59.94ms
step:1189/2330 train_time:71265ms step_avg:59.94ms
step:1190/2330 train_time:71327ms step_avg:59.94ms
step:1191/2330 train_time:71386ms step_avg:59.94ms
step:1192/2330 train_time:71448ms step_avg:59.94ms
step:1193/2330 train_time:71506ms step_avg:59.94ms
step:1194/2330 train_time:71567ms step_avg:59.94ms
step:1195/2330 train_time:71626ms step_avg:59.94ms
step:1196/2330 train_time:71687ms step_avg:59.94ms
step:1197/2330 train_time:71747ms step_avg:59.94ms
step:1198/2330 train_time:71808ms step_avg:59.94ms
step:1199/2330 train_time:71867ms step_avg:59.94ms
step:1200/2330 train_time:71929ms step_avg:59.94ms
step:1201/2330 train_time:71988ms step_avg:59.94ms
step:1202/2330 train_time:72050ms step_avg:59.94ms
step:1203/2330 train_time:72109ms step_avg:59.94ms
step:1204/2330 train_time:72171ms step_avg:59.94ms
step:1205/2330 train_time:72231ms step_avg:59.94ms
step:1206/2330 train_time:72292ms step_avg:59.94ms
step:1207/2330 train_time:72352ms step_avg:59.94ms
step:1208/2330 train_time:72413ms step_avg:59.94ms
step:1209/2330 train_time:72472ms step_avg:59.94ms
step:1210/2330 train_time:72534ms step_avg:59.95ms
step:1211/2330 train_time:72593ms step_avg:59.94ms
step:1212/2330 train_time:72655ms step_avg:59.95ms
step:1213/2330 train_time:72714ms step_avg:59.95ms
step:1214/2330 train_time:72776ms step_avg:59.95ms
step:1215/2330 train_time:72835ms step_avg:59.95ms
step:1216/2330 train_time:72897ms step_avg:59.95ms
step:1217/2330 train_time:72957ms step_avg:59.95ms
step:1218/2330 train_time:73018ms step_avg:59.95ms
step:1219/2330 train_time:73077ms step_avg:59.95ms
step:1220/2330 train_time:73139ms step_avg:59.95ms
step:1221/2330 train_time:73198ms step_avg:59.95ms
step:1222/2330 train_time:73259ms step_avg:59.95ms
step:1223/2330 train_time:73318ms step_avg:59.95ms
step:1224/2330 train_time:73379ms step_avg:59.95ms
step:1225/2330 train_time:73438ms step_avg:59.95ms
step:1226/2330 train_time:73500ms step_avg:59.95ms
step:1227/2330 train_time:73559ms step_avg:59.95ms
step:1228/2330 train_time:73620ms step_avg:59.95ms
step:1229/2330 train_time:73679ms step_avg:59.95ms
step:1230/2330 train_time:73740ms step_avg:59.95ms
step:1231/2330 train_time:73800ms step_avg:59.95ms
step:1232/2330 train_time:73861ms step_avg:59.95ms
step:1233/2330 train_time:73920ms step_avg:59.95ms
step:1234/2330 train_time:73981ms step_avg:59.95ms
step:1235/2330 train_time:74040ms step_avg:59.95ms
step:1236/2330 train_time:74101ms step_avg:59.95ms
step:1237/2330 train_time:74160ms step_avg:59.95ms
step:1238/2330 train_time:74221ms step_avg:59.95ms
step:1239/2330 train_time:74281ms step_avg:59.95ms
step:1240/2330 train_time:74342ms step_avg:59.95ms
step:1241/2330 train_time:74402ms step_avg:59.95ms
step:1242/2330 train_time:74463ms step_avg:59.95ms
step:1243/2330 train_time:74522ms step_avg:59.95ms
step:1244/2330 train_time:74584ms step_avg:59.95ms
step:1245/2330 train_time:74643ms step_avg:59.95ms
step:1246/2330 train_time:74704ms step_avg:59.96ms
step:1247/2330 train_time:74764ms step_avg:59.95ms
step:1248/2330 train_time:74826ms step_avg:59.96ms
step:1249/2330 train_time:74885ms step_avg:59.96ms
step:1250/2330 train_time:74947ms step_avg:59.96ms
step:1250/2330 val_loss:3.5182 train_time:75010ms step_avg:60.01ms
step:1251/2330 train_time:75032ms step_avg:59.98ms
step:1252/2330 train_time:75070ms step_avg:59.96ms
step:1253/2330 train_time:75135ms step_avg:59.96ms
step:1254/2330 train_time:75200ms step_avg:59.97ms
step:1255/2330 train_time:75259ms step_avg:59.97ms
step:1256/2330 train_time:75322ms step_avg:59.97ms
step:1257/2330 train_time:75382ms step_avg:59.97ms
step:1258/2330 train_time:75442ms step_avg:59.97ms
step:1259/2330 train_time:75500ms step_avg:59.97ms
step:1260/2330 train_time:75561ms step_avg:59.97ms
step:1261/2330 train_time:75619ms step_avg:59.97ms
step:1262/2330 train_time:75680ms step_avg:59.97ms
step:1263/2330 train_time:75738ms step_avg:59.97ms
step:1264/2330 train_time:75799ms step_avg:59.97ms
step:1265/2330 train_time:75857ms step_avg:59.97ms
step:1266/2330 train_time:75917ms step_avg:59.97ms
step:1267/2330 train_time:75977ms step_avg:59.97ms
step:1268/2330 train_time:76039ms step_avg:59.97ms
step:1269/2330 train_time:76100ms step_avg:59.97ms
step:1270/2330 train_time:76163ms step_avg:59.97ms
step:1271/2330 train_time:76223ms step_avg:59.97ms
step:1272/2330 train_time:76285ms step_avg:59.97ms
step:1273/2330 train_time:76344ms step_avg:59.97ms
step:1274/2330 train_time:76405ms step_avg:59.97ms
step:1275/2330 train_time:76464ms step_avg:59.97ms
step:1276/2330 train_time:76526ms step_avg:59.97ms
step:1277/2330 train_time:76585ms step_avg:59.97ms
step:1278/2330 train_time:76645ms step_avg:59.97ms
step:1279/2330 train_time:76704ms step_avg:59.97ms
step:1280/2330 train_time:76764ms step_avg:59.97ms
step:1281/2330 train_time:76823ms step_avg:59.97ms
step:1282/2330 train_time:76884ms step_avg:59.97ms
step:1283/2330 train_time:76944ms step_avg:59.97ms
step:1284/2330 train_time:77005ms step_avg:59.97ms
step:1285/2330 train_time:77065ms step_avg:59.97ms
step:1286/2330 train_time:77128ms step_avg:59.97ms
step:1287/2330 train_time:77188ms step_avg:59.97ms
step:1288/2330 train_time:77249ms step_avg:59.98ms
step:1289/2330 train_time:77308ms step_avg:59.98ms
step:1290/2330 train_time:77370ms step_avg:59.98ms
step:1291/2330 train_time:77430ms step_avg:59.98ms
step:1292/2330 train_time:77492ms step_avg:59.98ms
step:1293/2330 train_time:77551ms step_avg:59.98ms
step:1294/2330 train_time:77612ms step_avg:59.98ms
step:1295/2330 train_time:77672ms step_avg:59.98ms
step:1296/2330 train_time:77733ms step_avg:59.98ms
step:1297/2330 train_time:77792ms step_avg:59.98ms
step:1298/2330 train_time:77853ms step_avg:59.98ms
step:1299/2330 train_time:77914ms step_avg:59.98ms
step:1300/2330 train_time:77976ms step_avg:59.98ms
step:1301/2330 train_time:78035ms step_avg:59.98ms
step:1302/2330 train_time:78096ms step_avg:59.98ms
step:1303/2330 train_time:78156ms step_avg:59.98ms
step:1304/2330 train_time:78216ms step_avg:59.98ms
step:1305/2330 train_time:78276ms step_avg:59.98ms
step:1306/2330 train_time:78337ms step_avg:59.98ms
step:1307/2330 train_time:78396ms step_avg:59.98ms
step:1308/2330 train_time:78457ms step_avg:59.98ms
step:1309/2330 train_time:78517ms step_avg:59.98ms
step:1310/2330 train_time:78578ms step_avg:59.98ms
step:1311/2330 train_time:78638ms step_avg:59.98ms
step:1312/2330 train_time:78699ms step_avg:59.98ms
step:1313/2330 train_time:78759ms step_avg:59.98ms
step:1314/2330 train_time:78820ms step_avg:59.99ms
step:1315/2330 train_time:78879ms step_avg:59.98ms
step:1316/2330 train_time:78940ms step_avg:59.98ms
step:1317/2330 train_time:78999ms step_avg:59.98ms
step:1318/2330 train_time:79060ms step_avg:59.98ms
step:1319/2330 train_time:79119ms step_avg:59.98ms
step:1320/2330 train_time:79181ms step_avg:59.99ms
step:1321/2330 train_time:79240ms step_avg:59.98ms
step:1322/2330 train_time:79301ms step_avg:59.99ms
step:1323/2330 train_time:79360ms step_avg:59.99ms
step:1324/2330 train_time:79421ms step_avg:59.99ms
step:1325/2330 train_time:79481ms step_avg:59.99ms
step:1326/2330 train_time:79542ms step_avg:59.99ms
step:1327/2330 train_time:79601ms step_avg:59.99ms
step:1328/2330 train_time:79662ms step_avg:59.99ms
step:1329/2330 train_time:79722ms step_avg:59.99ms
step:1330/2330 train_time:79783ms step_avg:59.99ms
step:1331/2330 train_time:79842ms step_avg:59.99ms
step:1332/2330 train_time:79903ms step_avg:59.99ms
step:1333/2330 train_time:79962ms step_avg:59.99ms
step:1334/2330 train_time:80024ms step_avg:59.99ms
step:1335/2330 train_time:80083ms step_avg:59.99ms
step:1336/2330 train_time:80144ms step_avg:59.99ms
step:1337/2330 train_time:80203ms step_avg:59.99ms
step:1338/2330 train_time:80265ms step_avg:59.99ms
step:1339/2330 train_time:80325ms step_avg:59.99ms
step:1340/2330 train_time:80386ms step_avg:59.99ms
step:1341/2330 train_time:80446ms step_avg:59.99ms
step:1342/2330 train_time:80507ms step_avg:59.99ms
step:1343/2330 train_time:80566ms step_avg:59.99ms
step:1344/2330 train_time:80627ms step_avg:59.99ms
step:1345/2330 train_time:80687ms step_avg:59.99ms
step:1346/2330 train_time:80748ms step_avg:59.99ms
step:1347/2330 train_time:80807ms step_avg:59.99ms
step:1348/2330 train_time:80869ms step_avg:59.99ms
step:1349/2330 train_time:80928ms step_avg:59.99ms
step:1350/2330 train_time:80990ms step_avg:59.99ms
step:1351/2330 train_time:81050ms step_avg:59.99ms
step:1352/2330 train_time:81112ms step_avg:59.99ms
step:1353/2330 train_time:81171ms step_avg:59.99ms
step:1354/2330 train_time:81233ms step_avg:59.99ms
step:1355/2330 train_time:81292ms step_avg:59.99ms
step:1356/2330 train_time:81354ms step_avg:60.00ms
step:1357/2330 train_time:81414ms step_avg:60.00ms
step:1358/2330 train_time:81475ms step_avg:60.00ms
step:1359/2330 train_time:81534ms step_avg:60.00ms
step:1360/2330 train_time:81595ms step_avg:60.00ms
step:1361/2330 train_time:81654ms step_avg:60.00ms
step:1362/2330 train_time:81715ms step_avg:60.00ms
step:1363/2330 train_time:81774ms step_avg:60.00ms
step:1364/2330 train_time:81835ms step_avg:60.00ms
step:1365/2330 train_time:81894ms step_avg:60.00ms
step:1366/2330 train_time:81955ms step_avg:60.00ms
step:1367/2330 train_time:82015ms step_avg:60.00ms
step:1368/2330 train_time:82077ms step_avg:60.00ms
step:1369/2330 train_time:82136ms step_avg:60.00ms
step:1370/2330 train_time:82197ms step_avg:60.00ms
step:1371/2330 train_time:82256ms step_avg:60.00ms
step:1372/2330 train_time:82317ms step_avg:60.00ms
step:1373/2330 train_time:82377ms step_avg:60.00ms
step:1374/2330 train_time:82438ms step_avg:60.00ms
step:1375/2330 train_time:82496ms step_avg:60.00ms
step:1376/2330 train_time:82558ms step_avg:60.00ms
step:1377/2330 train_time:82617ms step_avg:60.00ms
step:1378/2330 train_time:82678ms step_avg:60.00ms
step:1379/2330 train_time:82737ms step_avg:60.00ms
step:1380/2330 train_time:82798ms step_avg:60.00ms
step:1381/2330 train_time:82858ms step_avg:60.00ms
step:1382/2330 train_time:82919ms step_avg:60.00ms
step:1383/2330 train_time:82979ms step_avg:60.00ms
step:1384/2330 train_time:83040ms step_avg:60.00ms
step:1385/2330 train_time:83099ms step_avg:60.00ms
step:1386/2330 train_time:83161ms step_avg:60.00ms
step:1387/2330 train_time:83220ms step_avg:60.00ms
step:1388/2330 train_time:83281ms step_avg:60.00ms
step:1389/2330 train_time:83340ms step_avg:60.00ms
step:1390/2330 train_time:83401ms step_avg:60.00ms
step:1391/2330 train_time:83460ms step_avg:60.00ms
step:1392/2330 train_time:83522ms step_avg:60.00ms
step:1393/2330 train_time:83581ms step_avg:60.00ms
step:1394/2330 train_time:83642ms step_avg:60.00ms
step:1395/2330 train_time:83701ms step_avg:60.00ms
step:1396/2330 train_time:83762ms step_avg:60.00ms
step:1397/2330 train_time:83821ms step_avg:60.00ms
step:1398/2330 train_time:83882ms step_avg:60.00ms
step:1399/2330 train_time:83941ms step_avg:60.00ms
step:1400/2330 train_time:84002ms step_avg:60.00ms
step:1401/2330 train_time:84061ms step_avg:60.00ms
step:1402/2330 train_time:84123ms step_avg:60.00ms
step:1403/2330 train_time:84182ms step_avg:60.00ms
step:1404/2330 train_time:84243ms step_avg:60.00ms
step:1405/2330 train_time:84302ms step_avg:60.00ms
step:1406/2330 train_time:84364ms step_avg:60.00ms
step:1407/2330 train_time:84424ms step_avg:60.00ms
step:1408/2330 train_time:84485ms step_avg:60.00ms
step:1409/2330 train_time:84543ms step_avg:60.00ms
step:1410/2330 train_time:84605ms step_avg:60.00ms
step:1411/2330 train_time:84664ms step_avg:60.00ms
step:1412/2330 train_time:84725ms step_avg:60.00ms
step:1413/2330 train_time:84785ms step_avg:60.00ms
step:1414/2330 train_time:84846ms step_avg:60.00ms
step:1415/2330 train_time:84904ms step_avg:60.00ms
step:1416/2330 train_time:84965ms step_avg:60.00ms
step:1417/2330 train_time:85025ms step_avg:60.00ms
step:1418/2330 train_time:85086ms step_avg:60.00ms
step:1419/2330 train_time:85145ms step_avg:60.00ms
step:1420/2330 train_time:85207ms step_avg:60.00ms
step:1421/2330 train_time:85266ms step_avg:60.00ms
step:1422/2330 train_time:85327ms step_avg:60.01ms
step:1423/2330 train_time:85387ms step_avg:60.00ms
step:1424/2330 train_time:85448ms step_avg:60.01ms
step:1425/2330 train_time:85507ms step_avg:60.01ms
step:1426/2330 train_time:85568ms step_avg:60.01ms
step:1427/2330 train_time:85628ms step_avg:60.01ms
step:1428/2330 train_time:85690ms step_avg:60.01ms
step:1429/2330 train_time:85749ms step_avg:60.01ms
step:1430/2330 train_time:85810ms step_avg:60.01ms
step:1431/2330 train_time:85870ms step_avg:60.01ms
step:1432/2330 train_time:85932ms step_avg:60.01ms
step:1433/2330 train_time:85991ms step_avg:60.01ms
step:1434/2330 train_time:86053ms step_avg:60.01ms
step:1435/2330 train_time:86112ms step_avg:60.01ms
step:1436/2330 train_time:86174ms step_avg:60.01ms
step:1437/2330 train_time:86233ms step_avg:60.01ms
step:1438/2330 train_time:86295ms step_avg:60.01ms
step:1439/2330 train_time:86354ms step_avg:60.01ms
step:1440/2330 train_time:86415ms step_avg:60.01ms
step:1441/2330 train_time:86474ms step_avg:60.01ms
step:1442/2330 train_time:86535ms step_avg:60.01ms
step:1443/2330 train_time:86594ms step_avg:60.01ms
step:1444/2330 train_time:86656ms step_avg:60.01ms
step:1445/2330 train_time:86714ms step_avg:60.01ms
step:1446/2330 train_time:86776ms step_avg:60.01ms
step:1447/2330 train_time:86836ms step_avg:60.01ms
step:1448/2330 train_time:86897ms step_avg:60.01ms
step:1449/2330 train_time:86956ms step_avg:60.01ms
step:1450/2330 train_time:87017ms step_avg:60.01ms
step:1451/2330 train_time:87076ms step_avg:60.01ms
step:1452/2330 train_time:87137ms step_avg:60.01ms
step:1453/2330 train_time:87196ms step_avg:60.01ms
step:1454/2330 train_time:87257ms step_avg:60.01ms
step:1455/2330 train_time:87316ms step_avg:60.01ms
step:1456/2330 train_time:87378ms step_avg:60.01ms
step:1457/2330 train_time:87437ms step_avg:60.01ms
step:1458/2330 train_time:87498ms step_avg:60.01ms
step:1459/2330 train_time:87557ms step_avg:60.01ms
step:1460/2330 train_time:87618ms step_avg:60.01ms
step:1461/2330 train_time:87677ms step_avg:60.01ms
step:1462/2330 train_time:87738ms step_avg:60.01ms
step:1463/2330 train_time:87797ms step_avg:60.01ms
step:1464/2330 train_time:87859ms step_avg:60.01ms
step:1465/2330 train_time:87918ms step_avg:60.01ms
step:1466/2330 train_time:87980ms step_avg:60.01ms
step:1467/2330 train_time:88039ms step_avg:60.01ms
step:1468/2330 train_time:88100ms step_avg:60.01ms
step:1469/2330 train_time:88159ms step_avg:60.01ms
step:1470/2330 train_time:88220ms step_avg:60.01ms
step:1471/2330 train_time:88279ms step_avg:60.01ms
step:1472/2330 train_time:88340ms step_avg:60.01ms
step:1473/2330 train_time:88399ms step_avg:60.01ms
step:1474/2330 train_time:88460ms step_avg:60.01ms
step:1475/2330 train_time:88520ms step_avg:60.01ms
step:1476/2330 train_time:88581ms step_avg:60.01ms
step:1477/2330 train_time:88641ms step_avg:60.01ms
step:1478/2330 train_time:88702ms step_avg:60.01ms
step:1479/2330 train_time:88761ms step_avg:60.01ms
step:1480/2330 train_time:88823ms step_avg:60.02ms
step:1481/2330 train_time:88882ms step_avg:60.01ms
step:1482/2330 train_time:88943ms step_avg:60.02ms
step:1483/2330 train_time:89001ms step_avg:60.01ms
step:1484/2330 train_time:89063ms step_avg:60.02ms
step:1485/2330 train_time:89122ms step_avg:60.02ms
step:1486/2330 train_time:89184ms step_avg:60.02ms
step:1487/2330 train_time:89243ms step_avg:60.02ms
step:1488/2330 train_time:89303ms step_avg:60.02ms
step:1489/2330 train_time:89362ms step_avg:60.01ms
step:1490/2330 train_time:89424ms step_avg:60.02ms
step:1491/2330 train_time:89483ms step_avg:60.02ms
step:1492/2330 train_time:89544ms step_avg:60.02ms
step:1493/2330 train_time:89603ms step_avg:60.02ms
step:1494/2330 train_time:89665ms step_avg:60.02ms
step:1495/2330 train_time:89725ms step_avg:60.02ms
step:1496/2330 train_time:89786ms step_avg:60.02ms
step:1497/2330 train_time:89845ms step_avg:60.02ms
step:1498/2330 train_time:89906ms step_avg:60.02ms
step:1499/2330 train_time:89965ms step_avg:60.02ms
step:1500/2330 train_time:90026ms step_avg:60.02ms
step:1500/2330 val_loss:3.4505 train_time:90089ms step_avg:60.06ms
step:1501/2330 train_time:90111ms step_avg:60.03ms
step:1502/2330 train_time:90148ms step_avg:60.02ms
step:1503/2330 train_time:90213ms step_avg:60.02ms
step:1504/2330 train_time:90283ms step_avg:60.03ms
step:1505/2330 train_time:90342ms step_avg:60.03ms
step:1506/2330 train_time:90404ms step_avg:60.03ms
step:1507/2330 train_time:90463ms step_avg:60.03ms
step:1508/2330 train_time:90523ms step_avg:60.03ms
step:1509/2330 train_time:90582ms step_avg:60.03ms
step:1510/2330 train_time:90643ms step_avg:60.03ms
step:1511/2330 train_time:90703ms step_avg:60.03ms
step:1512/2330 train_time:90764ms step_avg:60.03ms
step:1513/2330 train_time:90823ms step_avg:60.03ms
step:1514/2330 train_time:90884ms step_avg:60.03ms
step:1515/2330 train_time:90942ms step_avg:60.03ms
step:1516/2330 train_time:91003ms step_avg:60.03ms
step:1517/2330 train_time:91063ms step_avg:60.03ms
step:1518/2330 train_time:91125ms step_avg:60.03ms
step:1519/2330 train_time:91187ms step_avg:60.03ms
step:1520/2330 train_time:91251ms step_avg:60.03ms
step:1521/2330 train_time:91311ms step_avg:60.03ms
step:1522/2330 train_time:91373ms step_avg:60.03ms
step:1523/2330 train_time:91432ms step_avg:60.03ms
step:1524/2330 train_time:91493ms step_avg:60.03ms
step:1525/2330 train_time:91551ms step_avg:60.03ms
step:1526/2330 train_time:91613ms step_avg:60.03ms
step:1527/2330 train_time:91672ms step_avg:60.03ms
step:1528/2330 train_time:91733ms step_avg:60.03ms
step:1529/2330 train_time:91793ms step_avg:60.03ms
step:1530/2330 train_time:91855ms step_avg:60.04ms
step:1531/2330 train_time:91914ms step_avg:60.04ms
step:1532/2330 train_time:91975ms step_avg:60.04ms
step:1533/2330 train_time:92034ms step_avg:60.04ms
step:1534/2330 train_time:92096ms step_avg:60.04ms
step:1535/2330 train_time:92157ms step_avg:60.04ms
step:1536/2330 train_time:92219ms step_avg:60.04ms
step:1537/2330 train_time:92279ms step_avg:60.04ms
step:1538/2330 train_time:92341ms step_avg:60.04ms
step:1539/2330 train_time:92401ms step_avg:60.04ms
step:1540/2330 train_time:92463ms step_avg:60.04ms
step:1541/2330 train_time:92523ms step_avg:60.04ms
step:1542/2330 train_time:92585ms step_avg:60.04ms
step:1543/2330 train_time:92645ms step_avg:60.04ms
step:1544/2330 train_time:92708ms step_avg:60.04ms
step:1545/2330 train_time:92768ms step_avg:60.04ms
step:1546/2330 train_time:92829ms step_avg:60.04ms
step:1547/2330 train_time:92889ms step_avg:60.04ms
step:1548/2330 train_time:92950ms step_avg:60.05ms
step:1549/2330 train_time:93010ms step_avg:60.04ms
step:1550/2330 train_time:93071ms step_avg:60.05ms
step:1551/2330 train_time:93131ms step_avg:60.05ms
step:1552/2330 train_time:93193ms step_avg:60.05ms
step:1553/2330 train_time:93252ms step_avg:60.05ms
step:1554/2330 train_time:93315ms step_avg:60.05ms
step:1555/2330 train_time:93374ms step_avg:60.05ms
step:1556/2330 train_time:93436ms step_avg:60.05ms
step:1557/2330 train_time:93496ms step_avg:60.05ms
step:1558/2330 train_time:93558ms step_avg:60.05ms
step:1559/2330 train_time:93618ms step_avg:60.05ms
step:1560/2330 train_time:93680ms step_avg:60.05ms
step:1561/2330 train_time:93739ms step_avg:60.05ms
step:1562/2330 train_time:93801ms step_avg:60.05ms
step:1563/2330 train_time:93860ms step_avg:60.05ms
step:1564/2330 train_time:93922ms step_avg:60.05ms
step:1565/2330 train_time:93981ms step_avg:60.05ms
step:1566/2330 train_time:94043ms step_avg:60.05ms
step:1567/2330 train_time:94103ms step_avg:60.05ms
step:1568/2330 train_time:94166ms step_avg:60.05ms
step:1569/2330 train_time:94226ms step_avg:60.06ms
step:1570/2330 train_time:94289ms step_avg:60.06ms
step:1571/2330 train_time:94349ms step_avg:60.06ms
step:1572/2330 train_time:94411ms step_avg:60.06ms
step:1573/2330 train_time:94471ms step_avg:60.06ms
step:1574/2330 train_time:94532ms step_avg:60.06ms
step:1575/2330 train_time:94592ms step_avg:60.06ms
step:1576/2330 train_time:94654ms step_avg:60.06ms
step:1577/2330 train_time:94714ms step_avg:60.06ms
step:1578/2330 train_time:94776ms step_avg:60.06ms
step:1579/2330 train_time:94835ms step_avg:60.06ms
step:1580/2330 train_time:94897ms step_avg:60.06ms
step:1581/2330 train_time:94957ms step_avg:60.06ms
step:1582/2330 train_time:95019ms step_avg:60.06ms
step:1583/2330 train_time:95079ms step_avg:60.06ms
step:1584/2330 train_time:95141ms step_avg:60.06ms
step:1585/2330 train_time:95200ms step_avg:60.06ms
step:1586/2330 train_time:95262ms step_avg:60.06ms
step:1587/2330 train_time:95322ms step_avg:60.06ms
step:1588/2330 train_time:95385ms step_avg:60.07ms
step:1589/2330 train_time:95445ms step_avg:60.07ms
step:1590/2330 train_time:95508ms step_avg:60.07ms
step:1591/2330 train_time:95568ms step_avg:60.07ms
step:1592/2330 train_time:95630ms step_avg:60.07ms
step:1593/2330 train_time:95690ms step_avg:60.07ms
step:1594/2330 train_time:95751ms step_avg:60.07ms
step:1595/2330 train_time:95811ms step_avg:60.07ms
step:1596/2330 train_time:95872ms step_avg:60.07ms
step:1597/2330 train_time:95931ms step_avg:60.07ms
step:1598/2330 train_time:95992ms step_avg:60.07ms
step:1599/2330 train_time:96052ms step_avg:60.07ms
step:1600/2330 train_time:96114ms step_avg:60.07ms
step:1601/2330 train_time:96174ms step_avg:60.07ms
step:1602/2330 train_time:96236ms step_avg:60.07ms
step:1603/2330 train_time:96295ms step_avg:60.07ms
step:1604/2330 train_time:96358ms step_avg:60.07ms
step:1605/2330 train_time:96418ms step_avg:60.07ms
step:1606/2330 train_time:96480ms step_avg:60.07ms
step:1607/2330 train_time:96540ms step_avg:60.07ms
step:1608/2330 train_time:96601ms step_avg:60.08ms
step:1609/2330 train_time:96662ms step_avg:60.08ms
step:1610/2330 train_time:96723ms step_avg:60.08ms
step:1611/2330 train_time:96783ms step_avg:60.08ms
step:1612/2330 train_time:96844ms step_avg:60.08ms
step:1613/2330 train_time:96904ms step_avg:60.08ms
step:1614/2330 train_time:96967ms step_avg:60.08ms
step:1615/2330 train_time:97027ms step_avg:60.08ms
step:1616/2330 train_time:97088ms step_avg:60.08ms
step:1617/2330 train_time:97148ms step_avg:60.08ms
step:1618/2330 train_time:97210ms step_avg:60.08ms
step:1619/2330 train_time:97271ms step_avg:60.08ms
step:1620/2330 train_time:97332ms step_avg:60.08ms
step:1621/2330 train_time:97391ms step_avg:60.08ms
step:1622/2330 train_time:97453ms step_avg:60.08ms
step:1623/2330 train_time:97513ms step_avg:60.08ms
step:1624/2330 train_time:97575ms step_avg:60.08ms
step:1625/2330 train_time:97634ms step_avg:60.08ms
step:1626/2330 train_time:97696ms step_avg:60.08ms
step:1627/2330 train_time:97756ms step_avg:60.08ms
step:1628/2330 train_time:97818ms step_avg:60.08ms
step:1629/2330 train_time:97878ms step_avg:60.08ms
step:1630/2330 train_time:97939ms step_avg:60.09ms
step:1631/2330 train_time:97999ms step_avg:60.09ms
step:1632/2330 train_time:98061ms step_avg:60.09ms
step:1633/2330 train_time:98121ms step_avg:60.09ms
step:1634/2330 train_time:98184ms step_avg:60.09ms
step:1635/2330 train_time:98243ms step_avg:60.09ms
step:1636/2330 train_time:98305ms step_avg:60.09ms
step:1637/2330 train_time:98366ms step_avg:60.09ms
step:1638/2330 train_time:98428ms step_avg:60.09ms
step:1639/2330 train_time:98488ms step_avg:60.09ms
step:1640/2330 train_time:98550ms step_avg:60.09ms
step:1641/2330 train_time:98611ms step_avg:60.09ms
step:1642/2330 train_time:98672ms step_avg:60.09ms
step:1643/2330 train_time:98731ms step_avg:60.09ms
step:1644/2330 train_time:98793ms step_avg:60.09ms
step:1645/2330 train_time:98853ms step_avg:60.09ms
step:1646/2330 train_time:98915ms step_avg:60.09ms
step:1647/2330 train_time:98974ms step_avg:60.09ms
step:1648/2330 train_time:99035ms step_avg:60.09ms
step:1649/2330 train_time:99095ms step_avg:60.09ms
step:1650/2330 train_time:99158ms step_avg:60.10ms
step:1651/2330 train_time:99217ms step_avg:60.10ms
step:1652/2330 train_time:99279ms step_avg:60.10ms
step:1653/2330 train_time:99339ms step_avg:60.10ms
step:1654/2330 train_time:99401ms step_avg:60.10ms
step:1655/2330 train_time:99460ms step_avg:60.10ms
step:1656/2330 train_time:99523ms step_avg:60.10ms
step:1657/2330 train_time:99584ms step_avg:60.10ms
step:1658/2330 train_time:99646ms step_avg:60.10ms
step:1659/2330 train_time:99707ms step_avg:60.10ms
step:1660/2330 train_time:99769ms step_avg:60.10ms
step:1661/2330 train_time:99828ms step_avg:60.10ms
step:1662/2330 train_time:99890ms step_avg:60.10ms
step:1663/2330 train_time:99949ms step_avg:60.10ms
step:1664/2330 train_time:100011ms step_avg:60.10ms
step:1665/2330 train_time:100071ms step_avg:60.10ms
step:1666/2330 train_time:100132ms step_avg:60.10ms
step:1667/2330 train_time:100191ms step_avg:60.10ms
step:1668/2330 train_time:100253ms step_avg:60.10ms
step:1669/2330 train_time:100313ms step_avg:60.10ms
step:1670/2330 train_time:100375ms step_avg:60.10ms
step:1671/2330 train_time:100436ms step_avg:60.11ms
step:1672/2330 train_time:100499ms step_avg:60.11ms
step:1673/2330 train_time:100559ms step_avg:60.11ms
step:1674/2330 train_time:100620ms step_avg:60.11ms
step:1675/2330 train_time:100680ms step_avg:60.11ms
step:1676/2330 train_time:100742ms step_avg:60.11ms
step:1677/2330 train_time:100801ms step_avg:60.11ms
step:1678/2330 train_time:100863ms step_avg:60.11ms
step:1679/2330 train_time:100923ms step_avg:60.11ms
step:1680/2330 train_time:100985ms step_avg:60.11ms
step:1681/2330 train_time:101046ms step_avg:60.11ms
step:1682/2330 train_time:101108ms step_avg:60.11ms
step:1683/2330 train_time:101168ms step_avg:60.11ms
step:1684/2330 train_time:101229ms step_avg:60.11ms
step:1685/2330 train_time:101290ms step_avg:60.11ms
step:1686/2330 train_time:101352ms step_avg:60.11ms
step:1687/2330 train_time:101411ms step_avg:60.11ms
step:1688/2330 train_time:101473ms step_avg:60.11ms
step:1689/2330 train_time:101532ms step_avg:60.11ms
step:1690/2330 train_time:101594ms step_avg:60.11ms
step:1691/2330 train_time:101654ms step_avg:60.11ms
step:1692/2330 train_time:101716ms step_avg:60.12ms
step:1693/2330 train_time:101776ms step_avg:60.12ms
step:1694/2330 train_time:101838ms step_avg:60.12ms
step:1695/2330 train_time:101898ms step_avg:60.12ms
step:1696/2330 train_time:101960ms step_avg:60.12ms
step:1697/2330 train_time:102019ms step_avg:60.12ms
step:1698/2330 train_time:102081ms step_avg:60.12ms
step:1699/2330 train_time:102141ms step_avg:60.12ms
step:1700/2330 train_time:102202ms step_avg:60.12ms
step:1701/2330 train_time:102262ms step_avg:60.12ms
step:1702/2330 train_time:102324ms step_avg:60.12ms
step:1703/2330 train_time:102384ms step_avg:60.12ms
step:1704/2330 train_time:102447ms step_avg:60.12ms
step:1705/2330 train_time:102507ms step_avg:60.12ms
step:1706/2330 train_time:102569ms step_avg:60.12ms
step:1707/2330 train_time:102630ms step_avg:60.12ms
step:1708/2330 train_time:102692ms step_avg:60.12ms
step:1709/2330 train_time:102751ms step_avg:60.12ms
step:1710/2330 train_time:102813ms step_avg:60.12ms
step:1711/2330 train_time:102872ms step_avg:60.12ms
step:1712/2330 train_time:102934ms step_avg:60.13ms
step:1713/2330 train_time:102994ms step_avg:60.12ms
step:1714/2330 train_time:103056ms step_avg:60.13ms
step:1715/2330 train_time:103116ms step_avg:60.13ms
step:1716/2330 train_time:103178ms step_avg:60.13ms
step:1717/2330 train_time:103237ms step_avg:60.13ms
step:1718/2330 train_time:103299ms step_avg:60.13ms
step:1719/2330 train_time:103359ms step_avg:60.13ms
step:1720/2330 train_time:103420ms step_avg:60.13ms
step:1721/2330 train_time:103480ms step_avg:60.13ms
step:1722/2330 train_time:103542ms step_avg:60.13ms
step:1723/2330 train_time:103602ms step_avg:60.13ms
step:1724/2330 train_time:103664ms step_avg:60.13ms
step:1725/2330 train_time:103724ms step_avg:60.13ms
step:1726/2330 train_time:103787ms step_avg:60.13ms
step:1727/2330 train_time:103847ms step_avg:60.13ms
step:1728/2330 train_time:103910ms step_avg:60.13ms
step:1729/2330 train_time:103970ms step_avg:60.13ms
step:1730/2330 train_time:104032ms step_avg:60.13ms
step:1731/2330 train_time:104091ms step_avg:60.13ms
step:1732/2330 train_time:104152ms step_avg:60.13ms
step:1733/2330 train_time:104212ms step_avg:60.13ms
step:1734/2330 train_time:104274ms step_avg:60.13ms
step:1735/2330 train_time:104333ms step_avg:60.13ms
step:1736/2330 train_time:104395ms step_avg:60.14ms
step:1737/2330 train_time:104455ms step_avg:60.14ms
step:1738/2330 train_time:104516ms step_avg:60.14ms
step:1739/2330 train_time:104576ms step_avg:60.14ms
step:1740/2330 train_time:104638ms step_avg:60.14ms
step:1741/2330 train_time:104698ms step_avg:60.14ms
step:1742/2330 train_time:104760ms step_avg:60.14ms
step:1743/2330 train_time:104820ms step_avg:60.14ms
step:1744/2330 train_time:104881ms step_avg:60.14ms
step:1745/2330 train_time:104941ms step_avg:60.14ms
step:1746/2330 train_time:105003ms step_avg:60.14ms
step:1747/2330 train_time:105063ms step_avg:60.14ms
step:1748/2330 train_time:105126ms step_avg:60.14ms
step:1749/2330 train_time:105186ms step_avg:60.14ms
step:1750/2330 train_time:105248ms step_avg:60.14ms
step:1750/2330 val_loss:3.3808 train_time:105312ms step_avg:60.18ms
step:1751/2330 train_time:105334ms step_avg:60.16ms
step:1752/2330 train_time:105372ms step_avg:60.14ms
step:1753/2330 train_time:105431ms step_avg:60.14ms
step:1754/2330 train_time:105493ms step_avg:60.14ms
step:1755/2330 train_time:105554ms step_avg:60.14ms
step:1756/2330 train_time:105616ms step_avg:60.15ms
step:1757/2330 train_time:105676ms step_avg:60.15ms
step:1758/2330 train_time:105738ms step_avg:60.15ms
step:1759/2330 train_time:105797ms step_avg:60.15ms
step:1760/2330 train_time:105858ms step_avg:60.15ms
step:1761/2330 train_time:105917ms step_avg:60.15ms
step:1762/2330 train_time:105978ms step_avg:60.15ms
step:1763/2330 train_time:106036ms step_avg:60.15ms
step:1764/2330 train_time:106098ms step_avg:60.15ms
step:1765/2330 train_time:106157ms step_avg:60.15ms
step:1766/2330 train_time:106219ms step_avg:60.15ms
step:1767/2330 train_time:106279ms step_avg:60.15ms
step:1768/2330 train_time:106341ms step_avg:60.15ms
step:1769/2330 train_time:106401ms step_avg:60.15ms
step:1770/2330 train_time:106462ms step_avg:60.15ms
step:1771/2330 train_time:106523ms step_avg:60.15ms
step:1772/2330 train_time:106585ms step_avg:60.15ms
step:1773/2330 train_time:106645ms step_avg:60.15ms
step:1774/2330 train_time:106707ms step_avg:60.15ms
step:1775/2330 train_time:106767ms step_avg:60.15ms
step:1776/2330 train_time:106830ms step_avg:60.15ms
step:1777/2330 train_time:106889ms step_avg:60.15ms
step:1778/2330 train_time:106951ms step_avg:60.15ms
step:1779/2330 train_time:107010ms step_avg:60.15ms
step:1780/2330 train_time:107072ms step_avg:60.15ms
step:1781/2330 train_time:107132ms step_avg:60.15ms
step:1782/2330 train_time:107193ms step_avg:60.15ms
step:1783/2330 train_time:107253ms step_avg:60.15ms
step:1784/2330 train_time:107314ms step_avg:60.15ms
step:1785/2330 train_time:107374ms step_avg:60.15ms
step:1786/2330 train_time:107436ms step_avg:60.15ms
step:1787/2330 train_time:107496ms step_avg:60.15ms
step:1788/2330 train_time:107558ms step_avg:60.16ms
step:1789/2330 train_time:107619ms step_avg:60.16ms
step:1790/2330 train_time:107681ms step_avg:60.16ms
step:1791/2330 train_time:107741ms step_avg:60.16ms
step:1792/2330 train_time:107803ms step_avg:60.16ms
step:1793/2330 train_time:107862ms step_avg:60.16ms
step:1794/2330 train_time:107923ms step_avg:60.16ms
step:1795/2330 train_time:107983ms step_avg:60.16ms
step:1796/2330 train_time:108044ms step_avg:60.16ms
step:1797/2330 train_time:108104ms step_avg:60.16ms
step:1798/2330 train_time:108166ms step_avg:60.16ms
step:1799/2330 train_time:108226ms step_avg:60.16ms
step:1800/2330 train_time:108288ms step_avg:60.16ms
step:1801/2330 train_time:108348ms step_avg:60.16ms
step:1802/2330 train_time:108411ms step_avg:60.16ms
step:1803/2330 train_time:108471ms step_avg:60.16ms
step:1804/2330 train_time:108533ms step_avg:60.16ms
step:1805/2330 train_time:108592ms step_avg:60.16ms
step:1806/2330 train_time:108654ms step_avg:60.16ms
step:1807/2330 train_time:108714ms step_avg:60.16ms
step:1808/2330 train_time:108776ms step_avg:60.16ms
step:1809/2330 train_time:108836ms step_avg:60.16ms
step:1810/2330 train_time:108897ms step_avg:60.16ms
step:1811/2330 train_time:108957ms step_avg:60.16ms
step:1812/2330 train_time:109019ms step_avg:60.17ms
step:1813/2330 train_time:109079ms step_avg:60.16ms
step:1814/2330 train_time:109141ms step_avg:60.17ms
step:1815/2330 train_time:109201ms step_avg:60.17ms
step:1816/2330 train_time:109262ms step_avg:60.17ms
step:1817/2330 train_time:109322ms step_avg:60.17ms
step:1818/2330 train_time:109384ms step_avg:60.17ms
step:1819/2330 train_time:109444ms step_avg:60.17ms
step:1820/2330 train_time:109506ms step_avg:60.17ms
step:1821/2330 train_time:109566ms step_avg:60.17ms
step:1822/2330 train_time:109629ms step_avg:60.17ms
step:1823/2330 train_time:109689ms step_avg:60.17ms
step:1824/2330 train_time:109750ms step_avg:60.17ms
step:1825/2330 train_time:109810ms step_avg:60.17ms
step:1826/2330 train_time:109872ms step_avg:60.17ms
step:1827/2330 train_time:109932ms step_avg:60.17ms
step:1828/2330 train_time:109994ms step_avg:60.17ms
step:1829/2330 train_time:110053ms step_avg:60.17ms
step:1830/2330 train_time:110115ms step_avg:60.17ms
step:1831/2330 train_time:110174ms step_avg:60.17ms
step:1832/2330 train_time:110236ms step_avg:60.17ms
step:1833/2330 train_time:110296ms step_avg:60.17ms
step:1834/2330 train_time:110358ms step_avg:60.17ms
step:1835/2330 train_time:110419ms step_avg:60.17ms
step:1836/2330 train_time:110480ms step_avg:60.17ms
step:1837/2330 train_time:110540ms step_avg:60.17ms
step:1838/2330 train_time:110602ms step_avg:60.18ms
step:1839/2330 train_time:110662ms step_avg:60.18ms
step:1840/2330 train_time:110724ms step_avg:60.18ms
step:1841/2330 train_time:110783ms step_avg:60.18ms
step:1842/2330 train_time:110845ms step_avg:60.18ms
step:1843/2330 train_time:110905ms step_avg:60.18ms
step:1844/2330 train_time:110968ms step_avg:60.18ms
step:1845/2330 train_time:111028ms step_avg:60.18ms
step:1846/2330 train_time:111089ms step_avg:60.18ms
step:1847/2330 train_time:111149ms step_avg:60.18ms
step:1848/2330 train_time:111211ms step_avg:60.18ms
step:1849/2330 train_time:111271ms step_avg:60.18ms
step:1850/2330 train_time:111332ms step_avg:60.18ms
step:1851/2330 train_time:111392ms step_avg:60.18ms
step:1852/2330 train_time:111453ms step_avg:60.18ms
step:1853/2330 train_time:111513ms step_avg:60.18ms
step:1854/2330 train_time:111574ms step_avg:60.18ms
step:1855/2330 train_time:111634ms step_avg:60.18ms
step:1856/2330 train_time:111696ms step_avg:60.18ms
step:1857/2330 train_time:111756ms step_avg:60.18ms
step:1858/2330 train_time:111818ms step_avg:60.18ms
step:1859/2330 train_time:111877ms step_avg:60.18ms
step:1860/2330 train_time:111939ms step_avg:60.18ms
step:1861/2330 train_time:111998ms step_avg:60.18ms
step:1862/2330 train_time:112059ms step_avg:60.18ms
step:1863/2330 train_time:112119ms step_avg:60.18ms
step:1864/2330 train_time:112181ms step_avg:60.18ms
step:1865/2330 train_time:112241ms step_avg:60.18ms
step:1866/2330 train_time:112302ms step_avg:60.18ms
step:1867/2330 train_time:112362ms step_avg:60.18ms
step:1868/2330 train_time:112424ms step_avg:60.18ms
step:1869/2330 train_time:112484ms step_avg:60.18ms
step:1870/2330 train_time:112546ms step_avg:60.19ms
step:1871/2330 train_time:112607ms step_avg:60.19ms
step:1872/2330 train_time:112669ms step_avg:60.19ms
step:1873/2330 train_time:112729ms step_avg:60.19ms
step:1874/2330 train_time:112791ms step_avg:60.19ms
step:1875/2330 train_time:112851ms step_avg:60.19ms
step:1876/2330 train_time:112913ms step_avg:60.19ms
step:1877/2330 train_time:112973ms step_avg:60.19ms
step:1878/2330 train_time:113034ms step_avg:60.19ms
step:1879/2330 train_time:113093ms step_avg:60.19ms
step:1880/2330 train_time:113155ms step_avg:60.19ms
step:1881/2330 train_time:113215ms step_avg:60.19ms
step:1882/2330 train_time:113276ms step_avg:60.19ms
step:1883/2330 train_time:113336ms step_avg:60.19ms
step:1884/2330 train_time:113398ms step_avg:60.19ms
step:1885/2330 train_time:113457ms step_avg:60.19ms
step:1886/2330 train_time:113519ms step_avg:60.19ms
step:1887/2330 train_time:113579ms step_avg:60.19ms
step:1888/2330 train_time:113641ms step_avg:60.19ms
step:1889/2330 train_time:113701ms step_avg:60.19ms
step:1890/2330 train_time:113762ms step_avg:60.19ms
step:1891/2330 train_time:113822ms step_avg:60.19ms
step:1892/2330 train_time:113883ms step_avg:60.19ms
step:1893/2330 train_time:113943ms step_avg:60.19ms
step:1894/2330 train_time:114005ms step_avg:60.19ms
step:1895/2330 train_time:114065ms step_avg:60.19ms
step:1896/2330 train_time:114128ms step_avg:60.19ms
step:1897/2330 train_time:114188ms step_avg:60.19ms
step:1898/2330 train_time:114250ms step_avg:60.20ms
step:1899/2330 train_time:114310ms step_avg:60.20ms
step:1900/2330 train_time:114372ms step_avg:60.20ms
step:1901/2330 train_time:114432ms step_avg:60.20ms
step:1902/2330 train_time:114493ms step_avg:60.20ms
step:1903/2330 train_time:114553ms step_avg:60.20ms
step:1904/2330 train_time:114615ms step_avg:60.20ms
step:1905/2330 train_time:114675ms step_avg:60.20ms
step:1906/2330 train_time:114737ms step_avg:60.20ms
step:1907/2330 train_time:114797ms step_avg:60.20ms
step:1908/2330 train_time:114858ms step_avg:60.20ms
step:1909/2330 train_time:114918ms step_avg:60.20ms
step:1910/2330 train_time:114980ms step_avg:60.20ms
step:1911/2330 train_time:115040ms step_avg:60.20ms
step:1912/2330 train_time:115101ms step_avg:60.20ms
step:1913/2330 train_time:115161ms step_avg:60.20ms
step:1914/2330 train_time:115223ms step_avg:60.20ms
step:1915/2330 train_time:115282ms step_avg:60.20ms
step:1916/2330 train_time:115345ms step_avg:60.20ms
step:1917/2330 train_time:115405ms step_avg:60.20ms
step:1918/2330 train_time:115467ms step_avg:60.20ms
step:1919/2330 train_time:115527ms step_avg:60.20ms
step:1920/2330 train_time:115589ms step_avg:60.20ms
step:1921/2330 train_time:115650ms step_avg:60.20ms
step:1922/2330 train_time:115712ms step_avg:60.20ms
step:1923/2330 train_time:115773ms step_avg:60.20ms
step:1924/2330 train_time:115834ms step_avg:60.20ms
step:1925/2330 train_time:115893ms step_avg:60.20ms
step:1926/2330 train_time:115955ms step_avg:60.20ms
step:1927/2330 train_time:116014ms step_avg:60.20ms
step:1928/2330 train_time:116076ms step_avg:60.21ms
step:1929/2330 train_time:116135ms step_avg:60.20ms
step:1930/2330 train_time:116197ms step_avg:60.21ms
step:1931/2330 train_time:116257ms step_avg:60.21ms
step:1932/2330 train_time:116320ms step_avg:60.21ms
step:1933/2330 train_time:116380ms step_avg:60.21ms
step:1934/2330 train_time:116441ms step_avg:60.21ms
step:1935/2330 train_time:116500ms step_avg:60.21ms
step:1936/2330 train_time:116562ms step_avg:60.21ms
step:1937/2330 train_time:116622ms step_avg:60.21ms
step:1938/2330 train_time:116684ms step_avg:60.21ms
step:1939/2330 train_time:116744ms step_avg:60.21ms
step:1940/2330 train_time:116805ms step_avg:60.21ms
step:1941/2330 train_time:116866ms step_avg:60.21ms
step:1942/2330 train_time:116928ms step_avg:60.21ms
step:1943/2330 train_time:116988ms step_avg:60.21ms
step:1944/2330 train_time:117050ms step_avg:60.21ms
step:1945/2330 train_time:117109ms step_avg:60.21ms
step:1946/2330 train_time:117172ms step_avg:60.21ms
step:1947/2330 train_time:117231ms step_avg:60.21ms
step:1948/2330 train_time:117293ms step_avg:60.21ms
step:1949/2330 train_time:117353ms step_avg:60.21ms
step:1950/2330 train_time:117415ms step_avg:60.21ms
step:1951/2330 train_time:117474ms step_avg:60.21ms
step:1952/2330 train_time:117536ms step_avg:60.21ms
step:1953/2330 train_time:117596ms step_avg:60.21ms
step:1954/2330 train_time:117658ms step_avg:60.21ms
step:1955/2330 train_time:117718ms step_avg:60.21ms
step:1956/2330 train_time:117780ms step_avg:60.21ms
step:1957/2330 train_time:117840ms step_avg:60.21ms
step:1958/2330 train_time:117901ms step_avg:60.22ms
step:1959/2330 train_time:117961ms step_avg:60.21ms
step:1960/2330 train_time:118023ms step_avg:60.22ms
step:1961/2330 train_time:118082ms step_avg:60.22ms
step:1962/2330 train_time:118145ms step_avg:60.22ms
step:1963/2330 train_time:118205ms step_avg:60.22ms
step:1964/2330 train_time:118267ms step_avg:60.22ms
step:1965/2330 train_time:118327ms step_avg:60.22ms
step:1966/2330 train_time:118389ms step_avg:60.22ms
step:1967/2330 train_time:118450ms step_avg:60.22ms
step:1968/2330 train_time:118512ms step_avg:60.22ms
step:1969/2330 train_time:118572ms step_avg:60.22ms
step:1970/2330 train_time:118633ms step_avg:60.22ms
step:1971/2330 train_time:118693ms step_avg:60.22ms
step:1972/2330 train_time:118755ms step_avg:60.22ms
step:1973/2330 train_time:118815ms step_avg:60.22ms
step:1974/2330 train_time:118877ms step_avg:60.22ms
step:1975/2330 train_time:118936ms step_avg:60.22ms
step:1976/2330 train_time:118998ms step_avg:60.22ms
step:1977/2330 train_time:119058ms step_avg:60.22ms
step:1978/2330 train_time:119121ms step_avg:60.22ms
step:1979/2330 train_time:119181ms step_avg:60.22ms
step:1980/2330 train_time:119242ms step_avg:60.22ms
step:1981/2330 train_time:119302ms step_avg:60.22ms
step:1982/2330 train_time:119364ms step_avg:60.22ms
step:1983/2330 train_time:119424ms step_avg:60.22ms
step:1984/2330 train_time:119486ms step_avg:60.22ms
step:1985/2330 train_time:119545ms step_avg:60.22ms
step:1986/2330 train_time:119608ms step_avg:60.23ms
step:1987/2330 train_time:119667ms step_avg:60.23ms
step:1988/2330 train_time:119730ms step_avg:60.23ms
step:1989/2330 train_time:119790ms step_avg:60.23ms
step:1990/2330 train_time:119852ms step_avg:60.23ms
step:1991/2330 train_time:119913ms step_avg:60.23ms
step:1992/2330 train_time:119975ms step_avg:60.23ms
step:1993/2330 train_time:120034ms step_avg:60.23ms
step:1994/2330 train_time:120096ms step_avg:60.23ms
step:1995/2330 train_time:120155ms step_avg:60.23ms
step:1996/2330 train_time:120217ms step_avg:60.23ms
step:1997/2330 train_time:120277ms step_avg:60.23ms
step:1998/2330 train_time:120339ms step_avg:60.23ms
step:1999/2330 train_time:120398ms step_avg:60.23ms
step:2000/2330 train_time:120460ms step_avg:60.23ms
step:2000/2330 val_loss:3.3303 train_time:120524ms step_avg:60.26ms
step:2001/2330 train_time:120545ms step_avg:60.24ms
step:2002/2330 train_time:120586ms step_avg:60.23ms
step:2003/2330 train_time:120649ms step_avg:60.23ms
step:2004/2330 train_time:120712ms step_avg:60.24ms
step:2005/2330 train_time:120771ms step_avg:60.23ms
step:2006/2330 train_time:120833ms step_avg:60.24ms
step:2007/2330 train_time:120892ms step_avg:60.24ms
step:2008/2330 train_time:120954ms step_avg:60.24ms
step:2009/2330 train_time:121013ms step_avg:60.24ms
step:2010/2330 train_time:121074ms step_avg:60.24ms
step:2011/2330 train_time:121132ms step_avg:60.23ms
step:2012/2330 train_time:121194ms step_avg:60.24ms
step:2013/2330 train_time:121252ms step_avg:60.23ms
step:2014/2330 train_time:121313ms step_avg:60.23ms
step:2015/2330 train_time:121372ms step_avg:60.23ms
step:2016/2330 train_time:121433ms step_avg:60.23ms
step:2017/2330 train_time:121495ms step_avg:60.24ms
step:2018/2330 train_time:121559ms step_avg:60.24ms
step:2019/2330 train_time:121619ms step_avg:60.24ms
step:2020/2330 train_time:121682ms step_avg:60.24ms
step:2021/2330 train_time:121742ms step_avg:60.24ms
step:2022/2330 train_time:121805ms step_avg:60.24ms
step:2023/2330 train_time:121865ms step_avg:60.24ms
step:2024/2330 train_time:121926ms step_avg:60.24ms
step:2025/2330 train_time:121987ms step_avg:60.24ms
step:2026/2330 train_time:122048ms step_avg:60.24ms
step:2027/2330 train_time:122107ms step_avg:60.24ms
step:2028/2330 train_time:122168ms step_avg:60.24ms
step:2029/2330 train_time:122227ms step_avg:60.24ms
step:2030/2330 train_time:122288ms step_avg:60.24ms
step:2031/2330 train_time:122347ms step_avg:60.24ms
step:2032/2330 train_time:122408ms step_avg:60.24ms
step:2033/2330 train_time:122468ms step_avg:60.24ms
step:2034/2330 train_time:122531ms step_avg:60.24ms
step:2035/2330 train_time:122591ms step_avg:60.24ms
step:2036/2330 train_time:122654ms step_avg:60.24ms
step:2037/2330 train_time:122715ms step_avg:60.24ms
step:2038/2330 train_time:122776ms step_avg:60.24ms
step:2039/2330 train_time:122836ms step_avg:60.24ms
step:2040/2330 train_time:122898ms step_avg:60.24ms
step:2041/2330 train_time:122957ms step_avg:60.24ms
step:2042/2330 train_time:123019ms step_avg:60.24ms
step:2043/2330 train_time:123079ms step_avg:60.24ms
step:2044/2330 train_time:123141ms step_avg:60.24ms
step:2045/2330 train_time:123200ms step_avg:60.24ms
step:2046/2330 train_time:123262ms step_avg:60.25ms
step:2047/2330 train_time:123322ms step_avg:60.25ms
step:2048/2330 train_time:123384ms step_avg:60.25ms
step:2049/2330 train_time:123443ms step_avg:60.25ms
step:2050/2330 train_time:123505ms step_avg:60.25ms
step:2051/2330 train_time:123565ms step_avg:60.25ms
step:2052/2330 train_time:123627ms step_avg:60.25ms
step:2053/2330 train_time:123687ms step_avg:60.25ms
step:2054/2330 train_time:123748ms step_avg:60.25ms
step:2055/2330 train_time:123808ms step_avg:60.25ms
step:2056/2330 train_time:123870ms step_avg:60.25ms
step:2057/2330 train_time:123929ms step_avg:60.25ms
step:2058/2330 train_time:123990ms step_avg:60.25ms
step:2059/2330 train_time:124050ms step_avg:60.25ms
step:2060/2330 train_time:124112ms step_avg:60.25ms
step:2061/2330 train_time:124172ms step_avg:60.25ms
step:2062/2330 train_time:124234ms step_avg:60.25ms
step:2063/2330 train_time:124294ms step_avg:60.25ms
step:2064/2330 train_time:124355ms step_avg:60.25ms
step:2065/2330 train_time:124415ms step_avg:60.25ms
step:2066/2330 train_time:124477ms step_avg:60.25ms
step:2067/2330 train_time:124537ms step_avg:60.25ms
step:2068/2330 train_time:124598ms step_avg:60.25ms
step:2069/2330 train_time:124658ms step_avg:60.25ms
step:2070/2330 train_time:124721ms step_avg:60.25ms
step:2071/2330 train_time:124781ms step_avg:60.25ms
step:2072/2330 train_time:124843ms step_avg:60.25ms
step:2073/2330 train_time:124903ms step_avg:60.25ms
step:2074/2330 train_time:124965ms step_avg:60.25ms
step:2075/2330 train_time:125026ms step_avg:60.25ms
step:2076/2330 train_time:125087ms step_avg:60.25ms
step:2077/2330 train_time:125147ms step_avg:60.25ms
step:2078/2330 train_time:125208ms step_avg:60.25ms
step:2079/2330 train_time:125268ms step_avg:60.25ms
step:2080/2330 train_time:125329ms step_avg:60.25ms
step:2081/2330 train_time:125389ms step_avg:60.25ms
step:2082/2330 train_time:125452ms step_avg:60.26ms
step:2083/2330 train_time:125512ms step_avg:60.26ms
step:2084/2330 train_time:125574ms step_avg:60.26ms
step:2085/2330 train_time:125633ms step_avg:60.26ms
step:2086/2330 train_time:125696ms step_avg:60.26ms
step:2087/2330 train_time:125755ms step_avg:60.26ms
step:2088/2330 train_time:125817ms step_avg:60.26ms
step:2089/2330 train_time:125877ms step_avg:60.26ms
step:2090/2330 train_time:125939ms step_avg:60.26ms
step:2091/2330 train_time:125999ms step_avg:60.26ms
step:2092/2330 train_time:126061ms step_avg:60.26ms
step:2093/2330 train_time:126122ms step_avg:60.26ms
step:2094/2330 train_time:126184ms step_avg:60.26ms
step:2095/2330 train_time:126244ms step_avg:60.26ms
step:2096/2330 train_time:126306ms step_avg:60.26ms
step:2097/2330 train_time:126366ms step_avg:60.26ms
step:2098/2330 train_time:126428ms step_avg:60.26ms
step:2099/2330 train_time:126488ms step_avg:60.26ms
step:2100/2330 train_time:126550ms step_avg:60.26ms
step:2101/2330 train_time:126610ms step_avg:60.26ms
step:2102/2330 train_time:126672ms step_avg:60.26ms
step:2103/2330 train_time:126732ms step_avg:60.26ms
step:2104/2330 train_time:126794ms step_avg:60.26ms
step:2105/2330 train_time:126854ms step_avg:60.26ms
step:2106/2330 train_time:126915ms step_avg:60.26ms
step:2107/2330 train_time:126975ms step_avg:60.26ms
step:2108/2330 train_time:127037ms step_avg:60.26ms
step:2109/2330 train_time:127096ms step_avg:60.26ms
step:2110/2330 train_time:127158ms step_avg:60.26ms
step:2111/2330 train_time:127218ms step_avg:60.26ms
step:2112/2330 train_time:127280ms step_avg:60.26ms
step:2113/2330 train_time:127340ms step_avg:60.26ms
step:2114/2330 train_time:127402ms step_avg:60.27ms
step:2115/2330 train_time:127463ms step_avg:60.27ms
step:2116/2330 train_time:127525ms step_avg:60.27ms
step:2117/2330 train_time:127584ms step_avg:60.27ms
step:2118/2330 train_time:127646ms step_avg:60.27ms
step:2119/2330 train_time:127706ms step_avg:60.27ms
step:2120/2330 train_time:127767ms step_avg:60.27ms
step:2121/2330 train_time:127827ms step_avg:60.27ms
step:2122/2330 train_time:127888ms step_avg:60.27ms
step:2123/2330 train_time:127948ms step_avg:60.27ms
step:2124/2330 train_time:128010ms step_avg:60.27ms
step:2125/2330 train_time:128070ms step_avg:60.27ms
step:2126/2330 train_time:128132ms step_avg:60.27ms
step:2127/2330 train_time:128192ms step_avg:60.27ms
step:2128/2330 train_time:128255ms step_avg:60.27ms
step:2129/2330 train_time:128315ms step_avg:60.27ms
step:2130/2330 train_time:128376ms step_avg:60.27ms
step:2131/2330 train_time:128435ms step_avg:60.27ms
step:2132/2330 train_time:128497ms step_avg:60.27ms
step:2133/2330 train_time:128557ms step_avg:60.27ms
step:2134/2330 train_time:128619ms step_avg:60.27ms
step:2135/2330 train_time:128679ms step_avg:60.27ms
step:2136/2330 train_time:128741ms step_avg:60.27ms
step:2137/2330 train_time:128801ms step_avg:60.27ms
step:2138/2330 train_time:128863ms step_avg:60.27ms
step:2139/2330 train_time:128924ms step_avg:60.27ms
step:2140/2330 train_time:128986ms step_avg:60.27ms
step:2141/2330 train_time:129045ms step_avg:60.27ms
step:2142/2330 train_time:129107ms step_avg:60.27ms
step:2143/2330 train_time:129166ms step_avg:60.27ms
step:2144/2330 train_time:129228ms step_avg:60.27ms
step:2145/2330 train_time:129288ms step_avg:60.27ms
step:2146/2330 train_time:129350ms step_avg:60.27ms
step:2147/2330 train_time:129410ms step_avg:60.27ms
step:2148/2330 train_time:129472ms step_avg:60.28ms
step:2149/2330 train_time:129532ms step_avg:60.28ms
step:2150/2330 train_time:129594ms step_avg:60.28ms
step:2151/2330 train_time:129654ms step_avg:60.28ms
step:2152/2330 train_time:129716ms step_avg:60.28ms
step:2153/2330 train_time:129775ms step_avg:60.28ms
step:2154/2330 train_time:129836ms step_avg:60.28ms
step:2155/2330 train_time:129896ms step_avg:60.28ms
step:2156/2330 train_time:129957ms step_avg:60.28ms
step:2157/2330 train_time:130017ms step_avg:60.28ms
step:2158/2330 train_time:130080ms step_avg:60.28ms
step:2159/2330 train_time:130140ms step_avg:60.28ms
step:2160/2330 train_time:130202ms step_avg:60.28ms
step:2161/2330 train_time:130262ms step_avg:60.28ms
step:2162/2330 train_time:130324ms step_avg:60.28ms
step:2163/2330 train_time:130384ms step_avg:60.28ms
step:2164/2330 train_time:130446ms step_avg:60.28ms
step:2165/2330 train_time:130506ms step_avg:60.28ms
step:2166/2330 train_time:130568ms step_avg:60.28ms
step:2167/2330 train_time:130627ms step_avg:60.28ms
step:2168/2330 train_time:130688ms step_avg:60.28ms
step:2169/2330 train_time:130748ms step_avg:60.28ms
step:2170/2330 train_time:130810ms step_avg:60.28ms
step:2171/2330 train_time:130869ms step_avg:60.28ms
step:2172/2330 train_time:130931ms step_avg:60.28ms
step:2173/2330 train_time:130990ms step_avg:60.28ms
step:2174/2330 train_time:131053ms step_avg:60.28ms
step:2175/2330 train_time:131113ms step_avg:60.28ms
step:2176/2330 train_time:131175ms step_avg:60.28ms
step:2177/2330 train_time:131234ms step_avg:60.28ms
step:2178/2330 train_time:131296ms step_avg:60.28ms
step:2179/2330 train_time:131356ms step_avg:60.28ms
step:2180/2330 train_time:131417ms step_avg:60.28ms
step:2181/2330 train_time:131477ms step_avg:60.28ms
step:2182/2330 train_time:131539ms step_avg:60.28ms
step:2183/2330 train_time:131599ms step_avg:60.28ms
step:2184/2330 train_time:131662ms step_avg:60.28ms
step:2185/2330 train_time:131722ms step_avg:60.28ms
step:2186/2330 train_time:131784ms step_avg:60.29ms
step:2187/2330 train_time:131844ms step_avg:60.29ms
step:2188/2330 train_time:131906ms step_avg:60.29ms
step:2189/2330 train_time:131966ms step_avg:60.29ms
step:2190/2330 train_time:132027ms step_avg:60.29ms
step:2191/2330 train_time:132087ms step_avg:60.29ms
step:2192/2330 train_time:132149ms step_avg:60.29ms
step:2193/2330 train_time:132208ms step_avg:60.29ms
step:2194/2330 train_time:132270ms step_avg:60.29ms
step:2195/2330 train_time:132329ms step_avg:60.29ms
step:2196/2330 train_time:132391ms step_avg:60.29ms
step:2197/2330 train_time:132451ms step_avg:60.29ms
step:2198/2330 train_time:132513ms step_avg:60.29ms
step:2199/2330 train_time:132573ms step_avg:60.29ms
step:2200/2330 train_time:132634ms step_avg:60.29ms
step:2201/2330 train_time:132694ms step_avg:60.29ms
step:2202/2330 train_time:132756ms step_avg:60.29ms
step:2203/2330 train_time:132816ms step_avg:60.29ms
step:2204/2330 train_time:132878ms step_avg:60.29ms
step:2205/2330 train_time:132938ms step_avg:60.29ms
step:2206/2330 train_time:133001ms step_avg:60.29ms
step:2207/2330 train_time:133061ms step_avg:60.29ms
step:2208/2330 train_time:133123ms step_avg:60.29ms
step:2209/2330 train_time:133183ms step_avg:60.29ms
step:2210/2330 train_time:133245ms step_avg:60.29ms
step:2211/2330 train_time:133305ms step_avg:60.29ms
step:2212/2330 train_time:133367ms step_avg:60.29ms
step:2213/2330 train_time:133427ms step_avg:60.29ms
step:2214/2330 train_time:133488ms step_avg:60.29ms
step:2215/2330 train_time:133548ms step_avg:60.29ms
step:2216/2330 train_time:133609ms step_avg:60.29ms
step:2217/2330 train_time:133669ms step_avg:60.29ms
step:2218/2330 train_time:133731ms step_avg:60.29ms
step:2219/2330 train_time:133791ms step_avg:60.29ms
step:2220/2330 train_time:133853ms step_avg:60.29ms
step:2221/2330 train_time:133914ms step_avg:60.29ms
step:2222/2330 train_time:133976ms step_avg:60.30ms
step:2223/2330 train_time:134036ms step_avg:60.29ms
step:2224/2330 train_time:134097ms step_avg:60.30ms
step:2225/2330 train_time:134156ms step_avg:60.29ms
step:2226/2330 train_time:134218ms step_avg:60.30ms
step:2227/2330 train_time:134278ms step_avg:60.30ms
step:2228/2330 train_time:134340ms step_avg:60.30ms
step:2229/2330 train_time:134400ms step_avg:60.30ms
step:2230/2330 train_time:134463ms step_avg:60.30ms
step:2231/2330 train_time:134524ms step_avg:60.30ms
step:2232/2330 train_time:134586ms step_avg:60.30ms
step:2233/2330 train_time:134646ms step_avg:60.30ms
step:2234/2330 train_time:134707ms step_avg:60.30ms
step:2235/2330 train_time:134767ms step_avg:60.30ms
step:2236/2330 train_time:134829ms step_avg:60.30ms
step:2237/2330 train_time:134888ms step_avg:60.30ms
step:2238/2330 train_time:134950ms step_avg:60.30ms
step:2239/2330 train_time:135011ms step_avg:60.30ms
step:2240/2330 train_time:135073ms step_avg:60.30ms
step:2241/2330 train_time:135132ms step_avg:60.30ms
step:2242/2330 train_time:135194ms step_avg:60.30ms
step:2243/2330 train_time:135254ms step_avg:60.30ms
step:2244/2330 train_time:135316ms step_avg:60.30ms
step:2245/2330 train_time:135375ms step_avg:60.30ms
step:2246/2330 train_time:135436ms step_avg:60.30ms
step:2247/2330 train_time:135496ms step_avg:60.30ms
step:2248/2330 train_time:135558ms step_avg:60.30ms
step:2249/2330 train_time:135618ms step_avg:60.30ms
step:2250/2330 train_time:135681ms step_avg:60.30ms
step:2250/2330 val_loss:3.2907 train_time:135745ms step_avg:60.33ms
step:2251/2330 train_time:135767ms step_avg:60.31ms
step:2252/2330 train_time:135807ms step_avg:60.30ms
step:2253/2330 train_time:135869ms step_avg:60.31ms
step:2254/2330 train_time:135937ms step_avg:60.31ms
step:2255/2330 train_time:135996ms step_avg:60.31ms
step:2256/2330 train_time:136058ms step_avg:60.31ms
step:2257/2330 train_time:136117ms step_avg:60.31ms
step:2258/2330 train_time:136178ms step_avg:60.31ms
step:2259/2330 train_time:136237ms step_avg:60.31ms
step:2260/2330 train_time:136299ms step_avg:60.31ms
step:2261/2330 train_time:136358ms step_avg:60.31ms
step:2262/2330 train_time:136419ms step_avg:60.31ms
step:2263/2330 train_time:136478ms step_avg:60.31ms
step:2264/2330 train_time:136539ms step_avg:60.31ms
step:2265/2330 train_time:136598ms step_avg:60.31ms
step:2266/2330 train_time:136659ms step_avg:60.31ms
step:2267/2330 train_time:136720ms step_avg:60.31ms
step:2268/2330 train_time:136783ms step_avg:60.31ms
step:2269/2330 train_time:136845ms step_avg:60.31ms
step:2270/2330 train_time:136908ms step_avg:60.31ms
step:2271/2330 train_time:136968ms step_avg:60.31ms
step:2272/2330 train_time:137031ms step_avg:60.31ms
step:2273/2330 train_time:137090ms step_avg:60.31ms
step:2274/2330 train_time:137153ms step_avg:60.31ms
step:2275/2330 train_time:137212ms step_avg:60.31ms
step:2276/2330 train_time:137274ms step_avg:60.31ms
step:2277/2330 train_time:137334ms step_avg:60.31ms
step:2278/2330 train_time:137396ms step_avg:60.31ms
step:2279/2330 train_time:137455ms step_avg:60.31ms
step:2280/2330 train_time:137516ms step_avg:60.31ms
step:2281/2330 train_time:137575ms step_avg:60.31ms
step:2282/2330 train_time:137637ms step_avg:60.31ms
step:2283/2330 train_time:137697ms step_avg:60.31ms
step:2284/2330 train_time:137759ms step_avg:60.31ms
step:2285/2330 train_time:137820ms step_avg:60.31ms
step:2286/2330 train_time:137882ms step_avg:60.32ms
step:2287/2330 train_time:137943ms step_avg:60.32ms
step:2288/2330 train_time:138005ms step_avg:60.32ms
step:2289/2330 train_time:138065ms step_avg:60.32ms
step:2290/2330 train_time:138127ms step_avg:60.32ms
step:2291/2330 train_time:138187ms step_avg:60.32ms
step:2292/2330 train_time:138248ms step_avg:60.32ms
step:2293/2330 train_time:138308ms step_avg:60.32ms
step:2294/2330 train_time:138370ms step_avg:60.32ms
step:2295/2330 train_time:138430ms step_avg:60.32ms
step:2296/2330 train_time:138492ms step_avg:60.32ms
step:2297/2330 train_time:138551ms step_avg:60.32ms
step:2298/2330 train_time:138613ms step_avg:60.32ms
step:2299/2330 train_time:138674ms step_avg:60.32ms
step:2300/2330 train_time:138736ms step_avg:60.32ms
step:2301/2330 train_time:138796ms step_avg:60.32ms
step:2302/2330 train_time:138858ms step_avg:60.32ms
step:2303/2330 train_time:138918ms step_avg:60.32ms
step:2304/2330 train_time:138979ms step_avg:60.32ms
step:2305/2330 train_time:139039ms step_avg:60.32ms
step:2306/2330 train_time:139102ms step_avg:60.32ms
step:2307/2330 train_time:139162ms step_avg:60.32ms
step:2308/2330 train_time:139223ms step_avg:60.32ms
step:2309/2330 train_time:139283ms step_avg:60.32ms
step:2310/2330 train_time:139345ms step_avg:60.32ms
step:2311/2330 train_time:139405ms step_avg:60.32ms
step:2312/2330 train_time:139467ms step_avg:60.32ms
step:2313/2330 train_time:139527ms step_avg:60.32ms
step:2314/2330 train_time:139588ms step_avg:60.32ms
step:2315/2330 train_time:139647ms step_avg:60.32ms
step:2316/2330 train_time:139709ms step_avg:60.32ms
step:2317/2330 train_time:139770ms step_avg:60.32ms
step:2318/2330 train_time:139832ms step_avg:60.32ms
step:2319/2330 train_time:139893ms step_avg:60.32ms
step:2320/2330 train_time:139955ms step_avg:60.33ms
step:2321/2330 train_time:140015ms step_avg:60.33ms
step:2322/2330 train_time:140078ms step_avg:60.33ms
step:2323/2330 train_time:140137ms step_avg:60.33ms
step:2324/2330 train_time:140198ms step_avg:60.33ms
step:2325/2330 train_time:140257ms step_avg:60.33ms
step:2326/2330 train_time:140319ms step_avg:60.33ms
step:2327/2330 train_time:140379ms step_avg:60.33ms
step:2328/2330 train_time:140440ms step_avg:60.33ms
step:2329/2330 train_time:140500ms step_avg:60.33ms
step:2330/2330 train_time:140562ms step_avg:60.33ms
step:2330/2330 val_loss:3.2772 train_time:140626ms step_avg:60.35ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
coeffs_list = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in coeffs_list:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            # x_out = self.blocks[i](x, x0, lambdas[i], attn_args)
            # x_backout += backout_lambdas[i] * (x_out-x)
            # x = x_out
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i==8:
                x_backout=x

        # backout contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda*x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2290  # number of iterations to run
    iteration_extension = 40  # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.45  # fraction of training spent cooling down the learning rate
    momentum_cd_steps = 50  # number of iterations for muon momentum cooldown
    # evaluation and logging
    run_id: str = f"new/{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

def update_optimizer_params(step, optimizer1, optimizer2):
    # Update lr
    for group in optimizer1.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)

    # Warmup phase: gradually increase momentum from 0.85 to 0.95
    if step < 300:
        frac = step / 300
        momentum = 0.85 + frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum

    # Cooldown phase: gradually decrease momentum
    momentum_cd_start = args.num_iterations + args.iteration_extension - args.momentum_cd_steps
    if step > momentum_cd_start:
        frac = (step - momentum_cd_start) / args.momentum_cd_steps

        # Decay momentum from 0.95 to 0.85
        momentum = 0.95 - frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    update_optimizer_params(step, optimizer1, optimizer2)
    # only step Adam every other step
    if step%2==0:
        optimizer2.step()
        optimizer2.zero_grad(set_to_none=True)
    else:
        for opt in optimizers:
            opt.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
    
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Sat Oct  4 06:04:13 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   28C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   28C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   40C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   28C    P0            112W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          269231      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          269232      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          269233      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          269234      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          269235      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          269236      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          269237      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          269238      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          269232      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          269233      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          269234      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          269235      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          269236      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          269237      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          269238      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:94ms step_avg:93.77ms
step:2/2330 train_time:188ms step_avg:93.83ms
step:3/2330 train_time:209ms step_avg:69.62ms
step:4/2330 train_time:245ms step_avg:61.14ms
step:5/2330 train_time:302ms step_avg:60.31ms
step:6/2330 train_time:361ms step_avg:60.24ms
step:7/2330 train_time:420ms step_avg:59.93ms
step:8/2330 train_time:480ms step_avg:60.00ms
step:9/2330 train_time:539ms step_avg:59.84ms
step:10/2330 train_time:599ms step_avg:59.89ms
step:11/2330 train_time:657ms step_avg:59.74ms
step:12/2330 train_time:718ms step_avg:59.83ms
step:13/2330 train_time:776ms step_avg:59.71ms
step:14/2330 train_time:837ms step_avg:59.81ms
step:15/2330 train_time:896ms step_avg:59.70ms
step:16/2330 train_time:956ms step_avg:59.76ms
step:17/2330 train_time:1016ms step_avg:59.79ms
step:18/2330 train_time:1082ms step_avg:60.09ms
step:19/2330 train_time:1145ms step_avg:60.27ms
step:20/2330 train_time:1207ms step_avg:60.37ms
step:21/2330 train_time:1267ms step_avg:60.35ms
step:22/2330 train_time:1329ms step_avg:60.40ms
step:23/2330 train_time:1387ms step_avg:60.32ms
step:24/2330 train_time:1448ms step_avg:60.34ms
step:25/2330 train_time:1507ms step_avg:60.29ms
step:26/2330 train_time:1568ms step_avg:60.30ms
step:27/2330 train_time:1627ms step_avg:60.25ms
step:28/2330 train_time:1688ms step_avg:60.28ms
step:29/2330 train_time:1747ms step_avg:60.23ms
step:30/2330 train_time:1808ms step_avg:60.25ms
step:31/2330 train_time:1867ms step_avg:60.22ms
step:32/2330 train_time:1928ms step_avg:60.26ms
step:33/2330 train_time:1989ms step_avg:60.26ms
step:34/2330 train_time:2050ms step_avg:60.31ms
step:35/2330 train_time:2111ms step_avg:60.31ms
step:36/2330 train_time:2172ms step_avg:60.32ms
step:37/2330 train_time:2231ms step_avg:60.29ms
step:38/2330 train_time:2291ms step_avg:60.30ms
step:39/2330 train_time:2350ms step_avg:60.26ms
step:40/2330 train_time:2411ms step_avg:60.29ms
step:41/2330 train_time:2470ms step_avg:60.24ms
step:42/2330 train_time:2530ms step_avg:60.25ms
step:43/2330 train_time:2589ms step_avg:60.20ms
step:44/2330 train_time:2650ms step_avg:60.23ms
step:45/2330 train_time:2709ms step_avg:60.21ms
step:46/2330 train_time:2770ms step_avg:60.22ms
step:47/2330 train_time:2829ms step_avg:60.19ms
step:48/2330 train_time:2890ms step_avg:60.22ms
step:49/2330 train_time:2950ms step_avg:60.20ms
step:50/2330 train_time:3012ms step_avg:60.24ms
step:51/2330 train_time:3072ms step_avg:60.23ms
step:52/2330 train_time:3132ms step_avg:60.24ms
step:53/2330 train_time:3191ms step_avg:60.21ms
step:54/2330 train_time:3252ms step_avg:60.22ms
step:55/2330 train_time:3311ms step_avg:60.19ms
step:56/2330 train_time:3371ms step_avg:60.20ms
step:57/2330 train_time:3430ms step_avg:60.17ms
step:58/2330 train_time:3490ms step_avg:60.18ms
step:59/2330 train_time:3549ms step_avg:60.15ms
step:60/2330 train_time:3610ms step_avg:60.16ms
step:61/2330 train_time:3669ms step_avg:60.14ms
step:62/2330 train_time:3729ms step_avg:60.15ms
step:63/2330 train_time:3789ms step_avg:60.14ms
step:64/2330 train_time:3850ms step_avg:60.16ms
step:65/2330 train_time:3909ms step_avg:60.14ms
step:66/2330 train_time:3971ms step_avg:60.16ms
step:67/2330 train_time:4030ms step_avg:60.15ms
step:68/2330 train_time:4091ms step_avg:60.16ms
step:69/2330 train_time:4151ms step_avg:60.16ms
step:70/2330 train_time:4212ms step_avg:60.17ms
step:71/2330 train_time:4271ms step_avg:60.15ms
step:72/2330 train_time:4331ms step_avg:60.16ms
step:73/2330 train_time:4389ms step_avg:60.13ms
step:74/2330 train_time:4450ms step_avg:60.14ms
step:75/2330 train_time:4509ms step_avg:60.12ms
step:76/2330 train_time:4570ms step_avg:60.13ms
step:77/2330 train_time:4628ms step_avg:60.11ms
step:78/2330 train_time:4689ms step_avg:60.11ms
step:79/2330 train_time:4747ms step_avg:60.09ms
step:80/2330 train_time:4808ms step_avg:60.10ms
step:81/2330 train_time:4867ms step_avg:60.08ms
step:82/2330 train_time:4928ms step_avg:60.09ms
step:83/2330 train_time:4987ms step_avg:60.09ms
step:84/2330 train_time:5049ms step_avg:60.10ms
step:85/2330 train_time:5108ms step_avg:60.09ms
step:86/2330 train_time:5169ms step_avg:60.10ms
step:87/2330 train_time:5227ms step_avg:60.08ms
step:88/2330 train_time:5287ms step_avg:60.08ms
step:89/2330 train_time:5346ms step_avg:60.07ms
step:90/2330 train_time:5406ms step_avg:60.07ms
step:91/2330 train_time:5464ms step_avg:60.05ms
step:92/2330 train_time:5525ms step_avg:60.06ms
step:93/2330 train_time:5584ms step_avg:60.04ms
step:94/2330 train_time:5644ms step_avg:60.04ms
step:95/2330 train_time:5702ms step_avg:60.03ms
step:96/2330 train_time:5763ms step_avg:60.03ms
step:97/2330 train_time:5821ms step_avg:60.01ms
step:98/2330 train_time:5882ms step_avg:60.02ms
step:99/2330 train_time:5941ms step_avg:60.01ms
step:100/2330 train_time:6002ms step_avg:60.02ms
step:101/2330 train_time:6062ms step_avg:60.02ms
step:102/2330 train_time:6123ms step_avg:60.03ms
step:103/2330 train_time:6181ms step_avg:60.01ms
step:104/2330 train_time:6242ms step_avg:60.02ms
step:105/2330 train_time:6300ms step_avg:60.00ms
step:106/2330 train_time:6361ms step_avg:60.01ms
step:107/2330 train_time:6421ms step_avg:60.01ms
step:108/2330 train_time:6482ms step_avg:60.02ms
step:109/2330 train_time:6541ms step_avg:60.01ms
step:110/2330 train_time:6602ms step_avg:60.02ms
step:111/2330 train_time:6660ms step_avg:60.00ms
step:112/2330 train_time:6721ms step_avg:60.01ms
step:113/2330 train_time:6779ms step_avg:59.99ms
step:114/2330 train_time:6840ms step_avg:60.00ms
step:115/2330 train_time:6898ms step_avg:59.98ms
step:116/2330 train_time:6959ms step_avg:59.99ms
step:117/2330 train_time:7019ms step_avg:59.99ms
step:118/2330 train_time:7080ms step_avg:60.00ms
step:119/2330 train_time:7139ms step_avg:59.99ms
step:120/2330 train_time:7200ms step_avg:60.00ms
step:121/2330 train_time:7259ms step_avg:59.99ms
step:122/2330 train_time:7320ms step_avg:60.00ms
step:123/2330 train_time:7380ms step_avg:60.00ms
step:124/2330 train_time:7440ms step_avg:60.00ms
step:125/2330 train_time:7499ms step_avg:59.99ms
step:126/2330 train_time:7561ms step_avg:60.01ms
step:127/2330 train_time:7619ms step_avg:59.99ms
step:128/2330 train_time:7680ms step_avg:60.00ms
step:129/2330 train_time:7738ms step_avg:59.98ms
step:130/2330 train_time:7799ms step_avg:59.99ms
step:131/2330 train_time:7857ms step_avg:59.98ms
step:132/2330 train_time:7918ms step_avg:59.98ms
step:133/2330 train_time:7977ms step_avg:59.98ms
step:134/2330 train_time:8038ms step_avg:59.99ms
step:135/2330 train_time:8096ms step_avg:59.97ms
step:136/2330 train_time:8158ms step_avg:59.99ms
step:137/2330 train_time:8217ms step_avg:59.98ms
step:138/2330 train_time:8278ms step_avg:59.99ms
step:139/2330 train_time:8337ms step_avg:59.98ms
step:140/2330 train_time:8397ms step_avg:59.98ms
step:141/2330 train_time:8457ms step_avg:59.98ms
step:142/2330 train_time:8518ms step_avg:59.99ms
step:143/2330 train_time:8577ms step_avg:59.98ms
step:144/2330 train_time:8638ms step_avg:59.98ms
step:145/2330 train_time:8696ms step_avg:59.97ms
step:146/2330 train_time:8758ms step_avg:59.98ms
step:147/2330 train_time:8816ms step_avg:59.97ms
step:148/2330 train_time:8877ms step_avg:59.98ms
step:149/2330 train_time:8936ms step_avg:59.97ms
step:150/2330 train_time:8997ms step_avg:59.98ms
step:151/2330 train_time:9055ms step_avg:59.97ms
step:152/2330 train_time:9116ms step_avg:59.98ms
step:153/2330 train_time:9175ms step_avg:59.97ms
step:154/2330 train_time:9236ms step_avg:59.98ms
step:155/2330 train_time:9295ms step_avg:59.97ms
step:156/2330 train_time:9356ms step_avg:59.98ms
step:157/2330 train_time:9415ms step_avg:59.97ms
step:158/2330 train_time:9476ms step_avg:59.98ms
step:159/2330 train_time:9535ms step_avg:59.97ms
step:160/2330 train_time:9596ms step_avg:59.97ms
step:161/2330 train_time:9655ms step_avg:59.97ms
step:162/2330 train_time:9715ms step_avg:59.97ms
step:163/2330 train_time:9774ms step_avg:59.96ms
step:164/2330 train_time:9835ms step_avg:59.97ms
step:165/2330 train_time:9893ms step_avg:59.96ms
step:166/2330 train_time:9954ms step_avg:59.96ms
step:167/2330 train_time:10012ms step_avg:59.95ms
step:168/2330 train_time:10073ms step_avg:59.96ms
step:169/2330 train_time:10131ms step_avg:59.95ms
step:170/2330 train_time:10191ms step_avg:59.95ms
step:171/2330 train_time:10250ms step_avg:59.94ms
step:172/2330 train_time:10312ms step_avg:59.95ms
step:173/2330 train_time:10371ms step_avg:59.95ms
step:174/2330 train_time:10432ms step_avg:59.95ms
step:175/2330 train_time:10490ms step_avg:59.94ms
step:176/2330 train_time:10551ms step_avg:59.95ms
step:177/2330 train_time:10611ms step_avg:59.95ms
step:178/2330 train_time:10671ms step_avg:59.95ms
step:179/2330 train_time:10729ms step_avg:59.94ms
step:180/2330 train_time:10790ms step_avg:59.94ms
step:181/2330 train_time:10848ms step_avg:59.94ms
step:182/2330 train_time:10910ms step_avg:59.95ms
step:183/2330 train_time:10969ms step_avg:59.94ms
step:184/2330 train_time:11029ms step_avg:59.94ms
step:185/2330 train_time:11087ms step_avg:59.93ms
step:186/2330 train_time:11148ms step_avg:59.94ms
step:187/2330 train_time:11206ms step_avg:59.93ms
step:188/2330 train_time:11267ms step_avg:59.93ms
step:189/2330 train_time:11325ms step_avg:59.92ms
step:190/2330 train_time:11385ms step_avg:59.92ms
step:191/2330 train_time:11444ms step_avg:59.92ms
step:192/2330 train_time:11505ms step_avg:59.92ms
step:193/2330 train_time:11564ms step_avg:59.91ms
step:194/2330 train_time:11624ms step_avg:59.92ms
step:195/2330 train_time:11683ms step_avg:59.91ms
step:196/2330 train_time:11743ms step_avg:59.91ms
step:197/2330 train_time:11801ms step_avg:59.90ms
step:198/2330 train_time:11862ms step_avg:59.91ms
step:199/2330 train_time:11922ms step_avg:59.91ms
step:200/2330 train_time:11983ms step_avg:59.91ms
step:201/2330 train_time:12042ms step_avg:59.91ms
step:202/2330 train_time:12102ms step_avg:59.91ms
step:203/2330 train_time:12161ms step_avg:59.90ms
step:204/2330 train_time:12222ms step_avg:59.91ms
step:205/2330 train_time:12281ms step_avg:59.91ms
step:206/2330 train_time:12342ms step_avg:59.91ms
step:207/2330 train_time:12401ms step_avg:59.91ms
step:208/2330 train_time:12462ms step_avg:59.91ms
step:209/2330 train_time:12520ms step_avg:59.91ms
step:210/2330 train_time:12581ms step_avg:59.91ms
step:211/2330 train_time:12640ms step_avg:59.90ms
step:212/2330 train_time:12700ms step_avg:59.91ms
step:213/2330 train_time:12759ms step_avg:59.90ms
step:214/2330 train_time:12819ms step_avg:59.90ms
step:215/2330 train_time:12878ms step_avg:59.90ms
step:216/2330 train_time:12940ms step_avg:59.91ms
step:217/2330 train_time:12998ms step_avg:59.90ms
step:218/2330 train_time:13059ms step_avg:59.90ms
step:219/2330 train_time:13117ms step_avg:59.90ms
step:220/2330 train_time:13177ms step_avg:59.90ms
step:221/2330 train_time:13236ms step_avg:59.89ms
step:222/2330 train_time:13297ms step_avg:59.90ms
step:223/2330 train_time:13356ms step_avg:59.89ms
step:224/2330 train_time:13416ms step_avg:59.89ms
step:225/2330 train_time:13475ms step_avg:59.89ms
step:226/2330 train_time:13536ms step_avg:59.89ms
step:227/2330 train_time:13595ms step_avg:59.89ms
step:228/2330 train_time:13656ms step_avg:59.90ms
step:229/2330 train_time:13715ms step_avg:59.89ms
step:230/2330 train_time:13775ms step_avg:59.89ms
step:231/2330 train_time:13834ms step_avg:59.89ms
step:232/2330 train_time:13895ms step_avg:59.89ms
step:233/2330 train_time:13954ms step_avg:59.89ms
step:234/2330 train_time:14015ms step_avg:59.89ms
step:235/2330 train_time:14073ms step_avg:59.89ms
step:236/2330 train_time:14134ms step_avg:59.89ms
step:237/2330 train_time:14193ms step_avg:59.89ms
step:238/2330 train_time:14253ms step_avg:59.89ms
step:239/2330 train_time:14312ms step_avg:59.88ms
step:240/2330 train_time:14372ms step_avg:59.88ms
step:241/2330 train_time:14431ms step_avg:59.88ms
step:242/2330 train_time:14491ms step_avg:59.88ms
step:243/2330 train_time:14550ms step_avg:59.87ms
step:244/2330 train_time:14611ms step_avg:59.88ms
step:245/2330 train_time:14670ms step_avg:59.88ms
step:246/2330 train_time:14731ms step_avg:59.88ms
step:247/2330 train_time:14790ms step_avg:59.88ms
step:248/2330 train_time:14851ms step_avg:59.88ms
step:249/2330 train_time:14910ms step_avg:59.88ms
step:250/2330 train_time:14970ms step_avg:59.88ms
step:250/2330 val_loss:4.0941 train_time:15032ms step_avg:60.13ms
step:251/2330 train_time:15053ms step_avg:59.97ms
step:252/2330 train_time:15091ms step_avg:59.88ms
step:253/2330 train_time:15153ms step_avg:59.90ms
step:254/2330 train_time:15220ms step_avg:59.92ms
step:255/2330 train_time:15279ms step_avg:59.92ms
step:256/2330 train_time:15341ms step_avg:59.92ms
step:257/2330 train_time:15400ms step_avg:59.92ms
step:258/2330 train_time:15460ms step_avg:59.92ms
step:259/2330 train_time:15517ms step_avg:59.91ms
step:260/2330 train_time:15577ms step_avg:59.91ms
step:261/2330 train_time:15635ms step_avg:59.90ms
step:262/2330 train_time:15695ms step_avg:59.90ms
step:263/2330 train_time:15753ms step_avg:59.90ms
step:264/2330 train_time:15812ms step_avg:59.90ms
step:265/2330 train_time:15870ms step_avg:59.89ms
step:266/2330 train_time:15930ms step_avg:59.89ms
step:267/2330 train_time:15989ms step_avg:59.88ms
step:268/2330 train_time:16050ms step_avg:59.89ms
step:269/2330 train_time:16110ms step_avg:59.89ms
step:270/2330 train_time:16173ms step_avg:59.90ms
step:271/2330 train_time:16233ms step_avg:59.90ms
step:272/2330 train_time:16295ms step_avg:59.91ms
step:273/2330 train_time:16354ms step_avg:59.90ms
step:274/2330 train_time:16415ms step_avg:59.91ms
step:275/2330 train_time:16474ms step_avg:59.90ms
step:276/2330 train_time:16534ms step_avg:59.91ms
step:277/2330 train_time:16592ms step_avg:59.90ms
step:278/2330 train_time:16653ms step_avg:59.90ms
step:279/2330 train_time:16710ms step_avg:59.89ms
step:280/2330 train_time:16770ms step_avg:59.89ms
step:281/2330 train_time:16828ms step_avg:59.89ms
step:282/2330 train_time:16888ms step_avg:59.89ms
step:283/2330 train_time:16946ms step_avg:59.88ms
step:284/2330 train_time:17006ms step_avg:59.88ms
step:285/2330 train_time:17065ms step_avg:59.88ms
step:286/2330 train_time:17126ms step_avg:59.88ms
step:287/2330 train_time:17186ms step_avg:59.88ms
step:288/2330 train_time:17247ms step_avg:59.89ms
step:289/2330 train_time:17307ms step_avg:59.89ms
step:290/2330 train_time:17369ms step_avg:59.89ms
step:291/2330 train_time:17428ms step_avg:59.89ms
step:292/2330 train_time:17489ms step_avg:59.89ms
step:293/2330 train_time:17548ms step_avg:59.89ms
step:294/2330 train_time:17609ms step_avg:59.89ms
step:295/2330 train_time:17667ms step_avg:59.89ms
step:296/2330 train_time:17727ms step_avg:59.89ms
step:297/2330 train_time:17786ms step_avg:59.88ms
step:298/2330 train_time:17846ms step_avg:59.89ms
step:299/2330 train_time:17904ms step_avg:59.88ms
step:300/2330 train_time:17964ms step_avg:59.88ms
step:301/2330 train_time:18022ms step_avg:59.88ms
step:302/2330 train_time:18083ms step_avg:59.88ms
step:303/2330 train_time:18142ms step_avg:59.88ms
step:304/2330 train_time:18203ms step_avg:59.88ms
step:305/2330 train_time:18262ms step_avg:59.88ms
step:306/2330 train_time:18322ms step_avg:59.88ms
step:307/2330 train_time:18382ms step_avg:59.88ms
step:308/2330 train_time:18443ms step_avg:59.88ms
step:309/2330 train_time:18502ms step_avg:59.88ms
step:310/2330 train_time:18562ms step_avg:59.88ms
step:311/2330 train_time:18620ms step_avg:59.87ms
step:312/2330 train_time:18681ms step_avg:59.88ms
step:313/2330 train_time:18740ms step_avg:59.87ms
step:314/2330 train_time:18800ms step_avg:59.87ms
step:315/2330 train_time:18858ms step_avg:59.87ms
step:316/2330 train_time:18918ms step_avg:59.87ms
step:317/2330 train_time:18975ms step_avg:59.86ms
step:318/2330 train_time:19036ms step_avg:59.86ms
step:319/2330 train_time:19094ms step_avg:59.86ms
step:320/2330 train_time:19154ms step_avg:59.86ms
step:321/2330 train_time:19213ms step_avg:59.85ms
step:322/2330 train_time:19273ms step_avg:59.86ms
step:323/2330 train_time:19332ms step_avg:59.85ms
step:324/2330 train_time:19393ms step_avg:59.85ms
step:325/2330 train_time:19451ms step_avg:59.85ms
step:326/2330 train_time:19512ms step_avg:59.85ms
step:327/2330 train_time:19571ms step_avg:59.85ms
step:328/2330 train_time:19632ms step_avg:59.85ms
step:329/2330 train_time:19691ms step_avg:59.85ms
step:330/2330 train_time:19752ms step_avg:59.85ms
step:331/2330 train_time:19810ms step_avg:59.85ms
step:332/2330 train_time:19871ms step_avg:59.85ms
step:333/2330 train_time:19929ms step_avg:59.85ms
step:334/2330 train_time:19990ms step_avg:59.85ms
step:335/2330 train_time:20048ms step_avg:59.85ms
step:336/2330 train_time:20109ms step_avg:59.85ms
step:337/2330 train_time:20167ms step_avg:59.84ms
step:338/2330 train_time:20228ms step_avg:59.85ms
step:339/2330 train_time:20287ms step_avg:59.84ms
step:340/2330 train_time:20347ms step_avg:59.84ms
step:341/2330 train_time:20406ms step_avg:59.84ms
step:342/2330 train_time:20466ms step_avg:59.84ms
step:343/2330 train_time:20525ms step_avg:59.84ms
step:344/2330 train_time:20586ms step_avg:59.84ms
step:345/2330 train_time:20644ms step_avg:59.84ms
step:346/2330 train_time:20705ms step_avg:59.84ms
step:347/2330 train_time:20763ms step_avg:59.84ms
step:348/2330 train_time:20824ms step_avg:59.84ms
step:349/2330 train_time:20883ms step_avg:59.84ms
step:350/2330 train_time:20944ms step_avg:59.84ms
step:351/2330 train_time:21003ms step_avg:59.84ms
step:352/2330 train_time:21063ms step_avg:59.84ms
step:353/2330 train_time:21120ms step_avg:59.83ms
step:354/2330 train_time:21181ms step_avg:59.83ms
step:355/2330 train_time:21239ms step_avg:59.83ms
step:356/2330 train_time:21300ms step_avg:59.83ms
step:357/2330 train_time:21358ms step_avg:59.83ms
step:358/2330 train_time:21419ms step_avg:59.83ms
step:359/2330 train_time:21477ms step_avg:59.83ms
step:360/2330 train_time:21538ms step_avg:59.83ms
step:361/2330 train_time:21597ms step_avg:59.83ms
step:362/2330 train_time:21657ms step_avg:59.83ms
step:363/2330 train_time:21716ms step_avg:59.82ms
step:364/2330 train_time:21776ms step_avg:59.83ms
step:365/2330 train_time:21835ms step_avg:59.82ms
step:366/2330 train_time:21895ms step_avg:59.82ms
step:367/2330 train_time:21954ms step_avg:59.82ms
step:368/2330 train_time:22014ms step_avg:59.82ms
step:369/2330 train_time:22072ms step_avg:59.82ms
step:370/2330 train_time:22133ms step_avg:59.82ms
step:371/2330 train_time:22191ms step_avg:59.81ms
step:372/2330 train_time:22252ms step_avg:59.82ms
step:373/2330 train_time:22310ms step_avg:59.81ms
step:374/2330 train_time:22371ms step_avg:59.82ms
step:375/2330 train_time:22430ms step_avg:59.81ms
step:376/2330 train_time:22491ms step_avg:59.82ms
step:377/2330 train_time:22551ms step_avg:59.82ms
step:378/2330 train_time:22612ms step_avg:59.82ms
step:379/2330 train_time:22670ms step_avg:59.82ms
step:380/2330 train_time:22731ms step_avg:59.82ms
step:381/2330 train_time:22790ms step_avg:59.82ms
step:382/2330 train_time:22851ms step_avg:59.82ms
step:383/2330 train_time:22909ms step_avg:59.82ms
step:384/2330 train_time:22970ms step_avg:59.82ms
step:385/2330 train_time:23028ms step_avg:59.81ms
step:386/2330 train_time:23089ms step_avg:59.82ms
step:387/2330 train_time:23148ms step_avg:59.81ms
step:388/2330 train_time:23208ms step_avg:59.81ms
step:389/2330 train_time:23266ms step_avg:59.81ms
step:390/2330 train_time:23327ms step_avg:59.81ms
step:391/2330 train_time:23386ms step_avg:59.81ms
step:392/2330 train_time:23446ms step_avg:59.81ms
step:393/2330 train_time:23505ms step_avg:59.81ms
step:394/2330 train_time:23565ms step_avg:59.81ms
step:395/2330 train_time:23624ms step_avg:59.81ms
step:396/2330 train_time:23685ms step_avg:59.81ms
step:397/2330 train_time:23744ms step_avg:59.81ms
step:398/2330 train_time:23805ms step_avg:59.81ms
step:399/2330 train_time:23863ms step_avg:59.81ms
step:400/2330 train_time:23923ms step_avg:59.81ms
step:401/2330 train_time:23982ms step_avg:59.81ms
step:402/2330 train_time:24042ms step_avg:59.81ms
step:403/2330 train_time:24101ms step_avg:59.80ms
step:404/2330 train_time:24161ms step_avg:59.80ms
step:405/2330 train_time:24220ms step_avg:59.80ms
step:406/2330 train_time:24280ms step_avg:59.80ms
step:407/2330 train_time:24339ms step_avg:59.80ms
step:408/2330 train_time:24399ms step_avg:59.80ms
step:409/2330 train_time:24458ms step_avg:59.80ms
step:410/2330 train_time:24518ms step_avg:59.80ms
step:411/2330 train_time:24576ms step_avg:59.80ms
step:412/2330 train_time:24636ms step_avg:59.80ms
step:413/2330 train_time:24695ms step_avg:59.79ms
step:414/2330 train_time:24755ms step_avg:59.79ms
step:415/2330 train_time:24814ms step_avg:59.79ms
step:416/2330 train_time:24874ms step_avg:59.79ms
step:417/2330 train_time:24932ms step_avg:59.79ms
step:418/2330 train_time:24993ms step_avg:59.79ms
step:419/2330 train_time:25051ms step_avg:59.79ms
step:420/2330 train_time:25112ms step_avg:59.79ms
step:421/2330 train_time:25171ms step_avg:59.79ms
step:422/2330 train_time:25232ms step_avg:59.79ms
step:423/2330 train_time:25291ms step_avg:59.79ms
step:424/2330 train_time:25352ms step_avg:59.79ms
step:425/2330 train_time:25410ms step_avg:59.79ms
step:426/2330 train_time:25471ms step_avg:59.79ms
step:427/2330 train_time:25530ms step_avg:59.79ms
step:428/2330 train_time:25591ms step_avg:59.79ms
step:429/2330 train_time:25649ms step_avg:59.79ms
step:430/2330 train_time:25709ms step_avg:59.79ms
step:431/2330 train_time:25768ms step_avg:59.79ms
step:432/2330 train_time:25829ms step_avg:59.79ms
step:433/2330 train_time:25887ms step_avg:59.79ms
step:434/2330 train_time:25948ms step_avg:59.79ms
step:435/2330 train_time:26006ms step_avg:59.78ms
step:436/2330 train_time:26067ms step_avg:59.79ms
step:437/2330 train_time:26126ms step_avg:59.78ms
step:438/2330 train_time:26186ms step_avg:59.79ms
step:439/2330 train_time:26245ms step_avg:59.78ms
step:440/2330 train_time:26305ms step_avg:59.78ms
step:441/2330 train_time:26364ms step_avg:59.78ms
step:442/2330 train_time:26424ms step_avg:59.78ms
step:443/2330 train_time:26483ms step_avg:59.78ms
step:444/2330 train_time:26543ms step_avg:59.78ms
step:445/2330 train_time:26602ms step_avg:59.78ms
step:446/2330 train_time:26662ms step_avg:59.78ms
step:447/2330 train_time:26721ms step_avg:59.78ms
step:448/2330 train_time:26781ms step_avg:59.78ms
step:449/2330 train_time:26840ms step_avg:59.78ms
step:450/2330 train_time:26900ms step_avg:59.78ms
step:451/2330 train_time:26958ms step_avg:59.77ms
step:452/2330 train_time:27018ms step_avg:59.77ms
step:453/2330 train_time:27076ms step_avg:59.77ms
step:454/2330 train_time:27137ms step_avg:59.77ms
step:455/2330 train_time:27195ms step_avg:59.77ms
step:456/2330 train_time:27255ms step_avg:59.77ms
step:457/2330 train_time:27314ms step_avg:59.77ms
step:458/2330 train_time:27374ms step_avg:59.77ms
step:459/2330 train_time:27433ms step_avg:59.77ms
step:460/2330 train_time:27494ms step_avg:59.77ms
step:461/2330 train_time:27552ms step_avg:59.77ms
step:462/2330 train_time:27613ms step_avg:59.77ms
step:463/2330 train_time:27671ms step_avg:59.77ms
step:464/2330 train_time:27732ms step_avg:59.77ms
step:465/2330 train_time:27791ms step_avg:59.77ms
step:466/2330 train_time:27852ms step_avg:59.77ms
step:467/2330 train_time:27910ms step_avg:59.76ms
step:468/2330 train_time:27971ms step_avg:59.77ms
step:469/2330 train_time:28029ms step_avg:59.76ms
step:470/2330 train_time:28092ms step_avg:59.77ms
step:471/2330 train_time:28150ms step_avg:59.77ms
step:472/2330 train_time:28211ms step_avg:59.77ms
step:473/2330 train_time:28269ms step_avg:59.77ms
step:474/2330 train_time:28329ms step_avg:59.77ms
step:475/2330 train_time:28388ms step_avg:59.76ms
step:476/2330 train_time:28449ms step_avg:59.77ms
step:477/2330 train_time:28508ms step_avg:59.77ms
step:478/2330 train_time:28569ms step_avg:59.77ms
step:479/2330 train_time:28628ms step_avg:59.77ms
step:480/2330 train_time:28689ms step_avg:59.77ms
step:481/2330 train_time:28747ms step_avg:59.77ms
step:482/2330 train_time:28808ms step_avg:59.77ms
step:483/2330 train_time:28866ms step_avg:59.76ms
step:484/2330 train_time:28927ms step_avg:59.77ms
step:485/2330 train_time:28986ms step_avg:59.76ms
step:486/2330 train_time:29046ms step_avg:59.77ms
step:487/2330 train_time:29105ms step_avg:59.76ms
step:488/2330 train_time:29166ms step_avg:59.77ms
step:489/2330 train_time:29224ms step_avg:59.76ms
step:490/2330 train_time:29284ms step_avg:59.76ms
step:491/2330 train_time:29343ms step_avg:59.76ms
step:492/2330 train_time:29404ms step_avg:59.76ms
step:493/2330 train_time:29463ms step_avg:59.76ms
step:494/2330 train_time:29523ms step_avg:59.76ms
step:495/2330 train_time:29581ms step_avg:59.76ms
step:496/2330 train_time:29642ms step_avg:59.76ms
step:497/2330 train_time:29700ms step_avg:59.76ms
step:498/2330 train_time:29761ms step_avg:59.76ms
step:499/2330 train_time:29820ms step_avg:59.76ms
step:500/2330 train_time:29880ms step_avg:59.76ms
step:500/2330 val_loss:3.8226 train_time:29942ms step_avg:59.88ms
step:501/2330 train_time:29963ms step_avg:59.81ms
step:502/2330 train_time:30000ms step_avg:59.76ms
step:503/2330 train_time:30061ms step_avg:59.76ms
step:504/2330 train_time:30127ms step_avg:59.77ms
step:505/2330 train_time:30186ms step_avg:59.77ms
step:506/2330 train_time:30247ms step_avg:59.78ms
step:507/2330 train_time:30306ms step_avg:59.77ms
step:508/2330 train_time:30366ms step_avg:59.78ms
step:509/2330 train_time:30424ms step_avg:59.77ms
step:510/2330 train_time:30484ms step_avg:59.77ms
step:511/2330 train_time:30543ms step_avg:59.77ms
step:512/2330 train_time:30603ms step_avg:59.77ms
step:513/2330 train_time:30661ms step_avg:59.77ms
step:514/2330 train_time:30721ms step_avg:59.77ms
step:515/2330 train_time:30779ms step_avg:59.77ms
step:516/2330 train_time:30840ms step_avg:59.77ms
step:517/2330 train_time:30899ms step_avg:59.77ms
step:518/2330 train_time:30960ms step_avg:59.77ms
step:519/2330 train_time:31020ms step_avg:59.77ms
step:520/2330 train_time:31081ms step_avg:59.77ms
step:521/2330 train_time:31141ms step_avg:59.77ms
step:522/2330 train_time:31202ms step_avg:59.77ms
step:523/2330 train_time:31261ms step_avg:59.77ms
step:524/2330 train_time:31322ms step_avg:59.78ms
step:525/2330 train_time:31381ms step_avg:59.77ms
step:526/2330 train_time:31441ms step_avg:59.77ms
step:527/2330 train_time:31500ms step_avg:59.77ms
step:528/2330 train_time:31560ms step_avg:59.77ms
step:529/2330 train_time:31619ms step_avg:59.77ms
step:530/2330 train_time:31679ms step_avg:59.77ms
step:531/2330 train_time:31737ms step_avg:59.77ms
step:532/2330 train_time:31797ms step_avg:59.77ms
step:533/2330 train_time:31856ms step_avg:59.77ms
step:534/2330 train_time:31917ms step_avg:59.77ms
step:535/2330 train_time:31975ms step_avg:59.77ms
step:536/2330 train_time:32037ms step_avg:59.77ms
step:537/2330 train_time:32095ms step_avg:59.77ms
step:538/2330 train_time:32156ms step_avg:59.77ms
step:539/2330 train_time:32214ms step_avg:59.77ms
step:540/2330 train_time:32275ms step_avg:59.77ms
step:541/2330 train_time:32333ms step_avg:59.77ms
step:542/2330 train_time:32394ms step_avg:59.77ms
step:543/2330 train_time:32454ms step_avg:59.77ms
step:544/2330 train_time:32514ms step_avg:59.77ms
step:545/2330 train_time:32572ms step_avg:59.76ms
step:546/2330 train_time:32632ms step_avg:59.77ms
step:547/2330 train_time:32691ms step_avg:59.76ms
step:548/2330 train_time:32751ms step_avg:59.76ms
step:549/2330 train_time:32809ms step_avg:59.76ms
step:550/2330 train_time:32869ms step_avg:59.76ms
step:551/2330 train_time:32927ms step_avg:59.76ms
step:552/2330 train_time:32987ms step_avg:59.76ms
step:553/2330 train_time:33046ms step_avg:59.76ms
step:554/2330 train_time:33107ms step_avg:59.76ms
step:555/2330 train_time:33167ms step_avg:59.76ms
step:556/2330 train_time:33228ms step_avg:59.76ms
step:557/2330 train_time:33287ms step_avg:59.76ms
step:558/2330 train_time:33348ms step_avg:59.76ms
step:559/2330 train_time:33406ms step_avg:59.76ms
step:560/2330 train_time:33468ms step_avg:59.76ms
step:561/2330 train_time:33527ms step_avg:59.76ms
step:562/2330 train_time:33587ms step_avg:59.76ms
step:563/2330 train_time:33646ms step_avg:59.76ms
step:564/2330 train_time:33707ms step_avg:59.76ms
step:565/2330 train_time:33765ms step_avg:59.76ms
step:566/2330 train_time:33825ms step_avg:59.76ms
step:567/2330 train_time:33883ms step_avg:59.76ms
step:568/2330 train_time:33944ms step_avg:59.76ms
step:569/2330 train_time:34002ms step_avg:59.76ms
step:570/2330 train_time:34063ms step_avg:59.76ms
step:571/2330 train_time:34122ms step_avg:59.76ms
step:572/2330 train_time:34183ms step_avg:59.76ms
step:573/2330 train_time:34242ms step_avg:59.76ms
step:574/2330 train_time:34303ms step_avg:59.76ms
step:575/2330 train_time:34362ms step_avg:59.76ms
step:576/2330 train_time:34423ms step_avg:59.76ms
step:577/2330 train_time:34482ms step_avg:59.76ms
step:578/2330 train_time:34543ms step_avg:59.76ms
step:579/2330 train_time:34602ms step_avg:59.76ms
step:580/2330 train_time:34663ms step_avg:59.76ms
step:581/2330 train_time:34721ms step_avg:59.76ms
step:582/2330 train_time:34782ms step_avg:59.76ms
step:583/2330 train_time:34840ms step_avg:59.76ms
step:584/2330 train_time:34901ms step_avg:59.76ms
step:585/2330 train_time:34959ms step_avg:59.76ms
step:586/2330 train_time:35020ms step_avg:59.76ms
step:587/2330 train_time:35078ms step_avg:59.76ms
step:588/2330 train_time:35139ms step_avg:59.76ms
step:589/2330 train_time:35198ms step_avg:59.76ms
step:590/2330 train_time:35258ms step_avg:59.76ms
step:591/2330 train_time:35318ms step_avg:59.76ms
step:592/2330 train_time:35378ms step_avg:59.76ms
step:593/2330 train_time:35437ms step_avg:59.76ms
step:594/2330 train_time:35497ms step_avg:59.76ms
step:595/2330 train_time:35556ms step_avg:59.76ms
step:596/2330 train_time:35617ms step_avg:59.76ms
step:597/2330 train_time:35676ms step_avg:59.76ms
step:598/2330 train_time:35737ms step_avg:59.76ms
step:599/2330 train_time:35796ms step_avg:59.76ms
step:600/2330 train_time:35857ms step_avg:59.76ms
step:601/2330 train_time:35915ms step_avg:59.76ms
step:602/2330 train_time:35975ms step_avg:59.76ms
step:603/2330 train_time:36034ms step_avg:59.76ms
step:604/2330 train_time:36094ms step_avg:59.76ms
step:605/2330 train_time:36153ms step_avg:59.76ms
step:606/2330 train_time:36213ms step_avg:59.76ms
step:607/2330 train_time:36271ms step_avg:59.75ms
step:608/2330 train_time:36331ms step_avg:59.76ms
step:609/2330 train_time:36390ms step_avg:59.75ms
step:610/2330 train_time:36451ms step_avg:59.76ms
step:611/2330 train_time:36510ms step_avg:59.75ms
step:612/2330 train_time:36570ms step_avg:59.75ms
step:613/2330 train_time:36629ms step_avg:59.75ms
step:614/2330 train_time:36689ms step_avg:59.75ms
step:615/2330 train_time:36748ms step_avg:59.75ms
step:616/2330 train_time:36808ms step_avg:59.75ms
step:617/2330 train_time:36866ms step_avg:59.75ms
step:618/2330 train_time:36927ms step_avg:59.75ms
step:619/2330 train_time:36985ms step_avg:59.75ms
step:620/2330 train_time:37046ms step_avg:59.75ms
step:621/2330 train_time:37105ms step_avg:59.75ms
step:622/2330 train_time:37166ms step_avg:59.75ms
step:623/2330 train_time:37225ms step_avg:59.75ms
step:624/2330 train_time:37285ms step_avg:59.75ms
step:625/2330 train_time:37344ms step_avg:59.75ms
step:626/2330 train_time:37406ms step_avg:59.75ms
step:627/2330 train_time:37465ms step_avg:59.75ms
step:628/2330 train_time:37526ms step_avg:59.75ms
step:629/2330 train_time:37584ms step_avg:59.75ms
step:630/2330 train_time:37645ms step_avg:59.75ms
step:631/2330 train_time:37704ms step_avg:59.75ms
step:632/2330 train_time:37765ms step_avg:59.75ms
step:633/2330 train_time:37824ms step_avg:59.75ms
step:634/2330 train_time:37885ms step_avg:59.75ms
step:635/2330 train_time:37943ms step_avg:59.75ms
step:636/2330 train_time:38004ms step_avg:59.75ms
step:637/2330 train_time:38062ms step_avg:59.75ms
step:638/2330 train_time:38123ms step_avg:59.75ms
step:639/2330 train_time:38182ms step_avg:59.75ms
step:640/2330 train_time:38242ms step_avg:59.75ms
step:641/2330 train_time:38302ms step_avg:59.75ms
step:642/2330 train_time:38363ms step_avg:59.76ms
step:643/2330 train_time:38423ms step_avg:59.76ms
step:644/2330 train_time:38484ms step_avg:59.76ms
step:645/2330 train_time:38542ms step_avg:59.76ms
step:646/2330 train_time:38603ms step_avg:59.76ms
step:647/2330 train_time:38662ms step_avg:59.76ms
step:648/2330 train_time:38723ms step_avg:59.76ms
step:649/2330 train_time:38782ms step_avg:59.76ms
step:650/2330 train_time:38844ms step_avg:59.76ms
step:651/2330 train_time:38902ms step_avg:59.76ms
step:652/2330 train_time:38963ms step_avg:59.76ms
step:653/2330 train_time:39021ms step_avg:59.76ms
step:654/2330 train_time:39082ms step_avg:59.76ms
step:655/2330 train_time:39140ms step_avg:59.76ms
step:656/2330 train_time:39201ms step_avg:59.76ms
step:657/2330 train_time:39260ms step_avg:59.76ms
step:658/2330 train_time:39321ms step_avg:59.76ms
step:659/2330 train_time:39379ms step_avg:59.76ms
step:660/2330 train_time:39440ms step_avg:59.76ms
step:661/2330 train_time:39499ms step_avg:59.76ms
step:662/2330 train_time:39560ms step_avg:59.76ms
step:663/2330 train_time:39619ms step_avg:59.76ms
step:664/2330 train_time:39679ms step_avg:59.76ms
step:665/2330 train_time:39738ms step_avg:59.76ms
step:666/2330 train_time:39798ms step_avg:59.76ms
step:667/2330 train_time:39857ms step_avg:59.75ms
step:668/2330 train_time:39917ms step_avg:59.76ms
step:669/2330 train_time:39975ms step_avg:59.75ms
step:670/2330 train_time:40035ms step_avg:59.75ms
step:671/2330 train_time:40095ms step_avg:59.75ms
step:672/2330 train_time:40156ms step_avg:59.76ms
step:673/2330 train_time:40214ms step_avg:59.75ms
step:674/2330 train_time:40275ms step_avg:59.75ms
step:675/2330 train_time:40333ms step_avg:59.75ms
step:676/2330 train_time:40394ms step_avg:59.75ms
step:677/2330 train_time:40452ms step_avg:59.75ms
step:678/2330 train_time:40512ms step_avg:59.75ms
step:679/2330 train_time:40572ms step_avg:59.75ms
step:680/2330 train_time:40632ms step_avg:59.75ms
step:681/2330 train_time:40691ms step_avg:59.75ms
step:682/2330 train_time:40751ms step_avg:59.75ms
step:683/2330 train_time:40810ms step_avg:59.75ms
step:684/2330 train_time:40870ms step_avg:59.75ms
step:685/2330 train_time:40929ms step_avg:59.75ms
step:686/2330 train_time:40989ms step_avg:59.75ms
step:687/2330 train_time:41047ms step_avg:59.75ms
step:688/2330 train_time:41108ms step_avg:59.75ms
step:689/2330 train_time:41166ms step_avg:59.75ms
step:690/2330 train_time:41227ms step_avg:59.75ms
step:691/2330 train_time:41286ms step_avg:59.75ms
step:692/2330 train_time:41346ms step_avg:59.75ms
step:693/2330 train_time:41405ms step_avg:59.75ms
step:694/2330 train_time:41466ms step_avg:59.75ms
step:695/2330 train_time:41526ms step_avg:59.75ms
step:696/2330 train_time:41586ms step_avg:59.75ms
step:697/2330 train_time:41645ms step_avg:59.75ms
step:698/2330 train_time:41706ms step_avg:59.75ms
step:699/2330 train_time:41765ms step_avg:59.75ms
step:700/2330 train_time:41826ms step_avg:59.75ms
step:701/2330 train_time:41884ms step_avg:59.75ms
step:702/2330 train_time:41945ms step_avg:59.75ms
step:703/2330 train_time:42004ms step_avg:59.75ms
step:704/2330 train_time:42065ms step_avg:59.75ms
step:705/2330 train_time:42123ms step_avg:59.75ms
step:706/2330 train_time:42184ms step_avg:59.75ms
step:707/2330 train_time:42244ms step_avg:59.75ms
step:708/2330 train_time:42304ms step_avg:59.75ms
step:709/2330 train_time:42363ms step_avg:59.75ms
step:710/2330 train_time:42424ms step_avg:59.75ms
step:711/2330 train_time:42482ms step_avg:59.75ms
step:712/2330 train_time:42545ms step_avg:59.75ms
step:713/2330 train_time:42604ms step_avg:59.75ms
step:714/2330 train_time:42665ms step_avg:59.75ms
step:715/2330 train_time:42725ms step_avg:59.75ms
step:716/2330 train_time:42785ms step_avg:59.76ms
step:717/2330 train_time:42844ms step_avg:59.75ms
step:718/2330 train_time:42905ms step_avg:59.76ms
step:719/2330 train_time:42963ms step_avg:59.75ms
step:720/2330 train_time:43024ms step_avg:59.76ms
step:721/2330 train_time:43082ms step_avg:59.75ms
step:722/2330 train_time:43143ms step_avg:59.76ms
step:723/2330 train_time:43202ms step_avg:59.75ms
step:724/2330 train_time:43263ms step_avg:59.75ms
step:725/2330 train_time:43322ms step_avg:59.75ms
step:726/2330 train_time:43383ms step_avg:59.76ms
step:727/2330 train_time:43442ms step_avg:59.75ms
step:728/2330 train_time:43503ms step_avg:59.76ms
step:729/2330 train_time:43562ms step_avg:59.76ms
step:730/2330 train_time:43623ms step_avg:59.76ms
step:731/2330 train_time:43681ms step_avg:59.76ms
step:732/2330 train_time:43743ms step_avg:59.76ms
step:733/2330 train_time:43801ms step_avg:59.76ms
step:734/2330 train_time:43861ms step_avg:59.76ms
step:735/2330 train_time:43920ms step_avg:59.76ms
step:736/2330 train_time:43981ms step_avg:59.76ms
step:737/2330 train_time:44039ms step_avg:59.75ms
step:738/2330 train_time:44100ms step_avg:59.76ms
step:739/2330 train_time:44159ms step_avg:59.75ms
step:740/2330 train_time:44219ms step_avg:59.76ms
step:741/2330 train_time:44278ms step_avg:59.75ms
step:742/2330 train_time:44338ms step_avg:59.75ms
step:743/2330 train_time:44396ms step_avg:59.75ms
step:744/2330 train_time:44457ms step_avg:59.75ms
step:745/2330 train_time:44516ms step_avg:59.75ms
step:746/2330 train_time:44576ms step_avg:59.75ms
step:747/2330 train_time:44635ms step_avg:59.75ms
step:748/2330 train_time:44696ms step_avg:59.75ms
step:749/2330 train_time:44754ms step_avg:59.75ms
step:750/2330 train_time:44815ms step_avg:59.75ms
step:750/2330 val_loss:3.6933 train_time:44878ms step_avg:59.84ms
step:751/2330 train_time:44899ms step_avg:59.79ms
step:752/2330 train_time:44937ms step_avg:59.76ms
step:753/2330 train_time:44997ms step_avg:59.76ms
step:754/2330 train_time:45060ms step_avg:59.76ms
step:755/2330 train_time:45119ms step_avg:59.76ms
step:756/2330 train_time:45181ms step_avg:59.76ms
step:757/2330 train_time:45239ms step_avg:59.76ms
step:758/2330 train_time:45298ms step_avg:59.76ms
step:759/2330 train_time:45356ms step_avg:59.76ms
step:760/2330 train_time:45416ms step_avg:59.76ms
step:761/2330 train_time:45474ms step_avg:59.76ms
step:762/2330 train_time:45534ms step_avg:59.76ms
step:763/2330 train_time:45591ms step_avg:59.75ms
step:764/2330 train_time:45651ms step_avg:59.75ms
step:765/2330 train_time:45709ms step_avg:59.75ms
step:766/2330 train_time:45771ms step_avg:59.75ms
step:767/2330 train_time:45830ms step_avg:59.75ms
step:768/2330 train_time:45892ms step_avg:59.76ms
step:769/2330 train_time:45953ms step_avg:59.76ms
step:770/2330 train_time:46015ms step_avg:59.76ms
step:771/2330 train_time:46075ms step_avg:59.76ms
step:772/2330 train_time:46136ms step_avg:59.76ms
step:773/2330 train_time:46196ms step_avg:59.76ms
step:774/2330 train_time:46257ms step_avg:59.76ms
step:775/2330 train_time:46315ms step_avg:59.76ms
step:776/2330 train_time:46376ms step_avg:59.76ms
step:777/2330 train_time:46434ms step_avg:59.76ms
step:778/2330 train_time:46495ms step_avg:59.76ms
step:779/2330 train_time:46553ms step_avg:59.76ms
step:780/2330 train_time:46614ms step_avg:59.76ms
step:781/2330 train_time:46673ms step_avg:59.76ms
step:782/2330 train_time:46735ms step_avg:59.76ms
step:783/2330 train_time:46794ms step_avg:59.76ms
step:784/2330 train_time:46857ms step_avg:59.77ms
step:785/2330 train_time:46916ms step_avg:59.77ms
step:786/2330 train_time:46978ms step_avg:59.77ms
step:787/2330 train_time:47038ms step_avg:59.77ms
step:788/2330 train_time:47099ms step_avg:59.77ms
step:789/2330 train_time:47159ms step_avg:59.77ms
step:790/2330 train_time:47220ms step_avg:59.77ms
step:791/2330 train_time:47280ms step_avg:59.77ms
step:792/2330 train_time:47340ms step_avg:59.77ms
step:793/2330 train_time:47399ms step_avg:59.77ms
step:794/2330 train_time:47460ms step_avg:59.77ms
step:795/2330 train_time:47519ms step_avg:59.77ms
step:796/2330 train_time:47580ms step_avg:59.77ms
step:797/2330 train_time:47638ms step_avg:59.77ms
step:798/2330 train_time:47700ms step_avg:59.77ms
step:799/2330 train_time:47760ms step_avg:59.77ms
step:800/2330 train_time:47821ms step_avg:59.78ms
step:801/2330 train_time:47881ms step_avg:59.78ms
step:802/2330 train_time:47942ms step_avg:59.78ms
step:803/2330 train_time:48001ms step_avg:59.78ms
step:804/2330 train_time:48063ms step_avg:59.78ms
step:805/2330 train_time:48122ms step_avg:59.78ms
step:806/2330 train_time:48183ms step_avg:59.78ms
step:807/2330 train_time:48243ms step_avg:59.78ms
step:808/2330 train_time:48304ms step_avg:59.78ms
step:809/2330 train_time:48363ms step_avg:59.78ms
step:810/2330 train_time:48425ms step_avg:59.78ms
step:811/2330 train_time:48484ms step_avg:59.78ms
step:812/2330 train_time:48545ms step_avg:59.79ms
step:813/2330 train_time:48605ms step_avg:59.79ms
step:814/2330 train_time:48667ms step_avg:59.79ms
step:815/2330 train_time:48727ms step_avg:59.79ms
step:816/2330 train_time:48789ms step_avg:59.79ms
step:817/2330 train_time:48848ms step_avg:59.79ms
step:818/2330 train_time:48910ms step_avg:59.79ms
step:819/2330 train_time:48969ms step_avg:59.79ms
step:820/2330 train_time:49030ms step_avg:59.79ms
step:821/2330 train_time:49089ms step_avg:59.79ms
step:822/2330 train_time:49151ms step_avg:59.79ms
step:823/2330 train_time:49211ms step_avg:59.79ms
step:824/2330 train_time:49272ms step_avg:59.80ms
step:825/2330 train_time:49331ms step_avg:59.80ms
step:826/2330 train_time:49393ms step_avg:59.80ms
step:827/2330 train_time:49452ms step_avg:59.80ms
step:828/2330 train_time:49513ms step_avg:59.80ms
step:829/2330 train_time:49572ms step_avg:59.80ms
step:830/2330 train_time:49633ms step_avg:59.80ms
step:831/2330 train_time:49692ms step_avg:59.80ms
step:832/2330 train_time:49753ms step_avg:59.80ms
step:833/2330 train_time:49813ms step_avg:59.80ms
step:834/2330 train_time:49874ms step_avg:59.80ms
step:835/2330 train_time:49933ms step_avg:59.80ms
step:836/2330 train_time:49994ms step_avg:59.80ms
step:837/2330 train_time:50053ms step_avg:59.80ms
step:838/2330 train_time:50115ms step_avg:59.80ms
step:839/2330 train_time:50174ms step_avg:59.80ms
step:840/2330 train_time:50235ms step_avg:59.80ms
step:841/2330 train_time:50294ms step_avg:59.80ms
step:842/2330 train_time:50355ms step_avg:59.80ms
step:843/2330 train_time:50414ms step_avg:59.80ms
step:844/2330 train_time:50475ms step_avg:59.80ms
step:845/2330 train_time:50534ms step_avg:59.80ms
step:846/2330 train_time:50595ms step_avg:59.81ms
step:847/2330 train_time:50654ms step_avg:59.80ms
step:848/2330 train_time:50715ms step_avg:59.81ms
step:849/2330 train_time:50775ms step_avg:59.81ms
step:850/2330 train_time:50836ms step_avg:59.81ms
step:851/2330 train_time:50894ms step_avg:59.81ms
step:852/2330 train_time:50956ms step_avg:59.81ms
step:853/2330 train_time:51015ms step_avg:59.81ms
step:854/2330 train_time:51077ms step_avg:59.81ms
step:855/2330 train_time:51136ms step_avg:59.81ms
step:856/2330 train_time:51198ms step_avg:59.81ms
step:857/2330 train_time:51257ms step_avg:59.81ms
step:858/2330 train_time:51318ms step_avg:59.81ms
step:859/2330 train_time:51377ms step_avg:59.81ms
step:860/2330 train_time:51438ms step_avg:59.81ms
step:861/2330 train_time:51498ms step_avg:59.81ms
step:862/2330 train_time:51559ms step_avg:59.81ms
step:863/2330 train_time:51619ms step_avg:59.81ms
step:864/2330 train_time:51680ms step_avg:59.81ms
step:865/2330 train_time:51738ms step_avg:59.81ms
step:866/2330 train_time:51800ms step_avg:59.82ms
step:867/2330 train_time:51859ms step_avg:59.81ms
step:868/2330 train_time:51921ms step_avg:59.82ms
step:869/2330 train_time:51979ms step_avg:59.82ms
step:870/2330 train_time:52041ms step_avg:59.82ms
step:871/2330 train_time:52100ms step_avg:59.82ms
step:872/2330 train_time:52162ms step_avg:59.82ms
step:873/2330 train_time:52221ms step_avg:59.82ms
step:874/2330 train_time:52282ms step_avg:59.82ms
step:875/2330 train_time:52341ms step_avg:59.82ms
step:876/2330 train_time:52402ms step_avg:59.82ms
step:877/2330 train_time:52462ms step_avg:59.82ms
step:878/2330 train_time:52522ms step_avg:59.82ms
step:879/2330 train_time:52581ms step_avg:59.82ms
step:880/2330 train_time:52642ms step_avg:59.82ms
step:881/2330 train_time:52702ms step_avg:59.82ms
step:882/2330 train_time:52763ms step_avg:59.82ms
step:883/2330 train_time:52822ms step_avg:59.82ms
step:884/2330 train_time:52884ms step_avg:59.82ms
step:885/2330 train_time:52943ms step_avg:59.82ms
step:886/2330 train_time:53004ms step_avg:59.82ms
step:887/2330 train_time:53063ms step_avg:59.82ms
step:888/2330 train_time:53125ms step_avg:59.83ms
step:889/2330 train_time:53184ms step_avg:59.82ms
step:890/2330 train_time:53245ms step_avg:59.83ms
step:891/2330 train_time:53304ms step_avg:59.82ms
step:892/2330 train_time:53366ms step_avg:59.83ms
step:893/2330 train_time:53425ms step_avg:59.83ms
step:894/2330 train_time:53486ms step_avg:59.83ms
step:895/2330 train_time:53546ms step_avg:59.83ms
step:896/2330 train_time:53608ms step_avg:59.83ms
step:897/2330 train_time:53668ms step_avg:59.83ms
step:898/2330 train_time:53730ms step_avg:59.83ms
step:899/2330 train_time:53789ms step_avg:59.83ms
step:900/2330 train_time:53851ms step_avg:59.83ms
step:901/2330 train_time:53911ms step_avg:59.83ms
step:902/2330 train_time:53972ms step_avg:59.84ms
step:903/2330 train_time:54031ms step_avg:59.84ms
step:904/2330 train_time:54092ms step_avg:59.84ms
step:905/2330 train_time:54151ms step_avg:59.84ms
step:906/2330 train_time:54212ms step_avg:59.84ms
step:907/2330 train_time:54272ms step_avg:59.84ms
step:908/2330 train_time:54333ms step_avg:59.84ms
step:909/2330 train_time:54392ms step_avg:59.84ms
step:910/2330 train_time:54453ms step_avg:59.84ms
step:911/2330 train_time:54511ms step_avg:59.84ms
step:912/2330 train_time:54573ms step_avg:59.84ms
step:913/2330 train_time:54632ms step_avg:59.84ms
step:914/2330 train_time:54693ms step_avg:59.84ms
step:915/2330 train_time:54753ms step_avg:59.84ms
step:916/2330 train_time:54814ms step_avg:59.84ms
step:917/2330 train_time:54873ms step_avg:59.84ms
step:918/2330 train_time:54934ms step_avg:59.84ms
step:919/2330 train_time:54994ms step_avg:59.84ms
step:920/2330 train_time:55055ms step_avg:59.84ms
step:921/2330 train_time:55113ms step_avg:59.84ms
step:922/2330 train_time:55174ms step_avg:59.84ms
step:923/2330 train_time:55233ms step_avg:59.84ms
step:924/2330 train_time:55294ms step_avg:59.84ms
step:925/2330 train_time:55354ms step_avg:59.84ms
step:926/2330 train_time:55414ms step_avg:59.84ms
step:927/2330 train_time:55473ms step_avg:59.84ms
step:928/2330 train_time:55535ms step_avg:59.84ms
step:929/2330 train_time:55594ms step_avg:59.84ms
step:930/2330 train_time:55656ms step_avg:59.85ms
step:931/2330 train_time:55715ms step_avg:59.84ms
step:932/2330 train_time:55776ms step_avg:59.85ms
step:933/2330 train_time:55835ms step_avg:59.84ms
step:934/2330 train_time:55896ms step_avg:59.85ms
step:935/2330 train_time:55955ms step_avg:59.85ms
step:936/2330 train_time:56017ms step_avg:59.85ms
step:937/2330 train_time:56076ms step_avg:59.85ms
step:938/2330 train_time:56137ms step_avg:59.85ms
step:939/2330 train_time:56196ms step_avg:59.85ms
step:940/2330 train_time:56257ms step_avg:59.85ms
step:941/2330 train_time:56317ms step_avg:59.85ms
step:942/2330 train_time:56377ms step_avg:59.85ms
step:943/2330 train_time:56436ms step_avg:59.85ms
step:944/2330 train_time:56498ms step_avg:59.85ms
step:945/2330 train_time:56557ms step_avg:59.85ms
step:946/2330 train_time:56618ms step_avg:59.85ms
step:947/2330 train_time:56677ms step_avg:59.85ms
step:948/2330 train_time:56738ms step_avg:59.85ms
step:949/2330 train_time:56797ms step_avg:59.85ms
step:950/2330 train_time:56858ms step_avg:59.85ms
step:951/2330 train_time:56918ms step_avg:59.85ms
step:952/2330 train_time:56979ms step_avg:59.85ms
step:953/2330 train_time:57038ms step_avg:59.85ms
step:954/2330 train_time:57099ms step_avg:59.85ms
step:955/2330 train_time:57157ms step_avg:59.85ms
step:956/2330 train_time:57218ms step_avg:59.85ms
step:957/2330 train_time:57278ms step_avg:59.85ms
step:958/2330 train_time:57339ms step_avg:59.85ms
step:959/2330 train_time:57398ms step_avg:59.85ms
step:960/2330 train_time:57459ms step_avg:59.85ms
step:961/2330 train_time:57518ms step_avg:59.85ms
step:962/2330 train_time:57579ms step_avg:59.85ms
step:963/2330 train_time:57638ms step_avg:59.85ms
step:964/2330 train_time:57700ms step_avg:59.85ms
step:965/2330 train_time:57759ms step_avg:59.85ms
step:966/2330 train_time:57820ms step_avg:59.86ms
step:967/2330 train_time:57879ms step_avg:59.85ms
step:968/2330 train_time:57940ms step_avg:59.86ms
step:969/2330 train_time:57999ms step_avg:59.85ms
step:970/2330 train_time:58061ms step_avg:59.86ms
step:971/2330 train_time:58120ms step_avg:59.86ms
step:972/2330 train_time:58181ms step_avg:59.86ms
step:973/2330 train_time:58239ms step_avg:59.86ms
step:974/2330 train_time:58301ms step_avg:59.86ms
step:975/2330 train_time:58361ms step_avg:59.86ms
step:976/2330 train_time:58422ms step_avg:59.86ms
step:977/2330 train_time:58481ms step_avg:59.86ms
step:978/2330 train_time:58541ms step_avg:59.86ms
step:979/2330 train_time:58601ms step_avg:59.86ms
step:980/2330 train_time:58662ms step_avg:59.86ms
step:981/2330 train_time:58721ms step_avg:59.86ms
step:982/2330 train_time:58782ms step_avg:59.86ms
step:983/2330 train_time:58841ms step_avg:59.86ms
step:984/2330 train_time:58903ms step_avg:59.86ms
step:985/2330 train_time:58962ms step_avg:59.86ms
step:986/2330 train_time:59023ms step_avg:59.86ms
step:987/2330 train_time:59083ms step_avg:59.86ms
step:988/2330 train_time:59144ms step_avg:59.86ms
step:989/2330 train_time:59203ms step_avg:59.86ms
step:990/2330 train_time:59264ms step_avg:59.86ms
step:991/2330 train_time:59324ms step_avg:59.86ms
step:992/2330 train_time:59385ms step_avg:59.86ms
step:993/2330 train_time:59444ms step_avg:59.86ms
step:994/2330 train_time:59506ms step_avg:59.87ms
step:995/2330 train_time:59566ms step_avg:59.86ms
step:996/2330 train_time:59627ms step_avg:59.87ms
step:997/2330 train_time:59687ms step_avg:59.87ms
step:998/2330 train_time:59748ms step_avg:59.87ms
step:999/2330 train_time:59808ms step_avg:59.87ms
step:1000/2330 train_time:59869ms step_avg:59.87ms
step:1000/2330 val_loss:3.5801 train_time:59933ms step_avg:59.93ms
step:1001/2330 train_time:59954ms step_avg:59.89ms
step:1002/2330 train_time:59994ms step_avg:59.87ms
step:1003/2330 train_time:60053ms step_avg:59.87ms
step:1004/2330 train_time:60114ms step_avg:59.87ms
step:1005/2330 train_time:60175ms step_avg:59.88ms
step:1006/2330 train_time:60236ms step_avg:59.88ms
step:1007/2330 train_time:60295ms step_avg:59.88ms
step:1008/2330 train_time:60355ms step_avg:59.88ms
step:1009/2330 train_time:60414ms step_avg:59.87ms
step:1010/2330 train_time:60474ms step_avg:59.88ms
step:1011/2330 train_time:60533ms step_avg:59.87ms
step:1012/2330 train_time:60594ms step_avg:59.88ms
step:1013/2330 train_time:60652ms step_avg:59.87ms
step:1014/2330 train_time:60713ms step_avg:59.87ms
step:1015/2330 train_time:60772ms step_avg:59.87ms
step:1016/2330 train_time:60835ms step_avg:59.88ms
step:1017/2330 train_time:60900ms step_avg:59.88ms
step:1018/2330 train_time:60964ms step_avg:59.89ms
step:1019/2330 train_time:61024ms step_avg:59.89ms
step:1020/2330 train_time:61085ms step_avg:59.89ms
step:1021/2330 train_time:61144ms step_avg:59.89ms
step:1022/2330 train_time:61205ms step_avg:59.89ms
step:1023/2330 train_time:61263ms step_avg:59.89ms
step:1024/2330 train_time:61324ms step_avg:59.89ms
step:1025/2330 train_time:61383ms step_avg:59.89ms
step:1026/2330 train_time:61444ms step_avg:59.89ms
step:1027/2330 train_time:61503ms step_avg:59.89ms
step:1028/2330 train_time:61564ms step_avg:59.89ms
step:1029/2330 train_time:61622ms step_avg:59.89ms
step:1030/2330 train_time:61684ms step_avg:59.89ms
step:1031/2330 train_time:61742ms step_avg:59.89ms
step:1032/2330 train_time:61804ms step_avg:59.89ms
step:1033/2330 train_time:61864ms step_avg:59.89ms
step:1034/2330 train_time:61925ms step_avg:59.89ms
step:1035/2330 train_time:61986ms step_avg:59.89ms
step:1036/2330 train_time:62047ms step_avg:59.89ms
step:1037/2330 train_time:62107ms step_avg:59.89ms
step:1038/2330 train_time:62168ms step_avg:59.89ms
step:1039/2330 train_time:62228ms step_avg:59.89ms
step:1040/2330 train_time:62289ms step_avg:59.89ms
step:1041/2330 train_time:62349ms step_avg:59.89ms
step:1042/2330 train_time:62410ms step_avg:59.89ms
step:1043/2330 train_time:62469ms step_avg:59.89ms
step:1044/2330 train_time:62531ms step_avg:59.90ms
step:1045/2330 train_time:62590ms step_avg:59.89ms
step:1046/2330 train_time:62651ms step_avg:59.90ms
step:1047/2330 train_time:62711ms step_avg:59.90ms
step:1048/2330 train_time:62773ms step_avg:59.90ms
step:1049/2330 train_time:62833ms step_avg:59.90ms
step:1050/2330 train_time:62895ms step_avg:59.90ms
step:1051/2330 train_time:62954ms step_avg:59.90ms
step:1052/2330 train_time:63017ms step_avg:59.90ms
step:1053/2330 train_time:63076ms step_avg:59.90ms
step:1054/2330 train_time:63138ms step_avg:59.90ms
step:1055/2330 train_time:63197ms step_avg:59.90ms
step:1056/2330 train_time:63257ms step_avg:59.90ms
step:1057/2330 train_time:63316ms step_avg:59.90ms
step:1058/2330 train_time:63377ms step_avg:59.90ms
step:1059/2330 train_time:63436ms step_avg:59.90ms
step:1060/2330 train_time:63497ms step_avg:59.90ms
step:1061/2330 train_time:63557ms step_avg:59.90ms
step:1062/2330 train_time:63618ms step_avg:59.90ms
step:1063/2330 train_time:63677ms step_avg:59.90ms
step:1064/2330 train_time:63739ms step_avg:59.90ms
step:1065/2330 train_time:63799ms step_avg:59.90ms
step:1066/2330 train_time:63860ms step_avg:59.91ms
step:1067/2330 train_time:63920ms step_avg:59.91ms
step:1068/2330 train_time:63982ms step_avg:59.91ms
step:1069/2330 train_time:64041ms step_avg:59.91ms
step:1070/2330 train_time:64102ms step_avg:59.91ms
step:1071/2330 train_time:64162ms step_avg:59.91ms
step:1072/2330 train_time:64222ms step_avg:59.91ms
step:1073/2330 train_time:64282ms step_avg:59.91ms
step:1074/2330 train_time:64342ms step_avg:59.91ms
step:1075/2330 train_time:64401ms step_avg:59.91ms
step:1076/2330 train_time:64461ms step_avg:59.91ms
step:1077/2330 train_time:64520ms step_avg:59.91ms
step:1078/2330 train_time:64582ms step_avg:59.91ms
step:1079/2330 train_time:64641ms step_avg:59.91ms
step:1080/2330 train_time:64702ms step_avg:59.91ms
step:1081/2330 train_time:64763ms step_avg:59.91ms
step:1082/2330 train_time:64825ms step_avg:59.91ms
step:1083/2330 train_time:64884ms step_avg:59.91ms
step:1084/2330 train_time:64945ms step_avg:59.91ms
step:1085/2330 train_time:65004ms step_avg:59.91ms
step:1086/2330 train_time:65065ms step_avg:59.91ms
step:1087/2330 train_time:65125ms step_avg:59.91ms
step:1088/2330 train_time:65186ms step_avg:59.91ms
step:1089/2330 train_time:65245ms step_avg:59.91ms
step:1090/2330 train_time:65306ms step_avg:59.91ms
step:1091/2330 train_time:65365ms step_avg:59.91ms
step:1092/2330 train_time:65426ms step_avg:59.91ms
step:1093/2330 train_time:65485ms step_avg:59.91ms
step:1094/2330 train_time:65546ms step_avg:59.91ms
step:1095/2330 train_time:65606ms step_avg:59.91ms
step:1096/2330 train_time:65667ms step_avg:59.92ms
step:1097/2330 train_time:65727ms step_avg:59.92ms
step:1098/2330 train_time:65789ms step_avg:59.92ms
step:1099/2330 train_time:65848ms step_avg:59.92ms
step:1100/2330 train_time:65910ms step_avg:59.92ms
step:1101/2330 train_time:65970ms step_avg:59.92ms
step:1102/2330 train_time:66032ms step_avg:59.92ms
step:1103/2330 train_time:66091ms step_avg:59.92ms
step:1104/2330 train_time:66153ms step_avg:59.92ms
step:1105/2330 train_time:66213ms step_avg:59.92ms
step:1106/2330 train_time:66274ms step_avg:59.92ms
step:1107/2330 train_time:66334ms step_avg:59.92ms
step:1108/2330 train_time:66395ms step_avg:59.92ms
step:1109/2330 train_time:66455ms step_avg:59.92ms
step:1110/2330 train_time:66517ms step_avg:59.92ms
step:1111/2330 train_time:66575ms step_avg:59.92ms
step:1112/2330 train_time:66637ms step_avg:59.93ms
step:1113/2330 train_time:66696ms step_avg:59.92ms
step:1114/2330 train_time:66757ms step_avg:59.93ms
step:1115/2330 train_time:66816ms step_avg:59.92ms
step:1116/2330 train_time:66877ms step_avg:59.93ms
step:1117/2330 train_time:66936ms step_avg:59.93ms
step:1118/2330 train_time:66998ms step_avg:59.93ms
step:1119/2330 train_time:67057ms step_avg:59.93ms
step:1120/2330 train_time:67119ms step_avg:59.93ms
step:1121/2330 train_time:67178ms step_avg:59.93ms
step:1122/2330 train_time:67240ms step_avg:59.93ms
step:1123/2330 train_time:67299ms step_avg:59.93ms
step:1124/2330 train_time:67360ms step_avg:59.93ms
step:1125/2330 train_time:67419ms step_avg:59.93ms
step:1126/2330 train_time:67481ms step_avg:59.93ms
step:1127/2330 train_time:67540ms step_avg:59.93ms
step:1128/2330 train_time:67601ms step_avg:59.93ms
step:1129/2330 train_time:67661ms step_avg:59.93ms
step:1130/2330 train_time:67722ms step_avg:59.93ms
step:1131/2330 train_time:67781ms step_avg:59.93ms
step:1132/2330 train_time:67842ms step_avg:59.93ms
step:1133/2330 train_time:67901ms step_avg:59.93ms
step:1134/2330 train_time:67962ms step_avg:59.93ms
step:1135/2330 train_time:68022ms step_avg:59.93ms
step:1136/2330 train_time:68083ms step_avg:59.93ms
step:1137/2330 train_time:68142ms step_avg:59.93ms
step:1138/2330 train_time:68203ms step_avg:59.93ms
step:1139/2330 train_time:68263ms step_avg:59.93ms
step:1140/2330 train_time:68325ms step_avg:59.93ms
step:1141/2330 train_time:68384ms step_avg:59.93ms
step:1142/2330 train_time:68445ms step_avg:59.93ms
step:1143/2330 train_time:68504ms step_avg:59.93ms
step:1144/2330 train_time:68565ms step_avg:59.93ms
step:1145/2330 train_time:68624ms step_avg:59.93ms
step:1146/2330 train_time:68686ms step_avg:59.94ms
step:1147/2330 train_time:68745ms step_avg:59.93ms
step:1148/2330 train_time:68805ms step_avg:59.93ms
step:1149/2330 train_time:68864ms step_avg:59.93ms
step:1150/2330 train_time:68925ms step_avg:59.94ms
step:1151/2330 train_time:68985ms step_avg:59.93ms
step:1152/2330 train_time:69046ms step_avg:59.94ms
step:1153/2330 train_time:69105ms step_avg:59.94ms
step:1154/2330 train_time:69167ms step_avg:59.94ms
step:1155/2330 train_time:69226ms step_avg:59.94ms
step:1156/2330 train_time:69287ms step_avg:59.94ms
step:1157/2330 train_time:69346ms step_avg:59.94ms
step:1158/2330 train_time:69407ms step_avg:59.94ms
step:1159/2330 train_time:69467ms step_avg:59.94ms
step:1160/2330 train_time:69528ms step_avg:59.94ms
step:1161/2330 train_time:69587ms step_avg:59.94ms
step:1162/2330 train_time:69649ms step_avg:59.94ms
step:1163/2330 train_time:69708ms step_avg:59.94ms
step:1164/2330 train_time:69769ms step_avg:59.94ms
step:1165/2330 train_time:69829ms step_avg:59.94ms
step:1166/2330 train_time:69891ms step_avg:59.94ms
step:1167/2330 train_time:69951ms step_avg:59.94ms
step:1168/2330 train_time:70013ms step_avg:59.94ms
step:1169/2330 train_time:70072ms step_avg:59.94ms
step:1170/2330 train_time:70134ms step_avg:59.94ms
step:1171/2330 train_time:70194ms step_avg:59.94ms
step:1172/2330 train_time:70255ms step_avg:59.94ms
step:1173/2330 train_time:70314ms step_avg:59.94ms
step:1174/2330 train_time:70376ms step_avg:59.95ms
step:1175/2330 train_time:70435ms step_avg:59.94ms
step:1176/2330 train_time:70497ms step_avg:59.95ms
step:1177/2330 train_time:70556ms step_avg:59.95ms
step:1178/2330 train_time:70617ms step_avg:59.95ms
step:1179/2330 train_time:70676ms step_avg:59.95ms
step:1180/2330 train_time:70737ms step_avg:59.95ms
step:1181/2330 train_time:70796ms step_avg:59.95ms
step:1182/2330 train_time:70857ms step_avg:59.95ms
step:1183/2330 train_time:70917ms step_avg:59.95ms
step:1184/2330 train_time:70979ms step_avg:59.95ms
step:1185/2330 train_time:71038ms step_avg:59.95ms
step:1186/2330 train_time:71100ms step_avg:59.95ms
step:1187/2330 train_time:71159ms step_avg:59.95ms
step:1188/2330 train_time:71220ms step_avg:59.95ms
step:1189/2330 train_time:71279ms step_avg:59.95ms
step:1190/2330 train_time:71340ms step_avg:59.95ms
step:1191/2330 train_time:71399ms step_avg:59.95ms
step:1192/2330 train_time:71460ms step_avg:59.95ms
step:1193/2330 train_time:71519ms step_avg:59.95ms
step:1194/2330 train_time:71580ms step_avg:59.95ms
step:1195/2330 train_time:71639ms step_avg:59.95ms
step:1196/2330 train_time:71700ms step_avg:59.95ms
step:1197/2330 train_time:71759ms step_avg:59.95ms
step:1198/2330 train_time:71820ms step_avg:59.95ms
step:1199/2330 train_time:71879ms step_avg:59.95ms
step:1200/2330 train_time:71940ms step_avg:59.95ms
step:1201/2330 train_time:72000ms step_avg:59.95ms
step:1202/2330 train_time:72061ms step_avg:59.95ms
step:1203/2330 train_time:72120ms step_avg:59.95ms
step:1204/2330 train_time:72182ms step_avg:59.95ms
step:1205/2330 train_time:72242ms step_avg:59.95ms
step:1206/2330 train_time:72303ms step_avg:59.95ms
step:1207/2330 train_time:72363ms step_avg:59.95ms
step:1208/2330 train_time:72423ms step_avg:59.95ms
step:1209/2330 train_time:72482ms step_avg:59.95ms
step:1210/2330 train_time:72544ms step_avg:59.95ms
step:1211/2330 train_time:72603ms step_avg:59.95ms
step:1212/2330 train_time:72664ms step_avg:59.95ms
step:1213/2330 train_time:72723ms step_avg:59.95ms
step:1214/2330 train_time:72784ms step_avg:59.95ms
step:1215/2330 train_time:72842ms step_avg:59.95ms
step:1216/2330 train_time:72904ms step_avg:59.95ms
step:1217/2330 train_time:72963ms step_avg:59.95ms
step:1218/2330 train_time:73024ms step_avg:59.95ms
step:1219/2330 train_time:73084ms step_avg:59.95ms
step:1220/2330 train_time:73145ms step_avg:59.95ms
step:1221/2330 train_time:73204ms step_avg:59.95ms
step:1222/2330 train_time:73265ms step_avg:59.95ms
step:1223/2330 train_time:73324ms step_avg:59.95ms
step:1224/2330 train_time:73385ms step_avg:59.96ms
step:1225/2330 train_time:73444ms step_avg:59.95ms
step:1226/2330 train_time:73505ms step_avg:59.96ms
step:1227/2330 train_time:73564ms step_avg:59.95ms
step:1228/2330 train_time:73625ms step_avg:59.96ms
step:1229/2330 train_time:73684ms step_avg:59.95ms
step:1230/2330 train_time:73745ms step_avg:59.96ms
step:1231/2330 train_time:73805ms step_avg:59.96ms
step:1232/2330 train_time:73866ms step_avg:59.96ms
step:1233/2330 train_time:73925ms step_avg:59.96ms
step:1234/2330 train_time:73987ms step_avg:59.96ms
step:1235/2330 train_time:74046ms step_avg:59.96ms
step:1236/2330 train_time:74107ms step_avg:59.96ms
step:1237/2330 train_time:74166ms step_avg:59.96ms
step:1238/2330 train_time:74228ms step_avg:59.96ms
step:1239/2330 train_time:74287ms step_avg:59.96ms
step:1240/2330 train_time:74349ms step_avg:59.96ms
step:1241/2330 train_time:74408ms step_avg:59.96ms
step:1242/2330 train_time:74470ms step_avg:59.96ms
step:1243/2330 train_time:74529ms step_avg:59.96ms
step:1244/2330 train_time:74591ms step_avg:59.96ms
step:1245/2330 train_time:74650ms step_avg:59.96ms
step:1246/2330 train_time:74713ms step_avg:59.96ms
step:1247/2330 train_time:74773ms step_avg:59.96ms
step:1248/2330 train_time:74834ms step_avg:59.96ms
step:1249/2330 train_time:74894ms step_avg:59.96ms
step:1250/2330 train_time:74955ms step_avg:59.96ms
step:1250/2330 val_loss:3.5200 train_time:75018ms step_avg:60.01ms
step:1251/2330 train_time:75039ms step_avg:59.98ms
step:1252/2330 train_time:75078ms step_avg:59.97ms
step:1253/2330 train_time:75141ms step_avg:59.97ms
step:1254/2330 train_time:75206ms step_avg:59.97ms
step:1255/2330 train_time:75266ms step_avg:59.97ms
step:1256/2330 train_time:75328ms step_avg:59.97ms
step:1257/2330 train_time:75388ms step_avg:59.97ms
step:1258/2330 train_time:75449ms step_avg:59.98ms
step:1259/2330 train_time:75507ms step_avg:59.97ms
step:1260/2330 train_time:75568ms step_avg:59.97ms
step:1261/2330 train_time:75627ms step_avg:59.97ms
step:1262/2330 train_time:75688ms step_avg:59.97ms
step:1263/2330 train_time:75746ms step_avg:59.97ms
step:1264/2330 train_time:75808ms step_avg:59.97ms
step:1265/2330 train_time:75866ms step_avg:59.97ms
step:1266/2330 train_time:75928ms step_avg:59.97ms
step:1267/2330 train_time:75988ms step_avg:59.97ms
step:1268/2330 train_time:76051ms step_avg:59.98ms
step:1269/2330 train_time:76112ms step_avg:59.98ms
step:1270/2330 train_time:76174ms step_avg:59.98ms
step:1271/2330 train_time:76234ms step_avg:59.98ms
step:1272/2330 train_time:76295ms step_avg:59.98ms
step:1273/2330 train_time:76355ms step_avg:59.98ms
step:1274/2330 train_time:76416ms step_avg:59.98ms
step:1275/2330 train_time:76475ms step_avg:59.98ms
step:1276/2330 train_time:76537ms step_avg:59.98ms
step:1277/2330 train_time:76596ms step_avg:59.98ms
step:1278/2330 train_time:76657ms step_avg:59.98ms
step:1279/2330 train_time:76716ms step_avg:59.98ms
step:1280/2330 train_time:76777ms step_avg:59.98ms
step:1281/2330 train_time:76836ms step_avg:59.98ms
step:1282/2330 train_time:76897ms step_avg:59.98ms
step:1283/2330 train_time:76956ms step_avg:59.98ms
step:1284/2330 train_time:77017ms step_avg:59.98ms
step:1285/2330 train_time:77076ms step_avg:59.98ms
step:1286/2330 train_time:77137ms step_avg:59.98ms
step:1287/2330 train_time:77197ms step_avg:59.98ms
step:1288/2330 train_time:77259ms step_avg:59.98ms
step:1289/2330 train_time:77318ms step_avg:59.98ms
step:1290/2330 train_time:77379ms step_avg:59.98ms
step:1291/2330 train_time:77438ms step_avg:59.98ms
step:1292/2330 train_time:77499ms step_avg:59.98ms
step:1293/2330 train_time:77559ms step_avg:59.98ms
step:1294/2330 train_time:77620ms step_avg:59.98ms
step:1295/2330 train_time:77679ms step_avg:59.98ms
step:1296/2330 train_time:77740ms step_avg:59.98ms
step:1297/2330 train_time:77799ms step_avg:59.98ms
step:1298/2330 train_time:77859ms step_avg:59.98ms
step:1299/2330 train_time:77918ms step_avg:59.98ms
step:1300/2330 train_time:77979ms step_avg:59.98ms
step:1301/2330 train_time:78039ms step_avg:59.98ms
step:1302/2330 train_time:78100ms step_avg:59.98ms
step:1303/2330 train_time:78159ms step_avg:59.98ms
step:1304/2330 train_time:78221ms step_avg:59.99ms
step:1305/2330 train_time:78279ms step_avg:59.98ms
step:1306/2330 train_time:78341ms step_avg:59.99ms
step:1307/2330 train_time:78400ms step_avg:59.99ms
step:1308/2330 train_time:78462ms step_avg:59.99ms
step:1309/2330 train_time:78521ms step_avg:59.99ms
step:1310/2330 train_time:78583ms step_avg:59.99ms
step:1311/2330 train_time:78643ms step_avg:59.99ms
step:1312/2330 train_time:78704ms step_avg:59.99ms
step:1313/2330 train_time:78763ms step_avg:59.99ms
step:1314/2330 train_time:78824ms step_avg:59.99ms
step:1315/2330 train_time:78884ms step_avg:59.99ms
step:1316/2330 train_time:78946ms step_avg:59.99ms
step:1317/2330 train_time:79005ms step_avg:59.99ms
step:1318/2330 train_time:79067ms step_avg:59.99ms
step:1319/2330 train_time:79127ms step_avg:59.99ms
step:1320/2330 train_time:79188ms step_avg:59.99ms
step:1321/2330 train_time:79248ms step_avg:59.99ms
step:1322/2330 train_time:79310ms step_avg:59.99ms
step:1323/2330 train_time:79369ms step_avg:59.99ms
step:1324/2330 train_time:79430ms step_avg:59.99ms
step:1325/2330 train_time:79490ms step_avg:59.99ms
step:1326/2330 train_time:79551ms step_avg:59.99ms
step:1327/2330 train_time:79610ms step_avg:59.99ms
step:1328/2330 train_time:79671ms step_avg:59.99ms
step:1329/2330 train_time:79730ms step_avg:59.99ms
step:1330/2330 train_time:79791ms step_avg:59.99ms
step:1331/2330 train_time:79850ms step_avg:59.99ms
step:1332/2330 train_time:79911ms step_avg:59.99ms
step:1333/2330 train_time:79971ms step_avg:59.99ms
step:1334/2330 train_time:80032ms step_avg:59.99ms
step:1335/2330 train_time:80092ms step_avg:59.99ms
step:1336/2330 train_time:80153ms step_avg:59.99ms
step:1337/2330 train_time:80212ms step_avg:59.99ms
step:1338/2330 train_time:80273ms step_avg:60.00ms
step:1339/2330 train_time:80332ms step_avg:59.99ms
step:1340/2330 train_time:80394ms step_avg:60.00ms
step:1341/2330 train_time:80453ms step_avg:59.99ms
step:1342/2330 train_time:80514ms step_avg:60.00ms
step:1343/2330 train_time:80573ms step_avg:59.99ms
step:1344/2330 train_time:80634ms step_avg:60.00ms
step:1345/2330 train_time:80693ms step_avg:59.99ms
step:1346/2330 train_time:80754ms step_avg:60.00ms
step:1347/2330 train_time:80813ms step_avg:60.00ms
step:1348/2330 train_time:80875ms step_avg:60.00ms
step:1349/2330 train_time:80934ms step_avg:60.00ms
step:1350/2330 train_time:80996ms step_avg:60.00ms
step:1351/2330 train_time:81055ms step_avg:60.00ms
step:1352/2330 train_time:81117ms step_avg:60.00ms
step:1353/2330 train_time:81176ms step_avg:60.00ms
step:1354/2330 train_time:81237ms step_avg:60.00ms
step:1355/2330 train_time:81297ms step_avg:60.00ms
step:1356/2330 train_time:81358ms step_avg:60.00ms
step:1357/2330 train_time:81417ms step_avg:60.00ms
step:1358/2330 train_time:81479ms step_avg:60.00ms
step:1359/2330 train_time:81538ms step_avg:60.00ms
step:1360/2330 train_time:81599ms step_avg:60.00ms
step:1361/2330 train_time:81657ms step_avg:60.00ms
step:1362/2330 train_time:81719ms step_avg:60.00ms
step:1363/2330 train_time:81778ms step_avg:60.00ms
step:1364/2330 train_time:81838ms step_avg:60.00ms
step:1365/2330 train_time:81897ms step_avg:60.00ms
step:1366/2330 train_time:81958ms step_avg:60.00ms
step:1367/2330 train_time:82018ms step_avg:60.00ms
step:1368/2330 train_time:82079ms step_avg:60.00ms
step:1369/2330 train_time:82139ms step_avg:60.00ms
step:1370/2330 train_time:82200ms step_avg:60.00ms
step:1371/2330 train_time:82259ms step_avg:60.00ms
step:1372/2330 train_time:82320ms step_avg:60.00ms
step:1373/2330 train_time:82379ms step_avg:60.00ms
step:1374/2330 train_time:82441ms step_avg:60.00ms
step:1375/2330 train_time:82500ms step_avg:60.00ms
step:1376/2330 train_time:82561ms step_avg:60.00ms
step:1377/2330 train_time:82621ms step_avg:60.00ms
step:1378/2330 train_time:82682ms step_avg:60.00ms
step:1379/2330 train_time:82741ms step_avg:60.00ms
step:1380/2330 train_time:82803ms step_avg:60.00ms
step:1381/2330 train_time:82862ms step_avg:60.00ms
step:1382/2330 train_time:82924ms step_avg:60.00ms
step:1383/2330 train_time:82983ms step_avg:60.00ms
step:1384/2330 train_time:83045ms step_avg:60.00ms
step:1385/2330 train_time:83105ms step_avg:60.00ms
step:1386/2330 train_time:83166ms step_avg:60.00ms
step:1387/2330 train_time:83226ms step_avg:60.00ms
step:1388/2330 train_time:83287ms step_avg:60.01ms
step:1389/2330 train_time:83347ms step_avg:60.01ms
step:1390/2330 train_time:83409ms step_avg:60.01ms
step:1391/2330 train_time:83468ms step_avg:60.01ms
step:1392/2330 train_time:83530ms step_avg:60.01ms
step:1393/2330 train_time:83589ms step_avg:60.01ms
step:1394/2330 train_time:83650ms step_avg:60.01ms
step:1395/2330 train_time:83709ms step_avg:60.01ms
step:1396/2330 train_time:83771ms step_avg:60.01ms
step:1397/2330 train_time:83830ms step_avg:60.01ms
step:1398/2330 train_time:83891ms step_avg:60.01ms
step:1399/2330 train_time:83950ms step_avg:60.01ms
step:1400/2330 train_time:84011ms step_avg:60.01ms
step:1401/2330 train_time:84071ms step_avg:60.01ms
step:1402/2330 train_time:84132ms step_avg:60.01ms
step:1403/2330 train_time:84191ms step_avg:60.01ms
step:1404/2330 train_time:84253ms step_avg:60.01ms
step:1405/2330 train_time:84311ms step_avg:60.01ms
step:1406/2330 train_time:84373ms step_avg:60.01ms
step:1407/2330 train_time:84432ms step_avg:60.01ms
step:1408/2330 train_time:84494ms step_avg:60.01ms
step:1409/2330 train_time:84553ms step_avg:60.01ms
step:1410/2330 train_time:84614ms step_avg:60.01ms
step:1411/2330 train_time:84673ms step_avg:60.01ms
step:1412/2330 train_time:84735ms step_avg:60.01ms
step:1413/2330 train_time:84794ms step_avg:60.01ms
step:1414/2330 train_time:84856ms step_avg:60.01ms
step:1415/2330 train_time:84915ms step_avg:60.01ms
step:1416/2330 train_time:84976ms step_avg:60.01ms
step:1417/2330 train_time:85035ms step_avg:60.01ms
step:1418/2330 train_time:85096ms step_avg:60.01ms
step:1419/2330 train_time:85156ms step_avg:60.01ms
step:1420/2330 train_time:85218ms step_avg:60.01ms
step:1421/2330 train_time:85277ms step_avg:60.01ms
step:1422/2330 train_time:85339ms step_avg:60.01ms
step:1423/2330 train_time:85398ms step_avg:60.01ms
step:1424/2330 train_time:85459ms step_avg:60.01ms
step:1425/2330 train_time:85519ms step_avg:60.01ms
step:1426/2330 train_time:85580ms step_avg:60.01ms
step:1427/2330 train_time:85638ms step_avg:60.01ms
step:1428/2330 train_time:85700ms step_avg:60.01ms
step:1429/2330 train_time:85759ms step_avg:60.01ms
step:1430/2330 train_time:85821ms step_avg:60.01ms
step:1431/2330 train_time:85880ms step_avg:60.01ms
step:1432/2330 train_time:85940ms step_avg:60.01ms
step:1433/2330 train_time:86000ms step_avg:60.01ms
step:1434/2330 train_time:86061ms step_avg:60.01ms
step:1435/2330 train_time:86120ms step_avg:60.01ms
step:1436/2330 train_time:86182ms step_avg:60.02ms
step:1437/2330 train_time:86241ms step_avg:60.01ms
step:1438/2330 train_time:86303ms step_avg:60.02ms
step:1439/2330 train_time:86362ms step_avg:60.02ms
step:1440/2330 train_time:86424ms step_avg:60.02ms
step:1441/2330 train_time:86484ms step_avg:60.02ms
step:1442/2330 train_time:86545ms step_avg:60.02ms
step:1443/2330 train_time:86605ms step_avg:60.02ms
step:1444/2330 train_time:86666ms step_avg:60.02ms
step:1445/2330 train_time:86726ms step_avg:60.02ms
step:1446/2330 train_time:86788ms step_avg:60.02ms
step:1447/2330 train_time:86847ms step_avg:60.02ms
step:1448/2330 train_time:86909ms step_avg:60.02ms
step:1449/2330 train_time:86968ms step_avg:60.02ms
step:1450/2330 train_time:87029ms step_avg:60.02ms
step:1451/2330 train_time:87089ms step_avg:60.02ms
step:1452/2330 train_time:87150ms step_avg:60.02ms
step:1453/2330 train_time:87210ms step_avg:60.02ms
step:1454/2330 train_time:87272ms step_avg:60.02ms
step:1455/2330 train_time:87331ms step_avg:60.02ms
step:1456/2330 train_time:87392ms step_avg:60.02ms
step:1457/2330 train_time:87451ms step_avg:60.02ms
step:1458/2330 train_time:87513ms step_avg:60.02ms
step:1459/2330 train_time:87572ms step_avg:60.02ms
step:1460/2330 train_time:87633ms step_avg:60.02ms
step:1461/2330 train_time:87693ms step_avg:60.02ms
step:1462/2330 train_time:87754ms step_avg:60.02ms
step:1463/2330 train_time:87814ms step_avg:60.02ms
step:1464/2330 train_time:87875ms step_avg:60.02ms
step:1465/2330 train_time:87934ms step_avg:60.02ms
step:1466/2330 train_time:87996ms step_avg:60.02ms
step:1467/2330 train_time:88055ms step_avg:60.02ms
step:1468/2330 train_time:88116ms step_avg:60.02ms
step:1469/2330 train_time:88175ms step_avg:60.02ms
step:1470/2330 train_time:88236ms step_avg:60.02ms
step:1471/2330 train_time:88295ms step_avg:60.02ms
step:1472/2330 train_time:88356ms step_avg:60.02ms
step:1473/2330 train_time:88415ms step_avg:60.02ms
step:1474/2330 train_time:88476ms step_avg:60.02ms
step:1475/2330 train_time:88536ms step_avg:60.02ms
step:1476/2330 train_time:88597ms step_avg:60.02ms
step:1477/2330 train_time:88656ms step_avg:60.02ms
step:1478/2330 train_time:88717ms step_avg:60.03ms
step:1479/2330 train_time:88777ms step_avg:60.02ms
step:1480/2330 train_time:88839ms step_avg:60.03ms
step:1481/2330 train_time:88898ms step_avg:60.03ms
step:1482/2330 train_time:88959ms step_avg:60.03ms
step:1483/2330 train_time:89019ms step_avg:60.03ms
step:1484/2330 train_time:89080ms step_avg:60.03ms
step:1485/2330 train_time:89138ms step_avg:60.03ms
step:1486/2330 train_time:89199ms step_avg:60.03ms
step:1487/2330 train_time:89258ms step_avg:60.03ms
step:1488/2330 train_time:89320ms step_avg:60.03ms
step:1489/2330 train_time:89378ms step_avg:60.03ms
step:1490/2330 train_time:89439ms step_avg:60.03ms
step:1491/2330 train_time:89499ms step_avg:60.03ms
step:1492/2330 train_time:89560ms step_avg:60.03ms
step:1493/2330 train_time:89619ms step_avg:60.03ms
step:1494/2330 train_time:89681ms step_avg:60.03ms
step:1495/2330 train_time:89741ms step_avg:60.03ms
step:1496/2330 train_time:89802ms step_avg:60.03ms
step:1497/2330 train_time:89861ms step_avg:60.03ms
step:1498/2330 train_time:89923ms step_avg:60.03ms
step:1499/2330 train_time:89982ms step_avg:60.03ms
step:1500/2330 train_time:90044ms step_avg:60.03ms
step:1500/2330 val_loss:3.4526 train_time:90108ms step_avg:60.07ms
step:1501/2330 train_time:90128ms step_avg:60.05ms
step:1502/2330 train_time:90170ms step_avg:60.03ms
step:1503/2330 train_time:90231ms step_avg:60.03ms
step:1504/2330 train_time:90297ms step_avg:60.04ms
step:1505/2330 train_time:90358ms step_avg:60.04ms
step:1506/2330 train_time:90419ms step_avg:60.04ms
step:1507/2330 train_time:90478ms step_avg:60.04ms
step:1508/2330 train_time:90539ms step_avg:60.04ms
step:1509/2330 train_time:90597ms step_avg:60.04ms
step:1510/2330 train_time:90658ms step_avg:60.04ms
step:1511/2330 train_time:90717ms step_avg:60.04ms
step:1512/2330 train_time:90777ms step_avg:60.04ms
step:1513/2330 train_time:90836ms step_avg:60.04ms
step:1514/2330 train_time:90896ms step_avg:60.04ms
step:1515/2330 train_time:90955ms step_avg:60.04ms
step:1516/2330 train_time:91016ms step_avg:60.04ms
step:1517/2330 train_time:91076ms step_avg:60.04ms
step:1518/2330 train_time:91138ms step_avg:60.04ms
step:1519/2330 train_time:91198ms step_avg:60.04ms
step:1520/2330 train_time:91261ms step_avg:60.04ms
step:1521/2330 train_time:91321ms step_avg:60.04ms
step:1522/2330 train_time:91383ms step_avg:60.04ms
step:1523/2330 train_time:91443ms step_avg:60.04ms
step:1524/2330 train_time:91505ms step_avg:60.04ms
step:1525/2330 train_time:91564ms step_avg:60.04ms
step:1526/2330 train_time:91625ms step_avg:60.04ms
step:1527/2330 train_time:91684ms step_avg:60.04ms
step:1528/2330 train_time:91746ms step_avg:60.04ms
step:1529/2330 train_time:91805ms step_avg:60.04ms
step:1530/2330 train_time:91867ms step_avg:60.04ms
step:1531/2330 train_time:91926ms step_avg:60.04ms
step:1532/2330 train_time:91988ms step_avg:60.04ms
step:1533/2330 train_time:92049ms step_avg:60.04ms
step:1534/2330 train_time:92111ms step_avg:60.05ms
step:1535/2330 train_time:92171ms step_avg:60.05ms
step:1536/2330 train_time:92232ms step_avg:60.05ms
step:1537/2330 train_time:92293ms step_avg:60.05ms
step:1538/2330 train_time:92355ms step_avg:60.05ms
step:1539/2330 train_time:92415ms step_avg:60.05ms
step:1540/2330 train_time:92477ms step_avg:60.05ms
step:1541/2330 train_time:92536ms step_avg:60.05ms
step:1542/2330 train_time:92598ms step_avg:60.05ms
step:1543/2330 train_time:92658ms step_avg:60.05ms
step:1544/2330 train_time:92719ms step_avg:60.05ms
step:1545/2330 train_time:92779ms step_avg:60.05ms
step:1546/2330 train_time:92840ms step_avg:60.05ms
step:1547/2330 train_time:92900ms step_avg:60.05ms
step:1548/2330 train_time:92962ms step_avg:60.05ms
step:1549/2330 train_time:93022ms step_avg:60.05ms
step:1550/2330 train_time:93085ms step_avg:60.06ms
step:1551/2330 train_time:93146ms step_avg:60.06ms
step:1552/2330 train_time:93208ms step_avg:60.06ms
step:1553/2330 train_time:93268ms step_avg:60.06ms
step:1554/2330 train_time:93330ms step_avg:60.06ms
step:1555/2330 train_time:93390ms step_avg:60.06ms
step:1556/2330 train_time:93451ms step_avg:60.06ms
step:1557/2330 train_time:93511ms step_avg:60.06ms
step:1558/2330 train_time:93572ms step_avg:60.06ms
step:1559/2330 train_time:93632ms step_avg:60.06ms
step:1560/2330 train_time:93694ms step_avg:60.06ms
step:1561/2330 train_time:93753ms step_avg:60.06ms
step:1562/2330 train_time:93816ms step_avg:60.06ms
step:1563/2330 train_time:93876ms step_avg:60.06ms
step:1564/2330 train_time:93938ms step_avg:60.06ms
step:1565/2330 train_time:93998ms step_avg:60.06ms
step:1566/2330 train_time:94060ms step_avg:60.06ms
step:1567/2330 train_time:94119ms step_avg:60.06ms
step:1568/2330 train_time:94181ms step_avg:60.06ms
step:1569/2330 train_time:94241ms step_avg:60.06ms
step:1570/2330 train_time:94303ms step_avg:60.07ms
step:1571/2330 train_time:94363ms step_avg:60.07ms
step:1572/2330 train_time:94426ms step_avg:60.07ms
step:1573/2330 train_time:94486ms step_avg:60.07ms
step:1574/2330 train_time:94548ms step_avg:60.07ms
step:1575/2330 train_time:94607ms step_avg:60.07ms
step:1576/2330 train_time:94669ms step_avg:60.07ms
step:1577/2330 train_time:94729ms step_avg:60.07ms
step:1578/2330 train_time:94791ms step_avg:60.07ms
step:1579/2330 train_time:94850ms step_avg:60.07ms
step:1580/2330 train_time:94912ms step_avg:60.07ms
step:1581/2330 train_time:94972ms step_avg:60.07ms
step:1582/2330 train_time:95034ms step_avg:60.07ms
step:1583/2330 train_time:95094ms step_avg:60.07ms
step:1584/2330 train_time:95156ms step_avg:60.07ms
step:1585/2330 train_time:95216ms step_avg:60.07ms
step:1586/2330 train_time:95279ms step_avg:60.07ms
step:1587/2330 train_time:95338ms step_avg:60.07ms
step:1588/2330 train_time:95399ms step_avg:60.08ms
step:1589/2330 train_time:95459ms step_avg:60.08ms
step:1590/2330 train_time:95521ms step_avg:60.08ms
step:1591/2330 train_time:95581ms step_avg:60.08ms
step:1592/2330 train_time:95644ms step_avg:60.08ms
step:1593/2330 train_time:95704ms step_avg:60.08ms
step:1594/2330 train_time:95766ms step_avg:60.08ms
step:1595/2330 train_time:95826ms step_avg:60.08ms
step:1596/2330 train_time:95888ms step_avg:60.08ms
step:1597/2330 train_time:95948ms step_avg:60.08ms
step:1598/2330 train_time:96010ms step_avg:60.08ms
step:1599/2330 train_time:96069ms step_avg:60.08ms
step:1600/2330 train_time:96130ms step_avg:60.08ms
step:1601/2330 train_time:96190ms step_avg:60.08ms
step:1602/2330 train_time:96252ms step_avg:60.08ms
step:1603/2330 train_time:96312ms step_avg:60.08ms
step:1604/2330 train_time:96375ms step_avg:60.08ms
step:1605/2330 train_time:96435ms step_avg:60.08ms
step:1606/2330 train_time:96497ms step_avg:60.09ms
step:1607/2330 train_time:96557ms step_avg:60.09ms
step:1608/2330 train_time:96618ms step_avg:60.09ms
step:1609/2330 train_time:96678ms step_avg:60.09ms
step:1610/2330 train_time:96739ms step_avg:60.09ms
step:1611/2330 train_time:96799ms step_avg:60.09ms
step:1612/2330 train_time:96860ms step_avg:60.09ms
step:1613/2330 train_time:96920ms step_avg:60.09ms
step:1614/2330 train_time:96983ms step_avg:60.09ms
step:1615/2330 train_time:97044ms step_avg:60.09ms
step:1616/2330 train_time:97106ms step_avg:60.09ms
step:1617/2330 train_time:97166ms step_avg:60.09ms
step:1618/2330 train_time:97228ms step_avg:60.09ms
step:1619/2330 train_time:97288ms step_avg:60.09ms
step:1620/2330 train_time:97349ms step_avg:60.09ms
step:1621/2330 train_time:97409ms step_avg:60.09ms
step:1622/2330 train_time:97471ms step_avg:60.09ms
step:1623/2330 train_time:97530ms step_avg:60.09ms
step:1624/2330 train_time:97592ms step_avg:60.09ms
step:1625/2330 train_time:97652ms step_avg:60.09ms
step:1626/2330 train_time:97714ms step_avg:60.09ms
step:1627/2330 train_time:97774ms step_avg:60.09ms
step:1628/2330 train_time:97835ms step_avg:60.10ms
step:1629/2330 train_time:97895ms step_avg:60.10ms
step:1630/2330 train_time:97957ms step_avg:60.10ms
step:1631/2330 train_time:98017ms step_avg:60.10ms
step:1632/2330 train_time:98078ms step_avg:60.10ms
step:1633/2330 train_time:98138ms step_avg:60.10ms
step:1634/2330 train_time:98200ms step_avg:60.10ms
step:1635/2330 train_time:98259ms step_avg:60.10ms
step:1636/2330 train_time:98321ms step_avg:60.10ms
step:1637/2330 train_time:98381ms step_avg:60.10ms
step:1638/2330 train_time:98443ms step_avg:60.10ms
step:1639/2330 train_time:98504ms step_avg:60.10ms
step:1640/2330 train_time:98566ms step_avg:60.10ms
step:1641/2330 train_time:98626ms step_avg:60.10ms
step:1642/2330 train_time:98688ms step_avg:60.10ms
step:1643/2330 train_time:98748ms step_avg:60.10ms
step:1644/2330 train_time:98810ms step_avg:60.10ms
step:1645/2330 train_time:98869ms step_avg:60.10ms
step:1646/2330 train_time:98931ms step_avg:60.10ms
step:1647/2330 train_time:98991ms step_avg:60.10ms
step:1648/2330 train_time:99053ms step_avg:60.10ms
step:1649/2330 train_time:99112ms step_avg:60.10ms
step:1650/2330 train_time:99174ms step_avg:60.11ms
step:1651/2330 train_time:99233ms step_avg:60.10ms
step:1652/2330 train_time:99296ms step_avg:60.11ms
step:1653/2330 train_time:99356ms step_avg:60.11ms
step:1654/2330 train_time:99417ms step_avg:60.11ms
step:1655/2330 train_time:99477ms step_avg:60.11ms
step:1656/2330 train_time:99539ms step_avg:60.11ms
step:1657/2330 train_time:99598ms step_avg:60.11ms
step:1658/2330 train_time:99660ms step_avg:60.11ms
step:1659/2330 train_time:99720ms step_avg:60.11ms
step:1660/2330 train_time:99783ms step_avg:60.11ms
step:1661/2330 train_time:99843ms step_avg:60.11ms
step:1662/2330 train_time:99906ms step_avg:60.11ms
step:1663/2330 train_time:99966ms step_avg:60.11ms
step:1664/2330 train_time:100028ms step_avg:60.11ms
step:1665/2330 train_time:100088ms step_avg:60.11ms
step:1666/2330 train_time:100149ms step_avg:60.11ms
step:1667/2330 train_time:100209ms step_avg:60.11ms
step:1668/2330 train_time:100271ms step_avg:60.11ms
step:1669/2330 train_time:100331ms step_avg:60.11ms
step:1670/2330 train_time:100392ms step_avg:60.12ms
step:1671/2330 train_time:100452ms step_avg:60.12ms
step:1672/2330 train_time:100514ms step_avg:60.12ms
step:1673/2330 train_time:100574ms step_avg:60.12ms
step:1674/2330 train_time:100637ms step_avg:60.12ms
step:1675/2330 train_time:100697ms step_avg:60.12ms
step:1676/2330 train_time:100759ms step_avg:60.12ms
step:1677/2330 train_time:100818ms step_avg:60.12ms
step:1678/2330 train_time:100880ms step_avg:60.12ms
step:1679/2330 train_time:100939ms step_avg:60.12ms
step:1680/2330 train_time:101001ms step_avg:60.12ms
step:1681/2330 train_time:101062ms step_avg:60.12ms
step:1682/2330 train_time:101124ms step_avg:60.12ms
step:1683/2330 train_time:101184ms step_avg:60.12ms
step:1684/2330 train_time:101246ms step_avg:60.12ms
step:1685/2330 train_time:101306ms step_avg:60.12ms
step:1686/2330 train_time:101368ms step_avg:60.12ms
step:1687/2330 train_time:101428ms step_avg:60.12ms
step:1688/2330 train_time:101490ms step_avg:60.12ms
step:1689/2330 train_time:101549ms step_avg:60.12ms
step:1690/2330 train_time:101612ms step_avg:60.13ms
step:1691/2330 train_time:101671ms step_avg:60.12ms
step:1692/2330 train_time:101733ms step_avg:60.13ms
step:1693/2330 train_time:101793ms step_avg:60.13ms
step:1694/2330 train_time:101855ms step_avg:60.13ms
step:1695/2330 train_time:101915ms step_avg:60.13ms
step:1696/2330 train_time:101977ms step_avg:60.13ms
step:1697/2330 train_time:102037ms step_avg:60.13ms
step:1698/2330 train_time:102099ms step_avg:60.13ms
step:1699/2330 train_time:102158ms step_avg:60.13ms
step:1700/2330 train_time:102220ms step_avg:60.13ms
step:1701/2330 train_time:102279ms step_avg:60.13ms
step:1702/2330 train_time:102341ms step_avg:60.13ms
step:1703/2330 train_time:102401ms step_avg:60.13ms
step:1704/2330 train_time:102465ms step_avg:60.13ms
step:1705/2330 train_time:102525ms step_avg:60.13ms
step:1706/2330 train_time:102587ms step_avg:60.13ms
step:1707/2330 train_time:102647ms step_avg:60.13ms
step:1708/2330 train_time:102709ms step_avg:60.13ms
step:1709/2330 train_time:102768ms step_avg:60.13ms
step:1710/2330 train_time:102830ms step_avg:60.13ms
step:1711/2330 train_time:102890ms step_avg:60.13ms
step:1712/2330 train_time:102951ms step_avg:60.14ms
step:1713/2330 train_time:103011ms step_avg:60.13ms
step:1714/2330 train_time:103073ms step_avg:60.14ms
step:1715/2330 train_time:103132ms step_avg:60.14ms
step:1716/2330 train_time:103194ms step_avg:60.14ms
step:1717/2330 train_time:103254ms step_avg:60.14ms
step:1718/2330 train_time:103316ms step_avg:60.14ms
step:1719/2330 train_time:103376ms step_avg:60.14ms
step:1720/2330 train_time:103438ms step_avg:60.14ms
step:1721/2330 train_time:103497ms step_avg:60.14ms
step:1722/2330 train_time:103559ms step_avg:60.14ms
step:1723/2330 train_time:103618ms step_avg:60.14ms
step:1724/2330 train_time:103681ms step_avg:60.14ms
step:1725/2330 train_time:103741ms step_avg:60.14ms
step:1726/2330 train_time:103804ms step_avg:60.14ms
step:1727/2330 train_time:103863ms step_avg:60.14ms
step:1728/2330 train_time:103926ms step_avg:60.14ms
step:1729/2330 train_time:103985ms step_avg:60.14ms
step:1730/2330 train_time:104047ms step_avg:60.14ms
step:1731/2330 train_time:104107ms step_avg:60.14ms
step:1732/2330 train_time:104169ms step_avg:60.14ms
step:1733/2330 train_time:104229ms step_avg:60.14ms
step:1734/2330 train_time:104291ms step_avg:60.14ms
step:1735/2330 train_time:104350ms step_avg:60.14ms
step:1736/2330 train_time:104412ms step_avg:60.15ms
step:1737/2330 train_time:104472ms step_avg:60.14ms
step:1738/2330 train_time:104534ms step_avg:60.15ms
step:1739/2330 train_time:104593ms step_avg:60.15ms
step:1740/2330 train_time:104655ms step_avg:60.15ms
step:1741/2330 train_time:104715ms step_avg:60.15ms
step:1742/2330 train_time:104777ms step_avg:60.15ms
step:1743/2330 train_time:104837ms step_avg:60.15ms
step:1744/2330 train_time:104899ms step_avg:60.15ms
step:1745/2330 train_time:104958ms step_avg:60.15ms
step:1746/2330 train_time:105020ms step_avg:60.15ms
step:1747/2330 train_time:105079ms step_avg:60.15ms
step:1748/2330 train_time:105141ms step_avg:60.15ms
step:1749/2330 train_time:105202ms step_avg:60.15ms
step:1750/2330 train_time:105266ms step_avg:60.15ms
step:1750/2330 val_loss:3.3823 train_time:105330ms step_avg:60.19ms
step:1751/2330 train_time:105350ms step_avg:60.17ms
step:1752/2330 train_time:105388ms step_avg:60.15ms
step:1753/2330 train_time:105448ms step_avg:60.15ms
step:1754/2330 train_time:105510ms step_avg:60.15ms
step:1755/2330 train_time:105571ms step_avg:60.15ms
step:1756/2330 train_time:105634ms step_avg:60.16ms
step:1757/2330 train_time:105694ms step_avg:60.16ms
step:1758/2330 train_time:105755ms step_avg:60.16ms
step:1759/2330 train_time:105815ms step_avg:60.16ms
step:1760/2330 train_time:105875ms step_avg:60.16ms
step:1761/2330 train_time:105934ms step_avg:60.16ms
step:1762/2330 train_time:105996ms step_avg:60.16ms
step:1763/2330 train_time:106055ms step_avg:60.16ms
step:1764/2330 train_time:106117ms step_avg:60.16ms
step:1765/2330 train_time:106176ms step_avg:60.16ms
step:1766/2330 train_time:106243ms step_avg:60.16ms
step:1767/2330 train_time:106307ms step_avg:60.16ms
step:1768/2330 train_time:106369ms step_avg:60.16ms
step:1769/2330 train_time:106430ms step_avg:60.16ms
step:1770/2330 train_time:106492ms step_avg:60.16ms
step:1771/2330 train_time:106552ms step_avg:60.16ms
step:1772/2330 train_time:106614ms step_avg:60.17ms
step:1773/2330 train_time:106674ms step_avg:60.17ms
step:1774/2330 train_time:106735ms step_avg:60.17ms
step:1775/2330 train_time:106795ms step_avg:60.17ms
step:1776/2330 train_time:106856ms step_avg:60.17ms
step:1777/2330 train_time:106916ms step_avg:60.17ms
step:1778/2330 train_time:106977ms step_avg:60.17ms
step:1779/2330 train_time:107037ms step_avg:60.17ms
step:1780/2330 train_time:107098ms step_avg:60.17ms
step:1781/2330 train_time:107158ms step_avg:60.17ms
step:1782/2330 train_time:107221ms step_avg:60.17ms
step:1783/2330 train_time:107283ms step_avg:60.17ms
step:1784/2330 train_time:107345ms step_avg:60.17ms
step:1785/2330 train_time:107405ms step_avg:60.17ms
step:1786/2330 train_time:107466ms step_avg:60.17ms
step:1787/2330 train_time:107526ms step_avg:60.17ms
step:1788/2330 train_time:107588ms step_avg:60.17ms
step:1789/2330 train_time:107647ms step_avg:60.17ms
step:1790/2330 train_time:107710ms step_avg:60.17ms
step:1791/2330 train_time:107770ms step_avg:60.17ms
step:1792/2330 train_time:107831ms step_avg:60.17ms
step:1793/2330 train_time:107890ms step_avg:60.17ms
step:1794/2330 train_time:107951ms step_avg:60.17ms
step:1795/2330 train_time:108011ms step_avg:60.17ms
step:1796/2330 train_time:108072ms step_avg:60.17ms
step:1797/2330 train_time:108132ms step_avg:60.17ms
step:1798/2330 train_time:108194ms step_avg:60.17ms
step:1799/2330 train_time:108254ms step_avg:60.17ms
step:1800/2330 train_time:108318ms step_avg:60.18ms
step:1801/2330 train_time:108379ms step_avg:60.18ms
step:1802/2330 train_time:108442ms step_avg:60.18ms
step:1803/2330 train_time:108503ms step_avg:60.18ms
step:1804/2330 train_time:108565ms step_avg:60.18ms
step:1805/2330 train_time:108624ms step_avg:60.18ms
step:1806/2330 train_time:108685ms step_avg:60.18ms
step:1807/2330 train_time:108745ms step_avg:60.18ms
step:1808/2330 train_time:108806ms step_avg:60.18ms
step:1809/2330 train_time:108866ms step_avg:60.18ms
step:1810/2330 train_time:108927ms step_avg:60.18ms
step:1811/2330 train_time:108987ms step_avg:60.18ms
step:1812/2330 train_time:109049ms step_avg:60.18ms
step:1813/2330 train_time:109108ms step_avg:60.18ms
step:1814/2330 train_time:109170ms step_avg:60.18ms
step:1815/2330 train_time:109231ms step_avg:60.18ms
step:1816/2330 train_time:109293ms step_avg:60.18ms
step:1817/2330 train_time:109352ms step_avg:60.18ms
step:1818/2330 train_time:109415ms step_avg:60.18ms
step:1819/2330 train_time:109475ms step_avg:60.18ms
step:1820/2330 train_time:109538ms step_avg:60.19ms
step:1821/2330 train_time:109599ms step_avg:60.19ms
step:1822/2330 train_time:109661ms step_avg:60.19ms
step:1823/2330 train_time:109720ms step_avg:60.19ms
step:1824/2330 train_time:109782ms step_avg:60.19ms
step:1825/2330 train_time:109841ms step_avg:60.19ms
step:1826/2330 train_time:109903ms step_avg:60.19ms
step:1827/2330 train_time:109962ms step_avg:60.19ms
step:1828/2330 train_time:110024ms step_avg:60.19ms
step:1829/2330 train_time:110084ms step_avg:60.19ms
step:1830/2330 train_time:110146ms step_avg:60.19ms
step:1831/2330 train_time:110206ms step_avg:60.19ms
step:1832/2330 train_time:110269ms step_avg:60.19ms
step:1833/2330 train_time:110330ms step_avg:60.19ms
step:1834/2330 train_time:110391ms step_avg:60.19ms
step:1835/2330 train_time:110451ms step_avg:60.19ms
step:1836/2330 train_time:110512ms step_avg:60.19ms
step:1837/2330 train_time:110573ms step_avg:60.19ms
step:1838/2330 train_time:110635ms step_avg:60.19ms
step:1839/2330 train_time:110694ms step_avg:60.19ms
step:1840/2330 train_time:110756ms step_avg:60.19ms
step:1841/2330 train_time:110816ms step_avg:60.19ms
step:1842/2330 train_time:110878ms step_avg:60.19ms
step:1843/2330 train_time:110939ms step_avg:60.19ms
step:1844/2330 train_time:111001ms step_avg:60.20ms
step:1845/2330 train_time:111061ms step_avg:60.20ms
step:1846/2330 train_time:111123ms step_avg:60.20ms
step:1847/2330 train_time:111182ms step_avg:60.20ms
step:1848/2330 train_time:111243ms step_avg:60.20ms
step:1849/2330 train_time:111304ms step_avg:60.20ms
step:1850/2330 train_time:111366ms step_avg:60.20ms
step:1851/2330 train_time:111426ms step_avg:60.20ms
step:1852/2330 train_time:111488ms step_avg:60.20ms
step:1853/2330 train_time:111548ms step_avg:60.20ms
step:1854/2330 train_time:111610ms step_avg:60.20ms
step:1855/2330 train_time:111670ms step_avg:60.20ms
step:1856/2330 train_time:111732ms step_avg:60.20ms
step:1857/2330 train_time:111791ms step_avg:60.20ms
step:1858/2330 train_time:111852ms step_avg:60.20ms
step:1859/2330 train_time:111913ms step_avg:60.20ms
step:1860/2330 train_time:111975ms step_avg:60.20ms
step:1861/2330 train_time:112035ms step_avg:60.20ms
step:1862/2330 train_time:112097ms step_avg:60.20ms
step:1863/2330 train_time:112157ms step_avg:60.20ms
step:1864/2330 train_time:112220ms step_avg:60.20ms
step:1865/2330 train_time:112280ms step_avg:60.20ms
step:1866/2330 train_time:112342ms step_avg:60.20ms
step:1867/2330 train_time:112402ms step_avg:60.20ms
step:1868/2330 train_time:112463ms step_avg:60.21ms
step:1869/2330 train_time:112523ms step_avg:60.20ms
step:1870/2330 train_time:112584ms step_avg:60.21ms
step:1871/2330 train_time:112644ms step_avg:60.21ms
step:1872/2330 train_time:112706ms step_avg:60.21ms
step:1873/2330 train_time:112766ms step_avg:60.21ms
step:1874/2330 train_time:112827ms step_avg:60.21ms
step:1875/2330 train_time:112887ms step_avg:60.21ms
step:1876/2330 train_time:112949ms step_avg:60.21ms
step:1877/2330 train_time:113009ms step_avg:60.21ms
step:1878/2330 train_time:113071ms step_avg:60.21ms
step:1879/2330 train_time:113131ms step_avg:60.21ms
step:1880/2330 train_time:113192ms step_avg:60.21ms
step:1881/2330 train_time:113252ms step_avg:60.21ms
step:1882/2330 train_time:113314ms step_avg:60.21ms
step:1883/2330 train_time:113375ms step_avg:60.21ms
step:1884/2330 train_time:113437ms step_avg:60.21ms
step:1885/2330 train_time:113497ms step_avg:60.21ms
step:1886/2330 train_time:113560ms step_avg:60.21ms
step:1887/2330 train_time:113620ms step_avg:60.21ms
step:1888/2330 train_time:113681ms step_avg:60.21ms
step:1889/2330 train_time:113740ms step_avg:60.21ms
step:1890/2330 train_time:113802ms step_avg:60.21ms
step:1891/2330 train_time:113861ms step_avg:60.21ms
step:1892/2330 train_time:113923ms step_avg:60.21ms
step:1893/2330 train_time:113983ms step_avg:60.21ms
step:1894/2330 train_time:114045ms step_avg:60.21ms
step:1895/2330 train_time:114105ms step_avg:60.21ms
step:1896/2330 train_time:114167ms step_avg:60.21ms
step:1897/2330 train_time:114226ms step_avg:60.21ms
step:1898/2330 train_time:114288ms step_avg:60.21ms
step:1899/2330 train_time:114347ms step_avg:60.21ms
step:1900/2330 train_time:114409ms step_avg:60.22ms
step:1901/2330 train_time:114470ms step_avg:60.22ms
step:1902/2330 train_time:114531ms step_avg:60.22ms
step:1903/2330 train_time:114591ms step_avg:60.22ms
step:1904/2330 train_time:114653ms step_avg:60.22ms
step:1905/2330 train_time:114713ms step_avg:60.22ms
step:1906/2330 train_time:114775ms step_avg:60.22ms
step:1907/2330 train_time:114836ms step_avg:60.22ms
step:1908/2330 train_time:114899ms step_avg:60.22ms
step:1909/2330 train_time:114958ms step_avg:60.22ms
step:1910/2330 train_time:115021ms step_avg:60.22ms
step:1911/2330 train_time:115080ms step_avg:60.22ms
step:1912/2330 train_time:115142ms step_avg:60.22ms
step:1913/2330 train_time:115201ms step_avg:60.22ms
step:1914/2330 train_time:115263ms step_avg:60.22ms
step:1915/2330 train_time:115322ms step_avg:60.22ms
step:1916/2330 train_time:115384ms step_avg:60.22ms
step:1917/2330 train_time:115443ms step_avg:60.22ms
step:1918/2330 train_time:115506ms step_avg:60.22ms
step:1919/2330 train_time:115566ms step_avg:60.22ms
step:1920/2330 train_time:115628ms step_avg:60.22ms
step:1921/2330 train_time:115687ms step_avg:60.22ms
step:1922/2330 train_time:115748ms step_avg:60.22ms
step:1923/2330 train_time:115809ms step_avg:60.22ms
step:1924/2330 train_time:115872ms step_avg:60.22ms
step:1925/2330 train_time:115931ms step_avg:60.22ms
step:1926/2330 train_time:115992ms step_avg:60.22ms
step:1927/2330 train_time:116052ms step_avg:60.22ms
step:1928/2330 train_time:116115ms step_avg:60.23ms
step:1929/2330 train_time:116174ms step_avg:60.23ms
step:1930/2330 train_time:116236ms step_avg:60.23ms
step:1931/2330 train_time:116297ms step_avg:60.23ms
step:1932/2330 train_time:116359ms step_avg:60.23ms
step:1933/2330 train_time:116419ms step_avg:60.23ms
step:1934/2330 train_time:116481ms step_avg:60.23ms
step:1935/2330 train_time:116541ms step_avg:60.23ms
step:1936/2330 train_time:116602ms step_avg:60.23ms
step:1937/2330 train_time:116661ms step_avg:60.23ms
step:1938/2330 train_time:116723ms step_avg:60.23ms
step:1939/2330 train_time:116783ms step_avg:60.23ms
step:1940/2330 train_time:116845ms step_avg:60.23ms
step:1941/2330 train_time:116904ms step_avg:60.23ms
step:1942/2330 train_time:116967ms step_avg:60.23ms
step:1943/2330 train_time:117026ms step_avg:60.23ms
step:1944/2330 train_time:117088ms step_avg:60.23ms
step:1945/2330 train_time:117148ms step_avg:60.23ms
step:1946/2330 train_time:117209ms step_avg:60.23ms
step:1947/2330 train_time:117269ms step_avg:60.23ms
step:1948/2330 train_time:117331ms step_avg:60.23ms
step:1949/2330 train_time:117390ms step_avg:60.23ms
step:1950/2330 train_time:117452ms step_avg:60.23ms
step:1951/2330 train_time:117511ms step_avg:60.23ms
step:1952/2330 train_time:117573ms step_avg:60.23ms
step:1953/2330 train_time:117633ms step_avg:60.23ms
step:1954/2330 train_time:117696ms step_avg:60.23ms
step:1955/2330 train_time:117756ms step_avg:60.23ms
step:1956/2330 train_time:117818ms step_avg:60.23ms
step:1957/2330 train_time:117879ms step_avg:60.23ms
step:1958/2330 train_time:117941ms step_avg:60.24ms
step:1959/2330 train_time:118001ms step_avg:60.24ms
step:1960/2330 train_time:118062ms step_avg:60.24ms
step:1961/2330 train_time:118121ms step_avg:60.24ms
step:1962/2330 train_time:118183ms step_avg:60.24ms
step:1963/2330 train_time:118243ms step_avg:60.24ms
step:1964/2330 train_time:118305ms step_avg:60.24ms
step:1965/2330 train_time:118365ms step_avg:60.24ms
step:1966/2330 train_time:118426ms step_avg:60.24ms
step:1967/2330 train_time:118486ms step_avg:60.24ms
step:1968/2330 train_time:118547ms step_avg:60.24ms
step:1969/2330 train_time:118607ms step_avg:60.24ms
step:1970/2330 train_time:118669ms step_avg:60.24ms
step:1971/2330 train_time:118729ms step_avg:60.24ms
step:1972/2330 train_time:118790ms step_avg:60.24ms
step:1973/2330 train_time:118850ms step_avg:60.24ms
step:1974/2330 train_time:118912ms step_avg:60.24ms
step:1975/2330 train_time:118971ms step_avg:60.24ms
step:1976/2330 train_time:119033ms step_avg:60.24ms
step:1977/2330 train_time:119093ms step_avg:60.24ms
step:1978/2330 train_time:119155ms step_avg:60.24ms
step:1979/2330 train_time:119215ms step_avg:60.24ms
step:1980/2330 train_time:119277ms step_avg:60.24ms
step:1981/2330 train_time:119337ms step_avg:60.24ms
step:1982/2330 train_time:119400ms step_avg:60.24ms
step:1983/2330 train_time:119460ms step_avg:60.24ms
step:1984/2330 train_time:119521ms step_avg:60.24ms
step:1985/2330 train_time:119581ms step_avg:60.24ms
step:1986/2330 train_time:119642ms step_avg:60.24ms
step:1987/2330 train_time:119702ms step_avg:60.24ms
step:1988/2330 train_time:119764ms step_avg:60.24ms
step:1989/2330 train_time:119823ms step_avg:60.24ms
step:1990/2330 train_time:119885ms step_avg:60.24ms
step:1991/2330 train_time:119944ms step_avg:60.24ms
step:1992/2330 train_time:120006ms step_avg:60.24ms
step:1993/2330 train_time:120066ms step_avg:60.24ms
step:1994/2330 train_time:120129ms step_avg:60.25ms
step:1995/2330 train_time:120189ms step_avg:60.24ms
step:1996/2330 train_time:120250ms step_avg:60.25ms
step:1997/2330 train_time:120310ms step_avg:60.25ms
step:1998/2330 train_time:120372ms step_avg:60.25ms
step:1999/2330 train_time:120431ms step_avg:60.25ms
step:2000/2330 train_time:120493ms step_avg:60.25ms
step:2000/2330 val_loss:3.3323 train_time:120557ms step_avg:60.28ms
step:2001/2330 train_time:120578ms step_avg:60.26ms
step:2002/2330 train_time:120618ms step_avg:60.25ms
step:2003/2330 train_time:120682ms step_avg:60.25ms
step:2004/2330 train_time:120748ms step_avg:60.25ms
step:2005/2330 train_time:120808ms step_avg:60.25ms
step:2006/2330 train_time:120869ms step_avg:60.25ms
step:2007/2330 train_time:120929ms step_avg:60.25ms
step:2008/2330 train_time:120990ms step_avg:60.25ms
step:2009/2330 train_time:121049ms step_avg:60.25ms
step:2010/2330 train_time:121110ms step_avg:60.25ms
step:2011/2330 train_time:121169ms step_avg:60.25ms
step:2012/2330 train_time:121231ms step_avg:60.25ms
step:2013/2330 train_time:121290ms step_avg:60.25ms
step:2014/2330 train_time:121351ms step_avg:60.25ms
step:2015/2330 train_time:121409ms step_avg:60.25ms
step:2016/2330 train_time:121472ms step_avg:60.25ms
step:2017/2330 train_time:121534ms step_avg:60.25ms
step:2018/2330 train_time:121598ms step_avg:60.26ms
step:2019/2330 train_time:121658ms step_avg:60.26ms
step:2020/2330 train_time:121720ms step_avg:60.26ms
step:2021/2330 train_time:121781ms step_avg:60.26ms
step:2022/2330 train_time:121843ms step_avg:60.26ms
step:2023/2330 train_time:121904ms step_avg:60.26ms
step:2024/2330 train_time:121966ms step_avg:60.26ms
step:2025/2330 train_time:122025ms step_avg:60.26ms
step:2026/2330 train_time:122086ms step_avg:60.26ms
step:2027/2330 train_time:122145ms step_avg:60.26ms
step:2028/2330 train_time:122206ms step_avg:60.26ms
step:2029/2330 train_time:122265ms step_avg:60.26ms
step:2030/2330 train_time:122327ms step_avg:60.26ms
step:2031/2330 train_time:122386ms step_avg:60.26ms
step:2032/2330 train_time:122448ms step_avg:60.26ms
step:2033/2330 train_time:122507ms step_avg:60.26ms
step:2034/2330 train_time:122570ms step_avg:60.26ms
step:2035/2330 train_time:122632ms step_avg:60.26ms
step:2036/2330 train_time:122695ms step_avg:60.26ms
step:2037/2330 train_time:122755ms step_avg:60.26ms
step:2038/2330 train_time:122817ms step_avg:60.26ms
step:2039/2330 train_time:122877ms step_avg:60.26ms
step:2040/2330 train_time:122939ms step_avg:60.26ms
step:2041/2330 train_time:122999ms step_avg:60.26ms
step:2042/2330 train_time:123061ms step_avg:60.27ms
step:2043/2330 train_time:123121ms step_avg:60.26ms
step:2044/2330 train_time:123184ms step_avg:60.27ms
step:2045/2330 train_time:123244ms step_avg:60.27ms
step:2046/2330 train_time:123305ms step_avg:60.27ms
step:2047/2330 train_time:123365ms step_avg:60.27ms
step:2048/2330 train_time:123426ms step_avg:60.27ms
step:2049/2330 train_time:123486ms step_avg:60.27ms
step:2050/2330 train_time:123548ms step_avg:60.27ms
step:2051/2330 train_time:123608ms step_avg:60.27ms
step:2052/2330 train_time:123670ms step_avg:60.27ms
step:2053/2330 train_time:123730ms step_avg:60.27ms
step:2054/2330 train_time:123793ms step_avg:60.27ms
step:2055/2330 train_time:123853ms step_avg:60.27ms
step:2056/2330 train_time:123915ms step_avg:60.27ms
step:2057/2330 train_time:123974ms step_avg:60.27ms
step:2058/2330 train_time:124035ms step_avg:60.27ms
step:2059/2330 train_time:124095ms step_avg:60.27ms
step:2060/2330 train_time:124156ms step_avg:60.27ms
step:2061/2330 train_time:124215ms step_avg:60.27ms
step:2062/2330 train_time:124277ms step_avg:60.27ms
step:2063/2330 train_time:124338ms step_avg:60.27ms
step:2064/2330 train_time:124400ms step_avg:60.27ms
step:2065/2330 train_time:124460ms step_avg:60.27ms
step:2066/2330 train_time:124523ms step_avg:60.27ms
step:2067/2330 train_time:124583ms step_avg:60.27ms
step:2068/2330 train_time:124646ms step_avg:60.27ms
step:2069/2330 train_time:124706ms step_avg:60.27ms
step:2070/2330 train_time:124767ms step_avg:60.27ms
step:2071/2330 train_time:124827ms step_avg:60.27ms
step:2072/2330 train_time:124889ms step_avg:60.27ms
step:2073/2330 train_time:124950ms step_avg:60.27ms
step:2074/2330 train_time:125011ms step_avg:60.28ms
step:2075/2330 train_time:125071ms step_avg:60.28ms
step:2076/2330 train_time:125133ms step_avg:60.28ms
step:2077/2330 train_time:125193ms step_avg:60.28ms
step:2078/2330 train_time:125255ms step_avg:60.28ms
step:2079/2330 train_time:125314ms step_avg:60.28ms
step:2080/2330 train_time:125375ms step_avg:60.28ms
step:2081/2330 train_time:125434ms step_avg:60.28ms
step:2082/2330 train_time:125496ms step_avg:60.28ms
step:2083/2330 train_time:125556ms step_avg:60.28ms
step:2084/2330 train_time:125618ms step_avg:60.28ms
step:2085/2330 train_time:125679ms step_avg:60.28ms
step:2086/2330 train_time:125742ms step_avg:60.28ms
step:2087/2330 train_time:125803ms step_avg:60.28ms
step:2088/2330 train_time:125865ms step_avg:60.28ms
step:2089/2330 train_time:125925ms step_avg:60.28ms
step:2090/2330 train_time:125987ms step_avg:60.28ms
step:2091/2330 train_time:126046ms step_avg:60.28ms
step:2092/2330 train_time:126108ms step_avg:60.28ms
step:2093/2330 train_time:126168ms step_avg:60.28ms
step:2094/2330 train_time:126230ms step_avg:60.28ms
step:2095/2330 train_time:126290ms step_avg:60.28ms
step:2096/2330 train_time:126352ms step_avg:60.28ms
step:2097/2330 train_time:126412ms step_avg:60.28ms
step:2098/2330 train_time:126474ms step_avg:60.28ms
step:2099/2330 train_time:126534ms step_avg:60.28ms
step:2100/2330 train_time:126596ms step_avg:60.28ms
step:2101/2330 train_time:126656ms step_avg:60.28ms
step:2102/2330 train_time:126718ms step_avg:60.28ms
step:2103/2330 train_time:126777ms step_avg:60.28ms
step:2104/2330 train_time:126840ms step_avg:60.29ms
step:2105/2330 train_time:126900ms step_avg:60.29ms
step:2106/2330 train_time:126963ms step_avg:60.29ms
step:2107/2330 train_time:127023ms step_avg:60.29ms
step:2108/2330 train_time:127085ms step_avg:60.29ms
step:2109/2330 train_time:127145ms step_avg:60.29ms
step:2110/2330 train_time:127206ms step_avg:60.29ms
step:2111/2330 train_time:127266ms step_avg:60.29ms
step:2112/2330 train_time:127328ms step_avg:60.29ms
step:2113/2330 train_time:127387ms step_avg:60.29ms
step:2114/2330 train_time:127449ms step_avg:60.29ms
step:2115/2330 train_time:127509ms step_avg:60.29ms
step:2116/2330 train_time:127571ms step_avg:60.29ms
step:2117/2330 train_time:127632ms step_avg:60.29ms
step:2118/2330 train_time:127694ms step_avg:60.29ms
step:2119/2330 train_time:127754ms step_avg:60.29ms
step:2120/2330 train_time:127816ms step_avg:60.29ms
step:2121/2330 train_time:127875ms step_avg:60.29ms
step:2122/2330 train_time:127937ms step_avg:60.29ms
step:2123/2330 train_time:127997ms step_avg:60.29ms
step:2124/2330 train_time:128059ms step_avg:60.29ms
step:2125/2330 train_time:128119ms step_avg:60.29ms
step:2126/2330 train_time:128182ms step_avg:60.29ms
step:2127/2330 train_time:128242ms step_avg:60.29ms
step:2128/2330 train_time:128304ms step_avg:60.29ms
step:2129/2330 train_time:128365ms step_avg:60.29ms
step:2130/2330 train_time:128427ms step_avg:60.29ms
step:2131/2330 train_time:128486ms step_avg:60.29ms
step:2132/2330 train_time:128548ms step_avg:60.29ms
step:2133/2330 train_time:128608ms step_avg:60.29ms
step:2134/2330 train_time:128670ms step_avg:60.30ms
step:2135/2330 train_time:128729ms step_avg:60.29ms
step:2136/2330 train_time:128792ms step_avg:60.30ms
step:2137/2330 train_time:128852ms step_avg:60.30ms
step:2138/2330 train_time:128914ms step_avg:60.30ms
step:2139/2330 train_time:128974ms step_avg:60.30ms
step:2140/2330 train_time:129035ms step_avg:60.30ms
step:2141/2330 train_time:129095ms step_avg:60.30ms
step:2142/2330 train_time:129156ms step_avg:60.30ms
step:2143/2330 train_time:129216ms step_avg:60.30ms
step:2144/2330 train_time:129278ms step_avg:60.30ms
step:2145/2330 train_time:129338ms step_avg:60.30ms
step:2146/2330 train_time:129400ms step_avg:60.30ms
step:2147/2330 train_time:129460ms step_avg:60.30ms
step:2148/2330 train_time:129522ms step_avg:60.30ms
step:2149/2330 train_time:129583ms step_avg:60.30ms
step:2150/2330 train_time:129644ms step_avg:60.30ms
step:2151/2330 train_time:129705ms step_avg:60.30ms
step:2152/2330 train_time:129766ms step_avg:60.30ms
step:2153/2330 train_time:129827ms step_avg:60.30ms
step:2154/2330 train_time:129888ms step_avg:60.30ms
step:2155/2330 train_time:129948ms step_avg:60.30ms
step:2156/2330 train_time:130009ms step_avg:60.30ms
step:2157/2330 train_time:130069ms step_avg:60.30ms
step:2158/2330 train_time:130131ms step_avg:60.30ms
step:2159/2330 train_time:130191ms step_avg:60.30ms
step:2160/2330 train_time:130254ms step_avg:60.30ms
step:2161/2330 train_time:130314ms step_avg:60.30ms
step:2162/2330 train_time:130376ms step_avg:60.30ms
step:2163/2330 train_time:130436ms step_avg:60.30ms
step:2164/2330 train_time:130497ms step_avg:60.30ms
step:2165/2330 train_time:130557ms step_avg:60.30ms
step:2166/2330 train_time:130620ms step_avg:60.30ms
step:2167/2330 train_time:130680ms step_avg:60.30ms
step:2168/2330 train_time:130743ms step_avg:60.31ms
step:2169/2330 train_time:130803ms step_avg:60.31ms
step:2170/2330 train_time:130865ms step_avg:60.31ms
step:2171/2330 train_time:130925ms step_avg:60.31ms
step:2172/2330 train_time:130986ms step_avg:60.31ms
step:2173/2330 train_time:131046ms step_avg:60.31ms
step:2174/2330 train_time:131108ms step_avg:60.31ms
step:2175/2330 train_time:131167ms step_avg:60.31ms
step:2176/2330 train_time:131229ms step_avg:60.31ms
step:2177/2330 train_time:131289ms step_avg:60.31ms
step:2178/2330 train_time:131351ms step_avg:60.31ms
step:2179/2330 train_time:131412ms step_avg:60.31ms
step:2180/2330 train_time:131474ms step_avg:60.31ms
step:2181/2330 train_time:131534ms step_avg:60.31ms
step:2182/2330 train_time:131596ms step_avg:60.31ms
step:2183/2330 train_time:131655ms step_avg:60.31ms
step:2184/2330 train_time:131716ms step_avg:60.31ms
step:2185/2330 train_time:131776ms step_avg:60.31ms
step:2186/2330 train_time:131839ms step_avg:60.31ms
step:2187/2330 train_time:131898ms step_avg:60.31ms
step:2188/2330 train_time:131961ms step_avg:60.31ms
step:2189/2330 train_time:132021ms step_avg:60.31ms
step:2190/2330 train_time:132083ms step_avg:60.31ms
step:2191/2330 train_time:132143ms step_avg:60.31ms
step:2192/2330 train_time:132205ms step_avg:60.31ms
step:2193/2330 train_time:132265ms step_avg:60.31ms
step:2194/2330 train_time:132326ms step_avg:60.31ms
step:2195/2330 train_time:132386ms step_avg:60.31ms
step:2196/2330 train_time:132447ms step_avg:60.31ms
step:2197/2330 train_time:132507ms step_avg:60.31ms
step:2198/2330 train_time:132569ms step_avg:60.31ms
step:2199/2330 train_time:132629ms step_avg:60.31ms
step:2200/2330 train_time:132691ms step_avg:60.31ms
step:2201/2330 train_time:132751ms step_avg:60.31ms
step:2202/2330 train_time:132813ms step_avg:60.31ms
step:2203/2330 train_time:132873ms step_avg:60.31ms
step:2204/2330 train_time:132934ms step_avg:60.31ms
step:2205/2330 train_time:132993ms step_avg:60.31ms
step:2206/2330 train_time:133055ms step_avg:60.31ms
step:2207/2330 train_time:133114ms step_avg:60.31ms
step:2208/2330 train_time:133176ms step_avg:60.32ms
step:2209/2330 train_time:133236ms step_avg:60.32ms
step:2210/2330 train_time:133298ms step_avg:60.32ms
step:2211/2330 train_time:133359ms step_avg:60.32ms
step:2212/2330 train_time:133421ms step_avg:60.32ms
step:2213/2330 train_time:133481ms step_avg:60.32ms
step:2214/2330 train_time:133543ms step_avg:60.32ms
step:2215/2330 train_time:133604ms step_avg:60.32ms
step:2216/2330 train_time:133665ms step_avg:60.32ms
step:2217/2330 train_time:133725ms step_avg:60.32ms
step:2218/2330 train_time:133786ms step_avg:60.32ms
step:2219/2330 train_time:133846ms step_avg:60.32ms
step:2220/2330 train_time:133909ms step_avg:60.32ms
step:2221/2330 train_time:133968ms step_avg:60.32ms
step:2222/2330 train_time:134029ms step_avg:60.32ms
step:2223/2330 train_time:134089ms step_avg:60.32ms
step:2224/2330 train_time:134150ms step_avg:60.32ms
step:2225/2330 train_time:134210ms step_avg:60.32ms
step:2226/2330 train_time:134271ms step_avg:60.32ms
step:2227/2330 train_time:134332ms step_avg:60.32ms
step:2228/2330 train_time:134394ms step_avg:60.32ms
step:2229/2330 train_time:134453ms step_avg:60.32ms
step:2230/2330 train_time:134515ms step_avg:60.32ms
step:2231/2330 train_time:134575ms step_avg:60.32ms
step:2232/2330 train_time:134637ms step_avg:60.32ms
step:2233/2330 train_time:134696ms step_avg:60.32ms
step:2234/2330 train_time:134758ms step_avg:60.32ms
step:2235/2330 train_time:134818ms step_avg:60.32ms
step:2236/2330 train_time:134881ms step_avg:60.32ms
step:2237/2330 train_time:134941ms step_avg:60.32ms
step:2238/2330 train_time:135004ms step_avg:60.32ms
step:2239/2330 train_time:135064ms step_avg:60.32ms
step:2240/2330 train_time:135126ms step_avg:60.32ms
step:2241/2330 train_time:135185ms step_avg:60.32ms
step:2242/2330 train_time:135247ms step_avg:60.32ms
step:2243/2330 train_time:135306ms step_avg:60.32ms
step:2244/2330 train_time:135368ms step_avg:60.32ms
step:2245/2330 train_time:135428ms step_avg:60.32ms
step:2246/2330 train_time:135490ms step_avg:60.33ms
step:2247/2330 train_time:135550ms step_avg:60.32ms
step:2248/2330 train_time:135612ms step_avg:60.33ms
step:2249/2330 train_time:135672ms step_avg:60.33ms
step:2250/2330 train_time:135734ms step_avg:60.33ms
step:2250/2330 val_loss:3.2928 train_time:135798ms step_avg:60.35ms
step:2251/2330 train_time:135819ms step_avg:60.34ms
step:2252/2330 train_time:135857ms step_avg:60.33ms
step:2253/2330 train_time:135920ms step_avg:60.33ms
step:2254/2330 train_time:135985ms step_avg:60.33ms
step:2255/2330 train_time:136045ms step_avg:60.33ms
step:2256/2330 train_time:136107ms step_avg:60.33ms
step:2257/2330 train_time:136166ms step_avg:60.33ms
step:2258/2330 train_time:136228ms step_avg:60.33ms
step:2259/2330 train_time:136288ms step_avg:60.33ms
step:2260/2330 train_time:136350ms step_avg:60.33ms
step:2261/2330 train_time:136410ms step_avg:60.33ms
step:2262/2330 train_time:136471ms step_avg:60.33ms
step:2263/2330 train_time:136531ms step_avg:60.33ms
step:2264/2330 train_time:136592ms step_avg:60.33ms
step:2265/2330 train_time:136652ms step_avg:60.33ms
step:2266/2330 train_time:136714ms step_avg:60.33ms
step:2267/2330 train_time:136775ms step_avg:60.33ms
step:2268/2330 train_time:136839ms step_avg:60.33ms
step:2269/2330 train_time:136900ms step_avg:60.34ms
step:2270/2330 train_time:136962ms step_avg:60.34ms
step:2271/2330 train_time:137022ms step_avg:60.34ms
step:2272/2330 train_time:137084ms step_avg:60.34ms
step:2273/2330 train_time:137144ms step_avg:60.34ms
step:2274/2330 train_time:137205ms step_avg:60.34ms
step:2275/2330 train_time:137264ms step_avg:60.34ms
step:2276/2330 train_time:137326ms step_avg:60.34ms
step:2277/2330 train_time:137385ms step_avg:60.34ms
step:2278/2330 train_time:137446ms step_avg:60.34ms
step:2279/2330 train_time:137506ms step_avg:60.34ms
step:2280/2330 train_time:137567ms step_avg:60.34ms
step:2281/2330 train_time:137626ms step_avg:60.34ms
step:2282/2330 train_time:137688ms step_avg:60.34ms
step:2283/2330 train_time:137749ms step_avg:60.34ms
step:2284/2330 train_time:137812ms step_avg:60.34ms
step:2285/2330 train_time:137873ms step_avg:60.34ms
step:2286/2330 train_time:137936ms step_avg:60.34ms
step:2287/2330 train_time:137996ms step_avg:60.34ms
step:2288/2330 train_time:138058ms step_avg:60.34ms
step:2289/2330 train_time:138117ms step_avg:60.34ms
step:2290/2330 train_time:138179ms step_avg:60.34ms
step:2291/2330 train_time:138238ms step_avg:60.34ms
step:2292/2330 train_time:138300ms step_avg:60.34ms
step:2293/2330 train_time:138360ms step_avg:60.34ms
step:2294/2330 train_time:138422ms step_avg:60.34ms
step:2295/2330 train_time:138482ms step_avg:60.34ms
step:2296/2330 train_time:138544ms step_avg:60.34ms
step:2297/2330 train_time:138603ms step_avg:60.34ms
step:2298/2330 train_time:138665ms step_avg:60.34ms
step:2299/2330 train_time:138725ms step_avg:60.34ms
step:2300/2330 train_time:138786ms step_avg:60.34ms
step:2301/2330 train_time:138846ms step_avg:60.34ms
step:2302/2330 train_time:138908ms step_avg:60.34ms
step:2303/2330 train_time:138968ms step_avg:60.34ms
step:2304/2330 train_time:139031ms step_avg:60.34ms
step:2305/2330 train_time:139091ms step_avg:60.34ms
step:2306/2330 train_time:139153ms step_avg:60.34ms
step:2307/2330 train_time:139213ms step_avg:60.34ms
step:2308/2330 train_time:139275ms step_avg:60.34ms
step:2309/2330 train_time:139334ms step_avg:60.34ms
step:2310/2330 train_time:139396ms step_avg:60.34ms
step:2311/2330 train_time:139455ms step_avg:60.34ms
step:2312/2330 train_time:139517ms step_avg:60.34ms
step:2313/2330 train_time:139576ms step_avg:60.34ms
step:2314/2330 train_time:139638ms step_avg:60.34ms
step:2315/2330 train_time:139698ms step_avg:60.34ms
step:2316/2330 train_time:139760ms step_avg:60.35ms
step:2317/2330 train_time:139820ms step_avg:60.35ms
step:2318/2330 train_time:139882ms step_avg:60.35ms
step:2319/2330 train_time:139942ms step_avg:60.35ms
step:2320/2330 train_time:140004ms step_avg:60.35ms
step:2321/2330 train_time:140063ms step_avg:60.35ms
step:2322/2330 train_time:140125ms step_avg:60.35ms
step:2323/2330 train_time:140185ms step_avg:60.35ms
step:2324/2330 train_time:140247ms step_avg:60.35ms
step:2325/2330 train_time:140307ms step_avg:60.35ms
step:2326/2330 train_time:140369ms step_avg:60.35ms
step:2327/2330 train_time:140429ms step_avg:60.35ms
step:2328/2330 train_time:140491ms step_avg:60.35ms
step:2329/2330 train_time:140551ms step_avg:60.35ms
step:2330/2330 train_time:140613ms step_avg:60.35ms
step:2330/2330 val_loss:3.2796 train_time:140678ms step_avg:60.38ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
coeffs_list = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in coeffs_list:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            # x_out = self.blocks[i](x, x0, lambdas[i], attn_args)
            # x_backout += backout_lambdas[i] * (x_out-x)
            # x = x_out
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i==8:
                x_backout=x

        # backout contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda*x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2290  # number of iterations to run
    iteration_extension = 40  # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.45  # fraction of training spent cooling down the learning rate
    momentum_cd_steps = 50  # number of iterations for muon momentum cooldown
    # evaluation and logging
    run_id: str = f"new/{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

def update_optimizer_params(step, optimizer1, optimizer2):
    # Update lr
    for group in optimizer1.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)

    # Warmup phase: gradually increase momentum from 0.85 to 0.95
    if step < 300:
        frac = step / 300
        momentum = 0.85 + frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum

    # Cooldown phase: gradually decrease momentum
    momentum_cd_start = args.num_iterations + args.iteration_extension - args.momentum_cd_steps
    if step > momentum_cd_start:
        frac = (step - momentum_cd_start) / args.momentum_cd_steps

        # Decay momentum from 0.95 to 0.85
        momentum = 0.95 - frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    update_optimizer_params(step, optimizer1, optimizer2)
    # only step Adam every other step
    if step%2==0:
        optimizer2.step()
        optimizer2.zero_grad(set_to_none=True)
    else:
        for opt in optimizers:
            opt.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
    
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Sat Oct  4 06:00:33 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   28C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   28C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   29C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   40C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   28C    P0            112W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          267215      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          267216      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          267217      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          267218      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          267219      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          267220      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          267221      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          267222      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          267216      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          267217      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          267218      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          267219      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          267220      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          267221      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          267222      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:79ms step_avg:78.91ms
step:2/2330 train_time:179ms step_avg:89.49ms
step:3/2330 train_time:201ms step_avg:67.15ms
step:4/2330 train_time:235ms step_avg:58.71ms
step:5/2330 train_time:292ms step_avg:58.39ms
step:6/2330 train_time:352ms step_avg:58.74ms
step:7/2330 train_time:411ms step_avg:58.68ms
step:8/2330 train_time:471ms step_avg:58.88ms
step:9/2330 train_time:529ms step_avg:58.76ms
step:10/2330 train_time:589ms step_avg:58.91ms
step:11/2330 train_time:647ms step_avg:58.84ms
step:12/2330 train_time:708ms step_avg:58.98ms
step:13/2330 train_time:766ms step_avg:58.90ms
step:14/2330 train_time:826ms step_avg:59.00ms
step:15/2330 train_time:884ms step_avg:58.94ms
step:16/2330 train_time:944ms step_avg:59.02ms
step:17/2330 train_time:1003ms step_avg:59.01ms
step:18/2330 train_time:1067ms step_avg:59.29ms
step:19/2330 train_time:1130ms step_avg:59.45ms
step:20/2330 train_time:1193ms step_avg:59.64ms
step:21/2330 train_time:1253ms step_avg:59.64ms
step:22/2330 train_time:1315ms step_avg:59.75ms
step:23/2330 train_time:1372ms step_avg:59.67ms
step:24/2330 train_time:1433ms step_avg:59.71ms
step:25/2330 train_time:1492ms step_avg:59.67ms
step:26/2330 train_time:1552ms step_avg:59.71ms
step:27/2330 train_time:1611ms step_avg:59.68ms
step:28/2330 train_time:1672ms step_avg:59.71ms
step:29/2330 train_time:1731ms step_avg:59.67ms
step:30/2330 train_time:1791ms step_avg:59.71ms
step:31/2330 train_time:1850ms step_avg:59.69ms
step:32/2330 train_time:1911ms step_avg:59.72ms
step:33/2330 train_time:1971ms step_avg:59.71ms
step:34/2330 train_time:2033ms step_avg:59.78ms
step:35/2330 train_time:2092ms step_avg:59.77ms
step:36/2330 train_time:2154ms step_avg:59.83ms
step:37/2330 train_time:2213ms step_avg:59.82ms
step:38/2330 train_time:2275ms step_avg:59.87ms
step:39/2330 train_time:2334ms step_avg:59.84ms
step:40/2330 train_time:2394ms step_avg:59.86ms
step:41/2330 train_time:2453ms step_avg:59.83ms
step:42/2330 train_time:2514ms step_avg:59.86ms
step:43/2330 train_time:2574ms step_avg:59.85ms
step:44/2330 train_time:2634ms step_avg:59.87ms
step:45/2330 train_time:2693ms step_avg:59.85ms
step:46/2330 train_time:2754ms step_avg:59.88ms
step:47/2330 train_time:2813ms step_avg:59.86ms
step:48/2330 train_time:2874ms step_avg:59.88ms
step:49/2330 train_time:2933ms step_avg:59.86ms
step:50/2330 train_time:2995ms step_avg:59.90ms
step:51/2330 train_time:3055ms step_avg:59.90ms
step:52/2330 train_time:3116ms step_avg:59.92ms
step:53/2330 train_time:3175ms step_avg:59.91ms
step:54/2330 train_time:3237ms step_avg:59.95ms
step:55/2330 train_time:3296ms step_avg:59.93ms
step:56/2330 train_time:3358ms step_avg:59.97ms
step:57/2330 train_time:3417ms step_avg:59.94ms
step:58/2330 train_time:3477ms step_avg:59.95ms
step:59/2330 train_time:3536ms step_avg:59.93ms
step:60/2330 train_time:3597ms step_avg:59.94ms
step:61/2330 train_time:3656ms step_avg:59.93ms
step:62/2330 train_time:3716ms step_avg:59.94ms
step:63/2330 train_time:3776ms step_avg:59.94ms
step:64/2330 train_time:3838ms step_avg:59.96ms
step:65/2330 train_time:3897ms step_avg:59.95ms
step:66/2330 train_time:3958ms step_avg:59.97ms
step:67/2330 train_time:4017ms step_avg:59.96ms
step:68/2330 train_time:4079ms step_avg:59.98ms
step:69/2330 train_time:4138ms step_avg:59.97ms
step:70/2330 train_time:4200ms step_avg:60.00ms
step:71/2330 train_time:4259ms step_avg:59.99ms
step:72/2330 train_time:4320ms step_avg:60.00ms
step:73/2330 train_time:4378ms step_avg:59.97ms
step:74/2330 train_time:4440ms step_avg:60.00ms
step:75/2330 train_time:4498ms step_avg:59.98ms
step:76/2330 train_time:4560ms step_avg:59.99ms
step:77/2330 train_time:4618ms step_avg:59.97ms
step:78/2330 train_time:4678ms step_avg:59.98ms
step:79/2330 train_time:4738ms step_avg:59.98ms
step:80/2330 train_time:4800ms step_avg:60.00ms
step:81/2330 train_time:4859ms step_avg:59.99ms
step:82/2330 train_time:4920ms step_avg:60.00ms
step:83/2330 train_time:4979ms step_avg:59.99ms
step:84/2330 train_time:5040ms step_avg:60.00ms
step:85/2330 train_time:5098ms step_avg:59.98ms
step:86/2330 train_time:5159ms step_avg:59.99ms
step:87/2330 train_time:5220ms step_avg:60.00ms
step:88/2330 train_time:5281ms step_avg:60.01ms
step:89/2330 train_time:5339ms step_avg:59.99ms
step:90/2330 train_time:5400ms step_avg:60.00ms
step:91/2330 train_time:5459ms step_avg:59.98ms
step:92/2330 train_time:5519ms step_avg:59.99ms
step:93/2330 train_time:5578ms step_avg:59.97ms
step:94/2330 train_time:5638ms step_avg:59.98ms
step:95/2330 train_time:5697ms step_avg:59.97ms
step:96/2330 train_time:5758ms step_avg:59.98ms
step:97/2330 train_time:5816ms step_avg:59.96ms
step:98/2330 train_time:5877ms step_avg:59.97ms
step:99/2330 train_time:5936ms step_avg:59.96ms
step:100/2330 train_time:5997ms step_avg:59.97ms
step:101/2330 train_time:6056ms step_avg:59.96ms
step:102/2330 train_time:6117ms step_avg:59.97ms
step:103/2330 train_time:6176ms step_avg:59.96ms
step:104/2330 train_time:6237ms step_avg:59.97ms
step:105/2330 train_time:6296ms step_avg:59.96ms
step:106/2330 train_time:6357ms step_avg:59.97ms
step:107/2330 train_time:6416ms step_avg:59.96ms
step:108/2330 train_time:6477ms step_avg:59.97ms
step:109/2330 train_time:6537ms step_avg:59.97ms
step:110/2330 train_time:6598ms step_avg:59.98ms
step:111/2330 train_time:6656ms step_avg:59.97ms
step:112/2330 train_time:6717ms step_avg:59.98ms
step:113/2330 train_time:6776ms step_avg:59.96ms
step:114/2330 train_time:6836ms step_avg:59.97ms
step:115/2330 train_time:6895ms step_avg:59.96ms
step:116/2330 train_time:6956ms step_avg:59.97ms
step:117/2330 train_time:7016ms step_avg:59.96ms
step:118/2330 train_time:7076ms step_avg:59.97ms
step:119/2330 train_time:7136ms step_avg:59.96ms
step:120/2330 train_time:7197ms step_avg:59.98ms
step:121/2330 train_time:7257ms step_avg:59.97ms
step:122/2330 train_time:7318ms step_avg:59.98ms
step:123/2330 train_time:7377ms step_avg:59.97ms
step:124/2330 train_time:7438ms step_avg:59.98ms
step:125/2330 train_time:7497ms step_avg:59.97ms
step:126/2330 train_time:7558ms step_avg:59.98ms
step:127/2330 train_time:7616ms step_avg:59.97ms
step:128/2330 train_time:7677ms step_avg:59.97ms
step:129/2330 train_time:7735ms step_avg:59.96ms
step:130/2330 train_time:7796ms step_avg:59.97ms
step:131/2330 train_time:7854ms step_avg:59.96ms
step:132/2330 train_time:7916ms step_avg:59.97ms
step:133/2330 train_time:7974ms step_avg:59.96ms
step:134/2330 train_time:8035ms step_avg:59.97ms
step:135/2330 train_time:8094ms step_avg:59.96ms
step:136/2330 train_time:8156ms step_avg:59.97ms
step:137/2330 train_time:8214ms step_avg:59.96ms
step:138/2330 train_time:8276ms step_avg:59.97ms
step:139/2330 train_time:8335ms step_avg:59.96ms
step:140/2330 train_time:8396ms step_avg:59.97ms
step:141/2330 train_time:8455ms step_avg:59.96ms
step:142/2330 train_time:8515ms step_avg:59.97ms
step:143/2330 train_time:8574ms step_avg:59.96ms
step:144/2330 train_time:8635ms step_avg:59.96ms
step:145/2330 train_time:8693ms step_avg:59.95ms
step:146/2330 train_time:8754ms step_avg:59.96ms
step:147/2330 train_time:8813ms step_avg:59.95ms
step:148/2330 train_time:8873ms step_avg:59.95ms
step:149/2330 train_time:8932ms step_avg:59.95ms
step:150/2330 train_time:8993ms step_avg:59.95ms
step:151/2330 train_time:9051ms step_avg:59.94ms
step:152/2330 train_time:9112ms step_avg:59.94ms
step:153/2330 train_time:9170ms step_avg:59.93ms
step:154/2330 train_time:9230ms step_avg:59.93ms
step:155/2330 train_time:9288ms step_avg:59.93ms
step:156/2330 train_time:9349ms step_avg:59.93ms
step:157/2330 train_time:9408ms step_avg:59.93ms
step:158/2330 train_time:9469ms step_avg:59.93ms
step:159/2330 train_time:9527ms step_avg:59.92ms
step:160/2330 train_time:9588ms step_avg:59.92ms
step:161/2330 train_time:9647ms step_avg:59.92ms
step:162/2330 train_time:9708ms step_avg:59.92ms
step:163/2330 train_time:9767ms step_avg:59.92ms
step:164/2330 train_time:9827ms step_avg:59.92ms
step:165/2330 train_time:9885ms step_avg:59.91ms
step:166/2330 train_time:9945ms step_avg:59.91ms
step:167/2330 train_time:10004ms step_avg:59.91ms
step:168/2330 train_time:10065ms step_avg:59.91ms
step:169/2330 train_time:10123ms step_avg:59.90ms
step:170/2330 train_time:10183ms step_avg:59.90ms
step:171/2330 train_time:10241ms step_avg:59.89ms
step:172/2330 train_time:10302ms step_avg:59.89ms
step:173/2330 train_time:10360ms step_avg:59.89ms
step:174/2330 train_time:10421ms step_avg:59.89ms
step:175/2330 train_time:10480ms step_avg:59.88ms
step:176/2330 train_time:10541ms step_avg:59.89ms
step:177/2330 train_time:10600ms step_avg:59.89ms
step:178/2330 train_time:10661ms step_avg:59.89ms
step:179/2330 train_time:10719ms step_avg:59.88ms
step:180/2330 train_time:10780ms step_avg:59.89ms
step:181/2330 train_time:10839ms step_avg:59.88ms
step:182/2330 train_time:10900ms step_avg:59.89ms
step:183/2330 train_time:10959ms step_avg:59.88ms
step:184/2330 train_time:11019ms step_avg:59.89ms
step:185/2330 train_time:11078ms step_avg:59.88ms
step:186/2330 train_time:11140ms step_avg:59.89ms
step:187/2330 train_time:11198ms step_avg:59.88ms
step:188/2330 train_time:11258ms step_avg:59.88ms
step:189/2330 train_time:11317ms step_avg:59.88ms
step:190/2330 train_time:11377ms step_avg:59.88ms
step:191/2330 train_time:11436ms step_avg:59.87ms
step:192/2330 train_time:11497ms step_avg:59.88ms
step:193/2330 train_time:11555ms step_avg:59.87ms
step:194/2330 train_time:11616ms step_avg:59.88ms
step:195/2330 train_time:11675ms step_avg:59.87ms
step:196/2330 train_time:11736ms step_avg:59.88ms
step:197/2330 train_time:11795ms step_avg:59.87ms
step:198/2330 train_time:11856ms step_avg:59.88ms
step:199/2330 train_time:11914ms step_avg:59.87ms
step:200/2330 train_time:11975ms step_avg:59.87ms
step:201/2330 train_time:12034ms step_avg:59.87ms
step:202/2330 train_time:12095ms step_avg:59.87ms
step:203/2330 train_time:12154ms step_avg:59.87ms
step:204/2330 train_time:12214ms step_avg:59.87ms
step:205/2330 train_time:12273ms step_avg:59.87ms
step:206/2330 train_time:12333ms step_avg:59.87ms
step:207/2330 train_time:12392ms step_avg:59.86ms
step:208/2330 train_time:12453ms step_avg:59.87ms
step:209/2330 train_time:12512ms step_avg:59.86ms
step:210/2330 train_time:12572ms step_avg:59.87ms
step:211/2330 train_time:12630ms step_avg:59.86ms
step:212/2330 train_time:12691ms step_avg:59.86ms
step:213/2330 train_time:12749ms step_avg:59.86ms
step:214/2330 train_time:12810ms step_avg:59.86ms
step:215/2330 train_time:12868ms step_avg:59.85ms
step:216/2330 train_time:12928ms step_avg:59.85ms
step:217/2330 train_time:12987ms step_avg:59.85ms
step:218/2330 train_time:13048ms step_avg:59.85ms
step:219/2330 train_time:13106ms step_avg:59.85ms
step:220/2330 train_time:13167ms step_avg:59.85ms
step:221/2330 train_time:13225ms step_avg:59.84ms
step:222/2330 train_time:13285ms step_avg:59.84ms
step:223/2330 train_time:13344ms step_avg:59.84ms
step:224/2330 train_time:13405ms step_avg:59.84ms
step:225/2330 train_time:13463ms step_avg:59.84ms
step:226/2330 train_time:13524ms step_avg:59.84ms
step:227/2330 train_time:13582ms step_avg:59.83ms
step:228/2330 train_time:13642ms step_avg:59.83ms
step:229/2330 train_time:13701ms step_avg:59.83ms
step:230/2330 train_time:13762ms step_avg:59.83ms
step:231/2330 train_time:13820ms step_avg:59.83ms
step:232/2330 train_time:13881ms step_avg:59.83ms
step:233/2330 train_time:13941ms step_avg:59.83ms
step:234/2330 train_time:14001ms step_avg:59.83ms
step:235/2330 train_time:14060ms step_avg:59.83ms
step:236/2330 train_time:14120ms step_avg:59.83ms
step:237/2330 train_time:14179ms step_avg:59.83ms
step:238/2330 train_time:14240ms step_avg:59.83ms
step:239/2330 train_time:14299ms step_avg:59.83ms
step:240/2330 train_time:14360ms step_avg:59.83ms
step:241/2330 train_time:14418ms step_avg:59.83ms
step:242/2330 train_time:14479ms step_avg:59.83ms
step:243/2330 train_time:14537ms step_avg:59.83ms
step:244/2330 train_time:14598ms step_avg:59.83ms
step:245/2330 train_time:14656ms step_avg:59.82ms
step:246/2330 train_time:14717ms step_avg:59.83ms
step:247/2330 train_time:14775ms step_avg:59.82ms
step:248/2330 train_time:14836ms step_avg:59.82ms
step:249/2330 train_time:14895ms step_avg:59.82ms
step:250/2330 train_time:14956ms step_avg:59.82ms
step:250/2330 val_loss:4.0864 train_time:15019ms step_avg:60.08ms
step:251/2330 train_time:15042ms step_avg:59.93ms
step:252/2330 train_time:15079ms step_avg:59.84ms
step:253/2330 train_time:15144ms step_avg:59.86ms
step:254/2330 train_time:15207ms step_avg:59.87ms
step:255/2330 train_time:15266ms step_avg:59.87ms
step:256/2330 train_time:15327ms step_avg:59.87ms
step:257/2330 train_time:15385ms step_avg:59.86ms
step:258/2330 train_time:15445ms step_avg:59.86ms
step:259/2330 train_time:15503ms step_avg:59.86ms
step:260/2330 train_time:15563ms step_avg:59.86ms
step:261/2330 train_time:15621ms step_avg:59.85ms
step:262/2330 train_time:15681ms step_avg:59.85ms
step:263/2330 train_time:15739ms step_avg:59.84ms
step:264/2330 train_time:15799ms step_avg:59.84ms
step:265/2330 train_time:15856ms step_avg:59.83ms
step:266/2330 train_time:15917ms step_avg:59.84ms
step:267/2330 train_time:15976ms step_avg:59.84ms
step:268/2330 train_time:16037ms step_avg:59.84ms
step:269/2330 train_time:16097ms step_avg:59.84ms
step:270/2330 train_time:16160ms step_avg:59.85ms
step:271/2330 train_time:16220ms step_avg:59.85ms
step:272/2330 train_time:16282ms step_avg:59.86ms
step:273/2330 train_time:16342ms step_avg:59.86ms
step:274/2330 train_time:16403ms step_avg:59.86ms
step:275/2330 train_time:16461ms step_avg:59.86ms
step:276/2330 train_time:16522ms step_avg:59.86ms
step:277/2330 train_time:16580ms step_avg:59.85ms
step:278/2330 train_time:16640ms step_avg:59.86ms
step:279/2330 train_time:16698ms step_avg:59.85ms
step:280/2330 train_time:16758ms step_avg:59.85ms
step:281/2330 train_time:16816ms step_avg:59.84ms
step:282/2330 train_time:16877ms step_avg:59.85ms
step:283/2330 train_time:16935ms step_avg:59.84ms
step:284/2330 train_time:16996ms step_avg:59.85ms
step:285/2330 train_time:17055ms step_avg:59.84ms
step:286/2330 train_time:17116ms step_avg:59.85ms
step:287/2330 train_time:17176ms step_avg:59.85ms
step:288/2330 train_time:17237ms step_avg:59.85ms
step:289/2330 train_time:17296ms step_avg:59.85ms
step:290/2330 train_time:17357ms step_avg:59.85ms
step:291/2330 train_time:17416ms step_avg:59.85ms
step:292/2330 train_time:17477ms step_avg:59.85ms
step:293/2330 train_time:17535ms step_avg:59.85ms
step:294/2330 train_time:17595ms step_avg:59.85ms
step:295/2330 train_time:17653ms step_avg:59.84ms
step:296/2330 train_time:17713ms step_avg:59.84ms
step:297/2330 train_time:17771ms step_avg:59.84ms
step:298/2330 train_time:17832ms step_avg:59.84ms
step:299/2330 train_time:17889ms step_avg:59.83ms
step:300/2330 train_time:17950ms step_avg:59.83ms
step:301/2330 train_time:18008ms step_avg:59.83ms
step:302/2330 train_time:18068ms step_avg:59.83ms
step:303/2330 train_time:18127ms step_avg:59.83ms
step:304/2330 train_time:18188ms step_avg:59.83ms
step:305/2330 train_time:18247ms step_avg:59.83ms
step:306/2330 train_time:18307ms step_avg:59.83ms
step:307/2330 train_time:18366ms step_avg:59.82ms
step:308/2330 train_time:18426ms step_avg:59.82ms
step:309/2330 train_time:18484ms step_avg:59.82ms
step:310/2330 train_time:18544ms step_avg:59.82ms
step:311/2330 train_time:18603ms step_avg:59.82ms
step:312/2330 train_time:18663ms step_avg:59.82ms
step:313/2330 train_time:18721ms step_avg:59.81ms
step:314/2330 train_time:18782ms step_avg:59.82ms
step:315/2330 train_time:18840ms step_avg:59.81ms
step:316/2330 train_time:18901ms step_avg:59.81ms
step:317/2330 train_time:18960ms step_avg:59.81ms
step:318/2330 train_time:19021ms step_avg:59.81ms
step:319/2330 train_time:19080ms step_avg:59.81ms
step:320/2330 train_time:19141ms step_avg:59.82ms
step:321/2330 train_time:19200ms step_avg:59.81ms
step:322/2330 train_time:19261ms step_avg:59.82ms
step:323/2330 train_time:19320ms step_avg:59.81ms
step:324/2330 train_time:19381ms step_avg:59.82ms
step:325/2330 train_time:19440ms step_avg:59.82ms
step:326/2330 train_time:19501ms step_avg:59.82ms
step:327/2330 train_time:19560ms step_avg:59.82ms
step:328/2330 train_time:19621ms step_avg:59.82ms
step:329/2330 train_time:19679ms step_avg:59.82ms
step:330/2330 train_time:19740ms step_avg:59.82ms
step:331/2330 train_time:19798ms step_avg:59.81ms
step:332/2330 train_time:19859ms step_avg:59.82ms
step:333/2330 train_time:19918ms step_avg:59.81ms
step:334/2330 train_time:19979ms step_avg:59.82ms
step:335/2330 train_time:20038ms step_avg:59.82ms
step:336/2330 train_time:20099ms step_avg:59.82ms
step:337/2330 train_time:20157ms step_avg:59.81ms
step:338/2330 train_time:20218ms step_avg:59.82ms
step:339/2330 train_time:20277ms step_avg:59.81ms
step:340/2330 train_time:20338ms step_avg:59.82ms
step:341/2330 train_time:20396ms step_avg:59.81ms
step:342/2330 train_time:20457ms step_avg:59.82ms
step:343/2330 train_time:20515ms step_avg:59.81ms
step:344/2330 train_time:20575ms step_avg:59.81ms
step:345/2330 train_time:20634ms step_avg:59.81ms
step:346/2330 train_time:20694ms step_avg:59.81ms
step:347/2330 train_time:20752ms step_avg:59.80ms
step:348/2330 train_time:20812ms step_avg:59.81ms
step:349/2330 train_time:20871ms step_avg:59.80ms
step:350/2330 train_time:20933ms step_avg:59.81ms
step:351/2330 train_time:20991ms step_avg:59.80ms
step:352/2330 train_time:21052ms step_avg:59.81ms
step:353/2330 train_time:21109ms step_avg:59.80ms
step:354/2330 train_time:21170ms step_avg:59.80ms
step:355/2330 train_time:21229ms step_avg:59.80ms
step:356/2330 train_time:21289ms step_avg:59.80ms
step:357/2330 train_time:21348ms step_avg:59.80ms
step:358/2330 train_time:21408ms step_avg:59.80ms
step:359/2330 train_time:21466ms step_avg:59.79ms
step:360/2330 train_time:21527ms step_avg:59.80ms
step:361/2330 train_time:21585ms step_avg:59.79ms
step:362/2330 train_time:21646ms step_avg:59.79ms
step:363/2330 train_time:21704ms step_avg:59.79ms
step:364/2330 train_time:21765ms step_avg:59.79ms
step:365/2330 train_time:21824ms step_avg:59.79ms
step:366/2330 train_time:21885ms step_avg:59.79ms
step:367/2330 train_time:21943ms step_avg:59.79ms
step:368/2330 train_time:22004ms step_avg:59.79ms
step:369/2330 train_time:22063ms step_avg:59.79ms
step:370/2330 train_time:22124ms step_avg:59.80ms
step:371/2330 train_time:22183ms step_avg:59.79ms
step:372/2330 train_time:22244ms step_avg:59.80ms
step:373/2330 train_time:22302ms step_avg:59.79ms
step:374/2330 train_time:22362ms step_avg:59.79ms
step:375/2330 train_time:22421ms step_avg:59.79ms
step:376/2330 train_time:22483ms step_avg:59.79ms
step:377/2330 train_time:22541ms step_avg:59.79ms
step:378/2330 train_time:22603ms step_avg:59.80ms
step:379/2330 train_time:22661ms step_avg:59.79ms
step:380/2330 train_time:22722ms step_avg:59.79ms
step:381/2330 train_time:22780ms step_avg:59.79ms
step:382/2330 train_time:22841ms step_avg:59.79ms
step:383/2330 train_time:22900ms step_avg:59.79ms
step:384/2330 train_time:22961ms step_avg:59.79ms
step:385/2330 train_time:23020ms step_avg:59.79ms
step:386/2330 train_time:23082ms step_avg:59.80ms
step:387/2330 train_time:23141ms step_avg:59.80ms
step:388/2330 train_time:23201ms step_avg:59.80ms
step:389/2330 train_time:23260ms step_avg:59.79ms
step:390/2330 train_time:23321ms step_avg:59.80ms
step:391/2330 train_time:23380ms step_avg:59.79ms
step:392/2330 train_time:23440ms step_avg:59.80ms
step:393/2330 train_time:23499ms step_avg:59.79ms
step:394/2330 train_time:23559ms step_avg:59.79ms
step:395/2330 train_time:23617ms step_avg:59.79ms
step:396/2330 train_time:23678ms step_avg:59.79ms
step:397/2330 train_time:23737ms step_avg:59.79ms
step:398/2330 train_time:23797ms step_avg:59.79ms
step:399/2330 train_time:23856ms step_avg:59.79ms
step:400/2330 train_time:23916ms step_avg:59.79ms
step:401/2330 train_time:23975ms step_avg:59.79ms
step:402/2330 train_time:24036ms step_avg:59.79ms
step:403/2330 train_time:24095ms step_avg:59.79ms
step:404/2330 train_time:24155ms step_avg:59.79ms
step:405/2330 train_time:24214ms step_avg:59.79ms
step:406/2330 train_time:24274ms step_avg:59.79ms
step:407/2330 train_time:24333ms step_avg:59.79ms
step:408/2330 train_time:24394ms step_avg:59.79ms
step:409/2330 train_time:24453ms step_avg:59.79ms
step:410/2330 train_time:24513ms step_avg:59.79ms
step:411/2330 train_time:24572ms step_avg:59.79ms
step:412/2330 train_time:24632ms step_avg:59.79ms
step:413/2330 train_time:24691ms step_avg:59.78ms
step:414/2330 train_time:24751ms step_avg:59.79ms
step:415/2330 train_time:24810ms step_avg:59.78ms
step:416/2330 train_time:24870ms step_avg:59.78ms
step:417/2330 train_time:24929ms step_avg:59.78ms
step:418/2330 train_time:24989ms step_avg:59.78ms
step:419/2330 train_time:25048ms step_avg:59.78ms
step:420/2330 train_time:25107ms step_avg:59.78ms
step:421/2330 train_time:25166ms step_avg:59.78ms
step:422/2330 train_time:25226ms step_avg:59.78ms
step:423/2330 train_time:25285ms step_avg:59.78ms
step:424/2330 train_time:25345ms step_avg:59.78ms
step:425/2330 train_time:25404ms step_avg:59.77ms
step:426/2330 train_time:25465ms step_avg:59.78ms
step:427/2330 train_time:25523ms step_avg:59.77ms
step:428/2330 train_time:25584ms step_avg:59.78ms
step:429/2330 train_time:25643ms step_avg:59.77ms
step:430/2330 train_time:25704ms step_avg:59.78ms
step:431/2330 train_time:25763ms step_avg:59.77ms
step:432/2330 train_time:25823ms step_avg:59.78ms
step:433/2330 train_time:25882ms step_avg:59.77ms
step:434/2330 train_time:25942ms step_avg:59.78ms
step:435/2330 train_time:26001ms step_avg:59.77ms
step:436/2330 train_time:26062ms step_avg:59.78ms
step:437/2330 train_time:26121ms step_avg:59.77ms
step:438/2330 train_time:26182ms step_avg:59.78ms
step:439/2330 train_time:26241ms step_avg:59.77ms
step:440/2330 train_time:26301ms step_avg:59.77ms
step:441/2330 train_time:26359ms step_avg:59.77ms
step:442/2330 train_time:26420ms step_avg:59.77ms
step:443/2330 train_time:26479ms step_avg:59.77ms
step:444/2330 train_time:26539ms step_avg:59.77ms
step:445/2330 train_time:26598ms step_avg:59.77ms
step:446/2330 train_time:26659ms step_avg:59.77ms
step:447/2330 train_time:26717ms step_avg:59.77ms
step:448/2330 train_time:26779ms step_avg:59.77ms
step:449/2330 train_time:26838ms step_avg:59.77ms
step:450/2330 train_time:26899ms step_avg:59.78ms
step:451/2330 train_time:26957ms step_avg:59.77ms
step:452/2330 train_time:27018ms step_avg:59.78ms
step:453/2330 train_time:27077ms step_avg:59.77ms
step:454/2330 train_time:27138ms step_avg:59.77ms
step:455/2330 train_time:27197ms step_avg:59.77ms
step:456/2330 train_time:27257ms step_avg:59.77ms
step:457/2330 train_time:27316ms step_avg:59.77ms
step:458/2330 train_time:27377ms step_avg:59.77ms
step:459/2330 train_time:27435ms step_avg:59.77ms
step:460/2330 train_time:27496ms step_avg:59.77ms
step:461/2330 train_time:27554ms step_avg:59.77ms
step:462/2330 train_time:27615ms step_avg:59.77ms
step:463/2330 train_time:27673ms step_avg:59.77ms
step:464/2330 train_time:27734ms step_avg:59.77ms
step:465/2330 train_time:27792ms step_avg:59.77ms
step:466/2330 train_time:27853ms step_avg:59.77ms
step:467/2330 train_time:27911ms step_avg:59.77ms
step:468/2330 train_time:27972ms step_avg:59.77ms
step:469/2330 train_time:28031ms step_avg:59.77ms
step:470/2330 train_time:28091ms step_avg:59.77ms
step:471/2330 train_time:28150ms step_avg:59.77ms
step:472/2330 train_time:28210ms step_avg:59.77ms
step:473/2330 train_time:28269ms step_avg:59.76ms
step:474/2330 train_time:28329ms step_avg:59.77ms
step:475/2330 train_time:28387ms step_avg:59.76ms
step:476/2330 train_time:28448ms step_avg:59.76ms
step:477/2330 train_time:28506ms step_avg:59.76ms
step:478/2330 train_time:28566ms step_avg:59.76ms
step:479/2330 train_time:28624ms step_avg:59.76ms
step:480/2330 train_time:28685ms step_avg:59.76ms
step:481/2330 train_time:28744ms step_avg:59.76ms
step:482/2330 train_time:28804ms step_avg:59.76ms
step:483/2330 train_time:28863ms step_avg:59.76ms
step:484/2330 train_time:28924ms step_avg:59.76ms
step:485/2330 train_time:28983ms step_avg:59.76ms
step:486/2330 train_time:29043ms step_avg:59.76ms
step:487/2330 train_time:29102ms step_avg:59.76ms
step:488/2330 train_time:29163ms step_avg:59.76ms
step:489/2330 train_time:29221ms step_avg:59.76ms
step:490/2330 train_time:29283ms step_avg:59.76ms
step:491/2330 train_time:29342ms step_avg:59.76ms
step:492/2330 train_time:29402ms step_avg:59.76ms
step:493/2330 train_time:29461ms step_avg:59.76ms
step:494/2330 train_time:29521ms step_avg:59.76ms
step:495/2330 train_time:29580ms step_avg:59.76ms
step:496/2330 train_time:29640ms step_avg:59.76ms
step:497/2330 train_time:29699ms step_avg:59.76ms
step:498/2330 train_time:29760ms step_avg:59.76ms
step:499/2330 train_time:29819ms step_avg:59.76ms
step:500/2330 train_time:29880ms step_avg:59.76ms
step:500/2330 val_loss:3.8200 train_time:29943ms step_avg:59.89ms
step:501/2330 train_time:29965ms step_avg:59.81ms
step:502/2330 train_time:30002ms step_avg:59.76ms
step:503/2330 train_time:30061ms step_avg:59.76ms
step:504/2330 train_time:30125ms step_avg:59.77ms
step:505/2330 train_time:30185ms step_avg:59.77ms
step:506/2330 train_time:30245ms step_avg:59.77ms
step:507/2330 train_time:30303ms step_avg:59.77ms
step:508/2330 train_time:30363ms step_avg:59.77ms
step:509/2330 train_time:30420ms step_avg:59.77ms
step:510/2330 train_time:30480ms step_avg:59.77ms
step:511/2330 train_time:30538ms step_avg:59.76ms
step:512/2330 train_time:30598ms step_avg:59.76ms
step:513/2330 train_time:30656ms step_avg:59.76ms
step:514/2330 train_time:30716ms step_avg:59.76ms
step:515/2330 train_time:30773ms step_avg:59.75ms
step:516/2330 train_time:30833ms step_avg:59.75ms
step:517/2330 train_time:30892ms step_avg:59.75ms
step:518/2330 train_time:30953ms step_avg:59.75ms
step:519/2330 train_time:31013ms step_avg:59.75ms
step:520/2330 train_time:31075ms step_avg:59.76ms
step:521/2330 train_time:31134ms step_avg:59.76ms
step:522/2330 train_time:31195ms step_avg:59.76ms
step:523/2330 train_time:31253ms step_avg:59.76ms
step:524/2330 train_time:31315ms step_avg:59.76ms
step:525/2330 train_time:31373ms step_avg:59.76ms
step:526/2330 train_time:31433ms step_avg:59.76ms
step:527/2330 train_time:31492ms step_avg:59.76ms
step:528/2330 train_time:31552ms step_avg:59.76ms
step:529/2330 train_time:31612ms step_avg:59.76ms
step:530/2330 train_time:31671ms step_avg:59.76ms
step:531/2330 train_time:31729ms step_avg:59.75ms
step:532/2330 train_time:31790ms step_avg:59.76ms
step:533/2330 train_time:31848ms step_avg:59.75ms
step:534/2330 train_time:31909ms step_avg:59.76ms
step:535/2330 train_time:31969ms step_avg:59.75ms
step:536/2330 train_time:32030ms step_avg:59.76ms
step:537/2330 train_time:32090ms step_avg:59.76ms
step:538/2330 train_time:32151ms step_avg:59.76ms
step:539/2330 train_time:32210ms step_avg:59.76ms
step:540/2330 train_time:32271ms step_avg:59.76ms
step:541/2330 train_time:32331ms step_avg:59.76ms
step:542/2330 train_time:32392ms step_avg:59.76ms
step:543/2330 train_time:32450ms step_avg:59.76ms
step:544/2330 train_time:32511ms step_avg:59.76ms
step:545/2330 train_time:32569ms step_avg:59.76ms
step:546/2330 train_time:32630ms step_avg:59.76ms
step:547/2330 train_time:32688ms step_avg:59.76ms
step:548/2330 train_time:32748ms step_avg:59.76ms
step:549/2330 train_time:32806ms step_avg:59.76ms
step:550/2330 train_time:32867ms step_avg:59.76ms
step:551/2330 train_time:32924ms step_avg:59.75ms
step:552/2330 train_time:32985ms step_avg:59.76ms
step:553/2330 train_time:33044ms step_avg:59.75ms
step:554/2330 train_time:33104ms step_avg:59.75ms
step:555/2330 train_time:33163ms step_avg:59.75ms
step:556/2330 train_time:33224ms step_avg:59.75ms
step:557/2330 train_time:33283ms step_avg:59.75ms
step:558/2330 train_time:33343ms step_avg:59.76ms
step:559/2330 train_time:33402ms step_avg:59.75ms
step:560/2330 train_time:33462ms step_avg:59.75ms
step:561/2330 train_time:33521ms step_avg:59.75ms
step:562/2330 train_time:33582ms step_avg:59.75ms
step:563/2330 train_time:33640ms step_avg:59.75ms
step:564/2330 train_time:33700ms step_avg:59.75ms
step:565/2330 train_time:33758ms step_avg:59.75ms
step:566/2330 train_time:33818ms step_avg:59.75ms
step:567/2330 train_time:33876ms step_avg:59.75ms
step:568/2330 train_time:33936ms step_avg:59.75ms
step:569/2330 train_time:33994ms step_avg:59.74ms
step:570/2330 train_time:34054ms step_avg:59.74ms
step:571/2330 train_time:34113ms step_avg:59.74ms
step:572/2330 train_time:34174ms step_avg:59.74ms
step:573/2330 train_time:34233ms step_avg:59.74ms
step:574/2330 train_time:34293ms step_avg:59.74ms
step:575/2330 train_time:34352ms step_avg:59.74ms
step:576/2330 train_time:34413ms step_avg:59.74ms
step:577/2330 train_time:34472ms step_avg:59.74ms
step:578/2330 train_time:34533ms step_avg:59.75ms
step:579/2330 train_time:34592ms step_avg:59.74ms
step:580/2330 train_time:34652ms step_avg:59.75ms
step:581/2330 train_time:34712ms step_avg:59.74ms
step:582/2330 train_time:34772ms step_avg:59.75ms
step:583/2330 train_time:34830ms step_avg:59.74ms
step:584/2330 train_time:34891ms step_avg:59.74ms
step:585/2330 train_time:34949ms step_avg:59.74ms
step:586/2330 train_time:35010ms step_avg:59.74ms
step:587/2330 train_time:35068ms step_avg:59.74ms
step:588/2330 train_time:35129ms step_avg:59.74ms
step:589/2330 train_time:35187ms step_avg:59.74ms
step:590/2330 train_time:35248ms step_avg:59.74ms
step:591/2330 train_time:35307ms step_avg:59.74ms
step:592/2330 train_time:35368ms step_avg:59.74ms
step:593/2330 train_time:35427ms step_avg:59.74ms
step:594/2330 train_time:35488ms step_avg:59.74ms
step:595/2330 train_time:35547ms step_avg:59.74ms
step:596/2330 train_time:35608ms step_avg:59.74ms
step:597/2330 train_time:35666ms step_avg:59.74ms
step:598/2330 train_time:35727ms step_avg:59.74ms
step:599/2330 train_time:35785ms step_avg:59.74ms
step:600/2330 train_time:35846ms step_avg:59.74ms
step:601/2330 train_time:35904ms step_avg:59.74ms
step:602/2330 train_time:35964ms step_avg:59.74ms
step:603/2330 train_time:36023ms step_avg:59.74ms
step:604/2330 train_time:36083ms step_avg:59.74ms
step:605/2330 train_time:36141ms step_avg:59.74ms
step:606/2330 train_time:36202ms step_avg:59.74ms
step:607/2330 train_time:36260ms step_avg:59.74ms
step:608/2330 train_time:36320ms step_avg:59.74ms
step:609/2330 train_time:36378ms step_avg:59.73ms
step:610/2330 train_time:36439ms step_avg:59.74ms
step:611/2330 train_time:36497ms step_avg:59.73ms
step:612/2330 train_time:36558ms step_avg:59.74ms
step:613/2330 train_time:36617ms step_avg:59.73ms
step:614/2330 train_time:36677ms step_avg:59.73ms
step:615/2330 train_time:36736ms step_avg:59.73ms
step:616/2330 train_time:36796ms step_avg:59.73ms
step:617/2330 train_time:36854ms step_avg:59.73ms
step:618/2330 train_time:36915ms step_avg:59.73ms
step:619/2330 train_time:36973ms step_avg:59.73ms
step:620/2330 train_time:37033ms step_avg:59.73ms
step:621/2330 train_time:37091ms step_avg:59.73ms
step:622/2330 train_time:37152ms step_avg:59.73ms
step:623/2330 train_time:37210ms step_avg:59.73ms
step:624/2330 train_time:37270ms step_avg:59.73ms
step:625/2330 train_time:37330ms step_avg:59.73ms
step:626/2330 train_time:37391ms step_avg:59.73ms
step:627/2330 train_time:37450ms step_avg:59.73ms
step:628/2330 train_time:37512ms step_avg:59.73ms
step:629/2330 train_time:37571ms step_avg:59.73ms
step:630/2330 train_time:37631ms step_avg:59.73ms
step:631/2330 train_time:37690ms step_avg:59.73ms
step:632/2330 train_time:37751ms step_avg:59.73ms
step:633/2330 train_time:37809ms step_avg:59.73ms
step:634/2330 train_time:37870ms step_avg:59.73ms
step:635/2330 train_time:37928ms step_avg:59.73ms
step:636/2330 train_time:37989ms step_avg:59.73ms
step:637/2330 train_time:38047ms step_avg:59.73ms
step:638/2330 train_time:38108ms step_avg:59.73ms
step:639/2330 train_time:38166ms step_avg:59.73ms
step:640/2330 train_time:38227ms step_avg:59.73ms
step:641/2330 train_time:38285ms step_avg:59.73ms
step:642/2330 train_time:38346ms step_avg:59.73ms
step:643/2330 train_time:38404ms step_avg:59.73ms
step:644/2330 train_time:38465ms step_avg:59.73ms
step:645/2330 train_time:38524ms step_avg:59.73ms
step:646/2330 train_time:38585ms step_avg:59.73ms
step:647/2330 train_time:38643ms step_avg:59.73ms
step:648/2330 train_time:38704ms step_avg:59.73ms
step:649/2330 train_time:38763ms step_avg:59.73ms
step:650/2330 train_time:38824ms step_avg:59.73ms
step:651/2330 train_time:38882ms step_avg:59.73ms
step:652/2330 train_time:38943ms step_avg:59.73ms
step:653/2330 train_time:39001ms step_avg:59.73ms
step:654/2330 train_time:39061ms step_avg:59.73ms
step:655/2330 train_time:39120ms step_avg:59.72ms
step:656/2330 train_time:39181ms step_avg:59.73ms
step:657/2330 train_time:39239ms step_avg:59.72ms
step:658/2330 train_time:39300ms step_avg:59.73ms
step:659/2330 train_time:39357ms step_avg:59.72ms
step:660/2330 train_time:39418ms step_avg:59.72ms
step:661/2330 train_time:39477ms step_avg:59.72ms
step:662/2330 train_time:39537ms step_avg:59.72ms
step:663/2330 train_time:39595ms step_avg:59.72ms
step:664/2330 train_time:39655ms step_avg:59.72ms
step:665/2330 train_time:39715ms step_avg:59.72ms
step:666/2330 train_time:39775ms step_avg:59.72ms
step:667/2330 train_time:39834ms step_avg:59.72ms
step:668/2330 train_time:39894ms step_avg:59.72ms
step:669/2330 train_time:39952ms step_avg:59.72ms
step:670/2330 train_time:40013ms step_avg:59.72ms
step:671/2330 train_time:40071ms step_avg:59.72ms
step:672/2330 train_time:40132ms step_avg:59.72ms
step:673/2330 train_time:40191ms step_avg:59.72ms
step:674/2330 train_time:40251ms step_avg:59.72ms
step:675/2330 train_time:40310ms step_avg:59.72ms
step:676/2330 train_time:40371ms step_avg:59.72ms
step:677/2330 train_time:40430ms step_avg:59.72ms
step:678/2330 train_time:40490ms step_avg:59.72ms
step:679/2330 train_time:40549ms step_avg:59.72ms
step:680/2330 train_time:40611ms step_avg:59.72ms
step:681/2330 train_time:40669ms step_avg:59.72ms
step:682/2330 train_time:40731ms step_avg:59.72ms
step:683/2330 train_time:40789ms step_avg:59.72ms
step:684/2330 train_time:40850ms step_avg:59.72ms
step:685/2330 train_time:40909ms step_avg:59.72ms
step:686/2330 train_time:40969ms step_avg:59.72ms
step:687/2330 train_time:41028ms step_avg:59.72ms
step:688/2330 train_time:41089ms step_avg:59.72ms
step:689/2330 train_time:41148ms step_avg:59.72ms
step:690/2330 train_time:41208ms step_avg:59.72ms
step:691/2330 train_time:41266ms step_avg:59.72ms
step:692/2330 train_time:41328ms step_avg:59.72ms
step:693/2330 train_time:41386ms step_avg:59.72ms
step:694/2330 train_time:41447ms step_avg:59.72ms
step:695/2330 train_time:41505ms step_avg:59.72ms
step:696/2330 train_time:41565ms step_avg:59.72ms
step:697/2330 train_time:41625ms step_avg:59.72ms
step:698/2330 train_time:41686ms step_avg:59.72ms
step:699/2330 train_time:41745ms step_avg:59.72ms
step:700/2330 train_time:41805ms step_avg:59.72ms
step:701/2330 train_time:41863ms step_avg:59.72ms
step:702/2330 train_time:41924ms step_avg:59.72ms
step:703/2330 train_time:41982ms step_avg:59.72ms
step:704/2330 train_time:42043ms step_avg:59.72ms
step:705/2330 train_time:42101ms step_avg:59.72ms
step:706/2330 train_time:42161ms step_avg:59.72ms
step:707/2330 train_time:42219ms step_avg:59.72ms
step:708/2330 train_time:42280ms step_avg:59.72ms
step:709/2330 train_time:42339ms step_avg:59.72ms
step:710/2330 train_time:42399ms step_avg:59.72ms
step:711/2330 train_time:42457ms step_avg:59.71ms
step:712/2330 train_time:42518ms step_avg:59.72ms
step:713/2330 train_time:42576ms step_avg:59.71ms
step:714/2330 train_time:42636ms step_avg:59.71ms
step:715/2330 train_time:42695ms step_avg:59.71ms
step:716/2330 train_time:42754ms step_avg:59.71ms
step:717/2330 train_time:42813ms step_avg:59.71ms
step:718/2330 train_time:42874ms step_avg:59.71ms
step:719/2330 train_time:42933ms step_avg:59.71ms
step:720/2330 train_time:42993ms step_avg:59.71ms
step:721/2330 train_time:43052ms step_avg:59.71ms
step:722/2330 train_time:43113ms step_avg:59.71ms
step:723/2330 train_time:43172ms step_avg:59.71ms
step:724/2330 train_time:43233ms step_avg:59.71ms
step:725/2330 train_time:43292ms step_avg:59.71ms
step:726/2330 train_time:43352ms step_avg:59.71ms
step:727/2330 train_time:43411ms step_avg:59.71ms
step:728/2330 train_time:43472ms step_avg:59.71ms
step:729/2330 train_time:43531ms step_avg:59.71ms
step:730/2330 train_time:43592ms step_avg:59.72ms
step:731/2330 train_time:43651ms step_avg:59.71ms
step:732/2330 train_time:43712ms step_avg:59.72ms
step:733/2330 train_time:43770ms step_avg:59.71ms
step:734/2330 train_time:43831ms step_avg:59.72ms
step:735/2330 train_time:43890ms step_avg:59.71ms
step:736/2330 train_time:43950ms step_avg:59.72ms
step:737/2330 train_time:44009ms step_avg:59.71ms
step:738/2330 train_time:44070ms step_avg:59.71ms
step:739/2330 train_time:44129ms step_avg:59.71ms
step:740/2330 train_time:44189ms step_avg:59.72ms
step:741/2330 train_time:44248ms step_avg:59.71ms
step:742/2330 train_time:44309ms step_avg:59.72ms
step:743/2330 train_time:44367ms step_avg:59.71ms
step:744/2330 train_time:44429ms step_avg:59.72ms
step:745/2330 train_time:44489ms step_avg:59.72ms
step:746/2330 train_time:44549ms step_avg:59.72ms
step:747/2330 train_time:44608ms step_avg:59.72ms
step:748/2330 train_time:44668ms step_avg:59.72ms
step:749/2330 train_time:44727ms step_avg:59.72ms
step:750/2330 train_time:44788ms step_avg:59.72ms
step:750/2330 val_loss:3.6896 train_time:44851ms step_avg:59.80ms
step:751/2330 train_time:44873ms step_avg:59.75ms
step:752/2330 train_time:44911ms step_avg:59.72ms
step:753/2330 train_time:44974ms step_avg:59.73ms
step:754/2330 train_time:45037ms step_avg:59.73ms
step:755/2330 train_time:45097ms step_avg:59.73ms
step:756/2330 train_time:45157ms step_avg:59.73ms
step:757/2330 train_time:45216ms step_avg:59.73ms
step:758/2330 train_time:45276ms step_avg:59.73ms
step:759/2330 train_time:45333ms step_avg:59.73ms
step:760/2330 train_time:45393ms step_avg:59.73ms
step:761/2330 train_time:45451ms step_avg:59.73ms
step:762/2330 train_time:45511ms step_avg:59.73ms
step:763/2330 train_time:45569ms step_avg:59.72ms
step:764/2330 train_time:45630ms step_avg:59.72ms
step:765/2330 train_time:45688ms step_avg:59.72ms
step:766/2330 train_time:45748ms step_avg:59.72ms
step:767/2330 train_time:45808ms step_avg:59.72ms
step:768/2330 train_time:45871ms step_avg:59.73ms
step:769/2330 train_time:45933ms step_avg:59.73ms
step:770/2330 train_time:45996ms step_avg:59.74ms
step:771/2330 train_time:46057ms step_avg:59.74ms
step:772/2330 train_time:46119ms step_avg:59.74ms
step:773/2330 train_time:46179ms step_avg:59.74ms
step:774/2330 train_time:46240ms step_avg:59.74ms
step:775/2330 train_time:46299ms step_avg:59.74ms
step:776/2330 train_time:46360ms step_avg:59.74ms
step:777/2330 train_time:46419ms step_avg:59.74ms
step:778/2330 train_time:46480ms step_avg:59.74ms
step:779/2330 train_time:46539ms step_avg:59.74ms
step:780/2330 train_time:46599ms step_avg:59.74ms
step:781/2330 train_time:46658ms step_avg:59.74ms
step:782/2330 train_time:46720ms step_avg:59.74ms
step:783/2330 train_time:46779ms step_avg:59.74ms
step:784/2330 train_time:46841ms step_avg:59.75ms
step:785/2330 train_time:46901ms step_avg:59.75ms
step:786/2330 train_time:46963ms step_avg:59.75ms
step:787/2330 train_time:47022ms step_avg:59.75ms
step:788/2330 train_time:47084ms step_avg:59.75ms
step:789/2330 train_time:47143ms step_avg:59.75ms
step:790/2330 train_time:47204ms step_avg:59.75ms
step:791/2330 train_time:47263ms step_avg:59.75ms
step:792/2330 train_time:47325ms step_avg:59.75ms
step:793/2330 train_time:47384ms step_avg:59.75ms
step:794/2330 train_time:47445ms step_avg:59.75ms
step:795/2330 train_time:47504ms step_avg:59.75ms
step:796/2330 train_time:47566ms step_avg:59.76ms
step:797/2330 train_time:47625ms step_avg:59.76ms
step:798/2330 train_time:47686ms step_avg:59.76ms
step:799/2330 train_time:47745ms step_avg:59.76ms
step:800/2330 train_time:47807ms step_avg:59.76ms
step:801/2330 train_time:47866ms step_avg:59.76ms
step:802/2330 train_time:47928ms step_avg:59.76ms
step:803/2330 train_time:47987ms step_avg:59.76ms
step:804/2330 train_time:48049ms step_avg:59.76ms
step:805/2330 train_time:48109ms step_avg:59.76ms
step:806/2330 train_time:48171ms step_avg:59.77ms
step:807/2330 train_time:48231ms step_avg:59.77ms
step:808/2330 train_time:48293ms step_avg:59.77ms
step:809/2330 train_time:48352ms step_avg:59.77ms
step:810/2330 train_time:48414ms step_avg:59.77ms
step:811/2330 train_time:48473ms step_avg:59.77ms
step:812/2330 train_time:48535ms step_avg:59.77ms
step:813/2330 train_time:48594ms step_avg:59.77ms
step:814/2330 train_time:48655ms step_avg:59.77ms
step:815/2330 train_time:48714ms step_avg:59.77ms
step:816/2330 train_time:48775ms step_avg:59.77ms
step:817/2330 train_time:48834ms step_avg:59.77ms
step:818/2330 train_time:48896ms step_avg:59.77ms
step:819/2330 train_time:48955ms step_avg:59.77ms
step:820/2330 train_time:49017ms step_avg:59.78ms
step:821/2330 train_time:49076ms step_avg:59.78ms
step:822/2330 train_time:49137ms step_avg:59.78ms
step:823/2330 train_time:49196ms step_avg:59.78ms
step:824/2330 train_time:49257ms step_avg:59.78ms
step:825/2330 train_time:49317ms step_avg:59.78ms
step:826/2330 train_time:49379ms step_avg:59.78ms
step:827/2330 train_time:49438ms step_avg:59.78ms
step:828/2330 train_time:49499ms step_avg:59.78ms
step:829/2330 train_time:49558ms step_avg:59.78ms
step:830/2330 train_time:49620ms step_avg:59.78ms
step:831/2330 train_time:49679ms step_avg:59.78ms
step:832/2330 train_time:49740ms step_avg:59.78ms
step:833/2330 train_time:49799ms step_avg:59.78ms
step:834/2330 train_time:49860ms step_avg:59.78ms
step:835/2330 train_time:49919ms step_avg:59.78ms
step:836/2330 train_time:49980ms step_avg:59.78ms
step:837/2330 train_time:50039ms step_avg:59.78ms
step:838/2330 train_time:50100ms step_avg:59.78ms
step:839/2330 train_time:50159ms step_avg:59.78ms
step:840/2330 train_time:50221ms step_avg:59.79ms
step:841/2330 train_time:50281ms step_avg:59.79ms
step:842/2330 train_time:50342ms step_avg:59.79ms
step:843/2330 train_time:50401ms step_avg:59.79ms
step:844/2330 train_time:50463ms step_avg:59.79ms
step:845/2330 train_time:50522ms step_avg:59.79ms
step:846/2330 train_time:50583ms step_avg:59.79ms
step:847/2330 train_time:50642ms step_avg:59.79ms
step:848/2330 train_time:50703ms step_avg:59.79ms
step:849/2330 train_time:50762ms step_avg:59.79ms
step:850/2330 train_time:50823ms step_avg:59.79ms
step:851/2330 train_time:50882ms step_avg:59.79ms
step:852/2330 train_time:50943ms step_avg:59.79ms
step:853/2330 train_time:51001ms step_avg:59.79ms
step:854/2330 train_time:51062ms step_avg:59.79ms
step:855/2330 train_time:51123ms step_avg:59.79ms
step:856/2330 train_time:51184ms step_avg:59.79ms
step:857/2330 train_time:51243ms step_avg:59.79ms
step:858/2330 train_time:51304ms step_avg:59.79ms
step:859/2330 train_time:51363ms step_avg:59.79ms
step:860/2330 train_time:51424ms step_avg:59.80ms
step:861/2330 train_time:51484ms step_avg:59.80ms
step:862/2330 train_time:51545ms step_avg:59.80ms
step:863/2330 train_time:51604ms step_avg:59.80ms
step:864/2330 train_time:51665ms step_avg:59.80ms
step:865/2330 train_time:51724ms step_avg:59.80ms
step:866/2330 train_time:51786ms step_avg:59.80ms
step:867/2330 train_time:51845ms step_avg:59.80ms
step:868/2330 train_time:51906ms step_avg:59.80ms
step:869/2330 train_time:51966ms step_avg:59.80ms
step:870/2330 train_time:52027ms step_avg:59.80ms
step:871/2330 train_time:52087ms step_avg:59.80ms
step:872/2330 train_time:52149ms step_avg:59.80ms
step:873/2330 train_time:52208ms step_avg:59.80ms
step:874/2330 train_time:52270ms step_avg:59.81ms
step:875/2330 train_time:52329ms step_avg:59.80ms
step:876/2330 train_time:52391ms step_avg:59.81ms
step:877/2330 train_time:52450ms step_avg:59.81ms
step:878/2330 train_time:52511ms step_avg:59.81ms
step:879/2330 train_time:52571ms step_avg:59.81ms
step:880/2330 train_time:52633ms step_avg:59.81ms
step:881/2330 train_time:52693ms step_avg:59.81ms
step:882/2330 train_time:52755ms step_avg:59.81ms
step:883/2330 train_time:52814ms step_avg:59.81ms
step:884/2330 train_time:52875ms step_avg:59.81ms
step:885/2330 train_time:52934ms step_avg:59.81ms
step:886/2330 train_time:52995ms step_avg:59.81ms
step:887/2330 train_time:53054ms step_avg:59.81ms
step:888/2330 train_time:53115ms step_avg:59.81ms
step:889/2330 train_time:53174ms step_avg:59.81ms
step:890/2330 train_time:53235ms step_avg:59.81ms
step:891/2330 train_time:53294ms step_avg:59.81ms
step:892/2330 train_time:53356ms step_avg:59.82ms
step:893/2330 train_time:53415ms step_avg:59.82ms
step:894/2330 train_time:53476ms step_avg:59.82ms
step:895/2330 train_time:53536ms step_avg:59.82ms
step:896/2330 train_time:53597ms step_avg:59.82ms
step:897/2330 train_time:53656ms step_avg:59.82ms
step:898/2330 train_time:53717ms step_avg:59.82ms
step:899/2330 train_time:53776ms step_avg:59.82ms
step:900/2330 train_time:53837ms step_avg:59.82ms
step:901/2330 train_time:53896ms step_avg:59.82ms
step:902/2330 train_time:53957ms step_avg:59.82ms
step:903/2330 train_time:54016ms step_avg:59.82ms
step:904/2330 train_time:54077ms step_avg:59.82ms
step:905/2330 train_time:54136ms step_avg:59.82ms
step:906/2330 train_time:54197ms step_avg:59.82ms
step:907/2330 train_time:54256ms step_avg:59.82ms
step:908/2330 train_time:54317ms step_avg:59.82ms
step:909/2330 train_time:54376ms step_avg:59.82ms
step:910/2330 train_time:54438ms step_avg:59.82ms
step:911/2330 train_time:54497ms step_avg:59.82ms
step:912/2330 train_time:54558ms step_avg:59.82ms
step:913/2330 train_time:54617ms step_avg:59.82ms
step:914/2330 train_time:54678ms step_avg:59.82ms
step:915/2330 train_time:54738ms step_avg:59.82ms
step:916/2330 train_time:54799ms step_avg:59.82ms
step:917/2330 train_time:54858ms step_avg:59.82ms
step:918/2330 train_time:54919ms step_avg:59.82ms
step:919/2330 train_time:54978ms step_avg:59.82ms
step:920/2330 train_time:55039ms step_avg:59.82ms
step:921/2330 train_time:55098ms step_avg:59.82ms
step:922/2330 train_time:55159ms step_avg:59.83ms
step:923/2330 train_time:55218ms step_avg:59.82ms
step:924/2330 train_time:55279ms step_avg:59.83ms
step:925/2330 train_time:55338ms step_avg:59.82ms
step:926/2330 train_time:55399ms step_avg:59.83ms
step:927/2330 train_time:55458ms step_avg:59.83ms
step:928/2330 train_time:55519ms step_avg:59.83ms
step:929/2330 train_time:55579ms step_avg:59.83ms
step:930/2330 train_time:55640ms step_avg:59.83ms
step:931/2330 train_time:55699ms step_avg:59.83ms
step:932/2330 train_time:55760ms step_avg:59.83ms
step:933/2330 train_time:55820ms step_avg:59.83ms
step:934/2330 train_time:55881ms step_avg:59.83ms
step:935/2330 train_time:55940ms step_avg:59.83ms
step:936/2330 train_time:56000ms step_avg:59.83ms
step:937/2330 train_time:56059ms step_avg:59.83ms
step:938/2330 train_time:56120ms step_avg:59.83ms
step:939/2330 train_time:56179ms step_avg:59.83ms
step:940/2330 train_time:56240ms step_avg:59.83ms
step:941/2330 train_time:56299ms step_avg:59.83ms
step:942/2330 train_time:56360ms step_avg:59.83ms
step:943/2330 train_time:56420ms step_avg:59.83ms
step:944/2330 train_time:56481ms step_avg:59.83ms
step:945/2330 train_time:56540ms step_avg:59.83ms
step:946/2330 train_time:56601ms step_avg:59.83ms
step:947/2330 train_time:56661ms step_avg:59.83ms
step:948/2330 train_time:56722ms step_avg:59.83ms
step:949/2330 train_time:56782ms step_avg:59.83ms
step:950/2330 train_time:56843ms step_avg:59.83ms
step:951/2330 train_time:56902ms step_avg:59.83ms
step:952/2330 train_time:56964ms step_avg:59.84ms
step:953/2330 train_time:57023ms step_avg:59.84ms
step:954/2330 train_time:57084ms step_avg:59.84ms
step:955/2330 train_time:57143ms step_avg:59.84ms
step:956/2330 train_time:57204ms step_avg:59.84ms
step:957/2330 train_time:57263ms step_avg:59.84ms
step:958/2330 train_time:57325ms step_avg:59.84ms
step:959/2330 train_time:57384ms step_avg:59.84ms
step:960/2330 train_time:57445ms step_avg:59.84ms
step:961/2330 train_time:57504ms step_avg:59.84ms
step:962/2330 train_time:57566ms step_avg:59.84ms
step:963/2330 train_time:57625ms step_avg:59.84ms
step:964/2330 train_time:57687ms step_avg:59.84ms
step:965/2330 train_time:57746ms step_avg:59.84ms
step:966/2330 train_time:57808ms step_avg:59.84ms
step:967/2330 train_time:57867ms step_avg:59.84ms
step:968/2330 train_time:57929ms step_avg:59.84ms
step:969/2330 train_time:57989ms step_avg:59.84ms
step:970/2330 train_time:58050ms step_avg:59.85ms
step:971/2330 train_time:58109ms step_avg:59.84ms
step:972/2330 train_time:58170ms step_avg:59.85ms
step:973/2330 train_time:58230ms step_avg:59.85ms
step:974/2330 train_time:58291ms step_avg:59.85ms
step:975/2330 train_time:58350ms step_avg:59.85ms
step:976/2330 train_time:58412ms step_avg:59.85ms
step:977/2330 train_time:58471ms step_avg:59.85ms
step:978/2330 train_time:58533ms step_avg:59.85ms
step:979/2330 train_time:58593ms step_avg:59.85ms
step:980/2330 train_time:58654ms step_avg:59.85ms
step:981/2330 train_time:58714ms step_avg:59.85ms
step:982/2330 train_time:58775ms step_avg:59.85ms
step:983/2330 train_time:58835ms step_avg:59.85ms
step:984/2330 train_time:58896ms step_avg:59.85ms
step:985/2330 train_time:58955ms step_avg:59.85ms
step:986/2330 train_time:59016ms step_avg:59.85ms
step:987/2330 train_time:59075ms step_avg:59.85ms
step:988/2330 train_time:59136ms step_avg:59.85ms
step:989/2330 train_time:59195ms step_avg:59.85ms
step:990/2330 train_time:59256ms step_avg:59.85ms
step:991/2330 train_time:59316ms step_avg:59.85ms
step:992/2330 train_time:59377ms step_avg:59.86ms
step:993/2330 train_time:59436ms step_avg:59.85ms
step:994/2330 train_time:59496ms step_avg:59.86ms
step:995/2330 train_time:59556ms step_avg:59.85ms
step:996/2330 train_time:59617ms step_avg:59.86ms
step:997/2330 train_time:59676ms step_avg:59.86ms
step:998/2330 train_time:59737ms step_avg:59.86ms
step:999/2330 train_time:59796ms step_avg:59.86ms
step:1000/2330 train_time:59857ms step_avg:59.86ms
step:1000/2330 val_loss:3.5759 train_time:59921ms step_avg:59.92ms
step:1001/2330 train_time:59942ms step_avg:59.88ms
step:1002/2330 train_time:59980ms step_avg:59.86ms
step:1003/2330 train_time:60039ms step_avg:59.86ms
step:1004/2330 train_time:60102ms step_avg:59.86ms
step:1005/2330 train_time:60163ms step_avg:59.86ms
step:1006/2330 train_time:60225ms step_avg:59.87ms
step:1007/2330 train_time:60284ms step_avg:59.86ms
step:1008/2330 train_time:60344ms step_avg:59.87ms
step:1009/2330 train_time:60402ms step_avg:59.86ms
step:1010/2330 train_time:60462ms step_avg:59.86ms
step:1011/2330 train_time:60521ms step_avg:59.86ms
step:1012/2330 train_time:60581ms step_avg:59.86ms
step:1013/2330 train_time:60639ms step_avg:59.86ms
step:1014/2330 train_time:60700ms step_avg:59.86ms
step:1015/2330 train_time:60758ms step_avg:59.86ms
step:1016/2330 train_time:60821ms step_avg:59.86ms
step:1017/2330 train_time:60886ms step_avg:59.87ms
step:1018/2330 train_time:60949ms step_avg:59.87ms
step:1019/2330 train_time:61008ms step_avg:59.87ms
step:1020/2330 train_time:61069ms step_avg:59.87ms
step:1021/2330 train_time:61128ms step_avg:59.87ms
step:1022/2330 train_time:61191ms step_avg:59.87ms
step:1023/2330 train_time:61250ms step_avg:59.87ms
step:1024/2330 train_time:61312ms step_avg:59.87ms
step:1025/2330 train_time:61371ms step_avg:59.87ms
step:1026/2330 train_time:61433ms step_avg:59.88ms
step:1027/2330 train_time:61491ms step_avg:59.87ms
step:1028/2330 train_time:61552ms step_avg:59.88ms
step:1029/2330 train_time:61612ms step_avg:59.88ms
step:1030/2330 train_time:61673ms step_avg:59.88ms
step:1031/2330 train_time:61732ms step_avg:59.88ms
step:1032/2330 train_time:61796ms step_avg:59.88ms
step:1033/2330 train_time:61857ms step_avg:59.88ms
step:1034/2330 train_time:61919ms step_avg:59.88ms
step:1035/2330 train_time:61979ms step_avg:59.88ms
step:1036/2330 train_time:62040ms step_avg:59.88ms
step:1037/2330 train_time:62100ms step_avg:59.88ms
step:1038/2330 train_time:62161ms step_avg:59.88ms
step:1039/2330 train_time:62220ms step_avg:59.88ms
step:1040/2330 train_time:62281ms step_avg:59.89ms
step:1041/2330 train_time:62341ms step_avg:59.89ms
step:1042/2330 train_time:62402ms step_avg:59.89ms
step:1043/2330 train_time:62460ms step_avg:59.89ms
step:1044/2330 train_time:62522ms step_avg:59.89ms
step:1045/2330 train_time:62581ms step_avg:59.89ms
step:1046/2330 train_time:62641ms step_avg:59.89ms
step:1047/2330 train_time:62701ms step_avg:59.89ms
step:1048/2330 train_time:62763ms step_avg:59.89ms
step:1049/2330 train_time:62822ms step_avg:59.89ms
step:1050/2330 train_time:62884ms step_avg:59.89ms
step:1051/2330 train_time:62943ms step_avg:59.89ms
step:1052/2330 train_time:63005ms step_avg:59.89ms
step:1053/2330 train_time:63065ms step_avg:59.89ms
step:1054/2330 train_time:63126ms step_avg:59.89ms
step:1055/2330 train_time:63184ms step_avg:59.89ms
step:1056/2330 train_time:63246ms step_avg:59.89ms
step:1057/2330 train_time:63305ms step_avg:59.89ms
step:1058/2330 train_time:63366ms step_avg:59.89ms
step:1059/2330 train_time:63425ms step_avg:59.89ms
step:1060/2330 train_time:63486ms step_avg:59.89ms
step:1061/2330 train_time:63545ms step_avg:59.89ms
step:1062/2330 train_time:63607ms step_avg:59.89ms
step:1063/2330 train_time:63665ms step_avg:59.89ms
step:1064/2330 train_time:63726ms step_avg:59.89ms
step:1065/2330 train_time:63785ms step_avg:59.89ms
step:1066/2330 train_time:63847ms step_avg:59.89ms
step:1067/2330 train_time:63906ms step_avg:59.89ms
step:1068/2330 train_time:63967ms step_avg:59.89ms
step:1069/2330 train_time:64026ms step_avg:59.89ms
step:1070/2330 train_time:64087ms step_avg:59.89ms
step:1071/2330 train_time:64146ms step_avg:59.89ms
step:1072/2330 train_time:64207ms step_avg:59.89ms
step:1073/2330 train_time:64266ms step_avg:59.89ms
step:1074/2330 train_time:64328ms step_avg:59.90ms
step:1075/2330 train_time:64387ms step_avg:59.89ms
step:1076/2330 train_time:64448ms step_avg:59.90ms
step:1077/2330 train_time:64507ms step_avg:59.90ms
step:1078/2330 train_time:64569ms step_avg:59.90ms
step:1079/2330 train_time:64627ms step_avg:59.90ms
step:1080/2330 train_time:64689ms step_avg:59.90ms
step:1081/2330 train_time:64748ms step_avg:59.90ms
step:1082/2330 train_time:64810ms step_avg:59.90ms
step:1083/2330 train_time:64869ms step_avg:59.90ms
step:1084/2330 train_time:64931ms step_avg:59.90ms
step:1085/2330 train_time:64991ms step_avg:59.90ms
step:1086/2330 train_time:65052ms step_avg:59.90ms
step:1087/2330 train_time:65112ms step_avg:59.90ms
step:1088/2330 train_time:65173ms step_avg:59.90ms
step:1089/2330 train_time:65232ms step_avg:59.90ms
step:1090/2330 train_time:65294ms step_avg:59.90ms
step:1091/2330 train_time:65353ms step_avg:59.90ms
step:1092/2330 train_time:65415ms step_avg:59.90ms
step:1093/2330 train_time:65475ms step_avg:59.90ms
step:1094/2330 train_time:65537ms step_avg:59.91ms
step:1095/2330 train_time:65596ms step_avg:59.91ms
step:1096/2330 train_time:65658ms step_avg:59.91ms
step:1097/2330 train_time:65718ms step_avg:59.91ms
step:1098/2330 train_time:65779ms step_avg:59.91ms
step:1099/2330 train_time:65838ms step_avg:59.91ms
step:1100/2330 train_time:65899ms step_avg:59.91ms
step:1101/2330 train_time:65959ms step_avg:59.91ms
step:1102/2330 train_time:66021ms step_avg:59.91ms
step:1103/2330 train_time:66080ms step_avg:59.91ms
step:1104/2330 train_time:66142ms step_avg:59.91ms
step:1105/2330 train_time:66200ms step_avg:59.91ms
step:1106/2330 train_time:66262ms step_avg:59.91ms
step:1107/2330 train_time:66321ms step_avg:59.91ms
step:1108/2330 train_time:66382ms step_avg:59.91ms
step:1109/2330 train_time:66442ms step_avg:59.91ms
step:1110/2330 train_time:66504ms step_avg:59.91ms
step:1111/2330 train_time:66563ms step_avg:59.91ms
step:1112/2330 train_time:66624ms step_avg:59.91ms
step:1113/2330 train_time:66683ms step_avg:59.91ms
step:1114/2330 train_time:66743ms step_avg:59.91ms
step:1115/2330 train_time:66803ms step_avg:59.91ms
step:1116/2330 train_time:66864ms step_avg:59.91ms
step:1117/2330 train_time:66923ms step_avg:59.91ms
step:1118/2330 train_time:66984ms step_avg:59.91ms
step:1119/2330 train_time:67044ms step_avg:59.91ms
step:1120/2330 train_time:67105ms step_avg:59.92ms
step:1121/2330 train_time:67164ms step_avg:59.91ms
step:1122/2330 train_time:67225ms step_avg:59.92ms
step:1123/2330 train_time:67284ms step_avg:59.91ms
step:1124/2330 train_time:67346ms step_avg:59.92ms
step:1125/2330 train_time:67405ms step_avg:59.92ms
step:1126/2330 train_time:67466ms step_avg:59.92ms
step:1127/2330 train_time:67525ms step_avg:59.92ms
step:1128/2330 train_time:67587ms step_avg:59.92ms
step:1129/2330 train_time:67646ms step_avg:59.92ms
step:1130/2330 train_time:67707ms step_avg:59.92ms
step:1131/2330 train_time:67766ms step_avg:59.92ms
step:1132/2330 train_time:67827ms step_avg:59.92ms
step:1133/2330 train_time:67885ms step_avg:59.92ms
step:1134/2330 train_time:67947ms step_avg:59.92ms
step:1135/2330 train_time:68006ms step_avg:59.92ms
step:1136/2330 train_time:68067ms step_avg:59.92ms
step:1137/2330 train_time:68126ms step_avg:59.92ms
step:1138/2330 train_time:68187ms step_avg:59.92ms
step:1139/2330 train_time:68247ms step_avg:59.92ms
step:1140/2330 train_time:68308ms step_avg:59.92ms
step:1141/2330 train_time:68368ms step_avg:59.92ms
step:1142/2330 train_time:68429ms step_avg:59.92ms
step:1143/2330 train_time:68488ms step_avg:59.92ms
step:1144/2330 train_time:68549ms step_avg:59.92ms
step:1145/2330 train_time:68609ms step_avg:59.92ms
step:1146/2330 train_time:68670ms step_avg:59.92ms
step:1147/2330 train_time:68729ms step_avg:59.92ms
step:1148/2330 train_time:68791ms step_avg:59.92ms
step:1149/2330 train_time:68850ms step_avg:59.92ms
step:1150/2330 train_time:68912ms step_avg:59.92ms
step:1151/2330 train_time:68971ms step_avg:59.92ms
step:1152/2330 train_time:69033ms step_avg:59.92ms
step:1153/2330 train_time:69093ms step_avg:59.92ms
step:1154/2330 train_time:69154ms step_avg:59.93ms
step:1155/2330 train_time:69214ms step_avg:59.93ms
step:1156/2330 train_time:69276ms step_avg:59.93ms
step:1157/2330 train_time:69335ms step_avg:59.93ms
step:1158/2330 train_time:69396ms step_avg:59.93ms
step:1159/2330 train_time:69456ms step_avg:59.93ms
step:1160/2330 train_time:69517ms step_avg:59.93ms
step:1161/2330 train_time:69577ms step_avg:59.93ms
step:1162/2330 train_time:69638ms step_avg:59.93ms
step:1163/2330 train_time:69696ms step_avg:59.93ms
step:1164/2330 train_time:69758ms step_avg:59.93ms
step:1165/2330 train_time:69817ms step_avg:59.93ms
step:1166/2330 train_time:69878ms step_avg:59.93ms
step:1167/2330 train_time:69938ms step_avg:59.93ms
step:1168/2330 train_time:69999ms step_avg:59.93ms
step:1169/2330 train_time:70059ms step_avg:59.93ms
step:1170/2330 train_time:70120ms step_avg:59.93ms
step:1171/2330 train_time:70180ms step_avg:59.93ms
step:1172/2330 train_time:70241ms step_avg:59.93ms
step:1173/2330 train_time:70300ms step_avg:59.93ms
step:1174/2330 train_time:70362ms step_avg:59.93ms
step:1175/2330 train_time:70421ms step_avg:59.93ms
step:1176/2330 train_time:70482ms step_avg:59.93ms
step:1177/2330 train_time:70541ms step_avg:59.93ms
step:1178/2330 train_time:70602ms step_avg:59.93ms
step:1179/2330 train_time:70661ms step_avg:59.93ms
step:1180/2330 train_time:70722ms step_avg:59.93ms
step:1181/2330 train_time:70781ms step_avg:59.93ms
step:1182/2330 train_time:70843ms step_avg:59.93ms
step:1183/2330 train_time:70902ms step_avg:59.93ms
step:1184/2330 train_time:70963ms step_avg:59.94ms
step:1185/2330 train_time:71022ms step_avg:59.93ms
step:1186/2330 train_time:71084ms step_avg:59.94ms
step:1187/2330 train_time:71143ms step_avg:59.94ms
step:1188/2330 train_time:71205ms step_avg:59.94ms
step:1189/2330 train_time:71263ms step_avg:59.94ms
step:1190/2330 train_time:71325ms step_avg:59.94ms
step:1191/2330 train_time:71384ms step_avg:59.94ms
step:1192/2330 train_time:71446ms step_avg:59.94ms
step:1193/2330 train_time:71505ms step_avg:59.94ms
step:1194/2330 train_time:71566ms step_avg:59.94ms
step:1195/2330 train_time:71625ms step_avg:59.94ms
step:1196/2330 train_time:71687ms step_avg:59.94ms
step:1197/2330 train_time:71746ms step_avg:59.94ms
step:1198/2330 train_time:71807ms step_avg:59.94ms
step:1199/2330 train_time:71866ms step_avg:59.94ms
step:1200/2330 train_time:71928ms step_avg:59.94ms
step:1201/2330 train_time:71987ms step_avg:59.94ms
step:1202/2330 train_time:72048ms step_avg:59.94ms
step:1203/2330 train_time:72108ms step_avg:59.94ms
step:1204/2330 train_time:72169ms step_avg:59.94ms
step:1205/2330 train_time:72229ms step_avg:59.94ms
step:1206/2330 train_time:72291ms step_avg:59.94ms
step:1207/2330 train_time:72351ms step_avg:59.94ms
step:1208/2330 train_time:72412ms step_avg:59.94ms
step:1209/2330 train_time:72473ms step_avg:59.94ms
step:1210/2330 train_time:72534ms step_avg:59.95ms
step:1211/2330 train_time:72594ms step_avg:59.95ms
step:1212/2330 train_time:72655ms step_avg:59.95ms
step:1213/2330 train_time:72714ms step_avg:59.95ms
step:1214/2330 train_time:72776ms step_avg:59.95ms
step:1215/2330 train_time:72835ms step_avg:59.95ms
step:1216/2330 train_time:72897ms step_avg:59.95ms
step:1217/2330 train_time:72956ms step_avg:59.95ms
step:1218/2330 train_time:73017ms step_avg:59.95ms
step:1219/2330 train_time:73076ms step_avg:59.95ms
step:1220/2330 train_time:73138ms step_avg:59.95ms
step:1221/2330 train_time:73197ms step_avg:59.95ms
step:1222/2330 train_time:73259ms step_avg:59.95ms
step:1223/2330 train_time:73318ms step_avg:59.95ms
step:1224/2330 train_time:73379ms step_avg:59.95ms
step:1225/2330 train_time:73439ms step_avg:59.95ms
step:1226/2330 train_time:73500ms step_avg:59.95ms
step:1227/2330 train_time:73559ms step_avg:59.95ms
step:1228/2330 train_time:73620ms step_avg:59.95ms
step:1229/2330 train_time:73680ms step_avg:59.95ms
step:1230/2330 train_time:73741ms step_avg:59.95ms
step:1231/2330 train_time:73800ms step_avg:59.95ms
step:1232/2330 train_time:73861ms step_avg:59.95ms
step:1233/2330 train_time:73921ms step_avg:59.95ms
step:1234/2330 train_time:73982ms step_avg:59.95ms
step:1235/2330 train_time:74040ms step_avg:59.95ms
step:1236/2330 train_time:74101ms step_avg:59.95ms
step:1237/2330 train_time:74161ms step_avg:59.95ms
step:1238/2330 train_time:74222ms step_avg:59.95ms
step:1239/2330 train_time:74281ms step_avg:59.95ms
step:1240/2330 train_time:74342ms step_avg:59.95ms
step:1241/2330 train_time:74401ms step_avg:59.95ms
step:1242/2330 train_time:74462ms step_avg:59.95ms
step:1243/2330 train_time:74521ms step_avg:59.95ms
step:1244/2330 train_time:74583ms step_avg:59.95ms
step:1245/2330 train_time:74642ms step_avg:59.95ms
step:1246/2330 train_time:74703ms step_avg:59.95ms
step:1247/2330 train_time:74763ms step_avg:59.95ms
step:1248/2330 train_time:74824ms step_avg:59.96ms
step:1249/2330 train_time:74884ms step_avg:59.96ms
step:1250/2330 train_time:74946ms step_avg:59.96ms
step:1250/2330 val_loss:3.5179 train_time:75008ms step_avg:60.01ms
step:1251/2330 train_time:75030ms step_avg:59.98ms
step:1252/2330 train_time:75067ms step_avg:59.96ms
step:1253/2330 train_time:75129ms step_avg:59.96ms
step:1254/2330 train_time:75195ms step_avg:59.96ms
step:1255/2330 train_time:75255ms step_avg:59.96ms
step:1256/2330 train_time:75316ms step_avg:59.97ms
step:1257/2330 train_time:75375ms step_avg:59.96ms
step:1258/2330 train_time:75436ms step_avg:59.96ms
step:1259/2330 train_time:75494ms step_avg:59.96ms
step:1260/2330 train_time:75554ms step_avg:59.96ms
step:1261/2330 train_time:75613ms step_avg:59.96ms
step:1262/2330 train_time:75673ms step_avg:59.96ms
step:1263/2330 train_time:75731ms step_avg:59.96ms
step:1264/2330 train_time:75792ms step_avg:59.96ms
step:1265/2330 train_time:75850ms step_avg:59.96ms
step:1266/2330 train_time:75911ms step_avg:59.96ms
step:1267/2330 train_time:75971ms step_avg:59.96ms
step:1268/2330 train_time:76034ms step_avg:59.96ms
step:1269/2330 train_time:76096ms step_avg:59.96ms
step:1270/2330 train_time:76158ms step_avg:59.97ms
step:1271/2330 train_time:76218ms step_avg:59.97ms
step:1272/2330 train_time:76280ms step_avg:59.97ms
step:1273/2330 train_time:76339ms step_avg:59.97ms
step:1274/2330 train_time:76401ms step_avg:59.97ms
step:1275/2330 train_time:76460ms step_avg:59.97ms
step:1276/2330 train_time:76521ms step_avg:59.97ms
step:1277/2330 train_time:76580ms step_avg:59.97ms
step:1278/2330 train_time:76642ms step_avg:59.97ms
step:1279/2330 train_time:76701ms step_avg:59.97ms
step:1280/2330 train_time:76762ms step_avg:59.97ms
step:1281/2330 train_time:76821ms step_avg:59.97ms
step:1282/2330 train_time:76882ms step_avg:59.97ms
step:1283/2330 train_time:76941ms step_avg:59.97ms
step:1284/2330 train_time:77002ms step_avg:59.97ms
step:1285/2330 train_time:77063ms step_avg:59.97ms
step:1286/2330 train_time:77125ms step_avg:59.97ms
step:1287/2330 train_time:77186ms step_avg:59.97ms
step:1288/2330 train_time:77248ms step_avg:59.98ms
step:1289/2330 train_time:77309ms step_avg:59.98ms
step:1290/2330 train_time:77370ms step_avg:59.98ms
step:1291/2330 train_time:77430ms step_avg:59.98ms
step:1292/2330 train_time:77491ms step_avg:59.98ms
step:1293/2330 train_time:77550ms step_avg:59.98ms
step:1294/2330 train_time:77611ms step_avg:59.98ms
step:1295/2330 train_time:77670ms step_avg:59.98ms
step:1296/2330 train_time:77730ms step_avg:59.98ms
step:1297/2330 train_time:77789ms step_avg:59.98ms
step:1298/2330 train_time:77850ms step_avg:59.98ms
step:1299/2330 train_time:77909ms step_avg:59.98ms
step:1300/2330 train_time:77970ms step_avg:59.98ms
step:1301/2330 train_time:78031ms step_avg:59.98ms
step:1302/2330 train_time:78092ms step_avg:59.98ms
step:1303/2330 train_time:78151ms step_avg:59.98ms
step:1304/2330 train_time:78213ms step_avg:59.98ms
step:1305/2330 train_time:78273ms step_avg:59.98ms
step:1306/2330 train_time:78334ms step_avg:59.98ms
step:1307/2330 train_time:78393ms step_avg:59.98ms
step:1308/2330 train_time:78454ms step_avg:59.98ms
step:1309/2330 train_time:78513ms step_avg:59.98ms
step:1310/2330 train_time:78575ms step_avg:59.98ms
step:1311/2330 train_time:78633ms step_avg:59.98ms
step:1312/2330 train_time:78694ms step_avg:59.98ms
step:1313/2330 train_time:78753ms step_avg:59.98ms
step:1314/2330 train_time:78814ms step_avg:59.98ms
step:1315/2330 train_time:78872ms step_avg:59.98ms
step:1316/2330 train_time:78933ms step_avg:59.98ms
step:1317/2330 train_time:78994ms step_avg:59.98ms
step:1318/2330 train_time:79054ms step_avg:59.98ms
step:1319/2330 train_time:79114ms step_avg:59.98ms
step:1320/2330 train_time:79176ms step_avg:59.98ms
step:1321/2330 train_time:79235ms step_avg:59.98ms
step:1322/2330 train_time:79296ms step_avg:59.98ms
step:1323/2330 train_time:79355ms step_avg:59.98ms
step:1324/2330 train_time:79416ms step_avg:59.98ms
step:1325/2330 train_time:79475ms step_avg:59.98ms
step:1326/2330 train_time:79537ms step_avg:59.98ms
step:1327/2330 train_time:79596ms step_avg:59.98ms
step:1328/2330 train_time:79657ms step_avg:59.98ms
step:1329/2330 train_time:79716ms step_avg:59.98ms
step:1330/2330 train_time:79778ms step_avg:59.98ms
step:1331/2330 train_time:79836ms step_avg:59.98ms
step:1332/2330 train_time:79897ms step_avg:59.98ms
step:1333/2330 train_time:79957ms step_avg:59.98ms
step:1334/2330 train_time:80018ms step_avg:59.98ms
step:1335/2330 train_time:80077ms step_avg:59.98ms
step:1336/2330 train_time:80139ms step_avg:59.98ms
step:1337/2330 train_time:80198ms step_avg:59.98ms
step:1338/2330 train_time:80259ms step_avg:59.98ms
step:1339/2330 train_time:80318ms step_avg:59.98ms
step:1340/2330 train_time:80380ms step_avg:59.98ms
step:1341/2330 train_time:80439ms step_avg:59.98ms
step:1342/2330 train_time:80500ms step_avg:59.99ms
step:1343/2330 train_time:80559ms step_avg:59.98ms
step:1344/2330 train_time:80620ms step_avg:59.99ms
step:1345/2330 train_time:80680ms step_avg:59.98ms
step:1346/2330 train_time:80741ms step_avg:59.99ms
step:1347/2330 train_time:80801ms step_avg:59.99ms
step:1348/2330 train_time:80862ms step_avg:59.99ms
step:1349/2330 train_time:80921ms step_avg:59.99ms
step:1350/2330 train_time:80983ms step_avg:59.99ms
step:1351/2330 train_time:81043ms step_avg:59.99ms
step:1352/2330 train_time:81104ms step_avg:59.99ms
step:1353/2330 train_time:81164ms step_avg:59.99ms
step:1354/2330 train_time:81226ms step_avg:59.99ms
step:1355/2330 train_time:81286ms step_avg:59.99ms
step:1356/2330 train_time:81348ms step_avg:59.99ms
step:1357/2330 train_time:81407ms step_avg:59.99ms
step:1358/2330 train_time:81469ms step_avg:59.99ms
step:1359/2330 train_time:81528ms step_avg:59.99ms
step:1360/2330 train_time:81589ms step_avg:59.99ms
step:1361/2330 train_time:81650ms step_avg:59.99ms
step:1362/2330 train_time:81711ms step_avg:59.99ms
step:1363/2330 train_time:81770ms step_avg:59.99ms
step:1364/2330 train_time:81830ms step_avg:59.99ms
step:1365/2330 train_time:81890ms step_avg:59.99ms
step:1366/2330 train_time:81951ms step_avg:59.99ms
step:1367/2330 train_time:82010ms step_avg:59.99ms
step:1368/2330 train_time:82070ms step_avg:59.99ms
step:1369/2330 train_time:82130ms step_avg:59.99ms
step:1370/2330 train_time:82191ms step_avg:59.99ms
step:1371/2330 train_time:82250ms step_avg:59.99ms
step:1372/2330 train_time:82312ms step_avg:59.99ms
step:1373/2330 train_time:82371ms step_avg:59.99ms
step:1374/2330 train_time:82433ms step_avg:59.99ms
step:1375/2330 train_time:82492ms step_avg:59.99ms
step:1376/2330 train_time:82553ms step_avg:59.99ms
step:1377/2330 train_time:82613ms step_avg:59.99ms
step:1378/2330 train_time:82674ms step_avg:60.00ms
step:1379/2330 train_time:82733ms step_avg:59.99ms
step:1380/2330 train_time:82794ms step_avg:60.00ms
step:1381/2330 train_time:82853ms step_avg:59.99ms
step:1382/2330 train_time:82914ms step_avg:60.00ms
step:1383/2330 train_time:82974ms step_avg:60.00ms
step:1384/2330 train_time:83035ms step_avg:60.00ms
step:1385/2330 train_time:83094ms step_avg:60.00ms
step:1386/2330 train_time:83155ms step_avg:60.00ms
step:1387/2330 train_time:83214ms step_avg:60.00ms
step:1388/2330 train_time:83276ms step_avg:60.00ms
step:1389/2330 train_time:83335ms step_avg:60.00ms
step:1390/2330 train_time:83396ms step_avg:60.00ms
step:1391/2330 train_time:83455ms step_avg:60.00ms
step:1392/2330 train_time:83516ms step_avg:60.00ms
step:1393/2330 train_time:83575ms step_avg:60.00ms
step:1394/2330 train_time:83636ms step_avg:60.00ms
step:1395/2330 train_time:83696ms step_avg:60.00ms
step:1396/2330 train_time:83757ms step_avg:60.00ms
step:1397/2330 train_time:83816ms step_avg:60.00ms
step:1398/2330 train_time:83878ms step_avg:60.00ms
step:1399/2330 train_time:83936ms step_avg:60.00ms
step:1400/2330 train_time:83998ms step_avg:60.00ms
step:1401/2330 train_time:84057ms step_avg:60.00ms
step:1402/2330 train_time:84119ms step_avg:60.00ms
step:1403/2330 train_time:84178ms step_avg:60.00ms
step:1404/2330 train_time:84239ms step_avg:60.00ms
step:1405/2330 train_time:84298ms step_avg:60.00ms
step:1406/2330 train_time:84359ms step_avg:60.00ms
step:1407/2330 train_time:84418ms step_avg:60.00ms
step:1408/2330 train_time:84480ms step_avg:60.00ms
step:1409/2330 train_time:84539ms step_avg:60.00ms
step:1410/2330 train_time:84600ms step_avg:60.00ms
step:1411/2330 train_time:84659ms step_avg:60.00ms
step:1412/2330 train_time:84721ms step_avg:60.00ms
step:1413/2330 train_time:84781ms step_avg:60.00ms
step:1414/2330 train_time:84842ms step_avg:60.00ms
step:1415/2330 train_time:84902ms step_avg:60.00ms
step:1416/2330 train_time:84963ms step_avg:60.00ms
step:1417/2330 train_time:85022ms step_avg:60.00ms
step:1418/2330 train_time:85084ms step_avg:60.00ms
step:1419/2330 train_time:85143ms step_avg:60.00ms
step:1420/2330 train_time:85205ms step_avg:60.00ms
step:1421/2330 train_time:85265ms step_avg:60.00ms
step:1422/2330 train_time:85327ms step_avg:60.00ms
step:1423/2330 train_time:85387ms step_avg:60.00ms
step:1424/2330 train_time:85448ms step_avg:60.01ms
step:1425/2330 train_time:85508ms step_avg:60.01ms
step:1426/2330 train_time:85570ms step_avg:60.01ms
step:1427/2330 train_time:85630ms step_avg:60.01ms
step:1428/2330 train_time:85691ms step_avg:60.01ms
step:1429/2330 train_time:85750ms step_avg:60.01ms
step:1430/2330 train_time:85811ms step_avg:60.01ms
step:1431/2330 train_time:85870ms step_avg:60.01ms
step:1432/2330 train_time:85931ms step_avg:60.01ms
step:1433/2330 train_time:85990ms step_avg:60.01ms
step:1434/2330 train_time:86051ms step_avg:60.01ms
step:1435/2330 train_time:86111ms step_avg:60.01ms
step:1436/2330 train_time:86172ms step_avg:60.01ms
step:1437/2330 train_time:86231ms step_avg:60.01ms
step:1438/2330 train_time:86292ms step_avg:60.01ms
step:1439/2330 train_time:86352ms step_avg:60.01ms
step:1440/2330 train_time:86414ms step_avg:60.01ms
step:1441/2330 train_time:86473ms step_avg:60.01ms
step:1442/2330 train_time:86534ms step_avg:60.01ms
step:1443/2330 train_time:86593ms step_avg:60.01ms
step:1444/2330 train_time:86654ms step_avg:60.01ms
step:1445/2330 train_time:86714ms step_avg:60.01ms
step:1446/2330 train_time:86775ms step_avg:60.01ms
step:1447/2330 train_time:86834ms step_avg:60.01ms
step:1448/2330 train_time:86895ms step_avg:60.01ms
step:1449/2330 train_time:86953ms step_avg:60.01ms
step:1450/2330 train_time:87015ms step_avg:60.01ms
step:1451/2330 train_time:87074ms step_avg:60.01ms
step:1452/2330 train_time:87135ms step_avg:60.01ms
step:1453/2330 train_time:87194ms step_avg:60.01ms
step:1454/2330 train_time:87255ms step_avg:60.01ms
step:1455/2330 train_time:87314ms step_avg:60.01ms
step:1456/2330 train_time:87376ms step_avg:60.01ms
step:1457/2330 train_time:87435ms step_avg:60.01ms
step:1458/2330 train_time:87496ms step_avg:60.01ms
step:1459/2330 train_time:87555ms step_avg:60.01ms
step:1460/2330 train_time:87617ms step_avg:60.01ms
step:1461/2330 train_time:87676ms step_avg:60.01ms
step:1462/2330 train_time:87737ms step_avg:60.01ms
step:1463/2330 train_time:87796ms step_avg:60.01ms
step:1464/2330 train_time:87857ms step_avg:60.01ms
step:1465/2330 train_time:87916ms step_avg:60.01ms
step:1466/2330 train_time:87977ms step_avg:60.01ms
step:1467/2330 train_time:88035ms step_avg:60.01ms
step:1468/2330 train_time:88097ms step_avg:60.01ms
step:1469/2330 train_time:88156ms step_avg:60.01ms
step:1470/2330 train_time:88217ms step_avg:60.01ms
step:1471/2330 train_time:88277ms step_avg:60.01ms
step:1472/2330 train_time:88339ms step_avg:60.01ms
step:1473/2330 train_time:88397ms step_avg:60.01ms
step:1474/2330 train_time:88459ms step_avg:60.01ms
step:1475/2330 train_time:88518ms step_avg:60.01ms
step:1476/2330 train_time:88580ms step_avg:60.01ms
step:1477/2330 train_time:88639ms step_avg:60.01ms
step:1478/2330 train_time:88700ms step_avg:60.01ms
step:1479/2330 train_time:88759ms step_avg:60.01ms
step:1480/2330 train_time:88820ms step_avg:60.01ms
step:1481/2330 train_time:88879ms step_avg:60.01ms
step:1482/2330 train_time:88940ms step_avg:60.01ms
step:1483/2330 train_time:89000ms step_avg:60.01ms
step:1484/2330 train_time:89062ms step_avg:60.01ms
step:1485/2330 train_time:89121ms step_avg:60.01ms
step:1486/2330 train_time:89183ms step_avg:60.02ms
step:1487/2330 train_time:89243ms step_avg:60.02ms
step:1488/2330 train_time:89304ms step_avg:60.02ms
step:1489/2330 train_time:89364ms step_avg:60.02ms
step:1490/2330 train_time:89425ms step_avg:60.02ms
step:1491/2330 train_time:89485ms step_avg:60.02ms
step:1492/2330 train_time:89547ms step_avg:60.02ms
step:1493/2330 train_time:89606ms step_avg:60.02ms
step:1494/2330 train_time:89668ms step_avg:60.02ms
step:1495/2330 train_time:89728ms step_avg:60.02ms
step:1496/2330 train_time:89789ms step_avg:60.02ms
step:1497/2330 train_time:89849ms step_avg:60.02ms
step:1498/2330 train_time:89910ms step_avg:60.02ms
step:1499/2330 train_time:89969ms step_avg:60.02ms
step:1500/2330 train_time:90031ms step_avg:60.02ms
step:1500/2330 val_loss:3.4510 train_time:90095ms step_avg:60.06ms
step:1501/2330 train_time:90117ms step_avg:60.04ms
step:1502/2330 train_time:90158ms step_avg:60.03ms
step:1503/2330 train_time:90222ms step_avg:60.03ms
step:1504/2330 train_time:90286ms step_avg:60.03ms
step:1505/2330 train_time:90346ms step_avg:60.03ms
step:1506/2330 train_time:90408ms step_avg:60.03ms
step:1507/2330 train_time:90468ms step_avg:60.03ms
step:1508/2330 train_time:90529ms step_avg:60.03ms
step:1509/2330 train_time:90588ms step_avg:60.03ms
step:1510/2330 train_time:90649ms step_avg:60.03ms
step:1511/2330 train_time:90708ms step_avg:60.03ms
step:1512/2330 train_time:90769ms step_avg:60.03ms
step:1513/2330 train_time:90827ms step_avg:60.03ms
step:1514/2330 train_time:90888ms step_avg:60.03ms
step:1515/2330 train_time:90947ms step_avg:60.03ms
step:1516/2330 train_time:91009ms step_avg:60.03ms
step:1517/2330 train_time:91069ms step_avg:60.03ms
step:1518/2330 train_time:91132ms step_avg:60.03ms
step:1519/2330 train_time:91194ms step_avg:60.04ms
step:1520/2330 train_time:91256ms step_avg:60.04ms
step:1521/2330 train_time:91315ms step_avg:60.04ms
step:1522/2330 train_time:91378ms step_avg:60.04ms
step:1523/2330 train_time:91436ms step_avg:60.04ms
step:1524/2330 train_time:91498ms step_avg:60.04ms
step:1525/2330 train_time:91556ms step_avg:60.04ms
step:1526/2330 train_time:91618ms step_avg:60.04ms
step:1527/2330 train_time:91677ms step_avg:60.04ms
step:1528/2330 train_time:91738ms step_avg:60.04ms
step:1529/2330 train_time:91797ms step_avg:60.04ms
step:1530/2330 train_time:91859ms step_avg:60.04ms
step:1531/2330 train_time:91919ms step_avg:60.04ms
step:1532/2330 train_time:91980ms step_avg:60.04ms
step:1533/2330 train_time:92040ms step_avg:60.04ms
step:1534/2330 train_time:92101ms step_avg:60.04ms
step:1535/2330 train_time:92161ms step_avg:60.04ms
step:1536/2330 train_time:92223ms step_avg:60.04ms
step:1537/2330 train_time:92283ms step_avg:60.04ms
step:1538/2330 train_time:92345ms step_avg:60.04ms
step:1539/2330 train_time:92406ms step_avg:60.04ms
step:1540/2330 train_time:92469ms step_avg:60.04ms
step:1541/2330 train_time:92529ms step_avg:60.05ms
step:1542/2330 train_time:92591ms step_avg:60.05ms
step:1543/2330 train_time:92651ms step_avg:60.05ms
step:1544/2330 train_time:92713ms step_avg:60.05ms
step:1545/2330 train_time:92772ms step_avg:60.05ms
step:1546/2330 train_time:92834ms step_avg:60.05ms
step:1547/2330 train_time:92893ms step_avg:60.05ms
step:1548/2330 train_time:92955ms step_avg:60.05ms
step:1549/2330 train_time:93015ms step_avg:60.05ms
step:1550/2330 train_time:93077ms step_avg:60.05ms
step:1551/2330 train_time:93136ms step_avg:60.05ms
step:1552/2330 train_time:93198ms step_avg:60.05ms
step:1553/2330 train_time:93258ms step_avg:60.05ms
step:1554/2330 train_time:93321ms step_avg:60.05ms
step:1555/2330 train_time:93380ms step_avg:60.05ms
step:1556/2330 train_time:93442ms step_avg:60.05ms
step:1557/2330 train_time:93501ms step_avg:60.05ms
step:1558/2330 train_time:93563ms step_avg:60.05ms
step:1559/2330 train_time:93623ms step_avg:60.05ms
step:1560/2330 train_time:93684ms step_avg:60.05ms
step:1561/2330 train_time:93744ms step_avg:60.05ms
step:1562/2330 train_time:93807ms step_avg:60.06ms
step:1563/2330 train_time:93867ms step_avg:60.06ms
step:1564/2330 train_time:93929ms step_avg:60.06ms
step:1565/2330 train_time:93989ms step_avg:60.06ms
step:1566/2330 train_time:94051ms step_avg:60.06ms
step:1567/2330 train_time:94112ms step_avg:60.06ms
step:1568/2330 train_time:94174ms step_avg:60.06ms
step:1569/2330 train_time:94233ms step_avg:60.06ms
step:1570/2330 train_time:94295ms step_avg:60.06ms
step:1571/2330 train_time:94355ms step_avg:60.06ms
step:1572/2330 train_time:94417ms step_avg:60.06ms
step:1573/2330 train_time:94478ms step_avg:60.06ms
step:1574/2330 train_time:94540ms step_avg:60.06ms
step:1575/2330 train_time:94600ms step_avg:60.06ms
step:1576/2330 train_time:94661ms step_avg:60.06ms
step:1577/2330 train_time:94721ms step_avg:60.06ms
step:1578/2330 train_time:94783ms step_avg:60.07ms
step:1579/2330 train_time:94842ms step_avg:60.06ms
step:1580/2330 train_time:94904ms step_avg:60.07ms
step:1581/2330 train_time:94963ms step_avg:60.07ms
step:1582/2330 train_time:95026ms step_avg:60.07ms
step:1583/2330 train_time:95086ms step_avg:60.07ms
step:1584/2330 train_time:95149ms step_avg:60.07ms
step:1585/2330 train_time:95209ms step_avg:60.07ms
step:1586/2330 train_time:95271ms step_avg:60.07ms
step:1587/2330 train_time:95331ms step_avg:60.07ms
step:1588/2330 train_time:95393ms step_avg:60.07ms
step:1589/2330 train_time:95453ms step_avg:60.07ms
step:1590/2330 train_time:95514ms step_avg:60.07ms
step:1591/2330 train_time:95574ms step_avg:60.07ms
step:1592/2330 train_time:95635ms step_avg:60.07ms
step:1593/2330 train_time:95695ms step_avg:60.07ms
step:1594/2330 train_time:95757ms step_avg:60.07ms
step:1595/2330 train_time:95817ms step_avg:60.07ms
step:1596/2330 train_time:95879ms step_avg:60.07ms
step:1597/2330 train_time:95939ms step_avg:60.07ms
step:1598/2330 train_time:96000ms step_avg:60.08ms
step:1599/2330 train_time:96061ms step_avg:60.08ms
step:1600/2330 train_time:96123ms step_avg:60.08ms
step:1601/2330 train_time:96182ms step_avg:60.08ms
step:1602/2330 train_time:96243ms step_avg:60.08ms
step:1603/2330 train_time:96303ms step_avg:60.08ms
step:1604/2330 train_time:96365ms step_avg:60.08ms
step:1605/2330 train_time:96426ms step_avg:60.08ms
step:1606/2330 train_time:96488ms step_avg:60.08ms
step:1607/2330 train_time:96549ms step_avg:60.08ms
step:1608/2330 train_time:96612ms step_avg:60.08ms
step:1609/2330 train_time:96672ms step_avg:60.08ms
step:1610/2330 train_time:96733ms step_avg:60.08ms
step:1611/2330 train_time:96792ms step_avg:60.08ms
step:1612/2330 train_time:96854ms step_avg:60.08ms
step:1613/2330 train_time:96914ms step_avg:60.08ms
step:1614/2330 train_time:96976ms step_avg:60.08ms
step:1615/2330 train_time:97036ms step_avg:60.08ms
step:1616/2330 train_time:97098ms step_avg:60.09ms
step:1617/2330 train_time:97158ms step_avg:60.09ms
step:1618/2330 train_time:97220ms step_avg:60.09ms
step:1619/2330 train_time:97280ms step_avg:60.09ms
step:1620/2330 train_time:97341ms step_avg:60.09ms
step:1621/2330 train_time:97401ms step_avg:60.09ms
step:1622/2330 train_time:97463ms step_avg:60.09ms
step:1623/2330 train_time:97522ms step_avg:60.09ms
step:1624/2330 train_time:97585ms step_avg:60.09ms
step:1625/2330 train_time:97645ms step_avg:60.09ms
step:1626/2330 train_time:97707ms step_avg:60.09ms
step:1627/2330 train_time:97768ms step_avg:60.09ms
step:1628/2330 train_time:97830ms step_avg:60.09ms
step:1629/2330 train_time:97890ms step_avg:60.09ms
step:1630/2330 train_time:97952ms step_avg:60.09ms
step:1631/2330 train_time:98012ms step_avg:60.09ms
step:1632/2330 train_time:98073ms step_avg:60.09ms
step:1633/2330 train_time:98133ms step_avg:60.09ms
step:1634/2330 train_time:98194ms step_avg:60.09ms
step:1635/2330 train_time:98254ms step_avg:60.09ms
step:1636/2330 train_time:98316ms step_avg:60.10ms
step:1637/2330 train_time:98375ms step_avg:60.09ms
step:1638/2330 train_time:98438ms step_avg:60.10ms
step:1639/2330 train_time:98498ms step_avg:60.10ms
step:1640/2330 train_time:98560ms step_avg:60.10ms
step:1641/2330 train_time:98620ms step_avg:60.10ms
step:1642/2330 train_time:98682ms step_avg:60.10ms
step:1643/2330 train_time:98742ms step_avg:60.10ms
step:1644/2330 train_time:98803ms step_avg:60.10ms
step:1645/2330 train_time:98863ms step_avg:60.10ms
step:1646/2330 train_time:98925ms step_avg:60.10ms
step:1647/2330 train_time:98985ms step_avg:60.10ms
step:1648/2330 train_time:99047ms step_avg:60.10ms
step:1649/2330 train_time:99108ms step_avg:60.10ms
step:1650/2330 train_time:99171ms step_avg:60.10ms
step:1651/2330 train_time:99231ms step_avg:60.10ms
step:1652/2330 train_time:99293ms step_avg:60.10ms
step:1653/2330 train_time:99353ms step_avg:60.10ms
step:1654/2330 train_time:99414ms step_avg:60.11ms
step:1655/2330 train_time:99474ms step_avg:60.11ms
step:1656/2330 train_time:99536ms step_avg:60.11ms
step:1657/2330 train_time:99595ms step_avg:60.11ms
step:1658/2330 train_time:99657ms step_avg:60.11ms
step:1659/2330 train_time:99717ms step_avg:60.11ms
step:1660/2330 train_time:99780ms step_avg:60.11ms
step:1661/2330 train_time:99840ms step_avg:60.11ms
step:1662/2330 train_time:99901ms step_avg:60.11ms
step:1663/2330 train_time:99961ms step_avg:60.11ms
step:1664/2330 train_time:100023ms step_avg:60.11ms
step:1665/2330 train_time:100082ms step_avg:60.11ms
step:1666/2330 train_time:100144ms step_avg:60.11ms
step:1667/2330 train_time:100204ms step_avg:60.11ms
step:1668/2330 train_time:100267ms step_avg:60.11ms
step:1669/2330 train_time:100328ms step_avg:60.11ms
step:1670/2330 train_time:100390ms step_avg:60.11ms
step:1671/2330 train_time:100451ms step_avg:60.11ms
step:1672/2330 train_time:100514ms step_avg:60.12ms
step:1673/2330 train_time:100574ms step_avg:60.12ms
step:1674/2330 train_time:100635ms step_avg:60.12ms
step:1675/2330 train_time:100696ms step_avg:60.12ms
step:1676/2330 train_time:100757ms step_avg:60.12ms
step:1677/2330 train_time:100817ms step_avg:60.12ms
step:1678/2330 train_time:100878ms step_avg:60.12ms
step:1679/2330 train_time:100938ms step_avg:60.12ms
step:1680/2330 train_time:101000ms step_avg:60.12ms
step:1681/2330 train_time:101059ms step_avg:60.12ms
step:1682/2330 train_time:101122ms step_avg:60.12ms
step:1683/2330 train_time:101181ms step_avg:60.12ms
step:1684/2330 train_time:101243ms step_avg:60.12ms
step:1685/2330 train_time:101302ms step_avg:60.12ms
step:1686/2330 train_time:101365ms step_avg:60.12ms
step:1687/2330 train_time:101425ms step_avg:60.12ms
step:1688/2330 train_time:101487ms step_avg:60.12ms
step:1689/2330 train_time:101548ms step_avg:60.12ms
step:1690/2330 train_time:101610ms step_avg:60.12ms
step:1691/2330 train_time:101671ms step_avg:60.12ms
step:1692/2330 train_time:101733ms step_avg:60.13ms
step:1693/2330 train_time:101793ms step_avg:60.13ms
step:1694/2330 train_time:101854ms step_avg:60.13ms
step:1695/2330 train_time:101914ms step_avg:60.13ms
step:1696/2330 train_time:101976ms step_avg:60.13ms
step:1697/2330 train_time:102035ms step_avg:60.13ms
step:1698/2330 train_time:102098ms step_avg:60.13ms
step:1699/2330 train_time:102158ms step_avg:60.13ms
step:1700/2330 train_time:102220ms step_avg:60.13ms
step:1701/2330 train_time:102280ms step_avg:60.13ms
step:1702/2330 train_time:102342ms step_avg:60.13ms
step:1703/2330 train_time:102401ms step_avg:60.13ms
step:1704/2330 train_time:102462ms step_avg:60.13ms
step:1705/2330 train_time:102522ms step_avg:60.13ms
step:1706/2330 train_time:102585ms step_avg:60.13ms
step:1707/2330 train_time:102646ms step_avg:60.13ms
step:1708/2330 train_time:102708ms step_avg:60.13ms
step:1709/2330 train_time:102769ms step_avg:60.13ms
step:1710/2330 train_time:102830ms step_avg:60.13ms
step:1711/2330 train_time:102890ms step_avg:60.13ms
step:1712/2330 train_time:102953ms step_avg:60.14ms
step:1713/2330 train_time:103013ms step_avg:60.14ms
step:1714/2330 train_time:103074ms step_avg:60.14ms
step:1715/2330 train_time:103134ms step_avg:60.14ms
step:1716/2330 train_time:103196ms step_avg:60.14ms
step:1717/2330 train_time:103256ms step_avg:60.14ms
step:1718/2330 train_time:103318ms step_avg:60.14ms
step:1719/2330 train_time:103378ms step_avg:60.14ms
step:1720/2330 train_time:103440ms step_avg:60.14ms
step:1721/2330 train_time:103500ms step_avg:60.14ms
step:1722/2330 train_time:103561ms step_avg:60.14ms
step:1723/2330 train_time:103621ms step_avg:60.14ms
step:1724/2330 train_time:103683ms step_avg:60.14ms
step:1725/2330 train_time:103743ms step_avg:60.14ms
step:1726/2330 train_time:103804ms step_avg:60.14ms
step:1727/2330 train_time:103864ms step_avg:60.14ms
step:1728/2330 train_time:103926ms step_avg:60.14ms
step:1729/2330 train_time:103986ms step_avg:60.14ms
step:1730/2330 train_time:104049ms step_avg:60.14ms
step:1731/2330 train_time:104110ms step_avg:60.14ms
step:1732/2330 train_time:104172ms step_avg:60.15ms
step:1733/2330 train_time:104232ms step_avg:60.15ms
step:1734/2330 train_time:104294ms step_avg:60.15ms
step:1735/2330 train_time:104354ms step_avg:60.15ms
step:1736/2330 train_time:104415ms step_avg:60.15ms
step:1737/2330 train_time:104475ms step_avg:60.15ms
step:1738/2330 train_time:104537ms step_avg:60.15ms
step:1739/2330 train_time:104597ms step_avg:60.15ms
step:1740/2330 train_time:104659ms step_avg:60.15ms
step:1741/2330 train_time:104719ms step_avg:60.15ms
step:1742/2330 train_time:104781ms step_avg:60.15ms
step:1743/2330 train_time:104841ms step_avg:60.15ms
step:1744/2330 train_time:104902ms step_avg:60.15ms
step:1745/2330 train_time:104962ms step_avg:60.15ms
step:1746/2330 train_time:105024ms step_avg:60.15ms
step:1747/2330 train_time:105084ms step_avg:60.15ms
step:1748/2330 train_time:105146ms step_avg:60.15ms
step:1749/2330 train_time:105207ms step_avg:60.15ms
step:1750/2330 train_time:105269ms step_avg:60.15ms
step:1750/2330 val_loss:3.3815 train_time:105334ms step_avg:60.19ms
step:1751/2330 train_time:105355ms step_avg:60.17ms
step:1752/2330 train_time:105393ms step_avg:60.16ms
step:1753/2330 train_time:105453ms step_avg:60.16ms
step:1754/2330 train_time:105516ms step_avg:60.16ms
step:1755/2330 train_time:105577ms step_avg:60.16ms
step:1756/2330 train_time:105639ms step_avg:60.16ms
step:1757/2330 train_time:105698ms step_avg:60.16ms
step:1758/2330 train_time:105759ms step_avg:60.16ms
step:1759/2330 train_time:105818ms step_avg:60.16ms
step:1760/2330 train_time:105879ms step_avg:60.16ms
step:1761/2330 train_time:105938ms step_avg:60.16ms
step:1762/2330 train_time:105999ms step_avg:60.16ms
step:1763/2330 train_time:106058ms step_avg:60.16ms
step:1764/2330 train_time:106118ms step_avg:60.16ms
step:1765/2330 train_time:106177ms step_avg:60.16ms
step:1766/2330 train_time:106241ms step_avg:60.16ms
step:1767/2330 train_time:106305ms step_avg:60.16ms
step:1768/2330 train_time:106369ms step_avg:60.16ms
step:1769/2330 train_time:106429ms step_avg:60.16ms
step:1770/2330 train_time:106491ms step_avg:60.16ms
step:1771/2330 train_time:106552ms step_avg:60.16ms
step:1772/2330 train_time:106614ms step_avg:60.17ms
step:1773/2330 train_time:106674ms step_avg:60.17ms
step:1774/2330 train_time:106735ms step_avg:60.17ms
step:1775/2330 train_time:106795ms step_avg:60.17ms
step:1776/2330 train_time:106857ms step_avg:60.17ms
step:1777/2330 train_time:106916ms step_avg:60.17ms
step:1778/2330 train_time:106978ms step_avg:60.17ms
step:1779/2330 train_time:107037ms step_avg:60.17ms
step:1780/2330 train_time:107098ms step_avg:60.17ms
step:1781/2330 train_time:107158ms step_avg:60.17ms
step:1782/2330 train_time:107220ms step_avg:60.17ms
step:1783/2330 train_time:107282ms step_avg:60.17ms
step:1784/2330 train_time:107344ms step_avg:60.17ms
step:1785/2330 train_time:107404ms step_avg:60.17ms
step:1786/2330 train_time:107466ms step_avg:60.17ms
step:1787/2330 train_time:107526ms step_avg:60.17ms
step:1788/2330 train_time:107588ms step_avg:60.17ms
step:1789/2330 train_time:107649ms step_avg:60.17ms
step:1790/2330 train_time:107709ms step_avg:60.17ms
step:1791/2330 train_time:107768ms step_avg:60.17ms
step:1792/2330 train_time:107830ms step_avg:60.17ms
step:1793/2330 train_time:107889ms step_avg:60.17ms
step:1794/2330 train_time:107951ms step_avg:60.17ms
step:1795/2330 train_time:108010ms step_avg:60.17ms
step:1796/2330 train_time:108072ms step_avg:60.17ms
step:1797/2330 train_time:108132ms step_avg:60.17ms
step:1798/2330 train_time:108196ms step_avg:60.18ms
step:1799/2330 train_time:108256ms step_avg:60.18ms
step:1800/2330 train_time:108320ms step_avg:60.18ms
step:1801/2330 train_time:108380ms step_avg:60.18ms
step:1802/2330 train_time:108443ms step_avg:60.18ms
step:1803/2330 train_time:108502ms step_avg:60.18ms
step:1804/2330 train_time:108564ms step_avg:60.18ms
step:1805/2330 train_time:108623ms step_avg:60.18ms
step:1806/2330 train_time:108685ms step_avg:60.18ms
step:1807/2330 train_time:108745ms step_avg:60.18ms
step:1808/2330 train_time:108807ms step_avg:60.18ms
step:1809/2330 train_time:108867ms step_avg:60.18ms
step:1810/2330 train_time:108928ms step_avg:60.18ms
step:1811/2330 train_time:108987ms step_avg:60.18ms
step:1812/2330 train_time:109049ms step_avg:60.18ms
step:1813/2330 train_time:109109ms step_avg:60.18ms
step:1814/2330 train_time:109170ms step_avg:60.18ms
step:1815/2330 train_time:109230ms step_avg:60.18ms
step:1816/2330 train_time:109293ms step_avg:60.18ms
step:1817/2330 train_time:109354ms step_avg:60.18ms
step:1818/2330 train_time:109416ms step_avg:60.19ms
step:1819/2330 train_time:109477ms step_avg:60.19ms
step:1820/2330 train_time:109539ms step_avg:60.19ms
step:1821/2330 train_time:109600ms step_avg:60.19ms
step:1822/2330 train_time:109662ms step_avg:60.19ms
step:1823/2330 train_time:109721ms step_avg:60.19ms
step:1824/2330 train_time:109783ms step_avg:60.19ms
step:1825/2330 train_time:109842ms step_avg:60.19ms
step:1826/2330 train_time:109904ms step_avg:60.19ms
step:1827/2330 train_time:109964ms step_avg:60.19ms
step:1828/2330 train_time:110026ms step_avg:60.19ms
step:1829/2330 train_time:110086ms step_avg:60.19ms
step:1830/2330 train_time:110148ms step_avg:60.19ms
step:1831/2330 train_time:110208ms step_avg:60.19ms
step:1832/2330 train_time:110269ms step_avg:60.19ms
step:1833/2330 train_time:110329ms step_avg:60.19ms
step:1834/2330 train_time:110390ms step_avg:60.19ms
step:1835/2330 train_time:110451ms step_avg:60.19ms
step:1836/2330 train_time:110513ms step_avg:60.19ms
step:1837/2330 train_time:110574ms step_avg:60.19ms
step:1838/2330 train_time:110636ms step_avg:60.19ms
step:1839/2330 train_time:110696ms step_avg:60.19ms
step:1840/2330 train_time:110759ms step_avg:60.19ms
step:1841/2330 train_time:110819ms step_avg:60.19ms
step:1842/2330 train_time:110880ms step_avg:60.20ms
step:1843/2330 train_time:110940ms step_avg:60.20ms
step:1844/2330 train_time:111002ms step_avg:60.20ms
step:1845/2330 train_time:111061ms step_avg:60.20ms
step:1846/2330 train_time:111123ms step_avg:60.20ms
step:1847/2330 train_time:111182ms step_avg:60.20ms
step:1848/2330 train_time:111244ms step_avg:60.20ms
step:1849/2330 train_time:111304ms step_avg:60.20ms
step:1850/2330 train_time:111366ms step_avg:60.20ms
step:1851/2330 train_time:111426ms step_avg:60.20ms
step:1852/2330 train_time:111488ms step_avg:60.20ms
step:1853/2330 train_time:111548ms step_avg:60.20ms
step:1854/2330 train_time:111609ms step_avg:60.20ms
step:1855/2330 train_time:111669ms step_avg:60.20ms
step:1856/2330 train_time:111731ms step_avg:60.20ms
step:1857/2330 train_time:111790ms step_avg:60.20ms
step:1858/2330 train_time:111853ms step_avg:60.20ms
step:1859/2330 train_time:111913ms step_avg:60.20ms
step:1860/2330 train_time:111975ms step_avg:60.20ms
step:1861/2330 train_time:112036ms step_avg:60.20ms
step:1862/2330 train_time:112098ms step_avg:60.20ms
step:1863/2330 train_time:112158ms step_avg:60.20ms
step:1864/2330 train_time:112219ms step_avg:60.20ms
step:1865/2330 train_time:112279ms step_avg:60.20ms
step:1866/2330 train_time:112341ms step_avg:60.20ms
step:1867/2330 train_time:112400ms step_avg:60.20ms
step:1868/2330 train_time:112462ms step_avg:60.20ms
step:1869/2330 train_time:112521ms step_avg:60.20ms
step:1870/2330 train_time:112583ms step_avg:60.20ms
step:1871/2330 train_time:112643ms step_avg:60.20ms
step:1872/2330 train_time:112705ms step_avg:60.21ms
step:1873/2330 train_time:112765ms step_avg:60.21ms
step:1874/2330 train_time:112826ms step_avg:60.21ms
step:1875/2330 train_time:112886ms step_avg:60.21ms
step:1876/2330 train_time:112948ms step_avg:60.21ms
step:1877/2330 train_time:113008ms step_avg:60.21ms
step:1878/2330 train_time:113069ms step_avg:60.21ms
step:1879/2330 train_time:113128ms step_avg:60.21ms
step:1880/2330 train_time:113190ms step_avg:60.21ms
step:1881/2330 train_time:113250ms step_avg:60.21ms
step:1882/2330 train_time:113312ms step_avg:60.21ms
step:1883/2330 train_time:113372ms step_avg:60.21ms
step:1884/2330 train_time:113434ms step_avg:60.21ms
step:1885/2330 train_time:113494ms step_avg:60.21ms
step:1886/2330 train_time:113557ms step_avg:60.21ms
step:1887/2330 train_time:113617ms step_avg:60.21ms
step:1888/2330 train_time:113679ms step_avg:60.21ms
step:1889/2330 train_time:113739ms step_avg:60.21ms
step:1890/2330 train_time:113800ms step_avg:60.21ms
step:1891/2330 train_time:113860ms step_avg:60.21ms
step:1892/2330 train_time:113922ms step_avg:60.21ms
step:1893/2330 train_time:113981ms step_avg:60.21ms
step:1894/2330 train_time:114044ms step_avg:60.21ms
step:1895/2330 train_time:114103ms step_avg:60.21ms
step:1896/2330 train_time:114165ms step_avg:60.21ms
step:1897/2330 train_time:114224ms step_avg:60.21ms
step:1898/2330 train_time:114286ms step_avg:60.21ms
step:1899/2330 train_time:114346ms step_avg:60.21ms
step:1900/2330 train_time:114407ms step_avg:60.21ms
step:1901/2330 train_time:114467ms step_avg:60.21ms
step:1902/2330 train_time:114528ms step_avg:60.21ms
step:1903/2330 train_time:114588ms step_avg:60.21ms
step:1904/2330 train_time:114651ms step_avg:60.22ms
step:1905/2330 train_time:114711ms step_avg:60.22ms
step:1906/2330 train_time:114773ms step_avg:60.22ms
step:1907/2330 train_time:114832ms step_avg:60.22ms
step:1908/2330 train_time:114894ms step_avg:60.22ms
step:1909/2330 train_time:114955ms step_avg:60.22ms
step:1910/2330 train_time:115017ms step_avg:60.22ms
step:1911/2330 train_time:115078ms step_avg:60.22ms
step:1912/2330 train_time:115140ms step_avg:60.22ms
step:1913/2330 train_time:115199ms step_avg:60.22ms
step:1914/2330 train_time:115261ms step_avg:60.22ms
step:1915/2330 train_time:115321ms step_avg:60.22ms
step:1916/2330 train_time:115383ms step_avg:60.22ms
step:1917/2330 train_time:115443ms step_avg:60.22ms
step:1918/2330 train_time:115504ms step_avg:60.22ms
step:1919/2330 train_time:115564ms step_avg:60.22ms
step:1920/2330 train_time:115626ms step_avg:60.22ms
step:1921/2330 train_time:115685ms step_avg:60.22ms
step:1922/2330 train_time:115747ms step_avg:60.22ms
step:1923/2330 train_time:115807ms step_avg:60.22ms
step:1924/2330 train_time:115869ms step_avg:60.22ms
step:1925/2330 train_time:115928ms step_avg:60.22ms
step:1926/2330 train_time:115989ms step_avg:60.22ms
step:1927/2330 train_time:116050ms step_avg:60.22ms
step:1928/2330 train_time:116112ms step_avg:60.22ms
step:1929/2330 train_time:116172ms step_avg:60.22ms
step:1930/2330 train_time:116234ms step_avg:60.22ms
step:1931/2330 train_time:116294ms step_avg:60.22ms
step:1932/2330 train_time:116356ms step_avg:60.23ms
step:1933/2330 train_time:116417ms step_avg:60.23ms
step:1934/2330 train_time:116479ms step_avg:60.23ms
step:1935/2330 train_time:116539ms step_avg:60.23ms
step:1936/2330 train_time:116601ms step_avg:60.23ms
step:1937/2330 train_time:116660ms step_avg:60.23ms
step:1938/2330 train_time:116721ms step_avg:60.23ms
step:1939/2330 train_time:116781ms step_avg:60.23ms
step:1940/2330 train_time:116843ms step_avg:60.23ms
step:1941/2330 train_time:116903ms step_avg:60.23ms
step:1942/2330 train_time:116965ms step_avg:60.23ms
step:1943/2330 train_time:117025ms step_avg:60.23ms
step:1944/2330 train_time:117086ms step_avg:60.23ms
step:1945/2330 train_time:117146ms step_avg:60.23ms
step:1946/2330 train_time:117208ms step_avg:60.23ms
step:1947/2330 train_time:117268ms step_avg:60.23ms
step:1948/2330 train_time:117330ms step_avg:60.23ms
step:1949/2330 train_time:117389ms step_avg:60.23ms
step:1950/2330 train_time:117451ms step_avg:60.23ms
step:1951/2330 train_time:117511ms step_avg:60.23ms
step:1952/2330 train_time:117573ms step_avg:60.23ms
step:1953/2330 train_time:117634ms step_avg:60.23ms
step:1954/2330 train_time:117696ms step_avg:60.23ms
step:1955/2330 train_time:117756ms step_avg:60.23ms
step:1956/2330 train_time:117818ms step_avg:60.23ms
step:1957/2330 train_time:117878ms step_avg:60.23ms
step:1958/2330 train_time:117940ms step_avg:60.23ms
step:1959/2330 train_time:117999ms step_avg:60.23ms
step:1960/2330 train_time:118061ms step_avg:60.24ms
step:1961/2330 train_time:118120ms step_avg:60.23ms
step:1962/2330 train_time:118182ms step_avg:60.24ms
step:1963/2330 train_time:118241ms step_avg:60.24ms
step:1964/2330 train_time:118304ms step_avg:60.24ms
step:1965/2330 train_time:118363ms step_avg:60.24ms
step:1966/2330 train_time:118426ms step_avg:60.24ms
step:1967/2330 train_time:118485ms step_avg:60.24ms
step:1968/2330 train_time:118547ms step_avg:60.24ms
step:1969/2330 train_time:118607ms step_avg:60.24ms
step:1970/2330 train_time:118669ms step_avg:60.24ms
step:1971/2330 train_time:118728ms step_avg:60.24ms
step:1972/2330 train_time:118790ms step_avg:60.24ms
step:1973/2330 train_time:118849ms step_avg:60.24ms
step:1974/2330 train_time:118912ms step_avg:60.24ms
step:1975/2330 train_time:118971ms step_avg:60.24ms
step:1976/2330 train_time:119033ms step_avg:60.24ms
step:1977/2330 train_time:119093ms step_avg:60.24ms
step:1978/2330 train_time:119155ms step_avg:60.24ms
step:1979/2330 train_time:119215ms step_avg:60.24ms
step:1980/2330 train_time:119277ms step_avg:60.24ms
step:1981/2330 train_time:119337ms step_avg:60.24ms
step:1982/2330 train_time:119400ms step_avg:60.24ms
step:1983/2330 train_time:119459ms step_avg:60.24ms
step:1984/2330 train_time:119521ms step_avg:60.24ms
step:1985/2330 train_time:119581ms step_avg:60.24ms
step:1986/2330 train_time:119643ms step_avg:60.24ms
step:1987/2330 train_time:119702ms step_avg:60.24ms
step:1988/2330 train_time:119764ms step_avg:60.24ms
step:1989/2330 train_time:119823ms step_avg:60.24ms
step:1990/2330 train_time:119885ms step_avg:60.24ms
step:1991/2330 train_time:119945ms step_avg:60.24ms
step:1992/2330 train_time:120007ms step_avg:60.24ms
step:1993/2330 train_time:120067ms step_avg:60.24ms
step:1994/2330 train_time:120129ms step_avg:60.25ms
step:1995/2330 train_time:120189ms step_avg:60.25ms
step:1996/2330 train_time:120250ms step_avg:60.25ms
step:1997/2330 train_time:120310ms step_avg:60.25ms
step:1998/2330 train_time:120372ms step_avg:60.25ms
step:1999/2330 train_time:120432ms step_avg:60.25ms
step:2000/2330 train_time:120494ms step_avg:60.25ms
step:2000/2330 val_loss:3.3304 train_time:120558ms step_avg:60.28ms
step:2001/2330 train_time:120579ms step_avg:60.26ms
step:2002/2330 train_time:120620ms step_avg:60.25ms
step:2003/2330 train_time:120685ms step_avg:60.25ms
step:2004/2330 train_time:120751ms step_avg:60.25ms
step:2005/2330 train_time:120810ms step_avg:60.25ms
step:2006/2330 train_time:120872ms step_avg:60.26ms
step:2007/2330 train_time:120931ms step_avg:60.25ms
step:2008/2330 train_time:120992ms step_avg:60.25ms
step:2009/2330 train_time:121051ms step_avg:60.25ms
step:2010/2330 train_time:121111ms step_avg:60.25ms
step:2011/2330 train_time:121171ms step_avg:60.25ms
step:2012/2330 train_time:121232ms step_avg:60.25ms
step:2013/2330 train_time:121291ms step_avg:60.25ms
step:2014/2330 train_time:121352ms step_avg:60.25ms
step:2015/2330 train_time:121411ms step_avg:60.25ms
step:2016/2330 train_time:121473ms step_avg:60.25ms
step:2017/2330 train_time:121534ms step_avg:60.25ms
step:2018/2330 train_time:121597ms step_avg:60.26ms
step:2019/2330 train_time:121658ms step_avg:60.26ms
step:2020/2330 train_time:121720ms step_avg:60.26ms
step:2021/2330 train_time:121781ms step_avg:60.26ms
step:2022/2330 train_time:121844ms step_avg:60.26ms
step:2023/2330 train_time:121904ms step_avg:60.26ms
step:2024/2330 train_time:121966ms step_avg:60.26ms
step:2025/2330 train_time:122026ms step_avg:60.26ms
step:2026/2330 train_time:122087ms step_avg:60.26ms
step:2027/2330 train_time:122146ms step_avg:60.26ms
step:2028/2330 train_time:122207ms step_avg:60.26ms
step:2029/2330 train_time:122267ms step_avg:60.26ms
step:2030/2330 train_time:122328ms step_avg:60.26ms
step:2031/2330 train_time:122387ms step_avg:60.26ms
step:2032/2330 train_time:122449ms step_avg:60.26ms
step:2033/2330 train_time:122509ms step_avg:60.26ms
step:2034/2330 train_time:122572ms step_avg:60.26ms
step:2035/2330 train_time:122632ms step_avg:60.26ms
step:2036/2330 train_time:122695ms step_avg:60.26ms
step:2037/2330 train_time:122756ms step_avg:60.26ms
step:2038/2330 train_time:122819ms step_avg:60.26ms
step:2039/2330 train_time:122879ms step_avg:60.26ms
step:2040/2330 train_time:122940ms step_avg:60.26ms
step:2041/2330 train_time:123000ms step_avg:60.26ms
step:2042/2330 train_time:123062ms step_avg:60.27ms
step:2043/2330 train_time:123122ms step_avg:60.27ms
step:2044/2330 train_time:123185ms step_avg:60.27ms
step:2045/2330 train_time:123245ms step_avg:60.27ms
step:2046/2330 train_time:123307ms step_avg:60.27ms
step:2047/2330 train_time:123367ms step_avg:60.27ms
step:2048/2330 train_time:123429ms step_avg:60.27ms
step:2049/2330 train_time:123489ms step_avg:60.27ms
step:2050/2330 train_time:123550ms step_avg:60.27ms
step:2051/2330 train_time:123610ms step_avg:60.27ms
step:2052/2330 train_time:123672ms step_avg:60.27ms
step:2053/2330 train_time:123732ms step_avg:60.27ms
step:2054/2330 train_time:123795ms step_avg:60.27ms
step:2055/2330 train_time:123855ms step_avg:60.27ms
step:2056/2330 train_time:123917ms step_avg:60.27ms
step:2057/2330 train_time:123977ms step_avg:60.27ms
step:2058/2330 train_time:124038ms step_avg:60.27ms
step:2059/2330 train_time:124097ms step_avg:60.27ms
step:2060/2330 train_time:124158ms step_avg:60.27ms
step:2061/2330 train_time:124218ms step_avg:60.27ms
step:2062/2330 train_time:124279ms step_avg:60.27ms
step:2063/2330 train_time:124340ms step_avg:60.27ms
step:2064/2330 train_time:124402ms step_avg:60.27ms
step:2065/2330 train_time:124462ms step_avg:60.27ms
step:2066/2330 train_time:124525ms step_avg:60.27ms
step:2067/2330 train_time:124585ms step_avg:60.27ms
step:2068/2330 train_time:124648ms step_avg:60.27ms
step:2069/2330 train_time:124709ms step_avg:60.27ms
step:2070/2330 train_time:124770ms step_avg:60.28ms
step:2071/2330 train_time:124830ms step_avg:60.28ms
step:2072/2330 train_time:124892ms step_avg:60.28ms
step:2073/2330 train_time:124952ms step_avg:60.28ms
step:2074/2330 train_time:125015ms step_avg:60.28ms
step:2075/2330 train_time:125074ms step_avg:60.28ms
step:2076/2330 train_time:125136ms step_avg:60.28ms
step:2077/2330 train_time:125196ms step_avg:60.28ms
step:2078/2330 train_time:125258ms step_avg:60.28ms
step:2079/2330 train_time:125318ms step_avg:60.28ms
step:2080/2330 train_time:125379ms step_avg:60.28ms
step:2081/2330 train_time:125439ms step_avg:60.28ms
step:2082/2330 train_time:125501ms step_avg:60.28ms
step:2083/2330 train_time:125561ms step_avg:60.28ms
step:2084/2330 train_time:125624ms step_avg:60.28ms
step:2085/2330 train_time:125684ms step_avg:60.28ms
step:2086/2330 train_time:125747ms step_avg:60.28ms
step:2087/2330 train_time:125808ms step_avg:60.28ms
step:2088/2330 train_time:125869ms step_avg:60.28ms
step:2089/2330 train_time:125929ms step_avg:60.28ms
step:2090/2330 train_time:125991ms step_avg:60.28ms
step:2091/2330 train_time:126051ms step_avg:60.28ms
step:2092/2330 train_time:126112ms step_avg:60.28ms
step:2093/2330 train_time:126172ms step_avg:60.28ms
step:2094/2330 train_time:126234ms step_avg:60.28ms
step:2095/2330 train_time:126293ms step_avg:60.28ms
step:2096/2330 train_time:126355ms step_avg:60.28ms
step:2097/2330 train_time:126416ms step_avg:60.28ms
step:2098/2330 train_time:126478ms step_avg:60.28ms
step:2099/2330 train_time:126537ms step_avg:60.28ms
step:2100/2330 train_time:126599ms step_avg:60.29ms
step:2101/2330 train_time:126659ms step_avg:60.29ms
step:2102/2330 train_time:126721ms step_avg:60.29ms
step:2103/2330 train_time:126782ms step_avg:60.29ms
step:2104/2330 train_time:126845ms step_avg:60.29ms
step:2105/2330 train_time:126905ms step_avg:60.29ms
step:2106/2330 train_time:126968ms step_avg:60.29ms
step:2107/2330 train_time:127028ms step_avg:60.29ms
step:2108/2330 train_time:127090ms step_avg:60.29ms
step:2109/2330 train_time:127150ms step_avg:60.29ms
step:2110/2330 train_time:127212ms step_avg:60.29ms
step:2111/2330 train_time:127271ms step_avg:60.29ms
step:2112/2330 train_time:127333ms step_avg:60.29ms
step:2113/2330 train_time:127393ms step_avg:60.29ms
step:2114/2330 train_time:127455ms step_avg:60.29ms
step:2115/2330 train_time:127515ms step_avg:60.29ms
step:2116/2330 train_time:127576ms step_avg:60.29ms
step:2117/2330 train_time:127637ms step_avg:60.29ms
step:2118/2330 train_time:127698ms step_avg:60.29ms
step:2119/2330 train_time:127758ms step_avg:60.29ms
step:2120/2330 train_time:127820ms step_avg:60.29ms
step:2121/2330 train_time:127880ms step_avg:60.29ms
step:2122/2330 train_time:127943ms step_avg:60.29ms
step:2123/2330 train_time:128003ms step_avg:60.29ms
step:2124/2330 train_time:128066ms step_avg:60.29ms
step:2125/2330 train_time:128126ms step_avg:60.29ms
step:2126/2330 train_time:128187ms step_avg:60.30ms
step:2127/2330 train_time:128247ms step_avg:60.29ms
step:2128/2330 train_time:128309ms step_avg:60.30ms
step:2129/2330 train_time:128369ms step_avg:60.30ms
step:2130/2330 train_time:128431ms step_avg:60.30ms
step:2131/2330 train_time:128490ms step_avg:60.30ms
step:2132/2330 train_time:128552ms step_avg:60.30ms
step:2133/2330 train_time:128612ms step_avg:60.30ms
step:2134/2330 train_time:128675ms step_avg:60.30ms
step:2135/2330 train_time:128734ms step_avg:60.30ms
step:2136/2330 train_time:128796ms step_avg:60.30ms
step:2137/2330 train_time:128856ms step_avg:60.30ms
step:2138/2330 train_time:128918ms step_avg:60.30ms
step:2139/2330 train_time:128977ms step_avg:60.30ms
step:2140/2330 train_time:129039ms step_avg:60.30ms
step:2141/2330 train_time:129098ms step_avg:60.30ms
step:2142/2330 train_time:129160ms step_avg:60.30ms
step:2143/2330 train_time:129221ms step_avg:60.30ms
step:2144/2330 train_time:129284ms step_avg:60.30ms
step:2145/2330 train_time:129344ms step_avg:60.30ms
step:2146/2330 train_time:129406ms step_avg:60.30ms
step:2147/2330 train_time:129466ms step_avg:60.30ms
step:2148/2330 train_time:129528ms step_avg:60.30ms
step:2149/2330 train_time:129588ms step_avg:60.30ms
step:2150/2330 train_time:129649ms step_avg:60.30ms
step:2151/2330 train_time:129709ms step_avg:60.30ms
step:2152/2330 train_time:129771ms step_avg:60.30ms
step:2153/2330 train_time:129831ms step_avg:60.30ms
step:2154/2330 train_time:129893ms step_avg:60.30ms
step:2155/2330 train_time:129953ms step_avg:60.30ms
step:2156/2330 train_time:130015ms step_avg:60.30ms
step:2157/2330 train_time:130074ms step_avg:60.30ms
step:2158/2330 train_time:130137ms step_avg:60.30ms
step:2159/2330 train_time:130197ms step_avg:60.30ms
step:2160/2330 train_time:130258ms step_avg:60.30ms
step:2161/2330 train_time:130318ms step_avg:60.30ms
step:2162/2330 train_time:130380ms step_avg:60.31ms
step:2163/2330 train_time:130440ms step_avg:60.31ms
step:2164/2330 train_time:130503ms step_avg:60.31ms
step:2165/2330 train_time:130563ms step_avg:60.31ms
step:2166/2330 train_time:130626ms step_avg:60.31ms
step:2167/2330 train_time:130687ms step_avg:60.31ms
step:2168/2330 train_time:130748ms step_avg:60.31ms
step:2169/2330 train_time:130808ms step_avg:60.31ms
step:2170/2330 train_time:130869ms step_avg:60.31ms
step:2171/2330 train_time:130929ms step_avg:60.31ms
step:2172/2330 train_time:130991ms step_avg:60.31ms
step:2173/2330 train_time:131051ms step_avg:60.31ms
step:2174/2330 train_time:131113ms step_avg:60.31ms
step:2175/2330 train_time:131173ms step_avg:60.31ms
step:2176/2330 train_time:131235ms step_avg:60.31ms
step:2177/2330 train_time:131295ms step_avg:60.31ms
step:2178/2330 train_time:131357ms step_avg:60.31ms
step:2179/2330 train_time:131417ms step_avg:60.31ms
step:2180/2330 train_time:131478ms step_avg:60.31ms
step:2181/2330 train_time:131538ms step_avg:60.31ms
step:2182/2330 train_time:131601ms step_avg:60.31ms
step:2183/2330 train_time:131661ms step_avg:60.31ms
step:2184/2330 train_time:131723ms step_avg:60.31ms
step:2185/2330 train_time:131783ms step_avg:60.31ms
step:2186/2330 train_time:131846ms step_avg:60.31ms
step:2187/2330 train_time:131906ms step_avg:60.31ms
step:2188/2330 train_time:131968ms step_avg:60.31ms
step:2189/2330 train_time:132028ms step_avg:60.31ms
step:2190/2330 train_time:132090ms step_avg:60.32ms
step:2191/2330 train_time:132150ms step_avg:60.31ms
step:2192/2330 train_time:132212ms step_avg:60.32ms
step:2193/2330 train_time:132272ms step_avg:60.32ms
step:2194/2330 train_time:132334ms step_avg:60.32ms
step:2195/2330 train_time:132394ms step_avg:60.32ms
step:2196/2330 train_time:132456ms step_avg:60.32ms
step:2197/2330 train_time:132516ms step_avg:60.32ms
step:2198/2330 train_time:132578ms step_avg:60.32ms
step:2199/2330 train_time:132637ms step_avg:60.32ms
step:2200/2330 train_time:132698ms step_avg:60.32ms
step:2201/2330 train_time:132758ms step_avg:60.32ms
step:2202/2330 train_time:132820ms step_avg:60.32ms
step:2203/2330 train_time:132880ms step_avg:60.32ms
step:2204/2330 train_time:132943ms step_avg:60.32ms
step:2205/2330 train_time:133003ms step_avg:60.32ms
step:2206/2330 train_time:133065ms step_avg:60.32ms
step:2207/2330 train_time:133126ms step_avg:60.32ms
step:2208/2330 train_time:133188ms step_avg:60.32ms
step:2209/2330 train_time:133248ms step_avg:60.32ms
step:2210/2330 train_time:133309ms step_avg:60.32ms
step:2211/2330 train_time:133369ms step_avg:60.32ms
step:2212/2330 train_time:133430ms step_avg:60.32ms
step:2213/2330 train_time:133490ms step_avg:60.32ms
step:2214/2330 train_time:133551ms step_avg:60.32ms
step:2215/2330 train_time:133612ms step_avg:60.32ms
step:2216/2330 train_time:133674ms step_avg:60.32ms
step:2217/2330 train_time:133734ms step_avg:60.32ms
step:2218/2330 train_time:133796ms step_avg:60.32ms
step:2219/2330 train_time:133856ms step_avg:60.32ms
step:2220/2330 train_time:133917ms step_avg:60.32ms
step:2221/2330 train_time:133977ms step_avg:60.32ms
step:2222/2330 train_time:134039ms step_avg:60.32ms
step:2223/2330 train_time:134099ms step_avg:60.32ms
step:2224/2330 train_time:134161ms step_avg:60.32ms
step:2225/2330 train_time:134222ms step_avg:60.32ms
step:2226/2330 train_time:134284ms step_avg:60.33ms
step:2227/2330 train_time:134344ms step_avg:60.33ms
step:2228/2330 train_time:134407ms step_avg:60.33ms
step:2229/2330 train_time:134467ms step_avg:60.33ms
step:2230/2330 train_time:134529ms step_avg:60.33ms
step:2231/2330 train_time:134588ms step_avg:60.33ms
step:2232/2330 train_time:134649ms step_avg:60.33ms
step:2233/2330 train_time:134709ms step_avg:60.33ms
step:2234/2330 train_time:134771ms step_avg:60.33ms
step:2235/2330 train_time:134831ms step_avg:60.33ms
step:2236/2330 train_time:134893ms step_avg:60.33ms
step:2237/2330 train_time:134953ms step_avg:60.33ms
step:2238/2330 train_time:135015ms step_avg:60.33ms
step:2239/2330 train_time:135075ms step_avg:60.33ms
step:2240/2330 train_time:135137ms step_avg:60.33ms
step:2241/2330 train_time:135197ms step_avg:60.33ms
step:2242/2330 train_time:135258ms step_avg:60.33ms
step:2243/2330 train_time:135318ms step_avg:60.33ms
step:2244/2330 train_time:135380ms step_avg:60.33ms
step:2245/2330 train_time:135441ms step_avg:60.33ms
step:2246/2330 train_time:135503ms step_avg:60.33ms
step:2247/2330 train_time:135563ms step_avg:60.33ms
step:2248/2330 train_time:135625ms step_avg:60.33ms
step:2249/2330 train_time:135685ms step_avg:60.33ms
step:2250/2330 train_time:135748ms step_avg:60.33ms
step:2250/2330 val_loss:3.2909 train_time:135812ms step_avg:60.36ms
step:2251/2330 train_time:135834ms step_avg:60.34ms
step:2252/2330 train_time:135873ms step_avg:60.33ms
step:2253/2330 train_time:135938ms step_avg:60.34ms
step:2254/2330 train_time:136003ms step_avg:60.34ms
step:2255/2330 train_time:136063ms step_avg:60.34ms
step:2256/2330 train_time:136125ms step_avg:60.34ms
step:2257/2330 train_time:136184ms step_avg:60.34ms
step:2258/2330 train_time:136245ms step_avg:60.34ms
step:2259/2330 train_time:136304ms step_avg:60.34ms
step:2260/2330 train_time:136365ms step_avg:60.34ms
step:2261/2330 train_time:136424ms step_avg:60.34ms
step:2262/2330 train_time:136486ms step_avg:60.34ms
step:2263/2330 train_time:136545ms step_avg:60.34ms
step:2264/2330 train_time:136606ms step_avg:60.34ms
step:2265/2330 train_time:136665ms step_avg:60.34ms
step:2266/2330 train_time:136726ms step_avg:60.34ms
step:2267/2330 train_time:136786ms step_avg:60.34ms
step:2268/2330 train_time:136849ms step_avg:60.34ms
step:2269/2330 train_time:136910ms step_avg:60.34ms
step:2270/2330 train_time:136973ms step_avg:60.34ms
step:2271/2330 train_time:137034ms step_avg:60.34ms
step:2272/2330 train_time:137096ms step_avg:60.34ms
step:2273/2330 train_time:137156ms step_avg:60.34ms
step:2274/2330 train_time:137218ms step_avg:60.34ms
step:2275/2330 train_time:137277ms step_avg:60.34ms
step:2276/2330 train_time:137338ms step_avg:60.34ms
step:2277/2330 train_time:137399ms step_avg:60.34ms
step:2278/2330 train_time:137460ms step_avg:60.34ms
step:2279/2330 train_time:137519ms step_avg:60.34ms
step:2280/2330 train_time:137580ms step_avg:60.34ms
step:2281/2330 train_time:137639ms step_avg:60.34ms
step:2282/2330 train_time:137700ms step_avg:60.34ms
step:2283/2330 train_time:137760ms step_avg:60.34ms
step:2284/2330 train_time:137823ms step_avg:60.34ms
step:2285/2330 train_time:137883ms step_avg:60.34ms
step:2286/2330 train_time:137947ms step_avg:60.34ms
step:2287/2330 train_time:138008ms step_avg:60.34ms
step:2288/2330 train_time:138069ms step_avg:60.34ms
step:2289/2330 train_time:138129ms step_avg:60.34ms
step:2290/2330 train_time:138191ms step_avg:60.35ms
step:2291/2330 train_time:138251ms step_avg:60.35ms
step:2292/2330 train_time:138314ms step_avg:60.35ms
step:2293/2330 train_time:138374ms step_avg:60.35ms
step:2294/2330 train_time:138436ms step_avg:60.35ms
step:2295/2330 train_time:138496ms step_avg:60.35ms
step:2296/2330 train_time:138558ms step_avg:60.35ms
step:2297/2330 train_time:138617ms step_avg:60.35ms
step:2298/2330 train_time:138679ms step_avg:60.35ms
step:2299/2330 train_time:138740ms step_avg:60.35ms
step:2300/2330 train_time:138801ms step_avg:60.35ms
step:2301/2330 train_time:138861ms step_avg:60.35ms
step:2302/2330 train_time:138923ms step_avg:60.35ms
step:2303/2330 train_time:138982ms step_avg:60.35ms
step:2304/2330 train_time:139045ms step_avg:60.35ms
step:2305/2330 train_time:139105ms step_avg:60.35ms
step:2306/2330 train_time:139167ms step_avg:60.35ms
step:2307/2330 train_time:139227ms step_avg:60.35ms
step:2308/2330 train_time:139289ms step_avg:60.35ms
step:2309/2330 train_time:139349ms step_avg:60.35ms
step:2310/2330 train_time:139411ms step_avg:60.35ms
step:2311/2330 train_time:139470ms step_avg:60.35ms
step:2312/2330 train_time:139532ms step_avg:60.35ms
step:2313/2330 train_time:139592ms step_avg:60.35ms
step:2314/2330 train_time:139655ms step_avg:60.35ms
step:2315/2330 train_time:139715ms step_avg:60.35ms
step:2316/2330 train_time:139777ms step_avg:60.35ms
step:2317/2330 train_time:139838ms step_avg:60.35ms
step:2318/2330 train_time:139900ms step_avg:60.35ms
step:2319/2330 train_time:139959ms step_avg:60.35ms
step:2320/2330 train_time:140021ms step_avg:60.35ms
step:2321/2330 train_time:140081ms step_avg:60.35ms
step:2322/2330 train_time:140143ms step_avg:60.35ms
step:2323/2330 train_time:140202ms step_avg:60.35ms
step:2324/2330 train_time:140265ms step_avg:60.35ms
step:2325/2330 train_time:140325ms step_avg:60.35ms
step:2326/2330 train_time:140387ms step_avg:60.36ms
step:2327/2330 train_time:140447ms step_avg:60.36ms
step:2328/2330 train_time:140508ms step_avg:60.36ms
step:2329/2330 train_time:140568ms step_avg:60.36ms
step:2330/2330 train_time:140629ms step_avg:60.36ms
step:2330/2330 val_loss:3.2781 train_time:140693ms step_avg:60.38ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
coeffs_list = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in coeffs_list:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            # x_out = self.blocks[i](x, x0, lambdas[i], attn_args)
            # x_backout += backout_lambdas[i] * (x_out-x)
            # x = x_out
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i==8:
                x_backout=x

        # backout contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda*x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 2290  # number of iterations to run
    iteration_extension = 40  # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.45  # fraction of training spent cooling down the learning rate
    momentum_cd_steps = 50  # number of iterations for muon momentum cooldown
    # evaluation and logging
    run_id: str = f"new/{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

def update_optimizer_params(step, optimizer1, optimizer2):
    # Update lr
    for group in optimizer1.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        group["lr"] = group["initial_lr"] * get_lr(step)

    # Warmup phase: gradually increase momentum from 0.85 to 0.95
    if step < 300:
        frac = step / 300
        momentum = 0.85 + frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum

    # Cooldown phase: gradually decrease momentum
    momentum_cd_start = args.num_iterations + args.iteration_extension - args.momentum_cd_steps
    if step > momentum_cd_start:
        frac = (step - momentum_cd_start) / args.momentum_cd_steps

        # Decay momentum from 0.95 to 0.85
        momentum = 0.95 - frac * (0.95 - 0.85)
        for group in optimizer2.param_groups:
            group["momentum"] = momentum

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    update_optimizer_params(step, optimizer1, optimizer2)
    # only step Adam every other step
    if step%2==0:
        optimizer2.step()
        optimizer2.zero_grad(set_to_none=True)
    else:
        for opt in optimizers:
            opt.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
    
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Sat Oct  4 06:11:34 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   28C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   28C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   40C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   28C    P0            112W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          273275      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          273276      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          273277      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          273278      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          273279      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          273280      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          273281      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          273282      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          273276      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          273277      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          273278      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          273279      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          273280      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          273281      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          273282      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:71ms step_avg:70.54ms
step:2/2330 train_time:174ms step_avg:86.97ms
step:3/2330 train_time:195ms step_avg:64.99ms
step:4/2330 train_time:230ms step_avg:57.56ms
step:5/2330 train_time:287ms step_avg:57.41ms
step:6/2330 train_time:347ms step_avg:57.84ms
step:7/2330 train_time:405ms step_avg:57.85ms
step:8/2330 train_time:465ms step_avg:58.16ms
step:9/2330 train_time:523ms step_avg:58.11ms
step:10/2330 train_time:583ms step_avg:58.34ms
step:11/2330 train_time:642ms step_avg:58.35ms
step:12/2330 train_time:702ms step_avg:58.52ms
step:13/2330 train_time:760ms step_avg:58.48ms
step:14/2330 train_time:820ms step_avg:58.61ms
step:15/2330 train_time:879ms step_avg:58.58ms
step:16/2330 train_time:939ms step_avg:58.69ms
step:17/2330 train_time:998ms step_avg:58.71ms
step:18/2330 train_time:1062ms step_avg:59.01ms
step:19/2330 train_time:1125ms step_avg:59.21ms
step:20/2330 train_time:1188ms step_avg:59.41ms
step:21/2330 train_time:1249ms step_avg:59.47ms
step:22/2330 train_time:1309ms step_avg:59.52ms
step:23/2330 train_time:1368ms step_avg:59.49ms
step:24/2330 train_time:1429ms step_avg:59.53ms
step:25/2330 train_time:1487ms step_avg:59.49ms
step:26/2330 train_time:1548ms step_avg:59.54ms
step:27/2330 train_time:1607ms step_avg:59.50ms
step:28/2330 train_time:1667ms step_avg:59.55ms
step:29/2330 train_time:1726ms step_avg:59.52ms
step:30/2330 train_time:1787ms step_avg:59.58ms
step:31/2330 train_time:1846ms step_avg:59.55ms
step:32/2330 train_time:1907ms step_avg:59.59ms
step:33/2330 train_time:1965ms step_avg:59.56ms
step:34/2330 train_time:2028ms step_avg:59.64ms
step:35/2330 train_time:2088ms step_avg:59.66ms
step:36/2330 train_time:2150ms step_avg:59.73ms
step:37/2330 train_time:2210ms step_avg:59.72ms
step:38/2330 train_time:2271ms step_avg:59.75ms
step:39/2330 train_time:2330ms step_avg:59.75ms
step:40/2330 train_time:2391ms step_avg:59.78ms
step:41/2330 train_time:2450ms step_avg:59.75ms
step:42/2330 train_time:2511ms step_avg:59.78ms
step:43/2330 train_time:2570ms step_avg:59.76ms
step:44/2330 train_time:2631ms step_avg:59.80ms
step:45/2330 train_time:2691ms step_avg:59.79ms
step:46/2330 train_time:2752ms step_avg:59.83ms
step:47/2330 train_time:2811ms step_avg:59.81ms
step:48/2330 train_time:2873ms step_avg:59.84ms
step:49/2330 train_time:2931ms step_avg:59.83ms
step:50/2330 train_time:2993ms step_avg:59.87ms
step:51/2330 train_time:3053ms step_avg:59.87ms
step:52/2330 train_time:3115ms step_avg:59.90ms
step:53/2330 train_time:3174ms step_avg:59.88ms
step:54/2330 train_time:3235ms step_avg:59.91ms
step:55/2330 train_time:3295ms step_avg:59.91ms
step:56/2330 train_time:3356ms step_avg:59.93ms
step:57/2330 train_time:3414ms step_avg:59.90ms
step:58/2330 train_time:3475ms step_avg:59.92ms
step:59/2330 train_time:3534ms step_avg:59.89ms
step:60/2330 train_time:3595ms step_avg:59.91ms
step:61/2330 train_time:3653ms step_avg:59.89ms
step:62/2330 train_time:3715ms step_avg:59.92ms
step:63/2330 train_time:3775ms step_avg:59.92ms
step:64/2330 train_time:3836ms step_avg:59.93ms
step:65/2330 train_time:3894ms step_avg:59.91ms
step:66/2330 train_time:3955ms step_avg:59.93ms
step:67/2330 train_time:4015ms step_avg:59.92ms
step:68/2330 train_time:4076ms step_avg:59.94ms
step:69/2330 train_time:4135ms step_avg:59.93ms
step:70/2330 train_time:4197ms step_avg:59.96ms
step:71/2330 train_time:4256ms step_avg:59.94ms
step:72/2330 train_time:4317ms step_avg:59.95ms
step:73/2330 train_time:4376ms step_avg:59.94ms
step:74/2330 train_time:4436ms step_avg:59.95ms
step:75/2330 train_time:4494ms step_avg:59.92ms
step:76/2330 train_time:4555ms step_avg:59.94ms
step:77/2330 train_time:4615ms step_avg:59.93ms
step:78/2330 train_time:4676ms step_avg:59.94ms
step:79/2330 train_time:4734ms step_avg:59.93ms
step:80/2330 train_time:4795ms step_avg:59.94ms
step:81/2330 train_time:4854ms step_avg:59.92ms
step:82/2330 train_time:4915ms step_avg:59.94ms
step:83/2330 train_time:4974ms step_avg:59.93ms
step:84/2330 train_time:5035ms step_avg:59.94ms
step:85/2330 train_time:5095ms step_avg:59.94ms
step:86/2330 train_time:5156ms step_avg:59.95ms
step:87/2330 train_time:5216ms step_avg:59.95ms
step:88/2330 train_time:5277ms step_avg:59.96ms
step:89/2330 train_time:5335ms step_avg:59.94ms
step:90/2330 train_time:5396ms step_avg:59.95ms
step:91/2330 train_time:5454ms step_avg:59.93ms
step:92/2330 train_time:5515ms step_avg:59.95ms
step:93/2330 train_time:5574ms step_avg:59.94ms
step:94/2330 train_time:5635ms step_avg:59.95ms
step:95/2330 train_time:5694ms step_avg:59.94ms
step:96/2330 train_time:5755ms step_avg:59.95ms
step:97/2330 train_time:5814ms step_avg:59.94ms
step:98/2330 train_time:5875ms step_avg:59.95ms
step:99/2330 train_time:5934ms step_avg:59.93ms
step:100/2330 train_time:5995ms step_avg:59.95ms
step:101/2330 train_time:6053ms step_avg:59.93ms
step:102/2330 train_time:6115ms step_avg:59.95ms
step:103/2330 train_time:6174ms step_avg:59.94ms
step:104/2330 train_time:6235ms step_avg:59.95ms
step:105/2330 train_time:6294ms step_avg:59.94ms
step:106/2330 train_time:6355ms step_avg:59.96ms
step:107/2330 train_time:6414ms step_avg:59.94ms
step:108/2330 train_time:6475ms step_avg:59.96ms
step:109/2330 train_time:6534ms step_avg:59.94ms
step:110/2330 train_time:6595ms step_avg:59.95ms
step:111/2330 train_time:6653ms step_avg:59.94ms
step:112/2330 train_time:6714ms step_avg:59.95ms
step:113/2330 train_time:6772ms step_avg:59.93ms
step:114/2330 train_time:6833ms step_avg:59.94ms
step:115/2330 train_time:6892ms step_avg:59.93ms
step:116/2330 train_time:6952ms step_avg:59.93ms
step:117/2330 train_time:7012ms step_avg:59.93ms
step:118/2330 train_time:7074ms step_avg:59.95ms
step:119/2330 train_time:7133ms step_avg:59.94ms
step:120/2330 train_time:7194ms step_avg:59.95ms
step:121/2330 train_time:7254ms step_avg:59.95ms
step:122/2330 train_time:7315ms step_avg:59.96ms
step:123/2330 train_time:7374ms step_avg:59.95ms
step:124/2330 train_time:7435ms step_avg:59.96ms
step:125/2330 train_time:7493ms step_avg:59.94ms
step:126/2330 train_time:7553ms step_avg:59.95ms
step:127/2330 train_time:7612ms step_avg:59.94ms
step:128/2330 train_time:7673ms step_avg:59.95ms
step:129/2330 train_time:7732ms step_avg:59.94ms
step:130/2330 train_time:7792ms step_avg:59.94ms
step:131/2330 train_time:7851ms step_avg:59.93ms
step:132/2330 train_time:7912ms step_avg:59.94ms
step:133/2330 train_time:7971ms step_avg:59.93ms
step:134/2330 train_time:8032ms step_avg:59.94ms
step:135/2330 train_time:8092ms step_avg:59.94ms
step:136/2330 train_time:8153ms step_avg:59.95ms
step:137/2330 train_time:8213ms step_avg:59.95ms
step:138/2330 train_time:8274ms step_avg:59.95ms
step:139/2330 train_time:8333ms step_avg:59.95ms
step:140/2330 train_time:8393ms step_avg:59.95ms
step:141/2330 train_time:8452ms step_avg:59.94ms
step:142/2330 train_time:8512ms step_avg:59.95ms
step:143/2330 train_time:8571ms step_avg:59.94ms
step:144/2330 train_time:8631ms step_avg:59.94ms
step:145/2330 train_time:8690ms step_avg:59.93ms
step:146/2330 train_time:8750ms step_avg:59.93ms
step:147/2330 train_time:8809ms step_avg:59.92ms
step:148/2330 train_time:8870ms step_avg:59.93ms
step:149/2330 train_time:8929ms step_avg:59.93ms
step:150/2330 train_time:8990ms step_avg:59.93ms
step:151/2330 train_time:9048ms step_avg:59.92ms
step:152/2330 train_time:9110ms step_avg:59.93ms
step:153/2330 train_time:9169ms step_avg:59.93ms
step:154/2330 train_time:9230ms step_avg:59.93ms
step:155/2330 train_time:9288ms step_avg:59.92ms
step:156/2330 train_time:9349ms step_avg:59.93ms
step:157/2330 train_time:9408ms step_avg:59.93ms
step:158/2330 train_time:9469ms step_avg:59.93ms
step:159/2330 train_time:9528ms step_avg:59.92ms
step:160/2330 train_time:9588ms step_avg:59.93ms
step:161/2330 train_time:9647ms step_avg:59.92ms
step:162/2330 train_time:9707ms step_avg:59.92ms
step:163/2330 train_time:9765ms step_avg:59.91ms
step:164/2330 train_time:9826ms step_avg:59.91ms
step:165/2330 train_time:9884ms step_avg:59.90ms
step:166/2330 train_time:9945ms step_avg:59.91ms
step:167/2330 train_time:10003ms step_avg:59.90ms
step:168/2330 train_time:10064ms step_avg:59.90ms
step:169/2330 train_time:10123ms step_avg:59.90ms
step:170/2330 train_time:10183ms step_avg:59.90ms
step:171/2330 train_time:10242ms step_avg:59.89ms
step:172/2330 train_time:10303ms step_avg:59.90ms
step:173/2330 train_time:10362ms step_avg:59.89ms
step:174/2330 train_time:10423ms step_avg:59.90ms
step:175/2330 train_time:10481ms step_avg:59.89ms
step:176/2330 train_time:10543ms step_avg:59.90ms
step:177/2330 train_time:10602ms step_avg:59.90ms
step:178/2330 train_time:10662ms step_avg:59.90ms
step:179/2330 train_time:10720ms step_avg:59.89ms
step:180/2330 train_time:10780ms step_avg:59.89ms
step:181/2330 train_time:10839ms step_avg:59.88ms
step:182/2330 train_time:10899ms step_avg:59.89ms
step:183/2330 train_time:10958ms step_avg:59.88ms
step:184/2330 train_time:11018ms step_avg:59.88ms
step:185/2330 train_time:11077ms step_avg:59.87ms
step:186/2330 train_time:11137ms step_avg:59.88ms
step:187/2330 train_time:11196ms step_avg:59.87ms
step:188/2330 train_time:11256ms step_avg:59.87ms
step:189/2330 train_time:11315ms step_avg:59.87ms
step:190/2330 train_time:11375ms step_avg:59.87ms
step:191/2330 train_time:11434ms step_avg:59.86ms
step:192/2330 train_time:11496ms step_avg:59.87ms
step:193/2330 train_time:11555ms step_avg:59.87ms
step:194/2330 train_time:11615ms step_avg:59.87ms
step:195/2330 train_time:11674ms step_avg:59.87ms
step:196/2330 train_time:11735ms step_avg:59.87ms
step:197/2330 train_time:11793ms step_avg:59.86ms
step:198/2330 train_time:11854ms step_avg:59.87ms
step:199/2330 train_time:11913ms step_avg:59.86ms
step:200/2330 train_time:11974ms step_avg:59.87ms
step:201/2330 train_time:12033ms step_avg:59.87ms
step:202/2330 train_time:12095ms step_avg:59.87ms
step:203/2330 train_time:12154ms step_avg:59.87ms
step:204/2330 train_time:12215ms step_avg:59.88ms
step:205/2330 train_time:12273ms step_avg:59.87ms
step:206/2330 train_time:12334ms step_avg:59.87ms
step:207/2330 train_time:12394ms step_avg:59.87ms
step:208/2330 train_time:12455ms step_avg:59.88ms
step:209/2330 train_time:12515ms step_avg:59.88ms
step:210/2330 train_time:12576ms step_avg:59.88ms
step:211/2330 train_time:12634ms step_avg:59.88ms
step:212/2330 train_time:12695ms step_avg:59.88ms
step:213/2330 train_time:12753ms step_avg:59.87ms
step:214/2330 train_time:12814ms step_avg:59.88ms
step:215/2330 train_time:12872ms step_avg:59.87ms
step:216/2330 train_time:12933ms step_avg:59.88ms
step:217/2330 train_time:12992ms step_avg:59.87ms
step:218/2330 train_time:13053ms step_avg:59.87ms
step:219/2330 train_time:13111ms step_avg:59.87ms
step:220/2330 train_time:13172ms step_avg:59.87ms
step:221/2330 train_time:13231ms step_avg:59.87ms
step:222/2330 train_time:13292ms step_avg:59.87ms
step:223/2330 train_time:13351ms step_avg:59.87ms
step:224/2330 train_time:13412ms step_avg:59.87ms
step:225/2330 train_time:13471ms step_avg:59.87ms
step:226/2330 train_time:13533ms step_avg:59.88ms
step:227/2330 train_time:13591ms step_avg:59.87ms
step:228/2330 train_time:13652ms step_avg:59.88ms
step:229/2330 train_time:13711ms step_avg:59.88ms
step:230/2330 train_time:13772ms step_avg:59.88ms
step:231/2330 train_time:13831ms step_avg:59.87ms
step:232/2330 train_time:13891ms step_avg:59.88ms
step:233/2330 train_time:13950ms step_avg:59.87ms
step:234/2330 train_time:14010ms step_avg:59.87ms
step:235/2330 train_time:14069ms step_avg:59.87ms
step:236/2330 train_time:14130ms step_avg:59.87ms
step:237/2330 train_time:14188ms step_avg:59.87ms
step:238/2330 train_time:14249ms step_avg:59.87ms
step:239/2330 train_time:14308ms step_avg:59.86ms
step:240/2330 train_time:14369ms step_avg:59.87ms
step:241/2330 train_time:14428ms step_avg:59.87ms
step:242/2330 train_time:14489ms step_avg:59.87ms
step:243/2330 train_time:14548ms step_avg:59.87ms
step:244/2330 train_time:14608ms step_avg:59.87ms
step:245/2330 train_time:14666ms step_avg:59.86ms
step:246/2330 train_time:14727ms step_avg:59.86ms
step:247/2330 train_time:14785ms step_avg:59.86ms
step:248/2330 train_time:14846ms step_avg:59.86ms
step:249/2330 train_time:14904ms step_avg:59.86ms
step:250/2330 train_time:14965ms step_avg:59.86ms
step:250/2330 val_loss:4.0968 train_time:15027ms step_avg:60.11ms
step:251/2330 train_time:15047ms step_avg:59.95ms
step:252/2330 train_time:15085ms step_avg:59.86ms
step:253/2330 train_time:15149ms step_avg:59.88ms
step:254/2330 train_time:15218ms step_avg:59.91ms
step:255/2330 train_time:15279ms step_avg:59.92ms
step:256/2330 train_time:15340ms step_avg:59.92ms
step:257/2330 train_time:15398ms step_avg:59.92ms
step:258/2330 train_time:15459ms step_avg:59.92ms
step:259/2330 train_time:15517ms step_avg:59.91ms
step:260/2330 train_time:15577ms step_avg:59.91ms
step:261/2330 train_time:15635ms step_avg:59.90ms
step:262/2330 train_time:15695ms step_avg:59.90ms
step:263/2330 train_time:15752ms step_avg:59.90ms
step:264/2330 train_time:15812ms step_avg:59.89ms
step:265/2330 train_time:15870ms step_avg:59.89ms
step:266/2330 train_time:15930ms step_avg:59.89ms
step:267/2330 train_time:15991ms step_avg:59.89ms
step:268/2330 train_time:16051ms step_avg:59.89ms
step:269/2330 train_time:16110ms step_avg:59.89ms
step:270/2330 train_time:16173ms step_avg:59.90ms
step:271/2330 train_time:16234ms step_avg:59.90ms
step:272/2330 train_time:16296ms step_avg:59.91ms
step:273/2330 train_time:16354ms step_avg:59.91ms
step:274/2330 train_time:16415ms step_avg:59.91ms
step:275/2330 train_time:16474ms step_avg:59.91ms
step:276/2330 train_time:16534ms step_avg:59.91ms
step:277/2330 train_time:16592ms step_avg:59.90ms
step:278/2330 train_time:16652ms step_avg:59.90ms
step:279/2330 train_time:16710ms step_avg:59.89ms
step:280/2330 train_time:16770ms step_avg:59.89ms
step:281/2330 train_time:16828ms step_avg:59.89ms
step:282/2330 train_time:16888ms step_avg:59.89ms
step:283/2330 train_time:16946ms step_avg:59.88ms
step:284/2330 train_time:17006ms step_avg:59.88ms
step:285/2330 train_time:17065ms step_avg:59.88ms
step:286/2330 train_time:17126ms step_avg:59.88ms
step:287/2330 train_time:17186ms step_avg:59.88ms
step:288/2330 train_time:17246ms step_avg:59.88ms
step:289/2330 train_time:17305ms step_avg:59.88ms
step:290/2330 train_time:17367ms step_avg:59.89ms
step:291/2330 train_time:17426ms step_avg:59.88ms
step:292/2330 train_time:17487ms step_avg:59.89ms
step:293/2330 train_time:17545ms step_avg:59.88ms
step:294/2330 train_time:17606ms step_avg:59.88ms
step:295/2330 train_time:17664ms step_avg:59.88ms
step:296/2330 train_time:17724ms step_avg:59.88ms
step:297/2330 train_time:17782ms step_avg:59.87ms
step:298/2330 train_time:17842ms step_avg:59.87ms
step:299/2330 train_time:17900ms step_avg:59.87ms
step:300/2330 train_time:17961ms step_avg:59.87ms
step:301/2330 train_time:18020ms step_avg:59.87ms
step:302/2330 train_time:18081ms step_avg:59.87ms
step:303/2330 train_time:18140ms step_avg:59.87ms
step:304/2330 train_time:18202ms step_avg:59.88ms
step:305/2330 train_time:18261ms step_avg:59.87ms
step:306/2330 train_time:18322ms step_avg:59.87ms
step:307/2330 train_time:18380ms step_avg:59.87ms
step:308/2330 train_time:18441ms step_avg:59.87ms
step:309/2330 train_time:18500ms step_avg:59.87ms
step:310/2330 train_time:18561ms step_avg:59.87ms
step:311/2330 train_time:18620ms step_avg:59.87ms
step:312/2330 train_time:18680ms step_avg:59.87ms
step:313/2330 train_time:18738ms step_avg:59.87ms
step:314/2330 train_time:18798ms step_avg:59.87ms
step:315/2330 train_time:18857ms step_avg:59.86ms
step:316/2330 train_time:18917ms step_avg:59.86ms
step:317/2330 train_time:18976ms step_avg:59.86ms
step:318/2330 train_time:19037ms step_avg:59.86ms
step:319/2330 train_time:19096ms step_avg:59.86ms
step:320/2330 train_time:19157ms step_avg:59.87ms
step:321/2330 train_time:19216ms step_avg:59.86ms
step:322/2330 train_time:19277ms step_avg:59.87ms
step:323/2330 train_time:19336ms step_avg:59.86ms
step:324/2330 train_time:19397ms step_avg:59.87ms
step:325/2330 train_time:19456ms step_avg:59.86ms
step:326/2330 train_time:19517ms step_avg:59.87ms
step:327/2330 train_time:19575ms step_avg:59.86ms
step:328/2330 train_time:19636ms step_avg:59.86ms
step:329/2330 train_time:19694ms step_avg:59.86ms
step:330/2330 train_time:19754ms step_avg:59.86ms
step:331/2330 train_time:19813ms step_avg:59.86ms
step:332/2330 train_time:19873ms step_avg:59.86ms
step:333/2330 train_time:19932ms step_avg:59.86ms
step:334/2330 train_time:19992ms step_avg:59.86ms
step:335/2330 train_time:20050ms step_avg:59.85ms
step:336/2330 train_time:20111ms step_avg:59.85ms
step:337/2330 train_time:20169ms step_avg:59.85ms
step:338/2330 train_time:20230ms step_avg:59.85ms
step:339/2330 train_time:20289ms step_avg:59.85ms
step:340/2330 train_time:20349ms step_avg:59.85ms
step:341/2330 train_time:20408ms step_avg:59.85ms
step:342/2330 train_time:20469ms step_avg:59.85ms
step:343/2330 train_time:20528ms step_avg:59.85ms
step:344/2330 train_time:20588ms step_avg:59.85ms
step:345/2330 train_time:20646ms step_avg:59.84ms
step:346/2330 train_time:20706ms step_avg:59.85ms
step:347/2330 train_time:20765ms step_avg:59.84ms
step:348/2330 train_time:20826ms step_avg:59.84ms
step:349/2330 train_time:20884ms step_avg:59.84ms
step:350/2330 train_time:20945ms step_avg:59.84ms
step:351/2330 train_time:21003ms step_avg:59.84ms
step:352/2330 train_time:21064ms step_avg:59.84ms
step:353/2330 train_time:21123ms step_avg:59.84ms
step:354/2330 train_time:21183ms step_avg:59.84ms
step:355/2330 train_time:21241ms step_avg:59.84ms
step:356/2330 train_time:21302ms step_avg:59.84ms
step:357/2330 train_time:21361ms step_avg:59.83ms
step:358/2330 train_time:21422ms step_avg:59.84ms
step:359/2330 train_time:21480ms step_avg:59.83ms
step:360/2330 train_time:21541ms step_avg:59.84ms
step:361/2330 train_time:21600ms step_avg:59.83ms
step:362/2330 train_time:21661ms step_avg:59.84ms
step:363/2330 train_time:21720ms step_avg:59.84ms
step:364/2330 train_time:21781ms step_avg:59.84ms
step:365/2330 train_time:21840ms step_avg:59.84ms
step:366/2330 train_time:21900ms step_avg:59.84ms
step:367/2330 train_time:21959ms step_avg:59.83ms
step:368/2330 train_time:22020ms step_avg:59.84ms
step:369/2330 train_time:22079ms step_avg:59.83ms
step:370/2330 train_time:22140ms step_avg:59.84ms
step:371/2330 train_time:22199ms step_avg:59.83ms
step:372/2330 train_time:22260ms step_avg:59.84ms
step:373/2330 train_time:22318ms step_avg:59.83ms
step:374/2330 train_time:22379ms step_avg:59.84ms
step:375/2330 train_time:22438ms step_avg:59.84ms
step:376/2330 train_time:22500ms step_avg:59.84ms
step:377/2330 train_time:22558ms step_avg:59.84ms
step:378/2330 train_time:22619ms step_avg:59.84ms
step:379/2330 train_time:22678ms step_avg:59.84ms
step:380/2330 train_time:22738ms step_avg:59.84ms
step:381/2330 train_time:22797ms step_avg:59.83ms
step:382/2330 train_time:22857ms step_avg:59.84ms
step:383/2330 train_time:22916ms step_avg:59.83ms
step:384/2330 train_time:22976ms step_avg:59.83ms
step:385/2330 train_time:23035ms step_avg:59.83ms
step:386/2330 train_time:23095ms step_avg:59.83ms
step:387/2330 train_time:23154ms step_avg:59.83ms
step:388/2330 train_time:23214ms step_avg:59.83ms
step:389/2330 train_time:23273ms step_avg:59.83ms
step:390/2330 train_time:23334ms step_avg:59.83ms
step:391/2330 train_time:23392ms step_avg:59.83ms
step:392/2330 train_time:23453ms step_avg:59.83ms
step:393/2330 train_time:23511ms step_avg:59.83ms
step:394/2330 train_time:23572ms step_avg:59.83ms
step:395/2330 train_time:23631ms step_avg:59.82ms
step:396/2330 train_time:23692ms step_avg:59.83ms
step:397/2330 train_time:23749ms step_avg:59.82ms
step:398/2330 train_time:23810ms step_avg:59.83ms
step:399/2330 train_time:23869ms step_avg:59.82ms
step:400/2330 train_time:23929ms step_avg:59.82ms
step:401/2330 train_time:23988ms step_avg:59.82ms
step:402/2330 train_time:24049ms step_avg:59.82ms
step:403/2330 train_time:24107ms step_avg:59.82ms
step:404/2330 train_time:24167ms step_avg:59.82ms
step:405/2330 train_time:24225ms step_avg:59.82ms
step:406/2330 train_time:24286ms step_avg:59.82ms
step:407/2330 train_time:24344ms step_avg:59.81ms
step:408/2330 train_time:24404ms step_avg:59.81ms
step:409/2330 train_time:24463ms step_avg:59.81ms
step:410/2330 train_time:24525ms step_avg:59.82ms
step:411/2330 train_time:24583ms step_avg:59.81ms
step:412/2330 train_time:24644ms step_avg:59.82ms
step:413/2330 train_time:24703ms step_avg:59.81ms
step:414/2330 train_time:24763ms step_avg:59.81ms
step:415/2330 train_time:24822ms step_avg:59.81ms
step:416/2330 train_time:24883ms step_avg:59.81ms
step:417/2330 train_time:24941ms step_avg:59.81ms
step:418/2330 train_time:25002ms step_avg:59.81ms
step:419/2330 train_time:25060ms step_avg:59.81ms
step:420/2330 train_time:25121ms step_avg:59.81ms
step:421/2330 train_time:25179ms step_avg:59.81ms
step:422/2330 train_time:25240ms step_avg:59.81ms
step:423/2330 train_time:25299ms step_avg:59.81ms
step:424/2330 train_time:25359ms step_avg:59.81ms
step:425/2330 train_time:25418ms step_avg:59.81ms
step:426/2330 train_time:25479ms step_avg:59.81ms
step:427/2330 train_time:25538ms step_avg:59.81ms
step:428/2330 train_time:25599ms step_avg:59.81ms
step:429/2330 train_time:25657ms step_avg:59.81ms
step:430/2330 train_time:25719ms step_avg:59.81ms
step:431/2330 train_time:25778ms step_avg:59.81ms
step:432/2330 train_time:25839ms step_avg:59.81ms
step:433/2330 train_time:25897ms step_avg:59.81ms
step:434/2330 train_time:25958ms step_avg:59.81ms
step:435/2330 train_time:26016ms step_avg:59.81ms
step:436/2330 train_time:26077ms step_avg:59.81ms
step:437/2330 train_time:26135ms step_avg:59.81ms
step:438/2330 train_time:26196ms step_avg:59.81ms
step:439/2330 train_time:26255ms step_avg:59.81ms
step:440/2330 train_time:26315ms step_avg:59.81ms
step:441/2330 train_time:26373ms step_avg:59.80ms
step:442/2330 train_time:26434ms step_avg:59.81ms
step:443/2330 train_time:26492ms step_avg:59.80ms
step:444/2330 train_time:26552ms step_avg:59.80ms
step:445/2330 train_time:26611ms step_avg:59.80ms
step:446/2330 train_time:26672ms step_avg:59.80ms
step:447/2330 train_time:26730ms step_avg:59.80ms
step:448/2330 train_time:26791ms step_avg:59.80ms
step:449/2330 train_time:26850ms step_avg:59.80ms
step:450/2330 train_time:26911ms step_avg:59.80ms
step:451/2330 train_time:26970ms step_avg:59.80ms
step:452/2330 train_time:27030ms step_avg:59.80ms
step:453/2330 train_time:27088ms step_avg:59.80ms
step:454/2330 train_time:27148ms step_avg:59.80ms
step:455/2330 train_time:27206ms step_avg:59.79ms
step:456/2330 train_time:27266ms step_avg:59.79ms
step:457/2330 train_time:27325ms step_avg:59.79ms
step:458/2330 train_time:27385ms step_avg:59.79ms
step:459/2330 train_time:27443ms step_avg:59.79ms
step:460/2330 train_time:27503ms step_avg:59.79ms
step:461/2330 train_time:27562ms step_avg:59.79ms
step:462/2330 train_time:27623ms step_avg:59.79ms
step:463/2330 train_time:27682ms step_avg:59.79ms
step:464/2330 train_time:27743ms step_avg:59.79ms
step:465/2330 train_time:27802ms step_avg:59.79ms
step:466/2330 train_time:27863ms step_avg:59.79ms
step:467/2330 train_time:27922ms step_avg:59.79ms
step:468/2330 train_time:27983ms step_avg:59.79ms
step:469/2330 train_time:28042ms step_avg:59.79ms
step:470/2330 train_time:28102ms step_avg:59.79ms
step:471/2330 train_time:28161ms step_avg:59.79ms
step:472/2330 train_time:28221ms step_avg:59.79ms
step:473/2330 train_time:28280ms step_avg:59.79ms
step:474/2330 train_time:28340ms step_avg:59.79ms
step:475/2330 train_time:28399ms step_avg:59.79ms
step:476/2330 train_time:28460ms step_avg:59.79ms
step:477/2330 train_time:28519ms step_avg:59.79ms
step:478/2330 train_time:28579ms step_avg:59.79ms
step:479/2330 train_time:28638ms step_avg:59.79ms
step:480/2330 train_time:28699ms step_avg:59.79ms
step:481/2330 train_time:28758ms step_avg:59.79ms
step:482/2330 train_time:28819ms step_avg:59.79ms
step:483/2330 train_time:28878ms step_avg:59.79ms
step:484/2330 train_time:28939ms step_avg:59.79ms
step:485/2330 train_time:28998ms step_avg:59.79ms
step:486/2330 train_time:29059ms step_avg:59.79ms
step:487/2330 train_time:29118ms step_avg:59.79ms
step:488/2330 train_time:29179ms step_avg:59.79ms
step:489/2330 train_time:29237ms step_avg:59.79ms
step:490/2330 train_time:29298ms step_avg:59.79ms
step:491/2330 train_time:29356ms step_avg:59.79ms
step:492/2330 train_time:29417ms step_avg:59.79ms
step:493/2330 train_time:29475ms step_avg:59.79ms
step:494/2330 train_time:29536ms step_avg:59.79ms
step:495/2330 train_time:29595ms step_avg:59.79ms
step:496/2330 train_time:29656ms step_avg:59.79ms
step:497/2330 train_time:29714ms step_avg:59.79ms
step:498/2330 train_time:29775ms step_avg:59.79ms
step:499/2330 train_time:29834ms step_avg:59.79ms
step:500/2330 train_time:29894ms step_avg:59.79ms
step:500/2330 val_loss:3.8176 train_time:29956ms step_avg:59.91ms
step:501/2330 train_time:29976ms step_avg:59.83ms
step:502/2330 train_time:30015ms step_avg:59.79ms
step:503/2330 train_time:30076ms step_avg:59.79ms
step:504/2330 train_time:30142ms step_avg:59.81ms
step:505/2330 train_time:30202ms step_avg:59.81ms
step:506/2330 train_time:30262ms step_avg:59.81ms
step:507/2330 train_time:30320ms step_avg:59.80ms
step:508/2330 train_time:30380ms step_avg:59.80ms
step:509/2330 train_time:30437ms step_avg:59.80ms
step:510/2330 train_time:30497ms step_avg:59.80ms
step:511/2330 train_time:30555ms step_avg:59.79ms
step:512/2330 train_time:30614ms step_avg:59.79ms
step:513/2330 train_time:30672ms step_avg:59.79ms
step:514/2330 train_time:30732ms step_avg:59.79ms
step:515/2330 train_time:30790ms step_avg:59.79ms
step:516/2330 train_time:30850ms step_avg:59.79ms
step:517/2330 train_time:30908ms step_avg:59.78ms
step:518/2330 train_time:30969ms step_avg:59.79ms
step:519/2330 train_time:31029ms step_avg:59.79ms
step:520/2330 train_time:31091ms step_avg:59.79ms
step:521/2330 train_time:31151ms step_avg:59.79ms
step:522/2330 train_time:31213ms step_avg:59.79ms
step:523/2330 train_time:31272ms step_avg:59.79ms
step:524/2330 train_time:31333ms step_avg:59.80ms
step:525/2330 train_time:31391ms step_avg:59.79ms
step:526/2330 train_time:31452ms step_avg:59.79ms
step:527/2330 train_time:31510ms step_avg:59.79ms
step:528/2330 train_time:31570ms step_avg:59.79ms
step:529/2330 train_time:31628ms step_avg:59.79ms
step:530/2330 train_time:31688ms step_avg:59.79ms
step:531/2330 train_time:31747ms step_avg:59.79ms
step:532/2330 train_time:31807ms step_avg:59.79ms
step:533/2330 train_time:31865ms step_avg:59.78ms
step:534/2330 train_time:31925ms step_avg:59.78ms
step:535/2330 train_time:31984ms step_avg:59.78ms
step:536/2330 train_time:32045ms step_avg:59.78ms
step:537/2330 train_time:32104ms step_avg:59.78ms
step:538/2330 train_time:32166ms step_avg:59.79ms
step:539/2330 train_time:32226ms step_avg:59.79ms
step:540/2330 train_time:32286ms step_avg:59.79ms
step:541/2330 train_time:32345ms step_avg:59.79ms
step:542/2330 train_time:32406ms step_avg:59.79ms
step:543/2330 train_time:32464ms step_avg:59.79ms
step:544/2330 train_time:32525ms step_avg:59.79ms
step:545/2330 train_time:32583ms step_avg:59.78ms
step:546/2330 train_time:32643ms step_avg:59.79ms
step:547/2330 train_time:32701ms step_avg:59.78ms
step:548/2330 train_time:32761ms step_avg:59.78ms
step:549/2330 train_time:32819ms step_avg:59.78ms
step:550/2330 train_time:32879ms step_avg:59.78ms
step:551/2330 train_time:32937ms step_avg:59.78ms
step:552/2330 train_time:32997ms step_avg:59.78ms
step:553/2330 train_time:33055ms step_avg:59.77ms
step:554/2330 train_time:33116ms step_avg:59.78ms
step:555/2330 train_time:33175ms step_avg:59.78ms
step:556/2330 train_time:33237ms step_avg:59.78ms
step:557/2330 train_time:33295ms step_avg:59.78ms
step:558/2330 train_time:33357ms step_avg:59.78ms
step:559/2330 train_time:33416ms step_avg:59.78ms
step:560/2330 train_time:33477ms step_avg:59.78ms
step:561/2330 train_time:33535ms step_avg:59.78ms
step:562/2330 train_time:33595ms step_avg:59.78ms
step:563/2330 train_time:33654ms step_avg:59.78ms
step:564/2330 train_time:33715ms step_avg:59.78ms
step:565/2330 train_time:33773ms step_avg:59.78ms
step:566/2330 train_time:33835ms step_avg:59.78ms
step:567/2330 train_time:33893ms step_avg:59.78ms
step:568/2330 train_time:33954ms step_avg:59.78ms
step:569/2330 train_time:34012ms step_avg:59.78ms
step:570/2330 train_time:34074ms step_avg:59.78ms
step:571/2330 train_time:34132ms step_avg:59.78ms
step:572/2330 train_time:34194ms step_avg:59.78ms
step:573/2330 train_time:34253ms step_avg:59.78ms
step:574/2330 train_time:34314ms step_avg:59.78ms
step:575/2330 train_time:34374ms step_avg:59.78ms
step:576/2330 train_time:34435ms step_avg:59.78ms
step:577/2330 train_time:34493ms step_avg:59.78ms
step:578/2330 train_time:34555ms step_avg:59.78ms
step:579/2330 train_time:34613ms step_avg:59.78ms
step:580/2330 train_time:34674ms step_avg:59.78ms
step:581/2330 train_time:34733ms step_avg:59.78ms
step:582/2330 train_time:34794ms step_avg:59.78ms
step:583/2330 train_time:34852ms step_avg:59.78ms
step:584/2330 train_time:34912ms step_avg:59.78ms
step:585/2330 train_time:34971ms step_avg:59.78ms
step:586/2330 train_time:35032ms step_avg:59.78ms
step:587/2330 train_time:35090ms step_avg:59.78ms
step:588/2330 train_time:35151ms step_avg:59.78ms
step:589/2330 train_time:35209ms step_avg:59.78ms
step:590/2330 train_time:35270ms step_avg:59.78ms
step:591/2330 train_time:35329ms step_avg:59.78ms
step:592/2330 train_time:35390ms step_avg:59.78ms
step:593/2330 train_time:35449ms step_avg:59.78ms
step:594/2330 train_time:35510ms step_avg:59.78ms
step:595/2330 train_time:35570ms step_avg:59.78ms
step:596/2330 train_time:35631ms step_avg:59.78ms
step:597/2330 train_time:35690ms step_avg:59.78ms
step:598/2330 train_time:35750ms step_avg:59.78ms
step:599/2330 train_time:35809ms step_avg:59.78ms
step:600/2330 train_time:35870ms step_avg:59.78ms
step:601/2330 train_time:35928ms step_avg:59.78ms
step:602/2330 train_time:35988ms step_avg:59.78ms
step:603/2330 train_time:36047ms step_avg:59.78ms
step:604/2330 train_time:36107ms step_avg:59.78ms
step:605/2330 train_time:36165ms step_avg:59.78ms
step:606/2330 train_time:36226ms step_avg:59.78ms
step:607/2330 train_time:36285ms step_avg:59.78ms
step:608/2330 train_time:36345ms step_avg:59.78ms
step:609/2330 train_time:36403ms step_avg:59.78ms
step:610/2330 train_time:36464ms step_avg:59.78ms
step:611/2330 train_time:36523ms step_avg:59.78ms
step:612/2330 train_time:36584ms step_avg:59.78ms
step:613/2330 train_time:36643ms step_avg:59.78ms
step:614/2330 train_time:36703ms step_avg:59.78ms
step:615/2330 train_time:36762ms step_avg:59.78ms
step:616/2330 train_time:36824ms step_avg:59.78ms
step:617/2330 train_time:36883ms step_avg:59.78ms
step:618/2330 train_time:36943ms step_avg:59.78ms
step:619/2330 train_time:37001ms step_avg:59.77ms
step:620/2330 train_time:37062ms step_avg:59.78ms
step:621/2330 train_time:37120ms step_avg:59.77ms
step:622/2330 train_time:37180ms step_avg:59.77ms
step:623/2330 train_time:37238ms step_avg:59.77ms
step:624/2330 train_time:37298ms step_avg:59.77ms
step:625/2330 train_time:37356ms step_avg:59.77ms
step:626/2330 train_time:37416ms step_avg:59.77ms
step:627/2330 train_time:37475ms step_avg:59.77ms
step:628/2330 train_time:37536ms step_avg:59.77ms
step:629/2330 train_time:37595ms step_avg:59.77ms
step:630/2330 train_time:37656ms step_avg:59.77ms
step:631/2330 train_time:37715ms step_avg:59.77ms
step:632/2330 train_time:37776ms step_avg:59.77ms
step:633/2330 train_time:37834ms step_avg:59.77ms
step:634/2330 train_time:37895ms step_avg:59.77ms
step:635/2330 train_time:37953ms step_avg:59.77ms
step:636/2330 train_time:38014ms step_avg:59.77ms
step:637/2330 train_time:38073ms step_avg:59.77ms
step:638/2330 train_time:38133ms step_avg:59.77ms
step:639/2330 train_time:38191ms step_avg:59.77ms
step:640/2330 train_time:38252ms step_avg:59.77ms
step:641/2330 train_time:38311ms step_avg:59.77ms
step:642/2330 train_time:38372ms step_avg:59.77ms
step:643/2330 train_time:38431ms step_avg:59.77ms
step:644/2330 train_time:38492ms step_avg:59.77ms
step:645/2330 train_time:38551ms step_avg:59.77ms
step:646/2330 train_time:38612ms step_avg:59.77ms
step:647/2330 train_time:38670ms step_avg:59.77ms
step:648/2330 train_time:38731ms step_avg:59.77ms
step:649/2330 train_time:38790ms step_avg:59.77ms
step:650/2330 train_time:38851ms step_avg:59.77ms
step:651/2330 train_time:38910ms step_avg:59.77ms
step:652/2330 train_time:38971ms step_avg:59.77ms
step:653/2330 train_time:39030ms step_avg:59.77ms
step:654/2330 train_time:39091ms step_avg:59.77ms
step:655/2330 train_time:39149ms step_avg:59.77ms
step:656/2330 train_time:39210ms step_avg:59.77ms
step:657/2330 train_time:39268ms step_avg:59.77ms
step:658/2330 train_time:39329ms step_avg:59.77ms
step:659/2330 train_time:39387ms step_avg:59.77ms
step:660/2330 train_time:39448ms step_avg:59.77ms
step:661/2330 train_time:39506ms step_avg:59.77ms
step:662/2330 train_time:39567ms step_avg:59.77ms
step:663/2330 train_time:39626ms step_avg:59.77ms
step:664/2330 train_time:39687ms step_avg:59.77ms
step:665/2330 train_time:39745ms step_avg:59.77ms
step:666/2330 train_time:39806ms step_avg:59.77ms
step:667/2330 train_time:39864ms step_avg:59.77ms
step:668/2330 train_time:39925ms step_avg:59.77ms
step:669/2330 train_time:39984ms step_avg:59.77ms
step:670/2330 train_time:40044ms step_avg:59.77ms
step:671/2330 train_time:40103ms step_avg:59.77ms
step:672/2330 train_time:40163ms step_avg:59.77ms
step:673/2330 train_time:40222ms step_avg:59.76ms
step:674/2330 train_time:40282ms step_avg:59.77ms
step:675/2330 train_time:40340ms step_avg:59.76ms
step:676/2330 train_time:40400ms step_avg:59.76ms
step:677/2330 train_time:40459ms step_avg:59.76ms
step:678/2330 train_time:40519ms step_avg:59.76ms
step:679/2330 train_time:40577ms step_avg:59.76ms
step:680/2330 train_time:40638ms step_avg:59.76ms
step:681/2330 train_time:40696ms step_avg:59.76ms
step:682/2330 train_time:40757ms step_avg:59.76ms
step:683/2330 train_time:40815ms step_avg:59.76ms
step:684/2330 train_time:40876ms step_avg:59.76ms
step:685/2330 train_time:40935ms step_avg:59.76ms
step:686/2330 train_time:40996ms step_avg:59.76ms
step:687/2330 train_time:41055ms step_avg:59.76ms
step:688/2330 train_time:41116ms step_avg:59.76ms
step:689/2330 train_time:41175ms step_avg:59.76ms
step:690/2330 train_time:41236ms step_avg:59.76ms
step:691/2330 train_time:41295ms step_avg:59.76ms
step:692/2330 train_time:41356ms step_avg:59.76ms
step:693/2330 train_time:41414ms step_avg:59.76ms
step:694/2330 train_time:41476ms step_avg:59.76ms
step:695/2330 train_time:41535ms step_avg:59.76ms
step:696/2330 train_time:41595ms step_avg:59.76ms
step:697/2330 train_time:41653ms step_avg:59.76ms
step:698/2330 train_time:41714ms step_avg:59.76ms
step:699/2330 train_time:41773ms step_avg:59.76ms
step:700/2330 train_time:41834ms step_avg:59.76ms
step:701/2330 train_time:41892ms step_avg:59.76ms
step:702/2330 train_time:41953ms step_avg:59.76ms
step:703/2330 train_time:42012ms step_avg:59.76ms
step:704/2330 train_time:42073ms step_avg:59.76ms
step:705/2330 train_time:42132ms step_avg:59.76ms
step:706/2330 train_time:42193ms step_avg:59.76ms
step:707/2330 train_time:42252ms step_avg:59.76ms
step:708/2330 train_time:42313ms step_avg:59.76ms
step:709/2330 train_time:42372ms step_avg:59.76ms
step:710/2330 train_time:42433ms step_avg:59.77ms
step:711/2330 train_time:42492ms step_avg:59.76ms
step:712/2330 train_time:42553ms step_avg:59.77ms
step:713/2330 train_time:42612ms step_avg:59.76ms
step:714/2330 train_time:42673ms step_avg:59.77ms
step:715/2330 train_time:42731ms step_avg:59.76ms
step:716/2330 train_time:42792ms step_avg:59.77ms
step:717/2330 train_time:42851ms step_avg:59.76ms
step:718/2330 train_time:42912ms step_avg:59.77ms
step:719/2330 train_time:42970ms step_avg:59.76ms
step:720/2330 train_time:43031ms step_avg:59.77ms
step:721/2330 train_time:43090ms step_avg:59.76ms
step:722/2330 train_time:43152ms step_avg:59.77ms
step:723/2330 train_time:43211ms step_avg:59.77ms
step:724/2330 train_time:43272ms step_avg:59.77ms
step:725/2330 train_time:43332ms step_avg:59.77ms
step:726/2330 train_time:43393ms step_avg:59.77ms
step:727/2330 train_time:43452ms step_avg:59.77ms
step:728/2330 train_time:43513ms step_avg:59.77ms
step:729/2330 train_time:43571ms step_avg:59.77ms
step:730/2330 train_time:43632ms step_avg:59.77ms
step:731/2330 train_time:43690ms step_avg:59.77ms
step:732/2330 train_time:43752ms step_avg:59.77ms
step:733/2330 train_time:43810ms step_avg:59.77ms
step:734/2330 train_time:43871ms step_avg:59.77ms
step:735/2330 train_time:43930ms step_avg:59.77ms
step:736/2330 train_time:43991ms step_avg:59.77ms
step:737/2330 train_time:44050ms step_avg:59.77ms
step:738/2330 train_time:44111ms step_avg:59.77ms
step:739/2330 train_time:44170ms step_avg:59.77ms
step:740/2330 train_time:44231ms step_avg:59.77ms
step:741/2330 train_time:44289ms step_avg:59.77ms
step:742/2330 train_time:44350ms step_avg:59.77ms
step:743/2330 train_time:44409ms step_avg:59.77ms
step:744/2330 train_time:44470ms step_avg:59.77ms
step:745/2330 train_time:44528ms step_avg:59.77ms
step:746/2330 train_time:44589ms step_avg:59.77ms
step:747/2330 train_time:44648ms step_avg:59.77ms
step:748/2330 train_time:44708ms step_avg:59.77ms
step:749/2330 train_time:44767ms step_avg:59.77ms
step:750/2330 train_time:44827ms step_avg:59.77ms
step:750/2330 val_loss:3.6873 train_time:44890ms step_avg:59.85ms
step:751/2330 train_time:44910ms step_avg:59.80ms
step:752/2330 train_time:44950ms step_avg:59.77ms
step:753/2330 train_time:45010ms step_avg:59.77ms
step:754/2330 train_time:45075ms step_avg:59.78ms
step:755/2330 train_time:45135ms step_avg:59.78ms
step:756/2330 train_time:45196ms step_avg:59.78ms
step:757/2330 train_time:45254ms step_avg:59.78ms
step:758/2330 train_time:45315ms step_avg:59.78ms
step:759/2330 train_time:45373ms step_avg:59.78ms
step:760/2330 train_time:45433ms step_avg:59.78ms
step:761/2330 train_time:45491ms step_avg:59.78ms
step:762/2330 train_time:45551ms step_avg:59.78ms
step:763/2330 train_time:45608ms step_avg:59.77ms
step:764/2330 train_time:45668ms step_avg:59.77ms
step:765/2330 train_time:45726ms step_avg:59.77ms
step:766/2330 train_time:45787ms step_avg:59.77ms
step:767/2330 train_time:45846ms step_avg:59.77ms
step:768/2330 train_time:45908ms step_avg:59.78ms
step:769/2330 train_time:45969ms step_avg:59.78ms
step:770/2330 train_time:46032ms step_avg:59.78ms
step:771/2330 train_time:46093ms step_avg:59.78ms
step:772/2330 train_time:46155ms step_avg:59.79ms
step:773/2330 train_time:46215ms step_avg:59.79ms
step:774/2330 train_time:46276ms step_avg:59.79ms
step:775/2330 train_time:46336ms step_avg:59.79ms
step:776/2330 train_time:46397ms step_avg:59.79ms
step:777/2330 train_time:46455ms step_avg:59.79ms
step:778/2330 train_time:46517ms step_avg:59.79ms
step:779/2330 train_time:46575ms step_avg:59.79ms
step:780/2330 train_time:46637ms step_avg:59.79ms
step:781/2330 train_time:46696ms step_avg:59.79ms
step:782/2330 train_time:46757ms step_avg:59.79ms
step:783/2330 train_time:46817ms step_avg:59.79ms
step:784/2330 train_time:46879ms step_avg:59.79ms
step:785/2330 train_time:46939ms step_avg:59.80ms
step:786/2330 train_time:47002ms step_avg:59.80ms
step:787/2330 train_time:47061ms step_avg:59.80ms
step:788/2330 train_time:47123ms step_avg:59.80ms
step:789/2330 train_time:47182ms step_avg:59.80ms
step:790/2330 train_time:47243ms step_avg:59.80ms
step:791/2330 train_time:47302ms step_avg:59.80ms
step:792/2330 train_time:47363ms step_avg:59.80ms
step:793/2330 train_time:47422ms step_avg:59.80ms
step:794/2330 train_time:47484ms step_avg:59.80ms
step:795/2330 train_time:47542ms step_avg:59.80ms
step:796/2330 train_time:47603ms step_avg:59.80ms
step:797/2330 train_time:47662ms step_avg:59.80ms
step:798/2330 train_time:47724ms step_avg:59.80ms
step:799/2330 train_time:47784ms step_avg:59.80ms
step:800/2330 train_time:47845ms step_avg:59.81ms
step:801/2330 train_time:47904ms step_avg:59.81ms
step:802/2330 train_time:47966ms step_avg:59.81ms
step:803/2330 train_time:48025ms step_avg:59.81ms
step:804/2330 train_time:48086ms step_avg:59.81ms
step:805/2330 train_time:48145ms step_avg:59.81ms
step:806/2330 train_time:48206ms step_avg:59.81ms
step:807/2330 train_time:48265ms step_avg:59.81ms
step:808/2330 train_time:48327ms step_avg:59.81ms
step:809/2330 train_time:48386ms step_avg:59.81ms
step:810/2330 train_time:48447ms step_avg:59.81ms
step:811/2330 train_time:48506ms step_avg:59.81ms
step:812/2330 train_time:48567ms step_avg:59.81ms
step:813/2330 train_time:48626ms step_avg:59.81ms
step:814/2330 train_time:48688ms step_avg:59.81ms
step:815/2330 train_time:48747ms step_avg:59.81ms
step:816/2330 train_time:48808ms step_avg:59.81ms
step:817/2330 train_time:48868ms step_avg:59.81ms
step:818/2330 train_time:48930ms step_avg:59.82ms
step:819/2330 train_time:48989ms step_avg:59.82ms
step:820/2330 train_time:49050ms step_avg:59.82ms
step:821/2330 train_time:49109ms step_avg:59.82ms
step:822/2330 train_time:49171ms step_avg:59.82ms
step:823/2330 train_time:49230ms step_avg:59.82ms
step:824/2330 train_time:49291ms step_avg:59.82ms
step:825/2330 train_time:49351ms step_avg:59.82ms
step:826/2330 train_time:49412ms step_avg:59.82ms
step:827/2330 train_time:49471ms step_avg:59.82ms
step:828/2330 train_time:49533ms step_avg:59.82ms
step:829/2330 train_time:49593ms step_avg:59.82ms
step:830/2330 train_time:49655ms step_avg:59.83ms
step:831/2330 train_time:49714ms step_avg:59.82ms
step:832/2330 train_time:49776ms step_avg:59.83ms
step:833/2330 train_time:49835ms step_avg:59.83ms
step:834/2330 train_time:49897ms step_avg:59.83ms
step:835/2330 train_time:49956ms step_avg:59.83ms
step:836/2330 train_time:50017ms step_avg:59.83ms
step:837/2330 train_time:50077ms step_avg:59.83ms
step:838/2330 train_time:50139ms step_avg:59.83ms
step:839/2330 train_time:50198ms step_avg:59.83ms
step:840/2330 train_time:50260ms step_avg:59.83ms
step:841/2330 train_time:50319ms step_avg:59.83ms
step:842/2330 train_time:50381ms step_avg:59.83ms
step:843/2330 train_time:50440ms step_avg:59.83ms
step:844/2330 train_time:50501ms step_avg:59.84ms
step:845/2330 train_time:50560ms step_avg:59.83ms
step:846/2330 train_time:50621ms step_avg:59.84ms
step:847/2330 train_time:50680ms step_avg:59.84ms
step:848/2330 train_time:50741ms step_avg:59.84ms
step:849/2330 train_time:50801ms step_avg:59.84ms
step:850/2330 train_time:50862ms step_avg:59.84ms
step:851/2330 train_time:50921ms step_avg:59.84ms
step:852/2330 train_time:50983ms step_avg:59.84ms
step:853/2330 train_time:51042ms step_avg:59.84ms
step:854/2330 train_time:51103ms step_avg:59.84ms
step:855/2330 train_time:51163ms step_avg:59.84ms
step:856/2330 train_time:51224ms step_avg:59.84ms
step:857/2330 train_time:51283ms step_avg:59.84ms
step:858/2330 train_time:51345ms step_avg:59.84ms
step:859/2330 train_time:51403ms step_avg:59.84ms
step:860/2330 train_time:51465ms step_avg:59.84ms
step:861/2330 train_time:51524ms step_avg:59.84ms
step:862/2330 train_time:51585ms step_avg:59.84ms
step:863/2330 train_time:51644ms step_avg:59.84ms
step:864/2330 train_time:51705ms step_avg:59.84ms
step:865/2330 train_time:51764ms step_avg:59.84ms
step:866/2330 train_time:51826ms step_avg:59.85ms
step:867/2330 train_time:51885ms step_avg:59.84ms
step:868/2330 train_time:51946ms step_avg:59.85ms
step:869/2330 train_time:52005ms step_avg:59.84ms
step:870/2330 train_time:52067ms step_avg:59.85ms
step:871/2330 train_time:52127ms step_avg:59.85ms
step:872/2330 train_time:52188ms step_avg:59.85ms
step:873/2330 train_time:52247ms step_avg:59.85ms
step:874/2330 train_time:52308ms step_avg:59.85ms
step:875/2330 train_time:52368ms step_avg:59.85ms
step:876/2330 train_time:52429ms step_avg:59.85ms
step:877/2330 train_time:52487ms step_avg:59.85ms
step:878/2330 train_time:52549ms step_avg:59.85ms
step:879/2330 train_time:52608ms step_avg:59.85ms
step:880/2330 train_time:52669ms step_avg:59.85ms
step:881/2330 train_time:52728ms step_avg:59.85ms
step:882/2330 train_time:52789ms step_avg:59.85ms
step:883/2330 train_time:52848ms step_avg:59.85ms
step:884/2330 train_time:52910ms step_avg:59.85ms
step:885/2330 train_time:52969ms step_avg:59.85ms
step:886/2330 train_time:53030ms step_avg:59.85ms
step:887/2330 train_time:53090ms step_avg:59.85ms
step:888/2330 train_time:53151ms step_avg:59.85ms
step:889/2330 train_time:53210ms step_avg:59.85ms
step:890/2330 train_time:53272ms step_avg:59.86ms
step:891/2330 train_time:53331ms step_avg:59.86ms
step:892/2330 train_time:53393ms step_avg:59.86ms
step:893/2330 train_time:53452ms step_avg:59.86ms
step:894/2330 train_time:53514ms step_avg:59.86ms
step:895/2330 train_time:53573ms step_avg:59.86ms
step:896/2330 train_time:53635ms step_avg:59.86ms
step:897/2330 train_time:53694ms step_avg:59.86ms
step:898/2330 train_time:53755ms step_avg:59.86ms
step:899/2330 train_time:53814ms step_avg:59.86ms
step:900/2330 train_time:53876ms step_avg:59.86ms
step:901/2330 train_time:53936ms step_avg:59.86ms
step:902/2330 train_time:53998ms step_avg:59.86ms
step:903/2330 train_time:54057ms step_avg:59.86ms
step:904/2330 train_time:54119ms step_avg:59.87ms
step:905/2330 train_time:54179ms step_avg:59.87ms
step:906/2330 train_time:54240ms step_avg:59.87ms
step:907/2330 train_time:54299ms step_avg:59.87ms
step:908/2330 train_time:54360ms step_avg:59.87ms
step:909/2330 train_time:54419ms step_avg:59.87ms
step:910/2330 train_time:54480ms step_avg:59.87ms
step:911/2330 train_time:54540ms step_avg:59.87ms
step:912/2330 train_time:54601ms step_avg:59.87ms
step:913/2330 train_time:54660ms step_avg:59.87ms
step:914/2330 train_time:54722ms step_avg:59.87ms
step:915/2330 train_time:54781ms step_avg:59.87ms
step:916/2330 train_time:54842ms step_avg:59.87ms
step:917/2330 train_time:54901ms step_avg:59.87ms
step:918/2330 train_time:54962ms step_avg:59.87ms
step:919/2330 train_time:55022ms step_avg:59.87ms
step:920/2330 train_time:55083ms step_avg:59.87ms
step:921/2330 train_time:55142ms step_avg:59.87ms
step:922/2330 train_time:55203ms step_avg:59.87ms
step:923/2330 train_time:55263ms step_avg:59.87ms
step:924/2330 train_time:55325ms step_avg:59.88ms
step:925/2330 train_time:55383ms step_avg:59.87ms
step:926/2330 train_time:55445ms step_avg:59.88ms
step:927/2330 train_time:55504ms step_avg:59.88ms
step:928/2330 train_time:55565ms step_avg:59.88ms
step:929/2330 train_time:55624ms step_avg:59.88ms
step:930/2330 train_time:55686ms step_avg:59.88ms
step:931/2330 train_time:55745ms step_avg:59.88ms
step:932/2330 train_time:55806ms step_avg:59.88ms
step:933/2330 train_time:55865ms step_avg:59.88ms
step:934/2330 train_time:55927ms step_avg:59.88ms
step:935/2330 train_time:55986ms step_avg:59.88ms
step:936/2330 train_time:56047ms step_avg:59.88ms
step:937/2330 train_time:56106ms step_avg:59.88ms
step:938/2330 train_time:56168ms step_avg:59.88ms
step:939/2330 train_time:56227ms step_avg:59.88ms
step:940/2330 train_time:56288ms step_avg:59.88ms
step:941/2330 train_time:56347ms step_avg:59.88ms
step:942/2330 train_time:56408ms step_avg:59.88ms
step:943/2330 train_time:56468ms step_avg:59.88ms
step:944/2330 train_time:56529ms step_avg:59.88ms
step:945/2330 train_time:56588ms step_avg:59.88ms
step:946/2330 train_time:56649ms step_avg:59.88ms
step:947/2330 train_time:56708ms step_avg:59.88ms
step:948/2330 train_time:56770ms step_avg:59.88ms
step:949/2330 train_time:56829ms step_avg:59.88ms
step:950/2330 train_time:56890ms step_avg:59.88ms
step:951/2330 train_time:56949ms step_avg:59.88ms
step:952/2330 train_time:57011ms step_avg:59.89ms
step:953/2330 train_time:57070ms step_avg:59.89ms
step:954/2330 train_time:57132ms step_avg:59.89ms
step:955/2330 train_time:57191ms step_avg:59.89ms
step:956/2330 train_time:57252ms step_avg:59.89ms
step:957/2330 train_time:57311ms step_avg:59.89ms
step:958/2330 train_time:57373ms step_avg:59.89ms
step:959/2330 train_time:57432ms step_avg:59.89ms
step:960/2330 train_time:57494ms step_avg:59.89ms
step:961/2330 train_time:57553ms step_avg:59.89ms
step:962/2330 train_time:57615ms step_avg:59.89ms
step:963/2330 train_time:57674ms step_avg:59.89ms
step:964/2330 train_time:57735ms step_avg:59.89ms
step:965/2330 train_time:57795ms step_avg:59.89ms
step:966/2330 train_time:57857ms step_avg:59.89ms
step:967/2330 train_time:57916ms step_avg:59.89ms
step:968/2330 train_time:57978ms step_avg:59.89ms
step:969/2330 train_time:58038ms step_avg:59.89ms
step:970/2330 train_time:58099ms step_avg:59.90ms
step:971/2330 train_time:58159ms step_avg:59.90ms
step:972/2330 train_time:58220ms step_avg:59.90ms
step:973/2330 train_time:58280ms step_avg:59.90ms
step:974/2330 train_time:58341ms step_avg:59.90ms
step:975/2330 train_time:58401ms step_avg:59.90ms
step:976/2330 train_time:58462ms step_avg:59.90ms
step:977/2330 train_time:58522ms step_avg:59.90ms
step:978/2330 train_time:58583ms step_avg:59.90ms
step:979/2330 train_time:58642ms step_avg:59.90ms
step:980/2330 train_time:58703ms step_avg:59.90ms
step:981/2330 train_time:58762ms step_avg:59.90ms
step:982/2330 train_time:58824ms step_avg:59.90ms
step:983/2330 train_time:58883ms step_avg:59.90ms
step:984/2330 train_time:58945ms step_avg:59.90ms
step:985/2330 train_time:59004ms step_avg:59.90ms
step:986/2330 train_time:59066ms step_avg:59.90ms
step:987/2330 train_time:59125ms step_avg:59.90ms
step:988/2330 train_time:59186ms step_avg:59.90ms
step:989/2330 train_time:59245ms step_avg:59.90ms
step:990/2330 train_time:59306ms step_avg:59.91ms
step:991/2330 train_time:59365ms step_avg:59.90ms
step:992/2330 train_time:59426ms step_avg:59.91ms
step:993/2330 train_time:59485ms step_avg:59.90ms
step:994/2330 train_time:59546ms step_avg:59.91ms
step:995/2330 train_time:59605ms step_avg:59.90ms
step:996/2330 train_time:59667ms step_avg:59.91ms
step:997/2330 train_time:59726ms step_avg:59.91ms
step:998/2330 train_time:59787ms step_avg:59.91ms
step:999/2330 train_time:59846ms step_avg:59.91ms
step:1000/2330 train_time:59908ms step_avg:59.91ms
step:1000/2330 val_loss:3.5764 train_time:59971ms step_avg:59.97ms
step:1001/2330 train_time:59991ms step_avg:59.93ms
step:1002/2330 train_time:60032ms step_avg:59.91ms
step:1003/2330 train_time:60091ms step_avg:59.91ms
step:1004/2330 train_time:60153ms step_avg:59.91ms
step:1005/2330 train_time:60215ms step_avg:59.92ms
step:1006/2330 train_time:60277ms step_avg:59.92ms
step:1007/2330 train_time:60336ms step_avg:59.92ms
step:1008/2330 train_time:60397ms step_avg:59.92ms
step:1009/2330 train_time:60455ms step_avg:59.92ms
step:1010/2330 train_time:60516ms step_avg:59.92ms
step:1011/2330 train_time:60574ms step_avg:59.92ms
step:1012/2330 train_time:60635ms step_avg:59.92ms
step:1013/2330 train_time:60693ms step_avg:59.91ms
step:1014/2330 train_time:60754ms step_avg:59.92ms
step:1015/2330 train_time:60812ms step_avg:59.91ms
step:1016/2330 train_time:60874ms step_avg:59.91ms
step:1017/2330 train_time:60935ms step_avg:59.92ms
step:1018/2330 train_time:60997ms step_avg:59.92ms
step:1019/2330 train_time:61057ms step_avg:59.92ms
step:1020/2330 train_time:61120ms step_avg:59.92ms
step:1021/2330 train_time:61179ms step_avg:59.92ms
step:1022/2330 train_time:61241ms step_avg:59.92ms
step:1023/2330 train_time:61300ms step_avg:59.92ms
step:1024/2330 train_time:61361ms step_avg:59.92ms
step:1025/2330 train_time:61420ms step_avg:59.92ms
step:1026/2330 train_time:61481ms step_avg:59.92ms
step:1027/2330 train_time:61539ms step_avg:59.92ms
step:1028/2330 train_time:61600ms step_avg:59.92ms
step:1029/2330 train_time:61658ms step_avg:59.92ms
step:1030/2330 train_time:61719ms step_avg:59.92ms
step:1031/2330 train_time:61778ms step_avg:59.92ms
step:1032/2330 train_time:61839ms step_avg:59.92ms
step:1033/2330 train_time:61899ms step_avg:59.92ms
step:1034/2330 train_time:61961ms step_avg:59.92ms
step:1035/2330 train_time:62022ms step_avg:59.92ms
step:1036/2330 train_time:62083ms step_avg:59.93ms
step:1037/2330 train_time:62143ms step_avg:59.93ms
step:1038/2330 train_time:62204ms step_avg:59.93ms
step:1039/2330 train_time:62264ms step_avg:59.93ms
step:1040/2330 train_time:62326ms step_avg:59.93ms
step:1041/2330 train_time:62385ms step_avg:59.93ms
step:1042/2330 train_time:62448ms step_avg:59.93ms
step:1043/2330 train_time:62507ms step_avg:59.93ms
step:1044/2330 train_time:62569ms step_avg:59.93ms
step:1045/2330 train_time:62628ms step_avg:59.93ms
step:1046/2330 train_time:62691ms step_avg:59.93ms
step:1047/2330 train_time:62750ms step_avg:59.93ms
step:1048/2330 train_time:62812ms step_avg:59.94ms
step:1049/2330 train_time:62872ms step_avg:59.94ms
step:1050/2330 train_time:62934ms step_avg:59.94ms
step:1051/2330 train_time:62993ms step_avg:59.94ms
step:1052/2330 train_time:63054ms step_avg:59.94ms
step:1053/2330 train_time:63114ms step_avg:59.94ms
step:1054/2330 train_time:63175ms step_avg:59.94ms
step:1055/2330 train_time:63234ms step_avg:59.94ms
step:1056/2330 train_time:63295ms step_avg:59.94ms
step:1057/2330 train_time:63355ms step_avg:59.94ms
step:1058/2330 train_time:63417ms step_avg:59.94ms
step:1059/2330 train_time:63476ms step_avg:59.94ms
step:1060/2330 train_time:63538ms step_avg:59.94ms
step:1061/2330 train_time:63597ms step_avg:59.94ms
step:1062/2330 train_time:63659ms step_avg:59.94ms
step:1063/2330 train_time:63718ms step_avg:59.94ms
step:1064/2330 train_time:63779ms step_avg:59.94ms
step:1065/2330 train_time:63839ms step_avg:59.94ms
step:1066/2330 train_time:63900ms step_avg:59.94ms
step:1067/2330 train_time:63959ms step_avg:59.94ms
step:1068/2330 train_time:64021ms step_avg:59.94ms
step:1069/2330 train_time:64080ms step_avg:59.94ms
step:1070/2330 train_time:64141ms step_avg:59.95ms
step:1071/2330 train_time:64200ms step_avg:59.94ms
step:1072/2330 train_time:64262ms step_avg:59.95ms
step:1073/2330 train_time:64321ms step_avg:59.94ms
step:1074/2330 train_time:64382ms step_avg:59.95ms
step:1075/2330 train_time:64441ms step_avg:59.95ms
step:1076/2330 train_time:64502ms step_avg:59.95ms
step:1077/2330 train_time:64562ms step_avg:59.95ms
step:1078/2330 train_time:64624ms step_avg:59.95ms
step:1079/2330 train_time:64683ms step_avg:59.95ms
step:1080/2330 train_time:64744ms step_avg:59.95ms
step:1081/2330 train_time:64803ms step_avg:59.95ms
step:1082/2330 train_time:64864ms step_avg:59.95ms
step:1083/2330 train_time:64924ms step_avg:59.95ms
step:1084/2330 train_time:64986ms step_avg:59.95ms
step:1085/2330 train_time:65045ms step_avg:59.95ms
step:1086/2330 train_time:65108ms step_avg:59.95ms
step:1087/2330 train_time:65167ms step_avg:59.95ms
step:1088/2330 train_time:65229ms step_avg:59.95ms
step:1089/2330 train_time:65289ms step_avg:59.95ms
step:1090/2330 train_time:65351ms step_avg:59.96ms
step:1091/2330 train_time:65410ms step_avg:59.95ms
step:1092/2330 train_time:65472ms step_avg:59.96ms
step:1093/2330 train_time:65531ms step_avg:59.96ms
step:1094/2330 train_time:65592ms step_avg:59.96ms
step:1095/2330 train_time:65651ms step_avg:59.96ms
step:1096/2330 train_time:65712ms step_avg:59.96ms
step:1097/2330 train_time:65771ms step_avg:59.96ms
step:1098/2330 train_time:65832ms step_avg:59.96ms
step:1099/2330 train_time:65892ms step_avg:59.96ms
step:1100/2330 train_time:65953ms step_avg:59.96ms
step:1101/2330 train_time:66012ms step_avg:59.96ms
step:1102/2330 train_time:66073ms step_avg:59.96ms
step:1103/2330 train_time:66133ms step_avg:59.96ms
step:1104/2330 train_time:66194ms step_avg:59.96ms
step:1105/2330 train_time:66253ms step_avg:59.96ms
step:1106/2330 train_time:66315ms step_avg:59.96ms
step:1107/2330 train_time:66374ms step_avg:59.96ms
step:1108/2330 train_time:66435ms step_avg:59.96ms
step:1109/2330 train_time:66494ms step_avg:59.96ms
step:1110/2330 train_time:66556ms step_avg:59.96ms
step:1111/2330 train_time:66615ms step_avg:59.96ms
step:1112/2330 train_time:66675ms step_avg:59.96ms
step:1113/2330 train_time:66734ms step_avg:59.96ms
step:1114/2330 train_time:66795ms step_avg:59.96ms
step:1115/2330 train_time:66854ms step_avg:59.96ms
step:1116/2330 train_time:66916ms step_avg:59.96ms
step:1117/2330 train_time:66974ms step_avg:59.96ms
step:1118/2330 train_time:67036ms step_avg:59.96ms
step:1119/2330 train_time:67095ms step_avg:59.96ms
step:1120/2330 train_time:67156ms step_avg:59.96ms
step:1121/2330 train_time:67216ms step_avg:59.96ms
step:1122/2330 train_time:67277ms step_avg:59.96ms
step:1123/2330 train_time:67336ms step_avg:59.96ms
step:1124/2330 train_time:67397ms step_avg:59.96ms
step:1125/2330 train_time:67457ms step_avg:59.96ms
step:1126/2330 train_time:67517ms step_avg:59.96ms
step:1127/2330 train_time:67576ms step_avg:59.96ms
step:1128/2330 train_time:67637ms step_avg:59.96ms
step:1129/2330 train_time:67697ms step_avg:59.96ms
step:1130/2330 train_time:67758ms step_avg:59.96ms
step:1131/2330 train_time:67817ms step_avg:59.96ms
step:1132/2330 train_time:67878ms step_avg:59.96ms
step:1133/2330 train_time:67937ms step_avg:59.96ms
step:1134/2330 train_time:67999ms step_avg:59.96ms
step:1135/2330 train_time:68058ms step_avg:59.96ms
step:1136/2330 train_time:68119ms step_avg:59.96ms
step:1137/2330 train_time:68178ms step_avg:59.96ms
step:1138/2330 train_time:68239ms step_avg:59.96ms
step:1139/2330 train_time:68298ms step_avg:59.96ms
step:1140/2330 train_time:68360ms step_avg:59.97ms
step:1141/2330 train_time:68420ms step_avg:59.96ms
step:1142/2330 train_time:68480ms step_avg:59.97ms
step:1143/2330 train_time:68539ms step_avg:59.96ms
step:1144/2330 train_time:68600ms step_avg:59.96ms
step:1145/2330 train_time:68659ms step_avg:59.96ms
step:1146/2330 train_time:68721ms step_avg:59.97ms
step:1147/2330 train_time:68779ms step_avg:59.96ms
step:1148/2330 train_time:68840ms step_avg:59.97ms
step:1149/2330 train_time:68899ms step_avg:59.96ms
step:1150/2330 train_time:68960ms step_avg:59.97ms
step:1151/2330 train_time:69020ms step_avg:59.97ms
step:1152/2330 train_time:69082ms step_avg:59.97ms
step:1153/2330 train_time:69141ms step_avg:59.97ms
step:1154/2330 train_time:69202ms step_avg:59.97ms
step:1155/2330 train_time:69261ms step_avg:59.97ms
step:1156/2330 train_time:69322ms step_avg:59.97ms
step:1157/2330 train_time:69382ms step_avg:59.97ms
step:1158/2330 train_time:69444ms step_avg:59.97ms
step:1159/2330 train_time:69503ms step_avg:59.97ms
step:1160/2330 train_time:69564ms step_avg:59.97ms
step:1161/2330 train_time:69624ms step_avg:59.97ms
step:1162/2330 train_time:69685ms step_avg:59.97ms
step:1163/2330 train_time:69745ms step_avg:59.97ms
step:1164/2330 train_time:69807ms step_avg:59.97ms
step:1165/2330 train_time:69866ms step_avg:59.97ms
step:1166/2330 train_time:69928ms step_avg:59.97ms
step:1167/2330 train_time:69988ms step_avg:59.97ms
step:1168/2330 train_time:70050ms step_avg:59.97ms
step:1169/2330 train_time:70109ms step_avg:59.97ms
step:1170/2330 train_time:70171ms step_avg:59.97ms
step:1171/2330 train_time:70230ms step_avg:59.97ms
step:1172/2330 train_time:70291ms step_avg:59.98ms
step:1173/2330 train_time:70350ms step_avg:59.97ms
step:1174/2330 train_time:70412ms step_avg:59.98ms
step:1175/2330 train_time:70472ms step_avg:59.98ms
step:1176/2330 train_time:70534ms step_avg:59.98ms
step:1177/2330 train_time:70593ms step_avg:59.98ms
step:1178/2330 train_time:70654ms step_avg:59.98ms
step:1179/2330 train_time:70713ms step_avg:59.98ms
step:1180/2330 train_time:70774ms step_avg:59.98ms
step:1181/2330 train_time:70834ms step_avg:59.98ms
step:1182/2330 train_time:70896ms step_avg:59.98ms
step:1183/2330 train_time:70955ms step_avg:59.98ms
step:1184/2330 train_time:71017ms step_avg:59.98ms
step:1185/2330 train_time:71076ms step_avg:59.98ms
step:1186/2330 train_time:71137ms step_avg:59.98ms
step:1187/2330 train_time:71196ms step_avg:59.98ms
step:1188/2330 train_time:71257ms step_avg:59.98ms
step:1189/2330 train_time:71316ms step_avg:59.98ms
step:1190/2330 train_time:71378ms step_avg:59.98ms
step:1191/2330 train_time:71437ms step_avg:59.98ms
step:1192/2330 train_time:71498ms step_avg:59.98ms
step:1193/2330 train_time:71557ms step_avg:59.98ms
step:1194/2330 train_time:71619ms step_avg:59.98ms
step:1195/2330 train_time:71677ms step_avg:59.98ms
step:1196/2330 train_time:71738ms step_avg:59.98ms
step:1197/2330 train_time:71798ms step_avg:59.98ms
step:1198/2330 train_time:71859ms step_avg:59.98ms
step:1199/2330 train_time:71919ms step_avg:59.98ms
step:1200/2330 train_time:71979ms step_avg:59.98ms
step:1201/2330 train_time:72038ms step_avg:59.98ms
step:1202/2330 train_time:72099ms step_avg:59.98ms
step:1203/2330 train_time:72159ms step_avg:59.98ms
step:1204/2330 train_time:72221ms step_avg:59.98ms
step:1205/2330 train_time:72280ms step_avg:59.98ms
step:1206/2330 train_time:72341ms step_avg:59.98ms
step:1207/2330 train_time:72400ms step_avg:59.98ms
step:1208/2330 train_time:72461ms step_avg:59.98ms
step:1209/2330 train_time:72520ms step_avg:59.98ms
step:1210/2330 train_time:72581ms step_avg:59.98ms
step:1211/2330 train_time:72640ms step_avg:59.98ms
step:1212/2330 train_time:72701ms step_avg:59.98ms
step:1213/2330 train_time:72761ms step_avg:59.98ms
step:1214/2330 train_time:72822ms step_avg:59.99ms
step:1215/2330 train_time:72881ms step_avg:59.98ms
step:1216/2330 train_time:72942ms step_avg:59.99ms
step:1217/2330 train_time:73001ms step_avg:59.98ms
step:1218/2330 train_time:73063ms step_avg:59.99ms
step:1219/2330 train_time:73123ms step_avg:59.99ms
step:1220/2330 train_time:73185ms step_avg:59.99ms
step:1221/2330 train_time:73246ms step_avg:59.99ms
step:1222/2330 train_time:73307ms step_avg:59.99ms
step:1223/2330 train_time:73367ms step_avg:59.99ms
step:1224/2330 train_time:73428ms step_avg:59.99ms
step:1225/2330 train_time:73487ms step_avg:59.99ms
step:1226/2330 train_time:73549ms step_avg:59.99ms
step:1227/2330 train_time:73608ms step_avg:59.99ms
step:1228/2330 train_time:73670ms step_avg:59.99ms
step:1229/2330 train_time:73729ms step_avg:59.99ms
step:1230/2330 train_time:73791ms step_avg:59.99ms
step:1231/2330 train_time:73850ms step_avg:59.99ms
step:1232/2330 train_time:73911ms step_avg:59.99ms
step:1233/2330 train_time:73971ms step_avg:59.99ms
step:1234/2330 train_time:74032ms step_avg:59.99ms
step:1235/2330 train_time:74091ms step_avg:59.99ms
step:1236/2330 train_time:74153ms step_avg:59.99ms
step:1237/2330 train_time:74212ms step_avg:59.99ms
step:1238/2330 train_time:74273ms step_avg:59.99ms
step:1239/2330 train_time:74333ms step_avg:59.99ms
step:1240/2330 train_time:74394ms step_avg:59.99ms
step:1241/2330 train_time:74453ms step_avg:59.99ms
step:1242/2330 train_time:74514ms step_avg:60.00ms
step:1243/2330 train_time:74574ms step_avg:59.99ms
step:1244/2330 train_time:74635ms step_avg:60.00ms
step:1245/2330 train_time:74694ms step_avg:60.00ms
step:1246/2330 train_time:74755ms step_avg:60.00ms
step:1247/2330 train_time:74815ms step_avg:60.00ms
step:1248/2330 train_time:74876ms step_avg:60.00ms
step:1249/2330 train_time:74935ms step_avg:60.00ms
step:1250/2330 train_time:74997ms step_avg:60.00ms
step:1250/2330 val_loss:3.5163 train_time:75060ms step_avg:60.05ms
step:1251/2330 train_time:75080ms step_avg:60.02ms
step:1252/2330 train_time:75120ms step_avg:60.00ms
step:1253/2330 train_time:75181ms step_avg:60.00ms
step:1254/2330 train_time:75247ms step_avg:60.01ms
step:1255/2330 train_time:75308ms step_avg:60.01ms
step:1256/2330 train_time:75371ms step_avg:60.01ms
step:1257/2330 train_time:75430ms step_avg:60.01ms
step:1258/2330 train_time:75491ms step_avg:60.01ms
step:1259/2330 train_time:75550ms step_avg:60.01ms
step:1260/2330 train_time:75611ms step_avg:60.01ms
step:1261/2330 train_time:75669ms step_avg:60.01ms
step:1262/2330 train_time:75730ms step_avg:60.01ms
step:1263/2330 train_time:75789ms step_avg:60.01ms
step:1264/2330 train_time:75849ms step_avg:60.01ms
step:1265/2330 train_time:75907ms step_avg:60.01ms
step:1266/2330 train_time:75968ms step_avg:60.01ms
step:1267/2330 train_time:76027ms step_avg:60.01ms
step:1268/2330 train_time:76089ms step_avg:60.01ms
step:1269/2330 train_time:76150ms step_avg:60.01ms
step:1270/2330 train_time:76212ms step_avg:60.01ms
step:1271/2330 train_time:76272ms step_avg:60.01ms
step:1272/2330 train_time:76334ms step_avg:60.01ms
step:1273/2330 train_time:76393ms step_avg:60.01ms
step:1274/2330 train_time:76454ms step_avg:60.01ms
step:1275/2330 train_time:76513ms step_avg:60.01ms
step:1276/2330 train_time:76574ms step_avg:60.01ms
step:1277/2330 train_time:76633ms step_avg:60.01ms
step:1278/2330 train_time:76693ms step_avg:60.01ms
step:1279/2330 train_time:76752ms step_avg:60.01ms
step:1280/2330 train_time:76813ms step_avg:60.01ms
step:1281/2330 train_time:76871ms step_avg:60.01ms
step:1282/2330 train_time:76932ms step_avg:60.01ms
step:1283/2330 train_time:76991ms step_avg:60.01ms
step:1284/2330 train_time:77053ms step_avg:60.01ms
step:1285/2330 train_time:77112ms step_avg:60.01ms
step:1286/2330 train_time:77175ms step_avg:60.01ms
step:1287/2330 train_time:77235ms step_avg:60.01ms
step:1288/2330 train_time:77297ms step_avg:60.01ms
step:1289/2330 train_time:77356ms step_avg:60.01ms
step:1290/2330 train_time:77418ms step_avg:60.01ms
step:1291/2330 train_time:77477ms step_avg:60.01ms
step:1292/2330 train_time:77538ms step_avg:60.01ms
step:1293/2330 train_time:77597ms step_avg:60.01ms
step:1294/2330 train_time:77658ms step_avg:60.01ms
step:1295/2330 train_time:77717ms step_avg:60.01ms
step:1296/2330 train_time:77779ms step_avg:60.01ms
step:1297/2330 train_time:77839ms step_avg:60.01ms
step:1298/2330 train_time:77900ms step_avg:60.02ms
step:1299/2330 train_time:77960ms step_avg:60.02ms
step:1300/2330 train_time:78022ms step_avg:60.02ms
step:1301/2330 train_time:78082ms step_avg:60.02ms
step:1302/2330 train_time:78144ms step_avg:60.02ms
step:1303/2330 train_time:78203ms step_avg:60.02ms
step:1304/2330 train_time:78265ms step_avg:60.02ms
step:1305/2330 train_time:78325ms step_avg:60.02ms
step:1306/2330 train_time:78386ms step_avg:60.02ms
step:1307/2330 train_time:78446ms step_avg:60.02ms
step:1308/2330 train_time:78507ms step_avg:60.02ms
step:1309/2330 train_time:78566ms step_avg:60.02ms
step:1310/2330 train_time:78627ms step_avg:60.02ms
step:1311/2330 train_time:78686ms step_avg:60.02ms
step:1312/2330 train_time:78747ms step_avg:60.02ms
step:1313/2330 train_time:78806ms step_avg:60.02ms
step:1314/2330 train_time:78867ms step_avg:60.02ms
step:1315/2330 train_time:78926ms step_avg:60.02ms
step:1316/2330 train_time:78987ms step_avg:60.02ms
step:1317/2330 train_time:79047ms step_avg:60.02ms
step:1318/2330 train_time:79108ms step_avg:60.02ms
step:1319/2330 train_time:79168ms step_avg:60.02ms
step:1320/2330 train_time:79229ms step_avg:60.02ms
step:1321/2330 train_time:79288ms step_avg:60.02ms
step:1322/2330 train_time:79349ms step_avg:60.02ms
step:1323/2330 train_time:79409ms step_avg:60.02ms
step:1324/2330 train_time:79470ms step_avg:60.02ms
step:1325/2330 train_time:79529ms step_avg:60.02ms
step:1326/2330 train_time:79590ms step_avg:60.02ms
step:1327/2330 train_time:79649ms step_avg:60.02ms
step:1328/2330 train_time:79710ms step_avg:60.02ms
step:1329/2330 train_time:79769ms step_avg:60.02ms
step:1330/2330 train_time:79830ms step_avg:60.02ms
step:1331/2330 train_time:79889ms step_avg:60.02ms
step:1332/2330 train_time:79951ms step_avg:60.02ms
step:1333/2330 train_time:80010ms step_avg:60.02ms
step:1334/2330 train_time:80072ms step_avg:60.02ms
step:1335/2330 train_time:80131ms step_avg:60.02ms
step:1336/2330 train_time:80193ms step_avg:60.02ms
step:1337/2330 train_time:80252ms step_avg:60.02ms
step:1338/2330 train_time:80313ms step_avg:60.02ms
step:1339/2330 train_time:80372ms step_avg:60.02ms
step:1340/2330 train_time:80433ms step_avg:60.02ms
step:1341/2330 train_time:80493ms step_avg:60.02ms
step:1342/2330 train_time:80554ms step_avg:60.03ms
step:1343/2330 train_time:80613ms step_avg:60.02ms
step:1344/2330 train_time:80674ms step_avg:60.03ms
step:1345/2330 train_time:80733ms step_avg:60.02ms
step:1346/2330 train_time:80795ms step_avg:60.03ms
step:1347/2330 train_time:80854ms step_avg:60.03ms
step:1348/2330 train_time:80915ms step_avg:60.03ms
step:1349/2330 train_time:80974ms step_avg:60.03ms
step:1350/2330 train_time:81036ms step_avg:60.03ms
step:1351/2330 train_time:81095ms step_avg:60.03ms
step:1352/2330 train_time:81156ms step_avg:60.03ms
step:1353/2330 train_time:81216ms step_avg:60.03ms
step:1354/2330 train_time:81277ms step_avg:60.03ms
step:1355/2330 train_time:81337ms step_avg:60.03ms
step:1356/2330 train_time:81399ms step_avg:60.03ms
step:1357/2330 train_time:81459ms step_avg:60.03ms
step:1358/2330 train_time:81520ms step_avg:60.03ms
step:1359/2330 train_time:81580ms step_avg:60.03ms
step:1360/2330 train_time:81642ms step_avg:60.03ms
step:1361/2330 train_time:81701ms step_avg:60.03ms
step:1362/2330 train_time:81763ms step_avg:60.03ms
step:1363/2330 train_time:81822ms step_avg:60.03ms
step:1364/2330 train_time:81884ms step_avg:60.03ms
step:1365/2330 train_time:81943ms step_avg:60.03ms
step:1366/2330 train_time:82005ms step_avg:60.03ms
step:1367/2330 train_time:82065ms step_avg:60.03ms
step:1368/2330 train_time:82127ms step_avg:60.03ms
step:1369/2330 train_time:82186ms step_avg:60.03ms
step:1370/2330 train_time:82247ms step_avg:60.03ms
step:1371/2330 train_time:82307ms step_avg:60.03ms
step:1372/2330 train_time:82368ms step_avg:60.04ms
step:1373/2330 train_time:82427ms step_avg:60.03ms
step:1374/2330 train_time:82489ms step_avg:60.04ms
step:1375/2330 train_time:82548ms step_avg:60.04ms
step:1376/2330 train_time:82610ms step_avg:60.04ms
step:1377/2330 train_time:82669ms step_avg:60.04ms
step:1378/2330 train_time:82730ms step_avg:60.04ms
step:1379/2330 train_time:82789ms step_avg:60.04ms
step:1380/2330 train_time:82851ms step_avg:60.04ms
step:1381/2330 train_time:82911ms step_avg:60.04ms
step:1382/2330 train_time:82972ms step_avg:60.04ms
step:1383/2330 train_time:83031ms step_avg:60.04ms
step:1384/2330 train_time:83093ms step_avg:60.04ms
step:1385/2330 train_time:83152ms step_avg:60.04ms
step:1386/2330 train_time:83214ms step_avg:60.04ms
step:1387/2330 train_time:83272ms step_avg:60.04ms
step:1388/2330 train_time:83334ms step_avg:60.04ms
step:1389/2330 train_time:83392ms step_avg:60.04ms
step:1390/2330 train_time:83454ms step_avg:60.04ms
step:1391/2330 train_time:83513ms step_avg:60.04ms
step:1392/2330 train_time:83574ms step_avg:60.04ms
step:1393/2330 train_time:83633ms step_avg:60.04ms
step:1394/2330 train_time:83694ms step_avg:60.04ms
step:1395/2330 train_time:83754ms step_avg:60.04ms
step:1396/2330 train_time:83815ms step_avg:60.04ms
step:1397/2330 train_time:83874ms step_avg:60.04ms
step:1398/2330 train_time:83935ms step_avg:60.04ms
step:1399/2330 train_time:83995ms step_avg:60.04ms
step:1400/2330 train_time:84056ms step_avg:60.04ms
step:1401/2330 train_time:84115ms step_avg:60.04ms
step:1402/2330 train_time:84176ms step_avg:60.04ms
step:1403/2330 train_time:84236ms step_avg:60.04ms
step:1404/2330 train_time:84297ms step_avg:60.04ms
step:1405/2330 train_time:84356ms step_avg:60.04ms
step:1406/2330 train_time:84417ms step_avg:60.04ms
step:1407/2330 train_time:84476ms step_avg:60.04ms
step:1408/2330 train_time:84538ms step_avg:60.04ms
step:1409/2330 train_time:84597ms step_avg:60.04ms
step:1410/2330 train_time:84659ms step_avg:60.04ms
step:1411/2330 train_time:84718ms step_avg:60.04ms
step:1412/2330 train_time:84780ms step_avg:60.04ms
step:1413/2330 train_time:84839ms step_avg:60.04ms
step:1414/2330 train_time:84901ms step_avg:60.04ms
step:1415/2330 train_time:84960ms step_avg:60.04ms
step:1416/2330 train_time:85022ms step_avg:60.04ms
step:1417/2330 train_time:85082ms step_avg:60.04ms
step:1418/2330 train_time:85143ms step_avg:60.04ms
step:1419/2330 train_time:85203ms step_avg:60.04ms
step:1420/2330 train_time:85265ms step_avg:60.05ms
step:1421/2330 train_time:85324ms step_avg:60.05ms
step:1422/2330 train_time:85385ms step_avg:60.05ms
step:1423/2330 train_time:85445ms step_avg:60.05ms
step:1424/2330 train_time:85507ms step_avg:60.05ms
step:1425/2330 train_time:85566ms step_avg:60.05ms
step:1426/2330 train_time:85627ms step_avg:60.05ms
step:1427/2330 train_time:85686ms step_avg:60.05ms
step:1428/2330 train_time:85748ms step_avg:60.05ms
step:1429/2330 train_time:85807ms step_avg:60.05ms
step:1430/2330 train_time:85869ms step_avg:60.05ms
step:1431/2330 train_time:85928ms step_avg:60.05ms
step:1432/2330 train_time:85990ms step_avg:60.05ms
step:1433/2330 train_time:86049ms step_avg:60.05ms
step:1434/2330 train_time:86111ms step_avg:60.05ms
step:1435/2330 train_time:86170ms step_avg:60.05ms
step:1436/2330 train_time:86231ms step_avg:60.05ms
step:1437/2330 train_time:86291ms step_avg:60.05ms
step:1438/2330 train_time:86353ms step_avg:60.05ms
step:1439/2330 train_time:86412ms step_avg:60.05ms
step:1440/2330 train_time:86473ms step_avg:60.05ms
step:1441/2330 train_time:86531ms step_avg:60.05ms
step:1442/2330 train_time:86592ms step_avg:60.05ms
step:1443/2330 train_time:86651ms step_avg:60.05ms
step:1444/2330 train_time:86713ms step_avg:60.05ms
step:1445/2330 train_time:86772ms step_avg:60.05ms
step:1446/2330 train_time:86833ms step_avg:60.05ms
step:1447/2330 train_time:86892ms step_avg:60.05ms
step:1448/2330 train_time:86953ms step_avg:60.05ms
step:1449/2330 train_time:87012ms step_avg:60.05ms
step:1450/2330 train_time:87074ms step_avg:60.05ms
step:1451/2330 train_time:87133ms step_avg:60.05ms
step:1452/2330 train_time:87194ms step_avg:60.05ms
step:1453/2330 train_time:87254ms step_avg:60.05ms
step:1454/2330 train_time:87315ms step_avg:60.05ms
step:1455/2330 train_time:87375ms step_avg:60.05ms
step:1456/2330 train_time:87436ms step_avg:60.05ms
step:1457/2330 train_time:87495ms step_avg:60.05ms
step:1458/2330 train_time:87557ms step_avg:60.05ms
step:1459/2330 train_time:87615ms step_avg:60.05ms
step:1460/2330 train_time:87676ms step_avg:60.05ms
step:1461/2330 train_time:87735ms step_avg:60.05ms
step:1462/2330 train_time:87797ms step_avg:60.05ms
step:1463/2330 train_time:87857ms step_avg:60.05ms
step:1464/2330 train_time:87919ms step_avg:60.05ms
step:1465/2330 train_time:87978ms step_avg:60.05ms
step:1466/2330 train_time:88040ms step_avg:60.05ms
step:1467/2330 train_time:88099ms step_avg:60.05ms
step:1468/2330 train_time:88161ms step_avg:60.05ms
step:1469/2330 train_time:88220ms step_avg:60.05ms
step:1470/2330 train_time:88282ms step_avg:60.06ms
step:1471/2330 train_time:88342ms step_avg:60.06ms
step:1472/2330 train_time:88403ms step_avg:60.06ms
step:1473/2330 train_time:88463ms step_avg:60.06ms
step:1474/2330 train_time:88525ms step_avg:60.06ms
step:1475/2330 train_time:88584ms step_avg:60.06ms
step:1476/2330 train_time:88646ms step_avg:60.06ms
step:1477/2330 train_time:88705ms step_avg:60.06ms
step:1478/2330 train_time:88767ms step_avg:60.06ms
step:1479/2330 train_time:88826ms step_avg:60.06ms
step:1480/2330 train_time:88887ms step_avg:60.06ms
step:1481/2330 train_time:88947ms step_avg:60.06ms
step:1482/2330 train_time:89008ms step_avg:60.06ms
step:1483/2330 train_time:89067ms step_avg:60.06ms
step:1484/2330 train_time:89128ms step_avg:60.06ms
step:1485/2330 train_time:89187ms step_avg:60.06ms
step:1486/2330 train_time:89248ms step_avg:60.06ms
step:1487/2330 train_time:89308ms step_avg:60.06ms
step:1488/2330 train_time:89369ms step_avg:60.06ms
step:1489/2330 train_time:89429ms step_avg:60.06ms
step:1490/2330 train_time:89490ms step_avg:60.06ms
step:1491/2330 train_time:89549ms step_avg:60.06ms
step:1492/2330 train_time:89610ms step_avg:60.06ms
step:1493/2330 train_time:89669ms step_avg:60.06ms
step:1494/2330 train_time:89731ms step_avg:60.06ms
step:1495/2330 train_time:89790ms step_avg:60.06ms
step:1496/2330 train_time:89851ms step_avg:60.06ms
step:1497/2330 train_time:89910ms step_avg:60.06ms
step:1498/2330 train_time:89972ms step_avg:60.06ms
step:1499/2330 train_time:90031ms step_avg:60.06ms
step:1500/2330 train_time:90092ms step_avg:60.06ms
step:1500/2330 val_loss:3.4493 train_time:90155ms step_avg:60.10ms
step:1501/2330 train_time:90175ms step_avg:60.08ms
step:1502/2330 train_time:90214ms step_avg:60.06ms
step:1503/2330 train_time:90278ms step_avg:60.07ms
step:1504/2330 train_time:90342ms step_avg:60.07ms
step:1505/2330 train_time:90402ms step_avg:60.07ms
step:1506/2330 train_time:90463ms step_avg:60.07ms
step:1507/2330 train_time:90522ms step_avg:60.07ms
step:1508/2330 train_time:90583ms step_avg:60.07ms
step:1509/2330 train_time:90641ms step_avg:60.07ms
step:1510/2330 train_time:90702ms step_avg:60.07ms
step:1511/2330 train_time:90760ms step_avg:60.07ms
step:1512/2330 train_time:90821ms step_avg:60.07ms
step:1513/2330 train_time:90879ms step_avg:60.07ms
step:1514/2330 train_time:90940ms step_avg:60.07ms
step:1515/2330 train_time:90999ms step_avg:60.07ms
step:1516/2330 train_time:91059ms step_avg:60.07ms
step:1517/2330 train_time:91119ms step_avg:60.07ms
step:1518/2330 train_time:91181ms step_avg:60.07ms
step:1519/2330 train_time:91241ms step_avg:60.07ms
step:1520/2330 train_time:91304ms step_avg:60.07ms
step:1521/2330 train_time:91363ms step_avg:60.07ms
step:1522/2330 train_time:91425ms step_avg:60.07ms
step:1523/2330 train_time:91485ms step_avg:60.07ms
step:1524/2330 train_time:91546ms step_avg:60.07ms
step:1525/2330 train_time:91604ms step_avg:60.07ms
step:1526/2330 train_time:91665ms step_avg:60.07ms
step:1527/2330 train_time:91724ms step_avg:60.07ms
step:1528/2330 train_time:91785ms step_avg:60.07ms
step:1529/2330 train_time:91843ms step_avg:60.07ms
step:1530/2330 train_time:91905ms step_avg:60.07ms
step:1531/2330 train_time:91964ms step_avg:60.07ms
step:1532/2330 train_time:92026ms step_avg:60.07ms
step:1533/2330 train_time:92086ms step_avg:60.07ms
step:1534/2330 train_time:92148ms step_avg:60.07ms
step:1535/2330 train_time:92208ms step_avg:60.07ms
step:1536/2330 train_time:92270ms step_avg:60.07ms
step:1537/2330 train_time:92330ms step_avg:60.07ms
step:1538/2330 train_time:92393ms step_avg:60.07ms
step:1539/2330 train_time:92453ms step_avg:60.07ms
step:1540/2330 train_time:92515ms step_avg:60.07ms
step:1541/2330 train_time:92575ms step_avg:60.07ms
step:1542/2330 train_time:92637ms step_avg:60.08ms
step:1543/2330 train_time:92697ms step_avg:60.08ms
step:1544/2330 train_time:92759ms step_avg:60.08ms
step:1545/2330 train_time:92819ms step_avg:60.08ms
step:1546/2330 train_time:92880ms step_avg:60.08ms
step:1547/2330 train_time:92940ms step_avg:60.08ms
step:1548/2330 train_time:93001ms step_avg:60.08ms
step:1549/2330 train_time:93061ms step_avg:60.08ms
step:1550/2330 train_time:93123ms step_avg:60.08ms
step:1551/2330 train_time:93182ms step_avg:60.08ms
step:1552/2330 train_time:93244ms step_avg:60.08ms
step:1553/2330 train_time:93304ms step_avg:60.08ms
step:1554/2330 train_time:93367ms step_avg:60.08ms
step:1555/2330 train_time:93427ms step_avg:60.08ms
step:1556/2330 train_time:93490ms step_avg:60.08ms
step:1557/2330 train_time:93549ms step_avg:60.08ms
step:1558/2330 train_time:93611ms step_avg:60.08ms
step:1559/2330 train_time:93671ms step_avg:60.08ms
step:1560/2330 train_time:93733ms step_avg:60.09ms
step:1561/2330 train_time:93793ms step_avg:60.08ms
step:1562/2330 train_time:93855ms step_avg:60.09ms
step:1563/2330 train_time:93915ms step_avg:60.09ms
step:1564/2330 train_time:93977ms step_avg:60.09ms
step:1565/2330 train_time:94038ms step_avg:60.09ms
step:1566/2330 train_time:94100ms step_avg:60.09ms
step:1567/2330 train_time:94160ms step_avg:60.09ms
step:1568/2330 train_time:94221ms step_avg:60.09ms
step:1569/2330 train_time:94280ms step_avg:60.09ms
step:1570/2330 train_time:94342ms step_avg:60.09ms
step:1571/2330 train_time:94402ms step_avg:60.09ms
step:1572/2330 train_time:94464ms step_avg:60.09ms
step:1573/2330 train_time:94524ms step_avg:60.09ms
step:1574/2330 train_time:94586ms step_avg:60.09ms
step:1575/2330 train_time:94647ms step_avg:60.09ms
step:1576/2330 train_time:94709ms step_avg:60.09ms
step:1577/2330 train_time:94768ms step_avg:60.09ms
step:1578/2330 train_time:94830ms step_avg:60.09ms
step:1579/2330 train_time:94890ms step_avg:60.09ms
step:1580/2330 train_time:94952ms step_avg:60.10ms
step:1581/2330 train_time:95011ms step_avg:60.10ms
step:1582/2330 train_time:95073ms step_avg:60.10ms
step:1583/2330 train_time:95133ms step_avg:60.10ms
step:1584/2330 train_time:95195ms step_avg:60.10ms
step:1585/2330 train_time:95255ms step_avg:60.10ms
step:1586/2330 train_time:95318ms step_avg:60.10ms
step:1587/2330 train_time:95378ms step_avg:60.10ms
step:1588/2330 train_time:95440ms step_avg:60.10ms
step:1589/2330 train_time:95500ms step_avg:60.10ms
step:1590/2330 train_time:95562ms step_avg:60.10ms
step:1591/2330 train_time:95621ms step_avg:60.10ms
step:1592/2330 train_time:95683ms step_avg:60.10ms
step:1593/2330 train_time:95743ms step_avg:60.10ms
step:1594/2330 train_time:95805ms step_avg:60.10ms
step:1595/2330 train_time:95865ms step_avg:60.10ms
step:1596/2330 train_time:95927ms step_avg:60.10ms
step:1597/2330 train_time:95987ms step_avg:60.10ms
step:1598/2330 train_time:96048ms step_avg:60.11ms
step:1599/2330 train_time:96108ms step_avg:60.11ms
step:1600/2330 train_time:96170ms step_avg:60.11ms
step:1601/2330 train_time:96230ms step_avg:60.11ms
step:1602/2330 train_time:96292ms step_avg:60.11ms
step:1603/2330 train_time:96353ms step_avg:60.11ms
step:1604/2330 train_time:96417ms step_avg:60.11ms
step:1605/2330 train_time:96477ms step_avg:60.11ms
step:1606/2330 train_time:96539ms step_avg:60.11ms
step:1607/2330 train_time:96599ms step_avg:60.11ms
step:1608/2330 train_time:96661ms step_avg:60.11ms
step:1609/2330 train_time:96720ms step_avg:60.11ms
step:1610/2330 train_time:96782ms step_avg:60.11ms
step:1611/2330 train_time:96841ms step_avg:60.11ms
step:1612/2330 train_time:96903ms step_avg:60.11ms
step:1613/2330 train_time:96963ms step_avg:60.11ms
step:1614/2330 train_time:97024ms step_avg:60.11ms
step:1615/2330 train_time:97084ms step_avg:60.11ms
step:1616/2330 train_time:97146ms step_avg:60.12ms
step:1617/2330 train_time:97206ms step_avg:60.12ms
step:1618/2330 train_time:97268ms step_avg:60.12ms
step:1619/2330 train_time:97329ms step_avg:60.12ms
step:1620/2330 train_time:97391ms step_avg:60.12ms
step:1621/2330 train_time:97450ms step_avg:60.12ms
step:1622/2330 train_time:97512ms step_avg:60.12ms
step:1623/2330 train_time:97572ms step_avg:60.12ms
step:1624/2330 train_time:97635ms step_avg:60.12ms
step:1625/2330 train_time:97695ms step_avg:60.12ms
step:1626/2330 train_time:97757ms step_avg:60.12ms
step:1627/2330 train_time:97818ms step_avg:60.12ms
step:1628/2330 train_time:97880ms step_avg:60.12ms
step:1629/2330 train_time:97940ms step_avg:60.12ms
step:1630/2330 train_time:98002ms step_avg:60.12ms
step:1631/2330 train_time:98061ms step_avg:60.12ms
step:1632/2330 train_time:98122ms step_avg:60.12ms
step:1633/2330 train_time:98182ms step_avg:60.12ms
step:1634/2330 train_time:98244ms step_avg:60.12ms
step:1635/2330 train_time:98304ms step_avg:60.12ms
step:1636/2330 train_time:98366ms step_avg:60.13ms
step:1637/2330 train_time:98426ms step_avg:60.13ms
step:1638/2330 train_time:98489ms step_avg:60.13ms
step:1639/2330 train_time:98550ms step_avg:60.13ms
step:1640/2330 train_time:98611ms step_avg:60.13ms
step:1641/2330 train_time:98671ms step_avg:60.13ms
step:1642/2330 train_time:98732ms step_avg:60.13ms
step:1643/2330 train_time:98792ms step_avg:60.13ms
step:1644/2330 train_time:98854ms step_avg:60.13ms
step:1645/2330 train_time:98914ms step_avg:60.13ms
step:1646/2330 train_time:98977ms step_avg:60.13ms
step:1647/2330 train_time:99038ms step_avg:60.13ms
step:1648/2330 train_time:99099ms step_avg:60.13ms
step:1649/2330 train_time:99160ms step_avg:60.13ms
step:1650/2330 train_time:99222ms step_avg:60.13ms
step:1651/2330 train_time:99281ms step_avg:60.13ms
step:1652/2330 train_time:99343ms step_avg:60.14ms
step:1653/2330 train_time:99403ms step_avg:60.13ms
step:1654/2330 train_time:99465ms step_avg:60.14ms
step:1655/2330 train_time:99525ms step_avg:60.14ms
step:1656/2330 train_time:99587ms step_avg:60.14ms
step:1657/2330 train_time:99647ms step_avg:60.14ms
step:1658/2330 train_time:99709ms step_avg:60.14ms
step:1659/2330 train_time:99769ms step_avg:60.14ms
step:1660/2330 train_time:99831ms step_avg:60.14ms
step:1661/2330 train_time:99891ms step_avg:60.14ms
step:1662/2330 train_time:99953ms step_avg:60.14ms
step:1663/2330 train_time:100013ms step_avg:60.14ms
step:1664/2330 train_time:100075ms step_avg:60.14ms
step:1665/2330 train_time:100136ms step_avg:60.14ms
step:1666/2330 train_time:100198ms step_avg:60.14ms
step:1667/2330 train_time:100259ms step_avg:60.14ms
step:1668/2330 train_time:100321ms step_avg:60.14ms
step:1669/2330 train_time:100380ms step_avg:60.14ms
step:1670/2330 train_time:100442ms step_avg:60.14ms
step:1671/2330 train_time:100502ms step_avg:60.14ms
step:1672/2330 train_time:100563ms step_avg:60.15ms
step:1673/2330 train_time:100623ms step_avg:60.15ms
step:1674/2330 train_time:100686ms step_avg:60.15ms
step:1675/2330 train_time:100745ms step_avg:60.15ms
step:1676/2330 train_time:100807ms step_avg:60.15ms
step:1677/2330 train_time:100867ms step_avg:60.15ms
step:1678/2330 train_time:100929ms step_avg:60.15ms
step:1679/2330 train_time:100990ms step_avg:60.15ms
step:1680/2330 train_time:101052ms step_avg:60.15ms
step:1681/2330 train_time:101112ms step_avg:60.15ms
step:1682/2330 train_time:101174ms step_avg:60.15ms
step:1683/2330 train_time:101234ms step_avg:60.15ms
step:1684/2330 train_time:101296ms step_avg:60.15ms
step:1685/2330 train_time:101357ms step_avg:60.15ms
step:1686/2330 train_time:101419ms step_avg:60.15ms
step:1687/2330 train_time:101479ms step_avg:60.15ms
step:1688/2330 train_time:101540ms step_avg:60.15ms
step:1689/2330 train_time:101601ms step_avg:60.15ms
step:1690/2330 train_time:101662ms step_avg:60.16ms
step:1691/2330 train_time:101722ms step_avg:60.15ms
step:1692/2330 train_time:101784ms step_avg:60.16ms
step:1693/2330 train_time:101843ms step_avg:60.16ms
step:1694/2330 train_time:101906ms step_avg:60.16ms
step:1695/2330 train_time:101966ms step_avg:60.16ms
step:1696/2330 train_time:102028ms step_avg:60.16ms
step:1697/2330 train_time:102088ms step_avg:60.16ms
step:1698/2330 train_time:102149ms step_avg:60.16ms
step:1699/2330 train_time:102209ms step_avg:60.16ms
step:1700/2330 train_time:102271ms step_avg:60.16ms
step:1701/2330 train_time:102331ms step_avg:60.16ms
step:1702/2330 train_time:102393ms step_avg:60.16ms
step:1703/2330 train_time:102453ms step_avg:60.16ms
step:1704/2330 train_time:102516ms step_avg:60.16ms
step:1705/2330 train_time:102576ms step_avg:60.16ms
step:1706/2330 train_time:102638ms step_avg:60.16ms
step:1707/2330 train_time:102698ms step_avg:60.16ms
step:1708/2330 train_time:102761ms step_avg:60.16ms
step:1709/2330 train_time:102820ms step_avg:60.16ms
step:1710/2330 train_time:102882ms step_avg:60.16ms
step:1711/2330 train_time:102942ms step_avg:60.16ms
step:1712/2330 train_time:103003ms step_avg:60.17ms
step:1713/2330 train_time:103062ms step_avg:60.16ms
step:1714/2330 train_time:103124ms step_avg:60.17ms
step:1715/2330 train_time:103184ms step_avg:60.17ms
step:1716/2330 train_time:103246ms step_avg:60.17ms
step:1717/2330 train_time:103306ms step_avg:60.17ms
step:1718/2330 train_time:103368ms step_avg:60.17ms
step:1719/2330 train_time:103429ms step_avg:60.17ms
step:1720/2330 train_time:103491ms step_avg:60.17ms
step:1721/2330 train_time:103551ms step_avg:60.17ms
step:1722/2330 train_time:103612ms step_avg:60.17ms
step:1723/2330 train_time:103672ms step_avg:60.17ms
step:1724/2330 train_time:103735ms step_avg:60.17ms
step:1725/2330 train_time:103795ms step_avg:60.17ms
step:1726/2330 train_time:103857ms step_avg:60.17ms
step:1727/2330 train_time:103918ms step_avg:60.17ms
step:1728/2330 train_time:103980ms step_avg:60.17ms
step:1729/2330 train_time:104040ms step_avg:60.17ms
step:1730/2330 train_time:104102ms step_avg:60.17ms
step:1731/2330 train_time:104161ms step_avg:60.17ms
step:1732/2330 train_time:104223ms step_avg:60.18ms
step:1733/2330 train_time:104283ms step_avg:60.17ms
step:1734/2330 train_time:104345ms step_avg:60.18ms
step:1735/2330 train_time:104405ms step_avg:60.18ms
step:1736/2330 train_time:104467ms step_avg:60.18ms
step:1737/2330 train_time:104527ms step_avg:60.18ms
step:1738/2330 train_time:104588ms step_avg:60.18ms
step:1739/2330 train_time:104648ms step_avg:60.18ms
step:1740/2330 train_time:104710ms step_avg:60.18ms
step:1741/2330 train_time:104770ms step_avg:60.18ms
step:1742/2330 train_time:104832ms step_avg:60.18ms
step:1743/2330 train_time:104892ms step_avg:60.18ms
step:1744/2330 train_time:104954ms step_avg:60.18ms
step:1745/2330 train_time:105015ms step_avg:60.18ms
step:1746/2330 train_time:105077ms step_avg:60.18ms
step:1747/2330 train_time:105137ms step_avg:60.18ms
step:1748/2330 train_time:105199ms step_avg:60.18ms
step:1749/2330 train_time:105260ms step_avg:60.18ms
step:1750/2330 train_time:105321ms step_avg:60.18ms
step:1750/2330 val_loss:3.3799 train_time:105385ms step_avg:60.22ms
step:1751/2330 train_time:105406ms step_avg:60.20ms
step:1752/2330 train_time:105446ms step_avg:60.19ms
step:1753/2330 train_time:105505ms step_avg:60.19ms
step:1754/2330 train_time:105567ms step_avg:60.19ms
step:1755/2330 train_time:105628ms step_avg:60.19ms
step:1756/2330 train_time:105690ms step_avg:60.19ms
step:1757/2330 train_time:105749ms step_avg:60.19ms
step:1758/2330 train_time:105810ms step_avg:60.19ms
step:1759/2330 train_time:105869ms step_avg:60.19ms
step:1760/2330 train_time:105930ms step_avg:60.19ms
step:1761/2330 train_time:105989ms step_avg:60.19ms
step:1762/2330 train_time:106050ms step_avg:60.19ms
step:1763/2330 train_time:106109ms step_avg:60.19ms
step:1764/2330 train_time:106170ms step_avg:60.19ms
step:1765/2330 train_time:106230ms step_avg:60.19ms
step:1766/2330 train_time:106294ms step_avg:60.19ms
step:1767/2330 train_time:106358ms step_avg:60.19ms
step:1768/2330 train_time:106419ms step_avg:60.19ms
step:1769/2330 train_time:106479ms step_avg:60.19ms
step:1770/2330 train_time:106541ms step_avg:60.19ms
step:1771/2330 train_time:106602ms step_avg:60.19ms
step:1772/2330 train_time:106664ms step_avg:60.19ms
step:1773/2330 train_time:106723ms step_avg:60.19ms
step:1774/2330 train_time:106785ms step_avg:60.19ms
step:1775/2330 train_time:106844ms step_avg:60.19ms
step:1776/2330 train_time:106905ms step_avg:60.19ms
step:1777/2330 train_time:106965ms step_avg:60.19ms
step:1778/2330 train_time:107026ms step_avg:60.19ms
step:1779/2330 train_time:107086ms step_avg:60.19ms
step:1780/2330 train_time:107147ms step_avg:60.20ms
step:1781/2330 train_time:107206ms step_avg:60.19ms
step:1782/2330 train_time:107270ms step_avg:60.20ms
step:1783/2330 train_time:107331ms step_avg:60.20ms
step:1784/2330 train_time:107394ms step_avg:60.20ms
step:1785/2330 train_time:107455ms step_avg:60.20ms
step:1786/2330 train_time:107516ms step_avg:60.20ms
step:1787/2330 train_time:107577ms step_avg:60.20ms
step:1788/2330 train_time:107639ms step_avg:60.20ms
step:1789/2330 train_time:107699ms step_avg:60.20ms
step:1790/2330 train_time:107761ms step_avg:60.20ms
step:1791/2330 train_time:107820ms step_avg:60.20ms
step:1792/2330 train_time:107881ms step_avg:60.20ms
step:1793/2330 train_time:107941ms step_avg:60.20ms
step:1794/2330 train_time:108002ms step_avg:60.20ms
step:1795/2330 train_time:108062ms step_avg:60.20ms
step:1796/2330 train_time:108123ms step_avg:60.20ms
step:1797/2330 train_time:108183ms step_avg:60.20ms
step:1798/2330 train_time:108246ms step_avg:60.20ms
step:1799/2330 train_time:108306ms step_avg:60.20ms
step:1800/2330 train_time:108368ms step_avg:60.20ms
step:1801/2330 train_time:108430ms step_avg:60.21ms
step:1802/2330 train_time:108492ms step_avg:60.21ms
step:1803/2330 train_time:108552ms step_avg:60.21ms
step:1804/2330 train_time:108614ms step_avg:60.21ms
step:1805/2330 train_time:108674ms step_avg:60.21ms
step:1806/2330 train_time:108736ms step_avg:60.21ms
step:1807/2330 train_time:108795ms step_avg:60.21ms
step:1808/2330 train_time:108857ms step_avg:60.21ms
step:1809/2330 train_time:108917ms step_avg:60.21ms
step:1810/2330 train_time:108980ms step_avg:60.21ms
step:1811/2330 train_time:109040ms step_avg:60.21ms
step:1812/2330 train_time:109102ms step_avg:60.21ms
step:1813/2330 train_time:109162ms step_avg:60.21ms
step:1814/2330 train_time:109224ms step_avg:60.21ms
step:1815/2330 train_time:109284ms step_avg:60.21ms
step:1816/2330 train_time:109345ms step_avg:60.21ms
step:1817/2330 train_time:109405ms step_avg:60.21ms
step:1818/2330 train_time:109467ms step_avg:60.21ms
step:1819/2330 train_time:109527ms step_avg:60.21ms
step:1820/2330 train_time:109589ms step_avg:60.21ms
step:1821/2330 train_time:109649ms step_avg:60.21ms
step:1822/2330 train_time:109712ms step_avg:60.22ms
step:1823/2330 train_time:109772ms step_avg:60.22ms
step:1824/2330 train_time:109834ms step_avg:60.22ms
step:1825/2330 train_time:109894ms step_avg:60.22ms
step:1826/2330 train_time:109956ms step_avg:60.22ms
step:1827/2330 train_time:110015ms step_avg:60.22ms
step:1828/2330 train_time:110077ms step_avg:60.22ms
step:1829/2330 train_time:110137ms step_avg:60.22ms
step:1830/2330 train_time:110199ms step_avg:60.22ms
step:1831/2330 train_time:110260ms step_avg:60.22ms
step:1832/2330 train_time:110322ms step_avg:60.22ms
step:1833/2330 train_time:110383ms step_avg:60.22ms
step:1834/2330 train_time:110444ms step_avg:60.22ms
step:1835/2330 train_time:110504ms step_avg:60.22ms
step:1836/2330 train_time:110565ms step_avg:60.22ms
step:1837/2330 train_time:110625ms step_avg:60.22ms
step:1838/2330 train_time:110688ms step_avg:60.22ms
step:1839/2330 train_time:110748ms step_avg:60.22ms
step:1840/2330 train_time:110810ms step_avg:60.22ms
step:1841/2330 train_time:110869ms step_avg:60.22ms
step:1842/2330 train_time:110931ms step_avg:60.22ms
step:1843/2330 train_time:110991ms step_avg:60.22ms
step:1844/2330 train_time:111053ms step_avg:60.22ms
step:1845/2330 train_time:111113ms step_avg:60.22ms
step:1846/2330 train_time:111175ms step_avg:60.23ms
step:1847/2330 train_time:111235ms step_avg:60.22ms
step:1848/2330 train_time:111297ms step_avg:60.23ms
step:1849/2330 train_time:111358ms step_avg:60.23ms
step:1850/2330 train_time:111420ms step_avg:60.23ms
step:1851/2330 train_time:111481ms step_avg:60.23ms
step:1852/2330 train_time:111543ms step_avg:60.23ms
step:1853/2330 train_time:111603ms step_avg:60.23ms
step:1854/2330 train_time:111665ms step_avg:60.23ms
step:1855/2330 train_time:111725ms step_avg:60.23ms
step:1856/2330 train_time:111786ms step_avg:60.23ms
step:1857/2330 train_time:111846ms step_avg:60.23ms
step:1858/2330 train_time:111907ms step_avg:60.23ms
step:1859/2330 train_time:111967ms step_avg:60.23ms
step:1860/2330 train_time:112029ms step_avg:60.23ms
step:1861/2330 train_time:112089ms step_avg:60.23ms
step:1862/2330 train_time:112150ms step_avg:60.23ms
step:1863/2330 train_time:112210ms step_avg:60.23ms
step:1864/2330 train_time:112273ms step_avg:60.23ms
step:1865/2330 train_time:112332ms step_avg:60.23ms
step:1866/2330 train_time:112394ms step_avg:60.23ms
step:1867/2330 train_time:112453ms step_avg:60.23ms
step:1868/2330 train_time:112516ms step_avg:60.23ms
step:1869/2330 train_time:112576ms step_avg:60.23ms
step:1870/2330 train_time:112638ms step_avg:60.23ms
step:1871/2330 train_time:112698ms step_avg:60.23ms
step:1872/2330 train_time:112760ms step_avg:60.23ms
step:1873/2330 train_time:112820ms step_avg:60.23ms
step:1874/2330 train_time:112882ms step_avg:60.24ms
step:1875/2330 train_time:112942ms step_avg:60.24ms
step:1876/2330 train_time:113004ms step_avg:60.24ms
step:1877/2330 train_time:113064ms step_avg:60.24ms
step:1878/2330 train_time:113125ms step_avg:60.24ms
step:1879/2330 train_time:113185ms step_avg:60.24ms
step:1880/2330 train_time:113246ms step_avg:60.24ms
step:1881/2330 train_time:113306ms step_avg:60.24ms
step:1882/2330 train_time:113368ms step_avg:60.24ms
step:1883/2330 train_time:113428ms step_avg:60.24ms
step:1884/2330 train_time:113490ms step_avg:60.24ms
step:1885/2330 train_time:113550ms step_avg:60.24ms
step:1886/2330 train_time:113612ms step_avg:60.24ms
step:1887/2330 train_time:113672ms step_avg:60.24ms
step:1888/2330 train_time:113734ms step_avg:60.24ms
step:1889/2330 train_time:113794ms step_avg:60.24ms
step:1890/2330 train_time:113857ms step_avg:60.24ms
step:1891/2330 train_time:113917ms step_avg:60.24ms
step:1892/2330 train_time:113979ms step_avg:60.24ms
step:1893/2330 train_time:114038ms step_avg:60.24ms
step:1894/2330 train_time:114101ms step_avg:60.24ms
step:1895/2330 train_time:114160ms step_avg:60.24ms
step:1896/2330 train_time:114222ms step_avg:60.24ms
step:1897/2330 train_time:114282ms step_avg:60.24ms
step:1898/2330 train_time:114344ms step_avg:60.24ms
step:1899/2330 train_time:114404ms step_avg:60.24ms
step:1900/2330 train_time:114465ms step_avg:60.24ms
step:1901/2330 train_time:114525ms step_avg:60.24ms
step:1902/2330 train_time:114587ms step_avg:60.25ms
step:1903/2330 train_time:114646ms step_avg:60.24ms
step:1904/2330 train_time:114707ms step_avg:60.25ms
step:1905/2330 train_time:114767ms step_avg:60.25ms
step:1906/2330 train_time:114830ms step_avg:60.25ms
step:1907/2330 train_time:114890ms step_avg:60.25ms
step:1908/2330 train_time:114952ms step_avg:60.25ms
step:1909/2330 train_time:115011ms step_avg:60.25ms
step:1910/2330 train_time:115073ms step_avg:60.25ms
step:1911/2330 train_time:115133ms step_avg:60.25ms
step:1912/2330 train_time:115195ms step_avg:60.25ms
step:1913/2330 train_time:115255ms step_avg:60.25ms
step:1914/2330 train_time:115317ms step_avg:60.25ms
step:1915/2330 train_time:115377ms step_avg:60.25ms
step:1916/2330 train_time:115439ms step_avg:60.25ms
step:1917/2330 train_time:115500ms step_avg:60.25ms
step:1918/2330 train_time:115561ms step_avg:60.25ms
step:1919/2330 train_time:115621ms step_avg:60.25ms
step:1920/2330 train_time:115683ms step_avg:60.25ms
step:1921/2330 train_time:115744ms step_avg:60.25ms
step:1922/2330 train_time:115806ms step_avg:60.25ms
step:1923/2330 train_time:115865ms step_avg:60.25ms
step:1924/2330 train_time:115928ms step_avg:60.25ms
step:1925/2330 train_time:115988ms step_avg:60.25ms
step:1926/2330 train_time:116049ms step_avg:60.25ms
step:1927/2330 train_time:116108ms step_avg:60.25ms
step:1928/2330 train_time:116170ms step_avg:60.25ms
step:1929/2330 train_time:116230ms step_avg:60.25ms
step:1930/2330 train_time:116292ms step_avg:60.26ms
step:1931/2330 train_time:116352ms step_avg:60.25ms
step:1932/2330 train_time:116414ms step_avg:60.26ms
step:1933/2330 train_time:116474ms step_avg:60.26ms
step:1934/2330 train_time:116536ms step_avg:60.26ms
step:1935/2330 train_time:116596ms step_avg:60.26ms
step:1936/2330 train_time:116659ms step_avg:60.26ms
step:1937/2330 train_time:116719ms step_avg:60.26ms
step:1938/2330 train_time:116782ms step_avg:60.26ms
step:1939/2330 train_time:116843ms step_avg:60.26ms
step:1940/2330 train_time:116904ms step_avg:60.26ms
step:1941/2330 train_time:116964ms step_avg:60.26ms
step:1942/2330 train_time:117026ms step_avg:60.26ms
step:1943/2330 train_time:117085ms step_avg:60.26ms
step:1944/2330 train_time:117146ms step_avg:60.26ms
step:1945/2330 train_time:117206ms step_avg:60.26ms
step:1946/2330 train_time:117268ms step_avg:60.26ms
step:1947/2330 train_time:117328ms step_avg:60.26ms
step:1948/2330 train_time:117390ms step_avg:60.26ms
step:1949/2330 train_time:117450ms step_avg:60.26ms
step:1950/2330 train_time:117512ms step_avg:60.26ms
step:1951/2330 train_time:117572ms step_avg:60.26ms
step:1952/2330 train_time:117634ms step_avg:60.26ms
step:1953/2330 train_time:117693ms step_avg:60.26ms
step:1954/2330 train_time:117755ms step_avg:60.26ms
step:1955/2330 train_time:117815ms step_avg:60.26ms
step:1956/2330 train_time:117878ms step_avg:60.26ms
step:1957/2330 train_time:117938ms step_avg:60.26ms
step:1958/2330 train_time:118001ms step_avg:60.27ms
step:1959/2330 train_time:118061ms step_avg:60.27ms
step:1960/2330 train_time:118123ms step_avg:60.27ms
step:1961/2330 train_time:118183ms step_avg:60.27ms
step:1962/2330 train_time:118245ms step_avg:60.27ms
step:1963/2330 train_time:118304ms step_avg:60.27ms
step:1964/2330 train_time:118365ms step_avg:60.27ms
step:1965/2330 train_time:118425ms step_avg:60.27ms
step:1966/2330 train_time:118487ms step_avg:60.27ms
step:1967/2330 train_time:118547ms step_avg:60.27ms
step:1968/2330 train_time:118609ms step_avg:60.27ms
step:1969/2330 train_time:118669ms step_avg:60.27ms
step:1970/2330 train_time:118731ms step_avg:60.27ms
step:1971/2330 train_time:118791ms step_avg:60.27ms
step:1972/2330 train_time:118853ms step_avg:60.27ms
step:1973/2330 train_time:118913ms step_avg:60.27ms
step:1974/2330 train_time:118975ms step_avg:60.27ms
step:1975/2330 train_time:119035ms step_avg:60.27ms
step:1976/2330 train_time:119097ms step_avg:60.27ms
step:1977/2330 train_time:119157ms step_avg:60.27ms
step:1978/2330 train_time:119219ms step_avg:60.27ms
step:1979/2330 train_time:119279ms step_avg:60.27ms
step:1980/2330 train_time:119341ms step_avg:60.27ms
step:1981/2330 train_time:119401ms step_avg:60.27ms
step:1982/2330 train_time:119462ms step_avg:60.27ms
step:1983/2330 train_time:119522ms step_avg:60.27ms
step:1984/2330 train_time:119584ms step_avg:60.27ms
step:1985/2330 train_time:119644ms step_avg:60.27ms
step:1986/2330 train_time:119706ms step_avg:60.27ms
step:1987/2330 train_time:119765ms step_avg:60.27ms
step:1988/2330 train_time:119828ms step_avg:60.28ms
step:1989/2330 train_time:119887ms step_avg:60.28ms
step:1990/2330 train_time:119949ms step_avg:60.28ms
step:1991/2330 train_time:120009ms step_avg:60.28ms
step:1992/2330 train_time:120071ms step_avg:60.28ms
step:1993/2330 train_time:120131ms step_avg:60.28ms
step:1994/2330 train_time:120193ms step_avg:60.28ms
step:1995/2330 train_time:120253ms step_avg:60.28ms
step:1996/2330 train_time:120315ms step_avg:60.28ms
step:1997/2330 train_time:120375ms step_avg:60.28ms
step:1998/2330 train_time:120437ms step_avg:60.28ms
step:1999/2330 train_time:120497ms step_avg:60.28ms
step:2000/2330 train_time:120559ms step_avg:60.28ms
step:2000/2330 val_loss:3.3296 train_time:120624ms step_avg:60.31ms
step:2001/2330 train_time:120644ms step_avg:60.29ms
step:2002/2330 train_time:120687ms step_avg:60.28ms
step:2003/2330 train_time:120748ms step_avg:60.28ms
step:2004/2330 train_time:120815ms step_avg:60.29ms
step:2005/2330 train_time:120874ms step_avg:60.29ms
step:2006/2330 train_time:120937ms step_avg:60.29ms
step:2007/2330 train_time:120996ms step_avg:60.29ms
step:2008/2330 train_time:121058ms step_avg:60.29ms
step:2009/2330 train_time:121117ms step_avg:60.29ms
step:2010/2330 train_time:121178ms step_avg:60.29ms
step:2011/2330 train_time:121238ms step_avg:60.29ms
step:2012/2330 train_time:121299ms step_avg:60.29ms
step:2013/2330 train_time:121358ms step_avg:60.29ms
step:2014/2330 train_time:121420ms step_avg:60.29ms
step:2015/2330 train_time:121479ms step_avg:60.29ms
step:2016/2330 train_time:121540ms step_avg:60.29ms
step:2017/2330 train_time:121601ms step_avg:60.29ms
step:2018/2330 train_time:121663ms step_avg:60.29ms
step:2019/2330 train_time:121725ms step_avg:60.29ms
step:2020/2330 train_time:121788ms step_avg:60.29ms
step:2021/2330 train_time:121848ms step_avg:60.29ms
step:2022/2330 train_time:121910ms step_avg:60.29ms
step:2023/2330 train_time:121970ms step_avg:60.29ms
step:2024/2330 train_time:122032ms step_avg:60.29ms
step:2025/2330 train_time:122091ms step_avg:60.29ms
step:2026/2330 train_time:122152ms step_avg:60.29ms
step:2027/2330 train_time:122212ms step_avg:60.29ms
step:2028/2330 train_time:122273ms step_avg:60.29ms
step:2029/2330 train_time:122332ms step_avg:60.29ms
step:2030/2330 train_time:122394ms step_avg:60.29ms
step:2031/2330 train_time:122453ms step_avg:60.29ms
step:2032/2330 train_time:122514ms step_avg:60.29ms
step:2033/2330 train_time:122574ms step_avg:60.29ms
step:2034/2330 train_time:122637ms step_avg:60.29ms
step:2035/2330 train_time:122698ms step_avg:60.29ms
step:2036/2330 train_time:122762ms step_avg:60.30ms
step:2037/2330 train_time:122823ms step_avg:60.30ms
step:2038/2330 train_time:122885ms step_avg:60.30ms
step:2039/2330 train_time:122945ms step_avg:60.30ms
step:2040/2330 train_time:123007ms step_avg:60.30ms
step:2041/2330 train_time:123067ms step_avg:60.30ms
step:2042/2330 train_time:123128ms step_avg:60.30ms
step:2043/2330 train_time:123187ms step_avg:60.30ms
step:2044/2330 train_time:123249ms step_avg:60.30ms
step:2045/2330 train_time:123309ms step_avg:60.30ms
step:2046/2330 train_time:123370ms step_avg:60.30ms
step:2047/2330 train_time:123430ms step_avg:60.30ms
step:2048/2330 train_time:123492ms step_avg:60.30ms
step:2049/2330 train_time:123551ms step_avg:60.30ms
step:2050/2330 train_time:123613ms step_avg:60.30ms
step:2051/2330 train_time:123673ms step_avg:60.30ms
step:2052/2330 train_time:123736ms step_avg:60.30ms
step:2053/2330 train_time:123796ms step_avg:60.30ms
step:2054/2330 train_time:123858ms step_avg:60.30ms
step:2055/2330 train_time:123918ms step_avg:60.30ms
step:2056/2330 train_time:123981ms step_avg:60.30ms
step:2057/2330 train_time:124042ms step_avg:60.30ms
step:2058/2330 train_time:124104ms step_avg:60.30ms
step:2059/2330 train_time:124163ms step_avg:60.30ms
step:2060/2330 train_time:124225ms step_avg:60.30ms
step:2061/2330 train_time:124285ms step_avg:60.30ms
step:2062/2330 train_time:124346ms step_avg:60.30ms
step:2063/2330 train_time:124406ms step_avg:60.30ms
step:2064/2330 train_time:124467ms step_avg:60.30ms
step:2065/2330 train_time:124527ms step_avg:60.30ms
step:2066/2330 train_time:124589ms step_avg:60.30ms
step:2067/2330 train_time:124649ms step_avg:60.30ms
step:2068/2330 train_time:124711ms step_avg:60.31ms
step:2069/2330 train_time:124772ms step_avg:60.31ms
step:2070/2330 train_time:124835ms step_avg:60.31ms
step:2071/2330 train_time:124894ms step_avg:60.31ms
step:2072/2330 train_time:124956ms step_avg:60.31ms
step:2073/2330 train_time:125016ms step_avg:60.31ms
step:2074/2330 train_time:125077ms step_avg:60.31ms
step:2075/2330 train_time:125138ms step_avg:60.31ms
step:2076/2330 train_time:125201ms step_avg:60.31ms
step:2077/2330 train_time:125261ms step_avg:60.31ms
step:2078/2330 train_time:125323ms step_avg:60.31ms
step:2079/2330 train_time:125384ms step_avg:60.31ms
step:2080/2330 train_time:125446ms step_avg:60.31ms
step:2081/2330 train_time:125506ms step_avg:60.31ms
step:2082/2330 train_time:125567ms step_avg:60.31ms
step:2083/2330 train_time:125627ms step_avg:60.31ms
step:2084/2330 train_time:125688ms step_avg:60.31ms
step:2085/2330 train_time:125749ms step_avg:60.31ms
step:2086/2330 train_time:125811ms step_avg:60.31ms
step:2087/2330 train_time:125871ms step_avg:60.31ms
step:2088/2330 train_time:125932ms step_avg:60.31ms
step:2089/2330 train_time:125992ms step_avg:60.31ms
step:2090/2330 train_time:126053ms step_avg:60.31ms
step:2091/2330 train_time:126114ms step_avg:60.31ms
step:2092/2330 train_time:126176ms step_avg:60.31ms
step:2093/2330 train_time:126236ms step_avg:60.31ms
step:2094/2330 train_time:126299ms step_avg:60.31ms
step:2095/2330 train_time:126359ms step_avg:60.31ms
step:2096/2330 train_time:126421ms step_avg:60.32ms
step:2097/2330 train_time:126482ms step_avg:60.32ms
step:2098/2330 train_time:126544ms step_avg:60.32ms
step:2099/2330 train_time:126604ms step_avg:60.32ms
step:2100/2330 train_time:126666ms step_avg:60.32ms
step:2101/2330 train_time:126726ms step_avg:60.32ms
step:2102/2330 train_time:126788ms step_avg:60.32ms
step:2103/2330 train_time:126848ms step_avg:60.32ms
step:2104/2330 train_time:126910ms step_avg:60.32ms
step:2105/2330 train_time:126970ms step_avg:60.32ms
step:2106/2330 train_time:127032ms step_avg:60.32ms
step:2107/2330 train_time:127092ms step_avg:60.32ms
step:2108/2330 train_time:127154ms step_avg:60.32ms
step:2109/2330 train_time:127215ms step_avg:60.32ms
step:2110/2330 train_time:127277ms step_avg:60.32ms
step:2111/2330 train_time:127337ms step_avg:60.32ms
step:2112/2330 train_time:127399ms step_avg:60.32ms
step:2113/2330 train_time:127459ms step_avg:60.32ms
step:2114/2330 train_time:127522ms step_avg:60.32ms
step:2115/2330 train_time:127582ms step_avg:60.32ms
step:2116/2330 train_time:127644ms step_avg:60.32ms
step:2117/2330 train_time:127704ms step_avg:60.32ms
step:2118/2330 train_time:127765ms step_avg:60.32ms
step:2119/2330 train_time:127825ms step_avg:60.32ms
step:2120/2330 train_time:127886ms step_avg:60.32ms
step:2121/2330 train_time:127947ms step_avg:60.32ms
step:2122/2330 train_time:128009ms step_avg:60.32ms
step:2123/2330 train_time:128068ms step_avg:60.32ms
step:2124/2330 train_time:128131ms step_avg:60.33ms
step:2125/2330 train_time:128191ms step_avg:60.33ms
step:2126/2330 train_time:128253ms step_avg:60.33ms
step:2127/2330 train_time:128313ms step_avg:60.33ms
step:2128/2330 train_time:128375ms step_avg:60.33ms
step:2129/2330 train_time:128435ms step_avg:60.33ms
step:2130/2330 train_time:128496ms step_avg:60.33ms
step:2131/2330 train_time:128557ms step_avg:60.33ms
step:2132/2330 train_time:128619ms step_avg:60.33ms
step:2133/2330 train_time:128680ms step_avg:60.33ms
step:2134/2330 train_time:128742ms step_avg:60.33ms
step:2135/2330 train_time:128802ms step_avg:60.33ms
step:2136/2330 train_time:128864ms step_avg:60.33ms
step:2137/2330 train_time:128924ms step_avg:60.33ms
step:2138/2330 train_time:128986ms step_avg:60.33ms
step:2139/2330 train_time:129045ms step_avg:60.33ms
step:2140/2330 train_time:129107ms step_avg:60.33ms
step:2141/2330 train_time:129167ms step_avg:60.33ms
step:2142/2330 train_time:129229ms step_avg:60.33ms
step:2143/2330 train_time:129290ms step_avg:60.33ms
step:2144/2330 train_time:129352ms step_avg:60.33ms
step:2145/2330 train_time:129412ms step_avg:60.33ms
step:2146/2330 train_time:129475ms step_avg:60.33ms
step:2147/2330 train_time:129535ms step_avg:60.33ms
step:2148/2330 train_time:129596ms step_avg:60.33ms
step:2149/2330 train_time:129656ms step_avg:60.33ms
step:2150/2330 train_time:129719ms step_avg:60.33ms
step:2151/2330 train_time:129779ms step_avg:60.33ms
step:2152/2330 train_time:129841ms step_avg:60.34ms
step:2153/2330 train_time:129901ms step_avg:60.33ms
step:2154/2330 train_time:129963ms step_avg:60.34ms
step:2155/2330 train_time:130024ms step_avg:60.34ms
step:2156/2330 train_time:130086ms step_avg:60.34ms
step:2157/2330 train_time:130146ms step_avg:60.34ms
step:2158/2330 train_time:130208ms step_avg:60.34ms
step:2159/2330 train_time:130268ms step_avg:60.34ms
step:2160/2330 train_time:130329ms step_avg:60.34ms
step:2161/2330 train_time:130389ms step_avg:60.34ms
step:2162/2330 train_time:130451ms step_avg:60.34ms
step:2163/2330 train_time:130511ms step_avg:60.34ms
step:2164/2330 train_time:130574ms step_avg:60.34ms
step:2165/2330 train_time:130634ms step_avg:60.34ms
step:2166/2330 train_time:130696ms step_avg:60.34ms
step:2167/2330 train_time:130756ms step_avg:60.34ms
step:2168/2330 train_time:130818ms step_avg:60.34ms
step:2169/2330 train_time:130877ms step_avg:60.34ms
step:2170/2330 train_time:130940ms step_avg:60.34ms
step:2171/2330 train_time:131000ms step_avg:60.34ms
step:2172/2330 train_time:131062ms step_avg:60.34ms
step:2173/2330 train_time:131123ms step_avg:60.34ms
step:2174/2330 train_time:131185ms step_avg:60.34ms
step:2175/2330 train_time:131245ms step_avg:60.34ms
step:2176/2330 train_time:131307ms step_avg:60.34ms
step:2177/2330 train_time:131366ms step_avg:60.34ms
step:2178/2330 train_time:131428ms step_avg:60.34ms
step:2179/2330 train_time:131487ms step_avg:60.34ms
step:2180/2330 train_time:131549ms step_avg:60.34ms
step:2181/2330 train_time:131610ms step_avg:60.34ms
step:2182/2330 train_time:131672ms step_avg:60.34ms
step:2183/2330 train_time:131732ms step_avg:60.34ms
step:2184/2330 train_time:131794ms step_avg:60.35ms
step:2185/2330 train_time:131855ms step_avg:60.35ms
step:2186/2330 train_time:131916ms step_avg:60.35ms
step:2187/2330 train_time:131976ms step_avg:60.35ms
step:2188/2330 train_time:132039ms step_avg:60.35ms
step:2189/2330 train_time:132099ms step_avg:60.35ms
step:2190/2330 train_time:132161ms step_avg:60.35ms
step:2191/2330 train_time:132222ms step_avg:60.35ms
step:2192/2330 train_time:132284ms step_avg:60.35ms
step:2193/2330 train_time:132343ms step_avg:60.35ms
step:2194/2330 train_time:132405ms step_avg:60.35ms
step:2195/2330 train_time:132465ms step_avg:60.35ms
step:2196/2330 train_time:132528ms step_avg:60.35ms
step:2197/2330 train_time:132587ms step_avg:60.35ms
step:2198/2330 train_time:132649ms step_avg:60.35ms
step:2199/2330 train_time:132709ms step_avg:60.35ms
step:2200/2330 train_time:132771ms step_avg:60.35ms
step:2201/2330 train_time:132831ms step_avg:60.35ms
step:2202/2330 train_time:132893ms step_avg:60.35ms
step:2203/2330 train_time:132953ms step_avg:60.35ms
step:2204/2330 train_time:133015ms step_avg:60.35ms
step:2205/2330 train_time:133076ms step_avg:60.35ms
step:2206/2330 train_time:133138ms step_avg:60.35ms
step:2207/2330 train_time:133197ms step_avg:60.35ms
step:2208/2330 train_time:133260ms step_avg:60.35ms
step:2209/2330 train_time:133320ms step_avg:60.35ms
step:2210/2330 train_time:133382ms step_avg:60.35ms
step:2211/2330 train_time:133442ms step_avg:60.35ms
step:2212/2330 train_time:133505ms step_avg:60.35ms
step:2213/2330 train_time:133565ms step_avg:60.35ms
step:2214/2330 train_time:133627ms step_avg:60.36ms
step:2215/2330 train_time:133686ms step_avg:60.36ms
step:2216/2330 train_time:133748ms step_avg:60.36ms
step:2217/2330 train_time:133808ms step_avg:60.36ms
step:2218/2330 train_time:133870ms step_avg:60.36ms
step:2219/2330 train_time:133930ms step_avg:60.36ms
step:2220/2330 train_time:133992ms step_avg:60.36ms
step:2221/2330 train_time:134052ms step_avg:60.36ms
step:2222/2330 train_time:134115ms step_avg:60.36ms
step:2223/2330 train_time:134175ms step_avg:60.36ms
step:2224/2330 train_time:134236ms step_avg:60.36ms
step:2225/2330 train_time:134297ms step_avg:60.36ms
step:2226/2330 train_time:134359ms step_avg:60.36ms
step:2227/2330 train_time:134419ms step_avg:60.36ms
step:2228/2330 train_time:134482ms step_avg:60.36ms
step:2229/2330 train_time:134543ms step_avg:60.36ms
step:2230/2330 train_time:134605ms step_avg:60.36ms
step:2231/2330 train_time:134665ms step_avg:60.36ms
step:2232/2330 train_time:134726ms step_avg:60.36ms
step:2233/2330 train_time:134786ms step_avg:60.36ms
step:2234/2330 train_time:134847ms step_avg:60.36ms
step:2235/2330 train_time:134907ms step_avg:60.36ms
step:2236/2330 train_time:134968ms step_avg:60.36ms
step:2237/2330 train_time:135028ms step_avg:60.36ms
step:2238/2330 train_time:135090ms step_avg:60.36ms
step:2239/2330 train_time:135150ms step_avg:60.36ms
step:2240/2330 train_time:135213ms step_avg:60.36ms
step:2241/2330 train_time:135273ms step_avg:60.36ms
step:2242/2330 train_time:135335ms step_avg:60.36ms
step:2243/2330 train_time:135394ms step_avg:60.36ms
step:2244/2330 train_time:135456ms step_avg:60.36ms
step:2245/2330 train_time:135516ms step_avg:60.36ms
step:2246/2330 train_time:135578ms step_avg:60.36ms
step:2247/2330 train_time:135639ms step_avg:60.36ms
step:2248/2330 train_time:135702ms step_avg:60.37ms
step:2249/2330 train_time:135761ms step_avg:60.37ms
step:2250/2330 train_time:135824ms step_avg:60.37ms
step:2250/2330 val_loss:3.2900 train_time:135888ms step_avg:60.39ms
step:2251/2330 train_time:135909ms step_avg:60.38ms
step:2252/2330 train_time:135950ms step_avg:60.37ms
step:2253/2330 train_time:136013ms step_avg:60.37ms
step:2254/2330 train_time:136076ms step_avg:60.37ms
step:2255/2330 train_time:136136ms step_avg:60.37ms
step:2256/2330 train_time:136198ms step_avg:60.37ms
step:2257/2330 train_time:136257ms step_avg:60.37ms
step:2258/2330 train_time:136319ms step_avg:60.37ms
step:2259/2330 train_time:136378ms step_avg:60.37ms
step:2260/2330 train_time:136439ms step_avg:60.37ms
step:2261/2330 train_time:136498ms step_avg:60.37ms
step:2262/2330 train_time:136559ms step_avg:60.37ms
step:2263/2330 train_time:136618ms step_avg:60.37ms
step:2264/2330 train_time:136680ms step_avg:60.37ms
step:2265/2330 train_time:136739ms step_avg:60.37ms
step:2266/2330 train_time:136801ms step_avg:60.37ms
step:2267/2330 train_time:136863ms step_avg:60.37ms
step:2268/2330 train_time:136925ms step_avg:60.37ms
step:2269/2330 train_time:136985ms step_avg:60.37ms
step:2270/2330 train_time:137048ms step_avg:60.37ms
step:2271/2330 train_time:137108ms step_avg:60.37ms
step:2272/2330 train_time:137170ms step_avg:60.37ms
step:2273/2330 train_time:137230ms step_avg:60.37ms
step:2274/2330 train_time:137291ms step_avg:60.37ms
step:2275/2330 train_time:137350ms step_avg:60.37ms
step:2276/2330 train_time:137412ms step_avg:60.37ms
step:2277/2330 train_time:137471ms step_avg:60.37ms
step:2278/2330 train_time:137532ms step_avg:60.37ms
step:2279/2330 train_time:137591ms step_avg:60.37ms
step:2280/2330 train_time:137652ms step_avg:60.37ms
step:2281/2330 train_time:137712ms step_avg:60.37ms
step:2282/2330 train_time:137774ms step_avg:60.37ms
step:2283/2330 train_time:137834ms step_avg:60.37ms
step:2284/2330 train_time:137897ms step_avg:60.38ms
step:2285/2330 train_time:137958ms step_avg:60.38ms
step:2286/2330 train_time:138020ms step_avg:60.38ms
step:2287/2330 train_time:138081ms step_avg:60.38ms
step:2288/2330 train_time:138145ms step_avg:60.38ms
step:2289/2330 train_time:138205ms step_avg:60.38ms
step:2290/2330 train_time:138266ms step_avg:60.38ms
step:2291/2330 train_time:138326ms step_avg:60.38ms
step:2292/2330 train_time:138388ms step_avg:60.38ms
step:2293/2330 train_time:138447ms step_avg:60.38ms
step:2294/2330 train_time:138509ms step_avg:60.38ms
step:2295/2330 train_time:138568ms step_avg:60.38ms
step:2296/2330 train_time:138630ms step_avg:60.38ms
step:2297/2330 train_time:138689ms step_avg:60.38ms
step:2298/2330 train_time:138752ms step_avg:60.38ms
step:2299/2330 train_time:138812ms step_avg:60.38ms
step:2300/2330 train_time:138874ms step_avg:60.38ms
step:2301/2330 train_time:138934ms step_avg:60.38ms
step:2302/2330 train_time:138997ms step_avg:60.38ms
step:2303/2330 train_time:139057ms step_avg:60.38ms
step:2304/2330 train_time:139120ms step_avg:60.38ms
step:2305/2330 train_time:139181ms step_avg:60.38ms
step:2306/2330 train_time:139244ms step_avg:60.38ms
step:2307/2330 train_time:139303ms step_avg:60.38ms
step:2308/2330 train_time:139365ms step_avg:60.38ms
step:2309/2330 train_time:139424ms step_avg:60.38ms
step:2310/2330 train_time:139486ms step_avg:60.38ms
step:2311/2330 train_time:139545ms step_avg:60.38ms
step:2312/2330 train_time:139607ms step_avg:60.38ms
step:2313/2330 train_time:139667ms step_avg:60.38ms
step:2314/2330 train_time:139728ms step_avg:60.38ms
step:2315/2330 train_time:139788ms step_avg:60.38ms
step:2316/2330 train_time:139851ms step_avg:60.38ms
step:2317/2330 train_time:139911ms step_avg:60.38ms
step:2318/2330 train_time:139973ms step_avg:60.39ms
step:2319/2330 train_time:140034ms step_avg:60.39ms
step:2320/2330 train_time:140095ms step_avg:60.39ms
step:2321/2330 train_time:140155ms step_avg:60.39ms
step:2322/2330 train_time:140218ms step_avg:60.39ms
step:2323/2330 train_time:140278ms step_avg:60.39ms
step:2324/2330 train_time:140339ms step_avg:60.39ms
step:2325/2330 train_time:140399ms step_avg:60.39ms
step:2326/2330 train_time:140462ms step_avg:60.39ms
step:2327/2330 train_time:140521ms step_avg:60.39ms
step:2328/2330 train_time:140584ms step_avg:60.39ms
step:2329/2330 train_time:140643ms step_avg:60.39ms
step:2330/2330 train_time:140705ms step_avg:60.39ms
step:2330/2330 val_loss:3.2769 train_time:140769ms step_avg:60.42ms
peak memory allocated: 29226 MiB reserved: 44076 MiB

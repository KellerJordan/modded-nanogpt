import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Jan  7 08:54:01 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   26C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   34C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     70253      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     70254      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     70255      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     70256      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     70257      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     70258      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     70259      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     70260      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8299 train_time:0ms step_avg:0.03ms
step:1/1775 train_time:75ms step_avg:74.67ms
step:2/1775 train_time:99ms step_avg:49.65ms
step:3/1775 train_time:120ms step_avg:40.04ms
step:4/1775 train_time:144ms step_avg:35.94ms
step:5/1775 train_time:175ms step_avg:35.09ms
step:6/1775 train_time:259ms step_avg:43.16ms
step:7/1775 train_time:278ms step_avg:39.73ms
step:8/1775 train_time:305ms step_avg:38.08ms
step:9/1775 train_time:337ms step_avg:37.39ms
step:10/1775 train_time:371ms step_avg:37.07ms
step:11/1775 train_time:402ms step_avg:36.59ms
step:12/1775 train_time:436ms step_avg:36.37ms
step:13/1775 train_time:468ms step_avg:36.03ms
step:14/1775 train_time:503ms step_avg:35.91ms
step:15/1775 train_time:535ms step_avg:35.65ms
step:16/1775 train_time:569ms step_avg:35.58ms
step:17/1775 train_time:601ms step_avg:35.38ms
step:18/1775 train_time:636ms step_avg:35.31ms
step:19/1775 train_time:668ms step_avg:35.16ms
step:20/1775 train_time:702ms step_avg:35.10ms
step:21/1775 train_time:734ms step_avg:34.96ms
step:22/1775 train_time:768ms step_avg:34.91ms
step:23/1775 train_time:800ms step_avg:34.79ms
step:24/1775 train_time:834ms step_avg:34.77ms
step:25/1775 train_time:867ms step_avg:34.69ms
step:26/1775 train_time:900ms step_avg:34.63ms
step:27/1775 train_time:932ms step_avg:34.53ms
step:28/1775 train_time:967ms step_avg:34.52ms
step:29/1775 train_time:999ms step_avg:34.43ms
step:30/1775 train_time:1033ms step_avg:34.43ms
step:31/1775 train_time:1065ms step_avg:34.35ms
step:32/1775 train_time:1099ms step_avg:34.34ms
step:33/1775 train_time:1131ms step_avg:34.27ms
step:34/1775 train_time:1165ms step_avg:34.28ms
step:35/1775 train_time:1199ms step_avg:34.24ms
step:36/1775 train_time:1233ms step_avg:34.26ms
step:37/1775 train_time:1266ms step_avg:34.22ms
step:38/1775 train_time:1301ms step_avg:34.24ms
step:39/1775 train_time:1333ms step_avg:34.19ms
step:40/1775 train_time:1368ms step_avg:34.19ms
step:41/1775 train_time:1400ms step_avg:34.15ms
step:42/1775 train_time:1435ms step_avg:34.16ms
step:43/1775 train_time:1467ms step_avg:34.11ms
step:44/1775 train_time:1501ms step_avg:34.12ms
step:45/1775 train_time:1534ms step_avg:34.08ms
step:46/1775 train_time:1568ms step_avg:34.09ms
step:47/1775 train_time:1600ms step_avg:34.05ms
step:48/1775 train_time:1635ms step_avg:34.06ms
step:49/1775 train_time:1667ms step_avg:34.01ms
step:50/1775 train_time:1701ms step_avg:34.02ms
step:51/1775 train_time:1733ms step_avg:33.99ms
step:52/1775 train_time:1768ms step_avg:33.99ms
step:53/1775 train_time:1800ms step_avg:33.96ms
step:54/1775 train_time:1834ms step_avg:33.97ms
step:55/1775 train_time:1866ms step_avg:33.93ms
step:56/1775 train_time:1900ms step_avg:33.93ms
step:57/1775 train_time:1932ms step_avg:33.90ms
step:58/1775 train_time:1967ms step_avg:33.91ms
step:59/1775 train_time:1999ms step_avg:33.88ms
step:60/1775 train_time:2033ms step_avg:33.88ms
step:61/1775 train_time:2065ms step_avg:33.85ms
step:62/1775 train_time:2099ms step_avg:33.85ms
step:63/1775 train_time:2131ms step_avg:33.83ms
step:64/1775 train_time:2166ms step_avg:33.84ms
step:65/1775 train_time:2198ms step_avg:33.82ms
step:66/1775 train_time:2233ms step_avg:33.83ms
step:67/1775 train_time:2265ms step_avg:33.80ms
step:68/1775 train_time:2299ms step_avg:33.81ms
step:69/1775 train_time:2331ms step_avg:33.79ms
step:70/1775 train_time:2366ms step_avg:33.79ms
step:71/1775 train_time:2398ms step_avg:33.77ms
step:72/1775 train_time:2432ms step_avg:33.78ms
step:73/1775 train_time:2465ms step_avg:33.76ms
step:74/1775 train_time:2499ms step_avg:33.77ms
step:75/1775 train_time:2531ms step_avg:33.74ms
step:76/1775 train_time:2565ms step_avg:33.75ms
step:77/1775 train_time:2597ms step_avg:33.73ms
step:78/1775 train_time:2631ms step_avg:33.73ms
step:79/1775 train_time:2663ms step_avg:33.71ms
step:80/1775 train_time:2697ms step_avg:33.72ms
step:81/1775 train_time:2730ms step_avg:33.70ms
step:82/1775 train_time:2764ms step_avg:33.71ms
step:83/1775 train_time:2796ms step_avg:33.69ms
step:84/1775 train_time:2830ms step_avg:33.69ms
step:85/1775 train_time:2862ms step_avg:33.67ms
step:86/1775 train_time:2896ms step_avg:33.68ms
step:87/1775 train_time:2928ms step_avg:33.66ms
step:88/1775 train_time:2963ms step_avg:33.67ms
step:89/1775 train_time:2995ms step_avg:33.65ms
step:90/1775 train_time:3029ms step_avg:33.65ms
step:91/1775 train_time:3061ms step_avg:33.64ms
step:92/1775 train_time:3095ms step_avg:33.64ms
step:93/1775 train_time:3127ms step_avg:33.63ms
step:94/1775 train_time:3161ms step_avg:33.63ms
step:95/1775 train_time:3194ms step_avg:33.62ms
step:96/1775 train_time:3228ms step_avg:33.62ms
step:97/1775 train_time:3260ms step_avg:33.61ms
step:98/1775 train_time:3295ms step_avg:33.62ms
step:99/1775 train_time:3327ms step_avg:33.61ms
step:100/1775 train_time:3361ms step_avg:33.61ms
step:101/1775 train_time:3394ms step_avg:33.60ms
step:102/1775 train_time:3428ms step_avg:33.61ms
step:103/1775 train_time:3461ms step_avg:33.60ms
step:104/1775 train_time:3495ms step_avg:33.61ms
step:105/1775 train_time:3527ms step_avg:33.59ms
step:106/1775 train_time:3562ms step_avg:33.60ms
step:107/1775 train_time:3594ms step_avg:33.58ms
step:108/1775 train_time:3628ms step_avg:33.59ms
step:109/1775 train_time:3660ms step_avg:33.58ms
step:110/1775 train_time:3695ms step_avg:33.59ms
step:111/1775 train_time:3727ms step_avg:33.57ms
step:112/1775 train_time:3761ms step_avg:33.58ms
step:113/1775 train_time:3793ms step_avg:33.57ms
step:114/1775 train_time:3827ms step_avg:33.57ms
step:115/1775 train_time:3860ms step_avg:33.56ms
step:116/1775 train_time:3894ms step_avg:33.57ms
step:117/1775 train_time:3926ms step_avg:33.56ms
step:118/1775 train_time:3960ms step_avg:33.56ms
step:119/1775 train_time:3992ms step_avg:33.55ms
step:120/1775 train_time:4026ms step_avg:33.55ms
step:121/1775 train_time:4058ms step_avg:33.54ms
step:122/1775 train_time:4093ms step_avg:33.55ms
step:123/1775 train_time:4125ms step_avg:33.54ms
step:124/1775 train_time:4159ms step_avg:33.54ms
step:125/1775 train_time:4191ms step_avg:33.53ms
step:126/1775 train_time:4225ms step_avg:33.53ms
step:127/1775 train_time:4257ms step_avg:33.52ms
step:128/1775 train_time:4291ms step_avg:33.53ms
step:129/1775 train_time:4323ms step_avg:33.51ms
step:130/1775 train_time:4358ms step_avg:33.52ms
step:131/1775 train_time:4390ms step_avg:33.51ms
step:132/1775 train_time:4424ms step_avg:33.52ms
step:133/1775 train_time:4456ms step_avg:33.51ms
step:134/1775 train_time:4491ms step_avg:33.51ms
step:135/1775 train_time:4523ms step_avg:33.50ms
step:136/1775 train_time:4557ms step_avg:33.51ms
step:137/1775 train_time:4589ms step_avg:33.50ms
step:138/1775 train_time:4623ms step_avg:33.50ms
step:139/1775 train_time:4655ms step_avg:33.49ms
step:140/1775 train_time:4690ms step_avg:33.50ms
step:141/1775 train_time:4722ms step_avg:33.49ms
step:142/1775 train_time:4756ms step_avg:33.49ms
step:143/1775 train_time:4789ms step_avg:33.49ms
step:144/1775 train_time:4823ms step_avg:33.49ms
step:145/1775 train_time:4855ms step_avg:33.48ms
step:146/1775 train_time:4889ms step_avg:33.49ms
step:147/1775 train_time:4921ms step_avg:33.47ms
step:148/1775 train_time:4955ms step_avg:33.48ms
step:149/1775 train_time:4987ms step_avg:33.47ms
step:150/1775 train_time:5022ms step_avg:33.48ms
step:151/1775 train_time:5053ms step_avg:33.47ms
step:152/1775 train_time:5088ms step_avg:33.47ms
step:153/1775 train_time:5120ms step_avg:33.46ms
step:154/1775 train_time:5154ms step_avg:33.47ms
step:155/1775 train_time:5186ms step_avg:33.46ms
step:156/1775 train_time:5221ms step_avg:33.47ms
step:157/1775 train_time:5253ms step_avg:33.46ms
step:158/1775 train_time:5287ms step_avg:33.46ms
step:159/1775 train_time:5319ms step_avg:33.45ms
step:160/1775 train_time:5353ms step_avg:33.46ms
step:161/1775 train_time:5385ms step_avg:33.45ms
step:162/1775 train_time:5419ms step_avg:33.45ms
step:163/1775 train_time:5451ms step_avg:33.44ms
step:164/1775 train_time:5485ms step_avg:33.45ms
step:165/1775 train_time:5517ms step_avg:33.44ms
step:166/1775 train_time:5552ms step_avg:33.44ms
step:167/1775 train_time:5584ms step_avg:33.44ms
step:168/1775 train_time:5618ms step_avg:33.44ms
step:169/1775 train_time:5650ms step_avg:33.43ms
step:170/1775 train_time:5684ms step_avg:33.44ms
step:171/1775 train_time:5716ms step_avg:33.43ms
step:172/1775 train_time:5751ms step_avg:33.44ms
step:173/1775 train_time:5783ms step_avg:33.43ms
step:174/1775 train_time:5817ms step_avg:33.43ms
step:175/1775 train_time:5850ms step_avg:33.43ms
step:176/1775 train_time:5884ms step_avg:33.43ms
step:177/1775 train_time:5916ms step_avg:33.42ms
step:178/1775 train_time:5950ms step_avg:33.43ms
step:179/1775 train_time:5982ms step_avg:33.42ms
step:180/1775 train_time:6016ms step_avg:33.42ms
step:181/1775 train_time:6048ms step_avg:33.41ms
step:182/1775 train_time:6082ms step_avg:33.42ms
step:183/1775 train_time:6114ms step_avg:33.41ms
step:184/1775 train_time:6148ms step_avg:33.42ms
step:185/1775 train_time:6180ms step_avg:33.41ms
step:186/1775 train_time:6215ms step_avg:33.41ms
step:187/1775 train_time:6247ms step_avg:33.40ms
step:188/1775 train_time:6281ms step_avg:33.41ms
step:189/1775 train_time:6313ms step_avg:33.40ms
step:190/1775 train_time:6347ms step_avg:33.40ms
step:191/1775 train_time:6379ms step_avg:33.40ms
step:192/1775 train_time:6413ms step_avg:33.40ms
step:193/1775 train_time:6445ms step_avg:33.39ms
step:194/1775 train_time:6479ms step_avg:33.40ms
step:195/1775 train_time:6512ms step_avg:33.39ms
step:196/1775 train_time:6546ms step_avg:33.40ms
step:197/1775 train_time:6578ms step_avg:33.39ms
step:198/1775 train_time:6613ms step_avg:33.40ms
step:199/1775 train_time:6645ms step_avg:33.39ms
step:200/1775 train_time:6678ms step_avg:33.39ms
step:201/1775 train_time:6710ms step_avg:33.39ms
step:202/1775 train_time:6745ms step_avg:33.39ms
step:203/1775 train_time:6777ms step_avg:33.38ms
step:204/1775 train_time:6811ms step_avg:33.39ms
step:205/1775 train_time:6843ms step_avg:33.38ms
step:206/1775 train_time:6877ms step_avg:33.38ms
step:207/1775 train_time:6909ms step_avg:33.38ms
step:208/1775 train_time:6943ms step_avg:33.38ms
step:209/1775 train_time:6975ms step_avg:33.37ms
step:210/1775 train_time:7010ms step_avg:33.38ms
step:211/1775 train_time:7042ms step_avg:33.37ms
step:212/1775 train_time:7076ms step_avg:33.38ms
step:213/1775 train_time:7108ms step_avg:33.37ms
step:214/1775 train_time:7142ms step_avg:33.37ms
step:215/1775 train_time:7174ms step_avg:33.37ms
step:216/1775 train_time:7208ms step_avg:33.37ms
step:217/1775 train_time:7240ms step_avg:33.36ms
step:218/1775 train_time:7274ms step_avg:33.37ms
step:219/1775 train_time:7306ms step_avg:33.36ms
step:220/1775 train_time:7340ms step_avg:33.36ms
step:221/1775 train_time:7372ms step_avg:33.36ms
step:222/1775 train_time:7406ms step_avg:33.36ms
step:223/1775 train_time:7438ms step_avg:33.36ms
step:224/1775 train_time:7473ms step_avg:33.36ms
step:225/1775 train_time:7505ms step_avg:33.35ms
step:226/1775 train_time:7539ms step_avg:33.36ms
step:227/1775 train_time:7571ms step_avg:33.35ms
step:228/1775 train_time:7605ms step_avg:33.36ms
step:229/1775 train_time:7638ms step_avg:33.35ms
step:230/1775 train_time:7672ms step_avg:33.36ms
step:231/1775 train_time:7704ms step_avg:33.35ms
step:232/1775 train_time:7739ms step_avg:33.36ms
step:233/1775 train_time:7771ms step_avg:33.35ms
step:234/1775 train_time:7805ms step_avg:33.35ms
step:235/1775 train_time:7837ms step_avg:33.35ms
step:236/1775 train_time:7871ms step_avg:33.35ms
step:237/1775 train_time:7903ms step_avg:33.35ms
step:238/1775 train_time:7937ms step_avg:33.35ms
step:239/1775 train_time:7970ms step_avg:33.35ms
step:240/1775 train_time:8004ms step_avg:33.35ms
step:241/1775 train_time:8035ms step_avg:33.34ms
step:242/1775 train_time:8070ms step_avg:33.35ms
step:243/1775 train_time:8102ms step_avg:33.34ms
step:244/1775 train_time:8136ms step_avg:33.35ms
step:245/1775 train_time:8168ms step_avg:33.34ms
step:246/1775 train_time:8202ms step_avg:33.34ms
step:247/1775 train_time:8234ms step_avg:33.34ms
step:248/1775 train_time:8268ms step_avg:33.34ms
step:249/1775 train_time:8300ms step_avg:33.33ms
step:250/1775 train_time:8334ms step_avg:33.34ms
step:250/1775 val_loss:4.6248 train_time:8375ms step_avg:33.50ms
step:251/1775 train_time:8396ms step_avg:33.45ms
step:252/1775 train_time:8417ms step_avg:33.40ms
step:253/1775 train_time:8435ms step_avg:33.34ms
step:254/1775 train_time:8469ms step_avg:33.34ms
step:255/1775 train_time:8501ms step_avg:33.34ms
step:256/1775 train_time:8538ms step_avg:33.35ms
step:257/1775 train_time:8571ms step_avg:33.35ms
step:258/1775 train_time:8605ms step_avg:33.35ms
step:259/1775 train_time:8638ms step_avg:33.35ms
step:260/1775 train_time:8672ms step_avg:33.35ms
step:261/1775 train_time:8704ms step_avg:33.35ms
step:262/1775 train_time:8738ms step_avg:33.35ms
step:263/1775 train_time:8770ms step_avg:33.35ms
step:264/1775 train_time:8804ms step_avg:33.35ms
step:265/1775 train_time:8835ms step_avg:33.34ms
step:266/1775 train_time:8870ms step_avg:33.34ms
step:267/1775 train_time:8902ms step_avg:33.34ms
step:268/1775 train_time:8936ms step_avg:33.34ms
step:269/1775 train_time:8968ms step_avg:33.34ms
step:270/1775 train_time:9002ms step_avg:33.34ms
step:271/1775 train_time:9034ms step_avg:33.33ms
step:272/1775 train_time:9068ms step_avg:33.34ms
step:273/1775 train_time:9099ms step_avg:33.33ms
step:274/1775 train_time:9133ms step_avg:33.33ms
step:275/1775 train_time:9165ms step_avg:33.33ms
step:276/1775 train_time:9199ms step_avg:33.33ms
step:277/1775 train_time:9231ms step_avg:33.33ms
step:278/1775 train_time:9265ms step_avg:33.33ms
step:279/1775 train_time:9297ms step_avg:33.32ms
step:280/1775 train_time:9331ms step_avg:33.33ms
step:281/1775 train_time:9364ms step_avg:33.32ms
step:282/1775 train_time:9398ms step_avg:33.33ms
step:283/1775 train_time:9430ms step_avg:33.32ms
step:284/1775 train_time:9466ms step_avg:33.33ms
step:285/1775 train_time:9497ms step_avg:33.32ms
step:286/1775 train_time:9532ms step_avg:33.33ms
step:287/1775 train_time:9564ms step_avg:33.32ms
step:288/1775 train_time:9598ms step_avg:33.33ms
step:289/1775 train_time:9630ms step_avg:33.32ms
step:290/1775 train_time:9665ms step_avg:33.33ms
step:291/1775 train_time:9697ms step_avg:33.32ms
step:292/1775 train_time:9731ms step_avg:33.32ms
step:293/1775 train_time:9763ms step_avg:33.32ms
step:294/1775 train_time:9797ms step_avg:33.32ms
step:295/1775 train_time:9829ms step_avg:33.32ms
step:296/1775 train_time:9863ms step_avg:33.32ms
step:297/1775 train_time:9895ms step_avg:33.32ms
step:298/1775 train_time:9929ms step_avg:33.32ms
step:299/1775 train_time:9961ms step_avg:33.31ms
step:300/1775 train_time:9996ms step_avg:33.32ms
step:301/1775 train_time:10027ms step_avg:33.31ms
step:302/1775 train_time:10061ms step_avg:33.32ms
step:303/1775 train_time:10093ms step_avg:33.31ms
step:304/1775 train_time:10127ms step_avg:33.31ms
step:305/1775 train_time:10159ms step_avg:33.31ms
step:306/1775 train_time:10193ms step_avg:33.31ms
step:307/1775 train_time:10225ms step_avg:33.31ms
step:308/1775 train_time:10259ms step_avg:33.31ms
step:309/1775 train_time:10291ms step_avg:33.30ms
step:310/1775 train_time:10325ms step_avg:33.31ms
step:311/1775 train_time:10357ms step_avg:33.30ms
step:312/1775 train_time:10391ms step_avg:33.30ms
step:313/1775 train_time:10423ms step_avg:33.30ms
step:314/1775 train_time:10457ms step_avg:33.30ms
step:315/1775 train_time:10489ms step_avg:33.30ms
step:316/1775 train_time:10524ms step_avg:33.30ms
step:317/1775 train_time:10556ms step_avg:33.30ms
step:318/1775 train_time:10590ms step_avg:33.30ms
step:319/1775 train_time:10622ms step_avg:33.30ms
step:320/1775 train_time:10656ms step_avg:33.30ms
step:321/1775 train_time:10689ms step_avg:33.30ms
step:322/1775 train_time:10723ms step_avg:33.30ms
step:323/1775 train_time:10755ms step_avg:33.30ms
step:324/1775 train_time:10789ms step_avg:33.30ms
step:325/1775 train_time:10821ms step_avg:33.29ms
step:326/1775 train_time:10855ms step_avg:33.30ms
step:327/1775 train_time:10887ms step_avg:33.29ms
step:328/1775 train_time:10921ms step_avg:33.30ms
step:329/1775 train_time:10953ms step_avg:33.29ms
step:330/1775 train_time:10987ms step_avg:33.30ms
step:331/1775 train_time:11019ms step_avg:33.29ms
step:332/1775 train_time:11053ms step_avg:33.29ms
step:333/1775 train_time:11085ms step_avg:33.29ms
step:334/1775 train_time:11119ms step_avg:33.29ms
step:335/1775 train_time:11151ms step_avg:33.29ms
step:336/1775 train_time:11185ms step_avg:33.29ms
step:337/1775 train_time:11217ms step_avg:33.29ms
step:338/1775 train_time:11251ms step_avg:33.29ms
step:339/1775 train_time:11283ms step_avg:33.28ms
step:340/1775 train_time:11317ms step_avg:33.29ms
step:341/1775 train_time:11349ms step_avg:33.28ms
step:342/1775 train_time:11383ms step_avg:33.28ms
step:343/1775 train_time:11415ms step_avg:33.28ms
step:344/1775 train_time:11450ms step_avg:33.28ms
step:345/1775 train_time:11482ms step_avg:33.28ms
step:346/1775 train_time:11516ms step_avg:33.28ms
step:347/1775 train_time:11548ms step_avg:33.28ms
step:348/1775 train_time:11582ms step_avg:33.28ms
step:349/1775 train_time:11614ms step_avg:33.28ms
step:350/1775 train_time:11649ms step_avg:33.28ms
step:351/1775 train_time:11681ms step_avg:33.28ms
step:352/1775 train_time:11715ms step_avg:33.28ms
step:353/1775 train_time:11747ms step_avg:33.28ms
step:354/1775 train_time:11782ms step_avg:33.28ms
step:355/1775 train_time:11814ms step_avg:33.28ms
step:356/1775 train_time:11848ms step_avg:33.28ms
step:357/1775 train_time:11880ms step_avg:33.28ms
step:358/1775 train_time:11914ms step_avg:33.28ms
step:359/1775 train_time:11946ms step_avg:33.28ms
step:360/1775 train_time:11981ms step_avg:33.28ms
step:361/1775 train_time:12013ms step_avg:33.28ms
step:362/1775 train_time:12047ms step_avg:33.28ms
step:363/1775 train_time:12079ms step_avg:33.27ms
step:364/1775 train_time:12113ms step_avg:33.28ms
step:365/1775 train_time:12144ms step_avg:33.27ms
step:366/1775 train_time:12179ms step_avg:33.27ms
step:367/1775 train_time:12211ms step_avg:33.27ms
step:368/1775 train_time:12245ms step_avg:33.27ms
step:369/1775 train_time:12277ms step_avg:33.27ms
step:370/1775 train_time:12310ms step_avg:33.27ms
step:371/1775 train_time:12342ms step_avg:33.27ms
step:372/1775 train_time:12376ms step_avg:33.27ms
step:373/1775 train_time:12408ms step_avg:33.27ms
step:374/1775 train_time:12443ms step_avg:33.27ms
step:375/1775 train_time:12475ms step_avg:33.27ms
step:376/1775 train_time:12509ms step_avg:33.27ms
step:377/1775 train_time:12541ms step_avg:33.27ms
step:378/1775 train_time:12576ms step_avg:33.27ms
step:379/1775 train_time:12608ms step_avg:33.27ms
step:380/1775 train_time:12642ms step_avg:33.27ms
step:381/1775 train_time:12674ms step_avg:33.27ms
step:382/1775 train_time:12708ms step_avg:33.27ms
step:383/1775 train_time:12740ms step_avg:33.26ms
step:384/1775 train_time:12774ms step_avg:33.27ms
step:385/1775 train_time:12806ms step_avg:33.26ms
step:386/1775 train_time:12840ms step_avg:33.26ms
step:387/1775 train_time:12872ms step_avg:33.26ms
step:388/1775 train_time:12907ms step_avg:33.26ms
step:389/1775 train_time:12939ms step_avg:33.26ms
step:390/1775 train_time:12973ms step_avg:33.26ms
step:391/1775 train_time:13005ms step_avg:33.26ms
step:392/1775 train_time:13039ms step_avg:33.26ms
step:393/1775 train_time:13071ms step_avg:33.26ms
step:394/1775 train_time:13105ms step_avg:33.26ms
step:395/1775 train_time:13137ms step_avg:33.26ms
step:396/1775 train_time:13171ms step_avg:33.26ms
step:397/1775 train_time:13203ms step_avg:33.26ms
step:398/1775 train_time:13237ms step_avg:33.26ms
step:399/1775 train_time:13269ms step_avg:33.26ms
step:400/1775 train_time:13303ms step_avg:33.26ms
step:401/1775 train_time:13335ms step_avg:33.25ms
step:402/1775 train_time:13369ms step_avg:33.26ms
step:403/1775 train_time:13401ms step_avg:33.25ms
step:404/1775 train_time:13435ms step_avg:33.26ms
step:405/1775 train_time:13467ms step_avg:33.25ms
step:406/1775 train_time:13502ms step_avg:33.25ms
step:407/1775 train_time:13533ms step_avg:33.25ms
step:408/1775 train_time:13567ms step_avg:33.25ms
step:409/1775 train_time:13600ms step_avg:33.25ms
step:410/1775 train_time:13634ms step_avg:33.25ms
step:411/1775 train_time:13666ms step_avg:33.25ms
step:412/1775 train_time:13700ms step_avg:33.25ms
step:413/1775 train_time:13732ms step_avg:33.25ms
step:414/1775 train_time:13766ms step_avg:33.25ms
step:415/1775 train_time:13798ms step_avg:33.25ms
step:416/1775 train_time:13832ms step_avg:33.25ms
step:417/1775 train_time:13864ms step_avg:33.25ms
step:418/1775 train_time:13899ms step_avg:33.25ms
step:419/1775 train_time:13930ms step_avg:33.25ms
step:420/1775 train_time:13965ms step_avg:33.25ms
step:421/1775 train_time:13997ms step_avg:33.25ms
step:422/1775 train_time:14031ms step_avg:33.25ms
step:423/1775 train_time:14063ms step_avg:33.25ms
step:424/1775 train_time:14097ms step_avg:33.25ms
step:425/1775 train_time:14129ms step_avg:33.24ms
step:426/1775 train_time:14163ms step_avg:33.25ms
step:427/1775 train_time:14195ms step_avg:33.24ms
step:428/1775 train_time:14229ms step_avg:33.25ms
step:429/1775 train_time:14261ms step_avg:33.24ms
step:430/1775 train_time:14295ms step_avg:33.24ms
step:431/1775 train_time:14327ms step_avg:33.24ms
step:432/1775 train_time:14361ms step_avg:33.24ms
step:433/1775 train_time:14393ms step_avg:33.24ms
step:434/1775 train_time:14427ms step_avg:33.24ms
step:435/1775 train_time:14459ms step_avg:33.24ms
step:436/1775 train_time:14493ms step_avg:33.24ms
step:437/1775 train_time:14525ms step_avg:33.24ms
step:438/1775 train_time:14560ms step_avg:33.24ms
step:439/1775 train_time:14591ms step_avg:33.24ms
step:440/1775 train_time:14625ms step_avg:33.24ms
step:441/1775 train_time:14658ms step_avg:33.24ms
step:442/1775 train_time:14691ms step_avg:33.24ms
step:443/1775 train_time:14724ms step_avg:33.24ms
step:444/1775 train_time:14758ms step_avg:33.24ms
step:445/1775 train_time:14790ms step_avg:33.24ms
step:446/1775 train_time:14824ms step_avg:33.24ms
step:447/1775 train_time:14856ms step_avg:33.24ms
step:448/1775 train_time:14891ms step_avg:33.24ms
step:449/1775 train_time:14923ms step_avg:33.24ms
step:450/1775 train_time:14958ms step_avg:33.24ms
step:451/1775 train_time:14990ms step_avg:33.24ms
step:452/1775 train_time:15024ms step_avg:33.24ms
step:453/1775 train_time:15056ms step_avg:33.24ms
step:454/1775 train_time:15090ms step_avg:33.24ms
step:455/1775 train_time:15122ms step_avg:33.24ms
step:456/1775 train_time:15156ms step_avg:33.24ms
step:457/1775 train_time:15188ms step_avg:33.23ms
step:458/1775 train_time:15223ms step_avg:33.24ms
step:459/1775 train_time:15255ms step_avg:33.23ms
step:460/1775 train_time:15289ms step_avg:33.24ms
step:461/1775 train_time:15321ms step_avg:33.23ms
step:462/1775 train_time:15355ms step_avg:33.24ms
step:463/1775 train_time:15386ms step_avg:33.23ms
step:464/1775 train_time:15421ms step_avg:33.23ms
step:465/1775 train_time:15453ms step_avg:33.23ms
step:466/1775 train_time:15487ms step_avg:33.23ms
step:467/1775 train_time:15519ms step_avg:33.23ms
step:468/1775 train_time:15553ms step_avg:33.23ms
step:469/1775 train_time:15585ms step_avg:33.23ms
step:470/1775 train_time:15619ms step_avg:33.23ms
step:471/1775 train_time:15651ms step_avg:33.23ms
step:472/1775 train_time:15686ms step_avg:33.23ms
step:473/1775 train_time:15718ms step_avg:33.23ms
step:474/1775 train_time:15752ms step_avg:33.23ms
step:475/1775 train_time:15784ms step_avg:33.23ms
step:476/1775 train_time:15819ms step_avg:33.23ms
step:477/1775 train_time:15850ms step_avg:33.23ms
step:478/1775 train_time:15885ms step_avg:33.23ms
step:479/1775 train_time:15917ms step_avg:33.23ms
step:480/1775 train_time:15951ms step_avg:33.23ms
step:481/1775 train_time:15983ms step_avg:33.23ms
step:482/1775 train_time:16017ms step_avg:33.23ms
step:483/1775 train_time:16049ms step_avg:33.23ms
step:484/1775 train_time:16084ms step_avg:33.23ms
step:485/1775 train_time:16116ms step_avg:33.23ms
step:486/1775 train_time:16150ms step_avg:33.23ms
step:487/1775 train_time:16182ms step_avg:33.23ms
step:488/1775 train_time:16216ms step_avg:33.23ms
step:489/1775 train_time:16247ms step_avg:33.23ms
step:490/1775 train_time:16282ms step_avg:33.23ms
step:491/1775 train_time:16314ms step_avg:33.23ms
step:492/1775 train_time:16348ms step_avg:33.23ms
step:493/1775 train_time:16380ms step_avg:33.23ms
step:494/1775 train_time:16414ms step_avg:33.23ms
step:495/1775 train_time:16446ms step_avg:33.22ms
step:496/1775 train_time:16480ms step_avg:33.23ms
step:497/1775 train_time:16512ms step_avg:33.22ms
step:498/1775 train_time:16546ms step_avg:33.23ms
step:499/1775 train_time:16579ms step_avg:33.22ms
step:500/1775 train_time:16613ms step_avg:33.23ms
step:500/1775 val_loss:4.2730 train_time:16654ms step_avg:33.31ms
step:501/1775 train_time:16674ms step_avg:33.28ms
step:502/1775 train_time:16695ms step_avg:33.26ms
step:503/1775 train_time:16714ms step_avg:33.23ms
step:504/1775 train_time:16748ms step_avg:33.23ms
step:505/1775 train_time:16781ms step_avg:33.23ms
step:506/1775 train_time:16816ms step_avg:33.23ms
step:507/1775 train_time:16848ms step_avg:33.23ms
step:508/1775 train_time:16882ms step_avg:33.23ms
step:509/1775 train_time:16915ms step_avg:33.23ms
step:510/1775 train_time:16949ms step_avg:33.23ms
step:511/1775 train_time:16982ms step_avg:33.23ms
step:512/1775 train_time:17016ms step_avg:33.23ms
step:513/1775 train_time:17048ms step_avg:33.23ms
step:514/1775 train_time:17082ms step_avg:33.23ms
step:515/1775 train_time:17114ms step_avg:33.23ms
step:516/1775 train_time:17148ms step_avg:33.23ms
step:517/1775 train_time:17179ms step_avg:33.23ms
step:518/1775 train_time:17214ms step_avg:33.23ms
step:519/1775 train_time:17245ms step_avg:33.23ms
step:520/1775 train_time:17279ms step_avg:33.23ms
step:521/1775 train_time:17311ms step_avg:33.23ms
step:522/1775 train_time:17345ms step_avg:33.23ms
step:523/1775 train_time:17377ms step_avg:33.23ms
step:524/1775 train_time:17411ms step_avg:33.23ms
step:525/1775 train_time:17443ms step_avg:33.22ms
step:526/1775 train_time:17477ms step_avg:33.23ms
step:527/1775 train_time:17509ms step_avg:33.22ms
step:528/1775 train_time:17543ms step_avg:33.22ms
step:529/1775 train_time:17575ms step_avg:33.22ms
step:530/1775 train_time:17610ms step_avg:33.23ms
step:531/1775 train_time:17642ms step_avg:33.22ms
step:532/1775 train_time:17676ms step_avg:33.23ms
step:533/1775 train_time:17708ms step_avg:33.22ms
step:534/1775 train_time:17743ms step_avg:33.23ms
step:535/1775 train_time:17775ms step_avg:33.22ms
step:536/1775 train_time:17810ms step_avg:33.23ms
step:537/1775 train_time:17842ms step_avg:33.22ms
step:538/1775 train_time:17876ms step_avg:33.23ms
step:539/1775 train_time:17908ms step_avg:33.23ms
step:540/1775 train_time:17943ms step_avg:33.23ms
step:541/1775 train_time:17975ms step_avg:33.22ms
step:542/1775 train_time:18009ms step_avg:33.23ms
step:543/1775 train_time:18041ms step_avg:33.22ms
step:544/1775 train_time:18076ms step_avg:33.23ms
step:545/1775 train_time:18108ms step_avg:33.23ms
step:546/1775 train_time:18142ms step_avg:33.23ms
step:547/1775 train_time:18174ms step_avg:33.22ms
step:548/1775 train_time:18208ms step_avg:33.23ms
step:549/1775 train_time:18240ms step_avg:33.22ms
step:550/1775 train_time:18274ms step_avg:33.23ms
step:551/1775 train_time:18306ms step_avg:33.22ms
step:552/1775 train_time:18340ms step_avg:33.22ms
step:553/1775 train_time:18372ms step_avg:33.22ms
step:554/1775 train_time:18406ms step_avg:33.22ms
step:555/1775 train_time:18438ms step_avg:33.22ms
step:556/1775 train_time:18472ms step_avg:33.22ms
step:557/1775 train_time:18504ms step_avg:33.22ms
step:558/1775 train_time:18538ms step_avg:33.22ms
step:559/1775 train_time:18570ms step_avg:33.22ms
step:560/1775 train_time:18604ms step_avg:33.22ms
step:561/1775 train_time:18636ms step_avg:33.22ms
step:562/1775 train_time:18670ms step_avg:33.22ms
step:563/1775 train_time:18702ms step_avg:33.22ms
step:564/1775 train_time:18738ms step_avg:33.22ms
step:565/1775 train_time:18769ms step_avg:33.22ms
step:566/1775 train_time:18803ms step_avg:33.22ms
step:567/1775 train_time:18835ms step_avg:33.22ms
step:568/1775 train_time:18869ms step_avg:33.22ms
step:569/1775 train_time:18901ms step_avg:33.22ms
step:570/1775 train_time:18935ms step_avg:33.22ms
step:571/1775 train_time:18968ms step_avg:33.22ms
step:572/1775 train_time:19002ms step_avg:33.22ms
step:573/1775 train_time:19034ms step_avg:33.22ms
step:574/1775 train_time:19068ms step_avg:33.22ms
step:575/1775 train_time:19100ms step_avg:33.22ms
step:576/1775 train_time:19134ms step_avg:33.22ms
step:577/1775 train_time:19167ms step_avg:33.22ms
step:578/1775 train_time:19201ms step_avg:33.22ms
step:579/1775 train_time:19233ms step_avg:33.22ms
step:580/1775 train_time:19269ms step_avg:33.22ms
step:581/1775 train_time:19330ms step_avg:33.27ms
step:582/1775 train_time:19391ms step_avg:33.32ms
step:583/1775 train_time:19451ms step_avg:33.36ms
step:584/1775 train_time:19513ms step_avg:33.41ms
step:585/1775 train_time:19573ms step_avg:33.46ms
step:586/1775 train_time:19636ms step_avg:33.51ms
step:587/1775 train_time:19696ms step_avg:33.55ms
step:588/1775 train_time:19758ms step_avg:33.60ms
step:589/1775 train_time:19818ms step_avg:33.65ms
step:590/1775 train_time:19882ms step_avg:33.70ms
step:591/1775 train_time:19942ms step_avg:33.74ms
step:592/1775 train_time:20003ms step_avg:33.79ms
step:593/1775 train_time:20063ms step_avg:33.83ms
step:594/1775 train_time:20125ms step_avg:33.88ms
step:595/1775 train_time:20184ms step_avg:33.92ms
step:596/1775 train_time:20246ms step_avg:33.97ms
step:597/1775 train_time:20305ms step_avg:34.01ms
step:598/1775 train_time:20367ms step_avg:34.06ms
step:599/1775 train_time:20426ms step_avg:34.10ms
step:600/1775 train_time:20488ms step_avg:34.15ms
step:601/1775 train_time:20547ms step_avg:34.19ms
step:602/1775 train_time:20611ms step_avg:34.24ms
step:603/1775 train_time:20671ms step_avg:34.28ms
step:604/1775 train_time:20734ms step_avg:34.33ms
step:605/1775 train_time:20795ms step_avg:34.37ms
step:606/1775 train_time:20857ms step_avg:34.42ms
step:607/1775 train_time:20917ms step_avg:34.46ms
step:608/1775 train_time:20979ms step_avg:34.51ms
step:609/1775 train_time:21040ms step_avg:34.55ms
step:610/1775 train_time:21102ms step_avg:34.59ms
step:611/1775 train_time:21162ms step_avg:34.64ms
step:612/1775 train_time:21225ms step_avg:34.68ms
step:613/1775 train_time:21284ms step_avg:34.72ms
step:614/1775 train_time:21346ms step_avg:34.77ms
step:615/1775 train_time:21405ms step_avg:34.81ms
step:616/1775 train_time:21467ms step_avg:34.85ms
step:617/1775 train_time:21527ms step_avg:34.89ms
step:618/1775 train_time:21589ms step_avg:34.93ms
step:619/1775 train_time:21648ms step_avg:34.97ms
step:620/1775 train_time:21711ms step_avg:35.02ms
step:621/1775 train_time:21771ms step_avg:35.06ms
step:622/1775 train_time:21835ms step_avg:35.10ms
step:623/1775 train_time:21895ms step_avg:35.14ms
step:624/1775 train_time:21958ms step_avg:35.19ms
step:625/1775 train_time:22018ms step_avg:35.23ms
step:626/1775 train_time:22080ms step_avg:35.27ms
step:627/1775 train_time:22141ms step_avg:35.31ms
step:628/1775 train_time:22203ms step_avg:35.36ms
step:629/1775 train_time:22263ms step_avg:35.39ms
step:630/1775 train_time:22325ms step_avg:35.44ms
step:631/1775 train_time:22385ms step_avg:35.47ms
step:632/1775 train_time:22446ms step_avg:35.52ms
step:633/1775 train_time:22506ms step_avg:35.55ms
step:634/1775 train_time:22568ms step_avg:35.60ms
step:635/1775 train_time:22628ms step_avg:35.63ms
step:636/1775 train_time:22689ms step_avg:35.68ms
step:637/1775 train_time:22748ms step_avg:35.71ms
step:638/1775 train_time:22811ms step_avg:35.75ms
step:639/1775 train_time:22870ms step_avg:35.79ms
step:640/1775 train_time:22933ms step_avg:35.83ms
step:641/1775 train_time:22993ms step_avg:35.87ms
step:642/1775 train_time:23056ms step_avg:35.91ms
step:643/1775 train_time:23117ms step_avg:35.95ms
step:644/1775 train_time:23178ms step_avg:35.99ms
step:645/1775 train_time:23239ms step_avg:36.03ms
step:646/1775 train_time:23301ms step_avg:36.07ms
step:647/1775 train_time:23361ms step_avg:36.11ms
step:648/1775 train_time:23423ms step_avg:36.15ms
step:649/1775 train_time:23484ms step_avg:36.18ms
step:650/1775 train_time:23546ms step_avg:36.22ms
step:651/1775 train_time:23605ms step_avg:36.26ms
step:652/1775 train_time:23667ms step_avg:36.30ms
step:653/1775 train_time:23726ms step_avg:36.33ms
step:654/1775 train_time:23788ms step_avg:36.37ms
step:655/1775 train_time:23847ms step_avg:36.41ms
step:656/1775 train_time:23909ms step_avg:36.45ms
step:657/1775 train_time:23969ms step_avg:36.48ms
step:658/1775 train_time:24031ms step_avg:36.52ms
step:659/1775 train_time:24091ms step_avg:36.56ms
step:660/1775 train_time:24154ms step_avg:36.60ms
step:661/1775 train_time:24214ms step_avg:36.63ms
step:662/1775 train_time:24277ms step_avg:36.67ms
step:663/1775 train_time:24337ms step_avg:36.71ms
step:664/1775 train_time:24400ms step_avg:36.75ms
step:665/1775 train_time:24460ms step_avg:36.78ms
step:666/1775 train_time:24522ms step_avg:36.82ms
step:667/1775 train_time:24582ms step_avg:36.86ms
step:668/1775 train_time:24645ms step_avg:36.89ms
step:669/1775 train_time:24704ms step_avg:36.93ms
step:670/1775 train_time:24766ms step_avg:36.96ms
step:671/1775 train_time:24825ms step_avg:37.00ms
step:672/1775 train_time:24887ms step_avg:37.03ms
step:673/1775 train_time:24946ms step_avg:37.07ms
step:674/1775 train_time:25008ms step_avg:37.10ms
step:675/1775 train_time:25068ms step_avg:37.14ms
step:676/1775 train_time:25131ms step_avg:37.18ms
step:677/1775 train_time:25191ms step_avg:37.21ms
step:678/1775 train_time:25254ms step_avg:37.25ms
step:679/1775 train_time:25315ms step_avg:37.28ms
step:680/1775 train_time:25377ms step_avg:37.32ms
step:681/1775 train_time:25438ms step_avg:37.35ms
step:682/1775 train_time:25500ms step_avg:37.39ms
step:683/1775 train_time:25560ms step_avg:37.42ms
step:684/1775 train_time:25623ms step_avg:37.46ms
step:685/1775 train_time:25684ms step_avg:37.49ms
step:686/1775 train_time:25745ms step_avg:37.53ms
step:687/1775 train_time:25805ms step_avg:37.56ms
step:688/1775 train_time:25867ms step_avg:37.60ms
step:689/1775 train_time:25926ms step_avg:37.63ms
step:690/1775 train_time:25987ms step_avg:37.66ms
step:691/1775 train_time:26046ms step_avg:37.69ms
step:692/1775 train_time:26108ms step_avg:37.73ms
step:693/1775 train_time:26167ms step_avg:37.76ms
step:694/1775 train_time:26231ms step_avg:37.80ms
step:695/1775 train_time:26291ms step_avg:37.83ms
step:696/1775 train_time:26354ms step_avg:37.86ms
step:697/1775 train_time:26414ms step_avg:37.90ms
step:698/1775 train_time:26477ms step_avg:37.93ms
step:699/1775 train_time:26537ms step_avg:37.96ms
step:700/1775 train_time:26600ms step_avg:38.00ms
step:701/1775 train_time:26660ms step_avg:38.03ms
step:702/1775 train_time:26722ms step_avg:38.07ms
step:703/1775 train_time:26782ms step_avg:38.10ms
step:704/1775 train_time:26844ms step_avg:38.13ms
step:705/1775 train_time:26904ms step_avg:38.16ms
step:706/1775 train_time:26966ms step_avg:38.20ms
step:707/1775 train_time:27025ms step_avg:38.23ms
step:708/1775 train_time:27088ms step_avg:38.26ms
step:709/1775 train_time:27147ms step_avg:38.29ms
step:710/1775 train_time:27208ms step_avg:38.32ms
step:711/1775 train_time:27268ms step_avg:38.35ms
step:712/1775 train_time:27330ms step_avg:38.39ms
step:713/1775 train_time:27390ms step_avg:38.41ms
step:714/1775 train_time:27452ms step_avg:38.45ms
step:715/1775 train_time:27512ms step_avg:38.48ms
step:716/1775 train_time:27576ms step_avg:38.51ms
step:717/1775 train_time:27636ms step_avg:38.54ms
step:718/1775 train_time:27698ms step_avg:38.58ms
step:719/1775 train_time:27759ms step_avg:38.61ms
step:720/1775 train_time:27822ms step_avg:38.64ms
step:721/1775 train_time:27882ms step_avg:38.67ms
step:722/1775 train_time:27943ms step_avg:38.70ms
step:723/1775 train_time:28003ms step_avg:38.73ms
step:724/1775 train_time:28065ms step_avg:38.76ms
step:725/1775 train_time:28124ms step_avg:38.79ms
step:726/1775 train_time:28187ms step_avg:38.82ms
step:727/1775 train_time:28246ms step_avg:38.85ms
step:728/1775 train_time:28307ms step_avg:38.88ms
step:729/1775 train_time:28367ms step_avg:38.91ms
step:730/1775 train_time:28428ms step_avg:38.94ms
step:731/1775 train_time:28487ms step_avg:38.97ms
step:732/1775 train_time:28551ms step_avg:39.00ms
step:733/1775 train_time:28612ms step_avg:39.03ms
step:734/1775 train_time:28675ms step_avg:39.07ms
step:735/1775 train_time:28735ms step_avg:39.10ms
step:736/1775 train_time:28798ms step_avg:39.13ms
step:737/1775 train_time:28858ms step_avg:39.16ms
step:738/1775 train_time:28920ms step_avg:39.19ms
step:739/1775 train_time:28980ms step_avg:39.22ms
step:740/1775 train_time:29042ms step_avg:39.25ms
step:741/1775 train_time:29102ms step_avg:39.27ms
step:742/1775 train_time:29164ms step_avg:39.31ms
step:743/1775 train_time:29224ms step_avg:39.33ms
step:744/1775 train_time:29287ms step_avg:39.36ms
step:745/1775 train_time:29346ms step_avg:39.39ms
step:746/1775 train_time:29407ms step_avg:39.42ms
step:747/1775 train_time:29466ms step_avg:39.45ms
step:748/1775 train_time:29528ms step_avg:39.48ms
step:749/1775 train_time:29587ms step_avg:39.50ms
step:750/1775 train_time:29649ms step_avg:39.53ms
step:750/1775 val_loss:3.9919 train_time:29721ms step_avg:39.63ms
step:751/1775 train_time:29742ms step_avg:39.60ms
step:752/1775 train_time:29776ms step_avg:39.60ms
step:753/1775 train_time:29838ms step_avg:39.63ms
step:754/1775 train_time:29900ms step_avg:39.66ms
step:755/1775 train_time:29960ms step_avg:39.68ms
step:756/1775 train_time:30022ms step_avg:39.71ms
step:757/1775 train_time:30081ms step_avg:39.74ms
step:758/1775 train_time:30143ms step_avg:39.77ms
step:759/1775 train_time:30203ms step_avg:39.79ms
step:760/1775 train_time:30264ms step_avg:39.82ms
step:761/1775 train_time:30324ms step_avg:39.85ms
step:762/1775 train_time:30386ms step_avg:39.88ms
step:763/1775 train_time:30445ms step_avg:39.90ms
step:764/1775 train_time:30506ms step_avg:39.93ms
step:765/1775 train_time:30566ms step_avg:39.96ms
step:766/1775 train_time:30627ms step_avg:39.98ms
step:767/1775 train_time:30687ms step_avg:40.01ms
step:768/1775 train_time:30749ms step_avg:40.04ms
step:769/1775 train_time:30809ms step_avg:40.06ms
step:770/1775 train_time:30872ms step_avg:40.09ms
step:771/1775 train_time:30932ms step_avg:40.12ms
step:772/1775 train_time:30994ms step_avg:40.15ms
step:773/1775 train_time:31054ms step_avg:40.17ms
step:774/1775 train_time:31116ms step_avg:40.20ms
step:775/1775 train_time:31176ms step_avg:40.23ms
step:776/1775 train_time:31238ms step_avg:40.26ms
step:777/1775 train_time:31298ms step_avg:40.28ms
step:778/1775 train_time:31359ms step_avg:40.31ms
step:779/1775 train_time:31419ms step_avg:40.33ms
step:780/1775 train_time:31482ms step_avg:40.36ms
step:781/1775 train_time:31542ms step_avg:40.39ms
step:782/1775 train_time:31604ms step_avg:40.41ms
step:783/1775 train_time:31663ms step_avg:40.44ms
step:784/1775 train_time:31726ms step_avg:40.47ms
step:785/1775 train_time:31786ms step_avg:40.49ms
step:786/1775 train_time:31848ms step_avg:40.52ms
step:787/1775 train_time:31907ms step_avg:40.54ms
step:788/1775 train_time:31970ms step_avg:40.57ms
step:789/1775 train_time:32028ms step_avg:40.59ms
step:790/1775 train_time:32090ms step_avg:40.62ms
step:791/1775 train_time:32150ms step_avg:40.64ms
step:792/1775 train_time:32213ms step_avg:40.67ms
step:793/1775 train_time:32272ms step_avg:40.70ms
step:794/1775 train_time:32334ms step_avg:40.72ms
step:795/1775 train_time:32394ms step_avg:40.75ms
step:796/1775 train_time:32456ms step_avg:40.77ms
step:797/1775 train_time:32516ms step_avg:40.80ms
step:798/1775 train_time:32579ms step_avg:40.83ms
step:799/1775 train_time:32639ms step_avg:40.85ms
step:800/1775 train_time:32703ms step_avg:40.88ms
step:801/1775 train_time:32764ms step_avg:40.90ms
step:802/1775 train_time:32826ms step_avg:40.93ms
step:803/1775 train_time:32885ms step_avg:40.95ms
step:804/1775 train_time:32947ms step_avg:40.98ms
step:805/1775 train_time:33007ms step_avg:41.00ms
step:806/1775 train_time:33068ms step_avg:41.03ms
step:807/1775 train_time:33127ms step_avg:41.05ms
step:808/1775 train_time:33189ms step_avg:41.08ms
step:809/1775 train_time:33248ms step_avg:41.10ms
step:810/1775 train_time:33309ms step_avg:41.12ms
step:811/1775 train_time:33369ms step_avg:41.15ms
step:812/1775 train_time:33431ms step_avg:41.17ms
step:813/1775 train_time:33490ms step_avg:41.19ms
step:814/1775 train_time:33554ms step_avg:41.22ms
step:815/1775 train_time:33614ms step_avg:41.24ms
step:816/1775 train_time:33677ms step_avg:41.27ms
step:817/1775 train_time:33738ms step_avg:41.29ms
step:818/1775 train_time:33801ms step_avg:41.32ms
step:819/1775 train_time:33861ms step_avg:41.34ms
step:820/1775 train_time:33923ms step_avg:41.37ms
step:821/1775 train_time:33983ms step_avg:41.39ms
step:822/1775 train_time:34046ms step_avg:41.42ms
step:823/1775 train_time:34106ms step_avg:41.44ms
step:824/1775 train_time:34167ms step_avg:41.46ms
step:825/1775 train_time:34227ms step_avg:41.49ms
step:826/1775 train_time:34288ms step_avg:41.51ms
step:827/1775 train_time:34348ms step_avg:41.53ms
step:828/1775 train_time:34410ms step_avg:41.56ms
step:829/1775 train_time:34468ms step_avg:41.58ms
step:830/1775 train_time:34530ms step_avg:41.60ms
step:831/1775 train_time:34590ms step_avg:41.62ms
step:832/1775 train_time:34653ms step_avg:41.65ms
step:833/1775 train_time:34714ms step_avg:41.67ms
step:834/1775 train_time:34776ms step_avg:41.70ms
step:835/1775 train_time:34837ms step_avg:41.72ms
step:836/1775 train_time:34899ms step_avg:41.75ms
step:837/1775 train_time:34959ms step_avg:41.77ms
step:838/1775 train_time:35022ms step_avg:41.79ms
step:839/1775 train_time:35081ms step_avg:41.81ms
step:840/1775 train_time:35143ms step_avg:41.84ms
step:841/1775 train_time:35204ms step_avg:41.86ms
step:842/1775 train_time:35265ms step_avg:41.88ms
step:843/1775 train_time:35325ms step_avg:41.90ms
step:844/1775 train_time:35387ms step_avg:41.93ms
step:845/1775 train_time:35446ms step_avg:41.95ms
step:846/1775 train_time:35509ms step_avg:41.97ms
step:847/1775 train_time:35568ms step_avg:41.99ms
step:848/1775 train_time:35629ms step_avg:42.02ms
step:849/1775 train_time:35688ms step_avg:42.04ms
step:850/1775 train_time:35751ms step_avg:42.06ms
step:851/1775 train_time:35810ms step_avg:42.08ms
step:852/1775 train_time:35873ms step_avg:42.10ms
step:853/1775 train_time:35935ms step_avg:42.13ms
step:854/1775 train_time:35998ms step_avg:42.15ms
step:855/1775 train_time:36058ms step_avg:42.17ms
step:856/1775 train_time:36120ms step_avg:42.20ms
step:857/1775 train_time:36180ms step_avg:42.22ms
step:858/1775 train_time:36243ms step_avg:42.24ms
step:859/1775 train_time:36303ms step_avg:42.26ms
step:860/1775 train_time:36364ms step_avg:42.28ms
step:861/1775 train_time:36425ms step_avg:42.31ms
step:862/1775 train_time:36487ms step_avg:42.33ms
step:863/1775 train_time:36547ms step_avg:42.35ms
step:864/1775 train_time:36608ms step_avg:42.37ms
step:865/1775 train_time:36667ms step_avg:42.39ms
step:866/1775 train_time:36728ms step_avg:42.41ms
step:867/1775 train_time:36788ms step_avg:42.43ms
step:868/1775 train_time:36850ms step_avg:42.45ms
step:869/1775 train_time:36909ms step_avg:42.47ms
step:870/1775 train_time:36972ms step_avg:42.50ms
step:871/1775 train_time:37033ms step_avg:42.52ms
step:872/1775 train_time:37095ms step_avg:42.54ms
step:873/1775 train_time:37155ms step_avg:42.56ms
step:874/1775 train_time:37218ms step_avg:42.58ms
step:875/1775 train_time:37278ms step_avg:42.60ms
step:876/1775 train_time:37341ms step_avg:42.63ms
step:877/1775 train_time:37401ms step_avg:42.65ms
step:878/1775 train_time:37464ms step_avg:42.67ms
step:879/1775 train_time:37524ms step_avg:42.69ms
step:880/1775 train_time:37586ms step_avg:42.71ms
step:881/1775 train_time:37646ms step_avg:42.73ms
step:882/1775 train_time:37707ms step_avg:42.75ms
step:883/1775 train_time:37766ms step_avg:42.77ms
step:884/1775 train_time:37828ms step_avg:42.79ms
step:885/1775 train_time:37888ms step_avg:42.81ms
step:886/1775 train_time:37950ms step_avg:42.83ms
step:887/1775 train_time:38009ms step_avg:42.85ms
step:888/1775 train_time:38072ms step_avg:42.87ms
step:889/1775 train_time:38132ms step_avg:42.89ms
step:890/1775 train_time:38195ms step_avg:42.92ms
step:891/1775 train_time:38255ms step_avg:42.94ms
step:892/1775 train_time:38318ms step_avg:42.96ms
step:893/1775 train_time:38378ms step_avg:42.98ms
step:894/1775 train_time:38440ms step_avg:43.00ms
step:895/1775 train_time:38501ms step_avg:43.02ms
step:896/1775 train_time:38564ms step_avg:43.04ms
step:897/1775 train_time:38624ms step_avg:43.06ms
step:898/1775 train_time:38686ms step_avg:43.08ms
step:899/1775 train_time:38745ms step_avg:43.10ms
step:900/1775 train_time:38808ms step_avg:43.12ms
step:901/1775 train_time:38867ms step_avg:43.14ms
step:902/1775 train_time:38929ms step_avg:43.16ms
step:903/1775 train_time:38988ms step_avg:43.18ms
step:904/1775 train_time:39050ms step_avg:43.20ms
step:905/1775 train_time:39109ms step_avg:43.21ms
step:906/1775 train_time:39170ms step_avg:43.23ms
step:907/1775 train_time:39231ms step_avg:43.25ms
step:908/1775 train_time:39294ms step_avg:43.28ms
step:909/1775 train_time:39354ms step_avg:43.29ms
step:910/1775 train_time:39417ms step_avg:43.32ms
step:911/1775 train_time:39478ms step_avg:43.33ms
step:912/1775 train_time:39540ms step_avg:43.36ms
step:913/1775 train_time:39600ms step_avg:43.37ms
step:914/1775 train_time:39662ms step_avg:43.39ms
step:915/1775 train_time:39723ms step_avg:43.41ms
step:916/1775 train_time:39785ms step_avg:43.43ms
step:917/1775 train_time:39845ms step_avg:43.45ms
step:918/1775 train_time:39907ms step_avg:43.47ms
step:919/1775 train_time:39967ms step_avg:43.49ms
step:920/1775 train_time:40029ms step_avg:43.51ms
step:921/1775 train_time:40088ms step_avg:43.53ms
step:922/1775 train_time:40150ms step_avg:43.55ms
step:923/1775 train_time:40209ms step_avg:43.56ms
step:924/1775 train_time:40270ms step_avg:43.58ms
step:925/1775 train_time:40330ms step_avg:43.60ms
step:926/1775 train_time:40393ms step_avg:43.62ms
step:927/1775 train_time:40452ms step_avg:43.64ms
step:928/1775 train_time:40515ms step_avg:43.66ms
step:929/1775 train_time:40575ms step_avg:43.68ms
step:930/1775 train_time:40638ms step_avg:43.70ms
step:931/1775 train_time:40698ms step_avg:43.71ms
step:932/1775 train_time:40761ms step_avg:43.73ms
step:933/1775 train_time:40821ms step_avg:43.75ms
step:934/1775 train_time:40884ms step_avg:43.77ms
step:935/1775 train_time:40944ms step_avg:43.79ms
step:936/1775 train_time:41006ms step_avg:43.81ms
step:937/1775 train_time:41066ms step_avg:43.83ms
step:938/1775 train_time:41127ms step_avg:43.85ms
step:939/1775 train_time:41187ms step_avg:43.86ms
step:940/1775 train_time:41248ms step_avg:43.88ms
step:941/1775 train_time:41307ms step_avg:43.90ms
step:942/1775 train_time:41369ms step_avg:43.92ms
step:943/1775 train_time:41428ms step_avg:43.93ms
step:944/1775 train_time:41491ms step_avg:43.95ms
step:945/1775 train_time:41551ms step_avg:43.97ms
step:946/1775 train_time:41613ms step_avg:43.99ms
step:947/1775 train_time:41673ms step_avg:44.01ms
step:948/1775 train_time:41736ms step_avg:44.03ms
step:949/1775 train_time:41796ms step_avg:44.04ms
step:950/1775 train_time:41859ms step_avg:44.06ms
step:951/1775 train_time:41920ms step_avg:44.08ms
step:952/1775 train_time:41983ms step_avg:44.10ms
step:953/1775 train_time:42042ms step_avg:44.12ms
step:954/1775 train_time:42105ms step_avg:44.14ms
step:955/1775 train_time:42164ms step_avg:44.15ms
step:956/1775 train_time:42227ms step_avg:44.17ms
step:957/1775 train_time:42287ms step_avg:44.19ms
step:958/1775 train_time:42349ms step_avg:44.21ms
step:959/1775 train_time:42407ms step_avg:44.22ms
step:960/1775 train_time:42469ms step_avg:44.24ms
step:961/1775 train_time:42528ms step_avg:44.25ms
step:962/1775 train_time:42590ms step_avg:44.27ms
step:963/1775 train_time:42650ms step_avg:44.29ms
step:964/1775 train_time:42712ms step_avg:44.31ms
step:965/1775 train_time:42772ms step_avg:44.32ms
step:966/1775 train_time:42836ms step_avg:44.34ms
step:967/1775 train_time:42896ms step_avg:44.36ms
step:968/1775 train_time:42958ms step_avg:44.38ms
step:969/1775 train_time:43018ms step_avg:44.39ms
step:970/1775 train_time:43080ms step_avg:44.41ms
step:971/1775 train_time:43141ms step_avg:44.43ms
step:972/1775 train_time:43203ms step_avg:44.45ms
step:973/1775 train_time:43263ms step_avg:44.46ms
step:974/1775 train_time:43325ms step_avg:44.48ms
step:975/1775 train_time:43385ms step_avg:44.50ms
step:976/1775 train_time:43446ms step_avg:44.51ms
step:977/1775 train_time:43506ms step_avg:44.53ms
step:978/1775 train_time:43568ms step_avg:44.55ms
step:979/1775 train_time:43626ms step_avg:44.56ms
step:980/1775 train_time:43688ms step_avg:44.58ms
step:981/1775 train_time:43748ms step_avg:44.59ms
step:982/1775 train_time:43810ms step_avg:44.61ms
step:983/1775 train_time:43869ms step_avg:44.63ms
step:984/1775 train_time:43933ms step_avg:44.65ms
step:985/1775 train_time:43993ms step_avg:44.66ms
step:986/1775 train_time:44055ms step_avg:44.68ms
step:987/1775 train_time:44116ms step_avg:44.70ms
step:988/1775 train_time:44179ms step_avg:44.72ms
step:989/1775 train_time:44239ms step_avg:44.73ms
step:990/1775 train_time:44302ms step_avg:44.75ms
step:991/1775 train_time:44362ms step_avg:44.77ms
step:992/1775 train_time:44424ms step_avg:44.78ms
step:993/1775 train_time:44484ms step_avg:44.80ms
step:994/1775 train_time:44547ms step_avg:44.82ms
step:995/1775 train_time:44606ms step_avg:44.83ms
step:996/1775 train_time:44668ms step_avg:44.85ms
step:997/1775 train_time:44727ms step_avg:44.86ms
step:998/1775 train_time:44789ms step_avg:44.88ms
step:999/1775 train_time:44849ms step_avg:44.89ms
step:1000/1775 train_time:44910ms step_avg:44.91ms
step:1000/1775 val_loss:3.7334 train_time:44981ms step_avg:44.98ms
step:1001/1775 train_time:45006ms step_avg:44.96ms
step:1002/1775 train_time:45036ms step_avg:44.95ms
step:1003/1775 train_time:45098ms step_avg:44.96ms
step:1004/1775 train_time:45161ms step_avg:44.98ms
step:1005/1775 train_time:45220ms step_avg:45.00ms
step:1006/1775 train_time:45281ms step_avg:45.01ms
step:1007/1775 train_time:45341ms step_avg:45.03ms
step:1008/1775 train_time:45401ms step_avg:45.04ms
step:1009/1775 train_time:45461ms step_avg:45.06ms
step:1010/1775 train_time:45522ms step_avg:45.07ms
step:1011/1775 train_time:45582ms step_avg:45.09ms
step:1012/1775 train_time:45643ms step_avg:45.10ms
step:1013/1775 train_time:45701ms step_avg:45.11ms
step:1014/1775 train_time:45762ms step_avg:45.13ms
step:1015/1775 train_time:45822ms step_avg:45.14ms
step:1016/1775 train_time:45882ms step_avg:45.16ms
step:1017/1775 train_time:45943ms step_avg:45.17ms
step:1018/1775 train_time:46007ms step_avg:45.19ms
step:1019/1775 train_time:46068ms step_avg:45.21ms
step:1020/1775 train_time:46130ms step_avg:45.23ms
step:1021/1775 train_time:46189ms step_avg:45.24ms
step:1022/1775 train_time:46252ms step_avg:45.26ms
step:1023/1775 train_time:46312ms step_avg:45.27ms
step:1024/1775 train_time:46374ms step_avg:45.29ms
step:1025/1775 train_time:46434ms step_avg:45.30ms
step:1026/1775 train_time:46496ms step_avg:45.32ms
step:1027/1775 train_time:46555ms step_avg:45.33ms
step:1028/1775 train_time:46616ms step_avg:45.35ms
step:1029/1775 train_time:46675ms step_avg:45.36ms
step:1030/1775 train_time:46737ms step_avg:45.38ms
step:1031/1775 train_time:46798ms step_avg:45.39ms
step:1032/1775 train_time:46859ms step_avg:45.41ms
step:1033/1775 train_time:46920ms step_avg:45.42ms
step:1034/1775 train_time:46983ms step_avg:45.44ms
step:1035/1775 train_time:47044ms step_avg:45.45ms
step:1036/1775 train_time:47106ms step_avg:45.47ms
step:1037/1775 train_time:47165ms step_avg:45.48ms
step:1038/1775 train_time:47227ms step_avg:45.50ms
step:1039/1775 train_time:47286ms step_avg:45.51ms
step:1040/1775 train_time:47349ms step_avg:45.53ms
step:1041/1775 train_time:47408ms step_avg:45.54ms
step:1042/1775 train_time:47470ms step_avg:45.56ms
step:1043/1775 train_time:47530ms step_avg:45.57ms
step:1044/1775 train_time:47592ms step_avg:45.59ms
step:1045/1775 train_time:47652ms step_avg:45.60ms
step:1046/1775 train_time:47714ms step_avg:45.62ms
step:1047/1775 train_time:47774ms step_avg:45.63ms
step:1048/1775 train_time:47836ms step_avg:45.64ms
step:1049/1775 train_time:47896ms step_avg:45.66ms
step:1050/1775 train_time:47959ms step_avg:45.68ms
step:1051/1775 train_time:48020ms step_avg:45.69ms
step:1052/1775 train_time:48081ms step_avg:45.70ms
step:1053/1775 train_time:48143ms step_avg:45.72ms
step:1054/1775 train_time:48205ms step_avg:45.73ms
step:1055/1775 train_time:48264ms step_avg:45.75ms
step:1056/1775 train_time:48326ms step_avg:45.76ms
step:1057/1775 train_time:48385ms step_avg:45.78ms
step:1058/1775 train_time:48446ms step_avg:45.79ms
step:1059/1775 train_time:48505ms step_avg:45.80ms
step:1060/1775 train_time:48567ms step_avg:45.82ms
step:1061/1775 train_time:48626ms step_avg:45.83ms
step:1062/1775 train_time:48688ms step_avg:45.85ms
step:1063/1775 train_time:48748ms step_avg:45.86ms
step:1064/1775 train_time:48810ms step_avg:45.87ms
step:1065/1775 train_time:48869ms step_avg:45.89ms
step:1066/1775 train_time:48932ms step_avg:45.90ms
step:1067/1775 train_time:48993ms step_avg:45.92ms
step:1068/1775 train_time:49056ms step_avg:45.93ms
step:1069/1775 train_time:49116ms step_avg:45.95ms
step:1070/1775 train_time:49180ms step_avg:45.96ms
step:1071/1775 train_time:49240ms step_avg:45.98ms
step:1072/1775 train_time:49302ms step_avg:45.99ms
step:1073/1775 train_time:49362ms step_avg:46.00ms
step:1074/1775 train_time:49422ms step_avg:46.02ms
step:1075/1775 train_time:49482ms step_avg:46.03ms
step:1076/1775 train_time:49544ms step_avg:46.04ms
step:1077/1775 train_time:49604ms step_avg:46.06ms
step:1078/1775 train_time:49665ms step_avg:46.07ms
step:1079/1775 train_time:49724ms step_avg:46.08ms
step:1080/1775 train_time:49785ms step_avg:46.10ms
step:1081/1775 train_time:49844ms step_avg:46.11ms
step:1082/1775 train_time:49906ms step_avg:46.12ms
step:1083/1775 train_time:49967ms step_avg:46.14ms
step:1084/1775 train_time:50030ms step_avg:46.15ms
step:1085/1775 train_time:50090ms step_avg:46.17ms
step:1086/1775 train_time:50153ms step_avg:46.18ms
step:1087/1775 train_time:50214ms step_avg:46.19ms
step:1088/1775 train_time:50276ms step_avg:46.21ms
step:1089/1775 train_time:50336ms step_avg:46.22ms
step:1090/1775 train_time:50398ms step_avg:46.24ms
step:1091/1775 train_time:50458ms step_avg:46.25ms
step:1092/1775 train_time:50521ms step_avg:46.26ms
step:1093/1775 train_time:50581ms step_avg:46.28ms
step:1094/1775 train_time:50643ms step_avg:46.29ms
step:1095/1775 train_time:50702ms step_avg:46.30ms
step:1096/1775 train_time:50764ms step_avg:46.32ms
step:1097/1775 train_time:50823ms step_avg:46.33ms
step:1098/1775 train_time:50885ms step_avg:46.34ms
step:1099/1775 train_time:50944ms step_avg:46.35ms
step:1100/1775 train_time:51005ms step_avg:46.37ms
step:1101/1775 train_time:51064ms step_avg:46.38ms
step:1102/1775 train_time:51127ms step_avg:46.39ms
step:1103/1775 train_time:51186ms step_avg:46.41ms
step:1104/1775 train_time:51249ms step_avg:46.42ms
step:1105/1775 train_time:51309ms step_avg:46.43ms
step:1106/1775 train_time:51372ms step_avg:46.45ms
step:1107/1775 train_time:51433ms step_avg:46.46ms
step:1108/1775 train_time:51495ms step_avg:46.48ms
step:1109/1775 train_time:51555ms step_avg:46.49ms
step:1110/1775 train_time:51617ms step_avg:46.50ms
step:1111/1775 train_time:51677ms step_avg:46.51ms
step:1112/1775 train_time:51740ms step_avg:46.53ms
step:1113/1775 train_time:51799ms step_avg:46.54ms
step:1114/1775 train_time:51861ms step_avg:46.55ms
step:1115/1775 train_time:51921ms step_avg:46.57ms
step:1116/1775 train_time:51983ms step_avg:46.58ms
step:1117/1775 train_time:52042ms step_avg:46.59ms
step:1118/1775 train_time:52104ms step_avg:46.60ms
step:1119/1775 train_time:52163ms step_avg:46.62ms
step:1120/1775 train_time:52225ms step_avg:46.63ms
step:1121/1775 train_time:52284ms step_avg:46.64ms
step:1122/1775 train_time:52345ms step_avg:46.65ms
step:1123/1775 train_time:52406ms step_avg:46.67ms
step:1124/1775 train_time:52467ms step_avg:46.68ms
step:1125/1775 train_time:52527ms step_avg:46.69ms
step:1126/1775 train_time:52589ms step_avg:46.70ms
step:1127/1775 train_time:52649ms step_avg:46.72ms
step:1128/1775 train_time:52712ms step_avg:46.73ms
step:1129/1775 train_time:52773ms step_avg:46.74ms
step:1130/1775 train_time:52836ms step_avg:46.76ms
step:1131/1775 train_time:52896ms step_avg:46.77ms
step:1132/1775 train_time:52959ms step_avg:46.78ms
step:1133/1775 train_time:53020ms step_avg:46.80ms
step:1134/1775 train_time:53082ms step_avg:46.81ms
step:1135/1775 train_time:53141ms step_avg:46.82ms
step:1136/1775 train_time:53203ms step_avg:46.83ms
step:1137/1775 train_time:53263ms step_avg:46.84ms
step:1138/1775 train_time:53324ms step_avg:46.86ms
step:1139/1775 train_time:53383ms step_avg:46.87ms
step:1140/1775 train_time:53445ms step_avg:46.88ms
step:1141/1775 train_time:53503ms step_avg:46.89ms
step:1142/1775 train_time:53565ms step_avg:46.90ms
step:1143/1775 train_time:53625ms step_avg:46.92ms
step:1144/1775 train_time:53687ms step_avg:46.93ms
step:1145/1775 train_time:53747ms step_avg:46.94ms
step:1146/1775 train_time:53809ms step_avg:46.95ms
step:1147/1775 train_time:53869ms step_avg:46.97ms
step:1148/1775 train_time:53932ms step_avg:46.98ms
step:1149/1775 train_time:53992ms step_avg:46.99ms
step:1150/1775 train_time:54054ms step_avg:47.00ms
step:1151/1775 train_time:54114ms step_avg:47.01ms
step:1152/1775 train_time:54176ms step_avg:47.03ms
step:1153/1775 train_time:54236ms step_avg:47.04ms
step:1154/1775 train_time:54299ms step_avg:47.05ms
step:1155/1775 train_time:54359ms step_avg:47.06ms
step:1156/1775 train_time:54421ms step_avg:47.08ms
step:1157/1775 train_time:54480ms step_avg:47.09ms
step:1158/1775 train_time:54546ms step_avg:47.10ms
step:1159/1775 train_time:54633ms step_avg:47.14ms
step:1160/1775 train_time:54722ms step_avg:47.17ms
step:1161/1775 train_time:54808ms step_avg:47.21ms
step:1162/1775 train_time:54894ms step_avg:47.24ms
step:1163/1775 train_time:54981ms step_avg:47.27ms
step:1164/1775 train_time:55071ms step_avg:47.31ms
step:1165/1775 train_time:55157ms step_avg:47.35ms
step:1166/1775 train_time:55246ms step_avg:47.38ms
step:1167/1775 train_time:55333ms step_avg:47.41ms
step:1168/1775 train_time:55422ms step_avg:47.45ms
step:1169/1775 train_time:55509ms step_avg:47.48ms
step:1170/1775 train_time:55599ms step_avg:47.52ms
step:1171/1775 train_time:55686ms step_avg:47.55ms
step:1172/1775 train_time:55774ms step_avg:47.59ms
step:1173/1775 train_time:55861ms step_avg:47.62ms
step:1174/1775 train_time:55950ms step_avg:47.66ms
step:1175/1775 train_time:56035ms step_avg:47.69ms
step:1176/1775 train_time:56124ms step_avg:47.72ms
step:1177/1775 train_time:56211ms step_avg:47.76ms
step:1178/1775 train_time:56300ms step_avg:47.79ms
step:1179/1775 train_time:56386ms step_avg:47.83ms
step:1180/1775 train_time:56475ms step_avg:47.86ms
step:1181/1775 train_time:56562ms step_avg:47.89ms
step:1182/1775 train_time:56651ms step_avg:47.93ms
step:1183/1775 train_time:56737ms step_avg:47.96ms
step:1184/1775 train_time:56826ms step_avg:48.00ms
step:1185/1775 train_time:56913ms step_avg:48.03ms
step:1186/1775 train_time:57001ms step_avg:48.06ms
step:1187/1775 train_time:57089ms step_avg:48.10ms
step:1188/1775 train_time:57177ms step_avg:48.13ms
step:1189/1775 train_time:57262ms step_avg:48.16ms
step:1190/1775 train_time:57352ms step_avg:48.19ms
step:1191/1775 train_time:57438ms step_avg:48.23ms
step:1192/1775 train_time:57528ms step_avg:48.26ms
step:1193/1775 train_time:57613ms step_avg:48.29ms
step:1194/1775 train_time:57702ms step_avg:48.33ms
step:1195/1775 train_time:57788ms step_avg:48.36ms
step:1196/1775 train_time:57877ms step_avg:48.39ms
step:1197/1775 train_time:57963ms step_avg:48.42ms
step:1198/1775 train_time:58053ms step_avg:48.46ms
step:1199/1775 train_time:58138ms step_avg:48.49ms
step:1200/1775 train_time:58225ms step_avg:48.52ms
step:1201/1775 train_time:58312ms step_avg:48.55ms
step:1202/1775 train_time:58401ms step_avg:48.59ms
step:1203/1775 train_time:58489ms step_avg:48.62ms
step:1204/1775 train_time:58576ms step_avg:48.65ms
step:1205/1775 train_time:58661ms step_avg:48.68ms
step:1206/1775 train_time:58751ms step_avg:48.72ms
step:1207/1775 train_time:58838ms step_avg:48.75ms
step:1208/1775 train_time:58927ms step_avg:48.78ms
step:1209/1775 train_time:59014ms step_avg:48.81ms
step:1210/1775 train_time:59102ms step_avg:48.84ms
step:1211/1775 train_time:59188ms step_avg:48.88ms
step:1212/1775 train_time:59276ms step_avg:48.91ms
step:1213/1775 train_time:59362ms step_avg:48.94ms
step:1214/1775 train_time:59451ms step_avg:48.97ms
step:1215/1775 train_time:59536ms step_avg:49.00ms
step:1216/1775 train_time:59626ms step_avg:49.03ms
step:1217/1775 train_time:59713ms step_avg:49.07ms
step:1218/1775 train_time:59802ms step_avg:49.10ms
step:1219/1775 train_time:59889ms step_avg:49.13ms
step:1220/1775 train_time:59977ms step_avg:49.16ms
step:1221/1775 train_time:60063ms step_avg:49.19ms
step:1222/1775 train_time:60152ms step_avg:49.22ms
step:1223/1775 train_time:60238ms step_avg:49.25ms
step:1224/1775 train_time:60328ms step_avg:49.29ms
step:1225/1775 train_time:60414ms step_avg:49.32ms
step:1226/1775 train_time:60502ms step_avg:49.35ms
step:1227/1775 train_time:60589ms step_avg:49.38ms
step:1228/1775 train_time:60677ms step_avg:49.41ms
step:1229/1775 train_time:60764ms step_avg:49.44ms
step:1230/1775 train_time:60853ms step_avg:49.47ms
step:1231/1775 train_time:60939ms step_avg:49.50ms
step:1232/1775 train_time:61029ms step_avg:49.54ms
step:1233/1775 train_time:61114ms step_avg:49.57ms
step:1234/1775 train_time:61203ms step_avg:49.60ms
step:1235/1775 train_time:61289ms step_avg:49.63ms
step:1236/1775 train_time:61378ms step_avg:49.66ms
step:1237/1775 train_time:61463ms step_avg:49.69ms
step:1238/1775 train_time:61553ms step_avg:49.72ms
step:1239/1775 train_time:61638ms step_avg:49.75ms
step:1240/1775 train_time:61727ms step_avg:49.78ms
step:1241/1775 train_time:61813ms step_avg:49.81ms
step:1242/1775 train_time:61900ms step_avg:49.84ms
step:1243/1775 train_time:61988ms step_avg:49.87ms
step:1244/1775 train_time:62075ms step_avg:49.90ms
step:1245/1775 train_time:62161ms step_avg:49.93ms
step:1246/1775 train_time:62251ms step_avg:49.96ms
step:1247/1775 train_time:62336ms step_avg:49.99ms
step:1248/1775 train_time:62426ms step_avg:50.02ms
step:1249/1775 train_time:62513ms step_avg:50.05ms
step:1250/1775 train_time:62602ms step_avg:50.08ms
step:1250/1775 val_loss:3.5053 train_time:62701ms step_avg:50.16ms
step:1251/1775 train_time:62723ms step_avg:50.14ms
step:1252/1775 train_time:62785ms step_avg:50.15ms
step:1253/1775 train_time:62875ms step_avg:50.18ms
step:1254/1775 train_time:62963ms step_avg:50.21ms
step:1255/1775 train_time:63048ms step_avg:50.24ms
step:1256/1775 train_time:63135ms step_avg:50.27ms
step:1257/1775 train_time:63221ms step_avg:50.29ms
step:1258/1775 train_time:63309ms step_avg:50.32ms
step:1259/1775 train_time:63394ms step_avg:50.35ms
step:1260/1775 train_time:63483ms step_avg:50.38ms
step:1261/1775 train_time:63568ms step_avg:50.41ms
step:1262/1775 train_time:63659ms step_avg:50.44ms
step:1263/1775 train_time:63746ms step_avg:50.47ms
step:1264/1775 train_time:63839ms step_avg:50.51ms
step:1265/1775 train_time:63927ms step_avg:50.53ms
step:1266/1775 train_time:64015ms step_avg:50.56ms
step:1267/1775 train_time:64102ms step_avg:50.59ms
step:1268/1775 train_time:64190ms step_avg:50.62ms
step:1269/1775 train_time:64274ms step_avg:50.65ms
step:1270/1775 train_time:64363ms step_avg:50.68ms
step:1271/1775 train_time:64447ms step_avg:50.71ms
step:1272/1775 train_time:64536ms step_avg:50.74ms
step:1273/1775 train_time:64624ms step_avg:50.77ms
step:1274/1775 train_time:64714ms step_avg:50.80ms
step:1275/1775 train_time:64804ms step_avg:50.83ms
step:1276/1775 train_time:64893ms step_avg:50.86ms
step:1277/1775 train_time:64980ms step_avg:50.89ms
step:1278/1775 train_time:65068ms step_avg:50.91ms
step:1279/1775 train_time:65154ms step_avg:50.94ms
step:1280/1775 train_time:65242ms step_avg:50.97ms
step:1281/1775 train_time:65328ms step_avg:51.00ms
step:1282/1775 train_time:65416ms step_avg:51.03ms
step:1283/1775 train_time:65501ms step_avg:51.05ms
step:1284/1775 train_time:65591ms step_avg:51.08ms
step:1285/1775 train_time:65677ms step_avg:51.11ms
step:1286/1775 train_time:65766ms step_avg:51.14ms
step:1287/1775 train_time:65854ms step_avg:51.17ms
step:1288/1775 train_time:65943ms step_avg:51.20ms
step:1289/1775 train_time:66029ms step_avg:51.22ms
step:1290/1775 train_time:66118ms step_avg:51.25ms
step:1291/1775 train_time:66203ms step_avg:51.28ms
step:1292/1775 train_time:66292ms step_avg:51.31ms
step:1293/1775 train_time:66377ms step_avg:51.34ms
step:1294/1775 train_time:66466ms step_avg:51.36ms
step:1295/1775 train_time:66552ms step_avg:51.39ms
step:1296/1775 train_time:66640ms step_avg:51.42ms
step:1297/1775 train_time:66727ms step_avg:51.45ms
step:1298/1775 train_time:66816ms step_avg:51.48ms
step:1299/1775 train_time:66904ms step_avg:51.50ms
step:1300/1775 train_time:66995ms step_avg:51.53ms
step:1301/1775 train_time:67081ms step_avg:51.56ms
step:1302/1775 train_time:67170ms step_avg:51.59ms
step:1303/1775 train_time:67256ms step_avg:51.62ms
step:1304/1775 train_time:67344ms step_avg:51.64ms
step:1305/1775 train_time:67430ms step_avg:51.67ms
step:1306/1775 train_time:67519ms step_avg:51.70ms
step:1307/1775 train_time:67604ms step_avg:51.72ms
step:1308/1775 train_time:67693ms step_avg:51.75ms
step:1309/1775 train_time:67780ms step_avg:51.78ms
step:1310/1775 train_time:67869ms step_avg:51.81ms
step:1311/1775 train_time:67956ms step_avg:51.84ms
step:1312/1775 train_time:68045ms step_avg:51.86ms
step:1313/1775 train_time:68131ms step_avg:51.89ms
step:1314/1775 train_time:68220ms step_avg:51.92ms
step:1315/1775 train_time:68304ms step_avg:51.94ms
step:1316/1775 train_time:68393ms step_avg:51.97ms
step:1317/1775 train_time:68479ms step_avg:52.00ms
step:1318/1775 train_time:68568ms step_avg:52.02ms
step:1319/1775 train_time:68653ms step_avg:52.05ms
step:1320/1775 train_time:68743ms step_avg:52.08ms
step:1321/1775 train_time:68829ms step_avg:52.10ms
step:1322/1775 train_time:68918ms step_avg:52.13ms
step:1323/1775 train_time:69005ms step_avg:52.16ms
step:1324/1775 train_time:69095ms step_avg:52.19ms
step:1325/1775 train_time:69182ms step_avg:52.21ms
step:1326/1775 train_time:69270ms step_avg:52.24ms
step:1327/1775 train_time:69356ms step_avg:52.27ms
step:1328/1775 train_time:69444ms step_avg:52.29ms
step:1329/1775 train_time:69530ms step_avg:52.32ms
step:1330/1775 train_time:69619ms step_avg:52.35ms
step:1331/1775 train_time:69705ms step_avg:52.37ms
step:1332/1775 train_time:69795ms step_avg:52.40ms
step:1333/1775 train_time:69882ms step_avg:52.42ms
step:1334/1775 train_time:69970ms step_avg:52.45ms
step:1335/1775 train_time:70057ms step_avg:52.48ms
step:1336/1775 train_time:70145ms step_avg:52.50ms
step:1337/1775 train_time:70232ms step_avg:52.53ms
step:1338/1775 train_time:70321ms step_avg:52.56ms
step:1339/1775 train_time:70408ms step_avg:52.58ms
step:1340/1775 train_time:70499ms step_avg:52.61ms
step:1341/1775 train_time:70583ms step_avg:52.63ms
step:1342/1775 train_time:70673ms step_avg:52.66ms
step:1343/1775 train_time:70760ms step_avg:52.69ms
step:1344/1775 train_time:70848ms step_avg:52.71ms
step:1345/1775 train_time:70934ms step_avg:52.74ms
step:1346/1775 train_time:71023ms step_avg:52.77ms
step:1347/1775 train_time:71109ms step_avg:52.79ms
step:1348/1775 train_time:71200ms step_avg:52.82ms
step:1349/1775 train_time:71286ms step_avg:52.84ms
step:1350/1775 train_time:71375ms step_avg:52.87ms
step:1351/1775 train_time:71463ms step_avg:52.90ms
step:1352/1775 train_time:71551ms step_avg:52.92ms
step:1353/1775 train_time:71637ms step_avg:52.95ms
step:1354/1775 train_time:71725ms step_avg:52.97ms
step:1355/1775 train_time:71810ms step_avg:53.00ms
step:1356/1775 train_time:71901ms step_avg:53.02ms
step:1357/1775 train_time:71987ms step_avg:53.05ms
step:1358/1775 train_time:72078ms step_avg:53.08ms
step:1359/1775 train_time:72164ms step_avg:53.10ms
step:1360/1775 train_time:72252ms step_avg:53.13ms
step:1361/1775 train_time:72339ms step_avg:53.15ms
step:1362/1775 train_time:72427ms step_avg:53.18ms
step:1363/1775 train_time:72514ms step_avg:53.20ms
step:1364/1775 train_time:72603ms step_avg:53.23ms
step:1365/1775 train_time:72688ms step_avg:53.25ms
step:1366/1775 train_time:72778ms step_avg:53.28ms
step:1367/1775 train_time:72863ms step_avg:53.30ms
step:1368/1775 train_time:72952ms step_avg:53.33ms
step:1369/1775 train_time:73038ms step_avg:53.35ms
step:1370/1775 train_time:73127ms step_avg:53.38ms
step:1371/1775 train_time:73213ms step_avg:53.40ms
step:1372/1775 train_time:73302ms step_avg:53.43ms
step:1373/1775 train_time:73389ms step_avg:53.45ms
step:1374/1775 train_time:73479ms step_avg:53.48ms
step:1375/1775 train_time:73564ms step_avg:53.50ms
step:1376/1775 train_time:73653ms step_avg:53.53ms
step:1377/1775 train_time:73740ms step_avg:53.55ms
step:1378/1775 train_time:73829ms step_avg:53.58ms
step:1379/1775 train_time:73915ms step_avg:53.60ms
step:1380/1775 train_time:74004ms step_avg:53.63ms
step:1381/1775 train_time:74090ms step_avg:53.65ms
step:1382/1775 train_time:74178ms step_avg:53.67ms
step:1383/1775 train_time:74264ms step_avg:53.70ms
step:1384/1775 train_time:74354ms step_avg:53.72ms
step:1385/1775 train_time:74440ms step_avg:53.75ms
step:1386/1775 train_time:74529ms step_avg:53.77ms
step:1387/1775 train_time:74614ms step_avg:53.80ms
step:1388/1775 train_time:74704ms step_avg:53.82ms
step:1389/1775 train_time:74790ms step_avg:53.84ms
step:1390/1775 train_time:74879ms step_avg:53.87ms
step:1391/1775 train_time:74965ms step_avg:53.89ms
step:1392/1775 train_time:75054ms step_avg:53.92ms
step:1393/1775 train_time:75140ms step_avg:53.94ms
step:1394/1775 train_time:75227ms step_avg:53.97ms
step:1395/1775 train_time:75313ms step_avg:53.99ms
step:1396/1775 train_time:75403ms step_avg:54.01ms
step:1397/1775 train_time:75489ms step_avg:54.04ms
step:1398/1775 train_time:75578ms step_avg:54.06ms
step:1399/1775 train_time:75665ms step_avg:54.08ms
step:1400/1775 train_time:75753ms step_avg:54.11ms
step:1401/1775 train_time:75839ms step_avg:54.13ms
step:1402/1775 train_time:75928ms step_avg:54.16ms
step:1403/1775 train_time:76015ms step_avg:54.18ms
step:1404/1775 train_time:76104ms step_avg:54.20ms
step:1405/1775 train_time:76188ms step_avg:54.23ms
step:1406/1775 train_time:76277ms step_avg:54.25ms
step:1407/1775 train_time:76363ms step_avg:54.27ms
step:1408/1775 train_time:76452ms step_avg:54.30ms
step:1409/1775 train_time:76539ms step_avg:54.32ms
step:1410/1775 train_time:76627ms step_avg:54.35ms
step:1411/1775 train_time:76713ms step_avg:54.37ms
step:1412/1775 train_time:76803ms step_avg:54.39ms
step:1413/1775 train_time:76888ms step_avg:54.41ms
step:1414/1775 train_time:76978ms step_avg:54.44ms
step:1415/1775 train_time:77065ms step_avg:54.46ms
step:1416/1775 train_time:77153ms step_avg:54.49ms
step:1417/1775 train_time:77239ms step_avg:54.51ms
step:1418/1775 train_time:77327ms step_avg:54.53ms
step:1419/1775 train_time:77412ms step_avg:54.55ms
step:1420/1775 train_time:77502ms step_avg:54.58ms
step:1421/1775 train_time:77589ms step_avg:54.60ms
step:1422/1775 train_time:77680ms step_avg:54.63ms
step:1423/1775 train_time:77765ms step_avg:54.65ms
step:1424/1775 train_time:77853ms step_avg:54.67ms
step:1425/1775 train_time:77940ms step_avg:54.69ms
step:1426/1775 train_time:78029ms step_avg:54.72ms
step:1427/1775 train_time:78116ms step_avg:54.74ms
step:1428/1775 train_time:78203ms step_avg:54.76ms
step:1429/1775 train_time:78289ms step_avg:54.79ms
step:1430/1775 train_time:78379ms step_avg:54.81ms
step:1431/1775 train_time:78464ms step_avg:54.83ms
step:1432/1775 train_time:78552ms step_avg:54.85ms
step:1433/1775 train_time:78639ms step_avg:54.88ms
step:1434/1775 train_time:78727ms step_avg:54.90ms
step:1435/1775 train_time:78814ms step_avg:54.92ms
step:1436/1775 train_time:78904ms step_avg:54.95ms
step:1437/1775 train_time:78990ms step_avg:54.97ms
step:1438/1775 train_time:79080ms step_avg:54.99ms
step:1439/1775 train_time:79166ms step_avg:55.01ms
step:1440/1775 train_time:79254ms step_avg:55.04ms
step:1441/1775 train_time:79340ms step_avg:55.06ms
step:1442/1775 train_time:79429ms step_avg:55.08ms
step:1443/1775 train_time:79515ms step_avg:55.10ms
step:1444/1775 train_time:79604ms step_avg:55.13ms
step:1445/1775 train_time:79689ms step_avg:55.15ms
step:1446/1775 train_time:79779ms step_avg:55.17ms
step:1447/1775 train_time:79865ms step_avg:55.19ms
step:1448/1775 train_time:79955ms step_avg:55.22ms
step:1449/1775 train_time:80042ms step_avg:55.24ms
step:1450/1775 train_time:80129ms step_avg:55.26ms
step:1451/1775 train_time:80215ms step_avg:55.28ms
step:1452/1775 train_time:80304ms step_avg:55.31ms
step:1453/1775 train_time:80389ms step_avg:55.33ms
step:1454/1775 train_time:80480ms step_avg:55.35ms
step:1455/1775 train_time:80566ms step_avg:55.37ms
step:1456/1775 train_time:80655ms step_avg:55.39ms
step:1457/1775 train_time:80742ms step_avg:55.42ms
step:1458/1775 train_time:80832ms step_avg:55.44ms
step:1459/1775 train_time:80917ms step_avg:55.46ms
step:1460/1775 train_time:81005ms step_avg:55.48ms
step:1461/1775 train_time:81092ms step_avg:55.50ms
step:1462/1775 train_time:81182ms step_avg:55.53ms
step:1463/1775 train_time:81268ms step_avg:55.55ms
step:1464/1775 train_time:81357ms step_avg:55.57ms
step:1465/1775 train_time:81443ms step_avg:55.59ms
step:1466/1775 train_time:81531ms step_avg:55.61ms
step:1467/1775 train_time:81618ms step_avg:55.64ms
step:1468/1775 train_time:81707ms step_avg:55.66ms
step:1469/1775 train_time:81793ms step_avg:55.68ms
step:1470/1775 train_time:81883ms step_avg:55.70ms
step:1471/1775 train_time:81968ms step_avg:55.72ms
step:1472/1775 train_time:82057ms step_avg:55.74ms
step:1473/1775 train_time:82143ms step_avg:55.77ms
step:1474/1775 train_time:82233ms step_avg:55.79ms
step:1475/1775 train_time:82319ms step_avg:55.81ms
step:1476/1775 train_time:82407ms step_avg:55.83ms
step:1477/1775 train_time:82492ms step_avg:55.85ms
step:1478/1775 train_time:82582ms step_avg:55.87ms
step:1479/1775 train_time:82668ms step_avg:55.89ms
step:1480/1775 train_time:82757ms step_avg:55.92ms
step:1481/1775 train_time:82842ms step_avg:55.94ms
step:1482/1775 train_time:82932ms step_avg:55.96ms
step:1483/1775 train_time:83019ms step_avg:55.98ms
step:1484/1775 train_time:83106ms step_avg:56.00ms
step:1485/1775 train_time:83192ms step_avg:56.02ms
step:1486/1775 train_time:83281ms step_avg:56.04ms
step:1487/1775 train_time:83367ms step_avg:56.06ms
step:1488/1775 train_time:83455ms step_avg:56.09ms
step:1489/1775 train_time:83541ms step_avg:56.11ms
step:1490/1775 train_time:83630ms step_avg:56.13ms
step:1491/1775 train_time:83716ms step_avg:56.15ms
step:1492/1775 train_time:83805ms step_avg:56.17ms
step:1493/1775 train_time:83891ms step_avg:56.19ms
step:1494/1775 train_time:83979ms step_avg:56.21ms
step:1495/1775 train_time:84065ms step_avg:56.23ms
step:1496/1775 train_time:84154ms step_avg:56.25ms
step:1497/1775 train_time:84241ms step_avg:56.27ms
step:1498/1775 train_time:84329ms step_avg:56.29ms
step:1499/1775 train_time:84416ms step_avg:56.31ms
step:1500/1775 train_time:84504ms step_avg:56.34ms
step:1500/1775 val_loss:3.3771 train_time:84601ms step_avg:56.40ms
step:1501/1775 train_time:84625ms step_avg:56.38ms
step:1502/1775 train_time:84682ms step_avg:56.38ms
step:1503/1775 train_time:84769ms step_avg:56.40ms
step:1504/1775 train_time:84860ms step_avg:56.42ms
step:1505/1775 train_time:84945ms step_avg:56.44ms
step:1506/1775 train_time:85034ms step_avg:56.46ms
step:1507/1775 train_time:85119ms step_avg:56.48ms
step:1508/1775 train_time:85208ms step_avg:56.50ms
step:1509/1775 train_time:85293ms step_avg:56.52ms
step:1510/1775 train_time:85382ms step_avg:56.54ms
step:1511/1775 train_time:85467ms step_avg:56.56ms
step:1512/1775 train_time:85556ms step_avg:56.58ms
step:1513/1775 train_time:85643ms step_avg:56.61ms
step:1514/1775 train_time:85732ms step_avg:56.63ms
step:1515/1775 train_time:85820ms step_avg:56.65ms
step:1516/1775 train_time:85909ms step_avg:56.67ms
step:1517/1775 train_time:85995ms step_avg:56.69ms
step:1518/1775 train_time:86083ms step_avg:56.71ms
step:1519/1775 train_time:86169ms step_avg:56.73ms
step:1520/1775 train_time:86258ms step_avg:56.75ms
step:1521/1775 train_time:86343ms step_avg:56.77ms
step:1522/1775 train_time:86431ms step_avg:56.79ms
step:1523/1775 train_time:86516ms step_avg:56.81ms
step:1524/1775 train_time:86609ms step_avg:56.83ms
step:1525/1775 train_time:86695ms step_avg:56.85ms
step:1526/1775 train_time:86784ms step_avg:56.87ms
step:1527/1775 train_time:86870ms step_avg:56.89ms
step:1528/1775 train_time:86958ms step_avg:56.91ms
step:1529/1775 train_time:87043ms step_avg:56.93ms
step:1530/1775 train_time:87131ms step_avg:56.95ms
step:1531/1775 train_time:87216ms step_avg:56.97ms
step:1532/1775 train_time:87304ms step_avg:56.99ms
step:1533/1775 train_time:87390ms step_avg:57.01ms
step:1534/1775 train_time:87479ms step_avg:57.03ms
step:1535/1775 train_time:87566ms step_avg:57.05ms
step:1536/1775 train_time:87655ms step_avg:57.07ms
step:1537/1775 train_time:87742ms step_avg:57.09ms
step:1538/1775 train_time:87831ms step_avg:57.11ms
step:1539/1775 train_time:87917ms step_avg:57.13ms
step:1540/1775 train_time:88006ms step_avg:57.15ms
step:1541/1775 train_time:88091ms step_avg:57.16ms
step:1542/1775 train_time:88181ms step_avg:57.19ms
step:1543/1775 train_time:88268ms step_avg:57.21ms
step:1544/1775 train_time:88356ms step_avg:57.23ms
step:1545/1775 train_time:88441ms step_avg:57.24ms
step:1546/1775 train_time:88530ms step_avg:57.26ms
step:1547/1775 train_time:88615ms step_avg:57.28ms
step:1548/1775 train_time:88705ms step_avg:57.30ms
step:1549/1775 train_time:88791ms step_avg:57.32ms
step:1550/1775 train_time:88879ms step_avg:57.34ms
step:1551/1775 train_time:88966ms step_avg:57.36ms
step:1552/1775 train_time:89055ms step_avg:57.38ms
step:1553/1775 train_time:89140ms step_avg:57.40ms
step:1554/1775 train_time:89229ms step_avg:57.42ms
step:1555/1775 train_time:89315ms step_avg:57.44ms
step:1556/1775 train_time:89405ms step_avg:57.46ms
step:1557/1775 train_time:89491ms step_avg:57.48ms
step:1558/1775 train_time:89580ms step_avg:57.50ms
step:1559/1775 train_time:89667ms step_avg:57.52ms
step:1560/1775 train_time:89755ms step_avg:57.54ms
step:1561/1775 train_time:89842ms step_avg:57.55ms
step:1562/1775 train_time:89930ms step_avg:57.57ms
step:1563/1775 train_time:90017ms step_avg:57.59ms
step:1564/1775 train_time:90105ms step_avg:57.61ms
step:1565/1775 train_time:90190ms step_avg:57.63ms
step:1566/1775 train_time:90279ms step_avg:57.65ms
step:1567/1775 train_time:90366ms step_avg:57.67ms
step:1568/1775 train_time:90455ms step_avg:57.69ms
step:1569/1775 train_time:90541ms step_avg:57.71ms
step:1570/1775 train_time:90630ms step_avg:57.73ms
step:1571/1775 train_time:90716ms step_avg:57.74ms
step:1572/1775 train_time:90805ms step_avg:57.76ms
step:1573/1775 train_time:90892ms step_avg:57.78ms
step:1574/1775 train_time:90982ms step_avg:57.80ms
step:1575/1775 train_time:91068ms step_avg:57.82ms
step:1576/1775 train_time:91157ms step_avg:57.84ms
step:1577/1775 train_time:91243ms step_avg:57.86ms
step:1578/1775 train_time:91331ms step_avg:57.88ms
step:1579/1775 train_time:91416ms step_avg:57.89ms
step:1580/1775 train_time:91506ms step_avg:57.91ms
step:1581/1775 train_time:91591ms step_avg:57.93ms
step:1582/1775 train_time:91681ms step_avg:57.95ms
step:1583/1775 train_time:91768ms step_avg:57.97ms
step:1584/1775 train_time:91857ms step_avg:57.99ms
step:1585/1775 train_time:91942ms step_avg:58.01ms
step:1586/1775 train_time:92031ms step_avg:58.03ms
step:1587/1775 train_time:92118ms step_avg:58.05ms
step:1588/1775 train_time:92207ms step_avg:58.07ms
step:1589/1775 train_time:92293ms step_avg:58.08ms
step:1590/1775 train_time:92381ms step_avg:58.10ms
step:1591/1775 train_time:92467ms step_avg:58.12ms
step:1592/1775 train_time:92556ms step_avg:58.14ms
step:1593/1775 train_time:92643ms step_avg:58.16ms
step:1594/1775 train_time:92731ms step_avg:58.18ms
step:1595/1775 train_time:92817ms step_avg:58.19ms
step:1596/1775 train_time:92907ms step_avg:58.21ms
step:1597/1775 train_time:92994ms step_avg:58.23ms
step:1598/1775 train_time:93082ms step_avg:58.25ms
step:1599/1775 train_time:93170ms step_avg:58.27ms
step:1600/1775 train_time:93258ms step_avg:58.29ms
step:1601/1775 train_time:93344ms step_avg:58.30ms
step:1602/1775 train_time:93431ms step_avg:58.32ms
step:1603/1775 train_time:93517ms step_avg:58.34ms
step:1604/1775 train_time:93606ms step_avg:58.36ms
step:1605/1775 train_time:93692ms step_avg:58.38ms
step:1606/1775 train_time:93781ms step_avg:58.39ms
step:1607/1775 train_time:93868ms step_avg:58.41ms
step:1608/1775 train_time:93958ms step_avg:58.43ms
step:1609/1775 train_time:94044ms step_avg:58.45ms
step:1610/1775 train_time:94133ms step_avg:58.47ms
step:1611/1775 train_time:94220ms step_avg:58.49ms
step:1612/1775 train_time:94309ms step_avg:58.50ms
step:1613/1775 train_time:94395ms step_avg:58.52ms
step:1614/1775 train_time:94483ms step_avg:58.54ms
step:1615/1775 train_time:94570ms step_avg:58.56ms
step:1616/1775 train_time:94660ms step_avg:58.58ms
step:1617/1775 train_time:94746ms step_avg:58.59ms
step:1618/1775 train_time:94834ms step_avg:58.61ms
step:1619/1775 train_time:94919ms step_avg:58.63ms
step:1620/1775 train_time:95008ms step_avg:58.65ms
step:1621/1775 train_time:95096ms step_avg:58.66ms
step:1622/1775 train_time:95185ms step_avg:58.68ms
step:1623/1775 train_time:95271ms step_avg:58.70ms
step:1624/1775 train_time:95360ms step_avg:58.72ms
step:1625/1775 train_time:95447ms step_avg:58.74ms
step:1626/1775 train_time:95534ms step_avg:58.75ms
step:1627/1775 train_time:95620ms step_avg:58.77ms
step:1628/1775 train_time:95710ms step_avg:58.79ms
step:1629/1775 train_time:95796ms step_avg:58.81ms
step:1630/1775 train_time:95887ms step_avg:58.83ms
step:1631/1775 train_time:95972ms step_avg:58.84ms
step:1632/1775 train_time:96062ms step_avg:58.86ms
step:1633/1775 train_time:96149ms step_avg:58.88ms
step:1634/1775 train_time:96237ms step_avg:58.90ms
step:1635/1775 train_time:96322ms step_avg:58.91ms
step:1636/1775 train_time:96411ms step_avg:58.93ms
step:1637/1775 train_time:96498ms step_avg:58.95ms
step:1638/1775 train_time:96587ms step_avg:58.97ms
step:1639/1775 train_time:96672ms step_avg:58.98ms
step:1640/1775 train_time:96762ms step_avg:59.00ms
step:1641/1775 train_time:96849ms step_avg:59.02ms
step:1642/1775 train_time:96938ms step_avg:59.04ms
step:1643/1775 train_time:97024ms step_avg:59.05ms
step:1644/1775 train_time:97112ms step_avg:59.07ms
step:1645/1775 train_time:97198ms step_avg:59.09ms
step:1646/1775 train_time:97289ms step_avg:59.11ms
step:1647/1775 train_time:97374ms step_avg:59.12ms
step:1648/1775 train_time:97462ms step_avg:59.14ms
step:1649/1775 train_time:97548ms step_avg:59.16ms
step:1650/1775 train_time:97637ms step_avg:59.17ms
step:1651/1775 train_time:97723ms step_avg:59.19ms
step:1652/1775 train_time:97811ms step_avg:59.21ms
step:1653/1775 train_time:97898ms step_avg:59.22ms
step:1654/1775 train_time:97987ms step_avg:59.24ms
step:1655/1775 train_time:98073ms step_avg:59.26ms
step:1656/1775 train_time:98163ms step_avg:59.28ms
step:1657/1775 train_time:98250ms step_avg:59.29ms
step:1658/1775 train_time:98340ms step_avg:59.31ms
step:1659/1775 train_time:98425ms step_avg:59.33ms
step:1660/1775 train_time:98513ms step_avg:59.35ms
step:1661/1775 train_time:98599ms step_avg:59.36ms
step:1662/1775 train_time:98689ms step_avg:59.38ms
step:1663/1775 train_time:98774ms step_avg:59.39ms
step:1664/1775 train_time:98864ms step_avg:59.41ms
step:1665/1775 train_time:98949ms step_avg:59.43ms
step:1666/1775 train_time:99038ms step_avg:59.45ms
step:1667/1775 train_time:99124ms step_avg:59.46ms
step:1668/1775 train_time:99212ms step_avg:59.48ms
step:1669/1775 train_time:99300ms step_avg:59.50ms
step:1670/1775 train_time:99389ms step_avg:59.51ms
step:1671/1775 train_time:99475ms step_avg:59.53ms
step:1672/1775 train_time:99564ms step_avg:59.55ms
step:1673/1775 train_time:99650ms step_avg:59.56ms
step:1674/1775 train_time:99738ms step_avg:59.58ms
step:1675/1775 train_time:99824ms step_avg:59.60ms
step:1676/1775 train_time:99912ms step_avg:59.61ms
step:1677/1775 train_time:99998ms step_avg:59.63ms
step:1678/1775 train_time:100087ms step_avg:59.65ms
step:1679/1775 train_time:100173ms step_avg:59.66ms
step:1680/1775 train_time:100262ms step_avg:59.68ms
step:1681/1775 train_time:100348ms step_avg:59.70ms
step:1682/1775 train_time:100438ms step_avg:59.71ms
step:1683/1775 train_time:100523ms step_avg:59.73ms
step:1684/1775 train_time:100611ms step_avg:59.75ms
step:1685/1775 train_time:100698ms step_avg:59.76ms
step:1686/1775 train_time:100787ms step_avg:59.78ms
step:1687/1775 train_time:100873ms step_avg:59.79ms
step:1688/1775 train_time:100962ms step_avg:59.81ms
step:1689/1775 train_time:101047ms step_avg:59.83ms
step:1690/1775 train_time:101136ms step_avg:59.84ms
step:1691/1775 train_time:101222ms step_avg:59.86ms
step:1692/1775 train_time:101311ms step_avg:59.88ms
step:1693/1775 train_time:101398ms step_avg:59.89ms
step:1694/1775 train_time:101487ms step_avg:59.91ms
step:1695/1775 train_time:101573ms step_avg:59.92ms
step:1696/1775 train_time:101661ms step_avg:59.94ms
step:1697/1775 train_time:101747ms step_avg:59.96ms
step:1698/1775 train_time:101835ms step_avg:59.97ms
step:1699/1775 train_time:101921ms step_avg:59.99ms
step:1700/1775 train_time:102010ms step_avg:60.01ms
step:1701/1775 train_time:102096ms step_avg:60.02ms
step:1702/1775 train_time:102185ms step_avg:60.04ms
step:1703/1775 train_time:102271ms step_avg:60.05ms
step:1704/1775 train_time:102359ms step_avg:60.07ms
step:1705/1775 train_time:102447ms step_avg:60.09ms
step:1706/1775 train_time:102535ms step_avg:60.10ms
step:1707/1775 train_time:102621ms step_avg:60.12ms
step:1708/1775 train_time:102710ms step_avg:60.13ms
step:1709/1775 train_time:102796ms step_avg:60.15ms
step:1710/1775 train_time:102885ms step_avg:60.17ms
step:1711/1775 train_time:102971ms step_avg:60.18ms
step:1712/1775 train_time:103059ms step_avg:60.20ms
step:1713/1775 train_time:103146ms step_avg:60.21ms
step:1714/1775 train_time:103235ms step_avg:60.23ms
step:1715/1775 train_time:103322ms step_avg:60.25ms
step:1716/1775 train_time:103411ms step_avg:60.26ms
step:1717/1775 train_time:103497ms step_avg:60.28ms
step:1718/1775 train_time:103585ms step_avg:60.29ms
step:1719/1775 train_time:103672ms step_avg:60.31ms
step:1720/1775 train_time:103760ms step_avg:60.33ms
step:1721/1775 train_time:103845ms step_avg:60.34ms
step:1722/1775 train_time:103934ms step_avg:60.36ms
step:1723/1775 train_time:104020ms step_avg:60.37ms
step:1724/1775 train_time:104108ms step_avg:60.39ms
step:1725/1775 train_time:104196ms step_avg:60.40ms
step:1726/1775 train_time:104284ms step_avg:60.42ms
step:1727/1775 train_time:104370ms step_avg:60.43ms
step:1728/1775 train_time:104460ms step_avg:60.45ms
step:1729/1775 train_time:104547ms step_avg:60.47ms
step:1730/1775 train_time:104635ms step_avg:60.48ms
step:1731/1775 train_time:104721ms step_avg:60.50ms
step:1732/1775 train_time:104810ms step_avg:60.51ms
step:1733/1775 train_time:104896ms step_avg:60.53ms
step:1734/1775 train_time:104985ms step_avg:60.54ms
step:1735/1775 train_time:105071ms step_avg:60.56ms
step:1736/1775 train_time:105165ms step_avg:60.58ms
step:1737/1775 train_time:105251ms step_avg:60.59ms
step:1738/1775 train_time:105341ms step_avg:60.61ms
step:1739/1775 train_time:105427ms step_avg:60.63ms
step:1740/1775 train_time:105517ms step_avg:60.64ms
step:1741/1775 train_time:105605ms step_avg:60.66ms
step:1742/1775 train_time:105694ms step_avg:60.67ms
step:1743/1775 train_time:105780ms step_avg:60.69ms
step:1744/1775 train_time:105870ms step_avg:60.71ms
step:1745/1775 train_time:105955ms step_avg:60.72ms
step:1746/1775 train_time:106045ms step_avg:60.74ms
step:1747/1775 train_time:106132ms step_avg:60.75ms
step:1748/1775 train_time:106221ms step_avg:60.77ms
step:1749/1775 train_time:106308ms step_avg:60.78ms
step:1750/1775 train_time:106396ms step_avg:60.80ms
step:1750/1775 val_loss:3.2858 train_time:106496ms step_avg:60.85ms
step:1751/1775 train_time:106516ms step_avg:60.83ms
step:1752/1775 train_time:106578ms step_avg:60.83ms
step:1753/1775 train_time:106666ms step_avg:60.85ms
step:1754/1775 train_time:106756ms step_avg:60.86ms
step:1755/1775 train_time:106842ms step_avg:60.88ms
step:1756/1775 train_time:106930ms step_avg:60.89ms
step:1757/1775 train_time:107015ms step_avg:60.91ms
step:1758/1775 train_time:107103ms step_avg:60.92ms
step:1759/1775 train_time:107189ms step_avg:60.94ms
step:1760/1775 train_time:107277ms step_avg:60.95ms
step:1761/1775 train_time:107363ms step_avg:60.97ms
step:1762/1775 train_time:107452ms step_avg:60.98ms
step:1763/1775 train_time:107540ms step_avg:61.00ms
step:1764/1775 train_time:107633ms step_avg:61.02ms
step:1765/1775 train_time:107719ms step_avg:61.03ms
step:1766/1775 train_time:107809ms step_avg:61.05ms
step:1767/1775 train_time:107895ms step_avg:61.06ms
step:1768/1775 train_time:107982ms step_avg:61.08ms
step:1769/1775 train_time:108070ms step_avg:61.09ms
step:1770/1775 train_time:108158ms step_avg:61.11ms
step:1771/1775 train_time:108243ms step_avg:61.12ms
step:1772/1775 train_time:108332ms step_avg:61.14ms
step:1773/1775 train_time:108417ms step_avg:61.15ms
step:1774/1775 train_time:108508ms step_avg:61.17ms
step:1775/1775 train_time:108595ms step_avg:61.18ms
step:1775/1775 val_loss:3.2793 train_time:108695ms step_avg:61.24ms
peak memory allocated: 29148 MiB reserved: 44578 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Jan  7 09:01:12 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   44C    P0            128W /  700W |    1520MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   34C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   42C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     74158      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     74159      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     74160      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     74161      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     74162      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     74163      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     74164      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     74165      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8275 train_time:0ms step_avg:0.03ms
step:1/1775 train_time:75ms step_avg:75.44ms
step:2/1775 train_time:98ms step_avg:49.17ms
step:3/1775 train_time:118ms step_avg:39.32ms
step:4/1775 train_time:148ms step_avg:36.98ms
step:5/1775 train_time:180ms step_avg:35.97ms
step:6/1775 train_time:262ms step_avg:43.74ms
step:7/1775 train_time:280ms step_avg:39.98ms
step:8/1775 train_time:309ms step_avg:38.66ms
step:9/1775 train_time:341ms step_avg:37.91ms
step:10/1775 train_time:375ms step_avg:37.52ms
step:11/1775 train_time:407ms step_avg:37.02ms
step:12/1775 train_time:441ms step_avg:36.79ms
step:13/1775 train_time:473ms step_avg:36.42ms
step:14/1775 train_time:508ms step_avg:36.26ms
step:15/1775 train_time:540ms step_avg:35.99ms
step:16/1775 train_time:574ms step_avg:35.87ms
step:17/1775 train_time:606ms step_avg:35.64ms
step:18/1775 train_time:640ms step_avg:35.54ms
step:19/1775 train_time:672ms step_avg:35.35ms
step:20/1775 train_time:706ms step_avg:35.29ms
step:21/1775 train_time:738ms step_avg:35.13ms
step:22/1775 train_time:772ms step_avg:35.08ms
step:23/1775 train_time:804ms step_avg:34.96ms
step:24/1775 train_time:838ms step_avg:34.92ms
step:25/1775 train_time:870ms step_avg:34.79ms
step:26/1775 train_time:904ms step_avg:34.77ms
step:27/1775 train_time:937ms step_avg:34.69ms
step:28/1775 train_time:971ms step_avg:34.66ms
step:29/1775 train_time:1003ms step_avg:34.57ms
step:30/1775 train_time:1037ms step_avg:34.56ms
step:31/1775 train_time:1069ms step_avg:34.47ms
step:32/1775 train_time:1103ms step_avg:34.46ms
step:33/1775 train_time:1135ms step_avg:34.38ms
step:34/1775 train_time:1169ms step_avg:34.37ms
step:35/1775 train_time:1201ms step_avg:34.32ms
step:36/1775 train_time:1236ms step_avg:34.34ms
step:37/1775 train_time:1269ms step_avg:34.29ms
step:38/1775 train_time:1304ms step_avg:34.31ms
step:39/1775 train_time:1336ms step_avg:34.26ms
step:40/1775 train_time:1371ms step_avg:34.27ms
step:41/1775 train_time:1403ms step_avg:34.21ms
step:42/1775 train_time:1437ms step_avg:34.22ms
step:43/1775 train_time:1469ms step_avg:34.17ms
step:44/1775 train_time:1504ms step_avg:34.17ms
step:45/1775 train_time:1536ms step_avg:34.13ms
step:46/1775 train_time:1570ms step_avg:34.14ms
step:47/1775 train_time:1602ms step_avg:34.10ms
step:48/1775 train_time:1637ms step_avg:34.10ms
step:49/1775 train_time:1669ms step_avg:34.05ms
step:50/1775 train_time:1703ms step_avg:34.06ms
step:51/1775 train_time:1735ms step_avg:34.01ms
step:52/1775 train_time:1769ms step_avg:34.02ms
step:53/1775 train_time:1801ms step_avg:33.99ms
step:54/1775 train_time:1835ms step_avg:33.99ms
step:55/1775 train_time:1868ms step_avg:33.95ms
step:56/1775 train_time:1902ms step_avg:33.96ms
step:57/1775 train_time:1934ms step_avg:33.93ms
step:58/1775 train_time:1968ms step_avg:33.93ms
step:59/1775 train_time:2001ms step_avg:33.91ms
step:60/1775 train_time:2035ms step_avg:33.91ms
step:61/1775 train_time:2067ms step_avg:33.88ms
step:62/1775 train_time:2101ms step_avg:33.89ms
step:63/1775 train_time:2134ms step_avg:33.87ms
step:64/1775 train_time:2168ms step_avg:33.88ms
step:65/1775 train_time:2200ms step_avg:33.85ms
step:66/1775 train_time:2235ms step_avg:33.86ms
step:67/1775 train_time:2267ms step_avg:33.83ms
step:68/1775 train_time:2301ms step_avg:33.84ms
step:69/1775 train_time:2334ms step_avg:33.82ms
step:70/1775 train_time:2368ms step_avg:33.83ms
step:71/1775 train_time:2400ms step_avg:33.81ms
step:72/1775 train_time:2435ms step_avg:33.82ms
step:73/1775 train_time:2467ms step_avg:33.80ms
step:74/1775 train_time:2501ms step_avg:33.80ms
step:75/1775 train_time:2534ms step_avg:33.78ms
step:76/1775 train_time:2568ms step_avg:33.79ms
step:77/1775 train_time:2600ms step_avg:33.77ms
step:78/1775 train_time:2634ms step_avg:33.77ms
step:79/1775 train_time:2666ms step_avg:33.75ms
step:80/1775 train_time:2701ms step_avg:33.76ms
step:81/1775 train_time:2732ms step_avg:33.73ms
step:82/1775 train_time:2767ms step_avg:33.74ms
step:83/1775 train_time:2799ms step_avg:33.72ms
step:84/1775 train_time:2833ms step_avg:33.73ms
step:85/1775 train_time:2865ms step_avg:33.71ms
step:86/1775 train_time:2899ms step_avg:33.71ms
step:87/1775 train_time:2931ms step_avg:33.69ms
step:88/1775 train_time:2966ms step_avg:33.70ms
step:89/1775 train_time:2998ms step_avg:33.68ms
step:90/1775 train_time:3032ms step_avg:33.69ms
step:91/1775 train_time:3064ms step_avg:33.67ms
step:92/1775 train_time:3097ms step_avg:33.67ms
step:93/1775 train_time:3130ms step_avg:33.65ms
step:94/1775 train_time:3164ms step_avg:33.66ms
step:95/1775 train_time:3196ms step_avg:33.64ms
step:96/1775 train_time:3231ms step_avg:33.65ms
step:97/1775 train_time:3263ms step_avg:33.63ms
step:98/1775 train_time:3297ms step_avg:33.64ms
step:99/1775 train_time:3329ms step_avg:33.63ms
step:100/1775 train_time:3364ms step_avg:33.64ms
step:101/1775 train_time:3396ms step_avg:33.63ms
step:102/1775 train_time:3430ms step_avg:33.63ms
step:103/1775 train_time:3463ms step_avg:33.62ms
step:104/1775 train_time:3497ms step_avg:33.63ms
step:105/1775 train_time:3529ms step_avg:33.61ms
step:106/1775 train_time:3564ms step_avg:33.62ms
step:107/1775 train_time:3596ms step_avg:33.60ms
step:108/1775 train_time:3630ms step_avg:33.61ms
step:109/1775 train_time:3662ms step_avg:33.59ms
step:110/1775 train_time:3696ms step_avg:33.60ms
step:111/1775 train_time:3728ms step_avg:33.59ms
step:112/1775 train_time:3762ms step_avg:33.59ms
step:113/1775 train_time:3794ms step_avg:33.58ms
step:114/1775 train_time:3829ms step_avg:33.58ms
step:115/1775 train_time:3860ms step_avg:33.57ms
step:116/1775 train_time:3895ms step_avg:33.57ms
step:117/1775 train_time:3927ms step_avg:33.56ms
step:118/1775 train_time:3961ms step_avg:33.57ms
step:119/1775 train_time:3993ms step_avg:33.56ms
step:120/1775 train_time:4027ms step_avg:33.56ms
step:121/1775 train_time:4059ms step_avg:33.55ms
step:122/1775 train_time:4093ms step_avg:33.55ms
step:123/1775 train_time:4125ms step_avg:33.54ms
step:124/1775 train_time:4160ms step_avg:33.55ms
step:125/1775 train_time:4192ms step_avg:33.54ms
step:126/1775 train_time:4227ms step_avg:33.55ms
step:127/1775 train_time:4259ms step_avg:33.54ms
step:128/1775 train_time:4293ms step_avg:33.54ms
step:129/1775 train_time:4326ms step_avg:33.53ms
step:130/1775 train_time:4360ms step_avg:33.54ms
step:131/1775 train_time:4392ms step_avg:33.53ms
step:132/1775 train_time:4427ms step_avg:33.54ms
step:133/1775 train_time:4459ms step_avg:33.53ms
step:134/1775 train_time:4493ms step_avg:33.53ms
step:135/1775 train_time:4526ms step_avg:33.52ms
step:136/1775 train_time:4560ms step_avg:33.53ms
step:137/1775 train_time:4592ms step_avg:33.52ms
step:138/1775 train_time:4626ms step_avg:33.53ms
step:139/1775 train_time:4659ms step_avg:33.52ms
step:140/1775 train_time:4693ms step_avg:33.52ms
step:141/1775 train_time:4725ms step_avg:33.51ms
step:142/1775 train_time:4760ms step_avg:33.52ms
step:143/1775 train_time:4792ms step_avg:33.51ms
step:144/1775 train_time:4826ms step_avg:33.51ms
step:145/1775 train_time:4858ms step_avg:33.50ms
step:146/1775 train_time:4892ms step_avg:33.51ms
step:147/1775 train_time:4924ms step_avg:33.50ms
step:148/1775 train_time:4959ms step_avg:33.51ms
step:149/1775 train_time:4991ms step_avg:33.50ms
step:150/1775 train_time:5025ms step_avg:33.50ms
step:151/1775 train_time:5057ms step_avg:33.49ms
step:152/1775 train_time:5091ms step_avg:33.50ms
step:153/1775 train_time:5123ms step_avg:33.49ms
step:154/1775 train_time:5157ms step_avg:33.49ms
step:155/1775 train_time:5190ms step_avg:33.48ms
step:156/1775 train_time:5224ms step_avg:33.49ms
step:157/1775 train_time:5256ms step_avg:33.48ms
step:158/1775 train_time:5290ms step_avg:33.48ms
step:159/1775 train_time:5323ms step_avg:33.48ms
step:160/1775 train_time:5357ms step_avg:33.48ms
step:161/1775 train_time:5389ms step_avg:33.47ms
step:162/1775 train_time:5423ms step_avg:33.48ms
step:163/1775 train_time:5455ms step_avg:33.47ms
step:164/1775 train_time:5490ms step_avg:33.48ms
step:165/1775 train_time:5522ms step_avg:33.47ms
step:166/1775 train_time:5556ms step_avg:33.47ms
step:167/1775 train_time:5588ms step_avg:33.46ms
step:168/1775 train_time:5623ms step_avg:33.47ms
step:169/1775 train_time:5655ms step_avg:33.46ms
step:170/1775 train_time:5689ms step_avg:33.47ms
step:171/1775 train_time:5721ms step_avg:33.46ms
step:172/1775 train_time:5755ms step_avg:33.46ms
step:173/1775 train_time:5787ms step_avg:33.45ms
step:174/1775 train_time:5822ms step_avg:33.46ms
step:175/1775 train_time:5854ms step_avg:33.45ms
step:176/1775 train_time:5889ms step_avg:33.46ms
step:177/1775 train_time:5921ms step_avg:33.45ms
step:178/1775 train_time:5955ms step_avg:33.45ms
step:179/1775 train_time:5987ms step_avg:33.45ms
step:180/1775 train_time:6021ms step_avg:33.45ms
step:181/1775 train_time:6054ms step_avg:33.45ms
step:182/1775 train_time:6088ms step_avg:33.45ms
step:183/1775 train_time:6120ms step_avg:33.44ms
step:184/1775 train_time:6154ms step_avg:33.45ms
step:185/1775 train_time:6186ms step_avg:33.44ms
step:186/1775 train_time:6221ms step_avg:33.44ms
step:187/1775 train_time:6253ms step_avg:33.44ms
step:188/1775 train_time:6287ms step_avg:33.44ms
step:189/1775 train_time:6319ms step_avg:33.43ms
step:190/1775 train_time:6353ms step_avg:33.44ms
step:191/1775 train_time:6385ms step_avg:33.43ms
step:192/1775 train_time:6420ms step_avg:33.44ms
step:193/1775 train_time:6452ms step_avg:33.43ms
step:194/1775 train_time:6486ms step_avg:33.43ms
step:195/1775 train_time:6518ms step_avg:33.43ms
step:196/1775 train_time:6552ms step_avg:33.43ms
step:197/1775 train_time:6584ms step_avg:33.42ms
step:198/1775 train_time:6619ms step_avg:33.43ms
step:199/1775 train_time:6651ms step_avg:33.42ms
step:200/1775 train_time:6685ms step_avg:33.43ms
step:201/1775 train_time:6717ms step_avg:33.42ms
step:202/1775 train_time:6751ms step_avg:33.42ms
step:203/1775 train_time:6784ms step_avg:33.42ms
step:204/1775 train_time:6818ms step_avg:33.42ms
step:205/1775 train_time:6850ms step_avg:33.41ms
step:206/1775 train_time:6884ms step_avg:33.42ms
step:207/1775 train_time:6916ms step_avg:33.41ms
step:208/1775 train_time:6950ms step_avg:33.41ms
step:209/1775 train_time:6982ms step_avg:33.41ms
step:210/1775 train_time:7016ms step_avg:33.41ms
step:211/1775 train_time:7048ms step_avg:33.40ms
step:212/1775 train_time:7082ms step_avg:33.41ms
step:213/1775 train_time:7114ms step_avg:33.40ms
step:214/1775 train_time:7148ms step_avg:33.40ms
step:215/1775 train_time:7181ms step_avg:33.40ms
step:216/1775 train_time:7215ms step_avg:33.40ms
step:217/1775 train_time:7247ms step_avg:33.40ms
step:218/1775 train_time:7282ms step_avg:33.40ms
step:219/1775 train_time:7314ms step_avg:33.40ms
step:220/1775 train_time:7348ms step_avg:33.40ms
step:221/1775 train_time:7380ms step_avg:33.39ms
step:222/1775 train_time:7414ms step_avg:33.40ms
step:223/1775 train_time:7446ms step_avg:33.39ms
step:224/1775 train_time:7480ms step_avg:33.39ms
step:225/1775 train_time:7512ms step_avg:33.39ms
step:226/1775 train_time:7546ms step_avg:33.39ms
step:227/1775 train_time:7578ms step_avg:33.38ms
step:228/1775 train_time:7612ms step_avg:33.39ms
step:229/1775 train_time:7645ms step_avg:33.38ms
step:230/1775 train_time:7679ms step_avg:33.39ms
step:231/1775 train_time:7711ms step_avg:33.38ms
step:232/1775 train_time:7746ms step_avg:33.39ms
step:233/1775 train_time:7778ms step_avg:33.38ms
step:234/1775 train_time:7812ms step_avg:33.38ms
step:235/1775 train_time:7844ms step_avg:33.38ms
step:236/1775 train_time:7878ms step_avg:33.38ms
step:237/1775 train_time:7910ms step_avg:33.38ms
step:238/1775 train_time:7944ms step_avg:33.38ms
step:239/1775 train_time:7976ms step_avg:33.37ms
step:240/1775 train_time:8011ms step_avg:33.38ms
step:241/1775 train_time:8043ms step_avg:33.37ms
step:242/1775 train_time:8078ms step_avg:33.38ms
step:243/1775 train_time:8109ms step_avg:33.37ms
step:244/1775 train_time:8143ms step_avg:33.37ms
step:245/1775 train_time:8176ms step_avg:33.37ms
step:246/1775 train_time:8210ms step_avg:33.37ms
step:247/1775 train_time:8242ms step_avg:33.37ms
step:248/1775 train_time:8276ms step_avg:33.37ms
step:249/1775 train_time:8307ms step_avg:33.36ms
step:250/1775 train_time:8342ms step_avg:33.37ms
step:250/1775 val_loss:4.6006 train_time:8383ms step_avg:33.53ms
step:251/1775 train_time:8401ms step_avg:33.47ms
step:252/1775 train_time:8420ms step_avg:33.41ms
step:253/1775 train_time:8443ms step_avg:33.37ms
step:254/1775 train_time:8477ms step_avg:33.38ms
step:255/1775 train_time:8512ms step_avg:33.38ms
step:256/1775 train_time:8547ms step_avg:33.39ms
step:257/1775 train_time:8580ms step_avg:33.39ms
step:258/1775 train_time:8615ms step_avg:33.39ms
step:259/1775 train_time:8647ms step_avg:33.39ms
step:260/1775 train_time:8681ms step_avg:33.39ms
step:261/1775 train_time:8713ms step_avg:33.39ms
step:262/1775 train_time:8748ms step_avg:33.39ms
step:263/1775 train_time:8780ms step_avg:33.38ms
step:264/1775 train_time:8814ms step_avg:33.39ms
step:265/1775 train_time:8846ms step_avg:33.38ms
step:266/1775 train_time:8880ms step_avg:33.38ms
step:267/1775 train_time:8912ms step_avg:33.38ms
step:268/1775 train_time:8946ms step_avg:33.38ms
step:269/1775 train_time:8978ms step_avg:33.38ms
step:270/1775 train_time:9012ms step_avg:33.38ms
step:271/1775 train_time:9044ms step_avg:33.37ms
step:272/1775 train_time:9078ms step_avg:33.37ms
step:273/1775 train_time:9110ms step_avg:33.37ms
step:274/1775 train_time:9144ms step_avg:33.37ms
step:275/1775 train_time:9176ms step_avg:33.37ms
step:276/1775 train_time:9210ms step_avg:33.37ms
step:277/1775 train_time:9241ms step_avg:33.36ms
step:278/1775 train_time:9275ms step_avg:33.36ms
step:279/1775 train_time:9308ms step_avg:33.36ms
step:280/1775 train_time:9342ms step_avg:33.36ms
step:281/1775 train_time:9374ms step_avg:33.36ms
step:282/1775 train_time:9408ms step_avg:33.36ms
step:283/1775 train_time:9441ms step_avg:33.36ms
step:284/1775 train_time:9475ms step_avg:33.36ms
step:285/1775 train_time:9507ms step_avg:33.36ms
step:286/1775 train_time:9542ms step_avg:33.36ms
step:287/1775 train_time:9574ms step_avg:33.36ms
step:288/1775 train_time:9608ms step_avg:33.36ms
step:289/1775 train_time:9640ms step_avg:33.36ms
step:290/1775 train_time:9675ms step_avg:33.36ms
step:291/1775 train_time:9707ms step_avg:33.36ms
step:292/1775 train_time:9741ms step_avg:33.36ms
step:293/1775 train_time:9773ms step_avg:33.36ms
step:294/1775 train_time:9808ms step_avg:33.36ms
step:295/1775 train_time:9840ms step_avg:33.35ms
step:296/1775 train_time:9874ms step_avg:33.36ms
step:297/1775 train_time:9906ms step_avg:33.35ms
step:298/1775 train_time:9940ms step_avg:33.35ms
step:299/1775 train_time:9971ms step_avg:33.35ms
step:300/1775 train_time:10005ms step_avg:33.35ms
step:301/1775 train_time:10037ms step_avg:33.35ms
step:302/1775 train_time:10071ms step_avg:33.35ms
step:303/1775 train_time:10103ms step_avg:33.34ms
step:304/1775 train_time:10137ms step_avg:33.35ms
step:305/1775 train_time:10169ms step_avg:33.34ms
step:306/1775 train_time:10203ms step_avg:33.34ms
step:307/1775 train_time:10235ms step_avg:33.34ms
step:308/1775 train_time:10270ms step_avg:33.34ms
step:309/1775 train_time:10301ms step_avg:33.34ms
step:310/1775 train_time:10335ms step_avg:33.34ms
step:311/1775 train_time:10368ms step_avg:33.34ms
step:312/1775 train_time:10402ms step_avg:33.34ms
step:313/1775 train_time:10434ms step_avg:33.34ms
step:314/1775 train_time:10468ms step_avg:33.34ms
step:315/1775 train_time:10500ms step_avg:33.33ms
step:316/1775 train_time:10534ms step_avg:33.34ms
step:317/1775 train_time:10566ms step_avg:33.33ms
step:318/1775 train_time:10601ms step_avg:33.34ms
step:319/1775 train_time:10633ms step_avg:33.33ms
step:320/1775 train_time:10667ms step_avg:33.34ms
step:321/1775 train_time:10699ms step_avg:33.33ms
step:322/1775 train_time:10733ms step_avg:33.33ms
step:323/1775 train_time:10765ms step_avg:33.33ms
step:324/1775 train_time:10799ms step_avg:33.33ms
step:325/1775 train_time:10831ms step_avg:33.33ms
step:326/1775 train_time:10865ms step_avg:33.33ms
step:327/1775 train_time:10897ms step_avg:33.33ms
step:328/1775 train_time:10931ms step_avg:33.33ms
step:329/1775 train_time:10963ms step_avg:33.32ms
step:330/1775 train_time:10997ms step_avg:33.33ms
step:331/1775 train_time:11029ms step_avg:33.32ms
step:332/1775 train_time:11063ms step_avg:33.32ms
step:333/1775 train_time:11095ms step_avg:33.32ms
step:334/1775 train_time:11130ms step_avg:33.32ms
step:335/1775 train_time:11161ms step_avg:33.32ms
step:336/1775 train_time:11195ms step_avg:33.32ms
step:337/1775 train_time:11227ms step_avg:33.32ms
step:338/1775 train_time:11261ms step_avg:33.32ms
step:339/1775 train_time:11293ms step_avg:33.31ms
step:340/1775 train_time:11327ms step_avg:33.32ms
step:341/1775 train_time:11359ms step_avg:33.31ms
step:342/1775 train_time:11394ms step_avg:33.32ms
step:343/1775 train_time:11426ms step_avg:33.31ms
step:344/1775 train_time:11460ms step_avg:33.31ms
step:345/1775 train_time:11492ms step_avg:33.31ms
step:346/1775 train_time:11526ms step_avg:33.31ms
step:347/1775 train_time:11558ms step_avg:33.31ms
step:348/1775 train_time:11593ms step_avg:33.31ms
step:349/1775 train_time:11625ms step_avg:33.31ms
step:350/1775 train_time:11660ms step_avg:33.31ms
step:351/1775 train_time:11692ms step_avg:33.31ms
step:352/1775 train_time:11726ms step_avg:33.31ms
step:353/1775 train_time:11759ms step_avg:33.31ms
step:354/1775 train_time:11793ms step_avg:33.31ms
step:355/1775 train_time:11825ms step_avg:33.31ms
step:356/1775 train_time:11859ms step_avg:33.31ms
step:357/1775 train_time:11891ms step_avg:33.31ms
step:358/1775 train_time:11925ms step_avg:33.31ms
step:359/1775 train_time:11957ms step_avg:33.31ms
step:360/1775 train_time:11992ms step_avg:33.31ms
step:361/1775 train_time:12024ms step_avg:33.31ms
step:362/1775 train_time:12058ms step_avg:33.31ms
step:363/1775 train_time:12090ms step_avg:33.31ms
step:364/1775 train_time:12124ms step_avg:33.31ms
step:365/1775 train_time:12156ms step_avg:33.30ms
step:366/1775 train_time:12190ms step_avg:33.31ms
step:367/1775 train_time:12222ms step_avg:33.30ms
step:368/1775 train_time:12256ms step_avg:33.30ms
step:369/1775 train_time:12288ms step_avg:33.30ms
step:370/1775 train_time:12323ms step_avg:33.30ms
step:371/1775 train_time:12355ms step_avg:33.30ms
step:372/1775 train_time:12389ms step_avg:33.30ms
step:373/1775 train_time:12421ms step_avg:33.30ms
step:374/1775 train_time:12455ms step_avg:33.30ms
step:375/1775 train_time:12487ms step_avg:33.30ms
step:376/1775 train_time:12521ms step_avg:33.30ms
step:377/1775 train_time:12553ms step_avg:33.30ms
step:378/1775 train_time:12587ms step_avg:33.30ms
step:379/1775 train_time:12620ms step_avg:33.30ms
step:380/1775 train_time:12654ms step_avg:33.30ms
step:381/1775 train_time:12686ms step_avg:33.30ms
step:382/1775 train_time:12720ms step_avg:33.30ms
step:383/1775 train_time:12752ms step_avg:33.30ms
step:384/1775 train_time:12786ms step_avg:33.30ms
step:385/1775 train_time:12819ms step_avg:33.30ms
step:386/1775 train_time:12853ms step_avg:33.30ms
step:387/1775 train_time:12885ms step_avg:33.29ms
step:388/1775 train_time:12919ms step_avg:33.30ms
step:389/1775 train_time:12951ms step_avg:33.29ms
step:390/1775 train_time:12985ms step_avg:33.30ms
step:391/1775 train_time:13017ms step_avg:33.29ms
step:392/1775 train_time:13052ms step_avg:33.30ms
step:393/1775 train_time:13084ms step_avg:33.29ms
step:394/1775 train_time:13118ms step_avg:33.30ms
step:395/1775 train_time:13150ms step_avg:33.29ms
step:396/1775 train_time:13184ms step_avg:33.29ms
step:397/1775 train_time:13216ms step_avg:33.29ms
step:398/1775 train_time:13250ms step_avg:33.29ms
step:399/1775 train_time:13282ms step_avg:33.29ms
step:400/1775 train_time:13316ms step_avg:33.29ms
step:401/1775 train_time:13348ms step_avg:33.29ms
step:402/1775 train_time:13382ms step_avg:33.29ms
step:403/1775 train_time:13414ms step_avg:33.29ms
step:404/1775 train_time:13448ms step_avg:33.29ms
step:405/1775 train_time:13480ms step_avg:33.28ms
step:406/1775 train_time:13515ms step_avg:33.29ms
step:407/1775 train_time:13547ms step_avg:33.29ms
step:408/1775 train_time:13581ms step_avg:33.29ms
step:409/1775 train_time:13613ms step_avg:33.28ms
step:410/1775 train_time:13648ms step_avg:33.29ms
step:411/1775 train_time:13680ms step_avg:33.28ms
step:412/1775 train_time:13714ms step_avg:33.29ms
step:413/1775 train_time:13746ms step_avg:33.28ms
step:414/1775 train_time:13781ms step_avg:33.29ms
step:415/1775 train_time:13813ms step_avg:33.28ms
step:416/1775 train_time:13847ms step_avg:33.29ms
step:417/1775 train_time:13879ms step_avg:33.28ms
step:418/1775 train_time:13914ms step_avg:33.29ms
step:419/1775 train_time:13946ms step_avg:33.28ms
step:420/1775 train_time:13981ms step_avg:33.29ms
step:421/1775 train_time:14013ms step_avg:33.28ms
step:422/1775 train_time:14047ms step_avg:33.29ms
step:423/1775 train_time:14079ms step_avg:33.28ms
step:424/1775 train_time:14114ms step_avg:33.29ms
step:425/1775 train_time:14146ms step_avg:33.28ms
step:426/1775 train_time:14180ms step_avg:33.29ms
step:427/1775 train_time:14212ms step_avg:33.28ms
step:428/1775 train_time:14246ms step_avg:33.28ms
step:429/1775 train_time:14278ms step_avg:33.28ms
step:430/1775 train_time:14313ms step_avg:33.29ms
step:431/1775 train_time:14345ms step_avg:33.28ms
step:432/1775 train_time:14379ms step_avg:33.28ms
step:433/1775 train_time:14411ms step_avg:33.28ms
step:434/1775 train_time:14444ms step_avg:33.28ms
step:435/1775 train_time:14476ms step_avg:33.28ms
step:436/1775 train_time:14511ms step_avg:33.28ms
step:437/1775 train_time:14543ms step_avg:33.28ms
step:438/1775 train_time:14577ms step_avg:33.28ms
step:439/1775 train_time:14609ms step_avg:33.28ms
step:440/1775 train_time:14643ms step_avg:33.28ms
step:441/1775 train_time:14676ms step_avg:33.28ms
step:442/1775 train_time:14710ms step_avg:33.28ms
step:443/1775 train_time:14742ms step_avg:33.28ms
step:444/1775 train_time:14776ms step_avg:33.28ms
step:445/1775 train_time:14808ms step_avg:33.28ms
step:446/1775 train_time:14842ms step_avg:33.28ms
step:447/1775 train_time:14875ms step_avg:33.28ms
step:448/1775 train_time:14909ms step_avg:33.28ms
step:449/1775 train_time:14941ms step_avg:33.28ms
step:450/1775 train_time:14975ms step_avg:33.28ms
step:451/1775 train_time:15007ms step_avg:33.27ms
step:452/1775 train_time:15041ms step_avg:33.28ms
step:453/1775 train_time:15073ms step_avg:33.27ms
step:454/1775 train_time:15107ms step_avg:33.28ms
step:455/1775 train_time:15139ms step_avg:33.27ms
step:456/1775 train_time:15174ms step_avg:33.28ms
step:457/1775 train_time:15206ms step_avg:33.27ms
step:458/1775 train_time:15240ms step_avg:33.27ms
step:459/1775 train_time:15272ms step_avg:33.27ms
step:460/1775 train_time:15306ms step_avg:33.27ms
step:461/1775 train_time:15338ms step_avg:33.27ms
step:462/1775 train_time:15372ms step_avg:33.27ms
step:463/1775 train_time:15404ms step_avg:33.27ms
step:464/1775 train_time:15438ms step_avg:33.27ms
step:465/1775 train_time:15470ms step_avg:33.27ms
step:466/1775 train_time:15504ms step_avg:33.27ms
step:467/1775 train_time:15537ms step_avg:33.27ms
step:468/1775 train_time:15571ms step_avg:33.27ms
step:469/1775 train_time:15603ms step_avg:33.27ms
step:470/1775 train_time:15637ms step_avg:33.27ms
step:471/1775 train_time:15669ms step_avg:33.27ms
step:472/1775 train_time:15703ms step_avg:33.27ms
step:473/1775 train_time:15735ms step_avg:33.27ms
step:474/1775 train_time:15769ms step_avg:33.27ms
step:475/1775 train_time:15801ms step_avg:33.27ms
step:476/1775 train_time:15836ms step_avg:33.27ms
step:477/1775 train_time:15868ms step_avg:33.27ms
step:478/1775 train_time:15902ms step_avg:33.27ms
step:479/1775 train_time:15934ms step_avg:33.27ms
step:480/1775 train_time:15969ms step_avg:33.27ms
step:481/1775 train_time:16001ms step_avg:33.27ms
step:482/1775 train_time:16035ms step_avg:33.27ms
step:483/1775 train_time:16067ms step_avg:33.27ms
step:484/1775 train_time:16101ms step_avg:33.27ms
step:485/1775 train_time:16133ms step_avg:33.26ms
step:486/1775 train_time:16168ms step_avg:33.27ms
step:487/1775 train_time:16200ms step_avg:33.26ms
step:488/1775 train_time:16234ms step_avg:33.27ms
step:489/1775 train_time:16266ms step_avg:33.26ms
step:490/1775 train_time:16300ms step_avg:33.27ms
step:491/1775 train_time:16332ms step_avg:33.26ms
step:492/1775 train_time:16367ms step_avg:33.27ms
step:493/1775 train_time:16399ms step_avg:33.26ms
step:494/1775 train_time:16433ms step_avg:33.27ms
step:495/1775 train_time:16465ms step_avg:33.26ms
step:496/1775 train_time:16499ms step_avg:33.26ms
step:497/1775 train_time:16532ms step_avg:33.26ms
step:498/1775 train_time:16566ms step_avg:33.26ms
step:499/1775 train_time:16598ms step_avg:33.26ms
step:500/1775 train_time:16632ms step_avg:33.26ms
step:500/1775 val_loss:4.3277 train_time:16673ms step_avg:33.35ms
step:501/1775 train_time:16691ms step_avg:33.32ms
step:502/1775 train_time:16710ms step_avg:33.29ms
step:503/1775 train_time:16733ms step_avg:33.27ms
step:504/1775 train_time:16767ms step_avg:33.27ms
step:505/1775 train_time:16800ms step_avg:33.27ms
step:506/1775 train_time:16835ms step_avg:33.27ms
step:507/1775 train_time:16867ms step_avg:33.27ms
step:508/1775 train_time:16901ms step_avg:33.27ms
step:509/1775 train_time:16933ms step_avg:33.27ms
step:510/1775 train_time:16967ms step_avg:33.27ms
step:511/1775 train_time:16999ms step_avg:33.27ms
step:512/1775 train_time:17033ms step_avg:33.27ms
step:513/1775 train_time:17065ms step_avg:33.27ms
step:514/1775 train_time:17099ms step_avg:33.27ms
step:515/1775 train_time:17131ms step_avg:33.26ms
step:516/1775 train_time:17165ms step_avg:33.27ms
step:517/1775 train_time:17197ms step_avg:33.26ms
step:518/1775 train_time:17231ms step_avg:33.26ms
step:519/1775 train_time:17263ms step_avg:33.26ms
step:520/1775 train_time:17297ms step_avg:33.26ms
step:521/1775 train_time:17329ms step_avg:33.26ms
step:522/1775 train_time:17363ms step_avg:33.26ms
step:523/1775 train_time:17395ms step_avg:33.26ms
step:524/1775 train_time:17429ms step_avg:33.26ms
step:525/1775 train_time:17461ms step_avg:33.26ms
step:526/1775 train_time:17495ms step_avg:33.26ms
step:527/1775 train_time:17527ms step_avg:33.26ms
step:528/1775 train_time:17561ms step_avg:33.26ms
step:529/1775 train_time:17592ms step_avg:33.26ms
step:530/1775 train_time:17627ms step_avg:33.26ms
step:531/1775 train_time:17660ms step_avg:33.26ms
step:532/1775 train_time:17694ms step_avg:33.26ms
step:533/1775 train_time:17727ms step_avg:33.26ms
step:534/1775 train_time:17761ms step_avg:33.26ms
step:535/1775 train_time:17794ms step_avg:33.26ms
step:536/1775 train_time:17829ms step_avg:33.26ms
step:537/1775 train_time:17860ms step_avg:33.26ms
step:538/1775 train_time:17895ms step_avg:33.26ms
step:539/1775 train_time:17927ms step_avg:33.26ms
step:540/1775 train_time:17961ms step_avg:33.26ms
step:541/1775 train_time:17994ms step_avg:33.26ms
step:542/1775 train_time:18027ms step_avg:33.26ms
step:543/1775 train_time:18059ms step_avg:33.26ms
step:544/1775 train_time:18093ms step_avg:33.26ms
step:545/1775 train_time:18125ms step_avg:33.26ms
step:546/1775 train_time:18159ms step_avg:33.26ms
step:547/1775 train_time:18192ms step_avg:33.26ms
step:548/1775 train_time:18226ms step_avg:33.26ms
step:549/1775 train_time:18257ms step_avg:33.26ms
step:550/1775 train_time:18291ms step_avg:33.26ms
step:551/1775 train_time:18323ms step_avg:33.25ms
step:552/1775 train_time:18357ms step_avg:33.26ms
step:553/1775 train_time:18389ms step_avg:33.25ms
step:554/1775 train_time:18423ms step_avg:33.26ms
step:555/1775 train_time:18455ms step_avg:33.25ms
step:556/1775 train_time:18489ms step_avg:33.25ms
step:557/1775 train_time:18521ms step_avg:33.25ms
step:558/1775 train_time:18556ms step_avg:33.25ms
step:559/1775 train_time:18588ms step_avg:33.25ms
step:560/1775 train_time:18622ms step_avg:33.25ms
step:561/1775 train_time:18654ms step_avg:33.25ms
step:562/1775 train_time:18688ms step_avg:33.25ms
step:563/1775 train_time:18720ms step_avg:33.25ms
step:564/1775 train_time:18755ms step_avg:33.25ms
step:565/1775 train_time:18788ms step_avg:33.25ms
step:566/1775 train_time:18823ms step_avg:33.26ms
step:567/1775 train_time:18855ms step_avg:33.25ms
step:568/1775 train_time:18889ms step_avg:33.26ms
step:569/1775 train_time:18922ms step_avg:33.25ms
step:570/1775 train_time:18956ms step_avg:33.26ms
step:571/1775 train_time:18989ms step_avg:33.26ms
step:572/1775 train_time:19023ms step_avg:33.26ms
step:573/1775 train_time:19055ms step_avg:33.25ms
step:574/1775 train_time:19089ms step_avg:33.26ms
step:575/1775 train_time:19121ms step_avg:33.25ms
step:576/1775 train_time:19155ms step_avg:33.26ms
step:577/1775 train_time:19187ms step_avg:33.25ms
step:578/1775 train_time:19221ms step_avg:33.25ms
step:579/1775 train_time:19253ms step_avg:33.25ms
step:580/1775 train_time:19290ms step_avg:33.26ms
step:581/1775 train_time:19350ms step_avg:33.30ms
step:582/1775 train_time:19411ms step_avg:33.35ms
step:583/1775 train_time:19471ms step_avg:33.40ms
step:584/1775 train_time:19533ms step_avg:33.45ms
step:585/1775 train_time:19593ms step_avg:33.49ms
step:586/1775 train_time:19655ms step_avg:33.54ms
step:587/1775 train_time:19714ms step_avg:33.58ms
step:588/1775 train_time:19776ms step_avg:33.63ms
step:589/1775 train_time:19836ms step_avg:33.68ms
step:590/1775 train_time:19899ms step_avg:33.73ms
step:591/1775 train_time:19959ms step_avg:33.77ms
step:592/1775 train_time:20022ms step_avg:33.82ms
step:593/1775 train_time:20083ms step_avg:33.87ms
step:594/1775 train_time:20146ms step_avg:33.92ms
step:595/1775 train_time:20205ms step_avg:33.96ms
step:596/1775 train_time:20267ms step_avg:34.00ms
step:597/1775 train_time:20326ms step_avg:34.05ms
step:598/1775 train_time:20387ms step_avg:34.09ms
step:599/1775 train_time:20447ms step_avg:34.14ms
step:600/1775 train_time:20509ms step_avg:34.18ms
step:601/1775 train_time:20569ms step_avg:34.22ms
step:602/1775 train_time:20630ms step_avg:34.27ms
step:603/1775 train_time:20690ms step_avg:34.31ms
step:604/1775 train_time:20752ms step_avg:34.36ms
step:605/1775 train_time:20811ms step_avg:34.40ms
step:606/1775 train_time:20874ms step_avg:34.45ms
step:607/1775 train_time:20934ms step_avg:34.49ms
step:608/1775 train_time:20997ms step_avg:34.53ms
step:609/1775 train_time:21057ms step_avg:34.58ms
step:610/1775 train_time:21119ms step_avg:34.62ms
step:611/1775 train_time:21179ms step_avg:34.66ms
step:612/1775 train_time:21242ms step_avg:34.71ms
step:613/1775 train_time:21302ms step_avg:34.75ms
step:614/1775 train_time:21364ms step_avg:34.80ms
step:615/1775 train_time:21424ms step_avg:34.84ms
step:616/1775 train_time:21487ms step_avg:34.88ms
step:617/1775 train_time:21546ms step_avg:34.92ms
step:618/1775 train_time:21608ms step_avg:34.96ms
step:619/1775 train_time:21668ms step_avg:35.00ms
step:620/1775 train_time:21730ms step_avg:35.05ms
step:621/1775 train_time:21789ms step_avg:35.09ms
step:622/1775 train_time:21851ms step_avg:35.13ms
step:623/1775 train_time:21911ms step_avg:35.17ms
step:624/1775 train_time:21973ms step_avg:35.21ms
step:625/1775 train_time:22033ms step_avg:35.25ms
step:626/1775 train_time:22095ms step_avg:35.30ms
step:627/1775 train_time:22155ms step_avg:35.33ms
step:628/1775 train_time:22217ms step_avg:35.38ms
step:629/1775 train_time:22278ms step_avg:35.42ms
step:630/1775 train_time:22340ms step_avg:35.46ms
step:631/1775 train_time:22400ms step_avg:35.50ms
step:632/1775 train_time:22462ms step_avg:35.54ms
step:633/1775 train_time:22522ms step_avg:35.58ms
step:634/1775 train_time:22585ms step_avg:35.62ms
step:635/1775 train_time:22645ms step_avg:35.66ms
step:636/1775 train_time:22707ms step_avg:35.70ms
step:637/1775 train_time:22767ms step_avg:35.74ms
step:638/1775 train_time:22828ms step_avg:35.78ms
step:639/1775 train_time:22888ms step_avg:35.82ms
step:640/1775 train_time:22950ms step_avg:35.86ms
step:641/1775 train_time:23009ms step_avg:35.90ms
step:642/1775 train_time:23071ms step_avg:35.94ms
step:643/1775 train_time:23131ms step_avg:35.97ms
step:644/1775 train_time:23194ms step_avg:36.01ms
step:645/1775 train_time:23253ms step_avg:36.05ms
step:646/1775 train_time:23315ms step_avg:36.09ms
step:647/1775 train_time:23375ms step_avg:36.13ms
step:648/1775 train_time:23437ms step_avg:36.17ms
step:649/1775 train_time:23498ms step_avg:36.21ms
step:650/1775 train_time:23561ms step_avg:36.25ms
step:651/1775 train_time:23621ms step_avg:36.28ms
step:652/1775 train_time:23684ms step_avg:36.33ms
step:653/1775 train_time:23744ms step_avg:36.36ms
step:654/1775 train_time:23806ms step_avg:36.40ms
step:655/1775 train_time:23866ms step_avg:36.44ms
step:656/1775 train_time:23928ms step_avg:36.48ms
step:657/1775 train_time:23988ms step_avg:36.51ms
step:658/1775 train_time:24050ms step_avg:36.55ms
step:659/1775 train_time:24109ms step_avg:36.58ms
step:660/1775 train_time:24171ms step_avg:36.62ms
step:661/1775 train_time:24231ms step_avg:36.66ms
step:662/1775 train_time:24293ms step_avg:36.70ms
step:663/1775 train_time:24353ms step_avg:36.73ms
step:664/1775 train_time:24415ms step_avg:36.77ms
step:665/1775 train_time:24475ms step_avg:36.80ms
step:666/1775 train_time:24537ms step_avg:36.84ms
step:667/1775 train_time:24598ms step_avg:36.88ms
step:668/1775 train_time:24661ms step_avg:36.92ms
step:669/1775 train_time:24722ms step_avg:36.95ms
step:670/1775 train_time:24784ms step_avg:36.99ms
step:671/1775 train_time:24844ms step_avg:37.03ms
step:672/1775 train_time:24906ms step_avg:37.06ms
step:673/1775 train_time:24967ms step_avg:37.10ms
step:674/1775 train_time:25028ms step_avg:37.13ms
step:675/1775 train_time:25088ms step_avg:37.17ms
step:676/1775 train_time:25150ms step_avg:37.20ms
step:677/1775 train_time:25209ms step_avg:37.24ms
step:678/1775 train_time:25271ms step_avg:37.27ms
step:679/1775 train_time:25330ms step_avg:37.31ms
step:680/1775 train_time:25392ms step_avg:37.34ms
step:681/1775 train_time:25452ms step_avg:37.37ms
step:682/1775 train_time:25515ms step_avg:37.41ms
step:683/1775 train_time:25575ms step_avg:37.44ms
step:684/1775 train_time:25637ms step_avg:37.48ms
step:685/1775 train_time:25698ms step_avg:37.52ms
step:686/1775 train_time:25761ms step_avg:37.55ms
step:687/1775 train_time:25821ms step_avg:37.59ms
step:688/1775 train_time:25884ms step_avg:37.62ms
step:689/1775 train_time:25944ms step_avg:37.66ms
step:690/1775 train_time:26006ms step_avg:37.69ms
step:691/1775 train_time:26066ms step_avg:37.72ms
step:692/1775 train_time:26128ms step_avg:37.76ms
step:693/1775 train_time:26188ms step_avg:37.79ms
step:694/1775 train_time:26251ms step_avg:37.83ms
step:695/1775 train_time:26310ms step_avg:37.86ms
step:696/1775 train_time:26371ms step_avg:37.89ms
step:697/1775 train_time:26430ms step_avg:37.92ms
step:698/1775 train_time:26492ms step_avg:37.95ms
step:699/1775 train_time:26552ms step_avg:37.99ms
step:700/1775 train_time:26615ms step_avg:38.02ms
step:701/1775 train_time:26675ms step_avg:38.05ms
step:702/1775 train_time:26737ms step_avg:38.09ms
step:703/1775 train_time:26798ms step_avg:38.12ms
step:704/1775 train_time:26860ms step_avg:38.15ms
step:705/1775 train_time:26920ms step_avg:38.18ms
step:706/1775 train_time:26984ms step_avg:38.22ms
step:707/1775 train_time:27044ms step_avg:38.25ms
step:708/1775 train_time:27106ms step_avg:38.29ms
step:709/1775 train_time:27166ms step_avg:38.32ms
step:710/1775 train_time:27228ms step_avg:38.35ms
step:711/1775 train_time:27289ms step_avg:38.38ms
step:712/1775 train_time:27351ms step_avg:38.41ms
step:713/1775 train_time:27410ms step_avg:38.44ms
step:714/1775 train_time:27472ms step_avg:38.48ms
step:715/1775 train_time:27531ms step_avg:38.51ms
step:716/1775 train_time:27594ms step_avg:38.54ms
step:717/1775 train_time:27653ms step_avg:38.57ms
step:718/1775 train_time:27715ms step_avg:38.60ms
step:719/1775 train_time:27775ms step_avg:38.63ms
step:720/1775 train_time:27838ms step_avg:38.66ms
step:721/1775 train_time:27899ms step_avg:38.69ms
step:722/1775 train_time:27961ms step_avg:38.73ms
step:723/1775 train_time:28022ms step_avg:38.76ms
step:724/1775 train_time:28085ms step_avg:38.79ms
step:725/1775 train_time:28146ms step_avg:38.82ms
step:726/1775 train_time:28207ms step_avg:38.85ms
step:727/1775 train_time:28267ms step_avg:38.88ms
step:728/1775 train_time:28328ms step_avg:38.91ms
step:729/1775 train_time:28388ms step_avg:38.94ms
step:730/1775 train_time:28451ms step_avg:38.97ms
step:731/1775 train_time:28510ms step_avg:39.00ms
step:732/1775 train_time:28573ms step_avg:39.03ms
step:733/1775 train_time:28631ms step_avg:39.06ms
step:734/1775 train_time:28694ms step_avg:39.09ms
step:735/1775 train_time:28753ms step_avg:39.12ms
step:736/1775 train_time:28816ms step_avg:39.15ms
step:737/1775 train_time:28876ms step_avg:39.18ms
step:738/1775 train_time:28940ms step_avg:39.21ms
step:739/1775 train_time:29000ms step_avg:39.24ms
step:740/1775 train_time:29062ms step_avg:39.27ms
step:741/1775 train_time:29123ms step_avg:39.30ms
step:742/1775 train_time:29186ms step_avg:39.33ms
step:743/1775 train_time:29245ms step_avg:39.36ms
step:744/1775 train_time:29307ms step_avg:39.39ms
step:745/1775 train_time:29367ms step_avg:39.42ms
step:746/1775 train_time:29429ms step_avg:39.45ms
step:747/1775 train_time:29489ms step_avg:39.48ms
step:748/1775 train_time:29551ms step_avg:39.51ms
step:749/1775 train_time:29610ms step_avg:39.53ms
step:750/1775 train_time:29672ms step_avg:39.56ms
step:750/1775 val_loss:3.9917 train_time:29743ms step_avg:39.66ms
step:751/1775 train_time:29762ms step_avg:39.63ms
step:752/1775 train_time:29798ms step_avg:39.62ms
step:753/1775 train_time:29858ms step_avg:39.65ms
step:754/1775 train_time:29920ms step_avg:39.68ms
step:755/1775 train_time:29981ms step_avg:39.71ms
step:756/1775 train_time:30043ms step_avg:39.74ms
step:757/1775 train_time:30102ms step_avg:39.77ms
step:758/1775 train_time:30164ms step_avg:39.79ms
step:759/1775 train_time:30223ms step_avg:39.82ms
step:760/1775 train_time:30284ms step_avg:39.85ms
step:761/1775 train_time:30343ms step_avg:39.87ms
step:762/1775 train_time:30405ms step_avg:39.90ms
step:763/1775 train_time:30464ms step_avg:39.93ms
step:764/1775 train_time:30526ms step_avg:39.96ms
step:765/1775 train_time:30586ms step_avg:39.98ms
step:766/1775 train_time:30647ms step_avg:40.01ms
step:767/1775 train_time:30707ms step_avg:40.04ms
step:768/1775 train_time:30771ms step_avg:40.07ms
step:769/1775 train_time:30832ms step_avg:40.09ms
step:770/1775 train_time:30895ms step_avg:40.12ms
step:771/1775 train_time:30955ms step_avg:40.15ms
step:772/1775 train_time:31017ms step_avg:40.18ms
step:773/1775 train_time:31078ms step_avg:40.20ms
step:774/1775 train_time:31140ms step_avg:40.23ms
step:775/1775 train_time:31200ms step_avg:40.26ms
step:776/1775 train_time:31261ms step_avg:40.28ms
step:777/1775 train_time:31321ms step_avg:40.31ms
step:778/1775 train_time:31382ms step_avg:40.34ms
step:779/1775 train_time:31441ms step_avg:40.36ms
step:780/1775 train_time:31503ms step_avg:40.39ms
step:781/1775 train_time:31562ms step_avg:40.41ms
step:782/1775 train_time:31624ms step_avg:40.44ms
step:783/1775 train_time:31684ms step_avg:40.46ms
step:784/1775 train_time:31746ms step_avg:40.49ms
step:785/1775 train_time:31806ms step_avg:40.52ms
step:786/1775 train_time:31870ms step_avg:40.55ms
step:787/1775 train_time:31930ms step_avg:40.57ms
step:788/1775 train_time:31992ms step_avg:40.60ms
step:789/1775 train_time:32053ms step_avg:40.63ms
step:790/1775 train_time:32116ms step_avg:40.65ms
step:791/1775 train_time:32176ms step_avg:40.68ms
step:792/1775 train_time:32238ms step_avg:40.70ms
step:793/1775 train_time:32297ms step_avg:40.73ms
step:794/1775 train_time:32359ms step_avg:40.76ms
step:795/1775 train_time:32419ms step_avg:40.78ms
step:796/1775 train_time:32481ms step_avg:40.80ms
step:797/1775 train_time:32539ms step_avg:40.83ms
step:798/1775 train_time:32601ms step_avg:40.85ms
step:799/1775 train_time:32661ms step_avg:40.88ms
step:800/1775 train_time:32724ms step_avg:40.90ms
step:801/1775 train_time:32784ms step_avg:40.93ms
step:802/1775 train_time:32847ms step_avg:40.96ms
step:803/1775 train_time:32908ms step_avg:40.98ms
step:804/1775 train_time:32970ms step_avg:41.01ms
step:805/1775 train_time:33030ms step_avg:41.03ms
step:806/1775 train_time:33093ms step_avg:41.06ms
step:807/1775 train_time:33153ms step_avg:41.08ms
step:808/1775 train_time:33215ms step_avg:41.11ms
step:809/1775 train_time:33274ms step_avg:41.13ms
step:810/1775 train_time:33336ms step_avg:41.16ms
step:811/1775 train_time:33396ms step_avg:41.18ms
step:812/1775 train_time:33459ms step_avg:41.21ms
step:813/1775 train_time:33518ms step_avg:41.23ms
step:814/1775 train_time:33580ms step_avg:41.25ms
step:815/1775 train_time:33640ms step_avg:41.28ms
step:816/1775 train_time:33702ms step_avg:41.30ms
step:817/1775 train_time:33764ms step_avg:41.33ms
step:818/1775 train_time:33825ms step_avg:41.35ms
step:819/1775 train_time:33885ms step_avg:41.37ms
step:820/1775 train_time:33946ms step_avg:41.40ms
step:821/1775 train_time:34007ms step_avg:41.42ms
step:822/1775 train_time:34070ms step_avg:41.45ms
step:823/1775 train_time:34129ms step_avg:41.47ms
step:824/1775 train_time:34192ms step_avg:41.49ms
step:825/1775 train_time:34251ms step_avg:41.52ms
step:826/1775 train_time:34314ms step_avg:41.54ms
step:827/1775 train_time:34374ms step_avg:41.56ms
step:828/1775 train_time:34436ms step_avg:41.59ms
step:829/1775 train_time:34496ms step_avg:41.61ms
step:830/1775 train_time:34559ms step_avg:41.64ms
step:831/1775 train_time:34619ms step_avg:41.66ms
step:832/1775 train_time:34682ms step_avg:41.68ms
step:833/1775 train_time:34741ms step_avg:41.71ms
step:834/1775 train_time:34803ms step_avg:41.73ms
step:835/1775 train_time:34863ms step_avg:41.75ms
step:836/1775 train_time:34925ms step_avg:41.78ms
step:837/1775 train_time:34985ms step_avg:41.80ms
step:838/1775 train_time:35047ms step_avg:41.82ms
step:839/1775 train_time:35107ms step_avg:41.84ms
step:840/1775 train_time:35169ms step_avg:41.87ms
step:841/1775 train_time:35229ms step_avg:41.89ms
step:842/1775 train_time:35291ms step_avg:41.91ms
step:843/1775 train_time:35351ms step_avg:41.94ms
step:844/1775 train_time:35415ms step_avg:41.96ms
step:845/1775 train_time:35475ms step_avg:41.98ms
step:846/1775 train_time:35538ms step_avg:42.01ms
step:847/1775 train_time:35598ms step_avg:42.03ms
step:848/1775 train_time:35661ms step_avg:42.05ms
step:849/1775 train_time:35721ms step_avg:42.07ms
step:850/1775 train_time:35783ms step_avg:42.10ms
step:851/1775 train_time:35843ms step_avg:42.12ms
step:852/1775 train_time:35905ms step_avg:42.14ms
step:853/1775 train_time:35965ms step_avg:42.16ms
step:854/1775 train_time:36026ms step_avg:42.19ms
step:855/1775 train_time:36085ms step_avg:42.21ms
step:856/1775 train_time:36148ms step_avg:42.23ms
step:857/1775 train_time:36207ms step_avg:42.25ms
step:858/1775 train_time:36270ms step_avg:42.27ms
step:859/1775 train_time:36330ms step_avg:42.29ms
step:860/1775 train_time:36392ms step_avg:42.32ms
step:861/1775 train_time:36453ms step_avg:42.34ms
step:862/1775 train_time:36515ms step_avg:42.36ms
step:863/1775 train_time:36575ms step_avg:42.38ms
step:864/1775 train_time:36639ms step_avg:42.41ms
step:865/1775 train_time:36699ms step_avg:42.43ms
step:866/1775 train_time:36761ms step_avg:42.45ms
step:867/1775 train_time:36821ms step_avg:42.47ms
step:868/1775 train_time:36883ms step_avg:42.49ms
step:869/1775 train_time:36942ms step_avg:42.51ms
step:870/1775 train_time:37005ms step_avg:42.53ms
step:871/1775 train_time:37065ms step_avg:42.55ms
step:872/1775 train_time:37126ms step_avg:42.58ms
step:873/1775 train_time:37185ms step_avg:42.59ms
step:874/1775 train_time:37247ms step_avg:42.62ms
step:875/1775 train_time:37307ms step_avg:42.64ms
step:876/1775 train_time:37369ms step_avg:42.66ms
step:877/1775 train_time:37430ms step_avg:42.68ms
step:878/1775 train_time:37492ms step_avg:42.70ms
step:879/1775 train_time:37552ms step_avg:42.72ms
step:880/1775 train_time:37616ms step_avg:42.74ms
step:881/1775 train_time:37675ms step_avg:42.76ms
step:882/1775 train_time:37739ms step_avg:42.79ms
step:883/1775 train_time:37798ms step_avg:42.81ms
step:884/1775 train_time:37861ms step_avg:42.83ms
step:885/1775 train_time:37921ms step_avg:42.85ms
step:886/1775 train_time:37983ms step_avg:42.87ms
step:887/1775 train_time:38043ms step_avg:42.89ms
step:888/1775 train_time:38105ms step_avg:42.91ms
step:889/1775 train_time:38164ms step_avg:42.93ms
step:890/1775 train_time:38227ms step_avg:42.95ms
step:891/1775 train_time:38285ms step_avg:42.97ms
step:892/1775 train_time:38347ms step_avg:42.99ms
step:893/1775 train_time:38407ms step_avg:43.01ms
step:894/1775 train_time:38470ms step_avg:43.03ms
step:895/1775 train_time:38530ms step_avg:43.05ms
step:896/1775 train_time:38593ms step_avg:43.07ms
step:897/1775 train_time:38654ms step_avg:43.09ms
step:898/1775 train_time:38717ms step_avg:43.11ms
step:899/1775 train_time:38777ms step_avg:43.13ms
step:900/1775 train_time:38840ms step_avg:43.16ms
step:901/1775 train_time:38899ms step_avg:43.17ms
step:902/1775 train_time:38962ms step_avg:43.20ms
step:903/1775 train_time:39021ms step_avg:43.21ms
step:904/1775 train_time:39084ms step_avg:43.23ms
step:905/1775 train_time:39144ms step_avg:43.25ms
step:906/1775 train_time:39206ms step_avg:43.27ms
step:907/1775 train_time:39266ms step_avg:43.29ms
step:908/1775 train_time:39327ms step_avg:43.31ms
step:909/1775 train_time:39387ms step_avg:43.33ms
step:910/1775 train_time:39449ms step_avg:43.35ms
step:911/1775 train_time:39509ms step_avg:43.37ms
step:912/1775 train_time:39571ms step_avg:43.39ms
step:913/1775 train_time:39630ms step_avg:43.41ms
step:914/1775 train_time:39693ms step_avg:43.43ms
step:915/1775 train_time:39754ms step_avg:43.45ms
step:916/1775 train_time:39817ms step_avg:43.47ms
step:917/1775 train_time:39877ms step_avg:43.49ms
step:918/1775 train_time:39940ms step_avg:43.51ms
step:919/1775 train_time:40000ms step_avg:43.53ms
step:920/1775 train_time:40062ms step_avg:43.55ms
step:921/1775 train_time:40122ms step_avg:43.56ms
step:922/1775 train_time:40184ms step_avg:43.58ms
step:923/1775 train_time:40244ms step_avg:43.60ms
step:924/1775 train_time:40306ms step_avg:43.62ms
step:925/1775 train_time:40365ms step_avg:43.64ms
step:926/1775 train_time:40428ms step_avg:43.66ms
step:927/1775 train_time:40487ms step_avg:43.68ms
step:928/1775 train_time:40550ms step_avg:43.70ms
step:929/1775 train_time:40609ms step_avg:43.71ms
step:930/1775 train_time:40671ms step_avg:43.73ms
step:931/1775 train_time:40731ms step_avg:43.75ms
step:932/1775 train_time:40794ms step_avg:43.77ms
step:933/1775 train_time:40854ms step_avg:43.79ms
step:934/1775 train_time:40917ms step_avg:43.81ms
step:935/1775 train_time:40978ms step_avg:43.83ms
step:936/1775 train_time:41040ms step_avg:43.85ms
step:937/1775 train_time:41100ms step_avg:43.86ms
step:938/1775 train_time:41162ms step_avg:43.88ms
step:939/1775 train_time:41222ms step_avg:43.90ms
step:940/1775 train_time:41285ms step_avg:43.92ms
step:941/1775 train_time:41344ms step_avg:43.94ms
step:942/1775 train_time:41406ms step_avg:43.96ms
step:943/1775 train_time:41466ms step_avg:43.97ms
step:944/1775 train_time:41527ms step_avg:43.99ms
step:945/1775 train_time:41587ms step_avg:44.01ms
step:946/1775 train_time:41649ms step_avg:44.03ms
step:947/1775 train_time:41709ms step_avg:44.04ms
step:948/1775 train_time:41772ms step_avg:44.06ms
step:949/1775 train_time:41833ms step_avg:44.08ms
step:950/1775 train_time:41896ms step_avg:44.10ms
step:951/1775 train_time:41956ms step_avg:44.12ms
step:952/1775 train_time:42019ms step_avg:44.14ms
step:953/1775 train_time:42080ms step_avg:44.16ms
step:954/1775 train_time:42141ms step_avg:44.17ms
step:955/1775 train_time:42201ms step_avg:44.19ms
step:956/1775 train_time:42263ms step_avg:44.21ms
step:957/1775 train_time:42322ms step_avg:44.22ms
step:958/1775 train_time:42385ms step_avg:44.24ms
step:959/1775 train_time:42443ms step_avg:44.26ms
step:960/1775 train_time:42506ms step_avg:44.28ms
step:961/1775 train_time:42566ms step_avg:44.29ms
step:962/1775 train_time:42627ms step_avg:44.31ms
step:963/1775 train_time:42687ms step_avg:44.33ms
step:964/1775 train_time:42749ms step_avg:44.35ms
step:965/1775 train_time:42809ms step_avg:44.36ms
step:966/1775 train_time:42871ms step_avg:44.38ms
step:967/1775 train_time:42932ms step_avg:44.40ms
step:968/1775 train_time:42995ms step_avg:44.42ms
step:969/1775 train_time:43056ms step_avg:44.43ms
step:970/1775 train_time:43118ms step_avg:44.45ms
step:971/1775 train_time:43179ms step_avg:44.47ms
step:972/1775 train_time:43241ms step_avg:44.49ms
step:973/1775 train_time:43301ms step_avg:44.50ms
step:974/1775 train_time:43362ms step_avg:44.52ms
step:975/1775 train_time:43421ms step_avg:44.53ms
step:976/1775 train_time:43484ms step_avg:44.55ms
step:977/1775 train_time:43543ms step_avg:44.57ms
step:978/1775 train_time:43606ms step_avg:44.59ms
step:979/1775 train_time:43664ms step_avg:44.60ms
step:980/1775 train_time:43726ms step_avg:44.62ms
step:981/1775 train_time:43786ms step_avg:44.63ms
step:982/1775 train_time:43849ms step_avg:44.65ms
step:983/1775 train_time:43909ms step_avg:44.67ms
step:984/1775 train_time:43972ms step_avg:44.69ms
step:985/1775 train_time:44033ms step_avg:44.70ms
step:986/1775 train_time:44096ms step_avg:44.72ms
step:987/1775 train_time:44156ms step_avg:44.74ms
step:988/1775 train_time:44219ms step_avg:44.76ms
step:989/1775 train_time:44279ms step_avg:44.77ms
step:990/1775 train_time:44341ms step_avg:44.79ms
step:991/1775 train_time:44400ms step_avg:44.80ms
step:992/1775 train_time:44462ms step_avg:44.82ms
step:993/1775 train_time:44522ms step_avg:44.84ms
step:994/1775 train_time:44585ms step_avg:44.85ms
step:995/1775 train_time:44644ms step_avg:44.87ms
step:996/1775 train_time:44705ms step_avg:44.88ms
step:997/1775 train_time:44765ms step_avg:44.90ms
step:998/1775 train_time:44827ms step_avg:44.92ms
step:999/1775 train_time:44887ms step_avg:44.93ms
step:1000/1775 train_time:44950ms step_avg:44.95ms
step:1000/1775 val_loss:3.7395 train_time:45022ms step_avg:45.02ms
step:1001/1775 train_time:45041ms step_avg:45.00ms
step:1002/1775 train_time:45075ms step_avg:44.99ms
step:1003/1775 train_time:45137ms step_avg:45.00ms
step:1004/1775 train_time:45200ms step_avg:45.02ms
step:1005/1775 train_time:45260ms step_avg:45.03ms
step:1006/1775 train_time:45323ms step_avg:45.05ms
step:1007/1775 train_time:45382ms step_avg:45.07ms
step:1008/1775 train_time:45444ms step_avg:45.08ms
step:1009/1775 train_time:45503ms step_avg:45.10ms
step:1010/1775 train_time:45565ms step_avg:45.11ms
step:1011/1775 train_time:45625ms step_avg:45.13ms
step:1012/1775 train_time:45688ms step_avg:45.15ms
step:1013/1775 train_time:45748ms step_avg:45.16ms
step:1014/1775 train_time:45810ms step_avg:45.18ms
step:1015/1775 train_time:45870ms step_avg:45.19ms
step:1016/1775 train_time:45932ms step_avg:45.21ms
step:1017/1775 train_time:45991ms step_avg:45.22ms
step:1018/1775 train_time:46055ms step_avg:45.24ms
step:1019/1775 train_time:46116ms step_avg:45.26ms
step:1020/1775 train_time:46179ms step_avg:45.27ms
step:1021/1775 train_time:46239ms step_avg:45.29ms
step:1022/1775 train_time:46301ms step_avg:45.30ms
step:1023/1775 train_time:46361ms step_avg:45.32ms
step:1024/1775 train_time:46422ms step_avg:45.33ms
step:1025/1775 train_time:46481ms step_avg:45.35ms
step:1026/1775 train_time:46543ms step_avg:45.36ms
step:1027/1775 train_time:46603ms step_avg:45.38ms
step:1028/1775 train_time:46665ms step_avg:45.39ms
step:1029/1775 train_time:46724ms step_avg:45.41ms
step:1030/1775 train_time:46786ms step_avg:45.42ms
step:1031/1775 train_time:46845ms step_avg:45.44ms
step:1032/1775 train_time:46907ms step_avg:45.45ms
step:1033/1775 train_time:46968ms step_avg:45.47ms
step:1034/1775 train_time:47031ms step_avg:45.48ms
step:1035/1775 train_time:47092ms step_avg:45.50ms
step:1036/1775 train_time:47156ms step_avg:45.52ms
step:1037/1775 train_time:47216ms step_avg:45.53ms
step:1038/1775 train_time:47278ms step_avg:45.55ms
step:1039/1775 train_time:47338ms step_avg:45.56ms
step:1040/1775 train_time:47401ms step_avg:45.58ms
step:1041/1775 train_time:47460ms step_avg:45.59ms
step:1042/1775 train_time:47522ms step_avg:45.61ms
step:1043/1775 train_time:47581ms step_avg:45.62ms
step:1044/1775 train_time:47643ms step_avg:45.63ms
step:1045/1775 train_time:47702ms step_avg:45.65ms
step:1046/1775 train_time:47764ms step_avg:45.66ms
step:1047/1775 train_time:47823ms step_avg:45.68ms
step:1048/1775 train_time:47885ms step_avg:45.69ms
step:1049/1775 train_time:47944ms step_avg:45.70ms
step:1050/1775 train_time:48007ms step_avg:45.72ms
step:1051/1775 train_time:48068ms step_avg:45.74ms
step:1052/1775 train_time:48131ms step_avg:45.75ms
step:1053/1775 train_time:48192ms step_avg:45.77ms
step:1054/1775 train_time:48254ms step_avg:45.78ms
step:1055/1775 train_time:48314ms step_avg:45.80ms
step:1056/1775 train_time:48376ms step_avg:45.81ms
step:1057/1775 train_time:48436ms step_avg:45.82ms
step:1058/1775 train_time:48498ms step_avg:45.84ms
step:1059/1775 train_time:48559ms step_avg:45.85ms
step:1060/1775 train_time:48619ms step_avg:45.87ms
step:1061/1775 train_time:48679ms step_avg:45.88ms
step:1062/1775 train_time:48740ms step_avg:45.89ms
step:1063/1775 train_time:48800ms step_avg:45.91ms
step:1064/1775 train_time:48861ms step_avg:45.92ms
step:1065/1775 train_time:48921ms step_avg:45.94ms
step:1066/1775 train_time:48984ms step_avg:45.95ms
step:1067/1775 train_time:49043ms step_avg:45.96ms
step:1068/1775 train_time:49105ms step_avg:45.98ms
step:1069/1775 train_time:49165ms step_avg:45.99ms
step:1070/1775 train_time:49229ms step_avg:46.01ms
step:1071/1775 train_time:49289ms step_avg:46.02ms
step:1072/1775 train_time:49352ms step_avg:46.04ms
step:1073/1775 train_time:49412ms step_avg:46.05ms
step:1074/1775 train_time:49474ms step_avg:46.07ms
step:1075/1775 train_time:49535ms step_avg:46.08ms
step:1076/1775 train_time:49597ms step_avg:46.09ms
step:1077/1775 train_time:49657ms step_avg:46.11ms
step:1078/1775 train_time:49719ms step_avg:46.12ms
step:1079/1775 train_time:49779ms step_avg:46.13ms
step:1080/1775 train_time:49840ms step_avg:46.15ms
step:1081/1775 train_time:49900ms step_avg:46.16ms
step:1082/1775 train_time:49962ms step_avg:46.18ms
step:1083/1775 train_time:50021ms step_avg:46.19ms
step:1084/1775 train_time:50083ms step_avg:46.20ms
step:1085/1775 train_time:50143ms step_avg:46.21ms
step:1086/1775 train_time:50205ms step_avg:46.23ms
step:1087/1775 train_time:50265ms step_avg:46.24ms
step:1088/1775 train_time:50328ms step_avg:46.26ms
step:1089/1775 train_time:50388ms step_avg:46.27ms
step:1090/1775 train_time:50452ms step_avg:46.29ms
step:1091/1775 train_time:50511ms step_avg:46.30ms
step:1092/1775 train_time:50574ms step_avg:46.31ms
step:1093/1775 train_time:50634ms step_avg:46.33ms
step:1094/1775 train_time:50696ms step_avg:46.34ms
step:1095/1775 train_time:50756ms step_avg:46.35ms
step:1096/1775 train_time:50818ms step_avg:46.37ms
step:1097/1775 train_time:50878ms step_avg:46.38ms
step:1098/1775 train_time:50941ms step_avg:46.39ms
step:1099/1775 train_time:51000ms step_avg:46.41ms
step:1100/1775 train_time:51062ms step_avg:46.42ms
step:1101/1775 train_time:51121ms step_avg:46.43ms
step:1102/1775 train_time:51183ms step_avg:46.45ms
step:1103/1775 train_time:51242ms step_avg:46.46ms
step:1104/1775 train_time:51305ms step_avg:46.47ms
step:1105/1775 train_time:51364ms step_avg:46.48ms
step:1106/1775 train_time:51427ms step_avg:46.50ms
step:1107/1775 train_time:51488ms step_avg:46.51ms
step:1108/1775 train_time:51551ms step_avg:46.53ms
step:1109/1775 train_time:51610ms step_avg:46.54ms
step:1110/1775 train_time:51674ms step_avg:46.55ms
step:1111/1775 train_time:51734ms step_avg:46.57ms
step:1112/1775 train_time:51796ms step_avg:46.58ms
step:1113/1775 train_time:51856ms step_avg:46.59ms
step:1114/1775 train_time:51918ms step_avg:46.60ms
step:1115/1775 train_time:51978ms step_avg:46.62ms
step:1116/1775 train_time:52039ms step_avg:46.63ms
step:1117/1775 train_time:52098ms step_avg:46.64ms
step:1118/1775 train_time:52160ms step_avg:46.65ms
step:1119/1775 train_time:52219ms step_avg:46.67ms
step:1120/1775 train_time:52281ms step_avg:46.68ms
step:1121/1775 train_time:52341ms step_avg:46.69ms
step:1122/1775 train_time:52403ms step_avg:46.71ms
step:1123/1775 train_time:52463ms step_avg:46.72ms
step:1124/1775 train_time:52526ms step_avg:46.73ms
step:1125/1775 train_time:52586ms step_avg:46.74ms
step:1126/1775 train_time:52648ms step_avg:46.76ms
step:1127/1775 train_time:52709ms step_avg:46.77ms
step:1128/1775 train_time:52772ms step_avg:46.78ms
step:1129/1775 train_time:52833ms step_avg:46.80ms
step:1130/1775 train_time:52895ms step_avg:46.81ms
step:1131/1775 train_time:52955ms step_avg:46.82ms
step:1132/1775 train_time:53017ms step_avg:46.84ms
step:1133/1775 train_time:53077ms step_avg:46.85ms
step:1134/1775 train_time:53139ms step_avg:46.86ms
step:1135/1775 train_time:53198ms step_avg:46.87ms
step:1136/1775 train_time:53260ms step_avg:46.88ms
step:1137/1775 train_time:53320ms step_avg:46.89ms
step:1138/1775 train_time:53381ms step_avg:46.91ms
step:1139/1775 train_time:53441ms step_avg:46.92ms
step:1140/1775 train_time:53503ms step_avg:46.93ms
step:1141/1775 train_time:53562ms step_avg:46.94ms
step:1142/1775 train_time:53625ms step_avg:46.96ms
step:1143/1775 train_time:53685ms step_avg:46.97ms
step:1144/1775 train_time:53747ms step_avg:46.98ms
step:1145/1775 train_time:53807ms step_avg:46.99ms
step:1146/1775 train_time:53871ms step_avg:47.01ms
step:1147/1775 train_time:53931ms step_avg:47.02ms
step:1148/1775 train_time:53993ms step_avg:47.03ms
step:1149/1775 train_time:54054ms step_avg:47.04ms
step:1150/1775 train_time:54116ms step_avg:47.06ms
step:1151/1775 train_time:54175ms step_avg:47.07ms
step:1152/1775 train_time:54237ms step_avg:47.08ms
step:1153/1775 train_time:54297ms step_avg:47.09ms
step:1154/1775 train_time:54360ms step_avg:47.11ms
step:1155/1775 train_time:54420ms step_avg:47.12ms
step:1156/1775 train_time:54482ms step_avg:47.13ms
step:1157/1775 train_time:54541ms step_avg:47.14ms
step:1158/1775 train_time:54606ms step_avg:47.16ms
step:1159/1775 train_time:54692ms step_avg:47.19ms
step:1160/1775 train_time:54782ms step_avg:47.23ms
step:1161/1775 train_time:54868ms step_avg:47.26ms
step:1162/1775 train_time:54958ms step_avg:47.30ms
step:1163/1775 train_time:55046ms step_avg:47.33ms
step:1164/1775 train_time:55136ms step_avg:47.37ms
step:1165/1775 train_time:55223ms step_avg:47.40ms
step:1166/1775 train_time:55310ms step_avg:47.44ms
step:1167/1775 train_time:55397ms step_avg:47.47ms
step:1168/1775 train_time:55487ms step_avg:47.51ms
step:1169/1775 train_time:55574ms step_avg:47.54ms
step:1170/1775 train_time:55661ms step_avg:47.57ms
step:1171/1775 train_time:55747ms step_avg:47.61ms
step:1172/1775 train_time:55836ms step_avg:47.64ms
step:1173/1775 train_time:55923ms step_avg:47.67ms
step:1174/1775 train_time:56011ms step_avg:47.71ms
step:1175/1775 train_time:56100ms step_avg:47.74ms
step:1176/1775 train_time:56189ms step_avg:47.78ms
step:1177/1775 train_time:56276ms step_avg:47.81ms
step:1178/1775 train_time:56366ms step_avg:47.85ms
step:1179/1775 train_time:56451ms step_avg:47.88ms
step:1180/1775 train_time:56541ms step_avg:47.92ms
step:1181/1775 train_time:56628ms step_avg:47.95ms
step:1182/1775 train_time:56718ms step_avg:47.98ms
step:1183/1775 train_time:56804ms step_avg:48.02ms
step:1184/1775 train_time:56893ms step_avg:48.05ms
step:1185/1775 train_time:56979ms step_avg:48.08ms
step:1186/1775 train_time:57069ms step_avg:48.12ms
step:1187/1775 train_time:57155ms step_avg:48.15ms
step:1188/1775 train_time:57245ms step_avg:48.19ms
step:1189/1775 train_time:57331ms step_avg:48.22ms
step:1190/1775 train_time:57420ms step_avg:48.25ms
step:1191/1775 train_time:57507ms step_avg:48.28ms
step:1192/1775 train_time:57596ms step_avg:48.32ms
step:1193/1775 train_time:57681ms step_avg:48.35ms
step:1194/1775 train_time:57768ms step_avg:48.38ms
step:1195/1775 train_time:57854ms step_avg:48.41ms
step:1196/1775 train_time:57944ms step_avg:48.45ms
step:1197/1775 train_time:58031ms step_avg:48.48ms
step:1198/1775 train_time:58120ms step_avg:48.51ms
step:1199/1775 train_time:58207ms step_avg:48.55ms
step:1200/1775 train_time:58295ms step_avg:48.58ms
step:1201/1775 train_time:58381ms step_avg:48.61ms
step:1202/1775 train_time:58469ms step_avg:48.64ms
step:1203/1775 train_time:58556ms step_avg:48.68ms
step:1204/1775 train_time:58645ms step_avg:48.71ms
step:1205/1775 train_time:58730ms step_avg:48.74ms
step:1206/1775 train_time:58819ms step_avg:48.77ms
step:1207/1775 train_time:58905ms step_avg:48.80ms
step:1208/1775 train_time:58994ms step_avg:48.84ms
step:1209/1775 train_time:59079ms step_avg:48.87ms
step:1210/1775 train_time:59169ms step_avg:48.90ms
step:1211/1775 train_time:59256ms step_avg:48.93ms
step:1212/1775 train_time:59347ms step_avg:48.97ms
step:1213/1775 train_time:59433ms step_avg:49.00ms
step:1214/1775 train_time:59522ms step_avg:49.03ms
step:1215/1775 train_time:59607ms step_avg:49.06ms
step:1216/1775 train_time:59698ms step_avg:49.09ms
step:1217/1775 train_time:59784ms step_avg:49.12ms
step:1218/1775 train_time:59871ms step_avg:49.16ms
step:1219/1775 train_time:59958ms step_avg:49.19ms
step:1220/1775 train_time:60047ms step_avg:49.22ms
step:1221/1775 train_time:60132ms step_avg:49.25ms
step:1222/1775 train_time:60222ms step_avg:49.28ms
step:1223/1775 train_time:60308ms step_avg:49.31ms
step:1224/1775 train_time:60398ms step_avg:49.34ms
step:1225/1775 train_time:60485ms step_avg:49.38ms
step:1226/1775 train_time:60572ms step_avg:49.41ms
step:1227/1775 train_time:60659ms step_avg:49.44ms
step:1228/1775 train_time:60748ms step_avg:49.47ms
step:1229/1775 train_time:60833ms step_avg:49.50ms
step:1230/1775 train_time:60923ms step_avg:49.53ms
step:1231/1775 train_time:61008ms step_avg:49.56ms
step:1232/1775 train_time:61097ms step_avg:49.59ms
step:1233/1775 train_time:61184ms step_avg:49.62ms
step:1234/1775 train_time:61273ms step_avg:49.65ms
step:1235/1775 train_time:61360ms step_avg:49.68ms
step:1236/1775 train_time:61449ms step_avg:49.72ms
step:1237/1775 train_time:61534ms step_avg:49.74ms
step:1238/1775 train_time:61623ms step_avg:49.78ms
step:1239/1775 train_time:61709ms step_avg:49.81ms
step:1240/1775 train_time:61798ms step_avg:49.84ms
step:1241/1775 train_time:61886ms step_avg:49.87ms
step:1242/1775 train_time:61974ms step_avg:49.90ms
step:1243/1775 train_time:62060ms step_avg:49.93ms
step:1244/1775 train_time:62149ms step_avg:49.96ms
step:1245/1775 train_time:62235ms step_avg:49.99ms
step:1246/1775 train_time:62324ms step_avg:50.02ms
step:1247/1775 train_time:62408ms step_avg:50.05ms
step:1248/1775 train_time:62498ms step_avg:50.08ms
step:1249/1775 train_time:62584ms step_avg:50.11ms
step:1250/1775 train_time:62673ms step_avg:50.14ms
step:1250/1775 val_loss:3.5062 train_time:62774ms step_avg:50.22ms
step:1251/1775 train_time:62794ms step_avg:50.20ms
step:1252/1775 train_time:62856ms step_avg:50.20ms
step:1253/1775 train_time:62945ms step_avg:50.24ms
step:1254/1775 train_time:63034ms step_avg:50.27ms
step:1255/1775 train_time:63120ms step_avg:50.29ms
step:1256/1775 train_time:63208ms step_avg:50.32ms
step:1257/1775 train_time:63293ms step_avg:50.35ms
step:1258/1775 train_time:63383ms step_avg:50.38ms
step:1259/1775 train_time:63469ms step_avg:50.41ms
step:1260/1775 train_time:63557ms step_avg:50.44ms
step:1261/1775 train_time:63643ms step_avg:50.47ms
step:1262/1775 train_time:63734ms step_avg:50.50ms
step:1263/1775 train_time:63821ms step_avg:50.53ms
step:1264/1775 train_time:63913ms step_avg:50.56ms
step:1265/1775 train_time:64000ms step_avg:50.59ms
step:1266/1775 train_time:64090ms step_avg:50.62ms
step:1267/1775 train_time:64176ms step_avg:50.65ms
step:1268/1775 train_time:64265ms step_avg:50.68ms
step:1269/1775 train_time:64350ms step_avg:50.71ms
step:1270/1775 train_time:64437ms step_avg:50.74ms
step:1271/1775 train_time:64524ms step_avg:50.77ms
step:1272/1775 train_time:64613ms step_avg:50.80ms
step:1273/1775 train_time:64700ms step_avg:50.82ms
step:1274/1775 train_time:64790ms step_avg:50.86ms
step:1275/1775 train_time:64876ms step_avg:50.88ms
step:1276/1775 train_time:64966ms step_avg:50.91ms
step:1277/1775 train_time:65053ms step_avg:50.94ms
step:1278/1775 train_time:65140ms step_avg:50.97ms
step:1279/1775 train_time:65226ms step_avg:51.00ms
step:1280/1775 train_time:65315ms step_avg:51.03ms
step:1281/1775 train_time:65402ms step_avg:51.06ms
step:1282/1775 train_time:65490ms step_avg:51.08ms
step:1283/1775 train_time:65575ms step_avg:51.11ms
step:1284/1775 train_time:65664ms step_avg:51.14ms
step:1285/1775 train_time:65753ms step_avg:51.17ms
step:1286/1775 train_time:65842ms step_avg:51.20ms
step:1287/1775 train_time:65929ms step_avg:51.23ms
step:1288/1775 train_time:66017ms step_avg:51.26ms
step:1289/1775 train_time:66104ms step_avg:51.28ms
step:1290/1775 train_time:66193ms step_avg:51.31ms
step:1291/1775 train_time:66279ms step_avg:51.34ms
step:1292/1775 train_time:66367ms step_avg:51.37ms
step:1293/1775 train_time:66453ms step_avg:51.39ms
step:1294/1775 train_time:66542ms step_avg:51.42ms
step:1295/1775 train_time:66628ms step_avg:51.45ms
step:1296/1775 train_time:66716ms step_avg:51.48ms
step:1297/1775 train_time:66803ms step_avg:51.51ms
step:1298/1775 train_time:66892ms step_avg:51.53ms
step:1299/1775 train_time:66977ms step_avg:51.56ms
step:1300/1775 train_time:67067ms step_avg:51.59ms
step:1301/1775 train_time:67153ms step_avg:51.62ms
step:1302/1775 train_time:67242ms step_avg:51.65ms
step:1303/1775 train_time:67330ms step_avg:51.67ms
step:1304/1775 train_time:67418ms step_avg:51.70ms
step:1305/1775 train_time:67504ms step_avg:51.73ms
step:1306/1775 train_time:67593ms step_avg:51.76ms
step:1307/1775 train_time:67678ms step_avg:51.78ms
step:1308/1775 train_time:67768ms step_avg:51.81ms
step:1309/1775 train_time:67855ms step_avg:51.84ms
step:1310/1775 train_time:67944ms step_avg:51.87ms
step:1311/1775 train_time:68031ms step_avg:51.89ms
step:1312/1775 train_time:68120ms step_avg:51.92ms
step:1313/1775 train_time:68206ms step_avg:51.95ms
step:1314/1775 train_time:68294ms step_avg:51.97ms
step:1315/1775 train_time:68381ms step_avg:52.00ms
step:1316/1775 train_time:68471ms step_avg:52.03ms
step:1317/1775 train_time:68556ms step_avg:52.05ms
step:1318/1775 train_time:68645ms step_avg:52.08ms
step:1319/1775 train_time:68731ms step_avg:52.11ms
step:1320/1775 train_time:68819ms step_avg:52.14ms
step:1321/1775 train_time:68906ms step_avg:52.16ms
step:1322/1775 train_time:68994ms step_avg:52.19ms
step:1323/1775 train_time:69080ms step_avg:52.21ms
step:1324/1775 train_time:69170ms step_avg:52.24ms
step:1325/1775 train_time:69255ms step_avg:52.27ms
step:1326/1775 train_time:69344ms step_avg:52.30ms
step:1327/1775 train_time:69431ms step_avg:52.32ms
step:1328/1775 train_time:69519ms step_avg:52.35ms
step:1329/1775 train_time:69605ms step_avg:52.37ms
step:1330/1775 train_time:69693ms step_avg:52.40ms
step:1331/1775 train_time:69778ms step_avg:52.42ms
step:1332/1775 train_time:69868ms step_avg:52.45ms
step:1333/1775 train_time:69954ms step_avg:52.48ms
step:1334/1775 train_time:70043ms step_avg:52.51ms
step:1335/1775 train_time:70130ms step_avg:52.53ms
step:1336/1775 train_time:70217ms step_avg:52.56ms
step:1337/1775 train_time:70304ms step_avg:52.58ms
step:1338/1775 train_time:70393ms step_avg:52.61ms
step:1339/1775 train_time:70479ms step_avg:52.64ms
step:1340/1775 train_time:70568ms step_avg:52.66ms
step:1341/1775 train_time:70654ms step_avg:52.69ms
step:1342/1775 train_time:70742ms step_avg:52.71ms
step:1343/1775 train_time:70828ms step_avg:52.74ms
step:1344/1775 train_time:70916ms step_avg:52.77ms
step:1345/1775 train_time:71004ms step_avg:52.79ms
step:1346/1775 train_time:71093ms step_avg:52.82ms
step:1347/1775 train_time:71178ms step_avg:52.84ms
step:1348/1775 train_time:71268ms step_avg:52.87ms
step:1349/1775 train_time:71354ms step_avg:52.89ms
step:1350/1775 train_time:71443ms step_avg:52.92ms
step:1351/1775 train_time:71530ms step_avg:52.95ms
step:1352/1775 train_time:71617ms step_avg:52.97ms
step:1353/1775 train_time:71703ms step_avg:53.00ms
step:1354/1775 train_time:71792ms step_avg:53.02ms
step:1355/1775 train_time:71878ms step_avg:53.05ms
step:1356/1775 train_time:71967ms step_avg:53.07ms
step:1357/1775 train_time:72053ms step_avg:53.10ms
step:1358/1775 train_time:72142ms step_avg:53.12ms
step:1359/1775 train_time:72227ms step_avg:53.15ms
step:1360/1775 train_time:72315ms step_avg:53.17ms
step:1361/1775 train_time:72402ms step_avg:53.20ms
step:1362/1775 train_time:72491ms step_avg:53.22ms
step:1363/1775 train_time:72577ms step_avg:53.25ms
step:1364/1775 train_time:72665ms step_avg:53.27ms
step:1365/1775 train_time:72752ms step_avg:53.30ms
step:1366/1775 train_time:72841ms step_avg:53.32ms
step:1367/1775 train_time:72926ms step_avg:53.35ms
step:1368/1775 train_time:73015ms step_avg:53.37ms
step:1369/1775 train_time:73102ms step_avg:53.40ms
step:1370/1775 train_time:73190ms step_avg:53.42ms
step:1371/1775 train_time:73276ms step_avg:53.45ms
step:1372/1775 train_time:73365ms step_avg:53.47ms
step:1373/1775 train_time:73452ms step_avg:53.50ms
step:1374/1775 train_time:73540ms step_avg:53.52ms
step:1375/1775 train_time:73626ms step_avg:53.55ms
step:1376/1775 train_time:73714ms step_avg:53.57ms
step:1377/1775 train_time:73801ms step_avg:53.60ms
step:1378/1775 train_time:73889ms step_avg:53.62ms
step:1379/1775 train_time:73975ms step_avg:53.64ms
step:1380/1775 train_time:74066ms step_avg:53.67ms
step:1381/1775 train_time:74152ms step_avg:53.69ms
step:1382/1775 train_time:74240ms step_avg:53.72ms
step:1383/1775 train_time:74326ms step_avg:53.74ms
step:1384/1775 train_time:74415ms step_avg:53.77ms
step:1385/1775 train_time:74501ms step_avg:53.79ms
step:1386/1775 train_time:74591ms step_avg:53.82ms
step:1387/1775 train_time:74677ms step_avg:53.84ms
step:1388/1775 train_time:74766ms step_avg:53.87ms
step:1389/1775 train_time:74852ms step_avg:53.89ms
step:1390/1775 train_time:74941ms step_avg:53.91ms
step:1391/1775 train_time:75027ms step_avg:53.94ms
step:1392/1775 train_time:75115ms step_avg:53.96ms
step:1393/1775 train_time:75202ms step_avg:53.99ms
step:1394/1775 train_time:75291ms step_avg:54.01ms
step:1395/1775 train_time:75376ms step_avg:54.03ms
step:1396/1775 train_time:75465ms step_avg:54.06ms
step:1397/1775 train_time:75551ms step_avg:54.08ms
step:1398/1775 train_time:75640ms step_avg:54.11ms
step:1399/1775 train_time:75726ms step_avg:54.13ms
step:1400/1775 train_time:75815ms step_avg:54.15ms
step:1401/1775 train_time:75902ms step_avg:54.18ms
step:1402/1775 train_time:75991ms step_avg:54.20ms
step:1403/1775 train_time:76077ms step_avg:54.22ms
step:1404/1775 train_time:76165ms step_avg:54.25ms
step:1405/1775 train_time:76251ms step_avg:54.27ms
step:1406/1775 train_time:76339ms step_avg:54.30ms
step:1407/1775 train_time:76425ms step_avg:54.32ms
step:1408/1775 train_time:76515ms step_avg:54.34ms
step:1409/1775 train_time:76601ms step_avg:54.37ms
step:1410/1775 train_time:76690ms step_avg:54.39ms
step:1411/1775 train_time:76776ms step_avg:54.41ms
step:1412/1775 train_time:76865ms step_avg:54.44ms
step:1413/1775 train_time:76952ms step_avg:54.46ms
step:1414/1775 train_time:77041ms step_avg:54.48ms
step:1415/1775 train_time:77127ms step_avg:54.51ms
step:1416/1775 train_time:77215ms step_avg:54.53ms
step:1417/1775 train_time:77302ms step_avg:54.55ms
step:1418/1775 train_time:77392ms step_avg:54.58ms
step:1419/1775 train_time:77477ms step_avg:54.60ms
step:1420/1775 train_time:77566ms step_avg:54.62ms
step:1421/1775 train_time:77653ms step_avg:54.65ms
step:1422/1775 train_time:77741ms step_avg:54.67ms
step:1423/1775 train_time:77828ms step_avg:54.69ms
step:1424/1775 train_time:77915ms step_avg:54.72ms
step:1425/1775 train_time:78003ms step_avg:54.74ms
step:1426/1775 train_time:78092ms step_avg:54.76ms
step:1427/1775 train_time:78178ms step_avg:54.79ms
step:1428/1775 train_time:78267ms step_avg:54.81ms
step:1429/1775 train_time:78353ms step_avg:54.83ms
step:1430/1775 train_time:78443ms step_avg:54.85ms
step:1431/1775 train_time:78528ms step_avg:54.88ms
step:1432/1775 train_time:78616ms step_avg:54.90ms
step:1433/1775 train_time:78704ms step_avg:54.92ms
step:1434/1775 train_time:78793ms step_avg:54.95ms
step:1435/1775 train_time:78877ms step_avg:54.97ms
step:1436/1775 train_time:78967ms step_avg:54.99ms
step:1437/1775 train_time:79053ms step_avg:55.01ms
step:1438/1775 train_time:79143ms step_avg:55.04ms
step:1439/1775 train_time:79229ms step_avg:55.06ms
step:1440/1775 train_time:79317ms step_avg:55.08ms
step:1441/1775 train_time:79402ms step_avg:55.10ms
step:1442/1775 train_time:79492ms step_avg:55.13ms
step:1443/1775 train_time:79577ms step_avg:55.15ms
step:1444/1775 train_time:79667ms step_avg:55.17ms
step:1445/1775 train_time:79754ms step_avg:55.19ms
step:1446/1775 train_time:79843ms step_avg:55.22ms
step:1447/1775 train_time:79930ms step_avg:55.24ms
step:1448/1775 train_time:80018ms step_avg:55.26ms
step:1449/1775 train_time:80104ms step_avg:55.28ms
step:1450/1775 train_time:80193ms step_avg:55.31ms
step:1451/1775 train_time:80278ms step_avg:55.33ms
step:1452/1775 train_time:80367ms step_avg:55.35ms
step:1453/1775 train_time:80454ms step_avg:55.37ms
step:1454/1775 train_time:80544ms step_avg:55.39ms
step:1455/1775 train_time:80631ms step_avg:55.42ms
step:1456/1775 train_time:80721ms step_avg:55.44ms
step:1457/1775 train_time:80806ms step_avg:55.46ms
step:1458/1775 train_time:80894ms step_avg:55.48ms
step:1459/1775 train_time:80981ms step_avg:55.50ms
step:1460/1775 train_time:81070ms step_avg:55.53ms
step:1461/1775 train_time:81156ms step_avg:55.55ms
step:1462/1775 train_time:81245ms step_avg:55.57ms
step:1463/1775 train_time:81332ms step_avg:55.59ms
step:1464/1775 train_time:81419ms step_avg:55.61ms
step:1465/1775 train_time:81506ms step_avg:55.64ms
step:1466/1775 train_time:81594ms step_avg:55.66ms
step:1467/1775 train_time:81681ms step_avg:55.68ms
step:1468/1775 train_time:81772ms step_avg:55.70ms
step:1469/1775 train_time:81858ms step_avg:55.72ms
step:1470/1775 train_time:81946ms step_avg:55.75ms
step:1471/1775 train_time:82033ms step_avg:55.77ms
step:1472/1775 train_time:82122ms step_avg:55.79ms
step:1473/1775 train_time:82208ms step_avg:55.81ms
step:1474/1775 train_time:82296ms step_avg:55.83ms
step:1475/1775 train_time:82382ms step_avg:55.85ms
step:1476/1775 train_time:82472ms step_avg:55.88ms
step:1477/1775 train_time:82558ms step_avg:55.90ms
step:1478/1775 train_time:82645ms step_avg:55.92ms
step:1479/1775 train_time:82732ms step_avg:55.94ms
step:1480/1775 train_time:82822ms step_avg:55.96ms
step:1481/1775 train_time:82908ms step_avg:55.98ms
step:1482/1775 train_time:82996ms step_avg:56.00ms
step:1483/1775 train_time:83082ms step_avg:56.02ms
step:1484/1775 train_time:83171ms step_avg:56.05ms
step:1485/1775 train_time:83257ms step_avg:56.07ms
step:1486/1775 train_time:83347ms step_avg:56.09ms
step:1487/1775 train_time:83433ms step_avg:56.11ms
step:1488/1775 train_time:83522ms step_avg:56.13ms
step:1489/1775 train_time:83608ms step_avg:56.15ms
step:1490/1775 train_time:83697ms step_avg:56.17ms
step:1491/1775 train_time:83784ms step_avg:56.19ms
step:1492/1775 train_time:83874ms step_avg:56.22ms
step:1493/1775 train_time:83960ms step_avg:56.24ms
step:1494/1775 train_time:84049ms step_avg:56.26ms
step:1495/1775 train_time:84136ms step_avg:56.28ms
step:1496/1775 train_time:84225ms step_avg:56.30ms
step:1497/1775 train_time:84312ms step_avg:56.32ms
step:1498/1775 train_time:84401ms step_avg:56.34ms
step:1499/1775 train_time:84487ms step_avg:56.36ms
step:1500/1775 train_time:84576ms step_avg:56.38ms
step:1500/1775 val_loss:3.3759 train_time:84673ms step_avg:56.45ms
step:1501/1775 train_time:84693ms step_avg:56.42ms
step:1502/1775 train_time:84756ms step_avg:56.43ms
step:1503/1775 train_time:84844ms step_avg:56.45ms
step:1504/1775 train_time:84932ms step_avg:56.47ms
step:1505/1775 train_time:85018ms step_avg:56.49ms
step:1506/1775 train_time:85106ms step_avg:56.51ms
step:1507/1775 train_time:85192ms step_avg:56.53ms
step:1508/1775 train_time:85279ms step_avg:56.55ms
step:1509/1775 train_time:85364ms step_avg:56.57ms
step:1510/1775 train_time:85453ms step_avg:56.59ms
step:1511/1775 train_time:85538ms step_avg:56.61ms
step:1512/1775 train_time:85628ms step_avg:56.63ms
step:1513/1775 train_time:85715ms step_avg:56.65ms
step:1514/1775 train_time:85804ms step_avg:56.67ms
step:1515/1775 train_time:85892ms step_avg:56.69ms
step:1516/1775 train_time:85981ms step_avg:56.72ms
step:1517/1775 train_time:86067ms step_avg:56.74ms
step:1518/1775 train_time:86156ms step_avg:56.76ms
step:1519/1775 train_time:86240ms step_avg:56.77ms
step:1520/1775 train_time:86329ms step_avg:56.80ms
step:1521/1775 train_time:86415ms step_avg:56.81ms
step:1522/1775 train_time:86503ms step_avg:56.84ms
step:1523/1775 train_time:86589ms step_avg:56.85ms
step:1524/1775 train_time:86678ms step_avg:56.88ms
step:1525/1775 train_time:86765ms step_avg:56.89ms
step:1526/1775 train_time:86857ms step_avg:56.92ms
step:1527/1775 train_time:86943ms step_avg:56.94ms
step:1528/1775 train_time:87032ms step_avg:56.96ms
step:1529/1775 train_time:87118ms step_avg:56.98ms
step:1530/1775 train_time:87206ms step_avg:57.00ms
step:1531/1775 train_time:87291ms step_avg:57.02ms
step:1532/1775 train_time:87380ms step_avg:57.04ms
step:1533/1775 train_time:87466ms step_avg:57.06ms
step:1534/1775 train_time:87556ms step_avg:57.08ms
step:1535/1775 train_time:87642ms step_avg:57.10ms
step:1536/1775 train_time:87730ms step_avg:57.12ms
step:1537/1775 train_time:87818ms step_avg:57.14ms
step:1538/1775 train_time:87907ms step_avg:57.16ms
step:1539/1775 train_time:87993ms step_avg:57.18ms
step:1540/1775 train_time:88081ms step_avg:57.20ms
step:1541/1775 train_time:88168ms step_avg:57.21ms
step:1542/1775 train_time:88257ms step_avg:57.24ms
step:1543/1775 train_time:88342ms step_avg:57.25ms
step:1544/1775 train_time:88431ms step_avg:57.27ms
step:1545/1775 train_time:88518ms step_avg:57.29ms
step:1546/1775 train_time:88606ms step_avg:57.31ms
step:1547/1775 train_time:88692ms step_avg:57.33ms
step:1548/1775 train_time:88781ms step_avg:57.35ms
step:1549/1775 train_time:88869ms step_avg:57.37ms
step:1550/1775 train_time:88958ms step_avg:57.39ms
step:1551/1775 train_time:89043ms step_avg:57.41ms
step:1552/1775 train_time:89132ms step_avg:57.43ms
step:1553/1775 train_time:89218ms step_avg:57.45ms
step:1554/1775 train_time:89306ms step_avg:57.47ms
step:1555/1775 train_time:89391ms step_avg:57.49ms
step:1556/1775 train_time:89480ms step_avg:57.51ms
step:1557/1775 train_time:89565ms step_avg:57.52ms
step:1558/1775 train_time:89655ms step_avg:57.55ms
step:1559/1775 train_time:89741ms step_avg:57.56ms
step:1560/1775 train_time:89831ms step_avg:57.58ms
step:1561/1775 train_time:89917ms step_avg:57.60ms
step:1562/1775 train_time:90007ms step_avg:57.62ms
step:1563/1775 train_time:90093ms step_avg:57.64ms
step:1564/1775 train_time:90182ms step_avg:57.66ms
step:1565/1775 train_time:90268ms step_avg:57.68ms
step:1566/1775 train_time:90357ms step_avg:57.70ms
step:1567/1775 train_time:90443ms step_avg:57.72ms
step:1568/1775 train_time:90531ms step_avg:57.74ms
step:1569/1775 train_time:90617ms step_avg:57.75ms
step:1570/1775 train_time:90706ms step_avg:57.77ms
step:1571/1775 train_time:90793ms step_avg:57.79ms
step:1572/1775 train_time:90881ms step_avg:57.81ms
step:1573/1775 train_time:90966ms step_avg:57.83ms
step:1574/1775 train_time:91056ms step_avg:57.85ms
step:1575/1775 train_time:91142ms step_avg:57.87ms
step:1576/1775 train_time:91232ms step_avg:57.89ms
step:1577/1775 train_time:91318ms step_avg:57.91ms
step:1578/1775 train_time:91406ms step_avg:57.93ms
step:1579/1775 train_time:91491ms step_avg:57.94ms
step:1580/1775 train_time:91580ms step_avg:57.96ms
step:1581/1775 train_time:91667ms step_avg:57.98ms
step:1582/1775 train_time:91756ms step_avg:58.00ms
step:1583/1775 train_time:91842ms step_avg:58.02ms
step:1584/1775 train_time:91932ms step_avg:58.04ms
step:1585/1775 train_time:92018ms step_avg:58.06ms
step:1586/1775 train_time:92107ms step_avg:58.08ms
step:1587/1775 train_time:92194ms step_avg:58.09ms
step:1588/1775 train_time:92283ms step_avg:58.11ms
step:1589/1775 train_time:92370ms step_avg:58.13ms
step:1590/1775 train_time:92459ms step_avg:58.15ms
step:1591/1775 train_time:92543ms step_avg:58.17ms
step:1592/1775 train_time:92634ms step_avg:58.19ms
step:1593/1775 train_time:92720ms step_avg:58.20ms
step:1594/1775 train_time:92808ms step_avg:58.22ms
step:1595/1775 train_time:92895ms step_avg:58.24ms
step:1596/1775 train_time:92984ms step_avg:58.26ms
step:1597/1775 train_time:93070ms step_avg:58.28ms
step:1598/1775 train_time:93160ms step_avg:58.30ms
step:1599/1775 train_time:93247ms step_avg:58.32ms
step:1600/1775 train_time:93337ms step_avg:58.34ms
step:1601/1775 train_time:93422ms step_avg:58.35ms
step:1602/1775 train_time:93512ms step_avg:58.37ms
step:1603/1775 train_time:93598ms step_avg:58.39ms
step:1604/1775 train_time:93686ms step_avg:58.41ms
step:1605/1775 train_time:93772ms step_avg:58.43ms
step:1606/1775 train_time:93861ms step_avg:58.44ms
step:1607/1775 train_time:93947ms step_avg:58.46ms
step:1608/1775 train_time:94037ms step_avg:58.48ms
step:1609/1775 train_time:94122ms step_avg:58.50ms
step:1610/1775 train_time:94211ms step_avg:58.52ms
step:1611/1775 train_time:94298ms step_avg:58.53ms
step:1612/1775 train_time:94387ms step_avg:58.55ms
step:1613/1775 train_time:94472ms step_avg:58.57ms
step:1614/1775 train_time:94560ms step_avg:58.59ms
step:1615/1775 train_time:94646ms step_avg:58.60ms
step:1616/1775 train_time:94736ms step_avg:58.62ms
step:1617/1775 train_time:94822ms step_avg:58.64ms
step:1618/1775 train_time:94910ms step_avg:58.66ms
step:1619/1775 train_time:94997ms step_avg:58.68ms
step:1620/1775 train_time:95085ms step_avg:58.69ms
step:1621/1775 train_time:95172ms step_avg:58.71ms
step:1622/1775 train_time:95259ms step_avg:58.73ms
step:1623/1775 train_time:95346ms step_avg:58.75ms
step:1624/1775 train_time:95436ms step_avg:58.77ms
step:1625/1775 train_time:95522ms step_avg:58.78ms
step:1626/1775 train_time:95610ms step_avg:58.80ms
step:1627/1775 train_time:95697ms step_avg:58.82ms
step:1628/1775 train_time:95785ms step_avg:58.84ms
step:1629/1775 train_time:95873ms step_avg:58.85ms
step:1630/1775 train_time:95961ms step_avg:58.87ms
step:1631/1775 train_time:96048ms step_avg:58.89ms
step:1632/1775 train_time:96137ms step_avg:58.91ms
step:1633/1775 train_time:96223ms step_avg:58.92ms
step:1634/1775 train_time:96311ms step_avg:58.94ms
step:1635/1775 train_time:96399ms step_avg:58.96ms
step:1636/1775 train_time:96487ms step_avg:58.98ms
step:1637/1775 train_time:96574ms step_avg:58.99ms
step:1638/1775 train_time:96661ms step_avg:59.01ms
step:1639/1775 train_time:96747ms step_avg:59.03ms
step:1640/1775 train_time:96837ms step_avg:59.05ms
step:1641/1775 train_time:96923ms step_avg:59.06ms
step:1642/1775 train_time:97012ms step_avg:59.08ms
step:1643/1775 train_time:97099ms step_avg:59.10ms
step:1644/1775 train_time:97187ms step_avg:59.12ms
step:1645/1775 train_time:97272ms step_avg:59.13ms
step:1646/1775 train_time:97362ms step_avg:59.15ms
step:1647/1775 train_time:97448ms step_avg:59.17ms
step:1648/1775 train_time:97537ms step_avg:59.19ms
step:1649/1775 train_time:97622ms step_avg:59.20ms
step:1650/1775 train_time:97711ms step_avg:59.22ms
step:1651/1775 train_time:97798ms step_avg:59.24ms
step:1652/1775 train_time:97885ms step_avg:59.25ms
step:1653/1775 train_time:97970ms step_avg:59.27ms
step:1654/1775 train_time:98060ms step_avg:59.29ms
step:1655/1775 train_time:98146ms step_avg:59.30ms
step:1656/1775 train_time:98235ms step_avg:59.32ms
step:1657/1775 train_time:98320ms step_avg:59.34ms
step:1658/1775 train_time:98410ms step_avg:59.35ms
step:1659/1775 train_time:98496ms step_avg:59.37ms
step:1660/1775 train_time:98584ms step_avg:59.39ms
step:1661/1775 train_time:98671ms step_avg:59.40ms
step:1662/1775 train_time:98759ms step_avg:59.42ms
step:1663/1775 train_time:98844ms step_avg:59.44ms
step:1664/1775 train_time:98935ms step_avg:59.46ms
step:1665/1775 train_time:99020ms step_avg:59.47ms
step:1666/1775 train_time:99109ms step_avg:59.49ms
step:1667/1775 train_time:99196ms step_avg:59.51ms
step:1668/1775 train_time:99285ms step_avg:59.52ms
step:1669/1775 train_time:99372ms step_avg:59.54ms
step:1670/1775 train_time:99460ms step_avg:59.56ms
step:1671/1775 train_time:99546ms step_avg:59.57ms
step:1672/1775 train_time:99636ms step_avg:59.59ms
step:1673/1775 train_time:99722ms step_avg:59.61ms
step:1674/1775 train_time:99811ms step_avg:59.62ms
step:1675/1775 train_time:99897ms step_avg:59.64ms
step:1676/1775 train_time:99985ms step_avg:59.66ms
step:1677/1775 train_time:100071ms step_avg:59.67ms
step:1678/1775 train_time:100160ms step_avg:59.69ms
step:1679/1775 train_time:100246ms step_avg:59.71ms
step:1680/1775 train_time:100335ms step_avg:59.72ms
step:1681/1775 train_time:100421ms step_avg:59.74ms
step:1682/1775 train_time:100512ms step_avg:59.76ms
step:1683/1775 train_time:100598ms step_avg:59.77ms
step:1684/1775 train_time:100687ms step_avg:59.79ms
step:1685/1775 train_time:100773ms step_avg:59.81ms
step:1686/1775 train_time:100862ms step_avg:59.82ms
step:1687/1775 train_time:100947ms step_avg:59.84ms
step:1688/1775 train_time:101037ms step_avg:59.86ms
step:1689/1775 train_time:101122ms step_avg:59.87ms
step:1690/1775 train_time:101211ms step_avg:59.89ms
step:1691/1775 train_time:101298ms step_avg:59.90ms
step:1692/1775 train_time:101386ms step_avg:59.92ms
step:1693/1775 train_time:101471ms step_avg:59.94ms
step:1694/1775 train_time:101561ms step_avg:59.95ms
step:1695/1775 train_time:101648ms step_avg:59.97ms
step:1696/1775 train_time:101736ms step_avg:59.99ms
step:1697/1775 train_time:101821ms step_avg:60.00ms
step:1698/1775 train_time:101911ms step_avg:60.02ms
step:1699/1775 train_time:101997ms step_avg:60.03ms
step:1700/1775 train_time:102085ms step_avg:60.05ms
step:1701/1775 train_time:102172ms step_avg:60.07ms
step:1702/1775 train_time:102259ms step_avg:60.08ms
step:1703/1775 train_time:102346ms step_avg:60.10ms
step:1704/1775 train_time:102436ms step_avg:60.11ms
step:1705/1775 train_time:102521ms step_avg:60.13ms
step:1706/1775 train_time:102611ms step_avg:60.15ms
step:1707/1775 train_time:102698ms step_avg:60.16ms
step:1708/1775 train_time:102787ms step_avg:60.18ms
step:1709/1775 train_time:102873ms step_avg:60.20ms
step:1710/1775 train_time:102961ms step_avg:60.21ms
step:1711/1775 train_time:103047ms step_avg:60.23ms
step:1712/1775 train_time:103137ms step_avg:60.24ms
step:1713/1775 train_time:103223ms step_avg:60.26ms
step:1714/1775 train_time:103311ms step_avg:60.27ms
step:1715/1775 train_time:103398ms step_avg:60.29ms
step:1716/1775 train_time:103486ms step_avg:60.31ms
step:1717/1775 train_time:103572ms step_avg:60.32ms
step:1718/1775 train_time:103661ms step_avg:60.34ms
step:1719/1775 train_time:103746ms step_avg:60.35ms
step:1720/1775 train_time:103838ms step_avg:60.37ms
step:1721/1775 train_time:103923ms step_avg:60.39ms
step:1722/1775 train_time:104011ms step_avg:60.40ms
step:1723/1775 train_time:104098ms step_avg:60.42ms
step:1724/1775 train_time:104186ms step_avg:60.43ms
step:1725/1775 train_time:104272ms step_avg:60.45ms
step:1726/1775 train_time:104362ms step_avg:60.46ms
step:1727/1775 train_time:104448ms step_avg:60.48ms
step:1728/1775 train_time:104537ms step_avg:60.50ms
step:1729/1775 train_time:104622ms step_avg:60.51ms
step:1730/1775 train_time:104711ms step_avg:60.53ms
step:1731/1775 train_time:104798ms step_avg:60.54ms
step:1732/1775 train_time:104886ms step_avg:60.56ms
step:1733/1775 train_time:104972ms step_avg:60.57ms
step:1734/1775 train_time:105061ms step_avg:60.59ms
step:1735/1775 train_time:105148ms step_avg:60.60ms
step:1736/1775 train_time:105242ms step_avg:60.62ms
step:1737/1775 train_time:105329ms step_avg:60.64ms
step:1738/1775 train_time:105418ms step_avg:60.65ms
step:1739/1775 train_time:105504ms step_avg:60.67ms
step:1740/1775 train_time:105591ms step_avg:60.68ms
step:1741/1775 train_time:105679ms step_avg:60.70ms
step:1742/1775 train_time:105768ms step_avg:60.72ms
step:1743/1775 train_time:105855ms step_avg:60.73ms
step:1744/1775 train_time:105943ms step_avg:60.75ms
step:1745/1775 train_time:106030ms step_avg:60.76ms
step:1746/1775 train_time:106119ms step_avg:60.78ms
step:1747/1775 train_time:106204ms step_avg:60.79ms
step:1748/1775 train_time:106295ms step_avg:60.81ms
step:1749/1775 train_time:106381ms step_avg:60.82ms
step:1750/1775 train_time:106471ms step_avg:60.84ms
step:1750/1775 val_loss:3.2847 train_time:106569ms step_avg:60.90ms
step:1751/1775 train_time:106589ms step_avg:60.87ms
step:1752/1775 train_time:106650ms step_avg:60.87ms
step:1753/1775 train_time:106738ms step_avg:60.89ms
step:1754/1775 train_time:106828ms step_avg:60.91ms
step:1755/1775 train_time:106913ms step_avg:60.92ms
step:1756/1775 train_time:107001ms step_avg:60.93ms
step:1757/1775 train_time:107087ms step_avg:60.95ms
step:1758/1775 train_time:107176ms step_avg:60.96ms
step:1759/1775 train_time:107262ms step_avg:60.98ms
step:1760/1775 train_time:107350ms step_avg:60.99ms
step:1761/1775 train_time:107437ms step_avg:61.01ms
step:1762/1775 train_time:107526ms step_avg:61.03ms
step:1763/1775 train_time:107614ms step_avg:61.04ms
step:1764/1775 train_time:107707ms step_avg:61.06ms
step:1765/1775 train_time:107794ms step_avg:61.07ms
step:1766/1775 train_time:107884ms step_avg:61.09ms
step:1767/1775 train_time:107970ms step_avg:61.10ms
step:1768/1775 train_time:108059ms step_avg:61.12ms
step:1769/1775 train_time:108145ms step_avg:61.13ms
step:1770/1775 train_time:108234ms step_avg:61.15ms
step:1771/1775 train_time:108318ms step_avg:61.16ms
step:1772/1775 train_time:108409ms step_avg:61.18ms
step:1773/1775 train_time:108497ms step_avg:61.19ms
step:1774/1775 train_time:108586ms step_avg:61.21ms
step:1775/1775 train_time:108675ms step_avg:61.23ms
step:1775/1775 val_loss:3.2783 train_time:108775ms step_avg:61.28ms
peak memory allocated: 29148 MiB reserved: 45018 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Jan  7 09:08:27 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   44C    P0            128W /  700W |    1520MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   41C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     78072      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     78073      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     78074      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     78075      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     78076      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     78077      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     78078      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     78079      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8294 train_time:0ms step_avg:0.03ms
step:1/1775 train_time:91ms step_avg:90.62ms
step:2/1775 train_time:113ms step_avg:56.34ms
step:3/1775 train_time:132ms step_avg:43.94ms
step:4/1775 train_time:162ms step_avg:40.48ms
step:5/1775 train_time:194ms step_avg:38.75ms
step:6/1775 train_time:284ms step_avg:47.40ms
step:7/1775 train_time:301ms step_avg:43.06ms
step:8/1775 train_time:430ms step_avg:53.71ms
step:9/1775 train_time:462ms step_avg:51.29ms
step:10/1775 train_time:495ms step_avg:49.55ms
step:11/1775 train_time:528ms step_avg:47.97ms
step:12/1775 train_time:562ms step_avg:46.82ms
step:13/1775 train_time:594ms step_avg:45.67ms
step:14/1775 train_time:628ms step_avg:44.86ms
step:15/1775 train_time:660ms step_avg:44.01ms
step:16/1775 train_time:694ms step_avg:43.40ms
step:17/1775 train_time:727ms step_avg:42.74ms
step:18/1775 train_time:761ms step_avg:42.26ms
step:19/1775 train_time:793ms step_avg:41.71ms
step:20/1775 train_time:827ms step_avg:41.33ms
step:21/1775 train_time:859ms step_avg:40.89ms
step:22/1775 train_time:893ms step_avg:40.58ms
step:23/1775 train_time:925ms step_avg:40.22ms
step:24/1775 train_time:959ms step_avg:39.96ms
step:25/1775 train_time:991ms step_avg:39.64ms
step:26/1775 train_time:1025ms step_avg:39.43ms
step:27/1775 train_time:1057ms step_avg:39.15ms
step:28/1775 train_time:1091ms step_avg:38.98ms
step:29/1775 train_time:1124ms step_avg:38.74ms
step:30/1775 train_time:1158ms step_avg:38.59ms
step:31/1775 train_time:1190ms step_avg:38.39ms
step:32/1775 train_time:1224ms step_avg:38.26ms
step:33/1775 train_time:1256ms step_avg:38.07ms
step:34/1775 train_time:1291ms step_avg:37.97ms
step:35/1775 train_time:1324ms step_avg:37.82ms
step:36/1775 train_time:1358ms step_avg:37.73ms
step:37/1775 train_time:1391ms step_avg:37.60ms
step:38/1775 train_time:1426ms step_avg:37.53ms
step:39/1775 train_time:1459ms step_avg:37.40ms
step:40/1775 train_time:1493ms step_avg:37.34ms
step:41/1775 train_time:1526ms step_avg:37.22ms
step:42/1775 train_time:1561ms step_avg:37.16ms
step:43/1775 train_time:1593ms step_avg:37.05ms
step:44/1775 train_time:1628ms step_avg:36.99ms
step:45/1775 train_time:1660ms step_avg:36.90ms
step:46/1775 train_time:1695ms step_avg:36.84ms
step:47/1775 train_time:1727ms step_avg:36.74ms
step:48/1775 train_time:1761ms step_avg:36.69ms
step:49/1775 train_time:1793ms step_avg:36.60ms
step:50/1775 train_time:1828ms step_avg:36.55ms
step:51/1775 train_time:1860ms step_avg:36.47ms
step:52/1775 train_time:1894ms step_avg:36.42ms
step:53/1775 train_time:1926ms step_avg:36.34ms
step:54/1775 train_time:1960ms step_avg:36.30ms
step:55/1775 train_time:1992ms step_avg:36.22ms
step:56/1775 train_time:2026ms step_avg:36.19ms
step:57/1775 train_time:2059ms step_avg:36.12ms
step:58/1775 train_time:2093ms step_avg:36.09ms
step:59/1775 train_time:2126ms step_avg:36.03ms
step:60/1775 train_time:2160ms step_avg:35.99ms
step:61/1775 train_time:2192ms step_avg:35.93ms
step:62/1775 train_time:2226ms step_avg:35.91ms
step:63/1775 train_time:2259ms step_avg:35.85ms
step:64/1775 train_time:2293ms step_avg:35.83ms
step:65/1775 train_time:2326ms step_avg:35.78ms
step:66/1775 train_time:2360ms step_avg:35.76ms
step:67/1775 train_time:2392ms step_avg:35.71ms
step:68/1775 train_time:2427ms step_avg:35.69ms
step:69/1775 train_time:2459ms step_avg:35.64ms
step:70/1775 train_time:2494ms step_avg:35.63ms
step:71/1775 train_time:2526ms step_avg:35.58ms
step:72/1775 train_time:2560ms step_avg:35.56ms
step:73/1775 train_time:2593ms step_avg:35.52ms
step:74/1775 train_time:2627ms step_avg:35.50ms
step:75/1775 train_time:2660ms step_avg:35.46ms
step:76/1775 train_time:2694ms step_avg:35.44ms
step:77/1775 train_time:2726ms step_avg:35.40ms
step:78/1775 train_time:2761ms step_avg:35.39ms
step:79/1775 train_time:2793ms step_avg:35.35ms
step:80/1775 train_time:2827ms step_avg:35.34ms
step:81/1775 train_time:2859ms step_avg:35.30ms
step:82/1775 train_time:2893ms step_avg:35.28ms
step:83/1775 train_time:2925ms step_avg:35.25ms
step:84/1775 train_time:2959ms step_avg:35.23ms
step:85/1775 train_time:2991ms step_avg:35.19ms
step:86/1775 train_time:3025ms step_avg:35.18ms
step:87/1775 train_time:3058ms step_avg:35.15ms
step:88/1775 train_time:3092ms step_avg:35.14ms
step:89/1775 train_time:3124ms step_avg:35.10ms
step:90/1775 train_time:3158ms step_avg:35.09ms
step:91/1775 train_time:3191ms step_avg:35.06ms
step:92/1775 train_time:3225ms step_avg:35.05ms
step:93/1775 train_time:3257ms step_avg:35.02ms
step:94/1775 train_time:3291ms step_avg:35.01ms
step:95/1775 train_time:3324ms step_avg:34.99ms
step:96/1775 train_time:3358ms step_avg:34.98ms
step:97/1775 train_time:3390ms step_avg:34.95ms
step:98/1775 train_time:3425ms step_avg:34.94ms
step:99/1775 train_time:3457ms step_avg:34.92ms
step:100/1775 train_time:3491ms step_avg:34.91ms
step:101/1775 train_time:3523ms step_avg:34.88ms
step:102/1775 train_time:3557ms step_avg:34.87ms
step:103/1775 train_time:3589ms step_avg:34.85ms
step:104/1775 train_time:3624ms step_avg:34.84ms
step:105/1775 train_time:3656ms step_avg:34.82ms
step:106/1775 train_time:3690ms step_avg:34.81ms
step:107/1775 train_time:3722ms step_avg:34.79ms
step:108/1775 train_time:3756ms step_avg:34.78ms
step:109/1775 train_time:3789ms step_avg:34.76ms
step:110/1775 train_time:3823ms step_avg:34.75ms
step:111/1775 train_time:3855ms step_avg:34.73ms
step:112/1775 train_time:3889ms step_avg:34.73ms
step:113/1775 train_time:3921ms step_avg:34.70ms
step:114/1775 train_time:3955ms step_avg:34.70ms
step:115/1775 train_time:3988ms step_avg:34.67ms
step:116/1775 train_time:4022ms step_avg:34.67ms
step:117/1775 train_time:4054ms step_avg:34.65ms
step:118/1775 train_time:4089ms step_avg:34.65ms
step:119/1775 train_time:4121ms step_avg:34.63ms
step:120/1775 train_time:4155ms step_avg:34.63ms
step:121/1775 train_time:4187ms step_avg:34.61ms
step:122/1775 train_time:4221ms step_avg:34.60ms
step:123/1775 train_time:4253ms step_avg:34.58ms
step:124/1775 train_time:4288ms step_avg:34.58ms
step:125/1775 train_time:4320ms step_avg:34.56ms
step:126/1775 train_time:4354ms step_avg:34.56ms
step:127/1775 train_time:4387ms step_avg:34.54ms
step:128/1775 train_time:4421ms step_avg:34.54ms
step:129/1775 train_time:4453ms step_avg:34.52ms
step:130/1775 train_time:4487ms step_avg:34.52ms
step:131/1775 train_time:4520ms step_avg:34.50ms
step:132/1775 train_time:4554ms step_avg:34.50ms
step:133/1775 train_time:4586ms step_avg:34.48ms
step:134/1775 train_time:4621ms step_avg:34.48ms
step:135/1775 train_time:4653ms step_avg:34.47ms
step:136/1775 train_time:4687ms step_avg:34.47ms
step:137/1775 train_time:4719ms step_avg:34.45ms
step:138/1775 train_time:4754ms step_avg:34.45ms
step:139/1775 train_time:4785ms step_avg:34.43ms
step:140/1775 train_time:4820ms step_avg:34.43ms
step:141/1775 train_time:4852ms step_avg:34.41ms
step:142/1775 train_time:4886ms step_avg:34.41ms
step:143/1775 train_time:4918ms step_avg:34.39ms
step:144/1775 train_time:4953ms step_avg:34.39ms
step:145/1775 train_time:4985ms step_avg:34.38ms
step:146/1775 train_time:5019ms step_avg:34.37ms
step:147/1775 train_time:5051ms step_avg:34.36ms
step:148/1775 train_time:5085ms step_avg:34.36ms
step:149/1775 train_time:5117ms step_avg:34.34ms
step:150/1775 train_time:5151ms step_avg:34.34ms
step:151/1775 train_time:5183ms step_avg:34.33ms
step:152/1775 train_time:5217ms step_avg:34.32ms
step:153/1775 train_time:5249ms step_avg:34.31ms
step:154/1775 train_time:5284ms step_avg:34.31ms
step:155/1775 train_time:5316ms step_avg:34.29ms
step:156/1775 train_time:5350ms step_avg:34.29ms
step:157/1775 train_time:5382ms step_avg:34.28ms
step:158/1775 train_time:5416ms step_avg:34.28ms
step:159/1775 train_time:5448ms step_avg:34.26ms
step:160/1775 train_time:5482ms step_avg:34.26ms
step:161/1775 train_time:5514ms step_avg:34.25ms
step:162/1775 train_time:5549ms step_avg:34.25ms
step:163/1775 train_time:5581ms step_avg:34.24ms
step:164/1775 train_time:5615ms step_avg:34.24ms
step:165/1775 train_time:5647ms step_avg:34.23ms
step:166/1775 train_time:5682ms step_avg:34.23ms
step:167/1775 train_time:5714ms step_avg:34.21ms
step:168/1775 train_time:5748ms step_avg:34.21ms
step:169/1775 train_time:5780ms step_avg:34.20ms
step:170/1775 train_time:5814ms step_avg:34.20ms
step:171/1775 train_time:5846ms step_avg:34.19ms
step:172/1775 train_time:5880ms step_avg:34.19ms
step:173/1775 train_time:5912ms step_avg:34.17ms
step:174/1775 train_time:5947ms step_avg:34.18ms
step:175/1775 train_time:5979ms step_avg:34.16ms
step:176/1775 train_time:6013ms step_avg:34.16ms
step:177/1775 train_time:6045ms step_avg:34.15ms
step:178/1775 train_time:6079ms step_avg:34.15ms
step:179/1775 train_time:6111ms step_avg:34.14ms
step:180/1775 train_time:6145ms step_avg:34.14ms
step:181/1775 train_time:6177ms step_avg:34.13ms
step:182/1775 train_time:6211ms step_avg:34.13ms
step:183/1775 train_time:6243ms step_avg:34.12ms
step:184/1775 train_time:6277ms step_avg:34.12ms
step:185/1775 train_time:6310ms step_avg:34.11ms
step:186/1775 train_time:6344ms step_avg:34.11ms
step:187/1775 train_time:6376ms step_avg:34.10ms
step:188/1775 train_time:6410ms step_avg:34.10ms
step:189/1775 train_time:6442ms step_avg:34.08ms
step:190/1775 train_time:6476ms step_avg:34.08ms
step:191/1775 train_time:6508ms step_avg:34.07ms
step:192/1775 train_time:6542ms step_avg:34.08ms
step:193/1775 train_time:6574ms step_avg:34.06ms
step:194/1775 train_time:6609ms step_avg:34.07ms
step:195/1775 train_time:6641ms step_avg:34.06ms
step:196/1775 train_time:6675ms step_avg:34.06ms
step:197/1775 train_time:6707ms step_avg:34.04ms
step:198/1775 train_time:6741ms step_avg:34.04ms
step:199/1775 train_time:6773ms step_avg:34.03ms
step:200/1775 train_time:6807ms step_avg:34.03ms
step:201/1775 train_time:6839ms step_avg:34.02ms
step:202/1775 train_time:6873ms step_avg:34.02ms
step:203/1775 train_time:6905ms step_avg:34.02ms
step:204/1775 train_time:6940ms step_avg:34.02ms
step:205/1775 train_time:6972ms step_avg:34.01ms
step:206/1775 train_time:7006ms step_avg:34.01ms
step:207/1775 train_time:7038ms step_avg:34.00ms
step:208/1775 train_time:7072ms step_avg:34.00ms
step:209/1775 train_time:7104ms step_avg:33.99ms
step:210/1775 train_time:7138ms step_avg:33.99ms
step:211/1775 train_time:7170ms step_avg:33.98ms
step:212/1775 train_time:7204ms step_avg:33.98ms
step:213/1775 train_time:7236ms step_avg:33.97ms
step:214/1775 train_time:7270ms step_avg:33.97ms
step:215/1775 train_time:7302ms step_avg:33.96ms
step:216/1775 train_time:7336ms step_avg:33.96ms
step:217/1775 train_time:7368ms step_avg:33.96ms
step:218/1775 train_time:7403ms step_avg:33.96ms
step:219/1775 train_time:7435ms step_avg:33.95ms
step:220/1775 train_time:7469ms step_avg:33.95ms
step:221/1775 train_time:7501ms step_avg:33.94ms
step:222/1775 train_time:7535ms step_avg:33.94ms
step:223/1775 train_time:7567ms step_avg:33.93ms
step:224/1775 train_time:7601ms step_avg:33.93ms
step:225/1775 train_time:7633ms step_avg:33.92ms
step:226/1775 train_time:7667ms step_avg:33.93ms
step:227/1775 train_time:7699ms step_avg:33.92ms
step:228/1775 train_time:7733ms step_avg:33.92ms
step:229/1775 train_time:7765ms step_avg:33.91ms
step:230/1775 train_time:7799ms step_avg:33.91ms
step:231/1775 train_time:7831ms step_avg:33.90ms
step:232/1775 train_time:7865ms step_avg:33.90ms
step:233/1775 train_time:7897ms step_avg:33.89ms
step:234/1775 train_time:7932ms step_avg:33.90ms
step:235/1775 train_time:7964ms step_avg:33.89ms
step:236/1775 train_time:7998ms step_avg:33.89ms
step:237/1775 train_time:8030ms step_avg:33.88ms
step:238/1775 train_time:8064ms step_avg:33.88ms
step:239/1775 train_time:8096ms step_avg:33.87ms
step:240/1775 train_time:8130ms step_avg:33.88ms
step:241/1775 train_time:8162ms step_avg:33.87ms
step:242/1775 train_time:8196ms step_avg:33.87ms
step:243/1775 train_time:8228ms step_avg:33.86ms
step:244/1775 train_time:8262ms step_avg:33.86ms
step:245/1775 train_time:8294ms step_avg:33.85ms
step:246/1775 train_time:8328ms step_avg:33.85ms
step:247/1775 train_time:8360ms step_avg:33.85ms
step:248/1775 train_time:8395ms step_avg:33.85ms
step:249/1775 train_time:8427ms step_avg:33.84ms
step:250/1775 train_time:8461ms step_avg:33.84ms
step:250/1775 val_loss:4.6143 train_time:8502ms step_avg:34.01ms
step:251/1775 train_time:8520ms step_avg:33.95ms
step:252/1775 train_time:8539ms step_avg:33.88ms
step:253/1775 train_time:8562ms step_avg:33.84ms
step:254/1775 train_time:8598ms step_avg:33.85ms
step:255/1775 train_time:8630ms step_avg:33.84ms
step:256/1775 train_time:8665ms step_avg:33.85ms
step:257/1775 train_time:8696ms step_avg:33.84ms
step:258/1775 train_time:8730ms step_avg:33.84ms
step:259/1775 train_time:8762ms step_avg:33.83ms
step:260/1775 train_time:8797ms step_avg:33.83ms
step:261/1775 train_time:8829ms step_avg:33.83ms
step:262/1775 train_time:8863ms step_avg:33.83ms
step:263/1775 train_time:8894ms step_avg:33.82ms
step:264/1775 train_time:8928ms step_avg:33.82ms
step:265/1775 train_time:8960ms step_avg:33.81ms
step:266/1775 train_time:8995ms step_avg:33.81ms
step:267/1775 train_time:9027ms step_avg:33.81ms
step:268/1775 train_time:9061ms step_avg:33.81ms
step:269/1775 train_time:9092ms step_avg:33.80ms
step:270/1775 train_time:9126ms step_avg:33.80ms
step:271/1775 train_time:9158ms step_avg:33.79ms
step:272/1775 train_time:9192ms step_avg:33.80ms
step:273/1775 train_time:9224ms step_avg:33.79ms
step:274/1775 train_time:9258ms step_avg:33.79ms
step:275/1775 train_time:9291ms step_avg:33.78ms
step:276/1775 train_time:9325ms step_avg:33.78ms
step:277/1775 train_time:9356ms step_avg:33.78ms
step:278/1775 train_time:9390ms step_avg:33.78ms
step:279/1775 train_time:9422ms step_avg:33.77ms
step:280/1775 train_time:9457ms step_avg:33.77ms
step:281/1775 train_time:9489ms step_avg:33.77ms
step:282/1775 train_time:9524ms step_avg:33.77ms
step:283/1775 train_time:9557ms step_avg:33.77ms
step:284/1775 train_time:9592ms step_avg:33.77ms
step:285/1775 train_time:9624ms step_avg:33.77ms
step:286/1775 train_time:9658ms step_avg:33.77ms
step:287/1775 train_time:9691ms step_avg:33.77ms
step:288/1775 train_time:9724ms step_avg:33.77ms
step:289/1775 train_time:9756ms step_avg:33.76ms
step:290/1775 train_time:9791ms step_avg:33.76ms
step:291/1775 train_time:9822ms step_avg:33.75ms
step:292/1775 train_time:9857ms step_avg:33.76ms
step:293/1775 train_time:9889ms step_avg:33.75ms
step:294/1775 train_time:9923ms step_avg:33.75ms
step:295/1775 train_time:9955ms step_avg:33.75ms
step:296/1775 train_time:9989ms step_avg:33.75ms
step:297/1775 train_time:10021ms step_avg:33.74ms
step:298/1775 train_time:10055ms step_avg:33.74ms
step:299/1775 train_time:10087ms step_avg:33.74ms
step:300/1775 train_time:10121ms step_avg:33.74ms
step:301/1775 train_time:10152ms step_avg:33.73ms
step:302/1775 train_time:10186ms step_avg:33.73ms
step:303/1775 train_time:10218ms step_avg:33.72ms
step:304/1775 train_time:10252ms step_avg:33.72ms
step:305/1775 train_time:10284ms step_avg:33.72ms
step:306/1775 train_time:10318ms step_avg:33.72ms
step:307/1775 train_time:10350ms step_avg:33.71ms
step:308/1775 train_time:10384ms step_avg:33.71ms
step:309/1775 train_time:10416ms step_avg:33.71ms
step:310/1775 train_time:10450ms step_avg:33.71ms
step:311/1775 train_time:10482ms step_avg:33.70ms
step:312/1775 train_time:10516ms step_avg:33.71ms
step:313/1775 train_time:10549ms step_avg:33.70ms
step:314/1775 train_time:10583ms step_avg:33.70ms
step:315/1775 train_time:10615ms step_avg:33.70ms
step:316/1775 train_time:10650ms step_avg:33.70ms
step:317/1775 train_time:10682ms step_avg:33.70ms
step:318/1775 train_time:10716ms step_avg:33.70ms
step:319/1775 train_time:10748ms step_avg:33.69ms
step:320/1775 train_time:10782ms step_avg:33.69ms
step:321/1775 train_time:10814ms step_avg:33.69ms
step:322/1775 train_time:10848ms step_avg:33.69ms
step:323/1775 train_time:10881ms step_avg:33.69ms
step:324/1775 train_time:10915ms step_avg:33.69ms
step:325/1775 train_time:10947ms step_avg:33.68ms
step:326/1775 train_time:10981ms step_avg:33.68ms
step:327/1775 train_time:11013ms step_avg:33.68ms
step:328/1775 train_time:11047ms step_avg:33.68ms
step:329/1775 train_time:11079ms step_avg:33.67ms
step:330/1775 train_time:11113ms step_avg:33.68ms
step:331/1775 train_time:11145ms step_avg:33.67ms
step:332/1775 train_time:11180ms step_avg:33.67ms
step:333/1775 train_time:11211ms step_avg:33.67ms
step:334/1775 train_time:11245ms step_avg:33.67ms
step:335/1775 train_time:11277ms step_avg:33.66ms
step:336/1775 train_time:11311ms step_avg:33.66ms
step:337/1775 train_time:11343ms step_avg:33.66ms
step:338/1775 train_time:11377ms step_avg:33.66ms
step:339/1775 train_time:11409ms step_avg:33.66ms
step:340/1775 train_time:11443ms step_avg:33.66ms
step:341/1775 train_time:11475ms step_avg:33.65ms
step:342/1775 train_time:11509ms step_avg:33.65ms
step:343/1775 train_time:11541ms step_avg:33.65ms
step:344/1775 train_time:11576ms step_avg:33.65ms
step:345/1775 train_time:11608ms step_avg:33.65ms
step:346/1775 train_time:11642ms step_avg:33.65ms
step:347/1775 train_time:11674ms step_avg:33.64ms
step:348/1775 train_time:11708ms step_avg:33.64ms
step:349/1775 train_time:11740ms step_avg:33.64ms
step:350/1775 train_time:11774ms step_avg:33.64ms
step:351/1775 train_time:11806ms step_avg:33.64ms
step:352/1775 train_time:11841ms step_avg:33.64ms
step:353/1775 train_time:11873ms step_avg:33.63ms
step:354/1775 train_time:11907ms step_avg:33.64ms
step:355/1775 train_time:11939ms step_avg:33.63ms
step:356/1775 train_time:11973ms step_avg:33.63ms
step:357/1775 train_time:12005ms step_avg:33.63ms
step:358/1775 train_time:12039ms step_avg:33.63ms
step:359/1775 train_time:12071ms step_avg:33.62ms
step:360/1775 train_time:12105ms step_avg:33.62ms
step:361/1775 train_time:12137ms step_avg:33.62ms
step:362/1775 train_time:12171ms step_avg:33.62ms
step:363/1775 train_time:12203ms step_avg:33.62ms
step:364/1775 train_time:12237ms step_avg:33.62ms
step:365/1775 train_time:12269ms step_avg:33.61ms
step:366/1775 train_time:12303ms step_avg:33.62ms
step:367/1775 train_time:12335ms step_avg:33.61ms
step:368/1775 train_time:12369ms step_avg:33.61ms
step:369/1775 train_time:12402ms step_avg:33.61ms
step:370/1775 train_time:12436ms step_avg:33.61ms
step:371/1775 train_time:12468ms step_avg:33.61ms
step:372/1775 train_time:12502ms step_avg:33.61ms
step:373/1775 train_time:12534ms step_avg:33.60ms
step:374/1775 train_time:12568ms step_avg:33.61ms
step:375/1775 train_time:12600ms step_avg:33.60ms
step:376/1775 train_time:12634ms step_avg:33.60ms
step:377/1775 train_time:12666ms step_avg:33.60ms
step:378/1775 train_time:12701ms step_avg:33.60ms
step:379/1775 train_time:12733ms step_avg:33.60ms
step:380/1775 train_time:12767ms step_avg:33.60ms
step:381/1775 train_time:12799ms step_avg:33.59ms
step:382/1775 train_time:12833ms step_avg:33.59ms
step:383/1775 train_time:12865ms step_avg:33.59ms
step:384/1775 train_time:12899ms step_avg:33.59ms
step:385/1775 train_time:12931ms step_avg:33.59ms
step:386/1775 train_time:12965ms step_avg:33.59ms
step:387/1775 train_time:12997ms step_avg:33.58ms
step:388/1775 train_time:13032ms step_avg:33.59ms
step:389/1775 train_time:13063ms step_avg:33.58ms
step:390/1775 train_time:13097ms step_avg:33.58ms
step:391/1775 train_time:13130ms step_avg:33.58ms
step:392/1775 train_time:13164ms step_avg:33.58ms
step:393/1775 train_time:13196ms step_avg:33.58ms
step:394/1775 train_time:13230ms step_avg:33.58ms
step:395/1775 train_time:13262ms step_avg:33.57ms
step:396/1775 train_time:13296ms step_avg:33.58ms
step:397/1775 train_time:13328ms step_avg:33.57ms
step:398/1775 train_time:13362ms step_avg:33.57ms
step:399/1775 train_time:13394ms step_avg:33.57ms
step:400/1775 train_time:13428ms step_avg:33.57ms
step:401/1775 train_time:13460ms step_avg:33.57ms
step:402/1775 train_time:13494ms step_avg:33.57ms
step:403/1775 train_time:13526ms step_avg:33.56ms
step:404/1775 train_time:13560ms step_avg:33.56ms
step:405/1775 train_time:13592ms step_avg:33.56ms
step:406/1775 train_time:13626ms step_avg:33.56ms
step:407/1775 train_time:13658ms step_avg:33.56ms
step:408/1775 train_time:13692ms step_avg:33.56ms
step:409/1775 train_time:13724ms step_avg:33.55ms
step:410/1775 train_time:13758ms step_avg:33.56ms
step:411/1775 train_time:13790ms step_avg:33.55ms
step:412/1775 train_time:13824ms step_avg:33.55ms
step:413/1775 train_time:13857ms step_avg:33.55ms
step:414/1775 train_time:13891ms step_avg:33.55ms
step:415/1775 train_time:13923ms step_avg:33.55ms
step:416/1775 train_time:13958ms step_avg:33.55ms
step:417/1775 train_time:13990ms step_avg:33.55ms
step:418/1775 train_time:14024ms step_avg:33.55ms
step:419/1775 train_time:14057ms step_avg:33.55ms
step:420/1775 train_time:14091ms step_avg:33.55ms
step:421/1775 train_time:14123ms step_avg:33.55ms
step:422/1775 train_time:14157ms step_avg:33.55ms
step:423/1775 train_time:14189ms step_avg:33.54ms
step:424/1775 train_time:14223ms step_avg:33.54ms
step:425/1775 train_time:14255ms step_avg:33.54ms
step:426/1775 train_time:14289ms step_avg:33.54ms
step:427/1775 train_time:14321ms step_avg:33.54ms
step:428/1775 train_time:14355ms step_avg:33.54ms
step:429/1775 train_time:14387ms step_avg:33.54ms
step:430/1775 train_time:14421ms step_avg:33.54ms
step:431/1775 train_time:14453ms step_avg:33.53ms
step:432/1775 train_time:14487ms step_avg:33.54ms
step:433/1775 train_time:14519ms step_avg:33.53ms
step:434/1775 train_time:14553ms step_avg:33.53ms
step:435/1775 train_time:14585ms step_avg:33.53ms
step:436/1775 train_time:14619ms step_avg:33.53ms
step:437/1775 train_time:14651ms step_avg:33.53ms
step:438/1775 train_time:14685ms step_avg:33.53ms
step:439/1775 train_time:14717ms step_avg:33.52ms
step:440/1775 train_time:14752ms step_avg:33.53ms
step:441/1775 train_time:14784ms step_avg:33.52ms
step:442/1775 train_time:14818ms step_avg:33.53ms
step:443/1775 train_time:14851ms step_avg:33.52ms
step:444/1775 train_time:14885ms step_avg:33.52ms
step:445/1775 train_time:14917ms step_avg:33.52ms
step:446/1775 train_time:14951ms step_avg:33.52ms
step:447/1775 train_time:14983ms step_avg:33.52ms
step:448/1775 train_time:15017ms step_avg:33.52ms
step:449/1775 train_time:15050ms step_avg:33.52ms
step:450/1775 train_time:15083ms step_avg:33.52ms
step:451/1775 train_time:15115ms step_avg:33.52ms
step:452/1775 train_time:15150ms step_avg:33.52ms
step:453/1775 train_time:15182ms step_avg:33.51ms
step:454/1775 train_time:15216ms step_avg:33.52ms
step:455/1775 train_time:15248ms step_avg:33.51ms
step:456/1775 train_time:15282ms step_avg:33.51ms
step:457/1775 train_time:15314ms step_avg:33.51ms
step:458/1775 train_time:15348ms step_avg:33.51ms
step:459/1775 train_time:15380ms step_avg:33.51ms
step:460/1775 train_time:15414ms step_avg:33.51ms
step:461/1775 train_time:15446ms step_avg:33.51ms
step:462/1775 train_time:15480ms step_avg:33.51ms
step:463/1775 train_time:15512ms step_avg:33.50ms
step:464/1775 train_time:15546ms step_avg:33.50ms
step:465/1775 train_time:15578ms step_avg:33.50ms
step:466/1775 train_time:15612ms step_avg:33.50ms
step:467/1775 train_time:15644ms step_avg:33.50ms
step:468/1775 train_time:15679ms step_avg:33.50ms
step:469/1775 train_time:15711ms step_avg:33.50ms
step:470/1775 train_time:15745ms step_avg:33.50ms
step:471/1775 train_time:15777ms step_avg:33.50ms
step:472/1775 train_time:15811ms step_avg:33.50ms
step:473/1775 train_time:15843ms step_avg:33.49ms
step:474/1775 train_time:15877ms step_avg:33.50ms
step:475/1775 train_time:15909ms step_avg:33.49ms
step:476/1775 train_time:15943ms step_avg:33.49ms
step:477/1775 train_time:15975ms step_avg:33.49ms
step:478/1775 train_time:16009ms step_avg:33.49ms
step:479/1775 train_time:16041ms step_avg:33.49ms
step:480/1775 train_time:16076ms step_avg:33.49ms
step:481/1775 train_time:16108ms step_avg:33.49ms
step:482/1775 train_time:16142ms step_avg:33.49ms
step:483/1775 train_time:16174ms step_avg:33.49ms
step:484/1775 train_time:16208ms step_avg:33.49ms
step:485/1775 train_time:16240ms step_avg:33.49ms
step:486/1775 train_time:16274ms step_avg:33.49ms
step:487/1775 train_time:16306ms step_avg:33.48ms
step:488/1775 train_time:16341ms step_avg:33.49ms
step:489/1775 train_time:16372ms step_avg:33.48ms
step:490/1775 train_time:16406ms step_avg:33.48ms
step:491/1775 train_time:16439ms step_avg:33.48ms
step:492/1775 train_time:16473ms step_avg:33.48ms
step:493/1775 train_time:16504ms step_avg:33.48ms
step:494/1775 train_time:16539ms step_avg:33.48ms
step:495/1775 train_time:16571ms step_avg:33.48ms
step:496/1775 train_time:16605ms step_avg:33.48ms
step:497/1775 train_time:16637ms step_avg:33.47ms
step:498/1775 train_time:16671ms step_avg:33.48ms
step:499/1775 train_time:16703ms step_avg:33.47ms
step:500/1775 train_time:16737ms step_avg:33.47ms
step:500/1775 val_loss:4.2713 train_time:16778ms step_avg:33.56ms
step:501/1775 train_time:16796ms step_avg:33.53ms
step:502/1775 train_time:16814ms step_avg:33.49ms
step:503/1775 train_time:16840ms step_avg:33.48ms
step:504/1775 train_time:16875ms step_avg:33.48ms
step:505/1775 train_time:16907ms step_avg:33.48ms
step:506/1775 train_time:16941ms step_avg:33.48ms
step:507/1775 train_time:16973ms step_avg:33.48ms
step:508/1775 train_time:17008ms step_avg:33.48ms
step:509/1775 train_time:17039ms step_avg:33.48ms
step:510/1775 train_time:17074ms step_avg:33.48ms
step:511/1775 train_time:17106ms step_avg:33.47ms
step:512/1775 train_time:17140ms step_avg:33.48ms
step:513/1775 train_time:17171ms step_avg:33.47ms
step:514/1775 train_time:17205ms step_avg:33.47ms
step:515/1775 train_time:17238ms step_avg:33.47ms
step:516/1775 train_time:17271ms step_avg:33.47ms
step:517/1775 train_time:17303ms step_avg:33.47ms
step:518/1775 train_time:17337ms step_avg:33.47ms
step:519/1775 train_time:17369ms step_avg:33.47ms
step:520/1775 train_time:17403ms step_avg:33.47ms
step:521/1775 train_time:17435ms step_avg:33.46ms
step:522/1775 train_time:17469ms step_avg:33.47ms
step:523/1775 train_time:17501ms step_avg:33.46ms
step:524/1775 train_time:17535ms step_avg:33.46ms
step:525/1775 train_time:17567ms step_avg:33.46ms
step:526/1775 train_time:17601ms step_avg:33.46ms
step:527/1775 train_time:17633ms step_avg:33.46ms
step:528/1775 train_time:17666ms step_avg:33.46ms
step:529/1775 train_time:17699ms step_avg:33.46ms
step:530/1775 train_time:17734ms step_avg:33.46ms
step:531/1775 train_time:17766ms step_avg:33.46ms
step:532/1775 train_time:17801ms step_avg:33.46ms
step:533/1775 train_time:17834ms step_avg:33.46ms
step:534/1775 train_time:17868ms step_avg:33.46ms
step:535/1775 train_time:17900ms step_avg:33.46ms
step:536/1775 train_time:17935ms step_avg:33.46ms
step:537/1775 train_time:17967ms step_avg:33.46ms
step:538/1775 train_time:18001ms step_avg:33.46ms
step:539/1775 train_time:18034ms step_avg:33.46ms
step:540/1775 train_time:18068ms step_avg:33.46ms
step:541/1775 train_time:18100ms step_avg:33.46ms
step:542/1775 train_time:18134ms step_avg:33.46ms
step:543/1775 train_time:18166ms step_avg:33.46ms
step:544/1775 train_time:18200ms step_avg:33.46ms
step:545/1775 train_time:18232ms step_avg:33.45ms
step:546/1775 train_time:18267ms step_avg:33.46ms
step:547/1775 train_time:18298ms step_avg:33.45ms
step:548/1775 train_time:18333ms step_avg:33.45ms
step:549/1775 train_time:18365ms step_avg:33.45ms
step:550/1775 train_time:18399ms step_avg:33.45ms
step:551/1775 train_time:18431ms step_avg:33.45ms
step:552/1775 train_time:18465ms step_avg:33.45ms
step:553/1775 train_time:18497ms step_avg:33.45ms
step:554/1775 train_time:18531ms step_avg:33.45ms
step:555/1775 train_time:18563ms step_avg:33.45ms
step:556/1775 train_time:18597ms step_avg:33.45ms
step:557/1775 train_time:18629ms step_avg:33.45ms
step:558/1775 train_time:18663ms step_avg:33.45ms
step:559/1775 train_time:18695ms step_avg:33.44ms
step:560/1775 train_time:18730ms step_avg:33.45ms
step:561/1775 train_time:18762ms step_avg:33.44ms
step:562/1775 train_time:18796ms step_avg:33.44ms
step:563/1775 train_time:18828ms step_avg:33.44ms
step:564/1775 train_time:18862ms step_avg:33.44ms
step:565/1775 train_time:18894ms step_avg:33.44ms
step:566/1775 train_time:18928ms step_avg:33.44ms
step:567/1775 train_time:18960ms step_avg:33.44ms
step:568/1775 train_time:18995ms step_avg:33.44ms
step:569/1775 train_time:19027ms step_avg:33.44ms
step:570/1775 train_time:19061ms step_avg:33.44ms
step:571/1775 train_time:19094ms step_avg:33.44ms
step:572/1775 train_time:19128ms step_avg:33.44ms
step:573/1775 train_time:19160ms step_avg:33.44ms
step:574/1775 train_time:19194ms step_avg:33.44ms
step:575/1775 train_time:19226ms step_avg:33.44ms
step:576/1775 train_time:19260ms step_avg:33.44ms
step:577/1775 train_time:19292ms step_avg:33.44ms
step:578/1775 train_time:19326ms step_avg:33.44ms
step:579/1775 train_time:19358ms step_avg:33.43ms
step:580/1775 train_time:19395ms step_avg:33.44ms
step:581/1775 train_time:19456ms step_avg:33.49ms
step:582/1775 train_time:19517ms step_avg:33.53ms
step:583/1775 train_time:19576ms step_avg:33.58ms
step:584/1775 train_time:19638ms step_avg:33.63ms
step:585/1775 train_time:19698ms step_avg:33.67ms
step:586/1775 train_time:19759ms step_avg:33.72ms
step:587/1775 train_time:19820ms step_avg:33.76ms
step:588/1775 train_time:19881ms step_avg:33.81ms
step:589/1775 train_time:19941ms step_avg:33.86ms
step:590/1775 train_time:20004ms step_avg:33.91ms
step:591/1775 train_time:20063ms step_avg:33.95ms
step:592/1775 train_time:20125ms step_avg:34.00ms
step:593/1775 train_time:20185ms step_avg:34.04ms
step:594/1775 train_time:20248ms step_avg:34.09ms
step:595/1775 train_time:20307ms step_avg:34.13ms
step:596/1775 train_time:20369ms step_avg:34.18ms
step:597/1775 train_time:20430ms step_avg:34.22ms
step:598/1775 train_time:20492ms step_avg:34.27ms
step:599/1775 train_time:20552ms step_avg:34.31ms
step:600/1775 train_time:20615ms step_avg:34.36ms
step:601/1775 train_time:20675ms step_avg:34.40ms
step:602/1775 train_time:20737ms step_avg:34.45ms
step:603/1775 train_time:20797ms step_avg:34.49ms
step:604/1775 train_time:20858ms step_avg:34.53ms
step:605/1775 train_time:20918ms step_avg:34.58ms
step:606/1775 train_time:20980ms step_avg:34.62ms
step:607/1775 train_time:21040ms step_avg:34.66ms
step:608/1775 train_time:21103ms step_avg:34.71ms
step:609/1775 train_time:21162ms step_avg:34.75ms
step:610/1775 train_time:21225ms step_avg:34.79ms
step:611/1775 train_time:21283ms step_avg:34.83ms
step:612/1775 train_time:21346ms step_avg:34.88ms
step:613/1775 train_time:21405ms step_avg:34.92ms
step:614/1775 train_time:21467ms step_avg:34.96ms
step:615/1775 train_time:21527ms step_avg:35.00ms
step:616/1775 train_time:21590ms step_avg:35.05ms
step:617/1775 train_time:21651ms step_avg:35.09ms
step:618/1775 train_time:21714ms step_avg:35.14ms
step:619/1775 train_time:21774ms step_avg:35.18ms
step:620/1775 train_time:21836ms step_avg:35.22ms
step:621/1775 train_time:21896ms step_avg:35.26ms
step:622/1775 train_time:21957ms step_avg:35.30ms
step:623/1775 train_time:22018ms step_avg:35.34ms
step:624/1775 train_time:22079ms step_avg:35.38ms
step:625/1775 train_time:22140ms step_avg:35.42ms
step:626/1775 train_time:22202ms step_avg:35.47ms
step:627/1775 train_time:22262ms step_avg:35.51ms
step:628/1775 train_time:22324ms step_avg:35.55ms
step:629/1775 train_time:22383ms step_avg:35.58ms
step:630/1775 train_time:22446ms step_avg:35.63ms
step:631/1775 train_time:22505ms step_avg:35.67ms
step:632/1775 train_time:22568ms step_avg:35.71ms
step:633/1775 train_time:22627ms step_avg:35.75ms
step:634/1775 train_time:22690ms step_avg:35.79ms
step:635/1775 train_time:22750ms step_avg:35.83ms
step:636/1775 train_time:22813ms step_avg:35.87ms
step:637/1775 train_time:22873ms step_avg:35.91ms
step:638/1775 train_time:22936ms step_avg:35.95ms
step:639/1775 train_time:22995ms step_avg:35.99ms
step:640/1775 train_time:23057ms step_avg:36.03ms
step:641/1775 train_time:23117ms step_avg:36.06ms
step:642/1775 train_time:23179ms step_avg:36.10ms
step:643/1775 train_time:23238ms step_avg:36.14ms
step:644/1775 train_time:23300ms step_avg:36.18ms
step:645/1775 train_time:23360ms step_avg:36.22ms
step:646/1775 train_time:23423ms step_avg:36.26ms
step:647/1775 train_time:23483ms step_avg:36.30ms
step:648/1775 train_time:23545ms step_avg:36.34ms
step:649/1775 train_time:23605ms step_avg:36.37ms
step:650/1775 train_time:23667ms step_avg:36.41ms
step:651/1775 train_time:23727ms step_avg:36.45ms
step:652/1775 train_time:23789ms step_avg:36.49ms
step:653/1775 train_time:23849ms step_avg:36.52ms
step:654/1775 train_time:23911ms step_avg:36.56ms
step:655/1775 train_time:23972ms step_avg:36.60ms
step:656/1775 train_time:24035ms step_avg:36.64ms
step:657/1775 train_time:24095ms step_avg:36.67ms
step:658/1775 train_time:24157ms step_avg:36.71ms
step:659/1775 train_time:24216ms step_avg:36.75ms
step:660/1775 train_time:24279ms step_avg:36.79ms
step:661/1775 train_time:24338ms step_avg:36.82ms
step:662/1775 train_time:24399ms step_avg:36.86ms
step:663/1775 train_time:24459ms step_avg:36.89ms
step:664/1775 train_time:24521ms step_avg:36.93ms
step:665/1775 train_time:24581ms step_avg:36.96ms
step:666/1775 train_time:24644ms step_avg:37.00ms
step:667/1775 train_time:24704ms step_avg:37.04ms
step:668/1775 train_time:24766ms step_avg:37.07ms
step:669/1775 train_time:24826ms step_avg:37.11ms
step:670/1775 train_time:24888ms step_avg:37.15ms
step:671/1775 train_time:24949ms step_avg:37.18ms
step:672/1775 train_time:25011ms step_avg:37.22ms
step:673/1775 train_time:25071ms step_avg:37.25ms
step:674/1775 train_time:25135ms step_avg:37.29ms
step:675/1775 train_time:25195ms step_avg:37.33ms
step:676/1775 train_time:25257ms step_avg:37.36ms
step:677/1775 train_time:25316ms step_avg:37.39ms
step:678/1775 train_time:25377ms step_avg:37.43ms
step:679/1775 train_time:25437ms step_avg:37.46ms
step:680/1775 train_time:25499ms step_avg:37.50ms
step:681/1775 train_time:25558ms step_avg:37.53ms
step:682/1775 train_time:25621ms step_avg:37.57ms
step:683/1775 train_time:25680ms step_avg:37.60ms
step:684/1775 train_time:25743ms step_avg:37.64ms
step:685/1775 train_time:25803ms step_avg:37.67ms
step:686/1775 train_time:25865ms step_avg:37.70ms
step:687/1775 train_time:25924ms step_avg:37.74ms
step:688/1775 train_time:25986ms step_avg:37.77ms
step:689/1775 train_time:26046ms step_avg:37.80ms
step:690/1775 train_time:26109ms step_avg:37.84ms
step:691/1775 train_time:26169ms step_avg:37.87ms
step:692/1775 train_time:26232ms step_avg:37.91ms
step:693/1775 train_time:26292ms step_avg:37.94ms
step:694/1775 train_time:26355ms step_avg:37.98ms
step:695/1775 train_time:26414ms step_avg:38.01ms
step:696/1775 train_time:26477ms step_avg:38.04ms
step:697/1775 train_time:26538ms step_avg:38.07ms
step:698/1775 train_time:26600ms step_avg:38.11ms
step:699/1775 train_time:26660ms step_avg:38.14ms
step:700/1775 train_time:26722ms step_avg:38.17ms
step:701/1775 train_time:26781ms step_avg:38.20ms
step:702/1775 train_time:26844ms step_avg:38.24ms
step:703/1775 train_time:26903ms step_avg:38.27ms
step:704/1775 train_time:26965ms step_avg:38.30ms
step:705/1775 train_time:27024ms step_avg:38.33ms
step:706/1775 train_time:27086ms step_avg:38.36ms
step:707/1775 train_time:27145ms step_avg:38.39ms
step:708/1775 train_time:27207ms step_avg:38.43ms
step:709/1775 train_time:27267ms step_avg:38.46ms
step:710/1775 train_time:27330ms step_avg:38.49ms
step:711/1775 train_time:27392ms step_avg:38.53ms
step:712/1775 train_time:27455ms step_avg:38.56ms
step:713/1775 train_time:27516ms step_avg:38.59ms
step:714/1775 train_time:27578ms step_avg:38.62ms
step:715/1775 train_time:27638ms step_avg:38.65ms
step:716/1775 train_time:27700ms step_avg:38.69ms
step:717/1775 train_time:27759ms step_avg:38.72ms
step:718/1775 train_time:27821ms step_avg:38.75ms
step:719/1775 train_time:27880ms step_avg:38.78ms
step:720/1775 train_time:27942ms step_avg:38.81ms
step:721/1775 train_time:28003ms step_avg:38.84ms
step:722/1775 train_time:28064ms step_avg:38.87ms
step:723/1775 train_time:28124ms step_avg:38.90ms
step:724/1775 train_time:28185ms step_avg:38.93ms
step:725/1775 train_time:28245ms step_avg:38.96ms
step:726/1775 train_time:28307ms step_avg:38.99ms
step:727/1775 train_time:28368ms step_avg:39.02ms
step:728/1775 train_time:28431ms step_avg:39.05ms
step:729/1775 train_time:28492ms step_avg:39.08ms
step:730/1775 train_time:28555ms step_avg:39.12ms
step:731/1775 train_time:28615ms step_avg:39.14ms
step:732/1775 train_time:28677ms step_avg:39.18ms
step:733/1775 train_time:28737ms step_avg:39.20ms
step:734/1775 train_time:28799ms step_avg:39.24ms
step:735/1775 train_time:28859ms step_avg:39.26ms
step:736/1775 train_time:28921ms step_avg:39.29ms
step:737/1775 train_time:28980ms step_avg:39.32ms
step:738/1775 train_time:29043ms step_avg:39.35ms
step:739/1775 train_time:29102ms step_avg:39.38ms
step:740/1775 train_time:29164ms step_avg:39.41ms
step:741/1775 train_time:29224ms step_avg:39.44ms
step:742/1775 train_time:29285ms step_avg:39.47ms
step:743/1775 train_time:29345ms step_avg:39.49ms
step:744/1775 train_time:29407ms step_avg:39.53ms
step:745/1775 train_time:29468ms step_avg:39.55ms
step:746/1775 train_time:29531ms step_avg:39.59ms
step:747/1775 train_time:29591ms step_avg:39.61ms
step:748/1775 train_time:29654ms step_avg:39.64ms
step:749/1775 train_time:29715ms step_avg:39.67ms
step:750/1775 train_time:29777ms step_avg:39.70ms
step:750/1775 val_loss:3.9910 train_time:29846ms step_avg:39.79ms
step:751/1775 train_time:29865ms step_avg:39.77ms
step:752/1775 train_time:29900ms step_avg:39.76ms
step:753/1775 train_time:29961ms step_avg:39.79ms
step:754/1775 train_time:30023ms step_avg:39.82ms
step:755/1775 train_time:30084ms step_avg:39.85ms
step:756/1775 train_time:30146ms step_avg:39.88ms
step:757/1775 train_time:30206ms step_avg:39.90ms
step:758/1775 train_time:30268ms step_avg:39.93ms
step:759/1775 train_time:30328ms step_avg:39.96ms
step:760/1775 train_time:30389ms step_avg:39.99ms
step:761/1775 train_time:30449ms step_avg:40.01ms
step:762/1775 train_time:30511ms step_avg:40.04ms
step:763/1775 train_time:30570ms step_avg:40.07ms
step:764/1775 train_time:30632ms step_avg:40.09ms
step:765/1775 train_time:30691ms step_avg:40.12ms
step:766/1775 train_time:30752ms step_avg:40.15ms
step:767/1775 train_time:30812ms step_avg:40.17ms
step:768/1775 train_time:30875ms step_avg:40.20ms
step:769/1775 train_time:30935ms step_avg:40.23ms
step:770/1775 train_time:30999ms step_avg:40.26ms
step:771/1775 train_time:31059ms step_avg:40.28ms
step:772/1775 train_time:31121ms step_avg:40.31ms
step:773/1775 train_time:31181ms step_avg:40.34ms
step:774/1775 train_time:31243ms step_avg:40.37ms
step:775/1775 train_time:31303ms step_avg:40.39ms
step:776/1775 train_time:31366ms step_avg:40.42ms
step:777/1775 train_time:31426ms step_avg:40.44ms
step:778/1775 train_time:31488ms step_avg:40.47ms
step:779/1775 train_time:31548ms step_avg:40.50ms
step:780/1775 train_time:31609ms step_avg:40.52ms
step:781/1775 train_time:31669ms step_avg:40.55ms
step:782/1775 train_time:31731ms step_avg:40.58ms
step:783/1775 train_time:31790ms step_avg:40.60ms
step:784/1775 train_time:31852ms step_avg:40.63ms
step:785/1775 train_time:31913ms step_avg:40.65ms
step:786/1775 train_time:31976ms step_avg:40.68ms
step:787/1775 train_time:32036ms step_avg:40.71ms
step:788/1775 train_time:32098ms step_avg:40.73ms
step:789/1775 train_time:32157ms step_avg:40.76ms
step:790/1775 train_time:32219ms step_avg:40.78ms
step:791/1775 train_time:32279ms step_avg:40.81ms
step:792/1775 train_time:32341ms step_avg:40.83ms
step:793/1775 train_time:32401ms step_avg:40.86ms
step:794/1775 train_time:32463ms step_avg:40.89ms
step:795/1775 train_time:32523ms step_avg:40.91ms
step:796/1775 train_time:32585ms step_avg:40.94ms
step:797/1775 train_time:32645ms step_avg:40.96ms
step:798/1775 train_time:32707ms step_avg:40.99ms
step:799/1775 train_time:32767ms step_avg:41.01ms
step:800/1775 train_time:32830ms step_avg:41.04ms
step:801/1775 train_time:32890ms step_avg:41.06ms
step:802/1775 train_time:32952ms step_avg:41.09ms
step:803/1775 train_time:33013ms step_avg:41.11ms
step:804/1775 train_time:33074ms step_avg:41.14ms
step:805/1775 train_time:33134ms step_avg:41.16ms
step:806/1775 train_time:33196ms step_avg:41.19ms
step:807/1775 train_time:33256ms step_avg:41.21ms
step:808/1775 train_time:33318ms step_avg:41.24ms
step:809/1775 train_time:33378ms step_avg:41.26ms
step:810/1775 train_time:33440ms step_avg:41.28ms
step:811/1775 train_time:33499ms step_avg:41.31ms
step:812/1775 train_time:33561ms step_avg:41.33ms
step:813/1775 train_time:33620ms step_avg:41.35ms
step:814/1775 train_time:33682ms step_avg:41.38ms
step:815/1775 train_time:33744ms step_avg:41.40ms
step:816/1775 train_time:33807ms step_avg:41.43ms
step:817/1775 train_time:33868ms step_avg:41.45ms
step:818/1775 train_time:33930ms step_avg:41.48ms
step:819/1775 train_time:33990ms step_avg:41.50ms
step:820/1775 train_time:34052ms step_avg:41.53ms
step:821/1775 train_time:34112ms step_avg:41.55ms
step:822/1775 train_time:34173ms step_avg:41.57ms
step:823/1775 train_time:34233ms step_avg:41.60ms
step:824/1775 train_time:34295ms step_avg:41.62ms
step:825/1775 train_time:34354ms step_avg:41.64ms
step:826/1775 train_time:34417ms step_avg:41.67ms
step:827/1775 train_time:34476ms step_avg:41.69ms
step:828/1775 train_time:34538ms step_avg:41.71ms
step:829/1775 train_time:34597ms step_avg:41.73ms
step:830/1775 train_time:34659ms step_avg:41.76ms
step:831/1775 train_time:34719ms step_avg:41.78ms
step:832/1775 train_time:34781ms step_avg:41.80ms
step:833/1775 train_time:34842ms step_avg:41.83ms
step:834/1775 train_time:34905ms step_avg:41.85ms
step:835/1775 train_time:34966ms step_avg:41.88ms
step:836/1775 train_time:35028ms step_avg:41.90ms
step:837/1775 train_time:35088ms step_avg:41.92ms
step:838/1775 train_time:35150ms step_avg:41.94ms
step:839/1775 train_time:35210ms step_avg:41.97ms
step:840/1775 train_time:35272ms step_avg:41.99ms
step:841/1775 train_time:35331ms step_avg:42.01ms
step:842/1775 train_time:35393ms step_avg:42.03ms
step:843/1775 train_time:35453ms step_avg:42.06ms
step:844/1775 train_time:35516ms step_avg:42.08ms
step:845/1775 train_time:35575ms step_avg:42.10ms
step:846/1775 train_time:35638ms step_avg:42.13ms
step:847/1775 train_time:35697ms step_avg:42.15ms
step:848/1775 train_time:35759ms step_avg:42.17ms
step:849/1775 train_time:35818ms step_avg:42.19ms
step:850/1775 train_time:35881ms step_avg:42.21ms
step:851/1775 train_time:35942ms step_avg:42.23ms
step:852/1775 train_time:36005ms step_avg:42.26ms
step:853/1775 train_time:36065ms step_avg:42.28ms
step:854/1775 train_time:36127ms step_avg:42.30ms
step:855/1775 train_time:36187ms step_avg:42.32ms
step:856/1775 train_time:36249ms step_avg:42.35ms
step:857/1775 train_time:36309ms step_avg:42.37ms
step:858/1775 train_time:36371ms step_avg:42.39ms
step:859/1775 train_time:36432ms step_avg:42.41ms
step:860/1775 train_time:36494ms step_avg:42.43ms
step:861/1775 train_time:36553ms step_avg:42.45ms
step:862/1775 train_time:36615ms step_avg:42.48ms
step:863/1775 train_time:36674ms step_avg:42.50ms
step:864/1775 train_time:36736ms step_avg:42.52ms
step:865/1775 train_time:36795ms step_avg:42.54ms
step:866/1775 train_time:36858ms step_avg:42.56ms
step:867/1775 train_time:36917ms step_avg:42.58ms
step:868/1775 train_time:36979ms step_avg:42.60ms
step:869/1775 train_time:37039ms step_avg:42.62ms
step:870/1775 train_time:37101ms step_avg:42.65ms
step:871/1775 train_time:37162ms step_avg:42.67ms
step:872/1775 train_time:37225ms step_avg:42.69ms
step:873/1775 train_time:37286ms step_avg:42.71ms
step:874/1775 train_time:37349ms step_avg:42.73ms
step:875/1775 train_time:37409ms step_avg:42.75ms
step:876/1775 train_time:37471ms step_avg:42.78ms
step:877/1775 train_time:37531ms step_avg:42.80ms
step:878/1775 train_time:37593ms step_avg:42.82ms
step:879/1775 train_time:37652ms step_avg:42.84ms
step:880/1775 train_time:37714ms step_avg:42.86ms
step:881/1775 train_time:37774ms step_avg:42.88ms
step:882/1775 train_time:37837ms step_avg:42.90ms
step:883/1775 train_time:37895ms step_avg:42.92ms
step:884/1775 train_time:37958ms step_avg:42.94ms
step:885/1775 train_time:38017ms step_avg:42.96ms
step:886/1775 train_time:38079ms step_avg:42.98ms
step:887/1775 train_time:38138ms step_avg:43.00ms
step:888/1775 train_time:38200ms step_avg:43.02ms
step:889/1775 train_time:38260ms step_avg:43.04ms
step:890/1775 train_time:38323ms step_avg:43.06ms
step:891/1775 train_time:38384ms step_avg:43.08ms
step:892/1775 train_time:38447ms step_avg:43.10ms
step:893/1775 train_time:38507ms step_avg:43.12ms
step:894/1775 train_time:38570ms step_avg:43.14ms
step:895/1775 train_time:38629ms step_avg:43.16ms
step:896/1775 train_time:38691ms step_avg:43.18ms
step:897/1775 train_time:38751ms step_avg:43.20ms
step:898/1775 train_time:38813ms step_avg:43.22ms
step:899/1775 train_time:38873ms step_avg:43.24ms
step:900/1775 train_time:38935ms step_avg:43.26ms
step:901/1775 train_time:38994ms step_avg:43.28ms
step:902/1775 train_time:39057ms step_avg:43.30ms
step:903/1775 train_time:39116ms step_avg:43.32ms
step:904/1775 train_time:39178ms step_avg:43.34ms
step:905/1775 train_time:39238ms step_avg:43.36ms
step:906/1775 train_time:39299ms step_avg:43.38ms
step:907/1775 train_time:39359ms step_avg:43.39ms
step:908/1775 train_time:39421ms step_avg:43.42ms
step:909/1775 train_time:39482ms step_avg:43.43ms
step:910/1775 train_time:39545ms step_avg:43.46ms
step:911/1775 train_time:39605ms step_avg:43.47ms
step:912/1775 train_time:39667ms step_avg:43.50ms
step:913/1775 train_time:39728ms step_avg:43.51ms
step:914/1775 train_time:39790ms step_avg:43.53ms
step:915/1775 train_time:39851ms step_avg:43.55ms
step:916/1775 train_time:39913ms step_avg:43.57ms
step:917/1775 train_time:39972ms step_avg:43.59ms
step:918/1775 train_time:40035ms step_avg:43.61ms
step:919/1775 train_time:40094ms step_avg:43.63ms
step:920/1775 train_time:40156ms step_avg:43.65ms
step:921/1775 train_time:40216ms step_avg:43.67ms
step:922/1775 train_time:40277ms step_avg:43.68ms
step:923/1775 train_time:40336ms step_avg:43.70ms
step:924/1775 train_time:40399ms step_avg:43.72ms
step:925/1775 train_time:40458ms step_avg:43.74ms
step:926/1775 train_time:40521ms step_avg:43.76ms
step:927/1775 train_time:40581ms step_avg:43.78ms
step:928/1775 train_time:40644ms step_avg:43.80ms
step:929/1775 train_time:40705ms step_avg:43.82ms
step:930/1775 train_time:40768ms step_avg:43.84ms
step:931/1775 train_time:40828ms step_avg:43.85ms
step:932/1775 train_time:40891ms step_avg:43.87ms
step:933/1775 train_time:40951ms step_avg:43.89ms
step:934/1775 train_time:41013ms step_avg:43.91ms
step:935/1775 train_time:41072ms step_avg:43.93ms
step:936/1775 train_time:41135ms step_avg:43.95ms
step:937/1775 train_time:41194ms step_avg:43.96ms
step:938/1775 train_time:41255ms step_avg:43.98ms
step:939/1775 train_time:41315ms step_avg:44.00ms
step:940/1775 train_time:41377ms step_avg:44.02ms
step:941/1775 train_time:41436ms step_avg:44.03ms
step:942/1775 train_time:41498ms step_avg:44.05ms
step:943/1775 train_time:41557ms step_avg:44.07ms
step:944/1775 train_time:41620ms step_avg:44.09ms
step:945/1775 train_time:41680ms step_avg:44.11ms
step:946/1775 train_time:41742ms step_avg:44.13ms
step:947/1775 train_time:41804ms step_avg:44.14ms
step:948/1775 train_time:41866ms step_avg:44.16ms
step:949/1775 train_time:41927ms step_avg:44.18ms
step:950/1775 train_time:41989ms step_avg:44.20ms
step:951/1775 train_time:42049ms step_avg:44.22ms
step:952/1775 train_time:42111ms step_avg:44.23ms
step:953/1775 train_time:42171ms step_avg:44.25ms
step:954/1775 train_time:42232ms step_avg:44.27ms
step:955/1775 train_time:42291ms step_avg:44.28ms
step:956/1775 train_time:42354ms step_avg:44.30ms
step:957/1775 train_time:42414ms step_avg:44.32ms
step:958/1775 train_time:42475ms step_avg:44.34ms
step:959/1775 train_time:42535ms step_avg:44.35ms
step:960/1775 train_time:42597ms step_avg:44.37ms
step:961/1775 train_time:42657ms step_avg:44.39ms
step:962/1775 train_time:42718ms step_avg:44.41ms
step:963/1775 train_time:42778ms step_avg:44.42ms
step:964/1775 train_time:42841ms step_avg:44.44ms
step:965/1775 train_time:42901ms step_avg:44.46ms
step:966/1775 train_time:42964ms step_avg:44.48ms
step:967/1775 train_time:43025ms step_avg:44.49ms
step:968/1775 train_time:43088ms step_avg:44.51ms
step:969/1775 train_time:43148ms step_avg:44.53ms
step:970/1775 train_time:43210ms step_avg:44.55ms
step:971/1775 train_time:43271ms step_avg:44.56ms
step:972/1775 train_time:43333ms step_avg:44.58ms
step:973/1775 train_time:43392ms step_avg:44.60ms
step:974/1775 train_time:43454ms step_avg:44.61ms
step:975/1775 train_time:43513ms step_avg:44.63ms
step:976/1775 train_time:43575ms step_avg:44.65ms
step:977/1775 train_time:43635ms step_avg:44.66ms
step:978/1775 train_time:43697ms step_avg:44.68ms
step:979/1775 train_time:43757ms step_avg:44.70ms
step:980/1775 train_time:43818ms step_avg:44.71ms
step:981/1775 train_time:43878ms step_avg:44.73ms
step:982/1775 train_time:43940ms step_avg:44.75ms
step:983/1775 train_time:44000ms step_avg:44.76ms
step:984/1775 train_time:44063ms step_avg:44.78ms
step:985/1775 train_time:44124ms step_avg:44.80ms
step:986/1775 train_time:44187ms step_avg:44.81ms
step:987/1775 train_time:44247ms step_avg:44.83ms
step:988/1775 train_time:44310ms step_avg:44.85ms
step:989/1775 train_time:44370ms step_avg:44.86ms
step:990/1775 train_time:44433ms step_avg:44.88ms
step:991/1775 train_time:44492ms step_avg:44.90ms
step:992/1775 train_time:44553ms step_avg:44.91ms
step:993/1775 train_time:44613ms step_avg:44.93ms
step:994/1775 train_time:44675ms step_avg:44.94ms
step:995/1775 train_time:44735ms step_avg:44.96ms
step:996/1775 train_time:44797ms step_avg:44.98ms
step:997/1775 train_time:44856ms step_avg:44.99ms
step:998/1775 train_time:44918ms step_avg:45.01ms
step:999/1775 train_time:44978ms step_avg:45.02ms
step:1000/1775 train_time:45040ms step_avg:45.04ms
step:1000/1775 val_loss:3.7368 train_time:45111ms step_avg:45.11ms
step:1001/1775 train_time:45131ms step_avg:45.09ms
step:1002/1775 train_time:45166ms step_avg:45.08ms
step:1003/1775 train_time:45228ms step_avg:45.09ms
step:1004/1775 train_time:45290ms step_avg:45.11ms
step:1005/1775 train_time:45350ms step_avg:45.12ms
step:1006/1775 train_time:45411ms step_avg:45.14ms
step:1007/1775 train_time:45471ms step_avg:45.16ms
step:1008/1775 train_time:45532ms step_avg:45.17ms
step:1009/1775 train_time:45591ms step_avg:45.18ms
step:1010/1775 train_time:45652ms step_avg:45.20ms
step:1011/1775 train_time:45712ms step_avg:45.21ms
step:1012/1775 train_time:45774ms step_avg:45.23ms
step:1013/1775 train_time:45832ms step_avg:45.24ms
step:1014/1775 train_time:45894ms step_avg:45.26ms
step:1015/1775 train_time:45953ms step_avg:45.27ms
step:1016/1775 train_time:46015ms step_avg:45.29ms
step:1017/1775 train_time:46076ms step_avg:45.31ms
step:1018/1775 train_time:46141ms step_avg:45.33ms
step:1019/1775 train_time:46201ms step_avg:45.34ms
step:1020/1775 train_time:46263ms step_avg:45.36ms
step:1021/1775 train_time:46323ms step_avg:45.37ms
step:1022/1775 train_time:46385ms step_avg:45.39ms
step:1023/1775 train_time:46445ms step_avg:45.40ms
step:1024/1775 train_time:46507ms step_avg:45.42ms
step:1025/1775 train_time:46566ms step_avg:45.43ms
step:1026/1775 train_time:46628ms step_avg:45.45ms
step:1027/1775 train_time:46688ms step_avg:45.46ms
step:1028/1775 train_time:46750ms step_avg:45.48ms
step:1029/1775 train_time:46808ms step_avg:45.49ms
step:1030/1775 train_time:46870ms step_avg:45.51ms
step:1031/1775 train_time:46930ms step_avg:45.52ms
step:1032/1775 train_time:46991ms step_avg:45.53ms
step:1033/1775 train_time:47050ms step_avg:45.55ms
step:1034/1775 train_time:47113ms step_avg:45.56ms
step:1035/1775 train_time:47174ms step_avg:45.58ms
step:1036/1775 train_time:47236ms step_avg:45.59ms
step:1037/1775 train_time:47297ms step_avg:45.61ms
step:1038/1775 train_time:47359ms step_avg:45.63ms
step:1039/1775 train_time:47418ms step_avg:45.64ms
step:1040/1775 train_time:47481ms step_avg:45.65ms
step:1041/1775 train_time:47540ms step_avg:45.67ms
step:1042/1775 train_time:47602ms step_avg:45.68ms
step:1043/1775 train_time:47661ms step_avg:45.70ms
step:1044/1775 train_time:47723ms step_avg:45.71ms
step:1045/1775 train_time:47783ms step_avg:45.73ms
step:1046/1775 train_time:47846ms step_avg:45.74ms
step:1047/1775 train_time:47906ms step_avg:45.76ms
step:1048/1775 train_time:47968ms step_avg:45.77ms
step:1049/1775 train_time:48028ms step_avg:45.78ms
step:1050/1775 train_time:48090ms step_avg:45.80ms
step:1051/1775 train_time:48151ms step_avg:45.81ms
step:1052/1775 train_time:48213ms step_avg:45.83ms
step:1053/1775 train_time:48272ms step_avg:45.84ms
step:1054/1775 train_time:48334ms step_avg:45.86ms
step:1055/1775 train_time:48393ms step_avg:45.87ms
step:1056/1775 train_time:48455ms step_avg:45.89ms
step:1057/1775 train_time:48515ms step_avg:45.90ms
step:1058/1775 train_time:48578ms step_avg:45.91ms
step:1059/1775 train_time:48636ms step_avg:45.93ms
step:1060/1775 train_time:48698ms step_avg:45.94ms
step:1061/1775 train_time:48757ms step_avg:45.95ms
step:1062/1775 train_time:48819ms step_avg:45.97ms
step:1063/1775 train_time:48879ms step_avg:45.98ms
step:1064/1775 train_time:48941ms step_avg:46.00ms
step:1065/1775 train_time:49001ms step_avg:46.01ms
step:1066/1775 train_time:49063ms step_avg:46.03ms
step:1067/1775 train_time:49123ms step_avg:46.04ms
step:1068/1775 train_time:49186ms step_avg:46.05ms
step:1069/1775 train_time:49246ms step_avg:46.07ms
step:1070/1775 train_time:49308ms step_avg:46.08ms
step:1071/1775 train_time:49368ms step_avg:46.10ms
step:1072/1775 train_time:49430ms step_avg:46.11ms
step:1073/1775 train_time:49490ms step_avg:46.12ms
step:1074/1775 train_time:49552ms step_avg:46.14ms
step:1075/1775 train_time:49612ms step_avg:46.15ms
step:1076/1775 train_time:49673ms step_avg:46.16ms
step:1077/1775 train_time:49733ms step_avg:46.18ms
step:1078/1775 train_time:49794ms step_avg:46.19ms
step:1079/1775 train_time:49854ms step_avg:46.20ms
step:1080/1775 train_time:49916ms step_avg:46.22ms
step:1081/1775 train_time:49976ms step_avg:46.23ms
step:1082/1775 train_time:50038ms step_avg:46.25ms
step:1083/1775 train_time:50096ms step_avg:46.26ms
step:1084/1775 train_time:50159ms step_avg:46.27ms
step:1085/1775 train_time:50219ms step_avg:46.28ms
step:1086/1775 train_time:50282ms step_avg:46.30ms
step:1087/1775 train_time:50342ms step_avg:46.31ms
step:1088/1775 train_time:50405ms step_avg:46.33ms
step:1089/1775 train_time:50465ms step_avg:46.34ms
step:1090/1775 train_time:50527ms step_avg:46.36ms
step:1091/1775 train_time:50587ms step_avg:46.37ms
step:1092/1775 train_time:50649ms step_avg:46.38ms
step:1093/1775 train_time:50709ms step_avg:46.39ms
step:1094/1775 train_time:50771ms step_avg:46.41ms
step:1095/1775 train_time:50831ms step_avg:46.42ms
step:1096/1775 train_time:50893ms step_avg:46.43ms
step:1097/1775 train_time:50952ms step_avg:46.45ms
step:1098/1775 train_time:51013ms step_avg:46.46ms
step:1099/1775 train_time:51074ms step_avg:46.47ms
step:1100/1775 train_time:51136ms step_avg:46.49ms
step:1101/1775 train_time:51195ms step_avg:46.50ms
step:1102/1775 train_time:51258ms step_avg:46.51ms
step:1103/1775 train_time:51317ms step_avg:46.52ms
step:1104/1775 train_time:51380ms step_avg:46.54ms
step:1105/1775 train_time:51439ms step_avg:46.55ms
step:1106/1775 train_time:51501ms step_avg:46.57ms
step:1107/1775 train_time:51562ms step_avg:46.58ms
step:1108/1775 train_time:51623ms step_avg:46.59ms
step:1109/1775 train_time:51684ms step_avg:46.60ms
step:1110/1775 train_time:51747ms step_avg:46.62ms
step:1111/1775 train_time:51807ms step_avg:46.63ms
step:1112/1775 train_time:51869ms step_avg:46.65ms
step:1113/1775 train_time:51929ms step_avg:46.66ms
step:1114/1775 train_time:51991ms step_avg:46.67ms
step:1115/1775 train_time:52052ms step_avg:46.68ms
step:1116/1775 train_time:52113ms step_avg:46.70ms
step:1117/1775 train_time:52172ms step_avg:46.71ms
step:1118/1775 train_time:52234ms step_avg:46.72ms
step:1119/1775 train_time:52293ms step_avg:46.73ms
step:1120/1775 train_time:52356ms step_avg:46.75ms
step:1121/1775 train_time:52416ms step_avg:46.76ms
step:1122/1775 train_time:52478ms step_avg:46.77ms
step:1123/1775 train_time:52538ms step_avg:46.78ms
step:1124/1775 train_time:52599ms step_avg:46.80ms
step:1125/1775 train_time:52659ms step_avg:46.81ms
step:1126/1775 train_time:52721ms step_avg:46.82ms
step:1127/1775 train_time:52782ms step_avg:46.83ms
step:1128/1775 train_time:52845ms step_avg:46.85ms
step:1129/1775 train_time:52905ms step_avg:46.86ms
step:1130/1775 train_time:52968ms step_avg:46.87ms
step:1131/1775 train_time:53027ms step_avg:46.89ms
step:1132/1775 train_time:53090ms step_avg:46.90ms
step:1133/1775 train_time:53151ms step_avg:46.91ms
step:1134/1775 train_time:53214ms step_avg:46.93ms
step:1135/1775 train_time:53273ms step_avg:46.94ms
step:1136/1775 train_time:53335ms step_avg:46.95ms
step:1137/1775 train_time:53393ms step_avg:46.96ms
step:1138/1775 train_time:53456ms step_avg:46.97ms
step:1139/1775 train_time:53516ms step_avg:46.98ms
step:1140/1775 train_time:53578ms step_avg:47.00ms
step:1141/1775 train_time:53638ms step_avg:47.01ms
step:1142/1775 train_time:53700ms step_avg:47.02ms
step:1143/1775 train_time:53759ms step_avg:47.03ms
step:1144/1775 train_time:53821ms step_avg:47.05ms
step:1145/1775 train_time:53882ms step_avg:47.06ms
step:1146/1775 train_time:53945ms step_avg:47.07ms
step:1147/1775 train_time:54005ms step_avg:47.08ms
step:1148/1775 train_time:54067ms step_avg:47.10ms
step:1149/1775 train_time:54127ms step_avg:47.11ms
step:1150/1775 train_time:54190ms step_avg:47.12ms
step:1151/1775 train_time:54250ms step_avg:47.13ms
step:1152/1775 train_time:54312ms step_avg:47.15ms
step:1153/1775 train_time:54372ms step_avg:47.16ms
step:1154/1775 train_time:54434ms step_avg:47.17ms
step:1155/1775 train_time:54493ms step_avg:47.18ms
step:1156/1775 train_time:54555ms step_avg:47.19ms
step:1157/1775 train_time:54615ms step_avg:47.20ms
step:1158/1775 train_time:54681ms step_avg:47.22ms
step:1159/1775 train_time:54767ms step_avg:47.25ms
step:1160/1775 train_time:54857ms step_avg:47.29ms
step:1161/1775 train_time:54943ms step_avg:47.32ms
step:1162/1775 train_time:55033ms step_avg:47.36ms
step:1163/1775 train_time:55120ms step_avg:47.39ms
step:1164/1775 train_time:55208ms step_avg:47.43ms
step:1165/1775 train_time:55295ms step_avg:47.46ms
step:1166/1775 train_time:55385ms step_avg:47.50ms
step:1167/1775 train_time:55472ms step_avg:47.53ms
step:1168/1775 train_time:55561ms step_avg:47.57ms
step:1169/1775 train_time:55647ms step_avg:47.60ms
step:1170/1775 train_time:55734ms step_avg:47.64ms
step:1171/1775 train_time:55820ms step_avg:47.67ms
step:1172/1775 train_time:55908ms step_avg:47.70ms
step:1173/1775 train_time:55995ms step_avg:47.74ms
step:1174/1775 train_time:56084ms step_avg:47.77ms
step:1175/1775 train_time:56169ms step_avg:47.80ms
step:1176/1775 train_time:56259ms step_avg:47.84ms
step:1177/1775 train_time:56345ms step_avg:47.87ms
step:1178/1775 train_time:56434ms step_avg:47.91ms
step:1179/1775 train_time:56521ms step_avg:47.94ms
step:1180/1775 train_time:56609ms step_avg:47.97ms
step:1181/1775 train_time:56696ms step_avg:48.01ms
step:1182/1775 train_time:56785ms step_avg:48.04ms
step:1183/1775 train_time:56870ms step_avg:48.07ms
step:1184/1775 train_time:56959ms step_avg:48.11ms
step:1185/1775 train_time:57045ms step_avg:48.14ms
step:1186/1775 train_time:57133ms step_avg:48.17ms
step:1187/1775 train_time:57221ms step_avg:48.21ms
step:1188/1775 train_time:57309ms step_avg:48.24ms
step:1189/1775 train_time:57396ms step_avg:48.27ms
step:1190/1775 train_time:57485ms step_avg:48.31ms
step:1191/1775 train_time:57570ms step_avg:48.34ms
step:1192/1775 train_time:57660ms step_avg:48.37ms
step:1193/1775 train_time:57746ms step_avg:48.40ms
step:1194/1775 train_time:57834ms step_avg:48.44ms
step:1195/1775 train_time:57921ms step_avg:48.47ms
step:1196/1775 train_time:58009ms step_avg:48.50ms
step:1197/1775 train_time:58095ms step_avg:48.53ms
step:1198/1775 train_time:58184ms step_avg:48.57ms
step:1199/1775 train_time:58270ms step_avg:48.60ms
step:1200/1775 train_time:58360ms step_avg:48.63ms
step:1201/1775 train_time:58445ms step_avg:48.66ms
step:1202/1775 train_time:58534ms step_avg:48.70ms
step:1203/1775 train_time:58621ms step_avg:48.73ms
step:1204/1775 train_time:58710ms step_avg:48.76ms
step:1205/1775 train_time:58795ms step_avg:48.79ms
step:1206/1775 train_time:58884ms step_avg:48.83ms
step:1207/1775 train_time:58970ms step_avg:48.86ms
step:1208/1775 train_time:59059ms step_avg:48.89ms
step:1209/1775 train_time:59144ms step_avg:48.92ms
step:1210/1775 train_time:59232ms step_avg:48.95ms
step:1211/1775 train_time:59321ms step_avg:48.99ms
step:1212/1775 train_time:59410ms step_avg:49.02ms
step:1213/1775 train_time:59496ms step_avg:49.05ms
step:1214/1775 train_time:59585ms step_avg:49.08ms
step:1215/1775 train_time:59671ms step_avg:49.11ms
step:1216/1775 train_time:59760ms step_avg:49.14ms
step:1217/1775 train_time:59846ms step_avg:49.17ms
step:1218/1775 train_time:59935ms step_avg:49.21ms
step:1219/1775 train_time:60020ms step_avg:49.24ms
step:1220/1775 train_time:60108ms step_avg:49.27ms
step:1221/1775 train_time:60195ms step_avg:49.30ms
step:1222/1775 train_time:60284ms step_avg:49.33ms
step:1223/1775 train_time:60371ms step_avg:49.36ms
step:1224/1775 train_time:60460ms step_avg:49.40ms
step:1225/1775 train_time:60546ms step_avg:49.43ms
step:1226/1775 train_time:60634ms step_avg:49.46ms
step:1227/1775 train_time:60722ms step_avg:49.49ms
step:1228/1775 train_time:60810ms step_avg:49.52ms
step:1229/1775 train_time:60896ms step_avg:49.55ms
step:1230/1775 train_time:60985ms step_avg:49.58ms
step:1231/1775 train_time:61071ms step_avg:49.61ms
step:1232/1775 train_time:61160ms step_avg:49.64ms
step:1233/1775 train_time:61247ms step_avg:49.67ms
step:1234/1775 train_time:61335ms step_avg:49.70ms
step:1235/1775 train_time:61423ms step_avg:49.74ms
step:1236/1775 train_time:61513ms step_avg:49.77ms
step:1237/1775 train_time:61598ms step_avg:49.80ms
step:1238/1775 train_time:61686ms step_avg:49.83ms
step:1239/1775 train_time:61773ms step_avg:49.86ms
step:1240/1775 train_time:61862ms step_avg:49.89ms
step:1241/1775 train_time:61948ms step_avg:49.92ms
step:1242/1775 train_time:62036ms step_avg:49.95ms
step:1243/1775 train_time:62123ms step_avg:49.98ms
step:1244/1775 train_time:62212ms step_avg:50.01ms
step:1245/1775 train_time:62299ms step_avg:50.04ms
step:1246/1775 train_time:62387ms step_avg:50.07ms
step:1247/1775 train_time:62474ms step_avg:50.10ms
step:1248/1775 train_time:62562ms step_avg:50.13ms
step:1249/1775 train_time:62648ms step_avg:50.16ms
step:1250/1775 train_time:62736ms step_avg:50.19ms
step:1250/1775 val_loss:3.5048 train_time:62835ms step_avg:50.27ms
step:1251/1775 train_time:62854ms step_avg:50.24ms
step:1252/1775 train_time:62915ms step_avg:50.25ms
step:1253/1775 train_time:63003ms step_avg:50.28ms
step:1254/1775 train_time:63091ms step_avg:50.31ms
step:1255/1775 train_time:63177ms step_avg:50.34ms
step:1256/1775 train_time:63265ms step_avg:50.37ms
step:1257/1775 train_time:63350ms step_avg:50.40ms
step:1258/1775 train_time:63437ms step_avg:50.43ms
step:1259/1775 train_time:63523ms step_avg:50.46ms
step:1260/1775 train_time:63611ms step_avg:50.49ms
step:1261/1775 train_time:63695ms step_avg:50.51ms
step:1262/1775 train_time:63784ms step_avg:50.54ms
step:1263/1775 train_time:63873ms step_avg:50.57ms
step:1264/1775 train_time:63967ms step_avg:50.61ms
step:1265/1775 train_time:64054ms step_avg:50.64ms
step:1266/1775 train_time:64143ms step_avg:50.67ms
step:1267/1775 train_time:64229ms step_avg:50.69ms
step:1268/1775 train_time:64317ms step_avg:50.72ms
step:1269/1775 train_time:64403ms step_avg:50.75ms
step:1270/1775 train_time:64492ms step_avg:50.78ms
step:1271/1775 train_time:64578ms step_avg:50.81ms
step:1272/1775 train_time:64665ms step_avg:50.84ms
step:1273/1775 train_time:64752ms step_avg:50.87ms
step:1274/1775 train_time:64841ms step_avg:50.90ms
step:1275/1775 train_time:64930ms step_avg:50.93ms
step:1276/1775 train_time:65019ms step_avg:50.96ms
step:1277/1775 train_time:65107ms step_avg:50.98ms
step:1278/1775 train_time:65195ms step_avg:51.01ms
step:1279/1775 train_time:65280ms step_avg:51.04ms
step:1280/1775 train_time:65368ms step_avg:51.07ms
step:1281/1775 train_time:65453ms step_avg:51.10ms
step:1282/1775 train_time:65542ms step_avg:51.12ms
step:1283/1775 train_time:65628ms step_avg:51.15ms
step:1284/1775 train_time:65716ms step_avg:51.18ms
step:1285/1775 train_time:65803ms step_avg:51.21ms
step:1286/1775 train_time:65893ms step_avg:51.24ms
step:1287/1775 train_time:65979ms step_avg:51.27ms
step:1288/1775 train_time:66068ms step_avg:51.30ms
step:1289/1775 train_time:66154ms step_avg:51.32ms
step:1290/1775 train_time:66242ms step_avg:51.35ms
step:1291/1775 train_time:66329ms step_avg:51.38ms
step:1292/1775 train_time:66416ms step_avg:51.41ms
step:1293/1775 train_time:66503ms step_avg:51.43ms
step:1294/1775 train_time:66592ms step_avg:51.46ms
step:1295/1775 train_time:66678ms step_avg:51.49ms
step:1296/1775 train_time:66766ms step_avg:51.52ms
step:1297/1775 train_time:66852ms step_avg:51.54ms
step:1298/1775 train_time:66941ms step_avg:51.57ms
step:1299/1775 train_time:67028ms step_avg:51.60ms
step:1300/1775 train_time:67117ms step_avg:51.63ms
step:1301/1775 train_time:67205ms step_avg:51.66ms
step:1302/1775 train_time:67294ms step_avg:51.68ms
step:1303/1775 train_time:67378ms step_avg:51.71ms
step:1304/1775 train_time:67468ms step_avg:51.74ms
step:1305/1775 train_time:67553ms step_avg:51.76ms
step:1306/1775 train_time:67641ms step_avg:51.79ms
step:1307/1775 train_time:67728ms step_avg:51.82ms
step:1308/1775 train_time:67816ms step_avg:51.85ms
step:1309/1775 train_time:67903ms step_avg:51.87ms
step:1310/1775 train_time:67993ms step_avg:51.90ms
step:1311/1775 train_time:68078ms step_avg:51.93ms
step:1312/1775 train_time:68169ms step_avg:51.96ms
step:1313/1775 train_time:68253ms step_avg:51.98ms
step:1314/1775 train_time:68342ms step_avg:52.01ms
step:1315/1775 train_time:68430ms step_avg:52.04ms
step:1316/1775 train_time:68518ms step_avg:52.07ms
step:1317/1775 train_time:68605ms step_avg:52.09ms
step:1318/1775 train_time:68694ms step_avg:52.12ms
step:1319/1775 train_time:68780ms step_avg:52.15ms
step:1320/1775 train_time:68870ms step_avg:52.17ms
step:1321/1775 train_time:68955ms step_avg:52.20ms
step:1322/1775 train_time:69044ms step_avg:52.23ms
step:1323/1775 train_time:69131ms step_avg:52.25ms
step:1324/1775 train_time:69220ms step_avg:52.28ms
step:1325/1775 train_time:69306ms step_avg:52.31ms
step:1326/1775 train_time:69396ms step_avg:52.33ms
step:1327/1775 train_time:69481ms step_avg:52.36ms
step:1328/1775 train_time:69569ms step_avg:52.39ms
step:1329/1775 train_time:69655ms step_avg:52.41ms
step:1330/1775 train_time:69746ms step_avg:52.44ms
step:1331/1775 train_time:69833ms step_avg:52.47ms
step:1332/1775 train_time:69923ms step_avg:52.49ms
step:1333/1775 train_time:70009ms step_avg:52.52ms
step:1334/1775 train_time:70097ms step_avg:52.55ms
step:1335/1775 train_time:70185ms step_avg:52.57ms
step:1336/1775 train_time:70273ms step_avg:52.60ms
step:1337/1775 train_time:70358ms step_avg:52.62ms
step:1338/1775 train_time:70448ms step_avg:52.65ms
step:1339/1775 train_time:70533ms step_avg:52.68ms
step:1340/1775 train_time:70624ms step_avg:52.70ms
step:1341/1775 train_time:70709ms step_avg:52.73ms
step:1342/1775 train_time:70797ms step_avg:52.76ms
step:1343/1775 train_time:70885ms step_avg:52.78ms
step:1344/1775 train_time:70973ms step_avg:52.81ms
step:1345/1775 train_time:71059ms step_avg:52.83ms
step:1346/1775 train_time:71149ms step_avg:52.86ms
step:1347/1775 train_time:71235ms step_avg:52.88ms
step:1348/1775 train_time:71326ms step_avg:52.91ms
step:1349/1775 train_time:71413ms step_avg:52.94ms
step:1350/1775 train_time:71502ms step_avg:52.96ms
step:1351/1775 train_time:71588ms step_avg:52.99ms
step:1352/1775 train_time:71676ms step_avg:53.01ms
step:1353/1775 train_time:71761ms step_avg:53.04ms
step:1354/1775 train_time:71851ms step_avg:53.07ms
step:1355/1775 train_time:71936ms step_avg:53.09ms
step:1356/1775 train_time:72026ms step_avg:53.12ms
step:1357/1775 train_time:72112ms step_avg:53.14ms
step:1358/1775 train_time:72200ms step_avg:53.17ms
step:1359/1775 train_time:72287ms step_avg:53.19ms
step:1360/1775 train_time:72375ms step_avg:53.22ms
step:1361/1775 train_time:72462ms step_avg:53.24ms
step:1362/1775 train_time:72550ms step_avg:53.27ms
step:1363/1775 train_time:72636ms step_avg:53.29ms
step:1364/1775 train_time:72725ms step_avg:53.32ms
step:1365/1775 train_time:72812ms step_avg:53.34ms
step:1366/1775 train_time:72900ms step_avg:53.37ms
step:1367/1775 train_time:72987ms step_avg:53.39ms
step:1368/1775 train_time:73074ms step_avg:53.42ms
step:1369/1775 train_time:73160ms step_avg:53.44ms
step:1370/1775 train_time:73250ms step_avg:53.47ms
step:1371/1775 train_time:73336ms step_avg:53.49ms
step:1372/1775 train_time:73425ms step_avg:53.52ms
step:1373/1775 train_time:73512ms step_avg:53.54ms
step:1374/1775 train_time:73599ms step_avg:53.57ms
step:1375/1775 train_time:73686ms step_avg:53.59ms
step:1376/1775 train_time:73775ms step_avg:53.62ms
step:1377/1775 train_time:73861ms step_avg:53.64ms
step:1378/1775 train_time:73949ms step_avg:53.66ms
step:1379/1775 train_time:74034ms step_avg:53.69ms
step:1380/1775 train_time:74124ms step_avg:53.71ms
step:1381/1775 train_time:74211ms step_avg:53.74ms
step:1382/1775 train_time:74299ms step_avg:53.76ms
step:1383/1775 train_time:74385ms step_avg:53.79ms
step:1384/1775 train_time:74474ms step_avg:53.81ms
step:1385/1775 train_time:74561ms step_avg:53.83ms
step:1386/1775 train_time:74651ms step_avg:53.86ms
step:1387/1775 train_time:74736ms step_avg:53.88ms
step:1388/1775 train_time:74826ms step_avg:53.91ms
step:1389/1775 train_time:74913ms step_avg:53.93ms
step:1390/1775 train_time:75002ms step_avg:53.96ms
step:1391/1775 train_time:75089ms step_avg:53.98ms
step:1392/1775 train_time:75177ms step_avg:54.01ms
step:1393/1775 train_time:75263ms step_avg:54.03ms
step:1394/1775 train_time:75352ms step_avg:54.05ms
step:1395/1775 train_time:75437ms step_avg:54.08ms
step:1396/1775 train_time:75528ms step_avg:54.10ms
step:1397/1775 train_time:75613ms step_avg:54.13ms
step:1398/1775 train_time:75702ms step_avg:54.15ms
step:1399/1775 train_time:75790ms step_avg:54.17ms
step:1400/1775 train_time:75877ms step_avg:54.20ms
step:1401/1775 train_time:75965ms step_avg:54.22ms
step:1402/1775 train_time:76053ms step_avg:54.25ms
step:1403/1775 train_time:76138ms step_avg:54.27ms
step:1404/1775 train_time:76229ms step_avg:54.29ms
step:1405/1775 train_time:76314ms step_avg:54.32ms
step:1406/1775 train_time:76403ms step_avg:54.34ms
step:1407/1775 train_time:76488ms step_avg:54.36ms
step:1408/1775 train_time:76577ms step_avg:54.39ms
step:1409/1775 train_time:76665ms step_avg:54.41ms
step:1410/1775 train_time:76753ms step_avg:54.43ms
step:1411/1775 train_time:76839ms step_avg:54.46ms
step:1412/1775 train_time:76929ms step_avg:54.48ms
step:1413/1775 train_time:77014ms step_avg:54.50ms
step:1414/1775 train_time:77104ms step_avg:54.53ms
step:1415/1775 train_time:77192ms step_avg:54.55ms
step:1416/1775 train_time:77280ms step_avg:54.58ms
step:1417/1775 train_time:77365ms step_avg:54.60ms
step:1418/1775 train_time:77454ms step_avg:54.62ms
step:1419/1775 train_time:77540ms step_avg:54.64ms
step:1420/1775 train_time:77630ms step_avg:54.67ms
step:1421/1775 train_time:77716ms step_avg:54.69ms
step:1422/1775 train_time:77805ms step_avg:54.71ms
step:1423/1775 train_time:77893ms step_avg:54.74ms
step:1424/1775 train_time:77981ms step_avg:54.76ms
step:1425/1775 train_time:78067ms step_avg:54.78ms
step:1426/1775 train_time:78156ms step_avg:54.81ms
step:1427/1775 train_time:78241ms step_avg:54.83ms
step:1428/1775 train_time:78330ms step_avg:54.85ms
step:1429/1775 train_time:78415ms step_avg:54.87ms
step:1430/1775 train_time:78506ms step_avg:54.90ms
step:1431/1775 train_time:78592ms step_avg:54.92ms
step:1432/1775 train_time:78680ms step_avg:54.94ms
step:1433/1775 train_time:78767ms step_avg:54.97ms
step:1434/1775 train_time:78855ms step_avg:54.99ms
step:1435/1775 train_time:78941ms step_avg:55.01ms
step:1436/1775 train_time:79031ms step_avg:55.04ms
step:1437/1775 train_time:79117ms step_avg:55.06ms
step:1438/1775 train_time:79206ms step_avg:55.08ms
step:1439/1775 train_time:79293ms step_avg:55.10ms
step:1440/1775 train_time:79381ms step_avg:55.13ms
step:1441/1775 train_time:79467ms step_avg:55.15ms
step:1442/1775 train_time:79555ms step_avg:55.17ms
step:1443/1775 train_time:79641ms step_avg:55.19ms
step:1444/1775 train_time:79730ms step_avg:55.21ms
step:1445/1775 train_time:79815ms step_avg:55.24ms
step:1446/1775 train_time:79906ms step_avg:55.26ms
step:1447/1775 train_time:79993ms step_avg:55.28ms
step:1448/1775 train_time:80082ms step_avg:55.30ms
step:1449/1775 train_time:80166ms step_avg:55.33ms
step:1450/1775 train_time:80255ms step_avg:55.35ms
step:1451/1775 train_time:80341ms step_avg:55.37ms
step:1452/1775 train_time:80431ms step_avg:55.39ms
step:1453/1775 train_time:80517ms step_avg:55.41ms
step:1454/1775 train_time:80605ms step_avg:55.44ms
step:1455/1775 train_time:80692ms step_avg:55.46ms
step:1456/1775 train_time:80781ms step_avg:55.48ms
step:1457/1775 train_time:80867ms step_avg:55.50ms
step:1458/1775 train_time:80955ms step_avg:55.52ms
step:1459/1775 train_time:81041ms step_avg:55.55ms
step:1460/1775 train_time:81132ms step_avg:55.57ms
step:1461/1775 train_time:81217ms step_avg:55.59ms
step:1462/1775 train_time:81306ms step_avg:55.61ms
step:1463/1775 train_time:81393ms step_avg:55.63ms
step:1464/1775 train_time:81481ms step_avg:55.66ms
step:1465/1775 train_time:81568ms step_avg:55.68ms
step:1466/1775 train_time:81656ms step_avg:55.70ms
step:1467/1775 train_time:81743ms step_avg:55.72ms
step:1468/1775 train_time:81833ms step_avg:55.74ms
step:1469/1775 train_time:81919ms step_avg:55.76ms
step:1470/1775 train_time:82008ms step_avg:55.79ms
step:1471/1775 train_time:82095ms step_avg:55.81ms
step:1472/1775 train_time:82183ms step_avg:55.83ms
step:1473/1775 train_time:82270ms step_avg:55.85ms
step:1474/1775 train_time:82358ms step_avg:55.87ms
step:1475/1775 train_time:82445ms step_avg:55.89ms
step:1476/1775 train_time:82534ms step_avg:55.92ms
step:1477/1775 train_time:82619ms step_avg:55.94ms
step:1478/1775 train_time:82709ms step_avg:55.96ms
step:1479/1775 train_time:82794ms step_avg:55.98ms
step:1480/1775 train_time:82884ms step_avg:56.00ms
step:1481/1775 train_time:82970ms step_avg:56.02ms
step:1482/1775 train_time:83059ms step_avg:56.05ms
step:1483/1775 train_time:83145ms step_avg:56.07ms
step:1484/1775 train_time:83233ms step_avg:56.09ms
step:1485/1775 train_time:83319ms step_avg:56.11ms
step:1486/1775 train_time:83408ms step_avg:56.13ms
step:1487/1775 train_time:83495ms step_avg:56.15ms
step:1488/1775 train_time:83584ms step_avg:56.17ms
step:1489/1775 train_time:83670ms step_avg:56.19ms
step:1490/1775 train_time:83758ms step_avg:56.21ms
step:1491/1775 train_time:83844ms step_avg:56.23ms
step:1492/1775 train_time:83933ms step_avg:56.26ms
step:1493/1775 train_time:84019ms step_avg:56.28ms
step:1494/1775 train_time:84109ms step_avg:56.30ms
step:1495/1775 train_time:84195ms step_avg:56.32ms
step:1496/1775 train_time:84283ms step_avg:56.34ms
step:1497/1775 train_time:84369ms step_avg:56.36ms
step:1498/1775 train_time:84458ms step_avg:56.38ms
step:1499/1775 train_time:84545ms step_avg:56.40ms
step:1500/1775 train_time:84634ms step_avg:56.42ms
step:1500/1775 val_loss:3.3775 train_time:84732ms step_avg:56.49ms
step:1501/1775 train_time:84751ms step_avg:56.46ms
step:1502/1775 train_time:84813ms step_avg:56.47ms
step:1503/1775 train_time:84904ms step_avg:56.49ms
step:1504/1775 train_time:84993ms step_avg:56.51ms
step:1505/1775 train_time:85079ms step_avg:56.53ms
step:1506/1775 train_time:85165ms step_avg:56.55ms
step:1507/1775 train_time:85251ms step_avg:56.57ms
step:1508/1775 train_time:85340ms step_avg:56.59ms
step:1509/1775 train_time:85425ms step_avg:56.61ms
step:1510/1775 train_time:85513ms step_avg:56.63ms
step:1511/1775 train_time:85598ms step_avg:56.65ms
step:1512/1775 train_time:85686ms step_avg:56.67ms
step:1513/1775 train_time:85775ms step_avg:56.69ms
step:1514/1775 train_time:85867ms step_avg:56.72ms
step:1515/1775 train_time:85956ms step_avg:56.74ms
step:1516/1775 train_time:86045ms step_avg:56.76ms
step:1517/1775 train_time:86130ms step_avg:56.78ms
step:1518/1775 train_time:86219ms step_avg:56.80ms
step:1519/1775 train_time:86304ms step_avg:56.82ms
step:1520/1775 train_time:86393ms step_avg:56.84ms
step:1521/1775 train_time:86479ms step_avg:56.86ms
step:1522/1775 train_time:86566ms step_avg:56.88ms
step:1523/1775 train_time:86651ms step_avg:56.90ms
step:1524/1775 train_time:86743ms step_avg:56.92ms
step:1525/1775 train_time:86831ms step_avg:56.94ms
step:1526/1775 train_time:86923ms step_avg:56.96ms
step:1527/1775 train_time:87009ms step_avg:56.98ms
step:1528/1775 train_time:87096ms step_avg:57.00ms
step:1529/1775 train_time:87182ms step_avg:57.02ms
step:1530/1775 train_time:87270ms step_avg:57.04ms
step:1531/1775 train_time:87354ms step_avg:57.06ms
step:1532/1775 train_time:87443ms step_avg:57.08ms
step:1533/1775 train_time:87528ms step_avg:57.10ms
step:1534/1775 train_time:87618ms step_avg:57.12ms
step:1535/1775 train_time:87705ms step_avg:57.14ms
step:1536/1775 train_time:87796ms step_avg:57.16ms
step:1537/1775 train_time:87884ms step_avg:57.18ms
step:1538/1775 train_time:87973ms step_avg:57.20ms
step:1539/1775 train_time:88059ms step_avg:57.22ms
step:1540/1775 train_time:88146ms step_avg:57.24ms
step:1541/1775 train_time:88232ms step_avg:57.26ms
step:1542/1775 train_time:88321ms step_avg:57.28ms
step:1543/1775 train_time:88407ms step_avg:57.30ms
step:1544/1775 train_time:88495ms step_avg:57.32ms
step:1545/1775 train_time:88581ms step_avg:57.33ms
step:1546/1775 train_time:88671ms step_avg:57.35ms
step:1547/1775 train_time:88758ms step_avg:57.37ms
step:1548/1775 train_time:88846ms step_avg:57.39ms
step:1549/1775 train_time:88934ms step_avg:57.41ms
step:1550/1775 train_time:89023ms step_avg:57.43ms
step:1551/1775 train_time:89108ms step_avg:57.45ms
step:1552/1775 train_time:89198ms step_avg:57.47ms
step:1553/1775 train_time:89284ms step_avg:57.49ms
step:1554/1775 train_time:89373ms step_avg:57.51ms
step:1555/1775 train_time:89459ms step_avg:57.53ms
step:1556/1775 train_time:89547ms step_avg:57.55ms
step:1557/1775 train_time:89634ms step_avg:57.57ms
step:1558/1775 train_time:89722ms step_avg:57.59ms
step:1559/1775 train_time:89808ms step_avg:57.61ms
step:1560/1775 train_time:89899ms step_avg:57.63ms
step:1561/1775 train_time:89985ms step_avg:57.65ms
step:1562/1775 train_time:90075ms step_avg:57.67ms
step:1563/1775 train_time:90163ms step_avg:57.69ms
step:1564/1775 train_time:90251ms step_avg:57.71ms
step:1565/1775 train_time:90336ms step_avg:57.72ms
step:1566/1775 train_time:90425ms step_avg:57.74ms
step:1567/1775 train_time:90511ms step_avg:57.76ms
step:1568/1775 train_time:90601ms step_avg:57.78ms
step:1569/1775 train_time:90686ms step_avg:57.80ms
step:1570/1775 train_time:90777ms step_avg:57.82ms
step:1571/1775 train_time:90863ms step_avg:57.84ms
step:1572/1775 train_time:90951ms step_avg:57.86ms
step:1573/1775 train_time:91039ms step_avg:57.88ms
step:1574/1775 train_time:91126ms step_avg:57.89ms
step:1575/1775 train_time:91212ms step_avg:57.91ms
step:1576/1775 train_time:91302ms step_avg:57.93ms
step:1577/1775 train_time:91387ms step_avg:57.95ms
step:1578/1775 train_time:91476ms step_avg:57.97ms
step:1579/1775 train_time:91562ms step_avg:57.99ms
step:1580/1775 train_time:91651ms step_avg:58.01ms
step:1581/1775 train_time:91737ms step_avg:58.02ms
step:1582/1775 train_time:91824ms step_avg:58.04ms
step:1583/1775 train_time:91911ms step_avg:58.06ms
step:1584/1775 train_time:92001ms step_avg:58.08ms
step:1585/1775 train_time:92086ms step_avg:58.10ms
step:1586/1775 train_time:92176ms step_avg:58.12ms
step:1587/1775 train_time:92263ms step_avg:58.14ms
step:1588/1775 train_time:92351ms step_avg:58.16ms
step:1589/1775 train_time:92437ms step_avg:58.17ms
step:1590/1775 train_time:92526ms step_avg:58.19ms
step:1591/1775 train_time:92611ms step_avg:58.21ms
step:1592/1775 train_time:92702ms step_avg:58.23ms
step:1593/1775 train_time:92787ms step_avg:58.25ms
step:1594/1775 train_time:92877ms step_avg:58.27ms
step:1595/1775 train_time:92964ms step_avg:58.28ms
step:1596/1775 train_time:93053ms step_avg:58.30ms
step:1597/1775 train_time:93140ms step_avg:58.32ms
step:1598/1775 train_time:93227ms step_avg:58.34ms
step:1599/1775 train_time:93314ms step_avg:58.36ms
step:1600/1775 train_time:93403ms step_avg:58.38ms
step:1601/1775 train_time:93488ms step_avg:58.39ms
step:1602/1775 train_time:93578ms step_avg:58.41ms
step:1603/1775 train_time:93664ms step_avg:58.43ms
step:1604/1775 train_time:93752ms step_avg:58.45ms
step:1605/1775 train_time:93840ms step_avg:58.47ms
step:1606/1775 train_time:93928ms step_avg:58.49ms
step:1607/1775 train_time:94014ms step_avg:58.50ms
step:1608/1775 train_time:94103ms step_avg:58.52ms
step:1609/1775 train_time:94188ms step_avg:58.54ms
step:1610/1775 train_time:94278ms step_avg:58.56ms
step:1611/1775 train_time:94364ms step_avg:58.57ms
step:1612/1775 train_time:94454ms step_avg:58.59ms
step:1613/1775 train_time:94542ms step_avg:58.61ms
step:1614/1775 train_time:94630ms step_avg:58.63ms
step:1615/1775 train_time:94717ms step_avg:58.65ms
step:1616/1775 train_time:94806ms step_avg:58.67ms
step:1617/1775 train_time:94891ms step_avg:58.68ms
step:1618/1775 train_time:94980ms step_avg:58.70ms
step:1619/1775 train_time:95066ms step_avg:58.72ms
step:1620/1775 train_time:95155ms step_avg:58.74ms
step:1621/1775 train_time:95242ms step_avg:58.76ms
step:1622/1775 train_time:95330ms step_avg:58.77ms
step:1623/1775 train_time:95417ms step_avg:58.79ms
step:1624/1775 train_time:95506ms step_avg:58.81ms
step:1625/1775 train_time:95591ms step_avg:58.83ms
step:1626/1775 train_time:95680ms step_avg:58.84ms
step:1627/1775 train_time:95766ms step_avg:58.86ms
step:1628/1775 train_time:95856ms step_avg:58.88ms
step:1629/1775 train_time:95943ms step_avg:58.90ms
step:1630/1775 train_time:96032ms step_avg:58.92ms
step:1631/1775 train_time:96118ms step_avg:58.93ms
step:1632/1775 train_time:96206ms step_avg:58.95ms
step:1633/1775 train_time:96292ms step_avg:58.97ms
step:1634/1775 train_time:96381ms step_avg:58.98ms
step:1635/1775 train_time:96467ms step_avg:59.00ms
step:1636/1775 train_time:96555ms step_avg:59.02ms
step:1637/1775 train_time:96641ms step_avg:59.04ms
step:1638/1775 train_time:96730ms step_avg:59.05ms
step:1639/1775 train_time:96816ms step_avg:59.07ms
step:1640/1775 train_time:96904ms step_avg:59.09ms
step:1641/1775 train_time:96990ms step_avg:59.10ms
step:1642/1775 train_time:97080ms step_avg:59.12ms
step:1643/1775 train_time:97166ms step_avg:59.14ms
step:1644/1775 train_time:97254ms step_avg:59.16ms
step:1645/1775 train_time:97341ms step_avg:59.17ms
step:1646/1775 train_time:97429ms step_avg:59.19ms
step:1647/1775 train_time:97514ms step_avg:59.21ms
step:1648/1775 train_time:97604ms step_avg:59.23ms
step:1649/1775 train_time:97689ms step_avg:59.24ms
step:1650/1775 train_time:97779ms step_avg:59.26ms
step:1651/1775 train_time:97864ms step_avg:59.28ms
step:1652/1775 train_time:97953ms step_avg:59.29ms
step:1653/1775 train_time:98039ms step_avg:59.31ms
step:1654/1775 train_time:98127ms step_avg:59.33ms
step:1655/1775 train_time:98215ms step_avg:59.34ms
step:1656/1775 train_time:98303ms step_avg:59.36ms
step:1657/1775 train_time:98388ms step_avg:59.38ms
step:1658/1775 train_time:98478ms step_avg:59.40ms
step:1659/1775 train_time:98565ms step_avg:59.41ms
step:1660/1775 train_time:98653ms step_avg:59.43ms
step:1661/1775 train_time:98738ms step_avg:59.45ms
step:1662/1775 train_time:98826ms step_avg:59.46ms
step:1663/1775 train_time:98912ms step_avg:59.48ms
step:1664/1775 train_time:99003ms step_avg:59.50ms
step:1665/1775 train_time:99088ms step_avg:59.51ms
step:1666/1775 train_time:99178ms step_avg:59.53ms
step:1667/1775 train_time:99263ms step_avg:59.55ms
step:1668/1775 train_time:99352ms step_avg:59.56ms
step:1669/1775 train_time:99438ms step_avg:59.58ms
step:1670/1775 train_time:99526ms step_avg:59.60ms
step:1671/1775 train_time:99613ms step_avg:59.61ms
step:1672/1775 train_time:99703ms step_avg:59.63ms
step:1673/1775 train_time:99788ms step_avg:59.65ms
step:1674/1775 train_time:99877ms step_avg:59.66ms
step:1675/1775 train_time:99964ms step_avg:59.68ms
step:1676/1775 train_time:100052ms step_avg:59.70ms
step:1677/1775 train_time:100138ms step_avg:59.71ms
step:1678/1775 train_time:100225ms step_avg:59.73ms
step:1679/1775 train_time:100312ms step_avg:59.74ms
step:1680/1775 train_time:100400ms step_avg:59.76ms
step:1681/1775 train_time:100486ms step_avg:59.78ms
step:1682/1775 train_time:100576ms step_avg:59.80ms
step:1683/1775 train_time:100663ms step_avg:59.81ms
step:1684/1775 train_time:100751ms step_avg:59.83ms
step:1685/1775 train_time:100837ms step_avg:59.84ms
step:1686/1775 train_time:100925ms step_avg:59.86ms
step:1687/1775 train_time:101012ms step_avg:59.88ms
step:1688/1775 train_time:101101ms step_avg:59.89ms
step:1689/1775 train_time:101187ms step_avg:59.91ms
step:1690/1775 train_time:101276ms step_avg:59.93ms
step:1691/1775 train_time:101362ms step_avg:59.94ms
step:1692/1775 train_time:101449ms step_avg:59.96ms
step:1693/1775 train_time:101536ms step_avg:59.97ms
step:1694/1775 train_time:101625ms step_avg:59.99ms
step:1695/1775 train_time:101711ms step_avg:60.01ms
step:1696/1775 train_time:101801ms step_avg:60.02ms
step:1697/1775 train_time:101886ms step_avg:60.04ms
step:1698/1775 train_time:101976ms step_avg:60.06ms
step:1699/1775 train_time:102063ms step_avg:60.07ms
step:1700/1775 train_time:102150ms step_avg:60.09ms
step:1701/1775 train_time:102237ms step_avg:60.10ms
step:1702/1775 train_time:102324ms step_avg:60.12ms
step:1703/1775 train_time:102411ms step_avg:60.14ms
step:1704/1775 train_time:102500ms step_avg:60.15ms
step:1705/1775 train_time:102586ms step_avg:60.17ms
step:1706/1775 train_time:102675ms step_avg:60.18ms
step:1707/1775 train_time:102762ms step_avg:60.20ms
step:1708/1775 train_time:102850ms step_avg:60.22ms
step:1709/1775 train_time:102936ms step_avg:60.23ms
step:1710/1775 train_time:103025ms step_avg:60.25ms
step:1711/1775 train_time:103111ms step_avg:60.26ms
step:1712/1775 train_time:103201ms step_avg:60.28ms
step:1713/1775 train_time:103286ms step_avg:60.30ms
step:1714/1775 train_time:103375ms step_avg:60.31ms
step:1715/1775 train_time:103462ms step_avg:60.33ms
step:1716/1775 train_time:103551ms step_avg:60.34ms
step:1717/1775 train_time:103636ms step_avg:60.36ms
step:1718/1775 train_time:103725ms step_avg:60.38ms
step:1719/1775 train_time:103811ms step_avg:60.39ms
step:1720/1775 train_time:103900ms step_avg:60.41ms
step:1721/1775 train_time:103986ms step_avg:60.42ms
step:1722/1775 train_time:104077ms step_avg:60.44ms
step:1723/1775 train_time:104164ms step_avg:60.46ms
step:1724/1775 train_time:104252ms step_avg:60.47ms
step:1725/1775 train_time:104338ms step_avg:60.49ms
step:1726/1775 train_time:104426ms step_avg:60.50ms
step:1727/1775 train_time:104512ms step_avg:60.52ms
step:1728/1775 train_time:104601ms step_avg:60.53ms
step:1729/1775 train_time:104686ms step_avg:60.55ms
step:1730/1775 train_time:104775ms step_avg:60.56ms
step:1731/1775 train_time:104862ms step_avg:60.58ms
step:1732/1775 train_time:104950ms step_avg:60.59ms
step:1733/1775 train_time:105037ms step_avg:60.61ms
step:1734/1775 train_time:105125ms step_avg:60.63ms
step:1735/1775 train_time:105213ms step_avg:60.64ms
step:1736/1775 train_time:105306ms step_avg:60.66ms
step:1737/1775 train_time:105393ms step_avg:60.68ms
step:1738/1775 train_time:105482ms step_avg:60.69ms
step:1739/1775 train_time:105568ms step_avg:60.71ms
step:1740/1775 train_time:105657ms step_avg:60.72ms
step:1741/1775 train_time:105744ms step_avg:60.74ms
step:1742/1775 train_time:105832ms step_avg:60.75ms
step:1743/1775 train_time:105919ms step_avg:60.77ms
step:1744/1775 train_time:106007ms step_avg:60.78ms
step:1745/1775 train_time:106094ms step_avg:60.80ms
step:1746/1775 train_time:106183ms step_avg:60.81ms
step:1747/1775 train_time:106270ms step_avg:60.83ms
step:1748/1775 train_time:106359ms step_avg:60.85ms
step:1749/1775 train_time:106444ms step_avg:60.86ms
step:1750/1775 train_time:106535ms step_avg:60.88ms
step:1750/1775 val_loss:3.2861 train_time:106633ms step_avg:60.93ms
step:1751/1775 train_time:106653ms step_avg:60.91ms
step:1752/1775 train_time:106715ms step_avg:60.91ms
step:1753/1775 train_time:106802ms step_avg:60.93ms
step:1754/1775 train_time:106891ms step_avg:60.94ms
step:1755/1775 train_time:106978ms step_avg:60.96ms
step:1756/1775 train_time:107066ms step_avg:60.97ms
step:1757/1775 train_time:107151ms step_avg:60.99ms
step:1758/1775 train_time:107239ms step_avg:61.00ms
step:1759/1775 train_time:107326ms step_avg:61.02ms
step:1760/1775 train_time:107416ms step_avg:61.03ms
step:1761/1775 train_time:107501ms step_avg:61.05ms
step:1762/1775 train_time:107591ms step_avg:61.06ms
step:1763/1775 train_time:107681ms step_avg:61.08ms
step:1764/1775 train_time:107771ms step_avg:61.09ms
step:1765/1775 train_time:107860ms step_avg:61.11ms
step:1766/1775 train_time:107950ms step_avg:61.13ms
step:1767/1775 train_time:108038ms step_avg:61.14ms
step:1768/1775 train_time:108126ms step_avg:61.16ms
step:1769/1775 train_time:108211ms step_avg:61.17ms
step:1770/1775 train_time:108300ms step_avg:61.19ms
step:1771/1775 train_time:108385ms step_avg:61.20ms
step:1772/1775 train_time:108474ms step_avg:61.22ms
step:1773/1775 train_time:108562ms step_avg:61.23ms
step:1774/1775 train_time:108652ms step_avg:61.25ms
step:1775/1775 train_time:108741ms step_avg:61.26ms
step:1775/1775 val_loss:3.2796 train_time:108844ms step_avg:61.32ms
peak memory allocated: 29148 MiB reserved: 44718 MiB

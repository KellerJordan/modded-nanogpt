import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Jan  7 09:04:49 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   44C    P0            128W /  700W |    1520MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   41C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     76110      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     76111      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     76112      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     76113      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     76114      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     76115      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     76116      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     76117      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8295 train_time:0ms step_avg:0.03ms
step:1/1775 train_time:71ms step_avg:71.35ms
step:2/1775 train_time:95ms step_avg:47.56ms
step:3/1775 train_time:115ms step_avg:38.33ms
step:4/1775 train_time:146ms step_avg:36.54ms
step:5/1775 train_time:178ms step_avg:35.61ms
step:6/1775 train_time:262ms step_avg:43.66ms
step:7/1775 train_time:281ms step_avg:40.09ms
step:8/1775 train_time:413ms step_avg:51.59ms
step:9/1775 train_time:445ms step_avg:49.40ms
step:10/1775 train_time:479ms step_avg:47.87ms
step:11/1775 train_time:511ms step_avg:46.41ms
step:12/1775 train_time:545ms step_avg:45.38ms
step:13/1775 train_time:577ms step_avg:44.36ms
step:14/1775 train_time:611ms step_avg:43.65ms
step:15/1775 train_time:643ms step_avg:42.89ms
step:16/1775 train_time:678ms step_avg:42.35ms
step:17/1775 train_time:710ms step_avg:41.77ms
step:18/1775 train_time:744ms step_avg:41.33ms
step:19/1775 train_time:776ms step_avg:40.86ms
step:20/1775 train_time:810ms step_avg:40.52ms
step:21/1775 train_time:843ms step_avg:40.12ms
step:22/1775 train_time:877ms step_avg:39.86ms
step:23/1775 train_time:909ms step_avg:39.51ms
step:24/1775 train_time:943ms step_avg:39.29ms
step:25/1775 train_time:975ms step_avg:39.00ms
step:26/1775 train_time:1009ms step_avg:38.82ms
step:27/1775 train_time:1041ms step_avg:38.56ms
step:28/1775 train_time:1075ms step_avg:38.40ms
step:29/1775 train_time:1108ms step_avg:38.19ms
step:30/1775 train_time:1142ms step_avg:38.05ms
step:31/1775 train_time:1174ms step_avg:37.86ms
step:32/1775 train_time:1208ms step_avg:37.74ms
step:33/1775 train_time:1240ms step_avg:37.57ms
step:34/1775 train_time:1274ms step_avg:37.48ms
step:35/1775 train_time:1307ms step_avg:37.34ms
step:36/1775 train_time:1342ms step_avg:37.27ms
step:37/1775 train_time:1374ms step_avg:37.14ms
step:38/1775 train_time:1409ms step_avg:37.07ms
step:39/1775 train_time:1441ms step_avg:36.95ms
step:40/1775 train_time:1475ms step_avg:36.88ms
step:41/1775 train_time:1508ms step_avg:36.77ms
step:42/1775 train_time:1542ms step_avg:36.71ms
step:43/1775 train_time:1574ms step_avg:36.61ms
step:44/1775 train_time:1608ms step_avg:36.56ms
step:45/1775 train_time:1641ms step_avg:36.46ms
step:46/1775 train_time:1675ms step_avg:36.41ms
step:47/1775 train_time:1707ms step_avg:36.33ms
step:48/1775 train_time:1742ms step_avg:36.29ms
step:49/1775 train_time:1774ms step_avg:36.20ms
step:50/1775 train_time:1808ms step_avg:36.17ms
step:51/1775 train_time:1840ms step_avg:36.08ms
step:52/1775 train_time:1875ms step_avg:36.05ms
step:53/1775 train_time:1907ms step_avg:35.98ms
step:54/1775 train_time:1941ms step_avg:35.95ms
step:55/1775 train_time:1973ms step_avg:35.88ms
step:56/1775 train_time:2007ms step_avg:35.85ms
step:57/1775 train_time:2039ms step_avg:35.78ms
step:58/1775 train_time:2073ms step_avg:35.75ms
step:59/1775 train_time:2106ms step_avg:35.69ms
step:60/1775 train_time:2140ms step_avg:35.66ms
step:61/1775 train_time:2172ms step_avg:35.60ms
step:62/1775 train_time:2206ms step_avg:35.58ms
step:63/1775 train_time:2238ms step_avg:35.53ms
step:64/1775 train_time:2273ms step_avg:35.51ms
step:65/1775 train_time:2305ms step_avg:35.46ms
step:66/1775 train_time:2339ms step_avg:35.44ms
step:67/1775 train_time:2372ms step_avg:35.40ms
step:68/1775 train_time:2406ms step_avg:35.38ms
step:69/1775 train_time:2439ms step_avg:35.35ms
step:70/1775 train_time:2474ms step_avg:35.34ms
step:71/1775 train_time:2506ms step_avg:35.30ms
step:72/1775 train_time:2541ms step_avg:35.29ms
step:73/1775 train_time:2573ms step_avg:35.25ms
step:74/1775 train_time:2607ms step_avg:35.24ms
step:75/1775 train_time:2640ms step_avg:35.20ms
step:76/1775 train_time:2674ms step_avg:35.18ms
step:77/1775 train_time:2706ms step_avg:35.14ms
step:78/1775 train_time:2740ms step_avg:35.13ms
step:79/1775 train_time:2772ms step_avg:35.09ms
step:80/1775 train_time:2806ms step_avg:35.08ms
step:81/1775 train_time:2838ms step_avg:35.04ms
step:82/1775 train_time:2873ms step_avg:35.03ms
step:83/1775 train_time:2905ms step_avg:35.00ms
step:84/1775 train_time:2939ms step_avg:34.99ms
step:85/1775 train_time:2971ms step_avg:34.96ms
step:86/1775 train_time:3005ms step_avg:34.94ms
step:87/1775 train_time:3037ms step_avg:34.91ms
step:88/1775 train_time:3072ms step_avg:34.91ms
step:89/1775 train_time:3104ms step_avg:34.87ms
step:90/1775 train_time:3138ms step_avg:34.87ms
step:91/1775 train_time:3170ms step_avg:34.84ms
step:92/1775 train_time:3204ms step_avg:34.83ms
step:93/1775 train_time:3236ms step_avg:34.80ms
step:94/1775 train_time:3271ms step_avg:34.79ms
step:95/1775 train_time:3302ms step_avg:34.76ms
step:96/1775 train_time:3337ms step_avg:34.76ms
step:97/1775 train_time:3369ms step_avg:34.74ms
step:98/1775 train_time:3404ms step_avg:34.73ms
step:99/1775 train_time:3436ms step_avg:34.71ms
step:100/1775 train_time:3471ms step_avg:34.71ms
step:101/1775 train_time:3503ms step_avg:34.68ms
step:102/1775 train_time:3537ms step_avg:34.68ms
step:103/1775 train_time:3569ms step_avg:34.65ms
step:104/1775 train_time:3603ms step_avg:34.65ms
step:105/1775 train_time:3636ms step_avg:34.63ms
step:106/1775 train_time:3670ms step_avg:34.63ms
step:107/1775 train_time:3702ms step_avg:34.60ms
step:108/1775 train_time:3737ms step_avg:34.60ms
step:109/1775 train_time:3769ms step_avg:34.58ms
step:110/1775 train_time:3803ms step_avg:34.57ms
step:111/1775 train_time:3835ms step_avg:34.55ms
step:112/1775 train_time:3869ms step_avg:34.55ms
step:113/1775 train_time:3901ms step_avg:34.53ms
step:114/1775 train_time:3936ms step_avg:34.53ms
step:115/1775 train_time:3968ms step_avg:34.51ms
step:116/1775 train_time:4002ms step_avg:34.50ms
step:117/1775 train_time:4034ms step_avg:34.48ms
step:118/1775 train_time:4069ms step_avg:34.48ms
step:119/1775 train_time:4101ms step_avg:34.46ms
step:120/1775 train_time:4135ms step_avg:34.46ms
step:121/1775 train_time:4167ms step_avg:34.44ms
step:122/1775 train_time:4201ms step_avg:34.44ms
step:123/1775 train_time:4233ms step_avg:34.41ms
step:124/1775 train_time:4267ms step_avg:34.41ms
step:125/1775 train_time:4299ms step_avg:34.40ms
step:126/1775 train_time:4334ms step_avg:34.40ms
step:127/1775 train_time:4366ms step_avg:34.38ms
step:128/1775 train_time:4400ms step_avg:34.38ms
step:129/1775 train_time:4432ms step_avg:34.36ms
step:130/1775 train_time:4466ms step_avg:34.36ms
step:131/1775 train_time:4499ms step_avg:34.34ms
step:132/1775 train_time:4533ms step_avg:34.34ms
step:133/1775 train_time:4565ms step_avg:34.32ms
step:134/1775 train_time:4600ms step_avg:34.33ms
step:135/1775 train_time:4632ms step_avg:34.31ms
step:136/1775 train_time:4666ms step_avg:34.31ms
step:137/1775 train_time:4698ms step_avg:34.29ms
step:138/1775 train_time:4733ms step_avg:34.30ms
step:139/1775 train_time:4765ms step_avg:34.28ms
step:140/1775 train_time:4800ms step_avg:34.28ms
step:141/1775 train_time:4832ms step_avg:34.27ms
step:142/1775 train_time:4866ms step_avg:34.27ms
step:143/1775 train_time:4898ms step_avg:34.26ms
step:144/1775 train_time:4933ms step_avg:34.25ms
step:145/1775 train_time:4965ms step_avg:34.24ms
step:146/1775 train_time:4999ms step_avg:34.24ms
step:147/1775 train_time:5031ms step_avg:34.23ms
step:148/1775 train_time:5065ms step_avg:34.23ms
step:149/1775 train_time:5097ms step_avg:34.21ms
step:150/1775 train_time:5132ms step_avg:34.21ms
step:151/1775 train_time:5164ms step_avg:34.20ms
step:152/1775 train_time:5198ms step_avg:34.20ms
step:153/1775 train_time:5231ms step_avg:34.19ms
step:154/1775 train_time:5265ms step_avg:34.19ms
step:155/1775 train_time:5296ms step_avg:34.17ms
step:156/1775 train_time:5330ms step_avg:34.17ms
step:157/1775 train_time:5362ms step_avg:34.15ms
step:158/1775 train_time:5396ms step_avg:34.15ms
step:159/1775 train_time:5429ms step_avg:34.15ms
step:160/1775 train_time:5463ms step_avg:34.15ms
step:161/1775 train_time:5495ms step_avg:34.13ms
step:162/1775 train_time:5530ms step_avg:34.13ms
step:163/1775 train_time:5562ms step_avg:34.12ms
step:164/1775 train_time:5596ms step_avg:34.12ms
step:165/1775 train_time:5628ms step_avg:34.11ms
step:166/1775 train_time:5663ms step_avg:34.11ms
step:167/1775 train_time:5695ms step_avg:34.10ms
step:168/1775 train_time:5730ms step_avg:34.10ms
step:169/1775 train_time:5762ms step_avg:34.09ms
step:170/1775 train_time:5796ms step_avg:34.10ms
step:171/1775 train_time:5828ms step_avg:34.08ms
step:172/1775 train_time:5862ms step_avg:34.08ms
step:173/1775 train_time:5894ms step_avg:34.07ms
step:174/1775 train_time:5929ms step_avg:34.07ms
step:175/1775 train_time:5961ms step_avg:34.06ms
step:176/1775 train_time:5995ms step_avg:34.06ms
step:177/1775 train_time:6028ms step_avg:34.06ms
step:178/1775 train_time:6062ms step_avg:34.05ms
step:179/1775 train_time:6094ms step_avg:34.04ms
step:180/1775 train_time:6128ms step_avg:34.04ms
step:181/1775 train_time:6160ms step_avg:34.03ms
step:182/1775 train_time:6195ms step_avg:34.04ms
step:183/1775 train_time:6227ms step_avg:34.03ms
step:184/1775 train_time:6261ms step_avg:34.03ms
step:185/1775 train_time:6293ms step_avg:34.02ms
step:186/1775 train_time:6327ms step_avg:34.01ms
step:187/1775 train_time:6359ms step_avg:34.00ms
step:188/1775 train_time:6393ms step_avg:34.01ms
step:189/1775 train_time:6425ms step_avg:34.00ms
step:190/1775 train_time:6460ms step_avg:34.00ms
step:191/1775 train_time:6492ms step_avg:33.99ms
step:192/1775 train_time:6526ms step_avg:33.99ms
step:193/1775 train_time:6558ms step_avg:33.98ms
step:194/1775 train_time:6592ms step_avg:33.98ms
step:195/1775 train_time:6625ms step_avg:33.97ms
step:196/1775 train_time:6659ms step_avg:33.97ms
step:197/1775 train_time:6691ms step_avg:33.96ms
step:198/1775 train_time:6725ms step_avg:33.97ms
step:199/1775 train_time:6757ms step_avg:33.96ms
step:200/1775 train_time:6792ms step_avg:33.96ms
step:201/1775 train_time:6824ms step_avg:33.95ms
step:202/1775 train_time:6859ms step_avg:33.95ms
step:203/1775 train_time:6891ms step_avg:33.95ms
step:204/1775 train_time:6925ms step_avg:33.95ms
step:205/1775 train_time:6957ms step_avg:33.94ms
step:206/1775 train_time:6991ms step_avg:33.94ms
step:207/1775 train_time:7023ms step_avg:33.93ms
step:208/1775 train_time:7057ms step_avg:33.93ms
step:209/1775 train_time:7090ms step_avg:33.92ms
step:210/1775 train_time:7124ms step_avg:33.92ms
step:211/1775 train_time:7156ms step_avg:33.91ms
step:212/1775 train_time:7190ms step_avg:33.92ms
step:213/1775 train_time:7223ms step_avg:33.91ms
step:214/1775 train_time:7257ms step_avg:33.91ms
step:215/1775 train_time:7289ms step_avg:33.90ms
step:216/1775 train_time:7323ms step_avg:33.90ms
step:217/1775 train_time:7356ms step_avg:33.90ms
step:218/1775 train_time:7390ms step_avg:33.90ms
step:219/1775 train_time:7422ms step_avg:33.89ms
step:220/1775 train_time:7457ms step_avg:33.89ms
step:221/1775 train_time:7489ms step_avg:33.89ms
step:222/1775 train_time:7523ms step_avg:33.89ms
step:223/1775 train_time:7555ms step_avg:33.88ms
step:224/1775 train_time:7590ms step_avg:33.88ms
step:225/1775 train_time:7622ms step_avg:33.87ms
step:226/1775 train_time:7656ms step_avg:33.88ms
step:227/1775 train_time:7688ms step_avg:33.87ms
step:228/1775 train_time:7722ms step_avg:33.87ms
step:229/1775 train_time:7754ms step_avg:33.86ms
step:230/1775 train_time:7788ms step_avg:33.86ms
step:231/1775 train_time:7821ms step_avg:33.86ms
step:232/1775 train_time:7855ms step_avg:33.86ms
step:233/1775 train_time:7888ms step_avg:33.85ms
step:234/1775 train_time:7922ms step_avg:33.85ms
step:235/1775 train_time:7954ms step_avg:33.85ms
step:236/1775 train_time:7988ms step_avg:33.85ms
step:237/1775 train_time:8020ms step_avg:33.84ms
step:238/1775 train_time:8054ms step_avg:33.84ms
step:239/1775 train_time:8086ms step_avg:33.83ms
step:240/1775 train_time:8121ms step_avg:33.84ms
step:241/1775 train_time:8153ms step_avg:33.83ms
step:242/1775 train_time:8187ms step_avg:33.83ms
step:243/1775 train_time:8219ms step_avg:33.82ms
step:244/1775 train_time:8253ms step_avg:33.82ms
step:245/1775 train_time:8285ms step_avg:33.82ms
step:246/1775 train_time:8320ms step_avg:33.82ms
step:247/1775 train_time:8352ms step_avg:33.81ms
step:248/1775 train_time:8386ms step_avg:33.81ms
step:249/1775 train_time:8418ms step_avg:33.81ms
step:250/1775 train_time:8452ms step_avg:33.81ms
step:250/1775 val_loss:4.6034 train_time:8493ms step_avg:33.97ms
step:251/1775 train_time:8516ms step_avg:33.93ms
step:252/1775 train_time:8535ms step_avg:33.87ms
step:253/1775 train_time:8554ms step_avg:33.81ms
step:254/1775 train_time:8588ms step_avg:33.81ms
step:255/1775 train_time:8620ms step_avg:33.80ms
step:256/1775 train_time:8655ms step_avg:33.81ms
step:257/1775 train_time:8687ms step_avg:33.80ms
step:258/1775 train_time:8721ms step_avg:33.80ms
step:259/1775 train_time:8753ms step_avg:33.79ms
step:260/1775 train_time:8787ms step_avg:33.80ms
step:261/1775 train_time:8819ms step_avg:33.79ms
step:262/1775 train_time:8853ms step_avg:33.79ms
step:263/1775 train_time:8885ms step_avg:33.78ms
step:264/1775 train_time:8919ms step_avg:33.78ms
step:265/1775 train_time:8951ms step_avg:33.78ms
step:266/1775 train_time:8986ms step_avg:33.78ms
step:267/1775 train_time:9017ms step_avg:33.77ms
step:268/1775 train_time:9052ms step_avg:33.77ms
step:269/1775 train_time:9084ms step_avg:33.77ms
step:270/1775 train_time:9118ms step_avg:33.77ms
step:271/1775 train_time:9150ms step_avg:33.76ms
step:272/1775 train_time:9184ms step_avg:33.76ms
step:273/1775 train_time:9216ms step_avg:33.76ms
step:274/1775 train_time:9250ms step_avg:33.76ms
step:275/1775 train_time:9282ms step_avg:33.75ms
step:276/1775 train_time:9316ms step_avg:33.75ms
step:277/1775 train_time:9348ms step_avg:33.75ms
step:278/1775 train_time:9382ms step_avg:33.75ms
step:279/1775 train_time:9414ms step_avg:33.74ms
step:280/1775 train_time:9448ms step_avg:33.74ms
step:281/1775 train_time:9481ms step_avg:33.74ms
step:282/1775 train_time:9516ms step_avg:33.74ms
step:283/1775 train_time:9548ms step_avg:33.74ms
step:284/1775 train_time:9583ms step_avg:33.74ms
step:285/1775 train_time:9615ms step_avg:33.74ms
step:286/1775 train_time:9650ms step_avg:33.74ms
step:287/1775 train_time:9682ms step_avg:33.73ms
step:288/1775 train_time:9716ms step_avg:33.74ms
step:289/1775 train_time:9748ms step_avg:33.73ms
step:290/1775 train_time:9783ms step_avg:33.73ms
step:291/1775 train_time:9815ms step_avg:33.73ms
step:292/1775 train_time:9849ms step_avg:33.73ms
step:293/1775 train_time:9881ms step_avg:33.72ms
step:294/1775 train_time:9915ms step_avg:33.72ms
step:295/1775 train_time:9947ms step_avg:33.72ms
step:296/1775 train_time:9981ms step_avg:33.72ms
step:297/1775 train_time:10012ms step_avg:33.71ms
step:298/1775 train_time:10047ms step_avg:33.71ms
step:299/1775 train_time:10079ms step_avg:33.71ms
step:300/1775 train_time:10113ms step_avg:33.71ms
step:301/1775 train_time:10145ms step_avg:33.70ms
step:302/1775 train_time:10180ms step_avg:33.71ms
step:303/1775 train_time:10212ms step_avg:33.70ms
step:304/1775 train_time:10246ms step_avg:33.70ms
step:305/1775 train_time:10278ms step_avg:33.70ms
step:306/1775 train_time:10312ms step_avg:33.70ms
step:307/1775 train_time:10344ms step_avg:33.69ms
step:308/1775 train_time:10378ms step_avg:33.69ms
step:309/1775 train_time:10410ms step_avg:33.69ms
step:310/1775 train_time:10444ms step_avg:33.69ms
step:311/1775 train_time:10476ms step_avg:33.69ms
step:312/1775 train_time:10511ms step_avg:33.69ms
step:313/1775 train_time:10544ms step_avg:33.69ms
step:314/1775 train_time:10578ms step_avg:33.69ms
step:315/1775 train_time:10610ms step_avg:33.68ms
step:316/1775 train_time:10645ms step_avg:33.69ms
step:317/1775 train_time:10677ms step_avg:33.68ms
step:318/1775 train_time:10711ms step_avg:33.68ms
step:319/1775 train_time:10743ms step_avg:33.68ms
step:320/1775 train_time:10777ms step_avg:33.68ms
step:321/1775 train_time:10809ms step_avg:33.67ms
step:322/1775 train_time:10843ms step_avg:33.67ms
step:323/1775 train_time:10875ms step_avg:33.67ms
step:324/1775 train_time:10909ms step_avg:33.67ms
step:325/1775 train_time:10942ms step_avg:33.67ms
step:326/1775 train_time:10976ms step_avg:33.67ms
step:327/1775 train_time:11008ms step_avg:33.66ms
step:328/1775 train_time:11042ms step_avg:33.67ms
step:329/1775 train_time:11074ms step_avg:33.66ms
step:330/1775 train_time:11108ms step_avg:33.66ms
step:331/1775 train_time:11140ms step_avg:33.66ms
step:332/1775 train_time:11174ms step_avg:33.66ms
step:333/1775 train_time:11206ms step_avg:33.65ms
step:334/1775 train_time:11240ms step_avg:33.65ms
step:335/1775 train_time:11272ms step_avg:33.65ms
step:336/1775 train_time:11307ms step_avg:33.65ms
step:337/1775 train_time:11339ms step_avg:33.65ms
step:338/1775 train_time:11373ms step_avg:33.65ms
step:339/1775 train_time:11405ms step_avg:33.64ms
step:340/1775 train_time:11439ms step_avg:33.65ms
step:341/1775 train_time:11471ms step_avg:33.64ms
step:342/1775 train_time:11505ms step_avg:33.64ms
step:343/1775 train_time:11537ms step_avg:33.64ms
step:344/1775 train_time:11572ms step_avg:33.64ms
step:345/1775 train_time:11604ms step_avg:33.63ms
step:346/1775 train_time:11638ms step_avg:33.64ms
step:347/1775 train_time:11670ms step_avg:33.63ms
step:348/1775 train_time:11705ms step_avg:33.63ms
step:349/1775 train_time:11737ms step_avg:33.63ms
step:350/1775 train_time:11771ms step_avg:33.63ms
step:351/1775 train_time:11803ms step_avg:33.63ms
step:352/1775 train_time:11838ms step_avg:33.63ms
step:353/1775 train_time:11869ms step_avg:33.62ms
step:354/1775 train_time:11903ms step_avg:33.63ms
step:355/1775 train_time:11936ms step_avg:33.62ms
step:356/1775 train_time:11970ms step_avg:33.62ms
step:357/1775 train_time:12002ms step_avg:33.62ms
step:358/1775 train_time:12036ms step_avg:33.62ms
step:359/1775 train_time:12067ms step_avg:33.61ms
step:360/1775 train_time:12102ms step_avg:33.62ms
step:361/1775 train_time:12134ms step_avg:33.61ms
step:362/1775 train_time:12168ms step_avg:33.61ms
step:363/1775 train_time:12200ms step_avg:33.61ms
step:364/1775 train_time:12234ms step_avg:33.61ms
step:365/1775 train_time:12266ms step_avg:33.61ms
step:366/1775 train_time:12300ms step_avg:33.61ms
step:367/1775 train_time:12332ms step_avg:33.60ms
step:368/1775 train_time:12367ms step_avg:33.60ms
step:369/1775 train_time:12399ms step_avg:33.60ms
step:370/1775 train_time:12433ms step_avg:33.60ms
step:371/1775 train_time:12465ms step_avg:33.60ms
step:372/1775 train_time:12499ms step_avg:33.60ms
step:373/1775 train_time:12531ms step_avg:33.60ms
step:374/1775 train_time:12565ms step_avg:33.60ms
step:375/1775 train_time:12598ms step_avg:33.59ms
step:376/1775 train_time:12632ms step_avg:33.60ms
step:377/1775 train_time:12664ms step_avg:33.59ms
step:378/1775 train_time:12698ms step_avg:33.59ms
step:379/1775 train_time:12730ms step_avg:33.59ms
step:380/1775 train_time:12765ms step_avg:33.59ms
step:381/1775 train_time:12797ms step_avg:33.59ms
step:382/1775 train_time:12831ms step_avg:33.59ms
step:383/1775 train_time:12863ms step_avg:33.59ms
step:384/1775 train_time:12898ms step_avg:33.59ms
step:385/1775 train_time:12930ms step_avg:33.58ms
step:386/1775 train_time:12964ms step_avg:33.59ms
step:387/1775 train_time:12996ms step_avg:33.58ms
step:388/1775 train_time:13030ms step_avg:33.58ms
step:389/1775 train_time:13063ms step_avg:33.58ms
step:390/1775 train_time:13097ms step_avg:33.58ms
step:391/1775 train_time:13129ms step_avg:33.58ms
step:392/1775 train_time:13163ms step_avg:33.58ms
step:393/1775 train_time:13195ms step_avg:33.57ms
step:394/1775 train_time:13229ms step_avg:33.58ms
step:395/1775 train_time:13261ms step_avg:33.57ms
step:396/1775 train_time:13295ms step_avg:33.57ms
step:397/1775 train_time:13327ms step_avg:33.57ms
step:398/1775 train_time:13362ms step_avg:33.57ms
step:399/1775 train_time:13393ms step_avg:33.57ms
step:400/1775 train_time:13428ms step_avg:33.57ms
step:401/1775 train_time:13460ms step_avg:33.57ms
step:402/1775 train_time:13494ms step_avg:33.57ms
step:403/1775 train_time:13526ms step_avg:33.56ms
step:404/1775 train_time:13560ms step_avg:33.56ms
step:405/1775 train_time:13592ms step_avg:33.56ms
step:406/1775 train_time:13626ms step_avg:33.56ms
step:407/1775 train_time:13658ms step_avg:33.56ms
step:408/1775 train_time:13692ms step_avg:33.56ms
step:409/1775 train_time:13724ms step_avg:33.55ms
step:410/1775 train_time:13758ms step_avg:33.56ms
step:411/1775 train_time:13790ms step_avg:33.55ms
step:412/1775 train_time:13824ms step_avg:33.55ms
step:413/1775 train_time:13857ms step_avg:33.55ms
step:414/1775 train_time:13891ms step_avg:33.55ms
step:415/1775 train_time:13923ms step_avg:33.55ms
step:416/1775 train_time:13957ms step_avg:33.55ms
step:417/1775 train_time:13989ms step_avg:33.55ms
step:418/1775 train_time:14023ms step_avg:33.55ms
step:419/1775 train_time:14056ms step_avg:33.55ms
step:420/1775 train_time:14090ms step_avg:33.55ms
step:421/1775 train_time:14122ms step_avg:33.54ms
step:422/1775 train_time:14156ms step_avg:33.55ms
step:423/1775 train_time:14188ms step_avg:33.54ms
step:424/1775 train_time:14222ms step_avg:33.54ms
step:425/1775 train_time:14254ms step_avg:33.54ms
step:426/1775 train_time:14288ms step_avg:33.54ms
step:427/1775 train_time:14321ms step_avg:33.54ms
step:428/1775 train_time:14355ms step_avg:33.54ms
step:429/1775 train_time:14387ms step_avg:33.54ms
step:430/1775 train_time:14421ms step_avg:33.54ms
step:431/1775 train_time:14453ms step_avg:33.53ms
step:432/1775 train_time:14487ms step_avg:33.54ms
step:433/1775 train_time:14520ms step_avg:33.53ms
step:434/1775 train_time:14554ms step_avg:33.53ms
step:435/1775 train_time:14586ms step_avg:33.53ms
step:436/1775 train_time:14620ms step_avg:33.53ms
step:437/1775 train_time:14652ms step_avg:33.53ms
step:438/1775 train_time:14687ms step_avg:33.53ms
step:439/1775 train_time:14719ms step_avg:33.53ms
step:440/1775 train_time:14753ms step_avg:33.53ms
step:441/1775 train_time:14785ms step_avg:33.53ms
step:442/1775 train_time:14819ms step_avg:33.53ms
step:443/1775 train_time:14851ms step_avg:33.52ms
step:444/1775 train_time:14886ms step_avg:33.53ms
step:445/1775 train_time:14918ms step_avg:33.52ms
step:446/1775 train_time:14952ms step_avg:33.52ms
step:447/1775 train_time:14984ms step_avg:33.52ms
step:448/1775 train_time:15018ms step_avg:33.52ms
step:449/1775 train_time:15051ms step_avg:33.52ms
step:450/1775 train_time:15085ms step_avg:33.52ms
step:451/1775 train_time:15117ms step_avg:33.52ms
step:452/1775 train_time:15151ms step_avg:33.52ms
step:453/1775 train_time:15183ms step_avg:33.52ms
step:454/1775 train_time:15217ms step_avg:33.52ms
step:455/1775 train_time:15249ms step_avg:33.51ms
step:456/1775 train_time:15283ms step_avg:33.52ms
step:457/1775 train_time:15315ms step_avg:33.51ms
step:458/1775 train_time:15350ms step_avg:33.51ms
step:459/1775 train_time:15382ms step_avg:33.51ms
step:460/1775 train_time:15416ms step_avg:33.51ms
step:461/1775 train_time:15448ms step_avg:33.51ms
step:462/1775 train_time:15483ms step_avg:33.51ms
step:463/1775 train_time:15515ms step_avg:33.51ms
step:464/1775 train_time:15549ms step_avg:33.51ms
step:465/1775 train_time:15581ms step_avg:33.51ms
step:466/1775 train_time:15615ms step_avg:33.51ms
step:467/1775 train_time:15647ms step_avg:33.51ms
step:468/1775 train_time:15682ms step_avg:33.51ms
step:469/1775 train_time:15714ms step_avg:33.50ms
step:470/1775 train_time:15748ms step_avg:33.51ms
step:471/1775 train_time:15780ms step_avg:33.50ms
step:472/1775 train_time:15814ms step_avg:33.50ms
step:473/1775 train_time:15847ms step_avg:33.50ms
step:474/1775 train_time:15881ms step_avg:33.50ms
step:475/1775 train_time:15913ms step_avg:33.50ms
step:476/1775 train_time:15947ms step_avg:33.50ms
step:477/1775 train_time:15979ms step_avg:33.50ms
step:478/1775 train_time:16013ms step_avg:33.50ms
step:479/1775 train_time:16045ms step_avg:33.50ms
step:480/1775 train_time:16079ms step_avg:33.50ms
step:481/1775 train_time:16111ms step_avg:33.50ms
step:482/1775 train_time:16145ms step_avg:33.50ms
step:483/1775 train_time:16177ms step_avg:33.49ms
step:484/1775 train_time:16211ms step_avg:33.49ms
step:485/1775 train_time:16243ms step_avg:33.49ms
step:486/1775 train_time:16277ms step_avg:33.49ms
step:487/1775 train_time:16309ms step_avg:33.49ms
step:488/1775 train_time:16343ms step_avg:33.49ms
step:489/1775 train_time:16375ms step_avg:33.49ms
step:490/1775 train_time:16410ms step_avg:33.49ms
step:491/1775 train_time:16442ms step_avg:33.49ms
step:492/1775 train_time:16476ms step_avg:33.49ms
step:493/1775 train_time:16508ms step_avg:33.48ms
step:494/1775 train_time:16542ms step_avg:33.49ms
step:495/1775 train_time:16574ms step_avg:33.48ms
step:496/1775 train_time:16609ms step_avg:33.49ms
step:497/1775 train_time:16641ms step_avg:33.48ms
step:498/1775 train_time:16675ms step_avg:33.48ms
step:499/1775 train_time:16707ms step_avg:33.48ms
step:500/1775 train_time:16742ms step_avg:33.48ms
step:500/1775 val_loss:4.2715 train_time:16782ms step_avg:33.56ms
step:501/1775 train_time:16805ms step_avg:33.54ms
step:502/1775 train_time:16824ms step_avg:33.51ms
step:503/1775 train_time:16845ms step_avg:33.49ms
step:504/1775 train_time:16879ms step_avg:33.49ms
step:505/1775 train_time:16911ms step_avg:33.49ms
step:506/1775 train_time:16946ms step_avg:33.49ms
step:507/1775 train_time:16978ms step_avg:33.49ms
step:508/1775 train_time:17012ms step_avg:33.49ms
step:509/1775 train_time:17045ms step_avg:33.49ms
step:510/1775 train_time:17079ms step_avg:33.49ms
step:511/1775 train_time:17111ms step_avg:33.49ms
step:512/1775 train_time:17145ms step_avg:33.49ms
step:513/1775 train_time:17177ms step_avg:33.48ms
step:514/1775 train_time:17211ms step_avg:33.49ms
step:515/1775 train_time:17243ms step_avg:33.48ms
step:516/1775 train_time:17277ms step_avg:33.48ms
step:517/1775 train_time:17309ms step_avg:33.48ms
step:518/1775 train_time:17343ms step_avg:33.48ms
step:519/1775 train_time:17375ms step_avg:33.48ms
step:520/1775 train_time:17410ms step_avg:33.48ms
step:521/1775 train_time:17441ms step_avg:33.48ms
step:522/1775 train_time:17476ms step_avg:33.48ms
step:523/1775 train_time:17507ms step_avg:33.47ms
step:524/1775 train_time:17542ms step_avg:33.48ms
step:525/1775 train_time:17574ms step_avg:33.47ms
step:526/1775 train_time:17608ms step_avg:33.47ms
step:527/1775 train_time:17640ms step_avg:33.47ms
step:528/1775 train_time:17674ms step_avg:33.47ms
step:529/1775 train_time:17705ms step_avg:33.47ms
step:530/1775 train_time:17740ms step_avg:33.47ms
step:531/1775 train_time:17772ms step_avg:33.47ms
step:532/1775 train_time:17806ms step_avg:33.47ms
step:533/1775 train_time:17839ms step_avg:33.47ms
step:534/1775 train_time:17874ms step_avg:33.47ms
step:535/1775 train_time:17906ms step_avg:33.47ms
step:536/1775 train_time:17940ms step_avg:33.47ms
step:537/1775 train_time:17972ms step_avg:33.47ms
step:538/1775 train_time:18007ms step_avg:33.47ms
step:539/1775 train_time:18039ms step_avg:33.47ms
step:540/1775 train_time:18073ms step_avg:33.47ms
step:541/1775 train_time:18105ms step_avg:33.47ms
step:542/1775 train_time:18140ms step_avg:33.47ms
step:543/1775 train_time:18171ms step_avg:33.46ms
step:544/1775 train_time:18206ms step_avg:33.47ms
step:545/1775 train_time:18238ms step_avg:33.46ms
step:546/1775 train_time:18272ms step_avg:33.47ms
step:547/1775 train_time:18304ms step_avg:33.46ms
step:548/1775 train_time:18338ms step_avg:33.46ms
step:549/1775 train_time:18370ms step_avg:33.46ms
step:550/1775 train_time:18405ms step_avg:33.46ms
step:551/1775 train_time:18437ms step_avg:33.46ms
step:552/1775 train_time:18471ms step_avg:33.46ms
step:553/1775 train_time:18503ms step_avg:33.46ms
step:554/1775 train_time:18537ms step_avg:33.46ms
step:555/1775 train_time:18569ms step_avg:33.46ms
step:556/1775 train_time:18603ms step_avg:33.46ms
step:557/1775 train_time:18635ms step_avg:33.46ms
step:558/1775 train_time:18669ms step_avg:33.46ms
step:559/1775 train_time:18701ms step_avg:33.45ms
step:560/1775 train_time:18736ms step_avg:33.46ms
step:561/1775 train_time:18768ms step_avg:33.45ms
step:562/1775 train_time:18802ms step_avg:33.46ms
step:563/1775 train_time:18835ms step_avg:33.45ms
step:564/1775 train_time:18869ms step_avg:33.46ms
step:565/1775 train_time:18901ms step_avg:33.45ms
step:566/1775 train_time:18936ms step_avg:33.46ms
step:567/1775 train_time:18968ms step_avg:33.45ms
step:568/1775 train_time:19002ms step_avg:33.45ms
step:569/1775 train_time:19034ms step_avg:33.45ms
step:570/1775 train_time:19068ms step_avg:33.45ms
step:571/1775 train_time:19100ms step_avg:33.45ms
step:572/1775 train_time:19135ms step_avg:33.45ms
step:573/1775 train_time:19167ms step_avg:33.45ms
step:574/1775 train_time:19201ms step_avg:33.45ms
step:575/1775 train_time:19233ms step_avg:33.45ms
step:576/1775 train_time:19267ms step_avg:33.45ms
step:577/1775 train_time:19300ms step_avg:33.45ms
step:578/1775 train_time:19334ms step_avg:33.45ms
step:579/1775 train_time:19367ms step_avg:33.45ms
step:580/1775 train_time:19403ms step_avg:33.45ms
step:581/1775 train_time:19464ms step_avg:33.50ms
step:582/1775 train_time:19525ms step_avg:33.55ms
step:583/1775 train_time:19584ms step_avg:33.59ms
step:584/1775 train_time:19646ms step_avg:33.64ms
step:585/1775 train_time:19705ms step_avg:33.68ms
step:586/1775 train_time:19767ms step_avg:33.73ms
step:587/1775 train_time:19827ms step_avg:33.78ms
step:588/1775 train_time:19889ms step_avg:33.82ms
step:589/1775 train_time:19949ms step_avg:33.87ms
step:590/1775 train_time:20012ms step_avg:33.92ms
step:591/1775 train_time:20072ms step_avg:33.96ms
step:592/1775 train_time:20135ms step_avg:34.01ms
step:593/1775 train_time:20196ms step_avg:34.06ms
step:594/1775 train_time:20258ms step_avg:34.10ms
step:595/1775 train_time:20318ms step_avg:34.15ms
step:596/1775 train_time:20381ms step_avg:34.20ms
step:597/1775 train_time:20440ms step_avg:34.24ms
step:598/1775 train_time:20501ms step_avg:34.28ms
step:599/1775 train_time:20562ms step_avg:34.33ms
step:600/1775 train_time:20623ms step_avg:34.37ms
step:601/1775 train_time:20682ms step_avg:34.41ms
step:602/1775 train_time:20745ms step_avg:34.46ms
step:603/1775 train_time:20805ms step_avg:34.50ms
step:604/1775 train_time:20867ms step_avg:34.55ms
step:605/1775 train_time:20926ms step_avg:34.59ms
step:606/1775 train_time:20988ms step_avg:34.63ms
step:607/1775 train_time:21048ms step_avg:34.67ms
step:608/1775 train_time:21109ms step_avg:34.72ms
step:609/1775 train_time:21169ms step_avg:34.76ms
step:610/1775 train_time:21231ms step_avg:34.80ms
step:611/1775 train_time:21291ms step_avg:34.85ms
step:612/1775 train_time:21354ms step_avg:34.89ms
step:613/1775 train_time:21414ms step_avg:34.93ms
step:614/1775 train_time:21477ms step_avg:34.98ms
step:615/1775 train_time:21537ms step_avg:35.02ms
step:616/1775 train_time:21600ms step_avg:35.06ms
step:617/1775 train_time:21660ms step_avg:35.11ms
step:618/1775 train_time:21722ms step_avg:35.15ms
step:619/1775 train_time:21782ms step_avg:35.19ms
step:620/1775 train_time:21845ms step_avg:35.23ms
step:621/1775 train_time:21905ms step_avg:35.27ms
step:622/1775 train_time:21968ms step_avg:35.32ms
step:623/1775 train_time:22027ms step_avg:35.36ms
step:624/1775 train_time:22090ms step_avg:35.40ms
step:625/1775 train_time:22149ms step_avg:35.44ms
step:626/1775 train_time:22211ms step_avg:35.48ms
step:627/1775 train_time:22270ms step_avg:35.52ms
step:628/1775 train_time:22332ms step_avg:35.56ms
step:629/1775 train_time:22392ms step_avg:35.60ms
step:630/1775 train_time:22455ms step_avg:35.64ms
step:631/1775 train_time:22515ms step_avg:35.68ms
step:632/1775 train_time:22578ms step_avg:35.72ms
step:633/1775 train_time:22638ms step_avg:35.76ms
step:634/1775 train_time:22701ms step_avg:35.81ms
step:635/1775 train_time:22761ms step_avg:35.84ms
step:636/1775 train_time:22823ms step_avg:35.89ms
step:637/1775 train_time:22883ms step_avg:35.92ms
step:638/1775 train_time:22945ms step_avg:35.96ms
step:639/1775 train_time:23005ms step_avg:36.00ms
step:640/1775 train_time:23067ms step_avg:36.04ms
step:641/1775 train_time:23126ms step_avg:36.08ms
step:642/1775 train_time:23189ms step_avg:36.12ms
step:643/1775 train_time:23248ms step_avg:36.16ms
step:644/1775 train_time:23311ms step_avg:36.20ms
step:645/1775 train_time:23370ms step_avg:36.23ms
step:646/1775 train_time:23432ms step_avg:36.27ms
step:647/1775 train_time:23492ms step_avg:36.31ms
step:648/1775 train_time:23554ms step_avg:36.35ms
step:649/1775 train_time:23615ms step_avg:36.39ms
step:650/1775 train_time:23678ms step_avg:36.43ms
step:651/1775 train_time:23738ms step_avg:36.46ms
step:652/1775 train_time:23801ms step_avg:36.51ms
step:653/1775 train_time:23861ms step_avg:36.54ms
step:654/1775 train_time:23924ms step_avg:36.58ms
step:655/1775 train_time:23983ms step_avg:36.62ms
step:656/1775 train_time:24046ms step_avg:36.66ms
step:657/1775 train_time:24106ms step_avg:36.69ms
step:658/1775 train_time:24168ms step_avg:36.73ms
step:659/1775 train_time:24227ms step_avg:36.76ms
step:660/1775 train_time:24290ms step_avg:36.80ms
step:661/1775 train_time:24349ms step_avg:36.84ms
step:662/1775 train_time:24410ms step_avg:36.87ms
step:663/1775 train_time:24470ms step_avg:36.91ms
step:664/1775 train_time:24533ms step_avg:36.95ms
step:665/1775 train_time:24593ms step_avg:36.98ms
step:666/1775 train_time:24655ms step_avg:37.02ms
step:667/1775 train_time:24716ms step_avg:37.05ms
step:668/1775 train_time:24779ms step_avg:37.09ms
step:669/1775 train_time:24840ms step_avg:37.13ms
step:670/1775 train_time:24903ms step_avg:37.17ms
step:671/1775 train_time:24963ms step_avg:37.20ms
step:672/1775 train_time:25024ms step_avg:37.24ms
step:673/1775 train_time:25084ms step_avg:37.27ms
step:674/1775 train_time:25146ms step_avg:37.31ms
step:675/1775 train_time:25207ms step_avg:37.34ms
step:676/1775 train_time:25269ms step_avg:37.38ms
step:677/1775 train_time:25329ms step_avg:37.41ms
step:678/1775 train_time:25390ms step_avg:37.45ms
step:679/1775 train_time:25449ms step_avg:37.48ms
step:680/1775 train_time:25512ms step_avg:37.52ms
step:681/1775 train_time:25571ms step_avg:37.55ms
step:682/1775 train_time:25634ms step_avg:37.59ms
step:683/1775 train_time:25694ms step_avg:37.62ms
step:684/1775 train_time:25757ms step_avg:37.66ms
step:685/1775 train_time:25818ms step_avg:37.69ms
step:686/1775 train_time:25881ms step_avg:37.73ms
step:687/1775 train_time:25942ms step_avg:37.76ms
step:688/1775 train_time:26004ms step_avg:37.80ms
step:689/1775 train_time:26063ms step_avg:37.83ms
step:690/1775 train_time:26125ms step_avg:37.86ms
step:691/1775 train_time:26185ms step_avg:37.89ms
step:692/1775 train_time:26248ms step_avg:37.93ms
step:693/1775 train_time:26308ms step_avg:37.96ms
step:694/1775 train_time:26370ms step_avg:38.00ms
step:695/1775 train_time:26429ms step_avg:38.03ms
step:696/1775 train_time:26491ms step_avg:38.06ms
step:697/1775 train_time:26551ms step_avg:38.09ms
step:698/1775 train_time:26613ms step_avg:38.13ms
step:699/1775 train_time:26673ms step_avg:38.16ms
step:700/1775 train_time:26735ms step_avg:38.19ms
step:701/1775 train_time:26795ms step_avg:38.22ms
step:702/1775 train_time:26858ms step_avg:38.26ms
step:703/1775 train_time:26919ms step_avg:38.29ms
step:704/1775 train_time:26981ms step_avg:38.33ms
step:705/1775 train_time:27041ms step_avg:38.36ms
step:706/1775 train_time:27104ms step_avg:38.39ms
step:707/1775 train_time:27164ms step_avg:38.42ms
step:708/1775 train_time:27227ms step_avg:38.46ms
step:709/1775 train_time:27286ms step_avg:38.49ms
step:710/1775 train_time:27349ms step_avg:38.52ms
step:711/1775 train_time:27408ms step_avg:38.55ms
step:712/1775 train_time:27470ms step_avg:38.58ms
step:713/1775 train_time:27529ms step_avg:38.61ms
step:714/1775 train_time:27591ms step_avg:38.64ms
step:715/1775 train_time:27651ms step_avg:38.67ms
step:716/1775 train_time:27713ms step_avg:38.71ms
step:717/1775 train_time:27773ms step_avg:38.74ms
step:718/1775 train_time:27836ms step_avg:38.77ms
step:719/1775 train_time:27896ms step_avg:38.80ms
step:720/1775 train_time:27959ms step_avg:38.83ms
step:721/1775 train_time:28020ms step_avg:38.86ms
step:722/1775 train_time:28082ms step_avg:38.89ms
step:723/1775 train_time:28142ms step_avg:38.92ms
step:724/1775 train_time:28204ms step_avg:38.96ms
step:725/1775 train_time:28265ms step_avg:38.99ms
step:726/1775 train_time:28326ms step_avg:39.02ms
step:727/1775 train_time:28386ms step_avg:39.05ms
step:728/1775 train_time:28448ms step_avg:39.08ms
step:729/1775 train_time:28507ms step_avg:39.10ms
step:730/1775 train_time:28570ms step_avg:39.14ms
step:731/1775 train_time:28629ms step_avg:39.16ms
step:732/1775 train_time:28692ms step_avg:39.20ms
step:733/1775 train_time:28751ms step_avg:39.22ms
step:734/1775 train_time:28814ms step_avg:39.26ms
step:735/1775 train_time:28874ms step_avg:39.28ms
step:736/1775 train_time:28938ms step_avg:39.32ms
step:737/1775 train_time:28998ms step_avg:39.35ms
step:738/1775 train_time:29061ms step_avg:39.38ms
step:739/1775 train_time:29121ms step_avg:39.41ms
step:740/1775 train_time:29183ms step_avg:39.44ms
step:741/1775 train_time:29244ms step_avg:39.47ms
step:742/1775 train_time:29305ms step_avg:39.49ms
step:743/1775 train_time:29365ms step_avg:39.52ms
step:744/1775 train_time:29428ms step_avg:39.55ms
step:745/1775 train_time:29488ms step_avg:39.58ms
step:746/1775 train_time:29549ms step_avg:39.61ms
step:747/1775 train_time:29608ms step_avg:39.64ms
step:748/1775 train_time:29671ms step_avg:39.67ms
step:749/1775 train_time:29729ms step_avg:39.69ms
step:750/1775 train_time:29792ms step_avg:39.72ms
step:750/1775 val_loss:3.9959 train_time:29863ms step_avg:39.82ms
step:751/1775 train_time:29889ms step_avg:39.80ms
step:752/1775 train_time:29918ms step_avg:39.78ms
step:753/1775 train_time:29978ms step_avg:39.81ms
step:754/1775 train_time:30039ms step_avg:39.84ms
step:755/1775 train_time:30099ms step_avg:39.87ms
step:756/1775 train_time:30161ms step_avg:39.90ms
step:757/1775 train_time:30220ms step_avg:39.92ms
step:758/1775 train_time:30281ms step_avg:39.95ms
step:759/1775 train_time:30340ms step_avg:39.97ms
step:760/1775 train_time:30402ms step_avg:40.00ms
step:761/1775 train_time:30462ms step_avg:40.03ms
step:762/1775 train_time:30523ms step_avg:40.06ms
step:763/1775 train_time:30581ms step_avg:40.08ms
step:764/1775 train_time:30643ms step_avg:40.11ms
step:765/1775 train_time:30702ms step_avg:40.13ms
step:766/1775 train_time:30764ms step_avg:40.16ms
step:767/1775 train_time:30824ms step_avg:40.19ms
step:768/1775 train_time:30887ms step_avg:40.22ms
step:769/1775 train_time:30948ms step_avg:40.24ms
step:770/1775 train_time:31011ms step_avg:40.27ms
step:771/1775 train_time:31072ms step_avg:40.30ms
step:772/1775 train_time:31134ms step_avg:40.33ms
step:773/1775 train_time:31194ms step_avg:40.35ms
step:774/1775 train_time:31257ms step_avg:40.38ms
step:775/1775 train_time:31316ms step_avg:40.41ms
step:776/1775 train_time:31378ms step_avg:40.44ms
step:777/1775 train_time:31437ms step_avg:40.46ms
step:778/1775 train_time:31499ms step_avg:40.49ms
step:779/1775 train_time:31557ms step_avg:40.51ms
step:780/1775 train_time:31619ms step_avg:40.54ms
step:781/1775 train_time:31678ms step_avg:40.56ms
step:782/1775 train_time:31739ms step_avg:40.59ms
step:783/1775 train_time:31799ms step_avg:40.61ms
step:784/1775 train_time:31862ms step_avg:40.64ms
step:785/1775 train_time:31922ms step_avg:40.67ms
step:786/1775 train_time:31986ms step_avg:40.69ms
step:787/1775 train_time:32045ms step_avg:40.72ms
step:788/1775 train_time:32108ms step_avg:40.75ms
step:789/1775 train_time:32169ms step_avg:40.77ms
step:790/1775 train_time:32231ms step_avg:40.80ms
step:791/1775 train_time:32291ms step_avg:40.82ms
step:792/1775 train_time:32354ms step_avg:40.85ms
step:793/1775 train_time:32414ms step_avg:40.88ms
step:794/1775 train_time:32476ms step_avg:40.90ms
step:795/1775 train_time:32536ms step_avg:40.93ms
step:796/1775 train_time:32598ms step_avg:40.95ms
step:797/1775 train_time:32657ms step_avg:40.97ms
step:798/1775 train_time:32718ms step_avg:41.00ms
step:799/1775 train_time:32778ms step_avg:41.02ms
step:800/1775 train_time:32840ms step_avg:41.05ms
step:801/1775 train_time:32900ms step_avg:41.07ms
step:802/1775 train_time:32962ms step_avg:41.10ms
step:803/1775 train_time:33022ms step_avg:41.12ms
step:804/1775 train_time:33084ms step_avg:41.15ms
step:805/1775 train_time:33143ms step_avg:41.17ms
step:806/1775 train_time:33205ms step_avg:41.20ms
step:807/1775 train_time:33265ms step_avg:41.22ms
step:808/1775 train_time:33328ms step_avg:41.25ms
step:809/1775 train_time:33387ms step_avg:41.27ms
step:810/1775 train_time:33450ms step_avg:41.30ms
step:811/1775 train_time:33510ms step_avg:41.32ms
step:812/1775 train_time:33573ms step_avg:41.35ms
step:813/1775 train_time:33633ms step_avg:41.37ms
step:814/1775 train_time:33696ms step_avg:41.40ms
step:815/1775 train_time:33756ms step_avg:41.42ms
step:816/1775 train_time:33818ms step_avg:41.44ms
step:817/1775 train_time:33878ms step_avg:41.47ms
step:818/1775 train_time:33940ms step_avg:41.49ms
step:819/1775 train_time:34000ms step_avg:41.51ms
step:820/1775 train_time:34064ms step_avg:41.54ms
step:821/1775 train_time:34123ms step_avg:41.56ms
step:822/1775 train_time:34185ms step_avg:41.59ms
step:823/1775 train_time:34244ms step_avg:41.61ms
step:824/1775 train_time:34306ms step_avg:41.63ms
step:825/1775 train_time:34365ms step_avg:41.66ms
step:826/1775 train_time:34428ms step_avg:41.68ms
step:827/1775 train_time:34488ms step_avg:41.70ms
step:828/1775 train_time:34550ms step_avg:41.73ms
step:829/1775 train_time:34611ms step_avg:41.75ms
step:830/1775 train_time:34673ms step_avg:41.77ms
step:831/1775 train_time:34733ms step_avg:41.80ms
step:832/1775 train_time:34796ms step_avg:41.82ms
step:833/1775 train_time:34856ms step_avg:41.84ms
step:834/1775 train_time:34918ms step_avg:41.87ms
step:835/1775 train_time:34978ms step_avg:41.89ms
step:836/1775 train_time:35041ms step_avg:41.91ms
step:837/1775 train_time:35100ms step_avg:41.94ms
step:838/1775 train_time:35163ms step_avg:41.96ms
step:839/1775 train_time:35223ms step_avg:41.98ms
step:840/1775 train_time:35285ms step_avg:42.01ms
step:841/1775 train_time:35343ms step_avg:42.03ms
step:842/1775 train_time:35406ms step_avg:42.05ms
step:843/1775 train_time:35465ms step_avg:42.07ms
step:844/1775 train_time:35528ms step_avg:42.09ms
step:845/1775 train_time:35588ms step_avg:42.12ms
step:846/1775 train_time:35650ms step_avg:42.14ms
step:847/1775 train_time:35711ms step_avg:42.16ms
step:848/1775 train_time:35774ms step_avg:42.19ms
step:849/1775 train_time:35834ms step_avg:42.21ms
step:850/1775 train_time:35897ms step_avg:42.23ms
step:851/1775 train_time:35957ms step_avg:42.25ms
step:852/1775 train_time:36019ms step_avg:42.28ms
step:853/1775 train_time:36079ms step_avg:42.30ms
step:854/1775 train_time:36141ms step_avg:42.32ms
step:855/1775 train_time:36201ms step_avg:42.34ms
step:856/1775 train_time:36264ms step_avg:42.36ms
step:857/1775 train_time:36323ms step_avg:42.38ms
step:858/1775 train_time:36385ms step_avg:42.41ms
step:859/1775 train_time:36444ms step_avg:42.43ms
step:860/1775 train_time:36505ms step_avg:42.45ms
step:861/1775 train_time:36565ms step_avg:42.47ms
step:862/1775 train_time:36627ms step_avg:42.49ms
step:863/1775 train_time:36687ms step_avg:42.51ms
step:864/1775 train_time:36750ms step_avg:42.53ms
step:865/1775 train_time:36810ms step_avg:42.55ms
step:866/1775 train_time:36873ms step_avg:42.58ms
step:867/1775 train_time:36934ms step_avg:42.60ms
step:868/1775 train_time:36997ms step_avg:42.62ms
step:869/1775 train_time:37056ms step_avg:42.64ms
step:870/1775 train_time:37118ms step_avg:42.66ms
step:871/1775 train_time:37178ms step_avg:42.68ms
step:872/1775 train_time:37240ms step_avg:42.71ms
step:873/1775 train_time:37300ms step_avg:42.73ms
step:874/1775 train_time:37363ms step_avg:42.75ms
step:875/1775 train_time:37422ms step_avg:42.77ms
step:876/1775 train_time:37484ms step_avg:42.79ms
step:877/1775 train_time:37543ms step_avg:42.81ms
step:878/1775 train_time:37605ms step_avg:42.83ms
step:879/1775 train_time:37665ms step_avg:42.85ms
step:880/1775 train_time:37727ms step_avg:42.87ms
step:881/1775 train_time:37787ms step_avg:42.89ms
step:882/1775 train_time:37849ms step_avg:42.91ms
step:883/1775 train_time:37910ms step_avg:42.93ms
step:884/1775 train_time:37973ms step_avg:42.96ms
step:885/1775 train_time:38034ms step_avg:42.98ms
step:886/1775 train_time:38097ms step_avg:43.00ms
step:887/1775 train_time:38158ms step_avg:43.02ms
step:888/1775 train_time:38220ms step_avg:43.04ms
step:889/1775 train_time:38279ms step_avg:43.06ms
step:890/1775 train_time:38341ms step_avg:43.08ms
step:891/1775 train_time:38401ms step_avg:43.10ms
step:892/1775 train_time:38463ms step_avg:43.12ms
step:893/1775 train_time:38522ms step_avg:43.14ms
step:894/1775 train_time:38584ms step_avg:43.16ms
step:895/1775 train_time:38643ms step_avg:43.18ms
step:896/1775 train_time:38705ms step_avg:43.20ms
step:897/1775 train_time:38764ms step_avg:43.22ms
step:898/1775 train_time:38827ms step_avg:43.24ms
step:899/1775 train_time:38887ms step_avg:43.26ms
step:900/1775 train_time:38950ms step_avg:43.28ms
step:901/1775 train_time:39011ms step_avg:43.30ms
step:902/1775 train_time:39074ms step_avg:43.32ms
step:903/1775 train_time:39134ms step_avg:43.34ms
step:904/1775 train_time:39197ms step_avg:43.36ms
step:905/1775 train_time:39257ms step_avg:43.38ms
step:906/1775 train_time:39319ms step_avg:43.40ms
step:907/1775 train_time:39379ms step_avg:43.42ms
step:908/1775 train_time:39440ms step_avg:43.44ms
step:909/1775 train_time:39500ms step_avg:43.45ms
step:910/1775 train_time:39562ms step_avg:43.48ms
step:911/1775 train_time:39622ms step_avg:43.49ms
step:912/1775 train_time:39684ms step_avg:43.51ms
step:913/1775 train_time:39743ms step_avg:43.53ms
step:914/1775 train_time:39805ms step_avg:43.55ms
step:915/1775 train_time:39864ms step_avg:43.57ms
step:916/1775 train_time:39927ms step_avg:43.59ms
step:917/1775 train_time:39987ms step_avg:43.61ms
step:918/1775 train_time:40049ms step_avg:43.63ms
step:919/1775 train_time:40111ms step_avg:43.65ms
step:920/1775 train_time:40174ms step_avg:43.67ms
step:921/1775 train_time:40234ms step_avg:43.69ms
step:922/1775 train_time:40296ms step_avg:43.70ms
step:923/1775 train_time:40356ms step_avg:43.72ms
step:924/1775 train_time:40418ms step_avg:43.74ms
step:925/1775 train_time:40478ms step_avg:43.76ms
step:926/1775 train_time:40540ms step_avg:43.78ms
step:927/1775 train_time:40599ms step_avg:43.80ms
step:928/1775 train_time:40660ms step_avg:43.81ms
step:929/1775 train_time:40720ms step_avg:43.83ms
step:930/1775 train_time:40783ms step_avg:43.85ms
step:931/1775 train_time:40842ms step_avg:43.87ms
step:932/1775 train_time:40904ms step_avg:43.89ms
step:933/1775 train_time:40964ms step_avg:43.91ms
step:934/1775 train_time:41026ms step_avg:43.93ms
step:935/1775 train_time:41086ms step_avg:43.94ms
step:936/1775 train_time:41149ms step_avg:43.96ms
step:937/1775 train_time:41210ms step_avg:43.98ms
step:938/1775 train_time:41274ms step_avg:44.00ms
step:939/1775 train_time:41333ms step_avg:44.02ms
step:940/1775 train_time:41396ms step_avg:44.04ms
step:941/1775 train_time:41456ms step_avg:44.06ms
step:942/1775 train_time:41518ms step_avg:44.07ms
step:943/1775 train_time:41577ms step_avg:44.09ms
step:944/1775 train_time:41639ms step_avg:44.11ms
step:945/1775 train_time:41699ms step_avg:44.13ms
step:946/1775 train_time:41761ms step_avg:44.14ms
step:947/1775 train_time:41820ms step_avg:44.16ms
step:948/1775 train_time:41882ms step_avg:44.18ms
step:949/1775 train_time:41941ms step_avg:44.20ms
step:950/1775 train_time:42004ms step_avg:44.21ms
step:951/1775 train_time:42064ms step_avg:44.23ms
step:952/1775 train_time:42126ms step_avg:44.25ms
step:953/1775 train_time:42186ms step_avg:44.27ms
step:954/1775 train_time:42248ms step_avg:44.28ms
step:955/1775 train_time:42308ms step_avg:44.30ms
step:956/1775 train_time:42371ms step_avg:44.32ms
step:957/1775 train_time:42432ms step_avg:44.34ms
step:958/1775 train_time:42495ms step_avg:44.36ms
step:959/1775 train_time:42555ms step_avg:44.37ms
step:960/1775 train_time:42617ms step_avg:44.39ms
step:961/1775 train_time:42677ms step_avg:44.41ms
step:962/1775 train_time:42738ms step_avg:44.43ms
step:963/1775 train_time:42798ms step_avg:44.44ms
step:964/1775 train_time:42859ms step_avg:44.46ms
step:965/1775 train_time:42919ms step_avg:44.48ms
step:966/1775 train_time:42983ms step_avg:44.50ms
step:967/1775 train_time:43042ms step_avg:44.51ms
step:968/1775 train_time:43104ms step_avg:44.53ms
step:969/1775 train_time:43164ms step_avg:44.54ms
step:970/1775 train_time:43226ms step_avg:44.56ms
step:971/1775 train_time:43286ms step_avg:44.58ms
step:972/1775 train_time:43348ms step_avg:44.60ms
step:973/1775 train_time:43408ms step_avg:44.61ms
step:974/1775 train_time:43471ms step_avg:44.63ms
step:975/1775 train_time:43530ms step_avg:44.65ms
step:976/1775 train_time:43593ms step_avg:44.66ms
step:977/1775 train_time:43654ms step_avg:44.68ms
step:978/1775 train_time:43717ms step_avg:44.70ms
step:979/1775 train_time:43776ms step_avg:44.72ms
step:980/1775 train_time:43838ms step_avg:44.73ms
step:981/1775 train_time:43898ms step_avg:44.75ms
step:982/1775 train_time:43960ms step_avg:44.77ms
step:983/1775 train_time:44020ms step_avg:44.78ms
step:984/1775 train_time:44082ms step_avg:44.80ms
step:985/1775 train_time:44142ms step_avg:44.81ms
step:986/1775 train_time:44203ms step_avg:44.83ms
step:987/1775 train_time:44263ms step_avg:44.85ms
step:988/1775 train_time:44325ms step_avg:44.86ms
step:989/1775 train_time:44384ms step_avg:44.88ms
step:990/1775 train_time:44447ms step_avg:44.90ms
step:991/1775 train_time:44507ms step_avg:44.91ms
step:992/1775 train_time:44569ms step_avg:44.93ms
step:993/1775 train_time:44629ms step_avg:44.94ms
step:994/1775 train_time:44693ms step_avg:44.96ms
step:995/1775 train_time:44753ms step_avg:44.98ms
step:996/1775 train_time:44817ms step_avg:45.00ms
step:997/1775 train_time:44877ms step_avg:45.01ms
step:998/1775 train_time:44938ms step_avg:45.03ms
step:999/1775 train_time:44998ms step_avg:45.04ms
step:1000/1775 train_time:45060ms step_avg:45.06ms
step:1000/1775 val_loss:3.7391 train_time:45131ms step_avg:45.13ms
step:1001/1775 train_time:45155ms step_avg:45.11ms
step:1002/1775 train_time:45185ms step_avg:45.10ms
step:1003/1775 train_time:45247ms step_avg:45.11ms
step:1004/1775 train_time:45309ms step_avg:45.13ms
step:1005/1775 train_time:45369ms step_avg:45.14ms
step:1006/1775 train_time:45432ms step_avg:45.16ms
step:1007/1775 train_time:45491ms step_avg:45.18ms
step:1008/1775 train_time:45553ms step_avg:45.19ms
step:1009/1775 train_time:45612ms step_avg:45.21ms
step:1010/1775 train_time:45673ms step_avg:45.22ms
step:1011/1775 train_time:45732ms step_avg:45.23ms
step:1012/1775 train_time:45794ms step_avg:45.25ms
step:1013/1775 train_time:45854ms step_avg:45.27ms
step:1014/1775 train_time:45915ms step_avg:45.28ms
step:1015/1775 train_time:45975ms step_avg:45.30ms
step:1016/1775 train_time:46036ms step_avg:45.31ms
step:1017/1775 train_time:46096ms step_avg:45.33ms
step:1018/1775 train_time:46159ms step_avg:45.34ms
step:1019/1775 train_time:46220ms step_avg:45.36ms
step:1020/1775 train_time:46282ms step_avg:45.37ms
step:1021/1775 train_time:46342ms step_avg:45.39ms
step:1022/1775 train_time:46405ms step_avg:45.41ms
step:1023/1775 train_time:46465ms step_avg:45.42ms
step:1024/1775 train_time:46528ms step_avg:45.44ms
step:1025/1775 train_time:46587ms step_avg:45.45ms
step:1026/1775 train_time:46650ms step_avg:45.47ms
step:1027/1775 train_time:46710ms step_avg:45.48ms
step:1028/1775 train_time:46771ms step_avg:45.50ms
step:1029/1775 train_time:46830ms step_avg:45.51ms
step:1030/1775 train_time:46893ms step_avg:45.53ms
step:1031/1775 train_time:46952ms step_avg:45.54ms
step:1032/1775 train_time:47013ms step_avg:45.56ms
step:1033/1775 train_time:47074ms step_avg:45.57ms
step:1034/1775 train_time:47136ms step_avg:45.59ms
step:1035/1775 train_time:47196ms step_avg:45.60ms
step:1036/1775 train_time:47258ms step_avg:45.62ms
step:1037/1775 train_time:47318ms step_avg:45.63ms
step:1038/1775 train_time:47380ms step_avg:45.65ms
step:1039/1775 train_time:47440ms step_avg:45.66ms
step:1040/1775 train_time:47503ms step_avg:45.68ms
step:1041/1775 train_time:47562ms step_avg:45.69ms
step:1042/1775 train_time:47624ms step_avg:45.70ms
step:1043/1775 train_time:47684ms step_avg:45.72ms
step:1044/1775 train_time:47746ms step_avg:45.73ms
step:1045/1775 train_time:47806ms step_avg:45.75ms
step:1046/1775 train_time:47868ms step_avg:45.76ms
step:1047/1775 train_time:47928ms step_avg:45.78ms
step:1048/1775 train_time:47991ms step_avg:45.79ms
step:1049/1775 train_time:48051ms step_avg:45.81ms
step:1050/1775 train_time:48113ms step_avg:45.82ms
step:1051/1775 train_time:48173ms step_avg:45.84ms
step:1052/1775 train_time:48235ms step_avg:45.85ms
step:1053/1775 train_time:48295ms step_avg:45.86ms
step:1054/1775 train_time:48357ms step_avg:45.88ms
step:1055/1775 train_time:48417ms step_avg:45.89ms
step:1056/1775 train_time:48478ms step_avg:45.91ms
step:1057/1775 train_time:48537ms step_avg:45.92ms
step:1058/1775 train_time:48600ms step_avg:45.94ms
step:1059/1775 train_time:48658ms step_avg:45.95ms
step:1060/1775 train_time:48721ms step_avg:45.96ms
step:1061/1775 train_time:48780ms step_avg:45.98ms
step:1062/1775 train_time:48842ms step_avg:45.99ms
step:1063/1775 train_time:48903ms step_avg:46.00ms
step:1064/1775 train_time:48965ms step_avg:46.02ms
step:1065/1775 train_time:49025ms step_avg:46.03ms
step:1066/1775 train_time:49088ms step_avg:46.05ms
step:1067/1775 train_time:49149ms step_avg:46.06ms
step:1068/1775 train_time:49211ms step_avg:46.08ms
step:1069/1775 train_time:49271ms step_avg:46.09ms
step:1070/1775 train_time:49333ms step_avg:46.11ms
step:1071/1775 train_time:49392ms step_avg:46.12ms
step:1072/1775 train_time:49454ms step_avg:46.13ms
step:1073/1775 train_time:49514ms step_avg:46.15ms
step:1074/1775 train_time:49577ms step_avg:46.16ms
step:1075/1775 train_time:49637ms step_avg:46.17ms
step:1076/1775 train_time:49699ms step_avg:46.19ms
step:1077/1775 train_time:49757ms step_avg:46.20ms
step:1078/1775 train_time:49820ms step_avg:46.21ms
step:1079/1775 train_time:49879ms step_avg:46.23ms
step:1080/1775 train_time:49941ms step_avg:46.24ms
step:1081/1775 train_time:50001ms step_avg:46.25ms
step:1082/1775 train_time:50063ms step_avg:46.27ms
step:1083/1775 train_time:50122ms step_avg:46.28ms
step:1084/1775 train_time:50185ms step_avg:46.30ms
step:1085/1775 train_time:50247ms step_avg:46.31ms
step:1086/1775 train_time:50309ms step_avg:46.33ms
step:1087/1775 train_time:50369ms step_avg:46.34ms
step:1088/1775 train_time:50432ms step_avg:46.35ms
step:1089/1775 train_time:50492ms step_avg:46.37ms
step:1090/1775 train_time:50554ms step_avg:46.38ms
step:1091/1775 train_time:50613ms step_avg:46.39ms
step:1092/1775 train_time:50675ms step_avg:46.41ms
step:1093/1775 train_time:50735ms step_avg:46.42ms
step:1094/1775 train_time:50797ms step_avg:46.43ms
step:1095/1775 train_time:50856ms step_avg:46.44ms
step:1096/1775 train_time:50918ms step_avg:46.46ms
step:1097/1775 train_time:50978ms step_avg:46.47ms
step:1098/1775 train_time:51040ms step_avg:46.48ms
step:1099/1775 train_time:51099ms step_avg:46.50ms
step:1100/1775 train_time:51161ms step_avg:46.51ms
step:1101/1775 train_time:51221ms step_avg:46.52ms
step:1102/1775 train_time:51284ms step_avg:46.54ms
step:1103/1775 train_time:51344ms step_avg:46.55ms
step:1104/1775 train_time:51407ms step_avg:46.56ms
step:1105/1775 train_time:51467ms step_avg:46.58ms
step:1106/1775 train_time:51530ms step_avg:46.59ms
step:1107/1775 train_time:51590ms step_avg:46.60ms
step:1108/1775 train_time:51652ms step_avg:46.62ms
step:1109/1775 train_time:51711ms step_avg:46.63ms
step:1110/1775 train_time:51773ms step_avg:46.64ms
step:1111/1775 train_time:51833ms step_avg:46.65ms
step:1112/1775 train_time:51895ms step_avg:46.67ms
step:1113/1775 train_time:51955ms step_avg:46.68ms
step:1114/1775 train_time:52017ms step_avg:46.69ms
step:1115/1775 train_time:52077ms step_avg:46.71ms
step:1116/1775 train_time:52139ms step_avg:46.72ms
step:1117/1775 train_time:52199ms step_avg:46.73ms
step:1118/1775 train_time:52260ms step_avg:46.74ms
step:1119/1775 train_time:52320ms step_avg:46.76ms
step:1120/1775 train_time:52382ms step_avg:46.77ms
step:1121/1775 train_time:52442ms step_avg:46.78ms
step:1122/1775 train_time:52505ms step_avg:46.80ms
step:1123/1775 train_time:52566ms step_avg:46.81ms
step:1124/1775 train_time:52628ms step_avg:46.82ms
step:1125/1775 train_time:52688ms step_avg:46.83ms
step:1126/1775 train_time:52750ms step_avg:46.85ms
step:1127/1775 train_time:52810ms step_avg:46.86ms
step:1128/1775 train_time:52873ms step_avg:46.87ms
step:1129/1775 train_time:52932ms step_avg:46.88ms
step:1130/1775 train_time:52993ms step_avg:46.90ms
step:1131/1775 train_time:53053ms step_avg:46.91ms
step:1132/1775 train_time:53115ms step_avg:46.92ms
step:1133/1775 train_time:53175ms step_avg:46.93ms
step:1134/1775 train_time:53238ms step_avg:46.95ms
step:1135/1775 train_time:53298ms step_avg:46.96ms
step:1136/1775 train_time:53360ms step_avg:46.97ms
step:1137/1775 train_time:53419ms step_avg:46.98ms
step:1138/1775 train_time:53481ms step_avg:47.00ms
step:1139/1775 train_time:53540ms step_avg:47.01ms
step:1140/1775 train_time:53603ms step_avg:47.02ms
step:1141/1775 train_time:53662ms step_avg:47.03ms
step:1142/1775 train_time:53725ms step_avg:47.04ms
step:1143/1775 train_time:53785ms step_avg:47.06ms
step:1144/1775 train_time:53849ms step_avg:47.07ms
step:1145/1775 train_time:53909ms step_avg:47.08ms
step:1146/1775 train_time:53972ms step_avg:47.10ms
step:1147/1775 train_time:54032ms step_avg:47.11ms
step:1148/1775 train_time:54094ms step_avg:47.12ms
step:1149/1775 train_time:54153ms step_avg:47.13ms
step:1150/1775 train_time:54215ms step_avg:47.14ms
step:1151/1775 train_time:54275ms step_avg:47.15ms
step:1152/1775 train_time:54337ms step_avg:47.17ms
step:1153/1775 train_time:54397ms step_avg:47.18ms
step:1154/1775 train_time:54459ms step_avg:47.19ms
step:1155/1775 train_time:54519ms step_avg:47.20ms
step:1156/1775 train_time:54581ms step_avg:47.22ms
step:1157/1775 train_time:54640ms step_avg:47.23ms
step:1158/1775 train_time:54705ms step_avg:47.24ms
step:1159/1775 train_time:54791ms step_avg:47.27ms
step:1160/1775 train_time:54881ms step_avg:47.31ms
step:1161/1775 train_time:54967ms step_avg:47.34ms
step:1162/1775 train_time:55058ms step_avg:47.38ms
step:1163/1775 train_time:55146ms step_avg:47.42ms
step:1164/1775 train_time:55234ms step_avg:47.45ms
step:1165/1775 train_time:55323ms step_avg:47.49ms
step:1166/1775 train_time:55412ms step_avg:47.52ms
step:1167/1775 train_time:55498ms step_avg:47.56ms
step:1168/1775 train_time:55586ms step_avg:47.59ms
step:1169/1775 train_time:55672ms step_avg:47.62ms
step:1170/1775 train_time:55763ms step_avg:47.66ms
step:1171/1775 train_time:55847ms step_avg:47.69ms
step:1172/1775 train_time:55937ms step_avg:47.73ms
step:1173/1775 train_time:56024ms step_avg:47.76ms
step:1174/1775 train_time:56111ms step_avg:47.80ms
step:1175/1775 train_time:56199ms step_avg:47.83ms
step:1176/1775 train_time:56288ms step_avg:47.86ms
step:1177/1775 train_time:56376ms step_avg:47.90ms
step:1178/1775 train_time:56466ms step_avg:47.93ms
step:1179/1775 train_time:56550ms step_avg:47.96ms
step:1180/1775 train_time:56639ms step_avg:48.00ms
step:1181/1775 train_time:56725ms step_avg:48.03ms
step:1182/1775 train_time:56815ms step_avg:48.07ms
step:1183/1775 train_time:56901ms step_avg:48.10ms
step:1184/1775 train_time:56990ms step_avg:48.13ms
step:1185/1775 train_time:57075ms step_avg:48.16ms
step:1186/1775 train_time:57165ms step_avg:48.20ms
step:1187/1775 train_time:57252ms step_avg:48.23ms
step:1188/1775 train_time:57342ms step_avg:48.27ms
step:1189/1775 train_time:57428ms step_avg:48.30ms
step:1190/1775 train_time:57517ms step_avg:48.33ms
step:1191/1775 train_time:57602ms step_avg:48.36ms
step:1192/1775 train_time:57690ms step_avg:48.40ms
step:1193/1775 train_time:57775ms step_avg:48.43ms
step:1194/1775 train_time:57864ms step_avg:48.46ms
step:1195/1775 train_time:57950ms step_avg:48.49ms
step:1196/1775 train_time:58039ms step_avg:48.53ms
step:1197/1775 train_time:58126ms step_avg:48.56ms
step:1198/1775 train_time:58216ms step_avg:48.59ms
step:1199/1775 train_time:58303ms step_avg:48.63ms
step:1200/1775 train_time:58391ms step_avg:48.66ms
step:1201/1775 train_time:58478ms step_avg:48.69ms
step:1202/1775 train_time:58566ms step_avg:48.72ms
step:1203/1775 train_time:58653ms step_avg:48.76ms
step:1204/1775 train_time:58742ms step_avg:48.79ms
step:1205/1775 train_time:58827ms step_avg:48.82ms
step:1206/1775 train_time:58916ms step_avg:48.85ms
step:1207/1775 train_time:59003ms step_avg:48.88ms
step:1208/1775 train_time:59090ms step_avg:48.92ms
step:1209/1775 train_time:59177ms step_avg:48.95ms
step:1210/1775 train_time:59265ms step_avg:48.98ms
step:1211/1775 train_time:59351ms step_avg:49.01ms
step:1212/1775 train_time:59442ms step_avg:49.04ms
step:1213/1775 train_time:59527ms step_avg:49.07ms
step:1214/1775 train_time:59617ms step_avg:49.11ms
step:1215/1775 train_time:59704ms step_avg:49.14ms
step:1216/1775 train_time:59792ms step_avg:49.17ms
step:1217/1775 train_time:59879ms step_avg:49.20ms
step:1218/1775 train_time:59967ms step_avg:49.23ms
step:1219/1775 train_time:60055ms step_avg:49.27ms
step:1220/1775 train_time:60144ms step_avg:49.30ms
step:1221/1775 train_time:60229ms step_avg:49.33ms
step:1222/1775 train_time:60319ms step_avg:49.36ms
step:1223/1775 train_time:60405ms step_avg:49.39ms
step:1224/1775 train_time:60494ms step_avg:49.42ms
step:1225/1775 train_time:60581ms step_avg:49.45ms
step:1226/1775 train_time:60669ms step_avg:49.49ms
step:1227/1775 train_time:60755ms step_avg:49.51ms
step:1228/1775 train_time:60845ms step_avg:49.55ms
step:1229/1775 train_time:60931ms step_avg:49.58ms
step:1230/1775 train_time:61020ms step_avg:49.61ms
step:1231/1775 train_time:61106ms step_avg:49.64ms
step:1232/1775 train_time:61195ms step_avg:49.67ms
step:1233/1775 train_time:61283ms step_avg:49.70ms
step:1234/1775 train_time:61370ms step_avg:49.73ms
step:1235/1775 train_time:61457ms step_avg:49.76ms
step:1236/1775 train_time:61546ms step_avg:49.79ms
step:1237/1775 train_time:61632ms step_avg:49.82ms
step:1238/1775 train_time:61721ms step_avg:49.86ms
step:1239/1775 train_time:61807ms step_avg:49.88ms
step:1240/1775 train_time:61898ms step_avg:49.92ms
step:1241/1775 train_time:61984ms step_avg:49.95ms
step:1242/1775 train_time:62071ms step_avg:49.98ms
step:1243/1775 train_time:62158ms step_avg:50.01ms
step:1244/1775 train_time:62246ms step_avg:50.04ms
step:1245/1775 train_time:62334ms step_avg:50.07ms
step:1246/1775 train_time:62422ms step_avg:50.10ms
step:1247/1775 train_time:62508ms step_avg:50.13ms
step:1248/1775 train_time:62597ms step_avg:50.16ms
step:1249/1775 train_time:62684ms step_avg:50.19ms
step:1250/1775 train_time:62773ms step_avg:50.22ms
step:1250/1775 val_loss:3.5035 train_time:62872ms step_avg:50.30ms
step:1251/1775 train_time:62896ms step_avg:50.28ms
step:1252/1775 train_time:62953ms step_avg:50.28ms
step:1253/1775 train_time:63041ms step_avg:50.31ms
step:1254/1775 train_time:63132ms step_avg:50.34ms
step:1255/1775 train_time:63216ms step_avg:50.37ms
step:1256/1775 train_time:63304ms step_avg:50.40ms
step:1257/1775 train_time:63388ms step_avg:50.43ms
step:1258/1775 train_time:63476ms step_avg:50.46ms
step:1259/1775 train_time:63561ms step_avg:50.49ms
step:1260/1775 train_time:63649ms step_avg:50.52ms
step:1261/1775 train_time:63734ms step_avg:50.54ms
step:1262/1775 train_time:63826ms step_avg:50.58ms
step:1263/1775 train_time:63913ms step_avg:50.60ms
step:1264/1775 train_time:64006ms step_avg:50.64ms
step:1265/1775 train_time:64092ms step_avg:50.67ms
step:1266/1775 train_time:64182ms step_avg:50.70ms
step:1267/1775 train_time:64267ms step_avg:50.72ms
step:1268/1775 train_time:64355ms step_avg:50.75ms
step:1269/1775 train_time:64440ms step_avg:50.78ms
step:1270/1775 train_time:64527ms step_avg:50.81ms
step:1271/1775 train_time:64612ms step_avg:50.84ms
step:1272/1775 train_time:64701ms step_avg:50.87ms
step:1273/1775 train_time:64788ms step_avg:50.89ms
step:1274/1775 train_time:64878ms step_avg:50.92ms
step:1275/1775 train_time:64970ms step_avg:50.96ms
step:1276/1775 train_time:65059ms step_avg:50.99ms
step:1277/1775 train_time:65146ms step_avg:51.01ms
step:1278/1775 train_time:65234ms step_avg:51.04ms
step:1279/1775 train_time:65320ms step_avg:51.07ms
step:1280/1775 train_time:65409ms step_avg:51.10ms
step:1281/1775 train_time:65493ms step_avg:51.13ms
step:1282/1775 train_time:65582ms step_avg:51.16ms
step:1283/1775 train_time:65667ms step_avg:51.18ms
step:1284/1775 train_time:65755ms step_avg:51.21ms
step:1285/1775 train_time:65843ms step_avg:51.24ms
step:1286/1775 train_time:65932ms step_avg:51.27ms
step:1287/1775 train_time:66021ms step_avg:51.30ms
step:1288/1775 train_time:66111ms step_avg:51.33ms
step:1289/1775 train_time:66197ms step_avg:51.36ms
step:1290/1775 train_time:66284ms step_avg:51.38ms
step:1291/1775 train_time:66370ms step_avg:51.41ms
step:1292/1775 train_time:66457ms step_avg:51.44ms
step:1293/1775 train_time:66543ms step_avg:51.46ms
step:1294/1775 train_time:66632ms step_avg:51.49ms
step:1295/1775 train_time:66718ms step_avg:51.52ms
step:1296/1775 train_time:66806ms step_avg:51.55ms
step:1297/1775 train_time:66892ms step_avg:51.57ms
step:1298/1775 train_time:66983ms step_avg:51.60ms
step:1299/1775 train_time:67070ms step_avg:51.63ms
step:1300/1775 train_time:67159ms step_avg:51.66ms
step:1301/1775 train_time:67245ms step_avg:51.69ms
step:1302/1775 train_time:67333ms step_avg:51.72ms
step:1303/1775 train_time:67419ms step_avg:51.74ms
step:1304/1775 train_time:67508ms step_avg:51.77ms
step:1305/1775 train_time:67594ms step_avg:51.80ms
step:1306/1775 train_time:67682ms step_avg:51.82ms
step:1307/1775 train_time:67768ms step_avg:51.85ms
step:1308/1775 train_time:67857ms step_avg:51.88ms
step:1309/1775 train_time:67945ms step_avg:51.91ms
step:1310/1775 train_time:68034ms step_avg:51.93ms
step:1311/1775 train_time:68121ms step_avg:51.96ms
step:1312/1775 train_time:68211ms step_avg:51.99ms
step:1313/1775 train_time:68296ms step_avg:52.02ms
step:1314/1775 train_time:68385ms step_avg:52.04ms
step:1315/1775 train_time:68470ms step_avg:52.07ms
step:1316/1775 train_time:68558ms step_avg:52.10ms
step:1317/1775 train_time:68645ms step_avg:52.12ms
step:1318/1775 train_time:68734ms step_avg:52.15ms
step:1319/1775 train_time:68821ms step_avg:52.18ms
step:1320/1775 train_time:68909ms step_avg:52.20ms
step:1321/1775 train_time:68997ms step_avg:52.23ms
step:1322/1775 train_time:69086ms step_avg:52.26ms
step:1323/1775 train_time:69172ms step_avg:52.28ms
step:1324/1775 train_time:69263ms step_avg:52.31ms
step:1325/1775 train_time:69349ms step_avg:52.34ms
step:1326/1775 train_time:69438ms step_avg:52.37ms
step:1327/1775 train_time:69523ms step_avg:52.39ms
step:1328/1775 train_time:69611ms step_avg:52.42ms
step:1329/1775 train_time:69698ms step_avg:52.44ms
step:1330/1775 train_time:69787ms step_avg:52.47ms
step:1331/1775 train_time:69873ms step_avg:52.50ms
step:1332/1775 train_time:69963ms step_avg:52.52ms
step:1333/1775 train_time:70050ms step_avg:52.55ms
step:1334/1775 train_time:70138ms step_avg:52.58ms
step:1335/1775 train_time:70225ms step_avg:52.60ms
step:1336/1775 train_time:70313ms step_avg:52.63ms
step:1337/1775 train_time:70399ms step_avg:52.65ms
step:1338/1775 train_time:70488ms step_avg:52.68ms
step:1339/1775 train_time:70574ms step_avg:52.71ms
step:1340/1775 train_time:70664ms step_avg:52.73ms
step:1341/1775 train_time:70750ms step_avg:52.76ms
step:1342/1775 train_time:70838ms step_avg:52.79ms
step:1343/1775 train_time:70925ms step_avg:52.81ms
step:1344/1775 train_time:71012ms step_avg:52.84ms
step:1345/1775 train_time:71100ms step_avg:52.86ms
step:1346/1775 train_time:71189ms step_avg:52.89ms
step:1347/1775 train_time:71275ms step_avg:52.91ms
step:1348/1775 train_time:71364ms step_avg:52.94ms
step:1349/1775 train_time:71450ms step_avg:52.97ms
step:1350/1775 train_time:71539ms step_avg:52.99ms
step:1351/1775 train_time:71624ms step_avg:53.02ms
step:1352/1775 train_time:71712ms step_avg:53.04ms
step:1353/1775 train_time:71799ms step_avg:53.07ms
step:1354/1775 train_time:71888ms step_avg:53.09ms
step:1355/1775 train_time:71973ms step_avg:53.12ms
step:1356/1775 train_time:72062ms step_avg:53.14ms
step:1357/1775 train_time:72148ms step_avg:53.17ms
step:1358/1775 train_time:72237ms step_avg:53.19ms
step:1359/1775 train_time:72323ms step_avg:53.22ms
step:1360/1775 train_time:72412ms step_avg:53.24ms
step:1361/1775 train_time:72498ms step_avg:53.27ms
step:1362/1775 train_time:72586ms step_avg:53.29ms
step:1363/1775 train_time:72671ms step_avg:53.32ms
step:1364/1775 train_time:72760ms step_avg:53.34ms
step:1365/1775 train_time:72848ms step_avg:53.37ms
step:1366/1775 train_time:72936ms step_avg:53.39ms
step:1367/1775 train_time:73022ms step_avg:53.42ms
step:1368/1775 train_time:73110ms step_avg:53.44ms
step:1369/1775 train_time:73197ms step_avg:53.47ms
step:1370/1775 train_time:73286ms step_avg:53.49ms
step:1371/1775 train_time:73372ms step_avg:53.52ms
step:1372/1775 train_time:73463ms step_avg:53.54ms
step:1373/1775 train_time:73549ms step_avg:53.57ms
step:1374/1775 train_time:73637ms step_avg:53.59ms
step:1375/1775 train_time:73723ms step_avg:53.62ms
step:1376/1775 train_time:73811ms step_avg:53.64ms
step:1377/1775 train_time:73898ms step_avg:53.67ms
step:1378/1775 train_time:73987ms step_avg:53.69ms
step:1379/1775 train_time:74072ms step_avg:53.71ms
step:1380/1775 train_time:74162ms step_avg:53.74ms
step:1381/1775 train_time:74248ms step_avg:53.76ms
step:1382/1775 train_time:74337ms step_avg:53.79ms
step:1383/1775 train_time:74423ms step_avg:53.81ms
step:1384/1775 train_time:74511ms step_avg:53.84ms
step:1385/1775 train_time:74597ms step_avg:53.86ms
step:1386/1775 train_time:74688ms step_avg:53.89ms
step:1387/1775 train_time:74773ms step_avg:53.91ms
step:1388/1775 train_time:74863ms step_avg:53.94ms
step:1389/1775 train_time:74949ms step_avg:53.96ms
step:1390/1775 train_time:75038ms step_avg:53.98ms
step:1391/1775 train_time:75124ms step_avg:54.01ms
step:1392/1775 train_time:75212ms step_avg:54.03ms
step:1393/1775 train_time:75298ms step_avg:54.05ms
step:1394/1775 train_time:75387ms step_avg:54.08ms
step:1395/1775 train_time:75472ms step_avg:54.10ms
step:1396/1775 train_time:75562ms step_avg:54.13ms
step:1397/1775 train_time:75648ms step_avg:54.15ms
step:1398/1775 train_time:75737ms step_avg:54.18ms
step:1399/1775 train_time:75824ms step_avg:54.20ms
step:1400/1775 train_time:75911ms step_avg:54.22ms
step:1401/1775 train_time:75998ms step_avg:54.25ms
step:1402/1775 train_time:76086ms step_avg:54.27ms
step:1403/1775 train_time:76172ms step_avg:54.29ms
step:1404/1775 train_time:76262ms step_avg:54.32ms
step:1405/1775 train_time:76348ms step_avg:54.34ms
step:1406/1775 train_time:76436ms step_avg:54.36ms
step:1407/1775 train_time:76522ms step_avg:54.39ms
step:1408/1775 train_time:76611ms step_avg:54.41ms
step:1409/1775 train_time:76698ms step_avg:54.43ms
step:1410/1775 train_time:76787ms step_avg:54.46ms
step:1411/1775 train_time:76873ms step_avg:54.48ms
step:1412/1775 train_time:76962ms step_avg:54.51ms
step:1413/1775 train_time:77048ms step_avg:54.53ms
step:1414/1775 train_time:77138ms step_avg:54.55ms
step:1415/1775 train_time:77224ms step_avg:54.58ms
step:1416/1775 train_time:77312ms step_avg:54.60ms
step:1417/1775 train_time:77398ms step_avg:54.62ms
step:1418/1775 train_time:77488ms step_avg:54.65ms
step:1419/1775 train_time:77573ms step_avg:54.67ms
step:1420/1775 train_time:77663ms step_avg:54.69ms
step:1421/1775 train_time:77750ms step_avg:54.72ms
step:1422/1775 train_time:77838ms step_avg:54.74ms
step:1423/1775 train_time:77923ms step_avg:54.76ms
step:1424/1775 train_time:78011ms step_avg:54.78ms
step:1425/1775 train_time:78098ms step_avg:54.81ms
step:1426/1775 train_time:78187ms step_avg:54.83ms
step:1427/1775 train_time:78273ms step_avg:54.85ms
step:1428/1775 train_time:78362ms step_avg:54.88ms
step:1429/1775 train_time:78449ms step_avg:54.90ms
step:1430/1775 train_time:78537ms step_avg:54.92ms
step:1431/1775 train_time:78623ms step_avg:54.94ms
step:1432/1775 train_time:78711ms step_avg:54.97ms
step:1433/1775 train_time:78798ms step_avg:54.99ms
step:1434/1775 train_time:78888ms step_avg:55.01ms
step:1435/1775 train_time:78973ms step_avg:55.03ms
step:1436/1775 train_time:79062ms step_avg:55.06ms
step:1437/1775 train_time:79148ms step_avg:55.08ms
step:1438/1775 train_time:79236ms step_avg:55.10ms
step:1439/1775 train_time:79323ms step_avg:55.12ms
step:1440/1775 train_time:79410ms step_avg:55.15ms
step:1441/1775 train_time:79496ms step_avg:55.17ms
step:1442/1775 train_time:79585ms step_avg:55.19ms
step:1443/1775 train_time:79671ms step_avg:55.21ms
step:1444/1775 train_time:79761ms step_avg:55.24ms
step:1445/1775 train_time:79848ms step_avg:55.26ms
step:1446/1775 train_time:79937ms step_avg:55.28ms
step:1447/1775 train_time:80022ms step_avg:55.30ms
step:1448/1775 train_time:80111ms step_avg:55.33ms
step:1449/1775 train_time:80198ms step_avg:55.35ms
step:1450/1775 train_time:80287ms step_avg:55.37ms
step:1451/1775 train_time:80373ms step_avg:55.39ms
step:1452/1775 train_time:80461ms step_avg:55.41ms
step:1453/1775 train_time:80547ms step_avg:55.43ms
step:1454/1775 train_time:80636ms step_avg:55.46ms
step:1455/1775 train_time:80723ms step_avg:55.48ms
step:1456/1775 train_time:80811ms step_avg:55.50ms
step:1457/1775 train_time:80899ms step_avg:55.52ms
step:1458/1775 train_time:80988ms step_avg:55.55ms
step:1459/1775 train_time:81072ms step_avg:55.57ms
step:1460/1775 train_time:81162ms step_avg:55.59ms
step:1461/1775 train_time:81249ms step_avg:55.61ms
step:1462/1775 train_time:81337ms step_avg:55.63ms
step:1463/1775 train_time:81422ms step_avg:55.65ms
step:1464/1775 train_time:81510ms step_avg:55.68ms
step:1465/1775 train_time:81597ms step_avg:55.70ms
step:1466/1775 train_time:81686ms step_avg:55.72ms
step:1467/1775 train_time:81772ms step_avg:55.74ms
step:1468/1775 train_time:81862ms step_avg:55.76ms
step:1469/1775 train_time:81948ms step_avg:55.79ms
step:1470/1775 train_time:82037ms step_avg:55.81ms
step:1471/1775 train_time:82123ms step_avg:55.83ms
step:1472/1775 train_time:82211ms step_avg:55.85ms
step:1473/1775 train_time:82298ms step_avg:55.87ms
step:1474/1775 train_time:82387ms step_avg:55.89ms
step:1475/1775 train_time:82473ms step_avg:55.91ms
step:1476/1775 train_time:82561ms step_avg:55.94ms
step:1477/1775 train_time:82648ms step_avg:55.96ms
step:1478/1775 train_time:82737ms step_avg:55.98ms
step:1479/1775 train_time:82823ms step_avg:56.00ms
step:1480/1775 train_time:82911ms step_avg:56.02ms
step:1481/1775 train_time:82998ms step_avg:56.04ms
step:1482/1775 train_time:83086ms step_avg:56.06ms
step:1483/1775 train_time:83172ms step_avg:56.08ms
step:1484/1775 train_time:83261ms step_avg:56.11ms
step:1485/1775 train_time:83348ms step_avg:56.13ms
step:1486/1775 train_time:83436ms step_avg:56.15ms
step:1487/1775 train_time:83522ms step_avg:56.17ms
step:1488/1775 train_time:83611ms step_avg:56.19ms
step:1489/1775 train_time:83697ms step_avg:56.21ms
step:1490/1775 train_time:83787ms step_avg:56.23ms
step:1491/1775 train_time:83871ms step_avg:56.25ms
step:1492/1775 train_time:83962ms step_avg:56.27ms
step:1493/1775 train_time:84047ms step_avg:56.29ms
step:1494/1775 train_time:84136ms step_avg:56.32ms
step:1495/1775 train_time:84224ms step_avg:56.34ms
step:1496/1775 train_time:84311ms step_avg:56.36ms
step:1497/1775 train_time:84398ms step_avg:56.38ms
step:1498/1775 train_time:84487ms step_avg:56.40ms
step:1499/1775 train_time:84572ms step_avg:56.42ms
step:1500/1775 train_time:84662ms step_avg:56.44ms
step:1500/1775 val_loss:3.3752 train_time:84760ms step_avg:56.51ms
step:1501/1775 train_time:84783ms step_avg:56.48ms
step:1502/1775 train_time:84842ms step_avg:56.49ms
step:1503/1775 train_time:84929ms step_avg:56.51ms
step:1504/1775 train_time:85019ms step_avg:56.53ms
step:1505/1775 train_time:85104ms step_avg:56.55ms
step:1506/1775 train_time:85193ms step_avg:56.57ms
step:1507/1775 train_time:85277ms step_avg:56.59ms
step:1508/1775 train_time:85366ms step_avg:56.61ms
step:1509/1775 train_time:85451ms step_avg:56.63ms
step:1510/1775 train_time:85538ms step_avg:56.65ms
step:1511/1775 train_time:85624ms step_avg:56.67ms
step:1512/1775 train_time:85714ms step_avg:56.69ms
step:1513/1775 train_time:85801ms step_avg:56.71ms
step:1514/1775 train_time:85893ms step_avg:56.73ms
step:1515/1775 train_time:85979ms step_avg:56.75ms
step:1516/1775 train_time:86071ms step_avg:56.78ms
step:1517/1775 train_time:86158ms step_avg:56.80ms
step:1518/1775 train_time:86246ms step_avg:56.82ms
step:1519/1775 train_time:86331ms step_avg:56.83ms
step:1520/1775 train_time:86420ms step_avg:56.85ms
step:1521/1775 train_time:86505ms step_avg:56.87ms
step:1522/1775 train_time:86593ms step_avg:56.89ms
step:1523/1775 train_time:86680ms step_avg:56.91ms
step:1524/1775 train_time:86770ms step_avg:56.94ms
step:1525/1775 train_time:86858ms step_avg:56.96ms
step:1526/1775 train_time:86947ms step_avg:56.98ms
step:1527/1775 train_time:87035ms step_avg:57.00ms
step:1528/1775 train_time:87123ms step_avg:57.02ms
step:1529/1775 train_time:87208ms step_avg:57.04ms
step:1530/1775 train_time:87296ms step_avg:57.06ms
step:1531/1775 train_time:87380ms step_avg:57.07ms
step:1532/1775 train_time:87468ms step_avg:57.09ms
step:1533/1775 train_time:87555ms step_avg:57.11ms
step:1534/1775 train_time:87643ms step_avg:57.13ms
step:1535/1775 train_time:87729ms step_avg:57.15ms
step:1536/1775 train_time:87818ms step_avg:57.17ms
step:1537/1775 train_time:87906ms step_avg:57.19ms
step:1538/1775 train_time:87996ms step_avg:57.21ms
step:1539/1775 train_time:88082ms step_avg:57.23ms
step:1540/1775 train_time:88171ms step_avg:57.25ms
step:1541/1775 train_time:88258ms step_avg:57.27ms
step:1542/1775 train_time:88346ms step_avg:57.29ms
step:1543/1775 train_time:88431ms step_avg:57.31ms
step:1544/1775 train_time:88518ms step_avg:57.33ms
step:1545/1775 train_time:88603ms step_avg:57.35ms
step:1546/1775 train_time:88693ms step_avg:57.37ms
step:1547/1775 train_time:88779ms step_avg:57.39ms
step:1548/1775 train_time:88869ms step_avg:57.41ms
step:1549/1775 train_time:88956ms step_avg:57.43ms
step:1550/1775 train_time:89045ms step_avg:57.45ms
step:1551/1775 train_time:89131ms step_avg:57.47ms
step:1552/1775 train_time:89219ms step_avg:57.49ms
step:1553/1775 train_time:89305ms step_avg:57.51ms
step:1554/1775 train_time:89394ms step_avg:57.52ms
step:1555/1775 train_time:89479ms step_avg:57.54ms
step:1556/1775 train_time:89567ms step_avg:57.56ms
step:1557/1775 train_time:89654ms step_avg:57.58ms
step:1558/1775 train_time:89742ms step_avg:57.60ms
step:1559/1775 train_time:89829ms step_avg:57.62ms
step:1560/1775 train_time:89918ms step_avg:57.64ms
step:1561/1775 train_time:90005ms step_avg:57.66ms
step:1562/1775 train_time:90095ms step_avg:57.68ms
step:1563/1775 train_time:90181ms step_avg:57.70ms
step:1564/1775 train_time:90270ms step_avg:57.72ms
step:1565/1775 train_time:90355ms step_avg:57.74ms
step:1566/1775 train_time:90444ms step_avg:57.75ms
step:1567/1775 train_time:90530ms step_avg:57.77ms
step:1568/1775 train_time:90618ms step_avg:57.79ms
step:1569/1775 train_time:90703ms step_avg:57.81ms
step:1570/1775 train_time:90793ms step_avg:57.83ms
step:1571/1775 train_time:90879ms step_avg:57.85ms
step:1572/1775 train_time:90969ms step_avg:57.87ms
step:1573/1775 train_time:91057ms step_avg:57.89ms
step:1574/1775 train_time:91146ms step_avg:57.91ms
step:1575/1775 train_time:91231ms step_avg:57.92ms
step:1576/1775 train_time:91319ms step_avg:57.94ms
step:1577/1775 train_time:91406ms step_avg:57.96ms
step:1578/1775 train_time:91494ms step_avg:57.98ms
step:1579/1775 train_time:91580ms step_avg:58.00ms
step:1580/1775 train_time:91669ms step_avg:58.02ms
step:1581/1775 train_time:91756ms step_avg:58.04ms
step:1582/1775 train_time:91843ms step_avg:58.06ms
step:1583/1775 train_time:91931ms step_avg:58.07ms
step:1584/1775 train_time:92020ms step_avg:58.09ms
step:1585/1775 train_time:92106ms step_avg:58.11ms
step:1586/1775 train_time:92196ms step_avg:58.13ms
step:1587/1775 train_time:92282ms step_avg:58.15ms
step:1588/1775 train_time:92372ms step_avg:58.17ms
step:1589/1775 train_time:92458ms step_avg:58.19ms
step:1590/1775 train_time:92548ms step_avg:58.21ms
step:1591/1775 train_time:92636ms step_avg:58.22ms
step:1592/1775 train_time:92724ms step_avg:58.24ms
step:1593/1775 train_time:92809ms step_avg:58.26ms
step:1594/1775 train_time:92898ms step_avg:58.28ms
step:1595/1775 train_time:92985ms step_avg:58.30ms
step:1596/1775 train_time:93075ms step_avg:58.32ms
step:1597/1775 train_time:93160ms step_avg:58.33ms
step:1598/1775 train_time:93249ms step_avg:58.35ms
step:1599/1775 train_time:93336ms step_avg:58.37ms
step:1600/1775 train_time:93425ms step_avg:58.39ms
step:1601/1775 train_time:93511ms step_avg:58.41ms
step:1602/1775 train_time:93600ms step_avg:58.43ms
step:1603/1775 train_time:93687ms step_avg:58.44ms
step:1604/1775 train_time:93776ms step_avg:58.46ms
step:1605/1775 train_time:93861ms step_avg:58.48ms
step:1606/1775 train_time:93950ms step_avg:58.50ms
step:1607/1775 train_time:94037ms step_avg:58.52ms
step:1608/1775 train_time:94126ms step_avg:58.54ms
step:1609/1775 train_time:94213ms step_avg:58.55ms
step:1610/1775 train_time:94302ms step_avg:58.57ms
step:1611/1775 train_time:94388ms step_avg:58.59ms
step:1612/1775 train_time:94477ms step_avg:58.61ms
step:1613/1775 train_time:94562ms step_avg:58.62ms
step:1614/1775 train_time:94651ms step_avg:58.64ms
step:1615/1775 train_time:94738ms step_avg:58.66ms
step:1616/1775 train_time:94827ms step_avg:58.68ms
step:1617/1775 train_time:94913ms step_avg:58.70ms
step:1618/1775 train_time:95001ms step_avg:58.71ms
step:1619/1775 train_time:95086ms step_avg:58.73ms
step:1620/1775 train_time:95176ms step_avg:58.75ms
step:1621/1775 train_time:95263ms step_avg:58.77ms
step:1622/1775 train_time:95351ms step_avg:58.79ms
step:1623/1775 train_time:95439ms step_avg:58.80ms
step:1624/1775 train_time:95528ms step_avg:58.82ms
step:1625/1775 train_time:95613ms step_avg:58.84ms
step:1626/1775 train_time:95701ms step_avg:58.86ms
step:1627/1775 train_time:95786ms step_avg:58.87ms
step:1628/1775 train_time:95878ms step_avg:58.89ms
step:1629/1775 train_time:95963ms step_avg:58.91ms
step:1630/1775 train_time:96052ms step_avg:58.93ms
step:1631/1775 train_time:96139ms step_avg:58.94ms
step:1632/1775 train_time:96228ms step_avg:58.96ms
step:1633/1775 train_time:96315ms step_avg:58.98ms
step:1634/1775 train_time:96404ms step_avg:59.00ms
step:1635/1775 train_time:96491ms step_avg:59.02ms
step:1636/1775 train_time:96580ms step_avg:59.03ms
step:1637/1775 train_time:96666ms step_avg:59.05ms
step:1638/1775 train_time:96755ms step_avg:59.07ms
step:1639/1775 train_time:96841ms step_avg:59.09ms
step:1640/1775 train_time:96932ms step_avg:59.10ms
step:1641/1775 train_time:97017ms step_avg:59.12ms
step:1642/1775 train_time:97106ms step_avg:59.14ms
step:1643/1775 train_time:97193ms step_avg:59.16ms
step:1644/1775 train_time:97281ms step_avg:59.17ms
step:1645/1775 train_time:97368ms step_avg:59.19ms
step:1646/1775 train_time:97456ms step_avg:59.21ms
step:1647/1775 train_time:97542ms step_avg:59.22ms
step:1648/1775 train_time:97630ms step_avg:59.24ms
step:1649/1775 train_time:97717ms step_avg:59.26ms
step:1650/1775 train_time:97805ms step_avg:59.28ms
step:1651/1775 train_time:97892ms step_avg:59.29ms
step:1652/1775 train_time:97980ms step_avg:59.31ms
step:1653/1775 train_time:98066ms step_avg:59.33ms
step:1654/1775 train_time:98156ms step_avg:59.34ms
step:1655/1775 train_time:98242ms step_avg:59.36ms
step:1656/1775 train_time:98331ms step_avg:59.38ms
step:1657/1775 train_time:98417ms step_avg:59.39ms
step:1658/1775 train_time:98506ms step_avg:59.41ms
step:1659/1775 train_time:98593ms step_avg:59.43ms
step:1660/1775 train_time:98681ms step_avg:59.45ms
step:1661/1775 train_time:98768ms step_avg:59.46ms
step:1662/1775 train_time:98857ms step_avg:59.48ms
step:1663/1775 train_time:98942ms step_avg:59.50ms
step:1664/1775 train_time:99031ms step_avg:59.51ms
step:1665/1775 train_time:99117ms step_avg:59.53ms
step:1666/1775 train_time:99206ms step_avg:59.55ms
step:1667/1775 train_time:99293ms step_avg:59.56ms
step:1668/1775 train_time:99381ms step_avg:59.58ms
step:1669/1775 train_time:99469ms step_avg:59.60ms
step:1670/1775 train_time:99559ms step_avg:59.62ms
step:1671/1775 train_time:99645ms step_avg:59.63ms
step:1672/1775 train_time:99734ms step_avg:59.65ms
step:1673/1775 train_time:99820ms step_avg:59.67ms
step:1674/1775 train_time:99908ms step_avg:59.68ms
step:1675/1775 train_time:99994ms step_avg:59.70ms
step:1676/1775 train_time:100083ms step_avg:59.72ms
step:1677/1775 train_time:100170ms step_avg:59.73ms
step:1678/1775 train_time:100259ms step_avg:59.75ms
step:1679/1775 train_time:100345ms step_avg:59.76ms
step:1680/1775 train_time:100434ms step_avg:59.78ms
step:1681/1775 train_time:100519ms step_avg:59.80ms
step:1682/1775 train_time:100609ms step_avg:59.82ms
step:1683/1775 train_time:100695ms step_avg:59.83ms
step:1684/1775 train_time:100783ms step_avg:59.85ms
step:1685/1775 train_time:100870ms step_avg:59.86ms
step:1686/1775 train_time:100958ms step_avg:59.88ms
step:1687/1775 train_time:101043ms step_avg:59.89ms
step:1688/1775 train_time:101132ms step_avg:59.91ms
step:1689/1775 train_time:101217ms step_avg:59.93ms
step:1690/1775 train_time:101307ms step_avg:59.94ms
step:1691/1775 train_time:101393ms step_avg:59.96ms
step:1692/1775 train_time:101479ms step_avg:59.98ms
step:1693/1775 train_time:101567ms step_avg:59.99ms
step:1694/1775 train_time:101656ms step_avg:60.01ms
step:1695/1775 train_time:101741ms step_avg:60.02ms
step:1696/1775 train_time:101829ms step_avg:60.04ms
step:1697/1775 train_time:101916ms step_avg:60.06ms
step:1698/1775 train_time:102005ms step_avg:60.07ms
step:1699/1775 train_time:102092ms step_avg:60.09ms
step:1700/1775 train_time:102179ms step_avg:60.11ms
step:1701/1775 train_time:102267ms step_avg:60.12ms
step:1702/1775 train_time:102357ms step_avg:60.14ms
step:1703/1775 train_time:102443ms step_avg:60.15ms
step:1704/1775 train_time:102533ms step_avg:60.17ms
step:1705/1775 train_time:102618ms step_avg:60.19ms
step:1706/1775 train_time:102707ms step_avg:60.20ms
step:1707/1775 train_time:102792ms step_avg:60.22ms
step:1708/1775 train_time:102881ms step_avg:60.23ms
step:1709/1775 train_time:102968ms step_avg:60.25ms
step:1710/1775 train_time:103057ms step_avg:60.27ms
step:1711/1775 train_time:103142ms step_avg:60.28ms
step:1712/1775 train_time:103234ms step_avg:60.30ms
step:1713/1775 train_time:103319ms step_avg:60.31ms
step:1714/1775 train_time:103408ms step_avg:60.33ms
step:1715/1775 train_time:103495ms step_avg:60.35ms
step:1716/1775 train_time:103583ms step_avg:60.36ms
step:1717/1775 train_time:103671ms step_avg:60.38ms
step:1718/1775 train_time:103759ms step_avg:60.40ms
step:1719/1775 train_time:103845ms step_avg:60.41ms
step:1720/1775 train_time:103935ms step_avg:60.43ms
step:1721/1775 train_time:104020ms step_avg:60.44ms
step:1722/1775 train_time:104109ms step_avg:60.46ms
step:1723/1775 train_time:104197ms step_avg:60.47ms
step:1724/1775 train_time:104285ms step_avg:60.49ms
step:1725/1775 train_time:104373ms step_avg:60.51ms
step:1726/1775 train_time:104461ms step_avg:60.52ms
step:1727/1775 train_time:104546ms step_avg:60.54ms
step:1728/1775 train_time:104635ms step_avg:60.55ms
step:1729/1775 train_time:104719ms step_avg:60.57ms
step:1730/1775 train_time:104809ms step_avg:60.58ms
step:1731/1775 train_time:104896ms step_avg:60.60ms
step:1732/1775 train_time:104985ms step_avg:60.61ms
step:1733/1775 train_time:105072ms step_avg:60.63ms
step:1734/1775 train_time:105160ms step_avg:60.65ms
step:1735/1775 train_time:105246ms step_avg:60.66ms
step:1736/1775 train_time:105341ms step_avg:60.68ms
step:1737/1775 train_time:105427ms step_avg:60.69ms
step:1738/1775 train_time:105516ms step_avg:60.71ms
step:1739/1775 train_time:105602ms step_avg:60.73ms
step:1740/1775 train_time:105691ms step_avg:60.74ms
step:1741/1775 train_time:105777ms step_avg:60.76ms
step:1742/1775 train_time:105867ms step_avg:60.77ms
step:1743/1775 train_time:105954ms step_avg:60.79ms
step:1744/1775 train_time:106042ms step_avg:60.80ms
step:1745/1775 train_time:106129ms step_avg:60.82ms
step:1746/1775 train_time:106218ms step_avg:60.83ms
step:1747/1775 train_time:106304ms step_avg:60.85ms
step:1748/1775 train_time:106393ms step_avg:60.87ms
step:1749/1775 train_time:106480ms step_avg:60.88ms
step:1750/1775 train_time:106569ms step_avg:60.90ms
step:1750/1775 val_loss:3.2846 train_time:106666ms step_avg:60.95ms
step:1751/1775 train_time:106687ms step_avg:60.93ms
step:1752/1775 train_time:106747ms step_avg:60.93ms
step:1753/1775 train_time:106836ms step_avg:60.94ms
step:1754/1775 train_time:106925ms step_avg:60.96ms
step:1755/1775 train_time:107011ms step_avg:60.98ms
step:1756/1775 train_time:107100ms step_avg:60.99ms
step:1757/1775 train_time:107185ms step_avg:61.00ms
step:1758/1775 train_time:107272ms step_avg:61.02ms
step:1759/1775 train_time:107358ms step_avg:61.03ms
step:1760/1775 train_time:107448ms step_avg:61.05ms
step:1761/1775 train_time:107534ms step_avg:61.06ms
step:1762/1775 train_time:107624ms step_avg:61.08ms
step:1763/1775 train_time:107713ms step_avg:61.10ms
step:1764/1775 train_time:107804ms step_avg:61.11ms
step:1765/1775 train_time:107892ms step_avg:61.13ms
step:1766/1775 train_time:107982ms step_avg:61.15ms
step:1767/1775 train_time:108067ms step_avg:61.16ms
step:1768/1775 train_time:108158ms step_avg:61.18ms
step:1769/1775 train_time:108243ms step_avg:61.19ms
step:1770/1775 train_time:108332ms step_avg:61.20ms
step:1771/1775 train_time:108418ms step_avg:61.22ms
step:1772/1775 train_time:108505ms step_avg:61.23ms
step:1773/1775 train_time:108592ms step_avg:61.25ms
step:1774/1775 train_time:108683ms step_avg:61.26ms
step:1775/1775 train_time:108771ms step_avg:61.28ms
step:1775/1775 val_loss:3.2784 train_time:108870ms step_avg:61.34ms
peak memory allocated: 29148 MiB reserved: 44598 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Jan  7 08:57:37 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   44C    P0            129W /  700W |    1520MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   41C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     72204      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     72205      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     72206      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     72207      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     72208      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     72209      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     72210      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     72211      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8300 train_time:0ms step_avg:0.03ms
step:1/1775 train_time:77ms step_avg:77.45ms
step:2/1775 train_time:100ms step_avg:49.97ms
step:3/1775 train_time:119ms step_avg:39.69ms
step:4/1775 train_time:148ms step_avg:37.01ms
step:5/1775 train_time:180ms step_avg:35.97ms
step:6/1775 train_time:281ms step_avg:46.80ms
step:7/1775 train_time:304ms step_avg:43.42ms
step:8/1775 train_time:327ms step_avg:40.85ms
step:9/1775 train_time:359ms step_avg:39.84ms
step:10/1775 train_time:392ms step_avg:39.25ms
step:11/1775 train_time:425ms step_avg:38.61ms
step:12/1775 train_time:459ms step_avg:38.23ms
step:13/1775 train_time:491ms step_avg:37.75ms
step:14/1775 train_time:525ms step_avg:37.48ms
step:15/1775 train_time:557ms step_avg:37.12ms
step:16/1775 train_time:591ms step_avg:36.93ms
step:17/1775 train_time:623ms step_avg:36.64ms
step:18/1775 train_time:657ms step_avg:36.50ms
step:19/1775 train_time:689ms step_avg:36.27ms
step:20/1775 train_time:723ms step_avg:36.16ms
step:21/1775 train_time:756ms step_avg:35.98ms
step:22/1775 train_time:790ms step_avg:35.89ms
step:23/1775 train_time:822ms step_avg:35.73ms
step:24/1775 train_time:856ms step_avg:35.66ms
step:25/1775 train_time:888ms step_avg:35.51ms
step:26/1775 train_time:922ms step_avg:35.45ms
step:27/1775 train_time:954ms step_avg:35.33ms
step:28/1775 train_time:988ms step_avg:35.29ms
step:29/1775 train_time:1020ms step_avg:35.17ms
step:30/1775 train_time:1054ms step_avg:35.13ms
step:31/1775 train_time:1086ms step_avg:35.03ms
step:32/1775 train_time:1120ms step_avg:35.01ms
step:33/1775 train_time:1152ms step_avg:34.92ms
step:34/1775 train_time:1187ms step_avg:34.91ms
step:35/1775 train_time:1220ms step_avg:34.86ms
step:36/1775 train_time:1255ms step_avg:34.87ms
step:37/1775 train_time:1288ms step_avg:34.82ms
step:38/1775 train_time:1323ms step_avg:34.81ms
step:39/1775 train_time:1356ms step_avg:34.76ms
step:40/1775 train_time:1390ms step_avg:34.75ms
step:41/1775 train_time:1422ms step_avg:34.69ms
step:42/1775 train_time:1457ms step_avg:34.68ms
step:43/1775 train_time:1489ms step_avg:34.63ms
step:44/1775 train_time:1523ms step_avg:34.62ms
step:45/1775 train_time:1556ms step_avg:34.57ms
step:46/1775 train_time:1590ms step_avg:34.58ms
step:47/1775 train_time:1623ms step_avg:34.53ms
step:48/1775 train_time:1657ms step_avg:34.52ms
step:49/1775 train_time:1689ms step_avg:34.47ms
step:50/1775 train_time:1723ms step_avg:34.47ms
step:51/1775 train_time:1756ms step_avg:34.42ms
step:52/1775 train_time:1790ms step_avg:34.42ms
step:53/1775 train_time:1822ms step_avg:34.38ms
step:54/1775 train_time:1856ms step_avg:34.37ms
step:55/1775 train_time:1888ms step_avg:34.33ms
step:56/1775 train_time:1923ms step_avg:34.33ms
step:57/1775 train_time:1955ms step_avg:34.30ms
step:58/1775 train_time:1989ms step_avg:34.30ms
step:59/1775 train_time:2021ms step_avg:34.26ms
step:60/1775 train_time:2055ms step_avg:34.25ms
step:61/1775 train_time:2087ms step_avg:34.22ms
step:62/1775 train_time:2121ms step_avg:34.21ms
step:63/1775 train_time:2154ms step_avg:34.19ms
step:64/1775 train_time:2189ms step_avg:34.20ms
step:65/1775 train_time:2221ms step_avg:34.17ms
step:66/1775 train_time:2255ms step_avg:34.17ms
step:67/1775 train_time:2288ms step_avg:34.14ms
step:68/1775 train_time:2323ms step_avg:34.16ms
step:69/1775 train_time:2355ms step_avg:34.13ms
step:70/1775 train_time:2390ms step_avg:34.14ms
step:71/1775 train_time:2422ms step_avg:34.11ms
step:72/1775 train_time:2456ms step_avg:34.11ms
step:73/1775 train_time:2488ms step_avg:34.09ms
step:74/1775 train_time:2523ms step_avg:34.09ms
step:75/1775 train_time:2555ms step_avg:34.07ms
step:76/1775 train_time:2590ms step_avg:34.08ms
step:77/1775 train_time:2622ms step_avg:34.05ms
step:78/1775 train_time:2656ms step_avg:34.05ms
step:79/1775 train_time:2689ms step_avg:34.03ms
step:80/1775 train_time:2723ms step_avg:34.03ms
step:81/1775 train_time:2755ms step_avg:34.01ms
step:82/1775 train_time:2790ms step_avg:34.02ms
step:83/1775 train_time:2821ms step_avg:33.99ms
step:84/1775 train_time:2856ms step_avg:33.99ms
step:85/1775 train_time:2888ms step_avg:33.97ms
step:86/1775 train_time:2922ms step_avg:33.98ms
step:87/1775 train_time:2954ms step_avg:33.95ms
step:88/1775 train_time:2988ms step_avg:33.96ms
step:89/1775 train_time:3021ms step_avg:33.94ms
step:90/1775 train_time:3055ms step_avg:33.94ms
step:91/1775 train_time:3087ms step_avg:33.92ms
step:92/1775 train_time:3121ms step_avg:33.93ms
step:93/1775 train_time:3153ms step_avg:33.91ms
step:94/1775 train_time:3188ms step_avg:33.91ms
step:95/1775 train_time:3220ms step_avg:33.89ms
step:96/1775 train_time:3254ms step_avg:33.90ms
step:97/1775 train_time:3286ms step_avg:33.88ms
step:98/1775 train_time:3321ms step_avg:33.88ms
step:99/1775 train_time:3352ms step_avg:33.86ms
step:100/1775 train_time:3388ms step_avg:33.88ms
step:101/1775 train_time:3419ms step_avg:33.86ms
step:102/1775 train_time:3454ms step_avg:33.86ms
step:103/1775 train_time:3486ms step_avg:33.84ms
step:104/1775 train_time:3520ms step_avg:33.84ms
step:105/1775 train_time:3552ms step_avg:33.83ms
step:106/1775 train_time:3587ms step_avg:33.84ms
step:107/1775 train_time:3619ms step_avg:33.82ms
step:108/1775 train_time:3653ms step_avg:33.83ms
step:109/1775 train_time:3685ms step_avg:33.81ms
step:110/1775 train_time:3719ms step_avg:33.81ms
step:111/1775 train_time:3751ms step_avg:33.80ms
step:112/1775 train_time:3786ms step_avg:33.80ms
step:113/1775 train_time:3818ms step_avg:33.79ms
step:114/1775 train_time:3852ms step_avg:33.79ms
step:115/1775 train_time:3884ms step_avg:33.77ms
step:116/1775 train_time:3918ms step_avg:33.78ms
step:117/1775 train_time:3950ms step_avg:33.76ms
step:118/1775 train_time:3985ms step_avg:33.77ms
step:119/1775 train_time:4017ms step_avg:33.76ms
step:120/1775 train_time:4051ms step_avg:33.76ms
step:121/1775 train_time:4083ms step_avg:33.74ms
step:122/1775 train_time:4117ms step_avg:33.75ms
step:123/1775 train_time:4149ms step_avg:33.74ms
step:124/1775 train_time:4184ms step_avg:33.74ms
step:125/1775 train_time:4216ms step_avg:33.73ms
step:126/1775 train_time:4250ms step_avg:33.73ms
step:127/1775 train_time:4282ms step_avg:33.72ms
step:128/1775 train_time:4316ms step_avg:33.72ms
step:129/1775 train_time:4349ms step_avg:33.71ms
step:130/1775 train_time:4383ms step_avg:33.72ms
step:131/1775 train_time:4416ms step_avg:33.71ms
step:132/1775 train_time:4450ms step_avg:33.71ms
step:133/1775 train_time:4482ms step_avg:33.70ms
step:134/1775 train_time:4516ms step_avg:33.70ms
step:135/1775 train_time:4549ms step_avg:33.70ms
step:136/1775 train_time:4583ms step_avg:33.70ms
step:137/1775 train_time:4615ms step_avg:33.69ms
step:138/1775 train_time:4650ms step_avg:33.69ms
step:139/1775 train_time:4682ms step_avg:33.68ms
step:140/1775 train_time:4716ms step_avg:33.68ms
step:141/1775 train_time:4748ms step_avg:33.67ms
step:142/1775 train_time:4783ms step_avg:33.68ms
step:143/1775 train_time:4815ms step_avg:33.67ms
step:144/1775 train_time:4849ms step_avg:33.67ms
step:145/1775 train_time:4881ms step_avg:33.66ms
step:146/1775 train_time:4915ms step_avg:33.66ms
step:147/1775 train_time:4947ms step_avg:33.65ms
step:148/1775 train_time:4981ms step_avg:33.66ms
step:149/1775 train_time:5013ms step_avg:33.65ms
step:150/1775 train_time:5048ms step_avg:33.65ms
step:151/1775 train_time:5079ms step_avg:33.64ms
step:152/1775 train_time:5113ms step_avg:33.64ms
step:153/1775 train_time:5146ms step_avg:33.63ms
step:154/1775 train_time:5180ms step_avg:33.64ms
step:155/1775 train_time:5212ms step_avg:33.63ms
step:156/1775 train_time:5246ms step_avg:33.63ms
step:157/1775 train_time:5278ms step_avg:33.62ms
step:158/1775 train_time:5312ms step_avg:33.62ms
step:159/1775 train_time:5344ms step_avg:33.61ms
step:160/1775 train_time:5379ms step_avg:33.62ms
step:161/1775 train_time:5410ms step_avg:33.60ms
step:162/1775 train_time:5445ms step_avg:33.61ms
step:163/1775 train_time:5477ms step_avg:33.60ms
step:164/1775 train_time:5512ms step_avg:33.61ms
step:165/1775 train_time:5544ms step_avg:33.60ms
step:166/1775 train_time:5578ms step_avg:33.60ms
step:167/1775 train_time:5610ms step_avg:33.60ms
step:168/1775 train_time:5645ms step_avg:33.60ms
step:169/1775 train_time:5676ms step_avg:33.59ms
step:170/1775 train_time:5711ms step_avg:33.59ms
step:171/1775 train_time:5743ms step_avg:33.58ms
step:172/1775 train_time:5777ms step_avg:33.59ms
step:173/1775 train_time:5809ms step_avg:33.58ms
step:174/1775 train_time:5844ms step_avg:33.59ms
step:175/1775 train_time:5876ms step_avg:33.58ms
step:176/1775 train_time:5911ms step_avg:33.58ms
step:177/1775 train_time:5943ms step_avg:33.58ms
step:178/1775 train_time:5977ms step_avg:33.58ms
step:179/1775 train_time:6009ms step_avg:33.57ms
step:180/1775 train_time:6043ms step_avg:33.57ms
step:181/1775 train_time:6075ms step_avg:33.57ms
step:182/1775 train_time:6110ms step_avg:33.57ms
step:183/1775 train_time:6142ms step_avg:33.56ms
step:184/1775 train_time:6176ms step_avg:33.57ms
step:185/1775 train_time:6208ms step_avg:33.56ms
step:186/1775 train_time:6243ms step_avg:33.56ms
step:187/1775 train_time:6275ms step_avg:33.55ms
step:188/1775 train_time:6309ms step_avg:33.56ms
step:189/1775 train_time:6341ms step_avg:33.55ms
step:190/1775 train_time:6375ms step_avg:33.55ms
step:191/1775 train_time:6407ms step_avg:33.54ms
step:192/1775 train_time:6441ms step_avg:33.55ms
step:193/1775 train_time:6473ms step_avg:33.54ms
step:194/1775 train_time:6508ms step_avg:33.54ms
step:195/1775 train_time:6540ms step_avg:33.54ms
step:196/1775 train_time:6574ms step_avg:33.54ms
step:197/1775 train_time:6605ms step_avg:33.53ms
step:198/1775 train_time:6640ms step_avg:33.53ms
step:199/1775 train_time:6672ms step_avg:33.53ms
step:200/1775 train_time:6706ms step_avg:33.53ms
step:201/1775 train_time:6738ms step_avg:33.52ms
step:202/1775 train_time:6772ms step_avg:33.53ms
step:203/1775 train_time:6805ms step_avg:33.52ms
step:204/1775 train_time:6839ms step_avg:33.52ms
step:205/1775 train_time:6871ms step_avg:33.52ms
step:206/1775 train_time:6905ms step_avg:33.52ms
step:207/1775 train_time:6938ms step_avg:33.51ms
step:208/1775 train_time:6972ms step_avg:33.52ms
step:209/1775 train_time:7004ms step_avg:33.51ms
step:210/1775 train_time:7038ms step_avg:33.51ms
step:211/1775 train_time:7070ms step_avg:33.51ms
step:212/1775 train_time:7104ms step_avg:33.51ms
step:213/1775 train_time:7137ms step_avg:33.51ms
step:214/1775 train_time:7171ms step_avg:33.51ms
step:215/1775 train_time:7203ms step_avg:33.50ms
step:216/1775 train_time:7238ms step_avg:33.51ms
step:217/1775 train_time:7270ms step_avg:33.50ms
step:218/1775 train_time:7304ms step_avg:33.51ms
step:219/1775 train_time:7337ms step_avg:33.50ms
step:220/1775 train_time:7371ms step_avg:33.51ms
step:221/1775 train_time:7403ms step_avg:33.50ms
step:222/1775 train_time:7438ms step_avg:33.50ms
step:223/1775 train_time:7470ms step_avg:33.50ms
step:224/1775 train_time:7504ms step_avg:33.50ms
step:225/1775 train_time:7536ms step_avg:33.49ms
step:226/1775 train_time:7570ms step_avg:33.50ms
step:227/1775 train_time:7602ms step_avg:33.49ms
step:228/1775 train_time:7637ms step_avg:33.49ms
step:229/1775 train_time:7669ms step_avg:33.49ms
step:230/1775 train_time:7703ms step_avg:33.49ms
step:231/1775 train_time:7735ms step_avg:33.49ms
step:232/1775 train_time:7770ms step_avg:33.49ms
step:233/1775 train_time:7802ms step_avg:33.48ms
step:234/1775 train_time:7836ms step_avg:33.49ms
step:235/1775 train_time:7868ms step_avg:33.48ms
step:236/1775 train_time:7902ms step_avg:33.48ms
step:237/1775 train_time:7934ms step_avg:33.48ms
step:238/1775 train_time:7969ms step_avg:33.48ms
step:239/1775 train_time:8001ms step_avg:33.48ms
step:240/1775 train_time:8034ms step_avg:33.48ms
step:241/1775 train_time:8066ms step_avg:33.47ms
step:242/1775 train_time:8101ms step_avg:33.47ms
step:243/1775 train_time:8133ms step_avg:33.47ms
step:244/1775 train_time:8167ms step_avg:33.47ms
step:245/1775 train_time:8199ms step_avg:33.46ms
step:246/1775 train_time:8233ms step_avg:33.47ms
step:247/1775 train_time:8265ms step_avg:33.46ms
step:248/1775 train_time:8299ms step_avg:33.46ms
step:249/1775 train_time:8331ms step_avg:33.46ms
step:250/1775 train_time:8365ms step_avg:33.46ms
step:250/1775 val_loss:4.6225 train_time:8406ms step_avg:33.63ms
step:251/1775 train_time:8425ms step_avg:33.57ms
step:252/1775 train_time:8444ms step_avg:33.51ms
step:253/1775 train_time:8465ms step_avg:33.46ms
step:254/1775 train_time:8500ms step_avg:33.47ms
step:255/1775 train_time:8534ms step_avg:33.47ms
step:256/1775 train_time:8569ms step_avg:33.47ms
step:257/1775 train_time:8602ms step_avg:33.47ms
step:258/1775 train_time:8636ms step_avg:33.47ms
step:259/1775 train_time:8668ms step_avg:33.47ms
step:260/1775 train_time:8702ms step_avg:33.47ms
step:261/1775 train_time:8734ms step_avg:33.47ms
step:262/1775 train_time:8769ms step_avg:33.47ms
step:263/1775 train_time:8800ms step_avg:33.46ms
step:264/1775 train_time:8834ms step_avg:33.46ms
step:265/1775 train_time:8866ms step_avg:33.46ms
step:266/1775 train_time:8900ms step_avg:33.46ms
step:267/1775 train_time:8932ms step_avg:33.45ms
step:268/1775 train_time:8966ms step_avg:33.46ms
step:269/1775 train_time:8998ms step_avg:33.45ms
step:270/1775 train_time:9032ms step_avg:33.45ms
step:271/1775 train_time:9064ms step_avg:33.45ms
step:272/1775 train_time:9098ms step_avg:33.45ms
step:273/1775 train_time:9129ms step_avg:33.44ms
step:274/1775 train_time:9163ms step_avg:33.44ms
step:275/1775 train_time:9195ms step_avg:33.44ms
step:276/1775 train_time:9229ms step_avg:33.44ms
step:277/1775 train_time:9261ms step_avg:33.43ms
step:278/1775 train_time:9295ms step_avg:33.44ms
step:279/1775 train_time:9327ms step_avg:33.43ms
step:280/1775 train_time:9361ms step_avg:33.43ms
step:281/1775 train_time:9394ms step_avg:33.43ms
step:282/1775 train_time:9428ms step_avg:33.43ms
step:283/1775 train_time:9461ms step_avg:33.43ms
step:284/1775 train_time:9496ms step_avg:33.44ms
step:285/1775 train_time:9528ms step_avg:33.43ms
step:286/1775 train_time:9562ms step_avg:33.43ms
step:287/1775 train_time:9595ms step_avg:33.43ms
step:288/1775 train_time:9629ms step_avg:33.43ms
step:289/1775 train_time:9661ms step_avg:33.43ms
step:290/1775 train_time:9695ms step_avg:33.43ms
step:291/1775 train_time:9728ms step_avg:33.43ms
step:292/1775 train_time:9762ms step_avg:33.43ms
step:293/1775 train_time:9794ms step_avg:33.43ms
step:294/1775 train_time:9828ms step_avg:33.43ms
step:295/1775 train_time:9860ms step_avg:33.42ms
step:296/1775 train_time:9894ms step_avg:33.43ms
step:297/1775 train_time:9926ms step_avg:33.42ms
step:298/1775 train_time:9960ms step_avg:33.42ms
step:299/1775 train_time:9992ms step_avg:33.42ms
step:300/1775 train_time:10026ms step_avg:33.42ms
step:301/1775 train_time:10058ms step_avg:33.42ms
step:302/1775 train_time:10092ms step_avg:33.42ms
step:303/1775 train_time:10124ms step_avg:33.41ms
step:304/1775 train_time:10158ms step_avg:33.42ms
step:305/1775 train_time:10190ms step_avg:33.41ms
step:306/1775 train_time:10224ms step_avg:33.41ms
step:307/1775 train_time:10256ms step_avg:33.41ms
step:308/1775 train_time:10290ms step_avg:33.41ms
step:309/1775 train_time:10323ms step_avg:33.41ms
step:310/1775 train_time:10357ms step_avg:33.41ms
step:311/1775 train_time:10389ms step_avg:33.40ms
step:312/1775 train_time:10423ms step_avg:33.41ms
step:313/1775 train_time:10455ms step_avg:33.40ms
step:314/1775 train_time:10490ms step_avg:33.41ms
step:315/1775 train_time:10522ms step_avg:33.40ms
step:316/1775 train_time:10556ms step_avg:33.41ms
step:317/1775 train_time:10588ms step_avg:33.40ms
step:318/1775 train_time:10623ms step_avg:33.40ms
step:319/1775 train_time:10655ms step_avg:33.40ms
step:320/1775 train_time:10689ms step_avg:33.40ms
step:321/1775 train_time:10721ms step_avg:33.40ms
step:322/1775 train_time:10755ms step_avg:33.40ms
step:323/1775 train_time:10787ms step_avg:33.40ms
step:324/1775 train_time:10821ms step_avg:33.40ms
step:325/1775 train_time:10853ms step_avg:33.39ms
step:326/1775 train_time:10887ms step_avg:33.40ms
step:327/1775 train_time:10919ms step_avg:33.39ms
step:328/1775 train_time:10953ms step_avg:33.39ms
step:329/1775 train_time:10985ms step_avg:33.39ms
step:330/1775 train_time:11019ms step_avg:33.39ms
step:331/1775 train_time:11051ms step_avg:33.39ms
step:332/1775 train_time:11085ms step_avg:33.39ms
step:333/1775 train_time:11117ms step_avg:33.38ms
step:334/1775 train_time:11151ms step_avg:33.39ms
step:335/1775 train_time:11183ms step_avg:33.38ms
step:336/1775 train_time:11217ms step_avg:33.38ms
step:337/1775 train_time:11249ms step_avg:33.38ms
step:338/1775 train_time:11283ms step_avg:33.38ms
step:339/1775 train_time:11315ms step_avg:33.38ms
step:340/1775 train_time:11349ms step_avg:33.38ms
step:341/1775 train_time:11381ms step_avg:33.37ms
step:342/1775 train_time:11415ms step_avg:33.38ms
step:343/1775 train_time:11447ms step_avg:33.37ms
step:344/1775 train_time:11481ms step_avg:33.38ms
step:345/1775 train_time:11514ms step_avg:33.37ms
step:346/1775 train_time:11548ms step_avg:33.37ms
step:347/1775 train_time:11580ms step_avg:33.37ms
step:348/1775 train_time:11614ms step_avg:33.37ms
step:349/1775 train_time:11646ms step_avg:33.37ms
step:350/1775 train_time:11680ms step_avg:33.37ms
step:351/1775 train_time:11712ms step_avg:33.37ms
step:352/1775 train_time:11746ms step_avg:33.37ms
step:353/1775 train_time:11779ms step_avg:33.37ms
step:354/1775 train_time:11813ms step_avg:33.37ms
step:355/1775 train_time:11845ms step_avg:33.37ms
step:356/1775 train_time:11879ms step_avg:33.37ms
step:357/1775 train_time:11911ms step_avg:33.36ms
step:358/1775 train_time:11945ms step_avg:33.37ms
step:359/1775 train_time:11977ms step_avg:33.36ms
step:360/1775 train_time:12011ms step_avg:33.37ms
step:361/1775 train_time:12043ms step_avg:33.36ms
step:362/1775 train_time:12077ms step_avg:33.36ms
step:363/1775 train_time:12109ms step_avg:33.36ms
step:364/1775 train_time:12144ms step_avg:33.36ms
step:365/1775 train_time:12175ms step_avg:33.36ms
step:366/1775 train_time:12210ms step_avg:33.36ms
step:367/1775 train_time:12242ms step_avg:33.36ms
step:368/1775 train_time:12276ms step_avg:33.36ms
step:369/1775 train_time:12308ms step_avg:33.36ms
step:370/1775 train_time:12342ms step_avg:33.36ms
step:371/1775 train_time:12374ms step_avg:33.35ms
step:372/1775 train_time:12408ms step_avg:33.36ms
step:373/1775 train_time:12440ms step_avg:33.35ms
step:374/1775 train_time:12475ms step_avg:33.36ms
step:375/1775 train_time:12507ms step_avg:33.35ms
step:376/1775 train_time:12541ms step_avg:33.35ms
step:377/1775 train_time:12573ms step_avg:33.35ms
step:378/1775 train_time:12607ms step_avg:33.35ms
step:379/1775 train_time:12639ms step_avg:33.35ms
step:380/1775 train_time:12674ms step_avg:33.35ms
step:381/1775 train_time:12706ms step_avg:33.35ms
step:382/1775 train_time:12740ms step_avg:33.35ms
step:383/1775 train_time:12772ms step_avg:33.35ms
step:384/1775 train_time:12806ms step_avg:33.35ms
step:385/1775 train_time:12838ms step_avg:33.35ms
step:386/1775 train_time:12873ms step_avg:33.35ms
step:387/1775 train_time:12904ms step_avg:33.34ms
step:388/1775 train_time:12938ms step_avg:33.35ms
step:389/1775 train_time:12970ms step_avg:33.34ms
step:390/1775 train_time:13004ms step_avg:33.34ms
step:391/1775 train_time:13036ms step_avg:33.34ms
step:392/1775 train_time:13070ms step_avg:33.34ms
step:393/1775 train_time:13102ms step_avg:33.34ms
step:394/1775 train_time:13137ms step_avg:33.34ms
step:395/1775 train_time:13169ms step_avg:33.34ms
step:396/1775 train_time:13203ms step_avg:33.34ms
step:397/1775 train_time:13235ms step_avg:33.34ms
step:398/1775 train_time:13270ms step_avg:33.34ms
step:399/1775 train_time:13302ms step_avg:33.34ms
step:400/1775 train_time:13335ms step_avg:33.34ms
step:401/1775 train_time:13367ms step_avg:33.33ms
step:402/1775 train_time:13401ms step_avg:33.34ms
step:403/1775 train_time:13433ms step_avg:33.33ms
step:404/1775 train_time:13468ms step_avg:33.34ms
step:405/1775 train_time:13500ms step_avg:33.33ms
step:406/1775 train_time:13534ms step_avg:33.34ms
step:407/1775 train_time:13566ms step_avg:33.33ms
step:408/1775 train_time:13601ms step_avg:33.34ms
step:409/1775 train_time:13633ms step_avg:33.33ms
step:410/1775 train_time:13667ms step_avg:33.33ms
step:411/1775 train_time:13699ms step_avg:33.33ms
step:412/1775 train_time:13733ms step_avg:33.33ms
step:413/1775 train_time:13765ms step_avg:33.33ms
step:414/1775 train_time:13799ms step_avg:33.33ms
step:415/1775 train_time:13831ms step_avg:33.33ms
step:416/1775 train_time:13865ms step_avg:33.33ms
step:417/1775 train_time:13897ms step_avg:33.33ms
step:418/1775 train_time:13931ms step_avg:33.33ms
step:419/1775 train_time:13963ms step_avg:33.33ms
step:420/1775 train_time:13998ms step_avg:33.33ms
step:421/1775 train_time:14030ms step_avg:33.32ms
step:422/1775 train_time:14064ms step_avg:33.33ms
step:423/1775 train_time:14096ms step_avg:33.32ms
step:424/1775 train_time:14130ms step_avg:33.32ms
step:425/1775 train_time:14162ms step_avg:33.32ms
step:426/1775 train_time:14196ms step_avg:33.32ms
step:427/1775 train_time:14228ms step_avg:33.32ms
step:428/1775 train_time:14262ms step_avg:33.32ms
step:429/1775 train_time:14294ms step_avg:33.32ms
step:430/1775 train_time:14329ms step_avg:33.32ms
step:431/1775 train_time:14361ms step_avg:33.32ms
step:432/1775 train_time:14395ms step_avg:33.32ms
step:433/1775 train_time:14427ms step_avg:33.32ms
step:434/1775 train_time:14461ms step_avg:33.32ms
step:435/1775 train_time:14493ms step_avg:33.32ms
step:436/1775 train_time:14528ms step_avg:33.32ms
step:437/1775 train_time:14559ms step_avg:33.32ms
step:438/1775 train_time:14594ms step_avg:33.32ms
step:439/1775 train_time:14626ms step_avg:33.32ms
step:440/1775 train_time:14661ms step_avg:33.32ms
step:441/1775 train_time:14693ms step_avg:33.32ms
step:442/1775 train_time:14727ms step_avg:33.32ms
step:443/1775 train_time:14759ms step_avg:33.32ms
step:444/1775 train_time:14793ms step_avg:33.32ms
step:445/1775 train_time:14825ms step_avg:33.31ms
step:446/1775 train_time:14859ms step_avg:33.32ms
step:447/1775 train_time:14891ms step_avg:33.31ms
step:448/1775 train_time:14925ms step_avg:33.32ms
step:449/1775 train_time:14957ms step_avg:33.31ms
step:450/1775 train_time:14991ms step_avg:33.31ms
step:451/1775 train_time:15024ms step_avg:33.31ms
step:452/1775 train_time:15058ms step_avg:33.31ms
step:453/1775 train_time:15089ms step_avg:33.31ms
step:454/1775 train_time:15123ms step_avg:33.31ms
step:455/1775 train_time:15156ms step_avg:33.31ms
step:456/1775 train_time:15190ms step_avg:33.31ms
step:457/1775 train_time:15222ms step_avg:33.31ms
step:458/1775 train_time:15256ms step_avg:33.31ms
step:459/1775 train_time:15288ms step_avg:33.31ms
step:460/1775 train_time:15322ms step_avg:33.31ms
step:461/1775 train_time:15354ms step_avg:33.31ms
step:462/1775 train_time:15389ms step_avg:33.31ms
step:463/1775 train_time:15421ms step_avg:33.31ms
step:464/1775 train_time:15455ms step_avg:33.31ms
step:465/1775 train_time:15487ms step_avg:33.31ms
step:466/1775 train_time:15522ms step_avg:33.31ms
step:467/1775 train_time:15554ms step_avg:33.31ms
step:468/1775 train_time:15588ms step_avg:33.31ms
step:469/1775 train_time:15620ms step_avg:33.30ms
step:470/1775 train_time:15654ms step_avg:33.31ms
step:471/1775 train_time:15687ms step_avg:33.30ms
step:472/1775 train_time:15720ms step_avg:33.31ms
step:473/1775 train_time:15753ms step_avg:33.30ms
step:474/1775 train_time:15787ms step_avg:33.31ms
step:475/1775 train_time:15819ms step_avg:33.30ms
step:476/1775 train_time:15854ms step_avg:33.31ms
step:477/1775 train_time:15886ms step_avg:33.30ms
step:478/1775 train_time:15920ms step_avg:33.31ms
step:479/1775 train_time:15952ms step_avg:33.30ms
step:480/1775 train_time:15986ms step_avg:33.30ms
step:481/1775 train_time:16018ms step_avg:33.30ms
step:482/1775 train_time:16052ms step_avg:33.30ms
step:483/1775 train_time:16084ms step_avg:33.30ms
step:484/1775 train_time:16118ms step_avg:33.30ms
step:485/1775 train_time:16150ms step_avg:33.30ms
step:486/1775 train_time:16184ms step_avg:33.30ms
step:487/1775 train_time:16216ms step_avg:33.30ms
step:488/1775 train_time:16250ms step_avg:33.30ms
step:489/1775 train_time:16282ms step_avg:33.30ms
step:490/1775 train_time:16316ms step_avg:33.30ms
step:491/1775 train_time:16348ms step_avg:33.30ms
step:492/1775 train_time:16382ms step_avg:33.30ms
step:493/1775 train_time:16414ms step_avg:33.29ms
step:494/1775 train_time:16449ms step_avg:33.30ms
step:495/1775 train_time:16480ms step_avg:33.29ms
step:496/1775 train_time:16515ms step_avg:33.30ms
step:497/1775 train_time:16547ms step_avg:33.29ms
step:498/1775 train_time:16581ms step_avg:33.30ms
step:499/1775 train_time:16613ms step_avg:33.29ms
step:500/1775 train_time:16647ms step_avg:33.29ms
step:500/1775 val_loss:4.2857 train_time:16688ms step_avg:33.38ms
step:501/1775 train_time:16713ms step_avg:33.36ms
step:502/1775 train_time:16734ms step_avg:33.33ms
step:503/1775 train_time:16751ms step_avg:33.30ms
step:504/1775 train_time:16784ms step_avg:33.30ms
step:505/1775 train_time:16816ms step_avg:33.30ms
step:506/1775 train_time:16851ms step_avg:33.30ms
step:507/1775 train_time:16883ms step_avg:33.30ms
step:508/1775 train_time:16917ms step_avg:33.30ms
step:509/1775 train_time:16949ms step_avg:33.30ms
step:510/1775 train_time:16984ms step_avg:33.30ms
step:511/1775 train_time:17016ms step_avg:33.30ms
step:512/1775 train_time:17050ms step_avg:33.30ms
step:513/1775 train_time:17082ms step_avg:33.30ms
step:514/1775 train_time:17116ms step_avg:33.30ms
step:515/1775 train_time:17148ms step_avg:33.30ms
step:516/1775 train_time:17182ms step_avg:33.30ms
step:517/1775 train_time:17214ms step_avg:33.30ms
step:518/1775 train_time:17248ms step_avg:33.30ms
step:519/1775 train_time:17280ms step_avg:33.29ms
step:520/1775 train_time:17314ms step_avg:33.30ms
step:521/1775 train_time:17346ms step_avg:33.29ms
step:522/1775 train_time:17380ms step_avg:33.29ms
step:523/1775 train_time:17411ms step_avg:33.29ms
step:524/1775 train_time:17445ms step_avg:33.29ms
step:525/1775 train_time:17477ms step_avg:33.29ms
step:526/1775 train_time:17511ms step_avg:33.29ms
step:527/1775 train_time:17543ms step_avg:33.29ms
step:528/1775 train_time:17577ms step_avg:33.29ms
step:529/1775 train_time:17609ms step_avg:33.29ms
step:530/1775 train_time:17643ms step_avg:33.29ms
step:531/1775 train_time:17676ms step_avg:33.29ms
step:532/1775 train_time:17710ms step_avg:33.29ms
step:533/1775 train_time:17742ms step_avg:33.29ms
step:534/1775 train_time:17777ms step_avg:33.29ms
step:535/1775 train_time:17810ms step_avg:33.29ms
step:536/1775 train_time:17844ms step_avg:33.29ms
step:537/1775 train_time:17876ms step_avg:33.29ms
step:538/1775 train_time:17911ms step_avg:33.29ms
step:539/1775 train_time:17943ms step_avg:33.29ms
step:540/1775 train_time:17977ms step_avg:33.29ms
step:541/1775 train_time:18009ms step_avg:33.29ms
step:542/1775 train_time:18044ms step_avg:33.29ms
step:543/1775 train_time:18076ms step_avg:33.29ms
step:544/1775 train_time:18111ms step_avg:33.29ms
step:545/1775 train_time:18143ms step_avg:33.29ms
step:546/1775 train_time:18177ms step_avg:33.29ms
step:547/1775 train_time:18209ms step_avg:33.29ms
step:548/1775 train_time:18243ms step_avg:33.29ms
step:549/1775 train_time:18275ms step_avg:33.29ms
step:550/1775 train_time:18309ms step_avg:33.29ms
step:551/1775 train_time:18342ms step_avg:33.29ms
step:552/1775 train_time:18375ms step_avg:33.29ms
step:553/1775 train_time:18407ms step_avg:33.29ms
step:554/1775 train_time:18442ms step_avg:33.29ms
step:555/1775 train_time:18473ms step_avg:33.29ms
step:556/1775 train_time:18508ms step_avg:33.29ms
step:557/1775 train_time:18540ms step_avg:33.28ms
step:558/1775 train_time:18574ms step_avg:33.29ms
step:559/1775 train_time:18606ms step_avg:33.28ms
step:560/1775 train_time:18640ms step_avg:33.29ms
step:561/1775 train_time:18672ms step_avg:33.28ms
step:562/1775 train_time:18706ms step_avg:33.29ms
step:563/1775 train_time:18739ms step_avg:33.28ms
step:564/1775 train_time:18773ms step_avg:33.29ms
step:565/1775 train_time:18805ms step_avg:33.28ms
step:566/1775 train_time:18840ms step_avg:33.29ms
step:567/1775 train_time:18871ms step_avg:33.28ms
step:568/1775 train_time:18906ms step_avg:33.28ms
step:569/1775 train_time:18938ms step_avg:33.28ms
step:570/1775 train_time:18972ms step_avg:33.28ms
step:571/1775 train_time:19004ms step_avg:33.28ms
step:572/1775 train_time:19039ms step_avg:33.28ms
step:573/1775 train_time:19071ms step_avg:33.28ms
step:574/1775 train_time:19105ms step_avg:33.28ms
step:575/1775 train_time:19137ms step_avg:33.28ms
step:576/1775 train_time:19171ms step_avg:33.28ms
step:577/1775 train_time:19204ms step_avg:33.28ms
step:578/1775 train_time:19238ms step_avg:33.28ms
step:579/1775 train_time:19270ms step_avg:33.28ms
step:580/1775 train_time:19307ms step_avg:33.29ms
step:581/1775 train_time:19367ms step_avg:33.33ms
step:582/1775 train_time:19429ms step_avg:33.38ms
step:583/1775 train_time:19489ms step_avg:33.43ms
step:584/1775 train_time:19551ms step_avg:33.48ms
step:585/1775 train_time:19610ms step_avg:33.52ms
step:586/1775 train_time:19672ms step_avg:33.57ms
step:587/1775 train_time:19732ms step_avg:33.62ms
step:588/1775 train_time:19793ms step_avg:33.66ms
step:589/1775 train_time:19852ms step_avg:33.71ms
step:590/1775 train_time:19915ms step_avg:33.75ms
step:591/1775 train_time:19975ms step_avg:33.80ms
step:592/1775 train_time:20037ms step_avg:33.85ms
step:593/1775 train_time:20096ms step_avg:33.89ms
step:594/1775 train_time:20159ms step_avg:33.94ms
step:595/1775 train_time:20219ms step_avg:33.98ms
step:596/1775 train_time:20281ms step_avg:34.03ms
step:597/1775 train_time:20341ms step_avg:34.07ms
step:598/1775 train_time:20403ms step_avg:34.12ms
step:599/1775 train_time:20464ms step_avg:34.16ms
step:600/1775 train_time:20527ms step_avg:34.21ms
step:601/1775 train_time:20586ms step_avg:34.25ms
step:602/1775 train_time:20649ms step_avg:34.30ms
step:603/1775 train_time:20709ms step_avg:34.34ms
step:604/1775 train_time:20771ms step_avg:34.39ms
step:605/1775 train_time:20831ms step_avg:34.43ms
step:606/1775 train_time:20894ms step_avg:34.48ms
step:607/1775 train_time:20954ms step_avg:34.52ms
step:608/1775 train_time:21017ms step_avg:34.57ms
step:609/1775 train_time:21076ms step_avg:34.61ms
step:610/1775 train_time:21138ms step_avg:34.65ms
step:611/1775 train_time:21197ms step_avg:34.69ms
step:612/1775 train_time:21259ms step_avg:34.74ms
step:613/1775 train_time:21318ms step_avg:34.78ms
step:614/1775 train_time:21381ms step_avg:34.82ms
step:615/1775 train_time:21440ms step_avg:34.86ms
step:616/1775 train_time:21503ms step_avg:34.91ms
step:617/1775 train_time:21564ms step_avg:34.95ms
step:618/1775 train_time:21626ms step_avg:34.99ms
step:619/1775 train_time:21686ms step_avg:35.03ms
step:620/1775 train_time:21749ms step_avg:35.08ms
step:621/1775 train_time:21810ms step_avg:35.12ms
step:622/1775 train_time:21873ms step_avg:35.16ms
step:623/1775 train_time:21932ms step_avg:35.20ms
step:624/1775 train_time:21994ms step_avg:35.25ms
step:625/1775 train_time:22055ms step_avg:35.29ms
step:626/1775 train_time:22116ms step_avg:35.33ms
step:627/1775 train_time:22175ms step_avg:35.37ms
step:628/1775 train_time:22238ms step_avg:35.41ms
step:629/1775 train_time:22297ms step_avg:35.45ms
step:630/1775 train_time:22359ms step_avg:35.49ms
step:631/1775 train_time:22419ms step_avg:35.53ms
step:632/1775 train_time:22481ms step_avg:35.57ms
step:633/1775 train_time:22541ms step_avg:35.61ms
step:634/1775 train_time:22603ms step_avg:35.65ms
step:635/1775 train_time:22663ms step_avg:35.69ms
step:636/1775 train_time:22725ms step_avg:35.73ms
step:637/1775 train_time:22786ms step_avg:35.77ms
step:638/1775 train_time:22849ms step_avg:35.81ms
step:639/1775 train_time:22910ms step_avg:35.85ms
step:640/1775 train_time:22973ms step_avg:35.89ms
step:641/1775 train_time:23032ms step_avg:35.93ms
step:642/1775 train_time:23095ms step_avg:35.97ms
step:643/1775 train_time:23155ms step_avg:36.01ms
step:644/1775 train_time:23216ms step_avg:36.05ms
step:645/1775 train_time:23276ms step_avg:36.09ms
step:646/1775 train_time:23337ms step_avg:36.13ms
step:647/1775 train_time:23396ms step_avg:36.16ms
step:648/1775 train_time:23458ms step_avg:36.20ms
step:649/1775 train_time:23518ms step_avg:36.24ms
step:650/1775 train_time:23580ms step_avg:36.28ms
step:651/1775 train_time:23640ms step_avg:36.31ms
step:652/1775 train_time:23702ms step_avg:36.35ms
step:653/1775 train_time:23762ms step_avg:36.39ms
step:654/1775 train_time:23824ms step_avg:36.43ms
step:655/1775 train_time:23885ms step_avg:36.47ms
step:656/1775 train_time:23948ms step_avg:36.51ms
step:657/1775 train_time:24008ms step_avg:36.54ms
step:658/1775 train_time:24071ms step_avg:36.58ms
step:659/1775 train_time:24131ms step_avg:36.62ms
step:660/1775 train_time:24194ms step_avg:36.66ms
step:661/1775 train_time:24253ms step_avg:36.69ms
step:662/1775 train_time:24315ms step_avg:36.73ms
step:663/1775 train_time:24375ms step_avg:36.76ms
step:664/1775 train_time:24438ms step_avg:36.80ms
step:665/1775 train_time:24496ms step_avg:36.84ms
step:666/1775 train_time:24558ms step_avg:36.87ms
step:667/1775 train_time:24618ms step_avg:36.91ms
step:668/1775 train_time:24680ms step_avg:36.95ms
step:669/1775 train_time:24740ms step_avg:36.98ms
step:670/1775 train_time:24802ms step_avg:37.02ms
step:671/1775 train_time:24863ms step_avg:37.05ms
step:672/1775 train_time:24925ms step_avg:37.09ms
step:673/1775 train_time:24986ms step_avg:37.13ms
step:674/1775 train_time:25048ms step_avg:37.16ms
step:675/1775 train_time:25109ms step_avg:37.20ms
step:676/1775 train_time:25172ms step_avg:37.24ms
step:677/1775 train_time:25232ms step_avg:37.27ms
step:678/1775 train_time:25294ms step_avg:37.31ms
step:679/1775 train_time:25354ms step_avg:37.34ms
step:680/1775 train_time:25416ms step_avg:37.38ms
step:681/1775 train_time:25476ms step_avg:37.41ms
step:682/1775 train_time:25537ms step_avg:37.44ms
step:683/1775 train_time:25597ms step_avg:37.48ms
step:684/1775 train_time:25658ms step_avg:37.51ms
step:685/1775 train_time:25718ms step_avg:37.54ms
step:686/1775 train_time:25781ms step_avg:37.58ms
step:687/1775 train_time:25840ms step_avg:37.61ms
step:688/1775 train_time:25903ms step_avg:37.65ms
step:689/1775 train_time:25964ms step_avg:37.68ms
step:690/1775 train_time:26026ms step_avg:37.72ms
step:691/1775 train_time:26086ms step_avg:37.75ms
step:692/1775 train_time:26149ms step_avg:37.79ms
step:693/1775 train_time:26209ms step_avg:37.82ms
step:694/1775 train_time:26272ms step_avg:37.86ms
step:695/1775 train_time:26333ms step_avg:37.89ms
step:696/1775 train_time:26396ms step_avg:37.92ms
step:697/1775 train_time:26454ms step_avg:37.95ms
step:698/1775 train_time:26516ms step_avg:37.99ms
step:699/1775 train_time:26577ms step_avg:38.02ms
step:700/1775 train_time:26638ms step_avg:38.05ms
step:701/1775 train_time:26697ms step_avg:38.08ms
step:702/1775 train_time:26759ms step_avg:38.12ms
step:703/1775 train_time:26818ms step_avg:38.15ms
step:704/1775 train_time:26881ms step_avg:38.18ms
step:705/1775 train_time:26941ms step_avg:38.21ms
step:706/1775 train_time:27004ms step_avg:38.25ms
step:707/1775 train_time:27064ms step_avg:38.28ms
step:708/1775 train_time:27127ms step_avg:38.31ms
step:709/1775 train_time:27187ms step_avg:38.35ms
step:710/1775 train_time:27250ms step_avg:38.38ms
step:711/1775 train_time:27311ms step_avg:38.41ms
step:712/1775 train_time:27373ms step_avg:38.45ms
step:713/1775 train_time:27433ms step_avg:38.48ms
step:714/1775 train_time:27496ms step_avg:38.51ms
step:715/1775 train_time:27555ms step_avg:38.54ms
step:716/1775 train_time:27617ms step_avg:38.57ms
step:717/1775 train_time:27676ms step_avg:38.60ms
step:718/1775 train_time:27739ms step_avg:38.63ms
step:719/1775 train_time:27797ms step_avg:38.66ms
step:720/1775 train_time:27859ms step_avg:38.69ms
step:721/1775 train_time:27919ms step_avg:38.72ms
step:722/1775 train_time:27981ms step_avg:38.76ms
step:723/1775 train_time:28041ms step_avg:38.78ms
step:724/1775 train_time:28103ms step_avg:38.82ms
step:725/1775 train_time:28164ms step_avg:38.85ms
step:726/1775 train_time:28228ms step_avg:38.88ms
step:727/1775 train_time:28288ms step_avg:38.91ms
step:728/1775 train_time:28351ms step_avg:38.94ms
step:729/1775 train_time:28411ms step_avg:38.97ms
step:730/1775 train_time:28473ms step_avg:39.00ms
step:731/1775 train_time:28533ms step_avg:39.03ms
step:732/1775 train_time:28596ms step_avg:39.07ms
step:733/1775 train_time:28655ms step_avg:39.09ms
step:734/1775 train_time:28717ms step_avg:39.12ms
step:735/1775 train_time:28777ms step_avg:39.15ms
step:736/1775 train_time:28839ms step_avg:39.18ms
step:737/1775 train_time:28898ms step_avg:39.21ms
step:738/1775 train_time:28960ms step_avg:39.24ms
step:739/1775 train_time:29019ms step_avg:39.27ms
step:740/1775 train_time:29082ms step_avg:39.30ms
step:741/1775 train_time:29143ms step_avg:39.33ms
step:742/1775 train_time:29206ms step_avg:39.36ms
step:743/1775 train_time:29267ms step_avg:39.39ms
step:744/1775 train_time:29329ms step_avg:39.42ms
step:745/1775 train_time:29389ms step_avg:39.45ms
step:746/1775 train_time:29452ms step_avg:39.48ms
step:747/1775 train_time:29512ms step_avg:39.51ms
step:748/1775 train_time:29573ms step_avg:39.54ms
step:749/1775 train_time:29633ms step_avg:39.56ms
step:750/1775 train_time:29696ms step_avg:39.59ms
step:750/1775 val_loss:4.0067 train_time:29765ms step_avg:39.69ms
step:751/1775 train_time:29786ms step_avg:39.66ms
step:752/1775 train_time:29819ms step_avg:39.65ms
step:753/1775 train_time:29879ms step_avg:39.68ms
step:754/1775 train_time:29943ms step_avg:39.71ms
step:755/1775 train_time:30003ms step_avg:39.74ms
step:756/1775 train_time:30066ms step_avg:39.77ms
step:757/1775 train_time:30125ms step_avg:39.80ms
step:758/1775 train_time:30186ms step_avg:39.82ms
step:759/1775 train_time:30245ms step_avg:39.85ms
step:760/1775 train_time:30307ms step_avg:39.88ms
step:761/1775 train_time:30365ms step_avg:39.90ms
step:762/1775 train_time:30428ms step_avg:39.93ms
step:763/1775 train_time:30487ms step_avg:39.96ms
step:764/1775 train_time:30549ms step_avg:39.99ms
step:765/1775 train_time:30608ms step_avg:40.01ms
step:766/1775 train_time:30670ms step_avg:40.04ms
step:767/1775 train_time:30730ms step_avg:40.07ms
step:768/1775 train_time:30795ms step_avg:40.10ms
step:769/1775 train_time:30856ms step_avg:40.13ms
step:770/1775 train_time:30920ms step_avg:40.16ms
step:771/1775 train_time:30981ms step_avg:40.18ms
step:772/1775 train_time:31044ms step_avg:40.21ms
step:773/1775 train_time:31104ms step_avg:40.24ms
step:774/1775 train_time:31165ms step_avg:40.26ms
step:775/1775 train_time:31225ms step_avg:40.29ms
step:776/1775 train_time:31287ms step_avg:40.32ms
step:777/1775 train_time:31345ms step_avg:40.34ms
step:778/1775 train_time:31407ms step_avg:40.37ms
step:779/1775 train_time:31467ms step_avg:40.39ms
step:780/1775 train_time:31529ms step_avg:40.42ms
step:781/1775 train_time:31588ms step_avg:40.45ms
step:782/1775 train_time:31650ms step_avg:40.47ms
step:783/1775 train_time:31709ms step_avg:40.50ms
step:784/1775 train_time:31773ms step_avg:40.53ms
step:785/1775 train_time:31834ms step_avg:40.55ms
step:786/1775 train_time:31896ms step_avg:40.58ms
step:787/1775 train_time:31957ms step_avg:40.61ms
step:788/1775 train_time:32020ms step_avg:40.63ms
step:789/1775 train_time:32080ms step_avg:40.66ms
step:790/1775 train_time:32142ms step_avg:40.69ms
step:791/1775 train_time:32202ms step_avg:40.71ms
step:792/1775 train_time:32264ms step_avg:40.74ms
step:793/1775 train_time:32323ms step_avg:40.76ms
step:794/1775 train_time:32384ms step_avg:40.79ms
step:795/1775 train_time:32444ms step_avg:40.81ms
step:796/1775 train_time:32505ms step_avg:40.84ms
step:797/1775 train_time:32564ms step_avg:40.86ms
step:798/1775 train_time:32627ms step_avg:40.89ms
step:799/1775 train_time:32685ms step_avg:40.91ms
step:800/1775 train_time:32748ms step_avg:40.94ms
step:801/1775 train_time:32808ms step_avg:40.96ms
step:802/1775 train_time:32871ms step_avg:40.99ms
step:803/1775 train_time:32930ms step_avg:41.01ms
step:804/1775 train_time:32993ms step_avg:41.04ms
step:805/1775 train_time:33053ms step_avg:41.06ms
step:806/1775 train_time:33116ms step_avg:41.09ms
step:807/1775 train_time:33177ms step_avg:41.11ms
step:808/1775 train_time:33241ms step_avg:41.14ms
step:809/1775 train_time:33300ms step_avg:41.16ms
step:810/1775 train_time:33362ms step_avg:41.19ms
step:811/1775 train_time:33422ms step_avg:41.21ms
step:812/1775 train_time:33484ms step_avg:41.24ms
step:813/1775 train_time:33543ms step_avg:41.26ms
step:814/1775 train_time:33606ms step_avg:41.28ms
step:815/1775 train_time:33665ms step_avg:41.31ms
step:816/1775 train_time:33727ms step_avg:41.33ms
step:817/1775 train_time:33787ms step_avg:41.35ms
step:818/1775 train_time:33849ms step_avg:41.38ms
step:819/1775 train_time:33908ms step_avg:41.40ms
step:820/1775 train_time:33970ms step_avg:41.43ms
step:821/1775 train_time:34030ms step_avg:41.45ms
step:822/1775 train_time:34093ms step_avg:41.48ms
step:823/1775 train_time:34152ms step_avg:41.50ms
step:824/1775 train_time:34214ms step_avg:41.52ms
step:825/1775 train_time:34275ms step_avg:41.55ms
step:826/1775 train_time:34338ms step_avg:41.57ms
step:827/1775 train_time:34398ms step_avg:41.59ms
step:828/1775 train_time:34460ms step_avg:41.62ms
step:829/1775 train_time:34520ms step_avg:41.64ms
step:830/1775 train_time:34583ms step_avg:41.67ms
step:831/1775 train_time:34643ms step_avg:41.69ms
step:832/1775 train_time:34705ms step_avg:41.71ms
step:833/1775 train_time:34765ms step_avg:41.73ms
step:834/1775 train_time:34827ms step_avg:41.76ms
step:835/1775 train_time:34887ms step_avg:41.78ms
step:836/1775 train_time:34949ms step_avg:41.80ms
step:837/1775 train_time:35008ms step_avg:41.83ms
step:838/1775 train_time:35070ms step_avg:41.85ms
step:839/1775 train_time:35130ms step_avg:41.87ms
step:840/1775 train_time:35193ms step_avg:41.90ms
step:841/1775 train_time:35253ms step_avg:41.92ms
step:842/1775 train_time:35315ms step_avg:41.94ms
step:843/1775 train_time:35376ms step_avg:41.96ms
step:844/1775 train_time:35439ms step_avg:41.99ms
step:845/1775 train_time:35498ms step_avg:42.01ms
step:846/1775 train_time:35560ms step_avg:42.03ms
step:847/1775 train_time:35620ms step_avg:42.05ms
step:848/1775 train_time:35683ms step_avg:42.08ms
step:849/1775 train_time:35742ms step_avg:42.10ms
step:850/1775 train_time:35805ms step_avg:42.12ms
step:851/1775 train_time:35865ms step_avg:42.14ms
step:852/1775 train_time:35927ms step_avg:42.17ms
step:853/1775 train_time:35987ms step_avg:42.19ms
step:854/1775 train_time:36049ms step_avg:42.21ms
step:855/1775 train_time:36108ms step_avg:42.23ms
step:856/1775 train_time:36170ms step_avg:42.25ms
step:857/1775 train_time:36229ms step_avg:42.27ms
step:858/1775 train_time:36292ms step_avg:42.30ms
step:859/1775 train_time:36352ms step_avg:42.32ms
step:860/1775 train_time:36415ms step_avg:42.34ms
step:861/1775 train_time:36476ms step_avg:42.36ms
step:862/1775 train_time:36538ms step_avg:42.39ms
step:863/1775 train_time:36598ms step_avg:42.41ms
step:864/1775 train_time:36661ms step_avg:42.43ms
step:865/1775 train_time:36721ms step_avg:42.45ms
step:866/1775 train_time:36783ms step_avg:42.48ms
step:867/1775 train_time:36843ms step_avg:42.49ms
step:868/1775 train_time:36905ms step_avg:42.52ms
step:869/1775 train_time:36965ms step_avg:42.54ms
step:870/1775 train_time:37027ms step_avg:42.56ms
step:871/1775 train_time:37087ms step_avg:42.58ms
step:872/1775 train_time:37149ms step_avg:42.60ms
step:873/1775 train_time:37207ms step_avg:42.62ms
step:874/1775 train_time:37269ms step_avg:42.64ms
step:875/1775 train_time:37330ms step_avg:42.66ms
step:876/1775 train_time:37392ms step_avg:42.68ms
step:877/1775 train_time:37452ms step_avg:42.70ms
step:878/1775 train_time:37514ms step_avg:42.73ms
step:879/1775 train_time:37574ms step_avg:42.75ms
step:880/1775 train_time:37638ms step_avg:42.77ms
step:881/1775 train_time:37698ms step_avg:42.79ms
step:882/1775 train_time:37761ms step_avg:42.81ms
step:883/1775 train_time:37821ms step_avg:42.83ms
step:884/1775 train_time:37884ms step_avg:42.86ms
step:885/1775 train_time:37944ms step_avg:42.87ms
step:886/1775 train_time:38007ms step_avg:42.90ms
step:887/1775 train_time:38066ms step_avg:42.92ms
step:888/1775 train_time:38128ms step_avg:42.94ms
step:889/1775 train_time:38187ms step_avg:42.96ms
step:890/1775 train_time:38249ms step_avg:42.98ms
step:891/1775 train_time:38308ms step_avg:42.99ms
step:892/1775 train_time:38370ms step_avg:43.02ms
step:893/1775 train_time:38429ms step_avg:43.03ms
step:894/1775 train_time:38491ms step_avg:43.05ms
step:895/1775 train_time:38550ms step_avg:43.07ms
step:896/1775 train_time:38613ms step_avg:43.10ms
step:897/1775 train_time:38673ms step_avg:43.11ms
step:898/1775 train_time:38736ms step_avg:43.14ms
step:899/1775 train_time:38797ms step_avg:43.16ms
step:900/1775 train_time:38860ms step_avg:43.18ms
step:901/1775 train_time:38920ms step_avg:43.20ms
step:902/1775 train_time:38984ms step_avg:43.22ms
step:903/1775 train_time:39044ms step_avg:43.24ms
step:904/1775 train_time:39105ms step_avg:43.26ms
step:905/1775 train_time:39165ms step_avg:43.28ms
step:906/1775 train_time:39228ms step_avg:43.30ms
step:907/1775 train_time:39287ms step_avg:43.32ms
step:908/1775 train_time:39349ms step_avg:43.34ms
step:909/1775 train_time:39407ms step_avg:43.35ms
step:910/1775 train_time:39470ms step_avg:43.37ms
step:911/1775 train_time:39529ms step_avg:43.39ms
step:912/1775 train_time:39590ms step_avg:43.41ms
step:913/1775 train_time:39651ms step_avg:43.43ms
step:914/1775 train_time:39714ms step_avg:43.45ms
step:915/1775 train_time:39774ms step_avg:43.47ms
step:916/1775 train_time:39837ms step_avg:43.49ms
step:917/1775 train_time:39897ms step_avg:43.51ms
step:918/1775 train_time:39961ms step_avg:43.53ms
step:919/1775 train_time:40021ms step_avg:43.55ms
step:920/1775 train_time:40083ms step_avg:43.57ms
step:921/1775 train_time:40144ms step_avg:43.59ms
step:922/1775 train_time:40205ms step_avg:43.61ms
step:923/1775 train_time:40265ms step_avg:43.62ms
step:924/1775 train_time:40327ms step_avg:43.64ms
step:925/1775 train_time:40387ms step_avg:43.66ms
step:926/1775 train_time:40449ms step_avg:43.68ms
step:927/1775 train_time:40508ms step_avg:43.70ms
step:928/1775 train_time:40570ms step_avg:43.72ms
step:929/1775 train_time:40629ms step_avg:43.73ms
step:930/1775 train_time:40692ms step_avg:43.75ms
step:931/1775 train_time:40751ms step_avg:43.77ms
step:932/1775 train_time:40814ms step_avg:43.79ms
step:933/1775 train_time:40875ms step_avg:43.81ms
step:934/1775 train_time:40937ms step_avg:43.83ms
step:935/1775 train_time:40998ms step_avg:43.85ms
step:936/1775 train_time:41061ms step_avg:43.87ms
step:937/1775 train_time:41121ms step_avg:43.89ms
step:938/1775 train_time:41184ms step_avg:43.91ms
step:939/1775 train_time:41243ms step_avg:43.92ms
step:940/1775 train_time:41306ms step_avg:43.94ms
step:941/1775 train_time:41365ms step_avg:43.96ms
step:942/1775 train_time:41427ms step_avg:43.98ms
step:943/1775 train_time:41487ms step_avg:43.99ms
step:944/1775 train_time:41548ms step_avg:44.01ms
step:945/1775 train_time:41607ms step_avg:44.03ms
step:946/1775 train_time:41669ms step_avg:44.05ms
step:947/1775 train_time:41728ms step_avg:44.06ms
step:948/1775 train_time:41790ms step_avg:44.08ms
step:949/1775 train_time:41850ms step_avg:44.10ms
step:950/1775 train_time:41913ms step_avg:44.12ms
step:951/1775 train_time:41973ms step_avg:44.14ms
step:952/1775 train_time:42035ms step_avg:44.15ms
step:953/1775 train_time:42096ms step_avg:44.17ms
step:954/1775 train_time:42159ms step_avg:44.19ms
step:955/1775 train_time:42220ms step_avg:44.21ms
step:956/1775 train_time:42283ms step_avg:44.23ms
step:957/1775 train_time:42343ms step_avg:44.25ms
step:958/1775 train_time:42405ms step_avg:44.26ms
step:959/1775 train_time:42464ms step_avg:44.28ms
step:960/1775 train_time:42527ms step_avg:44.30ms
step:961/1775 train_time:42587ms step_avg:44.32ms
step:962/1775 train_time:42649ms step_avg:44.33ms
step:963/1775 train_time:42708ms step_avg:44.35ms
step:964/1775 train_time:42770ms step_avg:44.37ms
step:965/1775 train_time:42830ms step_avg:44.38ms
step:966/1775 train_time:42892ms step_avg:44.40ms
step:967/1775 train_time:42951ms step_avg:44.42ms
step:968/1775 train_time:43013ms step_avg:44.43ms
step:969/1775 train_time:43073ms step_avg:44.45ms
step:970/1775 train_time:43136ms step_avg:44.47ms
step:971/1775 train_time:43196ms step_avg:44.49ms
step:972/1775 train_time:43259ms step_avg:44.51ms
step:973/1775 train_time:43321ms step_avg:44.52ms
step:974/1775 train_time:43383ms step_avg:44.54ms
step:975/1775 train_time:43443ms step_avg:44.56ms
step:976/1775 train_time:43505ms step_avg:44.57ms
step:977/1775 train_time:43565ms step_avg:44.59ms
step:978/1775 train_time:43627ms step_avg:44.61ms
step:979/1775 train_time:43686ms step_avg:44.62ms
step:980/1775 train_time:43748ms step_avg:44.64ms
step:981/1775 train_time:43808ms step_avg:44.66ms
step:982/1775 train_time:43869ms step_avg:44.67ms
step:983/1775 train_time:43929ms step_avg:44.69ms
step:984/1775 train_time:43991ms step_avg:44.71ms
step:985/1775 train_time:44051ms step_avg:44.72ms
step:986/1775 train_time:44114ms step_avg:44.74ms
step:987/1775 train_time:44174ms step_avg:44.76ms
step:988/1775 train_time:44238ms step_avg:44.78ms
step:989/1775 train_time:44298ms step_avg:44.79ms
step:990/1775 train_time:44361ms step_avg:44.81ms
step:991/1775 train_time:44421ms step_avg:44.82ms
step:992/1775 train_time:44484ms step_avg:44.84ms
step:993/1775 train_time:44545ms step_avg:44.86ms
step:994/1775 train_time:44606ms step_avg:44.88ms
step:995/1775 train_time:44666ms step_avg:44.89ms
step:996/1775 train_time:44728ms step_avg:44.91ms
step:997/1775 train_time:44787ms step_avg:44.92ms
step:998/1775 train_time:44849ms step_avg:44.94ms
step:999/1775 train_time:44908ms step_avg:44.95ms
step:1000/1775 train_time:44969ms step_avg:44.97ms
step:1000/1775 val_loss:3.7399 train_time:45040ms step_avg:45.04ms
step:1001/1775 train_time:45061ms step_avg:45.02ms
step:1002/1775 train_time:45095ms step_avg:45.01ms
step:1003/1775 train_time:45157ms step_avg:45.02ms
step:1004/1775 train_time:45219ms step_avg:45.04ms
step:1005/1775 train_time:45279ms step_avg:45.05ms
step:1006/1775 train_time:45340ms step_avg:45.07ms
step:1007/1775 train_time:45400ms step_avg:45.08ms
step:1008/1775 train_time:45461ms step_avg:45.10ms
step:1009/1775 train_time:45520ms step_avg:45.11ms
step:1010/1775 train_time:45582ms step_avg:45.13ms
step:1011/1775 train_time:45641ms step_avg:45.14ms
step:1012/1775 train_time:45703ms step_avg:45.16ms
step:1013/1775 train_time:45762ms step_avg:45.17ms
step:1014/1775 train_time:45823ms step_avg:45.19ms
step:1015/1775 train_time:45882ms step_avg:45.20ms
step:1016/1775 train_time:45944ms step_avg:45.22ms
step:1017/1775 train_time:46003ms step_avg:45.23ms
step:1018/1775 train_time:46066ms step_avg:45.25ms
step:1019/1775 train_time:46128ms step_avg:45.27ms
step:1020/1775 train_time:46191ms step_avg:45.29ms
step:1021/1775 train_time:46251ms step_avg:45.30ms
step:1022/1775 train_time:46314ms step_avg:45.32ms
step:1023/1775 train_time:46374ms step_avg:45.33ms
step:1024/1775 train_time:46435ms step_avg:45.35ms
step:1025/1775 train_time:46495ms step_avg:45.36ms
step:1026/1775 train_time:46557ms step_avg:45.38ms
step:1027/1775 train_time:46616ms step_avg:45.39ms
step:1028/1775 train_time:46678ms step_avg:45.41ms
step:1029/1775 train_time:46737ms step_avg:45.42ms
step:1030/1775 train_time:46799ms step_avg:45.44ms
step:1031/1775 train_time:46857ms step_avg:45.45ms
step:1032/1775 train_time:46919ms step_avg:45.46ms
step:1033/1775 train_time:46978ms step_avg:45.48ms
step:1034/1775 train_time:47041ms step_avg:45.49ms
step:1035/1775 train_time:47100ms step_avg:45.51ms
step:1036/1775 train_time:47163ms step_avg:45.52ms
step:1037/1775 train_time:47223ms step_avg:45.54ms
step:1038/1775 train_time:47286ms step_avg:45.56ms
step:1039/1775 train_time:47346ms step_avg:45.57ms
step:1040/1775 train_time:47409ms step_avg:45.59ms
step:1041/1775 train_time:47470ms step_avg:45.60ms
step:1042/1775 train_time:47533ms step_avg:45.62ms
step:1043/1775 train_time:47594ms step_avg:45.63ms
step:1044/1775 train_time:47657ms step_avg:45.65ms
step:1045/1775 train_time:47715ms step_avg:45.66ms
step:1046/1775 train_time:47777ms step_avg:45.68ms
step:1047/1775 train_time:47836ms step_avg:45.69ms
step:1048/1775 train_time:47897ms step_avg:45.70ms
step:1049/1775 train_time:47957ms step_avg:45.72ms
step:1050/1775 train_time:48018ms step_avg:45.73ms
step:1051/1775 train_time:48078ms step_avg:45.75ms
step:1052/1775 train_time:48140ms step_avg:45.76ms
step:1053/1775 train_time:48199ms step_avg:45.77ms
step:1054/1775 train_time:48261ms step_avg:45.79ms
step:1055/1775 train_time:48321ms step_avg:45.80ms
step:1056/1775 train_time:48383ms step_avg:45.82ms
step:1057/1775 train_time:48443ms step_avg:45.83ms
step:1058/1775 train_time:48506ms step_avg:45.85ms
step:1059/1775 train_time:48566ms step_avg:45.86ms
step:1060/1775 train_time:48629ms step_avg:45.88ms
step:1061/1775 train_time:48690ms step_avg:45.89ms
step:1062/1775 train_time:48752ms step_avg:45.91ms
step:1063/1775 train_time:48812ms step_avg:45.92ms
step:1064/1775 train_time:48874ms step_avg:45.93ms
step:1065/1775 train_time:48934ms step_avg:45.95ms
step:1066/1775 train_time:48996ms step_avg:45.96ms
step:1067/1775 train_time:49056ms step_avg:45.98ms
step:1068/1775 train_time:49118ms step_avg:45.99ms
step:1069/1775 train_time:49177ms step_avg:46.00ms
step:1070/1775 train_time:49239ms step_avg:46.02ms
step:1071/1775 train_time:49299ms step_avg:46.03ms
step:1072/1775 train_time:49361ms step_avg:46.05ms
step:1073/1775 train_time:49420ms step_avg:46.06ms
step:1074/1775 train_time:49482ms step_avg:46.07ms
step:1075/1775 train_time:49542ms step_avg:46.09ms
step:1076/1775 train_time:49604ms step_avg:46.10ms
step:1077/1775 train_time:49663ms step_avg:46.11ms
step:1078/1775 train_time:49726ms step_avg:46.13ms
step:1079/1775 train_time:49786ms step_avg:46.14ms
step:1080/1775 train_time:49849ms step_avg:46.16ms
step:1081/1775 train_time:49909ms step_avg:46.17ms
step:1082/1775 train_time:49971ms step_avg:46.18ms
step:1083/1775 train_time:50032ms step_avg:46.20ms
step:1084/1775 train_time:50094ms step_avg:46.21ms
step:1085/1775 train_time:50155ms step_avg:46.23ms
step:1086/1775 train_time:50217ms step_avg:46.24ms
step:1087/1775 train_time:50276ms step_avg:46.25ms
step:1088/1775 train_time:50338ms step_avg:46.27ms
step:1089/1775 train_time:50398ms step_avg:46.28ms
step:1090/1775 train_time:50460ms step_avg:46.29ms
step:1091/1775 train_time:50519ms step_avg:46.30ms
step:1092/1775 train_time:50580ms step_avg:46.32ms
step:1093/1775 train_time:50639ms step_avg:46.33ms
step:1094/1775 train_time:50701ms step_avg:46.34ms
step:1095/1775 train_time:50761ms step_avg:46.36ms
step:1096/1775 train_time:50823ms step_avg:46.37ms
step:1097/1775 train_time:50883ms step_avg:46.38ms
step:1098/1775 train_time:50945ms step_avg:46.40ms
step:1099/1775 train_time:51005ms step_avg:46.41ms
step:1100/1775 train_time:51069ms step_avg:46.43ms
step:1101/1775 train_time:51130ms step_avg:46.44ms
step:1102/1775 train_time:51192ms step_avg:46.45ms
step:1103/1775 train_time:51253ms step_avg:46.47ms
step:1104/1775 train_time:51315ms step_avg:46.48ms
step:1105/1775 train_time:51375ms step_avg:46.49ms
step:1106/1775 train_time:51437ms step_avg:46.51ms
step:1107/1775 train_time:51497ms step_avg:46.52ms
step:1108/1775 train_time:51558ms step_avg:46.53ms
step:1109/1775 train_time:51619ms step_avg:46.55ms
step:1110/1775 train_time:51681ms step_avg:46.56ms
step:1111/1775 train_time:51740ms step_avg:46.57ms
step:1112/1775 train_time:51802ms step_avg:46.58ms
step:1113/1775 train_time:51861ms step_avg:46.60ms
step:1114/1775 train_time:51923ms step_avg:46.61ms
step:1115/1775 train_time:51982ms step_avg:46.62ms
step:1116/1775 train_time:52044ms step_avg:46.63ms
step:1117/1775 train_time:52105ms step_avg:46.65ms
step:1118/1775 train_time:52168ms step_avg:46.66ms
step:1119/1775 train_time:52228ms step_avg:46.67ms
step:1120/1775 train_time:52291ms step_avg:46.69ms
step:1121/1775 train_time:52351ms step_avg:46.70ms
step:1122/1775 train_time:52414ms step_avg:46.71ms
step:1123/1775 train_time:52474ms step_avg:46.73ms
step:1124/1775 train_time:52536ms step_avg:46.74ms
step:1125/1775 train_time:52596ms step_avg:46.75ms
step:1126/1775 train_time:52659ms step_avg:46.77ms
step:1127/1775 train_time:52717ms step_avg:46.78ms
step:1128/1775 train_time:52780ms step_avg:46.79ms
step:1129/1775 train_time:52839ms step_avg:46.80ms
step:1130/1775 train_time:52900ms step_avg:46.81ms
step:1131/1775 train_time:52960ms step_avg:46.83ms
step:1132/1775 train_time:53021ms step_avg:46.84ms
step:1133/1775 train_time:53081ms step_avg:46.85ms
step:1134/1775 train_time:53143ms step_avg:46.86ms
step:1135/1775 train_time:53203ms step_avg:46.87ms
step:1136/1775 train_time:53265ms step_avg:46.89ms
step:1137/1775 train_time:53325ms step_avg:46.90ms
step:1138/1775 train_time:53388ms step_avg:46.91ms
step:1139/1775 train_time:53448ms step_avg:46.93ms
step:1140/1775 train_time:53510ms step_avg:46.94ms
step:1141/1775 train_time:53571ms step_avg:46.95ms
step:1142/1775 train_time:53633ms step_avg:46.96ms
step:1143/1775 train_time:53693ms step_avg:46.98ms
step:1144/1775 train_time:53756ms step_avg:46.99ms
step:1145/1775 train_time:53815ms step_avg:47.00ms
step:1146/1775 train_time:53877ms step_avg:47.01ms
step:1147/1775 train_time:53938ms step_avg:47.03ms
step:1148/1775 train_time:53999ms step_avg:47.04ms
step:1149/1775 train_time:54059ms step_avg:47.05ms
step:1150/1775 train_time:54120ms step_avg:47.06ms
step:1151/1775 train_time:54179ms step_avg:47.07ms
step:1152/1775 train_time:54241ms step_avg:47.08ms
step:1153/1775 train_time:54301ms step_avg:47.10ms
step:1154/1775 train_time:54364ms step_avg:47.11ms
step:1155/1775 train_time:54423ms step_avg:47.12ms
step:1156/1775 train_time:54485ms step_avg:47.13ms
step:1157/1775 train_time:54545ms step_avg:47.14ms
step:1158/1775 train_time:54612ms step_avg:47.16ms
step:1159/1775 train_time:54699ms step_avg:47.20ms
step:1160/1775 train_time:54789ms step_avg:47.23ms
step:1161/1775 train_time:54876ms step_avg:47.27ms
step:1162/1775 train_time:54964ms step_avg:47.30ms
step:1163/1775 train_time:55051ms step_avg:47.34ms
step:1164/1775 train_time:55141ms step_avg:47.37ms
step:1165/1775 train_time:55229ms step_avg:47.41ms
step:1166/1775 train_time:55318ms step_avg:47.44ms
step:1167/1775 train_time:55404ms step_avg:47.48ms
step:1168/1775 train_time:55491ms step_avg:47.51ms
step:1169/1775 train_time:55577ms step_avg:47.54ms
step:1170/1775 train_time:55667ms step_avg:47.58ms
step:1171/1775 train_time:55753ms step_avg:47.61ms
step:1172/1775 train_time:55842ms step_avg:47.65ms
step:1173/1775 train_time:55929ms step_avg:47.68ms
step:1174/1775 train_time:56018ms step_avg:47.72ms
step:1175/1775 train_time:56103ms step_avg:47.75ms
step:1176/1775 train_time:56193ms step_avg:47.78ms
step:1177/1775 train_time:56280ms step_avg:47.82ms
step:1178/1775 train_time:56368ms step_avg:47.85ms
step:1179/1775 train_time:56455ms step_avg:47.88ms
step:1180/1775 train_time:56543ms step_avg:47.92ms
step:1181/1775 train_time:56630ms step_avg:47.95ms
step:1182/1775 train_time:56719ms step_avg:47.99ms
step:1183/1775 train_time:56806ms step_avg:48.02ms
step:1184/1775 train_time:56894ms step_avg:48.05ms
step:1185/1775 train_time:56980ms step_avg:48.08ms
step:1186/1775 train_time:57069ms step_avg:48.12ms
step:1187/1775 train_time:57155ms step_avg:48.15ms
step:1188/1775 train_time:57244ms step_avg:48.19ms
step:1189/1775 train_time:57330ms step_avg:48.22ms
step:1190/1775 train_time:57421ms step_avg:48.25ms
step:1191/1775 train_time:57506ms step_avg:48.28ms
step:1192/1775 train_time:57595ms step_avg:48.32ms
step:1193/1775 train_time:57682ms step_avg:48.35ms
step:1194/1775 train_time:57770ms step_avg:48.38ms
step:1195/1775 train_time:57856ms step_avg:48.41ms
step:1196/1775 train_time:57946ms step_avg:48.45ms
step:1197/1775 train_time:58032ms step_avg:48.48ms
step:1198/1775 train_time:58121ms step_avg:48.51ms
step:1199/1775 train_time:58207ms step_avg:48.55ms
step:1200/1775 train_time:58296ms step_avg:48.58ms
step:1201/1775 train_time:58382ms step_avg:48.61ms
step:1202/1775 train_time:58469ms step_avg:48.64ms
step:1203/1775 train_time:58556ms step_avg:48.67ms
step:1204/1775 train_time:58645ms step_avg:48.71ms
step:1205/1775 train_time:58730ms step_avg:48.74ms
step:1206/1775 train_time:58820ms step_avg:48.77ms
step:1207/1775 train_time:58907ms step_avg:48.80ms
step:1208/1775 train_time:58996ms step_avg:48.84ms
step:1209/1775 train_time:59083ms step_avg:48.87ms
step:1210/1775 train_time:59171ms step_avg:48.90ms
step:1211/1775 train_time:59258ms step_avg:48.93ms
step:1212/1775 train_time:59347ms step_avg:48.97ms
step:1213/1775 train_time:59433ms step_avg:49.00ms
step:1214/1775 train_time:59521ms step_avg:49.03ms
step:1215/1775 train_time:59608ms step_avg:49.06ms
step:1216/1775 train_time:59697ms step_avg:49.09ms
step:1217/1775 train_time:59785ms step_avg:49.12ms
step:1218/1775 train_time:59873ms step_avg:49.16ms
step:1219/1775 train_time:59960ms step_avg:49.19ms
step:1220/1775 train_time:60048ms step_avg:49.22ms
step:1221/1775 train_time:60134ms step_avg:49.25ms
step:1222/1775 train_time:60223ms step_avg:49.28ms
step:1223/1775 train_time:60311ms step_avg:49.31ms
step:1224/1775 train_time:60400ms step_avg:49.35ms
step:1225/1775 train_time:60487ms step_avg:49.38ms
step:1226/1775 train_time:60575ms step_avg:49.41ms
step:1227/1775 train_time:60661ms step_avg:49.44ms
step:1228/1775 train_time:60749ms step_avg:49.47ms
step:1229/1775 train_time:60835ms step_avg:49.50ms
step:1230/1775 train_time:60924ms step_avg:49.53ms
step:1231/1775 train_time:61010ms step_avg:49.56ms
step:1232/1775 train_time:61100ms step_avg:49.59ms
step:1233/1775 train_time:61188ms step_avg:49.63ms
step:1234/1775 train_time:61276ms step_avg:49.66ms
step:1235/1775 train_time:61363ms step_avg:49.69ms
step:1236/1775 train_time:61451ms step_avg:49.72ms
step:1237/1775 train_time:61537ms step_avg:49.75ms
step:1238/1775 train_time:61625ms step_avg:49.78ms
step:1239/1775 train_time:61710ms step_avg:49.81ms
step:1240/1775 train_time:61800ms step_avg:49.84ms
step:1241/1775 train_time:61887ms step_avg:49.87ms
step:1242/1775 train_time:61976ms step_avg:49.90ms
step:1243/1775 train_time:62061ms step_avg:49.93ms
step:1244/1775 train_time:62151ms step_avg:49.96ms
step:1245/1775 train_time:62237ms step_avg:49.99ms
step:1246/1775 train_time:62327ms step_avg:50.02ms
step:1247/1775 train_time:62412ms step_avg:50.05ms
step:1248/1775 train_time:62501ms step_avg:50.08ms
step:1249/1775 train_time:62589ms step_avg:50.11ms
step:1250/1775 train_time:62678ms step_avg:50.14ms
step:1250/1775 val_loss:3.5080 train_time:62776ms step_avg:50.22ms
step:1251/1775 train_time:62797ms step_avg:50.20ms
step:1252/1775 train_time:62859ms step_avg:50.21ms
step:1253/1775 train_time:62949ms step_avg:50.24ms
step:1254/1775 train_time:63039ms step_avg:50.27ms
step:1255/1775 train_time:63125ms step_avg:50.30ms
step:1256/1775 train_time:63212ms step_avg:50.33ms
step:1257/1775 train_time:63298ms step_avg:50.36ms
step:1258/1775 train_time:63385ms step_avg:50.39ms
step:1259/1775 train_time:63470ms step_avg:50.41ms
step:1260/1775 train_time:63559ms step_avg:50.44ms
step:1261/1775 train_time:63643ms step_avg:50.47ms
step:1262/1775 train_time:63733ms step_avg:50.50ms
step:1263/1775 train_time:63821ms step_avg:50.53ms
step:1264/1775 train_time:63913ms step_avg:50.56ms
step:1265/1775 train_time:64000ms step_avg:50.59ms
step:1266/1775 train_time:64089ms step_avg:50.62ms
step:1267/1775 train_time:64175ms step_avg:50.65ms
step:1268/1775 train_time:64264ms step_avg:50.68ms
step:1269/1775 train_time:64349ms step_avg:50.71ms
step:1270/1775 train_time:64437ms step_avg:50.74ms
step:1271/1775 train_time:64523ms step_avg:50.77ms
step:1272/1775 train_time:64610ms step_avg:50.79ms
step:1273/1775 train_time:64697ms step_avg:50.82ms
step:1274/1775 train_time:64788ms step_avg:50.85ms
step:1275/1775 train_time:64876ms step_avg:50.88ms
step:1276/1775 train_time:64966ms step_avg:50.91ms
step:1277/1775 train_time:65053ms step_avg:50.94ms
step:1278/1775 train_time:65142ms step_avg:50.97ms
step:1279/1775 train_time:65229ms step_avg:51.00ms
step:1280/1775 train_time:65318ms step_avg:51.03ms
step:1281/1775 train_time:65403ms step_avg:51.06ms
step:1282/1775 train_time:65492ms step_avg:51.09ms
step:1283/1775 train_time:65577ms step_avg:51.11ms
step:1284/1775 train_time:65666ms step_avg:51.14ms
step:1285/1775 train_time:65751ms step_avg:51.17ms
step:1286/1775 train_time:65840ms step_avg:51.20ms
step:1287/1775 train_time:65929ms step_avg:51.23ms
step:1288/1775 train_time:66018ms step_avg:51.26ms
step:1289/1775 train_time:66104ms step_avg:51.28ms
step:1290/1775 train_time:66195ms step_avg:51.31ms
step:1291/1775 train_time:66280ms step_avg:51.34ms
step:1292/1775 train_time:66369ms step_avg:51.37ms
step:1293/1775 train_time:66455ms step_avg:51.40ms
step:1294/1775 train_time:66543ms step_avg:51.42ms
step:1295/1775 train_time:66629ms step_avg:51.45ms
step:1296/1775 train_time:66718ms step_avg:51.48ms
step:1297/1775 train_time:66805ms step_avg:51.51ms
step:1298/1775 train_time:66897ms step_avg:51.54ms
step:1299/1775 train_time:66982ms step_avg:51.56ms
step:1300/1775 train_time:67072ms step_avg:51.59ms
step:1301/1775 train_time:67158ms step_avg:51.62ms
step:1302/1775 train_time:67247ms step_avg:51.65ms
step:1303/1775 train_time:67333ms step_avg:51.68ms
step:1304/1775 train_time:67422ms step_avg:51.70ms
step:1305/1775 train_time:67508ms step_avg:51.73ms
step:1306/1775 train_time:67598ms step_avg:51.76ms
step:1307/1775 train_time:67684ms step_avg:51.79ms
step:1308/1775 train_time:67774ms step_avg:51.81ms
step:1309/1775 train_time:67860ms step_avg:51.84ms
step:1310/1775 train_time:67948ms step_avg:51.87ms
step:1311/1775 train_time:68036ms step_avg:51.90ms
step:1312/1775 train_time:68125ms step_avg:51.92ms
step:1313/1775 train_time:68212ms step_avg:51.95ms
step:1314/1775 train_time:68300ms step_avg:51.98ms
step:1315/1775 train_time:68385ms step_avg:52.00ms
step:1316/1775 train_time:68474ms step_avg:52.03ms
step:1317/1775 train_time:68560ms step_avg:52.06ms
step:1318/1775 train_time:68650ms step_avg:52.09ms
step:1319/1775 train_time:68736ms step_avg:52.11ms
step:1320/1775 train_time:68825ms step_avg:52.14ms
step:1321/1775 train_time:68911ms step_avg:52.17ms
step:1322/1775 train_time:68999ms step_avg:52.19ms
step:1323/1775 train_time:69085ms step_avg:52.22ms
step:1324/1775 train_time:69174ms step_avg:52.25ms
step:1325/1775 train_time:69260ms step_avg:52.27ms
step:1326/1775 train_time:69349ms step_avg:52.30ms
step:1327/1775 train_time:69436ms step_avg:52.33ms
step:1328/1775 train_time:69524ms step_avg:52.35ms
step:1329/1775 train_time:69611ms step_avg:52.38ms
step:1330/1775 train_time:69699ms step_avg:52.41ms
step:1331/1775 train_time:69785ms step_avg:52.43ms
step:1332/1775 train_time:69874ms step_avg:52.46ms
step:1333/1775 train_time:69960ms step_avg:52.48ms
step:1334/1775 train_time:70049ms step_avg:52.51ms
step:1335/1775 train_time:70136ms step_avg:52.54ms
step:1336/1775 train_time:70224ms step_avg:52.56ms
step:1337/1775 train_time:70310ms step_avg:52.59ms
step:1338/1775 train_time:70399ms step_avg:52.61ms
step:1339/1775 train_time:70484ms step_avg:52.64ms
step:1340/1775 train_time:70574ms step_avg:52.67ms
step:1341/1775 train_time:70659ms step_avg:52.69ms
step:1342/1775 train_time:70747ms step_avg:52.72ms
step:1343/1775 train_time:70834ms step_avg:52.74ms
step:1344/1775 train_time:70922ms step_avg:52.77ms
step:1345/1775 train_time:71008ms step_avg:52.79ms
step:1346/1775 train_time:71097ms step_avg:52.82ms
step:1347/1775 train_time:71183ms step_avg:52.85ms
step:1348/1775 train_time:71274ms step_avg:52.87ms
step:1349/1775 train_time:71360ms step_avg:52.90ms
step:1350/1775 train_time:71448ms step_avg:52.92ms
step:1351/1775 train_time:71535ms step_avg:52.95ms
step:1352/1775 train_time:71623ms step_avg:52.98ms
step:1353/1775 train_time:71708ms step_avg:53.00ms
step:1354/1775 train_time:71797ms step_avg:53.03ms
step:1355/1775 train_time:71883ms step_avg:53.05ms
step:1356/1775 train_time:71973ms step_avg:53.08ms
step:1357/1775 train_time:72060ms step_avg:53.10ms
step:1358/1775 train_time:72148ms step_avg:53.13ms
step:1359/1775 train_time:72234ms step_avg:53.15ms
step:1360/1775 train_time:72323ms step_avg:53.18ms
step:1361/1775 train_time:72410ms step_avg:53.20ms
step:1362/1775 train_time:72499ms step_avg:53.23ms
step:1363/1775 train_time:72586ms step_avg:53.25ms
step:1364/1775 train_time:72675ms step_avg:53.28ms
step:1365/1775 train_time:72761ms step_avg:53.30ms
step:1366/1775 train_time:72849ms step_avg:53.33ms
step:1367/1775 train_time:72936ms step_avg:53.35ms
step:1368/1775 train_time:73024ms step_avg:53.38ms
step:1369/1775 train_time:73110ms step_avg:53.40ms
step:1370/1775 train_time:73200ms step_avg:53.43ms
step:1371/1775 train_time:73287ms step_avg:53.46ms
step:1372/1775 train_time:73377ms step_avg:53.48ms
step:1373/1775 train_time:73463ms step_avg:53.51ms
step:1374/1775 train_time:73552ms step_avg:53.53ms
step:1375/1775 train_time:73638ms step_avg:53.55ms
step:1376/1775 train_time:73727ms step_avg:53.58ms
step:1377/1775 train_time:73813ms step_avg:53.60ms
step:1378/1775 train_time:73901ms step_avg:53.63ms
step:1379/1775 train_time:73988ms step_avg:53.65ms
step:1380/1775 train_time:74078ms step_avg:53.68ms
step:1381/1775 train_time:74164ms step_avg:53.70ms
step:1382/1775 train_time:74253ms step_avg:53.73ms
step:1383/1775 train_time:74340ms step_avg:53.75ms
step:1384/1775 train_time:74428ms step_avg:53.78ms
step:1385/1775 train_time:74515ms step_avg:53.80ms
step:1386/1775 train_time:74603ms step_avg:53.83ms
step:1387/1775 train_time:74688ms step_avg:53.85ms
step:1388/1775 train_time:74777ms step_avg:53.87ms
step:1389/1775 train_time:74863ms step_avg:53.90ms
step:1390/1775 train_time:74952ms step_avg:53.92ms
step:1391/1775 train_time:75039ms step_avg:53.95ms
step:1392/1775 train_time:75127ms step_avg:53.97ms
step:1393/1775 train_time:75214ms step_avg:53.99ms
step:1394/1775 train_time:75303ms step_avg:54.02ms
step:1395/1775 train_time:75389ms step_avg:54.04ms
step:1396/1775 train_time:75478ms step_avg:54.07ms
step:1397/1775 train_time:75564ms step_avg:54.09ms
step:1398/1775 train_time:75654ms step_avg:54.12ms
step:1399/1775 train_time:75740ms step_avg:54.14ms
step:1400/1775 train_time:75829ms step_avg:54.16ms
step:1401/1775 train_time:75916ms step_avg:54.19ms
step:1402/1775 train_time:76004ms step_avg:54.21ms
step:1403/1775 train_time:76089ms step_avg:54.23ms
step:1404/1775 train_time:76178ms step_avg:54.26ms
step:1405/1775 train_time:76263ms step_avg:54.28ms
step:1406/1775 train_time:76353ms step_avg:54.30ms
step:1407/1775 train_time:76439ms step_avg:54.33ms
step:1408/1775 train_time:76530ms step_avg:54.35ms
step:1409/1775 train_time:76616ms step_avg:54.38ms
step:1410/1775 train_time:76705ms step_avg:54.40ms
step:1411/1775 train_time:76791ms step_avg:54.42ms
step:1412/1775 train_time:76879ms step_avg:54.45ms
step:1413/1775 train_time:76965ms step_avg:54.47ms
step:1414/1775 train_time:77055ms step_avg:54.49ms
step:1415/1775 train_time:77140ms step_avg:54.52ms
step:1416/1775 train_time:77229ms step_avg:54.54ms
step:1417/1775 train_time:77315ms step_avg:54.56ms
step:1418/1775 train_time:77405ms step_avg:54.59ms
step:1419/1775 train_time:77492ms step_avg:54.61ms
step:1420/1775 train_time:77580ms step_avg:54.63ms
step:1421/1775 train_time:77667ms step_avg:54.66ms
step:1422/1775 train_time:77755ms step_avg:54.68ms
step:1423/1775 train_time:77842ms step_avg:54.70ms
step:1424/1775 train_time:77931ms step_avg:54.73ms
step:1425/1775 train_time:78018ms step_avg:54.75ms
step:1426/1775 train_time:78108ms step_avg:54.77ms
step:1427/1775 train_time:78194ms step_avg:54.80ms
step:1428/1775 train_time:78282ms step_avg:54.82ms
step:1429/1775 train_time:78369ms step_avg:54.84ms
step:1430/1775 train_time:78457ms step_avg:54.87ms
step:1431/1775 train_time:78542ms step_avg:54.89ms
step:1432/1775 train_time:78634ms step_avg:54.91ms
step:1433/1775 train_time:78720ms step_avg:54.93ms
step:1434/1775 train_time:78809ms step_avg:54.96ms
step:1435/1775 train_time:78896ms step_avg:54.98ms
step:1436/1775 train_time:78985ms step_avg:55.00ms
step:1437/1775 train_time:79070ms step_avg:55.02ms
step:1438/1775 train_time:79159ms step_avg:55.05ms
step:1439/1775 train_time:79245ms step_avg:55.07ms
step:1440/1775 train_time:79334ms step_avg:55.09ms
step:1441/1775 train_time:79419ms step_avg:55.11ms
step:1442/1775 train_time:79509ms step_avg:55.14ms
step:1443/1775 train_time:79595ms step_avg:55.16ms
step:1444/1775 train_time:79684ms step_avg:55.18ms
step:1445/1775 train_time:79771ms step_avg:55.20ms
step:1446/1775 train_time:79860ms step_avg:55.23ms
step:1447/1775 train_time:79946ms step_avg:55.25ms
step:1448/1775 train_time:80035ms step_avg:55.27ms
step:1449/1775 train_time:80122ms step_avg:55.29ms
step:1450/1775 train_time:80209ms step_avg:55.32ms
step:1451/1775 train_time:80296ms step_avg:55.34ms
step:1452/1775 train_time:80385ms step_avg:55.36ms
step:1453/1775 train_time:80471ms step_avg:55.38ms
step:1454/1775 train_time:80560ms step_avg:55.41ms
step:1455/1775 train_time:80646ms step_avg:55.43ms
step:1456/1775 train_time:80737ms step_avg:55.45ms
step:1457/1775 train_time:80823ms step_avg:55.47ms
step:1458/1775 train_time:80911ms step_avg:55.49ms
step:1459/1775 train_time:80999ms step_avg:55.52ms
step:1460/1775 train_time:81088ms step_avg:55.54ms
step:1461/1775 train_time:81174ms step_avg:55.56ms
step:1462/1775 train_time:81262ms step_avg:55.58ms
step:1463/1775 train_time:81349ms step_avg:55.60ms
step:1464/1775 train_time:81437ms step_avg:55.63ms
step:1465/1775 train_time:81522ms step_avg:55.65ms
step:1466/1775 train_time:81611ms step_avg:55.67ms
step:1467/1775 train_time:81698ms step_avg:55.69ms
step:1468/1775 train_time:81788ms step_avg:55.71ms
step:1469/1775 train_time:81875ms step_avg:55.73ms
step:1470/1775 train_time:81964ms step_avg:55.76ms
step:1471/1775 train_time:82051ms step_avg:55.78ms
step:1472/1775 train_time:82139ms step_avg:55.80ms
step:1473/1775 train_time:82226ms step_avg:55.82ms
step:1474/1775 train_time:82315ms step_avg:55.84ms
step:1475/1775 train_time:82401ms step_avg:55.87ms
step:1476/1775 train_time:82489ms step_avg:55.89ms
step:1477/1775 train_time:82577ms step_avg:55.91ms
step:1478/1775 train_time:82666ms step_avg:55.93ms
step:1479/1775 train_time:82751ms step_avg:55.95ms
step:1480/1775 train_time:82840ms step_avg:55.97ms
step:1481/1775 train_time:82926ms step_avg:55.99ms
step:1482/1775 train_time:83015ms step_avg:56.02ms
step:1483/1775 train_time:83102ms step_avg:56.04ms
step:1484/1775 train_time:83191ms step_avg:56.06ms
step:1485/1775 train_time:83278ms step_avg:56.08ms
step:1486/1775 train_time:83366ms step_avg:56.10ms
step:1487/1775 train_time:83452ms step_avg:56.12ms
step:1488/1775 train_time:83540ms step_avg:56.14ms
step:1489/1775 train_time:83626ms step_avg:56.16ms
step:1490/1775 train_time:83715ms step_avg:56.18ms
step:1491/1775 train_time:83801ms step_avg:56.20ms
step:1492/1775 train_time:83890ms step_avg:56.23ms
step:1493/1775 train_time:83976ms step_avg:56.25ms
step:1494/1775 train_time:84065ms step_avg:56.27ms
step:1495/1775 train_time:84152ms step_avg:56.29ms
step:1496/1775 train_time:84240ms step_avg:56.31ms
step:1497/1775 train_time:84328ms step_avg:56.33ms
step:1498/1775 train_time:84417ms step_avg:56.35ms
step:1499/1775 train_time:84503ms step_avg:56.37ms
step:1500/1775 train_time:84592ms step_avg:56.39ms
step:1500/1775 val_loss:3.3772 train_time:84690ms step_avg:56.46ms
step:1501/1775 train_time:84709ms step_avg:56.43ms
step:1502/1775 train_time:84771ms step_avg:56.44ms
step:1503/1775 train_time:84858ms step_avg:56.46ms
step:1504/1775 train_time:84948ms step_avg:56.48ms
step:1505/1775 train_time:85033ms step_avg:56.50ms
step:1506/1775 train_time:85121ms step_avg:56.52ms
step:1507/1775 train_time:85207ms step_avg:56.54ms
step:1508/1775 train_time:85296ms step_avg:56.56ms
step:1509/1775 train_time:85380ms step_avg:56.58ms
step:1510/1775 train_time:85469ms step_avg:56.60ms
step:1511/1775 train_time:85554ms step_avg:56.62ms
step:1512/1775 train_time:85643ms step_avg:56.64ms
step:1513/1775 train_time:85732ms step_avg:56.66ms
step:1514/1775 train_time:85822ms step_avg:56.69ms
step:1515/1775 train_time:85910ms step_avg:56.71ms
step:1516/1775 train_time:85999ms step_avg:56.73ms
step:1517/1775 train_time:86083ms step_avg:56.75ms
step:1518/1775 train_time:86173ms step_avg:56.77ms
step:1519/1775 train_time:86259ms step_avg:56.79ms
step:1520/1775 train_time:86347ms step_avg:56.81ms
step:1521/1775 train_time:86433ms step_avg:56.83ms
step:1522/1775 train_time:86521ms step_avg:56.85ms
step:1523/1775 train_time:86607ms step_avg:56.87ms
step:1524/1775 train_time:86698ms step_avg:56.89ms
step:1525/1775 train_time:86785ms step_avg:56.91ms
step:1526/1775 train_time:86874ms step_avg:56.93ms
step:1527/1775 train_time:86960ms step_avg:56.95ms
step:1528/1775 train_time:87048ms step_avg:56.97ms
step:1529/1775 train_time:87135ms step_avg:56.99ms
step:1530/1775 train_time:87223ms step_avg:57.01ms
step:1531/1775 train_time:87308ms step_avg:57.03ms
step:1532/1775 train_time:87397ms step_avg:57.05ms
step:1533/1775 train_time:87482ms step_avg:57.07ms
step:1534/1775 train_time:87571ms step_avg:57.09ms
step:1535/1775 train_time:87659ms step_avg:57.11ms
step:1536/1775 train_time:87747ms step_avg:57.13ms
step:1537/1775 train_time:87835ms step_avg:57.15ms
step:1538/1775 train_time:87923ms step_avg:57.17ms
step:1539/1775 train_time:88010ms step_avg:57.19ms
step:1540/1775 train_time:88099ms step_avg:57.21ms
step:1541/1775 train_time:88184ms step_avg:57.23ms
step:1542/1775 train_time:88274ms step_avg:57.25ms
step:1543/1775 train_time:88359ms step_avg:57.26ms
step:1544/1775 train_time:88448ms step_avg:57.28ms
step:1545/1775 train_time:88533ms step_avg:57.30ms
step:1546/1775 train_time:88621ms step_avg:57.32ms
step:1547/1775 train_time:88708ms step_avg:57.34ms
step:1548/1775 train_time:88797ms step_avg:57.36ms
step:1549/1775 train_time:88883ms step_avg:57.38ms
step:1550/1775 train_time:88974ms step_avg:57.40ms
step:1551/1775 train_time:89060ms step_avg:57.42ms
step:1552/1775 train_time:89148ms step_avg:57.44ms
step:1553/1775 train_time:89236ms step_avg:57.46ms
step:1554/1775 train_time:89323ms step_avg:57.48ms
step:1555/1775 train_time:89409ms step_avg:57.50ms
step:1556/1775 train_time:89498ms step_avg:57.52ms
step:1557/1775 train_time:89584ms step_avg:57.54ms
step:1558/1775 train_time:89672ms step_avg:57.56ms
step:1559/1775 train_time:89760ms step_avg:57.58ms
step:1560/1775 train_time:89849ms step_avg:57.60ms
step:1561/1775 train_time:89936ms step_avg:57.61ms
step:1562/1775 train_time:90024ms step_avg:57.63ms
step:1563/1775 train_time:90112ms step_avg:57.65ms
step:1564/1775 train_time:90200ms step_avg:57.67ms
step:1565/1775 train_time:90285ms step_avg:57.69ms
step:1566/1775 train_time:90375ms step_avg:57.71ms
step:1567/1775 train_time:90460ms step_avg:57.73ms
step:1568/1775 train_time:90549ms step_avg:57.75ms
step:1569/1775 train_time:90635ms step_avg:57.77ms
step:1570/1775 train_time:90723ms step_avg:57.79ms
step:1571/1775 train_time:90809ms step_avg:57.80ms
step:1572/1775 train_time:90899ms step_avg:57.82ms
step:1573/1775 train_time:90985ms step_avg:57.84ms
step:1574/1775 train_time:91074ms step_avg:57.86ms
step:1575/1775 train_time:91159ms step_avg:57.88ms
step:1576/1775 train_time:91247ms step_avg:57.90ms
step:1577/1775 train_time:91335ms step_avg:57.92ms
step:1578/1775 train_time:91424ms step_avg:57.94ms
step:1579/1775 train_time:91510ms step_avg:57.95ms
step:1580/1775 train_time:91599ms step_avg:57.97ms
step:1581/1775 train_time:91683ms step_avg:57.99ms
step:1582/1775 train_time:91773ms step_avg:58.01ms
step:1583/1775 train_time:91860ms step_avg:58.03ms
step:1584/1775 train_time:91949ms step_avg:58.05ms
step:1585/1775 train_time:92036ms step_avg:58.07ms
step:1586/1775 train_time:92124ms step_avg:58.09ms
step:1587/1775 train_time:92210ms step_avg:58.10ms
step:1588/1775 train_time:92299ms step_avg:58.12ms
step:1589/1775 train_time:92385ms step_avg:58.14ms
step:1590/1775 train_time:92475ms step_avg:58.16ms
step:1591/1775 train_time:92561ms step_avg:58.18ms
step:1592/1775 train_time:92650ms step_avg:58.20ms
step:1593/1775 train_time:92736ms step_avg:58.21ms
step:1594/1775 train_time:92824ms step_avg:58.23ms
step:1595/1775 train_time:92910ms step_avg:58.25ms
step:1596/1775 train_time:93000ms step_avg:58.27ms
step:1597/1775 train_time:93087ms step_avg:58.29ms
step:1598/1775 train_time:93176ms step_avg:58.31ms
step:1599/1775 train_time:93261ms step_avg:58.32ms
step:1600/1775 train_time:93350ms step_avg:58.34ms
step:1601/1775 train_time:93438ms step_avg:58.36ms
step:1602/1775 train_time:93527ms step_avg:58.38ms
step:1603/1775 train_time:93613ms step_avg:58.40ms
step:1604/1775 train_time:93701ms step_avg:58.42ms
step:1605/1775 train_time:93788ms step_avg:58.43ms
step:1606/1775 train_time:93876ms step_avg:58.45ms
step:1607/1775 train_time:93962ms step_avg:58.47ms
step:1608/1775 train_time:94051ms step_avg:58.49ms
step:1609/1775 train_time:94138ms step_avg:58.51ms
step:1610/1775 train_time:94226ms step_avg:58.53ms
step:1611/1775 train_time:94312ms step_avg:58.54ms
step:1612/1775 train_time:94402ms step_avg:58.56ms
step:1613/1775 train_time:94488ms step_avg:58.58ms
step:1614/1775 train_time:94577ms step_avg:58.60ms
step:1615/1775 train_time:94664ms step_avg:58.62ms
step:1616/1775 train_time:94752ms step_avg:58.63ms
step:1617/1775 train_time:94838ms step_avg:58.65ms
step:1618/1775 train_time:94927ms step_avg:58.67ms
step:1619/1775 train_time:95013ms step_avg:58.69ms
step:1620/1775 train_time:95102ms step_avg:58.70ms
step:1621/1775 train_time:95187ms step_avg:58.72ms
step:1622/1775 train_time:95277ms step_avg:58.74ms
step:1623/1775 train_time:95364ms step_avg:58.76ms
step:1624/1775 train_time:95452ms step_avg:58.78ms
step:1625/1775 train_time:95539ms step_avg:58.79ms
step:1626/1775 train_time:95627ms step_avg:58.81ms
step:1627/1775 train_time:95712ms step_avg:58.83ms
step:1628/1775 train_time:95802ms step_avg:58.85ms
step:1629/1775 train_time:95888ms step_avg:58.86ms
step:1630/1775 train_time:95978ms step_avg:58.88ms
step:1631/1775 train_time:96064ms step_avg:58.90ms
step:1632/1775 train_time:96152ms step_avg:58.92ms
step:1633/1775 train_time:96239ms step_avg:58.93ms
step:1634/1775 train_time:96327ms step_avg:58.95ms
step:1635/1775 train_time:96413ms step_avg:58.97ms
step:1636/1775 train_time:96502ms step_avg:58.99ms
step:1637/1775 train_time:96588ms step_avg:59.00ms
step:1638/1775 train_time:96676ms step_avg:59.02ms
step:1639/1775 train_time:96762ms step_avg:59.04ms
step:1640/1775 train_time:96852ms step_avg:59.06ms
step:1641/1775 train_time:96938ms step_avg:59.07ms
step:1642/1775 train_time:97028ms step_avg:59.09ms
step:1643/1775 train_time:97115ms step_avg:59.11ms
step:1644/1775 train_time:97204ms step_avg:59.13ms
step:1645/1775 train_time:97289ms step_avg:59.14ms
step:1646/1775 train_time:97378ms step_avg:59.16ms
step:1647/1775 train_time:97464ms step_avg:59.18ms
step:1648/1775 train_time:97551ms step_avg:59.19ms
step:1649/1775 train_time:97639ms step_avg:59.21ms
step:1650/1775 train_time:97727ms step_avg:59.23ms
step:1651/1775 train_time:97814ms step_avg:59.25ms
step:1652/1775 train_time:97902ms step_avg:59.26ms
step:1653/1775 train_time:97989ms step_avg:59.28ms
step:1654/1775 train_time:98078ms step_avg:59.30ms
step:1655/1775 train_time:98164ms step_avg:59.31ms
step:1656/1775 train_time:98253ms step_avg:59.33ms
step:1657/1775 train_time:98340ms step_avg:59.35ms
step:1658/1775 train_time:98429ms step_avg:59.37ms
step:1659/1775 train_time:98515ms step_avg:59.38ms
step:1660/1775 train_time:98603ms step_avg:59.40ms
step:1661/1775 train_time:98689ms step_avg:59.42ms
step:1662/1775 train_time:98778ms step_avg:59.43ms
step:1663/1775 train_time:98864ms step_avg:59.45ms
step:1664/1775 train_time:98952ms step_avg:59.47ms
step:1665/1775 train_time:99039ms step_avg:59.48ms
step:1666/1775 train_time:99129ms step_avg:59.50ms
step:1667/1775 train_time:99215ms step_avg:59.52ms
step:1668/1775 train_time:99305ms step_avg:59.54ms
step:1669/1775 train_time:99391ms step_avg:59.55ms
step:1670/1775 train_time:99480ms step_avg:59.57ms
step:1671/1775 train_time:99566ms step_avg:59.58ms
step:1672/1775 train_time:99655ms step_avg:59.60ms
step:1673/1775 train_time:99739ms step_avg:59.62ms
step:1674/1775 train_time:99829ms step_avg:59.64ms
step:1675/1775 train_time:99915ms step_avg:59.65ms
step:1676/1775 train_time:100004ms step_avg:59.67ms
step:1677/1775 train_time:100092ms step_avg:59.69ms
step:1678/1775 train_time:100181ms step_avg:59.70ms
step:1679/1775 train_time:100267ms step_avg:59.72ms
step:1680/1775 train_time:100356ms step_avg:59.74ms
step:1681/1775 train_time:100442ms step_avg:59.75ms
step:1682/1775 train_time:100531ms step_avg:59.77ms
step:1683/1775 train_time:100617ms step_avg:59.78ms
step:1684/1775 train_time:100706ms step_avg:59.80ms
step:1685/1775 train_time:100792ms step_avg:59.82ms
step:1686/1775 train_time:100881ms step_avg:59.83ms
step:1687/1775 train_time:100967ms step_avg:59.85ms
step:1688/1775 train_time:101056ms step_avg:59.87ms
step:1689/1775 train_time:101141ms step_avg:59.88ms
step:1690/1775 train_time:101232ms step_avg:59.90ms
step:1691/1775 train_time:101319ms step_avg:59.92ms
step:1692/1775 train_time:101409ms step_avg:59.93ms
step:1693/1775 train_time:101494ms step_avg:59.95ms
step:1694/1775 train_time:101581ms step_avg:59.97ms
step:1695/1775 train_time:101668ms step_avg:59.98ms
step:1696/1775 train_time:101757ms step_avg:60.00ms
step:1697/1775 train_time:101843ms step_avg:60.01ms
step:1698/1775 train_time:101931ms step_avg:60.03ms
step:1699/1775 train_time:102017ms step_avg:60.05ms
step:1700/1775 train_time:102107ms step_avg:60.06ms
step:1701/1775 train_time:102194ms step_avg:60.08ms
step:1702/1775 train_time:102282ms step_avg:60.10ms
step:1703/1775 train_time:102369ms step_avg:60.11ms
step:1704/1775 train_time:102458ms step_avg:60.13ms
step:1705/1775 train_time:102544ms step_avg:60.14ms
step:1706/1775 train_time:102633ms step_avg:60.16ms
step:1707/1775 train_time:102719ms step_avg:60.18ms
step:1708/1775 train_time:102808ms step_avg:60.19ms
step:1709/1775 train_time:102894ms step_avg:60.21ms
step:1710/1775 train_time:102982ms step_avg:60.22ms
step:1711/1775 train_time:103069ms step_avg:60.24ms
step:1712/1775 train_time:103158ms step_avg:60.26ms
step:1713/1775 train_time:103243ms step_avg:60.27ms
step:1714/1775 train_time:103332ms step_avg:60.29ms
step:1715/1775 train_time:103420ms step_avg:60.30ms
step:1716/1775 train_time:103509ms step_avg:60.32ms
step:1717/1775 train_time:103595ms step_avg:60.33ms
step:1718/1775 train_time:103682ms step_avg:60.35ms
step:1719/1775 train_time:103768ms step_avg:60.37ms
step:1720/1775 train_time:103858ms step_avg:60.38ms
step:1721/1775 train_time:103943ms step_avg:60.40ms
step:1722/1775 train_time:104032ms step_avg:60.41ms
step:1723/1775 train_time:104120ms step_avg:60.43ms
step:1724/1775 train_time:104208ms step_avg:60.45ms
step:1725/1775 train_time:104295ms step_avg:60.46ms
step:1726/1775 train_time:104384ms step_avg:60.48ms
step:1727/1775 train_time:104471ms step_avg:60.49ms
step:1728/1775 train_time:104559ms step_avg:60.51ms
step:1729/1775 train_time:104645ms step_avg:60.52ms
step:1730/1775 train_time:104735ms step_avg:60.54ms
step:1731/1775 train_time:104821ms step_avg:60.55ms
step:1732/1775 train_time:104910ms step_avg:60.57ms
step:1733/1775 train_time:104997ms step_avg:60.59ms
step:1734/1775 train_time:105086ms step_avg:60.60ms
step:1735/1775 train_time:105171ms step_avg:60.62ms
step:1736/1775 train_time:105265ms step_avg:60.64ms
step:1737/1775 train_time:105352ms step_avg:60.65ms
step:1738/1775 train_time:105441ms step_avg:60.67ms
step:1739/1775 train_time:105527ms step_avg:60.68ms
step:1740/1775 train_time:105617ms step_avg:60.70ms
step:1741/1775 train_time:105704ms step_avg:60.71ms
step:1742/1775 train_time:105792ms step_avg:60.73ms
step:1743/1775 train_time:105878ms step_avg:60.74ms
step:1744/1775 train_time:105968ms step_avg:60.76ms
step:1745/1775 train_time:106055ms step_avg:60.78ms
step:1746/1775 train_time:106143ms step_avg:60.79ms
step:1747/1775 train_time:106231ms step_avg:60.81ms
step:1748/1775 train_time:106320ms step_avg:60.82ms
step:1749/1775 train_time:106406ms step_avg:60.84ms
step:1750/1775 train_time:106496ms step_avg:60.86ms
step:1750/1775 val_loss:3.2847 train_time:106594ms step_avg:60.91ms
step:1751/1775 train_time:106614ms step_avg:60.89ms
step:1752/1775 train_time:106673ms step_avg:60.89ms
step:1753/1775 train_time:106759ms step_avg:60.90ms
step:1754/1775 train_time:106847ms step_avg:60.92ms
step:1755/1775 train_time:106934ms step_avg:60.93ms
step:1756/1775 train_time:107023ms step_avg:60.95ms
step:1757/1775 train_time:107109ms step_avg:60.96ms
step:1758/1775 train_time:107197ms step_avg:60.98ms
step:1759/1775 train_time:107282ms step_avg:60.99ms
step:1760/1775 train_time:107373ms step_avg:61.01ms
step:1761/1775 train_time:107460ms step_avg:61.02ms
step:1762/1775 train_time:107550ms step_avg:61.04ms
step:1763/1775 train_time:107637ms step_avg:61.05ms
step:1764/1775 train_time:107728ms step_avg:61.07ms
step:1765/1775 train_time:107815ms step_avg:61.09ms
step:1766/1775 train_time:107907ms step_avg:61.10ms
step:1767/1775 train_time:107994ms step_avg:61.12ms
step:1768/1775 train_time:108082ms step_avg:61.13ms
step:1769/1775 train_time:108168ms step_avg:61.15ms
step:1770/1775 train_time:108257ms step_avg:61.16ms
step:1771/1775 train_time:108343ms step_avg:61.18ms
step:1772/1775 train_time:108433ms step_avg:61.19ms
step:1773/1775 train_time:108519ms step_avg:61.21ms
step:1774/1775 train_time:108607ms step_avg:61.22ms
step:1775/1775 train_time:108694ms step_avg:61.24ms
step:1775/1775 val_loss:3.2782 train_time:108795ms step_avg:61.29ms
peak memory allocated: 29148 MiB reserved: 44578 MiB

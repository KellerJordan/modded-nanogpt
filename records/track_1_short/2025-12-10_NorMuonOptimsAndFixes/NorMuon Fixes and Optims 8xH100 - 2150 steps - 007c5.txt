import uuid
run_id = f"NorMuon Fixes and Optims 8xH100 - 2150 steps - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2110  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 10 2025, 18:15:56) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 19:15:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   44C    P0            124W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   35C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   44C    P0            130W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   44C    P0            128W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   43C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           30797      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           30798      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           30799      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           30800      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           30801      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           30802      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           30803      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           30804      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           30798      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           30799      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           30800      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           30801      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           30802      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           30803      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           30804      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2150 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2150 train_time:83ms step_avg:82.57ms
step:2/2150 train_time:157ms step_avg:78.69ms
step:3/2150 train_time:179ms step_avg:59.58ms
step:4/2150 train_time:200ms step_avg:50.12ms
step:5/2150 train_time:221ms step_avg:44.26ms
step:6/2150 train_time:316ms step_avg:52.59ms
step:7/2150 train_time:347ms step_avg:49.59ms
step:8/2150 train_time:381ms step_avg:47.57ms
step:9/2150 train_time:413ms step_avg:45.86ms
step:10/2150 train_time:446ms step_avg:44.64ms
step:11/2150 train_time:479ms step_avg:43.52ms
step:12/2150 train_time:512ms step_avg:42.70ms
step:13/2150 train_time:545ms step_avg:41.93ms
step:14/2150 train_time:579ms step_avg:41.35ms
step:15/2150 train_time:611ms step_avg:40.77ms
step:16/2150 train_time:645ms step_avg:40.33ms
step:17/2150 train_time:678ms step_avg:39.86ms
step:18/2150 train_time:711ms step_avg:39.52ms
step:19/2150 train_time:744ms step_avg:39.17ms
step:20/2150 train_time:778ms step_avg:38.90ms
step:21/2150 train_time:811ms step_avg:38.61ms
step:22/2150 train_time:845ms step_avg:38.39ms
step:23/2150 train_time:877ms step_avg:38.13ms
step:24/2150 train_time:911ms step_avg:37.95ms
step:25/2150 train_time:944ms step_avg:37.74ms
step:26/2150 train_time:977ms step_avg:37.58ms
step:27/2150 train_time:1010ms step_avg:37.41ms
step:28/2150 train_time:1044ms step_avg:37.27ms
step:29/2150 train_time:1076ms step_avg:37.11ms
step:30/2150 train_time:1110ms step_avg:37.01ms
step:31/2150 train_time:1143ms step_avg:36.86ms
step:32/2150 train_time:1177ms step_avg:36.77ms
step:33/2150 train_time:1210ms step_avg:36.66ms
step:34/2150 train_time:1244ms step_avg:36.58ms
step:35/2150 train_time:1278ms step_avg:36.51ms
step:36/2150 train_time:1312ms step_avg:36.44ms
step:37/2150 train_time:1346ms step_avg:36.37ms
step:38/2150 train_time:1380ms step_avg:36.31ms
step:39/2150 train_time:1413ms step_avg:36.24ms
step:40/2150 train_time:1447ms step_avg:36.17ms
step:41/2150 train_time:1480ms step_avg:36.09ms
step:42/2150 train_time:1514ms step_avg:36.04ms
step:43/2150 train_time:1547ms step_avg:35.97ms
step:44/2150 train_time:1580ms step_avg:35.92ms
step:45/2150 train_time:1613ms step_avg:35.85ms
step:46/2150 train_time:1647ms step_avg:35.80ms
step:47/2150 train_time:1680ms step_avg:35.74ms
step:48/2150 train_time:1713ms step_avg:35.70ms
step:49/2150 train_time:1746ms step_avg:35.63ms
step:50/2150 train_time:1780ms step_avg:35.59ms
step:51/2150 train_time:1813ms step_avg:35.54ms
step:52/2150 train_time:1846ms step_avg:35.50ms
step:53/2150 train_time:1879ms step_avg:35.45ms
step:54/2150 train_time:1913ms step_avg:35.42ms
step:55/2150 train_time:1946ms step_avg:35.38ms
step:56/2150 train_time:1979ms step_avg:35.34ms
step:57/2150 train_time:2012ms step_avg:35.30ms
step:58/2150 train_time:2046ms step_avg:35.27ms
step:59/2150 train_time:2078ms step_avg:35.23ms
step:60/2150 train_time:2112ms step_avg:35.20ms
step:61/2150 train_time:2145ms step_avg:35.16ms
step:62/2150 train_time:2179ms step_avg:35.14ms
step:63/2150 train_time:2212ms step_avg:35.11ms
step:64/2150 train_time:2246ms step_avg:35.09ms
step:65/2150 train_time:2279ms step_avg:35.06ms
step:66/2150 train_time:2312ms step_avg:35.03ms
step:67/2150 train_time:2345ms step_avg:35.00ms
step:68/2150 train_time:2379ms step_avg:34.98ms
step:69/2150 train_time:2412ms step_avg:34.96ms
step:70/2150 train_time:2446ms step_avg:34.94ms
step:71/2150 train_time:2479ms step_avg:34.91ms
step:72/2150 train_time:2512ms step_avg:34.89ms
step:73/2150 train_time:2545ms step_avg:34.87ms
step:74/2150 train_time:2579ms step_avg:34.85ms
step:75/2150 train_time:2612ms step_avg:34.82ms
step:76/2150 train_time:2645ms step_avg:34.80ms
step:77/2150 train_time:2678ms step_avg:34.78ms
step:78/2150 train_time:2712ms step_avg:34.77ms
step:79/2150 train_time:2745ms step_avg:34.74ms
step:80/2150 train_time:2778ms step_avg:34.73ms
step:81/2150 train_time:2811ms step_avg:34.70ms
step:82/2150 train_time:2845ms step_avg:34.69ms
step:83/2150 train_time:2877ms step_avg:34.66ms
step:84/2150 train_time:2911ms step_avg:34.65ms
step:85/2150 train_time:2944ms step_avg:34.63ms
step:86/2150 train_time:2977ms step_avg:34.62ms
step:87/2150 train_time:3009ms step_avg:34.59ms
step:88/2150 train_time:3043ms step_avg:34.58ms
step:89/2150 train_time:3076ms step_avg:34.56ms
step:90/2150 train_time:3109ms step_avg:34.55ms
step:91/2150 train_time:3142ms step_avg:34.53ms
step:92/2150 train_time:3176ms step_avg:34.52ms
step:93/2150 train_time:3208ms step_avg:34.50ms
step:94/2150 train_time:3242ms step_avg:34.49ms
step:95/2150 train_time:3275ms step_avg:34.47ms
step:96/2150 train_time:3309ms step_avg:34.46ms
step:97/2150 train_time:3341ms step_avg:34.45ms
step:98/2150 train_time:3375ms step_avg:34.44ms
step:99/2150 train_time:3408ms step_avg:34.43ms
step:100/2150 train_time:3442ms step_avg:34.42ms
step:101/2150 train_time:3475ms step_avg:34.40ms
step:102/2150 train_time:3509ms step_avg:34.40ms
step:103/2150 train_time:3541ms step_avg:34.38ms
step:104/2150 train_time:3575ms step_avg:34.37ms
step:105/2150 train_time:3608ms step_avg:34.36ms
step:106/2150 train_time:3641ms step_avg:34.35ms
step:107/2150 train_time:3674ms step_avg:34.34ms
step:108/2150 train_time:3708ms step_avg:34.33ms
step:109/2150 train_time:3741ms step_avg:34.32ms
step:110/2150 train_time:3774ms step_avg:34.31ms
step:111/2150 train_time:3807ms step_avg:34.30ms
step:112/2150 train_time:3841ms step_avg:34.29ms
step:113/2150 train_time:3873ms step_avg:34.28ms
step:114/2150 train_time:3907ms step_avg:34.27ms
step:115/2150 train_time:3940ms step_avg:34.26ms
step:116/2150 train_time:3973ms step_avg:34.25ms
step:117/2150 train_time:4006ms step_avg:34.24ms
step:118/2150 train_time:4039ms step_avg:34.23ms
step:119/2150 train_time:4072ms step_avg:34.22ms
step:120/2150 train_time:4106ms step_avg:34.22ms
step:121/2150 train_time:4138ms step_avg:34.20ms
step:122/2150 train_time:4172ms step_avg:34.20ms
step:123/2150 train_time:4205ms step_avg:34.18ms
step:124/2150 train_time:4238ms step_avg:34.18ms
step:125/2150 train_time:4271ms step_avg:34.17ms
step:126/2150 train_time:4304ms step_avg:34.16ms
step:127/2150 train_time:4337ms step_avg:34.15ms
step:128/2150 train_time:4370ms step_avg:34.14ms
step:129/2150 train_time:4403ms step_avg:34.13ms
step:130/2150 train_time:4437ms step_avg:34.13ms
step:131/2150 train_time:4469ms step_avg:34.12ms
step:132/2150 train_time:4503ms step_avg:34.11ms
step:133/2150 train_time:4536ms step_avg:34.10ms
step:134/2150 train_time:4569ms step_avg:34.10ms
step:135/2150 train_time:4602ms step_avg:34.09ms
step:136/2150 train_time:4635ms step_avg:34.08ms
step:137/2150 train_time:4668ms step_avg:34.08ms
step:138/2150 train_time:4702ms step_avg:34.07ms
step:139/2150 train_time:4735ms step_avg:34.06ms
step:140/2150 train_time:4768ms step_avg:34.06ms
step:141/2150 train_time:4801ms step_avg:34.05ms
step:142/2150 train_time:4834ms step_avg:34.05ms
step:143/2150 train_time:4867ms step_avg:34.03ms
step:144/2150 train_time:4900ms step_avg:34.03ms
step:145/2150 train_time:4933ms step_avg:34.02ms
step:146/2150 train_time:4967ms step_avg:34.02ms
step:147/2150 train_time:4999ms step_avg:34.01ms
step:148/2150 train_time:5033ms step_avg:34.01ms
step:149/2150 train_time:5065ms step_avg:34.00ms
step:150/2150 train_time:5099ms step_avg:33.99ms
step:151/2150 train_time:5132ms step_avg:33.99ms
step:152/2150 train_time:5165ms step_avg:33.98ms
step:153/2150 train_time:5199ms step_avg:33.98ms
step:154/2150 train_time:5232ms step_avg:33.97ms
step:155/2150 train_time:5265ms step_avg:33.97ms
step:156/2150 train_time:5299ms step_avg:33.97ms
step:157/2150 train_time:5331ms step_avg:33.96ms
step:158/2150 train_time:5365ms step_avg:33.95ms
step:159/2150 train_time:5398ms step_avg:33.95ms
step:160/2150 train_time:5431ms step_avg:33.94ms
step:161/2150 train_time:5464ms step_avg:33.94ms
step:162/2150 train_time:5498ms step_avg:33.94ms
step:163/2150 train_time:5531ms step_avg:33.93ms
step:164/2150 train_time:5564ms step_avg:33.93ms
step:165/2150 train_time:5597ms step_avg:33.92ms
step:166/2150 train_time:5631ms step_avg:33.92ms
step:167/2150 train_time:5663ms step_avg:33.91ms
step:168/2150 train_time:5696ms step_avg:33.91ms
step:169/2150 train_time:5729ms step_avg:33.90ms
step:170/2150 train_time:5762ms step_avg:33.90ms
step:171/2150 train_time:5795ms step_avg:33.89ms
step:172/2150 train_time:5828ms step_avg:33.88ms
step:173/2150 train_time:5861ms step_avg:33.88ms
step:174/2150 train_time:5894ms step_avg:33.87ms
step:175/2150 train_time:5927ms step_avg:33.87ms
step:176/2150 train_time:5960ms step_avg:33.86ms
step:177/2150 train_time:5993ms step_avg:33.86ms
step:178/2150 train_time:6027ms step_avg:33.86ms
step:179/2150 train_time:6059ms step_avg:33.85ms
step:180/2150 train_time:6093ms step_avg:33.85ms
step:181/2150 train_time:6125ms step_avg:33.84ms
step:182/2150 train_time:6158ms step_avg:33.84ms
step:183/2150 train_time:6192ms step_avg:33.83ms
step:184/2150 train_time:6225ms step_avg:33.83ms
step:185/2150 train_time:6258ms step_avg:33.82ms
step:186/2150 train_time:6291ms step_avg:33.82ms
step:187/2150 train_time:6324ms step_avg:33.82ms
step:188/2150 train_time:6357ms step_avg:33.81ms
step:189/2150 train_time:6390ms step_avg:33.81ms
step:190/2150 train_time:6424ms step_avg:33.81ms
step:191/2150 train_time:6457ms step_avg:33.81ms
step:192/2150 train_time:6490ms step_avg:33.80ms
step:193/2150 train_time:6523ms step_avg:33.80ms
step:194/2150 train_time:6557ms step_avg:33.80ms
step:195/2150 train_time:6590ms step_avg:33.79ms
step:196/2150 train_time:6623ms step_avg:33.79ms
step:197/2150 train_time:6656ms step_avg:33.79ms
step:198/2150 train_time:6690ms step_avg:33.79ms
step:199/2150 train_time:6723ms step_avg:33.78ms
step:200/2150 train_time:6756ms step_avg:33.78ms
step:201/2150 train_time:6789ms step_avg:33.78ms
step:202/2150 train_time:6822ms step_avg:33.77ms
step:203/2150 train_time:6855ms step_avg:33.77ms
step:204/2150 train_time:6888ms step_avg:33.77ms
step:205/2150 train_time:6921ms step_avg:33.76ms
step:206/2150 train_time:6954ms step_avg:33.76ms
step:207/2150 train_time:6987ms step_avg:33.76ms
step:208/2150 train_time:7021ms step_avg:33.75ms
step:209/2150 train_time:7053ms step_avg:33.75ms
step:210/2150 train_time:7087ms step_avg:33.75ms
step:211/2150 train_time:7119ms step_avg:33.74ms
step:212/2150 train_time:7153ms step_avg:33.74ms
step:213/2150 train_time:7186ms step_avg:33.74ms
step:214/2150 train_time:7219ms step_avg:33.74ms
step:215/2150 train_time:7252ms step_avg:33.73ms
step:216/2150 train_time:7288ms step_avg:33.74ms
step:217/2150 train_time:7318ms step_avg:33.73ms
step:218/2150 train_time:7352ms step_avg:33.72ms
step:219/2150 train_time:7385ms step_avg:33.72ms
step:220/2150 train_time:7419ms step_avg:33.72ms
step:221/2150 train_time:7452ms step_avg:33.72ms
step:222/2150 train_time:7485ms step_avg:33.72ms
step:223/2150 train_time:7518ms step_avg:33.71ms
step:224/2150 train_time:7551ms step_avg:33.71ms
step:225/2150 train_time:7584ms step_avg:33.71ms
step:226/2150 train_time:7617ms step_avg:33.70ms
step:227/2150 train_time:7650ms step_avg:33.70ms
step:228/2150 train_time:7683ms step_avg:33.70ms
step:229/2150 train_time:7716ms step_avg:33.69ms
step:230/2150 train_time:7749ms step_avg:33.69ms
step:231/2150 train_time:7782ms step_avg:33.69ms
step:232/2150 train_time:7816ms step_avg:33.69ms
step:233/2150 train_time:7848ms step_avg:33.68ms
step:234/2150 train_time:7881ms step_avg:33.68ms
step:235/2150 train_time:7914ms step_avg:33.67ms
step:236/2150 train_time:7947ms step_avg:33.67ms
step:237/2150 train_time:7979ms step_avg:33.67ms
step:238/2150 train_time:8013ms step_avg:33.67ms
step:239/2150 train_time:8046ms step_avg:33.66ms
step:240/2150 train_time:8079ms step_avg:33.66ms
step:241/2150 train_time:8112ms step_avg:33.66ms
step:242/2150 train_time:8145ms step_avg:33.66ms
step:243/2150 train_time:8178ms step_avg:33.65ms
step:244/2150 train_time:8211ms step_avg:33.65ms
step:245/2150 train_time:8243ms step_avg:33.65ms
step:246/2150 train_time:8277ms step_avg:33.65ms
step:247/2150 train_time:8310ms step_avg:33.64ms
step:248/2150 train_time:8343ms step_avg:33.64ms
step:249/2150 train_time:8376ms step_avg:33.64ms
step:250/2150 train_time:8410ms step_avg:33.64ms
step:250/2150 val_loss:4.3140 train_time:8446ms step_avg:33.78ms
step:251/2150 train_time:8466ms step_avg:33.73ms
step:252/2150 train_time:8487ms step_avg:33.68ms
step:253/2150 train_time:8512ms step_avg:33.65ms
step:254/2150 train_time:8548ms step_avg:33.65ms
step:255/2150 train_time:8584ms step_avg:33.66ms
step:256/2150 train_time:8619ms step_avg:33.67ms
step:257/2150 train_time:8654ms step_avg:33.67ms
step:258/2150 train_time:8687ms step_avg:33.67ms
step:259/2150 train_time:8720ms step_avg:33.67ms
step:260/2150 train_time:8753ms step_avg:33.67ms
step:261/2150 train_time:8786ms step_avg:33.66ms
step:262/2150 train_time:8819ms step_avg:33.66ms
step:263/2150 train_time:8851ms step_avg:33.65ms
step:264/2150 train_time:8885ms step_avg:33.65ms
step:265/2150 train_time:8917ms step_avg:33.65ms
step:266/2150 train_time:8950ms step_avg:33.65ms
step:267/2150 train_time:8983ms step_avg:33.64ms
step:268/2150 train_time:9016ms step_avg:33.64ms
step:269/2150 train_time:9048ms step_avg:33.64ms
step:270/2150 train_time:9082ms step_avg:33.64ms
step:271/2150 train_time:9114ms step_avg:33.63ms
step:272/2150 train_time:9148ms step_avg:33.63ms
step:273/2150 train_time:9180ms step_avg:33.63ms
step:274/2150 train_time:9214ms step_avg:33.63ms
step:275/2150 train_time:9246ms step_avg:33.62ms
step:276/2150 train_time:9280ms step_avg:33.62ms
step:277/2150 train_time:9312ms step_avg:33.62ms
step:278/2150 train_time:9345ms step_avg:33.62ms
step:279/2150 train_time:9378ms step_avg:33.61ms
step:280/2150 train_time:9411ms step_avg:33.61ms
step:281/2150 train_time:9444ms step_avg:33.61ms
step:282/2150 train_time:9478ms step_avg:33.61ms
step:283/2150 train_time:9511ms step_avg:33.61ms
step:284/2150 train_time:9545ms step_avg:33.61ms
step:285/2150 train_time:9579ms step_avg:33.61ms
step:286/2150 train_time:9613ms step_avg:33.61ms
step:287/2150 train_time:9647ms step_avg:33.61ms
step:288/2150 train_time:9680ms step_avg:33.61ms
step:289/2150 train_time:9714ms step_avg:33.61ms
step:290/2150 train_time:9747ms step_avg:33.61ms
step:291/2150 train_time:9780ms step_avg:33.61ms
step:292/2150 train_time:9813ms step_avg:33.61ms
step:293/2150 train_time:9846ms step_avg:33.60ms
step:294/2150 train_time:9879ms step_avg:33.60ms
step:295/2150 train_time:9912ms step_avg:33.60ms
step:296/2150 train_time:9945ms step_avg:33.60ms
step:297/2150 train_time:9978ms step_avg:33.60ms
step:298/2150 train_time:10011ms step_avg:33.60ms
step:299/2150 train_time:10044ms step_avg:33.59ms
step:300/2150 train_time:10077ms step_avg:33.59ms
step:301/2150 train_time:10110ms step_avg:33.59ms
step:302/2150 train_time:10143ms step_avg:33.59ms
step:303/2150 train_time:10176ms step_avg:33.58ms
step:304/2150 train_time:10209ms step_avg:33.58ms
step:305/2150 train_time:10242ms step_avg:33.58ms
step:306/2150 train_time:10275ms step_avg:33.58ms
step:307/2150 train_time:10307ms step_avg:33.57ms
step:308/2150 train_time:10340ms step_avg:33.57ms
step:309/2150 train_time:10373ms step_avg:33.57ms
step:310/2150 train_time:10407ms step_avg:33.57ms
step:311/2150 train_time:10439ms step_avg:33.57ms
step:312/2150 train_time:10472ms step_avg:33.57ms
step:313/2150 train_time:10506ms step_avg:33.56ms
step:314/2150 train_time:10539ms step_avg:33.57ms
step:315/2150 train_time:10573ms step_avg:33.56ms
step:316/2150 train_time:10606ms step_avg:33.56ms
step:317/2150 train_time:10640ms step_avg:33.56ms
step:318/2150 train_time:10673ms step_avg:33.56ms
step:319/2150 train_time:10706ms step_avg:33.56ms
step:320/2150 train_time:10739ms step_avg:33.56ms
step:321/2150 train_time:10772ms step_avg:33.56ms
step:322/2150 train_time:10805ms step_avg:33.56ms
step:323/2150 train_time:10838ms step_avg:33.55ms
step:324/2150 train_time:10871ms step_avg:33.55ms
step:325/2150 train_time:10904ms step_avg:33.55ms
step:326/2150 train_time:10937ms step_avg:33.55ms
step:327/2150 train_time:10970ms step_avg:33.55ms
step:328/2150 train_time:11003ms step_avg:33.55ms
step:329/2150 train_time:11035ms step_avg:33.54ms
step:330/2150 train_time:11069ms step_avg:33.54ms
step:331/2150 train_time:11101ms step_avg:33.54ms
step:332/2150 train_time:11135ms step_avg:33.54ms
step:333/2150 train_time:11167ms step_avg:33.54ms
step:334/2150 train_time:11201ms step_avg:33.54ms
step:335/2150 train_time:11234ms step_avg:33.53ms
step:336/2150 train_time:11267ms step_avg:33.53ms
step:337/2150 train_time:11300ms step_avg:33.53ms
step:338/2150 train_time:11333ms step_avg:33.53ms
step:339/2150 train_time:11366ms step_avg:33.53ms
step:340/2150 train_time:11399ms step_avg:33.53ms
step:341/2150 train_time:11432ms step_avg:33.53ms
step:342/2150 train_time:11466ms step_avg:33.53ms
step:343/2150 train_time:11499ms step_avg:33.52ms
step:344/2150 train_time:11532ms step_avg:33.52ms
step:345/2150 train_time:11565ms step_avg:33.52ms
step:346/2150 train_time:11598ms step_avg:33.52ms
step:347/2150 train_time:11632ms step_avg:33.52ms
step:348/2150 train_time:11665ms step_avg:33.52ms
step:349/2150 train_time:11698ms step_avg:33.52ms
step:350/2150 train_time:11731ms step_avg:33.52ms
step:351/2150 train_time:11764ms step_avg:33.52ms
step:352/2150 train_time:11797ms step_avg:33.52ms
step:353/2150 train_time:11831ms step_avg:33.51ms
step:354/2150 train_time:11864ms step_avg:33.52ms
step:355/2150 train_time:11897ms step_avg:33.51ms
step:356/2150 train_time:11930ms step_avg:33.51ms
step:357/2150 train_time:11963ms step_avg:33.51ms
step:358/2150 train_time:11996ms step_avg:33.51ms
step:359/2150 train_time:12029ms step_avg:33.51ms
step:360/2150 train_time:12062ms step_avg:33.51ms
step:361/2150 train_time:12095ms step_avg:33.50ms
step:362/2150 train_time:12128ms step_avg:33.50ms
step:363/2150 train_time:12161ms step_avg:33.50ms
step:364/2150 train_time:12194ms step_avg:33.50ms
step:365/2150 train_time:12227ms step_avg:33.50ms
step:366/2150 train_time:12260ms step_avg:33.50ms
step:367/2150 train_time:12293ms step_avg:33.50ms
step:368/2150 train_time:12326ms step_avg:33.49ms
step:369/2150 train_time:12359ms step_avg:33.49ms
step:370/2150 train_time:12392ms step_avg:33.49ms
step:371/2150 train_time:12425ms step_avg:33.49ms
step:372/2150 train_time:12458ms step_avg:33.49ms
step:373/2150 train_time:12491ms step_avg:33.49ms
step:374/2150 train_time:12524ms step_avg:33.49ms
step:375/2150 train_time:12558ms step_avg:33.49ms
step:376/2150 train_time:12591ms step_avg:33.49ms
step:377/2150 train_time:12624ms step_avg:33.49ms
step:378/2150 train_time:12658ms step_avg:33.49ms
step:379/2150 train_time:12691ms step_avg:33.48ms
step:380/2150 train_time:12724ms step_avg:33.48ms
step:381/2150 train_time:12757ms step_avg:33.48ms
step:382/2150 train_time:12790ms step_avg:33.48ms
step:383/2150 train_time:12824ms step_avg:33.48ms
step:384/2150 train_time:12857ms step_avg:33.48ms
step:385/2150 train_time:12890ms step_avg:33.48ms
step:386/2150 train_time:12923ms step_avg:33.48ms
step:387/2150 train_time:12956ms step_avg:33.48ms
step:388/2150 train_time:12989ms step_avg:33.48ms
step:389/2150 train_time:13022ms step_avg:33.48ms
step:390/2150 train_time:13055ms step_avg:33.48ms
step:391/2150 train_time:13088ms step_avg:33.47ms
step:392/2150 train_time:13121ms step_avg:33.47ms
step:393/2150 train_time:13154ms step_avg:33.47ms
step:394/2150 train_time:13188ms step_avg:33.47ms
step:395/2150 train_time:13221ms step_avg:33.47ms
step:396/2150 train_time:13254ms step_avg:33.47ms
step:397/2150 train_time:13286ms step_avg:33.47ms
step:398/2150 train_time:13320ms step_avg:33.47ms
step:399/2150 train_time:13353ms step_avg:33.47ms
step:400/2150 train_time:13386ms step_avg:33.47ms
step:401/2150 train_time:13419ms step_avg:33.46ms
step:402/2150 train_time:13453ms step_avg:33.46ms
step:403/2150 train_time:13485ms step_avg:33.46ms
step:404/2150 train_time:13518ms step_avg:33.46ms
step:405/2150 train_time:13550ms step_avg:33.46ms
step:406/2150 train_time:13584ms step_avg:33.46ms
step:407/2150 train_time:13616ms step_avg:33.46ms
step:408/2150 train_time:13650ms step_avg:33.46ms
step:409/2150 train_time:13683ms step_avg:33.45ms
step:410/2150 train_time:13716ms step_avg:33.45ms
step:411/2150 train_time:13749ms step_avg:33.45ms
step:412/2150 train_time:13783ms step_avg:33.45ms
step:413/2150 train_time:13815ms step_avg:33.45ms
step:414/2150 train_time:13848ms step_avg:33.45ms
step:415/2150 train_time:13881ms step_avg:33.45ms
step:416/2150 train_time:13915ms step_avg:33.45ms
step:417/2150 train_time:13947ms step_avg:33.45ms
step:418/2150 train_time:13981ms step_avg:33.45ms
step:419/2150 train_time:14013ms step_avg:33.44ms
step:420/2150 train_time:14047ms step_avg:33.44ms
step:421/2150 train_time:14080ms step_avg:33.44ms
step:422/2150 train_time:14113ms step_avg:33.44ms
step:423/2150 train_time:14146ms step_avg:33.44ms
step:424/2150 train_time:14179ms step_avg:33.44ms
step:425/2150 train_time:14212ms step_avg:33.44ms
step:426/2150 train_time:14245ms step_avg:33.44ms
step:427/2150 train_time:14278ms step_avg:33.44ms
step:428/2150 train_time:14311ms step_avg:33.44ms
step:429/2150 train_time:14344ms step_avg:33.44ms
step:430/2150 train_time:14377ms step_avg:33.44ms
step:431/2150 train_time:14410ms step_avg:33.43ms
step:432/2150 train_time:14443ms step_avg:33.43ms
step:433/2150 train_time:14476ms step_avg:33.43ms
step:434/2150 train_time:14510ms step_avg:33.43ms
step:435/2150 train_time:14542ms step_avg:33.43ms
step:436/2150 train_time:14576ms step_avg:33.43ms
step:437/2150 train_time:14608ms step_avg:33.43ms
step:438/2150 train_time:14642ms step_avg:33.43ms
step:439/2150 train_time:14675ms step_avg:33.43ms
step:440/2150 train_time:14708ms step_avg:33.43ms
step:441/2150 train_time:14740ms step_avg:33.43ms
step:442/2150 train_time:14774ms step_avg:33.43ms
step:443/2150 train_time:14807ms step_avg:33.42ms
step:444/2150 train_time:14840ms step_avg:33.42ms
step:445/2150 train_time:14872ms step_avg:33.42ms
step:446/2150 train_time:14906ms step_avg:33.42ms
step:447/2150 train_time:14939ms step_avg:33.42ms
step:448/2150 train_time:14972ms step_avg:33.42ms
step:449/2150 train_time:15005ms step_avg:33.42ms
step:450/2150 train_time:15038ms step_avg:33.42ms
step:451/2150 train_time:15071ms step_avg:33.42ms
step:452/2150 train_time:15104ms step_avg:33.42ms
step:453/2150 train_time:15137ms step_avg:33.41ms
step:454/2150 train_time:15170ms step_avg:33.41ms
step:455/2150 train_time:15203ms step_avg:33.41ms
step:456/2150 train_time:15237ms step_avg:33.41ms
step:457/2150 train_time:15269ms step_avg:33.41ms
step:458/2150 train_time:15302ms step_avg:33.41ms
step:459/2150 train_time:15335ms step_avg:33.41ms
step:460/2150 train_time:15368ms step_avg:33.41ms
step:461/2150 train_time:15401ms step_avg:33.41ms
step:462/2150 train_time:15435ms step_avg:33.41ms
step:463/2150 train_time:15467ms step_avg:33.41ms
step:464/2150 train_time:15501ms step_avg:33.41ms
step:465/2150 train_time:15534ms step_avg:33.41ms
step:466/2150 train_time:15567ms step_avg:33.41ms
step:467/2150 train_time:15600ms step_avg:33.40ms
step:468/2150 train_time:15633ms step_avg:33.40ms
step:469/2150 train_time:15666ms step_avg:33.40ms
step:470/2150 train_time:15699ms step_avg:33.40ms
step:471/2150 train_time:15732ms step_avg:33.40ms
step:472/2150 train_time:15765ms step_avg:33.40ms
step:473/2150 train_time:15798ms step_avg:33.40ms
step:474/2150 train_time:15831ms step_avg:33.40ms
step:475/2150 train_time:15864ms step_avg:33.40ms
step:476/2150 train_time:15897ms step_avg:33.40ms
step:477/2150 train_time:15930ms step_avg:33.40ms
step:478/2150 train_time:15963ms step_avg:33.40ms
step:479/2150 train_time:15996ms step_avg:33.40ms
step:480/2150 train_time:16030ms step_avg:33.40ms
step:481/2150 train_time:16062ms step_avg:33.39ms
step:482/2150 train_time:16095ms step_avg:33.39ms
step:483/2150 train_time:16128ms step_avg:33.39ms
step:484/2150 train_time:16162ms step_avg:33.39ms
step:485/2150 train_time:16194ms step_avg:33.39ms
step:486/2150 train_time:16228ms step_avg:33.39ms
step:487/2150 train_time:16261ms step_avg:33.39ms
step:488/2150 train_time:16294ms step_avg:33.39ms
step:489/2150 train_time:16327ms step_avg:33.39ms
step:490/2150 train_time:16360ms step_avg:33.39ms
step:491/2150 train_time:16393ms step_avg:33.39ms
step:492/2150 train_time:16426ms step_avg:33.39ms
step:493/2150 train_time:16459ms step_avg:33.39ms
step:494/2150 train_time:16492ms step_avg:33.39ms
step:495/2150 train_time:16525ms step_avg:33.38ms
step:496/2150 train_time:16559ms step_avg:33.38ms
step:497/2150 train_time:16591ms step_avg:33.38ms
step:498/2150 train_time:16624ms step_avg:33.38ms
step:499/2150 train_time:16657ms step_avg:33.38ms
step:500/2150 train_time:16690ms step_avg:33.38ms
step:500/2150 val_loss:4.0190 train_time:16727ms step_avg:33.45ms
step:501/2150 train_time:16748ms step_avg:33.43ms
step:502/2150 train_time:16768ms step_avg:33.40ms
step:503/2150 train_time:16795ms step_avg:33.39ms
step:504/2150 train_time:16829ms step_avg:33.39ms
step:505/2150 train_time:16864ms step_avg:33.39ms
step:506/2150 train_time:16899ms step_avg:33.40ms
step:507/2150 train_time:16932ms step_avg:33.40ms
step:508/2150 train_time:16966ms step_avg:33.40ms
step:509/2150 train_time:16999ms step_avg:33.40ms
step:510/2150 train_time:17033ms step_avg:33.40ms
step:511/2150 train_time:17066ms step_avg:33.40ms
step:512/2150 train_time:17099ms step_avg:33.40ms
step:513/2150 train_time:17132ms step_avg:33.40ms
step:514/2150 train_time:17165ms step_avg:33.40ms
step:515/2150 train_time:17197ms step_avg:33.39ms
step:516/2150 train_time:17231ms step_avg:33.39ms
step:517/2150 train_time:17263ms step_avg:33.39ms
step:518/2150 train_time:17296ms step_avg:33.39ms
step:519/2150 train_time:17329ms step_avg:33.39ms
step:520/2150 train_time:17362ms step_avg:33.39ms
step:521/2150 train_time:17395ms step_avg:33.39ms
step:522/2150 train_time:17428ms step_avg:33.39ms
step:523/2150 train_time:17460ms step_avg:33.38ms
step:524/2150 train_time:17493ms step_avg:33.38ms
step:525/2150 train_time:17526ms step_avg:33.38ms
step:526/2150 train_time:17559ms step_avg:33.38ms
step:527/2150 train_time:17592ms step_avg:33.38ms
step:528/2150 train_time:17625ms step_avg:33.38ms
step:529/2150 train_time:17658ms step_avg:33.38ms
step:530/2150 train_time:17691ms step_avg:33.38ms
step:531/2150 train_time:17724ms step_avg:33.38ms
step:532/2150 train_time:17757ms step_avg:33.38ms
step:533/2150 train_time:17791ms step_avg:33.38ms
step:534/2150 train_time:17825ms step_avg:33.38ms
step:535/2150 train_time:17858ms step_avg:33.38ms
step:536/2150 train_time:17891ms step_avg:33.38ms
step:537/2150 train_time:17925ms step_avg:33.38ms
step:538/2150 train_time:17958ms step_avg:33.38ms
step:539/2150 train_time:17991ms step_avg:33.38ms
step:540/2150 train_time:18024ms step_avg:33.38ms
step:541/2150 train_time:18057ms step_avg:33.38ms
step:542/2150 train_time:18090ms step_avg:33.38ms
step:543/2150 train_time:18123ms step_avg:33.38ms
step:544/2150 train_time:18157ms step_avg:33.38ms
step:545/2150 train_time:18189ms step_avg:33.37ms
step:546/2150 train_time:18222ms step_avg:33.37ms
step:547/2150 train_time:18255ms step_avg:33.37ms
step:548/2150 train_time:18288ms step_avg:33.37ms
step:549/2150 train_time:18322ms step_avg:33.37ms
step:550/2150 train_time:18355ms step_avg:33.37ms
step:551/2150 train_time:18388ms step_avg:33.37ms
step:552/2150 train_time:18422ms step_avg:33.37ms
step:553/2150 train_time:18454ms step_avg:33.37ms
step:554/2150 train_time:18487ms step_avg:33.37ms
step:555/2150 train_time:18520ms step_avg:33.37ms
step:556/2150 train_time:18554ms step_avg:33.37ms
step:557/2150 train_time:18586ms step_avg:33.37ms
step:558/2150 train_time:18620ms step_avg:33.37ms
step:559/2150 train_time:18652ms step_avg:33.37ms
step:560/2150 train_time:18685ms step_avg:33.37ms
step:561/2150 train_time:18718ms step_avg:33.37ms
step:562/2150 train_time:18751ms step_avg:33.37ms
step:563/2150 train_time:18784ms step_avg:33.36ms
step:564/2150 train_time:18818ms step_avg:33.36ms
step:565/2150 train_time:18851ms step_avg:33.37ms
step:566/2150 train_time:18885ms step_avg:33.37ms
step:567/2150 train_time:18919ms step_avg:33.37ms
step:568/2150 train_time:18952ms step_avg:33.37ms
step:569/2150 train_time:18985ms step_avg:33.37ms
step:570/2150 train_time:19018ms step_avg:33.37ms
step:571/2150 train_time:19051ms step_avg:33.36ms
step:572/2150 train_time:19085ms step_avg:33.36ms
step:573/2150 train_time:19117ms step_avg:33.36ms
step:574/2150 train_time:19151ms step_avg:33.36ms
step:575/2150 train_time:19184ms step_avg:33.36ms
step:576/2150 train_time:19217ms step_avg:33.36ms
step:577/2150 train_time:19250ms step_avg:33.36ms
step:578/2150 train_time:19283ms step_avg:33.36ms
step:579/2150 train_time:19316ms step_avg:33.36ms
step:580/2150 train_time:19349ms step_avg:33.36ms
step:581/2150 train_time:19382ms step_avg:33.36ms
step:582/2150 train_time:19415ms step_avg:33.36ms
step:583/2150 train_time:19448ms step_avg:33.36ms
step:584/2150 train_time:19481ms step_avg:33.36ms
step:585/2150 train_time:19514ms step_avg:33.36ms
step:586/2150 train_time:19548ms step_avg:33.36ms
step:587/2150 train_time:19580ms step_avg:33.36ms
step:588/2150 train_time:19614ms step_avg:33.36ms
step:589/2150 train_time:19646ms step_avg:33.36ms
step:590/2150 train_time:19679ms step_avg:33.36ms
step:591/2150 train_time:19712ms step_avg:33.35ms
step:592/2150 train_time:19746ms step_avg:33.35ms
step:593/2150 train_time:19778ms step_avg:33.35ms
step:594/2150 train_time:19812ms step_avg:33.35ms
step:595/2150 train_time:19845ms step_avg:33.35ms
step:596/2150 train_time:19878ms step_avg:33.35ms
step:597/2150 train_time:19911ms step_avg:33.35ms
step:598/2150 train_time:19945ms step_avg:33.35ms
step:599/2150 train_time:19977ms step_avg:33.35ms
step:600/2150 train_time:20011ms step_avg:33.35ms
step:601/2150 train_time:20043ms step_avg:33.35ms
step:602/2150 train_time:20077ms step_avg:33.35ms
step:603/2150 train_time:20110ms step_avg:33.35ms
step:604/2150 train_time:20144ms step_avg:33.35ms
step:605/2150 train_time:20176ms step_avg:33.35ms
step:606/2150 train_time:20210ms step_avg:33.35ms
step:607/2150 train_time:20242ms step_avg:33.35ms
step:608/2150 train_time:20276ms step_avg:33.35ms
step:609/2150 train_time:20309ms step_avg:33.35ms
step:610/2150 train_time:20342ms step_avg:33.35ms
step:611/2150 train_time:20374ms step_avg:33.35ms
step:612/2150 train_time:20408ms step_avg:33.35ms
step:613/2150 train_time:20441ms step_avg:33.35ms
step:614/2150 train_time:20474ms step_avg:33.35ms
step:615/2150 train_time:20506ms step_avg:33.34ms
step:616/2150 train_time:20540ms step_avg:33.34ms
step:617/2150 train_time:20572ms step_avg:33.34ms
step:618/2150 train_time:20606ms step_avg:33.34ms
step:619/2150 train_time:20639ms step_avg:33.34ms
step:620/2150 train_time:20672ms step_avg:33.34ms
step:621/2150 train_time:20704ms step_avg:33.34ms
step:622/2150 train_time:20738ms step_avg:33.34ms
step:623/2150 train_time:20771ms step_avg:33.34ms
step:624/2150 train_time:20804ms step_avg:33.34ms
step:625/2150 train_time:20836ms step_avg:33.34ms
step:626/2150 train_time:20870ms step_avg:33.34ms
step:627/2150 train_time:20903ms step_avg:33.34ms
step:628/2150 train_time:20936ms step_avg:33.34ms
step:629/2150 train_time:20969ms step_avg:33.34ms
step:630/2150 train_time:21003ms step_avg:33.34ms
step:631/2150 train_time:21036ms step_avg:33.34ms
step:632/2150 train_time:21069ms step_avg:33.34ms
step:633/2150 train_time:21102ms step_avg:33.34ms
step:634/2150 train_time:21135ms step_avg:33.34ms
step:635/2150 train_time:21168ms step_avg:33.34ms
step:636/2150 train_time:21202ms step_avg:33.34ms
step:637/2150 train_time:21234ms step_avg:33.33ms
step:638/2150 train_time:21268ms step_avg:33.34ms
step:639/2150 train_time:21301ms step_avg:33.33ms
step:640/2150 train_time:21334ms step_avg:33.33ms
step:641/2150 train_time:21367ms step_avg:33.33ms
step:642/2150 train_time:21401ms step_avg:33.33ms
step:643/2150 train_time:21433ms step_avg:33.33ms
step:644/2150 train_time:21466ms step_avg:33.33ms
step:645/2150 train_time:21499ms step_avg:33.33ms
step:646/2150 train_time:21532ms step_avg:33.33ms
step:647/2150 train_time:21565ms step_avg:33.33ms
step:648/2150 train_time:21598ms step_avg:33.33ms
step:649/2150 train_time:21631ms step_avg:33.33ms
step:650/2150 train_time:21664ms step_avg:33.33ms
step:651/2150 train_time:21697ms step_avg:33.33ms
step:652/2150 train_time:21730ms step_avg:33.33ms
step:653/2150 train_time:21763ms step_avg:33.33ms
step:654/2150 train_time:21796ms step_avg:33.33ms
step:655/2150 train_time:21829ms step_avg:33.33ms
step:656/2150 train_time:21863ms step_avg:33.33ms
step:657/2150 train_time:21895ms step_avg:33.33ms
step:658/2150 train_time:21929ms step_avg:33.33ms
step:659/2150 train_time:21962ms step_avg:33.33ms
step:660/2150 train_time:21995ms step_avg:33.33ms
step:661/2150 train_time:22028ms step_avg:33.33ms
step:662/2150 train_time:22062ms step_avg:33.33ms
step:663/2150 train_time:22095ms step_avg:33.33ms
step:664/2150 train_time:22128ms step_avg:33.33ms
step:665/2150 train_time:22161ms step_avg:33.33ms
step:666/2150 train_time:22195ms step_avg:33.33ms
step:667/2150 train_time:22228ms step_avg:33.32ms
step:668/2150 train_time:22262ms step_avg:33.33ms
step:669/2150 train_time:22294ms step_avg:33.32ms
step:670/2150 train_time:22328ms step_avg:33.33ms
step:671/2150 train_time:22360ms step_avg:33.32ms
step:672/2150 train_time:22394ms step_avg:33.32ms
step:673/2150 train_time:22427ms step_avg:33.32ms
step:674/2150 train_time:22460ms step_avg:33.32ms
step:675/2150 train_time:22493ms step_avg:33.32ms
step:676/2150 train_time:22527ms step_avg:33.32ms
step:677/2150 train_time:22560ms step_avg:33.32ms
step:678/2150 train_time:22593ms step_avg:33.32ms
step:679/2150 train_time:22626ms step_avg:33.32ms
step:680/2150 train_time:22660ms step_avg:33.32ms
step:681/2150 train_time:22692ms step_avg:33.32ms
step:682/2150 train_time:22726ms step_avg:33.32ms
step:683/2150 train_time:22758ms step_avg:33.32ms
step:684/2150 train_time:22792ms step_avg:33.32ms
step:685/2150 train_time:22825ms step_avg:33.32ms
step:686/2150 train_time:22858ms step_avg:33.32ms
step:687/2150 train_time:22891ms step_avg:33.32ms
step:688/2150 train_time:22924ms step_avg:33.32ms
step:689/2150 train_time:22958ms step_avg:33.32ms
step:690/2150 train_time:22991ms step_avg:33.32ms
step:691/2150 train_time:23024ms step_avg:33.32ms
step:692/2150 train_time:23057ms step_avg:33.32ms
step:693/2150 train_time:23090ms step_avg:33.32ms
step:694/2150 train_time:23123ms step_avg:33.32ms
step:695/2150 train_time:23156ms step_avg:33.32ms
step:696/2150 train_time:23190ms step_avg:33.32ms
step:697/2150 train_time:23222ms step_avg:33.32ms
step:698/2150 train_time:23256ms step_avg:33.32ms
step:699/2150 train_time:23289ms step_avg:33.32ms
step:700/2150 train_time:23323ms step_avg:33.32ms
step:701/2150 train_time:23355ms step_avg:33.32ms
step:702/2150 train_time:23389ms step_avg:33.32ms
step:703/2150 train_time:23422ms step_avg:33.32ms
step:704/2150 train_time:23455ms step_avg:33.32ms
step:705/2150 train_time:23489ms step_avg:33.32ms
step:706/2150 train_time:23547ms step_avg:33.35ms
step:707/2150 train_time:23608ms step_avg:33.39ms
step:708/2150 train_time:23668ms step_avg:33.43ms
step:709/2150 train_time:23729ms step_avg:33.47ms
step:710/2150 train_time:23789ms step_avg:33.51ms
step:711/2150 train_time:23851ms step_avg:33.55ms
step:712/2150 train_time:23910ms step_avg:33.58ms
step:713/2150 train_time:23972ms step_avg:33.62ms
step:714/2150 train_time:24031ms step_avg:33.66ms
step:715/2150 train_time:24093ms step_avg:33.70ms
step:716/2150 train_time:24153ms step_avg:33.73ms
step:717/2150 train_time:24214ms step_avg:33.77ms
step:718/2150 train_time:24274ms step_avg:33.81ms
step:719/2150 train_time:24335ms step_avg:33.85ms
step:720/2150 train_time:24395ms step_avg:33.88ms
step:721/2150 train_time:24456ms step_avg:33.92ms
step:722/2150 train_time:24515ms step_avg:33.95ms
step:723/2150 train_time:24576ms step_avg:33.99ms
step:724/2150 train_time:24636ms step_avg:34.03ms
step:725/2150 train_time:24697ms step_avg:34.07ms
step:726/2150 train_time:24757ms step_avg:34.10ms
step:727/2150 train_time:24818ms step_avg:34.14ms
step:728/2150 train_time:24877ms step_avg:34.17ms
step:729/2150 train_time:24939ms step_avg:34.21ms
step:730/2150 train_time:24999ms step_avg:34.24ms
step:731/2150 train_time:25060ms step_avg:34.28ms
step:732/2150 train_time:25119ms step_avg:34.32ms
step:733/2150 train_time:25180ms step_avg:34.35ms
step:734/2150 train_time:25240ms step_avg:34.39ms
step:735/2150 train_time:25302ms step_avg:34.42ms
step:736/2150 train_time:25361ms step_avg:34.46ms
step:737/2150 train_time:25422ms step_avg:34.49ms
step:738/2150 train_time:25481ms step_avg:34.53ms
step:739/2150 train_time:25543ms step_avg:34.56ms
step:740/2150 train_time:25602ms step_avg:34.60ms
step:741/2150 train_time:25663ms step_avg:34.63ms
step:742/2150 train_time:25722ms step_avg:34.67ms
step:743/2150 train_time:25783ms step_avg:34.70ms
step:744/2150 train_time:25843ms step_avg:34.73ms
step:745/2150 train_time:25904ms step_avg:34.77ms
step:746/2150 train_time:25964ms step_avg:34.80ms
step:747/2150 train_time:26025ms step_avg:34.84ms
step:748/2150 train_time:26084ms step_avg:34.87ms
step:749/2150 train_time:26145ms step_avg:34.91ms
step:750/2150 train_time:26205ms step_avg:34.94ms
step:750/2150 val_loss:3.8713 train_time:26267ms step_avg:35.02ms
step:751/2150 train_time:26288ms step_avg:35.00ms
step:752/2150 train_time:26324ms step_avg:35.01ms
step:753/2150 train_time:26387ms step_avg:35.04ms
step:754/2150 train_time:26450ms step_avg:35.08ms
step:755/2150 train_time:26512ms step_avg:35.12ms
step:756/2150 train_time:26570ms step_avg:35.15ms
step:757/2150 train_time:26630ms step_avg:35.18ms
step:758/2150 train_time:26688ms step_avg:35.21ms
step:759/2150 train_time:26748ms step_avg:35.24ms
step:760/2150 train_time:26807ms step_avg:35.27ms
step:761/2150 train_time:26867ms step_avg:35.30ms
step:762/2150 train_time:26925ms step_avg:35.34ms
step:763/2150 train_time:26985ms step_avg:35.37ms
step:764/2150 train_time:27044ms step_avg:35.40ms
step:765/2150 train_time:27103ms step_avg:35.43ms
step:766/2150 train_time:27167ms step_avg:35.47ms
step:767/2150 train_time:27234ms step_avg:35.51ms
step:768/2150 train_time:27295ms step_avg:35.54ms
step:769/2150 train_time:27357ms step_avg:35.57ms
step:770/2150 train_time:27416ms step_avg:35.60ms
step:771/2150 train_time:27477ms step_avg:35.64ms
step:772/2150 train_time:27536ms step_avg:35.67ms
step:773/2150 train_time:27597ms step_avg:35.70ms
step:774/2150 train_time:27655ms step_avg:35.73ms
step:775/2150 train_time:27716ms step_avg:35.76ms
step:776/2150 train_time:27774ms step_avg:35.79ms
step:777/2150 train_time:27835ms step_avg:35.82ms
step:778/2150 train_time:27893ms step_avg:35.85ms
step:779/2150 train_time:27953ms step_avg:35.88ms
step:780/2150 train_time:28012ms step_avg:35.91ms
step:781/2150 train_time:28073ms step_avg:35.95ms
step:782/2150 train_time:28134ms step_avg:35.98ms
step:783/2150 train_time:28198ms step_avg:36.01ms
step:784/2150 train_time:28259ms step_avg:36.04ms
step:785/2150 train_time:28321ms step_avg:36.08ms
step:786/2150 train_time:28381ms step_avg:36.11ms
step:787/2150 train_time:28443ms step_avg:36.14ms
step:788/2150 train_time:28501ms step_avg:36.17ms
step:789/2150 train_time:28562ms step_avg:36.20ms
step:790/2150 train_time:28622ms step_avg:36.23ms
step:791/2150 train_time:28682ms step_avg:36.26ms
step:792/2150 train_time:28742ms step_avg:36.29ms
step:793/2150 train_time:28802ms step_avg:36.32ms
step:794/2150 train_time:28861ms step_avg:36.35ms
step:795/2150 train_time:28922ms step_avg:36.38ms
step:796/2150 train_time:28980ms step_avg:36.41ms
step:797/2150 train_time:29041ms step_avg:36.44ms
step:798/2150 train_time:29101ms step_avg:36.47ms
step:799/2150 train_time:29163ms step_avg:36.50ms
step:800/2150 train_time:29223ms step_avg:36.53ms
step:801/2150 train_time:29284ms step_avg:36.56ms
step:802/2150 train_time:29344ms step_avg:36.59ms
step:803/2150 train_time:29405ms step_avg:36.62ms
step:804/2150 train_time:29465ms step_avg:36.65ms
step:805/2150 train_time:29526ms step_avg:36.68ms
step:806/2150 train_time:29585ms step_avg:36.71ms
step:807/2150 train_time:29646ms step_avg:36.74ms
step:808/2150 train_time:29705ms step_avg:36.76ms
step:809/2150 train_time:29766ms step_avg:36.79ms
step:810/2150 train_time:29825ms step_avg:36.82ms
step:811/2150 train_time:29886ms step_avg:36.85ms
step:812/2150 train_time:29946ms step_avg:36.88ms
step:813/2150 train_time:30007ms step_avg:36.91ms
step:814/2150 train_time:30067ms step_avg:36.94ms
step:815/2150 train_time:30128ms step_avg:36.97ms
step:816/2150 train_time:30189ms step_avg:37.00ms
step:817/2150 train_time:30250ms step_avg:37.03ms
step:818/2150 train_time:30310ms step_avg:37.05ms
step:819/2150 train_time:30372ms step_avg:37.08ms
step:820/2150 train_time:30431ms step_avg:37.11ms
step:821/2150 train_time:30493ms step_avg:37.14ms
step:822/2150 train_time:30553ms step_avg:37.17ms
step:823/2150 train_time:30614ms step_avg:37.20ms
step:824/2150 train_time:30673ms step_avg:37.22ms
step:825/2150 train_time:30734ms step_avg:37.25ms
step:826/2150 train_time:30793ms step_avg:37.28ms
step:827/2150 train_time:30854ms step_avg:37.31ms
step:828/2150 train_time:30914ms step_avg:37.34ms
step:829/2150 train_time:30975ms step_avg:37.36ms
step:830/2150 train_time:31034ms step_avg:37.39ms
step:831/2150 train_time:31096ms step_avg:37.42ms
step:832/2150 train_time:31156ms step_avg:37.45ms
step:833/2150 train_time:31217ms step_avg:37.48ms
step:834/2150 train_time:31277ms step_avg:37.50ms
step:835/2150 train_time:31338ms step_avg:37.53ms
step:836/2150 train_time:31398ms step_avg:37.56ms
step:837/2150 train_time:31459ms step_avg:37.59ms
step:838/2150 train_time:31519ms step_avg:37.61ms
step:839/2150 train_time:31580ms step_avg:37.64ms
step:840/2150 train_time:31640ms step_avg:37.67ms
step:841/2150 train_time:31701ms step_avg:37.69ms
step:842/2150 train_time:31760ms step_avg:37.72ms
step:843/2150 train_time:31821ms step_avg:37.75ms
step:844/2150 train_time:31880ms step_avg:37.77ms
step:845/2150 train_time:31942ms step_avg:37.80ms
step:846/2150 train_time:32001ms step_avg:37.83ms
step:847/2150 train_time:32062ms step_avg:37.85ms
step:848/2150 train_time:32122ms step_avg:37.88ms
step:849/2150 train_time:32182ms step_avg:37.91ms
step:850/2150 train_time:32242ms step_avg:37.93ms
step:851/2150 train_time:32303ms step_avg:37.96ms
step:852/2150 train_time:32363ms step_avg:37.98ms
step:853/2150 train_time:32424ms step_avg:38.01ms
step:854/2150 train_time:32483ms step_avg:38.04ms
step:855/2150 train_time:32545ms step_avg:38.06ms
step:856/2150 train_time:32604ms step_avg:38.09ms
step:857/2150 train_time:32664ms step_avg:38.11ms
step:858/2150 train_time:32724ms step_avg:38.14ms
step:859/2150 train_time:32785ms step_avg:38.17ms
step:860/2150 train_time:32845ms step_avg:38.19ms
step:861/2150 train_time:32906ms step_avg:38.22ms
step:862/2150 train_time:32966ms step_avg:38.24ms
step:863/2150 train_time:33028ms step_avg:38.27ms
step:864/2150 train_time:33087ms step_avg:38.30ms
step:865/2150 train_time:33148ms step_avg:38.32ms
step:866/2150 train_time:33208ms step_avg:38.35ms
step:867/2150 train_time:33269ms step_avg:38.37ms
step:868/2150 train_time:33329ms step_avg:38.40ms
step:869/2150 train_time:33390ms step_avg:38.42ms
step:870/2150 train_time:33450ms step_avg:38.45ms
step:871/2150 train_time:33512ms step_avg:38.47ms
step:872/2150 train_time:33571ms step_avg:38.50ms
step:873/2150 train_time:33632ms step_avg:38.53ms
step:874/2150 train_time:33692ms step_avg:38.55ms
step:875/2150 train_time:33753ms step_avg:38.58ms
step:876/2150 train_time:33813ms step_avg:38.60ms
step:877/2150 train_time:33875ms step_avg:38.63ms
step:878/2150 train_time:33934ms step_avg:38.65ms
step:879/2150 train_time:33995ms step_avg:38.67ms
step:880/2150 train_time:34055ms step_avg:38.70ms
step:881/2150 train_time:34116ms step_avg:38.72ms
step:882/2150 train_time:34175ms step_avg:38.75ms
step:883/2150 train_time:34237ms step_avg:38.77ms
step:884/2150 train_time:34298ms step_avg:38.80ms
step:885/2150 train_time:34359ms step_avg:38.82ms
step:886/2150 train_time:34419ms step_avg:38.85ms
step:887/2150 train_time:34480ms step_avg:38.87ms
step:888/2150 train_time:34540ms step_avg:38.90ms
step:889/2150 train_time:34601ms step_avg:38.92ms
step:890/2150 train_time:34661ms step_avg:38.94ms
step:891/2150 train_time:34722ms step_avg:38.97ms
step:892/2150 train_time:34782ms step_avg:38.99ms
step:893/2150 train_time:34843ms step_avg:39.02ms
step:894/2150 train_time:34902ms step_avg:39.04ms
step:895/2150 train_time:34963ms step_avg:39.07ms
step:896/2150 train_time:35023ms step_avg:39.09ms
step:897/2150 train_time:35084ms step_avg:39.11ms
step:898/2150 train_time:35143ms step_avg:39.13ms
step:899/2150 train_time:35204ms step_avg:39.16ms
step:900/2150 train_time:35263ms step_avg:39.18ms
step:901/2150 train_time:35325ms step_avg:39.21ms
step:902/2150 train_time:35385ms step_avg:39.23ms
step:903/2150 train_time:35446ms step_avg:39.25ms
step:904/2150 train_time:35505ms step_avg:39.28ms
step:905/2150 train_time:35566ms step_avg:39.30ms
step:906/2150 train_time:35625ms step_avg:39.32ms
step:907/2150 train_time:35687ms step_avg:39.35ms
step:908/2150 train_time:35747ms step_avg:39.37ms
step:909/2150 train_time:35809ms step_avg:39.39ms
step:910/2150 train_time:35869ms step_avg:39.42ms
step:911/2150 train_time:35930ms step_avg:39.44ms
step:912/2150 train_time:35990ms step_avg:39.46ms
step:913/2150 train_time:36051ms step_avg:39.49ms
step:914/2150 train_time:36111ms step_avg:39.51ms
step:915/2150 train_time:36172ms step_avg:39.53ms
step:916/2150 train_time:36232ms step_avg:39.55ms
step:917/2150 train_time:36293ms step_avg:39.58ms
step:918/2150 train_time:36353ms step_avg:39.60ms
step:919/2150 train_time:36414ms step_avg:39.62ms
step:920/2150 train_time:36474ms step_avg:39.65ms
step:921/2150 train_time:36535ms step_avg:39.67ms
step:922/2150 train_time:36595ms step_avg:39.69ms
step:923/2150 train_time:36657ms step_avg:39.72ms
step:924/2150 train_time:36716ms step_avg:39.74ms
step:925/2150 train_time:36778ms step_avg:39.76ms
step:926/2150 train_time:36837ms step_avg:39.78ms
step:927/2150 train_time:36899ms step_avg:39.80ms
step:928/2150 train_time:36958ms step_avg:39.83ms
step:929/2150 train_time:37020ms step_avg:39.85ms
step:930/2150 train_time:37080ms step_avg:39.87ms
step:931/2150 train_time:37141ms step_avg:39.89ms
step:932/2150 train_time:37200ms step_avg:39.91ms
step:933/2150 train_time:37261ms step_avg:39.94ms
step:934/2150 train_time:37321ms step_avg:39.96ms
step:935/2150 train_time:37382ms step_avg:39.98ms
step:936/2150 train_time:37441ms step_avg:40.00ms
step:937/2150 train_time:37502ms step_avg:40.02ms
step:938/2150 train_time:37562ms step_avg:40.04ms
step:939/2150 train_time:37623ms step_avg:40.07ms
step:940/2150 train_time:37682ms step_avg:40.09ms
step:941/2150 train_time:37743ms step_avg:40.11ms
step:942/2150 train_time:37803ms step_avg:40.13ms
step:943/2150 train_time:37865ms step_avg:40.15ms
step:944/2150 train_time:37924ms step_avg:40.17ms
step:945/2150 train_time:37985ms step_avg:40.20ms
step:946/2150 train_time:38045ms step_avg:40.22ms
step:947/2150 train_time:38105ms step_avg:40.24ms
step:948/2150 train_time:38165ms step_avg:40.26ms
step:949/2150 train_time:38226ms step_avg:40.28ms
step:950/2150 train_time:38286ms step_avg:40.30ms
step:951/2150 train_time:38347ms step_avg:40.32ms
step:952/2150 train_time:38407ms step_avg:40.34ms
step:953/2150 train_time:38469ms step_avg:40.37ms
step:954/2150 train_time:38530ms step_avg:40.39ms
step:955/2150 train_time:38591ms step_avg:40.41ms
step:956/2150 train_time:38651ms step_avg:40.43ms
step:957/2150 train_time:38713ms step_avg:40.45ms
step:958/2150 train_time:38773ms step_avg:40.47ms
step:959/2150 train_time:38835ms step_avg:40.50ms
step:960/2150 train_time:38894ms step_avg:40.52ms
step:961/2150 train_time:38956ms step_avg:40.54ms
step:962/2150 train_time:39015ms step_avg:40.56ms
step:963/2150 train_time:39076ms step_avg:40.58ms
step:964/2150 train_time:39136ms step_avg:40.60ms
step:965/2150 train_time:39197ms step_avg:40.62ms
step:966/2150 train_time:39257ms step_avg:40.64ms
step:967/2150 train_time:39319ms step_avg:40.66ms
step:968/2150 train_time:39378ms step_avg:40.68ms
step:969/2150 train_time:39440ms step_avg:40.70ms
step:970/2150 train_time:39499ms step_avg:40.72ms
step:971/2150 train_time:39561ms step_avg:40.74ms
step:972/2150 train_time:39620ms step_avg:40.76ms
step:973/2150 train_time:39681ms step_avg:40.78ms
step:974/2150 train_time:39741ms step_avg:40.80ms
step:975/2150 train_time:39802ms step_avg:40.82ms
step:976/2150 train_time:39861ms step_avg:40.84ms
step:977/2150 train_time:39923ms step_avg:40.86ms
step:978/2150 train_time:39983ms step_avg:40.88ms
step:979/2150 train_time:40043ms step_avg:40.90ms
step:980/2150 train_time:40103ms step_avg:40.92ms
step:981/2150 train_time:40164ms step_avg:40.94ms
step:982/2150 train_time:40224ms step_avg:40.96ms
step:983/2150 train_time:40285ms step_avg:40.98ms
step:984/2150 train_time:40344ms step_avg:41.00ms
step:985/2150 train_time:40405ms step_avg:41.02ms
step:986/2150 train_time:40465ms step_avg:41.04ms
step:987/2150 train_time:40527ms step_avg:41.06ms
step:988/2150 train_time:40586ms step_avg:41.08ms
step:989/2150 train_time:40647ms step_avg:41.10ms
step:990/2150 train_time:40708ms step_avg:41.12ms
step:991/2150 train_time:40770ms step_avg:41.14ms
step:992/2150 train_time:40830ms step_avg:41.16ms
step:993/2150 train_time:40891ms step_avg:41.18ms
step:994/2150 train_time:40951ms step_avg:41.20ms
step:995/2150 train_time:41012ms step_avg:41.22ms
step:996/2150 train_time:41072ms step_avg:41.24ms
step:997/2150 train_time:41134ms step_avg:41.26ms
step:998/2150 train_time:41194ms step_avg:41.28ms
step:999/2150 train_time:41255ms step_avg:41.30ms
step:1000/2150 train_time:41314ms step_avg:41.31ms
step:1000/2150 val_loss:3.7126 train_time:41378ms step_avg:41.38ms
step:1001/2150 train_time:41399ms step_avg:41.36ms
step:1002/2150 train_time:41438ms step_avg:41.36ms
step:1003/2150 train_time:41505ms step_avg:41.38ms
step:1004/2150 train_time:41567ms step_avg:41.40ms
step:1005/2150 train_time:41628ms step_avg:41.42ms
step:1006/2150 train_time:41687ms step_avg:41.44ms
step:1007/2150 train_time:41748ms step_avg:41.46ms
step:1008/2150 train_time:41807ms step_avg:41.47ms
step:1009/2150 train_time:41867ms step_avg:41.49ms
step:1010/2150 train_time:41925ms step_avg:41.51ms
step:1011/2150 train_time:41986ms step_avg:41.53ms
step:1012/2150 train_time:42044ms step_avg:41.55ms
step:1013/2150 train_time:42105ms step_avg:41.56ms
step:1014/2150 train_time:42164ms step_avg:41.58ms
step:1015/2150 train_time:42225ms step_avg:41.60ms
step:1016/2150 train_time:42285ms step_avg:41.62ms
step:1017/2150 train_time:42348ms step_avg:41.64ms
step:1018/2150 train_time:42410ms step_avg:41.66ms
step:1019/2150 train_time:42474ms step_avg:41.68ms
step:1020/2150 train_time:42534ms step_avg:41.70ms
step:1021/2150 train_time:42596ms step_avg:41.72ms
step:1022/2150 train_time:42656ms step_avg:41.74ms
step:1023/2150 train_time:42718ms step_avg:41.76ms
step:1024/2150 train_time:42777ms step_avg:41.77ms
step:1025/2150 train_time:42837ms step_avg:41.79ms
step:1026/2150 train_time:42897ms step_avg:41.81ms
step:1027/2150 train_time:42958ms step_avg:41.83ms
step:1028/2150 train_time:43016ms step_avg:41.84ms
step:1029/2150 train_time:43076ms step_avg:41.86ms
step:1030/2150 train_time:43136ms step_avg:41.88ms
step:1031/2150 train_time:43197ms step_avg:41.90ms
step:1032/2150 train_time:43256ms step_avg:41.91ms
step:1033/2150 train_time:43317ms step_avg:41.93ms
step:1034/2150 train_time:43378ms step_avg:41.95ms
step:1035/2150 train_time:43440ms step_avg:41.97ms
step:1036/2150 train_time:43500ms step_avg:41.99ms
step:1037/2150 train_time:43562ms step_avg:42.01ms
step:1038/2150 train_time:43622ms step_avg:42.03ms
step:1039/2150 train_time:43684ms step_avg:42.04ms
step:1040/2150 train_time:43743ms step_avg:42.06ms
step:1041/2150 train_time:43805ms step_avg:42.08ms
step:1042/2150 train_time:43864ms step_avg:42.10ms
step:1043/2150 train_time:43926ms step_avg:42.11ms
step:1044/2150 train_time:43984ms step_avg:42.13ms
step:1045/2150 train_time:44046ms step_avg:42.15ms
step:1046/2150 train_time:44105ms step_avg:42.17ms
step:1047/2150 train_time:44166ms step_avg:42.18ms
step:1048/2150 train_time:44225ms step_avg:42.20ms
step:1049/2150 train_time:44286ms step_avg:42.22ms
step:1050/2150 train_time:44346ms step_avg:42.23ms
step:1051/2150 train_time:44409ms step_avg:42.25ms
step:1052/2150 train_time:44469ms step_avg:42.27ms
step:1053/2150 train_time:44532ms step_avg:42.29ms
step:1054/2150 train_time:44592ms step_avg:42.31ms
step:1055/2150 train_time:44654ms step_avg:42.33ms
step:1056/2150 train_time:44713ms step_avg:42.34ms
step:1057/2150 train_time:44776ms step_avg:42.36ms
step:1058/2150 train_time:44835ms step_avg:42.38ms
step:1059/2150 train_time:44896ms step_avg:42.39ms
step:1060/2150 train_time:44955ms step_avg:42.41ms
step:1061/2150 train_time:45017ms step_avg:42.43ms
step:1062/2150 train_time:45076ms step_avg:42.44ms
step:1063/2150 train_time:45137ms step_avg:42.46ms
step:1064/2150 train_time:45196ms step_avg:42.48ms
step:1065/2150 train_time:45256ms step_avg:42.49ms
step:1066/2150 train_time:45316ms step_avg:42.51ms
step:1067/2150 train_time:45378ms step_avg:42.53ms
step:1068/2150 train_time:45438ms step_avg:42.55ms
step:1069/2150 train_time:45500ms step_avg:42.56ms
step:1070/2150 train_time:45561ms step_avg:42.58ms
step:1071/2150 train_time:45623ms step_avg:42.60ms
step:1072/2150 train_time:45682ms step_avg:42.61ms
step:1073/2150 train_time:45744ms step_avg:42.63ms
step:1074/2150 train_time:45803ms step_avg:42.65ms
step:1075/2150 train_time:45864ms step_avg:42.66ms
step:1076/2150 train_time:45924ms step_avg:42.68ms
step:1077/2150 train_time:45986ms step_avg:42.70ms
step:1078/2150 train_time:46046ms step_avg:42.71ms
step:1079/2150 train_time:46107ms step_avg:42.73ms
step:1080/2150 train_time:46166ms step_avg:42.75ms
step:1081/2150 train_time:46228ms step_avg:42.76ms
step:1082/2150 train_time:46287ms step_avg:42.78ms
step:1083/2150 train_time:46349ms step_avg:42.80ms
step:1084/2150 train_time:46409ms step_avg:42.81ms
step:1085/2150 train_time:46470ms step_avg:42.83ms
step:1086/2150 train_time:46530ms step_avg:42.85ms
step:1087/2150 train_time:46592ms step_avg:42.86ms
step:1088/2150 train_time:46652ms step_avg:42.88ms
step:1089/2150 train_time:46714ms step_avg:42.90ms
step:1090/2150 train_time:46774ms step_avg:42.91ms
step:1091/2150 train_time:46836ms step_avg:42.93ms
step:1092/2150 train_time:46896ms step_avg:42.94ms
step:1093/2150 train_time:46957ms step_avg:42.96ms
step:1094/2150 train_time:47015ms step_avg:42.98ms
step:1095/2150 train_time:47076ms step_avg:42.99ms
step:1096/2150 train_time:47135ms step_avg:43.01ms
step:1097/2150 train_time:47197ms step_avg:43.02ms
step:1098/2150 train_time:47256ms step_avg:43.04ms
step:1099/2150 train_time:47318ms step_avg:43.06ms
step:1100/2150 train_time:47377ms step_avg:43.07ms
step:1101/2150 train_time:47438ms step_avg:43.09ms
step:1102/2150 train_time:47498ms step_avg:43.10ms
step:1103/2150 train_time:47560ms step_avg:43.12ms
step:1104/2150 train_time:47620ms step_avg:43.13ms
step:1105/2150 train_time:47681ms step_avg:43.15ms
step:1106/2150 train_time:47741ms step_avg:43.17ms
step:1107/2150 train_time:47803ms step_avg:43.18ms
step:1108/2150 train_time:47863ms step_avg:43.20ms
step:1109/2150 train_time:47924ms step_avg:43.21ms
step:1110/2150 train_time:47984ms step_avg:43.23ms
step:1111/2150 train_time:48045ms step_avg:43.24ms
step:1112/2150 train_time:48105ms step_avg:43.26ms
step:1113/2150 train_time:48166ms step_avg:43.28ms
step:1114/2150 train_time:48226ms step_avg:43.29ms
step:1115/2150 train_time:48287ms step_avg:43.31ms
step:1116/2150 train_time:48347ms step_avg:43.32ms
step:1117/2150 train_time:48408ms step_avg:43.34ms
step:1118/2150 train_time:48469ms step_avg:43.35ms
step:1119/2150 train_time:48530ms step_avg:43.37ms
step:1120/2150 train_time:48589ms step_avg:43.38ms
step:1121/2150 train_time:48651ms step_avg:43.40ms
step:1122/2150 train_time:48711ms step_avg:43.41ms
step:1123/2150 train_time:48773ms step_avg:43.43ms
step:1124/2150 train_time:48833ms step_avg:43.45ms
step:1125/2150 train_time:48896ms step_avg:43.46ms
step:1126/2150 train_time:48955ms step_avg:43.48ms
step:1127/2150 train_time:49017ms step_avg:43.49ms
step:1128/2150 train_time:49076ms step_avg:43.51ms
step:1129/2150 train_time:49137ms step_avg:43.52ms
step:1130/2150 train_time:49197ms step_avg:43.54ms
step:1131/2150 train_time:49258ms step_avg:43.55ms
step:1132/2150 train_time:49318ms step_avg:43.57ms
step:1133/2150 train_time:49378ms step_avg:43.58ms
step:1134/2150 train_time:49438ms step_avg:43.60ms
step:1135/2150 train_time:49499ms step_avg:43.61ms
step:1136/2150 train_time:49560ms step_avg:43.63ms
step:1137/2150 train_time:49621ms step_avg:43.64ms
step:1138/2150 train_time:49681ms step_avg:43.66ms
step:1139/2150 train_time:49742ms step_avg:43.67ms
step:1140/2150 train_time:49802ms step_avg:43.69ms
step:1141/2150 train_time:49864ms step_avg:43.70ms
step:1142/2150 train_time:49924ms step_avg:43.72ms
step:1143/2150 train_time:49985ms step_avg:43.73ms
step:1144/2150 train_time:50045ms step_avg:43.75ms
step:1145/2150 train_time:50107ms step_avg:43.76ms
step:1146/2150 train_time:50167ms step_avg:43.78ms
step:1147/2150 train_time:50228ms step_avg:43.79ms
step:1148/2150 train_time:50287ms step_avg:43.80ms
step:1149/2150 train_time:50349ms step_avg:43.82ms
step:1150/2150 train_time:50409ms step_avg:43.83ms
step:1151/2150 train_time:50470ms step_avg:43.85ms
step:1152/2150 train_time:50530ms step_avg:43.86ms
step:1153/2150 train_time:50592ms step_avg:43.88ms
step:1154/2150 train_time:50652ms step_avg:43.89ms
step:1155/2150 train_time:50716ms step_avg:43.91ms
step:1156/2150 train_time:50776ms step_avg:43.92ms
step:1157/2150 train_time:50837ms step_avg:43.94ms
step:1158/2150 train_time:50897ms step_avg:43.95ms
step:1159/2150 train_time:50958ms step_avg:43.97ms
step:1160/2150 train_time:51017ms step_avg:43.98ms
step:1161/2150 train_time:51078ms step_avg:44.00ms
step:1162/2150 train_time:51137ms step_avg:44.01ms
step:1163/2150 train_time:51198ms step_avg:44.02ms
step:1164/2150 train_time:51258ms step_avg:44.04ms
step:1165/2150 train_time:51320ms step_avg:44.05ms
step:1166/2150 train_time:51380ms step_avg:44.06ms
step:1167/2150 train_time:51441ms step_avg:44.08ms
step:1168/2150 train_time:51501ms step_avg:44.09ms
step:1169/2150 train_time:51562ms step_avg:44.11ms
step:1170/2150 train_time:51622ms step_avg:44.12ms
step:1171/2150 train_time:51684ms step_avg:44.14ms
step:1172/2150 train_time:51744ms step_avg:44.15ms
step:1173/2150 train_time:51805ms step_avg:44.16ms
step:1174/2150 train_time:51865ms step_avg:44.18ms
step:1175/2150 train_time:51926ms step_avg:44.19ms
step:1176/2150 train_time:51987ms step_avg:44.21ms
step:1177/2150 train_time:52049ms step_avg:44.22ms
step:1178/2150 train_time:52108ms step_avg:44.23ms
step:1179/2150 train_time:52169ms step_avg:44.25ms
step:1180/2150 train_time:52229ms step_avg:44.26ms
step:1181/2150 train_time:52290ms step_avg:44.28ms
step:1182/2150 train_time:52350ms step_avg:44.29ms
step:1183/2150 train_time:52412ms step_avg:44.30ms
step:1184/2150 train_time:52472ms step_avg:44.32ms
step:1185/2150 train_time:52534ms step_avg:44.33ms
step:1186/2150 train_time:52593ms step_avg:44.34ms
step:1187/2150 train_time:52655ms step_avg:44.36ms
step:1188/2150 train_time:52715ms step_avg:44.37ms
step:1189/2150 train_time:52777ms step_avg:44.39ms
step:1190/2150 train_time:52836ms step_avg:44.40ms
step:1191/2150 train_time:52898ms step_avg:44.41ms
step:1192/2150 train_time:52957ms step_avg:44.43ms
step:1193/2150 train_time:53019ms step_avg:44.44ms
step:1194/2150 train_time:53078ms step_avg:44.45ms
step:1195/2150 train_time:53139ms step_avg:44.47ms
step:1196/2150 train_time:53199ms step_avg:44.48ms
step:1197/2150 train_time:53259ms step_avg:44.49ms
step:1198/2150 train_time:53319ms step_avg:44.51ms
step:1199/2150 train_time:53380ms step_avg:44.52ms
step:1200/2150 train_time:53440ms step_avg:44.53ms
step:1201/2150 train_time:53502ms step_avg:44.55ms
step:1202/2150 train_time:53562ms step_avg:44.56ms
step:1203/2150 train_time:53624ms step_avg:44.58ms
step:1204/2150 train_time:53684ms step_avg:44.59ms
step:1205/2150 train_time:53746ms step_avg:44.60ms
step:1206/2150 train_time:53806ms step_avg:44.62ms
step:1207/2150 train_time:53868ms step_avg:44.63ms
step:1208/2150 train_time:53928ms step_avg:44.64ms
step:1209/2150 train_time:53990ms step_avg:44.66ms
step:1210/2150 train_time:54050ms step_avg:44.67ms
step:1211/2150 train_time:54111ms step_avg:44.68ms
step:1212/2150 train_time:54170ms step_avg:44.69ms
step:1213/2150 train_time:54232ms step_avg:44.71ms
step:1214/2150 train_time:54291ms step_avg:44.72ms
step:1215/2150 train_time:54352ms step_avg:44.73ms
step:1216/2150 train_time:54413ms step_avg:44.75ms
step:1217/2150 train_time:54476ms step_avg:44.76ms
step:1218/2150 train_time:54536ms step_avg:44.77ms
step:1219/2150 train_time:54597ms step_avg:44.79ms
step:1220/2150 train_time:54656ms step_avg:44.80ms
step:1221/2150 train_time:54718ms step_avg:44.81ms
step:1222/2150 train_time:54777ms step_avg:44.83ms
step:1223/2150 train_time:54839ms step_avg:44.84ms
step:1224/2150 train_time:54898ms step_avg:44.85ms
step:1225/2150 train_time:54960ms step_avg:44.86ms
step:1226/2150 train_time:55019ms step_avg:44.88ms
step:1227/2150 train_time:55081ms step_avg:44.89ms
step:1228/2150 train_time:55140ms step_avg:44.90ms
step:1229/2150 train_time:55202ms step_avg:44.92ms
step:1230/2150 train_time:55261ms step_avg:44.93ms
step:1231/2150 train_time:55323ms step_avg:44.94ms
step:1232/2150 train_time:55383ms step_avg:44.95ms
step:1233/2150 train_time:55445ms step_avg:44.97ms
step:1234/2150 train_time:55504ms step_avg:44.98ms
step:1235/2150 train_time:55566ms step_avg:44.99ms
step:1236/2150 train_time:55626ms step_avg:45.00ms
step:1237/2150 train_time:55688ms step_avg:45.02ms
step:1238/2150 train_time:55747ms step_avg:45.03ms
step:1239/2150 train_time:55809ms step_avg:45.04ms
step:1240/2150 train_time:55868ms step_avg:45.05ms
step:1241/2150 train_time:55930ms step_avg:45.07ms
step:1242/2150 train_time:55990ms step_avg:45.08ms
step:1243/2150 train_time:56052ms step_avg:45.09ms
step:1244/2150 train_time:56112ms step_avg:45.11ms
step:1245/2150 train_time:56173ms step_avg:45.12ms
step:1246/2150 train_time:56233ms step_avg:45.13ms
step:1247/2150 train_time:56295ms step_avg:45.14ms
step:1248/2150 train_time:56356ms step_avg:45.16ms
step:1249/2150 train_time:56417ms step_avg:45.17ms
step:1250/2150 train_time:56476ms step_avg:45.18ms
step:1250/2150 val_loss:3.5923 train_time:56540ms step_avg:45.23ms
step:1251/2150 train_time:56561ms step_avg:45.21ms
step:1252/2150 train_time:56599ms step_avg:45.21ms
step:1253/2150 train_time:56662ms step_avg:45.22ms
step:1254/2150 train_time:56723ms step_avg:45.23ms
step:1255/2150 train_time:56785ms step_avg:45.25ms
step:1256/2150 train_time:56845ms step_avg:45.26ms
step:1257/2150 train_time:56906ms step_avg:45.27ms
step:1258/2150 train_time:56965ms step_avg:45.28ms
step:1259/2150 train_time:57026ms step_avg:45.29ms
step:1260/2150 train_time:57084ms step_avg:45.31ms
step:1261/2150 train_time:57145ms step_avg:45.32ms
step:1262/2150 train_time:57204ms step_avg:45.33ms
step:1263/2150 train_time:57265ms step_avg:45.34ms
step:1264/2150 train_time:57326ms step_avg:45.35ms
step:1265/2150 train_time:57388ms step_avg:45.37ms
step:1266/2150 train_time:57447ms step_avg:45.38ms
step:1267/2150 train_time:57510ms step_avg:45.39ms
step:1268/2150 train_time:57571ms step_avg:45.40ms
step:1269/2150 train_time:57633ms step_avg:45.42ms
step:1270/2150 train_time:57694ms step_avg:45.43ms
step:1271/2150 train_time:57756ms step_avg:45.44ms
step:1272/2150 train_time:57816ms step_avg:45.45ms
step:1273/2150 train_time:57877ms step_avg:45.47ms
step:1274/2150 train_time:57936ms step_avg:45.48ms
step:1275/2150 train_time:57998ms step_avg:45.49ms
step:1276/2150 train_time:58057ms step_avg:45.50ms
step:1277/2150 train_time:58118ms step_avg:45.51ms
step:1278/2150 train_time:58178ms step_avg:45.52ms
step:1279/2150 train_time:58240ms step_avg:45.54ms
step:1280/2150 train_time:58299ms step_avg:45.55ms
step:1281/2150 train_time:58361ms step_avg:45.56ms
step:1282/2150 train_time:58420ms step_avg:45.57ms
step:1283/2150 train_time:58482ms step_avg:45.58ms
step:1284/2150 train_time:58542ms step_avg:45.59ms
step:1285/2150 train_time:58605ms step_avg:45.61ms
step:1286/2150 train_time:58665ms step_avg:45.62ms
step:1287/2150 train_time:58728ms step_avg:45.63ms
step:1288/2150 train_time:58788ms step_avg:45.64ms
step:1289/2150 train_time:58850ms step_avg:45.66ms
step:1290/2150 train_time:58909ms step_avg:45.67ms
step:1291/2150 train_time:58969ms step_avg:45.68ms
step:1292/2150 train_time:59029ms step_avg:45.69ms
step:1293/2150 train_time:59090ms step_avg:45.70ms
step:1294/2150 train_time:59149ms step_avg:45.71ms
step:1295/2150 train_time:59210ms step_avg:45.72ms
step:1296/2150 train_time:59269ms step_avg:45.73ms
step:1297/2150 train_time:59330ms step_avg:45.74ms
step:1298/2150 train_time:59390ms step_avg:45.76ms
step:1299/2150 train_time:59451ms step_avg:45.77ms
step:1300/2150 train_time:59511ms step_avg:45.78ms
step:1301/2150 train_time:59573ms step_avg:45.79ms
step:1302/2150 train_time:59633ms step_avg:45.80ms
step:1303/2150 train_time:59696ms step_avg:45.81ms
step:1304/2150 train_time:59756ms step_avg:45.83ms
step:1305/2150 train_time:59818ms step_avg:45.84ms
step:1306/2150 train_time:59879ms step_avg:45.85ms
step:1307/2150 train_time:59940ms step_avg:45.86ms
step:1308/2150 train_time:59999ms step_avg:45.87ms
step:1309/2150 train_time:60061ms step_avg:45.88ms
step:1310/2150 train_time:60120ms step_avg:45.89ms
step:1311/2150 train_time:60181ms step_avg:45.90ms
step:1312/2150 train_time:60241ms step_avg:45.92ms
step:1313/2150 train_time:60302ms step_avg:45.93ms
step:1314/2150 train_time:60361ms step_avg:45.94ms
step:1315/2150 train_time:60422ms step_avg:45.95ms
step:1316/2150 train_time:60482ms step_avg:45.96ms
step:1317/2150 train_time:60543ms step_avg:45.97ms
step:1318/2150 train_time:60603ms step_avg:45.98ms
step:1319/2150 train_time:60664ms step_avg:45.99ms
step:1320/2150 train_time:60724ms step_avg:46.00ms
step:1321/2150 train_time:60785ms step_avg:46.01ms
step:1322/2150 train_time:60845ms step_avg:46.02ms
step:1323/2150 train_time:60907ms step_avg:46.04ms
step:1324/2150 train_time:60966ms step_avg:46.05ms
step:1325/2150 train_time:61027ms step_avg:46.06ms
step:1326/2150 train_time:61087ms step_avg:46.07ms
step:1327/2150 train_time:61148ms step_avg:46.08ms
step:1328/2150 train_time:61207ms step_avg:46.09ms
step:1329/2150 train_time:61269ms step_avg:46.10ms
step:1330/2150 train_time:61328ms step_avg:46.11ms
step:1331/2150 train_time:61389ms step_avg:46.12ms
step:1332/2150 train_time:61448ms step_avg:46.13ms
step:1333/2150 train_time:61510ms step_avg:46.14ms
step:1334/2150 train_time:61570ms step_avg:46.15ms
step:1335/2150 train_time:61630ms step_avg:46.16ms
step:1336/2150 train_time:61690ms step_avg:46.18ms
step:1337/2150 train_time:61752ms step_avg:46.19ms
step:1338/2150 train_time:61811ms step_avg:46.20ms
step:1339/2150 train_time:61873ms step_avg:46.21ms
step:1340/2150 train_time:61933ms step_avg:46.22ms
step:1341/2150 train_time:61995ms step_avg:46.23ms
step:1342/2150 train_time:62055ms step_avg:46.24ms
step:1343/2150 train_time:62116ms step_avg:46.25ms
step:1344/2150 train_time:62176ms step_avg:46.26ms
step:1345/2150 train_time:62238ms step_avg:46.27ms
step:1346/2150 train_time:62297ms step_avg:46.28ms
step:1347/2150 train_time:62359ms step_avg:46.29ms
step:1348/2150 train_time:62420ms step_avg:46.31ms
step:1349/2150 train_time:62481ms step_avg:46.32ms
step:1350/2150 train_time:62541ms step_avg:46.33ms
step:1351/2150 train_time:62603ms step_avg:46.34ms
step:1352/2150 train_time:62663ms step_avg:46.35ms
step:1353/2150 train_time:62725ms step_avg:46.36ms
step:1354/2150 train_time:62785ms step_avg:46.37ms
step:1355/2150 train_time:62847ms step_avg:46.38ms
step:1356/2150 train_time:62907ms step_avg:46.39ms
step:1357/2150 train_time:62968ms step_avg:46.40ms
step:1358/2150 train_time:63027ms step_avg:46.41ms
step:1359/2150 train_time:63088ms step_avg:46.42ms
step:1360/2150 train_time:63147ms step_avg:46.43ms
step:1361/2150 train_time:63208ms step_avg:46.44ms
step:1362/2150 train_time:63267ms step_avg:46.45ms
step:1363/2150 train_time:63329ms step_avg:46.46ms
step:1364/2150 train_time:63389ms step_avg:46.47ms
step:1365/2150 train_time:63449ms step_avg:46.48ms
step:1366/2150 train_time:63509ms step_avg:46.49ms
step:1367/2150 train_time:63570ms step_avg:46.50ms
step:1368/2150 train_time:63630ms step_avg:46.51ms
step:1369/2150 train_time:63691ms step_avg:46.52ms
step:1370/2150 train_time:63751ms step_avg:46.53ms
step:1371/2150 train_time:63813ms step_avg:46.54ms
step:1372/2150 train_time:63873ms step_avg:46.55ms
step:1373/2150 train_time:63934ms step_avg:46.57ms
step:1374/2150 train_time:63994ms step_avg:46.58ms
step:1375/2150 train_time:64056ms step_avg:46.59ms
step:1376/2150 train_time:64115ms step_avg:46.60ms
step:1377/2150 train_time:64177ms step_avg:46.61ms
step:1378/2150 train_time:64238ms step_avg:46.62ms
step:1379/2150 train_time:64300ms step_avg:46.63ms
step:1380/2150 train_time:64359ms step_avg:46.64ms
step:1381/2150 train_time:64421ms step_avg:46.65ms
step:1382/2150 train_time:64480ms step_avg:46.66ms
step:1383/2150 train_time:64542ms step_avg:46.67ms
step:1384/2150 train_time:64601ms step_avg:46.68ms
step:1385/2150 train_time:64663ms step_avg:46.69ms
step:1386/2150 train_time:64722ms step_avg:46.70ms
step:1387/2150 train_time:64785ms step_avg:46.71ms
step:1388/2150 train_time:64844ms step_avg:46.72ms
step:1389/2150 train_time:64906ms step_avg:46.73ms
step:1390/2150 train_time:64967ms step_avg:46.74ms
step:1391/2150 train_time:65028ms step_avg:46.75ms
step:1392/2150 train_time:65088ms step_avg:46.76ms
step:1393/2150 train_time:65149ms step_avg:46.77ms
step:1394/2150 train_time:65208ms step_avg:46.78ms
step:1395/2150 train_time:65270ms step_avg:46.79ms
step:1396/2150 train_time:65329ms step_avg:46.80ms
step:1397/2150 train_time:65390ms step_avg:46.81ms
step:1398/2150 train_time:65450ms step_avg:46.82ms
step:1399/2150 train_time:65511ms step_avg:46.83ms
step:1400/2150 train_time:65571ms step_avg:46.84ms
step:1401/2150 train_time:65633ms step_avg:46.85ms
step:1402/2150 train_time:65693ms step_avg:46.86ms
step:1403/2150 train_time:65756ms step_avg:46.87ms
step:1404/2150 train_time:65815ms step_avg:46.88ms
step:1405/2150 train_time:65877ms step_avg:46.89ms
step:1406/2150 train_time:65937ms step_avg:46.90ms
step:1407/2150 train_time:65999ms step_avg:46.91ms
step:1408/2150 train_time:66060ms step_avg:46.92ms
step:1409/2150 train_time:66149ms step_avg:46.95ms
step:1410/2150 train_time:66237ms step_avg:46.98ms
step:1411/2150 train_time:66328ms step_avg:47.01ms
step:1412/2150 train_time:66416ms step_avg:47.04ms
step:1413/2150 train_time:66505ms step_avg:47.07ms
step:1414/2150 train_time:66594ms step_avg:47.10ms
step:1415/2150 train_time:66684ms step_avg:47.13ms
step:1416/2150 train_time:66772ms step_avg:47.16ms
step:1417/2150 train_time:66862ms step_avg:47.19ms
step:1418/2150 train_time:66950ms step_avg:47.21ms
step:1419/2150 train_time:67040ms step_avg:47.24ms
step:1420/2150 train_time:67129ms step_avg:47.27ms
step:1421/2150 train_time:67219ms step_avg:47.30ms
step:1422/2150 train_time:67307ms step_avg:47.33ms
step:1423/2150 train_time:67397ms step_avg:47.36ms
step:1424/2150 train_time:67486ms step_avg:47.39ms
step:1425/2150 train_time:67576ms step_avg:47.42ms
step:1426/2150 train_time:67663ms step_avg:47.45ms
step:1427/2150 train_time:67753ms step_avg:47.48ms
step:1428/2150 train_time:67842ms step_avg:47.51ms
step:1429/2150 train_time:67933ms step_avg:47.54ms
step:1430/2150 train_time:68021ms step_avg:47.57ms
step:1431/2150 train_time:68111ms step_avg:47.60ms
step:1432/2150 train_time:68199ms step_avg:47.63ms
step:1433/2150 train_time:68288ms step_avg:47.65ms
step:1434/2150 train_time:68378ms step_avg:47.68ms
step:1435/2150 train_time:68467ms step_avg:47.71ms
step:1436/2150 train_time:68554ms step_avg:47.74ms
step:1437/2150 train_time:68644ms step_avg:47.77ms
step:1438/2150 train_time:68732ms step_avg:47.80ms
step:1439/2150 train_time:68823ms step_avg:47.83ms
step:1440/2150 train_time:68911ms step_avg:47.86ms
step:1441/2150 train_time:69001ms step_avg:47.88ms
step:1442/2150 train_time:69088ms step_avg:47.91ms
step:1443/2150 train_time:69179ms step_avg:47.94ms
step:1444/2150 train_time:69268ms step_avg:47.97ms
step:1445/2150 train_time:69357ms step_avg:48.00ms
step:1446/2150 train_time:69446ms step_avg:48.03ms
step:1447/2150 train_time:69536ms step_avg:48.06ms
step:1448/2150 train_time:69624ms step_avg:48.08ms
step:1449/2150 train_time:69714ms step_avg:48.11ms
step:1450/2150 train_time:69802ms step_avg:48.14ms
step:1451/2150 train_time:69892ms step_avg:48.17ms
step:1452/2150 train_time:69980ms step_avg:48.20ms
step:1453/2150 train_time:70069ms step_avg:48.22ms
step:1454/2150 train_time:70156ms step_avg:48.25ms
step:1455/2150 train_time:70246ms step_avg:48.28ms
step:1456/2150 train_time:70333ms step_avg:48.31ms
step:1457/2150 train_time:70423ms step_avg:48.33ms
step:1458/2150 train_time:70512ms step_avg:48.36ms
step:1459/2150 train_time:70602ms step_avg:48.39ms
step:1460/2150 train_time:70690ms step_avg:48.42ms
step:1461/2150 train_time:70779ms step_avg:48.45ms
step:1462/2150 train_time:70867ms step_avg:48.47ms
step:1463/2150 train_time:70956ms step_avg:48.50ms
step:1464/2150 train_time:71044ms step_avg:48.53ms
step:1465/2150 train_time:71133ms step_avg:48.56ms
step:1466/2150 train_time:71221ms step_avg:48.58ms
step:1467/2150 train_time:71311ms step_avg:48.61ms
step:1468/2150 train_time:71400ms step_avg:48.64ms
step:1469/2150 train_time:71489ms step_avg:48.67ms
step:1470/2150 train_time:71577ms step_avg:48.69ms
step:1471/2150 train_time:71666ms step_avg:48.72ms
step:1472/2150 train_time:71754ms step_avg:48.75ms
step:1473/2150 train_time:71843ms step_avg:48.77ms
step:1474/2150 train_time:71932ms step_avg:48.80ms
step:1475/2150 train_time:72022ms step_avg:48.83ms
step:1476/2150 train_time:72110ms step_avg:48.86ms
step:1477/2150 train_time:72201ms step_avg:48.88ms
step:1478/2150 train_time:72289ms step_avg:48.91ms
step:1479/2150 train_time:72378ms step_avg:48.94ms
step:1480/2150 train_time:72466ms step_avg:48.96ms
step:1481/2150 train_time:72555ms step_avg:48.99ms
step:1482/2150 train_time:72644ms step_avg:49.02ms
step:1483/2150 train_time:72735ms step_avg:49.05ms
step:1484/2150 train_time:72824ms step_avg:49.07ms
step:1485/2150 train_time:72914ms step_avg:49.10ms
step:1486/2150 train_time:73001ms step_avg:49.13ms
step:1487/2150 train_time:73091ms step_avg:49.15ms
step:1488/2150 train_time:73179ms step_avg:49.18ms
step:1489/2150 train_time:73268ms step_avg:49.21ms
step:1490/2150 train_time:73355ms step_avg:49.23ms
step:1491/2150 train_time:73444ms step_avg:49.26ms
step:1492/2150 train_time:73533ms step_avg:49.28ms
step:1493/2150 train_time:73623ms step_avg:49.31ms
step:1494/2150 train_time:73712ms step_avg:49.34ms
step:1495/2150 train_time:73802ms step_avg:49.37ms
step:1496/2150 train_time:73890ms step_avg:49.39ms
step:1497/2150 train_time:73980ms step_avg:49.42ms
step:1498/2150 train_time:74068ms step_avg:49.44ms
step:1499/2150 train_time:74158ms step_avg:49.47ms
step:1500/2150 train_time:74246ms step_avg:49.50ms
step:1500/2150 val_loss:3.4925 train_time:74337ms step_avg:49.56ms
step:1501/2150 train_time:74359ms step_avg:49.54ms
step:1502/2150 train_time:74430ms step_avg:49.55ms
step:1503/2150 train_time:74523ms step_avg:49.58ms
step:1504/2150 train_time:74611ms step_avg:49.61ms
step:1505/2150 train_time:74699ms step_avg:49.63ms
step:1506/2150 train_time:74786ms step_avg:49.66ms
step:1507/2150 train_time:74874ms step_avg:49.68ms
step:1508/2150 train_time:74960ms step_avg:49.71ms
step:1509/2150 train_time:75048ms step_avg:49.73ms
step:1510/2150 train_time:75134ms step_avg:49.76ms
step:1511/2150 train_time:75223ms step_avg:49.78ms
step:1512/2150 train_time:75315ms step_avg:49.81ms
step:1513/2150 train_time:75410ms step_avg:49.84ms
step:1514/2150 train_time:75500ms step_avg:49.87ms
step:1515/2150 train_time:75591ms step_avg:49.89ms
step:1516/2150 train_time:75678ms step_avg:49.92ms
step:1517/2150 train_time:75767ms step_avg:49.95ms
step:1518/2150 train_time:75854ms step_avg:49.97ms
step:1519/2150 train_time:75942ms step_avg:49.99ms
step:1520/2150 train_time:76028ms step_avg:50.02ms
step:1521/2150 train_time:76115ms step_avg:50.04ms
step:1522/2150 train_time:76203ms step_avg:50.07ms
step:1523/2150 train_time:76293ms step_avg:50.09ms
step:1524/2150 train_time:76384ms step_avg:50.12ms
step:1525/2150 train_time:76474ms step_avg:50.15ms
step:1526/2150 train_time:76563ms step_avg:50.17ms
step:1527/2150 train_time:76653ms step_avg:50.20ms
step:1528/2150 train_time:76740ms step_avg:50.22ms
step:1529/2150 train_time:76829ms step_avg:50.25ms
step:1530/2150 train_time:76916ms step_avg:50.27ms
step:1531/2150 train_time:77005ms step_avg:50.30ms
step:1532/2150 train_time:77091ms step_avg:50.32ms
step:1533/2150 train_time:77180ms step_avg:50.35ms
step:1534/2150 train_time:77268ms step_avg:50.37ms
step:1535/2150 train_time:77358ms step_avg:50.40ms
step:1536/2150 train_time:77447ms step_avg:50.42ms
step:1537/2150 train_time:77538ms step_avg:50.45ms
step:1538/2150 train_time:77628ms step_avg:50.47ms
step:1539/2150 train_time:77717ms step_avg:50.50ms
step:1540/2150 train_time:77804ms step_avg:50.52ms
step:1541/2150 train_time:77894ms step_avg:50.55ms
step:1542/2150 train_time:77980ms step_avg:50.57ms
step:1543/2150 train_time:78068ms step_avg:50.60ms
step:1544/2150 train_time:78156ms step_avg:50.62ms
step:1545/2150 train_time:78244ms step_avg:50.64ms
step:1546/2150 train_time:78333ms step_avg:50.67ms
step:1547/2150 train_time:78423ms step_avg:50.69ms
step:1548/2150 train_time:78510ms step_avg:50.72ms
step:1549/2150 train_time:78600ms step_avg:50.74ms
step:1550/2150 train_time:78688ms step_avg:50.77ms
step:1551/2150 train_time:78778ms step_avg:50.79ms
step:1552/2150 train_time:78865ms step_avg:50.82ms
step:1553/2150 train_time:78954ms step_avg:50.84ms
step:1554/2150 train_time:79041ms step_avg:50.86ms
step:1555/2150 train_time:79131ms step_avg:50.89ms
step:1556/2150 train_time:79219ms step_avg:50.91ms
step:1557/2150 train_time:79309ms step_avg:50.94ms
step:1558/2150 train_time:79397ms step_avg:50.96ms
step:1559/2150 train_time:79487ms step_avg:50.99ms
step:1560/2150 train_time:79575ms step_avg:51.01ms
step:1561/2150 train_time:79664ms step_avg:51.03ms
step:1562/2150 train_time:79751ms step_avg:51.06ms
step:1563/2150 train_time:79840ms step_avg:51.08ms
step:1564/2150 train_time:79928ms step_avg:51.10ms
step:1565/2150 train_time:80016ms step_avg:51.13ms
step:1566/2150 train_time:80103ms step_avg:51.15ms
step:1567/2150 train_time:80192ms step_avg:51.18ms
step:1568/2150 train_time:80281ms step_avg:51.20ms
step:1569/2150 train_time:80370ms step_avg:51.22ms
step:1570/2150 train_time:80459ms step_avg:51.25ms
step:1571/2150 train_time:80548ms step_avg:51.27ms
step:1572/2150 train_time:80636ms step_avg:51.30ms
step:1573/2150 train_time:80726ms step_avg:51.32ms
step:1574/2150 train_time:80814ms step_avg:51.34ms
step:1575/2150 train_time:80903ms step_avg:51.37ms
step:1576/2150 train_time:80989ms step_avg:51.39ms
step:1577/2150 train_time:81078ms step_avg:51.41ms
step:1578/2150 train_time:81165ms step_avg:51.44ms
step:1579/2150 train_time:81254ms step_avg:51.46ms
step:1580/2150 train_time:81343ms step_avg:51.48ms
step:1581/2150 train_time:81432ms step_avg:51.51ms
step:1582/2150 train_time:81521ms step_avg:51.53ms
step:1583/2150 train_time:81611ms step_avg:51.55ms
step:1584/2150 train_time:81700ms step_avg:51.58ms
step:1585/2150 train_time:81790ms step_avg:51.60ms
step:1586/2150 train_time:81877ms step_avg:51.63ms
step:1587/2150 train_time:81966ms step_avg:51.65ms
step:1588/2150 train_time:82054ms step_avg:51.67ms
step:1589/2150 train_time:82143ms step_avg:51.69ms
step:1590/2150 train_time:82229ms step_avg:51.72ms
step:1591/2150 train_time:82318ms step_avg:51.74ms
step:1592/2150 train_time:82406ms step_avg:51.76ms
step:1593/2150 train_time:82495ms step_avg:51.79ms
step:1594/2150 train_time:82583ms step_avg:51.81ms
step:1595/2150 train_time:82673ms step_avg:51.83ms
step:1596/2150 train_time:82761ms step_avg:51.86ms
step:1597/2150 train_time:82850ms step_avg:51.88ms
step:1598/2150 train_time:82937ms step_avg:51.90ms
step:1599/2150 train_time:83026ms step_avg:51.92ms
step:1600/2150 train_time:83114ms step_avg:51.95ms
step:1601/2150 train_time:83203ms step_avg:51.97ms
step:1602/2150 train_time:83290ms step_avg:51.99ms
step:1603/2150 train_time:83380ms step_avg:52.02ms
step:1604/2150 train_time:83468ms step_avg:52.04ms
step:1605/2150 train_time:83558ms step_avg:52.06ms
step:1606/2150 train_time:83646ms step_avg:52.08ms
step:1607/2150 train_time:83736ms step_avg:52.11ms
step:1608/2150 train_time:83824ms step_avg:52.13ms
step:1609/2150 train_time:83913ms step_avg:52.15ms
step:1610/2150 train_time:84000ms step_avg:52.17ms
step:1611/2150 train_time:84090ms step_avg:52.20ms
step:1612/2150 train_time:84177ms step_avg:52.22ms
step:1613/2150 train_time:84266ms step_avg:52.24ms
step:1614/2150 train_time:84353ms step_avg:52.26ms
step:1615/2150 train_time:84442ms step_avg:52.29ms
step:1616/2150 train_time:84531ms step_avg:52.31ms
step:1617/2150 train_time:84621ms step_avg:52.33ms
step:1618/2150 train_time:84708ms step_avg:52.35ms
step:1619/2150 train_time:84798ms step_avg:52.38ms
step:1620/2150 train_time:84887ms step_avg:52.40ms
step:1621/2150 train_time:84976ms step_avg:52.42ms
step:1622/2150 train_time:85064ms step_avg:52.44ms
step:1623/2150 train_time:85153ms step_avg:52.47ms
step:1624/2150 train_time:85239ms step_avg:52.49ms
step:1625/2150 train_time:85328ms step_avg:52.51ms
step:1626/2150 train_time:85416ms step_avg:52.53ms
step:1627/2150 train_time:85505ms step_avg:52.55ms
step:1628/2150 train_time:85594ms step_avg:52.58ms
step:1629/2150 train_time:85683ms step_avg:52.60ms
step:1630/2150 train_time:85770ms step_avg:52.62ms
step:1631/2150 train_time:85860ms step_avg:52.64ms
step:1632/2150 train_time:85947ms step_avg:52.66ms
step:1633/2150 train_time:86036ms step_avg:52.69ms
step:1634/2150 train_time:86124ms step_avg:52.71ms
step:1635/2150 train_time:86212ms step_avg:52.73ms
step:1636/2150 train_time:86299ms step_avg:52.75ms
step:1637/2150 train_time:86389ms step_avg:52.77ms
step:1638/2150 train_time:86477ms step_avg:52.79ms
step:1639/2150 train_time:86567ms step_avg:52.82ms
step:1640/2150 train_time:86655ms step_avg:52.84ms
step:1641/2150 train_time:86745ms step_avg:52.86ms
step:1642/2150 train_time:86833ms step_avg:52.88ms
step:1643/2150 train_time:86922ms step_avg:52.90ms
step:1644/2150 train_time:87010ms step_avg:52.93ms
step:1645/2150 train_time:87099ms step_avg:52.95ms
step:1646/2150 train_time:87187ms step_avg:52.97ms
step:1647/2150 train_time:87276ms step_avg:52.99ms
step:1648/2150 train_time:87364ms step_avg:53.01ms
step:1649/2150 train_time:87453ms step_avg:53.03ms
step:1650/2150 train_time:87540ms step_avg:53.05ms
step:1651/2150 train_time:87631ms step_avg:53.08ms
step:1652/2150 train_time:87718ms step_avg:53.10ms
step:1653/2150 train_time:87808ms step_avg:53.12ms
step:1654/2150 train_time:87895ms step_avg:53.14ms
step:1655/2150 train_time:87986ms step_avg:53.16ms
step:1656/2150 train_time:88073ms step_avg:53.18ms
step:1657/2150 train_time:88162ms step_avg:53.21ms
step:1658/2150 train_time:88250ms step_avg:53.23ms
step:1659/2150 train_time:88339ms step_avg:53.25ms
step:1660/2150 train_time:88427ms step_avg:53.27ms
step:1661/2150 train_time:88516ms step_avg:53.29ms
step:1662/2150 train_time:88604ms step_avg:53.31ms
step:1663/2150 train_time:88693ms step_avg:53.33ms
step:1664/2150 train_time:88781ms step_avg:53.35ms
step:1665/2150 train_time:88870ms step_avg:53.38ms
step:1666/2150 train_time:88958ms step_avg:53.40ms
step:1667/2150 train_time:89048ms step_avg:53.42ms
step:1668/2150 train_time:89135ms step_avg:53.44ms
step:1669/2150 train_time:89225ms step_avg:53.46ms
step:1670/2150 train_time:89313ms step_avg:53.48ms
step:1671/2150 train_time:89402ms step_avg:53.50ms
step:1672/2150 train_time:89490ms step_avg:53.52ms
step:1673/2150 train_time:89579ms step_avg:53.54ms
step:1674/2150 train_time:89666ms step_avg:53.56ms
step:1675/2150 train_time:89757ms step_avg:53.59ms
step:1676/2150 train_time:89845ms step_avg:53.61ms
step:1677/2150 train_time:89935ms step_avg:53.63ms
step:1678/2150 train_time:90023ms step_avg:53.65ms
step:1679/2150 train_time:90112ms step_avg:53.67ms
step:1680/2150 train_time:90199ms step_avg:53.69ms
step:1681/2150 train_time:90289ms step_avg:53.71ms
step:1682/2150 train_time:90378ms step_avg:53.73ms
step:1683/2150 train_time:90467ms step_avg:53.75ms
step:1684/2150 train_time:90555ms step_avg:53.77ms
step:1685/2150 train_time:90644ms step_avg:53.79ms
step:1686/2150 train_time:90733ms step_avg:53.82ms
step:1687/2150 train_time:90822ms step_avg:53.84ms
step:1688/2150 train_time:90910ms step_avg:53.86ms
step:1689/2150 train_time:90999ms step_avg:53.88ms
step:1690/2150 train_time:91087ms step_avg:53.90ms
step:1691/2150 train_time:91176ms step_avg:53.92ms
step:1692/2150 train_time:91264ms step_avg:53.94ms
step:1693/2150 train_time:91354ms step_avg:53.96ms
step:1694/2150 train_time:91442ms step_avg:53.98ms
step:1695/2150 train_time:91531ms step_avg:54.00ms
step:1696/2150 train_time:91618ms step_avg:54.02ms
step:1697/2150 train_time:91708ms step_avg:54.04ms
step:1698/2150 train_time:91796ms step_avg:54.06ms
step:1699/2150 train_time:91885ms step_avg:54.08ms
step:1700/2150 train_time:91972ms step_avg:54.10ms
step:1701/2150 train_time:92061ms step_avg:54.12ms
step:1702/2150 train_time:92149ms step_avg:54.14ms
step:1703/2150 train_time:92238ms step_avg:54.16ms
step:1704/2150 train_time:92327ms step_avg:54.18ms
step:1705/2150 train_time:92416ms step_avg:54.20ms
step:1706/2150 train_time:92504ms step_avg:54.22ms
step:1707/2150 train_time:92593ms step_avg:54.24ms
step:1708/2150 train_time:92681ms step_avg:54.26ms
step:1709/2150 train_time:92770ms step_avg:54.28ms
step:1710/2150 train_time:92858ms step_avg:54.30ms
step:1711/2150 train_time:92946ms step_avg:54.32ms
step:1712/2150 train_time:93034ms step_avg:54.34ms
step:1713/2150 train_time:93123ms step_avg:54.36ms
step:1714/2150 train_time:93210ms step_avg:54.38ms
step:1715/2150 train_time:93300ms step_avg:54.40ms
step:1716/2150 train_time:93388ms step_avg:54.42ms
step:1717/2150 train_time:93477ms step_avg:54.44ms
step:1718/2150 train_time:93565ms step_avg:54.46ms
step:1719/2150 train_time:93655ms step_avg:54.48ms
step:1720/2150 train_time:93743ms step_avg:54.50ms
step:1721/2150 train_time:93833ms step_avg:54.52ms
step:1722/2150 train_time:93920ms step_avg:54.54ms
step:1723/2150 train_time:94009ms step_avg:54.56ms
step:1724/2150 train_time:94097ms step_avg:54.58ms
step:1725/2150 train_time:94187ms step_avg:54.60ms
step:1726/2150 train_time:94274ms step_avg:54.62ms
step:1727/2150 train_time:94364ms step_avg:54.64ms
step:1728/2150 train_time:94452ms step_avg:54.66ms
step:1729/2150 train_time:94542ms step_avg:54.68ms
step:1730/2150 train_time:94629ms step_avg:54.70ms
step:1731/2150 train_time:94719ms step_avg:54.72ms
step:1732/2150 train_time:94808ms step_avg:54.74ms
step:1733/2150 train_time:94898ms step_avg:54.76ms
step:1734/2150 train_time:94986ms step_avg:54.78ms
step:1735/2150 train_time:95075ms step_avg:54.80ms
step:1736/2150 train_time:95163ms step_avg:54.82ms
step:1737/2150 train_time:95252ms step_avg:54.84ms
step:1738/2150 train_time:95339ms step_avg:54.86ms
step:1739/2150 train_time:95429ms step_avg:54.88ms
step:1740/2150 train_time:95516ms step_avg:54.89ms
step:1741/2150 train_time:95605ms step_avg:54.91ms
step:1742/2150 train_time:95694ms step_avg:54.93ms
step:1743/2150 train_time:95783ms step_avg:54.95ms
step:1744/2150 train_time:95871ms step_avg:54.97ms
step:1745/2150 train_time:95961ms step_avg:54.99ms
step:1746/2150 train_time:96048ms step_avg:55.01ms
step:1747/2150 train_time:96137ms step_avg:55.03ms
step:1748/2150 train_time:96225ms step_avg:55.05ms
step:1749/2150 train_time:96314ms step_avg:55.07ms
step:1750/2150 train_time:96402ms step_avg:55.09ms
step:1750/2150 val_loss:3.3908 train_time:96494ms step_avg:55.14ms
step:1751/2150 train_time:96515ms step_avg:55.12ms
step:1752/2150 train_time:96584ms step_avg:55.13ms
step:1753/2150 train_time:96678ms step_avg:55.15ms
step:1754/2150 train_time:96767ms step_avg:55.17ms
step:1755/2150 train_time:96856ms step_avg:55.19ms
step:1756/2150 train_time:96942ms step_avg:55.21ms
step:1757/2150 train_time:97030ms step_avg:55.22ms
step:1758/2150 train_time:97116ms step_avg:55.24ms
step:1759/2150 train_time:97204ms step_avg:55.26ms
step:1760/2150 train_time:97290ms step_avg:55.28ms
step:1761/2150 train_time:97379ms step_avg:55.30ms
step:1762/2150 train_time:97468ms step_avg:55.32ms
step:1763/2150 train_time:97561ms step_avg:55.34ms
step:1764/2150 train_time:97651ms step_avg:55.36ms
step:1765/2150 train_time:97742ms step_avg:55.38ms
step:1766/2150 train_time:97830ms step_avg:55.40ms
step:1767/2150 train_time:97921ms step_avg:55.42ms
step:1768/2150 train_time:98009ms step_avg:55.43ms
step:1769/2150 train_time:98097ms step_avg:55.45ms
step:1770/2150 train_time:98183ms step_avg:55.47ms
step:1771/2150 train_time:98271ms step_avg:55.49ms
step:1772/2150 train_time:98357ms step_avg:55.51ms
step:1773/2150 train_time:98448ms step_avg:55.53ms
step:1774/2150 train_time:98536ms step_avg:55.54ms
step:1775/2150 train_time:98629ms step_avg:55.57ms
step:1776/2150 train_time:98717ms step_avg:55.58ms
step:1777/2150 train_time:98807ms step_avg:55.60ms
step:1778/2150 train_time:98895ms step_avg:55.62ms
step:1779/2150 train_time:98984ms step_avg:55.64ms
step:1780/2150 train_time:99072ms step_avg:55.66ms
step:1781/2150 train_time:99161ms step_avg:55.68ms
step:1782/2150 train_time:99247ms step_avg:55.69ms
step:1783/2150 train_time:99336ms step_avg:55.71ms
step:1784/2150 train_time:99425ms step_avg:55.73ms
step:1785/2150 train_time:99514ms step_avg:55.75ms
step:1786/2150 train_time:99603ms step_avg:55.77ms
step:1787/2150 train_time:99694ms step_avg:55.79ms
step:1788/2150 train_time:99782ms step_avg:55.81ms
step:1789/2150 train_time:99871ms step_avg:55.82ms
step:1790/2150 train_time:99958ms step_avg:55.84ms
step:1791/2150 train_time:100047ms step_avg:55.86ms
step:1792/2150 train_time:100135ms step_avg:55.88ms
step:1793/2150 train_time:100223ms step_avg:55.90ms
step:1794/2150 train_time:100309ms step_avg:55.91ms
step:1795/2150 train_time:100399ms step_avg:55.93ms
step:1796/2150 train_time:100487ms step_avg:55.95ms
step:1797/2150 train_time:100577ms step_avg:55.97ms
step:1798/2150 train_time:100665ms step_avg:55.99ms
step:1799/2150 train_time:100755ms step_avg:56.01ms
step:1800/2150 train_time:100843ms step_avg:56.02ms
step:1801/2150 train_time:100932ms step_avg:56.04ms
step:1802/2150 train_time:101019ms step_avg:56.06ms
step:1803/2150 train_time:101107ms step_avg:56.08ms
step:1804/2150 train_time:101194ms step_avg:56.09ms
step:1805/2150 train_time:101283ms step_avg:56.11ms
step:1806/2150 train_time:101370ms step_avg:56.13ms
step:1807/2150 train_time:101460ms step_avg:56.15ms
step:1808/2150 train_time:101548ms step_avg:56.17ms
step:1809/2150 train_time:101638ms step_avg:56.18ms
step:1810/2150 train_time:101727ms step_avg:56.20ms
step:1811/2150 train_time:101817ms step_avg:56.22ms
step:1812/2150 train_time:101905ms step_avg:56.24ms
step:1813/2150 train_time:101994ms step_avg:56.26ms
step:1814/2150 train_time:102083ms step_avg:56.28ms
step:1815/2150 train_time:102171ms step_avg:56.29ms
step:1816/2150 train_time:102258ms step_avg:56.31ms
step:1817/2150 train_time:102347ms step_avg:56.33ms
step:1818/2150 train_time:102435ms step_avg:56.34ms
step:1819/2150 train_time:102524ms step_avg:56.36ms
step:1820/2150 train_time:102612ms step_avg:56.38ms
step:1821/2150 train_time:102702ms step_avg:56.40ms
step:1822/2150 train_time:102791ms step_avg:56.42ms
step:1823/2150 train_time:102880ms step_avg:56.43ms
step:1824/2150 train_time:102967ms step_avg:56.45ms
step:1825/2150 train_time:103058ms step_avg:56.47ms
step:1826/2150 train_time:103145ms step_avg:56.49ms
step:1827/2150 train_time:103234ms step_avg:56.50ms
step:1828/2150 train_time:103320ms step_avg:56.52ms
step:1829/2150 train_time:103409ms step_avg:56.54ms
step:1830/2150 train_time:103497ms step_avg:56.56ms
step:1831/2150 train_time:103587ms step_avg:56.57ms
step:1832/2150 train_time:103675ms step_avg:56.59ms
step:1833/2150 train_time:103765ms step_avg:56.61ms
step:1834/2150 train_time:103853ms step_avg:56.63ms
step:1835/2150 train_time:103941ms step_avg:56.64ms
step:1836/2150 train_time:104029ms step_avg:56.66ms
step:1837/2150 train_time:104118ms step_avg:56.68ms
step:1838/2150 train_time:104206ms step_avg:56.70ms
step:1839/2150 train_time:104294ms step_avg:56.71ms
step:1840/2150 train_time:104382ms step_avg:56.73ms
step:1841/2150 train_time:104472ms step_avg:56.75ms
step:1842/2150 train_time:104560ms step_avg:56.76ms
step:1843/2150 train_time:104650ms step_avg:56.78ms
step:1844/2150 train_time:104739ms step_avg:56.80ms
step:1845/2150 train_time:104829ms step_avg:56.82ms
step:1846/2150 train_time:104917ms step_avg:56.83ms
step:1847/2150 train_time:105006ms step_avg:56.85ms
step:1848/2150 train_time:105094ms step_avg:56.87ms
step:1849/2150 train_time:105183ms step_avg:56.89ms
step:1850/2150 train_time:105269ms step_avg:56.90ms
step:1851/2150 train_time:105358ms step_avg:56.92ms
step:1852/2150 train_time:105447ms step_avg:56.94ms
step:1853/2150 train_time:105535ms step_avg:56.95ms
step:1854/2150 train_time:105623ms step_avg:56.97ms
step:1855/2150 train_time:105713ms step_avg:56.99ms
step:1856/2150 train_time:105801ms step_avg:57.00ms
step:1857/2150 train_time:105890ms step_avg:57.02ms
step:1858/2150 train_time:105979ms step_avg:57.04ms
step:1859/2150 train_time:106069ms step_avg:57.06ms
step:1860/2150 train_time:106157ms step_avg:57.07ms
step:1861/2150 train_time:106247ms step_avg:57.09ms
step:1862/2150 train_time:106334ms step_avg:57.11ms
step:1863/2150 train_time:106423ms step_avg:57.12ms
step:1864/2150 train_time:106511ms step_avg:57.14ms
step:1865/2150 train_time:106601ms step_avg:57.16ms
step:1866/2150 train_time:106689ms step_avg:57.18ms
step:1867/2150 train_time:106779ms step_avg:57.19ms
step:1868/2150 train_time:106866ms step_avg:57.21ms
step:1869/2150 train_time:106956ms step_avg:57.23ms
step:1870/2150 train_time:107044ms step_avg:57.24ms
step:1871/2150 train_time:107133ms step_avg:57.26ms
step:1872/2150 train_time:107220ms step_avg:57.28ms
step:1873/2150 train_time:107309ms step_avg:57.29ms
step:1874/2150 train_time:107398ms step_avg:57.31ms
step:1875/2150 train_time:107488ms step_avg:57.33ms
step:1876/2150 train_time:107576ms step_avg:57.34ms
step:1877/2150 train_time:107665ms step_avg:57.36ms
step:1878/2150 train_time:107753ms step_avg:57.38ms
step:1879/2150 train_time:107841ms step_avg:57.39ms
step:1880/2150 train_time:107928ms step_avg:57.41ms
step:1881/2150 train_time:108018ms step_avg:57.43ms
step:1882/2150 train_time:108106ms step_avg:57.44ms
step:1883/2150 train_time:108195ms step_avg:57.46ms
step:1884/2150 train_time:108282ms step_avg:57.47ms
step:1885/2150 train_time:108371ms step_avg:57.49ms
step:1886/2150 train_time:108459ms step_avg:57.51ms
step:1887/2150 train_time:108549ms step_avg:57.52ms
step:1888/2150 train_time:108637ms step_avg:57.54ms
step:1889/2150 train_time:108727ms step_avg:57.56ms
step:1890/2150 train_time:108815ms step_avg:57.57ms
step:1891/2150 train_time:108904ms step_avg:57.59ms
step:1892/2150 train_time:108991ms step_avg:57.61ms
step:1893/2150 train_time:109080ms step_avg:57.62ms
step:1894/2150 train_time:109167ms step_avg:57.64ms
step:1895/2150 train_time:109257ms step_avg:57.66ms
step:1896/2150 train_time:109345ms step_avg:57.67ms
step:1897/2150 train_time:109434ms step_avg:57.69ms
step:1898/2150 train_time:109522ms step_avg:57.70ms
step:1899/2150 train_time:109610ms step_avg:57.72ms
step:1900/2150 train_time:109698ms step_avg:57.74ms
step:1901/2150 train_time:109787ms step_avg:57.75ms
step:1902/2150 train_time:109875ms step_avg:57.77ms
step:1903/2150 train_time:109964ms step_avg:57.78ms
step:1904/2150 train_time:110052ms step_avg:57.80ms
step:1905/2150 train_time:110141ms step_avg:57.82ms
step:1906/2150 train_time:110229ms step_avg:57.83ms
step:1907/2150 train_time:110319ms step_avg:57.85ms
step:1908/2150 train_time:110408ms step_avg:57.87ms
step:1909/2150 train_time:110497ms step_avg:57.88ms
step:1910/2150 train_time:110585ms step_avg:57.90ms
step:1911/2150 train_time:110674ms step_avg:57.91ms
step:1912/2150 train_time:110762ms step_avg:57.93ms
step:1913/2150 train_time:110850ms step_avg:57.95ms
step:1914/2150 train_time:110938ms step_avg:57.96ms
step:1915/2150 train_time:111027ms step_avg:57.98ms
step:1916/2150 train_time:111116ms step_avg:57.99ms
step:1917/2150 train_time:111204ms step_avg:58.01ms
step:1918/2150 train_time:111291ms step_avg:58.02ms
step:1919/2150 train_time:111380ms step_avg:58.04ms
step:1920/2150 train_time:111467ms step_avg:58.06ms
step:1921/2150 train_time:111557ms step_avg:58.07ms
step:1922/2150 train_time:111644ms step_avg:58.09ms
step:1923/2150 train_time:111732ms step_avg:58.10ms
step:1924/2150 train_time:111820ms step_avg:58.12ms
step:1925/2150 train_time:111909ms step_avg:58.13ms
step:1926/2150 train_time:111997ms step_avg:58.15ms
step:1927/2150 train_time:112087ms step_avg:58.17ms
step:1928/2150 train_time:112175ms step_avg:58.18ms
step:1929/2150 train_time:112264ms step_avg:58.20ms
step:1930/2150 train_time:112352ms step_avg:58.21ms
step:1931/2150 train_time:112441ms step_avg:58.23ms
step:1932/2150 train_time:112530ms step_avg:58.25ms
step:1933/2150 train_time:112620ms step_avg:58.26ms
step:1934/2150 train_time:112707ms step_avg:58.28ms
step:1935/2150 train_time:112796ms step_avg:58.29ms
step:1936/2150 train_time:112883ms step_avg:58.31ms
step:1937/2150 train_time:112972ms step_avg:58.32ms
step:1938/2150 train_time:113060ms step_avg:58.34ms
step:1939/2150 train_time:113150ms step_avg:58.35ms
step:1940/2150 train_time:113238ms step_avg:58.37ms
step:1941/2150 train_time:113328ms step_avg:58.39ms
step:1942/2150 train_time:113416ms step_avg:58.40ms
step:1943/2150 train_time:113506ms step_avg:58.42ms
step:1944/2150 train_time:113593ms step_avg:58.43ms
step:1945/2150 train_time:113682ms step_avg:58.45ms
step:1946/2150 train_time:113770ms step_avg:58.46ms
step:1947/2150 train_time:113860ms step_avg:58.48ms
step:1948/2150 train_time:113948ms step_avg:58.49ms
step:1949/2150 train_time:114037ms step_avg:58.51ms
step:1950/2150 train_time:114126ms step_avg:58.53ms
step:1951/2150 train_time:114216ms step_avg:58.54ms
step:1952/2150 train_time:114303ms step_avg:58.56ms
step:1953/2150 train_time:114393ms step_avg:58.57ms
step:1954/2150 train_time:114480ms step_avg:58.59ms
step:1955/2150 train_time:114570ms step_avg:58.60ms
step:1956/2150 train_time:114658ms step_avg:58.62ms
step:1957/2150 train_time:114748ms step_avg:58.63ms
step:1958/2150 train_time:114836ms step_avg:58.65ms
step:1959/2150 train_time:114925ms step_avg:58.67ms
step:1960/2150 train_time:115013ms step_avg:58.68ms
step:1961/2150 train_time:115102ms step_avg:58.70ms
step:1962/2150 train_time:115190ms step_avg:58.71ms
step:1963/2150 train_time:115280ms step_avg:58.73ms
step:1964/2150 train_time:115367ms step_avg:58.74ms
step:1965/2150 train_time:115455ms step_avg:58.76ms
step:1966/2150 train_time:115543ms step_avg:58.77ms
step:1967/2150 train_time:115633ms step_avg:58.79ms
step:1968/2150 train_time:115721ms step_avg:58.80ms
step:1969/2150 train_time:115810ms step_avg:58.82ms
step:1970/2150 train_time:115898ms step_avg:58.83ms
step:1971/2150 train_time:115987ms step_avg:58.85ms
step:1972/2150 train_time:116075ms step_avg:58.86ms
step:1973/2150 train_time:116164ms step_avg:58.88ms
step:1974/2150 train_time:116252ms step_avg:58.89ms
step:1975/2150 train_time:116341ms step_avg:58.91ms
step:1976/2150 train_time:116429ms step_avg:58.92ms
step:1977/2150 train_time:116518ms step_avg:58.94ms
step:1978/2150 train_time:116605ms step_avg:58.95ms
step:1979/2150 train_time:116695ms step_avg:58.97ms
step:1980/2150 train_time:116783ms step_avg:58.98ms
step:1981/2150 train_time:116872ms step_avg:59.00ms
step:1982/2150 train_time:116959ms step_avg:59.01ms
step:1983/2150 train_time:117047ms step_avg:59.03ms
step:1984/2150 train_time:117135ms step_avg:59.04ms
step:1985/2150 train_time:117226ms step_avg:59.06ms
step:1986/2150 train_time:117313ms step_avg:59.07ms
step:1987/2150 train_time:117402ms step_avg:59.08ms
step:1988/2150 train_time:117489ms step_avg:59.10ms
step:1989/2150 train_time:117578ms step_avg:59.11ms
step:1990/2150 train_time:117666ms step_avg:59.13ms
step:1991/2150 train_time:117756ms step_avg:59.14ms
step:1992/2150 train_time:117844ms step_avg:59.16ms
step:1993/2150 train_time:117932ms step_avg:59.17ms
step:1994/2150 train_time:118020ms step_avg:59.19ms
step:1995/2150 train_time:118109ms step_avg:59.20ms
step:1996/2150 train_time:118197ms step_avg:59.22ms
step:1997/2150 train_time:118287ms step_avg:59.23ms
step:1998/2150 train_time:118375ms step_avg:59.25ms
step:1999/2150 train_time:118463ms step_avg:59.26ms
step:2000/2150 train_time:118551ms step_avg:59.28ms
step:2000/2150 val_loss:3.3145 train_time:118643ms step_avg:59.32ms
step:2001/2150 train_time:118664ms step_avg:59.30ms
step:2002/2150 train_time:118733ms step_avg:59.31ms
step:2003/2150 train_time:118827ms step_avg:59.32ms
step:2004/2150 train_time:118915ms step_avg:59.34ms
step:2005/2150 train_time:119003ms step_avg:59.35ms
step:2006/2150 train_time:119090ms step_avg:59.37ms
step:2007/2150 train_time:119178ms step_avg:59.38ms
step:2008/2150 train_time:119265ms step_avg:59.39ms
step:2009/2150 train_time:119353ms step_avg:59.41ms
step:2010/2150 train_time:119440ms step_avg:59.42ms
step:2011/2150 train_time:119528ms step_avg:59.44ms
step:2012/2150 train_time:119617ms step_avg:59.45ms
step:2013/2150 train_time:119708ms step_avg:59.47ms
step:2014/2150 train_time:119798ms step_avg:59.48ms
step:2015/2150 train_time:119888ms step_avg:59.50ms
step:2016/2150 train_time:119976ms step_avg:59.51ms
step:2017/2150 train_time:120064ms step_avg:59.53ms
step:2018/2150 train_time:120151ms step_avg:59.54ms
step:2019/2150 train_time:120241ms step_avg:59.55ms
step:2020/2150 train_time:120328ms step_avg:59.57ms
step:2021/2150 train_time:120416ms step_avg:59.58ms
step:2022/2150 train_time:120503ms step_avg:59.60ms
step:2023/2150 train_time:120592ms step_avg:59.61ms
step:2024/2150 train_time:120680ms step_avg:59.62ms
step:2025/2150 train_time:120770ms step_avg:59.64ms
step:2026/2150 train_time:120859ms step_avg:59.65ms
step:2027/2150 train_time:120948ms step_avg:59.67ms
step:2028/2150 train_time:121037ms step_avg:59.68ms
step:2029/2150 train_time:121125ms step_avg:59.70ms
step:2030/2150 train_time:121213ms step_avg:59.71ms
step:2031/2150 train_time:121301ms step_avg:59.72ms
step:2032/2150 train_time:121388ms step_avg:59.74ms
step:2033/2150 train_time:121476ms step_avg:59.75ms
step:2034/2150 train_time:121564ms step_avg:59.77ms
step:2035/2150 train_time:121654ms step_avg:59.78ms
step:2036/2150 train_time:121743ms step_avg:59.80ms
step:2037/2150 train_time:121832ms step_avg:59.81ms
step:2038/2150 train_time:121921ms step_avg:59.82ms
step:2039/2150 train_time:122009ms step_avg:59.84ms
step:2040/2150 train_time:122097ms step_avg:59.85ms
step:2041/2150 train_time:122185ms step_avg:59.87ms
step:2042/2150 train_time:122272ms step_avg:59.88ms
step:2043/2150 train_time:122361ms step_avg:59.89ms
step:2044/2150 train_time:122449ms step_avg:59.91ms
step:2045/2150 train_time:122537ms step_avg:59.92ms
step:2046/2150 train_time:122625ms step_avg:59.93ms
step:2047/2150 train_time:122714ms step_avg:59.95ms
step:2048/2150 train_time:122802ms step_avg:59.96ms
step:2049/2150 train_time:122892ms step_avg:59.98ms
step:2050/2150 train_time:122980ms step_avg:59.99ms
step:2051/2150 train_time:123069ms step_avg:60.00ms
step:2052/2150 train_time:123156ms step_avg:60.02ms
step:2053/2150 train_time:123246ms step_avg:60.03ms
step:2054/2150 train_time:123333ms step_avg:60.05ms
step:2055/2150 train_time:123422ms step_avg:60.06ms
step:2056/2150 train_time:123509ms step_avg:60.07ms
step:2057/2150 train_time:123598ms step_avg:60.09ms
step:2058/2150 train_time:123686ms step_avg:60.10ms
step:2059/2150 train_time:123775ms step_avg:60.11ms
step:2060/2150 train_time:123863ms step_avg:60.13ms
step:2061/2150 train_time:123953ms step_avg:60.14ms
step:2062/2150 train_time:124041ms step_avg:60.16ms
step:2063/2150 train_time:124130ms step_avg:60.17ms
step:2064/2150 train_time:124217ms step_avg:60.18ms
step:2065/2150 train_time:124306ms step_avg:60.20ms
step:2066/2150 train_time:124393ms step_avg:60.21ms
step:2067/2150 train_time:124482ms step_avg:60.22ms
step:2068/2150 train_time:124569ms step_avg:60.24ms
step:2069/2150 train_time:124659ms step_avg:60.25ms
step:2070/2150 train_time:124747ms step_avg:60.26ms
step:2071/2150 train_time:124836ms step_avg:60.28ms
step:2072/2150 train_time:124924ms step_avg:60.29ms
step:2073/2150 train_time:125014ms step_avg:60.31ms
step:2074/2150 train_time:125101ms step_avg:60.32ms
step:2075/2150 train_time:125190ms step_avg:60.33ms
step:2076/2150 train_time:125278ms step_avg:60.35ms
step:2077/2150 train_time:125367ms step_avg:60.36ms
step:2078/2150 train_time:125454ms step_avg:60.37ms
step:2079/2150 train_time:125543ms step_avg:60.39ms
step:2080/2150 train_time:125631ms step_avg:60.40ms
step:2081/2150 train_time:125721ms step_avg:60.41ms
step:2082/2150 train_time:125808ms step_avg:60.43ms
step:2083/2150 train_time:125897ms step_avg:60.44ms
step:2084/2150 train_time:125985ms step_avg:60.45ms
step:2085/2150 train_time:126074ms step_avg:60.47ms
step:2086/2150 train_time:126161ms step_avg:60.48ms
step:2087/2150 train_time:126251ms step_avg:60.49ms
step:2088/2150 train_time:126339ms step_avg:60.51ms
step:2089/2150 train_time:126428ms step_avg:60.52ms
step:2090/2150 train_time:126515ms step_avg:60.53ms
step:2091/2150 train_time:126605ms step_avg:60.55ms
step:2092/2150 train_time:126692ms step_avg:60.56ms
step:2093/2150 train_time:126781ms step_avg:60.57ms
step:2094/2150 train_time:126869ms step_avg:60.59ms
step:2095/2150 train_time:126957ms step_avg:60.60ms
step:2096/2150 train_time:127045ms step_avg:60.61ms
step:2097/2150 train_time:127135ms step_avg:60.63ms
step:2098/2150 train_time:127223ms step_avg:60.64ms
step:2099/2150 train_time:127312ms step_avg:60.65ms
step:2100/2150 train_time:127400ms step_avg:60.67ms
step:2101/2150 train_time:127488ms step_avg:60.68ms
step:2102/2150 train_time:127576ms step_avg:60.69ms
step:2103/2150 train_time:127665ms step_avg:60.71ms
step:2104/2150 train_time:127753ms step_avg:60.72ms
step:2105/2150 train_time:127844ms step_avg:60.73ms
step:2106/2150 train_time:127932ms step_avg:60.75ms
step:2107/2150 train_time:128021ms step_avg:60.76ms
step:2108/2150 train_time:128109ms step_avg:60.77ms
step:2109/2150 train_time:128197ms step_avg:60.79ms
step:2110/2150 train_time:128285ms step_avg:60.80ms
step:2111/2150 train_time:128374ms step_avg:60.81ms
step:2112/2150 train_time:128462ms step_avg:60.82ms
step:2113/2150 train_time:128551ms step_avg:60.84ms
step:2114/2150 train_time:128638ms step_avg:60.85ms
step:2115/2150 train_time:128728ms step_avg:60.86ms
step:2116/2150 train_time:128816ms step_avg:60.88ms
step:2117/2150 train_time:128906ms step_avg:60.89ms
step:2118/2150 train_time:128994ms step_avg:60.90ms
step:2119/2150 train_time:129084ms step_avg:60.92ms
step:2120/2150 train_time:129172ms step_avg:60.93ms
step:2121/2150 train_time:129261ms step_avg:60.94ms
step:2122/2150 train_time:129350ms step_avg:60.96ms
step:2123/2150 train_time:129440ms step_avg:60.97ms
step:2124/2150 train_time:129527ms step_avg:60.98ms
step:2125/2150 train_time:129617ms step_avg:61.00ms
step:2126/2150 train_time:129705ms step_avg:61.01ms
step:2127/2150 train_time:129794ms step_avg:61.02ms
step:2128/2150 train_time:129882ms step_avg:61.03ms
step:2129/2150 train_time:129970ms step_avg:61.05ms
step:2130/2150 train_time:130058ms step_avg:61.06ms
step:2131/2150 train_time:130148ms step_avg:61.07ms
step:2132/2150 train_time:130236ms step_avg:61.09ms
step:2133/2150 train_time:130326ms step_avg:61.10ms
step:2134/2150 train_time:130413ms step_avg:61.11ms
step:2135/2150 train_time:130503ms step_avg:61.13ms
step:2136/2150 train_time:130590ms step_avg:61.14ms
step:2137/2150 train_time:130680ms step_avg:61.15ms
step:2138/2150 train_time:130768ms step_avg:61.16ms
step:2139/2150 train_time:130856ms step_avg:61.18ms
step:2140/2150 train_time:130944ms step_avg:61.19ms
step:2141/2150 train_time:131032ms step_avg:61.20ms
step:2142/2150 train_time:131120ms step_avg:61.21ms
step:2143/2150 train_time:131210ms step_avg:61.23ms
step:2144/2150 train_time:131297ms step_avg:61.24ms
step:2145/2150 train_time:131387ms step_avg:61.25ms
step:2146/2150 train_time:131474ms step_avg:61.26ms
step:2147/2150 train_time:131564ms step_avg:61.28ms
step:2148/2150 train_time:131652ms step_avg:61.29ms
step:2149/2150 train_time:131741ms step_avg:61.30ms
step:2150/2150 train_time:131829ms step_avg:61.32ms
step:2150/2150 val_loss:3.2801 train_time:131921ms step_avg:61.36ms
peak memory allocated: 29707 MiB reserved: 44596 MiB

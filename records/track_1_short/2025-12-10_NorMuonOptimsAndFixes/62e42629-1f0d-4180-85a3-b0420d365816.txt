import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Dec 10 2025, 18:15:56) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 18:30:40 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   36C    P0            119W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   31C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   36C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   35C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              75      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A              76      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              77      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              78      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              79      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              80      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              81      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A              76      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A              77      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A              78      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A              79      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A              80      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A              81      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A              82      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:113ms step_avg:112.79ms
step:2/2160 train_time:177ms step_avg:88.30ms
step:3/2160 train_time:202ms step_avg:67.39ms
step:4/2160 train_time:231ms step_avg:57.72ms
step:5/2160 train_time:255ms step_avg:51.09ms
step:6/2160 train_time:348ms step_avg:58.05ms
step:7/2160 train_time:375ms step_avg:53.52ms
step:8/2160 train_time:408ms step_avg:50.96ms
step:9/2160 train_time:441ms step_avg:49.04ms
step:10/2160 train_time:474ms step_avg:47.44ms
step:11/2160 train_time:509ms step_avg:46.24ms
step:12/2160 train_time:542ms step_avg:45.16ms
step:13/2160 train_time:576ms step_avg:44.30ms
step:14/2160 train_time:609ms step_avg:43.53ms
step:15/2160 train_time:643ms step_avg:42.89ms
step:16/2160 train_time:677ms step_avg:42.29ms
step:17/2160 train_time:711ms step_avg:41.82ms
step:18/2160 train_time:744ms step_avg:41.35ms
step:19/2160 train_time:778ms step_avg:40.97ms
step:20/2160 train_time:812ms step_avg:40.59ms
step:21/2160 train_time:846ms step_avg:40.30ms
step:22/2160 train_time:880ms step_avg:40.00ms
step:23/2160 train_time:914ms step_avg:39.73ms
step:24/2160 train_time:948ms step_avg:39.48ms
step:25/2160 train_time:981ms step_avg:39.25ms
step:26/2160 train_time:1014ms step_avg:39.01ms
step:27/2160 train_time:1049ms step_avg:38.84ms
step:28/2160 train_time:1082ms step_avg:38.64ms
step:29/2160 train_time:1116ms step_avg:38.48ms
step:30/2160 train_time:1149ms step_avg:38.31ms
step:31/2160 train_time:1184ms step_avg:38.18ms
step:32/2160 train_time:1217ms step_avg:38.03ms
step:33/2160 train_time:1252ms step_avg:37.93ms
step:34/2160 train_time:1286ms step_avg:37.82ms
step:35/2160 train_time:1322ms step_avg:37.77ms
step:36/2160 train_time:1356ms step_avg:37.67ms
step:37/2160 train_time:1392ms step_avg:37.62ms
step:38/2160 train_time:1425ms step_avg:37.50ms
step:39/2160 train_time:1460ms step_avg:37.43ms
step:40/2160 train_time:1493ms step_avg:37.33ms
step:41/2160 train_time:1528ms step_avg:37.26ms
step:42/2160 train_time:1561ms step_avg:37.18ms
step:43/2160 train_time:1595ms step_avg:37.10ms
step:44/2160 train_time:1629ms step_avg:37.02ms
step:45/2160 train_time:1663ms step_avg:36.96ms
step:46/2160 train_time:1696ms step_avg:36.88ms
step:47/2160 train_time:1731ms step_avg:36.83ms
step:48/2160 train_time:1765ms step_avg:36.76ms
step:49/2160 train_time:1799ms step_avg:36.71ms
step:50/2160 train_time:1832ms step_avg:36.64ms
step:51/2160 train_time:1867ms step_avg:36.60ms
step:52/2160 train_time:1900ms step_avg:36.54ms
step:53/2160 train_time:1934ms step_avg:36.48ms
step:54/2160 train_time:1967ms step_avg:36.43ms
step:55/2160 train_time:2001ms step_avg:36.39ms
step:56/2160 train_time:2034ms step_avg:36.33ms
step:57/2160 train_time:2069ms step_avg:36.29ms
step:58/2160 train_time:2102ms step_avg:36.24ms
step:59/2160 train_time:2136ms step_avg:36.20ms
step:60/2160 train_time:2169ms step_avg:36.15ms
step:61/2160 train_time:2204ms step_avg:36.14ms
step:62/2160 train_time:2238ms step_avg:36.09ms
step:63/2160 train_time:2273ms step_avg:36.08ms
step:64/2160 train_time:2306ms step_avg:36.03ms
step:65/2160 train_time:2341ms step_avg:36.02ms
step:66/2160 train_time:2375ms step_avg:35.98ms
step:67/2160 train_time:2410ms step_avg:35.96ms
step:68/2160 train_time:2443ms step_avg:35.92ms
step:69/2160 train_time:2477ms step_avg:35.91ms
step:70/2160 train_time:2511ms step_avg:35.87ms
step:71/2160 train_time:2546ms step_avg:35.85ms
step:72/2160 train_time:2579ms step_avg:35.82ms
step:73/2160 train_time:2614ms step_avg:35.80ms
step:74/2160 train_time:2647ms step_avg:35.77ms
step:75/2160 train_time:2682ms step_avg:35.76ms
step:76/2160 train_time:2715ms step_avg:35.72ms
step:77/2160 train_time:2749ms step_avg:35.71ms
step:78/2160 train_time:2783ms step_avg:35.68ms
step:79/2160 train_time:2817ms step_avg:35.66ms
step:80/2160 train_time:2850ms step_avg:35.63ms
step:81/2160 train_time:2885ms step_avg:35.62ms
step:82/2160 train_time:2918ms step_avg:35.58ms
step:83/2160 train_time:2952ms step_avg:35.57ms
step:84/2160 train_time:2985ms step_avg:35.54ms
step:85/2160 train_time:3019ms step_avg:35.52ms
step:86/2160 train_time:3053ms step_avg:35.50ms
step:87/2160 train_time:3087ms step_avg:35.49ms
step:88/2160 train_time:3120ms step_avg:35.46ms
step:89/2160 train_time:3155ms step_avg:35.44ms
step:90/2160 train_time:3188ms step_avg:35.42ms
step:91/2160 train_time:3222ms step_avg:35.41ms
step:92/2160 train_time:3256ms step_avg:35.39ms
step:93/2160 train_time:3290ms step_avg:35.38ms
step:94/2160 train_time:3324ms step_avg:35.36ms
step:95/2160 train_time:3358ms step_avg:35.35ms
step:96/2160 train_time:3391ms step_avg:35.33ms
step:97/2160 train_time:3426ms step_avg:35.32ms
step:98/2160 train_time:3460ms step_avg:35.30ms
step:99/2160 train_time:3494ms step_avg:35.29ms
step:100/2160 train_time:3528ms step_avg:35.28ms
step:101/2160 train_time:3561ms step_avg:35.26ms
step:102/2160 train_time:3595ms step_avg:35.24ms
step:103/2160 train_time:3629ms step_avg:35.23ms
step:104/2160 train_time:3662ms step_avg:35.21ms
step:105/2160 train_time:3696ms step_avg:35.20ms
step:106/2160 train_time:3729ms step_avg:35.18ms
step:107/2160 train_time:3763ms step_avg:35.17ms
step:108/2160 train_time:3796ms step_avg:35.15ms
step:109/2160 train_time:3831ms step_avg:35.14ms
step:110/2160 train_time:3864ms step_avg:35.13ms
step:111/2160 train_time:3898ms step_avg:35.12ms
step:112/2160 train_time:3931ms step_avg:35.10ms
step:113/2160 train_time:3966ms step_avg:35.10ms
step:114/2160 train_time:3999ms step_avg:35.08ms
step:115/2160 train_time:4033ms step_avg:35.07ms
step:116/2160 train_time:4066ms step_avg:35.06ms
step:117/2160 train_time:4100ms step_avg:35.05ms
step:118/2160 train_time:4134ms step_avg:35.03ms
step:119/2160 train_time:4168ms step_avg:35.03ms
step:120/2160 train_time:4202ms step_avg:35.01ms
step:121/2160 train_time:4236ms step_avg:35.00ms
step:122/2160 train_time:4275ms step_avg:35.04ms
step:123/2160 train_time:4303ms step_avg:34.98ms
step:124/2160 train_time:4336ms step_avg:34.97ms
step:125/2160 train_time:4371ms step_avg:34.97ms
step:126/2160 train_time:4404ms step_avg:34.95ms
step:127/2160 train_time:4438ms step_avg:34.95ms
step:128/2160 train_time:4472ms step_avg:34.93ms
step:129/2160 train_time:4507ms step_avg:34.94ms
step:130/2160 train_time:4540ms step_avg:34.92ms
step:131/2160 train_time:4574ms step_avg:34.92ms
step:132/2160 train_time:4607ms step_avg:34.91ms
step:133/2160 train_time:4642ms step_avg:34.90ms
step:134/2160 train_time:4675ms step_avg:34.89ms
step:135/2160 train_time:4709ms step_avg:34.88ms
step:136/2160 train_time:4743ms step_avg:34.87ms
step:137/2160 train_time:4777ms step_avg:34.87ms
step:138/2160 train_time:4810ms step_avg:34.85ms
step:139/2160 train_time:4844ms step_avg:34.85ms
step:140/2160 train_time:4877ms step_avg:34.84ms
step:141/2160 train_time:4912ms step_avg:34.84ms
step:142/2160 train_time:4945ms step_avg:34.83ms
step:143/2160 train_time:4979ms step_avg:34.82ms
step:144/2160 train_time:5012ms step_avg:34.81ms
step:145/2160 train_time:5047ms step_avg:34.81ms
step:146/2160 train_time:5080ms step_avg:34.80ms
step:147/2160 train_time:5114ms step_avg:34.79ms
step:148/2160 train_time:5148ms step_avg:34.78ms
step:149/2160 train_time:5182ms step_avg:34.78ms
step:150/2160 train_time:5215ms step_avg:34.76ms
step:151/2160 train_time:5249ms step_avg:34.76ms
step:152/2160 train_time:5282ms step_avg:34.75ms
step:153/2160 train_time:5317ms step_avg:34.75ms
step:154/2160 train_time:5350ms step_avg:34.74ms
step:155/2160 train_time:5384ms step_avg:34.74ms
step:156/2160 train_time:5417ms step_avg:34.73ms
step:157/2160 train_time:5452ms step_avg:34.72ms
step:158/2160 train_time:5485ms step_avg:34.71ms
step:159/2160 train_time:5520ms step_avg:34.71ms
step:160/2160 train_time:5553ms step_avg:34.71ms
step:161/2160 train_time:5588ms step_avg:34.71ms
step:162/2160 train_time:5620ms step_avg:34.69ms
step:163/2160 train_time:5655ms step_avg:34.69ms
step:164/2160 train_time:5688ms step_avg:34.68ms
step:165/2160 train_time:5722ms step_avg:34.68ms
step:166/2160 train_time:5755ms step_avg:34.67ms
step:167/2160 train_time:5789ms step_avg:34.67ms
step:168/2160 train_time:5823ms step_avg:34.66ms
step:169/2160 train_time:5857ms step_avg:34.66ms
step:170/2160 train_time:5890ms step_avg:34.65ms
step:171/2160 train_time:5924ms step_avg:34.65ms
step:172/2160 train_time:5958ms step_avg:34.64ms
step:173/2160 train_time:5992ms step_avg:34.64ms
step:174/2160 train_time:6025ms step_avg:34.63ms
step:175/2160 train_time:6060ms step_avg:34.63ms
step:176/2160 train_time:6093ms step_avg:34.62ms
step:177/2160 train_time:6127ms step_avg:34.62ms
step:178/2160 train_time:6161ms step_avg:34.61ms
step:179/2160 train_time:6195ms step_avg:34.61ms
step:180/2160 train_time:6228ms step_avg:34.60ms
step:181/2160 train_time:6262ms step_avg:34.60ms
step:182/2160 train_time:6295ms step_avg:34.59ms
step:183/2160 train_time:6329ms step_avg:34.59ms
step:184/2160 train_time:6363ms step_avg:34.58ms
step:185/2160 train_time:6397ms step_avg:34.58ms
step:186/2160 train_time:6430ms step_avg:34.57ms
step:187/2160 train_time:6464ms step_avg:34.57ms
step:188/2160 train_time:6497ms step_avg:34.56ms
step:189/2160 train_time:6532ms step_avg:34.56ms
step:190/2160 train_time:6565ms step_avg:34.55ms
step:191/2160 train_time:6599ms step_avg:34.55ms
step:192/2160 train_time:6632ms step_avg:34.54ms
step:193/2160 train_time:6667ms step_avg:34.54ms
step:194/2160 train_time:6700ms step_avg:34.54ms
step:195/2160 train_time:6734ms step_avg:34.54ms
step:196/2160 train_time:6768ms step_avg:34.53ms
step:197/2160 train_time:6802ms step_avg:34.53ms
step:198/2160 train_time:6835ms step_avg:34.52ms
step:199/2160 train_time:6869ms step_avg:34.52ms
step:200/2160 train_time:6902ms step_avg:34.51ms
step:201/2160 train_time:6936ms step_avg:34.51ms
step:202/2160 train_time:6974ms step_avg:34.53ms
step:203/2160 train_time:7004ms step_avg:34.50ms
step:204/2160 train_time:7037ms step_avg:34.50ms
step:205/2160 train_time:7071ms step_avg:34.49ms
step:206/2160 train_time:7105ms step_avg:34.49ms
step:207/2160 train_time:7139ms step_avg:34.49ms
step:208/2160 train_time:7172ms step_avg:34.48ms
step:209/2160 train_time:7206ms step_avg:34.48ms
step:210/2160 train_time:7239ms step_avg:34.47ms
step:211/2160 train_time:7273ms step_avg:34.47ms
step:212/2160 train_time:7306ms step_avg:34.46ms
step:213/2160 train_time:7341ms step_avg:34.46ms
step:214/2160 train_time:7374ms step_avg:34.46ms
step:215/2160 train_time:7408ms step_avg:34.46ms
step:216/2160 train_time:7442ms step_avg:34.45ms
step:217/2160 train_time:7476ms step_avg:34.45ms
step:218/2160 train_time:7509ms step_avg:34.45ms
step:219/2160 train_time:7543ms step_avg:34.44ms
step:220/2160 train_time:7576ms step_avg:34.44ms
step:221/2160 train_time:7611ms step_avg:34.44ms
step:222/2160 train_time:7644ms step_avg:34.43ms
step:223/2160 train_time:7678ms step_avg:34.43ms
step:224/2160 train_time:7711ms step_avg:34.43ms
step:225/2160 train_time:7746ms step_avg:34.43ms
step:226/2160 train_time:7779ms step_avg:34.42ms
step:227/2160 train_time:7814ms step_avg:34.42ms
step:228/2160 train_time:7847ms step_avg:34.41ms
step:229/2160 train_time:7881ms step_avg:34.41ms
step:230/2160 train_time:7914ms step_avg:34.41ms
step:231/2160 train_time:7948ms step_avg:34.41ms
step:232/2160 train_time:7982ms step_avg:34.40ms
step:233/2160 train_time:8016ms step_avg:34.40ms
step:234/2160 train_time:8049ms step_avg:34.40ms
step:235/2160 train_time:8084ms step_avg:34.40ms
step:236/2160 train_time:8116ms step_avg:34.39ms
step:237/2160 train_time:8151ms step_avg:34.39ms
step:238/2160 train_time:8184ms step_avg:34.38ms
step:239/2160 train_time:8218ms step_avg:34.38ms
step:240/2160 train_time:8251ms step_avg:34.38ms
step:241/2160 train_time:8285ms step_avg:34.38ms
step:242/2160 train_time:8318ms step_avg:34.37ms
step:243/2160 train_time:8353ms step_avg:34.37ms
step:244/2160 train_time:8388ms step_avg:34.38ms
step:245/2160 train_time:8420ms step_avg:34.37ms
step:246/2160 train_time:8453ms step_avg:34.36ms
step:247/2160 train_time:8488ms step_avg:34.36ms
step:248/2160 train_time:8521ms step_avg:34.36ms
step:249/2160 train_time:8555ms step_avg:34.36ms
step:250/2160 train_time:8588ms step_avg:34.35ms
step:250/2160 val_loss:4.3043 train_time:8624ms step_avg:34.49ms
step:251/2160 train_time:8648ms step_avg:34.45ms
step:252/2160 train_time:8675ms step_avg:34.43ms
step:253/2160 train_time:8696ms step_avg:34.37ms
step:254/2160 train_time:8728ms step_avg:34.36ms
step:255/2160 train_time:8766ms step_avg:34.38ms
step:256/2160 train_time:8802ms step_avg:34.38ms
step:257/2160 train_time:8839ms step_avg:34.39ms
step:258/2160 train_time:8873ms step_avg:34.39ms
step:259/2160 train_time:8907ms step_avg:34.39ms
step:260/2160 train_time:8941ms step_avg:34.39ms
step:261/2160 train_time:8975ms step_avg:34.39ms
step:262/2160 train_time:9008ms step_avg:34.38ms
step:263/2160 train_time:9042ms step_avg:34.38ms
step:264/2160 train_time:9075ms step_avg:34.38ms
step:265/2160 train_time:9110ms step_avg:34.38ms
step:266/2160 train_time:9143ms step_avg:34.37ms
step:267/2160 train_time:9177ms step_avg:34.37ms
step:268/2160 train_time:9210ms step_avg:34.36ms
step:269/2160 train_time:9243ms step_avg:34.36ms
step:270/2160 train_time:9277ms step_avg:34.36ms
step:271/2160 train_time:9310ms step_avg:34.36ms
step:272/2160 train_time:9343ms step_avg:34.35ms
step:273/2160 train_time:9377ms step_avg:34.35ms
step:274/2160 train_time:9410ms step_avg:34.34ms
step:275/2160 train_time:9444ms step_avg:34.34ms
step:276/2160 train_time:9478ms step_avg:34.34ms
step:277/2160 train_time:9511ms step_avg:34.34ms
step:278/2160 train_time:9544ms step_avg:34.33ms
step:279/2160 train_time:9578ms step_avg:34.33ms
step:280/2160 train_time:9611ms step_avg:34.33ms
step:281/2160 train_time:9645ms step_avg:34.32ms
step:282/2160 train_time:9678ms step_avg:34.32ms
step:283/2160 train_time:9713ms step_avg:34.32ms
step:284/2160 train_time:9746ms step_avg:34.32ms
step:285/2160 train_time:9781ms step_avg:34.32ms
step:286/2160 train_time:9814ms step_avg:34.32ms
step:287/2160 train_time:9850ms step_avg:34.32ms
step:288/2160 train_time:9883ms step_avg:34.32ms
step:289/2160 train_time:9918ms step_avg:34.32ms
step:290/2160 train_time:9951ms step_avg:34.31ms
step:291/2160 train_time:9986ms step_avg:34.32ms
step:292/2160 train_time:10019ms step_avg:34.31ms
step:293/2160 train_time:10054ms step_avg:34.31ms
step:294/2160 train_time:10087ms step_avg:34.31ms
step:295/2160 train_time:10121ms step_avg:34.31ms
step:296/2160 train_time:10154ms step_avg:34.30ms
step:297/2160 train_time:10188ms step_avg:34.30ms
step:298/2160 train_time:10221ms step_avg:34.30ms
step:299/2160 train_time:10256ms step_avg:34.30ms
step:300/2160 train_time:10289ms step_avg:34.30ms
step:301/2160 train_time:10323ms step_avg:34.29ms
step:302/2160 train_time:10356ms step_avg:34.29ms
step:303/2160 train_time:10390ms step_avg:34.29ms
step:304/2160 train_time:10423ms step_avg:34.29ms
step:305/2160 train_time:10457ms step_avg:34.28ms
step:306/2160 train_time:10490ms step_avg:34.28ms
step:307/2160 train_time:10524ms step_avg:34.28ms
step:308/2160 train_time:10557ms step_avg:34.28ms
step:309/2160 train_time:10591ms step_avg:34.28ms
step:310/2160 train_time:10626ms step_avg:34.28ms
step:311/2160 train_time:10658ms step_avg:34.27ms
step:312/2160 train_time:10691ms step_avg:34.27ms
step:313/2160 train_time:10726ms step_avg:34.27ms
step:314/2160 train_time:10759ms step_avg:34.26ms
step:315/2160 train_time:10794ms step_avg:34.27ms
step:316/2160 train_time:10827ms step_avg:34.26ms
step:317/2160 train_time:10861ms step_avg:34.26ms
step:318/2160 train_time:10895ms step_avg:34.26ms
step:319/2160 train_time:10929ms step_avg:34.26ms
step:320/2160 train_time:10962ms step_avg:34.26ms
step:321/2160 train_time:10997ms step_avg:34.26ms
step:322/2160 train_time:11030ms step_avg:34.25ms
step:323/2160 train_time:11064ms step_avg:34.25ms
step:324/2160 train_time:11098ms step_avg:34.25ms
step:325/2160 train_time:11131ms step_avg:34.25ms
step:326/2160 train_time:11165ms step_avg:34.25ms
step:327/2160 train_time:11199ms step_avg:34.25ms
step:328/2160 train_time:11232ms step_avg:34.24ms
step:329/2160 train_time:11267ms step_avg:34.25ms
step:330/2160 train_time:11300ms step_avg:34.24ms
step:331/2160 train_time:11334ms step_avg:34.24ms
step:332/2160 train_time:11367ms step_avg:34.24ms
step:333/2160 train_time:11401ms step_avg:34.24ms
step:334/2160 train_time:11434ms step_avg:34.23ms
step:335/2160 train_time:11468ms step_avg:34.23ms
step:336/2160 train_time:11501ms step_avg:34.23ms
step:337/2160 train_time:11536ms step_avg:34.23ms
step:338/2160 train_time:11569ms step_avg:34.23ms
step:339/2160 train_time:11603ms step_avg:34.23ms
step:340/2160 train_time:11636ms step_avg:34.22ms
step:341/2160 train_time:11670ms step_avg:34.22ms
step:342/2160 train_time:11703ms step_avg:34.22ms
step:343/2160 train_time:11737ms step_avg:34.22ms
step:344/2160 train_time:11770ms step_avg:34.22ms
step:345/2160 train_time:11804ms step_avg:34.22ms
step:346/2160 train_time:11838ms step_avg:34.21ms
step:347/2160 train_time:11872ms step_avg:34.21ms
step:348/2160 train_time:11905ms step_avg:34.21ms
step:349/2160 train_time:11939ms step_avg:34.21ms
step:350/2160 train_time:11972ms step_avg:34.21ms
step:351/2160 train_time:12007ms step_avg:34.21ms
step:352/2160 train_time:12040ms step_avg:34.20ms
step:353/2160 train_time:12074ms step_avg:34.21ms
step:354/2160 train_time:12108ms step_avg:34.20ms
step:355/2160 train_time:12142ms step_avg:34.20ms
step:356/2160 train_time:12175ms step_avg:34.20ms
step:357/2160 train_time:12210ms step_avg:34.20ms
step:358/2160 train_time:12243ms step_avg:34.20ms
step:359/2160 train_time:12277ms step_avg:34.20ms
step:360/2160 train_time:12310ms step_avg:34.19ms
step:361/2160 train_time:12344ms step_avg:34.19ms
step:362/2160 train_time:12377ms step_avg:34.19ms
step:363/2160 train_time:12412ms step_avg:34.19ms
step:364/2160 train_time:12445ms step_avg:34.19ms
step:365/2160 train_time:12479ms step_avg:34.19ms
step:366/2160 train_time:12512ms step_avg:34.19ms
step:367/2160 train_time:12546ms step_avg:34.19ms
step:368/2160 train_time:12579ms step_avg:34.18ms
step:369/2160 train_time:12613ms step_avg:34.18ms
step:370/2160 train_time:12646ms step_avg:34.18ms
step:371/2160 train_time:12681ms step_avg:34.18ms
step:372/2160 train_time:12714ms step_avg:34.18ms
step:373/2160 train_time:12748ms step_avg:34.18ms
step:374/2160 train_time:12781ms step_avg:34.17ms
step:375/2160 train_time:12815ms step_avg:34.17ms
step:376/2160 train_time:12849ms step_avg:34.17ms
step:377/2160 train_time:12882ms step_avg:34.17ms
step:378/2160 train_time:12916ms step_avg:34.17ms
step:379/2160 train_time:12950ms step_avg:34.17ms
step:380/2160 train_time:12983ms step_avg:34.17ms
step:381/2160 train_time:13017ms step_avg:34.17ms
step:382/2160 train_time:13050ms step_avg:34.16ms
step:383/2160 train_time:13085ms step_avg:34.16ms
step:384/2160 train_time:13118ms step_avg:34.16ms
step:385/2160 train_time:13152ms step_avg:34.16ms
step:386/2160 train_time:13185ms step_avg:34.16ms
step:387/2160 train_time:13220ms step_avg:34.16ms
step:388/2160 train_time:13253ms step_avg:34.16ms
step:389/2160 train_time:13287ms step_avg:34.16ms
step:390/2160 train_time:13326ms step_avg:34.17ms
step:391/2160 train_time:13354ms step_avg:34.15ms
step:392/2160 train_time:13388ms step_avg:34.15ms
step:393/2160 train_time:13422ms step_avg:34.15ms
step:394/2160 train_time:13455ms step_avg:34.15ms
step:395/2160 train_time:13489ms step_avg:34.15ms
step:396/2160 train_time:13522ms step_avg:34.15ms
step:397/2160 train_time:13557ms step_avg:34.15ms
step:398/2160 train_time:13590ms step_avg:34.15ms
step:399/2160 train_time:13624ms step_avg:34.15ms
step:400/2160 train_time:13657ms step_avg:34.14ms
step:401/2160 train_time:13691ms step_avg:34.14ms
step:402/2160 train_time:13725ms step_avg:34.14ms
step:403/2160 train_time:13759ms step_avg:34.14ms
step:404/2160 train_time:13792ms step_avg:34.14ms
step:405/2160 train_time:13826ms step_avg:34.14ms
step:406/2160 train_time:13859ms step_avg:34.14ms
step:407/2160 train_time:13894ms step_avg:34.14ms
step:408/2160 train_time:13927ms step_avg:34.13ms
step:409/2160 train_time:13961ms step_avg:34.13ms
step:410/2160 train_time:13994ms step_avg:34.13ms
step:411/2160 train_time:14029ms step_avg:34.13ms
step:412/2160 train_time:14062ms step_avg:34.13ms
step:413/2160 train_time:14096ms step_avg:34.13ms
step:414/2160 train_time:14129ms step_avg:34.13ms
step:415/2160 train_time:14164ms step_avg:34.13ms
step:416/2160 train_time:14197ms step_avg:34.13ms
step:417/2160 train_time:14232ms step_avg:34.13ms
step:418/2160 train_time:14265ms step_avg:34.13ms
step:419/2160 train_time:14299ms step_avg:34.13ms
step:420/2160 train_time:14332ms step_avg:34.12ms
step:421/2160 train_time:14366ms step_avg:34.12ms
step:422/2160 train_time:14399ms step_avg:34.12ms
step:423/2160 train_time:14434ms step_avg:34.12ms
step:424/2160 train_time:14467ms step_avg:34.12ms
step:425/2160 train_time:14501ms step_avg:34.12ms
step:426/2160 train_time:14534ms step_avg:34.12ms
step:427/2160 train_time:14569ms step_avg:34.12ms
step:428/2160 train_time:14602ms step_avg:34.12ms
step:429/2160 train_time:14636ms step_avg:34.12ms
step:430/2160 train_time:14669ms step_avg:34.11ms
step:431/2160 train_time:14703ms step_avg:34.11ms
step:432/2160 train_time:14738ms step_avg:34.12ms
step:433/2160 train_time:14770ms step_avg:34.11ms
step:434/2160 train_time:14804ms step_avg:34.11ms
step:435/2160 train_time:14838ms step_avg:34.11ms
step:436/2160 train_time:14871ms step_avg:34.11ms
step:437/2160 train_time:14906ms step_avg:34.11ms
step:438/2160 train_time:14939ms step_avg:34.11ms
step:439/2160 train_time:14973ms step_avg:34.11ms
step:440/2160 train_time:15006ms step_avg:34.10ms
step:441/2160 train_time:15040ms step_avg:34.11ms
step:442/2160 train_time:15074ms step_avg:34.10ms
step:443/2160 train_time:15108ms step_avg:34.10ms
step:444/2160 train_time:15141ms step_avg:34.10ms
step:445/2160 train_time:15175ms step_avg:34.10ms
step:446/2160 train_time:15208ms step_avg:34.10ms
step:447/2160 train_time:15243ms step_avg:34.10ms
step:448/2160 train_time:15276ms step_avg:34.10ms
step:449/2160 train_time:15311ms step_avg:34.10ms
step:450/2160 train_time:15344ms step_avg:34.10ms
step:451/2160 train_time:15378ms step_avg:34.10ms
step:452/2160 train_time:15411ms step_avg:34.10ms
step:453/2160 train_time:15445ms step_avg:34.10ms
step:454/2160 train_time:15478ms step_avg:34.09ms
step:455/2160 train_time:15513ms step_avg:34.09ms
step:456/2160 train_time:15546ms step_avg:34.09ms
step:457/2160 train_time:15580ms step_avg:34.09ms
step:458/2160 train_time:15614ms step_avg:34.09ms
step:459/2160 train_time:15648ms step_avg:34.09ms
step:460/2160 train_time:15681ms step_avg:34.09ms
step:461/2160 train_time:15716ms step_avg:34.09ms
step:462/2160 train_time:15749ms step_avg:34.09ms
step:463/2160 train_time:15784ms step_avg:34.09ms
step:464/2160 train_time:15817ms step_avg:34.09ms
step:465/2160 train_time:15851ms step_avg:34.09ms
step:466/2160 train_time:15884ms step_avg:34.09ms
step:467/2160 train_time:15918ms step_avg:34.09ms
step:468/2160 train_time:15952ms step_avg:34.08ms
step:469/2160 train_time:15986ms step_avg:34.09ms
step:470/2160 train_time:16019ms step_avg:34.08ms
step:471/2160 train_time:16053ms step_avg:34.08ms
step:472/2160 train_time:16086ms step_avg:34.08ms
step:473/2160 train_time:16120ms step_avg:34.08ms
step:474/2160 train_time:16154ms step_avg:34.08ms
step:475/2160 train_time:16188ms step_avg:34.08ms
step:476/2160 train_time:16221ms step_avg:34.08ms
step:477/2160 train_time:16256ms step_avg:34.08ms
step:478/2160 train_time:16289ms step_avg:34.08ms
step:479/2160 train_time:16323ms step_avg:34.08ms
step:480/2160 train_time:16356ms step_avg:34.08ms
step:481/2160 train_time:16390ms step_avg:34.08ms
step:482/2160 train_time:16424ms step_avg:34.07ms
step:483/2160 train_time:16458ms step_avg:34.07ms
step:484/2160 train_time:16491ms step_avg:34.07ms
step:485/2160 train_time:16526ms step_avg:34.07ms
step:486/2160 train_time:16559ms step_avg:34.07ms
step:487/2160 train_time:16594ms step_avg:34.07ms
step:488/2160 train_time:16627ms step_avg:34.07ms
step:489/2160 train_time:16661ms step_avg:34.07ms
step:490/2160 train_time:16694ms step_avg:34.07ms
step:491/2160 train_time:16729ms step_avg:34.07ms
step:492/2160 train_time:16762ms step_avg:34.07ms
step:493/2160 train_time:16797ms step_avg:34.07ms
step:494/2160 train_time:16830ms step_avg:34.07ms
step:495/2160 train_time:16864ms step_avg:34.07ms
step:496/2160 train_time:16897ms step_avg:34.07ms
step:497/2160 train_time:16932ms step_avg:34.07ms
step:498/2160 train_time:16965ms step_avg:34.07ms
step:499/2160 train_time:16999ms step_avg:34.07ms
step:500/2160 train_time:17032ms step_avg:34.06ms
step:500/2160 val_loss:4.0074 train_time:17067ms step_avg:34.13ms
step:501/2160 train_time:17090ms step_avg:34.11ms
step:502/2160 train_time:17113ms step_avg:34.09ms
step:503/2160 train_time:17138ms step_avg:34.07ms
step:504/2160 train_time:17172ms step_avg:34.07ms
step:505/2160 train_time:17209ms step_avg:34.08ms
step:506/2160 train_time:17243ms step_avg:34.08ms
step:507/2160 train_time:17279ms step_avg:34.08ms
step:508/2160 train_time:17312ms step_avg:34.08ms
step:509/2160 train_time:17346ms step_avg:34.08ms
step:510/2160 train_time:17380ms step_avg:34.08ms
step:511/2160 train_time:17414ms step_avg:34.08ms
step:512/2160 train_time:17447ms step_avg:34.08ms
step:513/2160 train_time:17481ms step_avg:34.08ms
step:514/2160 train_time:17514ms step_avg:34.07ms
step:515/2160 train_time:17548ms step_avg:34.07ms
step:516/2160 train_time:17581ms step_avg:34.07ms
step:517/2160 train_time:17615ms step_avg:34.07ms
step:518/2160 train_time:17648ms step_avg:34.07ms
step:519/2160 train_time:17682ms step_avg:34.07ms
step:520/2160 train_time:17715ms step_avg:34.07ms
step:521/2160 train_time:17749ms step_avg:34.07ms
step:522/2160 train_time:17782ms step_avg:34.07ms
step:523/2160 train_time:17816ms step_avg:34.07ms
step:524/2160 train_time:17849ms step_avg:34.06ms
step:525/2160 train_time:17883ms step_avg:34.06ms
step:526/2160 train_time:17916ms step_avg:34.06ms
step:527/2160 train_time:17950ms step_avg:34.06ms
step:528/2160 train_time:17983ms step_avg:34.06ms
step:529/2160 train_time:18017ms step_avg:34.06ms
step:530/2160 train_time:18050ms step_avg:34.06ms
step:531/2160 train_time:18085ms step_avg:34.06ms
step:532/2160 train_time:18119ms step_avg:34.06ms
step:533/2160 train_time:18153ms step_avg:34.06ms
step:534/2160 train_time:18187ms step_avg:34.06ms
step:535/2160 train_time:18222ms step_avg:34.06ms
step:536/2160 train_time:18256ms step_avg:34.06ms
step:537/2160 train_time:18290ms step_avg:34.06ms
step:538/2160 train_time:18323ms step_avg:34.06ms
step:539/2160 train_time:18358ms step_avg:34.06ms
step:540/2160 train_time:18391ms step_avg:34.06ms
step:541/2160 train_time:18426ms step_avg:34.06ms
step:542/2160 train_time:18459ms step_avg:34.06ms
step:543/2160 train_time:18493ms step_avg:34.06ms
step:544/2160 train_time:18527ms step_avg:34.06ms
step:545/2160 train_time:18561ms step_avg:34.06ms
step:546/2160 train_time:18594ms step_avg:34.06ms
step:547/2160 train_time:18628ms step_avg:34.06ms
step:548/2160 train_time:18662ms step_avg:34.05ms
step:549/2160 train_time:18696ms step_avg:34.05ms
step:550/2160 train_time:18729ms step_avg:34.05ms
step:551/2160 train_time:18763ms step_avg:34.05ms
step:552/2160 train_time:18796ms step_avg:34.05ms
step:553/2160 train_time:18830ms step_avg:34.05ms
step:554/2160 train_time:18863ms step_avg:34.05ms
step:555/2160 train_time:18897ms step_avg:34.05ms
step:556/2160 train_time:18931ms step_avg:34.05ms
step:557/2160 train_time:18965ms step_avg:34.05ms
step:558/2160 train_time:18998ms step_avg:34.05ms
step:559/2160 train_time:19032ms step_avg:34.05ms
step:560/2160 train_time:19065ms step_avg:34.04ms
step:561/2160 train_time:19100ms step_avg:34.05ms
step:562/2160 train_time:19133ms step_avg:34.04ms
step:563/2160 train_time:19168ms step_avg:34.05ms
step:564/2160 train_time:19202ms step_avg:34.05ms
step:565/2160 train_time:19236ms step_avg:34.05ms
step:566/2160 train_time:19269ms step_avg:34.04ms
step:567/2160 train_time:19304ms step_avg:34.05ms
step:568/2160 train_time:19337ms step_avg:34.04ms
step:569/2160 train_time:19372ms step_avg:34.05ms
step:570/2160 train_time:19405ms step_avg:34.04ms
step:571/2160 train_time:19439ms step_avg:34.04ms
step:572/2160 train_time:19472ms step_avg:34.04ms
step:573/2160 train_time:19507ms step_avg:34.04ms
step:574/2160 train_time:19540ms step_avg:34.04ms
step:575/2160 train_time:19574ms step_avg:34.04ms
step:576/2160 train_time:19608ms step_avg:34.04ms
step:577/2160 train_time:19642ms step_avg:34.04ms
step:578/2160 train_time:19676ms step_avg:34.04ms
step:579/2160 train_time:19709ms step_avg:34.04ms
step:580/2160 train_time:19743ms step_avg:34.04ms
step:581/2160 train_time:19777ms step_avg:34.04ms
step:582/2160 train_time:19810ms step_avg:34.04ms
step:583/2160 train_time:19844ms step_avg:34.04ms
step:584/2160 train_time:19878ms step_avg:34.04ms
step:585/2160 train_time:19912ms step_avg:34.04ms
step:586/2160 train_time:19945ms step_avg:34.04ms
step:587/2160 train_time:19979ms step_avg:34.04ms
step:588/2160 train_time:20012ms step_avg:34.03ms
step:589/2160 train_time:20047ms step_avg:34.04ms
step:590/2160 train_time:20080ms step_avg:34.03ms
step:591/2160 train_time:20114ms step_avg:34.03ms
step:592/2160 train_time:20148ms step_avg:34.03ms
step:593/2160 train_time:20182ms step_avg:34.03ms
step:594/2160 train_time:20215ms step_avg:34.03ms
step:595/2160 train_time:20250ms step_avg:34.03ms
step:596/2160 train_time:20283ms step_avg:34.03ms
step:597/2160 train_time:20318ms step_avg:34.03ms
step:598/2160 train_time:20351ms step_avg:34.03ms
step:599/2160 train_time:20385ms step_avg:34.03ms
step:600/2160 train_time:20419ms step_avg:34.03ms
step:601/2160 train_time:20453ms step_avg:34.03ms
step:602/2160 train_time:20486ms step_avg:34.03ms
step:603/2160 train_time:20521ms step_avg:34.03ms
step:604/2160 train_time:20554ms step_avg:34.03ms
step:605/2160 train_time:20588ms step_avg:34.03ms
step:606/2160 train_time:20621ms step_avg:34.03ms
step:607/2160 train_time:20656ms step_avg:34.03ms
step:608/2160 train_time:20689ms step_avg:34.03ms
step:609/2160 train_time:20723ms step_avg:34.03ms
step:610/2160 train_time:20757ms step_avg:34.03ms
step:611/2160 train_time:20790ms step_avg:34.03ms
step:612/2160 train_time:20824ms step_avg:34.03ms
step:613/2160 train_time:20858ms step_avg:34.03ms
step:614/2160 train_time:20891ms step_avg:34.02ms
step:615/2160 train_time:20925ms step_avg:34.02ms
step:616/2160 train_time:20958ms step_avg:34.02ms
step:617/2160 train_time:20993ms step_avg:34.02ms
step:618/2160 train_time:21026ms step_avg:34.02ms
step:619/2160 train_time:21060ms step_avg:34.02ms
step:620/2160 train_time:21093ms step_avg:34.02ms
step:621/2160 train_time:21127ms step_avg:34.02ms
step:622/2160 train_time:21161ms step_avg:34.02ms
step:623/2160 train_time:21195ms step_avg:34.02ms
step:624/2160 train_time:21229ms step_avg:34.02ms
step:625/2160 train_time:21263ms step_avg:34.02ms
step:626/2160 train_time:21296ms step_avg:34.02ms
step:627/2160 train_time:21331ms step_avg:34.02ms
step:628/2160 train_time:21364ms step_avg:34.02ms
step:629/2160 train_time:21398ms step_avg:34.02ms
step:630/2160 train_time:21431ms step_avg:34.02ms
step:631/2160 train_time:21466ms step_avg:34.02ms
step:632/2160 train_time:21500ms step_avg:34.02ms
step:633/2160 train_time:21533ms step_avg:34.02ms
step:634/2160 train_time:21567ms step_avg:34.02ms
step:635/2160 train_time:21601ms step_avg:34.02ms
step:636/2160 train_time:21634ms step_avg:34.02ms
step:637/2160 train_time:21669ms step_avg:34.02ms
step:638/2160 train_time:21702ms step_avg:34.02ms
step:639/2160 train_time:21736ms step_avg:34.02ms
step:640/2160 train_time:21769ms step_avg:34.01ms
step:641/2160 train_time:21804ms step_avg:34.02ms
step:642/2160 train_time:21837ms step_avg:34.01ms
step:643/2160 train_time:21871ms step_avg:34.01ms
step:644/2160 train_time:21904ms step_avg:34.01ms
step:645/2160 train_time:21939ms step_avg:34.01ms
step:646/2160 train_time:21972ms step_avg:34.01ms
step:647/2160 train_time:22006ms step_avg:34.01ms
step:648/2160 train_time:22040ms step_avg:34.01ms
step:649/2160 train_time:22074ms step_avg:34.01ms
step:650/2160 train_time:22107ms step_avg:34.01ms
step:651/2160 train_time:22141ms step_avg:34.01ms
step:652/2160 train_time:22174ms step_avg:34.01ms
step:653/2160 train_time:22208ms step_avg:34.01ms
step:654/2160 train_time:22242ms step_avg:34.01ms
step:655/2160 train_time:22276ms step_avg:34.01ms
step:656/2160 train_time:22309ms step_avg:34.01ms
step:657/2160 train_time:22344ms step_avg:34.01ms
step:658/2160 train_time:22377ms step_avg:34.01ms
step:659/2160 train_time:22411ms step_avg:34.01ms
step:660/2160 train_time:22445ms step_avg:34.01ms
step:661/2160 train_time:22479ms step_avg:34.01ms
step:662/2160 train_time:22512ms step_avg:34.01ms
step:663/2160 train_time:22547ms step_avg:34.01ms
step:664/2160 train_time:22580ms step_avg:34.01ms
step:665/2160 train_time:22614ms step_avg:34.01ms
step:666/2160 train_time:22648ms step_avg:34.01ms
step:667/2160 train_time:22682ms step_avg:34.01ms
step:668/2160 train_time:22716ms step_avg:34.01ms
step:669/2160 train_time:22750ms step_avg:34.01ms
step:670/2160 train_time:22783ms step_avg:34.00ms
step:671/2160 train_time:22818ms step_avg:34.01ms
step:672/2160 train_time:22851ms step_avg:34.00ms
step:673/2160 train_time:22888ms step_avg:34.01ms
step:674/2160 train_time:22918ms step_avg:34.00ms
step:675/2160 train_time:22953ms step_avg:34.00ms
step:676/2160 train_time:22986ms step_avg:34.00ms
step:677/2160 train_time:23020ms step_avg:34.00ms
step:678/2160 train_time:23053ms step_avg:34.00ms
step:679/2160 train_time:23087ms step_avg:34.00ms
step:680/2160 train_time:23120ms step_avg:34.00ms
step:681/2160 train_time:23155ms step_avg:34.00ms
step:682/2160 train_time:23188ms step_avg:34.00ms
step:683/2160 train_time:23222ms step_avg:34.00ms
step:684/2160 train_time:23255ms step_avg:34.00ms
step:685/2160 train_time:23290ms step_avg:34.00ms
step:686/2160 train_time:23323ms step_avg:34.00ms
step:687/2160 train_time:23358ms step_avg:34.00ms
step:688/2160 train_time:23391ms step_avg:34.00ms
step:689/2160 train_time:23425ms step_avg:34.00ms
step:690/2160 train_time:23458ms step_avg:34.00ms
step:691/2160 train_time:23492ms step_avg:34.00ms
step:692/2160 train_time:23525ms step_avg:34.00ms
step:693/2160 train_time:23560ms step_avg:34.00ms
step:694/2160 train_time:23593ms step_avg:34.00ms
step:695/2160 train_time:23628ms step_avg:34.00ms
step:696/2160 train_time:23661ms step_avg:34.00ms
step:697/2160 train_time:23695ms step_avg:34.00ms
step:698/2160 train_time:23729ms step_avg:34.00ms
step:699/2160 train_time:23763ms step_avg:34.00ms
step:700/2160 train_time:23797ms step_avg:34.00ms
step:701/2160 train_time:23831ms step_avg:34.00ms
step:702/2160 train_time:23864ms step_avg:33.99ms
step:703/2160 train_time:23898ms step_avg:33.99ms
step:704/2160 train_time:23931ms step_avg:33.99ms
step:705/2160 train_time:23966ms step_avg:33.99ms
step:706/2160 train_time:23999ms step_avg:33.99ms
step:707/2160 train_time:24033ms step_avg:33.99ms
step:708/2160 train_time:24067ms step_avg:33.99ms
step:709/2160 train_time:24128ms step_avg:34.03ms
step:710/2160 train_time:24188ms step_avg:34.07ms
step:711/2160 train_time:24249ms step_avg:34.11ms
step:712/2160 train_time:24309ms step_avg:34.14ms
step:713/2160 train_time:24372ms step_avg:34.18ms
step:714/2160 train_time:24433ms step_avg:34.22ms
step:715/2160 train_time:24495ms step_avg:34.26ms
step:716/2160 train_time:24556ms step_avg:34.30ms
step:717/2160 train_time:24617ms step_avg:34.33ms
step:718/2160 train_time:24677ms step_avg:34.37ms
step:719/2160 train_time:24739ms step_avg:34.41ms
step:720/2160 train_time:24799ms step_avg:34.44ms
step:721/2160 train_time:24861ms step_avg:34.48ms
step:722/2160 train_time:24921ms step_avg:34.52ms
step:723/2160 train_time:24982ms step_avg:34.55ms
step:724/2160 train_time:25042ms step_avg:34.59ms
step:725/2160 train_time:25104ms step_avg:34.63ms
step:726/2160 train_time:25163ms step_avg:34.66ms
step:727/2160 train_time:25225ms step_avg:34.70ms
step:728/2160 train_time:25285ms step_avg:34.73ms
step:729/2160 train_time:25347ms step_avg:34.77ms
step:730/2160 train_time:25407ms step_avg:34.80ms
step:731/2160 train_time:25469ms step_avg:34.84ms
step:732/2160 train_time:25529ms step_avg:34.88ms
step:733/2160 train_time:25591ms step_avg:34.91ms
step:734/2160 train_time:25652ms step_avg:34.95ms
step:735/2160 train_time:25714ms step_avg:34.99ms
step:736/2160 train_time:25775ms step_avg:35.02ms
step:737/2160 train_time:25837ms step_avg:35.06ms
step:738/2160 train_time:25898ms step_avg:35.09ms
step:739/2160 train_time:25958ms step_avg:35.13ms
step:740/2160 train_time:26018ms step_avg:35.16ms
step:741/2160 train_time:26079ms step_avg:35.19ms
step:742/2160 train_time:26139ms step_avg:35.23ms
step:743/2160 train_time:26200ms step_avg:35.26ms
step:744/2160 train_time:26261ms step_avg:35.30ms
step:745/2160 train_time:26322ms step_avg:35.33ms
step:746/2160 train_time:26382ms step_avg:35.36ms
step:747/2160 train_time:26444ms step_avg:35.40ms
step:748/2160 train_time:26504ms step_avg:35.43ms
step:749/2160 train_time:26566ms step_avg:35.47ms
step:750/2160 train_time:26626ms step_avg:35.50ms
step:750/2160 val_loss:3.8453 train_time:26689ms step_avg:35.58ms
step:751/2160 train_time:26721ms step_avg:35.58ms
step:752/2160 train_time:26749ms step_avg:35.57ms
step:753/2160 train_time:26813ms step_avg:35.61ms
step:754/2160 train_time:26877ms step_avg:35.65ms
step:755/2160 train_time:26938ms step_avg:35.68ms
step:756/2160 train_time:26997ms step_avg:35.71ms
step:757/2160 train_time:27057ms step_avg:35.74ms
step:758/2160 train_time:27116ms step_avg:35.77ms
step:759/2160 train_time:27176ms step_avg:35.81ms
step:760/2160 train_time:27235ms step_avg:35.84ms
step:761/2160 train_time:27296ms step_avg:35.87ms
step:762/2160 train_time:27354ms step_avg:35.90ms
step:763/2160 train_time:27415ms step_avg:35.93ms
step:764/2160 train_time:27473ms step_avg:35.96ms
step:765/2160 train_time:27534ms step_avg:35.99ms
step:766/2160 train_time:27601ms step_avg:36.03ms
step:767/2160 train_time:27667ms step_avg:36.07ms
step:768/2160 train_time:27727ms step_avg:36.10ms
step:769/2160 train_time:27790ms step_avg:36.14ms
step:770/2160 train_time:27850ms step_avg:36.17ms
step:771/2160 train_time:27912ms step_avg:36.20ms
step:772/2160 train_time:27971ms step_avg:36.23ms
step:773/2160 train_time:28032ms step_avg:36.26ms
step:774/2160 train_time:28091ms step_avg:36.29ms
step:775/2160 train_time:28152ms step_avg:36.33ms
step:776/2160 train_time:28212ms step_avg:36.36ms
step:777/2160 train_time:28272ms step_avg:36.39ms
step:778/2160 train_time:28333ms step_avg:36.42ms
step:779/2160 train_time:28392ms step_avg:36.45ms
step:780/2160 train_time:28452ms step_avg:36.48ms
step:781/2160 train_time:28514ms step_avg:36.51ms
step:782/2160 train_time:28577ms step_avg:36.54ms
step:783/2160 train_time:28641ms step_avg:36.58ms
step:784/2160 train_time:28701ms step_avg:36.61ms
step:785/2160 train_time:28763ms step_avg:36.64ms
step:786/2160 train_time:28823ms step_avg:36.67ms
step:787/2160 train_time:28885ms step_avg:36.70ms
step:788/2160 train_time:28944ms step_avg:36.73ms
step:789/2160 train_time:29006ms step_avg:36.76ms
step:790/2160 train_time:29066ms step_avg:36.79ms
step:791/2160 train_time:29127ms step_avg:36.82ms
step:792/2160 train_time:29186ms step_avg:36.85ms
step:793/2160 train_time:29247ms step_avg:36.88ms
step:794/2160 train_time:29307ms step_avg:36.91ms
step:795/2160 train_time:29368ms step_avg:36.94ms
step:796/2160 train_time:29428ms step_avg:36.97ms
step:797/2160 train_time:29489ms step_avg:37.00ms
step:798/2160 train_time:29550ms step_avg:37.03ms
step:799/2160 train_time:29613ms step_avg:37.06ms
step:800/2160 train_time:29674ms step_avg:37.09ms
step:801/2160 train_time:29738ms step_avg:37.13ms
step:802/2160 train_time:29798ms step_avg:37.15ms
step:803/2160 train_time:29861ms step_avg:37.19ms
step:804/2160 train_time:29920ms step_avg:37.21ms
step:805/2160 train_time:29982ms step_avg:37.24ms
step:806/2160 train_time:30041ms step_avg:37.27ms
step:807/2160 train_time:30102ms step_avg:37.30ms
step:808/2160 train_time:30161ms step_avg:37.33ms
step:809/2160 train_time:30222ms step_avg:37.36ms
step:810/2160 train_time:30282ms step_avg:37.39ms
step:811/2160 train_time:30344ms step_avg:37.42ms
step:812/2160 train_time:30404ms step_avg:37.44ms
step:813/2160 train_time:30466ms step_avg:37.47ms
step:814/2160 train_time:30526ms step_avg:37.50ms
step:815/2160 train_time:30588ms step_avg:37.53ms
step:816/2160 train_time:30648ms step_avg:37.56ms
step:817/2160 train_time:30710ms step_avg:37.59ms
step:818/2160 train_time:30771ms step_avg:37.62ms
step:819/2160 train_time:30833ms step_avg:37.65ms
step:820/2160 train_time:30894ms step_avg:37.68ms
step:821/2160 train_time:30957ms step_avg:37.71ms
step:822/2160 train_time:31017ms step_avg:37.73ms
step:823/2160 train_time:31079ms step_avg:37.76ms
step:824/2160 train_time:31139ms step_avg:37.79ms
step:825/2160 train_time:31200ms step_avg:37.82ms
step:826/2160 train_time:31260ms step_avg:37.84ms
step:827/2160 train_time:31321ms step_avg:37.87ms
step:828/2160 train_time:31381ms step_avg:37.90ms
step:829/2160 train_time:31442ms step_avg:37.93ms
step:830/2160 train_time:31503ms step_avg:37.95ms
step:831/2160 train_time:31565ms step_avg:37.98ms
step:832/2160 train_time:31625ms step_avg:38.01ms
step:833/2160 train_time:31687ms step_avg:38.04ms
step:834/2160 train_time:31748ms step_avg:38.07ms
step:835/2160 train_time:31811ms step_avg:38.10ms
step:836/2160 train_time:31871ms step_avg:38.12ms
step:837/2160 train_time:31933ms step_avg:38.15ms
step:838/2160 train_time:31993ms step_avg:38.18ms
step:839/2160 train_time:32056ms step_avg:38.21ms
step:840/2160 train_time:32116ms step_avg:38.23ms
step:841/2160 train_time:32178ms step_avg:38.26ms
step:842/2160 train_time:32238ms step_avg:38.29ms
step:843/2160 train_time:32299ms step_avg:38.31ms
step:844/2160 train_time:32359ms step_avg:38.34ms
step:845/2160 train_time:32421ms step_avg:38.37ms
step:846/2160 train_time:32481ms step_avg:38.39ms
step:847/2160 train_time:32542ms step_avg:38.42ms
step:848/2160 train_time:32602ms step_avg:38.45ms
step:849/2160 train_time:32664ms step_avg:38.47ms
step:850/2160 train_time:32727ms step_avg:38.50ms
step:851/2160 train_time:32786ms step_avg:38.53ms
step:852/2160 train_time:32846ms step_avg:38.55ms
step:853/2160 train_time:32909ms step_avg:38.58ms
step:854/2160 train_time:32969ms step_avg:38.61ms
step:855/2160 train_time:33031ms step_avg:38.63ms
step:856/2160 train_time:33091ms step_avg:38.66ms
step:857/2160 train_time:33153ms step_avg:38.69ms
step:858/2160 train_time:33213ms step_avg:38.71ms
step:859/2160 train_time:33275ms step_avg:38.74ms
step:860/2160 train_time:33335ms step_avg:38.76ms
step:861/2160 train_time:33397ms step_avg:38.79ms
step:862/2160 train_time:33458ms step_avg:38.81ms
step:863/2160 train_time:33520ms step_avg:38.84ms
step:864/2160 train_time:33579ms step_avg:38.86ms
step:865/2160 train_time:33641ms step_avg:38.89ms
step:866/2160 train_time:33701ms step_avg:38.92ms
step:867/2160 train_time:33763ms step_avg:38.94ms
step:868/2160 train_time:33822ms step_avg:38.97ms
step:869/2160 train_time:33885ms step_avg:38.99ms
step:870/2160 train_time:33945ms step_avg:39.02ms
step:871/2160 train_time:34008ms step_avg:39.04ms
step:872/2160 train_time:34067ms step_avg:39.07ms
step:873/2160 train_time:34129ms step_avg:39.09ms
step:874/2160 train_time:34189ms step_avg:39.12ms
step:875/2160 train_time:34250ms step_avg:39.14ms
step:876/2160 train_time:34311ms step_avg:39.17ms
step:877/2160 train_time:34372ms step_avg:39.19ms
step:878/2160 train_time:34434ms step_avg:39.22ms
step:879/2160 train_time:34495ms step_avg:39.24ms
step:880/2160 train_time:34556ms step_avg:39.27ms
step:881/2160 train_time:34618ms step_avg:39.29ms
step:882/2160 train_time:34678ms step_avg:39.32ms
step:883/2160 train_time:34739ms step_avg:39.34ms
step:884/2160 train_time:34799ms step_avg:39.37ms
step:885/2160 train_time:34861ms step_avg:39.39ms
step:886/2160 train_time:34921ms step_avg:39.41ms
step:887/2160 train_time:34983ms step_avg:39.44ms
step:888/2160 train_time:35043ms step_avg:39.46ms
step:889/2160 train_time:35104ms step_avg:39.49ms
step:890/2160 train_time:35164ms step_avg:39.51ms
step:891/2160 train_time:35226ms step_avg:39.54ms
step:892/2160 train_time:35287ms step_avg:39.56ms
step:893/2160 train_time:35349ms step_avg:39.58ms
step:894/2160 train_time:35409ms step_avg:39.61ms
step:895/2160 train_time:35470ms step_avg:39.63ms
step:896/2160 train_time:35531ms step_avg:39.65ms
step:897/2160 train_time:35594ms step_avg:39.68ms
step:898/2160 train_time:35654ms step_avg:39.70ms
step:899/2160 train_time:35716ms step_avg:39.73ms
step:900/2160 train_time:35776ms step_avg:39.75ms
step:901/2160 train_time:35838ms step_avg:39.78ms
step:902/2160 train_time:35898ms step_avg:39.80ms
step:903/2160 train_time:35961ms step_avg:39.82ms
step:904/2160 train_time:36020ms step_avg:39.85ms
step:905/2160 train_time:36081ms step_avg:39.87ms
step:906/2160 train_time:36142ms step_avg:39.89ms
step:907/2160 train_time:36203ms step_avg:39.92ms
step:908/2160 train_time:36264ms step_avg:39.94ms
step:909/2160 train_time:36327ms step_avg:39.96ms
step:910/2160 train_time:36386ms step_avg:39.99ms
step:911/2160 train_time:36448ms step_avg:40.01ms
step:912/2160 train_time:36509ms step_avg:40.03ms
step:913/2160 train_time:36571ms step_avg:40.06ms
step:914/2160 train_time:36631ms step_avg:40.08ms
step:915/2160 train_time:36693ms step_avg:40.10ms
step:916/2160 train_time:36753ms step_avg:40.12ms
step:917/2160 train_time:36816ms step_avg:40.15ms
step:918/2160 train_time:36876ms step_avg:40.17ms
step:919/2160 train_time:36938ms step_avg:40.19ms
step:920/2160 train_time:36998ms step_avg:40.22ms
step:921/2160 train_time:37060ms step_avg:40.24ms
step:922/2160 train_time:37120ms step_avg:40.26ms
step:923/2160 train_time:37182ms step_avg:40.28ms
step:924/2160 train_time:37241ms step_avg:40.30ms
step:925/2160 train_time:37304ms step_avg:40.33ms
step:926/2160 train_time:37364ms step_avg:40.35ms
step:927/2160 train_time:37427ms step_avg:40.37ms
step:928/2160 train_time:37486ms step_avg:40.39ms
step:929/2160 train_time:37549ms step_avg:40.42ms
step:930/2160 train_time:37608ms step_avg:40.44ms
step:931/2160 train_time:37670ms step_avg:40.46ms
step:932/2160 train_time:37730ms step_avg:40.48ms
step:933/2160 train_time:37793ms step_avg:40.51ms
step:934/2160 train_time:37853ms step_avg:40.53ms
step:935/2160 train_time:37915ms step_avg:40.55ms
step:936/2160 train_time:37976ms step_avg:40.57ms
step:937/2160 train_time:38038ms step_avg:40.60ms
step:938/2160 train_time:38098ms step_avg:40.62ms
step:939/2160 train_time:38161ms step_avg:40.64ms
step:940/2160 train_time:38220ms step_avg:40.66ms
step:941/2160 train_time:38281ms step_avg:40.68ms
step:942/2160 train_time:38341ms step_avg:40.70ms
step:943/2160 train_time:38403ms step_avg:40.72ms
step:944/2160 train_time:38464ms step_avg:40.75ms
step:945/2160 train_time:38526ms step_avg:40.77ms
step:946/2160 train_time:38585ms step_avg:40.79ms
step:947/2160 train_time:38647ms step_avg:40.81ms
step:948/2160 train_time:38707ms step_avg:40.83ms
step:949/2160 train_time:38769ms step_avg:40.85ms
step:950/2160 train_time:38829ms step_avg:40.87ms
step:951/2160 train_time:38891ms step_avg:40.90ms
step:952/2160 train_time:38952ms step_avg:40.92ms
step:953/2160 train_time:39015ms step_avg:40.94ms
step:954/2160 train_time:39076ms step_avg:40.96ms
step:955/2160 train_time:39138ms step_avg:40.98ms
step:956/2160 train_time:39198ms step_avg:41.00ms
step:957/2160 train_time:39260ms step_avg:41.02ms
step:958/2160 train_time:39320ms step_avg:41.04ms
step:959/2160 train_time:39381ms step_avg:41.07ms
step:960/2160 train_time:39441ms step_avg:41.08ms
step:961/2160 train_time:39503ms step_avg:41.11ms
step:962/2160 train_time:39564ms step_avg:41.13ms
step:963/2160 train_time:39626ms step_avg:41.15ms
step:964/2160 train_time:39686ms step_avg:41.17ms
step:965/2160 train_time:39748ms step_avg:41.19ms
step:966/2160 train_time:39809ms step_avg:41.21ms
step:967/2160 train_time:39871ms step_avg:41.23ms
step:968/2160 train_time:39931ms step_avg:41.25ms
step:969/2160 train_time:39994ms step_avg:41.27ms
step:970/2160 train_time:40054ms step_avg:41.29ms
step:971/2160 train_time:40117ms step_avg:41.31ms
step:972/2160 train_time:40177ms step_avg:41.33ms
step:973/2160 train_time:40239ms step_avg:41.36ms
step:974/2160 train_time:40299ms step_avg:41.37ms
step:975/2160 train_time:40361ms step_avg:41.40ms
step:976/2160 train_time:40421ms step_avg:41.41ms
step:977/2160 train_time:40482ms step_avg:41.43ms
step:978/2160 train_time:40541ms step_avg:41.45ms
step:979/2160 train_time:40603ms step_avg:41.47ms
step:980/2160 train_time:40663ms step_avg:41.49ms
step:981/2160 train_time:40725ms step_avg:41.51ms
step:982/2160 train_time:40785ms step_avg:41.53ms
step:983/2160 train_time:40847ms step_avg:41.55ms
step:984/2160 train_time:40907ms step_avg:41.57ms
step:985/2160 train_time:40969ms step_avg:41.59ms
step:986/2160 train_time:41030ms step_avg:41.61ms
step:987/2160 train_time:41092ms step_avg:41.63ms
step:988/2160 train_time:41152ms step_avg:41.65ms
step:989/2160 train_time:41215ms step_avg:41.67ms
step:990/2160 train_time:41275ms step_avg:41.69ms
step:991/2160 train_time:41337ms step_avg:41.71ms
step:992/2160 train_time:41398ms step_avg:41.73ms
step:993/2160 train_time:41460ms step_avg:41.75ms
step:994/2160 train_time:41520ms step_avg:41.77ms
step:995/2160 train_time:41581ms step_avg:41.79ms
step:996/2160 train_time:41641ms step_avg:41.81ms
step:997/2160 train_time:41703ms step_avg:41.83ms
step:998/2160 train_time:41764ms step_avg:41.85ms
step:999/2160 train_time:41826ms step_avg:41.87ms
step:1000/2160 train_time:41886ms step_avg:41.89ms
step:1000/2160 val_loss:3.6853 train_time:41948ms step_avg:41.95ms
step:1001/2160 train_time:41972ms step_avg:41.93ms
step:1002/2160 train_time:42009ms step_avg:41.93ms
step:1003/2160 train_time:42074ms step_avg:41.95ms
step:1004/2160 train_time:42136ms step_avg:41.97ms
step:1005/2160 train_time:42198ms step_avg:41.99ms
step:1006/2160 train_time:42258ms step_avg:42.01ms
step:1007/2160 train_time:42320ms step_avg:42.03ms
step:1008/2160 train_time:42379ms step_avg:42.04ms
step:1009/2160 train_time:42440ms step_avg:42.06ms
step:1010/2160 train_time:42499ms step_avg:42.08ms
step:1011/2160 train_time:42561ms step_avg:42.10ms
step:1012/2160 train_time:42620ms step_avg:42.11ms
step:1013/2160 train_time:42681ms step_avg:42.13ms
step:1014/2160 train_time:42740ms step_avg:42.15ms
step:1015/2160 train_time:42802ms step_avg:42.17ms
step:1016/2160 train_time:42862ms step_avg:42.19ms
step:1017/2160 train_time:42926ms step_avg:42.21ms
step:1018/2160 train_time:42987ms step_avg:42.23ms
step:1019/2160 train_time:43050ms step_avg:42.25ms
step:1020/2160 train_time:43110ms step_avg:42.27ms
step:1021/2160 train_time:43175ms step_avg:42.29ms
step:1022/2160 train_time:43234ms step_avg:42.30ms
step:1023/2160 train_time:43296ms step_avg:42.32ms
step:1024/2160 train_time:43356ms step_avg:42.34ms
step:1025/2160 train_time:43418ms step_avg:42.36ms
step:1026/2160 train_time:43477ms step_avg:42.38ms
step:1027/2160 train_time:43539ms step_avg:42.39ms
step:1028/2160 train_time:43598ms step_avg:42.41ms
step:1029/2160 train_time:43659ms step_avg:42.43ms
step:1030/2160 train_time:43718ms step_avg:42.44ms
step:1031/2160 train_time:43779ms step_avg:42.46ms
step:1032/2160 train_time:43839ms step_avg:42.48ms
step:1033/2160 train_time:43901ms step_avg:42.50ms
step:1034/2160 train_time:43962ms step_avg:42.52ms
step:1035/2160 train_time:44026ms step_avg:42.54ms
step:1036/2160 train_time:44087ms step_avg:42.55ms
step:1037/2160 train_time:44149ms step_avg:42.57ms
step:1038/2160 train_time:44209ms step_avg:42.59ms
step:1039/2160 train_time:44272ms step_avg:42.61ms
step:1040/2160 train_time:44333ms step_avg:42.63ms
step:1041/2160 train_time:44395ms step_avg:42.65ms
step:1042/2160 train_time:44456ms step_avg:42.66ms
step:1043/2160 train_time:44518ms step_avg:42.68ms
step:1044/2160 train_time:44578ms step_avg:42.70ms
step:1045/2160 train_time:44639ms step_avg:42.72ms
step:1046/2160 train_time:44698ms step_avg:42.73ms
step:1047/2160 train_time:44759ms step_avg:42.75ms
step:1048/2160 train_time:44818ms step_avg:42.77ms
step:1049/2160 train_time:44880ms step_avg:42.78ms
step:1050/2160 train_time:44940ms step_avg:42.80ms
step:1051/2160 train_time:45003ms step_avg:42.82ms
step:1052/2160 train_time:45064ms step_avg:42.84ms
step:1053/2160 train_time:45127ms step_avg:42.86ms
step:1054/2160 train_time:45187ms step_avg:42.87ms
step:1055/2160 train_time:45249ms step_avg:42.89ms
step:1056/2160 train_time:45310ms step_avg:42.91ms
step:1057/2160 train_time:45371ms step_avg:42.92ms
step:1058/2160 train_time:45432ms step_avg:42.94ms
step:1059/2160 train_time:45494ms step_avg:42.96ms
step:1060/2160 train_time:45554ms step_avg:42.98ms
step:1061/2160 train_time:45616ms step_avg:42.99ms
step:1062/2160 train_time:45676ms step_avg:43.01ms
step:1063/2160 train_time:45738ms step_avg:43.03ms
step:1064/2160 train_time:45797ms step_avg:43.04ms
step:1065/2160 train_time:45859ms step_avg:43.06ms
step:1066/2160 train_time:45919ms step_avg:43.08ms
step:1067/2160 train_time:45981ms step_avg:43.09ms
step:1068/2160 train_time:46041ms step_avg:43.11ms
step:1069/2160 train_time:46103ms step_avg:43.13ms
step:1070/2160 train_time:46164ms step_avg:43.14ms
step:1071/2160 train_time:46227ms step_avg:43.16ms
step:1072/2160 train_time:46288ms step_avg:43.18ms
step:1073/2160 train_time:46350ms step_avg:43.20ms
step:1074/2160 train_time:46410ms step_avg:43.21ms
step:1075/2160 train_time:46473ms step_avg:43.23ms
step:1076/2160 train_time:46533ms step_avg:43.25ms
step:1077/2160 train_time:46595ms step_avg:43.26ms
step:1078/2160 train_time:46655ms step_avg:43.28ms
step:1079/2160 train_time:46718ms step_avg:43.30ms
step:1080/2160 train_time:46778ms step_avg:43.31ms
step:1081/2160 train_time:46839ms step_avg:43.33ms
step:1082/2160 train_time:46899ms step_avg:43.34ms
step:1083/2160 train_time:46961ms step_avg:43.36ms
step:1084/2160 train_time:47020ms step_avg:43.38ms
step:1085/2160 train_time:47082ms step_avg:43.39ms
step:1086/2160 train_time:47142ms step_avg:43.41ms
step:1087/2160 train_time:47204ms step_avg:43.43ms
step:1088/2160 train_time:47264ms step_avg:43.44ms
step:1089/2160 train_time:47327ms step_avg:43.46ms
step:1090/2160 train_time:47388ms step_avg:43.47ms
step:1091/2160 train_time:47450ms step_avg:43.49ms
step:1092/2160 train_time:47510ms step_avg:43.51ms
step:1093/2160 train_time:47572ms step_avg:43.52ms
step:1094/2160 train_time:47633ms step_avg:43.54ms
step:1095/2160 train_time:47695ms step_avg:43.56ms
step:1096/2160 train_time:47755ms step_avg:43.57ms
step:1097/2160 train_time:47817ms step_avg:43.59ms
step:1098/2160 train_time:47879ms step_avg:43.61ms
step:1099/2160 train_time:47939ms step_avg:43.62ms
step:1100/2160 train_time:47999ms step_avg:43.64ms
step:1101/2160 train_time:48060ms step_avg:43.65ms
step:1102/2160 train_time:48120ms step_avg:43.67ms
step:1103/2160 train_time:48182ms step_avg:43.68ms
step:1104/2160 train_time:48242ms step_avg:43.70ms
step:1105/2160 train_time:48305ms step_avg:43.71ms
step:1106/2160 train_time:48365ms step_avg:43.73ms
step:1107/2160 train_time:48428ms step_avg:43.75ms
step:1108/2160 train_time:48488ms step_avg:43.76ms
step:1109/2160 train_time:48550ms step_avg:43.78ms
step:1110/2160 train_time:48611ms step_avg:43.79ms
step:1111/2160 train_time:48673ms step_avg:43.81ms
step:1112/2160 train_time:48734ms step_avg:43.83ms
step:1113/2160 train_time:48797ms step_avg:43.84ms
step:1114/2160 train_time:48857ms step_avg:43.86ms
step:1115/2160 train_time:48919ms step_avg:43.87ms
step:1116/2160 train_time:48978ms step_avg:43.89ms
step:1117/2160 train_time:49040ms step_avg:43.90ms
step:1118/2160 train_time:49100ms step_avg:43.92ms
step:1119/2160 train_time:49162ms step_avg:43.93ms
step:1120/2160 train_time:49221ms step_avg:43.95ms
step:1121/2160 train_time:49283ms step_avg:43.96ms
step:1122/2160 train_time:49343ms step_avg:43.98ms
step:1123/2160 train_time:49405ms step_avg:43.99ms
step:1124/2160 train_time:49466ms step_avg:44.01ms
step:1125/2160 train_time:49528ms step_avg:44.02ms
step:1126/2160 train_time:49589ms step_avg:44.04ms
step:1127/2160 train_time:49651ms step_avg:44.06ms
step:1128/2160 train_time:49711ms step_avg:44.07ms
step:1129/2160 train_time:49774ms step_avg:44.09ms
step:1130/2160 train_time:49835ms step_avg:44.10ms
step:1131/2160 train_time:49897ms step_avg:44.12ms
step:1132/2160 train_time:49957ms step_avg:44.13ms
step:1133/2160 train_time:50019ms step_avg:44.15ms
step:1134/2160 train_time:50079ms step_avg:44.16ms
step:1135/2160 train_time:50140ms step_avg:44.18ms
step:1136/2160 train_time:50200ms step_avg:44.19ms
step:1137/2160 train_time:50261ms step_avg:44.20ms
step:1138/2160 train_time:50321ms step_avg:44.22ms
step:1139/2160 train_time:50382ms step_avg:44.23ms
step:1140/2160 train_time:50443ms step_avg:44.25ms
step:1141/2160 train_time:50506ms step_avg:44.26ms
step:1142/2160 train_time:50566ms step_avg:44.28ms
step:1143/2160 train_time:50629ms step_avg:44.29ms
step:1144/2160 train_time:50689ms step_avg:44.31ms
step:1145/2160 train_time:50752ms step_avg:44.33ms
step:1146/2160 train_time:50813ms step_avg:44.34ms
step:1147/2160 train_time:50875ms step_avg:44.35ms
step:1148/2160 train_time:50935ms step_avg:44.37ms
step:1149/2160 train_time:50999ms step_avg:44.39ms
step:1150/2160 train_time:51058ms step_avg:44.40ms
step:1151/2160 train_time:51120ms step_avg:44.41ms
step:1152/2160 train_time:51180ms step_avg:44.43ms
step:1153/2160 train_time:51241ms step_avg:44.44ms
step:1154/2160 train_time:51301ms step_avg:44.46ms
step:1155/2160 train_time:51363ms step_avg:44.47ms
step:1156/2160 train_time:51423ms step_avg:44.48ms
step:1157/2160 train_time:51484ms step_avg:44.50ms
step:1158/2160 train_time:51544ms step_avg:44.51ms
step:1159/2160 train_time:51607ms step_avg:44.53ms
step:1160/2160 train_time:51667ms step_avg:44.54ms
step:1161/2160 train_time:51729ms step_avg:44.56ms
step:1162/2160 train_time:51790ms step_avg:44.57ms
step:1163/2160 train_time:51853ms step_avg:44.59ms
step:1164/2160 train_time:51913ms step_avg:44.60ms
step:1165/2160 train_time:51976ms step_avg:44.61ms
step:1166/2160 train_time:52036ms step_avg:44.63ms
step:1167/2160 train_time:52098ms step_avg:44.64ms
step:1168/2160 train_time:52158ms step_avg:44.66ms
step:1169/2160 train_time:52219ms step_avg:44.67ms
step:1170/2160 train_time:52279ms step_avg:44.68ms
step:1171/2160 train_time:52341ms step_avg:44.70ms
step:1172/2160 train_time:52401ms step_avg:44.71ms
step:1173/2160 train_time:52462ms step_avg:44.73ms
step:1174/2160 train_time:52522ms step_avg:44.74ms
step:1175/2160 train_time:52585ms step_avg:44.75ms
step:1176/2160 train_time:52645ms step_avg:44.77ms
step:1177/2160 train_time:52707ms step_avg:44.78ms
step:1178/2160 train_time:52768ms step_avg:44.79ms
step:1179/2160 train_time:52830ms step_avg:44.81ms
step:1180/2160 train_time:52892ms step_avg:44.82ms
step:1181/2160 train_time:52955ms step_avg:44.84ms
step:1182/2160 train_time:53015ms step_avg:44.85ms
step:1183/2160 train_time:53076ms step_avg:44.87ms
step:1184/2160 train_time:53137ms step_avg:44.88ms
step:1185/2160 train_time:53199ms step_avg:44.89ms
step:1186/2160 train_time:53259ms step_avg:44.91ms
step:1187/2160 train_time:53321ms step_avg:44.92ms
step:1188/2160 train_time:53381ms step_avg:44.93ms
step:1189/2160 train_time:53442ms step_avg:44.95ms
step:1190/2160 train_time:53502ms step_avg:44.96ms
step:1191/2160 train_time:53568ms step_avg:44.98ms
step:1192/2160 train_time:53624ms step_avg:44.99ms
step:1193/2160 train_time:53687ms step_avg:45.00ms
step:1194/2160 train_time:53748ms step_avg:45.02ms
step:1195/2160 train_time:53810ms step_avg:45.03ms
step:1196/2160 train_time:53871ms step_avg:45.04ms
step:1197/2160 train_time:53935ms step_avg:45.06ms
step:1198/2160 train_time:53995ms step_avg:45.07ms
step:1199/2160 train_time:54057ms step_avg:45.08ms
step:1200/2160 train_time:54117ms step_avg:45.10ms
step:1201/2160 train_time:54179ms step_avg:45.11ms
step:1202/2160 train_time:54239ms step_avg:45.12ms
step:1203/2160 train_time:54300ms step_avg:45.14ms
step:1204/2160 train_time:54359ms step_avg:45.15ms
step:1205/2160 train_time:54421ms step_avg:45.16ms
step:1206/2160 train_time:54481ms step_avg:45.17ms
step:1207/2160 train_time:54542ms step_avg:45.19ms
step:1208/2160 train_time:54602ms step_avg:45.20ms
step:1209/2160 train_time:54664ms step_avg:45.21ms
step:1210/2160 train_time:54724ms step_avg:45.23ms
step:1211/2160 train_time:54786ms step_avg:45.24ms
step:1212/2160 train_time:54847ms step_avg:45.25ms
step:1213/2160 train_time:54909ms step_avg:45.27ms
step:1214/2160 train_time:54970ms step_avg:45.28ms
step:1215/2160 train_time:55033ms step_avg:45.29ms
step:1216/2160 train_time:55094ms step_avg:45.31ms
step:1217/2160 train_time:55157ms step_avg:45.32ms
step:1218/2160 train_time:55217ms step_avg:45.33ms
step:1219/2160 train_time:55278ms step_avg:45.35ms
step:1220/2160 train_time:55339ms step_avg:45.36ms
step:1221/2160 train_time:55401ms step_avg:45.37ms
step:1222/2160 train_time:55460ms step_avg:45.38ms
step:1223/2160 train_time:55522ms step_avg:45.40ms
step:1224/2160 train_time:55581ms step_avg:45.41ms
step:1225/2160 train_time:55642ms step_avg:45.42ms
step:1226/2160 train_time:55703ms step_avg:45.43ms
step:1227/2160 train_time:55765ms step_avg:45.45ms
step:1228/2160 train_time:55825ms step_avg:45.46ms
step:1229/2160 train_time:55887ms step_avg:45.47ms
step:1230/2160 train_time:55948ms step_avg:45.49ms
step:1231/2160 train_time:56010ms step_avg:45.50ms
step:1232/2160 train_time:56072ms step_avg:45.51ms
step:1233/2160 train_time:56135ms step_avg:45.53ms
step:1234/2160 train_time:56197ms step_avg:45.54ms
step:1235/2160 train_time:56258ms step_avg:45.55ms
step:1236/2160 train_time:56318ms step_avg:45.56ms
step:1237/2160 train_time:56379ms step_avg:45.58ms
step:1238/2160 train_time:56439ms step_avg:45.59ms
step:1239/2160 train_time:56501ms step_avg:45.60ms
step:1240/2160 train_time:56561ms step_avg:45.61ms
step:1241/2160 train_time:56622ms step_avg:45.63ms
step:1242/2160 train_time:56682ms step_avg:45.64ms
step:1243/2160 train_time:56743ms step_avg:45.65ms
step:1244/2160 train_time:56804ms step_avg:45.66ms
step:1245/2160 train_time:56866ms step_avg:45.68ms
step:1246/2160 train_time:56926ms step_avg:45.69ms
step:1247/2160 train_time:56989ms step_avg:45.70ms
step:1248/2160 train_time:57050ms step_avg:45.71ms
step:1249/2160 train_time:57112ms step_avg:45.73ms
step:1250/2160 train_time:57173ms step_avg:45.74ms
step:1250/2160 val_loss:3.5689 train_time:57235ms step_avg:45.79ms
step:1251/2160 train_time:57259ms step_avg:45.77ms
step:1252/2160 train_time:57297ms step_avg:45.76ms
step:1253/2160 train_time:57365ms step_avg:45.78ms
step:1254/2160 train_time:57426ms step_avg:45.79ms
step:1255/2160 train_time:57489ms step_avg:45.81ms
step:1256/2160 train_time:57549ms step_avg:45.82ms
step:1257/2160 train_time:57610ms step_avg:45.83ms
step:1258/2160 train_time:57669ms step_avg:45.84ms
step:1259/2160 train_time:57730ms step_avg:45.85ms
step:1260/2160 train_time:57790ms step_avg:45.86ms
step:1261/2160 train_time:57850ms step_avg:45.88ms
step:1262/2160 train_time:57909ms step_avg:45.89ms
step:1263/2160 train_time:57971ms step_avg:45.90ms
step:1264/2160 train_time:58030ms step_avg:45.91ms
step:1265/2160 train_time:58091ms step_avg:45.92ms
step:1266/2160 train_time:58151ms step_avg:45.93ms
step:1267/2160 train_time:58215ms step_avg:45.95ms
step:1268/2160 train_time:58277ms step_avg:45.96ms
step:1269/2160 train_time:58340ms step_avg:45.97ms
step:1270/2160 train_time:58402ms step_avg:45.99ms
step:1271/2160 train_time:58465ms step_avg:46.00ms
step:1272/2160 train_time:58526ms step_avg:46.01ms
step:1273/2160 train_time:58588ms step_avg:46.02ms
step:1274/2160 train_time:58648ms step_avg:46.03ms
step:1275/2160 train_time:58710ms step_avg:46.05ms
step:1276/2160 train_time:58770ms step_avg:46.06ms
step:1277/2160 train_time:58830ms step_avg:46.07ms
step:1278/2160 train_time:58889ms step_avg:46.08ms
step:1279/2160 train_time:58950ms step_avg:46.09ms
step:1280/2160 train_time:59009ms step_avg:46.10ms
step:1281/2160 train_time:59071ms step_avg:46.11ms
step:1282/2160 train_time:59131ms step_avg:46.12ms
step:1283/2160 train_time:59193ms step_avg:46.14ms
step:1284/2160 train_time:59254ms step_avg:46.15ms
step:1285/2160 train_time:59317ms step_avg:46.16ms
step:1286/2160 train_time:59378ms step_avg:46.17ms
step:1287/2160 train_time:59440ms step_avg:46.19ms
step:1288/2160 train_time:59501ms step_avg:46.20ms
step:1289/2160 train_time:59563ms step_avg:46.21ms
step:1290/2160 train_time:59624ms step_avg:46.22ms
step:1291/2160 train_time:59686ms step_avg:46.23ms
step:1292/2160 train_time:59746ms step_avg:46.24ms
step:1293/2160 train_time:59808ms step_avg:46.25ms
step:1294/2160 train_time:59868ms step_avg:46.27ms
step:1295/2160 train_time:59929ms step_avg:46.28ms
step:1296/2160 train_time:59989ms step_avg:46.29ms
step:1297/2160 train_time:60050ms step_avg:46.30ms
step:1298/2160 train_time:60110ms step_avg:46.31ms
step:1299/2160 train_time:60173ms step_avg:46.32ms
step:1300/2160 train_time:60234ms step_avg:46.33ms
step:1301/2160 train_time:60296ms step_avg:46.35ms
step:1302/2160 train_time:60357ms step_avg:46.36ms
step:1303/2160 train_time:60419ms step_avg:46.37ms
step:1304/2160 train_time:60479ms step_avg:46.38ms
step:1305/2160 train_time:60541ms step_avg:46.39ms
step:1306/2160 train_time:60602ms step_avg:46.40ms
step:1307/2160 train_time:60663ms step_avg:46.41ms
step:1308/2160 train_time:60724ms step_avg:46.42ms
step:1309/2160 train_time:60786ms step_avg:46.44ms
step:1310/2160 train_time:60846ms step_avg:46.45ms
step:1311/2160 train_time:60908ms step_avg:46.46ms
step:1312/2160 train_time:60968ms step_avg:46.47ms
step:1313/2160 train_time:61031ms step_avg:46.48ms
step:1314/2160 train_time:61091ms step_avg:46.49ms
step:1315/2160 train_time:61153ms step_avg:46.50ms
step:1316/2160 train_time:61212ms step_avg:46.51ms
step:1317/2160 train_time:61275ms step_avg:46.53ms
step:1318/2160 train_time:61335ms step_avg:46.54ms
step:1319/2160 train_time:61396ms step_avg:46.55ms
step:1320/2160 train_time:61457ms step_avg:46.56ms
step:1321/2160 train_time:61518ms step_avg:46.57ms
step:1322/2160 train_time:61578ms step_avg:46.58ms
step:1323/2160 train_time:61641ms step_avg:46.59ms
step:1324/2160 train_time:61701ms step_avg:46.60ms
step:1325/2160 train_time:61764ms step_avg:46.61ms
step:1326/2160 train_time:61824ms step_avg:46.62ms
step:1327/2160 train_time:61886ms step_avg:46.64ms
step:1328/2160 train_time:61946ms step_avg:46.65ms
step:1329/2160 train_time:62008ms step_avg:46.66ms
step:1330/2160 train_time:62069ms step_avg:46.67ms
step:1331/2160 train_time:62132ms step_avg:46.68ms
step:1332/2160 train_time:62192ms step_avg:46.69ms
step:1333/2160 train_time:62254ms step_avg:46.70ms
step:1334/2160 train_time:62314ms step_avg:46.71ms
step:1335/2160 train_time:62376ms step_avg:46.72ms
step:1336/2160 train_time:62436ms step_avg:46.73ms
step:1337/2160 train_time:62497ms step_avg:46.74ms
step:1338/2160 train_time:62557ms step_avg:46.75ms
step:1339/2160 train_time:62619ms step_avg:46.77ms
step:1340/2160 train_time:62680ms step_avg:46.78ms
step:1341/2160 train_time:62741ms step_avg:46.79ms
step:1342/2160 train_time:62802ms step_avg:46.80ms
step:1343/2160 train_time:62864ms step_avg:46.81ms
step:1344/2160 train_time:62924ms step_avg:46.82ms
step:1345/2160 train_time:62987ms step_avg:46.83ms
step:1346/2160 train_time:63047ms step_avg:46.84ms
step:1347/2160 train_time:63110ms step_avg:46.85ms
step:1348/2160 train_time:63170ms step_avg:46.86ms
step:1349/2160 train_time:63233ms step_avg:46.87ms
step:1350/2160 train_time:63292ms step_avg:46.88ms
step:1351/2160 train_time:63354ms step_avg:46.89ms
step:1352/2160 train_time:63414ms step_avg:46.90ms
step:1353/2160 train_time:63476ms step_avg:46.91ms
step:1354/2160 train_time:63536ms step_avg:46.92ms
step:1355/2160 train_time:63597ms step_avg:46.93ms
step:1356/2160 train_time:63656ms step_avg:46.94ms
step:1357/2160 train_time:63718ms step_avg:46.96ms
step:1358/2160 train_time:63778ms step_avg:46.96ms
step:1359/2160 train_time:63841ms step_avg:46.98ms
step:1360/2160 train_time:63901ms step_avg:46.99ms
step:1361/2160 train_time:63963ms step_avg:47.00ms
step:1362/2160 train_time:64024ms step_avg:47.01ms
step:1363/2160 train_time:64087ms step_avg:47.02ms
step:1364/2160 train_time:64148ms step_avg:47.03ms
step:1365/2160 train_time:64209ms step_avg:47.04ms
step:1366/2160 train_time:64269ms step_avg:47.05ms
step:1367/2160 train_time:64331ms step_avg:47.06ms
step:1368/2160 train_time:64391ms step_avg:47.07ms
step:1369/2160 train_time:64453ms step_avg:47.08ms
step:1370/2160 train_time:64514ms step_avg:47.09ms
step:1371/2160 train_time:64576ms step_avg:47.10ms
step:1372/2160 train_time:64636ms step_avg:47.11ms
step:1373/2160 train_time:64697ms step_avg:47.12ms
step:1374/2160 train_time:64757ms step_avg:47.13ms
step:1375/2160 train_time:64820ms step_avg:47.14ms
step:1376/2160 train_time:64879ms step_avg:47.15ms
step:1377/2160 train_time:64942ms step_avg:47.16ms
step:1378/2160 train_time:65003ms step_avg:47.17ms
step:1379/2160 train_time:65065ms step_avg:47.18ms
step:1380/2160 train_time:65126ms step_avg:47.19ms
step:1381/2160 train_time:65188ms step_avg:47.20ms
step:1382/2160 train_time:65248ms step_avg:47.21ms
step:1383/2160 train_time:65310ms step_avg:47.22ms
step:1384/2160 train_time:65370ms step_avg:47.23ms
step:1385/2160 train_time:65432ms step_avg:47.24ms
step:1386/2160 train_time:65491ms step_avg:47.25ms
step:1387/2160 train_time:65552ms step_avg:47.26ms
step:1388/2160 train_time:65612ms step_avg:47.27ms
step:1389/2160 train_time:65674ms step_avg:47.28ms
step:1390/2160 train_time:65735ms step_avg:47.29ms
step:1391/2160 train_time:65798ms step_avg:47.30ms
step:1392/2160 train_time:65857ms step_avg:47.31ms
step:1393/2160 train_time:65919ms step_avg:47.32ms
step:1394/2160 train_time:65979ms step_avg:47.33ms
step:1395/2160 train_time:66042ms step_avg:47.34ms
step:1396/2160 train_time:66103ms step_avg:47.35ms
step:1397/2160 train_time:66165ms step_avg:47.36ms
step:1398/2160 train_time:66226ms step_avg:47.37ms
step:1399/2160 train_time:66288ms step_avg:47.38ms
step:1400/2160 train_time:66349ms step_avg:47.39ms
step:1401/2160 train_time:66412ms step_avg:47.40ms
step:1402/2160 train_time:66471ms step_avg:47.41ms
step:1403/2160 train_time:66533ms step_avg:47.42ms
step:1404/2160 train_time:66593ms step_avg:47.43ms
step:1405/2160 train_time:66654ms step_avg:47.44ms
step:1406/2160 train_time:66714ms step_avg:47.45ms
step:1407/2160 train_time:66776ms step_avg:47.46ms
step:1408/2160 train_time:66836ms step_avg:47.47ms
step:1409/2160 train_time:66897ms step_avg:47.48ms
step:1410/2160 train_time:66958ms step_avg:47.49ms
step:1411/2160 train_time:67021ms step_avg:47.50ms
step:1412/2160 train_time:67081ms step_avg:47.51ms
step:1413/2160 train_time:67143ms step_avg:47.52ms
step:1414/2160 train_time:67204ms step_avg:47.53ms
step:1415/2160 train_time:67267ms step_avg:47.54ms
step:1416/2160 train_time:67357ms step_avg:47.57ms
step:1417/2160 train_time:67446ms step_avg:47.60ms
step:1418/2160 train_time:67535ms step_avg:47.63ms
step:1419/2160 train_time:67625ms step_avg:47.66ms
step:1420/2160 train_time:67715ms step_avg:47.69ms
step:1421/2160 train_time:67804ms step_avg:47.72ms
step:1422/2160 train_time:67894ms step_avg:47.75ms
step:1423/2160 train_time:67983ms step_avg:47.77ms
step:1424/2160 train_time:68071ms step_avg:47.80ms
step:1425/2160 train_time:68161ms step_avg:47.83ms
step:1426/2160 train_time:68250ms step_avg:47.86ms
step:1427/2160 train_time:68341ms step_avg:47.89ms
step:1428/2160 train_time:68429ms step_avg:47.92ms
step:1429/2160 train_time:68520ms step_avg:47.95ms
step:1430/2160 train_time:68608ms step_avg:47.98ms
step:1431/2160 train_time:68700ms step_avg:48.01ms
step:1432/2160 train_time:68788ms step_avg:48.04ms
step:1433/2160 train_time:68879ms step_avg:48.07ms
step:1434/2160 train_time:68967ms step_avg:48.09ms
step:1435/2160 train_time:69057ms step_avg:48.12ms
step:1436/2160 train_time:69144ms step_avg:48.15ms
step:1437/2160 train_time:69234ms step_avg:48.18ms
step:1438/2160 train_time:69322ms step_avg:48.21ms
step:1439/2160 train_time:69412ms step_avg:48.24ms
step:1440/2160 train_time:69501ms step_avg:48.26ms
step:1441/2160 train_time:69592ms step_avg:48.29ms
step:1442/2160 train_time:69680ms step_avg:48.32ms
step:1443/2160 train_time:69770ms step_avg:48.35ms
step:1444/2160 train_time:69858ms step_avg:48.38ms
step:1445/2160 train_time:69947ms step_avg:48.41ms
step:1446/2160 train_time:70036ms step_avg:48.43ms
step:1447/2160 train_time:70126ms step_avg:48.46ms
step:1448/2160 train_time:70215ms step_avg:48.49ms
step:1449/2160 train_time:70305ms step_avg:48.52ms
step:1450/2160 train_time:70394ms step_avg:48.55ms
step:1451/2160 train_time:70484ms step_avg:48.58ms
step:1452/2160 train_time:70573ms step_avg:48.60ms
step:1453/2160 train_time:70664ms step_avg:48.63ms
step:1454/2160 train_time:70752ms step_avg:48.66ms
step:1455/2160 train_time:70842ms step_avg:48.69ms
step:1456/2160 train_time:70930ms step_avg:48.72ms
step:1457/2160 train_time:71020ms step_avg:48.74ms
step:1458/2160 train_time:71108ms step_avg:48.77ms
step:1459/2160 train_time:71198ms step_avg:48.80ms
step:1460/2160 train_time:71286ms step_avg:48.83ms
step:1461/2160 train_time:71376ms step_avg:48.85ms
step:1462/2160 train_time:71464ms step_avg:48.88ms
step:1463/2160 train_time:71554ms step_avg:48.91ms
step:1464/2160 train_time:71642ms step_avg:48.94ms
step:1465/2160 train_time:71732ms step_avg:48.96ms
step:1466/2160 train_time:71820ms step_avg:48.99ms
step:1467/2160 train_time:71910ms step_avg:49.02ms
step:1468/2160 train_time:71998ms step_avg:49.05ms
step:1469/2160 train_time:72088ms step_avg:49.07ms
step:1470/2160 train_time:72177ms step_avg:49.10ms
step:1471/2160 train_time:72266ms step_avg:49.13ms
step:1472/2160 train_time:72354ms step_avg:49.15ms
step:1473/2160 train_time:72444ms step_avg:49.18ms
step:1474/2160 train_time:72532ms step_avg:49.21ms
step:1475/2160 train_time:72622ms step_avg:49.24ms
step:1476/2160 train_time:72710ms step_avg:49.26ms
step:1477/2160 train_time:72801ms step_avg:49.29ms
step:1478/2160 train_time:72889ms step_avg:49.32ms
step:1479/2160 train_time:72978ms step_avg:49.34ms
step:1480/2160 train_time:73067ms step_avg:49.37ms
step:1481/2160 train_time:73157ms step_avg:49.40ms
step:1482/2160 train_time:73244ms step_avg:49.42ms
step:1483/2160 train_time:73334ms step_avg:49.45ms
step:1484/2160 train_time:73422ms step_avg:49.48ms
step:1485/2160 train_time:73512ms step_avg:49.50ms
step:1486/2160 train_time:73601ms step_avg:49.53ms
step:1487/2160 train_time:73690ms step_avg:49.56ms
step:1488/2160 train_time:73778ms step_avg:49.58ms
step:1489/2160 train_time:73868ms step_avg:49.61ms
step:1490/2160 train_time:73957ms step_avg:49.64ms
step:1491/2160 train_time:74047ms step_avg:49.66ms
step:1492/2160 train_time:74135ms step_avg:49.69ms
step:1493/2160 train_time:74225ms step_avg:49.72ms
step:1494/2160 train_time:74314ms step_avg:49.74ms
step:1495/2160 train_time:74404ms step_avg:49.77ms
step:1496/2160 train_time:74492ms step_avg:49.79ms
step:1497/2160 train_time:74582ms step_avg:49.82ms
step:1498/2160 train_time:74671ms step_avg:49.85ms
step:1499/2160 train_time:74761ms step_avg:49.87ms
step:1500/2160 train_time:74849ms step_avg:49.90ms
step:1500/2160 val_loss:3.4693 train_time:74939ms step_avg:49.96ms
step:1501/2160 train_time:74964ms step_avg:49.94ms
step:1502/2160 train_time:75029ms step_avg:49.95ms
step:1503/2160 train_time:75122ms step_avg:49.98ms
step:1504/2160 train_time:75205ms step_avg:50.00ms
step:1505/2160 train_time:75292ms step_avg:50.03ms
step:1506/2160 train_time:75379ms step_avg:50.05ms
step:1507/2160 train_time:75467ms step_avg:50.08ms
step:1508/2160 train_time:75553ms step_avg:50.10ms
step:1509/2160 train_time:75641ms step_avg:50.13ms
step:1510/2160 train_time:75727ms step_avg:50.15ms
step:1511/2160 train_time:75819ms step_avg:50.18ms
step:1512/2160 train_time:75913ms step_avg:50.21ms
step:1513/2160 train_time:76007ms step_avg:50.24ms
step:1514/2160 train_time:76095ms step_avg:50.26ms
step:1515/2160 train_time:76183ms step_avg:50.29ms
step:1516/2160 train_time:76270ms step_avg:50.31ms
step:1517/2160 train_time:76359ms step_avg:50.34ms
step:1518/2160 train_time:76446ms step_avg:50.36ms
step:1519/2160 train_time:76534ms step_avg:50.38ms
step:1520/2160 train_time:76620ms step_avg:50.41ms
step:1521/2160 train_time:76709ms step_avg:50.43ms
step:1522/2160 train_time:76798ms step_avg:50.46ms
step:1523/2160 train_time:76891ms step_avg:50.49ms
step:1524/2160 train_time:76982ms step_avg:50.51ms
step:1525/2160 train_time:77071ms step_avg:50.54ms
step:1526/2160 train_time:77159ms step_avg:50.56ms
step:1527/2160 train_time:77247ms step_avg:50.59ms
step:1528/2160 train_time:77335ms step_avg:50.61ms
step:1529/2160 train_time:77423ms step_avg:50.64ms
step:1530/2160 train_time:77511ms step_avg:50.66ms
step:1531/2160 train_time:77600ms step_avg:50.69ms
step:1532/2160 train_time:77687ms step_avg:50.71ms
step:1533/2160 train_time:77776ms step_avg:50.73ms
step:1534/2160 train_time:77866ms step_avg:50.76ms
step:1535/2160 train_time:77959ms step_avg:50.79ms
step:1536/2160 train_time:78047ms step_avg:50.81ms
step:1537/2160 train_time:78136ms step_avg:50.84ms
step:1538/2160 train_time:78224ms step_avg:50.86ms
step:1539/2160 train_time:78313ms step_avg:50.89ms
step:1540/2160 train_time:78399ms step_avg:50.91ms
step:1541/2160 train_time:78488ms step_avg:50.93ms
step:1542/2160 train_time:78575ms step_avg:50.96ms
step:1543/2160 train_time:78664ms step_avg:50.98ms
step:1544/2160 train_time:78751ms step_avg:51.00ms
step:1545/2160 train_time:78841ms step_avg:51.03ms
step:1546/2160 train_time:78930ms step_avg:51.05ms
step:1547/2160 train_time:79021ms step_avg:51.08ms
step:1548/2160 train_time:79108ms step_avg:51.10ms
step:1549/2160 train_time:79198ms step_avg:51.13ms
step:1550/2160 train_time:79285ms step_avg:51.15ms
step:1551/2160 train_time:79374ms step_avg:51.18ms
step:1552/2160 train_time:79462ms step_avg:51.20ms
step:1553/2160 train_time:79550ms step_avg:51.22ms
step:1554/2160 train_time:79637ms step_avg:51.25ms
step:1555/2160 train_time:79727ms step_avg:51.27ms
step:1556/2160 train_time:79814ms step_avg:51.29ms
step:1557/2160 train_time:79905ms step_avg:51.32ms
step:1558/2160 train_time:79993ms step_avg:51.34ms
step:1559/2160 train_time:80083ms step_avg:51.37ms
step:1560/2160 train_time:80171ms step_avg:51.39ms
step:1561/2160 train_time:80261ms step_avg:51.42ms
step:1562/2160 train_time:80347ms step_avg:51.44ms
step:1563/2160 train_time:80436ms step_avg:51.46ms
step:1564/2160 train_time:80522ms step_avg:51.48ms
step:1565/2160 train_time:80612ms step_avg:51.51ms
step:1566/2160 train_time:80700ms step_avg:51.53ms
step:1567/2160 train_time:80789ms step_avg:51.56ms
step:1568/2160 train_time:80877ms step_avg:51.58ms
step:1569/2160 train_time:80967ms step_avg:51.60ms
step:1570/2160 train_time:81056ms step_avg:51.63ms
step:1571/2160 train_time:81145ms step_avg:51.65ms
step:1572/2160 train_time:81233ms step_avg:51.67ms
step:1573/2160 train_time:81324ms step_avg:51.70ms
step:1574/2160 train_time:81411ms step_avg:51.72ms
step:1575/2160 train_time:81501ms step_avg:51.75ms
step:1576/2160 train_time:81588ms step_avg:51.77ms
step:1577/2160 train_time:81677ms step_avg:51.79ms
step:1578/2160 train_time:81764ms step_avg:51.82ms
step:1579/2160 train_time:81853ms step_avg:51.84ms
step:1580/2160 train_time:81942ms step_avg:51.86ms
step:1581/2160 train_time:82031ms step_avg:51.89ms
step:1582/2160 train_time:82119ms step_avg:51.91ms
step:1583/2160 train_time:82208ms step_avg:51.93ms
step:1584/2160 train_time:82296ms step_avg:51.95ms
step:1585/2160 train_time:82386ms step_avg:51.98ms
step:1586/2160 train_time:82473ms step_avg:52.00ms
step:1587/2160 train_time:82563ms step_avg:52.02ms
step:1588/2160 train_time:82650ms step_avg:52.05ms
step:1589/2160 train_time:82740ms step_avg:52.07ms
step:1590/2160 train_time:82827ms step_avg:52.09ms
step:1591/2160 train_time:82917ms step_avg:52.12ms
step:1592/2160 train_time:83005ms step_avg:52.14ms
step:1593/2160 train_time:83095ms step_avg:52.16ms
step:1594/2160 train_time:83183ms step_avg:52.18ms
step:1595/2160 train_time:83272ms step_avg:52.21ms
step:1596/2160 train_time:83360ms step_avg:52.23ms
step:1597/2160 train_time:83449ms step_avg:52.25ms
step:1598/2160 train_time:83537ms step_avg:52.28ms
step:1599/2160 train_time:83628ms step_avg:52.30ms
step:1600/2160 train_time:83715ms step_avg:52.32ms
step:1601/2160 train_time:83804ms step_avg:52.35ms
step:1602/2160 train_time:83892ms step_avg:52.37ms
step:1603/2160 train_time:83982ms step_avg:52.39ms
step:1604/2160 train_time:84070ms step_avg:52.41ms
step:1605/2160 train_time:84160ms step_avg:52.44ms
step:1606/2160 train_time:84247ms step_avg:52.46ms
step:1607/2160 train_time:84336ms step_avg:52.48ms
step:1608/2160 train_time:84424ms step_avg:52.50ms
step:1609/2160 train_time:84513ms step_avg:52.53ms
step:1610/2160 train_time:84601ms step_avg:52.55ms
step:1611/2160 train_time:84690ms step_avg:52.57ms
step:1612/2160 train_time:84778ms step_avg:52.59ms
step:1613/2160 train_time:84867ms step_avg:52.61ms
step:1614/2160 train_time:84956ms step_avg:52.64ms
step:1615/2160 train_time:85045ms step_avg:52.66ms
step:1616/2160 train_time:85133ms step_avg:52.68ms
step:1617/2160 train_time:85223ms step_avg:52.70ms
step:1618/2160 train_time:85310ms step_avg:52.73ms
step:1619/2160 train_time:85401ms step_avg:52.75ms
step:1620/2160 train_time:85488ms step_avg:52.77ms
step:1621/2160 train_time:85578ms step_avg:52.79ms
step:1622/2160 train_time:85665ms step_avg:52.81ms
step:1623/2160 train_time:85755ms step_avg:52.84ms
step:1624/2160 train_time:85843ms step_avg:52.86ms
step:1625/2160 train_time:85932ms step_avg:52.88ms
step:1626/2160 train_time:86021ms step_avg:52.90ms
step:1627/2160 train_time:86110ms step_avg:52.93ms
step:1628/2160 train_time:86198ms step_avg:52.95ms
step:1629/2160 train_time:86288ms step_avg:52.97ms
step:1630/2160 train_time:86376ms step_avg:52.99ms
step:1631/2160 train_time:86466ms step_avg:53.01ms
step:1632/2160 train_time:86554ms step_avg:53.04ms
step:1633/2160 train_time:86644ms step_avg:53.06ms
step:1634/2160 train_time:86732ms step_avg:53.08ms
step:1635/2160 train_time:86822ms step_avg:53.10ms
step:1636/2160 train_time:86910ms step_avg:53.12ms
step:1637/2160 train_time:87000ms step_avg:53.15ms
step:1638/2160 train_time:87087ms step_avg:53.17ms
step:1639/2160 train_time:87176ms step_avg:53.19ms
step:1640/2160 train_time:87264ms step_avg:53.21ms
step:1641/2160 train_time:87354ms step_avg:53.23ms
step:1642/2160 train_time:87442ms step_avg:53.25ms
step:1643/2160 train_time:87532ms step_avg:53.28ms
step:1644/2160 train_time:87619ms step_avg:53.30ms
step:1645/2160 train_time:87709ms step_avg:53.32ms
step:1646/2160 train_time:87797ms step_avg:53.34ms
step:1647/2160 train_time:87886ms step_avg:53.36ms
step:1648/2160 train_time:87974ms step_avg:53.38ms
step:1649/2160 train_time:88063ms step_avg:53.40ms
step:1650/2160 train_time:88151ms step_avg:53.42ms
step:1651/2160 train_time:88240ms step_avg:53.45ms
step:1652/2160 train_time:88328ms step_avg:53.47ms
step:1653/2160 train_time:88417ms step_avg:53.49ms
step:1654/2160 train_time:88504ms step_avg:53.51ms
step:1655/2160 train_time:88594ms step_avg:53.53ms
step:1656/2160 train_time:88681ms step_avg:53.55ms
step:1657/2160 train_time:88771ms step_avg:53.57ms
step:1658/2160 train_time:88858ms step_avg:53.59ms
step:1659/2160 train_time:88948ms step_avg:53.62ms
step:1660/2160 train_time:89036ms step_avg:53.64ms
step:1661/2160 train_time:89126ms step_avg:53.66ms
step:1662/2160 train_time:89213ms step_avg:53.68ms
step:1663/2160 train_time:89304ms step_avg:53.70ms
step:1664/2160 train_time:89391ms step_avg:53.72ms
step:1665/2160 train_time:89481ms step_avg:53.74ms
step:1666/2160 train_time:89569ms step_avg:53.76ms
step:1667/2160 train_time:89659ms step_avg:53.78ms
step:1668/2160 train_time:89747ms step_avg:53.80ms
step:1669/2160 train_time:89836ms step_avg:53.83ms
step:1670/2160 train_time:89925ms step_avg:53.85ms
step:1671/2160 train_time:90014ms step_avg:53.87ms
step:1672/2160 train_time:90102ms step_avg:53.89ms
step:1673/2160 train_time:90191ms step_avg:53.91ms
step:1674/2160 train_time:90279ms step_avg:53.93ms
step:1675/2160 train_time:90368ms step_avg:53.95ms
step:1676/2160 train_time:90455ms step_avg:53.97ms
step:1677/2160 train_time:90546ms step_avg:53.99ms
step:1678/2160 train_time:90634ms step_avg:54.01ms
step:1679/2160 train_time:90723ms step_avg:54.03ms
step:1680/2160 train_time:90811ms step_avg:54.05ms
step:1681/2160 train_time:90901ms step_avg:54.08ms
step:1682/2160 train_time:90989ms step_avg:54.10ms
step:1683/2160 train_time:91079ms step_avg:54.12ms
step:1684/2160 train_time:91166ms step_avg:54.14ms
step:1685/2160 train_time:91256ms step_avg:54.16ms
step:1686/2160 train_time:91344ms step_avg:54.18ms
step:1687/2160 train_time:91433ms step_avg:54.20ms
step:1688/2160 train_time:91521ms step_avg:54.22ms
step:1689/2160 train_time:91610ms step_avg:54.24ms
step:1690/2160 train_time:91698ms step_avg:54.26ms
step:1691/2160 train_time:91788ms step_avg:54.28ms
step:1692/2160 train_time:91876ms step_avg:54.30ms
step:1693/2160 train_time:91966ms step_avg:54.32ms
step:1694/2160 train_time:92053ms step_avg:54.34ms
step:1695/2160 train_time:92144ms step_avg:54.36ms
step:1696/2160 train_time:92232ms step_avg:54.38ms
step:1697/2160 train_time:92322ms step_avg:54.40ms
step:1698/2160 train_time:92410ms step_avg:54.42ms
step:1699/2160 train_time:92499ms step_avg:54.44ms
step:1700/2160 train_time:92587ms step_avg:54.46ms
step:1701/2160 train_time:92677ms step_avg:54.48ms
step:1702/2160 train_time:92765ms step_avg:54.50ms
step:1703/2160 train_time:92855ms step_avg:54.52ms
step:1704/2160 train_time:92943ms step_avg:54.54ms
step:1705/2160 train_time:93032ms step_avg:54.56ms
step:1706/2160 train_time:93120ms step_avg:54.58ms
step:1707/2160 train_time:93210ms step_avg:54.60ms
step:1708/2160 train_time:93297ms step_avg:54.62ms
step:1709/2160 train_time:93388ms step_avg:54.64ms
step:1710/2160 train_time:93477ms step_avg:54.66ms
step:1711/2160 train_time:93566ms step_avg:54.69ms
step:1712/2160 train_time:93654ms step_avg:54.70ms
step:1713/2160 train_time:93744ms step_avg:54.72ms
step:1714/2160 train_time:93831ms step_avg:54.74ms
step:1715/2160 train_time:93921ms step_avg:54.76ms
step:1716/2160 train_time:94008ms step_avg:54.78ms
step:1717/2160 train_time:94097ms step_avg:54.80ms
step:1718/2160 train_time:94184ms step_avg:54.82ms
step:1719/2160 train_time:94274ms step_avg:54.84ms
step:1720/2160 train_time:94362ms step_avg:54.86ms
step:1721/2160 train_time:94452ms step_avg:54.88ms
step:1722/2160 train_time:94540ms step_avg:54.90ms
step:1723/2160 train_time:94630ms step_avg:54.92ms
step:1724/2160 train_time:94719ms step_avg:54.94ms
step:1725/2160 train_time:94809ms step_avg:54.96ms
step:1726/2160 train_time:94898ms step_avg:54.98ms
step:1727/2160 train_time:94987ms step_avg:55.00ms
step:1728/2160 train_time:95074ms step_avg:55.02ms
step:1729/2160 train_time:95164ms step_avg:55.04ms
step:1730/2160 train_time:95251ms step_avg:55.06ms
step:1731/2160 train_time:95340ms step_avg:55.08ms
step:1732/2160 train_time:95428ms step_avg:55.10ms
step:1733/2160 train_time:95517ms step_avg:55.12ms
step:1734/2160 train_time:95605ms step_avg:55.14ms
step:1735/2160 train_time:95695ms step_avg:55.16ms
step:1736/2160 train_time:95783ms step_avg:55.17ms
step:1737/2160 train_time:95873ms step_avg:55.19ms
step:1738/2160 train_time:95962ms step_avg:55.21ms
step:1739/2160 train_time:96051ms step_avg:55.23ms
step:1740/2160 train_time:96139ms step_avg:55.25ms
step:1741/2160 train_time:96228ms step_avg:55.27ms
step:1742/2160 train_time:96316ms step_avg:55.29ms
step:1743/2160 train_time:96406ms step_avg:55.31ms
step:1744/2160 train_time:96493ms step_avg:55.33ms
step:1745/2160 train_time:96584ms step_avg:55.35ms
step:1746/2160 train_time:96672ms step_avg:55.37ms
step:1747/2160 train_time:96761ms step_avg:55.39ms
step:1748/2160 train_time:96849ms step_avg:55.41ms
step:1749/2160 train_time:96939ms step_avg:55.43ms
step:1750/2160 train_time:97027ms step_avg:55.44ms
step:1750/2160 val_loss:3.3772 train_time:97116ms step_avg:55.50ms
step:1751/2160 train_time:97143ms step_avg:55.48ms
step:1752/2160 train_time:97206ms step_avg:55.48ms
step:1753/2160 train_time:97300ms step_avg:55.50ms
step:1754/2160 train_time:97389ms step_avg:55.52ms
step:1755/2160 train_time:97478ms step_avg:55.54ms
step:1756/2160 train_time:97565ms step_avg:55.56ms
step:1757/2160 train_time:97654ms step_avg:55.58ms
step:1758/2160 train_time:97740ms step_avg:55.60ms
step:1759/2160 train_time:97828ms step_avg:55.62ms
step:1760/2160 train_time:97917ms step_avg:55.63ms
step:1761/2160 train_time:98006ms step_avg:55.65ms
step:1762/2160 train_time:98095ms step_avg:55.67ms
step:1763/2160 train_time:98187ms step_avg:55.69ms
step:1764/2160 train_time:98277ms step_avg:55.71ms
step:1765/2160 train_time:98366ms step_avg:55.73ms
step:1766/2160 train_time:98454ms step_avg:55.75ms
step:1767/2160 train_time:98543ms step_avg:55.77ms
step:1768/2160 train_time:98629ms step_avg:55.79ms
step:1769/2160 train_time:98717ms step_avg:55.80ms
step:1770/2160 train_time:98805ms step_avg:55.82ms
step:1771/2160 train_time:98894ms step_avg:55.84ms
step:1772/2160 train_time:98981ms step_avg:55.86ms
step:1773/2160 train_time:99073ms step_avg:55.88ms
step:1774/2160 train_time:99161ms step_avg:55.90ms
step:1775/2160 train_time:99253ms step_avg:55.92ms
step:1776/2160 train_time:99342ms step_avg:55.94ms
step:1777/2160 train_time:99433ms step_avg:55.96ms
step:1778/2160 train_time:99520ms step_avg:55.97ms
step:1779/2160 train_time:99609ms step_avg:55.99ms
step:1780/2160 train_time:99696ms step_avg:56.01ms
step:1781/2160 train_time:99785ms step_avg:56.03ms
step:1782/2160 train_time:99872ms step_avg:56.04ms
step:1783/2160 train_time:99961ms step_avg:56.06ms
step:1784/2160 train_time:100049ms step_avg:56.08ms
step:1785/2160 train_time:100139ms step_avg:56.10ms
step:1786/2160 train_time:100228ms step_avg:56.12ms
step:1787/2160 train_time:100319ms step_avg:56.14ms
step:1788/2160 train_time:100407ms step_avg:56.16ms
step:1789/2160 train_time:100496ms step_avg:56.17ms
step:1790/2160 train_time:100583ms step_avg:56.19ms
step:1791/2160 train_time:100673ms step_avg:56.21ms
step:1792/2160 train_time:100760ms step_avg:56.23ms
step:1793/2160 train_time:100849ms step_avg:56.25ms
step:1794/2160 train_time:100936ms step_avg:56.26ms
step:1795/2160 train_time:101026ms step_avg:56.28ms
step:1796/2160 train_time:101114ms step_avg:56.30ms
step:1797/2160 train_time:101204ms step_avg:56.32ms
step:1798/2160 train_time:101293ms step_avg:56.34ms
step:1799/2160 train_time:101383ms step_avg:56.36ms
step:1800/2160 train_time:101471ms step_avg:56.37ms
step:1801/2160 train_time:101560ms step_avg:56.39ms
step:1802/2160 train_time:101648ms step_avg:56.41ms
step:1803/2160 train_time:101737ms step_avg:56.43ms
step:1804/2160 train_time:101825ms step_avg:56.44ms
step:1805/2160 train_time:101915ms step_avg:56.46ms
step:1806/2160 train_time:102002ms step_avg:56.48ms
step:1807/2160 train_time:102091ms step_avg:56.50ms
step:1808/2160 train_time:102179ms step_avg:56.51ms
step:1809/2160 train_time:102269ms step_avg:56.53ms
step:1810/2160 train_time:102357ms step_avg:56.55ms
step:1811/2160 train_time:102447ms step_avg:56.57ms
step:1812/2160 train_time:102534ms step_avg:56.59ms
step:1813/2160 train_time:102624ms step_avg:56.60ms
step:1814/2160 train_time:102711ms step_avg:56.62ms
step:1815/2160 train_time:102800ms step_avg:56.64ms
step:1816/2160 train_time:102888ms step_avg:56.66ms
step:1817/2160 train_time:102977ms step_avg:56.67ms
step:1818/2160 train_time:103065ms step_avg:56.69ms
step:1819/2160 train_time:103155ms step_avg:56.71ms
step:1820/2160 train_time:103244ms step_avg:56.73ms
step:1821/2160 train_time:103334ms step_avg:56.75ms
step:1822/2160 train_time:103422ms step_avg:56.76ms
step:1823/2160 train_time:103513ms step_avg:56.78ms
step:1824/2160 train_time:103600ms step_avg:56.80ms
step:1825/2160 train_time:103689ms step_avg:56.82ms
step:1826/2160 train_time:103777ms step_avg:56.83ms
step:1827/2160 train_time:103866ms step_avg:56.85ms
step:1828/2160 train_time:103954ms step_avg:56.87ms
step:1829/2160 train_time:104044ms step_avg:56.89ms
step:1830/2160 train_time:104132ms step_avg:56.90ms
step:1831/2160 train_time:104222ms step_avg:56.92ms
step:1832/2160 train_time:104310ms step_avg:56.94ms
step:1833/2160 train_time:104399ms step_avg:56.96ms
step:1834/2160 train_time:104488ms step_avg:56.97ms
step:1835/2160 train_time:104578ms step_avg:56.99ms
step:1836/2160 train_time:104665ms step_avg:57.01ms
step:1837/2160 train_time:104755ms step_avg:57.03ms
step:1838/2160 train_time:104843ms step_avg:57.04ms
step:1839/2160 train_time:104935ms step_avg:57.06ms
step:1840/2160 train_time:105024ms step_avg:57.08ms
step:1841/2160 train_time:105115ms step_avg:57.10ms
step:1842/2160 train_time:105203ms step_avg:57.11ms
step:1843/2160 train_time:105293ms step_avg:57.13ms
step:1844/2160 train_time:105381ms step_avg:57.15ms
step:1845/2160 train_time:105471ms step_avg:57.17ms
step:1846/2160 train_time:105558ms step_avg:57.18ms
step:1847/2160 train_time:105648ms step_avg:57.20ms
step:1848/2160 train_time:105736ms step_avg:57.22ms
step:1849/2160 train_time:105826ms step_avg:57.23ms
step:1850/2160 train_time:105913ms step_avg:57.25ms
step:1851/2160 train_time:106002ms step_avg:57.27ms
step:1852/2160 train_time:106090ms step_avg:57.28ms
step:1853/2160 train_time:106179ms step_avg:57.30ms
step:1854/2160 train_time:106267ms step_avg:57.32ms
step:1855/2160 train_time:106357ms step_avg:57.34ms
step:1856/2160 train_time:106446ms step_avg:57.35ms
step:1857/2160 train_time:106536ms step_avg:57.37ms
step:1858/2160 train_time:106623ms step_avg:57.39ms
step:1859/2160 train_time:106713ms step_avg:57.40ms
step:1860/2160 train_time:106799ms step_avg:57.42ms
step:1861/2160 train_time:106889ms step_avg:57.44ms
step:1862/2160 train_time:106978ms step_avg:57.45ms
step:1863/2160 train_time:107067ms step_avg:57.47ms
step:1864/2160 train_time:107156ms step_avg:57.49ms
step:1865/2160 train_time:107246ms step_avg:57.50ms
step:1866/2160 train_time:107334ms step_avg:57.52ms
step:1867/2160 train_time:107424ms step_avg:57.54ms
step:1868/2160 train_time:107512ms step_avg:57.55ms
step:1869/2160 train_time:107601ms step_avg:57.57ms
step:1870/2160 train_time:107689ms step_avg:57.59ms
step:1871/2160 train_time:107778ms step_avg:57.60ms
step:1872/2160 train_time:107866ms step_avg:57.62ms
step:1873/2160 train_time:107956ms step_avg:57.64ms
step:1874/2160 train_time:108044ms step_avg:57.65ms
step:1875/2160 train_time:108134ms step_avg:57.67ms
step:1876/2160 train_time:108222ms step_avg:57.69ms
step:1877/2160 train_time:108312ms step_avg:57.70ms
step:1878/2160 train_time:108400ms step_avg:57.72ms
step:1879/2160 train_time:108489ms step_avg:57.74ms
step:1880/2160 train_time:108576ms step_avg:57.75ms
step:1881/2160 train_time:108666ms step_avg:57.77ms
step:1882/2160 train_time:108754ms step_avg:57.79ms
step:1883/2160 train_time:108843ms step_avg:57.80ms
step:1884/2160 train_time:108931ms step_avg:57.82ms
step:1885/2160 train_time:109020ms step_avg:57.84ms
step:1886/2160 train_time:109108ms step_avg:57.85ms
step:1887/2160 train_time:109198ms step_avg:57.87ms
step:1888/2160 train_time:109286ms step_avg:57.88ms
step:1889/2160 train_time:109376ms step_avg:57.90ms
step:1890/2160 train_time:109465ms step_avg:57.92ms
step:1891/2160 train_time:109555ms step_avg:57.94ms
step:1892/2160 train_time:109643ms step_avg:57.95ms
step:1893/2160 train_time:109733ms step_avg:57.97ms
step:1894/2160 train_time:109821ms step_avg:57.98ms
step:1895/2160 train_time:109911ms step_avg:58.00ms
step:1896/2160 train_time:109998ms step_avg:58.02ms
step:1897/2160 train_time:110088ms step_avg:58.03ms
step:1898/2160 train_time:110175ms step_avg:58.05ms
step:1899/2160 train_time:110265ms step_avg:58.06ms
step:1900/2160 train_time:110354ms step_avg:58.08ms
step:1901/2160 train_time:110444ms step_avg:58.10ms
step:1902/2160 train_time:110532ms step_avg:58.11ms
step:1903/2160 train_time:110621ms step_avg:58.13ms
step:1904/2160 train_time:110709ms step_avg:58.15ms
step:1905/2160 train_time:110798ms step_avg:58.16ms
step:1906/2160 train_time:110886ms step_avg:58.18ms
step:1907/2160 train_time:110976ms step_avg:58.19ms
step:1908/2160 train_time:111068ms step_avg:58.21ms
step:1909/2160 train_time:111153ms step_avg:58.23ms
step:1910/2160 train_time:111241ms step_avg:58.24ms
step:1911/2160 train_time:111331ms step_avg:58.26ms
step:1912/2160 train_time:111418ms step_avg:58.27ms
step:1913/2160 train_time:111508ms step_avg:58.29ms
step:1914/2160 train_time:111596ms step_avg:58.30ms
step:1915/2160 train_time:111685ms step_avg:58.32ms
step:1916/2160 train_time:111773ms step_avg:58.34ms
step:1917/2160 train_time:111862ms step_avg:58.35ms
step:1918/2160 train_time:111949ms step_avg:58.37ms
step:1919/2160 train_time:112038ms step_avg:58.38ms
step:1920/2160 train_time:112126ms step_avg:58.40ms
step:1921/2160 train_time:112216ms step_avg:58.42ms
step:1922/2160 train_time:112303ms step_avg:58.43ms
step:1923/2160 train_time:112393ms step_avg:58.45ms
step:1924/2160 train_time:112482ms step_avg:58.46ms
step:1925/2160 train_time:112571ms step_avg:58.48ms
step:1926/2160 train_time:112658ms step_avg:58.49ms
step:1927/2160 train_time:112748ms step_avg:58.51ms
step:1928/2160 train_time:112835ms step_avg:58.52ms
step:1929/2160 train_time:112925ms step_avg:58.54ms
step:1930/2160 train_time:113013ms step_avg:58.56ms
step:1931/2160 train_time:113102ms step_avg:58.57ms
step:1932/2160 train_time:113189ms step_avg:58.59ms
step:1933/2160 train_time:113279ms step_avg:58.60ms
step:1934/2160 train_time:113367ms step_avg:58.62ms
step:1935/2160 train_time:113458ms step_avg:58.63ms
step:1936/2160 train_time:113547ms step_avg:58.65ms
step:1937/2160 train_time:113636ms step_avg:58.67ms
step:1938/2160 train_time:113724ms step_avg:58.68ms
step:1939/2160 train_time:113814ms step_avg:58.70ms
step:1940/2160 train_time:113901ms step_avg:58.71ms
step:1941/2160 train_time:113992ms step_avg:58.73ms
step:1942/2160 train_time:114078ms step_avg:58.74ms
step:1943/2160 train_time:114168ms step_avg:58.76ms
step:1944/2160 train_time:114256ms step_avg:58.77ms
step:1945/2160 train_time:114346ms step_avg:58.79ms
step:1946/2160 train_time:114433ms step_avg:58.80ms
step:1947/2160 train_time:114523ms step_avg:58.82ms
step:1948/2160 train_time:114611ms step_avg:58.84ms
step:1949/2160 train_time:114700ms step_avg:58.85ms
step:1950/2160 train_time:114788ms step_avg:58.87ms
step:1951/2160 train_time:114878ms step_avg:58.88ms
step:1952/2160 train_time:114966ms step_avg:58.90ms
step:1953/2160 train_time:115056ms step_avg:58.91ms
step:1954/2160 train_time:115144ms step_avg:58.93ms
step:1955/2160 train_time:115234ms step_avg:58.94ms
step:1956/2160 train_time:115321ms step_avg:58.96ms
step:1957/2160 train_time:115411ms step_avg:58.97ms
step:1958/2160 train_time:115499ms step_avg:58.99ms
step:1959/2160 train_time:115589ms step_avg:59.00ms
step:1960/2160 train_time:115676ms step_avg:59.02ms
step:1961/2160 train_time:115765ms step_avg:59.03ms
step:1962/2160 train_time:115853ms step_avg:59.05ms
step:1963/2160 train_time:115943ms step_avg:59.06ms
step:1964/2160 train_time:116031ms step_avg:59.08ms
step:1965/2160 train_time:116120ms step_avg:59.09ms
step:1966/2160 train_time:116207ms step_avg:59.11ms
step:1967/2160 train_time:116298ms step_avg:59.12ms
step:1968/2160 train_time:116386ms step_avg:59.14ms
step:1969/2160 train_time:116476ms step_avg:59.16ms
step:1970/2160 train_time:116564ms step_avg:59.17ms
step:1971/2160 train_time:116654ms step_avg:59.19ms
step:1972/2160 train_time:116742ms step_avg:59.20ms
step:1973/2160 train_time:116832ms step_avg:59.22ms
step:1974/2160 train_time:116920ms step_avg:59.23ms
step:1975/2160 train_time:117010ms step_avg:59.25ms
step:1976/2160 train_time:117097ms step_avg:59.26ms
step:1977/2160 train_time:117187ms step_avg:59.27ms
step:1978/2160 train_time:117274ms step_avg:59.29ms
step:1979/2160 train_time:117365ms step_avg:59.31ms
step:1980/2160 train_time:117453ms step_avg:59.32ms
step:1981/2160 train_time:117543ms step_avg:59.34ms
step:1982/2160 train_time:117631ms step_avg:59.35ms
step:1983/2160 train_time:117720ms step_avg:59.36ms
step:1984/2160 train_time:117809ms step_avg:59.38ms
step:1985/2160 train_time:117897ms step_avg:59.39ms
step:1986/2160 train_time:117985ms step_avg:59.41ms
step:1987/2160 train_time:118075ms step_avg:59.42ms
step:1988/2160 train_time:118163ms step_avg:59.44ms
step:1989/2160 train_time:118254ms step_avg:59.45ms
step:1990/2160 train_time:118341ms step_avg:59.47ms
step:1991/2160 train_time:118431ms step_avg:59.48ms
step:1992/2160 train_time:118519ms step_avg:59.50ms
step:1993/2160 train_time:118609ms step_avg:59.51ms
step:1994/2160 train_time:118697ms step_avg:59.53ms
step:1995/2160 train_time:118787ms step_avg:59.54ms
step:1996/2160 train_time:118874ms step_avg:59.56ms
step:1997/2160 train_time:118963ms step_avg:59.57ms
step:1998/2160 train_time:119051ms step_avg:59.59ms
step:1999/2160 train_time:119141ms step_avg:59.60ms
step:2000/2160 train_time:119228ms step_avg:59.61ms
step:2000/2160 val_loss:3.3090 train_time:119318ms step_avg:59.66ms
step:2001/2160 train_time:119342ms step_avg:59.64ms
step:2002/2160 train_time:119411ms step_avg:59.65ms
step:2003/2160 train_time:119509ms step_avg:59.66ms
step:2004/2160 train_time:119597ms step_avg:59.68ms
step:2005/2160 train_time:119686ms step_avg:59.69ms
step:2006/2160 train_time:119773ms step_avg:59.71ms
step:2007/2160 train_time:119862ms step_avg:59.72ms
step:2008/2160 train_time:119949ms step_avg:59.74ms
step:2009/2160 train_time:120038ms step_avg:59.75ms
step:2010/2160 train_time:120125ms step_avg:59.76ms
step:2011/2160 train_time:120215ms step_avg:59.78ms
step:2012/2160 train_time:120303ms step_avg:59.79ms
step:2013/2160 train_time:120395ms step_avg:59.81ms
step:2014/2160 train_time:120486ms step_avg:59.82ms
step:2015/2160 train_time:120577ms step_avg:59.84ms
step:2016/2160 train_time:120664ms step_avg:59.85ms
step:2017/2160 train_time:120754ms step_avg:59.87ms
step:2018/2160 train_time:120842ms step_avg:59.88ms
step:2019/2160 train_time:120930ms step_avg:59.90ms
step:2020/2160 train_time:121017ms step_avg:59.91ms
step:2021/2160 train_time:121106ms step_avg:59.92ms
step:2022/2160 train_time:121194ms step_avg:59.94ms
step:2023/2160 train_time:121284ms step_avg:59.95ms
step:2024/2160 train_time:121373ms step_avg:59.97ms
step:2025/2160 train_time:121464ms step_avg:59.98ms
step:2026/2160 train_time:121552ms step_avg:60.00ms
step:2027/2160 train_time:121643ms step_avg:60.01ms
step:2028/2160 train_time:121730ms step_avg:60.02ms
step:2029/2160 train_time:121820ms step_avg:60.04ms
step:2030/2160 train_time:121907ms step_avg:60.05ms
step:2031/2160 train_time:121995ms step_avg:60.07ms
step:2032/2160 train_time:122083ms step_avg:60.08ms
step:2033/2160 train_time:122172ms step_avg:60.09ms
step:2034/2160 train_time:122262ms step_avg:60.11ms
step:2035/2160 train_time:122352ms step_avg:60.12ms
step:2036/2160 train_time:122440ms step_avg:60.14ms
step:2037/2160 train_time:122530ms step_avg:60.15ms
step:2038/2160 train_time:122619ms step_avg:60.17ms
step:2039/2160 train_time:122708ms step_avg:60.18ms
step:2040/2160 train_time:122797ms step_avg:60.19ms
step:2041/2160 train_time:122887ms step_avg:60.21ms
step:2042/2160 train_time:122974ms step_avg:60.22ms
step:2043/2160 train_time:123064ms step_avg:60.24ms
step:2044/2160 train_time:123151ms step_avg:60.25ms
step:2045/2160 train_time:123242ms step_avg:60.26ms
step:2046/2160 train_time:123330ms step_avg:60.28ms
step:2047/2160 train_time:123420ms step_avg:60.29ms
step:2048/2160 train_time:123508ms step_avg:60.31ms
step:2049/2160 train_time:123597ms step_avg:60.32ms
step:2050/2160 train_time:123685ms step_avg:60.33ms
step:2051/2160 train_time:123776ms step_avg:60.35ms
step:2052/2160 train_time:123863ms step_avg:60.36ms
step:2053/2160 train_time:123954ms step_avg:60.38ms
step:2054/2160 train_time:124042ms step_avg:60.39ms
step:2055/2160 train_time:124131ms step_avg:60.40ms
step:2056/2160 train_time:124219ms step_avg:60.42ms
step:2057/2160 train_time:124309ms step_avg:60.43ms
step:2058/2160 train_time:124396ms step_avg:60.45ms
step:2059/2160 train_time:124486ms step_avg:60.46ms
step:2060/2160 train_time:124573ms step_avg:60.47ms
step:2061/2160 train_time:124663ms step_avg:60.49ms
step:2062/2160 train_time:124752ms step_avg:60.50ms
step:2063/2160 train_time:124841ms step_avg:60.51ms
step:2064/2160 train_time:124929ms step_avg:60.53ms
step:2065/2160 train_time:125018ms step_avg:60.54ms
step:2066/2160 train_time:125106ms step_avg:60.55ms
step:2067/2160 train_time:125200ms step_avg:60.57ms
step:2068/2160 train_time:125284ms step_avg:60.58ms
step:2069/2160 train_time:125374ms step_avg:60.60ms
step:2070/2160 train_time:125463ms step_avg:60.61ms
step:2071/2160 train_time:125553ms step_avg:60.62ms
step:2072/2160 train_time:125640ms step_avg:60.64ms
step:2073/2160 train_time:125729ms step_avg:60.65ms
step:2074/2160 train_time:125817ms step_avg:60.66ms
step:2075/2160 train_time:125907ms step_avg:60.68ms
step:2076/2160 train_time:125994ms step_avg:60.69ms
step:2077/2160 train_time:126084ms step_avg:60.70ms
step:2078/2160 train_time:126171ms step_avg:60.72ms
step:2079/2160 train_time:126261ms step_avg:60.73ms
step:2080/2160 train_time:126349ms step_avg:60.74ms
step:2081/2160 train_time:126439ms step_avg:60.76ms
step:2082/2160 train_time:126526ms step_avg:60.77ms
step:2083/2160 train_time:126617ms step_avg:60.79ms
step:2084/2160 train_time:126704ms step_avg:60.80ms
step:2085/2160 train_time:126794ms step_avg:60.81ms
step:2086/2160 train_time:126882ms step_avg:60.83ms
step:2087/2160 train_time:126971ms step_avg:60.84ms
step:2088/2160 train_time:127059ms step_avg:60.85ms
step:2089/2160 train_time:127149ms step_avg:60.87ms
step:2090/2160 train_time:127238ms step_avg:60.88ms
step:2091/2160 train_time:127328ms step_avg:60.89ms
step:2092/2160 train_time:127416ms step_avg:60.91ms
step:2093/2160 train_time:127506ms step_avg:60.92ms
step:2094/2160 train_time:127594ms step_avg:60.93ms
step:2095/2160 train_time:127685ms step_avg:60.95ms
step:2096/2160 train_time:127772ms step_avg:60.96ms
step:2097/2160 train_time:127861ms step_avg:60.97ms
step:2098/2160 train_time:127948ms step_avg:60.99ms
step:2099/2160 train_time:128037ms step_avg:61.00ms
step:2100/2160 train_time:128125ms step_avg:61.01ms
step:2101/2160 train_time:128214ms step_avg:61.03ms
step:2102/2160 train_time:128303ms step_avg:61.04ms
step:2103/2160 train_time:128391ms step_avg:61.05ms
step:2104/2160 train_time:128479ms step_avg:61.06ms
step:2105/2160 train_time:128569ms step_avg:61.08ms
step:2106/2160 train_time:128657ms step_avg:61.09ms
step:2107/2160 train_time:128747ms step_avg:61.10ms
step:2108/2160 train_time:128835ms step_avg:61.12ms
step:2109/2160 train_time:128924ms step_avg:61.13ms
step:2110/2160 train_time:129011ms step_avg:61.14ms
step:2111/2160 train_time:129101ms step_avg:61.16ms
step:2112/2160 train_time:129190ms step_avg:61.17ms
step:2113/2160 train_time:129281ms step_avg:61.18ms
step:2114/2160 train_time:129369ms step_avg:61.20ms
step:2115/2160 train_time:129458ms step_avg:61.21ms
step:2116/2160 train_time:129546ms step_avg:61.22ms
step:2117/2160 train_time:129636ms step_avg:61.24ms
step:2118/2160 train_time:129724ms step_avg:61.25ms
step:2119/2160 train_time:129813ms step_avg:61.26ms
step:2120/2160 train_time:129901ms step_avg:61.27ms
step:2121/2160 train_time:129990ms step_avg:61.29ms
step:2122/2160 train_time:130079ms step_avg:61.30ms
step:2123/2160 train_time:130169ms step_avg:61.31ms
step:2124/2160 train_time:130257ms step_avg:61.33ms
step:2125/2160 train_time:130346ms step_avg:61.34ms
step:2126/2160 train_time:130434ms step_avg:61.35ms
step:2127/2160 train_time:130524ms step_avg:61.37ms
step:2128/2160 train_time:130613ms step_avg:61.38ms
step:2129/2160 train_time:130704ms step_avg:61.39ms
step:2130/2160 train_time:130791ms step_avg:61.40ms
step:2131/2160 train_time:130881ms step_avg:61.42ms
step:2132/2160 train_time:130969ms step_avg:61.43ms
step:2133/2160 train_time:131059ms step_avg:61.44ms
step:2134/2160 train_time:131147ms step_avg:61.46ms
step:2135/2160 train_time:131237ms step_avg:61.47ms
step:2136/2160 train_time:131324ms step_avg:61.48ms
step:2137/2160 train_time:131415ms step_avg:61.49ms
step:2138/2160 train_time:131503ms step_avg:61.51ms
step:2139/2160 train_time:131593ms step_avg:61.52ms
step:2140/2160 train_time:131681ms step_avg:61.53ms
step:2141/2160 train_time:131770ms step_avg:61.55ms
step:2142/2160 train_time:131858ms step_avg:61.56ms
step:2143/2160 train_time:131948ms step_avg:61.57ms
step:2144/2160 train_time:132035ms step_avg:61.58ms
step:2145/2160 train_time:132125ms step_avg:61.60ms
step:2146/2160 train_time:132213ms step_avg:61.61ms
step:2147/2160 train_time:132303ms step_avg:61.62ms
step:2148/2160 train_time:132391ms step_avg:61.63ms
step:2149/2160 train_time:132482ms step_avg:61.65ms
step:2150/2160 train_time:132570ms step_avg:61.66ms
step:2151/2160 train_time:132660ms step_avg:61.67ms
step:2152/2160 train_time:132747ms step_avg:61.69ms
step:2153/2160 train_time:132837ms step_avg:61.70ms
step:2154/2160 train_time:132925ms step_avg:61.71ms
step:2155/2160 train_time:133015ms step_avg:61.72ms
step:2156/2160 train_time:133104ms step_avg:61.74ms
step:2157/2160 train_time:133193ms step_avg:61.75ms
step:2158/2160 train_time:133281ms step_avg:61.76ms
step:2159/2160 train_time:133371ms step_avg:61.77ms
step:2160/2160 train_time:133459ms step_avg:61.79ms
step:2160/2160 val_loss:3.2771 train_time:133550ms step_avg:61.83ms
peak memory allocated: 29892 MiB reserved: 45096 MiB

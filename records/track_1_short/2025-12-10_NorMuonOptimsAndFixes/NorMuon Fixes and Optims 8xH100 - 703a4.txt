import uuid
run_id = f"NorMuon Fixes and Optims 8xH100 - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2115  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 10 2025, 18:15:56) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 18:52:49 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   46C    P0            125W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   37C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   36C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   46C    P0            132W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   46C    P0            130W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   36C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   45C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   36C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           18254      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           18255      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           18256      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           18257      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           18258      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           18259      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           18260      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           18261      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           18255      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           18256      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           18257      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           18258      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           18259      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           18260      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           18261      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2155 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2155 train_time:87ms step_avg:87.14ms
step:2/2155 train_time:164ms step_avg:82.18ms
step:3/2155 train_time:187ms step_avg:62.26ms
step:4/2155 train_time:210ms step_avg:52.51ms
step:5/2155 train_time:236ms step_avg:47.23ms
step:6/2155 train_time:331ms step_avg:55.20ms
step:7/2155 train_time:385ms step_avg:55.03ms
step:8/2155 train_time:419ms step_avg:52.32ms
step:9/2155 train_time:451ms step_avg:50.06ms
step:10/2155 train_time:484ms step_avg:48.41ms
step:11/2155 train_time:516ms step_avg:46.95ms
step:12/2155 train_time:550ms step_avg:45.84ms
step:13/2155 train_time:583ms step_avg:44.81ms
step:14/2155 train_time:616ms step_avg:44.02ms
step:15/2155 train_time:649ms step_avg:43.27ms
step:16/2155 train_time:683ms step_avg:42.66ms
step:17/2155 train_time:715ms step_avg:42.07ms
step:18/2155 train_time:749ms step_avg:41.61ms
step:19/2155 train_time:781ms step_avg:41.12ms
step:20/2155 train_time:815ms step_avg:40.74ms
step:21/2155 train_time:847ms step_avg:40.34ms
step:22/2155 train_time:881ms step_avg:40.03ms
step:23/2155 train_time:913ms step_avg:39.71ms
step:24/2155 train_time:947ms step_avg:39.45ms
step:25/2155 train_time:979ms step_avg:39.17ms
step:26/2155 train_time:1013ms step_avg:38.96ms
step:27/2155 train_time:1046ms step_avg:38.72ms
step:28/2155 train_time:1079ms step_avg:38.55ms
step:29/2155 train_time:1112ms step_avg:38.33ms
step:30/2155 train_time:1145ms step_avg:38.17ms
step:31/2155 train_time:1177ms step_avg:37.98ms
step:32/2155 train_time:1211ms step_avg:37.83ms
step:33/2155 train_time:1244ms step_avg:37.68ms
step:34/2155 train_time:1278ms step_avg:37.60ms
step:35/2155 train_time:1314ms step_avg:37.55ms
step:36/2155 train_time:1349ms step_avg:37.47ms
step:37/2155 train_time:1383ms step_avg:37.39ms
step:38/2155 train_time:1417ms step_avg:37.29ms
step:39/2155 train_time:1451ms step_avg:37.20ms
step:40/2155 train_time:1484ms step_avg:37.10ms
step:41/2155 train_time:1518ms step_avg:37.01ms
step:42/2155 train_time:1551ms step_avg:36.94ms
step:43/2155 train_time:1584ms step_avg:36.84ms
step:44/2155 train_time:1618ms step_avg:36.76ms
step:45/2155 train_time:1651ms step_avg:36.69ms
step:46/2155 train_time:1684ms step_avg:36.62ms
step:47/2155 train_time:1717ms step_avg:36.54ms
step:48/2155 train_time:1751ms step_avg:36.48ms
step:49/2155 train_time:1784ms step_avg:36.40ms
step:50/2155 train_time:1817ms step_avg:36.35ms
step:51/2155 train_time:1850ms step_avg:36.28ms
step:52/2155 train_time:1884ms step_avg:36.23ms
step:53/2155 train_time:1916ms step_avg:36.16ms
step:54/2155 train_time:1950ms step_avg:36.12ms
step:55/2155 train_time:1983ms step_avg:36.05ms
step:56/2155 train_time:2016ms step_avg:36.00ms
step:57/2155 train_time:2049ms step_avg:35.94ms
step:58/2155 train_time:2082ms step_avg:35.90ms
step:59/2155 train_time:2115ms step_avg:35.84ms
step:60/2155 train_time:2148ms step_avg:35.80ms
step:61/2155 train_time:2181ms step_avg:35.75ms
step:62/2155 train_time:2214ms step_avg:35.72ms
step:63/2155 train_time:2247ms step_avg:35.67ms
step:64/2155 train_time:2281ms step_avg:35.64ms
step:65/2155 train_time:2314ms step_avg:35.61ms
step:66/2155 train_time:2348ms step_avg:35.58ms
step:67/2155 train_time:2382ms step_avg:35.55ms
step:68/2155 train_time:2416ms step_avg:35.52ms
step:69/2155 train_time:2449ms step_avg:35.49ms
step:70/2155 train_time:2483ms step_avg:35.47ms
step:71/2155 train_time:2516ms step_avg:35.43ms
step:72/2155 train_time:2549ms step_avg:35.41ms
step:73/2155 train_time:2582ms step_avg:35.37ms
step:74/2155 train_time:2616ms step_avg:35.35ms
step:75/2155 train_time:2649ms step_avg:35.32ms
step:76/2155 train_time:2682ms step_avg:35.29ms
step:77/2155 train_time:2715ms step_avg:35.26ms
step:78/2155 train_time:2749ms step_avg:35.24ms
step:79/2155 train_time:2782ms step_avg:35.21ms
step:80/2155 train_time:2815ms step_avg:35.19ms
step:81/2155 train_time:2848ms step_avg:35.16ms
step:82/2155 train_time:2882ms step_avg:35.14ms
step:83/2155 train_time:2915ms step_avg:35.12ms
step:84/2155 train_time:2948ms step_avg:35.10ms
step:85/2155 train_time:2981ms step_avg:35.07ms
step:86/2155 train_time:3015ms step_avg:35.05ms
step:87/2155 train_time:3047ms step_avg:35.03ms
step:88/2155 train_time:3081ms step_avg:35.01ms
step:89/2155 train_time:3114ms step_avg:34.98ms
step:90/2155 train_time:3147ms step_avg:34.97ms
step:91/2155 train_time:3180ms step_avg:34.94ms
step:92/2155 train_time:3213ms step_avg:34.92ms
step:93/2155 train_time:3246ms step_avg:34.90ms
step:94/2155 train_time:3279ms step_avg:34.89ms
step:95/2155 train_time:3313ms step_avg:34.88ms
step:96/2155 train_time:3347ms step_avg:34.86ms
step:97/2155 train_time:3380ms step_avg:34.84ms
step:98/2155 train_time:3414ms step_avg:34.83ms
step:99/2155 train_time:3447ms step_avg:34.81ms
step:100/2155 train_time:3480ms step_avg:34.80ms
step:101/2155 train_time:3513ms step_avg:34.78ms
step:102/2155 train_time:3546ms step_avg:34.77ms
step:103/2155 train_time:3579ms step_avg:34.75ms
step:104/2155 train_time:3613ms step_avg:34.74ms
step:105/2155 train_time:3646ms step_avg:34.73ms
step:106/2155 train_time:3679ms step_avg:34.71ms
step:107/2155 train_time:3712ms step_avg:34.69ms
step:108/2155 train_time:3746ms step_avg:34.68ms
step:109/2155 train_time:3779ms step_avg:34.67ms
step:110/2155 train_time:3812ms step_avg:34.66ms
step:111/2155 train_time:3845ms step_avg:34.64ms
step:112/2155 train_time:3878ms step_avg:34.63ms
step:113/2155 train_time:3911ms step_avg:34.61ms
step:114/2155 train_time:3945ms step_avg:34.60ms
step:115/2155 train_time:3977ms step_avg:34.59ms
step:116/2155 train_time:4011ms step_avg:34.58ms
step:117/2155 train_time:4043ms step_avg:34.56ms
step:118/2155 train_time:4077ms step_avg:34.55ms
step:119/2155 train_time:4109ms step_avg:34.53ms
step:120/2155 train_time:4142ms step_avg:34.52ms
step:121/2155 train_time:4175ms step_avg:34.51ms
step:122/2155 train_time:4209ms step_avg:34.50ms
step:123/2155 train_time:4241ms step_avg:34.48ms
step:124/2155 train_time:4275ms step_avg:34.48ms
step:125/2155 train_time:4308ms step_avg:34.46ms
step:126/2155 train_time:4341ms step_avg:34.45ms
step:127/2155 train_time:4374ms step_avg:34.44ms
step:128/2155 train_time:4407ms step_avg:34.43ms
step:129/2155 train_time:4440ms step_avg:34.42ms
step:130/2155 train_time:4474ms step_avg:34.41ms
step:131/2155 train_time:4506ms step_avg:34.40ms
step:132/2155 train_time:4540ms step_avg:34.39ms
step:133/2155 train_time:4573ms step_avg:34.39ms
step:134/2155 train_time:4607ms step_avg:34.38ms
step:135/2155 train_time:4640ms step_avg:34.37ms
step:136/2155 train_time:4673ms step_avg:34.36ms
step:137/2155 train_time:4706ms step_avg:34.35ms
step:138/2155 train_time:4740ms step_avg:34.35ms
step:139/2155 train_time:4773ms step_avg:34.34ms
step:140/2155 train_time:4806ms step_avg:34.33ms
step:141/2155 train_time:4839ms step_avg:34.32ms
step:142/2155 train_time:4873ms step_avg:34.32ms
step:143/2155 train_time:4906ms step_avg:34.31ms
step:144/2155 train_time:4940ms step_avg:34.30ms
step:145/2155 train_time:4972ms step_avg:34.29ms
step:146/2155 train_time:5006ms step_avg:34.29ms
step:147/2155 train_time:5038ms step_avg:34.27ms
step:148/2155 train_time:5072ms step_avg:34.27ms
step:149/2155 train_time:5104ms step_avg:34.26ms
step:150/2155 train_time:5138ms step_avg:34.25ms
step:151/2155 train_time:5171ms step_avg:34.24ms
step:152/2155 train_time:5204ms step_avg:34.24ms
step:153/2155 train_time:5237ms step_avg:34.23ms
step:154/2155 train_time:5270ms step_avg:34.22ms
step:155/2155 train_time:5303ms step_avg:34.21ms
step:156/2155 train_time:5336ms step_avg:34.20ms
step:157/2155 train_time:5369ms step_avg:34.20ms
step:158/2155 train_time:5402ms step_avg:34.19ms
step:159/2155 train_time:5435ms step_avg:34.18ms
step:160/2155 train_time:5469ms step_avg:34.18ms
step:161/2155 train_time:5502ms step_avg:34.17ms
step:162/2155 train_time:5535ms step_avg:34.17ms
step:163/2155 train_time:5568ms step_avg:34.16ms
step:164/2155 train_time:5601ms step_avg:34.15ms
step:165/2155 train_time:5633ms step_avg:34.14ms
step:166/2155 train_time:5667ms step_avg:34.14ms
step:167/2155 train_time:5700ms step_avg:34.13ms
step:168/2155 train_time:5733ms step_avg:34.13ms
step:169/2155 train_time:5766ms step_avg:34.12ms
step:170/2155 train_time:5799ms step_avg:34.11ms
step:171/2155 train_time:5832ms step_avg:34.11ms
step:172/2155 train_time:5865ms step_avg:34.10ms
step:173/2155 train_time:5898ms step_avg:34.09ms
step:174/2155 train_time:5932ms step_avg:34.09ms
step:175/2155 train_time:5964ms step_avg:34.08ms
step:176/2155 train_time:5998ms step_avg:34.08ms
step:177/2155 train_time:6031ms step_avg:34.07ms
step:178/2155 train_time:6064ms step_avg:34.07ms
step:179/2155 train_time:6097ms step_avg:34.06ms
step:180/2155 train_time:6130ms step_avg:34.06ms
step:181/2155 train_time:6163ms step_avg:34.05ms
step:182/2155 train_time:6196ms step_avg:34.05ms
step:183/2155 train_time:6229ms step_avg:34.04ms
step:184/2155 train_time:6262ms step_avg:34.04ms
step:185/2155 train_time:6295ms step_avg:34.03ms
step:186/2155 train_time:6328ms step_avg:34.02ms
step:187/2155 train_time:6361ms step_avg:34.02ms
step:188/2155 train_time:6395ms step_avg:34.01ms
step:189/2155 train_time:6428ms step_avg:34.01ms
step:190/2155 train_time:6461ms step_avg:34.01ms
step:191/2155 train_time:6494ms step_avg:34.00ms
step:192/2155 train_time:6527ms step_avg:34.00ms
step:193/2155 train_time:6560ms step_avg:33.99ms
step:194/2155 train_time:6594ms step_avg:33.99ms
step:195/2155 train_time:6627ms step_avg:33.98ms
step:196/2155 train_time:6660ms step_avg:33.98ms
step:197/2155 train_time:6693ms step_avg:33.98ms
step:198/2155 train_time:6727ms step_avg:33.97ms
step:199/2155 train_time:6760ms step_avg:33.97ms
step:200/2155 train_time:6793ms step_avg:33.97ms
step:201/2155 train_time:6826ms step_avg:33.96ms
step:202/2155 train_time:6859ms step_avg:33.96ms
step:203/2155 train_time:6892ms step_avg:33.95ms
step:204/2155 train_time:6926ms step_avg:33.95ms
step:205/2155 train_time:6958ms step_avg:33.94ms
step:206/2155 train_time:6991ms step_avg:33.94ms
step:207/2155 train_time:7024ms step_avg:33.93ms
step:208/2155 train_time:7058ms step_avg:33.93ms
step:209/2155 train_time:7090ms step_avg:33.92ms
step:210/2155 train_time:7123ms step_avg:33.92ms
step:211/2155 train_time:7156ms step_avg:33.91ms
step:212/2155 train_time:7189ms step_avg:33.91ms
step:213/2155 train_time:7222ms step_avg:33.91ms
step:214/2155 train_time:7255ms step_avg:33.90ms
step:215/2155 train_time:7288ms step_avg:33.90ms
step:216/2155 train_time:7321ms step_avg:33.90ms
step:217/2155 train_time:7354ms step_avg:33.89ms
step:218/2155 train_time:7388ms step_avg:33.89ms
step:219/2155 train_time:7420ms step_avg:33.88ms
step:220/2155 train_time:7454ms step_avg:33.88ms
step:221/2155 train_time:7486ms step_avg:33.88ms
step:222/2155 train_time:7520ms step_avg:33.87ms
step:223/2155 train_time:7552ms step_avg:33.87ms
step:224/2155 train_time:7586ms step_avg:33.87ms
step:225/2155 train_time:7618ms step_avg:33.86ms
step:226/2155 train_time:7652ms step_avg:33.86ms
step:227/2155 train_time:7685ms step_avg:33.85ms
step:228/2155 train_time:7718ms step_avg:33.85ms
step:229/2155 train_time:7751ms step_avg:33.85ms
step:230/2155 train_time:7784ms step_avg:33.84ms
step:231/2155 train_time:7816ms step_avg:33.84ms
step:232/2155 train_time:7850ms step_avg:33.84ms
step:233/2155 train_time:7882ms step_avg:33.83ms
step:234/2155 train_time:7916ms step_avg:33.83ms
step:235/2155 train_time:7949ms step_avg:33.82ms
step:236/2155 train_time:7982ms step_avg:33.82ms
step:237/2155 train_time:8015ms step_avg:33.82ms
step:238/2155 train_time:8048ms step_avg:33.82ms
step:239/2155 train_time:8081ms step_avg:33.81ms
step:240/2155 train_time:8115ms step_avg:33.81ms
step:241/2155 train_time:8147ms step_avg:33.81ms
step:242/2155 train_time:8181ms step_avg:33.81ms
step:243/2155 train_time:8213ms step_avg:33.80ms
step:244/2155 train_time:8246ms step_avg:33.80ms
step:245/2155 train_time:8279ms step_avg:33.79ms
step:246/2155 train_time:8312ms step_avg:33.79ms
step:247/2155 train_time:8345ms step_avg:33.79ms
step:248/2155 train_time:8379ms step_avg:33.78ms
step:249/2155 train_time:8411ms step_avg:33.78ms
step:250/2155 train_time:8444ms step_avg:33.78ms
step:250/2155 val_loss:4.3069 train_time:8480ms step_avg:33.92ms
step:251/2155 train_time:8502ms step_avg:33.87ms
step:252/2155 train_time:8524ms step_avg:33.82ms
step:253/2155 train_time:8547ms step_avg:33.78ms
step:254/2155 train_time:8584ms step_avg:33.80ms
step:255/2155 train_time:8621ms step_avg:33.81ms
step:256/2155 train_time:8657ms step_avg:33.82ms
step:257/2155 train_time:8691ms step_avg:33.82ms
step:258/2155 train_time:8725ms step_avg:33.82ms
step:259/2155 train_time:8758ms step_avg:33.81ms
step:260/2155 train_time:8791ms step_avg:33.81ms
step:261/2155 train_time:8824ms step_avg:33.81ms
step:262/2155 train_time:8858ms step_avg:33.81ms
step:263/2155 train_time:8890ms step_avg:33.80ms
step:264/2155 train_time:8923ms step_avg:33.80ms
step:265/2155 train_time:8956ms step_avg:33.79ms
step:266/2155 train_time:8989ms step_avg:33.79ms
step:267/2155 train_time:9021ms step_avg:33.79ms
step:268/2155 train_time:9055ms step_avg:33.79ms
step:269/2155 train_time:9087ms step_avg:33.78ms
step:270/2155 train_time:9120ms step_avg:33.78ms
step:271/2155 train_time:9153ms step_avg:33.77ms
step:272/2155 train_time:9186ms step_avg:33.77ms
step:273/2155 train_time:9218ms step_avg:33.77ms
step:274/2155 train_time:9252ms step_avg:33.77ms
step:275/2155 train_time:9284ms step_avg:33.76ms
step:276/2155 train_time:9318ms step_avg:33.76ms
step:277/2155 train_time:9350ms step_avg:33.75ms
step:278/2155 train_time:9383ms step_avg:33.75ms
step:279/2155 train_time:9416ms step_avg:33.75ms
step:280/2155 train_time:9449ms step_avg:33.75ms
step:281/2155 train_time:9482ms step_avg:33.74ms
step:282/2155 train_time:9516ms step_avg:33.74ms
step:283/2155 train_time:9549ms step_avg:33.74ms
step:284/2155 train_time:9584ms step_avg:33.75ms
step:285/2155 train_time:9618ms step_avg:33.75ms
step:286/2155 train_time:9651ms step_avg:33.75ms
step:287/2155 train_time:9685ms step_avg:33.75ms
step:288/2155 train_time:9719ms step_avg:33.75ms
step:289/2155 train_time:9752ms step_avg:33.74ms
step:290/2155 train_time:9785ms step_avg:33.74ms
step:291/2155 train_time:9818ms step_avg:33.74ms
step:292/2155 train_time:9851ms step_avg:33.74ms
step:293/2155 train_time:9884ms step_avg:33.73ms
step:294/2155 train_time:9917ms step_avg:33.73ms
step:295/2155 train_time:9950ms step_avg:33.73ms
step:296/2155 train_time:9984ms step_avg:33.73ms
step:297/2155 train_time:10016ms step_avg:33.73ms
step:298/2155 train_time:10050ms step_avg:33.72ms
step:299/2155 train_time:10082ms step_avg:33.72ms
step:300/2155 train_time:10116ms step_avg:33.72ms
step:301/2155 train_time:10148ms step_avg:33.71ms
step:302/2155 train_time:10181ms step_avg:33.71ms
step:303/2155 train_time:10214ms step_avg:33.71ms
step:304/2155 train_time:10247ms step_avg:33.71ms
step:305/2155 train_time:10280ms step_avg:33.70ms
step:306/2155 train_time:10313ms step_avg:33.70ms
step:307/2155 train_time:10346ms step_avg:33.70ms
step:308/2155 train_time:10379ms step_avg:33.70ms
step:309/2155 train_time:10411ms step_avg:33.69ms
step:310/2155 train_time:10444ms step_avg:33.69ms
step:311/2155 train_time:10477ms step_avg:33.69ms
step:312/2155 train_time:10511ms step_avg:33.69ms
step:313/2155 train_time:10544ms step_avg:33.69ms
step:314/2155 train_time:10578ms step_avg:33.69ms
step:315/2155 train_time:10611ms step_avg:33.68ms
step:316/2155 train_time:10644ms step_avg:33.68ms
step:317/2155 train_time:10678ms step_avg:33.68ms
step:318/2155 train_time:10711ms step_avg:33.68ms
step:319/2155 train_time:10744ms step_avg:33.68ms
step:320/2155 train_time:10778ms step_avg:33.68ms
step:321/2155 train_time:10810ms step_avg:33.68ms
step:322/2155 train_time:10844ms step_avg:33.68ms
step:323/2155 train_time:10877ms step_avg:33.67ms
step:324/2155 train_time:10910ms step_avg:33.67ms
step:325/2155 train_time:10943ms step_avg:33.67ms
step:326/2155 train_time:10976ms step_avg:33.67ms
step:327/2155 train_time:11009ms step_avg:33.67ms
step:328/2155 train_time:11043ms step_avg:33.67ms
step:329/2155 train_time:11075ms step_avg:33.66ms
step:330/2155 train_time:11108ms step_avg:33.66ms
step:331/2155 train_time:11141ms step_avg:33.66ms
step:332/2155 train_time:11175ms step_avg:33.66ms
step:333/2155 train_time:11207ms step_avg:33.65ms
step:334/2155 train_time:11241ms step_avg:33.65ms
step:335/2155 train_time:11273ms step_avg:33.65ms
step:336/2155 train_time:11306ms step_avg:33.65ms
step:337/2155 train_time:11339ms step_avg:33.65ms
step:338/2155 train_time:11372ms step_avg:33.65ms
step:339/2155 train_time:11405ms step_avg:33.64ms
step:340/2155 train_time:11439ms step_avg:33.64ms
step:341/2155 train_time:11472ms step_avg:33.64ms
step:342/2155 train_time:11505ms step_avg:33.64ms
step:343/2155 train_time:11539ms step_avg:33.64ms
step:344/2155 train_time:11572ms step_avg:33.64ms
step:345/2155 train_time:11605ms step_avg:33.64ms
step:346/2155 train_time:11638ms step_avg:33.64ms
step:347/2155 train_time:11672ms step_avg:33.64ms
step:348/2155 train_time:11705ms step_avg:33.64ms
step:349/2155 train_time:11739ms step_avg:33.63ms
step:350/2155 train_time:11772ms step_avg:33.63ms
step:351/2155 train_time:11805ms step_avg:33.63ms
step:352/2155 train_time:11839ms step_avg:33.63ms
step:353/2155 train_time:11872ms step_avg:33.63ms
step:354/2155 train_time:11905ms step_avg:33.63ms
step:355/2155 train_time:11938ms step_avg:33.63ms
step:356/2155 train_time:11971ms step_avg:33.63ms
step:357/2155 train_time:12004ms step_avg:33.63ms
step:358/2155 train_time:12038ms step_avg:33.62ms
step:359/2155 train_time:12071ms step_avg:33.62ms
step:360/2155 train_time:12104ms step_avg:33.62ms
step:361/2155 train_time:12137ms step_avg:33.62ms
step:362/2155 train_time:12171ms step_avg:33.62ms
step:363/2155 train_time:12203ms step_avg:33.62ms
step:364/2155 train_time:12236ms step_avg:33.62ms
step:365/2155 train_time:12269ms step_avg:33.61ms
step:366/2155 train_time:12302ms step_avg:33.61ms
step:367/2155 train_time:12335ms step_avg:33.61ms
step:368/2155 train_time:12368ms step_avg:33.61ms
step:369/2155 train_time:12401ms step_avg:33.61ms
step:370/2155 train_time:12434ms step_avg:33.61ms
step:371/2155 train_time:12467ms step_avg:33.61ms
step:372/2155 train_time:12501ms step_avg:33.60ms
step:373/2155 train_time:12533ms step_avg:33.60ms
step:374/2155 train_time:12567ms step_avg:33.60ms
step:375/2155 train_time:12599ms step_avg:33.60ms
step:376/2155 train_time:12633ms step_avg:33.60ms
step:377/2155 train_time:12666ms step_avg:33.60ms
step:378/2155 train_time:12699ms step_avg:33.60ms
step:379/2155 train_time:12732ms step_avg:33.59ms
step:380/2155 train_time:12765ms step_avg:33.59ms
step:381/2155 train_time:12798ms step_avg:33.59ms
step:382/2155 train_time:12831ms step_avg:33.59ms
step:383/2155 train_time:12864ms step_avg:33.59ms
step:384/2155 train_time:12897ms step_avg:33.59ms
step:385/2155 train_time:12930ms step_avg:33.59ms
step:386/2155 train_time:12964ms step_avg:33.59ms
step:387/2155 train_time:12997ms step_avg:33.58ms
step:388/2155 train_time:13030ms step_avg:33.58ms
step:389/2155 train_time:13063ms step_avg:33.58ms
step:390/2155 train_time:13096ms step_avg:33.58ms
step:391/2155 train_time:13129ms step_avg:33.58ms
step:392/2155 train_time:13162ms step_avg:33.58ms
step:393/2155 train_time:13195ms step_avg:33.57ms
step:394/2155 train_time:13228ms step_avg:33.57ms
step:395/2155 train_time:13261ms step_avg:33.57ms
step:396/2155 train_time:13295ms step_avg:33.57ms
step:397/2155 train_time:13327ms step_avg:33.57ms
step:398/2155 train_time:13360ms step_avg:33.57ms
step:399/2155 train_time:13393ms step_avg:33.57ms
step:400/2155 train_time:13426ms step_avg:33.57ms
step:401/2155 train_time:13459ms step_avg:33.56ms
step:402/2155 train_time:13492ms step_avg:33.56ms
step:403/2155 train_time:13525ms step_avg:33.56ms
step:404/2155 train_time:13558ms step_avg:33.56ms
step:405/2155 train_time:13591ms step_avg:33.56ms
step:406/2155 train_time:13624ms step_avg:33.56ms
step:407/2155 train_time:13657ms step_avg:33.56ms
step:408/2155 train_time:13690ms step_avg:33.56ms
step:409/2155 train_time:13723ms step_avg:33.55ms
step:410/2155 train_time:13756ms step_avg:33.55ms
step:411/2155 train_time:13789ms step_avg:33.55ms
step:412/2155 train_time:13823ms step_avg:33.55ms
step:413/2155 train_time:13855ms step_avg:33.55ms
step:414/2155 train_time:13889ms step_avg:33.55ms
step:415/2155 train_time:13921ms step_avg:33.55ms
step:416/2155 train_time:13955ms step_avg:33.55ms
step:417/2155 train_time:13987ms step_avg:33.54ms
step:418/2155 train_time:14021ms step_avg:33.54ms
step:419/2155 train_time:14053ms step_avg:33.54ms
step:420/2155 train_time:14087ms step_avg:33.54ms
step:421/2155 train_time:14120ms step_avg:33.54ms
step:422/2155 train_time:14153ms step_avg:33.54ms
step:423/2155 train_time:14186ms step_avg:33.54ms
step:424/2155 train_time:14219ms step_avg:33.54ms
step:425/2155 train_time:14252ms step_avg:33.53ms
step:426/2155 train_time:14285ms step_avg:33.53ms
step:427/2155 train_time:14318ms step_avg:33.53ms
step:428/2155 train_time:14351ms step_avg:33.53ms
step:429/2155 train_time:14384ms step_avg:33.53ms
step:430/2155 train_time:14417ms step_avg:33.53ms
step:431/2155 train_time:14450ms step_avg:33.53ms
step:432/2155 train_time:14483ms step_avg:33.53ms
step:433/2155 train_time:14516ms step_avg:33.52ms
step:434/2155 train_time:14549ms step_avg:33.52ms
step:435/2155 train_time:14582ms step_avg:33.52ms
step:436/2155 train_time:14615ms step_avg:33.52ms
step:437/2155 train_time:14648ms step_avg:33.52ms
step:438/2155 train_time:14681ms step_avg:33.52ms
step:439/2155 train_time:14714ms step_avg:33.52ms
step:440/2155 train_time:14747ms step_avg:33.52ms
step:441/2155 train_time:14780ms step_avg:33.52ms
step:442/2155 train_time:14814ms step_avg:33.51ms
step:443/2155 train_time:14847ms step_avg:33.51ms
step:444/2155 train_time:14880ms step_avg:33.51ms
step:445/2155 train_time:14913ms step_avg:33.51ms
step:446/2155 train_time:14946ms step_avg:33.51ms
step:447/2155 train_time:14979ms step_avg:33.51ms
step:448/2155 train_time:15012ms step_avg:33.51ms
step:449/2155 train_time:15046ms step_avg:33.51ms
step:450/2155 train_time:15079ms step_avg:33.51ms
step:451/2155 train_time:15112ms step_avg:33.51ms
step:452/2155 train_time:15146ms step_avg:33.51ms
step:453/2155 train_time:15178ms step_avg:33.51ms
step:454/2155 train_time:15212ms step_avg:33.51ms
step:455/2155 train_time:15245ms step_avg:33.50ms
step:456/2155 train_time:15278ms step_avg:33.50ms
step:457/2155 train_time:15311ms step_avg:33.50ms
step:458/2155 train_time:15344ms step_avg:33.50ms
step:459/2155 train_time:15377ms step_avg:33.50ms
step:460/2155 train_time:15410ms step_avg:33.50ms
step:461/2155 train_time:15443ms step_avg:33.50ms
step:462/2155 train_time:15476ms step_avg:33.50ms
step:463/2155 train_time:15509ms step_avg:33.50ms
step:464/2155 train_time:15542ms step_avg:33.50ms
step:465/2155 train_time:15575ms step_avg:33.50ms
step:466/2155 train_time:15609ms step_avg:33.49ms
step:467/2155 train_time:15641ms step_avg:33.49ms
step:468/2155 train_time:15675ms step_avg:33.49ms
step:469/2155 train_time:15708ms step_avg:33.49ms
step:470/2155 train_time:15741ms step_avg:33.49ms
step:471/2155 train_time:15773ms step_avg:33.49ms
step:472/2155 train_time:15807ms step_avg:33.49ms
step:473/2155 train_time:15839ms step_avg:33.49ms
step:474/2155 train_time:15873ms step_avg:33.49ms
step:475/2155 train_time:15905ms step_avg:33.49ms
step:476/2155 train_time:15939ms step_avg:33.48ms
step:477/2155 train_time:15972ms step_avg:33.48ms
step:478/2155 train_time:16005ms step_avg:33.48ms
step:479/2155 train_time:16039ms step_avg:33.48ms
step:480/2155 train_time:16072ms step_avg:33.48ms
step:481/2155 train_time:16105ms step_avg:33.48ms
step:482/2155 train_time:16138ms step_avg:33.48ms
step:483/2155 train_time:16171ms step_avg:33.48ms
step:484/2155 train_time:16205ms step_avg:33.48ms
step:485/2155 train_time:16237ms step_avg:33.48ms
step:486/2155 train_time:16271ms step_avg:33.48ms
step:487/2155 train_time:16303ms step_avg:33.48ms
step:488/2155 train_time:16337ms step_avg:33.48ms
step:489/2155 train_time:16370ms step_avg:33.48ms
step:490/2155 train_time:16403ms step_avg:33.48ms
step:491/2155 train_time:16436ms step_avg:33.47ms
step:492/2155 train_time:16469ms step_avg:33.47ms
step:493/2155 train_time:16502ms step_avg:33.47ms
step:494/2155 train_time:16535ms step_avg:33.47ms
step:495/2155 train_time:16568ms step_avg:33.47ms
step:496/2155 train_time:16601ms step_avg:33.47ms
step:497/2155 train_time:16634ms step_avg:33.47ms
step:498/2155 train_time:16667ms step_avg:33.47ms
step:499/2155 train_time:16699ms step_avg:33.47ms
step:500/2155 train_time:16733ms step_avg:33.47ms
step:500/2155 val_loss:4.0203 train_time:16768ms step_avg:33.54ms
step:501/2155 train_time:16790ms step_avg:33.51ms
step:502/2155 train_time:16812ms step_avg:33.49ms
step:503/2155 train_time:16835ms step_avg:33.47ms
step:504/2155 train_time:16868ms step_avg:33.47ms
step:505/2155 train_time:16903ms step_avg:33.47ms
step:506/2155 train_time:16937ms step_avg:33.47ms
step:507/2155 train_time:16971ms step_avg:33.47ms
step:508/2155 train_time:17004ms step_avg:33.47ms
step:509/2155 train_time:17037ms step_avg:33.47ms
step:510/2155 train_time:17071ms step_avg:33.47ms
step:511/2155 train_time:17104ms step_avg:33.47ms
step:512/2155 train_time:17137ms step_avg:33.47ms
step:513/2155 train_time:17170ms step_avg:33.47ms
step:514/2155 train_time:17203ms step_avg:33.47ms
step:515/2155 train_time:17235ms step_avg:33.47ms
step:516/2155 train_time:17269ms step_avg:33.47ms
step:517/2155 train_time:17301ms step_avg:33.46ms
step:518/2155 train_time:17334ms step_avg:33.46ms
step:519/2155 train_time:17366ms step_avg:33.46ms
step:520/2155 train_time:17400ms step_avg:33.46ms
step:521/2155 train_time:17432ms step_avg:33.46ms
step:522/2155 train_time:17465ms step_avg:33.46ms
step:523/2155 train_time:17497ms step_avg:33.46ms
step:524/2155 train_time:17531ms step_avg:33.46ms
step:525/2155 train_time:17563ms step_avg:33.45ms
step:526/2155 train_time:17596ms step_avg:33.45ms
step:527/2155 train_time:17629ms step_avg:33.45ms
step:528/2155 train_time:17662ms step_avg:33.45ms
step:529/2155 train_time:17695ms step_avg:33.45ms
step:530/2155 train_time:17728ms step_avg:33.45ms
step:531/2155 train_time:17761ms step_avg:33.45ms
step:532/2155 train_time:17795ms step_avg:33.45ms
step:533/2155 train_time:17828ms step_avg:33.45ms
step:534/2155 train_time:17862ms step_avg:33.45ms
step:535/2155 train_time:17896ms step_avg:33.45ms
step:536/2155 train_time:17929ms step_avg:33.45ms
step:537/2155 train_time:17963ms step_avg:33.45ms
step:538/2155 train_time:17996ms step_avg:33.45ms
step:539/2155 train_time:18029ms step_avg:33.45ms
step:540/2155 train_time:18063ms step_avg:33.45ms
step:541/2155 train_time:18096ms step_avg:33.45ms
step:542/2155 train_time:18129ms step_avg:33.45ms
step:543/2155 train_time:18162ms step_avg:33.45ms
step:544/2155 train_time:18195ms step_avg:33.45ms
step:545/2155 train_time:18228ms step_avg:33.45ms
step:546/2155 train_time:18261ms step_avg:33.44ms
step:547/2155 train_time:18294ms step_avg:33.44ms
step:548/2155 train_time:18327ms step_avg:33.44ms
step:549/2155 train_time:18359ms step_avg:33.44ms
step:550/2155 train_time:18393ms step_avg:33.44ms
step:551/2155 train_time:18425ms step_avg:33.44ms
step:552/2155 train_time:18459ms step_avg:33.44ms
step:553/2155 train_time:18491ms step_avg:33.44ms
step:554/2155 train_time:18524ms step_avg:33.44ms
step:555/2155 train_time:18556ms step_avg:33.44ms
step:556/2155 train_time:18590ms step_avg:33.43ms
step:557/2155 train_time:18622ms step_avg:33.43ms
step:558/2155 train_time:18656ms step_avg:33.43ms
step:559/2155 train_time:18689ms step_avg:33.43ms
step:560/2155 train_time:18722ms step_avg:33.43ms
step:561/2155 train_time:18755ms step_avg:33.43ms
step:562/2155 train_time:18789ms step_avg:33.43ms
step:563/2155 train_time:18821ms step_avg:33.43ms
step:564/2155 train_time:18855ms step_avg:33.43ms
step:565/2155 train_time:18888ms step_avg:33.43ms
step:566/2155 train_time:18922ms step_avg:33.43ms
step:567/2155 train_time:18955ms step_avg:33.43ms
step:568/2155 train_time:18989ms step_avg:33.43ms
step:569/2155 train_time:19022ms step_avg:33.43ms
step:570/2155 train_time:19055ms step_avg:33.43ms
step:571/2155 train_time:19088ms step_avg:33.43ms
step:572/2155 train_time:19121ms step_avg:33.43ms
step:573/2155 train_time:19154ms step_avg:33.43ms
step:574/2155 train_time:19188ms step_avg:33.43ms
step:575/2155 train_time:19220ms step_avg:33.43ms
step:576/2155 train_time:19253ms step_avg:33.43ms
step:577/2155 train_time:19286ms step_avg:33.42ms
step:578/2155 train_time:19319ms step_avg:33.42ms
step:579/2155 train_time:19352ms step_avg:33.42ms
step:580/2155 train_time:19386ms step_avg:33.42ms
step:581/2155 train_time:19418ms step_avg:33.42ms
step:582/2155 train_time:19451ms step_avg:33.42ms
step:583/2155 train_time:19484ms step_avg:33.42ms
step:584/2155 train_time:19518ms step_avg:33.42ms
step:585/2155 train_time:19550ms step_avg:33.42ms
step:586/2155 train_time:19584ms step_avg:33.42ms
step:587/2155 train_time:19616ms step_avg:33.42ms
step:588/2155 train_time:19649ms step_avg:33.42ms
step:589/2155 train_time:19682ms step_avg:33.42ms
step:590/2155 train_time:19716ms step_avg:33.42ms
step:591/2155 train_time:19748ms step_avg:33.41ms
step:592/2155 train_time:19781ms step_avg:33.41ms
step:593/2155 train_time:19814ms step_avg:33.41ms
step:594/2155 train_time:19847ms step_avg:33.41ms
step:595/2155 train_time:19880ms step_avg:33.41ms
step:596/2155 train_time:19914ms step_avg:33.41ms
step:597/2155 train_time:19947ms step_avg:33.41ms
step:598/2155 train_time:19980ms step_avg:33.41ms
step:599/2155 train_time:20013ms step_avg:33.41ms
step:600/2155 train_time:20047ms step_avg:33.41ms
step:601/2155 train_time:20079ms step_avg:33.41ms
step:602/2155 train_time:20113ms step_avg:33.41ms
step:603/2155 train_time:20146ms step_avg:33.41ms
step:604/2155 train_time:20179ms step_avg:33.41ms
step:605/2155 train_time:20212ms step_avg:33.41ms
step:606/2155 train_time:20246ms step_avg:33.41ms
step:607/2155 train_time:20279ms step_avg:33.41ms
step:608/2155 train_time:20312ms step_avg:33.41ms
step:609/2155 train_time:20345ms step_avg:33.41ms
step:610/2155 train_time:20378ms step_avg:33.41ms
step:611/2155 train_time:20411ms step_avg:33.41ms
step:612/2155 train_time:20444ms step_avg:33.41ms
step:613/2155 train_time:20476ms step_avg:33.40ms
step:614/2155 train_time:20509ms step_avg:33.40ms
step:615/2155 train_time:20542ms step_avg:33.40ms
step:616/2155 train_time:20575ms step_avg:33.40ms
step:617/2155 train_time:20608ms step_avg:33.40ms
step:618/2155 train_time:20641ms step_avg:33.40ms
step:619/2155 train_time:20674ms step_avg:33.40ms
step:620/2155 train_time:20708ms step_avg:33.40ms
step:621/2155 train_time:20740ms step_avg:33.40ms
step:622/2155 train_time:20774ms step_avg:33.40ms
step:623/2155 train_time:20807ms step_avg:33.40ms
step:624/2155 train_time:20840ms step_avg:33.40ms
step:625/2155 train_time:20873ms step_avg:33.40ms
step:626/2155 train_time:20906ms step_avg:33.40ms
step:627/2155 train_time:20939ms step_avg:33.40ms
step:628/2155 train_time:20972ms step_avg:33.40ms
step:629/2155 train_time:21004ms step_avg:33.39ms
step:630/2155 train_time:21038ms step_avg:33.39ms
step:631/2155 train_time:21071ms step_avg:33.39ms
step:632/2155 train_time:21104ms step_avg:33.39ms
step:633/2155 train_time:21137ms step_avg:33.39ms
step:634/2155 train_time:21170ms step_avg:33.39ms
step:635/2155 train_time:21203ms step_avg:33.39ms
step:636/2155 train_time:21236ms step_avg:33.39ms
step:637/2155 train_time:21269ms step_avg:33.39ms
step:638/2155 train_time:21302ms step_avg:33.39ms
step:639/2155 train_time:21335ms step_avg:33.39ms
step:640/2155 train_time:21369ms step_avg:33.39ms
step:641/2155 train_time:21402ms step_avg:33.39ms
step:642/2155 train_time:21435ms step_avg:33.39ms
step:643/2155 train_time:21468ms step_avg:33.39ms
step:644/2155 train_time:21501ms step_avg:33.39ms
step:645/2155 train_time:21534ms step_avg:33.39ms
step:646/2155 train_time:21568ms step_avg:33.39ms
step:647/2155 train_time:21601ms step_avg:33.39ms
step:648/2155 train_time:21634ms step_avg:33.39ms
step:649/2155 train_time:21667ms step_avg:33.39ms
step:650/2155 train_time:21700ms step_avg:33.39ms
step:651/2155 train_time:21733ms step_avg:33.38ms
step:652/2155 train_time:21766ms step_avg:33.38ms
step:653/2155 train_time:21799ms step_avg:33.38ms
step:654/2155 train_time:21832ms step_avg:33.38ms
step:655/2155 train_time:21865ms step_avg:33.38ms
step:656/2155 train_time:21899ms step_avg:33.38ms
step:657/2155 train_time:21932ms step_avg:33.38ms
step:658/2155 train_time:21965ms step_avg:33.38ms
step:659/2155 train_time:21998ms step_avg:33.38ms
step:660/2155 train_time:22032ms step_avg:33.38ms
step:661/2155 train_time:22064ms step_avg:33.38ms
step:662/2155 train_time:22098ms step_avg:33.38ms
step:663/2155 train_time:22130ms step_avg:33.38ms
step:664/2155 train_time:22164ms step_avg:33.38ms
step:665/2155 train_time:22197ms step_avg:33.38ms
step:666/2155 train_time:22230ms step_avg:33.38ms
step:667/2155 train_time:22262ms step_avg:33.38ms
step:668/2155 train_time:22296ms step_avg:33.38ms
step:669/2155 train_time:22328ms step_avg:33.38ms
step:670/2155 train_time:22362ms step_avg:33.38ms
step:671/2155 train_time:22395ms step_avg:33.37ms
step:672/2155 train_time:22428ms step_avg:33.37ms
step:673/2155 train_time:22461ms step_avg:33.37ms
step:674/2155 train_time:22494ms step_avg:33.37ms
step:675/2155 train_time:22527ms step_avg:33.37ms
step:676/2155 train_time:22561ms step_avg:33.37ms
step:677/2155 train_time:22594ms step_avg:33.37ms
step:678/2155 train_time:22627ms step_avg:33.37ms
step:679/2155 train_time:22660ms step_avg:33.37ms
step:680/2155 train_time:22693ms step_avg:33.37ms
step:681/2155 train_time:22726ms step_avg:33.37ms
step:682/2155 train_time:22760ms step_avg:33.37ms
step:683/2155 train_time:22793ms step_avg:33.37ms
step:684/2155 train_time:22826ms step_avg:33.37ms
step:685/2155 train_time:22859ms step_avg:33.37ms
step:686/2155 train_time:22892ms step_avg:33.37ms
step:687/2155 train_time:22925ms step_avg:33.37ms
step:688/2155 train_time:22958ms step_avg:33.37ms
step:689/2155 train_time:22991ms step_avg:33.37ms
step:690/2155 train_time:23024ms step_avg:33.37ms
step:691/2155 train_time:23057ms step_avg:33.37ms
step:692/2155 train_time:23090ms step_avg:33.37ms
step:693/2155 train_time:23123ms step_avg:33.37ms
step:694/2155 train_time:23157ms step_avg:33.37ms
step:695/2155 train_time:23190ms step_avg:33.37ms
step:696/2155 train_time:23223ms step_avg:33.37ms
step:697/2155 train_time:23256ms step_avg:33.37ms
step:698/2155 train_time:23289ms step_avg:33.37ms
step:699/2155 train_time:23322ms step_avg:33.36ms
step:700/2155 train_time:23355ms step_avg:33.36ms
step:701/2155 train_time:23388ms step_avg:33.36ms
step:702/2155 train_time:23421ms step_avg:33.36ms
step:703/2155 train_time:23454ms step_avg:33.36ms
step:704/2155 train_time:23487ms step_avg:33.36ms
step:705/2155 train_time:23520ms step_avg:33.36ms
step:706/2155 train_time:23554ms step_avg:33.36ms
step:707/2155 train_time:23613ms step_avg:33.40ms
step:708/2155 train_time:23673ms step_avg:33.44ms
step:709/2155 train_time:23734ms step_avg:33.47ms
step:710/2155 train_time:23794ms step_avg:33.51ms
step:711/2155 train_time:23855ms step_avg:33.55ms
step:712/2155 train_time:23914ms step_avg:33.59ms
step:713/2155 train_time:23975ms step_avg:33.63ms
step:714/2155 train_time:24034ms step_avg:33.66ms
step:715/2155 train_time:24096ms step_avg:33.70ms
step:716/2155 train_time:24155ms step_avg:33.74ms
step:717/2155 train_time:24217ms step_avg:33.78ms
step:718/2155 train_time:24277ms step_avg:33.81ms
step:719/2155 train_time:24338ms step_avg:33.85ms
step:720/2155 train_time:24397ms step_avg:33.88ms
step:721/2155 train_time:24458ms step_avg:33.92ms
step:722/2155 train_time:24516ms step_avg:33.96ms
step:723/2155 train_time:24578ms step_avg:33.99ms
step:724/2155 train_time:24637ms step_avg:34.03ms
step:725/2155 train_time:24698ms step_avg:34.07ms
step:726/2155 train_time:24757ms step_avg:34.10ms
step:727/2155 train_time:24819ms step_avg:34.14ms
step:728/2155 train_time:24879ms step_avg:34.17ms
step:729/2155 train_time:24940ms step_avg:34.21ms
step:730/2155 train_time:24999ms step_avg:34.25ms
step:731/2155 train_time:25060ms step_avg:34.28ms
step:732/2155 train_time:25120ms step_avg:34.32ms
step:733/2155 train_time:25181ms step_avg:34.35ms
step:734/2155 train_time:25240ms step_avg:34.39ms
step:735/2155 train_time:25301ms step_avg:34.42ms
step:736/2155 train_time:25361ms step_avg:34.46ms
step:737/2155 train_time:25422ms step_avg:34.49ms
step:738/2155 train_time:25481ms step_avg:34.53ms
step:739/2155 train_time:25542ms step_avg:34.56ms
step:740/2155 train_time:25602ms step_avg:34.60ms
step:741/2155 train_time:25663ms step_avg:34.63ms
step:742/2155 train_time:25723ms step_avg:34.67ms
step:743/2155 train_time:25784ms step_avg:34.70ms
step:744/2155 train_time:25844ms step_avg:34.74ms
step:745/2155 train_time:25906ms step_avg:34.77ms
step:746/2155 train_time:25966ms step_avg:34.81ms
step:747/2155 train_time:26027ms step_avg:34.84ms
step:748/2155 train_time:26087ms step_avg:34.88ms
step:749/2155 train_time:26148ms step_avg:34.91ms
step:750/2155 train_time:26207ms step_avg:34.94ms
step:750/2155 val_loss:3.8754 train_time:26270ms step_avg:35.03ms
step:751/2155 train_time:26293ms step_avg:35.01ms
step:752/2155 train_time:26329ms step_avg:35.01ms
step:753/2155 train_time:26394ms step_avg:35.05ms
step:754/2155 train_time:26456ms step_avg:35.09ms
step:755/2155 train_time:26517ms step_avg:35.12ms
step:756/2155 train_time:26577ms step_avg:35.15ms
step:757/2155 train_time:26637ms step_avg:35.19ms
step:758/2155 train_time:26695ms step_avg:35.22ms
step:759/2155 train_time:26755ms step_avg:35.25ms
step:760/2155 train_time:26814ms step_avg:35.28ms
step:761/2155 train_time:26874ms step_avg:35.31ms
step:762/2155 train_time:26933ms step_avg:35.35ms
step:763/2155 train_time:26994ms step_avg:35.38ms
step:764/2155 train_time:27054ms step_avg:35.41ms
step:765/2155 train_time:27114ms step_avg:35.44ms
step:766/2155 train_time:27177ms step_avg:35.48ms
step:767/2155 train_time:27242ms step_avg:35.52ms
step:768/2155 train_time:27305ms step_avg:35.55ms
step:769/2155 train_time:27367ms step_avg:35.59ms
step:770/2155 train_time:27428ms step_avg:35.62ms
step:771/2155 train_time:27489ms step_avg:35.65ms
step:772/2155 train_time:27548ms step_avg:35.68ms
step:773/2155 train_time:27609ms step_avg:35.72ms
step:774/2155 train_time:27668ms step_avg:35.75ms
step:775/2155 train_time:27728ms step_avg:35.78ms
step:776/2155 train_time:27787ms step_avg:35.81ms
step:777/2155 train_time:27848ms step_avg:35.84ms
step:778/2155 train_time:27907ms step_avg:35.87ms
step:779/2155 train_time:27968ms step_avg:35.90ms
step:780/2155 train_time:28026ms step_avg:35.93ms
step:781/2155 train_time:28087ms step_avg:35.96ms
step:782/2155 train_time:28147ms step_avg:35.99ms
step:783/2155 train_time:28209ms step_avg:36.03ms
step:784/2155 train_time:28270ms step_avg:36.06ms
step:785/2155 train_time:28332ms step_avg:36.09ms
step:786/2155 train_time:28392ms step_avg:36.12ms
step:787/2155 train_time:28453ms step_avg:36.15ms
step:788/2155 train_time:28512ms step_avg:36.18ms
step:789/2155 train_time:28573ms step_avg:36.21ms
step:790/2155 train_time:28632ms step_avg:36.24ms
step:791/2155 train_time:28692ms step_avg:36.27ms
step:792/2155 train_time:28752ms step_avg:36.30ms
step:793/2155 train_time:28812ms step_avg:36.33ms
step:794/2155 train_time:28871ms step_avg:36.36ms
step:795/2155 train_time:28932ms step_avg:36.39ms
step:796/2155 train_time:28991ms step_avg:36.42ms
step:797/2155 train_time:29051ms step_avg:36.45ms
step:798/2155 train_time:29111ms step_avg:36.48ms
step:799/2155 train_time:29172ms step_avg:36.51ms
step:800/2155 train_time:29233ms step_avg:36.54ms
step:801/2155 train_time:29295ms step_avg:36.57ms
step:802/2155 train_time:29355ms step_avg:36.60ms
step:803/2155 train_time:29416ms step_avg:36.63ms
step:804/2155 train_time:29475ms step_avg:36.66ms
step:805/2155 train_time:29536ms step_avg:36.69ms
step:806/2155 train_time:29595ms step_avg:36.72ms
step:807/2155 train_time:29656ms step_avg:36.75ms
step:808/2155 train_time:29715ms step_avg:36.78ms
step:809/2155 train_time:29777ms step_avg:36.81ms
step:810/2155 train_time:29836ms step_avg:36.83ms
step:811/2155 train_time:29897ms step_avg:36.86ms
step:812/2155 train_time:29956ms step_avg:36.89ms
step:813/2155 train_time:30017ms step_avg:36.92ms
step:814/2155 train_time:30077ms step_avg:36.95ms
step:815/2155 train_time:30138ms step_avg:36.98ms
step:816/2155 train_time:30198ms step_avg:37.01ms
step:817/2155 train_time:30260ms step_avg:37.04ms
step:818/2155 train_time:30319ms step_avg:37.06ms
step:819/2155 train_time:30380ms step_avg:37.09ms
step:820/2155 train_time:30440ms step_avg:37.12ms
step:821/2155 train_time:30501ms step_avg:37.15ms
step:822/2155 train_time:30561ms step_avg:37.18ms
step:823/2155 train_time:30622ms step_avg:37.21ms
step:824/2155 train_time:30681ms step_avg:37.23ms
step:825/2155 train_time:30743ms step_avg:37.26ms
step:826/2155 train_time:30802ms step_avg:37.29ms
step:827/2155 train_time:30863ms step_avg:37.32ms
step:828/2155 train_time:30923ms step_avg:37.35ms
step:829/2155 train_time:30984ms step_avg:37.38ms
step:830/2155 train_time:31043ms step_avg:37.40ms
step:831/2155 train_time:31105ms step_avg:37.43ms
step:832/2155 train_time:31164ms step_avg:37.46ms
step:833/2155 train_time:31226ms step_avg:37.49ms
step:834/2155 train_time:31285ms step_avg:37.51ms
step:835/2155 train_time:31347ms step_avg:37.54ms
step:836/2155 train_time:31407ms step_avg:37.57ms
step:837/2155 train_time:31469ms step_avg:37.60ms
step:838/2155 train_time:31528ms step_avg:37.62ms
step:839/2155 train_time:31589ms step_avg:37.65ms
step:840/2155 train_time:31648ms step_avg:37.68ms
step:841/2155 train_time:31709ms step_avg:37.70ms
step:842/2155 train_time:31768ms step_avg:37.73ms
step:843/2155 train_time:31830ms step_avg:37.76ms
step:844/2155 train_time:31890ms step_avg:37.78ms
step:845/2155 train_time:31951ms step_avg:37.81ms
step:846/2155 train_time:32010ms step_avg:37.84ms
step:847/2155 train_time:32071ms step_avg:37.86ms
step:848/2155 train_time:32131ms step_avg:37.89ms
step:849/2155 train_time:32191ms step_avg:37.92ms
step:850/2155 train_time:32251ms step_avg:37.94ms
step:851/2155 train_time:32312ms step_avg:37.97ms
step:852/2155 train_time:32371ms step_avg:37.99ms
step:853/2155 train_time:32432ms step_avg:38.02ms
step:854/2155 train_time:32491ms step_avg:38.05ms
step:855/2155 train_time:32552ms step_avg:38.07ms
step:856/2155 train_time:32612ms step_avg:38.10ms
step:857/2155 train_time:32672ms step_avg:38.12ms
step:858/2155 train_time:32731ms step_avg:38.15ms
step:859/2155 train_time:32793ms step_avg:38.18ms
step:860/2155 train_time:32852ms step_avg:38.20ms
step:861/2155 train_time:32913ms step_avg:38.23ms
step:862/2155 train_time:32972ms step_avg:38.25ms
step:863/2155 train_time:33033ms step_avg:38.28ms
step:864/2155 train_time:33092ms step_avg:38.30ms
step:865/2155 train_time:33154ms step_avg:38.33ms
step:866/2155 train_time:33214ms step_avg:38.35ms
step:867/2155 train_time:33275ms step_avg:38.38ms
step:868/2155 train_time:33335ms step_avg:38.40ms
step:869/2155 train_time:33396ms step_avg:38.43ms
step:870/2155 train_time:33456ms step_avg:38.46ms
step:871/2155 train_time:33517ms step_avg:38.48ms
step:872/2155 train_time:33577ms step_avg:38.51ms
step:873/2155 train_time:33638ms step_avg:38.53ms
step:874/2155 train_time:33698ms step_avg:38.56ms
step:875/2155 train_time:33759ms step_avg:38.58ms
step:876/2155 train_time:33819ms step_avg:38.61ms
step:877/2155 train_time:33880ms step_avg:38.63ms
step:878/2155 train_time:33940ms step_avg:38.66ms
step:879/2155 train_time:34001ms step_avg:38.68ms
step:880/2155 train_time:34061ms step_avg:38.71ms
step:881/2155 train_time:34123ms step_avg:38.73ms
step:882/2155 train_time:34183ms step_avg:38.76ms
step:883/2155 train_time:34245ms step_avg:38.78ms
step:884/2155 train_time:34305ms step_avg:38.81ms
step:885/2155 train_time:34366ms step_avg:38.83ms
step:886/2155 train_time:34426ms step_avg:38.86ms
step:887/2155 train_time:34487ms step_avg:38.88ms
step:888/2155 train_time:34547ms step_avg:38.90ms
step:889/2155 train_time:34608ms step_avg:38.93ms
step:890/2155 train_time:34668ms step_avg:38.95ms
step:891/2155 train_time:34729ms step_avg:38.98ms
step:892/2155 train_time:34788ms step_avg:39.00ms
step:893/2155 train_time:34850ms step_avg:39.03ms
step:894/2155 train_time:34910ms step_avg:39.05ms
step:895/2155 train_time:34970ms step_avg:39.07ms
step:896/2155 train_time:35030ms step_avg:39.10ms
step:897/2155 train_time:35091ms step_avg:39.12ms
step:898/2155 train_time:35151ms step_avg:39.14ms
step:899/2155 train_time:35212ms step_avg:39.17ms
step:900/2155 train_time:35271ms step_avg:39.19ms
step:901/2155 train_time:35332ms step_avg:39.21ms
step:902/2155 train_time:35392ms step_avg:39.24ms
step:903/2155 train_time:35452ms step_avg:39.26ms
step:904/2155 train_time:35512ms step_avg:39.28ms
step:905/2155 train_time:35572ms step_avg:39.31ms
step:906/2155 train_time:35631ms step_avg:39.33ms
step:907/2155 train_time:35692ms step_avg:39.35ms
step:908/2155 train_time:35752ms step_avg:39.37ms
step:909/2155 train_time:35813ms step_avg:39.40ms
step:910/2155 train_time:35872ms step_avg:39.42ms
step:911/2155 train_time:35933ms step_avg:39.44ms
step:912/2155 train_time:35993ms step_avg:39.47ms
step:913/2155 train_time:36054ms step_avg:39.49ms
step:914/2155 train_time:36114ms step_avg:39.51ms
step:915/2155 train_time:36175ms step_avg:39.54ms
step:916/2155 train_time:36234ms step_avg:39.56ms
step:917/2155 train_time:36296ms step_avg:39.58ms
step:918/2155 train_time:36357ms step_avg:39.60ms
step:919/2155 train_time:36419ms step_avg:39.63ms
step:920/2155 train_time:36478ms step_avg:39.65ms
step:921/2155 train_time:36538ms step_avg:39.67ms
step:922/2155 train_time:36598ms step_avg:39.69ms
step:923/2155 train_time:36659ms step_avg:39.72ms
step:924/2155 train_time:36718ms step_avg:39.74ms
step:925/2155 train_time:36780ms step_avg:39.76ms
step:926/2155 train_time:36840ms step_avg:39.78ms
step:927/2155 train_time:36902ms step_avg:39.81ms
step:928/2155 train_time:36962ms step_avg:39.83ms
step:929/2155 train_time:37023ms step_avg:39.85ms
step:930/2155 train_time:37083ms step_avg:39.87ms
step:931/2155 train_time:37144ms step_avg:39.90ms
step:932/2155 train_time:37204ms step_avg:39.92ms
step:933/2155 train_time:37265ms step_avg:39.94ms
step:934/2155 train_time:37325ms step_avg:39.96ms
step:935/2155 train_time:37387ms step_avg:39.99ms
step:936/2155 train_time:37447ms step_avg:40.01ms
step:937/2155 train_time:37508ms step_avg:40.03ms
step:938/2155 train_time:37567ms step_avg:40.05ms
step:939/2155 train_time:37628ms step_avg:40.07ms
step:940/2155 train_time:37687ms step_avg:40.09ms
step:941/2155 train_time:37749ms step_avg:40.12ms
step:942/2155 train_time:37808ms step_avg:40.14ms
step:943/2155 train_time:37869ms step_avg:40.16ms
step:944/2155 train_time:37928ms step_avg:40.18ms
step:945/2155 train_time:37989ms step_avg:40.20ms
step:946/2155 train_time:38049ms step_avg:40.22ms
step:947/2155 train_time:38110ms step_avg:40.24ms
step:948/2155 train_time:38170ms step_avg:40.26ms
step:949/2155 train_time:38231ms step_avg:40.29ms
step:950/2155 train_time:38291ms step_avg:40.31ms
step:951/2155 train_time:38352ms step_avg:40.33ms
step:952/2155 train_time:38411ms step_avg:40.35ms
step:953/2155 train_time:38472ms step_avg:40.37ms
step:954/2155 train_time:38531ms step_avg:40.39ms
step:955/2155 train_time:38592ms step_avg:40.41ms
step:956/2155 train_time:38652ms step_avg:40.43ms
step:957/2155 train_time:38713ms step_avg:40.45ms
step:958/2155 train_time:38772ms step_avg:40.47ms
step:959/2155 train_time:38833ms step_avg:40.49ms
step:960/2155 train_time:38892ms step_avg:40.51ms
step:961/2155 train_time:38953ms step_avg:40.53ms
step:962/2155 train_time:39013ms step_avg:40.55ms
step:963/2155 train_time:39074ms step_avg:40.58ms
step:964/2155 train_time:39133ms step_avg:40.59ms
step:965/2155 train_time:39195ms step_avg:40.62ms
step:966/2155 train_time:39254ms step_avg:40.64ms
step:967/2155 train_time:39315ms step_avg:40.66ms
step:968/2155 train_time:39375ms step_avg:40.68ms
step:969/2155 train_time:39436ms step_avg:40.70ms
step:970/2155 train_time:39496ms step_avg:40.72ms
step:971/2155 train_time:39558ms step_avg:40.74ms
step:972/2155 train_time:39618ms step_avg:40.76ms
step:973/2155 train_time:39680ms step_avg:40.78ms
step:974/2155 train_time:39739ms step_avg:40.80ms
step:975/2155 train_time:39800ms step_avg:40.82ms
step:976/2155 train_time:39860ms step_avg:40.84ms
step:977/2155 train_time:39921ms step_avg:40.86ms
step:978/2155 train_time:39981ms step_avg:40.88ms
step:979/2155 train_time:40043ms step_avg:40.90ms
step:980/2155 train_time:40103ms step_avg:40.92ms
step:981/2155 train_time:40165ms step_avg:40.94ms
step:982/2155 train_time:40224ms step_avg:40.96ms
step:983/2155 train_time:40285ms step_avg:40.98ms
step:984/2155 train_time:40345ms step_avg:41.00ms
step:985/2155 train_time:40406ms step_avg:41.02ms
step:986/2155 train_time:40466ms step_avg:41.04ms
step:987/2155 train_time:40528ms step_avg:41.06ms
step:988/2155 train_time:40587ms step_avg:41.08ms
step:989/2155 train_time:40647ms step_avg:41.10ms
step:990/2155 train_time:40707ms step_avg:41.12ms
step:991/2155 train_time:40768ms step_avg:41.14ms
step:992/2155 train_time:40827ms step_avg:41.16ms
step:993/2155 train_time:40888ms step_avg:41.18ms
step:994/2155 train_time:40947ms step_avg:41.19ms
step:995/2155 train_time:41009ms step_avg:41.22ms
step:996/2155 train_time:41069ms step_avg:41.23ms
step:997/2155 train_time:41130ms step_avg:41.25ms
step:998/2155 train_time:41190ms step_avg:41.27ms
step:999/2155 train_time:41251ms step_avg:41.29ms
step:1000/2155 train_time:41311ms step_avg:41.31ms
step:1000/2155 val_loss:3.7195 train_time:41374ms step_avg:41.37ms
step:1001/2155 train_time:41396ms step_avg:41.35ms
step:1002/2155 train_time:41434ms step_avg:41.35ms
step:1003/2155 train_time:41500ms step_avg:41.38ms
step:1004/2155 train_time:41564ms step_avg:41.40ms
step:1005/2155 train_time:41626ms step_avg:41.42ms
step:1006/2155 train_time:41686ms step_avg:41.44ms
step:1007/2155 train_time:41746ms step_avg:41.46ms
step:1008/2155 train_time:41805ms step_avg:41.47ms
step:1009/2155 train_time:41865ms step_avg:41.49ms
step:1010/2155 train_time:41924ms step_avg:41.51ms
step:1011/2155 train_time:41985ms step_avg:41.53ms
step:1012/2155 train_time:42044ms step_avg:41.55ms
step:1013/2155 train_time:42104ms step_avg:41.56ms
step:1014/2155 train_time:42163ms step_avg:41.58ms
step:1015/2155 train_time:42224ms step_avg:41.60ms
step:1016/2155 train_time:42283ms step_avg:41.62ms
step:1017/2155 train_time:42345ms step_avg:41.64ms
step:1018/2155 train_time:42406ms step_avg:41.66ms
step:1019/2155 train_time:42470ms step_avg:41.68ms
step:1020/2155 train_time:42531ms step_avg:41.70ms
step:1021/2155 train_time:42594ms step_avg:41.72ms
step:1022/2155 train_time:42654ms step_avg:41.74ms
step:1023/2155 train_time:42716ms step_avg:41.76ms
step:1024/2155 train_time:42776ms step_avg:41.77ms
step:1025/2155 train_time:42837ms step_avg:41.79ms
step:1026/2155 train_time:42897ms step_avg:41.81ms
step:1027/2155 train_time:42958ms step_avg:41.83ms
step:1028/2155 train_time:43017ms step_avg:41.85ms
step:1029/2155 train_time:43078ms step_avg:41.86ms
step:1030/2155 train_time:43138ms step_avg:41.88ms
step:1031/2155 train_time:43199ms step_avg:41.90ms
step:1032/2155 train_time:43258ms step_avg:41.92ms
step:1033/2155 train_time:43320ms step_avg:41.94ms
step:1034/2155 train_time:43381ms step_avg:41.95ms
step:1035/2155 train_time:43446ms step_avg:41.98ms
step:1036/2155 train_time:43507ms step_avg:41.99ms
step:1037/2155 train_time:43568ms step_avg:42.01ms
step:1038/2155 train_time:43628ms step_avg:42.03ms
step:1039/2155 train_time:43689ms step_avg:42.05ms
step:1040/2155 train_time:43749ms step_avg:42.07ms
step:1041/2155 train_time:43809ms step_avg:42.08ms
step:1042/2155 train_time:43869ms step_avg:42.10ms
step:1043/2155 train_time:43929ms step_avg:42.12ms
step:1044/2155 train_time:43989ms step_avg:42.14ms
step:1045/2155 train_time:44049ms step_avg:42.15ms
step:1046/2155 train_time:44109ms step_avg:42.17ms
step:1047/2155 train_time:44169ms step_avg:42.19ms
step:1048/2155 train_time:44229ms step_avg:42.20ms
step:1049/2155 train_time:44290ms step_avg:42.22ms
step:1050/2155 train_time:44349ms step_avg:42.24ms
step:1051/2155 train_time:44411ms step_avg:42.26ms
step:1052/2155 train_time:44471ms step_avg:42.27ms
step:1053/2155 train_time:44533ms step_avg:42.29ms
step:1054/2155 train_time:44593ms step_avg:42.31ms
step:1055/2155 train_time:44656ms step_avg:42.33ms
step:1056/2155 train_time:44715ms step_avg:42.34ms
step:1057/2155 train_time:44777ms step_avg:42.36ms
step:1058/2155 train_time:44836ms step_avg:42.38ms
step:1059/2155 train_time:44898ms step_avg:42.40ms
step:1060/2155 train_time:44957ms step_avg:42.41ms
step:1061/2155 train_time:45018ms step_avg:42.43ms
step:1062/2155 train_time:45078ms step_avg:42.45ms
step:1063/2155 train_time:45139ms step_avg:42.46ms
step:1064/2155 train_time:45198ms step_avg:42.48ms
step:1065/2155 train_time:45259ms step_avg:42.50ms
step:1066/2155 train_time:45319ms step_avg:42.51ms
step:1067/2155 train_time:45381ms step_avg:42.53ms
step:1068/2155 train_time:45441ms step_avg:42.55ms
step:1069/2155 train_time:45503ms step_avg:42.57ms
step:1070/2155 train_time:45564ms step_avg:42.58ms
step:1071/2155 train_time:45626ms step_avg:42.60ms
step:1072/2155 train_time:45685ms step_avg:42.62ms
step:1073/2155 train_time:45747ms step_avg:42.63ms
step:1074/2155 train_time:45808ms step_avg:42.65ms
step:1075/2155 train_time:45867ms step_avg:42.67ms
step:1076/2155 train_time:45926ms step_avg:42.68ms
step:1077/2155 train_time:45987ms step_avg:42.70ms
step:1078/2155 train_time:46046ms step_avg:42.71ms
step:1079/2155 train_time:46108ms step_avg:42.73ms
step:1080/2155 train_time:46167ms step_avg:42.75ms
step:1081/2155 train_time:46228ms step_avg:42.76ms
step:1082/2155 train_time:46287ms step_avg:42.78ms
step:1083/2155 train_time:46349ms step_avg:42.80ms
step:1084/2155 train_time:46408ms step_avg:42.81ms
step:1085/2155 train_time:46469ms step_avg:42.83ms
step:1086/2155 train_time:46529ms step_avg:42.84ms
step:1087/2155 train_time:46590ms step_avg:42.86ms
step:1088/2155 train_time:46650ms step_avg:42.88ms
step:1089/2155 train_time:46711ms step_avg:42.89ms
step:1090/2155 train_time:46770ms step_avg:42.91ms
step:1091/2155 train_time:46831ms step_avg:42.92ms
step:1092/2155 train_time:46890ms step_avg:42.94ms
step:1093/2155 train_time:46951ms step_avg:42.96ms
step:1094/2155 train_time:47011ms step_avg:42.97ms
step:1095/2155 train_time:47073ms step_avg:42.99ms
step:1096/2155 train_time:47133ms step_avg:43.00ms
step:1097/2155 train_time:47195ms step_avg:43.02ms
step:1098/2155 train_time:47255ms step_avg:43.04ms
step:1099/2155 train_time:47316ms step_avg:43.05ms
step:1100/2155 train_time:47376ms step_avg:43.07ms
step:1101/2155 train_time:47437ms step_avg:43.09ms
step:1102/2155 train_time:47497ms step_avg:43.10ms
step:1103/2155 train_time:47559ms step_avg:43.12ms
step:1104/2155 train_time:47619ms step_avg:43.13ms
step:1105/2155 train_time:47681ms step_avg:43.15ms
step:1106/2155 train_time:47741ms step_avg:43.17ms
step:1107/2155 train_time:47803ms step_avg:43.18ms
step:1108/2155 train_time:47863ms step_avg:43.20ms
step:1109/2155 train_time:47924ms step_avg:43.21ms
step:1110/2155 train_time:47984ms step_avg:43.23ms
step:1111/2155 train_time:48046ms step_avg:43.25ms
step:1112/2155 train_time:48106ms step_avg:43.26ms
step:1113/2155 train_time:48167ms step_avg:43.28ms
step:1114/2155 train_time:48226ms step_avg:43.29ms
step:1115/2155 train_time:48287ms step_avg:43.31ms
step:1116/2155 train_time:48346ms step_avg:43.32ms
step:1117/2155 train_time:48407ms step_avg:43.34ms
step:1118/2155 train_time:48467ms step_avg:43.35ms
step:1119/2155 train_time:48529ms step_avg:43.37ms
step:1120/2155 train_time:48589ms step_avg:43.38ms
step:1121/2155 train_time:48650ms step_avg:43.40ms
step:1122/2155 train_time:48710ms step_avg:43.41ms
step:1123/2155 train_time:48771ms step_avg:43.43ms
step:1124/2155 train_time:48832ms step_avg:43.44ms
step:1125/2155 train_time:48892ms step_avg:43.46ms
step:1126/2155 train_time:48952ms step_avg:43.47ms
step:1127/2155 train_time:49013ms step_avg:43.49ms
step:1128/2155 train_time:49073ms step_avg:43.50ms
step:1129/2155 train_time:49134ms step_avg:43.52ms
step:1130/2155 train_time:49194ms step_avg:43.53ms
step:1131/2155 train_time:49256ms step_avg:43.55ms
step:1132/2155 train_time:49316ms step_avg:43.57ms
step:1133/2155 train_time:49378ms step_avg:43.58ms
step:1134/2155 train_time:49438ms step_avg:43.60ms
step:1135/2155 train_time:49500ms step_avg:43.61ms
step:1136/2155 train_time:49559ms step_avg:43.63ms
step:1137/2155 train_time:49621ms step_avg:43.64ms
step:1138/2155 train_time:49680ms step_avg:43.66ms
step:1139/2155 train_time:49742ms step_avg:43.67ms
step:1140/2155 train_time:49802ms step_avg:43.69ms
step:1141/2155 train_time:49864ms step_avg:43.70ms
step:1142/2155 train_time:49924ms step_avg:43.72ms
step:1143/2155 train_time:49986ms step_avg:43.73ms
step:1144/2155 train_time:50046ms step_avg:43.75ms
step:1145/2155 train_time:50107ms step_avg:43.76ms
step:1146/2155 train_time:50166ms step_avg:43.77ms
step:1147/2155 train_time:50227ms step_avg:43.79ms
step:1148/2155 train_time:50286ms step_avg:43.80ms
step:1149/2155 train_time:50347ms step_avg:43.82ms
step:1150/2155 train_time:50406ms step_avg:43.83ms
step:1151/2155 train_time:50468ms step_avg:43.85ms
step:1152/2155 train_time:50527ms step_avg:43.86ms
step:1153/2155 train_time:50589ms step_avg:43.88ms
step:1154/2155 train_time:50649ms step_avg:43.89ms
step:1155/2155 train_time:50710ms step_avg:43.90ms
step:1156/2155 train_time:50770ms step_avg:43.92ms
step:1157/2155 train_time:50831ms step_avg:43.93ms
step:1158/2155 train_time:50891ms step_avg:43.95ms
step:1159/2155 train_time:50952ms step_avg:43.96ms
step:1160/2155 train_time:51013ms step_avg:43.98ms
step:1161/2155 train_time:51073ms step_avg:43.99ms
step:1162/2155 train_time:51133ms step_avg:44.00ms
step:1163/2155 train_time:51194ms step_avg:44.02ms
step:1164/2155 train_time:51254ms step_avg:44.03ms
step:1165/2155 train_time:51316ms step_avg:44.05ms
step:1166/2155 train_time:51376ms step_avg:44.06ms
step:1167/2155 train_time:51438ms step_avg:44.08ms
step:1168/2155 train_time:51498ms step_avg:44.09ms
step:1169/2155 train_time:51560ms step_avg:44.11ms
step:1170/2155 train_time:51619ms step_avg:44.12ms
step:1171/2155 train_time:51681ms step_avg:44.13ms
step:1172/2155 train_time:51741ms step_avg:44.15ms
step:1173/2155 train_time:51803ms step_avg:44.16ms
step:1174/2155 train_time:51863ms step_avg:44.18ms
step:1175/2155 train_time:51924ms step_avg:44.19ms
step:1176/2155 train_time:51984ms step_avg:44.20ms
step:1177/2155 train_time:52046ms step_avg:44.22ms
step:1178/2155 train_time:52105ms step_avg:44.23ms
step:1179/2155 train_time:52167ms step_avg:44.25ms
step:1180/2155 train_time:52226ms step_avg:44.26ms
step:1181/2155 train_time:52287ms step_avg:44.27ms
step:1182/2155 train_time:52346ms step_avg:44.29ms
step:1183/2155 train_time:52407ms step_avg:44.30ms
step:1184/2155 train_time:52466ms step_avg:44.31ms
step:1185/2155 train_time:52528ms step_avg:44.33ms
step:1186/2155 train_time:52587ms step_avg:44.34ms
step:1187/2155 train_time:52649ms step_avg:44.35ms
step:1188/2155 train_time:52708ms step_avg:44.37ms
step:1189/2155 train_time:52769ms step_avg:44.38ms
step:1190/2155 train_time:52829ms step_avg:44.39ms
step:1191/2155 train_time:52890ms step_avg:44.41ms
step:1192/2155 train_time:52950ms step_avg:44.42ms
step:1193/2155 train_time:53011ms step_avg:44.43ms
step:1194/2155 train_time:53071ms step_avg:44.45ms
step:1195/2155 train_time:53132ms step_avg:44.46ms
step:1196/2155 train_time:53192ms step_avg:44.47ms
step:1197/2155 train_time:53253ms step_avg:44.49ms
step:1198/2155 train_time:53313ms step_avg:44.50ms
step:1199/2155 train_time:53374ms step_avg:44.52ms
step:1200/2155 train_time:53434ms step_avg:44.53ms
step:1201/2155 train_time:53495ms step_avg:44.54ms
step:1202/2155 train_time:53555ms step_avg:44.55ms
step:1203/2155 train_time:53616ms step_avg:44.57ms
step:1204/2155 train_time:53677ms step_avg:44.58ms
step:1205/2155 train_time:53739ms step_avg:44.60ms
step:1206/2155 train_time:53799ms step_avg:44.61ms
step:1207/2155 train_time:53860ms step_avg:44.62ms
step:1208/2155 train_time:53919ms step_avg:44.64ms
step:1209/2155 train_time:53981ms step_avg:44.65ms
step:1210/2155 train_time:54041ms step_avg:44.66ms
step:1211/2155 train_time:54103ms step_avg:44.68ms
step:1212/2155 train_time:54163ms step_avg:44.69ms
step:1213/2155 train_time:54224ms step_avg:44.70ms
step:1214/2155 train_time:54284ms step_avg:44.71ms
step:1215/2155 train_time:54345ms step_avg:44.73ms
step:1216/2155 train_time:54405ms step_avg:44.74ms
step:1217/2155 train_time:54466ms step_avg:44.75ms
step:1218/2155 train_time:54525ms step_avg:44.77ms
step:1219/2155 train_time:54586ms step_avg:44.78ms
step:1220/2155 train_time:54646ms step_avg:44.79ms
step:1221/2155 train_time:54707ms step_avg:44.81ms
step:1222/2155 train_time:54767ms step_avg:44.82ms
step:1223/2155 train_time:54828ms step_avg:44.83ms
step:1224/2155 train_time:54887ms step_avg:44.84ms
step:1225/2155 train_time:54948ms step_avg:44.86ms
step:1226/2155 train_time:55008ms step_avg:44.87ms
step:1227/2155 train_time:55069ms step_avg:44.88ms
step:1228/2155 train_time:55129ms step_avg:44.89ms
step:1229/2155 train_time:55190ms step_avg:44.91ms
step:1230/2155 train_time:55250ms step_avg:44.92ms
step:1231/2155 train_time:55310ms step_avg:44.93ms
step:1232/2155 train_time:55370ms step_avg:44.94ms
step:1233/2155 train_time:55431ms step_avg:44.96ms
step:1234/2155 train_time:55490ms step_avg:44.97ms
step:1235/2155 train_time:55551ms step_avg:44.98ms
step:1236/2155 train_time:55611ms step_avg:44.99ms
step:1237/2155 train_time:55673ms step_avg:45.01ms
step:1238/2155 train_time:55733ms step_avg:45.02ms
step:1239/2155 train_time:55795ms step_avg:45.03ms
step:1240/2155 train_time:55854ms step_avg:45.04ms
step:1241/2155 train_time:55916ms step_avg:45.06ms
step:1242/2155 train_time:55975ms step_avg:45.07ms
step:1243/2155 train_time:56037ms step_avg:45.08ms
step:1244/2155 train_time:56096ms step_avg:45.09ms
step:1245/2155 train_time:56158ms step_avg:45.11ms
step:1246/2155 train_time:56218ms step_avg:45.12ms
step:1247/2155 train_time:56281ms step_avg:45.13ms
step:1248/2155 train_time:56340ms step_avg:45.14ms
step:1249/2155 train_time:56401ms step_avg:45.16ms
step:1250/2155 train_time:56461ms step_avg:45.17ms
step:1250/2155 val_loss:3.5916 train_time:56525ms step_avg:45.22ms
step:1251/2155 train_time:56548ms step_avg:45.20ms
step:1252/2155 train_time:56584ms step_avg:45.19ms
step:1253/2155 train_time:56647ms step_avg:45.21ms
step:1254/2155 train_time:56710ms step_avg:45.22ms
step:1255/2155 train_time:56772ms step_avg:45.24ms
step:1256/2155 train_time:56831ms step_avg:45.25ms
step:1257/2155 train_time:56893ms step_avg:45.26ms
step:1258/2155 train_time:56952ms step_avg:45.27ms
step:1259/2155 train_time:57013ms step_avg:45.28ms
step:1260/2155 train_time:57072ms step_avg:45.29ms
step:1261/2155 train_time:57132ms step_avg:45.31ms
step:1262/2155 train_time:57191ms step_avg:45.32ms
step:1263/2155 train_time:57252ms step_avg:45.33ms
step:1264/2155 train_time:57313ms step_avg:45.34ms
step:1265/2155 train_time:57374ms step_avg:45.36ms
step:1266/2155 train_time:57434ms step_avg:45.37ms
step:1267/2155 train_time:57498ms step_avg:45.38ms
step:1268/2155 train_time:57558ms step_avg:45.39ms
step:1269/2155 train_time:57624ms step_avg:45.41ms
step:1270/2155 train_time:57685ms step_avg:45.42ms
step:1271/2155 train_time:57746ms step_avg:45.43ms
step:1272/2155 train_time:57805ms step_avg:45.44ms
step:1273/2155 train_time:57866ms step_avg:45.46ms
step:1274/2155 train_time:57925ms step_avg:45.47ms
step:1275/2155 train_time:57986ms step_avg:45.48ms
step:1276/2155 train_time:58045ms step_avg:45.49ms
step:1277/2155 train_time:58105ms step_avg:45.50ms
step:1278/2155 train_time:58164ms step_avg:45.51ms
step:1279/2155 train_time:58224ms step_avg:45.52ms
step:1280/2155 train_time:58283ms step_avg:45.53ms
step:1281/2155 train_time:58344ms step_avg:45.55ms
step:1282/2155 train_time:58403ms step_avg:45.56ms
step:1283/2155 train_time:58465ms step_avg:45.57ms
step:1284/2155 train_time:58525ms step_avg:45.58ms
step:1285/2155 train_time:58587ms step_avg:45.59ms
step:1286/2155 train_time:58647ms step_avg:45.60ms
step:1287/2155 train_time:58709ms step_avg:45.62ms
step:1288/2155 train_time:58768ms step_avg:45.63ms
step:1289/2155 train_time:58830ms step_avg:45.64ms
step:1290/2155 train_time:58891ms step_avg:45.65ms
step:1291/2155 train_time:58952ms step_avg:45.66ms
step:1292/2155 train_time:59011ms step_avg:45.67ms
step:1293/2155 train_time:59072ms step_avg:45.69ms
step:1294/2155 train_time:59131ms step_avg:45.70ms
step:1295/2155 train_time:59192ms step_avg:45.71ms
step:1296/2155 train_time:59251ms step_avg:45.72ms
step:1297/2155 train_time:59313ms step_avg:45.73ms
step:1298/2155 train_time:59373ms step_avg:45.74ms
step:1299/2155 train_time:59434ms step_avg:45.75ms
step:1300/2155 train_time:59494ms step_avg:45.76ms
step:1301/2155 train_time:59555ms step_avg:45.78ms
step:1302/2155 train_time:59616ms step_avg:45.79ms
step:1303/2155 train_time:59678ms step_avg:45.80ms
step:1304/2155 train_time:59739ms step_avg:45.81ms
step:1305/2155 train_time:59801ms step_avg:45.82ms
step:1306/2155 train_time:59860ms step_avg:45.83ms
step:1307/2155 train_time:59922ms step_avg:45.85ms
step:1308/2155 train_time:59981ms step_avg:45.86ms
step:1309/2155 train_time:60041ms step_avg:45.87ms
step:1310/2155 train_time:60101ms step_avg:45.88ms
step:1311/2155 train_time:60162ms step_avg:45.89ms
step:1312/2155 train_time:60221ms step_avg:45.90ms
step:1313/2155 train_time:60283ms step_avg:45.91ms
step:1314/2155 train_time:60342ms step_avg:45.92ms
step:1315/2155 train_time:60403ms step_avg:45.93ms
step:1316/2155 train_time:60463ms step_avg:45.94ms
step:1317/2155 train_time:60524ms step_avg:45.96ms
step:1318/2155 train_time:60584ms step_avg:45.97ms
step:1319/2155 train_time:60646ms step_avg:45.98ms
step:1320/2155 train_time:60706ms step_avg:45.99ms
step:1321/2155 train_time:60767ms step_avg:46.00ms
step:1322/2155 train_time:60827ms step_avg:46.01ms
step:1323/2155 train_time:60888ms step_avg:46.02ms
step:1324/2155 train_time:60948ms step_avg:46.03ms
step:1325/2155 train_time:61008ms step_avg:46.04ms
step:1326/2155 train_time:61068ms step_avg:46.05ms
step:1327/2155 train_time:61129ms step_avg:46.07ms
step:1328/2155 train_time:61189ms step_avg:46.08ms
step:1329/2155 train_time:61250ms step_avg:46.09ms
step:1330/2155 train_time:61310ms step_avg:46.10ms
step:1331/2155 train_time:61372ms step_avg:46.11ms
step:1332/2155 train_time:61432ms step_avg:46.12ms
step:1333/2155 train_time:61494ms step_avg:46.13ms
step:1334/2155 train_time:61554ms step_avg:46.14ms
step:1335/2155 train_time:61617ms step_avg:46.15ms
step:1336/2155 train_time:61677ms step_avg:46.17ms
step:1337/2155 train_time:61739ms step_avg:46.18ms
step:1338/2155 train_time:61799ms step_avg:46.19ms
step:1339/2155 train_time:61861ms step_avg:46.20ms
step:1340/2155 train_time:61920ms step_avg:46.21ms
step:1341/2155 train_time:61982ms step_avg:46.22ms
step:1342/2155 train_time:62041ms step_avg:46.23ms
step:1343/2155 train_time:62103ms step_avg:46.24ms
step:1344/2155 train_time:62162ms step_avg:46.25ms
step:1345/2155 train_time:62223ms step_avg:46.26ms
step:1346/2155 train_time:62282ms step_avg:46.27ms
step:1347/2155 train_time:62343ms step_avg:46.28ms
step:1348/2155 train_time:62402ms step_avg:46.29ms
step:1349/2155 train_time:62464ms step_avg:46.30ms
step:1350/2155 train_time:62523ms step_avg:46.31ms
step:1351/2155 train_time:62584ms step_avg:46.32ms
step:1352/2155 train_time:62645ms step_avg:46.33ms
step:1353/2155 train_time:62706ms step_avg:46.35ms
step:1354/2155 train_time:62766ms step_avg:46.36ms
step:1355/2155 train_time:62827ms step_avg:46.37ms
step:1356/2155 train_time:62887ms step_avg:46.38ms
step:1357/2155 train_time:62948ms step_avg:46.39ms
step:1358/2155 train_time:63008ms step_avg:46.40ms
step:1359/2155 train_time:63068ms step_avg:46.41ms
step:1360/2155 train_time:63128ms step_avg:46.42ms
step:1361/2155 train_time:63189ms step_avg:46.43ms
step:1362/2155 train_time:63249ms step_avg:46.44ms
step:1363/2155 train_time:63309ms step_avg:46.45ms
step:1364/2155 train_time:63369ms step_avg:46.46ms
step:1365/2155 train_time:63431ms step_avg:46.47ms
step:1366/2155 train_time:63490ms step_avg:46.48ms
step:1367/2155 train_time:63552ms step_avg:46.49ms
step:1368/2155 train_time:63612ms step_avg:46.50ms
step:1369/2155 train_time:63674ms step_avg:46.51ms
step:1370/2155 train_time:63733ms step_avg:46.52ms
step:1371/2155 train_time:63795ms step_avg:46.53ms
step:1372/2155 train_time:63855ms step_avg:46.54ms
step:1373/2155 train_time:63917ms step_avg:46.55ms
step:1374/2155 train_time:63977ms step_avg:46.56ms
step:1375/2155 train_time:64038ms step_avg:46.57ms
step:1376/2155 train_time:64098ms step_avg:46.58ms
step:1377/2155 train_time:64159ms step_avg:46.59ms
step:1378/2155 train_time:64220ms step_avg:46.60ms
step:1379/2155 train_time:64281ms step_avg:46.61ms
step:1380/2155 train_time:64341ms step_avg:46.62ms
step:1381/2155 train_time:64403ms step_avg:46.64ms
step:1382/2155 train_time:64463ms step_avg:46.64ms
step:1383/2155 train_time:64524ms step_avg:46.66ms
step:1384/2155 train_time:64584ms step_avg:46.66ms
step:1385/2155 train_time:64644ms step_avg:46.67ms
step:1386/2155 train_time:64704ms step_avg:46.68ms
step:1387/2155 train_time:64765ms step_avg:46.69ms
step:1388/2155 train_time:64825ms step_avg:46.70ms
step:1389/2155 train_time:64886ms step_avg:46.71ms
step:1390/2155 train_time:64946ms step_avg:46.72ms
step:1391/2155 train_time:65007ms step_avg:46.73ms
step:1392/2155 train_time:65066ms step_avg:46.74ms
step:1393/2155 train_time:65128ms step_avg:46.75ms
step:1394/2155 train_time:65188ms step_avg:46.76ms
step:1395/2155 train_time:65249ms step_avg:46.77ms
step:1396/2155 train_time:65308ms step_avg:46.78ms
step:1397/2155 train_time:65369ms step_avg:46.79ms
step:1398/2155 train_time:65429ms step_avg:46.80ms
step:1399/2155 train_time:65490ms step_avg:46.81ms
step:1400/2155 train_time:65550ms step_avg:46.82ms
step:1401/2155 train_time:65612ms step_avg:46.83ms
step:1402/2155 train_time:65672ms step_avg:46.84ms
step:1403/2155 train_time:65733ms step_avg:46.85ms
step:1404/2155 train_time:65793ms step_avg:46.86ms
step:1405/2155 train_time:65855ms step_avg:46.87ms
step:1406/2155 train_time:65915ms step_avg:46.88ms
step:1407/2155 train_time:65976ms step_avg:46.89ms
step:1408/2155 train_time:66036ms step_avg:46.90ms
step:1409/2155 train_time:66097ms step_avg:46.91ms
step:1410/2155 train_time:66157ms step_avg:46.92ms
step:1411/2155 train_time:66219ms step_avg:46.93ms
step:1412/2155 train_time:66307ms step_avg:46.96ms
step:1413/2155 train_time:66397ms step_avg:46.99ms
step:1414/2155 train_time:66485ms step_avg:47.02ms
step:1415/2155 train_time:66574ms step_avg:47.05ms
step:1416/2155 train_time:66663ms step_avg:47.08ms
step:1417/2155 train_time:66753ms step_avg:47.11ms
step:1418/2155 train_time:66842ms step_avg:47.14ms
step:1419/2155 train_time:66931ms step_avg:47.17ms
step:1420/2155 train_time:67019ms step_avg:47.20ms
step:1421/2155 train_time:67108ms step_avg:47.23ms
step:1422/2155 train_time:67196ms step_avg:47.25ms
step:1423/2155 train_time:67285ms step_avg:47.28ms
step:1424/2155 train_time:67374ms step_avg:47.31ms
step:1425/2155 train_time:67464ms step_avg:47.34ms
step:1426/2155 train_time:67552ms step_avg:47.37ms
step:1427/2155 train_time:67641ms step_avg:47.40ms
step:1428/2155 train_time:67729ms step_avg:47.43ms
step:1429/2155 train_time:67818ms step_avg:47.46ms
step:1430/2155 train_time:67907ms step_avg:47.49ms
step:1431/2155 train_time:67996ms step_avg:47.52ms
step:1432/2155 train_time:68084ms step_avg:47.54ms
step:1433/2155 train_time:68173ms step_avg:47.57ms
step:1434/2155 train_time:68260ms step_avg:47.60ms
step:1435/2155 train_time:68351ms step_avg:47.63ms
step:1436/2155 train_time:68440ms step_avg:47.66ms
step:1437/2155 train_time:68530ms step_avg:47.69ms
step:1438/2155 train_time:68619ms step_avg:47.72ms
step:1439/2155 train_time:68708ms step_avg:47.75ms
step:1440/2155 train_time:68795ms step_avg:47.77ms
step:1441/2155 train_time:68886ms step_avg:47.80ms
step:1442/2155 train_time:68973ms step_avg:47.83ms
step:1443/2155 train_time:69062ms step_avg:47.86ms
step:1444/2155 train_time:69152ms step_avg:47.89ms
step:1445/2155 train_time:69241ms step_avg:47.92ms
step:1446/2155 train_time:69328ms step_avg:47.94ms
step:1447/2155 train_time:69418ms step_avg:47.97ms
step:1448/2155 train_time:69506ms step_avg:48.00ms
step:1449/2155 train_time:69596ms step_avg:48.03ms
step:1450/2155 train_time:69684ms step_avg:48.06ms
step:1451/2155 train_time:69772ms step_avg:48.09ms
step:1452/2155 train_time:69860ms step_avg:48.11ms
step:1453/2155 train_time:69950ms step_avg:48.14ms
step:1454/2155 train_time:70038ms step_avg:48.17ms
step:1455/2155 train_time:70128ms step_avg:48.20ms
step:1456/2155 train_time:70216ms step_avg:48.23ms
step:1457/2155 train_time:70304ms step_avg:48.25ms
step:1458/2155 train_time:70393ms step_avg:48.28ms
step:1459/2155 train_time:70483ms step_avg:48.31ms
step:1460/2155 train_time:70572ms step_avg:48.34ms
step:1461/2155 train_time:70661ms step_avg:48.36ms
step:1462/2155 train_time:70749ms step_avg:48.39ms
step:1463/2155 train_time:70839ms step_avg:48.42ms
step:1464/2155 train_time:70926ms step_avg:48.45ms
step:1465/2155 train_time:71016ms step_avg:48.47ms
step:1466/2155 train_time:71104ms step_avg:48.50ms
step:1467/2155 train_time:71193ms step_avg:48.53ms
step:1468/2155 train_time:71281ms step_avg:48.56ms
step:1469/2155 train_time:71371ms step_avg:48.58ms
step:1470/2155 train_time:71459ms step_avg:48.61ms
step:1471/2155 train_time:71549ms step_avg:48.64ms
step:1472/2155 train_time:71636ms step_avg:48.67ms
step:1473/2155 train_time:71726ms step_avg:48.69ms
step:1474/2155 train_time:71814ms step_avg:48.72ms
step:1475/2155 train_time:71903ms step_avg:48.75ms
step:1476/2155 train_time:71991ms step_avg:48.77ms
step:1477/2155 train_time:72080ms step_avg:48.80ms
step:1478/2155 train_time:72168ms step_avg:48.83ms
step:1479/2155 train_time:72257ms step_avg:48.86ms
step:1480/2155 train_time:72345ms step_avg:48.88ms
step:1481/2155 train_time:72435ms step_avg:48.91ms
step:1482/2155 train_time:72523ms step_avg:48.94ms
step:1483/2155 train_time:72613ms step_avg:48.96ms
step:1484/2155 train_time:72701ms step_avg:48.99ms
step:1485/2155 train_time:72791ms step_avg:49.02ms
step:1486/2155 train_time:72879ms step_avg:49.04ms
step:1487/2155 train_time:72968ms step_avg:49.07ms
step:1488/2155 train_time:73056ms step_avg:49.10ms
step:1489/2155 train_time:73146ms step_avg:49.12ms
step:1490/2155 train_time:73234ms step_avg:49.15ms
step:1491/2155 train_time:73323ms step_avg:49.18ms
step:1492/2155 train_time:73411ms step_avg:49.20ms
step:1493/2155 train_time:73499ms step_avg:49.23ms
step:1494/2155 train_time:73587ms step_avg:49.25ms
step:1495/2155 train_time:73676ms step_avg:49.28ms
step:1496/2155 train_time:73764ms step_avg:49.31ms
step:1497/2155 train_time:73853ms step_avg:49.33ms
step:1498/2155 train_time:73940ms step_avg:49.36ms
step:1499/2155 train_time:74030ms step_avg:49.39ms
step:1500/2155 train_time:74118ms step_avg:49.41ms
step:1500/2155 val_loss:3.4900 train_time:74209ms step_avg:49.47ms
step:1501/2155 train_time:74233ms step_avg:49.46ms
step:1502/2155 train_time:74300ms step_avg:49.47ms
step:1503/2155 train_time:74395ms step_avg:49.50ms
step:1504/2155 train_time:74483ms step_avg:49.52ms
step:1505/2155 train_time:74572ms step_avg:49.55ms
step:1506/2155 train_time:74658ms step_avg:49.57ms
step:1507/2155 train_time:74746ms step_avg:49.60ms
step:1508/2155 train_time:74832ms step_avg:49.62ms
step:1509/2155 train_time:74919ms step_avg:49.65ms
step:1510/2155 train_time:75005ms step_avg:49.67ms
step:1511/2155 train_time:75096ms step_avg:49.70ms
step:1512/2155 train_time:75191ms step_avg:49.73ms
step:1513/2155 train_time:75283ms step_avg:49.76ms
step:1514/2155 train_time:75374ms step_avg:49.78ms
step:1515/2155 train_time:75464ms step_avg:49.81ms
step:1516/2155 train_time:75551ms step_avg:49.84ms
step:1517/2155 train_time:75640ms step_avg:49.86ms
step:1518/2155 train_time:75727ms step_avg:49.89ms
step:1519/2155 train_time:75815ms step_avg:49.91ms
step:1520/2155 train_time:75902ms step_avg:49.94ms
step:1521/2155 train_time:75990ms step_avg:49.96ms
step:1522/2155 train_time:76077ms step_avg:49.98ms
step:1523/2155 train_time:76168ms step_avg:50.01ms
step:1524/2155 train_time:76257ms step_avg:50.04ms
step:1525/2155 train_time:76348ms step_avg:50.06ms
step:1526/2155 train_time:76437ms step_avg:50.09ms
step:1527/2155 train_time:76526ms step_avg:50.12ms
step:1528/2155 train_time:76614ms step_avg:50.14ms
step:1529/2155 train_time:76702ms step_avg:50.16ms
step:1530/2155 train_time:76788ms step_avg:50.19ms
step:1531/2155 train_time:76877ms step_avg:50.21ms
step:1532/2155 train_time:76963ms step_avg:50.24ms
step:1533/2155 train_time:77053ms step_avg:50.26ms
step:1534/2155 train_time:77141ms step_avg:50.29ms
step:1535/2155 train_time:77231ms step_avg:50.31ms
step:1536/2155 train_time:77320ms step_avg:50.34ms
step:1537/2155 train_time:77410ms step_avg:50.36ms
step:1538/2155 train_time:77498ms step_avg:50.39ms
step:1539/2155 train_time:77587ms step_avg:50.41ms
step:1540/2155 train_time:77675ms step_avg:50.44ms
step:1541/2155 train_time:77763ms step_avg:50.46ms
step:1542/2155 train_time:77850ms step_avg:50.49ms
step:1543/2155 train_time:77938ms step_avg:50.51ms
step:1544/2155 train_time:78026ms step_avg:50.54ms
step:1545/2155 train_time:78116ms step_avg:50.56ms
step:1546/2155 train_time:78204ms step_avg:50.58ms
step:1547/2155 train_time:78295ms step_avg:50.61ms
step:1548/2155 train_time:78383ms step_avg:50.63ms
step:1549/2155 train_time:78472ms step_avg:50.66ms
step:1550/2155 train_time:78559ms step_avg:50.68ms
step:1551/2155 train_time:78648ms step_avg:50.71ms
step:1552/2155 train_time:78735ms step_avg:50.73ms
step:1553/2155 train_time:78823ms step_avg:50.76ms
step:1554/2155 train_time:78911ms step_avg:50.78ms
step:1555/2155 train_time:79001ms step_avg:50.80ms
step:1556/2155 train_time:79088ms step_avg:50.83ms
step:1557/2155 train_time:79177ms step_avg:50.85ms
step:1558/2155 train_time:79264ms step_avg:50.88ms
step:1559/2155 train_time:79355ms step_avg:50.90ms
step:1560/2155 train_time:79443ms step_avg:50.92ms
step:1561/2155 train_time:79532ms step_avg:50.95ms
step:1562/2155 train_time:79620ms step_avg:50.97ms
step:1563/2155 train_time:79709ms step_avg:51.00ms
step:1564/2155 train_time:79796ms step_avg:51.02ms
step:1565/2155 train_time:79885ms step_avg:51.04ms
step:1566/2155 train_time:79972ms step_avg:51.07ms
step:1567/2155 train_time:80061ms step_avg:51.09ms
step:1568/2155 train_time:80148ms step_avg:51.12ms
step:1569/2155 train_time:80237ms step_avg:51.14ms
step:1570/2155 train_time:80326ms step_avg:51.16ms
step:1571/2155 train_time:80416ms step_avg:51.19ms
step:1572/2155 train_time:80503ms step_avg:51.21ms
step:1573/2155 train_time:80592ms step_avg:51.23ms
step:1574/2155 train_time:80680ms step_avg:51.26ms
step:1575/2155 train_time:80769ms step_avg:51.28ms
step:1576/2155 train_time:80857ms step_avg:51.31ms
step:1577/2155 train_time:80946ms step_avg:51.33ms
step:1578/2155 train_time:81033ms step_avg:51.35ms
step:1579/2155 train_time:81122ms step_avg:51.38ms
step:1580/2155 train_time:81210ms step_avg:51.40ms
step:1581/2155 train_time:81300ms step_avg:51.42ms
step:1582/2155 train_time:81388ms step_avg:51.45ms
step:1583/2155 train_time:81477ms step_avg:51.47ms
step:1584/2155 train_time:81564ms step_avg:51.49ms
step:1585/2155 train_time:81653ms step_avg:51.52ms
step:1586/2155 train_time:81741ms step_avg:51.54ms
step:1587/2155 train_time:81829ms step_avg:51.56ms
step:1588/2155 train_time:81917ms step_avg:51.58ms
step:1589/2155 train_time:82006ms step_avg:51.61ms
step:1590/2155 train_time:82094ms step_avg:51.63ms
step:1591/2155 train_time:82183ms step_avg:51.65ms
step:1592/2155 train_time:82272ms step_avg:51.68ms
step:1593/2155 train_time:82361ms step_avg:51.70ms
step:1594/2155 train_time:82449ms step_avg:51.72ms
step:1595/2155 train_time:82539ms step_avg:51.75ms
step:1596/2155 train_time:82626ms step_avg:51.77ms
step:1597/2155 train_time:82716ms step_avg:51.79ms
step:1598/2155 train_time:82803ms step_avg:51.82ms
step:1599/2155 train_time:82891ms step_avg:51.84ms
step:1600/2155 train_time:82979ms step_avg:51.86ms
step:1601/2155 train_time:83067ms step_avg:51.88ms
step:1602/2155 train_time:83155ms step_avg:51.91ms
step:1603/2155 train_time:83245ms step_avg:51.93ms
step:1604/2155 train_time:83333ms step_avg:51.95ms
step:1605/2155 train_time:83422ms step_avg:51.98ms
step:1606/2155 train_time:83509ms step_avg:52.00ms
step:1607/2155 train_time:83599ms step_avg:52.02ms
step:1608/2155 train_time:83687ms step_avg:52.04ms
step:1609/2155 train_time:83777ms step_avg:52.07ms
step:1610/2155 train_time:83864ms step_avg:52.09ms
step:1611/2155 train_time:83954ms step_avg:52.11ms
step:1612/2155 train_time:84042ms step_avg:52.13ms
step:1613/2155 train_time:84131ms step_avg:52.16ms
step:1614/2155 train_time:84219ms step_avg:52.18ms
step:1615/2155 train_time:84309ms step_avg:52.20ms
step:1616/2155 train_time:84396ms step_avg:52.23ms
step:1617/2155 train_time:84486ms step_avg:52.25ms
step:1618/2155 train_time:84574ms step_avg:52.27ms
step:1619/2155 train_time:84662ms step_avg:52.29ms
step:1620/2155 train_time:84750ms step_avg:52.31ms
step:1621/2155 train_time:84839ms step_avg:52.34ms
step:1622/2155 train_time:84926ms step_avg:52.36ms
step:1623/2155 train_time:85016ms step_avg:52.38ms
step:1624/2155 train_time:85104ms step_avg:52.40ms
step:1625/2155 train_time:85194ms step_avg:52.43ms
step:1626/2155 train_time:85281ms step_avg:52.45ms
step:1627/2155 train_time:85371ms step_avg:52.47ms
step:1628/2155 train_time:85458ms step_avg:52.49ms
step:1629/2155 train_time:85548ms step_avg:52.52ms
step:1630/2155 train_time:85635ms step_avg:52.54ms
step:1631/2155 train_time:85724ms step_avg:52.56ms
step:1632/2155 train_time:85812ms step_avg:52.58ms
step:1633/2155 train_time:85900ms step_avg:52.60ms
step:1634/2155 train_time:85988ms step_avg:52.62ms
step:1635/2155 train_time:86078ms step_avg:52.65ms
step:1636/2155 train_time:86165ms step_avg:52.67ms
step:1637/2155 train_time:86254ms step_avg:52.69ms
step:1638/2155 train_time:86341ms step_avg:52.71ms
step:1639/2155 train_time:86431ms step_avg:52.73ms
step:1640/2155 train_time:86519ms step_avg:52.76ms
step:1641/2155 train_time:86609ms step_avg:52.78ms
step:1642/2155 train_time:86696ms step_avg:52.80ms
step:1643/2155 train_time:86785ms step_avg:52.82ms
step:1644/2155 train_time:86873ms step_avg:52.84ms
step:1645/2155 train_time:86961ms step_avg:52.86ms
step:1646/2155 train_time:87049ms step_avg:52.89ms
step:1647/2155 train_time:87138ms step_avg:52.91ms
step:1648/2155 train_time:87226ms step_avg:52.93ms
step:1649/2155 train_time:87316ms step_avg:52.95ms
step:1650/2155 train_time:87404ms step_avg:52.97ms
step:1651/2155 train_time:87493ms step_avg:52.99ms
step:1652/2155 train_time:87581ms step_avg:53.02ms
step:1653/2155 train_time:87669ms step_avg:53.04ms
step:1654/2155 train_time:87757ms step_avg:53.06ms
step:1655/2155 train_time:87846ms step_avg:53.08ms
step:1656/2155 train_time:87934ms step_avg:53.10ms
step:1657/2155 train_time:88023ms step_avg:53.12ms
step:1658/2155 train_time:88111ms step_avg:53.14ms
step:1659/2155 train_time:88200ms step_avg:53.16ms
step:1660/2155 train_time:88288ms step_avg:53.19ms
step:1661/2155 train_time:88377ms step_avg:53.21ms
step:1662/2155 train_time:88464ms step_avg:53.23ms
step:1663/2155 train_time:88554ms step_avg:53.25ms
step:1664/2155 train_time:88640ms step_avg:53.27ms
step:1665/2155 train_time:88730ms step_avg:53.29ms
step:1666/2155 train_time:88817ms step_avg:53.31ms
step:1667/2155 train_time:88906ms step_avg:53.33ms
step:1668/2155 train_time:88994ms step_avg:53.35ms
step:1669/2155 train_time:89084ms step_avg:53.38ms
step:1670/2155 train_time:89172ms step_avg:53.40ms
step:1671/2155 train_time:89260ms step_avg:53.42ms
step:1672/2155 train_time:89348ms step_avg:53.44ms
step:1673/2155 train_time:89437ms step_avg:53.46ms
step:1674/2155 train_time:89524ms step_avg:53.48ms
step:1675/2155 train_time:89614ms step_avg:53.50ms
step:1676/2155 train_time:89701ms step_avg:53.52ms
step:1677/2155 train_time:89790ms step_avg:53.54ms
step:1678/2155 train_time:89879ms step_avg:53.56ms
step:1679/2155 train_time:89967ms step_avg:53.58ms
step:1680/2155 train_time:90056ms step_avg:53.60ms
step:1681/2155 train_time:90144ms step_avg:53.63ms
step:1682/2155 train_time:90232ms step_avg:53.65ms
step:1683/2155 train_time:90322ms step_avg:53.67ms
step:1684/2155 train_time:90411ms step_avg:53.69ms
step:1685/2155 train_time:90499ms step_avg:53.71ms
step:1686/2155 train_time:90586ms step_avg:53.73ms
step:1687/2155 train_time:90675ms step_avg:53.75ms
step:1688/2155 train_time:90762ms step_avg:53.77ms
step:1689/2155 train_time:90851ms step_avg:53.79ms
step:1690/2155 train_time:90938ms step_avg:53.81ms
step:1691/2155 train_time:91028ms step_avg:53.83ms
step:1692/2155 train_time:91116ms step_avg:53.85ms
step:1693/2155 train_time:91205ms step_avg:53.87ms
step:1694/2155 train_time:91293ms step_avg:53.89ms
step:1695/2155 train_time:91383ms step_avg:53.91ms
step:1696/2155 train_time:91472ms step_avg:53.93ms
step:1697/2155 train_time:91561ms step_avg:53.95ms
step:1698/2155 train_time:91648ms step_avg:53.97ms
step:1699/2155 train_time:91738ms step_avg:54.00ms
step:1700/2155 train_time:91824ms step_avg:54.01ms
step:1701/2155 train_time:91914ms step_avg:54.04ms
step:1702/2155 train_time:92002ms step_avg:54.06ms
step:1703/2155 train_time:92091ms step_avg:54.08ms
step:1704/2155 train_time:92178ms step_avg:54.10ms
step:1705/2155 train_time:92267ms step_avg:54.12ms
step:1706/2155 train_time:92355ms step_avg:54.14ms
step:1707/2155 train_time:92445ms step_avg:54.16ms
step:1708/2155 train_time:92533ms step_avg:54.18ms
step:1709/2155 train_time:92622ms step_avg:54.20ms
step:1710/2155 train_time:92709ms step_avg:54.22ms
step:1711/2155 train_time:92798ms step_avg:54.24ms
step:1712/2155 train_time:92886ms step_avg:54.26ms
step:1713/2155 train_time:92975ms step_avg:54.28ms
step:1714/2155 train_time:93062ms step_avg:54.30ms
step:1715/2155 train_time:93152ms step_avg:54.32ms
step:1716/2155 train_time:93239ms step_avg:54.34ms
step:1717/2155 train_time:93328ms step_avg:54.36ms
step:1718/2155 train_time:93415ms step_avg:54.37ms
step:1719/2155 train_time:93504ms step_avg:54.39ms
step:1720/2155 train_time:93592ms step_avg:54.41ms
step:1721/2155 train_time:93681ms step_avg:54.43ms
step:1722/2155 train_time:93768ms step_avg:54.45ms
step:1723/2155 train_time:93857ms step_avg:54.47ms
step:1724/2155 train_time:93944ms step_avg:54.49ms
step:1725/2155 train_time:94034ms step_avg:54.51ms
step:1726/2155 train_time:94120ms step_avg:54.53ms
step:1727/2155 train_time:94209ms step_avg:54.55ms
step:1728/2155 train_time:94296ms step_avg:54.57ms
step:1729/2155 train_time:94385ms step_avg:54.59ms
step:1730/2155 train_time:94473ms step_avg:54.61ms
step:1731/2155 train_time:94563ms step_avg:54.63ms
step:1732/2155 train_time:94651ms step_avg:54.65ms
step:1733/2155 train_time:94740ms step_avg:54.67ms
step:1734/2155 train_time:94828ms step_avg:54.69ms
step:1735/2155 train_time:94918ms step_avg:54.71ms
step:1736/2155 train_time:95006ms step_avg:54.73ms
step:1737/2155 train_time:95095ms step_avg:54.75ms
step:1738/2155 train_time:95182ms step_avg:54.77ms
step:1739/2155 train_time:95271ms step_avg:54.79ms
step:1740/2155 train_time:95359ms step_avg:54.80ms
step:1741/2155 train_time:95448ms step_avg:54.82ms
step:1742/2155 train_time:95536ms step_avg:54.84ms
step:1743/2155 train_time:95625ms step_avg:54.86ms
step:1744/2155 train_time:95713ms step_avg:54.88ms
step:1745/2155 train_time:95802ms step_avg:54.90ms
step:1746/2155 train_time:95890ms step_avg:54.92ms
step:1747/2155 train_time:95981ms step_avg:54.94ms
step:1748/2155 train_time:96069ms step_avg:54.96ms
step:1749/2155 train_time:96158ms step_avg:54.98ms
step:1750/2155 train_time:96246ms step_avg:55.00ms
step:1750/2155 val_loss:3.3893 train_time:96337ms step_avg:55.05ms
step:1751/2155 train_time:96360ms step_avg:55.03ms
step:1752/2155 train_time:96427ms step_avg:55.04ms
step:1753/2155 train_time:96522ms step_avg:55.06ms
step:1754/2155 train_time:96611ms step_avg:55.08ms
step:1755/2155 train_time:96700ms step_avg:55.10ms
step:1756/2155 train_time:96786ms step_avg:55.12ms
step:1757/2155 train_time:96873ms step_avg:55.14ms
step:1758/2155 train_time:96960ms step_avg:55.15ms
step:1759/2155 train_time:97049ms step_avg:55.17ms
step:1760/2155 train_time:97136ms step_avg:55.19ms
step:1761/2155 train_time:97225ms step_avg:55.21ms
step:1762/2155 train_time:97315ms step_avg:55.23ms
step:1763/2155 train_time:97407ms step_avg:55.25ms
step:1764/2155 train_time:97497ms step_avg:55.27ms
step:1765/2155 train_time:97587ms step_avg:55.29ms
step:1766/2155 train_time:97675ms step_avg:55.31ms
step:1767/2155 train_time:97764ms step_avg:55.33ms
step:1768/2155 train_time:97851ms step_avg:55.35ms
step:1769/2155 train_time:97940ms step_avg:55.36ms
step:1770/2155 train_time:98027ms step_avg:55.38ms
step:1771/2155 train_time:98115ms step_avg:55.40ms
step:1772/2155 train_time:98202ms step_avg:55.42ms
step:1773/2155 train_time:98292ms step_avg:55.44ms
step:1774/2155 train_time:98382ms step_avg:55.46ms
step:1775/2155 train_time:98472ms step_avg:55.48ms
step:1776/2155 train_time:98562ms step_avg:55.50ms
step:1777/2155 train_time:98650ms step_avg:55.52ms
step:1778/2155 train_time:98737ms step_avg:55.53ms
step:1779/2155 train_time:98826ms step_avg:55.55ms
step:1780/2155 train_time:98914ms step_avg:55.57ms
step:1781/2155 train_time:99002ms step_avg:55.59ms
step:1782/2155 train_time:99089ms step_avg:55.61ms
step:1783/2155 train_time:99177ms step_avg:55.62ms
step:1784/2155 train_time:99264ms step_avg:55.64ms
step:1785/2155 train_time:99353ms step_avg:55.66ms
step:1786/2155 train_time:99442ms step_avg:55.68ms
step:1787/2155 train_time:99532ms step_avg:55.70ms
step:1788/2155 train_time:99620ms step_avg:55.72ms
step:1789/2155 train_time:99709ms step_avg:55.73ms
step:1790/2155 train_time:99796ms step_avg:55.75ms
step:1791/2155 train_time:99885ms step_avg:55.77ms
step:1792/2155 train_time:99972ms step_avg:55.79ms
step:1793/2155 train_time:100061ms step_avg:55.81ms
step:1794/2155 train_time:100148ms step_avg:55.82ms
step:1795/2155 train_time:100237ms step_avg:55.84ms
step:1796/2155 train_time:100326ms step_avg:55.86ms
step:1797/2155 train_time:100416ms step_avg:55.88ms
step:1798/2155 train_time:100504ms step_avg:55.90ms
step:1799/2155 train_time:100594ms step_avg:55.92ms
step:1800/2155 train_time:100682ms step_avg:55.93ms
step:1801/2155 train_time:100771ms step_avg:55.95ms
step:1802/2155 train_time:100858ms step_avg:55.97ms
step:1803/2155 train_time:100946ms step_avg:55.99ms
step:1804/2155 train_time:101033ms step_avg:56.00ms
step:1805/2155 train_time:101122ms step_avg:56.02ms
step:1806/2155 train_time:101209ms step_avg:56.04ms
step:1807/2155 train_time:101298ms step_avg:56.06ms
step:1808/2155 train_time:101387ms step_avg:56.08ms
step:1809/2155 train_time:101476ms step_avg:56.10ms
step:1810/2155 train_time:101565ms step_avg:56.11ms
step:1811/2155 train_time:101654ms step_avg:56.13ms
step:1812/2155 train_time:101741ms step_avg:56.15ms
step:1813/2155 train_time:101830ms step_avg:56.17ms
step:1814/2155 train_time:101918ms step_avg:56.18ms
step:1815/2155 train_time:102007ms step_avg:56.20ms
step:1816/2155 train_time:102094ms step_avg:56.22ms
step:1817/2155 train_time:102184ms step_avg:56.24ms
step:1818/2155 train_time:102271ms step_avg:56.25ms
step:1819/2155 train_time:102359ms step_avg:56.27ms
step:1820/2155 train_time:102448ms step_avg:56.29ms
step:1821/2155 train_time:102538ms step_avg:56.31ms
step:1822/2155 train_time:102626ms step_avg:56.33ms
step:1823/2155 train_time:102715ms step_avg:56.34ms
step:1824/2155 train_time:102802ms step_avg:56.36ms
step:1825/2155 train_time:102891ms step_avg:56.38ms
step:1826/2155 train_time:102979ms step_avg:56.40ms
step:1827/2155 train_time:103067ms step_avg:56.41ms
step:1828/2155 train_time:103155ms step_avg:56.43ms
step:1829/2155 train_time:103244ms step_avg:56.45ms
step:1830/2155 train_time:103332ms step_avg:56.47ms
step:1831/2155 train_time:103421ms step_avg:56.48ms
step:1832/2155 train_time:103510ms step_avg:56.50ms
step:1833/2155 train_time:103599ms step_avg:56.52ms
step:1834/2155 train_time:103686ms step_avg:56.54ms
step:1835/2155 train_time:103775ms step_avg:56.55ms
step:1836/2155 train_time:103862ms step_avg:56.57ms
step:1837/2155 train_time:103952ms step_avg:56.59ms
step:1838/2155 train_time:104040ms step_avg:56.60ms
step:1839/2155 train_time:104128ms step_avg:56.62ms
step:1840/2155 train_time:104216ms step_avg:56.64ms
step:1841/2155 train_time:104305ms step_avg:56.66ms
step:1842/2155 train_time:104393ms step_avg:56.67ms
step:1843/2155 train_time:104483ms step_avg:56.69ms
step:1844/2155 train_time:104570ms step_avg:56.71ms
step:1845/2155 train_time:104660ms step_avg:56.73ms
step:1846/2155 train_time:104747ms step_avg:56.74ms
step:1847/2155 train_time:104836ms step_avg:56.76ms
step:1848/2155 train_time:104924ms step_avg:56.78ms
step:1849/2155 train_time:105012ms step_avg:56.79ms
step:1850/2155 train_time:105100ms step_avg:56.81ms
step:1851/2155 train_time:105189ms step_avg:56.83ms
step:1852/2155 train_time:105276ms step_avg:56.84ms
step:1853/2155 train_time:105366ms step_avg:56.86ms
step:1854/2155 train_time:105453ms step_avg:56.88ms
step:1855/2155 train_time:105544ms step_avg:56.90ms
step:1856/2155 train_time:105631ms step_avg:56.91ms
step:1857/2155 train_time:105720ms step_avg:56.93ms
step:1858/2155 train_time:105808ms step_avg:56.95ms
step:1859/2155 train_time:105897ms step_avg:56.96ms
step:1860/2155 train_time:105984ms step_avg:56.98ms
step:1861/2155 train_time:106072ms step_avg:57.00ms
step:1862/2155 train_time:106159ms step_avg:57.01ms
step:1863/2155 train_time:106248ms step_avg:57.03ms
step:1864/2155 train_time:106335ms step_avg:57.05ms
step:1865/2155 train_time:106425ms step_avg:57.06ms
step:1866/2155 train_time:106513ms step_avg:57.08ms
step:1867/2155 train_time:106602ms step_avg:57.10ms
step:1868/2155 train_time:106689ms step_avg:57.11ms
step:1869/2155 train_time:106778ms step_avg:57.13ms
step:1870/2155 train_time:106866ms step_avg:57.15ms
step:1871/2155 train_time:106956ms step_avg:57.16ms
step:1872/2155 train_time:107044ms step_avg:57.18ms
step:1873/2155 train_time:107132ms step_avg:57.20ms
step:1874/2155 train_time:107220ms step_avg:57.21ms
step:1875/2155 train_time:107310ms step_avg:57.23ms
step:1876/2155 train_time:107398ms step_avg:57.25ms
step:1877/2155 train_time:107487ms step_avg:57.27ms
step:1878/2155 train_time:107574ms step_avg:57.28ms
step:1879/2155 train_time:107664ms step_avg:57.30ms
step:1880/2155 train_time:107752ms step_avg:57.31ms
step:1881/2155 train_time:107842ms step_avg:57.33ms
step:1882/2155 train_time:107929ms step_avg:57.35ms
step:1883/2155 train_time:108019ms step_avg:57.37ms
step:1884/2155 train_time:108106ms step_avg:57.38ms
step:1885/2155 train_time:108194ms step_avg:57.40ms
step:1886/2155 train_time:108281ms step_avg:57.41ms
step:1887/2155 train_time:108370ms step_avg:57.43ms
step:1888/2155 train_time:108458ms step_avg:57.45ms
step:1889/2155 train_time:108547ms step_avg:57.46ms
step:1890/2155 train_time:108636ms step_avg:57.48ms
step:1891/2155 train_time:108726ms step_avg:57.50ms
step:1892/2155 train_time:108814ms step_avg:57.51ms
step:1893/2155 train_time:108904ms step_avg:57.53ms
step:1894/2155 train_time:108991ms step_avg:57.55ms
step:1895/2155 train_time:109080ms step_avg:57.56ms
step:1896/2155 train_time:109167ms step_avg:57.58ms
step:1897/2155 train_time:109256ms step_avg:57.59ms
step:1898/2155 train_time:109343ms step_avg:57.61ms
step:1899/2155 train_time:109432ms step_avg:57.63ms
step:1900/2155 train_time:109520ms step_avg:57.64ms
step:1901/2155 train_time:109609ms step_avg:57.66ms
step:1902/2155 train_time:109696ms step_avg:57.67ms
step:1903/2155 train_time:109787ms step_avg:57.69ms
step:1904/2155 train_time:109875ms step_avg:57.71ms
step:1905/2155 train_time:109964ms step_avg:57.72ms
step:1906/2155 train_time:110051ms step_avg:57.74ms
step:1907/2155 train_time:110140ms step_avg:57.76ms
step:1908/2155 train_time:110227ms step_avg:57.77ms
step:1909/2155 train_time:110317ms step_avg:57.79ms
step:1910/2155 train_time:110404ms step_avg:57.80ms
step:1911/2155 train_time:110493ms step_avg:57.82ms
step:1912/2155 train_time:110581ms step_avg:57.84ms
step:1913/2155 train_time:110669ms step_avg:57.85ms
step:1914/2155 train_time:110757ms step_avg:57.87ms
step:1915/2155 train_time:110846ms step_avg:57.88ms
step:1916/2155 train_time:110934ms step_avg:57.90ms
step:1917/2155 train_time:111023ms step_avg:57.91ms
step:1918/2155 train_time:111111ms step_avg:57.93ms
step:1919/2155 train_time:111199ms step_avg:57.95ms
step:1920/2155 train_time:111286ms step_avg:57.96ms
step:1921/2155 train_time:111375ms step_avg:57.98ms
step:1922/2155 train_time:111463ms step_avg:57.99ms
step:1923/2155 train_time:111552ms step_avg:58.01ms
step:1924/2155 train_time:111641ms step_avg:58.03ms
step:1925/2155 train_time:111730ms step_avg:58.04ms
step:1926/2155 train_time:111817ms step_avg:58.06ms
step:1927/2155 train_time:111907ms step_avg:58.07ms
step:1928/2155 train_time:111994ms step_avg:58.09ms
step:1929/2155 train_time:112084ms step_avg:58.10ms
step:1930/2155 train_time:112170ms step_avg:58.12ms
step:1931/2155 train_time:112258ms step_avg:58.13ms
step:1932/2155 train_time:112346ms step_avg:58.15ms
step:1933/2155 train_time:112434ms step_avg:58.17ms
step:1934/2155 train_time:112522ms step_avg:58.18ms
step:1935/2155 train_time:112612ms step_avg:58.20ms
step:1936/2155 train_time:112700ms step_avg:58.21ms
step:1937/2155 train_time:112789ms step_avg:58.23ms
step:1938/2155 train_time:112877ms step_avg:58.24ms
step:1939/2155 train_time:112966ms step_avg:58.26ms
step:1940/2155 train_time:113054ms step_avg:58.28ms
step:1941/2155 train_time:113143ms step_avg:58.29ms
step:1942/2155 train_time:113230ms step_avg:58.31ms
step:1943/2155 train_time:113319ms step_avg:58.32ms
step:1944/2155 train_time:113406ms step_avg:58.34ms
step:1945/2155 train_time:113495ms step_avg:58.35ms
step:1946/2155 train_time:113583ms step_avg:58.37ms
step:1947/2155 train_time:113672ms step_avg:58.38ms
step:1948/2155 train_time:113759ms step_avg:58.40ms
step:1949/2155 train_time:113849ms step_avg:58.41ms
step:1950/2155 train_time:113936ms step_avg:58.43ms
step:1951/2155 train_time:114025ms step_avg:58.44ms
step:1952/2155 train_time:114113ms step_avg:58.46ms
step:1953/2155 train_time:114202ms step_avg:58.47ms
step:1954/2155 train_time:114288ms step_avg:58.49ms
step:1955/2155 train_time:114377ms step_avg:58.51ms
step:1956/2155 train_time:114465ms step_avg:58.52ms
step:1957/2155 train_time:114554ms step_avg:58.54ms
step:1958/2155 train_time:114643ms step_avg:58.55ms
step:1959/2155 train_time:114731ms step_avg:58.57ms
step:1960/2155 train_time:114819ms step_avg:58.58ms
step:1961/2155 train_time:114908ms step_avg:58.60ms
step:1962/2155 train_time:114996ms step_avg:58.61ms
step:1963/2155 train_time:115085ms step_avg:58.63ms
step:1964/2155 train_time:115173ms step_avg:58.64ms
step:1965/2155 train_time:115262ms step_avg:58.66ms
step:1966/2155 train_time:115349ms step_avg:58.67ms
step:1967/2155 train_time:115438ms step_avg:58.69ms
step:1968/2155 train_time:115526ms step_avg:58.70ms
step:1969/2155 train_time:115614ms step_avg:58.72ms
step:1970/2155 train_time:115703ms step_avg:58.73ms
step:1971/2155 train_time:115792ms step_avg:58.75ms
step:1972/2155 train_time:115880ms step_avg:58.76ms
step:1973/2155 train_time:115969ms step_avg:58.78ms
step:1974/2155 train_time:116057ms step_avg:58.79ms
step:1975/2155 train_time:116146ms step_avg:58.81ms
step:1976/2155 train_time:116233ms step_avg:58.82ms
step:1977/2155 train_time:116322ms step_avg:58.84ms
step:1978/2155 train_time:116409ms step_avg:58.85ms
step:1979/2155 train_time:116498ms step_avg:58.87ms
step:1980/2155 train_time:116586ms step_avg:58.88ms
step:1981/2155 train_time:116674ms step_avg:58.90ms
step:1982/2155 train_time:116762ms step_avg:58.91ms
step:1983/2155 train_time:116852ms step_avg:58.93ms
step:1984/2155 train_time:116939ms step_avg:58.94ms
step:1985/2155 train_time:117028ms step_avg:58.96ms
step:1986/2155 train_time:117115ms step_avg:58.97ms
step:1987/2155 train_time:117205ms step_avg:58.99ms
step:1988/2155 train_time:117291ms step_avg:59.00ms
step:1989/2155 train_time:117381ms step_avg:59.01ms
step:1990/2155 train_time:117468ms step_avg:59.03ms
step:1991/2155 train_time:117557ms step_avg:59.04ms
step:1992/2155 train_time:117644ms step_avg:59.06ms
step:1993/2155 train_time:117733ms step_avg:59.07ms
step:1994/2155 train_time:117821ms step_avg:59.09ms
step:1995/2155 train_time:117910ms step_avg:59.10ms
step:1996/2155 train_time:117998ms step_avg:59.12ms
step:1997/2155 train_time:118087ms step_avg:59.13ms
step:1998/2155 train_time:118175ms step_avg:59.15ms
step:1999/2155 train_time:118265ms step_avg:59.16ms
step:2000/2155 train_time:118352ms step_avg:59.18ms
step:2000/2155 val_loss:3.3120 train_time:118444ms step_avg:59.22ms
step:2001/2155 train_time:118467ms step_avg:59.20ms
step:2002/2155 train_time:118533ms step_avg:59.21ms
step:2003/2155 train_time:118625ms step_avg:59.22ms
step:2004/2155 train_time:118713ms step_avg:59.24ms
step:2005/2155 train_time:118802ms step_avg:59.25ms
step:2006/2155 train_time:118889ms step_avg:59.27ms
step:2007/2155 train_time:118977ms step_avg:59.28ms
step:2008/2155 train_time:119063ms step_avg:59.29ms
step:2009/2155 train_time:119151ms step_avg:59.31ms
step:2010/2155 train_time:119238ms step_avg:59.32ms
step:2011/2155 train_time:119326ms step_avg:59.34ms
step:2012/2155 train_time:119415ms step_avg:59.35ms
step:2013/2155 train_time:119507ms step_avg:59.37ms
step:2014/2155 train_time:119595ms step_avg:59.38ms
step:2015/2155 train_time:119686ms step_avg:59.40ms
step:2016/2155 train_time:119773ms step_avg:59.41ms
step:2017/2155 train_time:119862ms step_avg:59.43ms
step:2018/2155 train_time:119949ms step_avg:59.44ms
step:2019/2155 train_time:120037ms step_avg:59.45ms
step:2020/2155 train_time:120124ms step_avg:59.47ms
step:2021/2155 train_time:120213ms step_avg:59.48ms
step:2022/2155 train_time:120301ms step_avg:59.50ms
step:2023/2155 train_time:120390ms step_avg:59.51ms
step:2024/2155 train_time:120479ms step_avg:59.53ms
step:2025/2155 train_time:120569ms step_avg:59.54ms
step:2026/2155 train_time:120657ms step_avg:59.55ms
step:2027/2155 train_time:120746ms step_avg:59.57ms
step:2028/2155 train_time:120835ms step_avg:59.58ms
step:2029/2155 train_time:120924ms step_avg:59.60ms
step:2030/2155 train_time:121010ms step_avg:59.61ms
step:2031/2155 train_time:121098ms step_avg:59.62ms
step:2032/2155 train_time:121185ms step_avg:59.64ms
step:2033/2155 train_time:121273ms step_avg:59.65ms
step:2034/2155 train_time:121360ms step_avg:59.67ms
step:2035/2155 train_time:121450ms step_avg:59.68ms
step:2036/2155 train_time:121538ms step_avg:59.69ms
step:2037/2155 train_time:121628ms step_avg:59.71ms
step:2038/2155 train_time:121715ms step_avg:59.72ms
step:2039/2155 train_time:121805ms step_avg:59.74ms
step:2040/2155 train_time:121892ms step_avg:59.75ms
step:2041/2155 train_time:121981ms step_avg:59.77ms
step:2042/2155 train_time:122068ms step_avg:59.78ms
step:2043/2155 train_time:122157ms step_avg:59.79ms
step:2044/2155 train_time:122244ms step_avg:59.81ms
step:2045/2155 train_time:122333ms step_avg:59.82ms
step:2046/2155 train_time:122420ms step_avg:59.83ms
step:2047/2155 train_time:122510ms step_avg:59.85ms
step:2048/2155 train_time:122597ms step_avg:59.86ms
step:2049/2155 train_time:122686ms step_avg:59.88ms
step:2050/2155 train_time:122775ms step_avg:59.89ms
step:2051/2155 train_time:122864ms step_avg:59.90ms
step:2052/2155 train_time:122952ms step_avg:59.92ms
step:2053/2155 train_time:123040ms step_avg:59.93ms
step:2054/2155 train_time:123127ms step_avg:59.94ms
step:2055/2155 train_time:123216ms step_avg:59.96ms
step:2056/2155 train_time:123303ms step_avg:59.97ms
step:2057/2155 train_time:123393ms step_avg:59.99ms
step:2058/2155 train_time:123480ms step_avg:60.00ms
step:2059/2155 train_time:123571ms step_avg:60.01ms
step:2060/2155 train_time:123658ms step_avg:60.03ms
step:2061/2155 train_time:123747ms step_avg:60.04ms
step:2062/2155 train_time:123835ms step_avg:60.06ms
step:2063/2155 train_time:123923ms step_avg:60.07ms
step:2064/2155 train_time:124011ms step_avg:60.08ms
step:2065/2155 train_time:124099ms step_avg:60.10ms
step:2066/2155 train_time:124186ms step_avg:60.11ms
step:2067/2155 train_time:124277ms step_avg:60.12ms
step:2068/2155 train_time:124366ms step_avg:60.14ms
step:2069/2155 train_time:124454ms step_avg:60.15ms
step:2070/2155 train_time:124541ms step_avg:60.16ms
step:2071/2155 train_time:124632ms step_avg:60.18ms
step:2072/2155 train_time:124720ms step_avg:60.19ms
step:2073/2155 train_time:124810ms step_avg:60.21ms
step:2074/2155 train_time:124897ms step_avg:60.22ms
step:2075/2155 train_time:124986ms step_avg:60.23ms
step:2076/2155 train_time:125074ms step_avg:60.25ms
step:2077/2155 train_time:125162ms step_avg:60.26ms
step:2078/2155 train_time:125251ms step_avg:60.27ms
step:2079/2155 train_time:125340ms step_avg:60.29ms
step:2080/2155 train_time:125428ms step_avg:60.30ms
step:2081/2155 train_time:125518ms step_avg:60.32ms
step:2082/2155 train_time:125606ms step_avg:60.33ms
step:2083/2155 train_time:125695ms step_avg:60.34ms
step:2084/2155 train_time:125783ms step_avg:60.36ms
step:2085/2155 train_time:125873ms step_avg:60.37ms
step:2086/2155 train_time:125961ms step_avg:60.38ms
step:2087/2155 train_time:126050ms step_avg:60.40ms
step:2088/2155 train_time:126137ms step_avg:60.41ms
step:2089/2155 train_time:126227ms step_avg:60.42ms
step:2090/2155 train_time:126315ms step_avg:60.44ms
step:2091/2155 train_time:126404ms step_avg:60.45ms
step:2092/2155 train_time:126493ms step_avg:60.46ms
step:2093/2155 train_time:126581ms step_avg:60.48ms
step:2094/2155 train_time:126669ms step_avg:60.49ms
step:2095/2155 train_time:126758ms step_avg:60.51ms
step:2096/2155 train_time:126846ms step_avg:60.52ms
step:2097/2155 train_time:126934ms step_avg:60.53ms
step:2098/2155 train_time:127021ms step_avg:60.54ms
step:2099/2155 train_time:127110ms step_avg:60.56ms
step:2100/2155 train_time:127197ms step_avg:60.57ms
step:2101/2155 train_time:127286ms step_avg:60.58ms
step:2102/2155 train_time:127374ms step_avg:60.60ms
step:2103/2155 train_time:127463ms step_avg:60.61ms
step:2104/2155 train_time:127550ms step_avg:60.62ms
step:2105/2155 train_time:127640ms step_avg:60.64ms
step:2106/2155 train_time:127727ms step_avg:60.65ms
step:2107/2155 train_time:127817ms step_avg:60.66ms
step:2108/2155 train_time:127904ms step_avg:60.68ms
step:2109/2155 train_time:127993ms step_avg:60.69ms
step:2110/2155 train_time:128081ms step_avg:60.70ms
step:2111/2155 train_time:128171ms step_avg:60.72ms
step:2112/2155 train_time:128258ms step_avg:60.73ms
step:2113/2155 train_time:128346ms step_avg:60.74ms
step:2114/2155 train_time:128433ms step_avg:60.75ms
step:2115/2155 train_time:128522ms step_avg:60.77ms
step:2116/2155 train_time:128609ms step_avg:60.78ms
step:2117/2155 train_time:128698ms step_avg:60.79ms
step:2118/2155 train_time:128786ms step_avg:60.81ms
step:2119/2155 train_time:128875ms step_avg:60.82ms
step:2120/2155 train_time:128962ms step_avg:60.83ms
step:2121/2155 train_time:129051ms step_avg:60.84ms
step:2122/2155 train_time:129139ms step_avg:60.86ms
step:2123/2155 train_time:129229ms step_avg:60.87ms
step:2124/2155 train_time:129316ms step_avg:60.88ms
step:2125/2155 train_time:129406ms step_avg:60.90ms
step:2126/2155 train_time:129493ms step_avg:60.91ms
step:2127/2155 train_time:129583ms step_avg:60.92ms
step:2128/2155 train_time:129671ms step_avg:60.94ms
step:2129/2155 train_time:129760ms step_avg:60.95ms
step:2130/2155 train_time:129848ms step_avg:60.96ms
step:2131/2155 train_time:129937ms step_avg:60.97ms
step:2132/2155 train_time:130024ms step_avg:60.99ms
step:2133/2155 train_time:130114ms step_avg:61.00ms
step:2134/2155 train_time:130202ms step_avg:61.01ms
step:2135/2155 train_time:130291ms step_avg:61.03ms
step:2136/2155 train_time:130379ms step_avg:61.04ms
step:2137/2155 train_time:130468ms step_avg:61.05ms
step:2138/2155 train_time:130556ms step_avg:61.06ms
step:2139/2155 train_time:130646ms step_avg:61.08ms
step:2140/2155 train_time:130733ms step_avg:61.09ms
step:2141/2155 train_time:130822ms step_avg:61.10ms
step:2142/2155 train_time:130910ms step_avg:61.12ms
step:2143/2155 train_time:130999ms step_avg:61.13ms
step:2144/2155 train_time:131087ms step_avg:61.14ms
step:2145/2155 train_time:131177ms step_avg:61.15ms
step:2146/2155 train_time:131264ms step_avg:61.17ms
step:2147/2155 train_time:131354ms step_avg:61.18ms
step:2148/2155 train_time:131441ms step_avg:61.19ms
step:2149/2155 train_time:131531ms step_avg:61.21ms
step:2150/2155 train_time:131618ms step_avg:61.22ms
step:2151/2155 train_time:131707ms step_avg:61.23ms
step:2152/2155 train_time:131795ms step_avg:61.24ms
step:2153/2155 train_time:131884ms step_avg:61.26ms
step:2154/2155 train_time:131972ms step_avg:61.27ms
step:2155/2155 train_time:132061ms step_avg:61.28ms
step:2155/2155 val_loss:3.2760 train_time:132150ms step_avg:61.32ms
peak memory allocated: 29892 MiB reserved: 44436 MiB

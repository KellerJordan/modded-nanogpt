import uuid
run_id = f"NorMuon Fixes and Optims 8xH100 - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2115  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 10 2025, 18:15:56) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 18:43:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   44C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   44C    P0            130W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   44C    P0            128W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   34C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   43C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           13590      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           13591      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           13592      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           13593      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           13594      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           13595      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           13596      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           13597      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           13591      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           13592      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           13593      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           13594      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           13595      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           13596      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           13597      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2155 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2155 train_time:86ms step_avg:86.46ms
step:2/2155 train_time:164ms step_avg:81.81ms
step:3/2155 train_time:185ms step_avg:61.79ms
step:4/2155 train_time:208ms step_avg:51.95ms
step:5/2155 train_time:235ms step_avg:46.91ms
step:6/2155 train_time:330ms step_avg:55.05ms
step:7/2155 train_time:394ms step_avg:56.26ms
step:8/2155 train_time:427ms step_avg:53.41ms
step:9/2155 train_time:459ms step_avg:51.04ms
step:10/2155 train_time:493ms step_avg:49.28ms
step:11/2155 train_time:525ms step_avg:47.74ms
step:12/2155 train_time:559ms step_avg:46.56ms
step:13/2155 train_time:591ms step_avg:45.46ms
step:14/2155 train_time:625ms step_avg:44.62ms
step:15/2155 train_time:657ms step_avg:43.79ms
step:16/2155 train_time:690ms step_avg:43.15ms
step:17/2155 train_time:723ms step_avg:42.54ms
step:18/2155 train_time:757ms step_avg:42.05ms
step:19/2155 train_time:789ms step_avg:41.55ms
step:20/2155 train_time:823ms step_avg:41.15ms
step:21/2155 train_time:856ms step_avg:40.74ms
step:22/2155 train_time:889ms step_avg:40.42ms
step:23/2155 train_time:921ms step_avg:40.06ms
step:24/2155 train_time:955ms step_avg:39.81ms
step:25/2155 train_time:988ms step_avg:39.50ms
step:26/2155 train_time:1021ms step_avg:39.28ms
step:27/2155 train_time:1054ms step_avg:39.02ms
step:28/2155 train_time:1087ms step_avg:38.83ms
step:29/2155 train_time:1120ms step_avg:38.61ms
step:30/2155 train_time:1153ms step_avg:38.45ms
step:31/2155 train_time:1186ms step_avg:38.25ms
step:32/2155 train_time:1219ms step_avg:38.11ms
step:33/2155 train_time:1252ms step_avg:37.94ms
step:34/2155 train_time:1286ms step_avg:37.81ms
step:35/2155 train_time:1321ms step_avg:37.74ms
step:36/2155 train_time:1356ms step_avg:37.66ms
step:37/2155 train_time:1390ms step_avg:37.58ms
step:38/2155 train_time:1424ms step_avg:37.49ms
step:39/2155 train_time:1458ms step_avg:37.39ms
step:40/2155 train_time:1492ms step_avg:37.30ms
step:41/2155 train_time:1525ms step_avg:37.18ms
step:42/2155 train_time:1558ms step_avg:37.10ms
step:43/2155 train_time:1591ms step_avg:37.00ms
step:44/2155 train_time:1625ms step_avg:36.93ms
step:45/2155 train_time:1658ms step_avg:36.84ms
step:46/2155 train_time:1692ms step_avg:36.78ms
step:47/2155 train_time:1724ms step_avg:36.69ms
step:48/2155 train_time:1758ms step_avg:36.63ms
step:49/2155 train_time:1791ms step_avg:36.54ms
step:50/2155 train_time:1824ms step_avg:36.49ms
step:51/2155 train_time:1857ms step_avg:36.41ms
step:52/2155 train_time:1891ms step_avg:36.36ms
step:53/2155 train_time:1923ms step_avg:36.28ms
step:54/2155 train_time:1957ms step_avg:36.23ms
step:55/2155 train_time:1989ms step_avg:36.17ms
step:56/2155 train_time:2023ms step_avg:36.12ms
step:57/2155 train_time:2056ms step_avg:36.07ms
step:58/2155 train_time:2090ms step_avg:36.03ms
step:59/2155 train_time:2122ms step_avg:35.97ms
step:60/2155 train_time:2156ms step_avg:35.93ms
step:61/2155 train_time:2189ms step_avg:35.88ms
step:62/2155 train_time:2222ms step_avg:35.85ms
step:63/2155 train_time:2255ms step_avg:35.79ms
step:64/2155 train_time:2289ms step_avg:35.76ms
step:65/2155 train_time:2322ms step_avg:35.72ms
step:66/2155 train_time:2356ms step_avg:35.69ms
step:67/2155 train_time:2389ms step_avg:35.65ms
step:68/2155 train_time:2422ms step_avg:35.62ms
step:69/2155 train_time:2455ms step_avg:35.59ms
step:70/2155 train_time:2489ms step_avg:35.56ms
step:71/2155 train_time:2522ms step_avg:35.52ms
step:72/2155 train_time:2556ms step_avg:35.50ms
step:73/2155 train_time:2589ms step_avg:35.46ms
step:74/2155 train_time:2622ms step_avg:35.44ms
step:75/2155 train_time:2655ms step_avg:35.40ms
step:76/2155 train_time:2688ms step_avg:35.37ms
step:77/2155 train_time:2721ms step_avg:35.34ms
step:78/2155 train_time:2755ms step_avg:35.32ms
step:79/2155 train_time:2788ms step_avg:35.29ms
step:80/2155 train_time:2822ms step_avg:35.27ms
step:81/2155 train_time:2854ms step_avg:35.23ms
step:82/2155 train_time:2887ms step_avg:35.21ms
step:83/2155 train_time:2920ms step_avg:35.18ms
step:84/2155 train_time:2953ms step_avg:35.16ms
step:85/2155 train_time:2986ms step_avg:35.13ms
step:86/2155 train_time:3019ms step_avg:35.11ms
step:87/2155 train_time:3052ms step_avg:35.08ms
step:88/2155 train_time:3085ms step_avg:35.06ms
step:89/2155 train_time:3118ms step_avg:35.03ms
step:90/2155 train_time:3152ms step_avg:35.02ms
step:91/2155 train_time:3184ms step_avg:34.99ms
step:92/2155 train_time:3218ms step_avg:34.97ms
step:93/2155 train_time:3250ms step_avg:34.95ms
step:94/2155 train_time:3284ms step_avg:34.94ms
step:95/2155 train_time:3317ms step_avg:34.92ms
step:96/2155 train_time:3351ms step_avg:34.90ms
step:97/2155 train_time:3384ms step_avg:34.88ms
step:98/2155 train_time:3417ms step_avg:34.87ms
step:99/2155 train_time:3450ms step_avg:34.85ms
step:100/2155 train_time:3484ms step_avg:34.84ms
step:101/2155 train_time:3517ms step_avg:34.82ms
step:102/2155 train_time:3551ms step_avg:34.81ms
step:103/2155 train_time:3583ms step_avg:34.79ms
step:104/2155 train_time:3617ms step_avg:34.78ms
step:105/2155 train_time:3650ms step_avg:34.76ms
step:106/2155 train_time:3683ms step_avg:34.75ms
step:107/2155 train_time:3716ms step_avg:34.73ms
step:108/2155 train_time:3750ms step_avg:34.72ms
step:109/2155 train_time:3782ms step_avg:34.70ms
step:110/2155 train_time:3816ms step_avg:34.69ms
step:111/2155 train_time:3849ms step_avg:34.67ms
step:112/2155 train_time:3882ms step_avg:34.66ms
step:113/2155 train_time:3915ms step_avg:34.65ms
step:114/2155 train_time:3949ms step_avg:34.64ms
step:115/2155 train_time:3981ms step_avg:34.62ms
step:116/2155 train_time:4015ms step_avg:34.61ms
step:117/2155 train_time:4047ms step_avg:34.59ms
step:118/2155 train_time:4081ms step_avg:34.58ms
step:119/2155 train_time:4113ms step_avg:34.56ms
step:120/2155 train_time:4146ms step_avg:34.55ms
step:121/2155 train_time:4179ms step_avg:34.54ms
step:122/2155 train_time:4213ms step_avg:34.53ms
step:123/2155 train_time:4246ms step_avg:34.52ms
step:124/2155 train_time:4279ms step_avg:34.51ms
step:125/2155 train_time:4312ms step_avg:34.50ms
step:126/2155 train_time:4346ms step_avg:34.49ms
step:127/2155 train_time:4378ms step_avg:34.47ms
step:128/2155 train_time:4412ms step_avg:34.47ms
step:129/2155 train_time:4444ms step_avg:34.45ms
step:130/2155 train_time:4478ms step_avg:34.45ms
step:131/2155 train_time:4511ms step_avg:34.44ms
step:132/2155 train_time:4545ms step_avg:34.43ms
step:133/2155 train_time:4577ms step_avg:34.42ms
step:134/2155 train_time:4611ms step_avg:34.41ms
step:135/2155 train_time:4644ms step_avg:34.40ms
step:136/2155 train_time:4677ms step_avg:34.39ms
step:137/2155 train_time:4710ms step_avg:34.38ms
step:138/2155 train_time:4744ms step_avg:34.37ms
step:139/2155 train_time:4777ms step_avg:34.36ms
step:140/2155 train_time:4810ms step_avg:34.36ms
step:141/2155 train_time:4843ms step_avg:34.35ms
step:142/2155 train_time:4877ms step_avg:34.34ms
step:143/2155 train_time:4910ms step_avg:34.33ms
step:144/2155 train_time:4943ms step_avg:34.33ms
step:145/2155 train_time:4977ms step_avg:34.32ms
step:146/2155 train_time:5010ms step_avg:34.31ms
step:147/2155 train_time:5043ms step_avg:34.31ms
step:148/2155 train_time:5076ms step_avg:34.30ms
step:149/2155 train_time:5109ms step_avg:34.29ms
step:150/2155 train_time:5143ms step_avg:34.28ms
step:151/2155 train_time:5175ms step_avg:34.27ms
step:152/2155 train_time:5208ms step_avg:34.26ms
step:153/2155 train_time:5241ms step_avg:34.25ms
step:154/2155 train_time:5275ms step_avg:34.25ms
step:155/2155 train_time:5307ms step_avg:34.24ms
step:156/2155 train_time:5340ms step_avg:34.23ms
step:157/2155 train_time:5373ms step_avg:34.22ms
step:158/2155 train_time:5406ms step_avg:34.22ms
step:159/2155 train_time:5439ms step_avg:34.21ms
step:160/2155 train_time:5473ms step_avg:34.21ms
step:161/2155 train_time:5506ms step_avg:34.20ms
step:162/2155 train_time:5540ms step_avg:34.19ms
step:163/2155 train_time:5573ms step_avg:34.19ms
step:164/2155 train_time:5606ms step_avg:34.18ms
step:165/2155 train_time:5639ms step_avg:34.17ms
step:166/2155 train_time:5672ms step_avg:34.17ms
step:167/2155 train_time:5705ms step_avg:34.16ms
step:168/2155 train_time:5738ms step_avg:34.16ms
step:169/2155 train_time:5771ms step_avg:34.15ms
step:170/2155 train_time:5805ms step_avg:34.15ms
step:171/2155 train_time:5838ms step_avg:34.14ms
step:172/2155 train_time:5871ms step_avg:34.14ms
step:173/2155 train_time:5904ms step_avg:34.13ms
step:174/2155 train_time:5938ms step_avg:34.12ms
step:175/2155 train_time:5970ms step_avg:34.12ms
step:176/2155 train_time:6004ms step_avg:34.11ms
step:177/2155 train_time:6037ms step_avg:34.11ms
step:178/2155 train_time:6070ms step_avg:34.10ms
step:179/2155 train_time:6103ms step_avg:34.10ms
step:180/2155 train_time:6137ms step_avg:34.09ms
step:181/2155 train_time:6169ms step_avg:34.08ms
step:182/2155 train_time:6202ms step_avg:34.08ms
step:183/2155 train_time:6235ms step_avg:34.07ms
step:184/2155 train_time:6268ms step_avg:34.07ms
step:185/2155 train_time:6301ms step_avg:34.06ms
step:186/2155 train_time:6335ms step_avg:34.06ms
step:187/2155 train_time:6367ms step_avg:34.05ms
step:188/2155 train_time:6401ms step_avg:34.05ms
step:189/2155 train_time:6434ms step_avg:34.04ms
step:190/2155 train_time:6467ms step_avg:34.04ms
step:191/2155 train_time:6499ms step_avg:34.03ms
step:192/2155 train_time:6533ms step_avg:34.03ms
step:193/2155 train_time:6566ms step_avg:34.02ms
step:194/2155 train_time:6599ms step_avg:34.02ms
step:195/2155 train_time:6632ms step_avg:34.01ms
step:196/2155 train_time:6666ms step_avg:34.01ms
step:197/2155 train_time:6698ms step_avg:34.00ms
step:198/2155 train_time:6732ms step_avg:34.00ms
step:199/2155 train_time:6764ms step_avg:33.99ms
step:200/2155 train_time:6798ms step_avg:33.99ms
step:201/2155 train_time:6830ms step_avg:33.98ms
step:202/2155 train_time:6864ms step_avg:33.98ms
step:203/2155 train_time:6897ms step_avg:33.97ms
step:204/2155 train_time:6930ms step_avg:33.97ms
step:205/2155 train_time:6962ms step_avg:33.96ms
step:206/2155 train_time:6996ms step_avg:33.96ms
step:207/2155 train_time:7029ms step_avg:33.96ms
step:208/2155 train_time:7062ms step_avg:33.95ms
step:209/2155 train_time:7095ms step_avg:33.95ms
step:210/2155 train_time:7128ms step_avg:33.94ms
step:211/2155 train_time:7161ms step_avg:33.94ms
step:212/2155 train_time:7195ms step_avg:33.94ms
step:213/2155 train_time:7228ms step_avg:33.93ms
step:214/2155 train_time:7261ms step_avg:33.93ms
step:215/2155 train_time:7294ms step_avg:33.93ms
step:216/2155 train_time:7327ms step_avg:33.92ms
step:217/2155 train_time:7360ms step_avg:33.92ms
step:218/2155 train_time:7394ms step_avg:33.92ms
step:219/2155 train_time:7426ms step_avg:33.91ms
step:220/2155 train_time:7460ms step_avg:33.91ms
step:221/2155 train_time:7493ms step_avg:33.90ms
step:222/2155 train_time:7526ms step_avg:33.90ms
step:223/2155 train_time:7558ms step_avg:33.89ms
step:224/2155 train_time:7592ms step_avg:33.89ms
step:225/2155 train_time:7624ms step_avg:33.89ms
step:226/2155 train_time:7658ms step_avg:33.88ms
step:227/2155 train_time:7691ms step_avg:33.88ms
step:228/2155 train_time:7724ms step_avg:33.88ms
step:229/2155 train_time:7757ms step_avg:33.87ms
step:230/2155 train_time:7790ms step_avg:33.87ms
step:231/2155 train_time:7822ms step_avg:33.86ms
step:232/2155 train_time:7856ms step_avg:33.86ms
step:233/2155 train_time:7888ms step_avg:33.86ms
step:234/2155 train_time:7922ms step_avg:33.85ms
step:235/2155 train_time:7954ms step_avg:33.85ms
step:236/2155 train_time:7987ms step_avg:33.84ms
step:237/2155 train_time:8020ms step_avg:33.84ms
step:238/2155 train_time:8053ms step_avg:33.84ms
step:239/2155 train_time:8086ms step_avg:33.83ms
step:240/2155 train_time:8119ms step_avg:33.83ms
step:241/2155 train_time:8152ms step_avg:33.83ms
step:242/2155 train_time:8186ms step_avg:33.82ms
step:243/2155 train_time:8219ms step_avg:33.82ms
step:244/2155 train_time:8252ms step_avg:33.82ms
step:245/2155 train_time:8285ms step_avg:33.82ms
step:246/2155 train_time:8318ms step_avg:33.81ms
step:247/2155 train_time:8351ms step_avg:33.81ms
step:248/2155 train_time:8384ms step_avg:33.81ms
step:249/2155 train_time:8417ms step_avg:33.81ms
step:250/2155 train_time:8451ms step_avg:33.80ms
step:250/2155 val_loss:4.3028 train_time:8487ms step_avg:33.95ms
step:251/2155 train_time:8508ms step_avg:33.90ms
step:252/2155 train_time:8529ms step_avg:33.85ms
step:253/2155 train_time:8552ms step_avg:33.80ms
step:254/2155 train_time:8586ms step_avg:33.80ms
step:255/2155 train_time:8621ms step_avg:33.81ms
step:256/2155 train_time:8657ms step_avg:33.82ms
step:257/2155 train_time:8692ms step_avg:33.82ms
step:258/2155 train_time:8725ms step_avg:33.82ms
step:259/2155 train_time:8759ms step_avg:33.82ms
step:260/2155 train_time:8793ms step_avg:33.82ms
step:261/2155 train_time:8826ms step_avg:33.82ms
step:262/2155 train_time:8859ms step_avg:33.81ms
step:263/2155 train_time:8891ms step_avg:33.81ms
step:264/2155 train_time:8925ms step_avg:33.81ms
step:265/2155 train_time:8957ms step_avg:33.80ms
step:266/2155 train_time:8990ms step_avg:33.80ms
step:267/2155 train_time:9023ms step_avg:33.79ms
step:268/2155 train_time:9057ms step_avg:33.79ms
step:269/2155 train_time:9089ms step_avg:33.79ms
step:270/2155 train_time:9122ms step_avg:33.79ms
step:271/2155 train_time:9155ms step_avg:33.78ms
step:272/2155 train_time:9188ms step_avg:33.78ms
step:273/2155 train_time:9221ms step_avg:33.77ms
step:274/2155 train_time:9254ms step_avg:33.77ms
step:275/2155 train_time:9286ms step_avg:33.77ms
step:276/2155 train_time:9320ms step_avg:33.77ms
step:277/2155 train_time:9352ms step_avg:33.76ms
step:278/2155 train_time:9385ms step_avg:33.76ms
step:279/2155 train_time:9417ms step_avg:33.75ms
step:280/2155 train_time:9451ms step_avg:33.75ms
step:281/2155 train_time:9483ms step_avg:33.75ms
step:282/2155 train_time:9517ms step_avg:33.75ms
step:283/2155 train_time:9549ms step_avg:33.74ms
step:284/2155 train_time:9583ms step_avg:33.74ms
step:285/2155 train_time:9616ms step_avg:33.74ms
step:286/2155 train_time:9650ms step_avg:33.74ms
step:287/2155 train_time:9683ms step_avg:33.74ms
step:288/2155 train_time:9716ms step_avg:33.74ms
step:289/2155 train_time:9750ms step_avg:33.74ms
step:290/2155 train_time:9783ms step_avg:33.74ms
step:291/2155 train_time:9816ms step_avg:33.73ms
step:292/2155 train_time:9849ms step_avg:33.73ms
step:293/2155 train_time:9882ms step_avg:33.73ms
step:294/2155 train_time:9916ms step_avg:33.73ms
step:295/2155 train_time:9948ms step_avg:33.72ms
step:296/2155 train_time:9981ms step_avg:33.72ms
step:297/2155 train_time:10014ms step_avg:33.72ms
step:298/2155 train_time:10048ms step_avg:33.72ms
step:299/2155 train_time:10080ms step_avg:33.71ms
step:300/2155 train_time:10113ms step_avg:33.71ms
step:301/2155 train_time:10146ms step_avg:33.71ms
step:302/2155 train_time:10180ms step_avg:33.71ms
step:303/2155 train_time:10212ms step_avg:33.70ms
step:304/2155 train_time:10246ms step_avg:33.70ms
step:305/2155 train_time:10278ms step_avg:33.70ms
step:306/2155 train_time:10311ms step_avg:33.70ms
step:307/2155 train_time:10344ms step_avg:33.69ms
step:308/2155 train_time:10377ms step_avg:33.69ms
step:309/2155 train_time:10409ms step_avg:33.69ms
step:310/2155 train_time:10443ms step_avg:33.69ms
step:311/2155 train_time:10475ms step_avg:33.68ms
step:312/2155 train_time:10508ms step_avg:33.68ms
step:313/2155 train_time:10541ms step_avg:33.68ms
step:314/2155 train_time:10574ms step_avg:33.68ms
step:315/2155 train_time:10607ms step_avg:33.67ms
step:316/2155 train_time:10641ms step_avg:33.67ms
step:317/2155 train_time:10674ms step_avg:33.67ms
step:318/2155 train_time:10707ms step_avg:33.67ms
step:319/2155 train_time:10741ms step_avg:33.67ms
step:320/2155 train_time:10774ms step_avg:33.67ms
step:321/2155 train_time:10807ms step_avg:33.67ms
step:322/2155 train_time:10841ms step_avg:33.67ms
step:323/2155 train_time:10873ms step_avg:33.66ms
step:324/2155 train_time:10906ms step_avg:33.66ms
step:325/2155 train_time:10939ms step_avg:33.66ms
step:326/2155 train_time:10972ms step_avg:33.66ms
step:327/2155 train_time:11005ms step_avg:33.66ms
step:328/2155 train_time:11039ms step_avg:33.66ms
step:329/2155 train_time:11071ms step_avg:33.65ms
step:330/2155 train_time:11105ms step_avg:33.65ms
step:331/2155 train_time:11137ms step_avg:33.65ms
step:332/2155 train_time:11171ms step_avg:33.65ms
step:333/2155 train_time:11203ms step_avg:33.64ms
step:334/2155 train_time:11237ms step_avg:33.64ms
step:335/2155 train_time:11269ms step_avg:33.64ms
step:336/2155 train_time:11303ms step_avg:33.64ms
step:337/2155 train_time:11335ms step_avg:33.64ms
step:338/2155 train_time:11369ms step_avg:33.63ms
step:339/2155 train_time:11401ms step_avg:33.63ms
step:340/2155 train_time:11435ms step_avg:33.63ms
step:341/2155 train_time:11467ms step_avg:33.63ms
step:342/2155 train_time:11500ms step_avg:33.63ms
step:343/2155 train_time:11533ms step_avg:33.62ms
step:344/2155 train_time:11567ms step_avg:33.62ms
step:345/2155 train_time:11599ms step_avg:33.62ms
step:346/2155 train_time:11633ms step_avg:33.62ms
step:347/2155 train_time:11666ms step_avg:33.62ms
step:348/2155 train_time:11699ms step_avg:33.62ms
step:349/2155 train_time:11732ms step_avg:33.61ms
step:350/2155 train_time:11765ms step_avg:33.61ms
step:351/2155 train_time:11798ms step_avg:33.61ms
step:352/2155 train_time:11831ms step_avg:33.61ms
step:353/2155 train_time:11864ms step_avg:33.61ms
step:354/2155 train_time:11897ms step_avg:33.61ms
step:355/2155 train_time:11930ms step_avg:33.60ms
step:356/2155 train_time:11963ms step_avg:33.60ms
step:357/2155 train_time:11996ms step_avg:33.60ms
step:358/2155 train_time:12029ms step_avg:33.60ms
step:359/2155 train_time:12061ms step_avg:33.60ms
step:360/2155 train_time:12095ms step_avg:33.60ms
step:361/2155 train_time:12128ms step_avg:33.59ms
step:362/2155 train_time:12161ms step_avg:33.59ms
step:363/2155 train_time:12193ms step_avg:33.59ms
step:364/2155 train_time:12227ms step_avg:33.59ms
step:365/2155 train_time:12260ms step_avg:33.59ms
step:366/2155 train_time:12293ms step_avg:33.59ms
step:367/2155 train_time:12325ms step_avg:33.58ms
step:368/2155 train_time:12359ms step_avg:33.58ms
step:369/2155 train_time:12391ms step_avg:33.58ms
step:370/2155 train_time:12425ms step_avg:33.58ms
step:371/2155 train_time:12458ms step_avg:33.58ms
step:372/2155 train_time:12491ms step_avg:33.58ms
step:373/2155 train_time:12524ms step_avg:33.58ms
step:374/2155 train_time:12557ms step_avg:33.58ms
step:375/2155 train_time:12590ms step_avg:33.57ms
step:376/2155 train_time:12623ms step_avg:33.57ms
step:377/2155 train_time:12656ms step_avg:33.57ms
step:378/2155 train_time:12689ms step_avg:33.57ms
step:379/2155 train_time:12722ms step_avg:33.57ms
step:380/2155 train_time:12756ms step_avg:33.57ms
step:381/2155 train_time:12789ms step_avg:33.57ms
step:382/2155 train_time:12822ms step_avg:33.57ms
step:383/2155 train_time:12855ms step_avg:33.56ms
step:384/2155 train_time:12888ms step_avg:33.56ms
step:385/2155 train_time:12922ms step_avg:33.56ms
step:386/2155 train_time:12955ms step_avg:33.56ms
step:387/2155 train_time:12988ms step_avg:33.56ms
step:388/2155 train_time:13021ms step_avg:33.56ms
step:389/2155 train_time:13054ms step_avg:33.56ms
step:390/2155 train_time:13087ms step_avg:33.56ms
step:391/2155 train_time:13120ms step_avg:33.56ms
step:392/2155 train_time:13153ms step_avg:33.55ms
step:393/2155 train_time:13186ms step_avg:33.55ms
step:394/2155 train_time:13220ms step_avg:33.55ms
step:395/2155 train_time:13252ms step_avg:33.55ms
step:396/2155 train_time:13285ms step_avg:33.55ms
step:397/2155 train_time:13317ms step_avg:33.55ms
step:398/2155 train_time:13351ms step_avg:33.54ms
step:399/2155 train_time:13384ms step_avg:33.54ms
step:400/2155 train_time:13417ms step_avg:33.54ms
step:401/2155 train_time:13449ms step_avg:33.54ms
step:402/2155 train_time:13483ms step_avg:33.54ms
step:403/2155 train_time:13515ms step_avg:33.54ms
step:404/2155 train_time:13549ms step_avg:33.54ms
step:405/2155 train_time:13581ms step_avg:33.53ms
step:406/2155 train_time:13615ms step_avg:33.53ms
step:407/2155 train_time:13647ms step_avg:33.53ms
step:408/2155 train_time:13681ms step_avg:33.53ms
step:409/2155 train_time:13714ms step_avg:33.53ms
step:410/2155 train_time:13747ms step_avg:33.53ms
step:411/2155 train_time:13780ms step_avg:33.53ms
step:412/2155 train_time:13813ms step_avg:33.53ms
step:413/2155 train_time:13846ms step_avg:33.52ms
step:414/2155 train_time:13879ms step_avg:33.53ms
step:415/2155 train_time:13912ms step_avg:33.52ms
step:416/2155 train_time:13945ms step_avg:33.52ms
step:417/2155 train_time:13977ms step_avg:33.52ms
step:418/2155 train_time:14011ms step_avg:33.52ms
step:419/2155 train_time:14043ms step_avg:33.52ms
step:420/2155 train_time:14077ms step_avg:33.52ms
step:421/2155 train_time:14109ms step_avg:33.51ms
step:422/2155 train_time:14143ms step_avg:33.51ms
step:423/2155 train_time:14175ms step_avg:33.51ms
step:424/2155 train_time:14209ms step_avg:33.51ms
step:425/2155 train_time:14241ms step_avg:33.51ms
step:426/2155 train_time:14275ms step_avg:33.51ms
step:427/2155 train_time:14308ms step_avg:33.51ms
step:428/2155 train_time:14341ms step_avg:33.51ms
step:429/2155 train_time:14374ms step_avg:33.51ms
step:430/2155 train_time:14407ms step_avg:33.50ms
step:431/2155 train_time:14439ms step_avg:33.50ms
step:432/2155 train_time:14473ms step_avg:33.50ms
step:433/2155 train_time:14505ms step_avg:33.50ms
step:434/2155 train_time:14539ms step_avg:33.50ms
step:435/2155 train_time:14571ms step_avg:33.50ms
step:436/2155 train_time:14605ms step_avg:33.50ms
step:437/2155 train_time:14637ms step_avg:33.49ms
step:438/2155 train_time:14670ms step_avg:33.49ms
step:439/2155 train_time:14703ms step_avg:33.49ms
step:440/2155 train_time:14736ms step_avg:33.49ms
step:441/2155 train_time:14769ms step_avg:33.49ms
step:442/2155 train_time:14803ms step_avg:33.49ms
step:443/2155 train_time:14835ms step_avg:33.49ms
step:444/2155 train_time:14869ms step_avg:33.49ms
step:445/2155 train_time:14901ms step_avg:33.49ms
step:446/2155 train_time:14934ms step_avg:33.49ms
step:447/2155 train_time:14967ms step_avg:33.48ms
step:448/2155 train_time:15000ms step_avg:33.48ms
step:449/2155 train_time:15032ms step_avg:33.48ms
step:450/2155 train_time:15066ms step_avg:33.48ms
step:451/2155 train_time:15098ms step_avg:33.48ms
step:452/2155 train_time:15132ms step_avg:33.48ms
step:453/2155 train_time:15164ms step_avg:33.48ms
step:454/2155 train_time:15197ms step_avg:33.47ms
step:455/2155 train_time:15230ms step_avg:33.47ms
step:456/2155 train_time:15264ms step_avg:33.47ms
step:457/2155 train_time:15296ms step_avg:33.47ms
step:458/2155 train_time:15330ms step_avg:33.47ms
step:459/2155 train_time:15362ms step_avg:33.47ms
step:460/2155 train_time:15396ms step_avg:33.47ms
step:461/2155 train_time:15428ms step_avg:33.47ms
step:462/2155 train_time:15461ms step_avg:33.47ms
step:463/2155 train_time:15494ms step_avg:33.46ms
step:464/2155 train_time:15528ms step_avg:33.46ms
step:465/2155 train_time:15560ms step_avg:33.46ms
step:466/2155 train_time:15594ms step_avg:33.46ms
step:467/2155 train_time:15626ms step_avg:33.46ms
step:468/2155 train_time:15660ms step_avg:33.46ms
step:469/2155 train_time:15692ms step_avg:33.46ms
step:470/2155 train_time:15726ms step_avg:33.46ms
step:471/2155 train_time:15758ms step_avg:33.46ms
step:472/2155 train_time:15792ms step_avg:33.46ms
step:473/2155 train_time:15825ms step_avg:33.46ms
step:474/2155 train_time:15858ms step_avg:33.46ms
step:475/2155 train_time:15891ms step_avg:33.45ms
step:476/2155 train_time:15924ms step_avg:33.45ms
step:477/2155 train_time:15956ms step_avg:33.45ms
step:478/2155 train_time:15990ms step_avg:33.45ms
step:479/2155 train_time:16023ms step_avg:33.45ms
step:480/2155 train_time:16056ms step_avg:33.45ms
step:481/2155 train_time:16089ms step_avg:33.45ms
step:482/2155 train_time:16122ms step_avg:33.45ms
step:483/2155 train_time:16155ms step_avg:33.45ms
step:484/2155 train_time:16189ms step_avg:33.45ms
step:485/2155 train_time:16221ms step_avg:33.45ms
step:486/2155 train_time:16255ms step_avg:33.45ms
step:487/2155 train_time:16288ms step_avg:33.44ms
step:488/2155 train_time:16321ms step_avg:33.44ms
step:489/2155 train_time:16353ms step_avg:33.44ms
step:490/2155 train_time:16387ms step_avg:33.44ms
step:491/2155 train_time:16419ms step_avg:33.44ms
step:492/2155 train_time:16452ms step_avg:33.44ms
step:493/2155 train_time:16485ms step_avg:33.44ms
step:494/2155 train_time:16519ms step_avg:33.44ms
step:495/2155 train_time:16552ms step_avg:33.44ms
step:496/2155 train_time:16585ms step_avg:33.44ms
step:497/2155 train_time:16618ms step_avg:33.44ms
step:498/2155 train_time:16651ms step_avg:33.44ms
step:499/2155 train_time:16684ms step_avg:33.44ms
step:500/2155 train_time:16718ms step_avg:33.44ms
step:500/2155 val_loss:4.0227 train_time:16754ms step_avg:33.51ms
step:501/2155 train_time:16775ms step_avg:33.48ms
step:502/2155 train_time:16796ms step_avg:33.46ms
step:503/2155 train_time:16820ms step_avg:33.44ms
step:504/2155 train_time:16853ms step_avg:33.44ms
step:505/2155 train_time:16887ms step_avg:33.44ms
step:506/2155 train_time:16921ms step_avg:33.44ms
step:507/2155 train_time:16954ms step_avg:33.44ms
step:508/2155 train_time:16987ms step_avg:33.44ms
step:509/2155 train_time:17020ms step_avg:33.44ms
step:510/2155 train_time:17053ms step_avg:33.44ms
step:511/2155 train_time:17086ms step_avg:33.44ms
step:512/2155 train_time:17119ms step_avg:33.44ms
step:513/2155 train_time:17151ms step_avg:33.43ms
step:514/2155 train_time:17185ms step_avg:33.43ms
step:515/2155 train_time:17217ms step_avg:33.43ms
step:516/2155 train_time:17250ms step_avg:33.43ms
step:517/2155 train_time:17283ms step_avg:33.43ms
step:518/2155 train_time:17316ms step_avg:33.43ms
step:519/2155 train_time:17348ms step_avg:33.43ms
step:520/2155 train_time:17381ms step_avg:33.43ms
step:521/2155 train_time:17414ms step_avg:33.42ms
step:522/2155 train_time:17448ms step_avg:33.42ms
step:523/2155 train_time:17480ms step_avg:33.42ms
step:524/2155 train_time:17513ms step_avg:33.42ms
step:525/2155 train_time:17545ms step_avg:33.42ms
step:526/2155 train_time:17579ms step_avg:33.42ms
step:527/2155 train_time:17611ms step_avg:33.42ms
step:528/2155 train_time:17645ms step_avg:33.42ms
step:529/2155 train_time:17678ms step_avg:33.42ms
step:530/2155 train_time:17712ms step_avg:33.42ms
step:531/2155 train_time:17746ms step_avg:33.42ms
step:532/2155 train_time:17779ms step_avg:33.42ms
step:533/2155 train_time:17813ms step_avg:33.42ms
step:534/2155 train_time:17847ms step_avg:33.42ms
step:535/2155 train_time:17880ms step_avg:33.42ms
step:536/2155 train_time:17914ms step_avg:33.42ms
step:537/2155 train_time:17947ms step_avg:33.42ms
step:538/2155 train_time:17980ms step_avg:33.42ms
step:539/2155 train_time:18013ms step_avg:33.42ms
step:540/2155 train_time:18047ms step_avg:33.42ms
step:541/2155 train_time:18080ms step_avg:33.42ms
step:542/2155 train_time:18113ms step_avg:33.42ms
step:543/2155 train_time:18145ms step_avg:33.42ms
step:544/2155 train_time:18179ms step_avg:33.42ms
step:545/2155 train_time:18211ms step_avg:33.42ms
step:546/2155 train_time:18245ms step_avg:33.42ms
step:547/2155 train_time:18277ms step_avg:33.41ms
step:548/2155 train_time:18310ms step_avg:33.41ms
step:549/2155 train_time:18343ms step_avg:33.41ms
step:550/2155 train_time:18376ms step_avg:33.41ms
step:551/2155 train_time:18408ms step_avg:33.41ms
step:552/2155 train_time:18442ms step_avg:33.41ms
step:553/2155 train_time:18474ms step_avg:33.41ms
step:554/2155 train_time:18508ms step_avg:33.41ms
step:555/2155 train_time:18540ms step_avg:33.41ms
step:556/2155 train_time:18573ms step_avg:33.41ms
step:557/2155 train_time:18606ms step_avg:33.40ms
step:558/2155 train_time:18639ms step_avg:33.40ms
step:559/2155 train_time:18672ms step_avg:33.40ms
step:560/2155 train_time:18706ms step_avg:33.40ms
step:561/2155 train_time:18738ms step_avg:33.40ms
step:562/2155 train_time:18772ms step_avg:33.40ms
step:563/2155 train_time:18806ms step_avg:33.40ms
step:564/2155 train_time:18839ms step_avg:33.40ms
step:565/2155 train_time:18872ms step_avg:33.40ms
step:566/2155 train_time:18906ms step_avg:33.40ms
step:567/2155 train_time:18939ms step_avg:33.40ms
step:568/2155 train_time:18972ms step_avg:33.40ms
step:569/2155 train_time:19005ms step_avg:33.40ms
step:570/2155 train_time:19038ms step_avg:33.40ms
step:571/2155 train_time:19071ms step_avg:33.40ms
step:572/2155 train_time:19105ms step_avg:33.40ms
step:573/2155 train_time:19137ms step_avg:33.40ms
step:574/2155 train_time:19171ms step_avg:33.40ms
step:575/2155 train_time:19204ms step_avg:33.40ms
step:576/2155 train_time:19237ms step_avg:33.40ms
step:577/2155 train_time:19270ms step_avg:33.40ms
step:578/2155 train_time:19303ms step_avg:33.40ms
step:579/2155 train_time:19336ms step_avg:33.40ms
step:580/2155 train_time:19369ms step_avg:33.40ms
step:581/2155 train_time:19402ms step_avg:33.39ms
step:582/2155 train_time:19435ms step_avg:33.39ms
step:583/2155 train_time:19468ms step_avg:33.39ms
step:584/2155 train_time:19501ms step_avg:33.39ms
step:585/2155 train_time:19534ms step_avg:33.39ms
step:586/2155 train_time:19567ms step_avg:33.39ms
step:587/2155 train_time:19600ms step_avg:33.39ms
step:588/2155 train_time:19633ms step_avg:33.39ms
step:589/2155 train_time:19666ms step_avg:33.39ms
step:590/2155 train_time:19699ms step_avg:33.39ms
step:591/2155 train_time:19732ms step_avg:33.39ms
step:592/2155 train_time:19765ms step_avg:33.39ms
step:593/2155 train_time:19798ms step_avg:33.39ms
step:594/2155 train_time:19831ms step_avg:33.39ms
step:595/2155 train_time:19864ms step_avg:33.38ms
step:596/2155 train_time:19897ms step_avg:33.38ms
step:597/2155 train_time:19931ms step_avg:33.38ms
step:598/2155 train_time:19964ms step_avg:33.38ms
step:599/2155 train_time:19997ms step_avg:33.38ms
step:600/2155 train_time:20031ms step_avg:33.38ms
step:601/2155 train_time:20063ms step_avg:33.38ms
step:602/2155 train_time:20097ms step_avg:33.38ms
step:603/2155 train_time:20130ms step_avg:33.38ms
step:604/2155 train_time:20163ms step_avg:33.38ms
step:605/2155 train_time:20196ms step_avg:33.38ms
step:606/2155 train_time:20230ms step_avg:33.38ms
step:607/2155 train_time:20262ms step_avg:33.38ms
step:608/2155 train_time:20296ms step_avg:33.38ms
step:609/2155 train_time:20328ms step_avg:33.38ms
step:610/2155 train_time:20361ms step_avg:33.38ms
step:611/2155 train_time:20394ms step_avg:33.38ms
step:612/2155 train_time:20428ms step_avg:33.38ms
step:613/2155 train_time:20460ms step_avg:33.38ms
step:614/2155 train_time:20494ms step_avg:33.38ms
step:615/2155 train_time:20526ms step_avg:33.38ms
step:616/2155 train_time:20560ms step_avg:33.38ms
step:617/2155 train_time:20593ms step_avg:33.38ms
step:618/2155 train_time:20626ms step_avg:33.38ms
step:619/2155 train_time:20659ms step_avg:33.37ms
step:620/2155 train_time:20692ms step_avg:33.37ms
step:621/2155 train_time:20725ms step_avg:33.37ms
step:622/2155 train_time:20758ms step_avg:33.37ms
step:623/2155 train_time:20791ms step_avg:33.37ms
step:624/2155 train_time:20825ms step_avg:33.37ms
step:625/2155 train_time:20857ms step_avg:33.37ms
step:626/2155 train_time:20891ms step_avg:33.37ms
step:627/2155 train_time:20923ms step_avg:33.37ms
step:628/2155 train_time:20957ms step_avg:33.37ms
step:629/2155 train_time:20990ms step_avg:33.37ms
step:630/2155 train_time:21024ms step_avg:33.37ms
step:631/2155 train_time:21057ms step_avg:33.37ms
step:632/2155 train_time:21090ms step_avg:33.37ms
step:633/2155 train_time:21124ms step_avg:33.37ms
step:634/2155 train_time:21157ms step_avg:33.37ms
step:635/2155 train_time:21190ms step_avg:33.37ms
step:636/2155 train_time:21223ms step_avg:33.37ms
step:637/2155 train_time:21256ms step_avg:33.37ms
step:638/2155 train_time:21290ms step_avg:33.37ms
step:639/2155 train_time:21322ms step_avg:33.37ms
step:640/2155 train_time:21355ms step_avg:33.37ms
step:641/2155 train_time:21388ms step_avg:33.37ms
step:642/2155 train_time:21422ms step_avg:33.37ms
step:643/2155 train_time:21455ms step_avg:33.37ms
step:644/2155 train_time:21488ms step_avg:33.37ms
step:645/2155 train_time:21520ms step_avg:33.36ms
step:646/2155 train_time:21554ms step_avg:33.36ms
step:647/2155 train_time:21587ms step_avg:33.36ms
step:648/2155 train_time:21620ms step_avg:33.36ms
step:649/2155 train_time:21653ms step_avg:33.36ms
step:650/2155 train_time:21687ms step_avg:33.36ms
step:651/2155 train_time:21719ms step_avg:33.36ms
step:652/2155 train_time:21753ms step_avg:33.36ms
step:653/2155 train_time:21785ms step_avg:33.36ms
step:654/2155 train_time:21819ms step_avg:33.36ms
step:655/2155 train_time:21851ms step_avg:33.36ms
step:656/2155 train_time:21885ms step_avg:33.36ms
step:657/2155 train_time:21918ms step_avg:33.36ms
step:658/2155 train_time:21951ms step_avg:33.36ms
step:659/2155 train_time:21984ms step_avg:33.36ms
step:660/2155 train_time:22017ms step_avg:33.36ms
step:661/2155 train_time:22051ms step_avg:33.36ms
step:662/2155 train_time:22085ms step_avg:33.36ms
step:663/2155 train_time:22117ms step_avg:33.36ms
step:664/2155 train_time:22151ms step_avg:33.36ms
step:665/2155 train_time:22184ms step_avg:33.36ms
step:666/2155 train_time:22217ms step_avg:33.36ms
step:667/2155 train_time:22250ms step_avg:33.36ms
step:668/2155 train_time:22284ms step_avg:33.36ms
step:669/2155 train_time:22316ms step_avg:33.36ms
step:670/2155 train_time:22350ms step_avg:33.36ms
step:671/2155 train_time:22384ms step_avg:33.36ms
step:672/2155 train_time:22417ms step_avg:33.36ms
step:673/2155 train_time:22450ms step_avg:33.36ms
step:674/2155 train_time:22483ms step_avg:33.36ms
step:675/2155 train_time:22516ms step_avg:33.36ms
step:676/2155 train_time:22550ms step_avg:33.36ms
step:677/2155 train_time:22583ms step_avg:33.36ms
step:678/2155 train_time:22616ms step_avg:33.36ms
step:679/2155 train_time:22649ms step_avg:33.36ms
step:680/2155 train_time:22682ms step_avg:33.36ms
step:681/2155 train_time:22715ms step_avg:33.36ms
step:682/2155 train_time:22748ms step_avg:33.36ms
step:683/2155 train_time:22781ms step_avg:33.35ms
step:684/2155 train_time:22814ms step_avg:33.35ms
step:685/2155 train_time:22847ms step_avg:33.35ms
step:686/2155 train_time:22881ms step_avg:33.35ms
step:687/2155 train_time:22913ms step_avg:33.35ms
step:688/2155 train_time:22947ms step_avg:33.35ms
step:689/2155 train_time:22980ms step_avg:33.35ms
step:690/2155 train_time:23013ms step_avg:33.35ms
step:691/2155 train_time:23046ms step_avg:33.35ms
step:692/2155 train_time:23079ms step_avg:33.35ms
step:693/2155 train_time:23112ms step_avg:33.35ms
step:694/2155 train_time:23146ms step_avg:33.35ms
step:695/2155 train_time:23178ms step_avg:33.35ms
step:696/2155 train_time:23212ms step_avg:33.35ms
step:697/2155 train_time:23244ms step_avg:33.35ms
step:698/2155 train_time:23278ms step_avg:33.35ms
step:699/2155 train_time:23311ms step_avg:33.35ms
step:700/2155 train_time:23345ms step_avg:33.35ms
step:701/2155 train_time:23377ms step_avg:33.35ms
step:702/2155 train_time:23411ms step_avg:33.35ms
step:703/2155 train_time:23444ms step_avg:33.35ms
step:704/2155 train_time:23478ms step_avg:33.35ms
step:705/2155 train_time:23511ms step_avg:33.35ms
step:706/2155 train_time:23545ms step_avg:33.35ms
step:707/2155 train_time:23604ms step_avg:33.39ms
step:708/2155 train_time:23663ms step_avg:33.42ms
step:709/2155 train_time:23724ms step_avg:33.46ms
step:710/2155 train_time:23784ms step_avg:33.50ms
step:711/2155 train_time:23845ms step_avg:33.54ms
step:712/2155 train_time:23905ms step_avg:33.57ms
step:713/2155 train_time:23966ms step_avg:33.61ms
step:714/2155 train_time:24026ms step_avg:33.65ms
step:715/2155 train_time:24088ms step_avg:33.69ms
step:716/2155 train_time:24147ms step_avg:33.72ms
step:717/2155 train_time:24208ms step_avg:33.76ms
step:718/2155 train_time:24268ms step_avg:33.80ms
step:719/2155 train_time:24330ms step_avg:33.84ms
step:720/2155 train_time:24389ms step_avg:33.87ms
step:721/2155 train_time:24450ms step_avg:33.91ms
step:722/2155 train_time:24509ms step_avg:33.95ms
step:723/2155 train_time:24570ms step_avg:33.98ms
step:724/2155 train_time:24629ms step_avg:34.02ms
step:725/2155 train_time:24690ms step_avg:34.06ms
step:726/2155 train_time:24750ms step_avg:34.09ms
step:727/2155 train_time:24810ms step_avg:34.13ms
step:728/2155 train_time:24870ms step_avg:34.16ms
step:729/2155 train_time:24931ms step_avg:34.20ms
step:730/2155 train_time:24991ms step_avg:34.23ms
step:731/2155 train_time:25053ms step_avg:34.27ms
step:732/2155 train_time:25112ms step_avg:34.31ms
step:733/2155 train_time:25173ms step_avg:34.34ms
step:734/2155 train_time:25232ms step_avg:34.38ms
step:735/2155 train_time:25293ms step_avg:34.41ms
step:736/2155 train_time:25354ms step_avg:34.45ms
step:737/2155 train_time:25415ms step_avg:34.48ms
step:738/2155 train_time:25475ms step_avg:34.52ms
step:739/2155 train_time:25536ms step_avg:34.56ms
step:740/2155 train_time:25595ms step_avg:34.59ms
step:741/2155 train_time:25656ms step_avg:34.62ms
step:742/2155 train_time:25715ms step_avg:34.66ms
step:743/2155 train_time:25776ms step_avg:34.69ms
step:744/2155 train_time:25836ms step_avg:34.73ms
step:745/2155 train_time:25897ms step_avg:34.76ms
step:746/2155 train_time:25956ms step_avg:34.79ms
step:747/2155 train_time:26018ms step_avg:34.83ms
step:748/2155 train_time:26078ms step_avg:34.86ms
step:749/2155 train_time:26140ms step_avg:34.90ms
step:750/2155 train_time:26199ms step_avg:34.93ms
step:750/2155 val_loss:3.8716 train_time:26263ms step_avg:35.02ms
step:751/2155 train_time:26285ms step_avg:35.00ms
step:752/2155 train_time:26321ms step_avg:35.00ms
step:753/2155 train_time:26381ms step_avg:35.03ms
step:754/2155 train_time:26442ms step_avg:35.07ms
step:755/2155 train_time:26505ms step_avg:35.11ms
step:756/2155 train_time:26564ms step_avg:35.14ms
step:757/2155 train_time:26624ms step_avg:35.17ms
step:758/2155 train_time:26683ms step_avg:35.20ms
step:759/2155 train_time:26743ms step_avg:35.23ms
step:760/2155 train_time:26802ms step_avg:35.27ms
step:761/2155 train_time:26861ms step_avg:35.30ms
step:762/2155 train_time:26920ms step_avg:35.33ms
step:763/2155 train_time:26980ms step_avg:35.36ms
step:764/2155 train_time:27039ms step_avg:35.39ms
step:765/2155 train_time:27098ms step_avg:35.42ms
step:766/2155 train_time:27158ms step_avg:35.45ms
step:767/2155 train_time:27226ms step_avg:35.50ms
step:768/2155 train_time:27288ms step_avg:35.53ms
step:769/2155 train_time:27348ms step_avg:35.56ms
step:770/2155 train_time:27408ms step_avg:35.59ms
step:771/2155 train_time:27469ms step_avg:35.63ms
step:772/2155 train_time:27528ms step_avg:35.66ms
step:773/2155 train_time:27588ms step_avg:35.69ms
step:774/2155 train_time:27647ms step_avg:35.72ms
step:775/2155 train_time:27707ms step_avg:35.75ms
step:776/2155 train_time:27766ms step_avg:35.78ms
step:777/2155 train_time:27826ms step_avg:35.81ms
step:778/2155 train_time:27885ms step_avg:35.84ms
step:779/2155 train_time:27946ms step_avg:35.87ms
step:780/2155 train_time:28004ms step_avg:35.90ms
step:781/2155 train_time:28065ms step_avg:35.93ms
step:782/2155 train_time:28125ms step_avg:35.96ms
step:783/2155 train_time:28187ms step_avg:36.00ms
step:784/2155 train_time:28246ms step_avg:36.03ms
step:785/2155 train_time:28308ms step_avg:36.06ms
step:786/2155 train_time:28368ms step_avg:36.09ms
step:787/2155 train_time:28429ms step_avg:36.12ms
step:788/2155 train_time:28488ms step_avg:36.15ms
step:789/2155 train_time:28549ms step_avg:36.18ms
step:790/2155 train_time:28608ms step_avg:36.21ms
step:791/2155 train_time:28669ms step_avg:36.24ms
step:792/2155 train_time:28728ms step_avg:36.27ms
step:793/2155 train_time:28789ms step_avg:36.30ms
step:794/2155 train_time:28848ms step_avg:36.33ms
step:795/2155 train_time:28908ms step_avg:36.36ms
step:796/2155 train_time:28967ms step_avg:36.39ms
step:797/2155 train_time:29027ms step_avg:36.42ms
step:798/2155 train_time:29087ms step_avg:36.45ms
step:799/2155 train_time:29147ms step_avg:36.48ms
step:800/2155 train_time:29207ms step_avg:36.51ms
step:801/2155 train_time:29268ms step_avg:36.54ms
step:802/2155 train_time:29328ms step_avg:36.57ms
step:803/2155 train_time:29389ms step_avg:36.60ms
step:804/2155 train_time:29448ms step_avg:36.63ms
step:805/2155 train_time:29509ms step_avg:36.66ms
step:806/2155 train_time:29568ms step_avg:36.69ms
step:807/2155 train_time:29629ms step_avg:36.71ms
step:808/2155 train_time:29688ms step_avg:36.74ms
step:809/2155 train_time:29749ms step_avg:36.77ms
step:810/2155 train_time:29807ms step_avg:36.80ms
step:811/2155 train_time:29868ms step_avg:36.83ms
step:812/2155 train_time:29927ms step_avg:36.86ms
step:813/2155 train_time:29988ms step_avg:36.89ms
step:814/2155 train_time:30047ms step_avg:36.91ms
step:815/2155 train_time:30107ms step_avg:36.94ms
step:816/2155 train_time:30167ms step_avg:36.97ms
step:817/2155 train_time:30228ms step_avg:37.00ms
step:818/2155 train_time:30287ms step_avg:37.03ms
step:819/2155 train_time:30348ms step_avg:37.06ms
step:820/2155 train_time:30407ms step_avg:37.08ms
step:821/2155 train_time:30468ms step_avg:37.11ms
step:822/2155 train_time:30527ms step_avg:37.14ms
step:823/2155 train_time:30589ms step_avg:37.17ms
step:824/2155 train_time:30648ms step_avg:37.19ms
step:825/2155 train_time:30708ms step_avg:37.22ms
step:826/2155 train_time:30768ms step_avg:37.25ms
step:827/2155 train_time:30828ms step_avg:37.28ms
step:828/2155 train_time:30888ms step_avg:37.30ms
step:829/2155 train_time:30949ms step_avg:37.33ms
step:830/2155 train_time:31008ms step_avg:37.36ms
step:831/2155 train_time:31068ms step_avg:37.39ms
step:832/2155 train_time:31128ms step_avg:37.41ms
step:833/2155 train_time:31189ms step_avg:37.44ms
step:834/2155 train_time:31248ms step_avg:37.47ms
step:835/2155 train_time:31309ms step_avg:37.50ms
step:836/2155 train_time:31368ms step_avg:37.52ms
step:837/2155 train_time:31430ms step_avg:37.55ms
step:838/2155 train_time:31490ms step_avg:37.58ms
step:839/2155 train_time:31551ms step_avg:37.61ms
step:840/2155 train_time:31610ms step_avg:37.63ms
step:841/2155 train_time:31671ms step_avg:37.66ms
step:842/2155 train_time:31730ms step_avg:37.68ms
step:843/2155 train_time:31790ms step_avg:37.71ms
step:844/2155 train_time:31850ms step_avg:37.74ms
step:845/2155 train_time:31911ms step_avg:37.76ms
step:846/2155 train_time:31970ms step_avg:37.79ms
step:847/2155 train_time:32031ms step_avg:37.82ms
step:848/2155 train_time:32090ms step_avg:37.84ms
step:849/2155 train_time:32151ms step_avg:37.87ms
step:850/2155 train_time:32211ms step_avg:37.89ms
step:851/2155 train_time:32272ms step_avg:37.92ms
step:852/2155 train_time:32331ms step_avg:37.95ms
step:853/2155 train_time:32391ms step_avg:37.97ms
step:854/2155 train_time:32451ms step_avg:38.00ms
step:855/2155 train_time:32511ms step_avg:38.02ms
step:856/2155 train_time:32570ms step_avg:38.05ms
step:857/2155 train_time:32631ms step_avg:38.08ms
step:858/2155 train_time:32691ms step_avg:38.10ms
step:859/2155 train_time:32753ms step_avg:38.13ms
step:860/2155 train_time:32813ms step_avg:38.15ms
step:861/2155 train_time:32873ms step_avg:38.18ms
step:862/2155 train_time:32932ms step_avg:38.20ms
step:863/2155 train_time:32993ms step_avg:38.23ms
step:864/2155 train_time:33053ms step_avg:38.26ms
step:865/2155 train_time:33115ms step_avg:38.28ms
step:866/2155 train_time:33174ms step_avg:38.31ms
step:867/2155 train_time:33236ms step_avg:38.33ms
step:868/2155 train_time:33295ms step_avg:38.36ms
step:869/2155 train_time:33356ms step_avg:38.38ms
step:870/2155 train_time:33416ms step_avg:38.41ms
step:871/2155 train_time:33478ms step_avg:38.44ms
step:872/2155 train_time:33537ms step_avg:38.46ms
step:873/2155 train_time:33598ms step_avg:38.49ms
step:874/2155 train_time:33657ms step_avg:38.51ms
step:875/2155 train_time:33719ms step_avg:38.54ms
step:876/2155 train_time:33778ms step_avg:38.56ms
step:877/2155 train_time:33840ms step_avg:38.59ms
step:878/2155 train_time:33898ms step_avg:38.61ms
step:879/2155 train_time:33959ms step_avg:38.63ms
step:880/2155 train_time:34018ms step_avg:38.66ms
step:881/2155 train_time:34079ms step_avg:38.68ms
step:882/2155 train_time:34138ms step_avg:38.71ms
step:883/2155 train_time:34199ms step_avg:38.73ms
step:884/2155 train_time:34258ms step_avg:38.75ms
step:885/2155 train_time:34319ms step_avg:38.78ms
step:886/2155 train_time:34378ms step_avg:38.80ms
step:887/2155 train_time:34440ms step_avg:38.83ms
step:888/2155 train_time:34500ms step_avg:38.85ms
step:889/2155 train_time:34561ms step_avg:38.88ms
step:890/2155 train_time:34620ms step_avg:38.90ms
step:891/2155 train_time:34681ms step_avg:38.92ms
step:892/2155 train_time:34741ms step_avg:38.95ms
step:893/2155 train_time:34803ms step_avg:38.97ms
step:894/2155 train_time:34862ms step_avg:39.00ms
step:895/2155 train_time:34923ms step_avg:39.02ms
step:896/2155 train_time:34982ms step_avg:39.04ms
step:897/2155 train_time:35043ms step_avg:39.07ms
step:898/2155 train_time:35102ms step_avg:39.09ms
step:899/2155 train_time:35163ms step_avg:39.11ms
step:900/2155 train_time:35223ms step_avg:39.14ms
step:901/2155 train_time:35284ms step_avg:39.16ms
step:902/2155 train_time:35343ms step_avg:39.18ms
step:903/2155 train_time:35404ms step_avg:39.21ms
step:904/2155 train_time:35464ms step_avg:39.23ms
step:905/2155 train_time:35525ms step_avg:39.25ms
step:906/2155 train_time:35585ms step_avg:39.28ms
step:907/2155 train_time:35645ms step_avg:39.30ms
step:908/2155 train_time:35704ms step_avg:39.32ms
step:909/2155 train_time:35765ms step_avg:39.35ms
step:910/2155 train_time:35824ms step_avg:39.37ms
step:911/2155 train_time:35885ms step_avg:39.39ms
step:912/2155 train_time:35944ms step_avg:39.41ms
step:913/2155 train_time:36004ms step_avg:39.44ms
step:914/2155 train_time:36063ms step_avg:39.46ms
step:915/2155 train_time:36124ms step_avg:39.48ms
step:916/2155 train_time:36184ms step_avg:39.50ms
step:917/2155 train_time:36245ms step_avg:39.53ms
step:918/2155 train_time:36304ms step_avg:39.55ms
step:919/2155 train_time:36365ms step_avg:39.57ms
step:920/2155 train_time:36425ms step_avg:39.59ms
step:921/2155 train_time:36486ms step_avg:39.62ms
step:922/2155 train_time:36545ms step_avg:39.64ms
step:923/2155 train_time:36606ms step_avg:39.66ms
step:924/2155 train_time:36665ms step_avg:39.68ms
step:925/2155 train_time:36726ms step_avg:39.70ms
step:926/2155 train_time:36785ms step_avg:39.73ms
step:927/2155 train_time:36846ms step_avg:39.75ms
step:928/2155 train_time:36905ms step_avg:39.77ms
step:929/2155 train_time:36965ms step_avg:39.79ms
step:930/2155 train_time:37025ms step_avg:39.81ms
step:931/2155 train_time:37086ms step_avg:39.83ms
step:932/2155 train_time:37144ms step_avg:39.85ms
step:933/2155 train_time:37205ms step_avg:39.88ms
step:934/2155 train_time:37264ms step_avg:39.90ms
step:935/2155 train_time:37325ms step_avg:39.92ms
step:936/2155 train_time:37385ms step_avg:39.94ms
step:937/2155 train_time:37446ms step_avg:39.96ms
step:938/2155 train_time:37505ms step_avg:39.98ms
step:939/2155 train_time:37565ms step_avg:40.01ms
step:940/2155 train_time:37624ms step_avg:40.03ms
step:941/2155 train_time:37686ms step_avg:40.05ms
step:942/2155 train_time:37745ms step_avg:40.07ms
step:943/2155 train_time:37805ms step_avg:40.09ms
step:944/2155 train_time:37864ms step_avg:40.11ms
step:945/2155 train_time:37925ms step_avg:40.13ms
step:946/2155 train_time:37984ms step_avg:40.15ms
step:947/2155 train_time:38045ms step_avg:40.17ms
step:948/2155 train_time:38104ms step_avg:40.19ms
step:949/2155 train_time:38165ms step_avg:40.22ms
step:950/2155 train_time:38225ms step_avg:40.24ms
step:951/2155 train_time:38286ms step_avg:40.26ms
step:952/2155 train_time:38345ms step_avg:40.28ms
step:953/2155 train_time:38406ms step_avg:40.30ms
step:954/2155 train_time:38465ms step_avg:40.32ms
step:955/2155 train_time:38526ms step_avg:40.34ms
step:956/2155 train_time:38586ms step_avg:40.36ms
step:957/2155 train_time:38647ms step_avg:40.38ms
step:958/2155 train_time:38706ms step_avg:40.40ms
step:959/2155 train_time:38767ms step_avg:40.42ms
step:960/2155 train_time:38827ms step_avg:40.44ms
step:961/2155 train_time:38888ms step_avg:40.47ms
step:962/2155 train_time:38947ms step_avg:40.49ms
step:963/2155 train_time:39007ms step_avg:40.51ms
step:964/2155 train_time:39067ms step_avg:40.53ms
step:965/2155 train_time:39127ms step_avg:40.55ms
step:966/2155 train_time:39187ms step_avg:40.57ms
step:967/2155 train_time:39248ms step_avg:40.59ms
step:968/2155 train_time:39307ms step_avg:40.61ms
step:969/2155 train_time:39368ms step_avg:40.63ms
step:970/2155 train_time:39427ms step_avg:40.65ms
step:971/2155 train_time:39489ms step_avg:40.67ms
step:972/2155 train_time:39548ms step_avg:40.69ms
step:973/2155 train_time:39608ms step_avg:40.71ms
step:974/2155 train_time:39667ms step_avg:40.73ms
step:975/2155 train_time:39728ms step_avg:40.75ms
step:976/2155 train_time:39787ms step_avg:40.77ms
step:977/2155 train_time:39848ms step_avg:40.79ms
step:978/2155 train_time:39907ms step_avg:40.81ms
step:979/2155 train_time:39968ms step_avg:40.83ms
step:980/2155 train_time:40028ms step_avg:40.85ms
step:981/2155 train_time:40089ms step_avg:40.87ms
step:982/2155 train_time:40149ms step_avg:40.88ms
step:983/2155 train_time:40209ms step_avg:40.90ms
step:984/2155 train_time:40268ms step_avg:40.92ms
step:985/2155 train_time:40329ms step_avg:40.94ms
step:986/2155 train_time:40388ms step_avg:40.96ms
step:987/2155 train_time:40449ms step_avg:40.98ms
step:988/2155 train_time:40508ms step_avg:41.00ms
step:989/2155 train_time:40570ms step_avg:41.02ms
step:990/2155 train_time:40630ms step_avg:41.04ms
step:991/2155 train_time:40692ms step_avg:41.06ms
step:992/2155 train_time:40752ms step_avg:41.08ms
step:993/2155 train_time:40813ms step_avg:41.10ms
step:994/2155 train_time:40873ms step_avg:41.12ms
step:995/2155 train_time:40934ms step_avg:41.14ms
step:996/2155 train_time:40993ms step_avg:41.16ms
step:997/2155 train_time:41055ms step_avg:41.18ms
step:998/2155 train_time:41115ms step_avg:41.20ms
step:999/2155 train_time:41176ms step_avg:41.22ms
step:1000/2155 train_time:41236ms step_avg:41.24ms
step:1000/2155 val_loss:3.7209 train_time:41300ms step_avg:41.30ms
step:1001/2155 train_time:41322ms step_avg:41.28ms
step:1002/2155 train_time:41359ms step_avg:41.28ms
step:1003/2155 train_time:41424ms step_avg:41.30ms
step:1004/2155 train_time:41484ms step_avg:41.32ms
step:1005/2155 train_time:41545ms step_avg:41.34ms
step:1006/2155 train_time:41604ms step_avg:41.36ms
step:1007/2155 train_time:41665ms step_avg:41.38ms
step:1008/2155 train_time:41724ms step_avg:41.39ms
step:1009/2155 train_time:41784ms step_avg:41.41ms
step:1010/2155 train_time:41842ms step_avg:41.43ms
step:1011/2155 train_time:41903ms step_avg:41.45ms
step:1012/2155 train_time:41961ms step_avg:41.46ms
step:1013/2155 train_time:42022ms step_avg:41.48ms
step:1014/2155 train_time:42081ms step_avg:41.50ms
step:1015/2155 train_time:42142ms step_avg:41.52ms
step:1016/2155 train_time:42201ms step_avg:41.54ms
step:1017/2155 train_time:42264ms step_avg:41.56ms
step:1018/2155 train_time:42324ms step_avg:41.58ms
step:1019/2155 train_time:42386ms step_avg:41.60ms
step:1020/2155 train_time:42448ms step_avg:41.62ms
step:1021/2155 train_time:42510ms step_avg:41.64ms
step:1022/2155 train_time:42569ms step_avg:41.65ms
step:1023/2155 train_time:42631ms step_avg:41.67ms
step:1024/2155 train_time:42690ms step_avg:41.69ms
step:1025/2155 train_time:42752ms step_avg:41.71ms
step:1026/2155 train_time:42811ms step_avg:41.73ms
step:1027/2155 train_time:42872ms step_avg:41.74ms
step:1028/2155 train_time:42931ms step_avg:41.76ms
step:1029/2155 train_time:42992ms step_avg:41.78ms
step:1030/2155 train_time:43052ms step_avg:41.80ms
step:1031/2155 train_time:43112ms step_avg:41.82ms
step:1032/2155 train_time:43172ms step_avg:41.83ms
step:1033/2155 train_time:43233ms step_avg:41.85ms
step:1034/2155 train_time:43293ms step_avg:41.87ms
step:1035/2155 train_time:43354ms step_avg:41.89ms
step:1036/2155 train_time:43415ms step_avg:41.91ms
step:1037/2155 train_time:43477ms step_avg:41.93ms
step:1038/2155 train_time:43538ms step_avg:41.94ms
step:1039/2155 train_time:43600ms step_avg:41.96ms
step:1040/2155 train_time:43660ms step_avg:41.98ms
step:1041/2155 train_time:43721ms step_avg:42.00ms
step:1042/2155 train_time:43780ms step_avg:42.02ms
step:1043/2155 train_time:43842ms step_avg:42.03ms
step:1044/2155 train_time:43901ms step_avg:42.05ms
step:1045/2155 train_time:43962ms step_avg:42.07ms
step:1046/2155 train_time:44021ms step_avg:42.08ms
step:1047/2155 train_time:44081ms step_avg:42.10ms
step:1048/2155 train_time:44140ms step_avg:42.12ms
step:1049/2155 train_time:44201ms step_avg:42.14ms
step:1050/2155 train_time:44261ms step_avg:42.15ms
step:1051/2155 train_time:44323ms step_avg:42.17ms
step:1052/2155 train_time:44382ms step_avg:42.19ms
step:1053/2155 train_time:44444ms step_avg:42.21ms
step:1054/2155 train_time:44504ms step_avg:42.22ms
step:1055/2155 train_time:44565ms step_avg:42.24ms
step:1056/2155 train_time:44625ms step_avg:42.26ms
step:1057/2155 train_time:44687ms step_avg:42.28ms
step:1058/2155 train_time:44746ms step_avg:42.29ms
step:1059/2155 train_time:44807ms step_avg:42.31ms
step:1060/2155 train_time:44867ms step_avg:42.33ms
step:1061/2155 train_time:44927ms step_avg:42.34ms
step:1062/2155 train_time:44987ms step_avg:42.36ms
step:1063/2155 train_time:45047ms step_avg:42.38ms
step:1064/2155 train_time:45107ms step_avg:42.39ms
step:1065/2155 train_time:45168ms step_avg:42.41ms
step:1066/2155 train_time:45227ms step_avg:42.43ms
step:1067/2155 train_time:45288ms step_avg:42.44ms
step:1068/2155 train_time:45347ms step_avg:42.46ms
step:1069/2155 train_time:45408ms step_avg:42.48ms
step:1070/2155 train_time:45468ms step_avg:42.49ms
step:1071/2155 train_time:45530ms step_avg:42.51ms
step:1072/2155 train_time:45589ms step_avg:42.53ms
step:1073/2155 train_time:45651ms step_avg:42.54ms
step:1074/2155 train_time:45710ms step_avg:42.56ms
step:1075/2155 train_time:45773ms step_avg:42.58ms
step:1076/2155 train_time:45832ms step_avg:42.60ms
step:1077/2155 train_time:45894ms step_avg:42.61ms
step:1078/2155 train_time:45953ms step_avg:42.63ms
step:1079/2155 train_time:46014ms step_avg:42.65ms
step:1080/2155 train_time:46074ms step_avg:42.66ms
step:1081/2155 train_time:46134ms step_avg:42.68ms
step:1082/2155 train_time:46194ms step_avg:42.69ms
step:1083/2155 train_time:46254ms step_avg:42.71ms
step:1084/2155 train_time:46314ms step_avg:42.72ms
step:1085/2155 train_time:46376ms step_avg:42.74ms
step:1086/2155 train_time:46436ms step_avg:42.76ms
step:1087/2155 train_time:46498ms step_avg:42.78ms
step:1088/2155 train_time:46558ms step_avg:42.79ms
step:1089/2155 train_time:46619ms step_avg:42.81ms
step:1090/2155 train_time:46679ms step_avg:42.82ms
step:1091/2155 train_time:46740ms step_avg:42.84ms
step:1092/2155 train_time:46799ms step_avg:42.86ms
step:1093/2155 train_time:46861ms step_avg:42.87ms
step:1094/2155 train_time:46919ms step_avg:42.89ms
step:1095/2155 train_time:46980ms step_avg:42.90ms
step:1096/2155 train_time:47039ms step_avg:42.92ms
step:1097/2155 train_time:47101ms step_avg:42.94ms
step:1098/2155 train_time:47160ms step_avg:42.95ms
step:1099/2155 train_time:47222ms step_avg:42.97ms
step:1100/2155 train_time:47281ms step_avg:42.98ms
step:1101/2155 train_time:47343ms step_avg:43.00ms
step:1102/2155 train_time:47402ms step_avg:43.01ms
step:1103/2155 train_time:47464ms step_avg:43.03ms
step:1104/2155 train_time:47524ms step_avg:43.05ms
step:1105/2155 train_time:47585ms step_avg:43.06ms
step:1106/2155 train_time:47644ms step_avg:43.08ms
step:1107/2155 train_time:47705ms step_avg:43.09ms
step:1108/2155 train_time:47764ms step_avg:43.11ms
step:1109/2155 train_time:47826ms step_avg:43.13ms
step:1110/2155 train_time:47885ms step_avg:43.14ms
step:1111/2155 train_time:47946ms step_avg:43.16ms
step:1112/2155 train_time:48004ms step_avg:43.17ms
step:1113/2155 train_time:48065ms step_avg:43.19ms
step:1114/2155 train_time:48125ms step_avg:43.20ms
step:1115/2155 train_time:48186ms step_avg:43.22ms
step:1116/2155 train_time:48245ms step_avg:43.23ms
step:1117/2155 train_time:48305ms step_avg:43.25ms
step:1118/2155 train_time:48364ms step_avg:43.26ms
step:1119/2155 train_time:48425ms step_avg:43.28ms
step:1120/2155 train_time:48485ms step_avg:43.29ms
step:1121/2155 train_time:48545ms step_avg:43.31ms
step:1122/2155 train_time:48605ms step_avg:43.32ms
step:1123/2155 train_time:48665ms step_avg:43.34ms
step:1124/2155 train_time:48725ms step_avg:43.35ms
step:1125/2155 train_time:48786ms step_avg:43.36ms
step:1126/2155 train_time:48845ms step_avg:43.38ms
step:1127/2155 train_time:48906ms step_avg:43.39ms
step:1128/2155 train_time:48965ms step_avg:43.41ms
step:1129/2155 train_time:49026ms step_avg:43.42ms
step:1130/2155 train_time:49085ms step_avg:43.44ms
step:1131/2155 train_time:49146ms step_avg:43.45ms
step:1132/2155 train_time:49205ms step_avg:43.47ms
step:1133/2155 train_time:49265ms step_avg:43.48ms
step:1134/2155 train_time:49325ms step_avg:43.50ms
step:1135/2155 train_time:49385ms step_avg:43.51ms
step:1136/2155 train_time:49444ms step_avg:43.52ms
step:1137/2155 train_time:49505ms step_avg:43.54ms
step:1138/2155 train_time:49565ms step_avg:43.55ms
step:1139/2155 train_time:49626ms step_avg:43.57ms
step:1140/2155 train_time:49685ms step_avg:43.58ms
step:1141/2155 train_time:49746ms step_avg:43.60ms
step:1142/2155 train_time:49805ms step_avg:43.61ms
step:1143/2155 train_time:49866ms step_avg:43.63ms
step:1144/2155 train_time:49925ms step_avg:43.64ms
step:1145/2155 train_time:49986ms step_avg:43.66ms
step:1146/2155 train_time:50044ms step_avg:43.67ms
step:1147/2155 train_time:50106ms step_avg:43.68ms
step:1148/2155 train_time:50165ms step_avg:43.70ms
step:1149/2155 train_time:50226ms step_avg:43.71ms
step:1150/2155 train_time:50285ms step_avg:43.73ms
step:1151/2155 train_time:50346ms step_avg:43.74ms
step:1152/2155 train_time:50405ms step_avg:43.75ms
step:1153/2155 train_time:50466ms step_avg:43.77ms
step:1154/2155 train_time:50525ms step_avg:43.78ms
step:1155/2155 train_time:50587ms step_avg:43.80ms
step:1156/2155 train_time:50646ms step_avg:43.81ms
step:1157/2155 train_time:50707ms step_avg:43.83ms
step:1158/2155 train_time:50767ms step_avg:43.84ms
step:1159/2155 train_time:50828ms step_avg:43.86ms
step:1160/2155 train_time:50887ms step_avg:43.87ms
step:1161/2155 train_time:50948ms step_avg:43.88ms
step:1162/2155 train_time:51007ms step_avg:43.90ms
step:1163/2155 train_time:51068ms step_avg:43.91ms
step:1164/2155 train_time:51127ms step_avg:43.92ms
step:1165/2155 train_time:51189ms step_avg:43.94ms
step:1166/2155 train_time:51249ms step_avg:43.95ms
step:1167/2155 train_time:51309ms step_avg:43.97ms
step:1168/2155 train_time:51369ms step_avg:43.98ms
step:1169/2155 train_time:51430ms step_avg:43.99ms
step:1170/2155 train_time:51490ms step_avg:44.01ms
step:1171/2155 train_time:51552ms step_avg:44.02ms
step:1172/2155 train_time:51611ms step_avg:44.04ms
step:1173/2155 train_time:51672ms step_avg:44.05ms
step:1174/2155 train_time:51732ms step_avg:44.06ms
step:1175/2155 train_time:51793ms step_avg:44.08ms
step:1176/2155 train_time:51853ms step_avg:44.09ms
step:1177/2155 train_time:51915ms step_avg:44.11ms
step:1178/2155 train_time:51975ms step_avg:44.12ms
step:1179/2155 train_time:52036ms step_avg:44.14ms
step:1180/2155 train_time:52096ms step_avg:44.15ms
step:1181/2155 train_time:52158ms step_avg:44.16ms
step:1182/2155 train_time:52217ms step_avg:44.18ms
step:1183/2155 train_time:52279ms step_avg:44.19ms
step:1184/2155 train_time:52338ms step_avg:44.20ms
step:1185/2155 train_time:52400ms step_avg:44.22ms
step:1186/2155 train_time:52459ms step_avg:44.23ms
step:1187/2155 train_time:52521ms step_avg:44.25ms
step:1188/2155 train_time:52581ms step_avg:44.26ms
step:1189/2155 train_time:52643ms step_avg:44.27ms
step:1190/2155 train_time:52702ms step_avg:44.29ms
step:1191/2155 train_time:52763ms step_avg:44.30ms
step:1192/2155 train_time:52823ms step_avg:44.31ms
step:1193/2155 train_time:52884ms step_avg:44.33ms
step:1194/2155 train_time:52943ms step_avg:44.34ms
step:1195/2155 train_time:53004ms step_avg:44.36ms
step:1196/2155 train_time:53064ms step_avg:44.37ms
step:1197/2155 train_time:53125ms step_avg:44.38ms
step:1198/2155 train_time:53184ms step_avg:44.39ms
step:1199/2155 train_time:53245ms step_avg:44.41ms
step:1200/2155 train_time:53304ms step_avg:44.42ms
step:1201/2155 train_time:53365ms step_avg:44.43ms
step:1202/2155 train_time:53424ms step_avg:44.45ms
step:1203/2155 train_time:53485ms step_avg:44.46ms
step:1204/2155 train_time:53544ms step_avg:44.47ms
step:1205/2155 train_time:53605ms step_avg:44.49ms
step:1206/2155 train_time:53665ms step_avg:44.50ms
step:1207/2155 train_time:53726ms step_avg:44.51ms
step:1208/2155 train_time:53785ms step_avg:44.52ms
step:1209/2155 train_time:53847ms step_avg:44.54ms
step:1210/2155 train_time:53907ms step_avg:44.55ms
step:1211/2155 train_time:53967ms step_avg:44.56ms
step:1212/2155 train_time:54026ms step_avg:44.58ms
step:1213/2155 train_time:54088ms step_avg:44.59ms
step:1214/2155 train_time:54147ms step_avg:44.60ms
step:1215/2155 train_time:54209ms step_avg:44.62ms
step:1216/2155 train_time:54268ms step_avg:44.63ms
step:1217/2155 train_time:54330ms step_avg:44.64ms
step:1218/2155 train_time:54390ms step_avg:44.65ms
step:1219/2155 train_time:54451ms step_avg:44.67ms
step:1220/2155 train_time:54510ms step_avg:44.68ms
step:1221/2155 train_time:54573ms step_avg:44.70ms
step:1222/2155 train_time:54633ms step_avg:44.71ms
step:1223/2155 train_time:54694ms step_avg:44.72ms
step:1224/2155 train_time:54754ms step_avg:44.73ms
step:1225/2155 train_time:54815ms step_avg:44.75ms
step:1226/2155 train_time:54875ms step_avg:44.76ms
step:1227/2155 train_time:54937ms step_avg:44.77ms
step:1228/2155 train_time:54996ms step_avg:44.79ms
step:1229/2155 train_time:55058ms step_avg:44.80ms
step:1230/2155 train_time:55117ms step_avg:44.81ms
step:1231/2155 train_time:55179ms step_avg:44.82ms
step:1232/2155 train_time:55238ms step_avg:44.84ms
step:1233/2155 train_time:55300ms step_avg:44.85ms
step:1234/2155 train_time:55359ms step_avg:44.86ms
step:1235/2155 train_time:55420ms step_avg:44.87ms
step:1236/2155 train_time:55480ms step_avg:44.89ms
step:1237/2155 train_time:55541ms step_avg:44.90ms
step:1238/2155 train_time:55601ms step_avg:44.91ms
step:1239/2155 train_time:55662ms step_avg:44.92ms
step:1240/2155 train_time:55721ms step_avg:44.94ms
step:1241/2155 train_time:55783ms step_avg:44.95ms
step:1242/2155 train_time:55842ms step_avg:44.96ms
step:1243/2155 train_time:55904ms step_avg:44.97ms
step:1244/2155 train_time:55963ms step_avg:44.99ms
step:1245/2155 train_time:56025ms step_avg:45.00ms
step:1246/2155 train_time:56084ms step_avg:45.01ms
step:1247/2155 train_time:56145ms step_avg:45.02ms
step:1248/2155 train_time:56204ms step_avg:45.04ms
step:1249/2155 train_time:56266ms step_avg:45.05ms
step:1250/2155 train_time:56325ms step_avg:45.06ms
step:1250/2155 val_loss:3.5947 train_time:56389ms step_avg:45.11ms
step:1251/2155 train_time:56410ms step_avg:45.09ms
step:1252/2155 train_time:56448ms step_avg:45.09ms
step:1253/2155 train_time:56513ms step_avg:45.10ms
step:1254/2155 train_time:56575ms step_avg:45.12ms
step:1255/2155 train_time:56637ms step_avg:45.13ms
step:1256/2155 train_time:56696ms step_avg:45.14ms
step:1257/2155 train_time:56757ms step_avg:45.15ms
step:1258/2155 train_time:56815ms step_avg:45.16ms
step:1259/2155 train_time:56876ms step_avg:45.18ms
step:1260/2155 train_time:56935ms step_avg:45.19ms
step:1261/2155 train_time:56995ms step_avg:45.20ms
step:1262/2155 train_time:57054ms step_avg:45.21ms
step:1263/2155 train_time:57115ms step_avg:45.22ms
step:1264/2155 train_time:57175ms step_avg:45.23ms
step:1265/2155 train_time:57236ms step_avg:45.25ms
step:1266/2155 train_time:57296ms step_avg:45.26ms
step:1267/2155 train_time:57359ms step_avg:45.27ms
step:1268/2155 train_time:57420ms step_avg:45.28ms
step:1269/2155 train_time:57483ms step_avg:45.30ms
step:1270/2155 train_time:57543ms step_avg:45.31ms
step:1271/2155 train_time:57606ms step_avg:45.32ms
step:1272/2155 train_time:57665ms step_avg:45.33ms
step:1273/2155 train_time:57725ms step_avg:45.35ms
step:1274/2155 train_time:57785ms step_avg:45.36ms
step:1275/2155 train_time:57845ms step_avg:45.37ms
step:1276/2155 train_time:57905ms step_avg:45.38ms
step:1277/2155 train_time:57965ms step_avg:45.39ms
step:1278/2155 train_time:58024ms step_avg:45.40ms
step:1279/2155 train_time:58085ms step_avg:45.41ms
step:1280/2155 train_time:58144ms step_avg:45.42ms
step:1281/2155 train_time:58205ms step_avg:45.44ms
step:1282/2155 train_time:58265ms step_avg:45.45ms
step:1283/2155 train_time:58326ms step_avg:45.46ms
step:1284/2155 train_time:58386ms step_avg:45.47ms
step:1285/2155 train_time:58448ms step_avg:45.48ms
step:1286/2155 train_time:58508ms step_avg:45.50ms
step:1287/2155 train_time:58570ms step_avg:45.51ms
step:1288/2155 train_time:58630ms step_avg:45.52ms
step:1289/2155 train_time:58692ms step_avg:45.53ms
step:1290/2155 train_time:58752ms step_avg:45.54ms
step:1291/2155 train_time:58813ms step_avg:45.56ms
step:1292/2155 train_time:58872ms step_avg:45.57ms
step:1293/2155 train_time:58933ms step_avg:45.58ms
step:1294/2155 train_time:58993ms step_avg:45.59ms
step:1295/2155 train_time:59054ms step_avg:45.60ms
step:1296/2155 train_time:59113ms step_avg:45.61ms
step:1297/2155 train_time:59174ms step_avg:45.62ms
step:1298/2155 train_time:59234ms step_avg:45.63ms
step:1299/2155 train_time:59296ms step_avg:45.65ms
step:1300/2155 train_time:59356ms step_avg:45.66ms
step:1301/2155 train_time:59418ms step_avg:45.67ms
step:1302/2155 train_time:59478ms step_avg:45.68ms
step:1303/2155 train_time:59540ms step_avg:45.69ms
step:1304/2155 train_time:59600ms step_avg:45.71ms
step:1305/2155 train_time:59662ms step_avg:45.72ms
step:1306/2155 train_time:59721ms step_avg:45.73ms
step:1307/2155 train_time:59781ms step_avg:45.74ms
step:1308/2155 train_time:59840ms step_avg:45.75ms
step:1309/2155 train_time:59901ms step_avg:45.76ms
step:1310/2155 train_time:59960ms step_avg:45.77ms
step:1311/2155 train_time:60021ms step_avg:45.78ms
step:1312/2155 train_time:60080ms step_avg:45.79ms
step:1313/2155 train_time:60141ms step_avg:45.80ms
step:1314/2155 train_time:60200ms step_avg:45.81ms
step:1315/2155 train_time:60261ms step_avg:45.83ms
step:1316/2155 train_time:60321ms step_avg:45.84ms
step:1317/2155 train_time:60382ms step_avg:45.85ms
step:1318/2155 train_time:60441ms step_avg:45.86ms
step:1319/2155 train_time:60503ms step_avg:45.87ms
step:1320/2155 train_time:60562ms step_avg:45.88ms
step:1321/2155 train_time:60623ms step_avg:45.89ms
step:1322/2155 train_time:60682ms step_avg:45.90ms
step:1323/2155 train_time:60744ms step_avg:45.91ms
step:1324/2155 train_time:60803ms step_avg:45.92ms
step:1325/2155 train_time:60864ms step_avg:45.94ms
step:1326/2155 train_time:60923ms step_avg:45.94ms
step:1327/2155 train_time:60984ms step_avg:45.96ms
step:1328/2155 train_time:61043ms step_avg:45.97ms
step:1329/2155 train_time:61104ms step_avg:45.98ms
step:1330/2155 train_time:61164ms step_avg:45.99ms
step:1331/2155 train_time:61225ms step_avg:46.00ms
step:1332/2155 train_time:61284ms step_avg:46.01ms
step:1333/2155 train_time:61345ms step_avg:46.02ms
step:1334/2155 train_time:61405ms step_avg:46.03ms
step:1335/2155 train_time:61466ms step_avg:46.04ms
step:1336/2155 train_time:61526ms step_avg:46.05ms
step:1337/2155 train_time:61587ms step_avg:46.06ms
step:1338/2155 train_time:61647ms step_avg:46.07ms
step:1339/2155 train_time:61708ms step_avg:46.09ms
step:1340/2155 train_time:61768ms step_avg:46.10ms
step:1341/2155 train_time:61830ms step_avg:46.11ms
step:1342/2155 train_time:61890ms step_avg:46.12ms
step:1343/2155 train_time:61951ms step_avg:46.13ms
step:1344/2155 train_time:62010ms step_avg:46.14ms
step:1345/2155 train_time:62072ms step_avg:46.15ms
step:1346/2155 train_time:62131ms step_avg:46.16ms
step:1347/2155 train_time:62193ms step_avg:46.17ms
step:1348/2155 train_time:62253ms step_avg:46.18ms
step:1349/2155 train_time:62314ms step_avg:46.19ms
step:1350/2155 train_time:62374ms step_avg:46.20ms
step:1351/2155 train_time:62435ms step_avg:46.21ms
step:1352/2155 train_time:62495ms step_avg:46.22ms
step:1353/2155 train_time:62557ms step_avg:46.24ms
step:1354/2155 train_time:62617ms step_avg:46.25ms
step:1355/2155 train_time:62679ms step_avg:46.26ms
step:1356/2155 train_time:62739ms step_avg:46.27ms
step:1357/2155 train_time:62800ms step_avg:46.28ms
step:1358/2155 train_time:62859ms step_avg:46.29ms
step:1359/2155 train_time:62920ms step_avg:46.30ms
step:1360/2155 train_time:62979ms step_avg:46.31ms
step:1361/2155 train_time:63040ms step_avg:46.32ms
step:1362/2155 train_time:63099ms step_avg:46.33ms
step:1363/2155 train_time:63160ms step_avg:46.34ms
step:1364/2155 train_time:63219ms step_avg:46.35ms
step:1365/2155 train_time:63281ms step_avg:46.36ms
step:1366/2155 train_time:63341ms step_avg:46.37ms
step:1367/2155 train_time:63402ms step_avg:46.38ms
step:1368/2155 train_time:63462ms step_avg:46.39ms
step:1369/2155 train_time:63523ms step_avg:46.40ms
step:1370/2155 train_time:63583ms step_avg:46.41ms
step:1371/2155 train_time:63644ms step_avg:46.42ms
step:1372/2155 train_time:63703ms step_avg:46.43ms
step:1373/2155 train_time:63764ms step_avg:46.44ms
step:1374/2155 train_time:63823ms step_avg:46.45ms
step:1375/2155 train_time:63883ms step_avg:46.46ms
step:1376/2155 train_time:63943ms step_avg:46.47ms
step:1377/2155 train_time:64004ms step_avg:46.48ms
step:1378/2155 train_time:64064ms step_avg:46.49ms
step:1379/2155 train_time:64124ms step_avg:46.50ms
step:1380/2155 train_time:64183ms step_avg:46.51ms
step:1381/2155 train_time:64245ms step_avg:46.52ms
step:1382/2155 train_time:64305ms step_avg:46.53ms
step:1383/2155 train_time:64365ms step_avg:46.54ms
step:1384/2155 train_time:64425ms step_avg:46.55ms
step:1385/2155 train_time:64486ms step_avg:46.56ms
step:1386/2155 train_time:64546ms step_avg:46.57ms
step:1387/2155 train_time:64608ms step_avg:46.58ms
step:1388/2155 train_time:64667ms step_avg:46.59ms
step:1389/2155 train_time:64729ms step_avg:46.60ms
step:1390/2155 train_time:64790ms step_avg:46.61ms
step:1391/2155 train_time:64851ms step_avg:46.62ms
step:1392/2155 train_time:64911ms step_avg:46.63ms
step:1393/2155 train_time:64973ms step_avg:46.64ms
step:1394/2155 train_time:65033ms step_avg:46.65ms
step:1395/2155 train_time:65095ms step_avg:46.66ms
step:1396/2155 train_time:65155ms step_avg:46.67ms
step:1397/2155 train_time:65216ms step_avg:46.68ms
step:1398/2155 train_time:65276ms step_avg:46.69ms
step:1399/2155 train_time:65338ms step_avg:46.70ms
step:1400/2155 train_time:65398ms step_avg:46.71ms
step:1401/2155 train_time:65461ms step_avg:46.72ms
step:1402/2155 train_time:65520ms step_avg:46.73ms
step:1403/2155 train_time:65582ms step_avg:46.74ms
step:1404/2155 train_time:65641ms step_avg:46.75ms
step:1405/2155 train_time:65702ms step_avg:46.76ms
step:1406/2155 train_time:65761ms step_avg:46.77ms
step:1407/2155 train_time:65821ms step_avg:46.78ms
step:1408/2155 train_time:65881ms step_avg:46.79ms
step:1409/2155 train_time:65942ms step_avg:46.80ms
step:1410/2155 train_time:66001ms step_avg:46.81ms
step:1411/2155 train_time:66063ms step_avg:46.82ms
step:1412/2155 train_time:66150ms step_avg:46.85ms
step:1413/2155 train_time:66240ms step_avg:46.88ms
step:1414/2155 train_time:66330ms step_avg:46.91ms
step:1415/2155 train_time:66419ms step_avg:46.94ms
step:1416/2155 train_time:66508ms step_avg:46.97ms
step:1417/2155 train_time:66597ms step_avg:47.00ms
step:1418/2155 train_time:66685ms step_avg:47.03ms
step:1419/2155 train_time:66774ms step_avg:47.06ms
step:1420/2155 train_time:66862ms step_avg:47.09ms
step:1421/2155 train_time:66952ms step_avg:47.12ms
step:1422/2155 train_time:67040ms step_avg:47.14ms
step:1423/2155 train_time:67130ms step_avg:47.17ms
step:1424/2155 train_time:67217ms step_avg:47.20ms
step:1425/2155 train_time:67307ms step_avg:47.23ms
step:1426/2155 train_time:67395ms step_avg:47.26ms
step:1427/2155 train_time:67485ms step_avg:47.29ms
step:1428/2155 train_time:67572ms step_avg:47.32ms
step:1429/2155 train_time:67662ms step_avg:47.35ms
step:1430/2155 train_time:67750ms step_avg:47.38ms
step:1431/2155 train_time:67839ms step_avg:47.41ms
step:1432/2155 train_time:67927ms step_avg:47.43ms
step:1433/2155 train_time:68016ms step_avg:47.46ms
step:1434/2155 train_time:68103ms step_avg:47.49ms
step:1435/2155 train_time:68194ms step_avg:47.52ms
step:1436/2155 train_time:68282ms step_avg:47.55ms
step:1437/2155 train_time:68372ms step_avg:47.58ms
step:1438/2155 train_time:68460ms step_avg:47.61ms
step:1439/2155 train_time:68551ms step_avg:47.64ms
step:1440/2155 train_time:68639ms step_avg:47.67ms
step:1441/2155 train_time:68730ms step_avg:47.70ms
step:1442/2155 train_time:68818ms step_avg:47.72ms
step:1443/2155 train_time:68907ms step_avg:47.75ms
step:1444/2155 train_time:68995ms step_avg:47.78ms
step:1445/2155 train_time:69084ms step_avg:47.81ms
step:1446/2155 train_time:69173ms step_avg:47.84ms
step:1447/2155 train_time:69262ms step_avg:47.87ms
step:1448/2155 train_time:69350ms step_avg:47.89ms
step:1449/2155 train_time:69439ms step_avg:47.92ms
step:1450/2155 train_time:69527ms step_avg:47.95ms
step:1451/2155 train_time:69617ms step_avg:47.98ms
step:1452/2155 train_time:69705ms step_avg:48.01ms
step:1453/2155 train_time:69794ms step_avg:48.03ms
step:1454/2155 train_time:69882ms step_avg:48.06ms
step:1455/2155 train_time:69970ms step_avg:48.09ms
step:1456/2155 train_time:70058ms step_avg:48.12ms
step:1457/2155 train_time:70147ms step_avg:48.14ms
step:1458/2155 train_time:70235ms step_avg:48.17ms
step:1459/2155 train_time:70325ms step_avg:48.20ms
step:1460/2155 train_time:70412ms step_avg:48.23ms
step:1461/2155 train_time:70501ms step_avg:48.26ms
step:1462/2155 train_time:70591ms step_avg:48.28ms
step:1463/2155 train_time:70680ms step_avg:48.31ms
step:1464/2155 train_time:70768ms step_avg:48.34ms
step:1465/2155 train_time:70857ms step_avg:48.37ms
step:1466/2155 train_time:70945ms step_avg:48.39ms
step:1467/2155 train_time:71033ms step_avg:48.42ms
step:1468/2155 train_time:71122ms step_avg:48.45ms
step:1469/2155 train_time:71211ms step_avg:48.48ms
step:1470/2155 train_time:71300ms step_avg:48.50ms
step:1471/2155 train_time:71389ms step_avg:48.53ms
step:1472/2155 train_time:71477ms step_avg:48.56ms
step:1473/2155 train_time:71567ms step_avg:48.59ms
step:1474/2155 train_time:71654ms step_avg:48.61ms
step:1475/2155 train_time:71743ms step_avg:48.64ms
step:1476/2155 train_time:71831ms step_avg:48.67ms
step:1477/2155 train_time:71921ms step_avg:48.69ms
step:1478/2155 train_time:72009ms step_avg:48.72ms
step:1479/2155 train_time:72098ms step_avg:48.75ms
step:1480/2155 train_time:72186ms step_avg:48.77ms
step:1481/2155 train_time:72275ms step_avg:48.80ms
step:1482/2155 train_time:72363ms step_avg:48.83ms
step:1483/2155 train_time:72453ms step_avg:48.86ms
step:1484/2155 train_time:72542ms step_avg:48.88ms
step:1485/2155 train_time:72632ms step_avg:48.91ms
step:1486/2155 train_time:72719ms step_avg:48.94ms
step:1487/2155 train_time:72808ms step_avg:48.96ms
step:1488/2155 train_time:72896ms step_avg:48.99ms
step:1489/2155 train_time:72985ms step_avg:49.02ms
step:1490/2155 train_time:73073ms step_avg:49.04ms
step:1491/2155 train_time:73163ms step_avg:49.07ms
step:1492/2155 train_time:73250ms step_avg:49.10ms
step:1493/2155 train_time:73340ms step_avg:49.12ms
step:1494/2155 train_time:73428ms step_avg:49.15ms
step:1495/2155 train_time:73518ms step_avg:49.18ms
step:1496/2155 train_time:73606ms step_avg:49.20ms
step:1497/2155 train_time:73695ms step_avg:49.23ms
step:1498/2155 train_time:73784ms step_avg:49.25ms
step:1499/2155 train_time:73874ms step_avg:49.28ms
step:1500/2155 train_time:73961ms step_avg:49.31ms
step:1500/2155 val_loss:3.4912 train_time:74053ms step_avg:49.37ms
step:1501/2155 train_time:74075ms step_avg:49.35ms
step:1502/2155 train_time:74138ms step_avg:49.36ms
step:1503/2155 train_time:74236ms step_avg:49.39ms
step:1504/2155 train_time:74326ms step_avg:49.42ms
step:1505/2155 train_time:74414ms step_avg:49.44ms
step:1506/2155 train_time:74499ms step_avg:49.47ms
step:1507/2155 train_time:74586ms step_avg:49.49ms
step:1508/2155 train_time:74672ms step_avg:49.52ms
step:1509/2155 train_time:74759ms step_avg:49.54ms
step:1510/2155 train_time:74845ms step_avg:49.57ms
step:1511/2155 train_time:74936ms step_avg:49.59ms
step:1512/2155 train_time:75033ms step_avg:49.62ms
step:1513/2155 train_time:75123ms step_avg:49.65ms
step:1514/2155 train_time:75212ms step_avg:49.68ms
step:1515/2155 train_time:75301ms step_avg:49.70ms
step:1516/2155 train_time:75388ms step_avg:49.73ms
step:1517/2155 train_time:75475ms step_avg:49.75ms
step:1518/2155 train_time:75561ms step_avg:49.78ms
step:1519/2155 train_time:75648ms step_avg:49.80ms
step:1520/2155 train_time:75734ms step_avg:49.82ms
step:1521/2155 train_time:75821ms step_avg:49.85ms
step:1522/2155 train_time:75909ms step_avg:49.87ms
step:1523/2155 train_time:76000ms step_avg:49.90ms
step:1524/2155 train_time:76090ms step_avg:49.93ms
step:1525/2155 train_time:76180ms step_avg:49.95ms
step:1526/2155 train_time:76268ms step_avg:49.98ms
step:1527/2155 train_time:76356ms step_avg:50.00ms
step:1528/2155 train_time:76443ms step_avg:50.03ms
step:1529/2155 train_time:76531ms step_avg:50.05ms
step:1530/2155 train_time:76617ms step_avg:50.08ms
step:1531/2155 train_time:76704ms step_avg:50.10ms
step:1532/2155 train_time:76791ms step_avg:50.12ms
step:1533/2155 train_time:76879ms step_avg:50.15ms
step:1534/2155 train_time:76968ms step_avg:50.17ms
step:1535/2155 train_time:77058ms step_avg:50.20ms
step:1536/2155 train_time:77147ms step_avg:50.23ms
step:1537/2155 train_time:77236ms step_avg:50.25ms
step:1538/2155 train_time:77324ms step_avg:50.28ms
step:1539/2155 train_time:77413ms step_avg:50.30ms
step:1540/2155 train_time:77500ms step_avg:50.32ms
step:1541/2155 train_time:77589ms step_avg:50.35ms
step:1542/2155 train_time:77674ms step_avg:50.37ms
step:1543/2155 train_time:77762ms step_avg:50.40ms
step:1544/2155 train_time:77849ms step_avg:50.42ms
step:1545/2155 train_time:77937ms step_avg:50.44ms
step:1546/2155 train_time:78026ms step_avg:50.47ms
step:1547/2155 train_time:78116ms step_avg:50.50ms
step:1548/2155 train_time:78204ms step_avg:50.52ms
step:1549/2155 train_time:78293ms step_avg:50.54ms
step:1550/2155 train_time:78381ms step_avg:50.57ms
step:1551/2155 train_time:78469ms step_avg:50.59ms
step:1552/2155 train_time:78556ms step_avg:50.62ms
step:1553/2155 train_time:78644ms step_avg:50.64ms
step:1554/2155 train_time:78731ms step_avg:50.66ms
step:1555/2155 train_time:78819ms step_avg:50.69ms
step:1556/2155 train_time:78907ms step_avg:50.71ms
step:1557/2155 train_time:78997ms step_avg:50.74ms
step:1558/2155 train_time:79086ms step_avg:50.76ms
step:1559/2155 train_time:79175ms step_avg:50.79ms
step:1560/2155 train_time:79263ms step_avg:50.81ms
step:1561/2155 train_time:79352ms step_avg:50.83ms
step:1562/2155 train_time:79439ms step_avg:50.86ms
step:1563/2155 train_time:79527ms step_avg:50.88ms
step:1564/2155 train_time:79614ms step_avg:50.90ms
step:1565/2155 train_time:79703ms step_avg:50.93ms
step:1566/2155 train_time:79790ms step_avg:50.95ms
step:1567/2155 train_time:79879ms step_avg:50.98ms
step:1568/2155 train_time:79966ms step_avg:51.00ms
step:1569/2155 train_time:80056ms step_avg:51.02ms
step:1570/2155 train_time:80145ms step_avg:51.05ms
step:1571/2155 train_time:80235ms step_avg:51.07ms
step:1572/2155 train_time:80322ms step_avg:51.10ms
step:1573/2155 train_time:80411ms step_avg:51.12ms
step:1574/2155 train_time:80498ms step_avg:51.14ms
step:1575/2155 train_time:80588ms step_avg:51.17ms
step:1576/2155 train_time:80675ms step_avg:51.19ms
step:1577/2155 train_time:80763ms step_avg:51.21ms
step:1578/2155 train_time:80851ms step_avg:51.24ms
step:1579/2155 train_time:80939ms step_avg:51.26ms
step:1580/2155 train_time:81027ms step_avg:51.28ms
step:1581/2155 train_time:81116ms step_avg:51.31ms
step:1582/2155 train_time:81203ms step_avg:51.33ms
step:1583/2155 train_time:81294ms step_avg:51.35ms
step:1584/2155 train_time:81382ms step_avg:51.38ms
step:1585/2155 train_time:81471ms step_avg:51.40ms
step:1586/2155 train_time:81558ms step_avg:51.42ms
step:1587/2155 train_time:81647ms step_avg:51.45ms
step:1588/2155 train_time:81734ms step_avg:51.47ms
step:1589/2155 train_time:81823ms step_avg:51.49ms
step:1590/2155 train_time:81910ms step_avg:51.52ms
step:1591/2155 train_time:81998ms step_avg:51.54ms
step:1592/2155 train_time:82086ms step_avg:51.56ms
step:1593/2155 train_time:82175ms step_avg:51.59ms
step:1594/2155 train_time:82264ms step_avg:51.61ms
step:1595/2155 train_time:82354ms step_avg:51.63ms
step:1596/2155 train_time:82441ms step_avg:51.65ms
step:1597/2155 train_time:82531ms step_avg:51.68ms
step:1598/2155 train_time:82618ms step_avg:51.70ms
step:1599/2155 train_time:82707ms step_avg:51.72ms
step:1600/2155 train_time:82794ms step_avg:51.75ms
step:1601/2155 train_time:82882ms step_avg:51.77ms
step:1602/2155 train_time:82970ms step_avg:51.79ms
step:1603/2155 train_time:83059ms step_avg:51.81ms
step:1604/2155 train_time:83147ms step_avg:51.84ms
step:1605/2155 train_time:83235ms step_avg:51.86ms
step:1606/2155 train_time:83322ms step_avg:51.88ms
step:1607/2155 train_time:83412ms step_avg:51.91ms
step:1608/2155 train_time:83499ms step_avg:51.93ms
step:1609/2155 train_time:83589ms step_avg:51.95ms
step:1610/2155 train_time:83676ms step_avg:51.97ms
step:1611/2155 train_time:83765ms step_avg:52.00ms
step:1612/2155 train_time:83852ms step_avg:52.02ms
step:1613/2155 train_time:83941ms step_avg:52.04ms
step:1614/2155 train_time:84029ms step_avg:52.06ms
step:1615/2155 train_time:84117ms step_avg:52.09ms
step:1616/2155 train_time:84205ms step_avg:52.11ms
step:1617/2155 train_time:84294ms step_avg:52.13ms
step:1618/2155 train_time:84383ms step_avg:52.15ms
step:1619/2155 train_time:84472ms step_avg:52.18ms
step:1620/2155 train_time:84561ms step_avg:52.20ms
step:1621/2155 train_time:84650ms step_avg:52.22ms
step:1622/2155 train_time:84737ms step_avg:52.24ms
step:1623/2155 train_time:84826ms step_avg:52.26ms
step:1624/2155 train_time:84913ms step_avg:52.29ms
step:1625/2155 train_time:85003ms step_avg:52.31ms
step:1626/2155 train_time:85090ms step_avg:52.33ms
step:1627/2155 train_time:85179ms step_avg:52.35ms
step:1628/2155 train_time:85266ms step_avg:52.37ms
step:1629/2155 train_time:85356ms step_avg:52.40ms
step:1630/2155 train_time:85444ms step_avg:52.42ms
step:1631/2155 train_time:85533ms step_avg:52.44ms
step:1632/2155 train_time:85620ms step_avg:52.46ms
step:1633/2155 train_time:85709ms step_avg:52.49ms
step:1634/2155 train_time:85795ms step_avg:52.51ms
step:1635/2155 train_time:85884ms step_avg:52.53ms
step:1636/2155 train_time:85971ms step_avg:52.55ms
step:1637/2155 train_time:86060ms step_avg:52.57ms
step:1638/2155 train_time:86147ms step_avg:52.59ms
step:1639/2155 train_time:86236ms step_avg:52.61ms
step:1640/2155 train_time:86324ms step_avg:52.64ms
step:1641/2155 train_time:86412ms step_avg:52.66ms
step:1642/2155 train_time:86501ms step_avg:52.68ms
step:1643/2155 train_time:86590ms step_avg:52.70ms
step:1644/2155 train_time:86677ms step_avg:52.72ms
step:1645/2155 train_time:86766ms step_avg:52.75ms
step:1646/2155 train_time:86854ms step_avg:52.77ms
step:1647/2155 train_time:86942ms step_avg:52.79ms
step:1648/2155 train_time:87030ms step_avg:52.81ms
step:1649/2155 train_time:87118ms step_avg:52.83ms
step:1650/2155 train_time:87205ms step_avg:52.85ms
step:1651/2155 train_time:87295ms step_avg:52.87ms
step:1652/2155 train_time:87382ms step_avg:52.89ms
step:1653/2155 train_time:87471ms step_avg:52.92ms
step:1654/2155 train_time:87559ms step_avg:52.94ms
step:1655/2155 train_time:87647ms step_avg:52.96ms
step:1656/2155 train_time:87736ms step_avg:52.98ms
step:1657/2155 train_time:87825ms step_avg:53.00ms
step:1658/2155 train_time:87912ms step_avg:53.02ms
step:1659/2155 train_time:88001ms step_avg:53.04ms
step:1660/2155 train_time:88089ms step_avg:53.07ms
step:1661/2155 train_time:88177ms step_avg:53.09ms
step:1662/2155 train_time:88264ms step_avg:53.11ms
step:1663/2155 train_time:88354ms step_avg:53.13ms
step:1664/2155 train_time:88441ms step_avg:53.15ms
step:1665/2155 train_time:88531ms step_avg:53.17ms
step:1666/2155 train_time:88619ms step_avg:53.19ms
step:1667/2155 train_time:88708ms step_avg:53.21ms
step:1668/2155 train_time:88796ms step_avg:53.23ms
step:1669/2155 train_time:88885ms step_avg:53.26ms
step:1670/2155 train_time:88972ms step_avg:53.28ms
step:1671/2155 train_time:89061ms step_avg:53.30ms
step:1672/2155 train_time:89148ms step_avg:53.32ms
step:1673/2155 train_time:89237ms step_avg:53.34ms
step:1674/2155 train_time:89325ms step_avg:53.36ms
step:1675/2155 train_time:89415ms step_avg:53.38ms
step:1676/2155 train_time:89503ms step_avg:53.40ms
step:1677/2155 train_time:89592ms step_avg:53.42ms
step:1678/2155 train_time:89680ms step_avg:53.44ms
step:1679/2155 train_time:89769ms step_avg:53.47ms
step:1680/2155 train_time:89857ms step_avg:53.49ms
step:1681/2155 train_time:89945ms step_avg:53.51ms
step:1682/2155 train_time:90032ms step_avg:53.53ms
step:1683/2155 train_time:90121ms step_avg:53.55ms
step:1684/2155 train_time:90209ms step_avg:53.57ms
step:1685/2155 train_time:90298ms step_avg:53.59ms
step:1686/2155 train_time:90386ms step_avg:53.61ms
step:1687/2155 train_time:90475ms step_avg:53.63ms
step:1688/2155 train_time:90562ms step_avg:53.65ms
step:1689/2155 train_time:90651ms step_avg:53.67ms
step:1690/2155 train_time:90739ms step_avg:53.69ms
step:1691/2155 train_time:90828ms step_avg:53.71ms
step:1692/2155 train_time:90914ms step_avg:53.73ms
step:1693/2155 train_time:91004ms step_avg:53.75ms
step:1694/2155 train_time:91091ms step_avg:53.77ms
step:1695/2155 train_time:91179ms step_avg:53.79ms
step:1696/2155 train_time:91267ms step_avg:53.81ms
step:1697/2155 train_time:91356ms step_avg:53.83ms
step:1698/2155 train_time:91444ms step_avg:53.85ms
step:1699/2155 train_time:91534ms step_avg:53.87ms
step:1700/2155 train_time:91622ms step_avg:53.90ms
step:1701/2155 train_time:91711ms step_avg:53.92ms
step:1702/2155 train_time:91799ms step_avg:53.94ms
step:1703/2155 train_time:91887ms step_avg:53.96ms
step:1704/2155 train_time:91974ms step_avg:53.98ms
step:1705/2155 train_time:92064ms step_avg:54.00ms
step:1706/2155 train_time:92152ms step_avg:54.02ms
step:1707/2155 train_time:92241ms step_avg:54.04ms
step:1708/2155 train_time:92329ms step_avg:54.06ms
step:1709/2155 train_time:92417ms step_avg:54.08ms
step:1710/2155 train_time:92504ms step_avg:54.10ms
step:1711/2155 train_time:92594ms step_avg:54.12ms
step:1712/2155 train_time:92682ms step_avg:54.14ms
step:1713/2155 train_time:92771ms step_avg:54.16ms
step:1714/2155 train_time:92858ms step_avg:54.18ms
step:1715/2155 train_time:92947ms step_avg:54.20ms
step:1716/2155 train_time:93035ms step_avg:54.22ms
step:1717/2155 train_time:93124ms step_avg:54.24ms
step:1718/2155 train_time:93212ms step_avg:54.26ms
step:1719/2155 train_time:93302ms step_avg:54.28ms
step:1720/2155 train_time:93390ms step_avg:54.30ms
step:1721/2155 train_time:93479ms step_avg:54.32ms
step:1722/2155 train_time:93567ms step_avg:54.34ms
step:1723/2155 train_time:93656ms step_avg:54.36ms
step:1724/2155 train_time:93743ms step_avg:54.38ms
step:1725/2155 train_time:93833ms step_avg:54.40ms
step:1726/2155 train_time:93920ms step_avg:54.41ms
step:1727/2155 train_time:94008ms step_avg:54.43ms
step:1728/2155 train_time:94096ms step_avg:54.45ms
step:1729/2155 train_time:94185ms step_avg:54.47ms
step:1730/2155 train_time:94273ms step_avg:54.49ms
step:1731/2155 train_time:94362ms step_avg:54.51ms
step:1732/2155 train_time:94450ms step_avg:54.53ms
step:1733/2155 train_time:94538ms step_avg:54.55ms
step:1734/2155 train_time:94626ms step_avg:54.57ms
step:1735/2155 train_time:94714ms step_avg:54.59ms
step:1736/2155 train_time:94802ms step_avg:54.61ms
step:1737/2155 train_time:94891ms step_avg:54.63ms
step:1738/2155 train_time:94978ms step_avg:54.65ms
step:1739/2155 train_time:95067ms step_avg:54.67ms
step:1740/2155 train_time:95156ms step_avg:54.69ms
step:1741/2155 train_time:95245ms step_avg:54.71ms
step:1742/2155 train_time:95333ms step_avg:54.73ms
step:1743/2155 train_time:95421ms step_avg:54.75ms
step:1744/2155 train_time:95508ms step_avg:54.76ms
step:1745/2155 train_time:95596ms step_avg:54.78ms
step:1746/2155 train_time:95684ms step_avg:54.80ms
step:1747/2155 train_time:95773ms step_avg:54.82ms
step:1748/2155 train_time:95861ms step_avg:54.84ms
step:1749/2155 train_time:95950ms step_avg:54.86ms
step:1750/2155 train_time:96037ms step_avg:54.88ms
step:1750/2155 val_loss:3.3924 train_time:96128ms step_avg:54.93ms
step:1751/2155 train_time:96150ms step_avg:54.91ms
step:1752/2155 train_time:96218ms step_avg:54.92ms
step:1753/2155 train_time:96310ms step_avg:54.94ms
step:1754/2155 train_time:96399ms step_avg:54.96ms
step:1755/2155 train_time:96488ms step_avg:54.98ms
step:1756/2155 train_time:96576ms step_avg:55.00ms
step:1757/2155 train_time:96663ms step_avg:55.02ms
step:1758/2155 train_time:96750ms step_avg:55.03ms
step:1759/2155 train_time:96839ms step_avg:55.05ms
step:1760/2155 train_time:96927ms step_avg:55.07ms
step:1761/2155 train_time:97015ms step_avg:55.09ms
step:1762/2155 train_time:97103ms step_avg:55.11ms
step:1763/2155 train_time:97195ms step_avg:55.13ms
step:1764/2155 train_time:97285ms step_avg:55.15ms
step:1765/2155 train_time:97375ms step_avg:55.17ms
step:1766/2155 train_time:97462ms step_avg:55.19ms
step:1767/2155 train_time:97551ms step_avg:55.21ms
step:1768/2155 train_time:97637ms step_avg:55.22ms
step:1769/2155 train_time:97726ms step_avg:55.24ms
step:1770/2155 train_time:97814ms step_avg:55.26ms
step:1771/2155 train_time:97902ms step_avg:55.28ms
step:1772/2155 train_time:97988ms step_avg:55.30ms
step:1773/2155 train_time:98077ms step_avg:55.32ms
step:1774/2155 train_time:98165ms step_avg:55.34ms
step:1775/2155 train_time:98255ms step_avg:55.36ms
step:1776/2155 train_time:98344ms step_avg:55.37ms
step:1777/2155 train_time:98434ms step_avg:55.39ms
step:1778/2155 train_time:98521ms step_avg:55.41ms
step:1779/2155 train_time:98611ms step_avg:55.43ms
step:1780/2155 train_time:98698ms step_avg:55.45ms
step:1781/2155 train_time:98787ms step_avg:55.47ms
step:1782/2155 train_time:98875ms step_avg:55.49ms
step:1783/2155 train_time:98963ms step_avg:55.50ms
step:1784/2155 train_time:99049ms step_avg:55.52ms
step:1785/2155 train_time:99139ms step_avg:55.54ms
step:1786/2155 train_time:99228ms step_avg:55.56ms
step:1787/2155 train_time:99318ms step_avg:55.58ms
step:1788/2155 train_time:99406ms step_avg:55.60ms
step:1789/2155 train_time:99495ms step_avg:55.61ms
step:1790/2155 train_time:99582ms step_avg:55.63ms
step:1791/2155 train_time:99671ms step_avg:55.65ms
step:1792/2155 train_time:99759ms step_avg:55.67ms
step:1793/2155 train_time:99847ms step_avg:55.69ms
step:1794/2155 train_time:99935ms step_avg:55.70ms
step:1795/2155 train_time:100024ms step_avg:55.72ms
step:1796/2155 train_time:100112ms step_avg:55.74ms
step:1797/2155 train_time:100202ms step_avg:55.76ms
step:1798/2155 train_time:100290ms step_avg:55.78ms
step:1799/2155 train_time:100381ms step_avg:55.80ms
step:1800/2155 train_time:100468ms step_avg:55.82ms
step:1801/2155 train_time:100557ms step_avg:55.83ms
step:1802/2155 train_time:100645ms step_avg:55.85ms
step:1803/2155 train_time:100734ms step_avg:55.87ms
step:1804/2155 train_time:100821ms step_avg:55.89ms
step:1805/2155 train_time:100909ms step_avg:55.91ms
step:1806/2155 train_time:100997ms step_avg:55.92ms
step:1807/2155 train_time:101085ms step_avg:55.94ms
step:1808/2155 train_time:101174ms step_avg:55.96ms
step:1809/2155 train_time:101264ms step_avg:55.98ms
step:1810/2155 train_time:101352ms step_avg:56.00ms
step:1811/2155 train_time:101442ms step_avg:56.01ms
step:1812/2155 train_time:101530ms step_avg:56.03ms
step:1813/2155 train_time:101618ms step_avg:56.05ms
step:1814/2155 train_time:101706ms step_avg:56.07ms
step:1815/2155 train_time:101796ms step_avg:56.09ms
step:1816/2155 train_time:101884ms step_avg:56.10ms
step:1817/2155 train_time:101973ms step_avg:56.12ms
step:1818/2155 train_time:102061ms step_avg:56.14ms
step:1819/2155 train_time:102150ms step_avg:56.16ms
step:1820/2155 train_time:102238ms step_avg:56.17ms
step:1821/2155 train_time:102328ms step_avg:56.19ms
step:1822/2155 train_time:102416ms step_avg:56.21ms
step:1823/2155 train_time:102505ms step_avg:56.23ms
step:1824/2155 train_time:102592ms step_avg:56.25ms
step:1825/2155 train_time:102682ms step_avg:56.26ms
step:1826/2155 train_time:102769ms step_avg:56.28ms
step:1827/2155 train_time:102858ms step_avg:56.30ms
step:1828/2155 train_time:102945ms step_avg:56.32ms
step:1829/2155 train_time:103033ms step_avg:56.33ms
step:1830/2155 train_time:103121ms step_avg:56.35ms
step:1831/2155 train_time:103211ms step_avg:56.37ms
step:1832/2155 train_time:103300ms step_avg:56.39ms
step:1833/2155 train_time:103389ms step_avg:56.40ms
step:1834/2155 train_time:103476ms step_avg:56.42ms
step:1835/2155 train_time:103565ms step_avg:56.44ms
step:1836/2155 train_time:103652ms step_avg:56.46ms
step:1837/2155 train_time:103742ms step_avg:56.47ms
step:1838/2155 train_time:103830ms step_avg:56.49ms
step:1839/2155 train_time:103919ms step_avg:56.51ms
step:1840/2155 train_time:104007ms step_avg:56.53ms
step:1841/2155 train_time:104095ms step_avg:56.54ms
step:1842/2155 train_time:104183ms step_avg:56.56ms
step:1843/2155 train_time:104272ms step_avg:56.58ms
step:1844/2155 train_time:104360ms step_avg:56.59ms
step:1845/2155 train_time:104449ms step_avg:56.61ms
step:1846/2155 train_time:104537ms step_avg:56.63ms
step:1847/2155 train_time:104626ms step_avg:56.65ms
step:1848/2155 train_time:104713ms step_avg:56.66ms
step:1849/2155 train_time:104802ms step_avg:56.68ms
step:1850/2155 train_time:104890ms step_avg:56.70ms
step:1851/2155 train_time:104979ms step_avg:56.71ms
step:1852/2155 train_time:105066ms step_avg:56.73ms
step:1853/2155 train_time:105156ms step_avg:56.75ms
step:1854/2155 train_time:105244ms step_avg:56.77ms
step:1855/2155 train_time:105334ms step_avg:56.78ms
step:1856/2155 train_time:105421ms step_avg:56.80ms
step:1857/2155 train_time:105511ms step_avg:56.82ms
step:1858/2155 train_time:105598ms step_avg:56.83ms
step:1859/2155 train_time:105687ms step_avg:56.85ms
step:1860/2155 train_time:105776ms step_avg:56.87ms
step:1861/2155 train_time:105865ms step_avg:56.89ms
step:1862/2155 train_time:105952ms step_avg:56.90ms
step:1863/2155 train_time:106041ms step_avg:56.92ms
step:1864/2155 train_time:106129ms step_avg:56.94ms
step:1865/2155 train_time:106219ms step_avg:56.95ms
step:1866/2155 train_time:106307ms step_avg:56.97ms
step:1867/2155 train_time:106395ms step_avg:56.99ms
step:1868/2155 train_time:106483ms step_avg:57.00ms
step:1869/2155 train_time:106572ms step_avg:57.02ms
step:1870/2155 train_time:106660ms step_avg:57.04ms
step:1871/2155 train_time:106749ms step_avg:57.05ms
step:1872/2155 train_time:106837ms step_avg:57.07ms
step:1873/2155 train_time:106926ms step_avg:57.09ms
step:1874/2155 train_time:107014ms step_avg:57.10ms
step:1875/2155 train_time:107103ms step_avg:57.12ms
step:1876/2155 train_time:107191ms step_avg:57.14ms
step:1877/2155 train_time:107280ms step_avg:57.15ms
step:1878/2155 train_time:107366ms step_avg:57.17ms
step:1879/2155 train_time:107456ms step_avg:57.19ms
step:1880/2155 train_time:107544ms step_avg:57.20ms
step:1881/2155 train_time:107634ms step_avg:57.22ms
step:1882/2155 train_time:107721ms step_avg:57.24ms
step:1883/2155 train_time:107811ms step_avg:57.25ms
step:1884/2155 train_time:107898ms step_avg:57.27ms
step:1885/2155 train_time:107987ms step_avg:57.29ms
step:1886/2155 train_time:108076ms step_avg:57.30ms
step:1887/2155 train_time:108166ms step_avg:57.32ms
step:1888/2155 train_time:108254ms step_avg:57.34ms
step:1889/2155 train_time:108343ms step_avg:57.35ms
step:1890/2155 train_time:108431ms step_avg:57.37ms
step:1891/2155 train_time:108519ms step_avg:57.39ms
step:1892/2155 train_time:108607ms step_avg:57.40ms
step:1893/2155 train_time:108696ms step_avg:57.42ms
step:1894/2155 train_time:108783ms step_avg:57.44ms
step:1895/2155 train_time:108872ms step_avg:57.45ms
step:1896/2155 train_time:108960ms step_avg:57.47ms
step:1897/2155 train_time:109049ms step_avg:57.48ms
step:1898/2155 train_time:109137ms step_avg:57.50ms
step:1899/2155 train_time:109227ms step_avg:57.52ms
step:1900/2155 train_time:109315ms step_avg:57.53ms
step:1901/2155 train_time:109404ms step_avg:57.55ms
step:1902/2155 train_time:109492ms step_avg:57.57ms
step:1903/2155 train_time:109581ms step_avg:57.58ms
step:1904/2155 train_time:109669ms step_avg:57.60ms
step:1905/2155 train_time:109759ms step_avg:57.62ms
step:1906/2155 train_time:109845ms step_avg:57.63ms
step:1907/2155 train_time:109935ms step_avg:57.65ms
step:1908/2155 train_time:110022ms step_avg:57.66ms
step:1909/2155 train_time:110111ms step_avg:57.68ms
step:1910/2155 train_time:110199ms step_avg:57.70ms
step:1911/2155 train_time:110289ms step_avg:57.71ms
step:1912/2155 train_time:110376ms step_avg:57.73ms
step:1913/2155 train_time:110465ms step_avg:57.74ms
step:1914/2155 train_time:110553ms step_avg:57.76ms
step:1915/2155 train_time:110642ms step_avg:57.78ms
step:1916/2155 train_time:110730ms step_avg:57.79ms
step:1917/2155 train_time:110819ms step_avg:57.81ms
step:1918/2155 train_time:110906ms step_avg:57.82ms
step:1919/2155 train_time:110996ms step_avg:57.84ms
step:1920/2155 train_time:111084ms step_avg:57.86ms
step:1921/2155 train_time:111172ms step_avg:57.87ms
step:1922/2155 train_time:111261ms step_avg:57.89ms
step:1923/2155 train_time:111349ms step_avg:57.90ms
step:1924/2155 train_time:111437ms step_avg:57.92ms
step:1925/2155 train_time:111526ms step_avg:57.94ms
step:1926/2155 train_time:111614ms step_avg:57.95ms
step:1927/2155 train_time:111704ms step_avg:57.97ms
step:1928/2155 train_time:111792ms step_avg:57.98ms
step:1929/2155 train_time:111881ms step_avg:58.00ms
step:1930/2155 train_time:111968ms step_avg:58.01ms
step:1931/2155 train_time:112058ms step_avg:58.03ms
step:1932/2155 train_time:112146ms step_avg:58.05ms
step:1933/2155 train_time:112236ms step_avg:58.06ms
step:1934/2155 train_time:112324ms step_avg:58.08ms
step:1935/2155 train_time:112413ms step_avg:58.09ms
step:1936/2155 train_time:112501ms step_avg:58.11ms
step:1937/2155 train_time:112590ms step_avg:58.13ms
step:1938/2155 train_time:112678ms step_avg:58.14ms
step:1939/2155 train_time:112768ms step_avg:58.16ms
step:1940/2155 train_time:112856ms step_avg:58.17ms
step:1941/2155 train_time:112946ms step_avg:58.19ms
step:1942/2155 train_time:113033ms step_avg:58.20ms
step:1943/2155 train_time:113123ms step_avg:58.22ms
step:1944/2155 train_time:113211ms step_avg:58.24ms
step:1945/2155 train_time:113300ms step_avg:58.25ms
step:1946/2155 train_time:113387ms step_avg:58.27ms
step:1947/2155 train_time:113476ms step_avg:58.28ms
step:1948/2155 train_time:113563ms step_avg:58.30ms
step:1949/2155 train_time:113652ms step_avg:58.31ms
step:1950/2155 train_time:113739ms step_avg:58.33ms
step:1951/2155 train_time:113829ms step_avg:58.34ms
step:1952/2155 train_time:113917ms step_avg:58.36ms
step:1953/2155 train_time:114005ms step_avg:58.37ms
step:1954/2155 train_time:114094ms step_avg:58.39ms
step:1955/2155 train_time:114183ms step_avg:58.41ms
step:1956/2155 train_time:114271ms step_avg:58.42ms
step:1957/2155 train_time:114360ms step_avg:58.44ms
step:1958/2155 train_time:114447ms step_avg:58.45ms
step:1959/2155 train_time:114536ms step_avg:58.47ms
step:1960/2155 train_time:114624ms step_avg:58.48ms
step:1961/2155 train_time:114713ms step_avg:58.50ms
step:1962/2155 train_time:114800ms step_avg:58.51ms
step:1963/2155 train_time:114889ms step_avg:58.53ms
step:1964/2155 train_time:114977ms step_avg:58.54ms
step:1965/2155 train_time:115066ms step_avg:58.56ms
step:1966/2155 train_time:115154ms step_avg:58.57ms
step:1967/2155 train_time:115243ms step_avg:58.59ms
step:1968/2155 train_time:115331ms step_avg:58.60ms
step:1969/2155 train_time:115420ms step_avg:58.62ms
step:1970/2155 train_time:115508ms step_avg:58.63ms
step:1971/2155 train_time:115596ms step_avg:58.65ms
step:1972/2155 train_time:115684ms step_avg:58.66ms
step:1973/2155 train_time:115772ms step_avg:58.68ms
step:1974/2155 train_time:115861ms step_avg:58.69ms
step:1975/2155 train_time:115949ms step_avg:58.71ms
step:1976/2155 train_time:116037ms step_avg:58.72ms
step:1977/2155 train_time:116127ms step_avg:58.74ms
step:1978/2155 train_time:116215ms step_avg:58.75ms
step:1979/2155 train_time:116304ms step_avg:58.77ms
step:1980/2155 train_time:116391ms step_avg:58.78ms
step:1981/2155 train_time:116480ms step_avg:58.80ms
step:1982/2155 train_time:116567ms step_avg:58.81ms
step:1983/2155 train_time:116656ms step_avg:58.83ms
step:1984/2155 train_time:116744ms step_avg:58.84ms
step:1985/2155 train_time:116833ms step_avg:58.86ms
step:1986/2155 train_time:116922ms step_avg:58.87ms
step:1987/2155 train_time:117011ms step_avg:58.89ms
step:1988/2155 train_time:117099ms step_avg:58.90ms
step:1989/2155 train_time:117189ms step_avg:58.92ms
step:1990/2155 train_time:117276ms step_avg:58.93ms
step:1991/2155 train_time:117365ms step_avg:58.95ms
step:1992/2155 train_time:117453ms step_avg:58.96ms
step:1993/2155 train_time:117543ms step_avg:58.98ms
step:1994/2155 train_time:117630ms step_avg:58.99ms
step:1995/2155 train_time:117719ms step_avg:59.01ms
step:1996/2155 train_time:117806ms step_avg:59.02ms
step:1997/2155 train_time:117896ms step_avg:59.04ms
step:1998/2155 train_time:117984ms step_avg:59.05ms
step:1999/2155 train_time:118073ms step_avg:59.07ms
step:2000/2155 train_time:118161ms step_avg:59.08ms
step:2000/2155 val_loss:3.3140 train_time:118253ms step_avg:59.13ms
step:2001/2155 train_time:118275ms step_avg:59.11ms
step:2002/2155 train_time:118341ms step_avg:59.11ms
step:2003/2155 train_time:118434ms step_avg:59.13ms
step:2004/2155 train_time:118524ms step_avg:59.14ms
step:2005/2155 train_time:118613ms step_avg:59.16ms
step:2006/2155 train_time:118700ms step_avg:59.17ms
step:2007/2155 train_time:118788ms step_avg:59.19ms
step:2008/2155 train_time:118875ms step_avg:59.20ms
step:2009/2155 train_time:118963ms step_avg:59.21ms
step:2010/2155 train_time:119051ms step_avg:59.23ms
step:2011/2155 train_time:119138ms step_avg:59.24ms
step:2012/2155 train_time:119227ms step_avg:59.26ms
step:2013/2155 train_time:119318ms step_avg:59.27ms
step:2014/2155 train_time:119408ms step_avg:59.29ms
step:2015/2155 train_time:119500ms step_avg:59.31ms
step:2016/2155 train_time:119588ms step_avg:59.32ms
step:2017/2155 train_time:119677ms step_avg:59.33ms
step:2018/2155 train_time:119765ms step_avg:59.35ms
step:2019/2155 train_time:119854ms step_avg:59.36ms
step:2020/2155 train_time:119942ms step_avg:59.38ms
step:2021/2155 train_time:120030ms step_avg:59.39ms
step:2022/2155 train_time:120117ms step_avg:59.41ms
step:2023/2155 train_time:120207ms step_avg:59.42ms
step:2024/2155 train_time:120294ms step_avg:59.43ms
step:2025/2155 train_time:120385ms step_avg:59.45ms
step:2026/2155 train_time:120473ms step_avg:59.46ms
step:2027/2155 train_time:120563ms step_avg:59.48ms
step:2028/2155 train_time:120650ms step_avg:59.49ms
step:2029/2155 train_time:120739ms step_avg:59.51ms
step:2030/2155 train_time:120826ms step_avg:59.52ms
step:2031/2155 train_time:120914ms step_avg:59.53ms
step:2032/2155 train_time:121001ms step_avg:59.55ms
step:2033/2155 train_time:121090ms step_avg:59.56ms
step:2034/2155 train_time:121177ms step_avg:59.58ms
step:2035/2155 train_time:121266ms step_avg:59.59ms
step:2036/2155 train_time:121354ms step_avg:59.60ms
step:2037/2155 train_time:121443ms step_avg:59.62ms
step:2038/2155 train_time:121531ms step_avg:59.63ms
step:2039/2155 train_time:121620ms step_avg:59.65ms
step:2040/2155 train_time:121707ms step_avg:59.66ms
step:2041/2155 train_time:121796ms step_avg:59.67ms
step:2042/2155 train_time:121883ms step_avg:59.69ms
step:2043/2155 train_time:121972ms step_avg:59.70ms
step:2044/2155 train_time:122060ms step_avg:59.72ms
step:2045/2155 train_time:122149ms step_avg:59.73ms
step:2046/2155 train_time:122236ms step_avg:59.74ms
step:2047/2155 train_time:122326ms step_avg:59.76ms
step:2048/2155 train_time:122414ms step_avg:59.77ms
step:2049/2155 train_time:122504ms step_avg:59.79ms
step:2050/2155 train_time:122591ms step_avg:59.80ms
step:2051/2155 train_time:122681ms step_avg:59.82ms
step:2052/2155 train_time:122769ms step_avg:59.83ms
step:2053/2155 train_time:122857ms step_avg:59.84ms
step:2054/2155 train_time:122945ms step_avg:59.86ms
step:2055/2155 train_time:123034ms step_avg:59.87ms
step:2056/2155 train_time:123121ms step_avg:59.88ms
step:2057/2155 train_time:123211ms step_avg:59.90ms
step:2058/2155 train_time:123298ms step_avg:59.91ms
step:2059/2155 train_time:123389ms step_avg:59.93ms
step:2060/2155 train_time:123477ms step_avg:59.94ms
step:2061/2155 train_time:123566ms step_avg:59.95ms
step:2062/2155 train_time:123654ms step_avg:59.97ms
step:2063/2155 train_time:123742ms step_avg:59.98ms
step:2064/2155 train_time:123830ms step_avg:60.00ms
step:2065/2155 train_time:123919ms step_avg:60.01ms
step:2066/2155 train_time:124007ms step_avg:60.02ms
step:2067/2155 train_time:124096ms step_avg:60.04ms
step:2068/2155 train_time:124184ms step_avg:60.05ms
step:2069/2155 train_time:124274ms step_avg:60.06ms
step:2070/2155 train_time:124361ms step_avg:60.08ms
step:2071/2155 train_time:124451ms step_avg:60.09ms
step:2072/2155 train_time:124541ms step_avg:60.11ms
step:2073/2155 train_time:124630ms step_avg:60.12ms
step:2074/2155 train_time:124718ms step_avg:60.13ms
step:2075/2155 train_time:124808ms step_avg:60.15ms
step:2076/2155 train_time:124894ms step_avg:60.16ms
step:2077/2155 train_time:124984ms step_avg:60.18ms
step:2078/2155 train_time:125071ms step_avg:60.19ms
step:2079/2155 train_time:125160ms step_avg:60.20ms
step:2080/2155 train_time:125248ms step_avg:60.22ms
step:2081/2155 train_time:125337ms step_avg:60.23ms
step:2082/2155 train_time:125426ms step_avg:60.24ms
step:2083/2155 train_time:125515ms step_avg:60.26ms
step:2084/2155 train_time:125603ms step_avg:60.27ms
step:2085/2155 train_time:125693ms step_avg:60.28ms
step:2086/2155 train_time:125781ms step_avg:60.30ms
step:2087/2155 train_time:125870ms step_avg:60.31ms
step:2088/2155 train_time:125957ms step_avg:60.32ms
step:2089/2155 train_time:126047ms step_avg:60.34ms
step:2090/2155 train_time:126134ms step_avg:60.35ms
step:2091/2155 train_time:126223ms step_avg:60.37ms
step:2092/2155 train_time:126312ms step_avg:60.38ms
step:2093/2155 train_time:126401ms step_avg:60.39ms
step:2094/2155 train_time:126490ms step_avg:60.41ms
step:2095/2155 train_time:126579ms step_avg:60.42ms
step:2096/2155 train_time:126666ms step_avg:60.43ms
step:2097/2155 train_time:126755ms step_avg:60.45ms
step:2098/2155 train_time:126843ms step_avg:60.46ms
step:2099/2155 train_time:126931ms step_avg:60.47ms
step:2100/2155 train_time:127019ms step_avg:60.49ms
step:2101/2155 train_time:127108ms step_avg:60.50ms
step:2102/2155 train_time:127194ms step_avg:60.51ms
step:2103/2155 train_time:127283ms step_avg:60.52ms
step:2104/2155 train_time:127372ms step_avg:60.54ms
step:2105/2155 train_time:127461ms step_avg:60.55ms
step:2106/2155 train_time:127549ms step_avg:60.56ms
step:2107/2155 train_time:127639ms step_avg:60.58ms
step:2108/2155 train_time:127726ms step_avg:60.59ms
step:2109/2155 train_time:127815ms step_avg:60.60ms
step:2110/2155 train_time:127902ms step_avg:60.62ms
step:2111/2155 train_time:127992ms step_avg:60.63ms
step:2112/2155 train_time:128078ms step_avg:60.64ms
step:2113/2155 train_time:128168ms step_avg:60.66ms
step:2114/2155 train_time:128256ms step_avg:60.67ms
step:2115/2155 train_time:128345ms step_avg:60.68ms
step:2116/2155 train_time:128432ms step_avg:60.70ms
step:2117/2155 train_time:128522ms step_avg:60.71ms
step:2118/2155 train_time:128609ms step_avg:60.72ms
step:2119/2155 train_time:128699ms step_avg:60.74ms
step:2120/2155 train_time:128787ms step_avg:60.75ms
step:2121/2155 train_time:128876ms step_avg:60.76ms
step:2122/2155 train_time:128963ms step_avg:60.77ms
step:2123/2155 train_time:129053ms step_avg:60.79ms
step:2124/2155 train_time:129140ms step_avg:60.80ms
step:2125/2155 train_time:129230ms step_avg:60.81ms
step:2126/2155 train_time:129317ms step_avg:60.83ms
step:2127/2155 train_time:129407ms step_avg:60.84ms
step:2128/2155 train_time:129494ms step_avg:60.85ms
step:2129/2155 train_time:129583ms step_avg:60.87ms
step:2130/2155 train_time:129672ms step_avg:60.88ms
step:2131/2155 train_time:129761ms step_avg:60.89ms
step:2132/2155 train_time:129849ms step_avg:60.90ms
step:2133/2155 train_time:129938ms step_avg:60.92ms
step:2134/2155 train_time:130026ms step_avg:60.93ms
step:2135/2155 train_time:130115ms step_avg:60.94ms
step:2136/2155 train_time:130203ms step_avg:60.96ms
step:2137/2155 train_time:130292ms step_avg:60.97ms
step:2138/2155 train_time:130380ms step_avg:60.98ms
step:2139/2155 train_time:130469ms step_avg:61.00ms
step:2140/2155 train_time:130557ms step_avg:61.01ms
step:2141/2155 train_time:130646ms step_avg:61.02ms
step:2142/2155 train_time:130734ms step_avg:61.03ms
step:2143/2155 train_time:130824ms step_avg:61.05ms
step:2144/2155 train_time:130912ms step_avg:61.06ms
step:2145/2155 train_time:131001ms step_avg:61.07ms
step:2146/2155 train_time:131088ms step_avg:61.08ms
step:2147/2155 train_time:131177ms step_avg:61.10ms
step:2148/2155 train_time:131265ms step_avg:61.11ms
step:2149/2155 train_time:131354ms step_avg:61.12ms
step:2150/2155 train_time:131442ms step_avg:61.14ms
step:2151/2155 train_time:131531ms step_avg:61.15ms
step:2152/2155 train_time:131619ms step_avg:61.16ms
step:2153/2155 train_time:131709ms step_avg:61.17ms
step:2154/2155 train_time:131796ms step_avg:61.19ms
step:2155/2155 train_time:131884ms step_avg:61.20ms
step:2155/2155 val_loss:3.2782 train_time:131973ms step_avg:61.24ms
peak memory allocated: 29707 MiB reserved: 44676 MiB

import uuid
run_id = f"NorMuon Fixes and Optims 8xH100 - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2115  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 10 2025, 18:15:56) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 18:56:40 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   44C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   35C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   33C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   43C    P0            130W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   44C    P0            127W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   34C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   43C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   34C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           20346      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           20347      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           20348      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           20349      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           20350      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           20351      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           20352      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           20353      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           20347      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           20348      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           20349      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           20350      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           20351      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           20352      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           20353      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2155 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2155 train_time:94ms step_avg:94.04ms
step:2/2155 train_time:172ms step_avg:85.97ms
step:3/2155 train_time:195ms step_avg:64.94ms
step:4/2155 train_time:219ms step_avg:54.71ms
step:5/2155 train_time:241ms step_avg:48.17ms
step:6/2155 train_time:358ms step_avg:59.72ms
step:7/2155 train_time:381ms step_avg:54.48ms
step:8/2155 train_time:415ms step_avg:51.87ms
step:9/2155 train_time:447ms step_avg:49.67ms
step:10/2155 train_time:481ms step_avg:48.05ms
step:11/2155 train_time:513ms step_avg:46.66ms
step:12/2155 train_time:547ms step_avg:45.57ms
step:13/2155 train_time:579ms step_avg:44.56ms
step:14/2155 train_time:613ms step_avg:43.78ms
step:15/2155 train_time:645ms step_avg:43.03ms
step:16/2155 train_time:679ms step_avg:42.45ms
step:17/2155 train_time:711ms step_avg:41.85ms
step:18/2155 train_time:745ms step_avg:41.40ms
step:19/2155 train_time:778ms step_avg:40.95ms
step:20/2155 train_time:812ms step_avg:40.58ms
step:21/2155 train_time:845ms step_avg:40.23ms
step:22/2155 train_time:879ms step_avg:39.93ms
step:23/2155 train_time:911ms step_avg:39.61ms
step:24/2155 train_time:945ms step_avg:39.37ms
step:25/2155 train_time:977ms step_avg:39.08ms
step:26/2155 train_time:1011ms step_avg:38.87ms
step:27/2155 train_time:1044ms step_avg:38.65ms
step:28/2155 train_time:1077ms step_avg:38.48ms
step:29/2155 train_time:1111ms step_avg:38.29ms
step:30/2155 train_time:1144ms step_avg:38.14ms
step:31/2155 train_time:1177ms step_avg:37.96ms
step:32/2155 train_time:1210ms step_avg:37.83ms
step:33/2155 train_time:1243ms step_avg:37.67ms
step:34/2155 train_time:1278ms step_avg:37.58ms
step:35/2155 train_time:1312ms step_avg:37.49ms
step:36/2155 train_time:1346ms step_avg:37.39ms
step:37/2155 train_time:1380ms step_avg:37.29ms
step:38/2155 train_time:1414ms step_avg:37.21ms
step:39/2155 train_time:1447ms step_avg:37.11ms
step:40/2155 train_time:1481ms step_avg:37.02ms
step:41/2155 train_time:1514ms step_avg:36.93ms
step:42/2155 train_time:1548ms step_avg:36.85ms
step:43/2155 train_time:1580ms step_avg:36.76ms
step:44/2155 train_time:1614ms step_avg:36.68ms
step:45/2155 train_time:1647ms step_avg:36.61ms
step:46/2155 train_time:1681ms step_avg:36.54ms
step:47/2155 train_time:1714ms step_avg:36.46ms
step:48/2155 train_time:1748ms step_avg:36.41ms
step:49/2155 train_time:1780ms step_avg:36.33ms
step:50/2155 train_time:1814ms step_avg:36.28ms
step:51/2155 train_time:1847ms step_avg:36.21ms
step:52/2155 train_time:1880ms step_avg:36.16ms
step:53/2155 train_time:1913ms step_avg:36.09ms
step:54/2155 train_time:1946ms step_avg:36.04ms
step:55/2155 train_time:1979ms step_avg:35.99ms
step:56/2155 train_time:2013ms step_avg:35.94ms
step:57/2155 train_time:2046ms step_avg:35.90ms
step:58/2155 train_time:2080ms step_avg:35.86ms
step:59/2155 train_time:2113ms step_avg:35.81ms
step:60/2155 train_time:2146ms step_avg:35.77ms
step:61/2155 train_time:2180ms step_avg:35.73ms
step:62/2155 train_time:2213ms step_avg:35.70ms
step:63/2155 train_time:2247ms step_avg:35.66ms
step:64/2155 train_time:2280ms step_avg:35.63ms
step:65/2155 train_time:2314ms step_avg:35.60ms
step:66/2155 train_time:2348ms step_avg:35.57ms
step:67/2155 train_time:2381ms step_avg:35.53ms
step:68/2155 train_time:2414ms step_avg:35.51ms
step:69/2155 train_time:2448ms step_avg:35.48ms
step:70/2155 train_time:2482ms step_avg:35.46ms
step:71/2155 train_time:2515ms step_avg:35.43ms
step:72/2155 train_time:2549ms step_avg:35.40ms
step:73/2155 train_time:2582ms step_avg:35.37ms
step:74/2155 train_time:2615ms step_avg:35.34ms
step:75/2155 train_time:2648ms step_avg:35.31ms
step:76/2155 train_time:2682ms step_avg:35.29ms
step:77/2155 train_time:2715ms step_avg:35.26ms
step:78/2155 train_time:2749ms step_avg:35.25ms
step:79/2155 train_time:2781ms step_avg:35.20ms
step:80/2155 train_time:2815ms step_avg:35.18ms
step:81/2155 train_time:2847ms step_avg:35.15ms
step:82/2155 train_time:2881ms step_avg:35.13ms
step:83/2155 train_time:2914ms step_avg:35.10ms
step:84/2155 train_time:2947ms step_avg:35.09ms
step:85/2155 train_time:2980ms step_avg:35.06ms
step:86/2155 train_time:3014ms step_avg:35.04ms
step:87/2155 train_time:3047ms step_avg:35.02ms
step:88/2155 train_time:3080ms step_avg:35.00ms
step:89/2155 train_time:3113ms step_avg:34.98ms
step:90/2155 train_time:3147ms step_avg:34.96ms
step:91/2155 train_time:3180ms step_avg:34.94ms
step:92/2155 train_time:3213ms step_avg:34.93ms
step:93/2155 train_time:3246ms step_avg:34.91ms
step:94/2155 train_time:3280ms step_avg:34.89ms
step:95/2155 train_time:3313ms step_avg:34.87ms
step:96/2155 train_time:3346ms step_avg:34.86ms
step:97/2155 train_time:3380ms step_avg:34.84ms
step:98/2155 train_time:3413ms step_avg:34.83ms
step:99/2155 train_time:3447ms step_avg:34.82ms
step:100/2155 train_time:3480ms step_avg:34.80ms
step:101/2155 train_time:3514ms step_avg:34.79ms
step:102/2155 train_time:3547ms step_avg:34.78ms
step:103/2155 train_time:3580ms step_avg:34.76ms
step:104/2155 train_time:3614ms step_avg:34.75ms
step:105/2155 train_time:3647ms step_avg:34.73ms
step:106/2155 train_time:3681ms step_avg:34.72ms
step:107/2155 train_time:3713ms step_avg:34.70ms
step:108/2155 train_time:3747ms step_avg:34.69ms
step:109/2155 train_time:3780ms step_avg:34.68ms
step:110/2155 train_time:3813ms step_avg:34.67ms
step:111/2155 train_time:3846ms step_avg:34.65ms
step:112/2155 train_time:3880ms step_avg:34.64ms
step:113/2155 train_time:3912ms step_avg:34.62ms
step:114/2155 train_time:3946ms step_avg:34.61ms
step:115/2155 train_time:3979ms step_avg:34.60ms
step:116/2155 train_time:4012ms step_avg:34.59ms
step:117/2155 train_time:4045ms step_avg:34.58ms
step:118/2155 train_time:4079ms step_avg:34.57ms
step:119/2155 train_time:4112ms step_avg:34.55ms
step:120/2155 train_time:4145ms step_avg:34.54ms
step:121/2155 train_time:4178ms step_avg:34.53ms
step:122/2155 train_time:4212ms step_avg:34.52ms
step:123/2155 train_time:4245ms step_avg:34.51ms
step:124/2155 train_time:4278ms step_avg:34.50ms
step:125/2155 train_time:4312ms step_avg:34.49ms
step:126/2155 train_time:4345ms step_avg:34.48ms
step:127/2155 train_time:4378ms step_avg:34.47ms
step:128/2155 train_time:4412ms step_avg:34.46ms
step:129/2155 train_time:4444ms step_avg:34.45ms
step:130/2155 train_time:4478ms step_avg:34.44ms
step:131/2155 train_time:4511ms step_avg:34.43ms
step:132/2155 train_time:4544ms step_avg:34.43ms
step:133/2155 train_time:4577ms step_avg:34.41ms
step:134/2155 train_time:4610ms step_avg:34.41ms
step:135/2155 train_time:4643ms step_avg:34.39ms
step:136/2155 train_time:4676ms step_avg:34.38ms
step:137/2155 train_time:4709ms step_avg:34.37ms
step:138/2155 train_time:4742ms step_avg:34.36ms
step:139/2155 train_time:4775ms step_avg:34.35ms
step:140/2155 train_time:4809ms step_avg:34.35ms
step:141/2155 train_time:4841ms step_avg:34.33ms
step:142/2155 train_time:4875ms step_avg:34.33ms
step:143/2155 train_time:4908ms step_avg:34.32ms
step:144/2155 train_time:4941ms step_avg:34.31ms
step:145/2155 train_time:4974ms step_avg:34.30ms
step:146/2155 train_time:5007ms step_avg:34.30ms
step:147/2155 train_time:5040ms step_avg:34.28ms
step:148/2155 train_time:5073ms step_avg:34.28ms
step:149/2155 train_time:5106ms step_avg:34.27ms
step:150/2155 train_time:5140ms step_avg:34.26ms
step:151/2155 train_time:5172ms step_avg:34.25ms
step:152/2155 train_time:5205ms step_avg:34.24ms
step:153/2155 train_time:5238ms step_avg:34.24ms
step:154/2155 train_time:5272ms step_avg:34.23ms
step:155/2155 train_time:5304ms step_avg:34.22ms
step:156/2155 train_time:5338ms step_avg:34.22ms
step:157/2155 train_time:5371ms step_avg:34.21ms
step:158/2155 train_time:5404ms step_avg:34.20ms
step:159/2155 train_time:5437ms step_avg:34.20ms
step:160/2155 train_time:5471ms step_avg:34.19ms
step:161/2155 train_time:5503ms step_avg:34.18ms
step:162/2155 train_time:5537ms step_avg:34.18ms
step:163/2155 train_time:5570ms step_avg:34.17ms
step:164/2155 train_time:5603ms step_avg:34.17ms
step:165/2155 train_time:5638ms step_avg:34.17ms
step:166/2155 train_time:5669ms step_avg:34.15ms
step:167/2155 train_time:5701ms step_avg:34.14ms
step:168/2155 train_time:5735ms step_avg:34.14ms
step:169/2155 train_time:5767ms step_avg:34.13ms
step:170/2155 train_time:5801ms step_avg:34.12ms
step:171/2155 train_time:5833ms step_avg:34.11ms
step:172/2155 train_time:5867ms step_avg:34.11ms
step:173/2155 train_time:5899ms step_avg:34.10ms
step:174/2155 train_time:5933ms step_avg:34.10ms
step:175/2155 train_time:5966ms step_avg:34.09ms
step:176/2155 train_time:5999ms step_avg:34.08ms
step:177/2155 train_time:6032ms step_avg:34.08ms
step:178/2155 train_time:6065ms step_avg:34.07ms
step:179/2155 train_time:6098ms step_avg:34.06ms
step:180/2155 train_time:6131ms step_avg:34.06ms
step:181/2155 train_time:6164ms step_avg:34.05ms
step:182/2155 train_time:6197ms step_avg:34.05ms
step:183/2155 train_time:6229ms step_avg:34.04ms
step:184/2155 train_time:6265ms step_avg:34.05ms
step:185/2155 train_time:6296ms step_avg:34.03ms
step:186/2155 train_time:6329ms step_avg:34.03ms
step:187/2155 train_time:6362ms step_avg:34.02ms
step:188/2155 train_time:6395ms step_avg:34.02ms
step:189/2155 train_time:6428ms step_avg:34.01ms
step:190/2155 train_time:6461ms step_avg:34.01ms
step:191/2155 train_time:6494ms step_avg:34.00ms
step:192/2155 train_time:6528ms step_avg:34.00ms
step:193/2155 train_time:6560ms step_avg:33.99ms
step:194/2155 train_time:6594ms step_avg:33.99ms
step:195/2155 train_time:6626ms step_avg:33.98ms
step:196/2155 train_time:6660ms step_avg:33.98ms
step:197/2155 train_time:6693ms step_avg:33.97ms
step:198/2155 train_time:6726ms step_avg:33.97ms
step:199/2155 train_time:6759ms step_avg:33.97ms
step:200/2155 train_time:6793ms step_avg:33.96ms
step:201/2155 train_time:6825ms step_avg:33.96ms
step:202/2155 train_time:6859ms step_avg:33.95ms
step:203/2155 train_time:6892ms step_avg:33.95ms
step:204/2155 train_time:6925ms step_avg:33.95ms
step:205/2155 train_time:6958ms step_avg:33.94ms
step:206/2155 train_time:6991ms step_avg:33.94ms
step:207/2155 train_time:7024ms step_avg:33.93ms
step:208/2155 train_time:7057ms step_avg:33.93ms
step:209/2155 train_time:7090ms step_avg:33.92ms
step:210/2155 train_time:7123ms step_avg:33.92ms
step:211/2155 train_time:7156ms step_avg:33.92ms
step:212/2155 train_time:7190ms step_avg:33.91ms
step:213/2155 train_time:7222ms step_avg:33.91ms
step:214/2155 train_time:7256ms step_avg:33.90ms
step:215/2155 train_time:7288ms step_avg:33.90ms
step:216/2155 train_time:7321ms step_avg:33.90ms
step:217/2155 train_time:7354ms step_avg:33.89ms
step:218/2155 train_time:7388ms step_avg:33.89ms
step:219/2155 train_time:7421ms step_avg:33.88ms
step:220/2155 train_time:7454ms step_avg:33.88ms
step:221/2155 train_time:7487ms step_avg:33.88ms
step:222/2155 train_time:7520ms step_avg:33.87ms
step:223/2155 train_time:7553ms step_avg:33.87ms
step:224/2155 train_time:7586ms step_avg:33.87ms
step:225/2155 train_time:7619ms step_avg:33.86ms
step:226/2155 train_time:7652ms step_avg:33.86ms
step:227/2155 train_time:7685ms step_avg:33.85ms
step:228/2155 train_time:7718ms step_avg:33.85ms
step:229/2155 train_time:7751ms step_avg:33.85ms
step:230/2155 train_time:7784ms step_avg:33.84ms
step:231/2155 train_time:7817ms step_avg:33.84ms
step:232/2155 train_time:7850ms step_avg:33.84ms
step:233/2155 train_time:7883ms step_avg:33.83ms
step:234/2155 train_time:7917ms step_avg:33.83ms
step:235/2155 train_time:7949ms step_avg:33.83ms
step:236/2155 train_time:7982ms step_avg:33.82ms
step:237/2155 train_time:8015ms step_avg:33.82ms
step:238/2155 train_time:8048ms step_avg:33.82ms
step:239/2155 train_time:8081ms step_avg:33.81ms
step:240/2155 train_time:8115ms step_avg:33.81ms
step:241/2155 train_time:8148ms step_avg:33.81ms
step:242/2155 train_time:8181ms step_avg:33.81ms
step:243/2155 train_time:8214ms step_avg:33.80ms
step:244/2155 train_time:8247ms step_avg:33.80ms
step:245/2155 train_time:8280ms step_avg:33.80ms
step:246/2155 train_time:8314ms step_avg:33.80ms
step:247/2155 train_time:8347ms step_avg:33.79ms
step:248/2155 train_time:8380ms step_avg:33.79ms
step:249/2155 train_time:8413ms step_avg:33.79ms
step:250/2155 train_time:8447ms step_avg:33.79ms
step:250/2155 val_loss:4.3154 train_time:8483ms step_avg:33.93ms
step:251/2155 train_time:8505ms step_avg:33.89ms
step:252/2155 train_time:8528ms step_avg:33.84ms
step:253/2155 train_time:8549ms step_avg:33.79ms
step:254/2155 train_time:8582ms step_avg:33.79ms
step:255/2155 train_time:8616ms step_avg:33.79ms
step:256/2155 train_time:8651ms step_avg:33.79ms
step:257/2155 train_time:8686ms step_avg:33.80ms
step:258/2155 train_time:8721ms step_avg:33.80ms
step:259/2155 train_time:8754ms step_avg:33.80ms
step:260/2155 train_time:8787ms step_avg:33.80ms
step:261/2155 train_time:8820ms step_avg:33.79ms
step:262/2155 train_time:8853ms step_avg:33.79ms
step:263/2155 train_time:8886ms step_avg:33.79ms
step:264/2155 train_time:8919ms step_avg:33.78ms
step:265/2155 train_time:8952ms step_avg:33.78ms
step:266/2155 train_time:8985ms step_avg:33.78ms
step:267/2155 train_time:9017ms step_avg:33.77ms
step:268/2155 train_time:9051ms step_avg:33.77ms
step:269/2155 train_time:9083ms step_avg:33.76ms
step:270/2155 train_time:9116ms step_avg:33.76ms
step:271/2155 train_time:9148ms step_avg:33.76ms
step:272/2155 train_time:9181ms step_avg:33.76ms
step:273/2155 train_time:9214ms step_avg:33.75ms
step:274/2155 train_time:9247ms step_avg:33.75ms
step:275/2155 train_time:9280ms step_avg:33.74ms
step:276/2155 train_time:9313ms step_avg:33.74ms
step:277/2155 train_time:9345ms step_avg:33.74ms
step:278/2155 train_time:9379ms step_avg:33.74ms
step:279/2155 train_time:9411ms step_avg:33.73ms
step:280/2155 train_time:9444ms step_avg:33.73ms
step:281/2155 train_time:9477ms step_avg:33.72ms
step:282/2155 train_time:9510ms step_avg:33.72ms
step:283/2155 train_time:9543ms step_avg:33.72ms
step:284/2155 train_time:9577ms step_avg:33.72ms
step:285/2155 train_time:9610ms step_avg:33.72ms
step:286/2155 train_time:9644ms step_avg:33.72ms
step:287/2155 train_time:9678ms step_avg:33.72ms
step:288/2155 train_time:9711ms step_avg:33.72ms
step:289/2155 train_time:9744ms step_avg:33.72ms
step:290/2155 train_time:9778ms step_avg:33.72ms
step:291/2155 train_time:9811ms step_avg:33.71ms
step:292/2155 train_time:9844ms step_avg:33.71ms
step:293/2155 train_time:9877ms step_avg:33.71ms
step:294/2155 train_time:9910ms step_avg:33.71ms
step:295/2155 train_time:9943ms step_avg:33.71ms
step:296/2155 train_time:9977ms step_avg:33.70ms
step:297/2155 train_time:10009ms step_avg:33.70ms
step:298/2155 train_time:10043ms step_avg:33.70ms
step:299/2155 train_time:10075ms step_avg:33.70ms
step:300/2155 train_time:10108ms step_avg:33.69ms
step:301/2155 train_time:10141ms step_avg:33.69ms
step:302/2155 train_time:10174ms step_avg:33.69ms
step:303/2155 train_time:10206ms step_avg:33.68ms
step:304/2155 train_time:10240ms step_avg:33.68ms
step:305/2155 train_time:10272ms step_avg:33.68ms
step:306/2155 train_time:10305ms step_avg:33.68ms
step:307/2155 train_time:10338ms step_avg:33.67ms
step:308/2155 train_time:10371ms step_avg:33.67ms
step:309/2155 train_time:10403ms step_avg:33.67ms
step:310/2155 train_time:10437ms step_avg:33.67ms
step:311/2155 train_time:10469ms step_avg:33.66ms
step:312/2155 train_time:10503ms step_avg:33.66ms
step:313/2155 train_time:10535ms step_avg:33.66ms
step:314/2155 train_time:10569ms step_avg:33.66ms
step:315/2155 train_time:10602ms step_avg:33.66ms
step:316/2155 train_time:10635ms step_avg:33.66ms
step:317/2155 train_time:10668ms step_avg:33.65ms
step:318/2155 train_time:10702ms step_avg:33.65ms
step:319/2155 train_time:10735ms step_avg:33.65ms
step:320/2155 train_time:10768ms step_avg:33.65ms
step:321/2155 train_time:10801ms step_avg:33.65ms
step:322/2155 train_time:10835ms step_avg:33.65ms
step:323/2155 train_time:10867ms step_avg:33.64ms
step:324/2155 train_time:10901ms step_avg:33.64ms
step:325/2155 train_time:10933ms step_avg:33.64ms
step:326/2155 train_time:10967ms step_avg:33.64ms
step:327/2155 train_time:10999ms step_avg:33.64ms
step:328/2155 train_time:11033ms step_avg:33.64ms
step:329/2155 train_time:11065ms step_avg:33.63ms
step:330/2155 train_time:11098ms step_avg:33.63ms
step:331/2155 train_time:11131ms step_avg:33.63ms
step:332/2155 train_time:11164ms step_avg:33.63ms
step:333/2155 train_time:11197ms step_avg:33.62ms
step:334/2155 train_time:11230ms step_avg:33.62ms
step:335/2155 train_time:11263ms step_avg:33.62ms
step:336/2155 train_time:11296ms step_avg:33.62ms
step:337/2155 train_time:11329ms step_avg:33.62ms
step:338/2155 train_time:11362ms step_avg:33.62ms
step:339/2155 train_time:11395ms step_avg:33.61ms
step:340/2155 train_time:11428ms step_avg:33.61ms
step:341/2155 train_time:11460ms step_avg:33.61ms
step:342/2155 train_time:11493ms step_avg:33.61ms
step:343/2155 train_time:11526ms step_avg:33.60ms
step:344/2155 train_time:11560ms step_avg:33.60ms
step:345/2155 train_time:11592ms step_avg:33.60ms
step:346/2155 train_time:11625ms step_avg:33.60ms
step:347/2155 train_time:11659ms step_avg:33.60ms
step:348/2155 train_time:11692ms step_avg:33.60ms
step:349/2155 train_time:11725ms step_avg:33.60ms
step:350/2155 train_time:11759ms step_avg:33.60ms
step:351/2155 train_time:11792ms step_avg:33.59ms
step:352/2155 train_time:11825ms step_avg:33.59ms
step:353/2155 train_time:11858ms step_avg:33.59ms
step:354/2155 train_time:11891ms step_avg:33.59ms
step:355/2155 train_time:11924ms step_avg:33.59ms
step:356/2155 train_time:11957ms step_avg:33.59ms
step:357/2155 train_time:11990ms step_avg:33.59ms
step:358/2155 train_time:12023ms step_avg:33.58ms
step:359/2155 train_time:12055ms step_avg:33.58ms
step:360/2155 train_time:12089ms step_avg:33.58ms
step:361/2155 train_time:12121ms step_avg:33.58ms
step:362/2155 train_time:12155ms step_avg:33.58ms
step:363/2155 train_time:12187ms step_avg:33.57ms
step:364/2155 train_time:12221ms step_avg:33.57ms
step:365/2155 train_time:12253ms step_avg:33.57ms
step:366/2155 train_time:12287ms step_avg:33.57ms
step:367/2155 train_time:12320ms step_avg:33.57ms
step:368/2155 train_time:12353ms step_avg:33.57ms
step:369/2155 train_time:12389ms step_avg:33.57ms
step:370/2155 train_time:12419ms step_avg:33.56ms
step:371/2155 train_time:12451ms step_avg:33.56ms
step:372/2155 train_time:12485ms step_avg:33.56ms
step:373/2155 train_time:12517ms step_avg:33.56ms
step:374/2155 train_time:12550ms step_avg:33.56ms
step:375/2155 train_time:12583ms step_avg:33.55ms
step:376/2155 train_time:12616ms step_avg:33.55ms
step:377/2155 train_time:12649ms step_avg:33.55ms
step:378/2155 train_time:12682ms step_avg:33.55ms
step:379/2155 train_time:12715ms step_avg:33.55ms
step:380/2155 train_time:12748ms step_avg:33.55ms
step:381/2155 train_time:12781ms step_avg:33.55ms
step:382/2155 train_time:12815ms step_avg:33.55ms
step:383/2155 train_time:12848ms step_avg:33.54ms
step:384/2155 train_time:12881ms step_avg:33.54ms
step:385/2155 train_time:12915ms step_avg:33.54ms
step:386/2155 train_time:12948ms step_avg:33.54ms
step:387/2155 train_time:12981ms step_avg:33.54ms
step:388/2155 train_time:13014ms step_avg:33.54ms
step:389/2155 train_time:13047ms step_avg:33.54ms
step:390/2155 train_time:13081ms step_avg:33.54ms
step:391/2155 train_time:13114ms step_avg:33.54ms
step:392/2155 train_time:13147ms step_avg:33.54ms
step:393/2155 train_time:13179ms step_avg:33.53ms
step:394/2155 train_time:13213ms step_avg:33.53ms
step:395/2155 train_time:13245ms step_avg:33.53ms
step:396/2155 train_time:13278ms step_avg:33.53ms
step:397/2155 train_time:13311ms step_avg:33.53ms
step:398/2155 train_time:13344ms step_avg:33.53ms
step:399/2155 train_time:13377ms step_avg:33.53ms
step:400/2155 train_time:13410ms step_avg:33.53ms
step:401/2155 train_time:13442ms step_avg:33.52ms
step:402/2155 train_time:13476ms step_avg:33.52ms
step:403/2155 train_time:13508ms step_avg:33.52ms
step:404/2155 train_time:13542ms step_avg:33.52ms
step:405/2155 train_time:13574ms step_avg:33.52ms
step:406/2155 train_time:13608ms step_avg:33.52ms
step:407/2155 train_time:13640ms step_avg:33.51ms
step:408/2155 train_time:13674ms step_avg:33.51ms
step:409/2155 train_time:13706ms step_avg:33.51ms
step:410/2155 train_time:13740ms step_avg:33.51ms
step:411/2155 train_time:13772ms step_avg:33.51ms
step:412/2155 train_time:13806ms step_avg:33.51ms
step:413/2155 train_time:13838ms step_avg:33.51ms
step:414/2155 train_time:13872ms step_avg:33.51ms
step:415/2155 train_time:13905ms step_avg:33.51ms
step:416/2155 train_time:13938ms step_avg:33.50ms
step:417/2155 train_time:13971ms step_avg:33.50ms
step:418/2155 train_time:14004ms step_avg:33.50ms
step:419/2155 train_time:14037ms step_avg:33.50ms
step:420/2155 train_time:14070ms step_avg:33.50ms
step:421/2155 train_time:14103ms step_avg:33.50ms
step:422/2155 train_time:14137ms step_avg:33.50ms
step:423/2155 train_time:14169ms step_avg:33.50ms
step:424/2155 train_time:14203ms step_avg:33.50ms
step:425/2155 train_time:14236ms step_avg:33.50ms
step:426/2155 train_time:14269ms step_avg:33.50ms
step:427/2155 train_time:14302ms step_avg:33.49ms
step:428/2155 train_time:14335ms step_avg:33.49ms
step:429/2155 train_time:14368ms step_avg:33.49ms
step:430/2155 train_time:14401ms step_avg:33.49ms
step:431/2155 train_time:14434ms step_avg:33.49ms
step:432/2155 train_time:14467ms step_avg:33.49ms
step:433/2155 train_time:14500ms step_avg:33.49ms
step:434/2155 train_time:14533ms step_avg:33.49ms
step:435/2155 train_time:14566ms step_avg:33.49ms
step:436/2155 train_time:14599ms step_avg:33.48ms
step:437/2155 train_time:14632ms step_avg:33.48ms
step:438/2155 train_time:14665ms step_avg:33.48ms
step:439/2155 train_time:14698ms step_avg:33.48ms
step:440/2155 train_time:14731ms step_avg:33.48ms
step:441/2155 train_time:14764ms step_avg:33.48ms
step:442/2155 train_time:14798ms step_avg:33.48ms
step:443/2155 train_time:14830ms step_avg:33.48ms
step:444/2155 train_time:14864ms step_avg:33.48ms
step:445/2155 train_time:14896ms step_avg:33.47ms
step:446/2155 train_time:14930ms step_avg:33.47ms
step:447/2155 train_time:14962ms step_avg:33.47ms
step:448/2155 train_time:14996ms step_avg:33.47ms
step:449/2155 train_time:15029ms step_avg:33.47ms
step:450/2155 train_time:15062ms step_avg:33.47ms
step:451/2155 train_time:15094ms step_avg:33.47ms
step:452/2155 train_time:15128ms step_avg:33.47ms
step:453/2155 train_time:15160ms step_avg:33.47ms
step:454/2155 train_time:15194ms step_avg:33.47ms
step:455/2155 train_time:15226ms step_avg:33.46ms
step:456/2155 train_time:15259ms step_avg:33.46ms
step:457/2155 train_time:15292ms step_avg:33.46ms
step:458/2155 train_time:15326ms step_avg:33.46ms
step:459/2155 train_time:15358ms step_avg:33.46ms
step:460/2155 train_time:15392ms step_avg:33.46ms
step:461/2155 train_time:15424ms step_avg:33.46ms
step:462/2155 train_time:15458ms step_avg:33.46ms
step:463/2155 train_time:15493ms step_avg:33.46ms
step:464/2155 train_time:15524ms step_avg:33.46ms
step:465/2155 train_time:15557ms step_avg:33.46ms
step:466/2155 train_time:15590ms step_avg:33.46ms
step:467/2155 train_time:15622ms step_avg:33.45ms
step:468/2155 train_time:15656ms step_avg:33.45ms
step:469/2155 train_time:15688ms step_avg:33.45ms
step:470/2155 train_time:15722ms step_avg:33.45ms
step:471/2155 train_time:15754ms step_avg:33.45ms
step:472/2155 train_time:15788ms step_avg:33.45ms
step:473/2155 train_time:15820ms step_avg:33.45ms
step:474/2155 train_time:15854ms step_avg:33.45ms
step:475/2155 train_time:15886ms step_avg:33.44ms
step:476/2155 train_time:15919ms step_avg:33.44ms
step:477/2155 train_time:15952ms step_avg:33.44ms
step:478/2155 train_time:15986ms step_avg:33.44ms
step:479/2155 train_time:16018ms step_avg:33.44ms
step:480/2155 train_time:16052ms step_avg:33.44ms
step:481/2155 train_time:16084ms step_avg:33.44ms
step:482/2155 train_time:16118ms step_avg:33.44ms
step:483/2155 train_time:16150ms step_avg:33.44ms
step:484/2155 train_time:16184ms step_avg:33.44ms
step:485/2155 train_time:16216ms step_avg:33.44ms
step:486/2155 train_time:16250ms step_avg:33.44ms
step:487/2155 train_time:16283ms step_avg:33.44ms
step:488/2155 train_time:16316ms step_avg:33.43ms
step:489/2155 train_time:16349ms step_avg:33.43ms
step:490/2155 train_time:16382ms step_avg:33.43ms
step:491/2155 train_time:16415ms step_avg:33.43ms
step:492/2155 train_time:16448ms step_avg:33.43ms
step:493/2155 train_time:16481ms step_avg:33.43ms
step:494/2155 train_time:16514ms step_avg:33.43ms
step:495/2155 train_time:16547ms step_avg:33.43ms
step:496/2155 train_time:16580ms step_avg:33.43ms
step:497/2155 train_time:16613ms step_avg:33.43ms
step:498/2155 train_time:16647ms step_avg:33.43ms
step:499/2155 train_time:16680ms step_avg:33.43ms
step:500/2155 train_time:16713ms step_avg:33.43ms
step:500/2155 val_loss:4.0226 train_time:16749ms step_avg:33.50ms
step:501/2155 train_time:16772ms step_avg:33.48ms
step:502/2155 train_time:16794ms step_avg:33.45ms
step:503/2155 train_time:16817ms step_avg:33.43ms
step:504/2155 train_time:16851ms step_avg:33.44ms
step:505/2155 train_time:16886ms step_avg:33.44ms
step:506/2155 train_time:16920ms step_avg:33.44ms
step:507/2155 train_time:16954ms step_avg:33.44ms
step:508/2155 train_time:16988ms step_avg:33.44ms
step:509/2155 train_time:17021ms step_avg:33.44ms
step:510/2155 train_time:17054ms step_avg:33.44ms
step:511/2155 train_time:17087ms step_avg:33.44ms
step:512/2155 train_time:17121ms step_avg:33.44ms
step:513/2155 train_time:17153ms step_avg:33.44ms
step:514/2155 train_time:17186ms step_avg:33.44ms
step:515/2155 train_time:17219ms step_avg:33.43ms
step:516/2155 train_time:17252ms step_avg:33.43ms
step:517/2155 train_time:17285ms step_avg:33.43ms
step:518/2155 train_time:17318ms step_avg:33.43ms
step:519/2155 train_time:17350ms step_avg:33.43ms
step:520/2155 train_time:17383ms step_avg:33.43ms
step:521/2155 train_time:17416ms step_avg:33.43ms
step:522/2155 train_time:17449ms step_avg:33.43ms
step:523/2155 train_time:17481ms step_avg:33.42ms
step:524/2155 train_time:17514ms step_avg:33.42ms
step:525/2155 train_time:17547ms step_avg:33.42ms
step:526/2155 train_time:17580ms step_avg:33.42ms
step:527/2155 train_time:17612ms step_avg:33.42ms
step:528/2155 train_time:17646ms step_avg:33.42ms
step:529/2155 train_time:17678ms step_avg:33.42ms
step:530/2155 train_time:17712ms step_avg:33.42ms
step:531/2155 train_time:17746ms step_avg:33.42ms
step:532/2155 train_time:17778ms step_avg:33.42ms
step:533/2155 train_time:17811ms step_avg:33.42ms
step:534/2155 train_time:17845ms step_avg:33.42ms
step:535/2155 train_time:17879ms step_avg:33.42ms
step:536/2155 train_time:17912ms step_avg:33.42ms
step:537/2155 train_time:17946ms step_avg:33.42ms
step:538/2155 train_time:17979ms step_avg:33.42ms
step:539/2155 train_time:18012ms step_avg:33.42ms
step:540/2155 train_time:18046ms step_avg:33.42ms
step:541/2155 train_time:18079ms step_avg:33.42ms
step:542/2155 train_time:18112ms step_avg:33.42ms
step:543/2155 train_time:18145ms step_avg:33.42ms
step:544/2155 train_time:18179ms step_avg:33.42ms
step:545/2155 train_time:18212ms step_avg:33.42ms
step:546/2155 train_time:18245ms step_avg:33.42ms
step:547/2155 train_time:18278ms step_avg:33.41ms
step:548/2155 train_time:18311ms step_avg:33.41ms
step:549/2155 train_time:18344ms step_avg:33.41ms
step:550/2155 train_time:18378ms step_avg:33.41ms
step:551/2155 train_time:18410ms step_avg:33.41ms
step:552/2155 train_time:18443ms step_avg:33.41ms
step:553/2155 train_time:18476ms step_avg:33.41ms
step:554/2155 train_time:18509ms step_avg:33.41ms
step:555/2155 train_time:18541ms step_avg:33.41ms
step:556/2155 train_time:18575ms step_avg:33.41ms
step:557/2155 train_time:18607ms step_avg:33.41ms
step:558/2155 train_time:18641ms step_avg:33.41ms
step:559/2155 train_time:18673ms step_avg:33.40ms
step:560/2155 train_time:18707ms step_avg:33.40ms
step:561/2155 train_time:18739ms step_avg:33.40ms
step:562/2155 train_time:18773ms step_avg:33.40ms
step:563/2155 train_time:18806ms step_avg:33.40ms
step:564/2155 train_time:18840ms step_avg:33.40ms
step:565/2155 train_time:18873ms step_avg:33.40ms
step:566/2155 train_time:18906ms step_avg:33.40ms
step:567/2155 train_time:18940ms step_avg:33.40ms
step:568/2155 train_time:18974ms step_avg:33.40ms
step:569/2155 train_time:19007ms step_avg:33.40ms
step:570/2155 train_time:19040ms step_avg:33.40ms
step:571/2155 train_time:19073ms step_avg:33.40ms
step:572/2155 train_time:19107ms step_avg:33.40ms
step:573/2155 train_time:19139ms step_avg:33.40ms
step:574/2155 train_time:19173ms step_avg:33.40ms
step:575/2155 train_time:19206ms step_avg:33.40ms
step:576/2155 train_time:19239ms step_avg:33.40ms
step:577/2155 train_time:19272ms step_avg:33.40ms
step:578/2155 train_time:19305ms step_avg:33.40ms
step:579/2155 train_time:19338ms step_avg:33.40ms
step:580/2155 train_time:19371ms step_avg:33.40ms
step:581/2155 train_time:19404ms step_avg:33.40ms
step:582/2155 train_time:19437ms step_avg:33.40ms
step:583/2155 train_time:19470ms step_avg:33.40ms
step:584/2155 train_time:19503ms step_avg:33.40ms
step:585/2155 train_time:19536ms step_avg:33.40ms
step:586/2155 train_time:19569ms step_avg:33.39ms
step:587/2155 train_time:19602ms step_avg:33.39ms
step:588/2155 train_time:19635ms step_avg:33.39ms
step:589/2155 train_time:19668ms step_avg:33.39ms
step:590/2155 train_time:19701ms step_avg:33.39ms
step:591/2155 train_time:19734ms step_avg:33.39ms
step:592/2155 train_time:19768ms step_avg:33.39ms
step:593/2155 train_time:19801ms step_avg:33.39ms
step:594/2155 train_time:19834ms step_avg:33.39ms
step:595/2155 train_time:19867ms step_avg:33.39ms
step:596/2155 train_time:19900ms step_avg:33.39ms
step:597/2155 train_time:19934ms step_avg:33.39ms
step:598/2155 train_time:19968ms step_avg:33.39ms
step:599/2155 train_time:20001ms step_avg:33.39ms
step:600/2155 train_time:20034ms step_avg:33.39ms
step:601/2155 train_time:20068ms step_avg:33.39ms
step:602/2155 train_time:20101ms step_avg:33.39ms
step:603/2155 train_time:20134ms step_avg:33.39ms
step:604/2155 train_time:20167ms step_avg:33.39ms
step:605/2155 train_time:20200ms step_avg:33.39ms
step:606/2155 train_time:20234ms step_avg:33.39ms
step:607/2155 train_time:20267ms step_avg:33.39ms
step:608/2155 train_time:20300ms step_avg:33.39ms
step:609/2155 train_time:20333ms step_avg:33.39ms
step:610/2155 train_time:20366ms step_avg:33.39ms
step:611/2155 train_time:20400ms step_avg:33.39ms
step:612/2155 train_time:20433ms step_avg:33.39ms
step:613/2155 train_time:20465ms step_avg:33.39ms
step:614/2155 train_time:20499ms step_avg:33.39ms
step:615/2155 train_time:20531ms step_avg:33.38ms
step:616/2155 train_time:20565ms step_avg:33.38ms
step:617/2155 train_time:20597ms step_avg:33.38ms
step:618/2155 train_time:20630ms step_avg:33.38ms
step:619/2155 train_time:20663ms step_avg:33.38ms
step:620/2155 train_time:20696ms step_avg:33.38ms
step:621/2155 train_time:20729ms step_avg:33.38ms
step:622/2155 train_time:20762ms step_avg:33.38ms
step:623/2155 train_time:20795ms step_avg:33.38ms
step:624/2155 train_time:20829ms step_avg:33.38ms
step:625/2155 train_time:20861ms step_avg:33.38ms
step:626/2155 train_time:20895ms step_avg:33.38ms
step:627/2155 train_time:20927ms step_avg:33.38ms
step:628/2155 train_time:20961ms step_avg:33.38ms
step:629/2155 train_time:20993ms step_avg:33.38ms
step:630/2155 train_time:21027ms step_avg:33.38ms
step:631/2155 train_time:21060ms step_avg:33.38ms
step:632/2155 train_time:21093ms step_avg:33.38ms
step:633/2155 train_time:21126ms step_avg:33.37ms
step:634/2155 train_time:21159ms step_avg:33.37ms
step:635/2155 train_time:21193ms step_avg:33.37ms
step:636/2155 train_time:21226ms step_avg:33.37ms
step:637/2155 train_time:21259ms step_avg:33.37ms
step:638/2155 train_time:21292ms step_avg:33.37ms
step:639/2155 train_time:21325ms step_avg:33.37ms
step:640/2155 train_time:21358ms step_avg:33.37ms
step:641/2155 train_time:21391ms step_avg:33.37ms
step:642/2155 train_time:21424ms step_avg:33.37ms
step:643/2155 train_time:21457ms step_avg:33.37ms
step:644/2155 train_time:21490ms step_avg:33.37ms
step:645/2155 train_time:21523ms step_avg:33.37ms
step:646/2155 train_time:21557ms step_avg:33.37ms
step:647/2155 train_time:21589ms step_avg:33.37ms
step:648/2155 train_time:21623ms step_avg:33.37ms
step:649/2155 train_time:21656ms step_avg:33.37ms
step:650/2155 train_time:21689ms step_avg:33.37ms
step:651/2155 train_time:21722ms step_avg:33.37ms
step:652/2155 train_time:21756ms step_avg:33.37ms
step:653/2155 train_time:21788ms step_avg:33.37ms
step:654/2155 train_time:21822ms step_avg:33.37ms
step:655/2155 train_time:21854ms step_avg:33.37ms
step:656/2155 train_time:21888ms step_avg:33.37ms
step:657/2155 train_time:21921ms step_avg:33.37ms
step:658/2155 train_time:21955ms step_avg:33.37ms
step:659/2155 train_time:21988ms step_avg:33.37ms
step:660/2155 train_time:22021ms step_avg:33.37ms
step:661/2155 train_time:22054ms step_avg:33.36ms
step:662/2155 train_time:22087ms step_avg:33.36ms
step:663/2155 train_time:22120ms step_avg:33.36ms
step:664/2155 train_time:22153ms step_avg:33.36ms
step:665/2155 train_time:22186ms step_avg:33.36ms
step:666/2155 train_time:22220ms step_avg:33.36ms
step:667/2155 train_time:22253ms step_avg:33.36ms
step:668/2155 train_time:22286ms step_avg:33.36ms
step:669/2155 train_time:22319ms step_avg:33.36ms
step:670/2155 train_time:22352ms step_avg:33.36ms
step:671/2155 train_time:22385ms step_avg:33.36ms
step:672/2155 train_time:22418ms step_avg:33.36ms
step:673/2155 train_time:22451ms step_avg:33.36ms
step:674/2155 train_time:22485ms step_avg:33.36ms
step:675/2155 train_time:22518ms step_avg:33.36ms
step:676/2155 train_time:22551ms step_avg:33.36ms
step:677/2155 train_time:22585ms step_avg:33.36ms
step:678/2155 train_time:22618ms step_avg:33.36ms
step:679/2155 train_time:22651ms step_avg:33.36ms
step:680/2155 train_time:22684ms step_avg:33.36ms
step:681/2155 train_time:22717ms step_avg:33.36ms
step:682/2155 train_time:22750ms step_avg:33.36ms
step:683/2155 train_time:22783ms step_avg:33.36ms
step:684/2155 train_time:22816ms step_avg:33.36ms
step:685/2155 train_time:22849ms step_avg:33.36ms
step:686/2155 train_time:22883ms step_avg:33.36ms
step:687/2155 train_time:22915ms step_avg:33.36ms
step:688/2155 train_time:22949ms step_avg:33.36ms
step:689/2155 train_time:22981ms step_avg:33.35ms
step:690/2155 train_time:23014ms step_avg:33.35ms
step:691/2155 train_time:23047ms step_avg:33.35ms
step:692/2155 train_time:23081ms step_avg:33.35ms
step:693/2155 train_time:23114ms step_avg:33.35ms
step:694/2155 train_time:23147ms step_avg:33.35ms
step:695/2155 train_time:23180ms step_avg:33.35ms
step:696/2155 train_time:23214ms step_avg:33.35ms
step:697/2155 train_time:23246ms step_avg:33.35ms
step:698/2155 train_time:23279ms step_avg:33.35ms
step:699/2155 train_time:23313ms step_avg:33.35ms
step:700/2155 train_time:23346ms step_avg:33.35ms
step:701/2155 train_time:23379ms step_avg:33.35ms
step:702/2155 train_time:23412ms step_avg:33.35ms
step:703/2155 train_time:23445ms step_avg:33.35ms
step:704/2155 train_time:23478ms step_avg:33.35ms
step:705/2155 train_time:23511ms step_avg:33.35ms
step:706/2155 train_time:23546ms step_avg:33.35ms
step:707/2155 train_time:23605ms step_avg:33.39ms
step:708/2155 train_time:23665ms step_avg:33.42ms
step:709/2155 train_time:23727ms step_avg:33.46ms
step:710/2155 train_time:23786ms step_avg:33.50ms
step:711/2155 train_time:23848ms step_avg:33.54ms
step:712/2155 train_time:23907ms step_avg:33.58ms
step:713/2155 train_time:23969ms step_avg:33.62ms
step:714/2155 train_time:24029ms step_avg:33.65ms
step:715/2155 train_time:24090ms step_avg:33.69ms
step:716/2155 train_time:24149ms step_avg:33.73ms
step:717/2155 train_time:24210ms step_avg:33.77ms
step:718/2155 train_time:24269ms step_avg:33.80ms
step:719/2155 train_time:24331ms step_avg:33.84ms
step:720/2155 train_time:24390ms step_avg:33.88ms
step:721/2155 train_time:24452ms step_avg:33.91ms
step:722/2155 train_time:24511ms step_avg:33.95ms
step:723/2155 train_time:24572ms step_avg:33.99ms
step:724/2155 train_time:24631ms step_avg:34.02ms
step:725/2155 train_time:24692ms step_avg:34.06ms
step:726/2155 train_time:24752ms step_avg:34.09ms
step:727/2155 train_time:24812ms step_avg:34.13ms
step:728/2155 train_time:24872ms step_avg:34.16ms
step:729/2155 train_time:24932ms step_avg:34.20ms
step:730/2155 train_time:24992ms step_avg:34.24ms
step:731/2155 train_time:25053ms step_avg:34.27ms
step:732/2155 train_time:25112ms step_avg:34.31ms
step:733/2155 train_time:25173ms step_avg:34.34ms
step:734/2155 train_time:25232ms step_avg:34.38ms
step:735/2155 train_time:25295ms step_avg:34.41ms
step:736/2155 train_time:25354ms step_avg:34.45ms
step:737/2155 train_time:25415ms step_avg:34.48ms
step:738/2155 train_time:25474ms step_avg:34.52ms
step:739/2155 train_time:25535ms step_avg:34.55ms
step:740/2155 train_time:25595ms step_avg:34.59ms
step:741/2155 train_time:25655ms step_avg:34.62ms
step:742/2155 train_time:25716ms step_avg:34.66ms
step:743/2155 train_time:25777ms step_avg:34.69ms
step:744/2155 train_time:25836ms step_avg:34.73ms
step:745/2155 train_time:25898ms step_avg:34.76ms
step:746/2155 train_time:25958ms step_avg:34.80ms
step:747/2155 train_time:26018ms step_avg:34.83ms
step:748/2155 train_time:26078ms step_avg:34.86ms
step:749/2155 train_time:26139ms step_avg:34.90ms
step:750/2155 train_time:26199ms step_avg:34.93ms
step:750/2155 val_loss:3.8746 train_time:26263ms step_avg:35.02ms
step:751/2155 train_time:26286ms step_avg:35.00ms
step:752/2155 train_time:26321ms step_avg:35.00ms
step:753/2155 train_time:26385ms step_avg:35.04ms
step:754/2155 train_time:26448ms step_avg:35.08ms
step:755/2155 train_time:26508ms step_avg:35.11ms
step:756/2155 train_time:26568ms step_avg:35.14ms
step:757/2155 train_time:26627ms step_avg:35.17ms
step:758/2155 train_time:26686ms step_avg:35.21ms
step:759/2155 train_time:26745ms step_avg:35.24ms
step:760/2155 train_time:26804ms step_avg:35.27ms
step:761/2155 train_time:26864ms step_avg:35.30ms
step:762/2155 train_time:26922ms step_avg:35.33ms
step:763/2155 train_time:26982ms step_avg:35.36ms
step:764/2155 train_time:27041ms step_avg:35.39ms
step:765/2155 train_time:27100ms step_avg:35.43ms
step:766/2155 train_time:27163ms step_avg:35.46ms
step:767/2155 train_time:27230ms step_avg:35.50ms
step:768/2155 train_time:27291ms step_avg:35.54ms
step:769/2155 train_time:27354ms step_avg:35.57ms
step:770/2155 train_time:27413ms step_avg:35.60ms
step:771/2155 train_time:27476ms step_avg:35.64ms
step:772/2155 train_time:27534ms step_avg:35.67ms
step:773/2155 train_time:27594ms step_avg:35.70ms
step:774/2155 train_time:27653ms step_avg:35.73ms
step:775/2155 train_time:27714ms step_avg:35.76ms
step:776/2155 train_time:27772ms step_avg:35.79ms
step:777/2155 train_time:27833ms step_avg:35.82ms
step:778/2155 train_time:27891ms step_avg:35.85ms
step:779/2155 train_time:27952ms step_avg:35.88ms
step:780/2155 train_time:28011ms step_avg:35.91ms
step:781/2155 train_time:28072ms step_avg:35.94ms
step:782/2155 train_time:28134ms step_avg:35.98ms
step:783/2155 train_time:28197ms step_avg:36.01ms
step:784/2155 train_time:28257ms step_avg:36.04ms
step:785/2155 train_time:28320ms step_avg:36.08ms
step:786/2155 train_time:28379ms step_avg:36.11ms
step:787/2155 train_time:28440ms step_avg:36.14ms
step:788/2155 train_time:28499ms step_avg:36.17ms
step:789/2155 train_time:28560ms step_avg:36.20ms
step:790/2155 train_time:28619ms step_avg:36.23ms
step:791/2155 train_time:28679ms step_avg:36.26ms
step:792/2155 train_time:28738ms step_avg:36.29ms
step:793/2155 train_time:28799ms step_avg:36.32ms
step:794/2155 train_time:28858ms step_avg:36.35ms
step:795/2155 train_time:28919ms step_avg:36.38ms
step:796/2155 train_time:28978ms step_avg:36.40ms
step:797/2155 train_time:29038ms step_avg:36.43ms
step:798/2155 train_time:29098ms step_avg:36.46ms
step:799/2155 train_time:29159ms step_avg:36.49ms
step:800/2155 train_time:29219ms step_avg:36.52ms
step:801/2155 train_time:29280ms step_avg:36.55ms
step:802/2155 train_time:29340ms step_avg:36.58ms
step:803/2155 train_time:29401ms step_avg:36.61ms
step:804/2155 train_time:29461ms step_avg:36.64ms
step:805/2155 train_time:29522ms step_avg:36.67ms
step:806/2155 train_time:29582ms step_avg:36.70ms
step:807/2155 train_time:29643ms step_avg:36.73ms
step:808/2155 train_time:29702ms step_avg:36.76ms
step:809/2155 train_time:29763ms step_avg:36.79ms
step:810/2155 train_time:29822ms step_avg:36.82ms
step:811/2155 train_time:29883ms step_avg:36.85ms
step:812/2155 train_time:29942ms step_avg:36.87ms
step:813/2155 train_time:30003ms step_avg:36.90ms
step:814/2155 train_time:30063ms step_avg:36.93ms
step:815/2155 train_time:30124ms step_avg:36.96ms
step:816/2155 train_time:30184ms step_avg:36.99ms
step:817/2155 train_time:30246ms step_avg:37.02ms
step:818/2155 train_time:30306ms step_avg:37.05ms
step:819/2155 train_time:30367ms step_avg:37.08ms
step:820/2155 train_time:30427ms step_avg:37.11ms
step:821/2155 train_time:30488ms step_avg:37.13ms
step:822/2155 train_time:30547ms step_avg:37.16ms
step:823/2155 train_time:30609ms step_avg:37.19ms
step:824/2155 train_time:30669ms step_avg:37.22ms
step:825/2155 train_time:30730ms step_avg:37.25ms
step:826/2155 train_time:30789ms step_avg:37.27ms
step:827/2155 train_time:30850ms step_avg:37.30ms
step:828/2155 train_time:30909ms step_avg:37.33ms
step:829/2155 train_time:30970ms step_avg:37.36ms
step:830/2155 train_time:31030ms step_avg:37.39ms
step:831/2155 train_time:31091ms step_avg:37.41ms
step:832/2155 train_time:31150ms step_avg:37.44ms
step:833/2155 train_time:31212ms step_avg:37.47ms
step:834/2155 train_time:31271ms step_avg:37.50ms
step:835/2155 train_time:31333ms step_avg:37.52ms
step:836/2155 train_time:31392ms step_avg:37.55ms
step:837/2155 train_time:31454ms step_avg:37.58ms
step:838/2155 train_time:31514ms step_avg:37.61ms
step:839/2155 train_time:31576ms step_avg:37.64ms
step:840/2155 train_time:31635ms step_avg:37.66ms
step:841/2155 train_time:31696ms step_avg:37.69ms
step:842/2155 train_time:31755ms step_avg:37.71ms
step:843/2155 train_time:31816ms step_avg:37.74ms
step:844/2155 train_time:31875ms step_avg:37.77ms
step:845/2155 train_time:31937ms step_avg:37.80ms
step:846/2155 train_time:31996ms step_avg:37.82ms
step:847/2155 train_time:32058ms step_avg:37.85ms
step:848/2155 train_time:32117ms step_avg:37.87ms
step:849/2155 train_time:32178ms step_avg:37.90ms
step:850/2155 train_time:32238ms step_avg:37.93ms
step:851/2155 train_time:32299ms step_avg:37.95ms
step:852/2155 train_time:32358ms step_avg:37.98ms
step:853/2155 train_time:32419ms step_avg:38.01ms
step:854/2155 train_time:32479ms step_avg:38.03ms
step:855/2155 train_time:32540ms step_avg:38.06ms
step:856/2155 train_time:32599ms step_avg:38.08ms
step:857/2155 train_time:32659ms step_avg:38.11ms
step:858/2155 train_time:32719ms step_avg:38.13ms
step:859/2155 train_time:32779ms step_avg:38.16ms
step:860/2155 train_time:32839ms step_avg:38.18ms
step:861/2155 train_time:32899ms step_avg:38.21ms
step:862/2155 train_time:32958ms step_avg:38.23ms
step:863/2155 train_time:33019ms step_avg:38.26ms
step:864/2155 train_time:33078ms step_avg:38.29ms
step:865/2155 train_time:33140ms step_avg:38.31ms
step:866/2155 train_time:33199ms step_avg:38.34ms
step:867/2155 train_time:33260ms step_avg:38.36ms
step:868/2155 train_time:33320ms step_avg:38.39ms
step:869/2155 train_time:33381ms step_avg:38.41ms
step:870/2155 train_time:33441ms step_avg:38.44ms
step:871/2155 train_time:33502ms step_avg:38.46ms
step:872/2155 train_time:33562ms step_avg:38.49ms
step:873/2155 train_time:33623ms step_avg:38.51ms
step:874/2155 train_time:33683ms step_avg:38.54ms
step:875/2155 train_time:33744ms step_avg:38.56ms
step:876/2155 train_time:33803ms step_avg:38.59ms
step:877/2155 train_time:33865ms step_avg:38.61ms
step:878/2155 train_time:33924ms step_avg:38.64ms
step:879/2155 train_time:33986ms step_avg:38.66ms
step:880/2155 train_time:34045ms step_avg:38.69ms
step:881/2155 train_time:34106ms step_avg:38.71ms
step:882/2155 train_time:34165ms step_avg:38.74ms
step:883/2155 train_time:34226ms step_avg:38.76ms
step:884/2155 train_time:34286ms step_avg:38.78ms
step:885/2155 train_time:34347ms step_avg:38.81ms
step:886/2155 train_time:34407ms step_avg:38.83ms
step:887/2155 train_time:34468ms step_avg:38.86ms
step:888/2155 train_time:34528ms step_avg:38.88ms
step:889/2155 train_time:34589ms step_avg:38.91ms
step:890/2155 train_time:34649ms step_avg:38.93ms
step:891/2155 train_time:34710ms step_avg:38.96ms
step:892/2155 train_time:34770ms step_avg:38.98ms
step:893/2155 train_time:34831ms step_avg:39.00ms
step:894/2155 train_time:34890ms step_avg:39.03ms
step:895/2155 train_time:34951ms step_avg:39.05ms
step:896/2155 train_time:35011ms step_avg:39.07ms
step:897/2155 train_time:35074ms step_avg:39.10ms
step:898/2155 train_time:35132ms step_avg:39.12ms
step:899/2155 train_time:35193ms step_avg:39.15ms
step:900/2155 train_time:35253ms step_avg:39.17ms
step:901/2155 train_time:35314ms step_avg:39.19ms
step:902/2155 train_time:35373ms step_avg:39.22ms
step:903/2155 train_time:35435ms step_avg:39.24ms
step:904/2155 train_time:35495ms step_avg:39.26ms
step:905/2155 train_time:35556ms step_avg:39.29ms
step:906/2155 train_time:35616ms step_avg:39.31ms
step:907/2155 train_time:35677ms step_avg:39.33ms
step:908/2155 train_time:35736ms step_avg:39.36ms
step:909/2155 train_time:35797ms step_avg:39.38ms
step:910/2155 train_time:35856ms step_avg:39.40ms
step:911/2155 train_time:35918ms step_avg:39.43ms
step:912/2155 train_time:35977ms step_avg:39.45ms
step:913/2155 train_time:36038ms step_avg:39.47ms
step:914/2155 train_time:36097ms step_avg:39.49ms
step:915/2155 train_time:36159ms step_avg:39.52ms
step:916/2155 train_time:36218ms step_avg:39.54ms
step:917/2155 train_time:36279ms step_avg:39.56ms
step:918/2155 train_time:36338ms step_avg:39.58ms
step:919/2155 train_time:36398ms step_avg:39.61ms
step:920/2155 train_time:36458ms step_avg:39.63ms
step:921/2155 train_time:36519ms step_avg:39.65ms
step:922/2155 train_time:36579ms step_avg:39.67ms
step:923/2155 train_time:36640ms step_avg:39.70ms
step:924/2155 train_time:36699ms step_avg:39.72ms
step:925/2155 train_time:36760ms step_avg:39.74ms
step:926/2155 train_time:36819ms step_avg:39.76ms
step:927/2155 train_time:36880ms step_avg:39.78ms
step:928/2155 train_time:36939ms step_avg:39.80ms
step:929/2155 train_time:37000ms step_avg:39.83ms
step:930/2155 train_time:37059ms step_avg:39.85ms
step:931/2155 train_time:37120ms step_avg:39.87ms
step:932/2155 train_time:37179ms step_avg:39.89ms
step:933/2155 train_time:37241ms step_avg:39.92ms
step:934/2155 train_time:37301ms step_avg:39.94ms
step:935/2155 train_time:37361ms step_avg:39.96ms
step:936/2155 train_time:37421ms step_avg:39.98ms
step:937/2155 train_time:37482ms step_avg:40.00ms
step:938/2155 train_time:37542ms step_avg:40.02ms
step:939/2155 train_time:37603ms step_avg:40.05ms
step:940/2155 train_time:37663ms step_avg:40.07ms
step:941/2155 train_time:37723ms step_avg:40.09ms
step:942/2155 train_time:37783ms step_avg:40.11ms
step:943/2155 train_time:37844ms step_avg:40.13ms
step:944/2155 train_time:37903ms step_avg:40.15ms
step:945/2155 train_time:37964ms step_avg:40.17ms
step:946/2155 train_time:38024ms step_avg:40.19ms
step:947/2155 train_time:38086ms step_avg:40.22ms
step:948/2155 train_time:38146ms step_avg:40.24ms
step:949/2155 train_time:38208ms step_avg:40.26ms
step:950/2155 train_time:38267ms step_avg:40.28ms
step:951/2155 train_time:38328ms step_avg:40.30ms
step:952/2155 train_time:38388ms step_avg:40.32ms
step:953/2155 train_time:38450ms step_avg:40.35ms
step:954/2155 train_time:38509ms step_avg:40.37ms
step:955/2155 train_time:38571ms step_avg:40.39ms
step:956/2155 train_time:38630ms step_avg:40.41ms
step:957/2155 train_time:38691ms step_avg:40.43ms
step:958/2155 train_time:38751ms step_avg:40.45ms
step:959/2155 train_time:38812ms step_avg:40.47ms
step:960/2155 train_time:38871ms step_avg:40.49ms
step:961/2155 train_time:38933ms step_avg:40.51ms
step:962/2155 train_time:38993ms step_avg:40.53ms
step:963/2155 train_time:39055ms step_avg:40.56ms
step:964/2155 train_time:39115ms step_avg:40.58ms
step:965/2155 train_time:39177ms step_avg:40.60ms
step:966/2155 train_time:39236ms step_avg:40.62ms
step:967/2155 train_time:39297ms step_avg:40.64ms
step:968/2155 train_time:39357ms step_avg:40.66ms
step:969/2155 train_time:39418ms step_avg:40.68ms
step:970/2155 train_time:39478ms step_avg:40.70ms
step:971/2155 train_time:39539ms step_avg:40.72ms
step:972/2155 train_time:39599ms step_avg:40.74ms
step:973/2155 train_time:39659ms step_avg:40.76ms
step:974/2155 train_time:39719ms step_avg:40.78ms
step:975/2155 train_time:39780ms step_avg:40.80ms
step:976/2155 train_time:39840ms step_avg:40.82ms
step:977/2155 train_time:39901ms step_avg:40.84ms
step:978/2155 train_time:39961ms step_avg:40.86ms
step:979/2155 train_time:40022ms step_avg:40.88ms
step:980/2155 train_time:40082ms step_avg:40.90ms
step:981/2155 train_time:40143ms step_avg:40.92ms
step:982/2155 train_time:40203ms step_avg:40.94ms
step:983/2155 train_time:40264ms step_avg:40.96ms
step:984/2155 train_time:40324ms step_avg:40.98ms
step:985/2155 train_time:40385ms step_avg:41.00ms
step:986/2155 train_time:40444ms step_avg:41.02ms
step:987/2155 train_time:40506ms step_avg:41.04ms
step:988/2155 train_time:40565ms step_avg:41.06ms
step:989/2155 train_time:40626ms step_avg:41.08ms
step:990/2155 train_time:40686ms step_avg:41.10ms
step:991/2155 train_time:40748ms step_avg:41.12ms
step:992/2155 train_time:40807ms step_avg:41.14ms
step:993/2155 train_time:40868ms step_avg:41.16ms
step:994/2155 train_time:40927ms step_avg:41.17ms
step:995/2155 train_time:40989ms step_avg:41.19ms
step:996/2155 train_time:41049ms step_avg:41.21ms
step:997/2155 train_time:41110ms step_avg:41.23ms
step:998/2155 train_time:41170ms step_avg:41.25ms
step:999/2155 train_time:41232ms step_avg:41.27ms
step:1000/2155 train_time:41291ms step_avg:41.29ms
step:1000/2155 val_loss:3.7133 train_time:41355ms step_avg:41.36ms
step:1001/2155 train_time:41378ms step_avg:41.34ms
step:1002/2155 train_time:41414ms step_avg:41.33ms
step:1003/2155 train_time:41479ms step_avg:41.36ms
step:1004/2155 train_time:41542ms step_avg:41.38ms
step:1005/2155 train_time:41603ms step_avg:41.40ms
step:1006/2155 train_time:41663ms step_avg:41.41ms
step:1007/2155 train_time:41724ms step_avg:41.43ms
step:1008/2155 train_time:41783ms step_avg:41.45ms
step:1009/2155 train_time:41844ms step_avg:41.47ms
step:1010/2155 train_time:41902ms step_avg:41.49ms
step:1011/2155 train_time:41963ms step_avg:41.51ms
step:1012/2155 train_time:42022ms step_avg:41.52ms
step:1013/2155 train_time:42082ms step_avg:41.54ms
step:1014/2155 train_time:42141ms step_avg:41.56ms
step:1015/2155 train_time:42201ms step_avg:41.58ms
step:1016/2155 train_time:42261ms step_avg:41.59ms
step:1017/2155 train_time:42324ms step_avg:41.62ms
step:1018/2155 train_time:42385ms step_avg:41.64ms
step:1019/2155 train_time:42447ms step_avg:41.66ms
step:1020/2155 train_time:42508ms step_avg:41.67ms
step:1021/2155 train_time:42571ms step_avg:41.70ms
step:1022/2155 train_time:42631ms step_avg:41.71ms
step:1023/2155 train_time:42693ms step_avg:41.73ms
step:1024/2155 train_time:42753ms step_avg:41.75ms
step:1025/2155 train_time:42815ms step_avg:41.77ms
step:1026/2155 train_time:42874ms step_avg:41.79ms
step:1027/2155 train_time:42935ms step_avg:41.81ms
step:1028/2155 train_time:42994ms step_avg:41.82ms
step:1029/2155 train_time:43054ms step_avg:41.84ms
step:1030/2155 train_time:43113ms step_avg:41.86ms
step:1031/2155 train_time:43174ms step_avg:41.88ms
step:1032/2155 train_time:43234ms step_avg:41.89ms
step:1033/2155 train_time:43296ms step_avg:41.91ms
step:1034/2155 train_time:43357ms step_avg:41.93ms
step:1035/2155 train_time:43418ms step_avg:41.95ms
step:1036/2155 train_time:43479ms step_avg:41.97ms
step:1037/2155 train_time:43540ms step_avg:41.99ms
step:1038/2155 train_time:43601ms step_avg:42.00ms
step:1039/2155 train_time:43663ms step_avg:42.02ms
step:1040/2155 train_time:43725ms step_avg:42.04ms
step:1041/2155 train_time:43784ms step_avg:42.06ms
step:1042/2155 train_time:43843ms step_avg:42.08ms
step:1043/2155 train_time:43904ms step_avg:42.09ms
step:1044/2155 train_time:43964ms step_avg:42.11ms
step:1045/2155 train_time:44025ms step_avg:42.13ms
step:1046/2155 train_time:44084ms step_avg:42.15ms
step:1047/2155 train_time:44145ms step_avg:42.16ms
step:1048/2155 train_time:44205ms step_avg:42.18ms
step:1049/2155 train_time:44267ms step_avg:42.20ms
step:1050/2155 train_time:44328ms step_avg:42.22ms
step:1051/2155 train_time:44389ms step_avg:42.24ms
step:1052/2155 train_time:44450ms step_avg:42.25ms
step:1053/2155 train_time:44512ms step_avg:42.27ms
step:1054/2155 train_time:44573ms step_avg:42.29ms
step:1055/2155 train_time:44635ms step_avg:42.31ms
step:1056/2155 train_time:44695ms step_avg:42.32ms
step:1057/2155 train_time:44756ms step_avg:42.34ms
step:1058/2155 train_time:44815ms step_avg:42.36ms
step:1059/2155 train_time:44877ms step_avg:42.38ms
step:1060/2155 train_time:44936ms step_avg:42.39ms
step:1061/2155 train_time:44998ms step_avg:42.41ms
step:1062/2155 train_time:45057ms step_avg:42.43ms
step:1063/2155 train_time:45118ms step_avg:42.44ms
step:1064/2155 train_time:45177ms step_avg:42.46ms
step:1065/2155 train_time:45238ms step_avg:42.48ms
step:1066/2155 train_time:45298ms step_avg:42.49ms
step:1067/2155 train_time:45359ms step_avg:42.51ms
step:1068/2155 train_time:45421ms step_avg:42.53ms
step:1069/2155 train_time:45480ms step_avg:42.54ms
step:1070/2155 train_time:45540ms step_avg:42.56ms
step:1071/2155 train_time:45602ms step_avg:42.58ms
step:1072/2155 train_time:45662ms step_avg:42.60ms
step:1073/2155 train_time:45723ms step_avg:42.61ms
step:1074/2155 train_time:45783ms step_avg:42.63ms
step:1075/2155 train_time:45844ms step_avg:42.65ms
step:1076/2155 train_time:45904ms step_avg:42.66ms
step:1077/2155 train_time:45965ms step_avg:42.68ms
step:1078/2155 train_time:46024ms step_avg:42.69ms
step:1079/2155 train_time:46085ms step_avg:42.71ms
step:1080/2155 train_time:46145ms step_avg:42.73ms
step:1081/2155 train_time:46206ms step_avg:42.74ms
step:1082/2155 train_time:46266ms step_avg:42.76ms
step:1083/2155 train_time:46327ms step_avg:42.78ms
step:1084/2155 train_time:46387ms step_avg:42.79ms
step:1085/2155 train_time:46449ms step_avg:42.81ms
step:1086/2155 train_time:46509ms step_avg:42.83ms
step:1087/2155 train_time:46571ms step_avg:42.84ms
step:1088/2155 train_time:46631ms step_avg:42.86ms
step:1089/2155 train_time:46693ms step_avg:42.88ms
step:1090/2155 train_time:46752ms step_avg:42.89ms
step:1091/2155 train_time:46814ms step_avg:42.91ms
step:1092/2155 train_time:46873ms step_avg:42.92ms
step:1093/2155 train_time:46935ms step_avg:42.94ms
step:1094/2155 train_time:46994ms step_avg:42.96ms
step:1095/2155 train_time:47056ms step_avg:42.97ms
step:1096/2155 train_time:47116ms step_avg:42.99ms
step:1097/2155 train_time:47178ms step_avg:43.01ms
step:1098/2155 train_time:47237ms step_avg:43.02ms
step:1099/2155 train_time:47298ms step_avg:43.04ms
step:1100/2155 train_time:47358ms step_avg:43.05ms
step:1101/2155 train_time:47419ms step_avg:43.07ms
step:1102/2155 train_time:47479ms step_avg:43.08ms
step:1103/2155 train_time:47541ms step_avg:43.10ms
step:1104/2155 train_time:47600ms step_avg:43.12ms
step:1105/2155 train_time:47662ms step_avg:43.13ms
step:1106/2155 train_time:47722ms step_avg:43.15ms
step:1107/2155 train_time:47783ms step_avg:43.16ms
step:1108/2155 train_time:47843ms step_avg:43.18ms
step:1109/2155 train_time:47904ms step_avg:43.20ms
step:1110/2155 train_time:47964ms step_avg:43.21ms
step:1111/2155 train_time:48025ms step_avg:43.23ms
step:1112/2155 train_time:48084ms step_avg:43.24ms
step:1113/2155 train_time:48145ms step_avg:43.26ms
step:1114/2155 train_time:48205ms step_avg:43.27ms
step:1115/2155 train_time:48267ms step_avg:43.29ms
step:1116/2155 train_time:48327ms step_avg:43.30ms
step:1117/2155 train_time:48389ms step_avg:43.32ms
step:1118/2155 train_time:48449ms step_avg:43.34ms
step:1119/2155 train_time:48511ms step_avg:43.35ms
step:1120/2155 train_time:48571ms step_avg:43.37ms
step:1121/2155 train_time:48633ms step_avg:43.38ms
step:1122/2155 train_time:48693ms step_avg:43.40ms
step:1123/2155 train_time:48755ms step_avg:43.41ms
step:1124/2155 train_time:48814ms step_avg:43.43ms
step:1125/2155 train_time:48876ms step_avg:43.45ms
step:1126/2155 train_time:48936ms step_avg:43.46ms
step:1127/2155 train_time:48998ms step_avg:43.48ms
step:1128/2155 train_time:49057ms step_avg:43.49ms
step:1129/2155 train_time:49119ms step_avg:43.51ms
step:1130/2155 train_time:49177ms step_avg:43.52ms
step:1131/2155 train_time:49238ms step_avg:43.54ms
step:1132/2155 train_time:49298ms step_avg:43.55ms
step:1133/2155 train_time:49359ms step_avg:43.56ms
step:1134/2155 train_time:49418ms step_avg:43.58ms
step:1135/2155 train_time:49480ms step_avg:43.59ms
step:1136/2155 train_time:49539ms step_avg:43.61ms
step:1137/2155 train_time:49600ms step_avg:43.62ms
step:1138/2155 train_time:49660ms step_avg:43.64ms
step:1139/2155 train_time:49722ms step_avg:43.65ms
step:1140/2155 train_time:49783ms step_avg:43.67ms
step:1141/2155 train_time:49843ms step_avg:43.68ms
step:1142/2155 train_time:49902ms step_avg:43.70ms
step:1143/2155 train_time:49963ms step_avg:43.71ms
step:1144/2155 train_time:50023ms step_avg:43.73ms
step:1145/2155 train_time:50084ms step_avg:43.74ms
step:1146/2155 train_time:50144ms step_avg:43.76ms
step:1147/2155 train_time:50206ms step_avg:43.77ms
step:1148/2155 train_time:50266ms step_avg:43.79ms
step:1149/2155 train_time:50328ms step_avg:43.80ms
step:1150/2155 train_time:50387ms step_avg:43.81ms
step:1151/2155 train_time:50448ms step_avg:43.83ms
step:1152/2155 train_time:50508ms step_avg:43.84ms
step:1153/2155 train_time:50570ms step_avg:43.86ms
step:1154/2155 train_time:50630ms step_avg:43.87ms
step:1155/2155 train_time:50692ms step_avg:43.89ms
step:1156/2155 train_time:50751ms step_avg:43.90ms
step:1157/2155 train_time:50813ms step_avg:43.92ms
step:1158/2155 train_time:50873ms step_avg:43.93ms
step:1159/2155 train_time:50935ms step_avg:43.95ms
step:1160/2155 train_time:50994ms step_avg:43.96ms
step:1161/2155 train_time:51056ms step_avg:43.98ms
step:1162/2155 train_time:51116ms step_avg:43.99ms
step:1163/2155 train_time:51178ms step_avg:44.01ms
step:1164/2155 train_time:51238ms step_avg:44.02ms
step:1165/2155 train_time:51298ms step_avg:44.03ms
step:1166/2155 train_time:51358ms step_avg:44.05ms
step:1167/2155 train_time:51419ms step_avg:44.06ms
step:1168/2155 train_time:51479ms step_avg:44.07ms
step:1169/2155 train_time:51540ms step_avg:44.09ms
step:1170/2155 train_time:51600ms step_avg:44.10ms
step:1171/2155 train_time:51662ms step_avg:44.12ms
step:1172/2155 train_time:51722ms step_avg:44.13ms
step:1173/2155 train_time:51782ms step_avg:44.15ms
step:1174/2155 train_time:51843ms step_avg:44.16ms
step:1175/2155 train_time:51903ms step_avg:44.17ms
step:1176/2155 train_time:51964ms step_avg:44.19ms
step:1177/2155 train_time:52025ms step_avg:44.20ms
step:1178/2155 train_time:52085ms step_avg:44.21ms
step:1179/2155 train_time:52146ms step_avg:44.23ms
step:1180/2155 train_time:52206ms step_avg:44.24ms
step:1181/2155 train_time:52267ms step_avg:44.26ms
step:1182/2155 train_time:52327ms step_avg:44.27ms
step:1183/2155 train_time:52389ms step_avg:44.28ms
step:1184/2155 train_time:52448ms step_avg:44.30ms
step:1185/2155 train_time:52510ms step_avg:44.31ms
step:1186/2155 train_time:52570ms step_avg:44.33ms
step:1187/2155 train_time:52631ms step_avg:44.34ms
step:1188/2155 train_time:52691ms step_avg:44.35ms
step:1189/2155 train_time:52753ms step_avg:44.37ms
step:1190/2155 train_time:52813ms step_avg:44.38ms
step:1191/2155 train_time:52875ms step_avg:44.40ms
step:1192/2155 train_time:52935ms step_avg:44.41ms
step:1193/2155 train_time:52996ms step_avg:44.42ms
step:1194/2155 train_time:53055ms step_avg:44.43ms
step:1195/2155 train_time:53117ms step_avg:44.45ms
step:1196/2155 train_time:53177ms step_avg:44.46ms
step:1197/2155 train_time:53238ms step_avg:44.48ms
step:1198/2155 train_time:53297ms step_avg:44.49ms
step:1199/2155 train_time:53358ms step_avg:44.50ms
step:1200/2155 train_time:53417ms step_avg:44.51ms
step:1201/2155 train_time:53479ms step_avg:44.53ms
step:1202/2155 train_time:53538ms step_avg:44.54ms
step:1203/2155 train_time:53600ms step_avg:44.55ms
step:1204/2155 train_time:53659ms step_avg:44.57ms
step:1205/2155 train_time:53722ms step_avg:44.58ms
step:1206/2155 train_time:53782ms step_avg:44.59ms
step:1207/2155 train_time:53843ms step_avg:44.61ms
step:1208/2155 train_time:53902ms step_avg:44.62ms
step:1209/2155 train_time:53964ms step_avg:44.64ms
step:1210/2155 train_time:54024ms step_avg:44.65ms
step:1211/2155 train_time:54085ms step_avg:44.66ms
step:1212/2155 train_time:54145ms step_avg:44.67ms
step:1213/2155 train_time:54207ms step_avg:44.69ms
step:1214/2155 train_time:54267ms step_avg:44.70ms
step:1215/2155 train_time:54330ms step_avg:44.72ms
step:1216/2155 train_time:54388ms step_avg:44.73ms
step:1217/2155 train_time:54450ms step_avg:44.74ms
step:1218/2155 train_time:54510ms step_avg:44.75ms
step:1219/2155 train_time:54572ms step_avg:44.77ms
step:1220/2155 train_time:54632ms step_avg:44.78ms
step:1221/2155 train_time:54694ms step_avg:44.79ms
step:1222/2155 train_time:54754ms step_avg:44.81ms
step:1223/2155 train_time:54815ms step_avg:44.82ms
step:1224/2155 train_time:54875ms step_avg:44.83ms
step:1225/2155 train_time:54937ms step_avg:44.85ms
step:1226/2155 train_time:54997ms step_avg:44.86ms
step:1227/2155 train_time:55058ms step_avg:44.87ms
step:1228/2155 train_time:55117ms step_avg:44.88ms
step:1229/2155 train_time:55178ms step_avg:44.90ms
step:1230/2155 train_time:55237ms step_avg:44.91ms
step:1231/2155 train_time:55299ms step_avg:44.92ms
step:1232/2155 train_time:55358ms step_avg:44.93ms
step:1233/2155 train_time:55419ms step_avg:44.95ms
step:1234/2155 train_time:55479ms step_avg:44.96ms
step:1235/2155 train_time:55541ms step_avg:44.97ms
step:1236/2155 train_time:55600ms step_avg:44.98ms
step:1237/2155 train_time:55661ms step_avg:45.00ms
step:1238/2155 train_time:55721ms step_avg:45.01ms
step:1239/2155 train_time:55782ms step_avg:45.02ms
step:1240/2155 train_time:55843ms step_avg:45.03ms
step:1241/2155 train_time:55903ms step_avg:45.05ms
step:1242/2155 train_time:55963ms step_avg:45.06ms
step:1243/2155 train_time:56023ms step_avg:45.07ms
step:1244/2155 train_time:56083ms step_avg:45.08ms
step:1245/2155 train_time:56145ms step_avg:45.10ms
step:1246/2155 train_time:56204ms step_avg:45.11ms
step:1247/2155 train_time:56266ms step_avg:45.12ms
step:1248/2155 train_time:56326ms step_avg:45.13ms
step:1249/2155 train_time:56388ms step_avg:45.15ms
step:1250/2155 train_time:56447ms step_avg:45.16ms
step:1250/2155 val_loss:3.6027 train_time:56511ms step_avg:45.21ms
step:1251/2155 train_time:56534ms step_avg:45.19ms
step:1252/2155 train_time:56570ms step_avg:45.18ms
step:1253/2155 train_time:56636ms step_avg:45.20ms
step:1254/2155 train_time:56697ms step_avg:45.21ms
step:1255/2155 train_time:56759ms step_avg:45.23ms
step:1256/2155 train_time:56818ms step_avg:45.24ms
step:1257/2155 train_time:56878ms step_avg:45.25ms
step:1258/2155 train_time:56937ms step_avg:45.26ms
step:1259/2155 train_time:56998ms step_avg:45.27ms
step:1260/2155 train_time:57057ms step_avg:45.28ms
step:1261/2155 train_time:57118ms step_avg:45.30ms
step:1262/2155 train_time:57177ms step_avg:45.31ms
step:1263/2155 train_time:57237ms step_avg:45.32ms
step:1264/2155 train_time:57296ms step_avg:45.33ms
step:1265/2155 train_time:57356ms step_avg:45.34ms
step:1266/2155 train_time:57415ms step_avg:45.35ms
step:1267/2155 train_time:57478ms step_avg:45.37ms
step:1268/2155 train_time:57539ms step_avg:45.38ms
step:1269/2155 train_time:57602ms step_avg:45.39ms
step:1270/2155 train_time:57663ms step_avg:45.40ms
step:1271/2155 train_time:57725ms step_avg:45.42ms
step:1272/2155 train_time:57784ms step_avg:45.43ms
step:1273/2155 train_time:57846ms step_avg:45.44ms
step:1274/2155 train_time:57905ms step_avg:45.45ms
step:1275/2155 train_time:57966ms step_avg:45.46ms
step:1276/2155 train_time:58026ms step_avg:45.47ms
step:1277/2155 train_time:58087ms step_avg:45.49ms
step:1278/2155 train_time:58146ms step_avg:45.50ms
step:1279/2155 train_time:58208ms step_avg:45.51ms
step:1280/2155 train_time:58266ms step_avg:45.52ms
step:1281/2155 train_time:58328ms step_avg:45.53ms
step:1282/2155 train_time:58387ms step_avg:45.54ms
step:1283/2155 train_time:58449ms step_avg:45.56ms
step:1284/2155 train_time:58509ms step_avg:45.57ms
step:1285/2155 train_time:58572ms step_avg:45.58ms
step:1286/2155 train_time:58633ms step_avg:45.59ms
step:1287/2155 train_time:58695ms step_avg:45.61ms
step:1288/2155 train_time:58755ms step_avg:45.62ms
step:1289/2155 train_time:58816ms step_avg:45.63ms
step:1290/2155 train_time:58875ms step_avg:45.64ms
step:1291/2155 train_time:58936ms step_avg:45.65ms
step:1292/2155 train_time:58995ms step_avg:45.66ms
step:1293/2155 train_time:59059ms step_avg:45.68ms
step:1294/2155 train_time:59116ms step_avg:45.68ms
step:1295/2155 train_time:59177ms step_avg:45.70ms
step:1296/2155 train_time:59235ms step_avg:45.71ms
step:1297/2155 train_time:59296ms step_avg:45.72ms
step:1298/2155 train_time:59355ms step_avg:45.73ms
step:1299/2155 train_time:59417ms step_avg:45.74ms
step:1300/2155 train_time:59477ms step_avg:45.75ms
step:1301/2155 train_time:59539ms step_avg:45.76ms
step:1302/2155 train_time:59600ms step_avg:45.78ms
step:1303/2155 train_time:59661ms step_avg:45.79ms
step:1304/2155 train_time:59721ms step_avg:45.80ms
step:1305/2155 train_time:59783ms step_avg:45.81ms
step:1306/2155 train_time:59842ms step_avg:45.82ms
step:1307/2155 train_time:59904ms step_avg:45.83ms
step:1308/2155 train_time:59964ms step_avg:45.84ms
step:1309/2155 train_time:60024ms step_avg:45.86ms
step:1310/2155 train_time:60083ms step_avg:45.87ms
step:1311/2155 train_time:60145ms step_avg:45.88ms
step:1312/2155 train_time:60204ms step_avg:45.89ms
step:1313/2155 train_time:60266ms step_avg:45.90ms
step:1314/2155 train_time:60325ms step_avg:45.91ms
step:1315/2155 train_time:60387ms step_avg:45.92ms
step:1316/2155 train_time:60447ms step_avg:45.93ms
step:1317/2155 train_time:60509ms step_avg:45.94ms
step:1318/2155 train_time:60569ms step_avg:45.96ms
step:1319/2155 train_time:60631ms step_avg:45.97ms
step:1320/2155 train_time:60691ms step_avg:45.98ms
step:1321/2155 train_time:60753ms step_avg:45.99ms
step:1322/2155 train_time:60814ms step_avg:46.00ms
step:1323/2155 train_time:60875ms step_avg:46.01ms
step:1324/2155 train_time:60934ms step_avg:46.02ms
step:1325/2155 train_time:60995ms step_avg:46.03ms
step:1326/2155 train_time:61054ms step_avg:46.04ms
step:1327/2155 train_time:61116ms step_avg:46.06ms
step:1328/2155 train_time:61175ms step_avg:46.07ms
step:1329/2155 train_time:61236ms step_avg:46.08ms
step:1330/2155 train_time:61296ms step_avg:46.09ms
step:1331/2155 train_time:61357ms step_avg:46.10ms
step:1332/2155 train_time:61416ms step_avg:46.11ms
step:1333/2155 train_time:61478ms step_avg:46.12ms
step:1334/2155 train_time:61538ms step_avg:46.13ms
step:1335/2155 train_time:61599ms step_avg:46.14ms
step:1336/2155 train_time:61660ms step_avg:46.15ms
step:1337/2155 train_time:61720ms step_avg:46.16ms
step:1338/2155 train_time:61780ms step_avg:46.17ms
step:1339/2155 train_time:61842ms step_avg:46.19ms
step:1340/2155 train_time:61902ms step_avg:46.20ms
step:1341/2155 train_time:61964ms step_avg:46.21ms
step:1342/2155 train_time:62024ms step_avg:46.22ms
step:1343/2155 train_time:62084ms step_avg:46.23ms
step:1344/2155 train_time:62144ms step_avg:46.24ms
step:1345/2155 train_time:62205ms step_avg:46.25ms
step:1346/2155 train_time:62265ms step_avg:46.26ms
step:1347/2155 train_time:62327ms step_avg:46.27ms
step:1348/2155 train_time:62386ms step_avg:46.28ms
step:1349/2155 train_time:62448ms step_avg:46.29ms
step:1350/2155 train_time:62508ms step_avg:46.30ms
step:1351/2155 train_time:62571ms step_avg:46.31ms
step:1352/2155 train_time:62632ms step_avg:46.33ms
step:1353/2155 train_time:62693ms step_avg:46.34ms
step:1354/2155 train_time:62753ms step_avg:46.35ms
step:1355/2155 train_time:62814ms step_avg:46.36ms
step:1356/2155 train_time:62874ms step_avg:46.37ms
step:1357/2155 train_time:62936ms step_avg:46.38ms
step:1358/2155 train_time:62995ms step_avg:46.39ms
step:1359/2155 train_time:63056ms step_avg:46.40ms
step:1360/2155 train_time:63115ms step_avg:46.41ms
step:1361/2155 train_time:63176ms step_avg:46.42ms
step:1362/2155 train_time:63235ms step_avg:46.43ms
step:1363/2155 train_time:63297ms step_avg:46.44ms
step:1364/2155 train_time:63357ms step_avg:46.45ms
step:1365/2155 train_time:63419ms step_avg:46.46ms
step:1366/2155 train_time:63478ms step_avg:46.47ms
step:1367/2155 train_time:63540ms step_avg:46.48ms
step:1368/2155 train_time:63600ms step_avg:46.49ms
step:1369/2155 train_time:63661ms step_avg:46.50ms
step:1370/2155 train_time:63720ms step_avg:46.51ms
step:1371/2155 train_time:63781ms step_avg:46.52ms
step:1372/2155 train_time:63841ms step_avg:46.53ms
step:1373/2155 train_time:63902ms step_avg:46.54ms
step:1374/2155 train_time:63963ms step_avg:46.55ms
step:1375/2155 train_time:64023ms step_avg:46.56ms
step:1376/2155 train_time:64083ms step_avg:46.57ms
step:1377/2155 train_time:64145ms step_avg:46.58ms
step:1378/2155 train_time:64205ms step_avg:46.59ms
step:1379/2155 train_time:64266ms step_avg:46.60ms
step:1380/2155 train_time:64326ms step_avg:46.61ms
step:1381/2155 train_time:64387ms step_avg:46.62ms
step:1382/2155 train_time:64447ms step_avg:46.63ms
step:1383/2155 train_time:64509ms step_avg:46.64ms
step:1384/2155 train_time:64570ms step_avg:46.65ms
step:1385/2155 train_time:64631ms step_avg:46.67ms
step:1386/2155 train_time:64690ms step_avg:46.67ms
step:1387/2155 train_time:64752ms step_avg:46.68ms
step:1388/2155 train_time:64812ms step_avg:46.69ms
step:1389/2155 train_time:64874ms step_avg:46.71ms
step:1390/2155 train_time:64934ms step_avg:46.72ms
step:1391/2155 train_time:64996ms step_avg:46.73ms
step:1392/2155 train_time:65055ms step_avg:46.74ms
step:1393/2155 train_time:65116ms step_avg:46.75ms
step:1394/2155 train_time:65175ms step_avg:46.75ms
step:1395/2155 train_time:65236ms step_avg:46.76ms
step:1396/2155 train_time:65296ms step_avg:46.77ms
step:1397/2155 train_time:65357ms step_avg:46.78ms
step:1398/2155 train_time:65417ms step_avg:46.79ms
step:1399/2155 train_time:65479ms step_avg:46.80ms
step:1400/2155 train_time:65539ms step_avg:46.81ms
step:1401/2155 train_time:65600ms step_avg:46.82ms
step:1402/2155 train_time:65660ms step_avg:46.83ms
step:1403/2155 train_time:65722ms step_avg:46.84ms
step:1404/2155 train_time:65782ms step_avg:46.85ms
step:1405/2155 train_time:65843ms step_avg:46.86ms
step:1406/2155 train_time:65903ms step_avg:46.87ms
step:1407/2155 train_time:65965ms step_avg:46.88ms
step:1408/2155 train_time:66024ms step_avg:46.89ms
step:1409/2155 train_time:66086ms step_avg:46.90ms
step:1410/2155 train_time:66146ms step_avg:46.91ms
step:1411/2155 train_time:66209ms step_avg:46.92ms
step:1412/2155 train_time:66298ms step_avg:46.95ms
step:1413/2155 train_time:66388ms step_avg:46.98ms
step:1414/2155 train_time:66476ms step_avg:47.01ms
step:1415/2155 train_time:66565ms step_avg:47.04ms
step:1416/2155 train_time:66654ms step_avg:47.07ms
step:1417/2155 train_time:66744ms step_avg:47.10ms
step:1418/2155 train_time:66832ms step_avg:47.13ms
step:1419/2155 train_time:66922ms step_avg:47.16ms
step:1420/2155 train_time:67010ms step_avg:47.19ms
step:1421/2155 train_time:67100ms step_avg:47.22ms
step:1422/2155 train_time:67188ms step_avg:47.25ms
step:1423/2155 train_time:67278ms step_avg:47.28ms
step:1424/2155 train_time:67366ms step_avg:47.31ms
step:1425/2155 train_time:67456ms step_avg:47.34ms
step:1426/2155 train_time:67544ms step_avg:47.37ms
step:1427/2155 train_time:67634ms step_avg:47.40ms
step:1428/2155 train_time:67721ms step_avg:47.42ms
step:1429/2155 train_time:67811ms step_avg:47.45ms
step:1430/2155 train_time:67900ms step_avg:47.48ms
step:1431/2155 train_time:67989ms step_avg:47.51ms
step:1432/2155 train_time:68077ms step_avg:47.54ms
step:1433/2155 train_time:68169ms step_avg:47.57ms
step:1434/2155 train_time:68254ms step_avg:47.60ms
step:1435/2155 train_time:68343ms step_avg:47.63ms
step:1436/2155 train_time:68431ms step_avg:47.65ms
step:1437/2155 train_time:68520ms step_avg:47.68ms
step:1438/2155 train_time:68608ms step_avg:47.71ms
step:1439/2155 train_time:68697ms step_avg:47.74ms
step:1440/2155 train_time:68787ms step_avg:47.77ms
step:1441/2155 train_time:68876ms step_avg:47.80ms
step:1442/2155 train_time:68963ms step_avg:47.82ms
step:1443/2155 train_time:69053ms step_avg:47.85ms
step:1444/2155 train_time:69141ms step_avg:47.88ms
step:1445/2155 train_time:69230ms step_avg:47.91ms
step:1446/2155 train_time:69318ms step_avg:47.94ms
step:1447/2155 train_time:69408ms step_avg:47.97ms
step:1448/2155 train_time:69496ms step_avg:47.99ms
step:1449/2155 train_time:69586ms step_avg:48.02ms
step:1450/2155 train_time:69673ms step_avg:48.05ms
step:1451/2155 train_time:69763ms step_avg:48.08ms
step:1452/2155 train_time:69851ms step_avg:48.11ms
step:1453/2155 train_time:69940ms step_avg:48.14ms
step:1454/2155 train_time:70028ms step_avg:48.16ms
step:1455/2155 train_time:70118ms step_avg:48.19ms
step:1456/2155 train_time:70206ms step_avg:48.22ms
step:1457/2155 train_time:70295ms step_avg:48.25ms
step:1458/2155 train_time:70384ms step_avg:48.27ms
step:1459/2155 train_time:70473ms step_avg:48.30ms
step:1460/2155 train_time:70561ms step_avg:48.33ms
step:1461/2155 train_time:70651ms step_avg:48.36ms
step:1462/2155 train_time:70738ms step_avg:48.38ms
step:1463/2155 train_time:70828ms step_avg:48.41ms
step:1464/2155 train_time:70916ms step_avg:48.44ms
step:1465/2155 train_time:71006ms step_avg:48.47ms
step:1466/2155 train_time:71094ms step_avg:48.50ms
step:1467/2155 train_time:71184ms step_avg:48.52ms
step:1468/2155 train_time:71272ms step_avg:48.55ms
step:1469/2155 train_time:71362ms step_avg:48.58ms
step:1470/2155 train_time:71449ms step_avg:48.60ms
step:1471/2155 train_time:71539ms step_avg:48.63ms
step:1472/2155 train_time:71627ms step_avg:48.66ms
step:1473/2155 train_time:71716ms step_avg:48.69ms
step:1474/2155 train_time:71804ms step_avg:48.71ms
step:1475/2155 train_time:71892ms step_avg:48.74ms
step:1476/2155 train_time:71981ms step_avg:48.77ms
step:1477/2155 train_time:72070ms step_avg:48.79ms
step:1478/2155 train_time:72158ms step_avg:48.82ms
step:1479/2155 train_time:72248ms step_avg:48.85ms
step:1480/2155 train_time:72337ms step_avg:48.88ms
step:1481/2155 train_time:72427ms step_avg:48.90ms
step:1482/2155 train_time:72514ms step_avg:48.93ms
step:1483/2155 train_time:72604ms step_avg:48.96ms
step:1484/2155 train_time:72692ms step_avg:48.98ms
step:1485/2155 train_time:72783ms step_avg:49.01ms
step:1486/2155 train_time:72870ms step_avg:49.04ms
step:1487/2155 train_time:72960ms step_avg:49.07ms
step:1488/2155 train_time:73048ms step_avg:49.09ms
step:1489/2155 train_time:73137ms step_avg:49.12ms
step:1490/2155 train_time:73224ms step_avg:49.14ms
step:1491/2155 train_time:73313ms step_avg:49.17ms
step:1492/2155 train_time:73401ms step_avg:49.20ms
step:1493/2155 train_time:73491ms step_avg:49.22ms
step:1494/2155 train_time:73579ms step_avg:49.25ms
step:1495/2155 train_time:73669ms step_avg:49.28ms
step:1496/2155 train_time:73757ms step_avg:49.30ms
step:1497/2155 train_time:73848ms step_avg:49.33ms
step:1498/2155 train_time:73936ms step_avg:49.36ms
step:1499/2155 train_time:74026ms step_avg:49.38ms
step:1500/2155 train_time:74113ms step_avg:49.41ms
step:1500/2155 val_loss:3.4931 train_time:74204ms step_avg:49.47ms
step:1501/2155 train_time:74231ms step_avg:49.45ms
step:1502/2155 train_time:74296ms step_avg:49.46ms
step:1503/2155 train_time:74393ms step_avg:49.50ms
step:1504/2155 train_time:74479ms step_avg:49.52ms
step:1505/2155 train_time:74568ms step_avg:49.55ms
step:1506/2155 train_time:74655ms step_avg:49.57ms
step:1507/2155 train_time:74743ms step_avg:49.60ms
step:1508/2155 train_time:74830ms step_avg:49.62ms
step:1509/2155 train_time:74917ms step_avg:49.65ms
step:1510/2155 train_time:75004ms step_avg:49.67ms
step:1511/2155 train_time:75097ms step_avg:49.70ms
step:1512/2155 train_time:75191ms step_avg:49.73ms
step:1513/2155 train_time:75282ms step_avg:49.76ms
step:1514/2155 train_time:75371ms step_avg:49.78ms
step:1515/2155 train_time:75461ms step_avg:49.81ms
step:1516/2155 train_time:75548ms step_avg:49.83ms
step:1517/2155 train_time:75637ms step_avg:49.86ms
step:1518/2155 train_time:75724ms step_avg:49.88ms
step:1519/2155 train_time:75812ms step_avg:49.91ms
step:1520/2155 train_time:75898ms step_avg:49.93ms
step:1521/2155 train_time:75985ms step_avg:49.96ms
step:1522/2155 train_time:76074ms step_avg:49.98ms
step:1523/2155 train_time:76167ms step_avg:50.01ms
step:1524/2155 train_time:76257ms step_avg:50.04ms
step:1525/2155 train_time:76347ms step_avg:50.06ms
step:1526/2155 train_time:76435ms step_avg:50.09ms
step:1527/2155 train_time:76524ms step_avg:50.11ms
step:1528/2155 train_time:76612ms step_avg:50.14ms
step:1529/2155 train_time:76700ms step_avg:50.16ms
step:1530/2155 train_time:76787ms step_avg:50.19ms
step:1531/2155 train_time:76876ms step_avg:50.21ms
step:1532/2155 train_time:76963ms step_avg:50.24ms
step:1533/2155 train_time:77052ms step_avg:50.26ms
step:1534/2155 train_time:77141ms step_avg:50.29ms
step:1535/2155 train_time:77231ms step_avg:50.31ms
step:1536/2155 train_time:77320ms step_avg:50.34ms
step:1537/2155 train_time:77409ms step_avg:50.36ms
step:1538/2155 train_time:77497ms step_avg:50.39ms
step:1539/2155 train_time:77587ms step_avg:50.41ms
step:1540/2155 train_time:77674ms step_avg:50.44ms
step:1541/2155 train_time:77762ms step_avg:50.46ms
step:1542/2155 train_time:77849ms step_avg:50.49ms
step:1543/2155 train_time:77938ms step_avg:50.51ms
step:1544/2155 train_time:78026ms step_avg:50.53ms
step:1545/2155 train_time:78115ms step_avg:50.56ms
step:1546/2155 train_time:78203ms step_avg:50.58ms
step:1547/2155 train_time:78293ms step_avg:50.61ms
step:1548/2155 train_time:78381ms step_avg:50.63ms
step:1549/2155 train_time:78470ms step_avg:50.66ms
step:1550/2155 train_time:78558ms step_avg:50.68ms
step:1551/2155 train_time:78647ms step_avg:50.71ms
step:1552/2155 train_time:78734ms step_avg:50.73ms
step:1553/2155 train_time:78822ms step_avg:50.75ms
step:1554/2155 train_time:78910ms step_avg:50.78ms
step:1555/2155 train_time:78999ms step_avg:50.80ms
step:1556/2155 train_time:79087ms step_avg:50.83ms
step:1557/2155 train_time:79176ms step_avg:50.85ms
step:1558/2155 train_time:79264ms step_avg:50.88ms
step:1559/2155 train_time:79354ms step_avg:50.90ms
step:1560/2155 train_time:79442ms step_avg:50.92ms
step:1561/2155 train_time:79531ms step_avg:50.95ms
step:1562/2155 train_time:79618ms step_avg:50.97ms
step:1563/2155 train_time:79706ms step_avg:51.00ms
step:1564/2155 train_time:79794ms step_avg:51.02ms
step:1565/2155 train_time:79882ms step_avg:51.04ms
step:1566/2155 train_time:79970ms step_avg:51.07ms
step:1567/2155 train_time:80060ms step_avg:51.09ms
step:1568/2155 train_time:80147ms step_avg:51.11ms
step:1569/2155 train_time:80237ms step_avg:51.14ms
step:1570/2155 train_time:80324ms step_avg:51.16ms
step:1571/2155 train_time:80415ms step_avg:51.19ms
step:1572/2155 train_time:80503ms step_avg:51.21ms
step:1573/2155 train_time:80592ms step_avg:51.23ms
step:1574/2155 train_time:80680ms step_avg:51.26ms
step:1575/2155 train_time:80769ms step_avg:51.28ms
step:1576/2155 train_time:80856ms step_avg:51.30ms
step:1577/2155 train_time:80945ms step_avg:51.33ms
step:1578/2155 train_time:81032ms step_avg:51.35ms
step:1579/2155 train_time:81121ms step_avg:51.37ms
step:1580/2155 train_time:81209ms step_avg:51.40ms
step:1581/2155 train_time:81298ms step_avg:51.42ms
step:1582/2155 train_time:81386ms step_avg:51.45ms
step:1583/2155 train_time:81476ms step_avg:51.47ms
step:1584/2155 train_time:81564ms step_avg:51.49ms
step:1585/2155 train_time:81653ms step_avg:51.52ms
step:1586/2155 train_time:81741ms step_avg:51.54ms
step:1587/2155 train_time:81829ms step_avg:51.56ms
step:1588/2155 train_time:81917ms step_avg:51.58ms
step:1589/2155 train_time:82006ms step_avg:51.61ms
step:1590/2155 train_time:82094ms step_avg:51.63ms
step:1591/2155 train_time:82183ms step_avg:51.65ms
step:1592/2155 train_time:82271ms step_avg:51.68ms
step:1593/2155 train_time:82360ms step_avg:51.70ms
step:1594/2155 train_time:82449ms step_avg:51.72ms
step:1595/2155 train_time:82539ms step_avg:51.75ms
step:1596/2155 train_time:82627ms step_avg:51.77ms
step:1597/2155 train_time:82715ms step_avg:51.79ms
step:1598/2155 train_time:82803ms step_avg:51.82ms
step:1599/2155 train_time:82890ms step_avg:51.84ms
step:1600/2155 train_time:82978ms step_avg:51.86ms
step:1601/2155 train_time:83066ms step_avg:51.88ms
step:1602/2155 train_time:83154ms step_avg:51.91ms
step:1603/2155 train_time:83243ms step_avg:51.93ms
step:1604/2155 train_time:83331ms step_avg:51.95ms
step:1605/2155 train_time:83421ms step_avg:51.98ms
step:1606/2155 train_time:83508ms step_avg:52.00ms
step:1607/2155 train_time:83598ms step_avg:52.02ms
step:1608/2155 train_time:83686ms step_avg:52.04ms
step:1609/2155 train_time:83775ms step_avg:52.07ms
step:1610/2155 train_time:83862ms step_avg:52.09ms
step:1611/2155 train_time:83951ms step_avg:52.11ms
step:1612/2155 train_time:84039ms step_avg:52.13ms
step:1613/2155 train_time:84127ms step_avg:52.16ms
step:1614/2155 train_time:84215ms step_avg:52.18ms
step:1615/2155 train_time:84304ms step_avg:52.20ms
step:1616/2155 train_time:84392ms step_avg:52.22ms
step:1617/2155 train_time:84482ms step_avg:52.25ms
step:1618/2155 train_time:84571ms step_avg:52.27ms
step:1619/2155 train_time:84661ms step_avg:52.29ms
step:1620/2155 train_time:84749ms step_avg:52.31ms
step:1621/2155 train_time:84838ms step_avg:52.34ms
step:1622/2155 train_time:84925ms step_avg:52.36ms
step:1623/2155 train_time:85014ms step_avg:52.38ms
step:1624/2155 train_time:85102ms step_avg:52.40ms
step:1625/2155 train_time:85191ms step_avg:52.43ms
step:1626/2155 train_time:85282ms step_avg:52.45ms
step:1627/2155 train_time:85369ms step_avg:52.47ms
step:1628/2155 train_time:85457ms step_avg:52.49ms
step:1629/2155 train_time:85545ms step_avg:52.51ms
step:1630/2155 train_time:85633ms step_avg:52.54ms
step:1631/2155 train_time:85722ms step_avg:52.56ms
step:1632/2155 train_time:85810ms step_avg:52.58ms
step:1633/2155 train_time:85900ms step_avg:52.60ms
step:1634/2155 train_time:85987ms step_avg:52.62ms
step:1635/2155 train_time:86076ms step_avg:52.65ms
step:1636/2155 train_time:86164ms step_avg:52.67ms
step:1637/2155 train_time:86254ms step_avg:52.69ms
step:1638/2155 train_time:86341ms step_avg:52.71ms
step:1639/2155 train_time:86431ms step_avg:52.73ms
step:1640/2155 train_time:86519ms step_avg:52.76ms
step:1641/2155 train_time:86609ms step_avg:52.78ms
step:1642/2155 train_time:86697ms step_avg:52.80ms
step:1643/2155 train_time:86787ms step_avg:52.82ms
step:1644/2155 train_time:86873ms step_avg:52.84ms
step:1645/2155 train_time:86962ms step_avg:52.86ms
step:1646/2155 train_time:87050ms step_avg:52.89ms
step:1647/2155 train_time:87140ms step_avg:52.91ms
step:1648/2155 train_time:87228ms step_avg:52.93ms
step:1649/2155 train_time:87317ms step_avg:52.95ms
step:1650/2155 train_time:87405ms step_avg:52.97ms
step:1651/2155 train_time:87494ms step_avg:52.99ms
step:1652/2155 train_time:87582ms step_avg:53.02ms
step:1653/2155 train_time:87671ms step_avg:53.04ms
step:1654/2155 train_time:87759ms step_avg:53.06ms
step:1655/2155 train_time:87849ms step_avg:53.08ms
step:1656/2155 train_time:87938ms step_avg:53.10ms
step:1657/2155 train_time:88026ms step_avg:53.12ms
step:1658/2155 train_time:88114ms step_avg:53.14ms
step:1659/2155 train_time:88203ms step_avg:53.17ms
step:1660/2155 train_time:88291ms step_avg:53.19ms
step:1661/2155 train_time:88380ms step_avg:53.21ms
step:1662/2155 train_time:88467ms step_avg:53.23ms
step:1663/2155 train_time:88557ms step_avg:53.25ms
step:1664/2155 train_time:88644ms step_avg:53.27ms
step:1665/2155 train_time:88733ms step_avg:53.29ms
step:1666/2155 train_time:88821ms step_avg:53.31ms
step:1667/2155 train_time:88910ms step_avg:53.34ms
step:1668/2155 train_time:88998ms step_avg:53.36ms
step:1669/2155 train_time:89087ms step_avg:53.38ms
step:1670/2155 train_time:89175ms step_avg:53.40ms
step:1671/2155 train_time:89264ms step_avg:53.42ms
step:1672/2155 train_time:89352ms step_avg:53.44ms
step:1673/2155 train_time:89441ms step_avg:53.46ms
step:1674/2155 train_time:89528ms step_avg:53.48ms
step:1675/2155 train_time:89617ms step_avg:53.50ms
step:1676/2155 train_time:89706ms step_avg:53.52ms
step:1677/2155 train_time:89794ms step_avg:53.54ms
step:1678/2155 train_time:89882ms step_avg:53.57ms
step:1679/2155 train_time:89972ms step_avg:53.59ms
step:1680/2155 train_time:90059ms step_avg:53.61ms
step:1681/2155 train_time:90149ms step_avg:53.63ms
step:1682/2155 train_time:90237ms step_avg:53.65ms
step:1683/2155 train_time:90326ms step_avg:53.67ms
step:1684/2155 train_time:90413ms step_avg:53.69ms
step:1685/2155 train_time:90503ms step_avg:53.71ms
step:1686/2155 train_time:90591ms step_avg:53.73ms
step:1687/2155 train_time:90680ms step_avg:53.75ms
step:1688/2155 train_time:90767ms step_avg:53.77ms
step:1689/2155 train_time:90856ms step_avg:53.79ms
step:1690/2155 train_time:90943ms step_avg:53.81ms
step:1691/2155 train_time:91032ms step_avg:53.83ms
step:1692/2155 train_time:91119ms step_avg:53.85ms
step:1693/2155 train_time:91208ms step_avg:53.87ms
step:1694/2155 train_time:91296ms step_avg:53.89ms
step:1695/2155 train_time:91386ms step_avg:53.91ms
step:1696/2155 train_time:91474ms step_avg:53.94ms
step:1697/2155 train_time:91564ms step_avg:53.96ms
step:1698/2155 train_time:91652ms step_avg:53.98ms
step:1699/2155 train_time:91742ms step_avg:54.00ms
step:1700/2155 train_time:91830ms step_avg:54.02ms
step:1701/2155 train_time:91919ms step_avg:54.04ms
step:1702/2155 train_time:92007ms step_avg:54.06ms
step:1703/2155 train_time:92097ms step_avg:54.08ms
step:1704/2155 train_time:92185ms step_avg:54.10ms
step:1705/2155 train_time:92274ms step_avg:54.12ms
step:1706/2155 train_time:92362ms step_avg:54.14ms
step:1707/2155 train_time:92451ms step_avg:54.16ms
step:1708/2155 train_time:92539ms step_avg:54.18ms
step:1709/2155 train_time:92628ms step_avg:54.20ms
step:1710/2155 train_time:92715ms step_avg:54.22ms
step:1711/2155 train_time:92804ms step_avg:54.24ms
step:1712/2155 train_time:92892ms step_avg:54.26ms
step:1713/2155 train_time:92981ms step_avg:54.28ms
step:1714/2155 train_time:93069ms step_avg:54.30ms
step:1715/2155 train_time:93158ms step_avg:54.32ms
step:1716/2155 train_time:93246ms step_avg:54.34ms
step:1717/2155 train_time:93335ms step_avg:54.36ms
step:1718/2155 train_time:93423ms step_avg:54.38ms
step:1719/2155 train_time:93512ms step_avg:54.40ms
step:1720/2155 train_time:93600ms step_avg:54.42ms
step:1721/2155 train_time:93689ms step_avg:54.44ms
step:1722/2155 train_time:93776ms step_avg:54.46ms
step:1723/2155 train_time:93865ms step_avg:54.48ms
step:1724/2155 train_time:93953ms step_avg:54.50ms
step:1725/2155 train_time:94042ms step_avg:54.52ms
step:1726/2155 train_time:94130ms step_avg:54.54ms
step:1727/2155 train_time:94219ms step_avg:54.56ms
step:1728/2155 train_time:94306ms step_avg:54.58ms
step:1729/2155 train_time:94396ms step_avg:54.60ms
step:1730/2155 train_time:94483ms step_avg:54.61ms
step:1731/2155 train_time:94572ms step_avg:54.63ms
step:1732/2155 train_time:94659ms step_avg:54.65ms
step:1733/2155 train_time:94749ms step_avg:54.67ms
step:1734/2155 train_time:94836ms step_avg:54.69ms
step:1735/2155 train_time:94925ms step_avg:54.71ms
step:1736/2155 train_time:95014ms step_avg:54.73ms
step:1737/2155 train_time:95102ms step_avg:54.75ms
step:1738/2155 train_time:95190ms step_avg:54.77ms
step:1739/2155 train_time:95279ms step_avg:54.79ms
step:1740/2155 train_time:95367ms step_avg:54.81ms
step:1741/2155 train_time:95455ms step_avg:54.83ms
step:1742/2155 train_time:95542ms step_avg:54.85ms
step:1743/2155 train_time:95632ms step_avg:54.87ms
step:1744/2155 train_time:95719ms step_avg:54.88ms
step:1745/2155 train_time:95808ms step_avg:54.90ms
step:1746/2155 train_time:95896ms step_avg:54.92ms
step:1747/2155 train_time:95984ms step_avg:54.94ms
step:1748/2155 train_time:96073ms step_avg:54.96ms
step:1749/2155 train_time:96162ms step_avg:54.98ms
step:1750/2155 train_time:96250ms step_avg:55.00ms
step:1750/2155 val_loss:3.3940 train_time:96341ms step_avg:55.05ms
step:1751/2155 train_time:96365ms step_avg:55.03ms
step:1752/2155 train_time:96433ms step_avg:55.04ms
step:1753/2155 train_time:96530ms step_avg:55.07ms
step:1754/2155 train_time:96620ms step_avg:55.09ms
step:1755/2155 train_time:96709ms step_avg:55.10ms
step:1756/2155 train_time:96795ms step_avg:55.12ms
step:1757/2155 train_time:96882ms step_avg:55.14ms
step:1758/2155 train_time:96969ms step_avg:55.16ms
step:1759/2155 train_time:97056ms step_avg:55.18ms
step:1760/2155 train_time:97143ms step_avg:55.20ms
step:1761/2155 train_time:97232ms step_avg:55.21ms
step:1762/2155 train_time:97323ms step_avg:55.23ms
step:1763/2155 train_time:97411ms step_avg:55.25ms
step:1764/2155 train_time:97502ms step_avg:55.27ms
step:1765/2155 train_time:97592ms step_avg:55.29ms
step:1766/2155 train_time:97680ms step_avg:55.31ms
step:1767/2155 train_time:97769ms step_avg:55.33ms
step:1768/2155 train_time:97855ms step_avg:55.35ms
step:1769/2155 train_time:97945ms step_avg:55.37ms
step:1770/2155 train_time:98031ms step_avg:55.38ms
step:1771/2155 train_time:98118ms step_avg:55.40ms
step:1772/2155 train_time:98206ms step_avg:55.42ms
step:1773/2155 train_time:98296ms step_avg:55.44ms
step:1774/2155 train_time:98385ms step_avg:55.46ms
step:1775/2155 train_time:98476ms step_avg:55.48ms
step:1776/2155 train_time:98565ms step_avg:55.50ms
step:1777/2155 train_time:98656ms step_avg:55.52ms
step:1778/2155 train_time:98743ms step_avg:55.54ms
step:1779/2155 train_time:98832ms step_avg:55.55ms
step:1780/2155 train_time:98919ms step_avg:55.57ms
step:1781/2155 train_time:99007ms step_avg:55.59ms
step:1782/2155 train_time:99095ms step_avg:55.61ms
step:1783/2155 train_time:99183ms step_avg:55.63ms
step:1784/2155 train_time:99272ms step_avg:55.65ms
step:1785/2155 train_time:99361ms step_avg:55.66ms
step:1786/2155 train_time:99450ms step_avg:55.68ms
step:1787/2155 train_time:99539ms step_avg:55.70ms
step:1788/2155 train_time:99628ms step_avg:55.72ms
step:1789/2155 train_time:99717ms step_avg:55.74ms
step:1790/2155 train_time:99805ms step_avg:55.76ms
step:1791/2155 train_time:99893ms step_avg:55.77ms
step:1792/2155 train_time:99979ms step_avg:55.79ms
step:1793/2155 train_time:100068ms step_avg:55.81ms
step:1794/2155 train_time:100154ms step_avg:55.83ms
step:1795/2155 train_time:100242ms step_avg:55.85ms
step:1796/2155 train_time:100331ms step_avg:55.86ms
step:1797/2155 train_time:100420ms step_avg:55.88ms
step:1798/2155 train_time:100510ms step_avg:55.90ms
step:1799/2155 train_time:100600ms step_avg:55.92ms
step:1800/2155 train_time:100688ms step_avg:55.94ms
step:1801/2155 train_time:100777ms step_avg:55.96ms
step:1802/2155 train_time:100864ms step_avg:55.97ms
step:1803/2155 train_time:100952ms step_avg:55.99ms
step:1804/2155 train_time:101040ms step_avg:56.01ms
step:1805/2155 train_time:101129ms step_avg:56.03ms
step:1806/2155 train_time:101216ms step_avg:56.04ms
step:1807/2155 train_time:101305ms step_avg:56.06ms
step:1808/2155 train_time:101394ms step_avg:56.08ms
step:1809/2155 train_time:101483ms step_avg:56.10ms
step:1810/2155 train_time:101571ms step_avg:56.12ms
step:1811/2155 train_time:101660ms step_avg:56.13ms
step:1812/2155 train_time:101748ms step_avg:56.15ms
step:1813/2155 train_time:101837ms step_avg:56.17ms
step:1814/2155 train_time:101925ms step_avg:56.19ms
step:1815/2155 train_time:102014ms step_avg:56.21ms
step:1816/2155 train_time:102100ms step_avg:56.22ms
step:1817/2155 train_time:102190ms step_avg:56.24ms
step:1818/2155 train_time:102277ms step_avg:56.26ms
step:1819/2155 train_time:102367ms step_avg:56.28ms
step:1820/2155 train_time:102455ms step_avg:56.29ms
step:1821/2155 train_time:102544ms step_avg:56.31ms
step:1822/2155 train_time:102632ms step_avg:56.33ms
step:1823/2155 train_time:102722ms step_avg:56.35ms
step:1824/2155 train_time:102809ms step_avg:56.36ms
step:1825/2155 train_time:102898ms step_avg:56.38ms
step:1826/2155 train_time:102986ms step_avg:56.40ms
step:1827/2155 train_time:103074ms step_avg:56.42ms
step:1828/2155 train_time:103162ms step_avg:56.43ms
step:1829/2155 train_time:103250ms step_avg:56.45ms
step:1830/2155 train_time:103337ms step_avg:56.47ms
step:1831/2155 train_time:103426ms step_avg:56.49ms
step:1832/2155 train_time:103514ms step_avg:56.50ms
step:1833/2155 train_time:103604ms step_avg:56.52ms
step:1834/2155 train_time:103691ms step_avg:56.54ms
step:1835/2155 train_time:103780ms step_avg:56.56ms
step:1836/2155 train_time:103868ms step_avg:56.57ms
step:1837/2155 train_time:103957ms step_avg:56.59ms
step:1838/2155 train_time:104045ms step_avg:56.61ms
step:1839/2155 train_time:104134ms step_avg:56.63ms
step:1840/2155 train_time:104221ms step_avg:56.64ms
step:1841/2155 train_time:104311ms step_avg:56.66ms
step:1842/2155 train_time:104399ms step_avg:56.68ms
step:1843/2155 train_time:104488ms step_avg:56.69ms
step:1844/2155 train_time:104577ms step_avg:56.71ms
step:1845/2155 train_time:104665ms step_avg:56.73ms
step:1846/2155 train_time:104753ms step_avg:56.75ms
step:1847/2155 train_time:104842ms step_avg:56.76ms
step:1848/2155 train_time:104931ms step_avg:56.78ms
step:1849/2155 train_time:105018ms step_avg:56.80ms
step:1850/2155 train_time:105105ms step_avg:56.81ms
step:1851/2155 train_time:105195ms step_avg:56.83ms
step:1852/2155 train_time:105282ms step_avg:56.85ms
step:1853/2155 train_time:105371ms step_avg:56.87ms
step:1854/2155 train_time:105459ms step_avg:56.88ms
step:1855/2155 train_time:105548ms step_avg:56.90ms
step:1856/2155 train_time:105636ms step_avg:56.92ms
step:1857/2155 train_time:105725ms step_avg:56.93ms
step:1858/2155 train_time:105813ms step_avg:56.95ms
step:1859/2155 train_time:105901ms step_avg:56.97ms
step:1860/2155 train_time:105988ms step_avg:56.98ms
step:1861/2155 train_time:106077ms step_avg:57.00ms
step:1862/2155 train_time:106165ms step_avg:57.02ms
step:1863/2155 train_time:106254ms step_avg:57.03ms
step:1864/2155 train_time:106342ms step_avg:57.05ms
step:1865/2155 train_time:106431ms step_avg:57.07ms
step:1866/2155 train_time:106520ms step_avg:57.08ms
step:1867/2155 train_time:106610ms step_avg:57.10ms
step:1868/2155 train_time:106697ms step_avg:57.12ms
step:1869/2155 train_time:106787ms step_avg:57.14ms
step:1870/2155 train_time:106874ms step_avg:57.15ms
step:1871/2155 train_time:106963ms step_avg:57.17ms
step:1872/2155 train_time:107050ms step_avg:57.18ms
step:1873/2155 train_time:107138ms step_avg:57.20ms
step:1874/2155 train_time:107226ms step_avg:57.22ms
step:1875/2155 train_time:107316ms step_avg:57.23ms
step:1876/2155 train_time:107404ms step_avg:57.25ms
step:1877/2155 train_time:107493ms step_avg:57.27ms
step:1878/2155 train_time:107580ms step_avg:57.28ms
step:1879/2155 train_time:107670ms step_avg:57.30ms
step:1880/2155 train_time:107757ms step_avg:57.32ms
step:1881/2155 train_time:107845ms step_avg:57.33ms
step:1882/2155 train_time:107933ms step_avg:57.35ms
step:1883/2155 train_time:108022ms step_avg:57.37ms
step:1884/2155 train_time:108110ms step_avg:57.38ms
step:1885/2155 train_time:108198ms step_avg:57.40ms
step:1886/2155 train_time:108286ms step_avg:57.42ms
step:1887/2155 train_time:108376ms step_avg:57.43ms
step:1888/2155 train_time:108464ms step_avg:57.45ms
step:1889/2155 train_time:108553ms step_avg:57.47ms
step:1890/2155 train_time:108641ms step_avg:57.48ms
step:1891/2155 train_time:108730ms step_avg:57.50ms
step:1892/2155 train_time:108818ms step_avg:57.51ms
step:1893/2155 train_time:108907ms step_avg:57.53ms
step:1894/2155 train_time:108994ms step_avg:57.55ms
step:1895/2155 train_time:109083ms step_avg:57.56ms
step:1896/2155 train_time:109170ms step_avg:57.58ms
step:1897/2155 train_time:109259ms step_avg:57.60ms
step:1898/2155 train_time:109346ms step_avg:57.61ms
step:1899/2155 train_time:109437ms step_avg:57.63ms
step:1900/2155 train_time:109525ms step_avg:57.64ms
step:1901/2155 train_time:109614ms step_avg:57.66ms
step:1902/2155 train_time:109702ms step_avg:57.68ms
step:1903/2155 train_time:109792ms step_avg:57.69ms
step:1904/2155 train_time:109879ms step_avg:57.71ms
step:1905/2155 train_time:109969ms step_avg:57.73ms
step:1906/2155 train_time:110056ms step_avg:57.74ms
step:1907/2155 train_time:110145ms step_avg:57.76ms
step:1908/2155 train_time:110232ms step_avg:57.77ms
step:1909/2155 train_time:110321ms step_avg:57.79ms
step:1910/2155 train_time:110409ms step_avg:57.81ms
step:1911/2155 train_time:110498ms step_avg:57.82ms
step:1912/2155 train_time:110587ms step_avg:57.84ms
step:1913/2155 train_time:110676ms step_avg:57.85ms
step:1914/2155 train_time:110764ms step_avg:57.87ms
step:1915/2155 train_time:110853ms step_avg:57.89ms
step:1916/2155 train_time:110940ms step_avg:57.90ms
step:1917/2155 train_time:111030ms step_avg:57.92ms
step:1918/2155 train_time:111117ms step_avg:57.93ms
step:1919/2155 train_time:111206ms step_avg:57.95ms
step:1920/2155 train_time:111293ms step_avg:57.97ms
step:1921/2155 train_time:111382ms step_avg:57.98ms
step:1922/2155 train_time:111470ms step_avg:58.00ms
step:1923/2155 train_time:111558ms step_avg:58.01ms
step:1924/2155 train_time:111646ms step_avg:58.03ms
step:1925/2155 train_time:111735ms step_avg:58.04ms
step:1926/2155 train_time:111822ms step_avg:58.06ms
step:1927/2155 train_time:111912ms step_avg:58.08ms
step:1928/2155 train_time:111999ms step_avg:58.09ms
step:1929/2155 train_time:112089ms step_avg:58.11ms
step:1930/2155 train_time:112176ms step_avg:58.12ms
step:1931/2155 train_time:112265ms step_avg:58.14ms
step:1932/2155 train_time:112353ms step_avg:58.15ms
step:1933/2155 train_time:112443ms step_avg:58.17ms
step:1934/2155 train_time:112530ms step_avg:58.19ms
step:1935/2155 train_time:112619ms step_avg:58.20ms
step:1936/2155 train_time:112707ms step_avg:58.22ms
step:1937/2155 train_time:112796ms step_avg:58.23ms
step:1938/2155 train_time:112884ms step_avg:58.25ms
step:1939/2155 train_time:112974ms step_avg:58.26ms
step:1940/2155 train_time:113062ms step_avg:58.28ms
step:1941/2155 train_time:113151ms step_avg:58.30ms
step:1942/2155 train_time:113238ms step_avg:58.31ms
step:1943/2155 train_time:113328ms step_avg:58.33ms
step:1944/2155 train_time:113415ms step_avg:58.34ms
step:1945/2155 train_time:113504ms step_avg:58.36ms
step:1946/2155 train_time:113591ms step_avg:58.37ms
step:1947/2155 train_time:113680ms step_avg:58.39ms
step:1948/2155 train_time:113768ms step_avg:58.40ms
step:1949/2155 train_time:113857ms step_avg:58.42ms
step:1950/2155 train_time:113945ms step_avg:58.43ms
step:1951/2155 train_time:114035ms step_avg:58.45ms
step:1952/2155 train_time:114123ms step_avg:58.46ms
step:1953/2155 train_time:114213ms step_avg:58.48ms
step:1954/2155 train_time:114300ms step_avg:58.50ms
step:1955/2155 train_time:114388ms step_avg:58.51ms
step:1956/2155 train_time:114476ms step_avg:58.53ms
step:1957/2155 train_time:114565ms step_avg:58.54ms
step:1958/2155 train_time:114652ms step_avg:58.56ms
step:1959/2155 train_time:114740ms step_avg:58.57ms
step:1960/2155 train_time:114828ms step_avg:58.59ms
step:1961/2155 train_time:114918ms step_avg:58.60ms
step:1962/2155 train_time:115005ms step_avg:58.62ms
step:1963/2155 train_time:115094ms step_avg:58.63ms
step:1964/2155 train_time:115182ms step_avg:58.65ms
step:1965/2155 train_time:115271ms step_avg:58.66ms
step:1966/2155 train_time:115358ms step_avg:58.68ms
step:1967/2155 train_time:115447ms step_avg:58.69ms
step:1968/2155 train_time:115534ms step_avg:58.71ms
step:1969/2155 train_time:115623ms step_avg:58.72ms
step:1970/2155 train_time:115710ms step_avg:58.74ms
step:1971/2155 train_time:115799ms step_avg:58.75ms
step:1972/2155 train_time:115887ms step_avg:58.77ms
step:1973/2155 train_time:115976ms step_avg:58.78ms
step:1974/2155 train_time:116064ms step_avg:58.80ms
step:1975/2155 train_time:116152ms step_avg:58.81ms
step:1976/2155 train_time:116240ms step_avg:58.83ms
step:1977/2155 train_time:116330ms step_avg:58.84ms
step:1978/2155 train_time:116418ms step_avg:58.86ms
step:1979/2155 train_time:116507ms step_avg:58.87ms
step:1980/2155 train_time:116595ms step_avg:58.89ms
step:1981/2155 train_time:116684ms step_avg:58.90ms
step:1982/2155 train_time:116771ms step_avg:58.92ms
step:1983/2155 train_time:116860ms step_avg:58.93ms
step:1984/2155 train_time:116948ms step_avg:58.95ms
step:1985/2155 train_time:117037ms step_avg:58.96ms
step:1986/2155 train_time:117125ms step_avg:58.98ms
step:1987/2155 train_time:117214ms step_avg:58.99ms
step:1988/2155 train_time:117301ms step_avg:59.00ms
step:1989/2155 train_time:117390ms step_avg:59.02ms
step:1990/2155 train_time:117477ms step_avg:59.03ms
step:1991/2155 train_time:117566ms step_avg:59.05ms
step:1992/2155 train_time:117654ms step_avg:59.06ms
step:1993/2155 train_time:117742ms step_avg:59.08ms
step:1994/2155 train_time:117831ms step_avg:59.09ms
step:1995/2155 train_time:117920ms step_avg:59.11ms
step:1996/2155 train_time:118008ms step_avg:59.12ms
step:1997/2155 train_time:118098ms step_avg:59.14ms
step:1998/2155 train_time:118186ms step_avg:59.15ms
step:1999/2155 train_time:118275ms step_avg:59.17ms
step:2000/2155 train_time:118362ms step_avg:59.18ms
step:2000/2155 val_loss:3.3173 train_time:118453ms step_avg:59.23ms
step:2001/2155 train_time:118477ms step_avg:59.21ms
step:2002/2155 train_time:118542ms step_avg:59.21ms
step:2003/2155 train_time:118635ms step_avg:59.23ms
step:2004/2155 train_time:118724ms step_avg:59.24ms
step:2005/2155 train_time:118811ms step_avg:59.26ms
step:2006/2155 train_time:118897ms step_avg:59.27ms
step:2007/2155 train_time:118985ms step_avg:59.29ms
step:2008/2155 train_time:119073ms step_avg:59.30ms
step:2009/2155 train_time:119160ms step_avg:59.31ms
step:2010/2155 train_time:119248ms step_avg:59.33ms
step:2011/2155 train_time:119336ms step_avg:59.34ms
step:2012/2155 train_time:119425ms step_avg:59.36ms
step:2013/2155 train_time:119517ms step_avg:59.37ms
step:2014/2155 train_time:119606ms step_avg:59.39ms
step:2015/2155 train_time:119697ms step_avg:59.40ms
step:2016/2155 train_time:119784ms step_avg:59.42ms
step:2017/2155 train_time:119872ms step_avg:59.43ms
step:2018/2155 train_time:119959ms step_avg:59.44ms
step:2019/2155 train_time:120047ms step_avg:59.46ms
step:2020/2155 train_time:120134ms step_avg:59.47ms
step:2021/2155 train_time:120222ms step_avg:59.49ms
step:2022/2155 train_time:120310ms step_avg:59.50ms
step:2023/2155 train_time:120400ms step_avg:59.52ms
step:2024/2155 train_time:120488ms step_avg:59.53ms
step:2025/2155 train_time:120578ms step_avg:59.54ms
step:2026/2155 train_time:120666ms step_avg:59.56ms
step:2027/2155 train_time:120756ms step_avg:59.57ms
step:2028/2155 train_time:120843ms step_avg:59.59ms
step:2029/2155 train_time:120932ms step_avg:59.60ms
step:2030/2155 train_time:121019ms step_avg:59.62ms
step:2031/2155 train_time:121106ms step_avg:59.63ms
step:2032/2155 train_time:121193ms step_avg:59.64ms
step:2033/2155 train_time:121282ms step_avg:59.66ms
step:2034/2155 train_time:121370ms step_avg:59.67ms
step:2035/2155 train_time:121462ms step_avg:59.69ms
step:2036/2155 train_time:121549ms step_avg:59.70ms
step:2037/2155 train_time:121638ms step_avg:59.71ms
step:2038/2155 train_time:121726ms step_avg:59.73ms
step:2039/2155 train_time:121816ms step_avg:59.74ms
step:2040/2155 train_time:121904ms step_avg:59.76ms
step:2041/2155 train_time:121992ms step_avg:59.77ms
step:2042/2155 train_time:122079ms step_avg:59.78ms
step:2043/2155 train_time:122167ms step_avg:59.80ms
step:2044/2155 train_time:122253ms step_avg:59.81ms
step:2045/2155 train_time:122343ms step_avg:59.83ms
step:2046/2155 train_time:122432ms step_avg:59.84ms
step:2047/2155 train_time:122522ms step_avg:59.85ms
step:2048/2155 train_time:122610ms step_avg:59.87ms
step:2049/2155 train_time:122699ms step_avg:59.88ms
step:2050/2155 train_time:122788ms step_avg:59.90ms
step:2051/2155 train_time:122878ms step_avg:59.91ms
step:2052/2155 train_time:122965ms step_avg:59.92ms
step:2053/2155 train_time:123054ms step_avg:59.94ms
step:2054/2155 train_time:123141ms step_avg:59.95ms
step:2055/2155 train_time:123230ms step_avg:59.97ms
step:2056/2155 train_time:123317ms step_avg:59.98ms
step:2057/2155 train_time:123405ms step_avg:59.99ms
step:2058/2155 train_time:123493ms step_avg:60.01ms
step:2059/2155 train_time:123583ms step_avg:60.02ms
step:2060/2155 train_time:123670ms step_avg:60.03ms
step:2061/2155 train_time:123760ms step_avg:60.05ms
step:2062/2155 train_time:123848ms step_avg:60.06ms
step:2063/2155 train_time:123937ms step_avg:60.08ms
step:2064/2155 train_time:124024ms step_avg:60.09ms
step:2065/2155 train_time:124113ms step_avg:60.10ms
step:2066/2155 train_time:124200ms step_avg:60.12ms
step:2067/2155 train_time:124289ms step_avg:60.13ms
step:2068/2155 train_time:124377ms step_avg:60.14ms
step:2069/2155 train_time:124466ms step_avg:60.16ms
step:2070/2155 train_time:124554ms step_avg:60.17ms
step:2071/2155 train_time:124643ms step_avg:60.18ms
step:2072/2155 train_time:124730ms step_avg:60.20ms
step:2073/2155 train_time:124820ms step_avg:60.21ms
step:2074/2155 train_time:124906ms step_avg:60.22ms
step:2075/2155 train_time:124996ms step_avg:60.24ms
step:2076/2155 train_time:125083ms step_avg:60.25ms
step:2077/2155 train_time:125172ms step_avg:60.27ms
step:2078/2155 train_time:125260ms step_avg:60.28ms
step:2079/2155 train_time:125349ms step_avg:60.29ms
step:2080/2155 train_time:125436ms step_avg:60.31ms
step:2081/2155 train_time:125525ms step_avg:60.32ms
step:2082/2155 train_time:125613ms step_avg:60.33ms
step:2083/2155 train_time:125703ms step_avg:60.35ms
step:2084/2155 train_time:125791ms step_avg:60.36ms
step:2085/2155 train_time:125880ms step_avg:60.37ms
step:2086/2155 train_time:125967ms step_avg:60.39ms
step:2087/2155 train_time:126057ms step_avg:60.40ms
step:2088/2155 train_time:126145ms step_avg:60.41ms
step:2089/2155 train_time:126234ms step_avg:60.43ms
step:2090/2155 train_time:126322ms step_avg:60.44ms
step:2091/2155 train_time:126411ms step_avg:60.45ms
step:2092/2155 train_time:126498ms step_avg:60.47ms
step:2093/2155 train_time:126587ms step_avg:60.48ms
step:2094/2155 train_time:126674ms step_avg:60.49ms
step:2095/2155 train_time:126764ms step_avg:60.51ms
step:2096/2155 train_time:126852ms step_avg:60.52ms
step:2097/2155 train_time:126940ms step_avg:60.53ms
step:2098/2155 train_time:127028ms step_avg:60.55ms
step:2099/2155 train_time:127117ms step_avg:60.56ms
step:2100/2155 train_time:127204ms step_avg:60.57ms
step:2101/2155 train_time:127293ms step_avg:60.59ms
step:2102/2155 train_time:127381ms step_avg:60.60ms
step:2103/2155 train_time:127470ms step_avg:60.61ms
step:2104/2155 train_time:127557ms step_avg:60.63ms
step:2105/2155 train_time:127646ms step_avg:60.64ms
step:2106/2155 train_time:127734ms step_avg:60.65ms
step:2107/2155 train_time:127823ms step_avg:60.67ms
step:2108/2155 train_time:127910ms step_avg:60.68ms
step:2109/2155 train_time:127999ms step_avg:60.69ms
step:2110/2155 train_time:128087ms step_avg:60.70ms
step:2111/2155 train_time:128176ms step_avg:60.72ms
step:2112/2155 train_time:128264ms step_avg:60.73ms
step:2113/2155 train_time:128352ms step_avg:60.74ms
step:2114/2155 train_time:128440ms step_avg:60.76ms
step:2115/2155 train_time:128529ms step_avg:60.77ms
step:2116/2155 train_time:128616ms step_avg:60.78ms
step:2117/2155 train_time:128705ms step_avg:60.80ms
step:2118/2155 train_time:128793ms step_avg:60.81ms
step:2119/2155 train_time:128883ms step_avg:60.82ms
step:2120/2155 train_time:128972ms step_avg:60.84ms
step:2121/2155 train_time:129061ms step_avg:60.85ms
step:2122/2155 train_time:129149ms step_avg:60.86ms
step:2123/2155 train_time:129238ms step_avg:60.88ms
step:2124/2155 train_time:129325ms step_avg:60.89ms
step:2125/2155 train_time:129415ms step_avg:60.90ms
step:2126/2155 train_time:129503ms step_avg:60.91ms
step:2127/2155 train_time:129592ms step_avg:60.93ms
step:2128/2155 train_time:129681ms step_avg:60.94ms
step:2129/2155 train_time:129769ms step_avg:60.95ms
step:2130/2155 train_time:129857ms step_avg:60.97ms
step:2131/2155 train_time:129947ms step_avg:60.98ms
step:2132/2155 train_time:130035ms step_avg:60.99ms
step:2133/2155 train_time:130125ms step_avg:61.01ms
step:2134/2155 train_time:130213ms step_avg:61.02ms
step:2135/2155 train_time:130302ms step_avg:61.03ms
step:2136/2155 train_time:130390ms step_avg:61.04ms
step:2137/2155 train_time:130479ms step_avg:61.06ms
step:2138/2155 train_time:130567ms step_avg:61.07ms
step:2139/2155 train_time:130656ms step_avg:61.08ms
step:2140/2155 train_time:130744ms step_avg:61.10ms
step:2141/2155 train_time:130833ms step_avg:61.11ms
step:2142/2155 train_time:130921ms step_avg:61.12ms
step:2143/2155 train_time:131010ms step_avg:61.13ms
step:2144/2155 train_time:131099ms step_avg:61.15ms
step:2145/2155 train_time:131188ms step_avg:61.16ms
step:2146/2155 train_time:131275ms step_avg:61.17ms
step:2147/2155 train_time:131364ms step_avg:61.19ms
step:2148/2155 train_time:131452ms step_avg:61.20ms
step:2149/2155 train_time:131541ms step_avg:61.21ms
step:2150/2155 train_time:131629ms step_avg:61.22ms
step:2151/2155 train_time:131718ms step_avg:61.24ms
step:2152/2155 train_time:131805ms step_avg:61.25ms
step:2153/2155 train_time:131895ms step_avg:61.26ms
step:2154/2155 train_time:131983ms step_avg:61.27ms
step:2155/2155 train_time:132073ms step_avg:61.29ms
step:2155/2155 val_loss:3.2813 train_time:132162ms step_avg:61.33ms
peak memory allocated: 29892 MiB reserved: 44796 MiB

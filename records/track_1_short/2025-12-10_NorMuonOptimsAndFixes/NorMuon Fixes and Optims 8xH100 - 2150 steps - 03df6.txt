import uuid
run_id = f"NorMuon Fixes and Optims 8xH100 - 2150 steps - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2110  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 10 2025, 18:15:56) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 19:12:06 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   44C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   35C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   44C    P0            131W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   44C    P0            128W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   34C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   43C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   34C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           28708      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           28709      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           28710      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           28711      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           28712      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           28713      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           28714      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           28715      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           28709      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           28710      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           28711      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           28712      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           28713      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           28714      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           28715      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2150 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2150 train_time:133ms step_avg:132.77ms
step:2/2150 train_time:427ms step_avg:213.32ms
step:3/2150 train_time:507ms step_avg:169.02ms
step:4/2150 train_time:602ms step_avg:150.49ms
step:5/2150 train_time:835ms step_avg:167.05ms
step:6/2150 train_time:1510ms step_avg:251.59ms
step:7/2150 train_time:1559ms step_avg:222.70ms
step:8/2150 train_time:1592ms step_avg:199.03ms
step:9/2150 train_time:1624ms step_avg:180.48ms
step:10/2150 train_time:1658ms step_avg:165.77ms
step:11/2150 train_time:1690ms step_avg:153.65ms
step:12/2150 train_time:1724ms step_avg:143.64ms
step:13/2150 train_time:1756ms step_avg:135.08ms
step:14/2150 train_time:1790ms step_avg:127.83ms
step:15/2150 train_time:1822ms step_avg:121.48ms
step:16/2150 train_time:1856ms step_avg:115.98ms
step:17/2150 train_time:1888ms step_avg:111.08ms
step:18/2150 train_time:1922ms step_avg:106.78ms
step:19/2150 train_time:1954ms step_avg:102.86ms
step:20/2150 train_time:1988ms step_avg:99.40ms
step:21/2150 train_time:2021ms step_avg:96.24ms
step:22/2150 train_time:2055ms step_avg:93.39ms
step:23/2150 train_time:2087ms step_avg:90.74ms
step:24/2150 train_time:2121ms step_avg:88.36ms
step:25/2150 train_time:2154ms step_avg:86.15ms
step:26/2150 train_time:2187ms step_avg:84.12ms
step:27/2150 train_time:2220ms step_avg:82.21ms
step:28/2150 train_time:2254ms step_avg:80.49ms
step:29/2150 train_time:2286ms step_avg:78.82ms
step:30/2150 train_time:2319ms step_avg:77.31ms
step:31/2150 train_time:2352ms step_avg:75.86ms
step:32/2150 train_time:2386ms step_avg:74.55ms
step:33/2150 train_time:2418ms step_avg:73.27ms
step:34/2150 train_time:2452ms step_avg:72.12ms
step:35/2150 train_time:2486ms step_avg:71.03ms
step:36/2150 train_time:2520ms step_avg:70.00ms
step:37/2150 train_time:2554ms step_avg:69.03ms
step:38/2150 train_time:2588ms step_avg:68.10ms
step:39/2150 train_time:2621ms step_avg:67.21ms
step:40/2150 train_time:2655ms step_avg:66.37ms
step:41/2150 train_time:2688ms step_avg:65.57ms
step:42/2150 train_time:2722ms step_avg:64.80ms
step:43/2150 train_time:2755ms step_avg:64.07ms
step:44/2150 train_time:2788ms step_avg:63.37ms
step:45/2150 train_time:2821ms step_avg:62.69ms
step:46/2150 train_time:2855ms step_avg:62.06ms
step:47/2150 train_time:2888ms step_avg:61.44ms
step:48/2150 train_time:2921ms step_avg:60.86ms
step:49/2150 train_time:2954ms step_avg:60.29ms
step:50/2150 train_time:2988ms step_avg:59.76ms
step:51/2150 train_time:3020ms step_avg:59.22ms
step:52/2150 train_time:3054ms step_avg:58.73ms
step:53/2150 train_time:3086ms step_avg:58.23ms
step:54/2150 train_time:3120ms step_avg:57.78ms
step:55/2150 train_time:3153ms step_avg:57.32ms
step:56/2150 train_time:3186ms step_avg:56.90ms
step:57/2150 train_time:3219ms step_avg:56.47ms
step:58/2150 train_time:3252ms step_avg:56.08ms
step:59/2150 train_time:3285ms step_avg:55.67ms
step:60/2150 train_time:3318ms step_avg:55.30ms
step:61/2150 train_time:3351ms step_avg:54.93ms
step:62/2150 train_time:3384ms step_avg:54.59ms
step:63/2150 train_time:3417ms step_avg:54.24ms
step:64/2150 train_time:3451ms step_avg:53.92ms
step:65/2150 train_time:3484ms step_avg:53.60ms
step:66/2150 train_time:3518ms step_avg:53.30ms
step:67/2150 train_time:3551ms step_avg:53.00ms
step:68/2150 train_time:3585ms step_avg:52.71ms
step:69/2150 train_time:3618ms step_avg:52.43ms
step:70/2150 train_time:3651ms step_avg:52.16ms
step:71/2150 train_time:3684ms step_avg:51.89ms
step:72/2150 train_time:3718ms step_avg:51.64ms
step:73/2150 train_time:3751ms step_avg:51.38ms
step:74/2150 train_time:3784ms step_avg:51.14ms
step:75/2150 train_time:3817ms step_avg:50.90ms
step:76/2150 train_time:3851ms step_avg:50.67ms
step:77/2150 train_time:3884ms step_avg:50.44ms
step:78/2150 train_time:3918ms step_avg:50.23ms
step:79/2150 train_time:3951ms step_avg:50.01ms
step:80/2150 train_time:3984ms step_avg:49.80ms
step:81/2150 train_time:4017ms step_avg:49.59ms
step:82/2150 train_time:4050ms step_avg:49.39ms
step:83/2150 train_time:4083ms step_avg:49.19ms
step:84/2150 train_time:4116ms step_avg:49.01ms
step:85/2150 train_time:4149ms step_avg:48.81ms
step:86/2150 train_time:4182ms step_avg:48.63ms
step:87/2150 train_time:4215ms step_avg:48.45ms
step:88/2150 train_time:4248ms step_avg:48.28ms
step:89/2150 train_time:4281ms step_avg:48.10ms
step:90/2150 train_time:4315ms step_avg:47.94ms
step:91/2150 train_time:4347ms step_avg:47.77ms
step:92/2150 train_time:4381ms step_avg:47.62ms
step:93/2150 train_time:4414ms step_avg:47.46ms
step:94/2150 train_time:4447ms step_avg:47.31ms
step:95/2150 train_time:4480ms step_avg:47.16ms
step:96/2150 train_time:4514ms step_avg:47.02ms
step:97/2150 train_time:4546ms step_avg:46.87ms
step:98/2150 train_time:4580ms step_avg:46.73ms
step:99/2150 train_time:4613ms step_avg:46.59ms
step:100/2150 train_time:4647ms step_avg:46.47ms
step:101/2150 train_time:4679ms step_avg:46.33ms
step:102/2150 train_time:4713ms step_avg:46.20ms
step:103/2150 train_time:4745ms step_avg:46.07ms
step:104/2150 train_time:4779ms step_avg:45.95ms
step:105/2150 train_time:4811ms step_avg:45.82ms
step:106/2150 train_time:4845ms step_avg:45.70ms
step:107/2150 train_time:4878ms step_avg:45.59ms
step:108/2150 train_time:4912ms step_avg:45.48ms
step:109/2150 train_time:4944ms step_avg:45.36ms
step:110/2150 train_time:4978ms step_avg:45.25ms
step:111/2150 train_time:5010ms step_avg:45.14ms
step:112/2150 train_time:5044ms step_avg:45.03ms
step:113/2150 train_time:5077ms step_avg:44.92ms
step:114/2150 train_time:5110ms step_avg:44.83ms
step:115/2150 train_time:5143ms step_avg:44.72ms
step:116/2150 train_time:5176ms step_avg:44.62ms
step:117/2150 train_time:5209ms step_avg:44.52ms
step:118/2150 train_time:5242ms step_avg:44.43ms
step:119/2150 train_time:5275ms step_avg:44.33ms
step:120/2150 train_time:5309ms step_avg:44.24ms
step:121/2150 train_time:5341ms step_avg:44.14ms
step:122/2150 train_time:5375ms step_avg:44.06ms
step:123/2150 train_time:5408ms step_avg:43.97ms
step:124/2150 train_time:5441ms step_avg:43.88ms
step:125/2150 train_time:5474ms step_avg:43.79ms
step:126/2150 train_time:5507ms step_avg:43.71ms
step:127/2150 train_time:5540ms step_avg:43.62ms
step:128/2150 train_time:5574ms step_avg:43.54ms
step:129/2150 train_time:5606ms step_avg:43.46ms
step:130/2150 train_time:5640ms step_avg:43.39ms
step:131/2150 train_time:5673ms step_avg:43.31ms
step:132/2150 train_time:5707ms step_avg:43.23ms
step:133/2150 train_time:5740ms step_avg:43.15ms
step:134/2150 train_time:5773ms step_avg:43.08ms
step:135/2150 train_time:5806ms step_avg:43.01ms
step:136/2150 train_time:5839ms step_avg:42.94ms
step:137/2150 train_time:5872ms step_avg:42.86ms
step:138/2150 train_time:5905ms step_avg:42.79ms
step:139/2150 train_time:5938ms step_avg:42.72ms
step:140/2150 train_time:5971ms step_avg:42.65ms
step:141/2150 train_time:6004ms step_avg:42.58ms
step:142/2150 train_time:6038ms step_avg:42.52ms
step:143/2150 train_time:6070ms step_avg:42.45ms
step:144/2150 train_time:6104ms step_avg:42.39ms
step:145/2150 train_time:6136ms step_avg:42.32ms
step:146/2150 train_time:6169ms step_avg:42.26ms
step:147/2150 train_time:6202ms step_avg:42.19ms
step:148/2150 train_time:6236ms step_avg:42.13ms
step:149/2150 train_time:6269ms step_avg:42.07ms
step:150/2150 train_time:6302ms step_avg:42.01ms
step:151/2150 train_time:6335ms step_avg:41.95ms
step:152/2150 train_time:6368ms step_avg:41.89ms
step:153/2150 train_time:6401ms step_avg:41.83ms
step:154/2150 train_time:6434ms step_avg:41.78ms
step:155/2150 train_time:6467ms step_avg:41.72ms
step:156/2150 train_time:6500ms step_avg:41.67ms
step:157/2150 train_time:6533ms step_avg:41.61ms
step:158/2150 train_time:6566ms step_avg:41.56ms
step:159/2150 train_time:6598ms step_avg:41.50ms
step:160/2150 train_time:6632ms step_avg:41.45ms
step:161/2150 train_time:6664ms step_avg:41.39ms
step:162/2150 train_time:6698ms step_avg:41.34ms
step:163/2150 train_time:6730ms step_avg:41.29ms
step:164/2150 train_time:6764ms step_avg:41.24ms
step:165/2150 train_time:6796ms step_avg:41.19ms
step:166/2150 train_time:6830ms step_avg:41.14ms
step:167/2150 train_time:6863ms step_avg:41.09ms
step:168/2150 train_time:6896ms step_avg:41.05ms
step:169/2150 train_time:6928ms step_avg:41.00ms
step:170/2150 train_time:6962ms step_avg:40.95ms
step:171/2150 train_time:6995ms step_avg:40.90ms
step:172/2150 train_time:7028ms step_avg:40.86ms
step:173/2150 train_time:7061ms step_avg:40.81ms
step:174/2150 train_time:7094ms step_avg:40.77ms
step:175/2150 train_time:7127ms step_avg:40.72ms
step:176/2150 train_time:7160ms step_avg:40.68ms
step:177/2150 train_time:7193ms step_avg:40.64ms
step:178/2150 train_time:7226ms step_avg:40.60ms
step:179/2150 train_time:7259ms step_avg:40.55ms
step:180/2150 train_time:7292ms step_avg:40.51ms
step:181/2150 train_time:7325ms step_avg:40.47ms
step:182/2150 train_time:7358ms step_avg:40.43ms
step:183/2150 train_time:7391ms step_avg:40.39ms
step:184/2150 train_time:7424ms step_avg:40.35ms
step:185/2150 train_time:7457ms step_avg:40.31ms
step:186/2150 train_time:7491ms step_avg:40.27ms
step:187/2150 train_time:7524ms step_avg:40.23ms
step:188/2150 train_time:7557ms step_avg:40.20ms
step:189/2150 train_time:7590ms step_avg:40.16ms
step:190/2150 train_time:7623ms step_avg:40.12ms
step:191/2150 train_time:7656ms step_avg:40.08ms
step:192/2150 train_time:7690ms step_avg:40.05ms
step:193/2150 train_time:7722ms step_avg:40.01ms
step:194/2150 train_time:7756ms step_avg:39.98ms
step:195/2150 train_time:7789ms step_avg:39.94ms
step:196/2150 train_time:7822ms step_avg:39.91ms
step:197/2150 train_time:7855ms step_avg:39.87ms
step:198/2150 train_time:7889ms step_avg:39.84ms
step:199/2150 train_time:7922ms step_avg:39.81ms
step:200/2150 train_time:7955ms step_avg:39.78ms
step:201/2150 train_time:7988ms step_avg:39.74ms
step:202/2150 train_time:8022ms step_avg:39.71ms
step:203/2150 train_time:8055ms step_avg:39.68ms
step:204/2150 train_time:8088ms step_avg:39.65ms
step:205/2150 train_time:8121ms step_avg:39.61ms
step:206/2150 train_time:8154ms step_avg:39.58ms
step:207/2150 train_time:8187ms step_avg:39.55ms
step:208/2150 train_time:8220ms step_avg:39.52ms
step:209/2150 train_time:8253ms step_avg:39.49ms
step:210/2150 train_time:8286ms step_avg:39.46ms
step:211/2150 train_time:8319ms step_avg:39.42ms
step:212/2150 train_time:8352ms step_avg:39.40ms
step:213/2150 train_time:8385ms step_avg:39.37ms
step:214/2150 train_time:8418ms step_avg:39.34ms
step:215/2150 train_time:8451ms step_avg:39.31ms
step:216/2150 train_time:8484ms step_avg:39.28ms
step:217/2150 train_time:8517ms step_avg:39.25ms
step:218/2150 train_time:8550ms step_avg:39.22ms
step:219/2150 train_time:8583ms step_avg:39.19ms
step:220/2150 train_time:8616ms step_avg:39.17ms
step:221/2150 train_time:8649ms step_avg:39.14ms
step:222/2150 train_time:8682ms step_avg:39.11ms
step:223/2150 train_time:8715ms step_avg:39.08ms
step:224/2150 train_time:8749ms step_avg:39.06ms
step:225/2150 train_time:8781ms step_avg:39.03ms
step:226/2150 train_time:8814ms step_avg:39.00ms
step:227/2150 train_time:8847ms step_avg:38.97ms
step:228/2150 train_time:8881ms step_avg:38.95ms
step:229/2150 train_time:8913ms step_avg:38.92ms
step:230/2150 train_time:8946ms step_avg:38.90ms
step:231/2150 train_time:8979ms step_avg:38.87ms
step:232/2150 train_time:9013ms step_avg:38.85ms
step:233/2150 train_time:9045ms step_avg:38.82ms
step:234/2150 train_time:9079ms step_avg:38.80ms
step:235/2150 train_time:9111ms step_avg:38.77ms
step:236/2150 train_time:9145ms step_avg:38.75ms
step:237/2150 train_time:9177ms step_avg:38.72ms
step:238/2150 train_time:9210ms step_avg:38.70ms
step:239/2150 train_time:9243ms step_avg:38.67ms
step:240/2150 train_time:9276ms step_avg:38.65ms
step:241/2150 train_time:9309ms step_avg:38.63ms
step:242/2150 train_time:9342ms step_avg:38.60ms
step:243/2150 train_time:9375ms step_avg:38.58ms
step:244/2150 train_time:9409ms step_avg:38.56ms
step:245/2150 train_time:9441ms step_avg:38.54ms
step:246/2150 train_time:9475ms step_avg:38.52ms
step:247/2150 train_time:9507ms step_avg:38.49ms
step:248/2150 train_time:9541ms step_avg:38.47ms
step:249/2150 train_time:9573ms step_avg:38.45ms
step:250/2150 train_time:9607ms step_avg:38.43ms
step:250/2150 val_loss:4.3099 train_time:9643ms step_avg:38.57ms
step:251/2150 train_time:9665ms step_avg:38.50ms
step:252/2150 train_time:9687ms step_avg:38.44ms
step:253/2150 train_time:9709ms step_avg:38.38ms
step:254/2150 train_time:9743ms step_avg:38.36ms
step:255/2150 train_time:9779ms step_avg:38.35ms
step:256/2150 train_time:9815ms step_avg:38.34ms
step:257/2150 train_time:9850ms step_avg:38.33ms
step:258/2150 train_time:9883ms step_avg:38.31ms
step:259/2150 train_time:9916ms step_avg:38.29ms
step:260/2150 train_time:9950ms step_avg:38.27ms
step:261/2150 train_time:9982ms step_avg:38.25ms
step:262/2150 train_time:10016ms step_avg:38.23ms
step:263/2150 train_time:10048ms step_avg:38.20ms
step:264/2150 train_time:10081ms step_avg:38.19ms
step:265/2150 train_time:10113ms step_avg:38.16ms
step:266/2150 train_time:10147ms step_avg:38.15ms
step:267/2150 train_time:10179ms step_avg:38.12ms
step:268/2150 train_time:10213ms step_avg:38.11ms
step:269/2150 train_time:10245ms step_avg:38.08ms
step:270/2150 train_time:10278ms step_avg:38.07ms
step:271/2150 train_time:10310ms step_avg:38.05ms
step:272/2150 train_time:10344ms step_avg:38.03ms
step:273/2150 train_time:10376ms step_avg:38.01ms
step:274/2150 train_time:10409ms step_avg:37.99ms
step:275/2150 train_time:10442ms step_avg:37.97ms
step:276/2150 train_time:10475ms step_avg:37.95ms
step:277/2150 train_time:10507ms step_avg:37.93ms
step:278/2150 train_time:10541ms step_avg:37.92ms
step:279/2150 train_time:10573ms step_avg:37.90ms
step:280/2150 train_time:10607ms step_avg:37.88ms
step:281/2150 train_time:10639ms step_avg:37.86ms
step:282/2150 train_time:10672ms step_avg:37.85ms
step:283/2150 train_time:10706ms step_avg:37.83ms
step:284/2150 train_time:10740ms step_avg:37.82ms
step:285/2150 train_time:10773ms step_avg:37.80ms
step:286/2150 train_time:10808ms step_avg:37.79ms
step:287/2150 train_time:10842ms step_avg:37.78ms
step:288/2150 train_time:10876ms step_avg:37.76ms
step:289/2150 train_time:10909ms step_avg:37.75ms
step:290/2150 train_time:10942ms step_avg:37.73ms
step:291/2150 train_time:10975ms step_avg:37.71ms
step:292/2150 train_time:11008ms step_avg:37.70ms
step:293/2150 train_time:11040ms step_avg:37.68ms
step:294/2150 train_time:11074ms step_avg:37.67ms
step:295/2150 train_time:11107ms step_avg:37.65ms
step:296/2150 train_time:11140ms step_avg:37.64ms
step:297/2150 train_time:11173ms step_avg:37.62ms
step:298/2150 train_time:11206ms step_avg:37.60ms
step:299/2150 train_time:11238ms step_avg:37.59ms
step:300/2150 train_time:11272ms step_avg:37.57ms
step:301/2150 train_time:11304ms step_avg:37.56ms
step:302/2150 train_time:11338ms step_avg:37.54ms
step:303/2150 train_time:11370ms step_avg:37.52ms
step:304/2150 train_time:11403ms step_avg:37.51ms
step:305/2150 train_time:11435ms step_avg:37.49ms
step:306/2150 train_time:11469ms step_avg:37.48ms
step:307/2150 train_time:11501ms step_avg:37.46ms
step:308/2150 train_time:11534ms step_avg:37.45ms
step:309/2150 train_time:11567ms step_avg:37.43ms
step:310/2150 train_time:11600ms step_avg:37.42ms
step:311/2150 train_time:11633ms step_avg:37.40ms
step:312/2150 train_time:11666ms step_avg:37.39ms
step:313/2150 train_time:11699ms step_avg:37.38ms
step:314/2150 train_time:11732ms step_avg:37.36ms
step:315/2150 train_time:11765ms step_avg:37.35ms
step:316/2150 train_time:11799ms step_avg:37.34ms
step:317/2150 train_time:11832ms step_avg:37.32ms
step:318/2150 train_time:11866ms step_avg:37.31ms
step:319/2150 train_time:11899ms step_avg:37.30ms
step:320/2150 train_time:11932ms step_avg:37.29ms
step:321/2150 train_time:11965ms step_avg:37.28ms
step:322/2150 train_time:11999ms step_avg:37.26ms
step:323/2150 train_time:12031ms step_avg:37.25ms
step:324/2150 train_time:12064ms step_avg:37.24ms
step:325/2150 train_time:12097ms step_avg:37.22ms
step:326/2150 train_time:12130ms step_avg:37.21ms
step:327/2150 train_time:12163ms step_avg:37.20ms
step:328/2150 train_time:12196ms step_avg:37.18ms
step:329/2150 train_time:12229ms step_avg:37.17ms
step:330/2150 train_time:12262ms step_avg:37.16ms
step:331/2150 train_time:12295ms step_avg:37.14ms
step:332/2150 train_time:12329ms step_avg:37.13ms
step:333/2150 train_time:12360ms step_avg:37.12ms
step:334/2150 train_time:12393ms step_avg:37.11ms
step:335/2150 train_time:12426ms step_avg:37.09ms
step:336/2150 train_time:12459ms step_avg:37.08ms
step:337/2150 train_time:12492ms step_avg:37.07ms
step:338/2150 train_time:12525ms step_avg:37.06ms
step:339/2150 train_time:12558ms step_avg:37.04ms
step:340/2150 train_time:12591ms step_avg:37.03ms
step:341/2150 train_time:12623ms step_avg:37.02ms
step:342/2150 train_time:12657ms step_avg:37.01ms
step:343/2150 train_time:12689ms step_avg:36.99ms
step:344/2150 train_time:12723ms step_avg:36.98ms
step:345/2150 train_time:12756ms step_avg:36.97ms
step:346/2150 train_time:12789ms step_avg:36.96ms
step:347/2150 train_time:12822ms step_avg:36.95ms
step:348/2150 train_time:12856ms step_avg:36.94ms
step:349/2150 train_time:12888ms step_avg:36.93ms
step:350/2150 train_time:12922ms step_avg:36.92ms
step:351/2150 train_time:12955ms step_avg:36.91ms
step:352/2150 train_time:12988ms step_avg:36.90ms
step:353/2150 train_time:13021ms step_avg:36.89ms
step:354/2150 train_time:13055ms step_avg:36.88ms
step:355/2150 train_time:13088ms step_avg:36.87ms
step:356/2150 train_time:13121ms step_avg:36.86ms
step:357/2150 train_time:13154ms step_avg:36.85ms
step:358/2150 train_time:13187ms step_avg:36.84ms
step:359/2150 train_time:13220ms step_avg:36.82ms
step:360/2150 train_time:13253ms step_avg:36.81ms
step:361/2150 train_time:13285ms step_avg:36.80ms
step:362/2150 train_time:13319ms step_avg:36.79ms
step:363/2150 train_time:13352ms step_avg:36.78ms
step:364/2150 train_time:13385ms step_avg:36.77ms
step:365/2150 train_time:13418ms step_avg:36.76ms
step:366/2150 train_time:13451ms step_avg:36.75ms
step:367/2150 train_time:13484ms step_avg:36.74ms
step:368/2150 train_time:13517ms step_avg:36.73ms
step:369/2150 train_time:13550ms step_avg:36.72ms
step:370/2150 train_time:13583ms step_avg:36.71ms
step:371/2150 train_time:13616ms step_avg:36.70ms
step:372/2150 train_time:13649ms step_avg:36.69ms
step:373/2150 train_time:13681ms step_avg:36.68ms
step:374/2150 train_time:13714ms step_avg:36.67ms
step:375/2150 train_time:13747ms step_avg:36.66ms
step:376/2150 train_time:13781ms step_avg:36.65ms
step:377/2150 train_time:13814ms step_avg:36.64ms
step:378/2150 train_time:13847ms step_avg:36.63ms
step:379/2150 train_time:13880ms step_avg:36.62ms
step:380/2150 train_time:13913ms step_avg:36.61ms
step:381/2150 train_time:13946ms step_avg:36.60ms
step:382/2150 train_time:13980ms step_avg:36.60ms
step:383/2150 train_time:14013ms step_avg:36.59ms
step:384/2150 train_time:14046ms step_avg:36.58ms
step:385/2150 train_time:14079ms step_avg:36.57ms
step:386/2150 train_time:14113ms step_avg:36.56ms
step:387/2150 train_time:14145ms step_avg:36.55ms
step:388/2150 train_time:14179ms step_avg:36.54ms
step:389/2150 train_time:14211ms step_avg:36.53ms
step:390/2150 train_time:14244ms step_avg:36.52ms
step:391/2150 train_time:14277ms step_avg:36.51ms
step:392/2150 train_time:14311ms step_avg:36.51ms
step:393/2150 train_time:14343ms step_avg:36.50ms
step:394/2150 train_time:14377ms step_avg:36.49ms
step:395/2150 train_time:14409ms step_avg:36.48ms
step:396/2150 train_time:14442ms step_avg:36.47ms
step:397/2150 train_time:14475ms step_avg:36.46ms
step:398/2150 train_time:14508ms step_avg:36.45ms
step:399/2150 train_time:14541ms step_avg:36.44ms
step:400/2150 train_time:14574ms step_avg:36.44ms
step:401/2150 train_time:14607ms step_avg:36.43ms
step:402/2150 train_time:14640ms step_avg:36.42ms
step:403/2150 train_time:14673ms step_avg:36.41ms
step:404/2150 train_time:14706ms step_avg:36.40ms
step:405/2150 train_time:14739ms step_avg:36.39ms
step:406/2150 train_time:14773ms step_avg:36.39ms
step:407/2150 train_time:14805ms step_avg:36.38ms
step:408/2150 train_time:14838ms step_avg:36.37ms
step:409/2150 train_time:14871ms step_avg:36.36ms
step:410/2150 train_time:14905ms step_avg:36.35ms
step:411/2150 train_time:14937ms step_avg:36.34ms
step:412/2150 train_time:14970ms step_avg:36.34ms
step:413/2150 train_time:15003ms step_avg:36.33ms
step:414/2150 train_time:15036ms step_avg:36.32ms
step:415/2150 train_time:15069ms step_avg:36.31ms
step:416/2150 train_time:15102ms step_avg:36.30ms
step:417/2150 train_time:15134ms step_avg:36.29ms
step:418/2150 train_time:15168ms step_avg:36.29ms
step:419/2150 train_time:15201ms step_avg:36.28ms
step:420/2150 train_time:15234ms step_avg:36.27ms
step:421/2150 train_time:15267ms step_avg:36.26ms
step:422/2150 train_time:15300ms step_avg:36.26ms
step:423/2150 train_time:15332ms step_avg:36.25ms
step:424/2150 train_time:15366ms step_avg:36.24ms
step:425/2150 train_time:15398ms step_avg:36.23ms
step:426/2150 train_time:15432ms step_avg:36.22ms
step:427/2150 train_time:15464ms step_avg:36.22ms
step:428/2150 train_time:15497ms step_avg:36.21ms
step:429/2150 train_time:15530ms step_avg:36.20ms
step:430/2150 train_time:15564ms step_avg:36.19ms
step:431/2150 train_time:15596ms step_avg:36.19ms
step:432/2150 train_time:15630ms step_avg:36.18ms
step:433/2150 train_time:15662ms step_avg:36.17ms
step:434/2150 train_time:15696ms step_avg:36.17ms
step:435/2150 train_time:15728ms step_avg:36.16ms
step:436/2150 train_time:15762ms step_avg:36.15ms
step:437/2150 train_time:15795ms step_avg:36.14ms
step:438/2150 train_time:15828ms step_avg:36.14ms
step:439/2150 train_time:15861ms step_avg:36.13ms
step:440/2150 train_time:15894ms step_avg:36.12ms
step:441/2150 train_time:15926ms step_avg:36.11ms
step:442/2150 train_time:15960ms step_avg:36.11ms
step:443/2150 train_time:15993ms step_avg:36.10ms
step:444/2150 train_time:16026ms step_avg:36.10ms
step:445/2150 train_time:16060ms step_avg:36.09ms
step:446/2150 train_time:16093ms step_avg:36.08ms
step:447/2150 train_time:16125ms step_avg:36.07ms
step:448/2150 train_time:16159ms step_avg:36.07ms
step:449/2150 train_time:16191ms step_avg:36.06ms
step:450/2150 train_time:16225ms step_avg:36.05ms
step:451/2150 train_time:16257ms step_avg:36.05ms
step:452/2150 train_time:16291ms step_avg:36.04ms
step:453/2150 train_time:16323ms step_avg:36.03ms
step:454/2150 train_time:16356ms step_avg:36.03ms
step:455/2150 train_time:16389ms step_avg:36.02ms
step:456/2150 train_time:16422ms step_avg:36.01ms
step:457/2150 train_time:16454ms step_avg:36.00ms
step:458/2150 train_time:16488ms step_avg:36.00ms
step:459/2150 train_time:16520ms step_avg:35.99ms
step:460/2150 train_time:16554ms step_avg:35.99ms
step:461/2150 train_time:16586ms step_avg:35.98ms
step:462/2150 train_time:16620ms step_avg:35.97ms
step:463/2150 train_time:16652ms step_avg:35.97ms
step:464/2150 train_time:16686ms step_avg:35.96ms
step:465/2150 train_time:16718ms step_avg:35.95ms
step:466/2150 train_time:16752ms step_avg:35.95ms
step:467/2150 train_time:16784ms step_avg:35.94ms
step:468/2150 train_time:16817ms step_avg:35.93ms
step:469/2150 train_time:16850ms step_avg:35.93ms
step:470/2150 train_time:16883ms step_avg:35.92ms
step:471/2150 train_time:16916ms step_avg:35.91ms
step:472/2150 train_time:16949ms step_avg:35.91ms
step:473/2150 train_time:16982ms step_avg:35.90ms
step:474/2150 train_time:17016ms step_avg:35.90ms
step:475/2150 train_time:17048ms step_avg:35.89ms
step:476/2150 train_time:17082ms step_avg:35.89ms
step:477/2150 train_time:17115ms step_avg:35.88ms
step:478/2150 train_time:17148ms step_avg:35.87ms
step:479/2150 train_time:17181ms step_avg:35.87ms
step:480/2150 train_time:17214ms step_avg:35.86ms
step:481/2150 train_time:17247ms step_avg:35.86ms
step:482/2150 train_time:17280ms step_avg:35.85ms
step:483/2150 train_time:17313ms step_avg:35.84ms
step:484/2150 train_time:17346ms step_avg:35.84ms
step:485/2150 train_time:17378ms step_avg:35.83ms
step:486/2150 train_time:17412ms step_avg:35.83ms
step:487/2150 train_time:17444ms step_avg:35.82ms
step:488/2150 train_time:17478ms step_avg:35.82ms
step:489/2150 train_time:17510ms step_avg:35.81ms
step:490/2150 train_time:17544ms step_avg:35.80ms
step:491/2150 train_time:17576ms step_avg:35.80ms
step:492/2150 train_time:17609ms step_avg:35.79ms
step:493/2150 train_time:17642ms step_avg:35.78ms
step:494/2150 train_time:17675ms step_avg:35.78ms
step:495/2150 train_time:17708ms step_avg:35.77ms
step:496/2150 train_time:17741ms step_avg:35.77ms
step:497/2150 train_time:17774ms step_avg:35.76ms
step:498/2150 train_time:17808ms step_avg:35.76ms
step:499/2150 train_time:17840ms step_avg:35.75ms
step:500/2150 train_time:17873ms step_avg:35.75ms
step:500/2150 val_loss:4.0213 train_time:17910ms step_avg:35.82ms
step:501/2150 train_time:17931ms step_avg:35.79ms
step:502/2150 train_time:17953ms step_avg:35.76ms
step:503/2150 train_time:17979ms step_avg:35.74ms
step:504/2150 train_time:18012ms step_avg:35.74ms
step:505/2150 train_time:18046ms step_avg:35.74ms
step:506/2150 train_time:18081ms step_avg:35.73ms
step:507/2150 train_time:18115ms step_avg:35.73ms
step:508/2150 train_time:18148ms step_avg:35.72ms
step:509/2150 train_time:18181ms step_avg:35.72ms
step:510/2150 train_time:18214ms step_avg:35.71ms
step:511/2150 train_time:18247ms step_avg:35.71ms
step:512/2150 train_time:18281ms step_avg:35.70ms
step:513/2150 train_time:18313ms step_avg:35.70ms
step:514/2150 train_time:18346ms step_avg:35.69ms
step:515/2150 train_time:18379ms step_avg:35.69ms
step:516/2150 train_time:18412ms step_avg:35.68ms
step:517/2150 train_time:18444ms step_avg:35.68ms
step:518/2150 train_time:18478ms step_avg:35.67ms
step:519/2150 train_time:18510ms step_avg:35.66ms
step:520/2150 train_time:18543ms step_avg:35.66ms
step:521/2150 train_time:18575ms step_avg:35.65ms
step:522/2150 train_time:18609ms step_avg:35.65ms
step:523/2150 train_time:18641ms step_avg:35.64ms
step:524/2150 train_time:18674ms step_avg:35.64ms
step:525/2150 train_time:18707ms step_avg:35.63ms
step:526/2150 train_time:18740ms step_avg:35.63ms
step:527/2150 train_time:18773ms step_avg:35.62ms
step:528/2150 train_time:18806ms step_avg:35.62ms
step:529/2150 train_time:18839ms step_avg:35.61ms
step:530/2150 train_time:18873ms step_avg:35.61ms
step:531/2150 train_time:18905ms step_avg:35.60ms
step:532/2150 train_time:18939ms step_avg:35.60ms
step:533/2150 train_time:18972ms step_avg:35.59ms
step:534/2150 train_time:19005ms step_avg:35.59ms
step:535/2150 train_time:19039ms step_avg:35.59ms
step:536/2150 train_time:19073ms step_avg:35.58ms
step:537/2150 train_time:19106ms step_avg:35.58ms
step:538/2150 train_time:19139ms step_avg:35.58ms
step:539/2150 train_time:19173ms step_avg:35.57ms
step:540/2150 train_time:19206ms step_avg:35.57ms
step:541/2150 train_time:19239ms step_avg:35.56ms
step:542/2150 train_time:19272ms step_avg:35.56ms
step:543/2150 train_time:19305ms step_avg:35.55ms
step:544/2150 train_time:19339ms step_avg:35.55ms
step:545/2150 train_time:19372ms step_avg:35.54ms
step:546/2150 train_time:19405ms step_avg:35.54ms
step:547/2150 train_time:19437ms step_avg:35.53ms
step:548/2150 train_time:19471ms step_avg:35.53ms
step:549/2150 train_time:19503ms step_avg:35.52ms
step:550/2150 train_time:19536ms step_avg:35.52ms
step:551/2150 train_time:19569ms step_avg:35.52ms
step:552/2150 train_time:19602ms step_avg:35.51ms
step:553/2150 train_time:19634ms step_avg:35.51ms
step:554/2150 train_time:19668ms step_avg:35.50ms
step:555/2150 train_time:19700ms step_avg:35.50ms
step:556/2150 train_time:19734ms step_avg:35.49ms
step:557/2150 train_time:19766ms step_avg:35.49ms
step:558/2150 train_time:19799ms step_avg:35.48ms
step:559/2150 train_time:19832ms step_avg:35.48ms
step:560/2150 train_time:19865ms step_avg:35.47ms
step:561/2150 train_time:19898ms step_avg:35.47ms
step:562/2150 train_time:19931ms step_avg:35.46ms
step:563/2150 train_time:19965ms step_avg:35.46ms
step:564/2150 train_time:19998ms step_avg:35.46ms
step:565/2150 train_time:20031ms step_avg:35.45ms
step:566/2150 train_time:20065ms step_avg:35.45ms
step:567/2150 train_time:20098ms step_avg:35.45ms
step:568/2150 train_time:20131ms step_avg:35.44ms
step:569/2150 train_time:20164ms step_avg:35.44ms
step:570/2150 train_time:20198ms step_avg:35.43ms
step:571/2150 train_time:20231ms step_avg:35.43ms
step:572/2150 train_time:20264ms step_avg:35.43ms
step:573/2150 train_time:20297ms step_avg:35.42ms
step:574/2150 train_time:20331ms step_avg:35.42ms
step:575/2150 train_time:20363ms step_avg:35.41ms
step:576/2150 train_time:20397ms step_avg:35.41ms
step:577/2150 train_time:20429ms step_avg:35.41ms
step:578/2150 train_time:20463ms step_avg:35.40ms
step:579/2150 train_time:20495ms step_avg:35.40ms
step:580/2150 train_time:20529ms step_avg:35.40ms
step:581/2150 train_time:20561ms step_avg:35.39ms
step:582/2150 train_time:20595ms step_avg:35.39ms
step:583/2150 train_time:20627ms step_avg:35.38ms
step:584/2150 train_time:20660ms step_avg:35.38ms
step:585/2150 train_time:20693ms step_avg:35.37ms
step:586/2150 train_time:20726ms step_avg:35.37ms
step:587/2150 train_time:20759ms step_avg:35.36ms
step:588/2150 train_time:20792ms step_avg:35.36ms
step:589/2150 train_time:20824ms step_avg:35.36ms
step:590/2150 train_time:20858ms step_avg:35.35ms
step:591/2150 train_time:20891ms step_avg:35.35ms
step:592/2150 train_time:20924ms step_avg:35.34ms
step:593/2150 train_time:20957ms step_avg:35.34ms
step:594/2150 train_time:20990ms step_avg:35.34ms
step:595/2150 train_time:21023ms step_avg:35.33ms
step:596/2150 train_time:21056ms step_avg:35.33ms
step:597/2150 train_time:21089ms step_avg:35.32ms
step:598/2150 train_time:21122ms step_avg:35.32ms
step:599/2150 train_time:21155ms step_avg:35.32ms
step:600/2150 train_time:21189ms step_avg:35.31ms
step:601/2150 train_time:21222ms step_avg:35.31ms
step:602/2150 train_time:21255ms step_avg:35.31ms
step:603/2150 train_time:21288ms step_avg:35.30ms
step:604/2150 train_time:21321ms step_avg:35.30ms
step:605/2150 train_time:21354ms step_avg:35.30ms
step:606/2150 train_time:21388ms step_avg:35.29ms
step:607/2150 train_time:21420ms step_avg:35.29ms
step:608/2150 train_time:21453ms step_avg:35.29ms
step:609/2150 train_time:21486ms step_avg:35.28ms
step:610/2150 train_time:21520ms step_avg:35.28ms
step:611/2150 train_time:21552ms step_avg:35.27ms
step:612/2150 train_time:21586ms step_avg:35.27ms
step:613/2150 train_time:21618ms step_avg:35.27ms
step:614/2150 train_time:21652ms step_avg:35.26ms
step:615/2150 train_time:21684ms step_avg:35.26ms
step:616/2150 train_time:21717ms step_avg:35.26ms
step:617/2150 train_time:21750ms step_avg:35.25ms
step:618/2150 train_time:21783ms step_avg:35.25ms
step:619/2150 train_time:21816ms step_avg:35.24ms
step:620/2150 train_time:21850ms step_avg:35.24ms
step:621/2150 train_time:21882ms step_avg:35.24ms
step:622/2150 train_time:21915ms step_avg:35.23ms
step:623/2150 train_time:21949ms step_avg:35.23ms
step:624/2150 train_time:21982ms step_avg:35.23ms
step:625/2150 train_time:22015ms step_avg:35.22ms
step:626/2150 train_time:22049ms step_avg:35.22ms
step:627/2150 train_time:22081ms step_avg:35.22ms
step:628/2150 train_time:22115ms step_avg:35.21ms
step:629/2150 train_time:22148ms step_avg:35.21ms
step:630/2150 train_time:22181ms step_avg:35.21ms
step:631/2150 train_time:22214ms step_avg:35.20ms
step:632/2150 train_time:22247ms step_avg:35.20ms
step:633/2150 train_time:22280ms step_avg:35.20ms
step:634/2150 train_time:22313ms step_avg:35.19ms
step:635/2150 train_time:22346ms step_avg:35.19ms
step:636/2150 train_time:22379ms step_avg:35.19ms
step:637/2150 train_time:22412ms step_avg:35.18ms
step:638/2150 train_time:22446ms step_avg:35.18ms
step:639/2150 train_time:22479ms step_avg:35.18ms
step:640/2150 train_time:22512ms step_avg:35.18ms
step:641/2150 train_time:22545ms step_avg:35.17ms
step:642/2150 train_time:22578ms step_avg:35.17ms
step:643/2150 train_time:22611ms step_avg:35.16ms
step:644/2150 train_time:22644ms step_avg:35.16ms
step:645/2150 train_time:22677ms step_avg:35.16ms
step:646/2150 train_time:22710ms step_avg:35.15ms
step:647/2150 train_time:22742ms step_avg:35.15ms
step:648/2150 train_time:22776ms step_avg:35.15ms
step:649/2150 train_time:22808ms step_avg:35.14ms
step:650/2150 train_time:22841ms step_avg:35.14ms
step:651/2150 train_time:22874ms step_avg:35.14ms
step:652/2150 train_time:22908ms step_avg:35.13ms
step:653/2150 train_time:22940ms step_avg:35.13ms
step:654/2150 train_time:22974ms step_avg:35.13ms
step:655/2150 train_time:23006ms step_avg:35.12ms
step:656/2150 train_time:23040ms step_avg:35.12ms
step:657/2150 train_time:23073ms step_avg:35.12ms
step:658/2150 train_time:23106ms step_avg:35.12ms
step:659/2150 train_time:23139ms step_avg:35.11ms
step:660/2150 train_time:23172ms step_avg:35.11ms
step:661/2150 train_time:23205ms step_avg:35.11ms
step:662/2150 train_time:23238ms step_avg:35.10ms
step:663/2150 train_time:23271ms step_avg:35.10ms
step:664/2150 train_time:23304ms step_avg:35.10ms
step:665/2150 train_time:23337ms step_avg:35.09ms
step:666/2150 train_time:23371ms step_avg:35.09ms
step:667/2150 train_time:23404ms step_avg:35.09ms
step:668/2150 train_time:23437ms step_avg:35.09ms
step:669/2150 train_time:23470ms step_avg:35.08ms
step:670/2150 train_time:23503ms step_avg:35.08ms
step:671/2150 train_time:23536ms step_avg:35.08ms
step:672/2150 train_time:23570ms step_avg:35.07ms
step:673/2150 train_time:23602ms step_avg:35.07ms
step:674/2150 train_time:23635ms step_avg:35.07ms
step:675/2150 train_time:23668ms step_avg:35.06ms
step:676/2150 train_time:23701ms step_avg:35.06ms
step:677/2150 train_time:23734ms step_avg:35.06ms
step:678/2150 train_time:23767ms step_avg:35.05ms
step:679/2150 train_time:23800ms step_avg:35.05ms
step:680/2150 train_time:23833ms step_avg:35.05ms
step:681/2150 train_time:23866ms step_avg:35.04ms
step:682/2150 train_time:23899ms step_avg:35.04ms
step:683/2150 train_time:23932ms step_avg:35.04ms
step:684/2150 train_time:23965ms step_avg:35.04ms
step:685/2150 train_time:23998ms step_avg:35.03ms
step:686/2150 train_time:24031ms step_avg:35.03ms
step:687/2150 train_time:24065ms step_avg:35.03ms
step:688/2150 train_time:24098ms step_avg:35.03ms
step:689/2150 train_time:24131ms step_avg:35.02ms
step:690/2150 train_time:24164ms step_avg:35.02ms
step:691/2150 train_time:24197ms step_avg:35.02ms
step:692/2150 train_time:24230ms step_avg:35.02ms
step:693/2150 train_time:24263ms step_avg:35.01ms
step:694/2150 train_time:24297ms step_avg:35.01ms
step:695/2150 train_time:24330ms step_avg:35.01ms
step:696/2150 train_time:24364ms step_avg:35.01ms
step:697/2150 train_time:24397ms step_avg:35.00ms
step:698/2150 train_time:24430ms step_avg:35.00ms
step:699/2150 train_time:24463ms step_avg:35.00ms
step:700/2150 train_time:24497ms step_avg:35.00ms
step:701/2150 train_time:24530ms step_avg:34.99ms
step:702/2150 train_time:24563ms step_avg:34.99ms
step:703/2150 train_time:24596ms step_avg:34.99ms
step:704/2150 train_time:24629ms step_avg:34.98ms
step:705/2150 train_time:24663ms step_avg:34.98ms
step:706/2150 train_time:24721ms step_avg:35.02ms
step:707/2150 train_time:24782ms step_avg:35.05ms
step:708/2150 train_time:24841ms step_avg:35.09ms
step:709/2150 train_time:24903ms step_avg:35.12ms
step:710/2150 train_time:24962ms step_avg:35.16ms
step:711/2150 train_time:25023ms step_avg:35.19ms
step:712/2150 train_time:25082ms step_avg:35.23ms
step:713/2150 train_time:25143ms step_avg:35.26ms
step:714/2150 train_time:25203ms step_avg:35.30ms
step:715/2150 train_time:25265ms step_avg:35.34ms
step:716/2150 train_time:25324ms step_avg:35.37ms
step:717/2150 train_time:25386ms step_avg:35.41ms
step:718/2150 train_time:25445ms step_avg:35.44ms
step:719/2150 train_time:25507ms step_avg:35.48ms
step:720/2150 train_time:25566ms step_avg:35.51ms
step:721/2150 train_time:25628ms step_avg:35.54ms
step:722/2150 train_time:25687ms step_avg:35.58ms
step:723/2150 train_time:25748ms step_avg:35.61ms
step:724/2150 train_time:25807ms step_avg:35.65ms
step:725/2150 train_time:25868ms step_avg:35.68ms
step:726/2150 train_time:25927ms step_avg:35.71ms
step:727/2150 train_time:25988ms step_avg:35.75ms
step:728/2150 train_time:26048ms step_avg:35.78ms
step:729/2150 train_time:26109ms step_avg:35.81ms
step:730/2150 train_time:26169ms step_avg:35.85ms
step:731/2150 train_time:26230ms step_avg:35.88ms
step:732/2150 train_time:26289ms step_avg:35.91ms
step:733/2150 train_time:26351ms step_avg:35.95ms
step:734/2150 train_time:26410ms step_avg:35.98ms
step:735/2150 train_time:26472ms step_avg:36.02ms
step:736/2150 train_time:26531ms step_avg:36.05ms
step:737/2150 train_time:26592ms step_avg:36.08ms
step:738/2150 train_time:26652ms step_avg:36.11ms
step:739/2150 train_time:26714ms step_avg:36.15ms
step:740/2150 train_time:26773ms step_avg:36.18ms
step:741/2150 train_time:26833ms step_avg:36.21ms
step:742/2150 train_time:26892ms step_avg:36.24ms
step:743/2150 train_time:26953ms step_avg:36.28ms
step:744/2150 train_time:27013ms step_avg:36.31ms
step:745/2150 train_time:27074ms step_avg:36.34ms
step:746/2150 train_time:27134ms step_avg:36.37ms
step:747/2150 train_time:27195ms step_avg:36.41ms
step:748/2150 train_time:27254ms step_avg:36.44ms
step:749/2150 train_time:27316ms step_avg:36.47ms
step:750/2150 train_time:27375ms step_avg:36.50ms
step:750/2150 val_loss:3.8733 train_time:27438ms step_avg:36.58ms
step:751/2150 train_time:27461ms step_avg:36.57ms
step:752/2150 train_time:27495ms step_avg:36.56ms
step:753/2150 train_time:27558ms step_avg:36.60ms
step:754/2150 train_time:27622ms step_avg:36.63ms
step:755/2150 train_time:27684ms step_avg:36.67ms
step:756/2150 train_time:27743ms step_avg:36.70ms
step:757/2150 train_time:27803ms step_avg:36.73ms
step:758/2150 train_time:27862ms step_avg:36.76ms
step:759/2150 train_time:27923ms step_avg:36.79ms
step:760/2150 train_time:27981ms step_avg:36.82ms
step:761/2150 train_time:28041ms step_avg:36.85ms
step:762/2150 train_time:28099ms step_avg:36.88ms
step:763/2150 train_time:28158ms step_avg:36.90ms
step:764/2150 train_time:28217ms step_avg:36.93ms
step:765/2150 train_time:28276ms step_avg:36.96ms
step:766/2150 train_time:28339ms step_avg:37.00ms
step:767/2150 train_time:28408ms step_avg:37.04ms
step:768/2150 train_time:28469ms step_avg:37.07ms
step:769/2150 train_time:28531ms step_avg:37.10ms
step:770/2150 train_time:28590ms step_avg:37.13ms
step:771/2150 train_time:28651ms step_avg:37.16ms
step:772/2150 train_time:28710ms step_avg:37.19ms
step:773/2150 train_time:28771ms step_avg:37.22ms
step:774/2150 train_time:28830ms step_avg:37.25ms
step:775/2150 train_time:28891ms step_avg:37.28ms
step:776/2150 train_time:28950ms step_avg:37.31ms
step:777/2150 train_time:29010ms step_avg:37.34ms
step:778/2150 train_time:29069ms step_avg:37.36ms
step:779/2150 train_time:29129ms step_avg:37.39ms
step:780/2150 train_time:29188ms step_avg:37.42ms
step:781/2150 train_time:29248ms step_avg:37.45ms
step:782/2150 train_time:29309ms step_avg:37.48ms
step:783/2150 train_time:29371ms step_avg:37.51ms
step:784/2150 train_time:29430ms step_avg:37.54ms
step:785/2150 train_time:29493ms step_avg:37.57ms
step:786/2150 train_time:29553ms step_avg:37.60ms
step:787/2150 train_time:29614ms step_avg:37.63ms
step:788/2150 train_time:29674ms step_avg:37.66ms
step:789/2150 train_time:29734ms step_avg:37.69ms
step:790/2150 train_time:29793ms step_avg:37.71ms
step:791/2150 train_time:29854ms step_avg:37.74ms
step:792/2150 train_time:29913ms step_avg:37.77ms
step:793/2150 train_time:29973ms step_avg:37.80ms
step:794/2150 train_time:30032ms step_avg:37.82ms
step:795/2150 train_time:30092ms step_avg:37.85ms
step:796/2150 train_time:30151ms step_avg:37.88ms
step:797/2150 train_time:30212ms step_avg:37.91ms
step:798/2150 train_time:30272ms step_avg:37.94ms
step:799/2150 train_time:30334ms step_avg:37.97ms
step:800/2150 train_time:30394ms step_avg:37.99ms
step:801/2150 train_time:30455ms step_avg:38.02ms
step:802/2150 train_time:30515ms step_avg:38.05ms
step:803/2150 train_time:30577ms step_avg:38.08ms
step:804/2150 train_time:30636ms step_avg:38.11ms
step:805/2150 train_time:30697ms step_avg:38.13ms
step:806/2150 train_time:30757ms step_avg:38.16ms
step:807/2150 train_time:30818ms step_avg:38.19ms
step:808/2150 train_time:30877ms step_avg:38.21ms
step:809/2150 train_time:30937ms step_avg:38.24ms
step:810/2150 train_time:30996ms step_avg:38.27ms
step:811/2150 train_time:31056ms step_avg:38.29ms
step:812/2150 train_time:31115ms step_avg:38.32ms
step:813/2150 train_time:31177ms step_avg:38.35ms
step:814/2150 train_time:31236ms step_avg:38.37ms
step:815/2150 train_time:31297ms step_avg:38.40ms
step:816/2150 train_time:31357ms step_avg:38.43ms
step:817/2150 train_time:31419ms step_avg:38.46ms
step:818/2150 train_time:31479ms step_avg:38.48ms
step:819/2150 train_time:31540ms step_avg:38.51ms
step:820/2150 train_time:31600ms step_avg:38.54ms
step:821/2150 train_time:31660ms step_avg:38.56ms
step:822/2150 train_time:31719ms step_avg:38.59ms
step:823/2150 train_time:31781ms step_avg:38.62ms
step:824/2150 train_time:31840ms step_avg:38.64ms
step:825/2150 train_time:31901ms step_avg:38.67ms
step:826/2150 train_time:31959ms step_avg:38.69ms
step:827/2150 train_time:32020ms step_avg:38.72ms
step:828/2150 train_time:32079ms step_avg:38.74ms
step:829/2150 train_time:32140ms step_avg:38.77ms
step:830/2150 train_time:32199ms step_avg:38.79ms
step:831/2150 train_time:32260ms step_avg:38.82ms
step:832/2150 train_time:32319ms step_avg:38.85ms
step:833/2150 train_time:32381ms step_avg:38.87ms
step:834/2150 train_time:32441ms step_avg:38.90ms
step:835/2150 train_time:32502ms step_avg:38.92ms
step:836/2150 train_time:32561ms step_avg:38.95ms
step:837/2150 train_time:32622ms step_avg:38.98ms
step:838/2150 train_time:32681ms step_avg:39.00ms
step:839/2150 train_time:32743ms step_avg:39.03ms
step:840/2150 train_time:32802ms step_avg:39.05ms
step:841/2150 train_time:32863ms step_avg:39.08ms
step:842/2150 train_time:32921ms step_avg:39.10ms
step:843/2150 train_time:32982ms step_avg:39.12ms
step:844/2150 train_time:33041ms step_avg:39.15ms
step:845/2150 train_time:33102ms step_avg:39.17ms
step:846/2150 train_time:33162ms step_avg:39.20ms
step:847/2150 train_time:33223ms step_avg:39.22ms
step:848/2150 train_time:33282ms step_avg:39.25ms
step:849/2150 train_time:33345ms step_avg:39.28ms
step:850/2150 train_time:33404ms step_avg:39.30ms
step:851/2150 train_time:33465ms step_avg:39.32ms
step:852/2150 train_time:33524ms step_avg:39.35ms
step:853/2150 train_time:33586ms step_avg:39.37ms
step:854/2150 train_time:33645ms step_avg:39.40ms
step:855/2150 train_time:33706ms step_avg:39.42ms
step:856/2150 train_time:33765ms step_avg:39.44ms
step:857/2150 train_time:33826ms step_avg:39.47ms
step:858/2150 train_time:33885ms step_avg:39.49ms
step:859/2150 train_time:33946ms step_avg:39.52ms
step:860/2150 train_time:34006ms step_avg:39.54ms
step:861/2150 train_time:34066ms step_avg:39.57ms
step:862/2150 train_time:34125ms step_avg:39.59ms
step:863/2150 train_time:34186ms step_avg:39.61ms
step:864/2150 train_time:34246ms step_avg:39.64ms
step:865/2150 train_time:34308ms step_avg:39.66ms
step:866/2150 train_time:34367ms step_avg:39.69ms
step:867/2150 train_time:34428ms step_avg:39.71ms
step:868/2150 train_time:34487ms step_avg:39.73ms
step:869/2150 train_time:34548ms step_avg:39.76ms
step:870/2150 train_time:34607ms step_avg:39.78ms
step:871/2150 train_time:34668ms step_avg:39.80ms
step:872/2150 train_time:34727ms step_avg:39.82ms
step:873/2150 train_time:34788ms step_avg:39.85ms
step:874/2150 train_time:34847ms step_avg:39.87ms
step:875/2150 train_time:34908ms step_avg:39.90ms
step:876/2150 train_time:34967ms step_avg:39.92ms
step:877/2150 train_time:35029ms step_avg:39.94ms
step:878/2150 train_time:35088ms step_avg:39.96ms
step:879/2150 train_time:35149ms step_avg:39.99ms
step:880/2150 train_time:35208ms step_avg:40.01ms
step:881/2150 train_time:35269ms step_avg:40.03ms
step:882/2150 train_time:35329ms step_avg:40.06ms
step:883/2150 train_time:35390ms step_avg:40.08ms
step:884/2150 train_time:35449ms step_avg:40.10ms
step:885/2150 train_time:35510ms step_avg:40.12ms
step:886/2150 train_time:35569ms step_avg:40.15ms
step:887/2150 train_time:35630ms step_avg:40.17ms
step:888/2150 train_time:35690ms step_avg:40.19ms
step:889/2150 train_time:35751ms step_avg:40.21ms
step:890/2150 train_time:35810ms step_avg:40.24ms
step:891/2150 train_time:35871ms step_avg:40.26ms
step:892/2150 train_time:35931ms step_avg:40.28ms
step:893/2150 train_time:35992ms step_avg:40.30ms
step:894/2150 train_time:36051ms step_avg:40.33ms
step:895/2150 train_time:36112ms step_avg:40.35ms
step:896/2150 train_time:36172ms step_avg:40.37ms
step:897/2150 train_time:36233ms step_avg:40.39ms
step:898/2150 train_time:36293ms step_avg:40.42ms
step:899/2150 train_time:36355ms step_avg:40.44ms
step:900/2150 train_time:36414ms step_avg:40.46ms
step:901/2150 train_time:36475ms step_avg:40.48ms
step:902/2150 train_time:36535ms step_avg:40.50ms
step:903/2150 train_time:36596ms step_avg:40.53ms
step:904/2150 train_time:36656ms step_avg:40.55ms
step:905/2150 train_time:36718ms step_avg:40.57ms
step:906/2150 train_time:36778ms step_avg:40.59ms
step:907/2150 train_time:36840ms step_avg:40.62ms
step:908/2150 train_time:36899ms step_avg:40.64ms
step:909/2150 train_time:36961ms step_avg:40.66ms
step:910/2150 train_time:37020ms step_avg:40.68ms
step:911/2150 train_time:37082ms step_avg:40.70ms
step:912/2150 train_time:37141ms step_avg:40.73ms
step:913/2150 train_time:37202ms step_avg:40.75ms
step:914/2150 train_time:37261ms step_avg:40.77ms
step:915/2150 train_time:37322ms step_avg:40.79ms
step:916/2150 train_time:37382ms step_avg:40.81ms
step:917/2150 train_time:37443ms step_avg:40.83ms
step:918/2150 train_time:37503ms step_avg:40.85ms
step:919/2150 train_time:37564ms step_avg:40.87ms
step:920/2150 train_time:37623ms step_avg:40.89ms
step:921/2150 train_time:37685ms step_avg:40.92ms
step:922/2150 train_time:37745ms step_avg:40.94ms
step:923/2150 train_time:37807ms step_avg:40.96ms
step:924/2150 train_time:37866ms step_avg:40.98ms
step:925/2150 train_time:37927ms step_avg:41.00ms
step:926/2150 train_time:37986ms step_avg:41.02ms
step:927/2150 train_time:38047ms step_avg:41.04ms
step:928/2150 train_time:38107ms step_avg:41.06ms
step:929/2150 train_time:38167ms step_avg:41.08ms
step:930/2150 train_time:38227ms step_avg:41.10ms
step:931/2150 train_time:38287ms step_avg:41.12ms
step:932/2150 train_time:38347ms step_avg:41.14ms
step:933/2150 train_time:38408ms step_avg:41.17ms
step:934/2150 train_time:38468ms step_avg:41.19ms
step:935/2150 train_time:38529ms step_avg:41.21ms
step:936/2150 train_time:38588ms step_avg:41.23ms
step:937/2150 train_time:38650ms step_avg:41.25ms
step:938/2150 train_time:38709ms step_avg:41.27ms
step:939/2150 train_time:38770ms step_avg:41.29ms
step:940/2150 train_time:38829ms step_avg:41.31ms
step:941/2150 train_time:38890ms step_avg:41.33ms
step:942/2150 train_time:38950ms step_avg:41.35ms
step:943/2150 train_time:39011ms step_avg:41.37ms
step:944/2150 train_time:39071ms step_avg:41.39ms
step:945/2150 train_time:39132ms step_avg:41.41ms
step:946/2150 train_time:39191ms step_avg:41.43ms
step:947/2150 train_time:39252ms step_avg:41.45ms
step:948/2150 train_time:39312ms step_avg:41.47ms
step:949/2150 train_time:39373ms step_avg:41.49ms
step:950/2150 train_time:39433ms step_avg:41.51ms
step:951/2150 train_time:39494ms step_avg:41.53ms
step:952/2150 train_time:39553ms step_avg:41.55ms
step:953/2150 train_time:39615ms step_avg:41.57ms
step:954/2150 train_time:39675ms step_avg:41.59ms
step:955/2150 train_time:39737ms step_avg:41.61ms
step:956/2150 train_time:39796ms step_avg:41.63ms
step:957/2150 train_time:39857ms step_avg:41.65ms
step:958/2150 train_time:39917ms step_avg:41.67ms
step:959/2150 train_time:39979ms step_avg:41.69ms
step:960/2150 train_time:40039ms step_avg:41.71ms
step:961/2150 train_time:40101ms step_avg:41.73ms
step:962/2150 train_time:40160ms step_avg:41.75ms
step:963/2150 train_time:40221ms step_avg:41.77ms
step:964/2150 train_time:40281ms step_avg:41.78ms
step:965/2150 train_time:40342ms step_avg:41.80ms
step:966/2150 train_time:40401ms step_avg:41.82ms
step:967/2150 train_time:40463ms step_avg:41.84ms
step:968/2150 train_time:40522ms step_avg:41.86ms
step:969/2150 train_time:40584ms step_avg:41.88ms
step:970/2150 train_time:40644ms step_avg:41.90ms
step:971/2150 train_time:40705ms step_avg:41.92ms
step:972/2150 train_time:40764ms step_avg:41.94ms
step:973/2150 train_time:40826ms step_avg:41.96ms
step:974/2150 train_time:40885ms step_avg:41.98ms
step:975/2150 train_time:40947ms step_avg:42.00ms
step:976/2150 train_time:41005ms step_avg:42.01ms
step:977/2150 train_time:41067ms step_avg:42.03ms
step:978/2150 train_time:41126ms step_avg:42.05ms
step:979/2150 train_time:41187ms step_avg:42.07ms
step:980/2150 train_time:41247ms step_avg:42.09ms
step:981/2150 train_time:41308ms step_avg:42.11ms
step:982/2150 train_time:41367ms step_avg:42.13ms
step:983/2150 train_time:41428ms step_avg:42.14ms
step:984/2150 train_time:41488ms step_avg:42.16ms
step:985/2150 train_time:41548ms step_avg:42.18ms
step:986/2150 train_time:41608ms step_avg:42.20ms
step:987/2150 train_time:41668ms step_avg:42.22ms
step:988/2150 train_time:41727ms step_avg:42.23ms
step:989/2150 train_time:41789ms step_avg:42.25ms
step:990/2150 train_time:41848ms step_avg:42.27ms
step:991/2150 train_time:41909ms step_avg:42.29ms
step:992/2150 train_time:41968ms step_avg:42.31ms
step:993/2150 train_time:42029ms step_avg:42.33ms
step:994/2150 train_time:42089ms step_avg:42.34ms
step:995/2150 train_time:42150ms step_avg:42.36ms
step:996/2150 train_time:42210ms step_avg:42.38ms
step:997/2150 train_time:42271ms step_avg:42.40ms
step:998/2150 train_time:42331ms step_avg:42.42ms
step:999/2150 train_time:42392ms step_avg:42.43ms
step:1000/2150 train_time:42452ms step_avg:42.45ms
step:1000/2150 val_loss:3.7163 train_time:42516ms step_avg:42.52ms
step:1001/2150 train_time:42538ms step_avg:42.50ms
step:1002/2150 train_time:42575ms step_avg:42.49ms
step:1003/2150 train_time:42642ms step_avg:42.51ms
step:1004/2150 train_time:42703ms step_avg:42.53ms
step:1005/2150 train_time:42764ms step_avg:42.55ms
step:1006/2150 train_time:42823ms step_avg:42.57ms
step:1007/2150 train_time:42884ms step_avg:42.59ms
step:1008/2150 train_time:42943ms step_avg:42.60ms
step:1009/2150 train_time:43004ms step_avg:42.62ms
step:1010/2150 train_time:43063ms step_avg:42.64ms
step:1011/2150 train_time:43123ms step_avg:42.65ms
step:1012/2150 train_time:43182ms step_avg:42.67ms
step:1013/2150 train_time:43242ms step_avg:42.69ms
step:1014/2150 train_time:43301ms step_avg:42.70ms
step:1015/2150 train_time:43362ms step_avg:42.72ms
step:1016/2150 train_time:43421ms step_avg:42.74ms
step:1017/2150 train_time:43484ms step_avg:42.76ms
step:1018/2150 train_time:43545ms step_avg:42.77ms
step:1019/2150 train_time:43608ms step_avg:42.79ms
step:1020/2150 train_time:43668ms step_avg:42.81ms
step:1021/2150 train_time:43730ms step_avg:42.83ms
step:1022/2150 train_time:43790ms step_avg:42.85ms
step:1023/2150 train_time:43851ms step_avg:42.87ms
step:1024/2150 train_time:43911ms step_avg:42.88ms
step:1025/2150 train_time:43972ms step_avg:42.90ms
step:1026/2150 train_time:44031ms step_avg:42.91ms
step:1027/2150 train_time:44092ms step_avg:42.93ms
step:1028/2150 train_time:44152ms step_avg:42.95ms
step:1029/2150 train_time:44213ms step_avg:42.97ms
step:1030/2150 train_time:44272ms step_avg:42.98ms
step:1031/2150 train_time:44333ms step_avg:43.00ms
step:1032/2150 train_time:44392ms step_avg:43.02ms
step:1033/2150 train_time:44454ms step_avg:43.03ms
step:1034/2150 train_time:44513ms step_avg:43.05ms
step:1035/2150 train_time:44575ms step_avg:43.07ms
step:1036/2150 train_time:44636ms step_avg:43.08ms
step:1037/2150 train_time:44699ms step_avg:43.10ms
step:1038/2150 train_time:44759ms step_avg:43.12ms
step:1039/2150 train_time:44821ms step_avg:43.14ms
step:1040/2150 train_time:44881ms step_avg:43.15ms
step:1041/2150 train_time:44942ms step_avg:43.17ms
step:1042/2150 train_time:45001ms step_avg:43.19ms
step:1043/2150 train_time:45062ms step_avg:43.20ms
step:1044/2150 train_time:45121ms step_avg:43.22ms
step:1045/2150 train_time:45182ms step_avg:43.24ms
step:1046/2150 train_time:45241ms step_avg:43.25ms
step:1047/2150 train_time:45301ms step_avg:43.27ms
step:1048/2150 train_time:45361ms step_avg:43.28ms
step:1049/2150 train_time:45422ms step_avg:43.30ms
step:1050/2150 train_time:45482ms step_avg:43.32ms
step:1051/2150 train_time:45543ms step_avg:43.33ms
step:1052/2150 train_time:45603ms step_avg:43.35ms
step:1053/2150 train_time:45665ms step_avg:43.37ms
step:1054/2150 train_time:45725ms step_avg:43.38ms
step:1055/2150 train_time:45787ms step_avg:43.40ms
step:1056/2150 train_time:45847ms step_avg:43.42ms
step:1057/2150 train_time:45908ms step_avg:43.43ms
step:1058/2150 train_time:45967ms step_avg:43.45ms
step:1059/2150 train_time:46028ms step_avg:43.46ms
step:1060/2150 train_time:46087ms step_avg:43.48ms
step:1061/2150 train_time:46147ms step_avg:43.49ms
step:1062/2150 train_time:46206ms step_avg:43.51ms
step:1063/2150 train_time:46267ms step_avg:43.53ms
step:1064/2150 train_time:46327ms step_avg:43.54ms
step:1065/2150 train_time:46388ms step_avg:43.56ms
step:1066/2150 train_time:46447ms step_avg:43.57ms
step:1067/2150 train_time:46509ms step_avg:43.59ms
step:1068/2150 train_time:46568ms step_avg:43.60ms
step:1069/2150 train_time:46629ms step_avg:43.62ms
step:1070/2150 train_time:46689ms step_avg:43.63ms
step:1071/2150 train_time:46751ms step_avg:43.65ms
step:1072/2150 train_time:46811ms step_avg:43.67ms
step:1073/2150 train_time:46873ms step_avg:43.68ms
step:1074/2150 train_time:46933ms step_avg:43.70ms
step:1075/2150 train_time:46994ms step_avg:43.72ms
step:1076/2150 train_time:47054ms step_avg:43.73ms
step:1077/2150 train_time:47114ms step_avg:43.75ms
step:1078/2150 train_time:47174ms step_avg:43.76ms
step:1079/2150 train_time:47235ms step_avg:43.78ms
step:1080/2150 train_time:47294ms step_avg:43.79ms
step:1081/2150 train_time:47355ms step_avg:43.81ms
step:1082/2150 train_time:47414ms step_avg:43.82ms
step:1083/2150 train_time:47475ms step_avg:43.84ms
step:1084/2150 train_time:47535ms step_avg:43.85ms
step:1085/2150 train_time:47597ms step_avg:43.87ms
step:1086/2150 train_time:47657ms step_avg:43.88ms
step:1087/2150 train_time:47720ms step_avg:43.90ms
step:1088/2150 train_time:47780ms step_avg:43.92ms
step:1089/2150 train_time:47841ms step_avg:43.93ms
step:1090/2150 train_time:47901ms step_avg:43.95ms
step:1091/2150 train_time:47963ms step_avg:43.96ms
step:1092/2150 train_time:48023ms step_avg:43.98ms
step:1093/2150 train_time:48084ms step_avg:43.99ms
step:1094/2150 train_time:48143ms step_avg:44.01ms
step:1095/2150 train_time:48204ms step_avg:44.02ms
step:1096/2150 train_time:48263ms step_avg:44.04ms
step:1097/2150 train_time:48324ms step_avg:44.05ms
step:1098/2150 train_time:48383ms step_avg:44.06ms
step:1099/2150 train_time:48444ms step_avg:44.08ms
step:1100/2150 train_time:48503ms step_avg:44.09ms
step:1101/2150 train_time:48565ms step_avg:44.11ms
step:1102/2150 train_time:48625ms step_avg:44.12ms
step:1103/2150 train_time:48686ms step_avg:44.14ms
step:1104/2150 train_time:48746ms step_avg:44.15ms
step:1105/2150 train_time:48806ms step_avg:44.17ms
step:1106/2150 train_time:48866ms step_avg:44.18ms
step:1107/2150 train_time:48927ms step_avg:44.20ms
step:1108/2150 train_time:48987ms step_avg:44.21ms
step:1109/2150 train_time:49048ms step_avg:44.23ms
step:1110/2150 train_time:49107ms step_avg:44.24ms
step:1111/2150 train_time:49168ms step_avg:44.26ms
step:1112/2150 train_time:49227ms step_avg:44.27ms
step:1113/2150 train_time:49288ms step_avg:44.28ms
step:1114/2150 train_time:49347ms step_avg:44.30ms
step:1115/2150 train_time:49409ms step_avg:44.31ms
step:1116/2150 train_time:49469ms step_avg:44.33ms
step:1117/2150 train_time:49531ms step_avg:44.34ms
step:1118/2150 train_time:49591ms step_avg:44.36ms
step:1119/2150 train_time:49652ms step_avg:44.37ms
step:1120/2150 train_time:49712ms step_avg:44.39ms
step:1121/2150 train_time:49772ms step_avg:44.40ms
step:1122/2150 train_time:49832ms step_avg:44.41ms
step:1123/2150 train_time:49893ms step_avg:44.43ms
step:1124/2150 train_time:49953ms step_avg:44.44ms
step:1125/2150 train_time:50015ms step_avg:44.46ms
step:1126/2150 train_time:50074ms step_avg:44.47ms
step:1127/2150 train_time:50136ms step_avg:44.49ms
step:1128/2150 train_time:50195ms step_avg:44.50ms
step:1129/2150 train_time:50256ms step_avg:44.51ms
step:1130/2150 train_time:50316ms step_avg:44.53ms
step:1131/2150 train_time:50378ms step_avg:44.54ms
step:1132/2150 train_time:50437ms step_avg:44.56ms
step:1133/2150 train_time:50498ms step_avg:44.57ms
step:1134/2150 train_time:50558ms step_avg:44.58ms
step:1135/2150 train_time:50619ms step_avg:44.60ms
step:1136/2150 train_time:50679ms step_avg:44.61ms
step:1137/2150 train_time:50740ms step_avg:44.63ms
step:1138/2150 train_time:50800ms step_avg:44.64ms
step:1139/2150 train_time:50863ms step_avg:44.66ms
step:1140/2150 train_time:50922ms step_avg:44.67ms
step:1141/2150 train_time:50983ms step_avg:44.68ms
step:1142/2150 train_time:51042ms step_avg:44.70ms
step:1143/2150 train_time:51103ms step_avg:44.71ms
step:1144/2150 train_time:51163ms step_avg:44.72ms
step:1145/2150 train_time:51223ms step_avg:44.74ms
step:1146/2150 train_time:51283ms step_avg:44.75ms
step:1147/2150 train_time:51344ms step_avg:44.76ms
step:1148/2150 train_time:51403ms step_avg:44.78ms
step:1149/2150 train_time:51464ms step_avg:44.79ms
step:1150/2150 train_time:51524ms step_avg:44.80ms
step:1151/2150 train_time:51585ms step_avg:44.82ms
step:1152/2150 train_time:51645ms step_avg:44.83ms
step:1153/2150 train_time:51706ms step_avg:44.84ms
step:1154/2150 train_time:51765ms step_avg:44.86ms
step:1155/2150 train_time:51826ms step_avg:44.87ms
step:1156/2150 train_time:51886ms step_avg:44.88ms
step:1157/2150 train_time:51946ms step_avg:44.90ms
step:1158/2150 train_time:52005ms step_avg:44.91ms
step:1159/2150 train_time:52067ms step_avg:44.92ms
step:1160/2150 train_time:52126ms step_avg:44.94ms
step:1161/2150 train_time:52188ms step_avg:44.95ms
step:1162/2150 train_time:52247ms step_avg:44.96ms
step:1163/2150 train_time:52308ms step_avg:44.98ms
step:1164/2150 train_time:52368ms step_avg:44.99ms
step:1165/2150 train_time:52429ms step_avg:45.00ms
step:1166/2150 train_time:52488ms step_avg:45.02ms
step:1167/2150 train_time:52550ms step_avg:45.03ms
step:1168/2150 train_time:52610ms step_avg:45.04ms
step:1169/2150 train_time:52671ms step_avg:45.06ms
step:1170/2150 train_time:52731ms step_avg:45.07ms
step:1171/2150 train_time:52792ms step_avg:45.08ms
step:1172/2150 train_time:52852ms step_avg:45.10ms
step:1173/2150 train_time:52914ms step_avg:45.11ms
step:1174/2150 train_time:52975ms step_avg:45.12ms
step:1175/2150 train_time:53037ms step_avg:45.14ms
step:1176/2150 train_time:53096ms step_avg:45.15ms
step:1177/2150 train_time:53157ms step_avg:45.16ms
step:1178/2150 train_time:53217ms step_avg:45.18ms
step:1179/2150 train_time:53279ms step_avg:45.19ms
step:1180/2150 train_time:53339ms step_avg:45.20ms
step:1181/2150 train_time:53400ms step_avg:45.22ms
step:1182/2150 train_time:53459ms step_avg:45.23ms
step:1183/2150 train_time:53520ms step_avg:45.24ms
step:1184/2150 train_time:53580ms step_avg:45.25ms
step:1185/2150 train_time:53641ms step_avg:45.27ms
step:1186/2150 train_time:53701ms step_avg:45.28ms
step:1187/2150 train_time:53763ms step_avg:45.29ms
step:1188/2150 train_time:53823ms step_avg:45.31ms
step:1189/2150 train_time:53884ms step_avg:45.32ms
step:1190/2150 train_time:53943ms step_avg:45.33ms
step:1191/2150 train_time:54005ms step_avg:45.34ms
step:1192/2150 train_time:54064ms step_avg:45.36ms
step:1193/2150 train_time:54125ms step_avg:45.37ms
step:1194/2150 train_time:54185ms step_avg:45.38ms
step:1195/2150 train_time:54245ms step_avg:45.39ms
step:1196/2150 train_time:54304ms step_avg:45.40ms
step:1197/2150 train_time:54366ms step_avg:45.42ms
step:1198/2150 train_time:54426ms step_avg:45.43ms
step:1199/2150 train_time:54486ms step_avg:45.44ms
step:1200/2150 train_time:54546ms step_avg:45.46ms
step:1201/2150 train_time:54607ms step_avg:45.47ms
step:1202/2150 train_time:54666ms step_avg:45.48ms
step:1203/2150 train_time:54728ms step_avg:45.49ms
step:1204/2150 train_time:54787ms step_avg:45.50ms
step:1205/2150 train_time:54849ms step_avg:45.52ms
step:1206/2150 train_time:54909ms step_avg:45.53ms
step:1207/2150 train_time:54970ms step_avg:45.54ms
step:1208/2150 train_time:55030ms step_avg:45.55ms
step:1209/2150 train_time:55091ms step_avg:45.57ms
step:1210/2150 train_time:55150ms step_avg:45.58ms
step:1211/2150 train_time:55212ms step_avg:45.59ms
step:1212/2150 train_time:55271ms step_avg:45.60ms
step:1213/2150 train_time:55333ms step_avg:45.62ms
step:1214/2150 train_time:55392ms step_avg:45.63ms
step:1215/2150 train_time:55454ms step_avg:45.64ms
step:1216/2150 train_time:55514ms step_avg:45.65ms
step:1217/2150 train_time:55576ms step_avg:45.67ms
step:1218/2150 train_time:55635ms step_avg:45.68ms
step:1219/2150 train_time:55697ms step_avg:45.69ms
step:1220/2150 train_time:55757ms step_avg:45.70ms
step:1221/2150 train_time:55818ms step_avg:45.72ms
step:1222/2150 train_time:55878ms step_avg:45.73ms
step:1223/2150 train_time:55940ms step_avg:45.74ms
step:1224/2150 train_time:56000ms step_avg:45.75ms
step:1225/2150 train_time:56061ms step_avg:45.76ms
step:1226/2150 train_time:56120ms step_avg:45.78ms
step:1227/2150 train_time:56181ms step_avg:45.79ms
step:1228/2150 train_time:56240ms step_avg:45.80ms
step:1229/2150 train_time:56302ms step_avg:45.81ms
step:1230/2150 train_time:56362ms step_avg:45.82ms
step:1231/2150 train_time:56423ms step_avg:45.84ms
step:1232/2150 train_time:56483ms step_avg:45.85ms
step:1233/2150 train_time:56544ms step_avg:45.86ms
step:1234/2150 train_time:56603ms step_avg:45.87ms
step:1235/2150 train_time:56664ms step_avg:45.88ms
step:1236/2150 train_time:56724ms step_avg:45.89ms
step:1237/2150 train_time:56785ms step_avg:45.91ms
step:1238/2150 train_time:56844ms step_avg:45.92ms
step:1239/2150 train_time:56906ms step_avg:45.93ms
step:1240/2150 train_time:56965ms step_avg:45.94ms
step:1241/2150 train_time:57026ms step_avg:45.95ms
step:1242/2150 train_time:57086ms step_avg:45.96ms
step:1243/2150 train_time:57146ms step_avg:45.97ms
step:1244/2150 train_time:57206ms step_avg:45.99ms
step:1245/2150 train_time:57266ms step_avg:46.00ms
step:1246/2150 train_time:57326ms step_avg:46.01ms
step:1247/2150 train_time:57386ms step_avg:46.02ms
step:1248/2150 train_time:57446ms step_avg:46.03ms
step:1249/2150 train_time:57507ms step_avg:46.04ms
step:1250/2150 train_time:57567ms step_avg:46.05ms
step:1250/2150 val_loss:3.5965 train_time:57630ms step_avg:46.10ms
step:1251/2150 train_time:57653ms step_avg:46.09ms
step:1252/2150 train_time:57690ms step_avg:46.08ms
step:1253/2150 train_time:57755ms step_avg:46.09ms
step:1254/2150 train_time:57816ms step_avg:46.11ms
step:1255/2150 train_time:57878ms step_avg:46.12ms
step:1256/2150 train_time:57937ms step_avg:46.13ms
step:1257/2150 train_time:57998ms step_avg:46.14ms
step:1258/2150 train_time:58057ms step_avg:46.15ms
step:1259/2150 train_time:58117ms step_avg:46.16ms
step:1260/2150 train_time:58176ms step_avg:46.17ms
step:1261/2150 train_time:58237ms step_avg:46.18ms
step:1262/2150 train_time:58296ms step_avg:46.19ms
step:1263/2150 train_time:58356ms step_avg:46.20ms
step:1264/2150 train_time:58416ms step_avg:46.22ms
step:1265/2150 train_time:58476ms step_avg:46.23ms
step:1266/2150 train_time:58536ms step_avg:46.24ms
step:1267/2150 train_time:58600ms step_avg:46.25ms
step:1268/2150 train_time:58662ms step_avg:46.26ms
step:1269/2150 train_time:58725ms step_avg:46.28ms
step:1270/2150 train_time:58785ms step_avg:46.29ms
step:1271/2150 train_time:58847ms step_avg:46.30ms
step:1272/2150 train_time:58907ms step_avg:46.31ms
step:1273/2150 train_time:58968ms step_avg:46.32ms
step:1274/2150 train_time:59027ms step_avg:46.33ms
step:1275/2150 train_time:59088ms step_avg:46.34ms
step:1276/2150 train_time:59148ms step_avg:46.35ms
step:1277/2150 train_time:59208ms step_avg:46.37ms
step:1278/2150 train_time:59267ms step_avg:46.37ms
step:1279/2150 train_time:59328ms step_avg:46.39ms
step:1280/2150 train_time:59388ms step_avg:46.40ms
step:1281/2150 train_time:59449ms step_avg:46.41ms
step:1282/2150 train_time:59509ms step_avg:46.42ms
step:1283/2150 train_time:59571ms step_avg:46.43ms
step:1284/2150 train_time:59632ms step_avg:46.44ms
step:1285/2150 train_time:59695ms step_avg:46.46ms
step:1286/2150 train_time:59756ms step_avg:46.47ms
step:1287/2150 train_time:59819ms step_avg:46.48ms
step:1288/2150 train_time:59878ms step_avg:46.49ms
step:1289/2150 train_time:59939ms step_avg:46.50ms
step:1290/2150 train_time:59998ms step_avg:46.51ms
step:1291/2150 train_time:60058ms step_avg:46.52ms
step:1292/2150 train_time:60118ms step_avg:46.53ms
step:1293/2150 train_time:60179ms step_avg:46.54ms
step:1294/2150 train_time:60238ms step_avg:46.55ms
step:1295/2150 train_time:60299ms step_avg:46.56ms
step:1296/2150 train_time:60358ms step_avg:46.57ms
step:1297/2150 train_time:60419ms step_avg:46.58ms
step:1298/2150 train_time:60479ms step_avg:46.59ms
step:1299/2150 train_time:60540ms step_avg:46.60ms
step:1300/2150 train_time:60600ms step_avg:46.62ms
step:1301/2150 train_time:60662ms step_avg:46.63ms
step:1302/2150 train_time:60722ms step_avg:46.64ms
step:1303/2150 train_time:60784ms step_avg:46.65ms
step:1304/2150 train_time:60844ms step_avg:46.66ms
step:1305/2150 train_time:60906ms step_avg:46.67ms
step:1306/2150 train_time:60966ms step_avg:46.68ms
step:1307/2150 train_time:61027ms step_avg:46.69ms
step:1308/2150 train_time:61086ms step_avg:46.70ms
step:1309/2150 train_time:61147ms step_avg:46.71ms
step:1310/2150 train_time:61207ms step_avg:46.72ms
step:1311/2150 train_time:61269ms step_avg:46.73ms
step:1312/2150 train_time:61328ms step_avg:46.74ms
step:1313/2150 train_time:61389ms step_avg:46.75ms
step:1314/2150 train_time:61449ms step_avg:46.76ms
step:1315/2150 train_time:61511ms step_avg:46.78ms
step:1316/2150 train_time:61571ms step_avg:46.79ms
step:1317/2150 train_time:61633ms step_avg:46.80ms
step:1318/2150 train_time:61692ms step_avg:46.81ms
step:1319/2150 train_time:61754ms step_avg:46.82ms
step:1320/2150 train_time:61815ms step_avg:46.83ms
step:1321/2150 train_time:61877ms step_avg:46.84ms
step:1322/2150 train_time:61936ms step_avg:46.85ms
step:1323/2150 train_time:61997ms step_avg:46.86ms
step:1324/2150 train_time:62057ms step_avg:46.87ms
step:1325/2150 train_time:62118ms step_avg:46.88ms
step:1326/2150 train_time:62178ms step_avg:46.89ms
step:1327/2150 train_time:62239ms step_avg:46.90ms
step:1328/2150 train_time:62300ms step_avg:46.91ms
step:1329/2150 train_time:62361ms step_avg:46.92ms
step:1330/2150 train_time:62420ms step_avg:46.93ms
step:1331/2150 train_time:62481ms step_avg:46.94ms
step:1332/2150 train_time:62541ms step_avg:46.95ms
step:1333/2150 train_time:62602ms step_avg:46.96ms
step:1334/2150 train_time:62663ms step_avg:46.97ms
step:1335/2150 train_time:62724ms step_avg:46.98ms
step:1336/2150 train_time:62785ms step_avg:46.99ms
step:1337/2150 train_time:62846ms step_avg:47.01ms
step:1338/2150 train_time:62906ms step_avg:47.01ms
step:1339/2150 train_time:62968ms step_avg:47.03ms
step:1340/2150 train_time:63028ms step_avg:47.04ms
step:1341/2150 train_time:63089ms step_avg:47.05ms
step:1342/2150 train_time:63149ms step_avg:47.06ms
step:1343/2150 train_time:63210ms step_avg:47.07ms
step:1344/2150 train_time:63270ms step_avg:47.08ms
step:1345/2150 train_time:63332ms step_avg:47.09ms
step:1346/2150 train_time:63391ms step_avg:47.10ms
step:1347/2150 train_time:63453ms step_avg:47.11ms
step:1348/2150 train_time:63512ms step_avg:47.12ms
step:1349/2150 train_time:63574ms step_avg:47.13ms
step:1350/2150 train_time:63634ms step_avg:47.14ms
step:1351/2150 train_time:63696ms step_avg:47.15ms
step:1352/2150 train_time:63756ms step_avg:47.16ms
step:1353/2150 train_time:63818ms step_avg:47.17ms
step:1354/2150 train_time:63877ms step_avg:47.18ms
step:1355/2150 train_time:63938ms step_avg:47.19ms
step:1356/2150 train_time:63998ms step_avg:47.20ms
step:1357/2150 train_time:64059ms step_avg:47.21ms
step:1358/2150 train_time:64118ms step_avg:47.22ms
step:1359/2150 train_time:64179ms step_avg:47.23ms
step:1360/2150 train_time:64238ms step_avg:47.23ms
step:1361/2150 train_time:64299ms step_avg:47.24ms
step:1362/2150 train_time:64360ms step_avg:47.25ms
step:1363/2150 train_time:64420ms step_avg:47.26ms
step:1364/2150 train_time:64480ms step_avg:47.27ms
step:1365/2150 train_time:64540ms step_avg:47.28ms
step:1366/2150 train_time:64600ms step_avg:47.29ms
step:1367/2150 train_time:64661ms step_avg:47.30ms
step:1368/2150 train_time:64721ms step_avg:47.31ms
step:1369/2150 train_time:64782ms step_avg:47.32ms
step:1370/2150 train_time:64842ms step_avg:47.33ms
step:1371/2150 train_time:64903ms step_avg:47.34ms
step:1372/2150 train_time:64963ms step_avg:47.35ms
step:1373/2150 train_time:65025ms step_avg:47.36ms
step:1374/2150 train_time:65085ms step_avg:47.37ms
step:1375/2150 train_time:65146ms step_avg:47.38ms
step:1376/2150 train_time:65206ms step_avg:47.39ms
step:1377/2150 train_time:65268ms step_avg:47.40ms
step:1378/2150 train_time:65328ms step_avg:47.41ms
step:1379/2150 train_time:65390ms step_avg:47.42ms
step:1380/2150 train_time:65449ms step_avg:47.43ms
step:1381/2150 train_time:65510ms step_avg:47.44ms
step:1382/2150 train_time:65570ms step_avg:47.45ms
step:1383/2150 train_time:65631ms step_avg:47.46ms
step:1384/2150 train_time:65691ms step_avg:47.46ms
step:1385/2150 train_time:65752ms step_avg:47.47ms
step:1386/2150 train_time:65813ms step_avg:47.48ms
step:1387/2150 train_time:65876ms step_avg:47.50ms
step:1388/2150 train_time:65935ms step_avg:47.50ms
step:1389/2150 train_time:65997ms step_avg:47.51ms
step:1390/2150 train_time:66056ms step_avg:47.52ms
step:1391/2150 train_time:66118ms step_avg:47.53ms
step:1392/2150 train_time:66177ms step_avg:47.54ms
step:1393/2150 train_time:66238ms step_avg:47.55ms
step:1394/2150 train_time:66297ms step_avg:47.56ms
step:1395/2150 train_time:66358ms step_avg:47.57ms
step:1396/2150 train_time:66418ms step_avg:47.58ms
step:1397/2150 train_time:66479ms step_avg:47.59ms
step:1398/2150 train_time:66539ms step_avg:47.60ms
step:1399/2150 train_time:66600ms step_avg:47.61ms
step:1400/2150 train_time:66660ms step_avg:47.61ms
step:1401/2150 train_time:66721ms step_avg:47.62ms
step:1402/2150 train_time:66781ms step_avg:47.63ms
step:1403/2150 train_time:66842ms step_avg:47.64ms
step:1404/2150 train_time:66902ms step_avg:47.65ms
step:1405/2150 train_time:66964ms step_avg:47.66ms
step:1406/2150 train_time:67024ms step_avg:47.67ms
step:1407/2150 train_time:67086ms step_avg:47.68ms
step:1408/2150 train_time:67147ms step_avg:47.69ms
step:1409/2150 train_time:67236ms step_avg:47.72ms
step:1410/2150 train_time:67325ms step_avg:47.75ms
step:1411/2150 train_time:67414ms step_avg:47.78ms
step:1412/2150 train_time:67502ms step_avg:47.81ms
step:1413/2150 train_time:67592ms step_avg:47.84ms
step:1414/2150 train_time:67680ms step_avg:47.86ms
step:1415/2150 train_time:67769ms step_avg:47.89ms
step:1416/2150 train_time:67857ms step_avg:47.92ms
step:1417/2150 train_time:67948ms step_avg:47.95ms
step:1418/2150 train_time:68036ms step_avg:47.98ms
step:1419/2150 train_time:68125ms step_avg:48.01ms
step:1420/2150 train_time:68213ms step_avg:48.04ms
step:1421/2150 train_time:68304ms step_avg:48.07ms
step:1422/2150 train_time:68391ms step_avg:48.10ms
step:1423/2150 train_time:68481ms step_avg:48.12ms
step:1424/2150 train_time:68570ms step_avg:48.15ms
step:1425/2150 train_time:68659ms step_avg:48.18ms
step:1426/2150 train_time:68748ms step_avg:48.21ms
step:1427/2150 train_time:68837ms step_avg:48.24ms
step:1428/2150 train_time:68925ms step_avg:48.27ms
step:1429/2150 train_time:69015ms step_avg:48.30ms
step:1430/2150 train_time:69103ms step_avg:48.32ms
step:1431/2150 train_time:69192ms step_avg:48.35ms
step:1432/2150 train_time:69281ms step_avg:48.38ms
step:1433/2150 train_time:69370ms step_avg:48.41ms
step:1434/2150 train_time:69457ms step_avg:48.44ms
step:1435/2150 train_time:69546ms step_avg:48.46ms
step:1436/2150 train_time:69634ms step_avg:48.49ms
step:1437/2150 train_time:69725ms step_avg:48.52ms
step:1438/2150 train_time:69814ms step_avg:48.55ms
step:1439/2150 train_time:69903ms step_avg:48.58ms
step:1440/2150 train_time:69991ms step_avg:48.61ms
step:1441/2150 train_time:70081ms step_avg:48.63ms
step:1442/2150 train_time:70168ms step_avg:48.66ms
step:1443/2150 train_time:70258ms step_avg:48.69ms
step:1444/2150 train_time:70345ms step_avg:48.72ms
step:1445/2150 train_time:70434ms step_avg:48.74ms
step:1446/2150 train_time:70522ms step_avg:48.77ms
step:1447/2150 train_time:70611ms step_avg:48.80ms
step:1448/2150 train_time:70699ms step_avg:48.83ms
step:1449/2150 train_time:70789ms step_avg:48.85ms
step:1450/2150 train_time:70876ms step_avg:48.88ms
step:1451/2150 train_time:70967ms step_avg:48.91ms
step:1452/2150 train_time:71055ms step_avg:48.94ms
step:1453/2150 train_time:71145ms step_avg:48.96ms
step:1454/2150 train_time:71232ms step_avg:48.99ms
step:1455/2150 train_time:71322ms step_avg:49.02ms
step:1456/2150 train_time:71410ms step_avg:49.04ms
step:1457/2150 train_time:71499ms step_avg:49.07ms
step:1458/2150 train_time:71588ms step_avg:49.10ms
step:1459/2150 train_time:71678ms step_avg:49.13ms
step:1460/2150 train_time:71765ms step_avg:49.15ms
step:1461/2150 train_time:71855ms step_avg:49.18ms
step:1462/2150 train_time:71942ms step_avg:49.21ms
step:1463/2150 train_time:72031ms step_avg:49.24ms
step:1464/2150 train_time:72120ms step_avg:49.26ms
step:1465/2150 train_time:72210ms step_avg:49.29ms
step:1466/2150 train_time:72298ms step_avg:49.32ms
step:1467/2150 train_time:72387ms step_avg:49.34ms
step:1468/2150 train_time:72475ms step_avg:49.37ms
step:1469/2150 train_time:72564ms step_avg:49.40ms
step:1470/2150 train_time:72652ms step_avg:49.42ms
step:1471/2150 train_time:72741ms step_avg:49.45ms
step:1472/2150 train_time:72830ms step_avg:49.48ms
step:1473/2150 train_time:72919ms step_avg:49.50ms
step:1474/2150 train_time:73006ms step_avg:49.53ms
step:1475/2150 train_time:73096ms step_avg:49.56ms
step:1476/2150 train_time:73183ms step_avg:49.58ms
step:1477/2150 train_time:73273ms step_avg:49.61ms
step:1478/2150 train_time:73361ms step_avg:49.64ms
step:1479/2150 train_time:73451ms step_avg:49.66ms
step:1480/2150 train_time:73538ms step_avg:49.69ms
step:1481/2150 train_time:73627ms step_avg:49.71ms
step:1482/2150 train_time:73715ms step_avg:49.74ms
step:1483/2150 train_time:73804ms step_avg:49.77ms
step:1484/2150 train_time:73892ms step_avg:49.79ms
step:1485/2150 train_time:73981ms step_avg:49.82ms
step:1486/2150 train_time:74068ms step_avg:49.84ms
step:1487/2150 train_time:74158ms step_avg:49.87ms
step:1488/2150 train_time:74246ms step_avg:49.90ms
step:1489/2150 train_time:74335ms step_avg:49.92ms
step:1490/2150 train_time:74423ms step_avg:49.95ms
step:1491/2150 train_time:74512ms step_avg:49.97ms
step:1492/2150 train_time:74601ms step_avg:50.00ms
step:1493/2150 train_time:74691ms step_avg:50.03ms
step:1494/2150 train_time:74780ms step_avg:50.05ms
step:1495/2150 train_time:74870ms step_avg:50.08ms
step:1496/2150 train_time:74957ms step_avg:50.10ms
step:1497/2150 train_time:75046ms step_avg:50.13ms
step:1498/2150 train_time:75133ms step_avg:50.16ms
step:1499/2150 train_time:75223ms step_avg:50.18ms
step:1500/2150 train_time:75310ms step_avg:50.21ms
step:1500/2150 val_loss:3.4943 train_time:75402ms step_avg:50.27ms
step:1501/2150 train_time:75425ms step_avg:50.25ms
step:1502/2150 train_time:75494ms step_avg:50.26ms
step:1503/2150 train_time:75595ms step_avg:50.30ms
step:1504/2150 train_time:75686ms step_avg:50.32ms
step:1505/2150 train_time:75774ms step_avg:50.35ms
step:1506/2150 train_time:75860ms step_avg:50.37ms
step:1507/2150 train_time:75948ms step_avg:50.40ms
step:1508/2150 train_time:76034ms step_avg:50.42ms
step:1509/2150 train_time:76122ms step_avg:50.45ms
step:1510/2150 train_time:76209ms step_avg:50.47ms
step:1511/2150 train_time:76296ms step_avg:50.49ms
step:1512/2150 train_time:76386ms step_avg:50.52ms
step:1513/2150 train_time:76477ms step_avg:50.55ms
step:1514/2150 train_time:76572ms step_avg:50.58ms
step:1515/2150 train_time:76664ms step_avg:50.60ms
step:1516/2150 train_time:76752ms step_avg:50.63ms
step:1517/2150 train_time:76840ms step_avg:50.65ms
step:1518/2150 train_time:76926ms step_avg:50.68ms
step:1519/2150 train_time:77015ms step_avg:50.70ms
step:1520/2150 train_time:77101ms step_avg:50.72ms
step:1521/2150 train_time:77188ms step_avg:50.75ms
step:1522/2150 train_time:77275ms step_avg:50.77ms
step:1523/2150 train_time:77364ms step_avg:50.80ms
step:1524/2150 train_time:77454ms step_avg:50.82ms
step:1525/2150 train_time:77545ms step_avg:50.85ms
step:1526/2150 train_time:77633ms step_avg:50.87ms
step:1527/2150 train_time:77724ms step_avg:50.90ms
step:1528/2150 train_time:77812ms step_avg:50.92ms
step:1529/2150 train_time:77900ms step_avg:50.95ms
step:1530/2150 train_time:77987ms step_avg:50.97ms
step:1531/2150 train_time:78076ms step_avg:51.00ms
step:1532/2150 train_time:78162ms step_avg:51.02ms
step:1533/2150 train_time:78250ms step_avg:51.04ms
step:1534/2150 train_time:78337ms step_avg:51.07ms
step:1535/2150 train_time:78427ms step_avg:51.09ms
step:1536/2150 train_time:78516ms step_avg:51.12ms
step:1537/2150 train_time:78606ms step_avg:51.14ms
step:1538/2150 train_time:78694ms step_avg:51.17ms
step:1539/2150 train_time:78784ms step_avg:51.19ms
step:1540/2150 train_time:78871ms step_avg:51.22ms
step:1541/2150 train_time:78960ms step_avg:51.24ms
step:1542/2150 train_time:79047ms step_avg:51.26ms
step:1543/2150 train_time:79135ms step_avg:51.29ms
step:1544/2150 train_time:79222ms step_avg:51.31ms
step:1545/2150 train_time:79310ms step_avg:51.33ms
step:1546/2150 train_time:79398ms step_avg:51.36ms
step:1547/2150 train_time:79489ms step_avg:51.38ms
step:1548/2150 train_time:79576ms step_avg:51.41ms
step:1549/2150 train_time:79666ms step_avg:51.43ms
step:1550/2150 train_time:79754ms step_avg:51.45ms
step:1551/2150 train_time:79843ms step_avg:51.48ms
step:1552/2150 train_time:79931ms step_avg:51.50ms
step:1553/2150 train_time:80020ms step_avg:51.53ms
step:1554/2150 train_time:80106ms step_avg:51.55ms
step:1555/2150 train_time:80196ms step_avg:51.57ms
step:1556/2150 train_time:80283ms step_avg:51.60ms
step:1557/2150 train_time:80371ms step_avg:51.62ms
step:1558/2150 train_time:80461ms step_avg:51.64ms
step:1559/2150 train_time:80550ms step_avg:51.67ms
step:1560/2150 train_time:80638ms step_avg:51.69ms
step:1561/2150 train_time:80727ms step_avg:51.72ms
step:1562/2150 train_time:80815ms step_avg:51.74ms
step:1563/2150 train_time:80903ms step_avg:51.76ms
step:1564/2150 train_time:80991ms step_avg:51.78ms
step:1565/2150 train_time:81079ms step_avg:51.81ms
step:1566/2150 train_time:81166ms step_avg:51.83ms
step:1567/2150 train_time:81256ms step_avg:51.85ms
step:1568/2150 train_time:81343ms step_avg:51.88ms
step:1569/2150 train_time:81431ms step_avg:51.90ms
step:1570/2150 train_time:81521ms step_avg:51.92ms
step:1571/2150 train_time:81609ms step_avg:51.95ms
step:1572/2150 train_time:81697ms step_avg:51.97ms
step:1573/2150 train_time:81786ms step_avg:51.99ms
step:1574/2150 train_time:81874ms step_avg:52.02ms
step:1575/2150 train_time:81963ms step_avg:52.04ms
step:1576/2150 train_time:82051ms step_avg:52.06ms
step:1577/2150 train_time:82140ms step_avg:52.09ms
step:1578/2150 train_time:82227ms step_avg:52.11ms
step:1579/2150 train_time:82316ms step_avg:52.13ms
step:1580/2150 train_time:82404ms step_avg:52.15ms
step:1581/2150 train_time:82493ms step_avg:52.18ms
step:1582/2150 train_time:82581ms step_avg:52.20ms
step:1583/2150 train_time:82671ms step_avg:52.22ms
step:1584/2150 train_time:82759ms step_avg:52.25ms
step:1585/2150 train_time:82849ms step_avg:52.27ms
step:1586/2150 train_time:82937ms step_avg:52.29ms
step:1587/2150 train_time:83026ms step_avg:52.32ms
step:1588/2150 train_time:83113ms step_avg:52.34ms
step:1589/2150 train_time:83202ms step_avg:52.36ms
step:1590/2150 train_time:83290ms step_avg:52.38ms
step:1591/2150 train_time:83379ms step_avg:52.41ms
step:1592/2150 train_time:83467ms step_avg:52.43ms
step:1593/2150 train_time:83556ms step_avg:52.45ms
step:1594/2150 train_time:83644ms step_avg:52.47ms
step:1595/2150 train_time:83733ms step_avg:52.50ms
step:1596/2150 train_time:83821ms step_avg:52.52ms
step:1597/2150 train_time:83911ms step_avg:52.54ms
step:1598/2150 train_time:83998ms step_avg:52.56ms
step:1599/2150 train_time:84087ms step_avg:52.59ms
step:1600/2150 train_time:84175ms step_avg:52.61ms
step:1601/2150 train_time:84264ms step_avg:52.63ms
step:1602/2150 train_time:84352ms step_avg:52.65ms
step:1603/2150 train_time:84443ms step_avg:52.68ms
step:1604/2150 train_time:84530ms step_avg:52.70ms
step:1605/2150 train_time:84618ms step_avg:52.72ms
step:1606/2150 train_time:84706ms step_avg:52.74ms
step:1607/2150 train_time:84795ms step_avg:52.77ms
step:1608/2150 train_time:84883ms step_avg:52.79ms
step:1609/2150 train_time:84972ms step_avg:52.81ms
step:1610/2150 train_time:85060ms step_avg:52.83ms
step:1611/2150 train_time:85150ms step_avg:52.86ms
step:1612/2150 train_time:85237ms step_avg:52.88ms
step:1613/2150 train_time:85326ms step_avg:52.90ms
step:1614/2150 train_time:85416ms step_avg:52.92ms
step:1615/2150 train_time:85505ms step_avg:52.94ms
step:1616/2150 train_time:85592ms step_avg:52.97ms
step:1617/2150 train_time:85682ms step_avg:52.99ms
step:1618/2150 train_time:85769ms step_avg:53.01ms
step:1619/2150 train_time:85858ms step_avg:53.03ms
step:1620/2150 train_time:85946ms step_avg:53.05ms
step:1621/2150 train_time:86035ms step_avg:53.08ms
step:1622/2150 train_time:86122ms step_avg:53.10ms
step:1623/2150 train_time:86211ms step_avg:53.12ms
step:1624/2150 train_time:86299ms step_avg:53.14ms
step:1625/2150 train_time:86388ms step_avg:53.16ms
step:1626/2150 train_time:86476ms step_avg:53.18ms
step:1627/2150 train_time:86564ms step_avg:53.20ms
step:1628/2150 train_time:86652ms step_avg:53.23ms
step:1629/2150 train_time:86741ms step_avg:53.25ms
step:1630/2150 train_time:86829ms step_avg:53.27ms
step:1631/2150 train_time:86917ms step_avg:53.29ms
step:1632/2150 train_time:87004ms step_avg:53.31ms
step:1633/2150 train_time:87093ms step_avg:53.33ms
step:1634/2150 train_time:87181ms step_avg:53.35ms
step:1635/2150 train_time:87270ms step_avg:53.38ms
step:1636/2150 train_time:87358ms step_avg:53.40ms
step:1637/2150 train_time:87447ms step_avg:53.42ms
step:1638/2150 train_time:87535ms step_avg:53.44ms
step:1639/2150 train_time:87625ms step_avg:53.46ms
step:1640/2150 train_time:87713ms step_avg:53.48ms
step:1641/2150 train_time:87802ms step_avg:53.51ms
step:1642/2150 train_time:87890ms step_avg:53.53ms
step:1643/2150 train_time:87979ms step_avg:53.55ms
step:1644/2150 train_time:88067ms step_avg:53.57ms
step:1645/2150 train_time:88156ms step_avg:53.59ms
step:1646/2150 train_time:88243ms step_avg:53.61ms
step:1647/2150 train_time:88333ms step_avg:53.63ms
step:1648/2150 train_time:88421ms step_avg:53.65ms
step:1649/2150 train_time:88510ms step_avg:53.67ms
step:1650/2150 train_time:88598ms step_avg:53.70ms
step:1651/2150 train_time:88687ms step_avg:53.72ms
step:1652/2150 train_time:88775ms step_avg:53.74ms
step:1653/2150 train_time:88864ms step_avg:53.76ms
step:1654/2150 train_time:88952ms step_avg:53.78ms
step:1655/2150 train_time:89041ms step_avg:53.80ms
step:1656/2150 train_time:89129ms step_avg:53.82ms
step:1657/2150 train_time:89218ms step_avg:53.84ms
step:1658/2150 train_time:89305ms step_avg:53.86ms
step:1659/2150 train_time:89394ms step_avg:53.88ms
step:1660/2150 train_time:89482ms step_avg:53.90ms
step:1661/2150 train_time:89570ms step_avg:53.93ms
step:1662/2150 train_time:89658ms step_avg:53.95ms
step:1663/2150 train_time:89747ms step_avg:53.97ms
step:1664/2150 train_time:89835ms step_avg:53.99ms
step:1665/2150 train_time:89925ms step_avg:54.01ms
step:1666/2150 train_time:90012ms step_avg:54.03ms
step:1667/2150 train_time:90101ms step_avg:54.05ms
step:1668/2150 train_time:90189ms step_avg:54.07ms
step:1669/2150 train_time:90278ms step_avg:54.09ms
step:1670/2150 train_time:90366ms step_avg:54.11ms
step:1671/2150 train_time:90456ms step_avg:54.13ms
step:1672/2150 train_time:90544ms step_avg:54.15ms
step:1673/2150 train_time:90632ms step_avg:54.17ms
step:1674/2150 train_time:90721ms step_avg:54.19ms
step:1675/2150 train_time:90810ms step_avg:54.21ms
step:1676/2150 train_time:90897ms step_avg:54.23ms
step:1677/2150 train_time:90987ms step_avg:54.26ms
step:1678/2150 train_time:91075ms step_avg:54.28ms
step:1679/2150 train_time:91164ms step_avg:54.30ms
step:1680/2150 train_time:91252ms step_avg:54.32ms
step:1681/2150 train_time:91340ms step_avg:54.34ms
step:1682/2150 train_time:91428ms step_avg:54.36ms
step:1683/2150 train_time:91517ms step_avg:54.38ms
step:1684/2150 train_time:91604ms step_avg:54.40ms
step:1685/2150 train_time:91693ms step_avg:54.42ms
step:1686/2150 train_time:91781ms step_avg:54.44ms
step:1687/2150 train_time:91870ms step_avg:54.46ms
step:1688/2150 train_time:91958ms step_avg:54.48ms
step:1689/2150 train_time:92048ms step_avg:54.50ms
step:1690/2150 train_time:92135ms step_avg:54.52ms
step:1691/2150 train_time:92224ms step_avg:54.54ms
step:1692/2150 train_time:92312ms step_avg:54.56ms
step:1693/2150 train_time:92402ms step_avg:54.58ms
step:1694/2150 train_time:92490ms step_avg:54.60ms
step:1695/2150 train_time:92578ms step_avg:54.62ms
step:1696/2150 train_time:92666ms step_avg:54.64ms
step:1697/2150 train_time:92755ms step_avg:54.66ms
step:1698/2150 train_time:92842ms step_avg:54.68ms
step:1699/2150 train_time:92932ms step_avg:54.70ms
step:1700/2150 train_time:93020ms step_avg:54.72ms
step:1701/2150 train_time:93108ms step_avg:54.74ms
step:1702/2150 train_time:93196ms step_avg:54.76ms
step:1703/2150 train_time:93285ms step_avg:54.78ms
step:1704/2150 train_time:93372ms step_avg:54.80ms
step:1705/2150 train_time:93463ms step_avg:54.82ms
step:1706/2150 train_time:93550ms step_avg:54.84ms
step:1707/2150 train_time:93640ms step_avg:54.86ms
step:1708/2150 train_time:93727ms step_avg:54.88ms
step:1709/2150 train_time:93816ms step_avg:54.90ms
step:1710/2150 train_time:93904ms step_avg:54.91ms
step:1711/2150 train_time:93993ms step_avg:54.93ms
step:1712/2150 train_time:94081ms step_avg:54.95ms
step:1713/2150 train_time:94169ms step_avg:54.97ms
step:1714/2150 train_time:94257ms step_avg:54.99ms
step:1715/2150 train_time:94345ms step_avg:55.01ms
step:1716/2150 train_time:94432ms step_avg:55.03ms
step:1717/2150 train_time:94522ms step_avg:55.05ms
step:1718/2150 train_time:94609ms step_avg:55.07ms
step:1719/2150 train_time:94699ms step_avg:55.09ms
step:1720/2150 train_time:94788ms step_avg:55.11ms
step:1721/2150 train_time:94877ms step_avg:55.13ms
step:1722/2150 train_time:94965ms step_avg:55.15ms
step:1723/2150 train_time:95054ms step_avg:55.17ms
step:1724/2150 train_time:95141ms step_avg:55.19ms
step:1725/2150 train_time:95231ms step_avg:55.21ms
step:1726/2150 train_time:95318ms step_avg:55.22ms
step:1727/2150 train_time:95407ms step_avg:55.24ms
step:1728/2150 train_time:95495ms step_avg:55.26ms
step:1729/2150 train_time:95585ms step_avg:55.28ms
step:1730/2150 train_time:95673ms step_avg:55.30ms
step:1731/2150 train_time:95762ms step_avg:55.32ms
step:1732/2150 train_time:95850ms step_avg:55.34ms
step:1733/2150 train_time:95940ms step_avg:55.36ms
step:1734/2150 train_time:96027ms step_avg:55.38ms
step:1735/2150 train_time:96116ms step_avg:55.40ms
step:1736/2150 train_time:96203ms step_avg:55.42ms
step:1737/2150 train_time:96292ms step_avg:55.44ms
step:1738/2150 train_time:96380ms step_avg:55.45ms
step:1739/2150 train_time:96470ms step_avg:55.47ms
step:1740/2150 train_time:96558ms step_avg:55.49ms
step:1741/2150 train_time:96646ms step_avg:55.51ms
step:1742/2150 train_time:96734ms step_avg:55.53ms
step:1743/2150 train_time:96825ms step_avg:55.55ms
step:1744/2150 train_time:96912ms step_avg:55.57ms
step:1745/2150 train_time:97001ms step_avg:55.59ms
step:1746/2150 train_time:97089ms step_avg:55.61ms
step:1747/2150 train_time:97178ms step_avg:55.63ms
step:1748/2150 train_time:97265ms step_avg:55.64ms
step:1749/2150 train_time:97354ms step_avg:55.66ms
step:1750/2150 train_time:97443ms step_avg:55.68ms
step:1750/2150 val_loss:3.3927 train_time:97534ms step_avg:55.73ms
step:1751/2150 train_time:97557ms step_avg:55.72ms
step:1752/2150 train_time:97627ms step_avg:55.72ms
step:1753/2150 train_time:97720ms step_avg:55.74ms
step:1754/2150 train_time:97808ms step_avg:55.76ms
step:1755/2150 train_time:97897ms step_avg:55.78ms
step:1756/2150 train_time:97983ms step_avg:55.80ms
step:1757/2150 train_time:98071ms step_avg:55.82ms
step:1758/2150 train_time:98157ms step_avg:55.83ms
step:1759/2150 train_time:98245ms step_avg:55.85ms
step:1760/2150 train_time:98331ms step_avg:55.87ms
step:1761/2150 train_time:98419ms step_avg:55.89ms
step:1762/2150 train_time:98508ms step_avg:55.91ms
step:1763/2150 train_time:98600ms step_avg:55.93ms
step:1764/2150 train_time:98691ms step_avg:55.95ms
step:1765/2150 train_time:98782ms step_avg:55.97ms
step:1766/2150 train_time:98869ms step_avg:55.98ms
step:1767/2150 train_time:98957ms step_avg:56.00ms
step:1768/2150 train_time:99044ms step_avg:56.02ms
step:1769/2150 train_time:99132ms step_avg:56.04ms
step:1770/2150 train_time:99218ms step_avg:56.06ms
step:1771/2150 train_time:99306ms step_avg:56.07ms
step:1772/2150 train_time:99393ms step_avg:56.09ms
step:1773/2150 train_time:99483ms step_avg:56.11ms
step:1774/2150 train_time:99571ms step_avg:56.13ms
step:1775/2150 train_time:99662ms step_avg:56.15ms
step:1776/2150 train_time:99752ms step_avg:56.17ms
step:1777/2150 train_time:99841ms step_avg:56.19ms
step:1778/2150 train_time:99929ms step_avg:56.20ms
step:1779/2150 train_time:100017ms step_avg:56.22ms
step:1780/2150 train_time:100104ms step_avg:56.24ms
step:1781/2150 train_time:100193ms step_avg:56.26ms
step:1782/2150 train_time:100280ms step_avg:56.27ms
step:1783/2150 train_time:100368ms step_avg:56.29ms
step:1784/2150 train_time:100455ms step_avg:56.31ms
step:1785/2150 train_time:100545ms step_avg:56.33ms
step:1786/2150 train_time:100634ms step_avg:56.35ms
step:1787/2150 train_time:100725ms step_avg:56.37ms
step:1788/2150 train_time:100813ms step_avg:56.38ms
step:1789/2150 train_time:100903ms step_avg:56.40ms
step:1790/2150 train_time:100991ms step_avg:56.42ms
step:1791/2150 train_time:101079ms step_avg:56.44ms
step:1792/2150 train_time:101165ms step_avg:56.45ms
step:1793/2150 train_time:101254ms step_avg:56.47ms
step:1794/2150 train_time:101341ms step_avg:56.49ms
step:1795/2150 train_time:101430ms step_avg:56.51ms
step:1796/2150 train_time:101517ms step_avg:56.52ms
step:1797/2150 train_time:101607ms step_avg:56.54ms
step:1798/2150 train_time:101696ms step_avg:56.56ms
step:1799/2150 train_time:101786ms step_avg:56.58ms
step:1800/2150 train_time:101874ms step_avg:56.60ms
step:1801/2150 train_time:101963ms step_avg:56.61ms
step:1802/2150 train_time:102050ms step_avg:56.63ms
step:1803/2150 train_time:102139ms step_avg:56.65ms
step:1804/2150 train_time:102226ms step_avg:56.67ms
step:1805/2150 train_time:102316ms step_avg:56.68ms
step:1806/2150 train_time:102404ms step_avg:56.70ms
step:1807/2150 train_time:102493ms step_avg:56.72ms
step:1808/2150 train_time:102581ms step_avg:56.74ms
step:1809/2150 train_time:102671ms step_avg:56.76ms
step:1810/2150 train_time:102759ms step_avg:56.77ms
step:1811/2150 train_time:102850ms step_avg:56.79ms
step:1812/2150 train_time:102937ms step_avg:56.81ms
step:1813/2150 train_time:103026ms step_avg:56.83ms
step:1814/2150 train_time:103113ms step_avg:56.84ms
step:1815/2150 train_time:103201ms step_avg:56.86ms
step:1816/2150 train_time:103289ms step_avg:56.88ms
step:1817/2150 train_time:103378ms step_avg:56.89ms
step:1818/2150 train_time:103465ms step_avg:56.91ms
step:1819/2150 train_time:103555ms step_avg:56.93ms
step:1820/2150 train_time:103643ms step_avg:56.95ms
step:1821/2150 train_time:103733ms step_avg:56.96ms
step:1822/2150 train_time:103821ms step_avg:56.98ms
step:1823/2150 train_time:103910ms step_avg:57.00ms
step:1824/2150 train_time:103997ms step_avg:57.02ms
step:1825/2150 train_time:104086ms step_avg:57.03ms
step:1826/2150 train_time:104173ms step_avg:57.05ms
step:1827/2150 train_time:104262ms step_avg:57.07ms
step:1828/2150 train_time:104349ms step_avg:57.08ms
step:1829/2150 train_time:104438ms step_avg:57.10ms
step:1830/2150 train_time:104526ms step_avg:57.12ms
step:1831/2150 train_time:104616ms step_avg:57.14ms
step:1832/2150 train_time:104703ms step_avg:57.15ms
step:1833/2150 train_time:104793ms step_avg:57.17ms
step:1834/2150 train_time:104880ms step_avg:57.19ms
step:1835/2150 train_time:104969ms step_avg:57.20ms
step:1836/2150 train_time:105058ms step_avg:57.22ms
step:1837/2150 train_time:105147ms step_avg:57.24ms
step:1838/2150 train_time:105234ms step_avg:57.25ms
step:1839/2150 train_time:105323ms step_avg:57.27ms
step:1840/2150 train_time:105411ms step_avg:57.29ms
step:1841/2150 train_time:105500ms step_avg:57.31ms
step:1842/2150 train_time:105588ms step_avg:57.32ms
step:1843/2150 train_time:105677ms step_avg:57.34ms
step:1844/2150 train_time:105765ms step_avg:57.36ms
step:1845/2150 train_time:105855ms step_avg:57.37ms
step:1846/2150 train_time:105942ms step_avg:57.39ms
step:1847/2150 train_time:106031ms step_avg:57.41ms
step:1848/2150 train_time:106119ms step_avg:57.42ms
step:1849/2150 train_time:106207ms step_avg:57.44ms
step:1850/2150 train_time:106295ms step_avg:57.46ms
step:1851/2150 train_time:106383ms step_avg:57.47ms
step:1852/2150 train_time:106471ms step_avg:57.49ms
step:1853/2150 train_time:106560ms step_avg:57.51ms
step:1854/2150 train_time:106647ms step_avg:57.52ms
step:1855/2150 train_time:106737ms step_avg:57.54ms
step:1856/2150 train_time:106824ms step_avg:57.56ms
step:1857/2150 train_time:106913ms step_avg:57.57ms
step:1858/2150 train_time:107000ms step_avg:57.59ms
step:1859/2150 train_time:107089ms step_avg:57.61ms
step:1860/2150 train_time:107176ms step_avg:57.62ms
step:1861/2150 train_time:107264ms step_avg:57.64ms
step:1862/2150 train_time:107352ms step_avg:57.65ms
step:1863/2150 train_time:107441ms step_avg:57.67ms
step:1864/2150 train_time:107528ms step_avg:57.69ms
step:1865/2150 train_time:107618ms step_avg:57.70ms
step:1866/2150 train_time:107707ms step_avg:57.72ms
step:1867/2150 train_time:107796ms step_avg:57.74ms
step:1868/2150 train_time:107884ms step_avg:57.75ms
step:1869/2150 train_time:107974ms step_avg:57.77ms
step:1870/2150 train_time:108061ms step_avg:57.79ms
step:1871/2150 train_time:108150ms step_avg:57.80ms
step:1872/2150 train_time:108237ms step_avg:57.82ms
step:1873/2150 train_time:108326ms step_avg:57.84ms
step:1874/2150 train_time:108413ms step_avg:57.85ms
step:1875/2150 train_time:108502ms step_avg:57.87ms
step:1876/2150 train_time:108589ms step_avg:57.88ms
step:1877/2150 train_time:108678ms step_avg:57.90ms
step:1878/2150 train_time:108766ms step_avg:57.92ms
step:1879/2150 train_time:108857ms step_avg:57.93ms
step:1880/2150 train_time:108944ms step_avg:57.95ms
step:1881/2150 train_time:109033ms step_avg:57.97ms
step:1882/2150 train_time:109121ms step_avg:57.98ms
step:1883/2150 train_time:109209ms step_avg:58.00ms
step:1884/2150 train_time:109297ms step_avg:58.01ms
step:1885/2150 train_time:109386ms step_avg:58.03ms
step:1886/2150 train_time:109474ms step_avg:58.05ms
step:1887/2150 train_time:109563ms step_avg:58.06ms
step:1888/2150 train_time:109651ms step_avg:58.08ms
step:1889/2150 train_time:109740ms step_avg:58.09ms
step:1890/2150 train_time:109828ms step_avg:58.11ms
step:1891/2150 train_time:109918ms step_avg:58.13ms
step:1892/2150 train_time:110005ms step_avg:58.14ms
step:1893/2150 train_time:110094ms step_avg:58.16ms
step:1894/2150 train_time:110181ms step_avg:58.17ms
step:1895/2150 train_time:110270ms step_avg:58.19ms
step:1896/2150 train_time:110358ms step_avg:58.21ms
step:1897/2150 train_time:110446ms step_avg:58.22ms
step:1898/2150 train_time:110534ms step_avg:58.24ms
step:1899/2150 train_time:110623ms step_avg:58.25ms
step:1900/2150 train_time:110712ms step_avg:58.27ms
step:1901/2150 train_time:110801ms step_avg:58.29ms
step:1902/2150 train_time:110889ms step_avg:58.30ms
step:1903/2150 train_time:110979ms step_avg:58.32ms
step:1904/2150 train_time:111066ms step_avg:58.33ms
step:1905/2150 train_time:111155ms step_avg:58.35ms
step:1906/2150 train_time:111243ms step_avg:58.36ms
step:1907/2150 train_time:111332ms step_avg:58.38ms
step:1908/2150 train_time:111420ms step_avg:58.40ms
step:1909/2150 train_time:111510ms step_avg:58.41ms
step:1910/2150 train_time:111597ms step_avg:58.43ms
step:1911/2150 train_time:111686ms step_avg:58.44ms
step:1912/2150 train_time:111773ms step_avg:58.46ms
step:1913/2150 train_time:111863ms step_avg:58.48ms
step:1914/2150 train_time:111951ms step_avg:58.49ms
step:1915/2150 train_time:112039ms step_avg:58.51ms
step:1916/2150 train_time:112127ms step_avg:58.52ms
step:1917/2150 train_time:112216ms step_avg:58.54ms
step:1918/2150 train_time:112303ms step_avg:58.55ms
step:1919/2150 train_time:112392ms step_avg:58.57ms
step:1920/2150 train_time:112480ms step_avg:58.58ms
step:1921/2150 train_time:112568ms step_avg:58.60ms
step:1922/2150 train_time:112656ms step_avg:58.61ms
step:1923/2150 train_time:112745ms step_avg:58.63ms
step:1924/2150 train_time:112833ms step_avg:58.65ms
step:1925/2150 train_time:112923ms step_avg:58.66ms
step:1926/2150 train_time:113011ms step_avg:58.68ms
step:1927/2150 train_time:113100ms step_avg:58.69ms
step:1928/2150 train_time:113189ms step_avg:58.71ms
step:1929/2150 train_time:113277ms step_avg:58.72ms
step:1930/2150 train_time:113365ms step_avg:58.74ms
step:1931/2150 train_time:113454ms step_avg:58.75ms
step:1932/2150 train_time:113542ms step_avg:58.77ms
step:1933/2150 train_time:113631ms step_avg:58.78ms
step:1934/2150 train_time:113718ms step_avg:58.80ms
step:1935/2150 train_time:113807ms step_avg:58.82ms
step:1936/2150 train_time:113895ms step_avg:58.83ms
step:1937/2150 train_time:113985ms step_avg:58.85ms
step:1938/2150 train_time:114072ms step_avg:58.86ms
step:1939/2150 train_time:114161ms step_avg:58.88ms
step:1940/2150 train_time:114250ms step_avg:58.89ms
step:1941/2150 train_time:114338ms step_avg:58.91ms
step:1942/2150 train_time:114427ms step_avg:58.92ms
step:1943/2150 train_time:114517ms step_avg:58.94ms
step:1944/2150 train_time:114605ms step_avg:58.95ms
step:1945/2150 train_time:114693ms step_avg:58.97ms
step:1946/2150 train_time:114780ms step_avg:58.98ms
step:1947/2150 train_time:114869ms step_avg:59.00ms
step:1948/2150 train_time:114957ms step_avg:59.01ms
step:1949/2150 train_time:115046ms step_avg:59.03ms
step:1950/2150 train_time:115134ms step_avg:59.04ms
step:1951/2150 train_time:115224ms step_avg:59.06ms
step:1952/2150 train_time:115311ms step_avg:59.07ms
step:1953/2150 train_time:115400ms step_avg:59.09ms
step:1954/2150 train_time:115488ms step_avg:59.10ms
step:1955/2150 train_time:115577ms step_avg:59.12ms
step:1956/2150 train_time:115664ms step_avg:59.13ms
step:1957/2150 train_time:115754ms step_avg:59.15ms
step:1958/2150 train_time:115841ms step_avg:59.16ms
step:1959/2150 train_time:115929ms step_avg:59.18ms
step:1960/2150 train_time:116017ms step_avg:59.19ms
step:1961/2150 train_time:116106ms step_avg:59.21ms
step:1962/2150 train_time:116194ms step_avg:59.22ms
step:1963/2150 train_time:116283ms step_avg:59.24ms
step:1964/2150 train_time:116370ms step_avg:59.25ms
step:1965/2150 train_time:116460ms step_avg:59.27ms
step:1966/2150 train_time:116547ms step_avg:59.28ms
step:1967/2150 train_time:116637ms step_avg:59.30ms
step:1968/2150 train_time:116725ms step_avg:59.31ms
step:1969/2150 train_time:116814ms step_avg:59.33ms
step:1970/2150 train_time:116902ms step_avg:59.34ms
step:1971/2150 train_time:116991ms step_avg:59.36ms
step:1972/2150 train_time:117079ms step_avg:59.37ms
step:1973/2150 train_time:117167ms step_avg:59.39ms
step:1974/2150 train_time:117255ms step_avg:59.40ms
step:1975/2150 train_time:117343ms step_avg:59.41ms
step:1976/2150 train_time:117431ms step_avg:59.43ms
step:1977/2150 train_time:117520ms step_avg:59.44ms
step:1978/2150 train_time:117608ms step_avg:59.46ms
step:1979/2150 train_time:117697ms step_avg:59.47ms
step:1980/2150 train_time:117785ms step_avg:59.49ms
step:1981/2150 train_time:117874ms step_avg:59.50ms
step:1982/2150 train_time:117961ms step_avg:59.52ms
step:1983/2150 train_time:118050ms step_avg:59.53ms
step:1984/2150 train_time:118138ms step_avg:59.55ms
step:1985/2150 train_time:118227ms step_avg:59.56ms
step:1986/2150 train_time:118315ms step_avg:59.57ms
step:1987/2150 train_time:118404ms step_avg:59.59ms
step:1988/2150 train_time:118491ms step_avg:59.60ms
step:1989/2150 train_time:118580ms step_avg:59.62ms
step:1990/2150 train_time:118667ms step_avg:59.63ms
step:1991/2150 train_time:118757ms step_avg:59.65ms
step:1992/2150 train_time:118845ms step_avg:59.66ms
step:1993/2150 train_time:118934ms step_avg:59.68ms
step:1994/2150 train_time:119023ms step_avg:59.69ms
step:1995/2150 train_time:119112ms step_avg:59.71ms
step:1996/2150 train_time:119200ms step_avg:59.72ms
step:1997/2150 train_time:119289ms step_avg:59.73ms
step:1998/2150 train_time:119376ms step_avg:59.75ms
step:1999/2150 train_time:119465ms step_avg:59.76ms
step:2000/2150 train_time:119553ms step_avg:59.78ms
step:2000/2150 val_loss:3.3154 train_time:119643ms step_avg:59.82ms
step:2001/2150 train_time:119666ms step_avg:59.80ms
step:2002/2150 train_time:119733ms step_avg:59.81ms
step:2003/2150 train_time:119826ms step_avg:59.82ms
step:2004/2150 train_time:119914ms step_avg:59.84ms
step:2005/2150 train_time:120002ms step_avg:59.85ms
step:2006/2150 train_time:120088ms step_avg:59.86ms
step:2007/2150 train_time:120176ms step_avg:59.88ms
step:2008/2150 train_time:120262ms step_avg:59.89ms
step:2009/2150 train_time:120351ms step_avg:59.91ms
step:2010/2150 train_time:120437ms step_avg:59.92ms
step:2011/2150 train_time:120525ms step_avg:59.93ms
step:2012/2150 train_time:120614ms step_avg:59.95ms
step:2013/2150 train_time:120705ms step_avg:59.96ms
step:2014/2150 train_time:120796ms step_avg:59.98ms
step:2015/2150 train_time:120885ms step_avg:59.99ms
step:2016/2150 train_time:120972ms step_avg:60.01ms
step:2017/2150 train_time:121061ms step_avg:60.02ms
step:2018/2150 train_time:121148ms step_avg:60.03ms
step:2019/2150 train_time:121237ms step_avg:60.05ms
step:2020/2150 train_time:121324ms step_avg:60.06ms
step:2021/2150 train_time:121412ms step_avg:60.08ms
step:2022/2150 train_time:121499ms step_avg:60.09ms
step:2023/2150 train_time:121590ms step_avg:60.10ms
step:2024/2150 train_time:121682ms step_avg:60.12ms
step:2025/2150 train_time:121772ms step_avg:60.13ms
step:2026/2150 train_time:121861ms step_avg:60.15ms
step:2027/2150 train_time:121950ms step_avg:60.16ms
step:2028/2150 train_time:122037ms step_avg:60.18ms
step:2029/2150 train_time:122125ms step_avg:60.19ms
step:2030/2150 train_time:122212ms step_avg:60.20ms
step:2031/2150 train_time:122300ms step_avg:60.22ms
step:2032/2150 train_time:122387ms step_avg:60.23ms
step:2033/2150 train_time:122475ms step_avg:60.24ms
step:2034/2150 train_time:122562ms step_avg:60.26ms
step:2035/2150 train_time:122653ms step_avg:60.27ms
step:2036/2150 train_time:122742ms step_avg:60.29ms
step:2037/2150 train_time:122832ms step_avg:60.30ms
step:2038/2150 train_time:122920ms step_avg:60.31ms
step:2039/2150 train_time:123009ms step_avg:60.33ms
step:2040/2150 train_time:123097ms step_avg:60.34ms
step:2041/2150 train_time:123185ms step_avg:60.36ms
step:2042/2150 train_time:123271ms step_avg:60.37ms
step:2043/2150 train_time:123359ms step_avg:60.38ms
step:2044/2150 train_time:123446ms step_avg:60.39ms
step:2045/2150 train_time:123535ms step_avg:60.41ms
step:2046/2150 train_time:123623ms step_avg:60.42ms
step:2047/2150 train_time:123713ms step_avg:60.44ms
step:2048/2150 train_time:123801ms step_avg:60.45ms
step:2049/2150 train_time:123890ms step_avg:60.46ms
step:2050/2150 train_time:123978ms step_avg:60.48ms
step:2051/2150 train_time:124067ms step_avg:60.49ms
step:2052/2150 train_time:124154ms step_avg:60.50ms
step:2053/2150 train_time:124243ms step_avg:60.52ms
step:2054/2150 train_time:124330ms step_avg:60.53ms
step:2055/2150 train_time:124418ms step_avg:60.54ms
step:2056/2150 train_time:124506ms step_avg:60.56ms
step:2057/2150 train_time:124594ms step_avg:60.57ms
step:2058/2150 train_time:124683ms step_avg:60.58ms
step:2059/2150 train_time:124772ms step_avg:60.60ms
step:2060/2150 train_time:124861ms step_avg:60.61ms
step:2061/2150 train_time:124951ms step_avg:60.63ms
step:2062/2150 train_time:125039ms step_avg:60.64ms
step:2063/2150 train_time:125128ms step_avg:60.65ms
step:2064/2150 train_time:125216ms step_avg:60.67ms
step:2065/2150 train_time:125304ms step_avg:60.68ms
step:2066/2150 train_time:125391ms step_avg:60.69ms
step:2067/2150 train_time:125480ms step_avg:60.71ms
step:2068/2150 train_time:125567ms step_avg:60.72ms
step:2069/2150 train_time:125656ms step_avg:60.73ms
step:2070/2150 train_time:125744ms step_avg:60.75ms
step:2071/2150 train_time:125833ms step_avg:60.76ms
step:2072/2150 train_time:125921ms step_avg:60.77ms
step:2073/2150 train_time:126010ms step_avg:60.79ms
step:2074/2150 train_time:126098ms step_avg:60.80ms
step:2075/2150 train_time:126188ms step_avg:60.81ms
step:2076/2150 train_time:126274ms step_avg:60.83ms
step:2077/2150 train_time:126364ms step_avg:60.84ms
step:2078/2150 train_time:126452ms step_avg:60.85ms
step:2079/2150 train_time:126541ms step_avg:60.87ms
step:2080/2150 train_time:126629ms step_avg:60.88ms
step:2081/2150 train_time:126719ms step_avg:60.89ms
step:2082/2150 train_time:126807ms step_avg:60.91ms
step:2083/2150 train_time:126895ms step_avg:60.92ms
step:2084/2150 train_time:126983ms step_avg:60.93ms
step:2085/2150 train_time:127073ms step_avg:60.95ms
step:2086/2150 train_time:127161ms step_avg:60.96ms
step:2087/2150 train_time:127250ms step_avg:60.97ms
step:2088/2150 train_time:127337ms step_avg:60.99ms
step:2089/2150 train_time:127426ms step_avg:61.00ms
step:2090/2150 train_time:127514ms step_avg:61.01ms
step:2091/2150 train_time:127603ms step_avg:61.03ms
step:2092/2150 train_time:127690ms step_avg:61.04ms
step:2093/2150 train_time:127780ms step_avg:61.05ms
step:2094/2150 train_time:127868ms step_avg:61.06ms
step:2095/2150 train_time:127957ms step_avg:61.08ms
step:2096/2150 train_time:128045ms step_avg:61.09ms
step:2097/2150 train_time:128133ms step_avg:61.10ms
step:2098/2150 train_time:128220ms step_avg:61.12ms
step:2099/2150 train_time:128310ms step_avg:61.13ms
step:2100/2150 train_time:128398ms step_avg:61.14ms
step:2101/2150 train_time:128486ms step_avg:61.15ms
step:2102/2150 train_time:128575ms step_avg:61.17ms
step:2103/2150 train_time:128663ms step_avg:61.18ms
step:2104/2150 train_time:128751ms step_avg:61.19ms
step:2105/2150 train_time:128841ms step_avg:61.21ms
step:2106/2150 train_time:128928ms step_avg:61.22ms
step:2107/2150 train_time:129017ms step_avg:61.23ms
step:2108/2150 train_time:129104ms step_avg:61.24ms
step:2109/2150 train_time:129193ms step_avg:61.26ms
step:2110/2150 train_time:129281ms step_avg:61.27ms
step:2111/2150 train_time:129370ms step_avg:61.28ms
step:2112/2150 train_time:129457ms step_avg:61.30ms
step:2113/2150 train_time:129546ms step_avg:61.31ms
step:2114/2150 train_time:129634ms step_avg:61.32ms
step:2115/2150 train_time:129724ms step_avg:61.34ms
step:2116/2150 train_time:129812ms step_avg:61.35ms
step:2117/2150 train_time:129901ms step_avg:61.36ms
step:2118/2150 train_time:129988ms step_avg:61.37ms
step:2119/2150 train_time:130078ms step_avg:61.39ms
step:2120/2150 train_time:130166ms step_avg:61.40ms
step:2121/2150 train_time:130255ms step_avg:61.41ms
step:2122/2150 train_time:130343ms step_avg:61.42ms
step:2123/2150 train_time:130432ms step_avg:61.44ms
step:2124/2150 train_time:130520ms step_avg:61.45ms
step:2125/2150 train_time:130609ms step_avg:61.46ms
step:2126/2150 train_time:130697ms step_avg:61.48ms
step:2127/2150 train_time:130786ms step_avg:61.49ms
step:2128/2150 train_time:130876ms step_avg:61.50ms
step:2129/2150 train_time:130965ms step_avg:61.51ms
step:2130/2150 train_time:131052ms step_avg:61.53ms
step:2131/2150 train_time:131143ms step_avg:61.54ms
step:2132/2150 train_time:131232ms step_avg:61.55ms
step:2133/2150 train_time:131321ms step_avg:61.57ms
step:2134/2150 train_time:131409ms step_avg:61.58ms
step:2135/2150 train_time:131497ms step_avg:61.59ms
step:2136/2150 train_time:131586ms step_avg:61.60ms
step:2137/2150 train_time:131675ms step_avg:61.62ms
step:2138/2150 train_time:131763ms step_avg:61.63ms
step:2139/2150 train_time:131853ms step_avg:61.64ms
step:2140/2150 train_time:131940ms step_avg:61.65ms
step:2141/2150 train_time:132029ms step_avg:61.67ms
step:2142/2150 train_time:132117ms step_avg:61.68ms
step:2143/2150 train_time:132207ms step_avg:61.69ms
step:2144/2150 train_time:132295ms step_avg:61.70ms
step:2145/2150 train_time:132384ms step_avg:61.72ms
step:2146/2150 train_time:132471ms step_avg:61.73ms
step:2147/2150 train_time:132560ms step_avg:61.74ms
step:2148/2150 train_time:132648ms step_avg:61.75ms
step:2149/2150 train_time:132737ms step_avg:61.77ms
step:2150/2150 train_time:132825ms step_avg:61.78ms
step:2150/2150 val_loss:3.2808 train_time:132917ms step_avg:61.82ms
peak memory allocated: 29707 MiB reserved: 44996 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Dec 10 2025, 18:15:56) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 18:39:37 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   41C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   32C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   41C    P0            129W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   42C    P0            126W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   33C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   41C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   33C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           11477      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           11478      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           11479      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           11480      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           11481      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           11482      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           11483      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           11484      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           11478      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           11479      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           11480      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           11481      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           11482      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           11483      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           11484      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:105ms step_avg:105.26ms
step:2/2160 train_time:153ms step_avg:76.52ms
step:3/2160 train_time:176ms step_avg:58.57ms
step:4/2160 train_time:200ms step_avg:49.92ms
step:5/2160 train_time:222ms step_avg:44.38ms
step:6/2160 train_time:375ms step_avg:62.43ms
step:7/2160 train_time:395ms step_avg:56.43ms
step:8/2160 train_time:419ms step_avg:52.32ms
step:9/2160 train_time:452ms step_avg:50.24ms
step:10/2160 train_time:485ms step_avg:48.52ms
step:11/2160 train_time:519ms step_avg:47.20ms
step:12/2160 train_time:552ms step_avg:46.03ms
step:13/2160 train_time:586ms step_avg:45.09ms
step:14/2160 train_time:620ms step_avg:44.25ms
step:15/2160 train_time:654ms step_avg:43.57ms
step:16/2160 train_time:687ms step_avg:42.93ms
step:17/2160 train_time:721ms step_avg:42.40ms
step:18/2160 train_time:754ms step_avg:41.89ms
step:19/2160 train_time:788ms step_avg:41.48ms
step:20/2160 train_time:821ms step_avg:41.07ms
step:21/2160 train_time:855ms step_avg:40.73ms
step:22/2160 train_time:889ms step_avg:40.39ms
step:23/2160 train_time:923ms step_avg:40.14ms
step:24/2160 train_time:956ms step_avg:39.85ms
step:25/2160 train_time:991ms step_avg:39.62ms
step:26/2160 train_time:1024ms step_avg:39.39ms
step:27/2160 train_time:1058ms step_avg:39.18ms
step:28/2160 train_time:1091ms step_avg:38.97ms
step:29/2160 train_time:1125ms step_avg:38.80ms
step:30/2160 train_time:1159ms step_avg:38.63ms
step:31/2160 train_time:1192ms step_avg:38.47ms
step:32/2160 train_time:1226ms step_avg:38.31ms
step:33/2160 train_time:1260ms step_avg:38.18ms
step:34/2160 train_time:1294ms step_avg:38.05ms
step:35/2160 train_time:1330ms step_avg:38.00ms
step:36/2160 train_time:1364ms step_avg:37.88ms
step:37/2160 train_time:1399ms step_avg:37.81ms
step:38/2160 train_time:1432ms step_avg:37.69ms
step:39/2160 train_time:1467ms step_avg:37.63ms
step:40/2160 train_time:1501ms step_avg:37.52ms
step:41/2160 train_time:1535ms step_avg:37.44ms
step:42/2160 train_time:1568ms step_avg:37.34ms
step:43/2160 train_time:1602ms step_avg:37.26ms
step:44/2160 train_time:1636ms step_avg:37.17ms
step:45/2160 train_time:1670ms step_avg:37.10ms
step:46/2160 train_time:1703ms step_avg:37.02ms
step:47/2160 train_time:1737ms step_avg:36.96ms
step:48/2160 train_time:1770ms step_avg:36.88ms
step:49/2160 train_time:1804ms step_avg:36.82ms
step:50/2160 train_time:1838ms step_avg:36.75ms
step:51/2160 train_time:1872ms step_avg:36.70ms
step:52/2160 train_time:1905ms step_avg:36.64ms
step:53/2160 train_time:1939ms step_avg:36.59ms
step:54/2160 train_time:1973ms step_avg:36.53ms
step:55/2160 train_time:2007ms step_avg:36.49ms
step:56/2160 train_time:2040ms step_avg:36.43ms
step:57/2160 train_time:2074ms step_avg:36.38ms
step:58/2160 train_time:2107ms step_avg:36.33ms
step:59/2160 train_time:2141ms step_avg:36.29ms
step:60/2160 train_time:2174ms step_avg:36.24ms
step:61/2160 train_time:2209ms step_avg:36.21ms
step:62/2160 train_time:2242ms step_avg:36.16ms
step:63/2160 train_time:2276ms step_avg:36.13ms
step:64/2160 train_time:2310ms step_avg:36.09ms
step:65/2160 train_time:2345ms step_avg:36.07ms
step:66/2160 train_time:2378ms step_avg:36.03ms
step:67/2160 train_time:2413ms step_avg:36.01ms
step:68/2160 train_time:2446ms step_avg:35.97ms
step:69/2160 train_time:2481ms step_avg:35.95ms
step:70/2160 train_time:2514ms step_avg:35.92ms
step:71/2160 train_time:2549ms step_avg:35.90ms
step:72/2160 train_time:2582ms step_avg:35.86ms
step:73/2160 train_time:2617ms step_avg:35.84ms
step:74/2160 train_time:2650ms step_avg:35.81ms
step:75/2160 train_time:2684ms step_avg:35.79ms
step:76/2160 train_time:2718ms step_avg:35.76ms
step:77/2160 train_time:2752ms step_avg:35.74ms
step:78/2160 train_time:2785ms step_avg:35.71ms
step:79/2160 train_time:2819ms step_avg:35.69ms
step:80/2160 train_time:2852ms step_avg:35.66ms
step:81/2160 train_time:2887ms step_avg:35.64ms
step:82/2160 train_time:2920ms step_avg:35.61ms
step:83/2160 train_time:2954ms step_avg:35.59ms
step:84/2160 train_time:2987ms step_avg:35.56ms
step:85/2160 train_time:3022ms step_avg:35.55ms
step:86/2160 train_time:3055ms step_avg:35.52ms
step:87/2160 train_time:3089ms step_avg:35.50ms
step:88/2160 train_time:3122ms step_avg:35.48ms
step:89/2160 train_time:3156ms step_avg:35.46ms
step:90/2160 train_time:3189ms step_avg:35.44ms
step:91/2160 train_time:3224ms step_avg:35.43ms
step:92/2160 train_time:3257ms step_avg:35.40ms
step:93/2160 train_time:3291ms step_avg:35.39ms
step:94/2160 train_time:3325ms step_avg:35.37ms
step:95/2160 train_time:3359ms step_avg:35.36ms
step:96/2160 train_time:3392ms step_avg:35.33ms
step:97/2160 train_time:3427ms step_avg:35.33ms
step:98/2160 train_time:3460ms step_avg:35.31ms
step:99/2160 train_time:3494ms step_avg:35.30ms
step:100/2160 train_time:3528ms step_avg:35.28ms
step:101/2160 train_time:3562ms step_avg:35.27ms
step:102/2160 train_time:3595ms step_avg:35.24ms
step:103/2160 train_time:3630ms step_avg:35.24ms
step:104/2160 train_time:3663ms step_avg:35.22ms
step:105/2160 train_time:3697ms step_avg:35.21ms
step:106/2160 train_time:3730ms step_avg:35.19ms
step:107/2160 train_time:3765ms step_avg:35.19ms
step:108/2160 train_time:3798ms step_avg:35.17ms
step:109/2160 train_time:3832ms step_avg:35.16ms
step:110/2160 train_time:3865ms step_avg:35.14ms
step:111/2160 train_time:3899ms step_avg:35.13ms
step:112/2160 train_time:3933ms step_avg:35.11ms
step:113/2160 train_time:3967ms step_avg:35.10ms
step:114/2160 train_time:4000ms step_avg:35.09ms
step:115/2160 train_time:4034ms step_avg:35.08ms
step:116/2160 train_time:4067ms step_avg:35.06ms
step:117/2160 train_time:4101ms step_avg:35.05ms
step:118/2160 train_time:4134ms step_avg:35.04ms
step:119/2160 train_time:4168ms step_avg:35.03ms
step:120/2160 train_time:4202ms step_avg:35.01ms
step:121/2160 train_time:4236ms step_avg:35.01ms
step:122/2160 train_time:4269ms step_avg:34.99ms
step:123/2160 train_time:4304ms step_avg:34.99ms
step:124/2160 train_time:4337ms step_avg:34.98ms
step:125/2160 train_time:4372ms step_avg:34.97ms
step:126/2160 train_time:4405ms step_avg:34.96ms
step:127/2160 train_time:4439ms step_avg:34.95ms
step:128/2160 train_time:4473ms step_avg:34.94ms
step:129/2160 train_time:4507ms step_avg:34.94ms
step:130/2160 train_time:4540ms step_avg:34.92ms
step:131/2160 train_time:4574ms step_avg:34.92ms
step:132/2160 train_time:4607ms step_avg:34.90ms
step:133/2160 train_time:4642ms step_avg:34.90ms
step:134/2160 train_time:4675ms step_avg:34.89ms
step:135/2160 train_time:4709ms step_avg:34.88ms
step:136/2160 train_time:4742ms step_avg:34.87ms
step:137/2160 train_time:4776ms step_avg:34.86ms
step:138/2160 train_time:4809ms step_avg:34.85ms
step:139/2160 train_time:4844ms step_avg:34.85ms
step:140/2160 train_time:4877ms step_avg:34.84ms
step:141/2160 train_time:4911ms step_avg:34.83ms
step:142/2160 train_time:4945ms step_avg:34.82ms
step:143/2160 train_time:4978ms step_avg:34.81ms
step:144/2160 train_time:5011ms step_avg:34.80ms
step:145/2160 train_time:5046ms step_avg:34.80ms
step:146/2160 train_time:5079ms step_avg:34.79ms
step:147/2160 train_time:5113ms step_avg:34.78ms
step:148/2160 train_time:5146ms step_avg:34.77ms
step:149/2160 train_time:5180ms step_avg:34.77ms
step:150/2160 train_time:5214ms step_avg:34.76ms
step:151/2160 train_time:5248ms step_avg:34.75ms
step:152/2160 train_time:5281ms step_avg:34.74ms
step:153/2160 train_time:5315ms step_avg:34.74ms
step:154/2160 train_time:5348ms step_avg:34.73ms
step:155/2160 train_time:5382ms step_avg:34.72ms
step:156/2160 train_time:5415ms step_avg:34.71ms
step:157/2160 train_time:5449ms step_avg:34.71ms
step:158/2160 train_time:5483ms step_avg:34.70ms
step:159/2160 train_time:5517ms step_avg:34.70ms
step:160/2160 train_time:5550ms step_avg:34.69ms
step:161/2160 train_time:5584ms step_avg:34.68ms
step:162/2160 train_time:5617ms step_avg:34.67ms
step:163/2160 train_time:5651ms step_avg:34.67ms
step:164/2160 train_time:5684ms step_avg:34.66ms
step:165/2160 train_time:5719ms step_avg:34.66ms
step:166/2160 train_time:5752ms step_avg:34.65ms
step:167/2160 train_time:5786ms step_avg:34.65ms
step:168/2160 train_time:5820ms step_avg:34.64ms
step:169/2160 train_time:5853ms step_avg:34.64ms
step:170/2160 train_time:5887ms step_avg:34.63ms
step:171/2160 train_time:5921ms step_avg:34.62ms
step:172/2160 train_time:5954ms step_avg:34.61ms
step:173/2160 train_time:5988ms step_avg:34.61ms
step:174/2160 train_time:6022ms step_avg:34.61ms
step:175/2160 train_time:6055ms step_avg:34.60ms
step:176/2160 train_time:6089ms step_avg:34.59ms
step:177/2160 train_time:6124ms step_avg:34.60ms
step:178/2160 train_time:6156ms step_avg:34.59ms
step:179/2160 train_time:6190ms step_avg:34.58ms
step:180/2160 train_time:6224ms step_avg:34.58ms
step:181/2160 train_time:6258ms step_avg:34.57ms
step:182/2160 train_time:6291ms step_avg:34.57ms
step:183/2160 train_time:6325ms step_avg:34.57ms
step:184/2160 train_time:6359ms step_avg:34.56ms
step:185/2160 train_time:6393ms step_avg:34.56ms
step:186/2160 train_time:6426ms step_avg:34.55ms
step:187/2160 train_time:6460ms step_avg:34.55ms
step:188/2160 train_time:6494ms step_avg:34.54ms
step:189/2160 train_time:6528ms step_avg:34.54ms
step:190/2160 train_time:6561ms step_avg:34.53ms
step:191/2160 train_time:6595ms step_avg:34.53ms
step:192/2160 train_time:6629ms step_avg:34.52ms
step:193/2160 train_time:6663ms step_avg:34.52ms
step:194/2160 train_time:6696ms step_avg:34.52ms
step:195/2160 train_time:6730ms step_avg:34.51ms
step:196/2160 train_time:6763ms step_avg:34.50ms
step:197/2160 train_time:6797ms step_avg:34.50ms
step:198/2160 train_time:6830ms step_avg:34.50ms
step:199/2160 train_time:6865ms step_avg:34.50ms
step:200/2160 train_time:6898ms step_avg:34.49ms
step:201/2160 train_time:6932ms step_avg:34.49ms
step:202/2160 train_time:6965ms step_avg:34.48ms
step:203/2160 train_time:7000ms step_avg:34.48ms
step:204/2160 train_time:7033ms step_avg:34.47ms
step:205/2160 train_time:7067ms step_avg:34.47ms
step:206/2160 train_time:7100ms step_avg:34.47ms
step:207/2160 train_time:7134ms step_avg:34.46ms
step:208/2160 train_time:7167ms step_avg:34.46ms
step:209/2160 train_time:7202ms step_avg:34.46ms
step:210/2160 train_time:7235ms step_avg:34.45ms
step:211/2160 train_time:7270ms step_avg:34.45ms
step:212/2160 train_time:7303ms step_avg:34.45ms
step:213/2160 train_time:7337ms step_avg:34.45ms
step:214/2160 train_time:7370ms step_avg:34.44ms
step:215/2160 train_time:7404ms step_avg:34.44ms
step:216/2160 train_time:7437ms step_avg:34.43ms
step:217/2160 train_time:7472ms step_avg:34.43ms
step:218/2160 train_time:7505ms step_avg:34.43ms
step:219/2160 train_time:7539ms step_avg:34.43ms
step:220/2160 train_time:7572ms step_avg:34.42ms
step:221/2160 train_time:7607ms step_avg:34.42ms
step:222/2160 train_time:7640ms step_avg:34.41ms
step:223/2160 train_time:7674ms step_avg:34.41ms
step:224/2160 train_time:7707ms step_avg:34.41ms
step:225/2160 train_time:7741ms step_avg:34.40ms
step:226/2160 train_time:7774ms step_avg:34.40ms
step:227/2160 train_time:7808ms step_avg:34.40ms
step:228/2160 train_time:7841ms step_avg:34.39ms
step:229/2160 train_time:7875ms step_avg:34.39ms
step:230/2160 train_time:7908ms step_avg:34.38ms
step:231/2160 train_time:7945ms step_avg:34.39ms
step:232/2160 train_time:7975ms step_avg:34.38ms
step:233/2160 train_time:8009ms step_avg:34.38ms
step:234/2160 train_time:8043ms step_avg:34.37ms
step:235/2160 train_time:8077ms step_avg:34.37ms
step:236/2160 train_time:8110ms step_avg:34.36ms
step:237/2160 train_time:8144ms step_avg:34.36ms
step:238/2160 train_time:8177ms step_avg:34.36ms
step:239/2160 train_time:8212ms step_avg:34.36ms
step:240/2160 train_time:8245ms step_avg:34.35ms
step:241/2160 train_time:8279ms step_avg:34.35ms
step:242/2160 train_time:8312ms step_avg:34.35ms
step:243/2160 train_time:8347ms step_avg:34.35ms
step:244/2160 train_time:8380ms step_avg:34.34ms
step:245/2160 train_time:8414ms step_avg:34.34ms
step:246/2160 train_time:8447ms step_avg:34.34ms
step:247/2160 train_time:8482ms step_avg:34.34ms
step:248/2160 train_time:8515ms step_avg:34.33ms
step:249/2160 train_time:8549ms step_avg:34.33ms
step:250/2160 train_time:8582ms step_avg:34.33ms
step:250/2160 val_loss:4.3056 train_time:8617ms step_avg:34.47ms
step:251/2160 train_time:8639ms step_avg:34.42ms
step:252/2160 train_time:8661ms step_avg:34.37ms
step:253/2160 train_time:8688ms step_avg:34.34ms
step:254/2160 train_time:8723ms step_avg:34.34ms
step:255/2160 train_time:8760ms step_avg:34.35ms
step:256/2160 train_time:8795ms step_avg:34.35ms
step:257/2160 train_time:8830ms step_avg:34.36ms
step:258/2160 train_time:8863ms step_avg:34.35ms
step:259/2160 train_time:8898ms step_avg:34.36ms
step:260/2160 train_time:8931ms step_avg:34.35ms
step:261/2160 train_time:8965ms step_avg:34.35ms
step:262/2160 train_time:8999ms step_avg:34.35ms
step:263/2160 train_time:9033ms step_avg:34.34ms
step:264/2160 train_time:9066ms step_avg:34.34ms
step:265/2160 train_time:9100ms step_avg:34.34ms
step:266/2160 train_time:9133ms step_avg:34.33ms
step:267/2160 train_time:9167ms step_avg:34.33ms
step:268/2160 train_time:9200ms step_avg:34.33ms
step:269/2160 train_time:9234ms step_avg:34.33ms
step:270/2160 train_time:9267ms step_avg:34.32ms
step:271/2160 train_time:9300ms step_avg:34.32ms
step:272/2160 train_time:9333ms step_avg:34.31ms
step:273/2160 train_time:9367ms step_avg:34.31ms
step:274/2160 train_time:9400ms step_avg:34.31ms
step:275/2160 train_time:9434ms step_avg:34.31ms
step:276/2160 train_time:9467ms step_avg:34.30ms
step:277/2160 train_time:9501ms step_avg:34.30ms
step:278/2160 train_time:9534ms step_avg:34.30ms
step:279/2160 train_time:9568ms step_avg:34.29ms
step:280/2160 train_time:9601ms step_avg:34.29ms
step:281/2160 train_time:9635ms step_avg:34.29ms
step:282/2160 train_time:9668ms step_avg:34.29ms
step:283/2160 train_time:9702ms step_avg:34.28ms
step:284/2160 train_time:9736ms step_avg:34.28ms
step:285/2160 train_time:9770ms step_avg:34.28ms
step:286/2160 train_time:9804ms step_avg:34.28ms
step:287/2160 train_time:9838ms step_avg:34.28ms
step:288/2160 train_time:9871ms step_avg:34.28ms
step:289/2160 train_time:9907ms step_avg:34.28ms
step:290/2160 train_time:9939ms step_avg:34.27ms
step:291/2160 train_time:9973ms step_avg:34.27ms
step:292/2160 train_time:10006ms step_avg:34.27ms
step:293/2160 train_time:10040ms step_avg:34.27ms
step:294/2160 train_time:10073ms step_avg:34.26ms
step:295/2160 train_time:10107ms step_avg:34.26ms
step:296/2160 train_time:10140ms step_avg:34.26ms
step:297/2160 train_time:10175ms step_avg:34.26ms
step:298/2160 train_time:10208ms step_avg:34.25ms
step:299/2160 train_time:10242ms step_avg:34.25ms
step:300/2160 train_time:10275ms step_avg:34.25ms
step:301/2160 train_time:10309ms step_avg:34.25ms
step:302/2160 train_time:10342ms step_avg:34.24ms
step:303/2160 train_time:10376ms step_avg:34.24ms
step:304/2160 train_time:10409ms step_avg:34.24ms
step:305/2160 train_time:10443ms step_avg:34.24ms
step:306/2160 train_time:10477ms step_avg:34.24ms
step:307/2160 train_time:10510ms step_avg:34.24ms
step:308/2160 train_time:10543ms step_avg:34.23ms
step:309/2160 train_time:10577ms step_avg:34.23ms
step:310/2160 train_time:10610ms step_avg:34.23ms
step:311/2160 train_time:10644ms step_avg:34.23ms
step:312/2160 train_time:10678ms step_avg:34.22ms
step:313/2160 train_time:10712ms step_avg:34.22ms
step:314/2160 train_time:10745ms step_avg:34.22ms
step:315/2160 train_time:10779ms step_avg:34.22ms
step:316/2160 train_time:10812ms step_avg:34.22ms
step:317/2160 train_time:10847ms step_avg:34.22ms
step:318/2160 train_time:10880ms step_avg:34.21ms
step:319/2160 train_time:10914ms step_avg:34.21ms
step:320/2160 train_time:10947ms step_avg:34.21ms
step:321/2160 train_time:10981ms step_avg:34.21ms
step:322/2160 train_time:11014ms step_avg:34.21ms
step:323/2160 train_time:11048ms step_avg:34.21ms
step:324/2160 train_time:11082ms step_avg:34.20ms
step:325/2160 train_time:11116ms step_avg:34.20ms
step:326/2160 train_time:11149ms step_avg:34.20ms
step:327/2160 train_time:11183ms step_avg:34.20ms
step:328/2160 train_time:11217ms step_avg:34.20ms
step:329/2160 train_time:11250ms step_avg:34.20ms
step:330/2160 train_time:11284ms step_avg:34.19ms
step:331/2160 train_time:11318ms step_avg:34.19ms
step:332/2160 train_time:11351ms step_avg:34.19ms
step:333/2160 train_time:11385ms step_avg:34.19ms
step:334/2160 train_time:11418ms step_avg:34.19ms
step:335/2160 train_time:11453ms step_avg:34.19ms
step:336/2160 train_time:11486ms step_avg:34.18ms
step:337/2160 train_time:11520ms step_avg:34.18ms
step:338/2160 train_time:11553ms step_avg:34.18ms
step:339/2160 train_time:11587ms step_avg:34.18ms
step:340/2160 train_time:11620ms step_avg:34.18ms
step:341/2160 train_time:11654ms step_avg:34.18ms
step:342/2160 train_time:11687ms step_avg:34.17ms
step:343/2160 train_time:11721ms step_avg:34.17ms
step:344/2160 train_time:11754ms step_avg:34.17ms
step:345/2160 train_time:11789ms step_avg:34.17ms
step:346/2160 train_time:11822ms step_avg:34.17ms
step:347/2160 train_time:11856ms step_avg:34.17ms
step:348/2160 train_time:11889ms step_avg:34.16ms
step:349/2160 train_time:11923ms step_avg:34.16ms
step:350/2160 train_time:11956ms step_avg:34.16ms
step:351/2160 train_time:11990ms step_avg:34.16ms
step:352/2160 train_time:12023ms step_avg:34.16ms
step:353/2160 train_time:12058ms step_avg:34.16ms
step:354/2160 train_time:12091ms step_avg:34.15ms
step:355/2160 train_time:12125ms step_avg:34.15ms
step:356/2160 train_time:12158ms step_avg:34.15ms
step:357/2160 train_time:12193ms step_avg:34.15ms
step:358/2160 train_time:12226ms step_avg:34.15ms
step:359/2160 train_time:12260ms step_avg:34.15ms
step:360/2160 train_time:12293ms step_avg:34.15ms
step:361/2160 train_time:12327ms step_avg:34.15ms
step:362/2160 train_time:12360ms step_avg:34.14ms
step:363/2160 train_time:12394ms step_avg:34.14ms
step:364/2160 train_time:12427ms step_avg:34.14ms
step:365/2160 train_time:12461ms step_avg:34.14ms
step:366/2160 train_time:12495ms step_avg:34.14ms
step:367/2160 train_time:12528ms step_avg:34.14ms
step:368/2160 train_time:12562ms step_avg:34.14ms
step:369/2160 train_time:12595ms step_avg:34.13ms
step:370/2160 train_time:12628ms step_avg:34.13ms
step:371/2160 train_time:12662ms step_avg:34.13ms
step:372/2160 train_time:12696ms step_avg:34.13ms
step:373/2160 train_time:12730ms step_avg:34.13ms
step:374/2160 train_time:12763ms step_avg:34.13ms
step:375/2160 train_time:12798ms step_avg:34.13ms
step:376/2160 train_time:12830ms step_avg:34.12ms
step:377/2160 train_time:12865ms step_avg:34.12ms
step:378/2160 train_time:12898ms step_avg:34.12ms
step:379/2160 train_time:12932ms step_avg:34.12ms
step:380/2160 train_time:12965ms step_avg:34.12ms
step:381/2160 train_time:12999ms step_avg:34.12ms
step:382/2160 train_time:13032ms step_avg:34.12ms
step:383/2160 train_time:13066ms step_avg:34.12ms
step:384/2160 train_time:13099ms step_avg:34.11ms
step:385/2160 train_time:13134ms step_avg:34.11ms
step:386/2160 train_time:13167ms step_avg:34.11ms
step:387/2160 train_time:13202ms step_avg:34.11ms
step:388/2160 train_time:13235ms step_avg:34.11ms
step:389/2160 train_time:13269ms step_avg:34.11ms
step:390/2160 train_time:13302ms step_avg:34.11ms
step:391/2160 train_time:13336ms step_avg:34.11ms
step:392/2160 train_time:13369ms step_avg:34.10ms
step:393/2160 train_time:13403ms step_avg:34.10ms
step:394/2160 train_time:13436ms step_avg:34.10ms
step:395/2160 train_time:13470ms step_avg:34.10ms
step:396/2160 train_time:13503ms step_avg:34.10ms
step:397/2160 train_time:13537ms step_avg:34.10ms
step:398/2160 train_time:13570ms step_avg:34.10ms
step:399/2160 train_time:13605ms step_avg:34.10ms
step:400/2160 train_time:13638ms step_avg:34.09ms
step:401/2160 train_time:13672ms step_avg:34.09ms
step:402/2160 train_time:13705ms step_avg:34.09ms
step:403/2160 train_time:13739ms step_avg:34.09ms
step:404/2160 train_time:13772ms step_avg:34.09ms
step:405/2160 train_time:13807ms step_avg:34.09ms
step:406/2160 train_time:13840ms step_avg:34.09ms
step:407/2160 train_time:13874ms step_avg:34.09ms
step:408/2160 train_time:13907ms step_avg:34.09ms
step:409/2160 train_time:13941ms step_avg:34.09ms
step:410/2160 train_time:13974ms step_avg:34.08ms
step:411/2160 train_time:14008ms step_avg:34.08ms
step:412/2160 train_time:14042ms step_avg:34.08ms
step:413/2160 train_time:14077ms step_avg:34.08ms
step:414/2160 train_time:14110ms step_avg:34.08ms
step:415/2160 train_time:14144ms step_avg:34.08ms
step:416/2160 train_time:14177ms step_avg:34.08ms
step:417/2160 train_time:14211ms step_avg:34.08ms
step:418/2160 train_time:14244ms step_avg:34.08ms
step:419/2160 train_time:14279ms step_avg:34.08ms
step:420/2160 train_time:14312ms step_avg:34.08ms
step:421/2160 train_time:14346ms step_avg:34.08ms
step:422/2160 train_time:14380ms step_avg:34.07ms
step:423/2160 train_time:14414ms step_avg:34.07ms
step:424/2160 train_time:14447ms step_avg:34.07ms
step:425/2160 train_time:14481ms step_avg:34.07ms
step:426/2160 train_time:14514ms step_avg:34.07ms
step:427/2160 train_time:14548ms step_avg:34.07ms
step:428/2160 train_time:14581ms step_avg:34.07ms
step:429/2160 train_time:14615ms step_avg:34.07ms
step:430/2160 train_time:14648ms step_avg:34.07ms
step:431/2160 train_time:14683ms step_avg:34.07ms
step:432/2160 train_time:14716ms step_avg:34.06ms
step:433/2160 train_time:14750ms step_avg:34.06ms
step:434/2160 train_time:14783ms step_avg:34.06ms
step:435/2160 train_time:14817ms step_avg:34.06ms
step:436/2160 train_time:14850ms step_avg:34.06ms
step:437/2160 train_time:14885ms step_avg:34.06ms
step:438/2160 train_time:14918ms step_avg:34.06ms
step:439/2160 train_time:14952ms step_avg:34.06ms
step:440/2160 train_time:14985ms step_avg:34.06ms
step:441/2160 train_time:15019ms step_avg:34.06ms
step:442/2160 train_time:15053ms step_avg:34.06ms
step:443/2160 train_time:15087ms step_avg:34.06ms
step:444/2160 train_time:15120ms step_avg:34.05ms
step:445/2160 train_time:15154ms step_avg:34.05ms
step:446/2160 train_time:15187ms step_avg:34.05ms
step:447/2160 train_time:15222ms step_avg:34.05ms
step:448/2160 train_time:15255ms step_avg:34.05ms
step:449/2160 train_time:15289ms step_avg:34.05ms
step:450/2160 train_time:15322ms step_avg:34.05ms
step:451/2160 train_time:15356ms step_avg:34.05ms
step:452/2160 train_time:15390ms step_avg:34.05ms
step:453/2160 train_time:15424ms step_avg:34.05ms
step:454/2160 train_time:15457ms step_avg:34.05ms
step:455/2160 train_time:15491ms step_avg:34.05ms
step:456/2160 train_time:15524ms step_avg:34.04ms
step:457/2160 train_time:15559ms step_avg:34.05ms
step:458/2160 train_time:15592ms step_avg:34.04ms
step:459/2160 train_time:15626ms step_avg:34.04ms
step:460/2160 train_time:15659ms step_avg:34.04ms
step:461/2160 train_time:15693ms step_avg:34.04ms
step:462/2160 train_time:15726ms step_avg:34.04ms
step:463/2160 train_time:15760ms step_avg:34.04ms
step:464/2160 train_time:15794ms step_avg:34.04ms
step:465/2160 train_time:15827ms step_avg:34.04ms
step:466/2160 train_time:15860ms step_avg:34.04ms
step:467/2160 train_time:15895ms step_avg:34.04ms
step:468/2160 train_time:15928ms step_avg:34.03ms
step:469/2160 train_time:15962ms step_avg:34.03ms
step:470/2160 train_time:15996ms step_avg:34.03ms
step:471/2160 train_time:16029ms step_avg:34.03ms
step:472/2160 train_time:16063ms step_avg:34.03ms
step:473/2160 train_time:16100ms step_avg:34.04ms
step:474/2160 train_time:16130ms step_avg:34.03ms
step:475/2160 train_time:16164ms step_avg:34.03ms
step:476/2160 train_time:16197ms step_avg:34.03ms
step:477/2160 train_time:16231ms step_avg:34.03ms
step:478/2160 train_time:16265ms step_avg:34.03ms
step:479/2160 train_time:16299ms step_avg:34.03ms
step:480/2160 train_time:16332ms step_avg:34.03ms
step:481/2160 train_time:16366ms step_avg:34.03ms
step:482/2160 train_time:16399ms step_avg:34.02ms
step:483/2160 train_time:16434ms step_avg:34.02ms
step:484/2160 train_time:16467ms step_avg:34.02ms
step:485/2160 train_time:16501ms step_avg:34.02ms
step:486/2160 train_time:16534ms step_avg:34.02ms
step:487/2160 train_time:16568ms step_avg:34.02ms
step:488/2160 train_time:16602ms step_avg:34.02ms
step:489/2160 train_time:16636ms step_avg:34.02ms
step:490/2160 train_time:16669ms step_avg:34.02ms
step:491/2160 train_time:16703ms step_avg:34.02ms
step:492/2160 train_time:16736ms step_avg:34.02ms
step:493/2160 train_time:16770ms step_avg:34.02ms
step:494/2160 train_time:16803ms step_avg:34.01ms
step:495/2160 train_time:16837ms step_avg:34.01ms
step:496/2160 train_time:16870ms step_avg:34.01ms
step:497/2160 train_time:16904ms step_avg:34.01ms
step:498/2160 train_time:16938ms step_avg:34.01ms
step:499/2160 train_time:16972ms step_avg:34.01ms
step:500/2160 train_time:17005ms step_avg:34.01ms
step:500/2160 val_loss:4.0105 train_time:17040ms step_avg:34.08ms
step:501/2160 train_time:17064ms step_avg:34.06ms
step:502/2160 train_time:17086ms step_avg:34.04ms
step:503/2160 train_time:17113ms step_avg:34.02ms
step:504/2160 train_time:17147ms step_avg:34.02ms
step:505/2160 train_time:17182ms step_avg:34.02ms
step:506/2160 train_time:17216ms step_avg:34.02ms
step:507/2160 train_time:17251ms step_avg:34.03ms
step:508/2160 train_time:17284ms step_avg:34.02ms
step:509/2160 train_time:17318ms step_avg:34.02ms
step:510/2160 train_time:17352ms step_avg:34.02ms
step:511/2160 train_time:17386ms step_avg:34.02ms
step:512/2160 train_time:17419ms step_avg:34.02ms
step:513/2160 train_time:17453ms step_avg:34.02ms
step:514/2160 train_time:17486ms step_avg:34.02ms
step:515/2160 train_time:17520ms step_avg:34.02ms
step:516/2160 train_time:17553ms step_avg:34.02ms
step:517/2160 train_time:17587ms step_avg:34.02ms
step:518/2160 train_time:17620ms step_avg:34.02ms
step:519/2160 train_time:17653ms step_avg:34.01ms
step:520/2160 train_time:17686ms step_avg:34.01ms
step:521/2160 train_time:17721ms step_avg:34.01ms
step:522/2160 train_time:17753ms step_avg:34.01ms
step:523/2160 train_time:17787ms step_avg:34.01ms
step:524/2160 train_time:17820ms step_avg:34.01ms
step:525/2160 train_time:17854ms step_avg:34.01ms
step:526/2160 train_time:17887ms step_avg:34.01ms
step:527/2160 train_time:17921ms step_avg:34.00ms
step:528/2160 train_time:17954ms step_avg:34.00ms
step:529/2160 train_time:17989ms step_avg:34.01ms
step:530/2160 train_time:18022ms step_avg:34.00ms
step:531/2160 train_time:18058ms step_avg:34.01ms
step:532/2160 train_time:18091ms step_avg:34.01ms
step:533/2160 train_time:18126ms step_avg:34.01ms
step:534/2160 train_time:18159ms step_avg:34.01ms
step:535/2160 train_time:18195ms step_avg:34.01ms
step:536/2160 train_time:18228ms step_avg:34.01ms
step:537/2160 train_time:18262ms step_avg:34.01ms
step:538/2160 train_time:18295ms step_avg:34.01ms
step:539/2160 train_time:18330ms step_avg:34.01ms
step:540/2160 train_time:18363ms step_avg:34.01ms
step:541/2160 train_time:18398ms step_avg:34.01ms
step:542/2160 train_time:18431ms step_avg:34.01ms
step:543/2160 train_time:18465ms step_avg:34.01ms
step:544/2160 train_time:18498ms step_avg:34.00ms
step:545/2160 train_time:18532ms step_avg:34.00ms
step:546/2160 train_time:18566ms step_avg:34.00ms
step:547/2160 train_time:18599ms step_avg:34.00ms
step:548/2160 train_time:18632ms step_avg:34.00ms
step:549/2160 train_time:18666ms step_avg:34.00ms
step:550/2160 train_time:18700ms step_avg:34.00ms
step:551/2160 train_time:18733ms step_avg:34.00ms
step:552/2160 train_time:18767ms step_avg:34.00ms
step:553/2160 train_time:18801ms step_avg:34.00ms
step:554/2160 train_time:18834ms step_avg:34.00ms
step:555/2160 train_time:18868ms step_avg:34.00ms
step:556/2160 train_time:18901ms step_avg:34.00ms
step:557/2160 train_time:18935ms step_avg:33.99ms
step:558/2160 train_time:18968ms step_avg:33.99ms
step:559/2160 train_time:19003ms step_avg:33.99ms
step:560/2160 train_time:19037ms step_avg:33.99ms
step:561/2160 train_time:19070ms step_avg:33.99ms
step:562/2160 train_time:19104ms step_avg:33.99ms
step:563/2160 train_time:19138ms step_avg:33.99ms
step:564/2160 train_time:19172ms step_avg:33.99ms
step:565/2160 train_time:19206ms step_avg:33.99ms
step:566/2160 train_time:19239ms step_avg:33.99ms
step:567/2160 train_time:19274ms step_avg:33.99ms
step:568/2160 train_time:19307ms step_avg:33.99ms
step:569/2160 train_time:19342ms step_avg:33.99ms
step:570/2160 train_time:19375ms step_avg:33.99ms
step:571/2160 train_time:19409ms step_avg:33.99ms
step:572/2160 train_time:19442ms step_avg:33.99ms
step:573/2160 train_time:19477ms step_avg:33.99ms
step:574/2160 train_time:19510ms step_avg:33.99ms
step:575/2160 train_time:19544ms step_avg:33.99ms
step:576/2160 train_time:19577ms step_avg:33.99ms
step:577/2160 train_time:19612ms step_avg:33.99ms
step:578/2160 train_time:19645ms step_avg:33.99ms
step:579/2160 train_time:19679ms step_avg:33.99ms
step:580/2160 train_time:19712ms step_avg:33.99ms
step:581/2160 train_time:19746ms step_avg:33.99ms
step:582/2160 train_time:19779ms step_avg:33.98ms
step:583/2160 train_time:19813ms step_avg:33.98ms
step:584/2160 train_time:19846ms step_avg:33.98ms
step:585/2160 train_time:19880ms step_avg:33.98ms
step:586/2160 train_time:19914ms step_avg:33.98ms
step:587/2160 train_time:19947ms step_avg:33.98ms
step:588/2160 train_time:19980ms step_avg:33.98ms
step:589/2160 train_time:20015ms step_avg:33.98ms
step:590/2160 train_time:20048ms step_avg:33.98ms
step:591/2160 train_time:20082ms step_avg:33.98ms
step:592/2160 train_time:20115ms step_avg:33.98ms
step:593/2160 train_time:20150ms step_avg:33.98ms
step:594/2160 train_time:20183ms step_avg:33.98ms
step:595/2160 train_time:20217ms step_avg:33.98ms
step:596/2160 train_time:20250ms step_avg:33.98ms
step:597/2160 train_time:20285ms step_avg:33.98ms
step:598/2160 train_time:20318ms step_avg:33.98ms
step:599/2160 train_time:20353ms step_avg:33.98ms
step:600/2160 train_time:20386ms step_avg:33.98ms
step:601/2160 train_time:20420ms step_avg:33.98ms
step:602/2160 train_time:20453ms step_avg:33.98ms
step:603/2160 train_time:20487ms step_avg:33.98ms
step:604/2160 train_time:20521ms step_avg:33.97ms
step:605/2160 train_time:20555ms step_avg:33.98ms
step:606/2160 train_time:20588ms step_avg:33.97ms
step:607/2160 train_time:20622ms step_avg:33.97ms
step:608/2160 train_time:20656ms step_avg:33.97ms
step:609/2160 train_time:20690ms step_avg:33.97ms
step:610/2160 train_time:20723ms step_avg:33.97ms
step:611/2160 train_time:20757ms step_avg:33.97ms
step:612/2160 train_time:20791ms step_avg:33.97ms
step:613/2160 train_time:20825ms step_avg:33.97ms
step:614/2160 train_time:20858ms step_avg:33.97ms
step:615/2160 train_time:20892ms step_avg:33.97ms
step:616/2160 train_time:20925ms step_avg:33.97ms
step:617/2160 train_time:20959ms step_avg:33.97ms
step:618/2160 train_time:20993ms step_avg:33.97ms
step:619/2160 train_time:21027ms step_avg:33.97ms
step:620/2160 train_time:21060ms step_avg:33.97ms
step:621/2160 train_time:21095ms step_avg:33.97ms
step:622/2160 train_time:21128ms step_avg:33.97ms
step:623/2160 train_time:21163ms step_avg:33.97ms
step:624/2160 train_time:21196ms step_avg:33.97ms
step:625/2160 train_time:21230ms step_avg:33.97ms
step:626/2160 train_time:21263ms step_avg:33.97ms
step:627/2160 train_time:21298ms step_avg:33.97ms
step:628/2160 train_time:21331ms step_avg:33.97ms
step:629/2160 train_time:21365ms step_avg:33.97ms
step:630/2160 train_time:21398ms step_avg:33.97ms
step:631/2160 train_time:21433ms step_avg:33.97ms
step:632/2160 train_time:21466ms step_avg:33.96ms
step:633/2160 train_time:21500ms step_avg:33.97ms
step:634/2160 train_time:21533ms step_avg:33.96ms
step:635/2160 train_time:21568ms step_avg:33.97ms
step:636/2160 train_time:21601ms step_avg:33.96ms
step:637/2160 train_time:21636ms step_avg:33.96ms
step:638/2160 train_time:21669ms step_avg:33.96ms
step:639/2160 train_time:21703ms step_avg:33.96ms
step:640/2160 train_time:21736ms step_avg:33.96ms
step:641/2160 train_time:21771ms step_avg:33.96ms
step:642/2160 train_time:21804ms step_avg:33.96ms
step:643/2160 train_time:21838ms step_avg:33.96ms
step:644/2160 train_time:21871ms step_avg:33.96ms
step:645/2160 train_time:21905ms step_avg:33.96ms
step:646/2160 train_time:21938ms step_avg:33.96ms
step:647/2160 train_time:21973ms step_avg:33.96ms
step:648/2160 train_time:22006ms step_avg:33.96ms
step:649/2160 train_time:22040ms step_avg:33.96ms
step:650/2160 train_time:22073ms step_avg:33.96ms
step:651/2160 train_time:22108ms step_avg:33.96ms
step:652/2160 train_time:22141ms step_avg:33.96ms
step:653/2160 train_time:22175ms step_avg:33.96ms
step:654/2160 train_time:22209ms step_avg:33.96ms
step:655/2160 train_time:22243ms step_avg:33.96ms
step:656/2160 train_time:22276ms step_avg:33.96ms
step:657/2160 train_time:22310ms step_avg:33.96ms
step:658/2160 train_time:22344ms step_avg:33.96ms
step:659/2160 train_time:22377ms step_avg:33.96ms
step:660/2160 train_time:22411ms step_avg:33.96ms
step:661/2160 train_time:22445ms step_avg:33.96ms
step:662/2160 train_time:22478ms step_avg:33.96ms
step:663/2160 train_time:22513ms step_avg:33.96ms
step:664/2160 train_time:22546ms step_avg:33.96ms
step:665/2160 train_time:22580ms step_avg:33.96ms
step:666/2160 train_time:22613ms step_avg:33.95ms
step:667/2160 train_time:22648ms step_avg:33.96ms
step:668/2160 train_time:22681ms step_avg:33.95ms
step:669/2160 train_time:22715ms step_avg:33.95ms
step:670/2160 train_time:22749ms step_avg:33.95ms
step:671/2160 train_time:22783ms step_avg:33.95ms
step:672/2160 train_time:22816ms step_avg:33.95ms
step:673/2160 train_time:22851ms step_avg:33.95ms
step:674/2160 train_time:22884ms step_avg:33.95ms
step:675/2160 train_time:22918ms step_avg:33.95ms
step:676/2160 train_time:22951ms step_avg:33.95ms
step:677/2160 train_time:22985ms step_avg:33.95ms
step:678/2160 train_time:23018ms step_avg:33.95ms
step:679/2160 train_time:23053ms step_avg:33.95ms
step:680/2160 train_time:23086ms step_avg:33.95ms
step:681/2160 train_time:23120ms step_avg:33.95ms
step:682/2160 train_time:23153ms step_avg:33.95ms
step:683/2160 train_time:23188ms step_avg:33.95ms
step:684/2160 train_time:23221ms step_avg:33.95ms
step:685/2160 train_time:23255ms step_avg:33.95ms
step:686/2160 train_time:23288ms step_avg:33.95ms
step:687/2160 train_time:23323ms step_avg:33.95ms
step:688/2160 train_time:23356ms step_avg:33.95ms
step:689/2160 train_time:23390ms step_avg:33.95ms
step:690/2160 train_time:23423ms step_avg:33.95ms
step:691/2160 train_time:23457ms step_avg:33.95ms
step:692/2160 train_time:23491ms step_avg:33.95ms
step:693/2160 train_time:23525ms step_avg:33.95ms
step:694/2160 train_time:23558ms step_avg:33.95ms
step:695/2160 train_time:23592ms step_avg:33.95ms
step:696/2160 train_time:23626ms step_avg:33.94ms
step:697/2160 train_time:23660ms step_avg:33.94ms
step:698/2160 train_time:23693ms step_avg:33.94ms
step:699/2160 train_time:23728ms step_avg:33.95ms
step:700/2160 train_time:23761ms step_avg:33.94ms
step:701/2160 train_time:23795ms step_avg:33.94ms
step:702/2160 train_time:23828ms step_avg:33.94ms
step:703/2160 train_time:23863ms step_avg:33.94ms
step:704/2160 train_time:23896ms step_avg:33.94ms
step:705/2160 train_time:23930ms step_avg:33.94ms
step:706/2160 train_time:23963ms step_avg:33.94ms
step:707/2160 train_time:23997ms step_avg:33.94ms
step:708/2160 train_time:24032ms step_avg:33.94ms
step:709/2160 train_time:24092ms step_avg:33.98ms
step:710/2160 train_time:24152ms step_avg:34.02ms
step:711/2160 train_time:24214ms step_avg:34.06ms
step:712/2160 train_time:24274ms step_avg:34.09ms
step:713/2160 train_time:24336ms step_avg:34.13ms
step:714/2160 train_time:24396ms step_avg:34.17ms
step:715/2160 train_time:24458ms step_avg:34.21ms
step:716/2160 train_time:24519ms step_avg:34.24ms
step:717/2160 train_time:24581ms step_avg:34.28ms
step:718/2160 train_time:24642ms step_avg:34.32ms
step:719/2160 train_time:24703ms step_avg:34.36ms
step:720/2160 train_time:24763ms step_avg:34.39ms
step:721/2160 train_time:24825ms step_avg:34.43ms
step:722/2160 train_time:24885ms step_avg:34.47ms
step:723/2160 train_time:24946ms step_avg:34.50ms
step:724/2160 train_time:25005ms step_avg:34.54ms
step:725/2160 train_time:25066ms step_avg:34.57ms
step:726/2160 train_time:25127ms step_avg:34.61ms
step:727/2160 train_time:25188ms step_avg:34.65ms
step:728/2160 train_time:25248ms step_avg:34.68ms
step:729/2160 train_time:25310ms step_avg:34.72ms
step:730/2160 train_time:25370ms step_avg:34.75ms
step:731/2160 train_time:25432ms step_avg:34.79ms
step:732/2160 train_time:25492ms step_avg:34.83ms
step:733/2160 train_time:25974ms step_avg:35.44ms
step:734/2160 train_time:25999ms step_avg:35.42ms
step:735/2160 train_time:26058ms step_avg:35.45ms
step:736/2160 train_time:26117ms step_avg:35.48ms
step:737/2160 train_time:26177ms step_avg:35.52ms
step:738/2160 train_time:26236ms step_avg:35.55ms
step:739/2160 train_time:26297ms step_avg:35.58ms
step:740/2160 train_time:26356ms step_avg:35.62ms
step:741/2160 train_time:26416ms step_avg:35.65ms
step:742/2160 train_time:26475ms step_avg:35.68ms
step:743/2160 train_time:26535ms step_avg:35.71ms
step:744/2160 train_time:26594ms step_avg:35.75ms
step:745/2160 train_time:26655ms step_avg:35.78ms
step:746/2160 train_time:26714ms step_avg:35.81ms
step:747/2160 train_time:26774ms step_avg:35.84ms
step:748/2160 train_time:26834ms step_avg:35.87ms
step:749/2160 train_time:26902ms step_avg:35.92ms
step:750/2160 train_time:26969ms step_avg:35.96ms
step:750/2160 val_loss:3.8518 train_time:27033ms step_avg:36.04ms
step:751/2160 train_time:27056ms step_avg:36.03ms
step:752/2160 train_time:27093ms step_avg:36.03ms
step:753/2160 train_time:27160ms step_avg:36.07ms
step:754/2160 train_time:27222ms step_avg:36.10ms
step:755/2160 train_time:27284ms step_avg:36.14ms
step:756/2160 train_time:27343ms step_avg:36.17ms
step:757/2160 train_time:27403ms step_avg:36.20ms
step:758/2160 train_time:27462ms step_avg:36.23ms
step:759/2160 train_time:27523ms step_avg:36.26ms
step:760/2160 train_time:27582ms step_avg:36.29ms
step:761/2160 train_time:27642ms step_avg:36.32ms
step:762/2160 train_time:27701ms step_avg:36.35ms
step:763/2160 train_time:27762ms step_avg:36.38ms
step:764/2160 train_time:27820ms step_avg:36.41ms
step:765/2160 train_time:27881ms step_avg:36.45ms
step:766/2160 train_time:27944ms step_avg:36.48ms
step:767/2160 train_time:28010ms step_avg:36.52ms
step:768/2160 train_time:28072ms step_avg:36.55ms
step:769/2160 train_time:28135ms step_avg:36.59ms
step:770/2160 train_time:28196ms step_avg:36.62ms
step:771/2160 train_time:28258ms step_avg:36.65ms
step:772/2160 train_time:28318ms step_avg:36.68ms
step:773/2160 train_time:28379ms step_avg:36.71ms
step:774/2160 train_time:28438ms step_avg:36.74ms
step:775/2160 train_time:28499ms step_avg:36.77ms
step:776/2160 train_time:28559ms step_avg:36.80ms
step:777/2160 train_time:28620ms step_avg:36.83ms
step:778/2160 train_time:28678ms step_avg:36.86ms
step:779/2160 train_time:28740ms step_avg:36.89ms
step:780/2160 train_time:28799ms step_avg:36.92ms
step:781/2160 train_time:28860ms step_avg:36.95ms
step:782/2160 train_time:28921ms step_avg:36.98ms
step:783/2160 train_time:28985ms step_avg:37.02ms
step:784/2160 train_time:29046ms step_avg:37.05ms
step:785/2160 train_time:29109ms step_avg:37.08ms
step:786/2160 train_time:29170ms step_avg:37.11ms
step:787/2160 train_time:29232ms step_avg:37.14ms
step:788/2160 train_time:29292ms step_avg:37.17ms
step:789/2160 train_time:29354ms step_avg:37.20ms
step:790/2160 train_time:29414ms step_avg:37.23ms
step:791/2160 train_time:29475ms step_avg:37.26ms
step:792/2160 train_time:29535ms step_avg:37.29ms
step:793/2160 train_time:29597ms step_avg:37.32ms
step:794/2160 train_time:29656ms step_avg:37.35ms
step:795/2160 train_time:29717ms step_avg:37.38ms
step:796/2160 train_time:29776ms step_avg:37.41ms
step:797/2160 train_time:29838ms step_avg:37.44ms
step:798/2160 train_time:29898ms step_avg:37.47ms
step:799/2160 train_time:29961ms step_avg:37.50ms
step:800/2160 train_time:30021ms step_avg:37.53ms
step:801/2160 train_time:30083ms step_avg:37.56ms
step:802/2160 train_time:30143ms step_avg:37.59ms
step:803/2160 train_time:30206ms step_avg:37.62ms
step:804/2160 train_time:30266ms step_avg:37.64ms
step:805/2160 train_time:30328ms step_avg:37.67ms
step:806/2160 train_time:30387ms step_avg:37.70ms
step:807/2160 train_time:30449ms step_avg:37.73ms
step:808/2160 train_time:30509ms step_avg:37.76ms
step:809/2160 train_time:30570ms step_avg:37.79ms
step:810/2160 train_time:30630ms step_avg:37.81ms
step:811/2160 train_time:30692ms step_avg:37.84ms
step:812/2160 train_time:30751ms step_avg:37.87ms
step:813/2160 train_time:30813ms step_avg:37.90ms
step:814/2160 train_time:30874ms step_avg:37.93ms
step:815/2160 train_time:30936ms step_avg:37.96ms
step:816/2160 train_time:30997ms step_avg:37.99ms
step:817/2160 train_time:31058ms step_avg:38.01ms
step:818/2160 train_time:31119ms step_avg:38.04ms
step:819/2160 train_time:31181ms step_avg:38.07ms
step:820/2160 train_time:31241ms step_avg:38.10ms
step:821/2160 train_time:31303ms step_avg:38.13ms
step:822/2160 train_time:31363ms step_avg:38.15ms
step:823/2160 train_time:31424ms step_avg:38.18ms
step:824/2160 train_time:31484ms step_avg:38.21ms
step:825/2160 train_time:31546ms step_avg:38.24ms
step:826/2160 train_time:31605ms step_avg:38.26ms
step:827/2160 train_time:31667ms step_avg:38.29ms
step:828/2160 train_time:31727ms step_avg:38.32ms
step:829/2160 train_time:31789ms step_avg:38.35ms
step:830/2160 train_time:31849ms step_avg:38.37ms
step:831/2160 train_time:31912ms step_avg:38.40ms
step:832/2160 train_time:31972ms step_avg:38.43ms
step:833/2160 train_time:32035ms step_avg:38.46ms
step:834/2160 train_time:32096ms step_avg:38.48ms
step:835/2160 train_time:32159ms step_avg:38.51ms
step:836/2160 train_time:32219ms step_avg:38.54ms
step:837/2160 train_time:32281ms step_avg:38.57ms
step:838/2160 train_time:32341ms step_avg:38.59ms
step:839/2160 train_time:32402ms step_avg:38.62ms
step:840/2160 train_time:32462ms step_avg:38.65ms
step:841/2160 train_time:32523ms step_avg:38.67ms
step:842/2160 train_time:32583ms step_avg:38.70ms
step:843/2160 train_time:32644ms step_avg:38.72ms
step:844/2160 train_time:32704ms step_avg:38.75ms
step:845/2160 train_time:32766ms step_avg:38.78ms
step:846/2160 train_time:32825ms step_avg:38.80ms
step:847/2160 train_time:32888ms step_avg:38.83ms
step:848/2160 train_time:32948ms step_avg:38.85ms
step:849/2160 train_time:33010ms step_avg:38.88ms
step:850/2160 train_time:33071ms step_avg:38.91ms
step:851/2160 train_time:33133ms step_avg:38.93ms
step:852/2160 train_time:33193ms step_avg:38.96ms
step:853/2160 train_time:33255ms step_avg:38.99ms
step:854/2160 train_time:33315ms step_avg:39.01ms
step:855/2160 train_time:33377ms step_avg:39.04ms
step:856/2160 train_time:33437ms step_avg:39.06ms
step:857/2160 train_time:33499ms step_avg:39.09ms
step:858/2160 train_time:33559ms step_avg:39.11ms
step:859/2160 train_time:33621ms step_avg:39.14ms
step:860/2160 train_time:33684ms step_avg:39.17ms
step:861/2160 train_time:33742ms step_avg:39.19ms
step:862/2160 train_time:33802ms step_avg:39.21ms
step:863/2160 train_time:33864ms step_avg:39.24ms
step:864/2160 train_time:33924ms step_avg:39.26ms
step:865/2160 train_time:33986ms step_avg:39.29ms
step:866/2160 train_time:34046ms step_avg:39.31ms
step:867/2160 train_time:34109ms step_avg:39.34ms
step:868/2160 train_time:34168ms step_avg:39.36ms
step:869/2160 train_time:34230ms step_avg:39.39ms
step:870/2160 train_time:34291ms step_avg:39.41ms
step:871/2160 train_time:34353ms step_avg:39.44ms
step:872/2160 train_time:34413ms step_avg:39.46ms
step:873/2160 train_time:34475ms step_avg:39.49ms
step:874/2160 train_time:34535ms step_avg:39.51ms
step:875/2160 train_time:34597ms step_avg:39.54ms
step:876/2160 train_time:34659ms step_avg:39.56ms
step:877/2160 train_time:34721ms step_avg:39.59ms
step:878/2160 train_time:34780ms step_avg:39.61ms
step:879/2160 train_time:34842ms step_avg:39.64ms
step:880/2160 train_time:34901ms step_avg:39.66ms
step:881/2160 train_time:34964ms step_avg:39.69ms
step:882/2160 train_time:35024ms step_avg:39.71ms
step:883/2160 train_time:35086ms step_avg:39.74ms
step:884/2160 train_time:35146ms step_avg:39.76ms
step:885/2160 train_time:35208ms step_avg:39.78ms
step:886/2160 train_time:35268ms step_avg:39.81ms
step:887/2160 train_time:35331ms step_avg:39.83ms
step:888/2160 train_time:35391ms step_avg:39.85ms
step:889/2160 train_time:35453ms step_avg:39.88ms
step:890/2160 train_time:35513ms step_avg:39.90ms
step:891/2160 train_time:35575ms step_avg:39.93ms
step:892/2160 train_time:35635ms step_avg:39.95ms
step:893/2160 train_time:35698ms step_avg:39.97ms
step:894/2160 train_time:35758ms step_avg:40.00ms
step:895/2160 train_time:35820ms step_avg:40.02ms
step:896/2160 train_time:35879ms step_avg:40.04ms
step:897/2160 train_time:35941ms step_avg:40.07ms
step:898/2160 train_time:36001ms step_avg:40.09ms
step:899/2160 train_time:36063ms step_avg:40.12ms
step:900/2160 train_time:36124ms step_avg:40.14ms
step:901/2160 train_time:36185ms step_avg:40.16ms
step:902/2160 train_time:36245ms step_avg:40.18ms
step:903/2160 train_time:36307ms step_avg:40.21ms
step:904/2160 train_time:36367ms step_avg:40.23ms
step:905/2160 train_time:36429ms step_avg:40.25ms
step:906/2160 train_time:36489ms step_avg:40.27ms
step:907/2160 train_time:36551ms step_avg:40.30ms
step:908/2160 train_time:36612ms step_avg:40.32ms
step:909/2160 train_time:36674ms step_avg:40.34ms
step:910/2160 train_time:36734ms step_avg:40.37ms
step:911/2160 train_time:36796ms step_avg:40.39ms
step:912/2160 train_time:36857ms step_avg:40.41ms
step:913/2160 train_time:36919ms step_avg:40.44ms
step:914/2160 train_time:36979ms step_avg:40.46ms
step:915/2160 train_time:37041ms step_avg:40.48ms
step:916/2160 train_time:37101ms step_avg:40.50ms
step:917/2160 train_time:37163ms step_avg:40.53ms
step:918/2160 train_time:37222ms step_avg:40.55ms
step:919/2160 train_time:37284ms step_avg:40.57ms
step:920/2160 train_time:37344ms step_avg:40.59ms
step:921/2160 train_time:37406ms step_avg:40.61ms
step:922/2160 train_time:37466ms step_avg:40.64ms
step:923/2160 train_time:37529ms step_avg:40.66ms
step:924/2160 train_time:37589ms step_avg:40.68ms
step:925/2160 train_time:37651ms step_avg:40.70ms
step:926/2160 train_time:37712ms step_avg:40.73ms
step:927/2160 train_time:37774ms step_avg:40.75ms
step:928/2160 train_time:37835ms step_avg:40.77ms
step:929/2160 train_time:37898ms step_avg:40.79ms
step:930/2160 train_time:37958ms step_avg:40.82ms
step:931/2160 train_time:38021ms step_avg:40.84ms
step:932/2160 train_time:38081ms step_avg:40.86ms
step:933/2160 train_time:38142ms step_avg:40.88ms
step:934/2160 train_time:38202ms step_avg:40.90ms
step:935/2160 train_time:38263ms step_avg:40.92ms
step:936/2160 train_time:38323ms step_avg:40.94ms
step:937/2160 train_time:38384ms step_avg:40.97ms
step:938/2160 train_time:38444ms step_avg:40.99ms
step:939/2160 train_time:38506ms step_avg:41.01ms
step:940/2160 train_time:38566ms step_avg:41.03ms
step:941/2160 train_time:38628ms step_avg:41.05ms
step:942/2160 train_time:38689ms step_avg:41.07ms
step:943/2160 train_time:38751ms step_avg:41.09ms
step:944/2160 train_time:38812ms step_avg:41.11ms
step:945/2160 train_time:38875ms step_avg:41.14ms
step:946/2160 train_time:38935ms step_avg:41.16ms
step:947/2160 train_time:38998ms step_avg:41.18ms
step:948/2160 train_time:39059ms step_avg:41.20ms
step:949/2160 train_time:39121ms step_avg:41.22ms
step:950/2160 train_time:39181ms step_avg:41.24ms
step:951/2160 train_time:39242ms step_avg:41.26ms
step:952/2160 train_time:39302ms step_avg:41.28ms
step:953/2160 train_time:39364ms step_avg:41.31ms
step:954/2160 train_time:39423ms step_avg:41.32ms
step:955/2160 train_time:39485ms step_avg:41.35ms
step:956/2160 train_time:39545ms step_avg:41.36ms
step:957/2160 train_time:39607ms step_avg:41.39ms
step:958/2160 train_time:39667ms step_avg:41.41ms
step:959/2160 train_time:39730ms step_avg:41.43ms
step:960/2160 train_time:39790ms step_avg:41.45ms
step:961/2160 train_time:39853ms step_avg:41.47ms
step:962/2160 train_time:39913ms step_avg:41.49ms
step:963/2160 train_time:39975ms step_avg:41.51ms
step:964/2160 train_time:40035ms step_avg:41.53ms
step:965/2160 train_time:40098ms step_avg:41.55ms
step:966/2160 train_time:40158ms step_avg:41.57ms
step:967/2160 train_time:40221ms step_avg:41.59ms
step:968/2160 train_time:40280ms step_avg:41.61ms
step:969/2160 train_time:40342ms step_avg:41.63ms
step:970/2160 train_time:40402ms step_avg:41.65ms
step:971/2160 train_time:40464ms step_avg:41.67ms
step:972/2160 train_time:40523ms step_avg:41.69ms
step:973/2160 train_time:40585ms step_avg:41.71ms
step:974/2160 train_time:40645ms step_avg:41.73ms
step:975/2160 train_time:40708ms step_avg:41.75ms
step:976/2160 train_time:40768ms step_avg:41.77ms
step:977/2160 train_time:40830ms step_avg:41.79ms
step:978/2160 train_time:40891ms step_avg:41.81ms
step:979/2160 train_time:40953ms step_avg:41.83ms
step:980/2160 train_time:41014ms step_avg:41.85ms
step:981/2160 train_time:41077ms step_avg:41.87ms
step:982/2160 train_time:41137ms step_avg:41.89ms
step:983/2160 train_time:41199ms step_avg:41.91ms
step:984/2160 train_time:41259ms step_avg:41.93ms
step:985/2160 train_time:41321ms step_avg:41.95ms
step:986/2160 train_time:41381ms step_avg:41.97ms
step:987/2160 train_time:41443ms step_avg:41.99ms
step:988/2160 train_time:41502ms step_avg:42.01ms
step:989/2160 train_time:41564ms step_avg:42.03ms
step:990/2160 train_time:41623ms step_avg:42.04ms
step:991/2160 train_time:41685ms step_avg:42.06ms
step:992/2160 train_time:41746ms step_avg:42.08ms
step:993/2160 train_time:41808ms step_avg:42.10ms
step:994/2160 train_time:41868ms step_avg:42.12ms
step:995/2160 train_time:41930ms step_avg:42.14ms
step:996/2160 train_time:41990ms step_avg:42.16ms
step:997/2160 train_time:42053ms step_avg:42.18ms
step:998/2160 train_time:42113ms step_avg:42.20ms
step:999/2160 train_time:42176ms step_avg:42.22ms
step:1000/2160 train_time:42236ms step_avg:42.24ms
step:1000/2160 val_loss:3.6866 train_time:42299ms step_avg:42.30ms
step:1001/2160 train_time:42321ms step_avg:42.28ms
step:1002/2160 train_time:42360ms step_avg:42.28ms
step:1003/2160 train_time:42425ms step_avg:42.30ms
step:1004/2160 train_time:42489ms step_avg:42.32ms
step:1005/2160 train_time:42551ms step_avg:42.34ms
step:1006/2160 train_time:42612ms step_avg:42.36ms
step:1007/2160 train_time:42673ms step_avg:42.38ms
step:1008/2160 train_time:42732ms step_avg:42.39ms
step:1009/2160 train_time:42793ms step_avg:42.41ms
step:1010/2160 train_time:42852ms step_avg:42.43ms
step:1011/2160 train_time:42914ms step_avg:42.45ms
step:1012/2160 train_time:42974ms step_avg:42.46ms
step:1013/2160 train_time:43036ms step_avg:42.48ms
step:1014/2160 train_time:43096ms step_avg:42.50ms
step:1015/2160 train_time:43157ms step_avg:42.52ms
step:1016/2160 train_time:43216ms step_avg:42.54ms
step:1017/2160 train_time:43278ms step_avg:42.55ms
step:1018/2160 train_time:43339ms step_avg:42.57ms
step:1019/2160 train_time:43402ms step_avg:42.59ms
step:1020/2160 train_time:43463ms step_avg:42.61ms
step:1021/2160 train_time:43526ms step_avg:42.63ms
step:1022/2160 train_time:43586ms step_avg:42.65ms
step:1023/2160 train_time:43648ms step_avg:42.67ms
step:1024/2160 train_time:43708ms step_avg:42.68ms
step:1025/2160 train_time:43769ms step_avg:42.70ms
step:1026/2160 train_time:43829ms step_avg:42.72ms
step:1027/2160 train_time:43890ms step_avg:42.74ms
step:1028/2160 train_time:43950ms step_avg:42.75ms
step:1029/2160 train_time:44012ms step_avg:42.77ms
step:1030/2160 train_time:44071ms step_avg:42.79ms
step:1031/2160 train_time:44133ms step_avg:42.81ms
step:1032/2160 train_time:44194ms step_avg:42.82ms
step:1033/2160 train_time:44256ms step_avg:42.84ms
step:1034/2160 train_time:44316ms step_avg:42.86ms
step:1035/2160 train_time:44379ms step_avg:42.88ms
step:1036/2160 train_time:44439ms step_avg:42.89ms
step:1037/2160 train_time:44502ms step_avg:42.91ms
step:1038/2160 train_time:44562ms step_avg:42.93ms
step:1039/2160 train_time:44625ms step_avg:42.95ms
step:1040/2160 train_time:44685ms step_avg:42.97ms
step:1041/2160 train_time:44747ms step_avg:42.98ms
step:1042/2160 train_time:44807ms step_avg:43.00ms
step:1043/2160 train_time:44868ms step_avg:43.02ms
step:1044/2160 train_time:44928ms step_avg:43.03ms
step:1045/2160 train_time:44990ms step_avg:43.05ms
step:1046/2160 train_time:45049ms step_avg:43.07ms
step:1047/2160 train_time:45111ms step_avg:43.09ms
step:1048/2160 train_time:45172ms step_avg:43.10ms
step:1049/2160 train_time:45235ms step_avg:43.12ms
step:1050/2160 train_time:45296ms step_avg:43.14ms
step:1051/2160 train_time:45359ms step_avg:43.16ms
step:1052/2160 train_time:45419ms step_avg:43.17ms
step:1053/2160 train_time:45481ms step_avg:43.19ms
step:1054/2160 train_time:45541ms step_avg:43.21ms
step:1055/2160 train_time:45602ms step_avg:43.23ms
step:1056/2160 train_time:45662ms step_avg:43.24ms
step:1057/2160 train_time:45723ms step_avg:43.26ms
step:1058/2160 train_time:45783ms step_avg:43.27ms
step:1059/2160 train_time:45845ms step_avg:43.29ms
step:1060/2160 train_time:45905ms step_avg:43.31ms
step:1061/2160 train_time:45967ms step_avg:43.32ms
step:1062/2160 train_time:46033ms step_avg:43.35ms
step:1063/2160 train_time:46090ms step_avg:43.36ms
step:1064/2160 train_time:46150ms step_avg:43.37ms
step:1065/2160 train_time:46212ms step_avg:43.39ms
step:1066/2160 train_time:46273ms step_avg:43.41ms
step:1067/2160 train_time:46337ms step_avg:43.43ms
step:1068/2160 train_time:46398ms step_avg:43.44ms
step:1069/2160 train_time:46460ms step_avg:43.46ms
step:1070/2160 train_time:46520ms step_avg:43.48ms
step:1071/2160 train_time:46582ms step_avg:43.49ms
step:1072/2160 train_time:46642ms step_avg:43.51ms
step:1073/2160 train_time:46703ms step_avg:43.53ms
step:1074/2160 train_time:46763ms step_avg:43.54ms
step:1075/2160 train_time:46824ms step_avg:43.56ms
step:1076/2160 train_time:46884ms step_avg:43.57ms
step:1077/2160 train_time:46946ms step_avg:43.59ms
step:1078/2160 train_time:47006ms step_avg:43.60ms
step:1079/2160 train_time:47068ms step_avg:43.62ms
step:1080/2160 train_time:47129ms step_avg:43.64ms
step:1081/2160 train_time:47191ms step_avg:43.65ms
step:1082/2160 train_time:47252ms step_avg:43.67ms
step:1083/2160 train_time:47314ms step_avg:43.69ms
step:1084/2160 train_time:47375ms step_avg:43.70ms
step:1085/2160 train_time:47439ms step_avg:43.72ms
step:1086/2160 train_time:47498ms step_avg:43.74ms
step:1087/2160 train_time:47560ms step_avg:43.75ms
step:1088/2160 train_time:47619ms step_avg:43.77ms
step:1089/2160 train_time:47681ms step_avg:43.78ms
step:1090/2160 train_time:47741ms step_avg:43.80ms
step:1091/2160 train_time:47803ms step_avg:43.82ms
step:1092/2160 train_time:47863ms step_avg:43.83ms
step:1093/2160 train_time:47925ms step_avg:43.85ms
step:1094/2160 train_time:47985ms step_avg:43.86ms
step:1095/2160 train_time:48047ms step_avg:43.88ms
step:1096/2160 train_time:48107ms step_avg:43.89ms
step:1097/2160 train_time:48170ms step_avg:43.91ms
step:1098/2160 train_time:48232ms step_avg:43.93ms
step:1099/2160 train_time:48295ms step_avg:43.94ms
step:1100/2160 train_time:48356ms step_avg:43.96ms
step:1101/2160 train_time:48417ms step_avg:43.98ms
step:1102/2160 train_time:48478ms step_avg:43.99ms
step:1103/2160 train_time:48540ms step_avg:44.01ms
step:1104/2160 train_time:48599ms step_avg:44.02ms
step:1105/2160 train_time:48661ms step_avg:44.04ms
step:1106/2160 train_time:48721ms step_avg:44.05ms
step:1107/2160 train_time:48782ms step_avg:44.07ms
step:1108/2160 train_time:48842ms step_avg:44.08ms
step:1109/2160 train_time:48904ms step_avg:44.10ms
step:1110/2160 train_time:48963ms step_avg:44.11ms
step:1111/2160 train_time:49026ms step_avg:44.13ms
step:1112/2160 train_time:49086ms step_avg:44.14ms
step:1113/2160 train_time:49148ms step_avg:44.16ms
step:1114/2160 train_time:49209ms step_avg:44.17ms
step:1115/2160 train_time:49272ms step_avg:44.19ms
step:1116/2160 train_time:49333ms step_avg:44.21ms
step:1117/2160 train_time:49395ms step_avg:44.22ms
step:1118/2160 train_time:49455ms step_avg:44.24ms
step:1119/2160 train_time:49518ms step_avg:44.25ms
step:1120/2160 train_time:49578ms step_avg:44.27ms
step:1121/2160 train_time:49639ms step_avg:44.28ms
step:1122/2160 train_time:49699ms step_avg:44.30ms
step:1123/2160 train_time:49761ms step_avg:44.31ms
step:1124/2160 train_time:49820ms step_avg:44.32ms
step:1125/2160 train_time:49882ms step_avg:44.34ms
step:1126/2160 train_time:49941ms step_avg:44.35ms
step:1127/2160 train_time:50003ms step_avg:44.37ms
step:1128/2160 train_time:50063ms step_avg:44.38ms
step:1129/2160 train_time:50125ms step_avg:44.40ms
step:1130/2160 train_time:50185ms step_avg:44.41ms
step:1131/2160 train_time:50248ms step_avg:44.43ms
step:1132/2160 train_time:50308ms step_avg:44.44ms
step:1133/2160 train_time:50371ms step_avg:44.46ms
step:1134/2160 train_time:50431ms step_avg:44.47ms
step:1135/2160 train_time:50494ms step_avg:44.49ms
step:1136/2160 train_time:50554ms step_avg:44.50ms
step:1137/2160 train_time:50616ms step_avg:44.52ms
step:1138/2160 train_time:50676ms step_avg:44.53ms
step:1139/2160 train_time:50738ms step_avg:44.55ms
step:1140/2160 train_time:50799ms step_avg:44.56ms
step:1141/2160 train_time:50860ms step_avg:44.57ms
step:1142/2160 train_time:50920ms step_avg:44.59ms
step:1143/2160 train_time:50982ms step_avg:44.60ms
step:1144/2160 train_time:51042ms step_avg:44.62ms
step:1145/2160 train_time:51105ms step_avg:44.63ms
step:1146/2160 train_time:51165ms step_avg:44.65ms
step:1147/2160 train_time:51227ms step_avg:44.66ms
step:1148/2160 train_time:51287ms step_avg:44.68ms
step:1149/2160 train_time:51349ms step_avg:44.69ms
step:1150/2160 train_time:51409ms step_avg:44.70ms
step:1151/2160 train_time:51472ms step_avg:44.72ms
step:1152/2160 train_time:51533ms step_avg:44.73ms
step:1153/2160 train_time:51596ms step_avg:44.75ms
step:1154/2160 train_time:51656ms step_avg:44.76ms
step:1155/2160 train_time:51718ms step_avg:44.78ms
step:1156/2160 train_time:51778ms step_avg:44.79ms
step:1157/2160 train_time:51839ms step_avg:44.81ms
step:1158/2160 train_time:51899ms step_avg:44.82ms
step:1159/2160 train_time:51960ms step_avg:44.83ms
step:1160/2160 train_time:52020ms step_avg:44.84ms
step:1161/2160 train_time:52082ms step_avg:44.86ms
step:1162/2160 train_time:52143ms step_avg:44.87ms
step:1163/2160 train_time:52206ms step_avg:44.89ms
step:1164/2160 train_time:52265ms step_avg:44.90ms
step:1165/2160 train_time:52327ms step_avg:44.92ms
step:1166/2160 train_time:52388ms step_avg:44.93ms
step:1167/2160 train_time:52450ms step_avg:44.94ms
step:1168/2160 train_time:52510ms step_avg:44.96ms
step:1169/2160 train_time:52572ms step_avg:44.97ms
step:1170/2160 train_time:52633ms step_avg:44.99ms
step:1171/2160 train_time:52696ms step_avg:45.00ms
step:1172/2160 train_time:52756ms step_avg:45.01ms
step:1173/2160 train_time:52818ms step_avg:45.03ms
step:1174/2160 train_time:52878ms step_avg:45.04ms
step:1175/2160 train_time:52940ms step_avg:45.06ms
step:1176/2160 train_time:53000ms step_avg:45.07ms
step:1177/2160 train_time:53061ms step_avg:45.08ms
step:1178/2160 train_time:53121ms step_avg:45.09ms
step:1179/2160 train_time:53183ms step_avg:45.11ms
step:1180/2160 train_time:53244ms step_avg:45.12ms
step:1181/2160 train_time:53306ms step_avg:45.14ms
step:1182/2160 train_time:53366ms step_avg:45.15ms
step:1183/2160 train_time:53428ms step_avg:45.16ms
step:1184/2160 train_time:53489ms step_avg:45.18ms
step:1185/2160 train_time:53551ms step_avg:45.19ms
step:1186/2160 train_time:53612ms step_avg:45.20ms
step:1187/2160 train_time:53675ms step_avg:45.22ms
step:1188/2160 train_time:53735ms step_avg:45.23ms
step:1189/2160 train_time:53798ms step_avg:45.25ms
step:1190/2160 train_time:53859ms step_avg:45.26ms
step:1191/2160 train_time:53920ms step_avg:45.27ms
step:1192/2160 train_time:53980ms step_avg:45.29ms
step:1193/2160 train_time:54042ms step_avg:45.30ms
step:1194/2160 train_time:54101ms step_avg:45.31ms
step:1195/2160 train_time:54163ms step_avg:45.32ms
step:1196/2160 train_time:54223ms step_avg:45.34ms
step:1197/2160 train_time:54285ms step_avg:45.35ms
step:1198/2160 train_time:54345ms step_avg:45.36ms
step:1199/2160 train_time:54407ms step_avg:45.38ms
step:1200/2160 train_time:54467ms step_avg:45.39ms
step:1201/2160 train_time:54529ms step_avg:45.40ms
step:1202/2160 train_time:54589ms step_avg:45.42ms
step:1203/2160 train_time:54653ms step_avg:45.43ms
step:1204/2160 train_time:54713ms step_avg:45.44ms
step:1205/2160 train_time:54776ms step_avg:45.46ms
step:1206/2160 train_time:54836ms step_avg:45.47ms
step:1207/2160 train_time:54899ms step_avg:45.48ms
step:1208/2160 train_time:54958ms step_avg:45.50ms
step:1209/2160 train_time:55020ms step_avg:45.51ms
step:1210/2160 train_time:55079ms step_avg:45.52ms
step:1211/2160 train_time:55141ms step_avg:45.53ms
step:1212/2160 train_time:55200ms step_avg:45.54ms
step:1213/2160 train_time:55262ms step_avg:45.56ms
step:1214/2160 train_time:55322ms step_avg:45.57ms
step:1215/2160 train_time:55384ms step_avg:45.58ms
step:1216/2160 train_time:55444ms step_avg:45.60ms
step:1217/2160 train_time:55507ms step_avg:45.61ms
step:1218/2160 train_time:55568ms step_avg:45.62ms
step:1219/2160 train_time:55630ms step_avg:45.64ms
step:1220/2160 train_time:55691ms step_avg:45.65ms
step:1221/2160 train_time:55754ms step_avg:45.66ms
step:1222/2160 train_time:55815ms step_avg:45.68ms
step:1223/2160 train_time:55877ms step_avg:45.69ms
step:1224/2160 train_time:55937ms step_avg:45.70ms
step:1225/2160 train_time:56000ms step_avg:45.71ms
step:1226/2160 train_time:56059ms step_avg:45.73ms
step:1227/2160 train_time:56120ms step_avg:45.74ms
step:1228/2160 train_time:56180ms step_avg:45.75ms
step:1229/2160 train_time:56242ms step_avg:45.76ms
step:1230/2160 train_time:56301ms step_avg:45.77ms
step:1231/2160 train_time:56363ms step_avg:45.79ms
step:1232/2160 train_time:56423ms step_avg:45.80ms
step:1233/2160 train_time:56486ms step_avg:45.81ms
step:1234/2160 train_time:56546ms step_avg:45.82ms
step:1235/2160 train_time:56608ms step_avg:45.84ms
step:1236/2160 train_time:56669ms step_avg:45.85ms
step:1237/2160 train_time:56732ms step_avg:45.86ms
step:1238/2160 train_time:56793ms step_avg:45.87ms
step:1239/2160 train_time:56856ms step_avg:45.89ms
step:1240/2160 train_time:56916ms step_avg:45.90ms
step:1241/2160 train_time:56978ms step_avg:45.91ms
step:1242/2160 train_time:57038ms step_avg:45.92ms
step:1243/2160 train_time:57100ms step_avg:45.94ms
step:1244/2160 train_time:57160ms step_avg:45.95ms
step:1245/2160 train_time:57221ms step_avg:45.96ms
step:1246/2160 train_time:57281ms step_avg:45.97ms
step:1247/2160 train_time:57342ms step_avg:45.98ms
step:1248/2160 train_time:57402ms step_avg:46.00ms
step:1249/2160 train_time:57464ms step_avg:46.01ms
step:1250/2160 train_time:57524ms step_avg:46.02ms
step:1250/2160 val_loss:3.5697 train_time:57588ms step_avg:46.07ms
step:1251/2160 train_time:57611ms step_avg:46.05ms
step:1252/2160 train_time:57654ms step_avg:46.05ms
step:1253/2160 train_time:57716ms step_avg:46.06ms
step:1254/2160 train_time:57778ms step_avg:46.08ms
step:1255/2160 train_time:57840ms step_avg:46.09ms
step:1256/2160 train_time:57900ms step_avg:46.10ms
step:1257/2160 train_time:57961ms step_avg:46.11ms
step:1258/2160 train_time:58021ms step_avg:46.12ms
step:1259/2160 train_time:58083ms step_avg:46.13ms
step:1260/2160 train_time:58143ms step_avg:46.15ms
step:1261/2160 train_time:58206ms step_avg:46.16ms
step:1262/2160 train_time:58266ms step_avg:46.17ms
step:1263/2160 train_time:58328ms step_avg:46.18ms
step:1264/2160 train_time:58388ms step_avg:46.19ms
step:1265/2160 train_time:58449ms step_avg:46.21ms
step:1266/2160 train_time:58510ms step_avg:46.22ms
step:1267/2160 train_time:58574ms step_avg:46.23ms
step:1268/2160 train_time:58634ms step_avg:46.24ms
step:1269/2160 train_time:58697ms step_avg:46.25ms
step:1270/2160 train_time:58758ms step_avg:46.27ms
step:1271/2160 train_time:58820ms step_avg:46.28ms
step:1272/2160 train_time:58880ms step_avg:46.29ms
step:1273/2160 train_time:58942ms step_avg:46.30ms
step:1274/2160 train_time:59002ms step_avg:46.31ms
step:1275/2160 train_time:59064ms step_avg:46.32ms
step:1276/2160 train_time:59124ms step_avg:46.34ms
step:1277/2160 train_time:59186ms step_avg:46.35ms
step:1278/2160 train_time:59245ms step_avg:46.36ms
step:1279/2160 train_time:59307ms step_avg:46.37ms
step:1280/2160 train_time:59366ms step_avg:46.38ms
step:1281/2160 train_time:59428ms step_avg:46.39ms
step:1282/2160 train_time:59488ms step_avg:46.40ms
step:1283/2160 train_time:59551ms step_avg:46.42ms
step:1284/2160 train_time:59611ms step_avg:46.43ms
step:1285/2160 train_time:59673ms step_avg:46.44ms
step:1286/2160 train_time:59734ms step_avg:46.45ms
step:1287/2160 train_time:59795ms step_avg:46.46ms
step:1288/2160 train_time:59856ms step_avg:46.47ms
step:1289/2160 train_time:59917ms step_avg:46.48ms
step:1290/2160 train_time:59978ms step_avg:46.49ms
step:1291/2160 train_time:60039ms step_avg:46.51ms
step:1292/2160 train_time:60099ms step_avg:46.52ms
step:1293/2160 train_time:60161ms step_avg:46.53ms
step:1294/2160 train_time:60222ms step_avg:46.54ms
step:1295/2160 train_time:60284ms step_avg:46.55ms
step:1296/2160 train_time:60344ms step_avg:46.56ms
step:1297/2160 train_time:60406ms step_avg:46.57ms
step:1298/2160 train_time:60467ms step_avg:46.58ms
step:1299/2160 train_time:60530ms step_avg:46.60ms
step:1300/2160 train_time:60590ms step_avg:46.61ms
step:1301/2160 train_time:60653ms step_avg:46.62ms
step:1302/2160 train_time:60712ms step_avg:46.63ms
step:1303/2160 train_time:60773ms step_avg:46.64ms
step:1304/2160 train_time:60834ms step_avg:46.65ms
step:1305/2160 train_time:60896ms step_avg:46.66ms
step:1306/2160 train_time:60956ms step_avg:46.67ms
step:1307/2160 train_time:61018ms step_avg:46.69ms
step:1308/2160 train_time:61078ms step_avg:46.70ms
step:1309/2160 train_time:61140ms step_avg:46.71ms
step:1310/2160 train_time:61200ms step_avg:46.72ms
step:1311/2160 train_time:61262ms step_avg:46.73ms
step:1312/2160 train_time:61322ms step_avg:46.74ms
step:1313/2160 train_time:61385ms step_avg:46.75ms
step:1314/2160 train_time:61445ms step_avg:46.76ms
step:1315/2160 train_time:61508ms step_avg:46.77ms
step:1316/2160 train_time:61569ms step_avg:46.78ms
step:1317/2160 train_time:61631ms step_avg:46.80ms
step:1318/2160 train_time:61691ms step_avg:46.81ms
step:1319/2160 train_time:61752ms step_avg:46.82ms
step:1320/2160 train_time:61812ms step_avg:46.83ms
step:1321/2160 train_time:61873ms step_avg:46.84ms
step:1322/2160 train_time:61933ms step_avg:46.85ms
step:1323/2160 train_time:61995ms step_avg:46.86ms
step:1324/2160 train_time:62055ms step_avg:46.87ms
step:1325/2160 train_time:62117ms step_avg:46.88ms
step:1326/2160 train_time:62176ms step_avg:46.89ms
step:1327/2160 train_time:62238ms step_avg:46.90ms
step:1328/2160 train_time:62299ms step_avg:46.91ms
step:1329/2160 train_time:62361ms step_avg:46.92ms
step:1330/2160 train_time:62422ms step_avg:46.93ms
step:1331/2160 train_time:62485ms step_avg:46.95ms
step:1332/2160 train_time:62546ms step_avg:46.96ms
step:1333/2160 train_time:62609ms step_avg:46.97ms
step:1334/2160 train_time:62669ms step_avg:46.98ms
step:1335/2160 train_time:62731ms step_avg:46.99ms
step:1336/2160 train_time:62791ms step_avg:47.00ms
step:1337/2160 train_time:62852ms step_avg:47.01ms
step:1338/2160 train_time:62911ms step_avg:47.02ms
step:1339/2160 train_time:62973ms step_avg:47.03ms
step:1340/2160 train_time:63033ms step_avg:47.04ms
step:1341/2160 train_time:63095ms step_avg:47.05ms
step:1342/2160 train_time:63155ms step_avg:47.06ms
step:1343/2160 train_time:63217ms step_avg:47.07ms
step:1344/2160 train_time:63277ms step_avg:47.08ms
step:1345/2160 train_time:63338ms step_avg:47.09ms
step:1346/2160 train_time:63399ms step_avg:47.10ms
step:1347/2160 train_time:63462ms step_avg:47.11ms
step:1348/2160 train_time:63523ms step_avg:47.12ms
step:1349/2160 train_time:63586ms step_avg:47.14ms
step:1350/2160 train_time:63647ms step_avg:47.15ms
step:1351/2160 train_time:63710ms step_avg:47.16ms
step:1352/2160 train_time:63769ms step_avg:47.17ms
step:1353/2160 train_time:63831ms step_avg:47.18ms
step:1354/2160 train_time:63891ms step_avg:47.19ms
step:1355/2160 train_time:63952ms step_avg:47.20ms
step:1356/2160 train_time:64011ms step_avg:47.21ms
step:1357/2160 train_time:64073ms step_avg:47.22ms
step:1358/2160 train_time:64134ms step_avg:47.23ms
step:1359/2160 train_time:64196ms step_avg:47.24ms
step:1360/2160 train_time:64256ms step_avg:47.25ms
step:1361/2160 train_time:64318ms step_avg:47.26ms
step:1362/2160 train_time:64379ms step_avg:47.27ms
step:1363/2160 train_time:64441ms step_avg:47.28ms
step:1364/2160 train_time:64502ms step_avg:47.29ms
step:1365/2160 train_time:64565ms step_avg:47.30ms
step:1366/2160 train_time:64626ms step_avg:47.31ms
step:1367/2160 train_time:64689ms step_avg:47.32ms
step:1368/2160 train_time:64749ms step_avg:47.33ms
step:1369/2160 train_time:64810ms step_avg:47.34ms
step:1370/2160 train_time:64870ms step_avg:47.35ms
step:1371/2160 train_time:64932ms step_avg:47.36ms
step:1372/2160 train_time:64991ms step_avg:47.37ms
step:1373/2160 train_time:65052ms step_avg:47.38ms
step:1374/2160 train_time:65112ms step_avg:47.39ms
step:1375/2160 train_time:65173ms step_avg:47.40ms
step:1376/2160 train_time:65235ms step_avg:47.41ms
step:1377/2160 train_time:65297ms step_avg:47.42ms
step:1378/2160 train_time:65357ms step_avg:47.43ms
step:1379/2160 train_time:65419ms step_avg:47.44ms
step:1380/2160 train_time:65480ms step_avg:47.45ms
step:1381/2160 train_time:65543ms step_avg:47.46ms
step:1382/2160 train_time:65604ms step_avg:47.47ms
step:1383/2160 train_time:65666ms step_avg:47.48ms
step:1384/2160 train_time:65726ms step_avg:47.49ms
step:1385/2160 train_time:65788ms step_avg:47.50ms
step:1386/2160 train_time:65848ms step_avg:47.51ms
step:1387/2160 train_time:65910ms step_avg:47.52ms
step:1388/2160 train_time:65970ms step_avg:47.53ms
step:1389/2160 train_time:66031ms step_avg:47.54ms
step:1390/2160 train_time:66091ms step_avg:47.55ms
step:1391/2160 train_time:66152ms step_avg:47.56ms
step:1392/2160 train_time:66212ms step_avg:47.57ms
step:1393/2160 train_time:66276ms step_avg:47.58ms
step:1394/2160 train_time:66335ms step_avg:47.59ms
step:1395/2160 train_time:66397ms step_avg:47.60ms
step:1396/2160 train_time:66457ms step_avg:47.61ms
step:1397/2160 train_time:66520ms step_avg:47.62ms
step:1398/2160 train_time:66581ms step_avg:47.63ms
step:1399/2160 train_time:66644ms step_avg:47.64ms
step:1400/2160 train_time:66704ms step_avg:47.65ms
step:1401/2160 train_time:66768ms step_avg:47.66ms
step:1402/2160 train_time:66828ms step_avg:47.67ms
step:1403/2160 train_time:66890ms step_avg:47.68ms
step:1404/2160 train_time:66950ms step_avg:47.69ms
step:1405/2160 train_time:67011ms step_avg:47.69ms
step:1406/2160 train_time:67071ms step_avg:47.70ms
step:1407/2160 train_time:67132ms step_avg:47.71ms
step:1408/2160 train_time:67192ms step_avg:47.72ms
step:1409/2160 train_time:67254ms step_avg:47.73ms
step:1410/2160 train_time:67314ms step_avg:47.74ms
step:1411/2160 train_time:67377ms step_avg:47.75ms
step:1412/2160 train_time:67437ms step_avg:47.76ms
step:1413/2160 train_time:67500ms step_avg:47.77ms
step:1414/2160 train_time:67560ms step_avg:47.78ms
step:1415/2160 train_time:67622ms step_avg:47.79ms
step:1416/2160 train_time:67712ms step_avg:47.82ms
step:1417/2160 train_time:67802ms step_avg:47.85ms
step:1418/2160 train_time:67891ms step_avg:47.88ms
step:1419/2160 train_time:67982ms step_avg:47.91ms
step:1420/2160 train_time:68070ms step_avg:47.94ms
step:1421/2160 train_time:68160ms step_avg:47.97ms
step:1422/2160 train_time:68248ms step_avg:47.99ms
step:1423/2160 train_time:68338ms step_avg:48.02ms
step:1424/2160 train_time:68426ms step_avg:48.05ms
step:1425/2160 train_time:68516ms step_avg:48.08ms
step:1426/2160 train_time:68604ms step_avg:48.11ms
step:1427/2160 train_time:68694ms step_avg:48.14ms
step:1428/2160 train_time:68782ms step_avg:48.17ms
step:1429/2160 train_time:68875ms step_avg:48.20ms
step:1430/2160 train_time:68961ms step_avg:48.22ms
step:1431/2160 train_time:69051ms step_avg:48.25ms
step:1432/2160 train_time:69139ms step_avg:48.28ms
step:1433/2160 train_time:69230ms step_avg:48.31ms
step:1434/2160 train_time:69317ms step_avg:48.34ms
step:1435/2160 train_time:69406ms step_avg:48.37ms
step:1436/2160 train_time:69494ms step_avg:48.39ms
step:1437/2160 train_time:69584ms step_avg:48.42ms
step:1438/2160 train_time:69672ms step_avg:48.45ms
step:1439/2160 train_time:69763ms step_avg:48.48ms
step:1440/2160 train_time:69852ms step_avg:48.51ms
step:1441/2160 train_time:69942ms step_avg:48.54ms
step:1442/2160 train_time:70031ms step_avg:48.57ms
step:1443/2160 train_time:70122ms step_avg:48.59ms
step:1444/2160 train_time:70210ms step_avg:48.62ms
step:1445/2160 train_time:70300ms step_avg:48.65ms
step:1446/2160 train_time:70388ms step_avg:48.68ms
step:1447/2160 train_time:70478ms step_avg:48.71ms
step:1448/2160 train_time:70566ms step_avg:48.73ms
step:1449/2160 train_time:70657ms step_avg:48.76ms
step:1450/2160 train_time:70745ms step_avg:48.79ms
step:1451/2160 train_time:70834ms step_avg:48.82ms
step:1452/2160 train_time:70922ms step_avg:48.84ms
step:1453/2160 train_time:71013ms step_avg:48.87ms
step:1454/2160 train_time:71101ms step_avg:48.90ms
step:1455/2160 train_time:71191ms step_avg:48.93ms
step:1456/2160 train_time:71280ms step_avg:48.96ms
step:1457/2160 train_time:71369ms step_avg:48.98ms
step:1458/2160 train_time:71458ms step_avg:49.01ms
step:1459/2160 train_time:71547ms step_avg:49.04ms
step:1460/2160 train_time:71636ms step_avg:49.07ms
step:1461/2160 train_time:71725ms step_avg:49.09ms
step:1462/2160 train_time:71813ms step_avg:49.12ms
step:1463/2160 train_time:71903ms step_avg:49.15ms
step:1464/2160 train_time:71993ms step_avg:49.18ms
step:1465/2160 train_time:72083ms step_avg:49.20ms
step:1466/2160 train_time:72172ms step_avg:49.23ms
step:1467/2160 train_time:72262ms step_avg:49.26ms
step:1468/2160 train_time:72349ms step_avg:49.28ms
step:1469/2160 train_time:72439ms step_avg:49.31ms
step:1470/2160 train_time:72527ms step_avg:49.34ms
step:1471/2160 train_time:72616ms step_avg:49.37ms
step:1472/2160 train_time:72704ms step_avg:49.39ms
step:1473/2160 train_time:72793ms step_avg:49.42ms
step:1474/2160 train_time:72882ms step_avg:49.44ms
step:1475/2160 train_time:72972ms step_avg:49.47ms
step:1476/2160 train_time:73061ms step_avg:49.50ms
step:1477/2160 train_time:73151ms step_avg:49.53ms
step:1478/2160 train_time:73239ms step_avg:49.55ms
step:1479/2160 train_time:73330ms step_avg:49.58ms
step:1480/2160 train_time:73418ms step_avg:49.61ms
step:1481/2160 train_time:73508ms step_avg:49.63ms
step:1482/2160 train_time:73596ms step_avg:49.66ms
step:1483/2160 train_time:73686ms step_avg:49.69ms
step:1484/2160 train_time:73774ms step_avg:49.71ms
step:1485/2160 train_time:73863ms step_avg:49.74ms
step:1486/2160 train_time:73952ms step_avg:49.77ms
step:1487/2160 train_time:74042ms step_avg:49.79ms
step:1488/2160 train_time:74130ms step_avg:49.82ms
step:1489/2160 train_time:74220ms step_avg:49.85ms
step:1490/2160 train_time:74309ms step_avg:49.87ms
step:1491/2160 train_time:74399ms step_avg:49.90ms
step:1492/2160 train_time:74487ms step_avg:49.92ms
step:1493/2160 train_time:74577ms step_avg:49.95ms
step:1494/2160 train_time:74665ms step_avg:49.98ms
step:1495/2160 train_time:74754ms step_avg:50.00ms
step:1496/2160 train_time:74842ms step_avg:50.03ms
step:1497/2160 train_time:74931ms step_avg:50.05ms
step:1498/2160 train_time:75019ms step_avg:50.08ms
step:1499/2160 train_time:75109ms step_avg:50.11ms
step:1500/2160 train_time:75198ms step_avg:50.13ms
step:1500/2160 val_loss:3.4705 train_time:75288ms step_avg:50.19ms
step:1501/2160 train_time:75312ms step_avg:50.17ms
step:1502/2160 train_time:75380ms step_avg:50.19ms
step:1503/2160 train_time:75476ms step_avg:50.22ms
step:1504/2160 train_time:75566ms step_avg:50.24ms
step:1505/2160 train_time:75655ms step_avg:50.27ms
step:1506/2160 train_time:75741ms step_avg:50.29ms
step:1507/2160 train_time:75829ms step_avg:50.32ms
step:1508/2160 train_time:75915ms step_avg:50.34ms
step:1509/2160 train_time:76004ms step_avg:50.37ms
step:1510/2160 train_time:76090ms step_avg:50.39ms
step:1511/2160 train_time:76181ms step_avg:50.42ms
step:1512/2160 train_time:76279ms step_avg:50.45ms
step:1513/2160 train_time:76371ms step_avg:50.48ms
step:1514/2160 train_time:76458ms step_avg:50.50ms
step:1515/2160 train_time:76548ms step_avg:50.53ms
step:1516/2160 train_time:76636ms step_avg:50.55ms
step:1517/2160 train_time:76725ms step_avg:50.58ms
step:1518/2160 train_time:76811ms step_avg:50.60ms
step:1519/2160 train_time:76900ms step_avg:50.63ms
step:1520/2160 train_time:76987ms step_avg:50.65ms
step:1521/2160 train_time:77076ms step_avg:50.67ms
step:1522/2160 train_time:77166ms step_avg:50.70ms
step:1523/2160 train_time:77261ms step_avg:50.73ms
step:1524/2160 train_time:77354ms step_avg:50.76ms
step:1525/2160 train_time:77444ms step_avg:50.78ms
step:1526/2160 train_time:77532ms step_avg:50.81ms
step:1527/2160 train_time:77622ms step_avg:50.83ms
step:1528/2160 train_time:77710ms step_avg:50.86ms
step:1529/2160 train_time:77798ms step_avg:50.88ms
step:1530/2160 train_time:77885ms step_avg:50.91ms
step:1531/2160 train_time:77974ms step_avg:50.93ms
step:1532/2160 train_time:78061ms step_avg:50.95ms
step:1533/2160 train_time:78152ms step_avg:50.98ms
step:1534/2160 train_time:78240ms step_avg:51.00ms
step:1535/2160 train_time:78333ms step_avg:51.03ms
step:1536/2160 train_time:78422ms step_avg:51.06ms
step:1537/2160 train_time:78512ms step_avg:51.08ms
step:1538/2160 train_time:78600ms step_avg:51.11ms
step:1539/2160 train_time:78689ms step_avg:51.13ms
step:1540/2160 train_time:78776ms step_avg:51.15ms
step:1541/2160 train_time:78866ms step_avg:51.18ms
step:1542/2160 train_time:78953ms step_avg:51.20ms
step:1543/2160 train_time:79042ms step_avg:51.23ms
step:1544/2160 train_time:79131ms step_avg:51.25ms
step:1545/2160 train_time:79221ms step_avg:51.28ms
step:1546/2160 train_time:79309ms step_avg:51.30ms
step:1547/2160 train_time:79400ms step_avg:51.33ms
step:1548/2160 train_time:79488ms step_avg:51.35ms
step:1549/2160 train_time:79578ms step_avg:51.37ms
step:1550/2160 train_time:79667ms step_avg:51.40ms
step:1551/2160 train_time:79756ms step_avg:51.42ms
step:1552/2160 train_time:79843ms step_avg:51.45ms
step:1553/2160 train_time:79931ms step_avg:51.47ms
step:1554/2160 train_time:80018ms step_avg:51.49ms
step:1555/2160 train_time:80108ms step_avg:51.52ms
step:1556/2160 train_time:80196ms step_avg:51.54ms
step:1557/2160 train_time:80286ms step_avg:51.56ms
step:1558/2160 train_time:80375ms step_avg:51.59ms
step:1559/2160 train_time:80465ms step_avg:51.61ms
step:1560/2160 train_time:80552ms step_avg:51.64ms
step:1561/2160 train_time:80643ms step_avg:51.66ms
step:1562/2160 train_time:80731ms step_avg:51.68ms
step:1563/2160 train_time:80820ms step_avg:51.71ms
step:1564/2160 train_time:80912ms step_avg:51.73ms
step:1565/2160 train_time:80997ms step_avg:51.76ms
step:1566/2160 train_time:81085ms step_avg:51.78ms
step:1567/2160 train_time:81175ms step_avg:51.80ms
step:1568/2160 train_time:81263ms step_avg:51.83ms
step:1569/2160 train_time:81354ms step_avg:51.85ms
step:1570/2160 train_time:81443ms step_avg:51.87ms
step:1571/2160 train_time:81535ms step_avg:51.90ms
step:1572/2160 train_time:81622ms step_avg:51.92ms
step:1573/2160 train_time:81713ms step_avg:51.95ms
step:1574/2160 train_time:81800ms step_avg:51.97ms
step:1575/2160 train_time:81889ms step_avg:51.99ms
step:1576/2160 train_time:81977ms step_avg:52.02ms
step:1577/2160 train_time:82067ms step_avg:52.04ms
step:1578/2160 train_time:82156ms step_avg:52.06ms
step:1579/2160 train_time:82245ms step_avg:52.09ms
step:1580/2160 train_time:82333ms step_avg:52.11ms
step:1581/2160 train_time:82423ms step_avg:52.13ms
step:1582/2160 train_time:82511ms step_avg:52.16ms
step:1583/2160 train_time:82601ms step_avg:52.18ms
step:1584/2160 train_time:82689ms step_avg:52.20ms
step:1585/2160 train_time:82779ms step_avg:52.23ms
step:1586/2160 train_time:82866ms step_avg:52.25ms
step:1587/2160 train_time:82955ms step_avg:52.27ms
step:1588/2160 train_time:83043ms step_avg:52.29ms
step:1589/2160 train_time:83133ms step_avg:52.32ms
step:1590/2160 train_time:83221ms step_avg:52.34ms
step:1591/2160 train_time:83313ms step_avg:52.37ms
step:1592/2160 train_time:83401ms step_avg:52.39ms
step:1593/2160 train_time:83492ms step_avg:52.41ms
step:1594/2160 train_time:83579ms step_avg:52.43ms
step:1595/2160 train_time:83669ms step_avg:52.46ms
step:1596/2160 train_time:83757ms step_avg:52.48ms
step:1597/2160 train_time:83846ms step_avg:52.50ms
step:1598/2160 train_time:83934ms step_avg:52.52ms
step:1599/2160 train_time:84023ms step_avg:52.55ms
step:1600/2160 train_time:84111ms step_avg:52.57ms
step:1601/2160 train_time:84201ms step_avg:52.59ms
step:1602/2160 train_time:84289ms step_avg:52.61ms
step:1603/2160 train_time:84378ms step_avg:52.64ms
step:1604/2160 train_time:84467ms step_avg:52.66ms
step:1605/2160 train_time:84556ms step_avg:52.68ms
step:1606/2160 train_time:84645ms step_avg:52.71ms
step:1607/2160 train_time:84734ms step_avg:52.73ms
step:1608/2160 train_time:84823ms step_avg:52.75ms
step:1609/2160 train_time:84912ms step_avg:52.77ms
step:1610/2160 train_time:85000ms step_avg:52.79ms
step:1611/2160 train_time:85089ms step_avg:52.82ms
step:1612/2160 train_time:85177ms step_avg:52.84ms
step:1613/2160 train_time:85267ms step_avg:52.86ms
step:1614/2160 train_time:85355ms step_avg:52.88ms
step:1615/2160 train_time:85445ms step_avg:52.91ms
step:1616/2160 train_time:85533ms step_avg:52.93ms
step:1617/2160 train_time:85623ms step_avg:52.95ms
step:1618/2160 train_time:85710ms step_avg:52.97ms
step:1619/2160 train_time:85800ms step_avg:53.00ms
step:1620/2160 train_time:85888ms step_avg:53.02ms
step:1621/2160 train_time:85977ms step_avg:53.04ms
step:1622/2160 train_time:86065ms step_avg:53.06ms
step:1623/2160 train_time:86154ms step_avg:53.08ms
step:1624/2160 train_time:86242ms step_avg:53.10ms
step:1625/2160 train_time:86333ms step_avg:53.13ms
step:1626/2160 train_time:86421ms step_avg:53.15ms
step:1627/2160 train_time:86512ms step_avg:53.17ms
step:1628/2160 train_time:86599ms step_avg:53.19ms
step:1629/2160 train_time:86689ms step_avg:53.22ms
step:1630/2160 train_time:86776ms step_avg:53.24ms
step:1631/2160 train_time:86867ms step_avg:53.26ms
step:1632/2160 train_time:86955ms step_avg:53.28ms
step:1633/2160 train_time:87045ms step_avg:53.30ms
step:1634/2160 train_time:87134ms step_avg:53.33ms
step:1635/2160 train_time:87223ms step_avg:53.35ms
step:1636/2160 train_time:87311ms step_avg:53.37ms
step:1637/2160 train_time:87401ms step_avg:53.39ms
step:1638/2160 train_time:87489ms step_avg:53.41ms
step:1639/2160 train_time:87578ms step_avg:53.43ms
step:1640/2160 train_time:87666ms step_avg:53.45ms
step:1641/2160 train_time:87756ms step_avg:53.48ms
step:1642/2160 train_time:87844ms step_avg:53.50ms
step:1643/2160 train_time:87934ms step_avg:53.52ms
step:1644/2160 train_time:88022ms step_avg:53.54ms
step:1645/2160 train_time:88113ms step_avg:53.56ms
step:1646/2160 train_time:88201ms step_avg:53.58ms
step:1647/2160 train_time:88291ms step_avg:53.61ms
step:1648/2160 train_time:88379ms step_avg:53.63ms
step:1649/2160 train_time:88468ms step_avg:53.65ms
step:1650/2160 train_time:88556ms step_avg:53.67ms
step:1651/2160 train_time:88645ms step_avg:53.69ms
step:1652/2160 train_time:88734ms step_avg:53.71ms
step:1653/2160 train_time:88823ms step_avg:53.73ms
step:1654/2160 train_time:88911ms step_avg:53.76ms
step:1655/2160 train_time:89000ms step_avg:53.78ms
step:1656/2160 train_time:89089ms step_avg:53.80ms
step:1657/2160 train_time:89179ms step_avg:53.82ms
step:1658/2160 train_time:89268ms step_avg:53.84ms
step:1659/2160 train_time:89357ms step_avg:53.86ms
step:1660/2160 train_time:89446ms step_avg:53.88ms
step:1661/2160 train_time:89536ms step_avg:53.91ms
step:1662/2160 train_time:89624ms step_avg:53.93ms
step:1663/2160 train_time:89715ms step_avg:53.95ms
step:1664/2160 train_time:89802ms step_avg:53.97ms
step:1665/2160 train_time:89892ms step_avg:53.99ms
step:1666/2160 train_time:89980ms step_avg:54.01ms
step:1667/2160 train_time:90070ms step_avg:54.03ms
step:1668/2160 train_time:90157ms step_avg:54.05ms
step:1669/2160 train_time:90247ms step_avg:54.07ms
step:1670/2160 train_time:90335ms step_avg:54.09ms
step:1671/2160 train_time:90426ms step_avg:54.11ms
step:1672/2160 train_time:90515ms step_avg:54.14ms
step:1673/2160 train_time:90604ms step_avg:54.16ms
step:1674/2160 train_time:90694ms step_avg:54.18ms
step:1675/2160 train_time:90783ms step_avg:54.20ms
step:1676/2160 train_time:90872ms step_avg:54.22ms
step:1677/2160 train_time:90961ms step_avg:54.24ms
step:1678/2160 train_time:91049ms step_avg:54.26ms
step:1679/2160 train_time:91138ms step_avg:54.28ms
step:1680/2160 train_time:91225ms step_avg:54.30ms
step:1681/2160 train_time:91316ms step_avg:54.32ms
step:1682/2160 train_time:91404ms step_avg:54.34ms
step:1683/2160 train_time:91494ms step_avg:54.36ms
step:1684/2160 train_time:91581ms step_avg:54.38ms
step:1685/2160 train_time:91672ms step_avg:54.40ms
step:1686/2160 train_time:91759ms step_avg:54.42ms
step:1687/2160 train_time:91849ms step_avg:54.45ms
step:1688/2160 train_time:91938ms step_avg:54.47ms
step:1689/2160 train_time:92028ms step_avg:54.49ms
step:1690/2160 train_time:92120ms step_avg:54.51ms
step:1691/2160 train_time:92205ms step_avg:54.53ms
step:1692/2160 train_time:92293ms step_avg:54.55ms
step:1693/2160 train_time:92383ms step_avg:54.57ms
step:1694/2160 train_time:92470ms step_avg:54.59ms
step:1695/2160 train_time:92560ms step_avg:54.61ms
step:1696/2160 train_time:92647ms step_avg:54.63ms
step:1697/2160 train_time:92738ms step_avg:54.65ms
step:1698/2160 train_time:92826ms step_avg:54.67ms
step:1699/2160 train_time:92917ms step_avg:54.69ms
step:1700/2160 train_time:93006ms step_avg:54.71ms
step:1701/2160 train_time:93097ms step_avg:54.73ms
step:1702/2160 train_time:93184ms step_avg:54.75ms
step:1703/2160 train_time:93274ms step_avg:54.77ms
step:1704/2160 train_time:93361ms step_avg:54.79ms
step:1705/2160 train_time:93451ms step_avg:54.81ms
step:1706/2160 train_time:93538ms step_avg:54.83ms
step:1707/2160 train_time:93628ms step_avg:54.85ms
step:1708/2160 train_time:93716ms step_avg:54.87ms
step:1709/2160 train_time:93807ms step_avg:54.89ms
step:1710/2160 train_time:93895ms step_avg:54.91ms
step:1711/2160 train_time:93984ms step_avg:54.93ms
step:1712/2160 train_time:94073ms step_avg:54.95ms
step:1713/2160 train_time:94162ms step_avg:54.97ms
step:1714/2160 train_time:94250ms step_avg:54.99ms
step:1715/2160 train_time:94339ms step_avg:55.01ms
step:1716/2160 train_time:94427ms step_avg:55.03ms
step:1717/2160 train_time:94517ms step_avg:55.05ms
step:1718/2160 train_time:94606ms step_avg:55.07ms
step:1719/2160 train_time:94696ms step_avg:55.09ms
step:1720/2160 train_time:94783ms step_avg:55.11ms
step:1721/2160 train_time:94874ms step_avg:55.13ms
step:1722/2160 train_time:94962ms step_avg:55.15ms
step:1723/2160 train_time:95052ms step_avg:55.17ms
step:1724/2160 train_time:95139ms step_avg:55.18ms
step:1725/2160 train_time:95230ms step_avg:55.21ms
step:1726/2160 train_time:95316ms step_avg:55.22ms
step:1727/2160 train_time:95406ms step_avg:55.24ms
step:1728/2160 train_time:95495ms step_avg:55.26ms
step:1729/2160 train_time:95584ms step_avg:55.28ms
step:1730/2160 train_time:95672ms step_avg:55.30ms
step:1731/2160 train_time:95761ms step_avg:55.32ms
step:1732/2160 train_time:95849ms step_avg:55.34ms
step:1733/2160 train_time:95939ms step_avg:55.36ms
step:1734/2160 train_time:96027ms step_avg:55.38ms
step:1735/2160 train_time:96117ms step_avg:55.40ms
step:1736/2160 train_time:96205ms step_avg:55.42ms
step:1737/2160 train_time:96295ms step_avg:55.44ms
step:1738/2160 train_time:96382ms step_avg:55.46ms
step:1739/2160 train_time:96471ms step_avg:55.48ms
step:1740/2160 train_time:96559ms step_avg:55.49ms
step:1741/2160 train_time:96649ms step_avg:55.51ms
step:1742/2160 train_time:96737ms step_avg:55.53ms
step:1743/2160 train_time:96827ms step_avg:55.55ms
step:1744/2160 train_time:96915ms step_avg:55.57ms
step:1745/2160 train_time:97005ms step_avg:55.59ms
step:1746/2160 train_time:97093ms step_avg:55.61ms
step:1747/2160 train_time:97182ms step_avg:55.63ms
step:1748/2160 train_time:97271ms step_avg:55.65ms
step:1749/2160 train_time:97361ms step_avg:55.67ms
step:1750/2160 train_time:97449ms step_avg:55.69ms
step:1750/2160 val_loss:3.3777 train_time:97538ms step_avg:55.74ms
step:1751/2160 train_time:97561ms step_avg:55.72ms
step:1752/2160 train_time:97632ms step_avg:55.73ms
step:1753/2160 train_time:97727ms step_avg:55.75ms
step:1754/2160 train_time:97816ms step_avg:55.77ms
step:1755/2160 train_time:97906ms step_avg:55.79ms
step:1756/2160 train_time:97993ms step_avg:55.80ms
step:1757/2160 train_time:98082ms step_avg:55.82ms
step:1758/2160 train_time:98170ms step_avg:55.84ms
step:1759/2160 train_time:98259ms step_avg:55.86ms
step:1760/2160 train_time:98347ms step_avg:55.88ms
step:1761/2160 train_time:98436ms step_avg:55.90ms
step:1762/2160 train_time:98527ms step_avg:55.92ms
step:1763/2160 train_time:98618ms step_avg:55.94ms
step:1764/2160 train_time:98708ms step_avg:55.96ms
step:1765/2160 train_time:98800ms step_avg:55.98ms
step:1766/2160 train_time:98887ms step_avg:55.99ms
step:1767/2160 train_time:98976ms step_avg:56.01ms
step:1768/2160 train_time:99064ms step_avg:56.03ms
step:1769/2160 train_time:99153ms step_avg:56.05ms
step:1770/2160 train_time:99239ms step_avg:56.07ms
step:1771/2160 train_time:99329ms step_avg:56.09ms
step:1772/2160 train_time:99417ms step_avg:56.10ms
step:1773/2160 train_time:99508ms step_avg:56.12ms
step:1774/2160 train_time:99596ms step_avg:56.14ms
step:1775/2160 train_time:99689ms step_avg:56.16ms
step:1776/2160 train_time:99778ms step_avg:56.18ms
step:1777/2160 train_time:99870ms step_avg:56.20ms
step:1778/2160 train_time:99957ms step_avg:56.22ms
step:1779/2160 train_time:100046ms step_avg:56.24ms
step:1780/2160 train_time:100133ms step_avg:56.25ms
step:1781/2160 train_time:100223ms step_avg:56.27ms
step:1782/2160 train_time:100310ms step_avg:56.29ms
step:1783/2160 train_time:100400ms step_avg:56.31ms
step:1784/2160 train_time:100487ms step_avg:56.33ms
step:1785/2160 train_time:100578ms step_avg:56.35ms
step:1786/2160 train_time:100668ms step_avg:56.37ms
step:1787/2160 train_time:100758ms step_avg:56.38ms
step:1788/2160 train_time:100846ms step_avg:56.40ms
step:1789/2160 train_time:100936ms step_avg:56.42ms
step:1790/2160 train_time:101025ms step_avg:56.44ms
step:1791/2160 train_time:101114ms step_avg:56.46ms
step:1792/2160 train_time:101202ms step_avg:56.47ms
step:1793/2160 train_time:101292ms step_avg:56.49ms
step:1794/2160 train_time:101379ms step_avg:56.51ms
step:1795/2160 train_time:101472ms step_avg:56.53ms
step:1796/2160 train_time:101559ms step_avg:56.55ms
step:1797/2160 train_time:101649ms step_avg:56.57ms
step:1798/2160 train_time:101738ms step_avg:56.58ms
step:1799/2160 train_time:101828ms step_avg:56.60ms
step:1800/2160 train_time:101917ms step_avg:56.62ms
step:1801/2160 train_time:102007ms step_avg:56.64ms
step:1802/2160 train_time:102094ms step_avg:56.66ms
step:1803/2160 train_time:102184ms step_avg:56.67ms
step:1804/2160 train_time:102271ms step_avg:56.69ms
step:1805/2160 train_time:102361ms step_avg:56.71ms
step:1806/2160 train_time:102448ms step_avg:56.73ms
step:1807/2160 train_time:102537ms step_avg:56.74ms
step:1808/2160 train_time:102626ms step_avg:56.76ms
step:1809/2160 train_time:102716ms step_avg:56.78ms
step:1810/2160 train_time:102805ms step_avg:56.80ms
step:1811/2160 train_time:102895ms step_avg:56.82ms
step:1812/2160 train_time:102983ms step_avg:56.83ms
step:1813/2160 train_time:103072ms step_avg:56.85ms
step:1814/2160 train_time:103161ms step_avg:56.87ms
step:1815/2160 train_time:103250ms step_avg:56.89ms
step:1816/2160 train_time:103338ms step_avg:56.90ms
step:1817/2160 train_time:103429ms step_avg:56.92ms
step:1818/2160 train_time:103518ms step_avg:56.94ms
step:1819/2160 train_time:103607ms step_avg:56.96ms
step:1820/2160 train_time:103696ms step_avg:56.98ms
step:1821/2160 train_time:103786ms step_avg:56.99ms
step:1822/2160 train_time:103874ms step_avg:57.01ms
step:1823/2160 train_time:103964ms step_avg:57.03ms
step:1824/2160 train_time:104052ms step_avg:57.05ms
step:1825/2160 train_time:104142ms step_avg:57.06ms
step:1826/2160 train_time:104230ms step_avg:57.08ms
step:1827/2160 train_time:104319ms step_avg:57.10ms
step:1828/2160 train_time:104407ms step_avg:57.12ms
step:1829/2160 train_time:104496ms step_avg:57.13ms
step:1830/2160 train_time:104584ms step_avg:57.15ms
step:1831/2160 train_time:104673ms step_avg:57.17ms
step:1832/2160 train_time:104762ms step_avg:57.18ms
step:1833/2160 train_time:104851ms step_avg:57.20ms
step:1834/2160 train_time:104939ms step_avg:57.22ms
step:1835/2160 train_time:105029ms step_avg:57.24ms
step:1836/2160 train_time:105117ms step_avg:57.25ms
step:1837/2160 train_time:105207ms step_avg:57.27ms
step:1838/2160 train_time:105296ms step_avg:57.29ms
step:1839/2160 train_time:105386ms step_avg:57.31ms
step:1840/2160 train_time:105473ms step_avg:57.32ms
step:1841/2160 train_time:105564ms step_avg:57.34ms
step:1842/2160 train_time:105651ms step_avg:57.36ms
step:1843/2160 train_time:105741ms step_avg:57.37ms
step:1844/2160 train_time:105830ms step_avg:57.39ms
step:1845/2160 train_time:105920ms step_avg:57.41ms
step:1846/2160 train_time:106008ms step_avg:57.43ms
step:1847/2160 train_time:106098ms step_avg:57.44ms
step:1848/2160 train_time:106186ms step_avg:57.46ms
step:1849/2160 train_time:106275ms step_avg:57.48ms
step:1850/2160 train_time:106363ms step_avg:57.49ms
step:1851/2160 train_time:106452ms step_avg:57.51ms
step:1852/2160 train_time:106540ms step_avg:57.53ms
step:1853/2160 train_time:106631ms step_avg:57.55ms
step:1854/2160 train_time:106719ms step_avg:57.56ms
step:1855/2160 train_time:106809ms step_avg:57.58ms
step:1856/2160 train_time:106897ms step_avg:57.60ms
step:1857/2160 train_time:106988ms step_avg:57.61ms
step:1858/2160 train_time:107075ms step_avg:57.63ms
step:1859/2160 train_time:107166ms step_avg:57.65ms
step:1860/2160 train_time:107254ms step_avg:57.66ms
step:1861/2160 train_time:107343ms step_avg:57.68ms
step:1862/2160 train_time:107431ms step_avg:57.70ms
step:1863/2160 train_time:107521ms step_avg:57.71ms
step:1864/2160 train_time:107609ms step_avg:57.73ms
step:1865/2160 train_time:107699ms step_avg:57.75ms
step:1866/2160 train_time:107787ms step_avg:57.76ms
step:1867/2160 train_time:107876ms step_avg:57.78ms
step:1868/2160 train_time:107965ms step_avg:57.80ms
step:1869/2160 train_time:108054ms step_avg:57.81ms
step:1870/2160 train_time:108142ms step_avg:57.83ms
step:1871/2160 train_time:108232ms step_avg:57.85ms
step:1872/2160 train_time:108319ms step_avg:57.86ms
step:1873/2160 train_time:108408ms step_avg:57.88ms
step:1874/2160 train_time:108496ms step_avg:57.90ms
step:1875/2160 train_time:108587ms step_avg:57.91ms
step:1876/2160 train_time:108675ms step_avg:57.93ms
step:1877/2160 train_time:108766ms step_avg:57.95ms
step:1878/2160 train_time:108855ms step_avg:57.96ms
step:1879/2160 train_time:108945ms step_avg:57.98ms
step:1880/2160 train_time:109032ms step_avg:58.00ms
step:1881/2160 train_time:109122ms step_avg:58.01ms
step:1882/2160 train_time:109209ms step_avg:58.03ms
step:1883/2160 train_time:109299ms step_avg:58.05ms
step:1884/2160 train_time:109386ms step_avg:58.06ms
step:1885/2160 train_time:109476ms step_avg:58.08ms
step:1886/2160 train_time:109564ms step_avg:58.09ms
step:1887/2160 train_time:109653ms step_avg:58.11ms
step:1888/2160 train_time:109741ms step_avg:58.13ms
step:1889/2160 train_time:109830ms step_avg:58.14ms
step:1890/2160 train_time:109919ms step_avg:58.16ms
step:1891/2160 train_time:110009ms step_avg:58.18ms
step:1892/2160 train_time:110097ms step_avg:58.19ms
step:1893/2160 train_time:110187ms step_avg:58.21ms
step:1894/2160 train_time:110275ms step_avg:58.22ms
step:1895/2160 train_time:110365ms step_avg:58.24ms
step:1896/2160 train_time:110453ms step_avg:58.26ms
step:1897/2160 train_time:110545ms step_avg:58.27ms
step:1898/2160 train_time:110630ms step_avg:58.29ms
step:1899/2160 train_time:110721ms step_avg:58.30ms
step:1900/2160 train_time:110808ms step_avg:58.32ms
step:1901/2160 train_time:110898ms step_avg:58.34ms
step:1902/2160 train_time:110986ms step_avg:58.35ms
step:1903/2160 train_time:111076ms step_avg:58.37ms
step:1904/2160 train_time:111164ms step_avg:58.38ms
step:1905/2160 train_time:111254ms step_avg:58.40ms
step:1906/2160 train_time:111341ms step_avg:58.42ms
step:1907/2160 train_time:111432ms step_avg:58.43ms
step:1908/2160 train_time:111519ms step_avg:58.45ms
step:1909/2160 train_time:111608ms step_avg:58.46ms
step:1910/2160 train_time:111697ms step_avg:58.48ms
step:1911/2160 train_time:111787ms step_avg:58.50ms
step:1912/2160 train_time:111875ms step_avg:58.51ms
step:1913/2160 train_time:111965ms step_avg:58.53ms
step:1914/2160 train_time:112053ms step_avg:58.54ms
step:1915/2160 train_time:112143ms step_avg:58.56ms
step:1916/2160 train_time:112230ms step_avg:58.58ms
step:1917/2160 train_time:112320ms step_avg:58.59ms
step:1918/2160 train_time:112407ms step_avg:58.61ms
step:1919/2160 train_time:112497ms step_avg:58.62ms
step:1920/2160 train_time:112585ms step_avg:58.64ms
step:1921/2160 train_time:112675ms step_avg:58.65ms
step:1922/2160 train_time:112763ms step_avg:58.67ms
step:1923/2160 train_time:112853ms step_avg:58.69ms
step:1924/2160 train_time:112941ms step_avg:58.70ms
step:1925/2160 train_time:113031ms step_avg:58.72ms
step:1926/2160 train_time:113119ms step_avg:58.73ms
step:1927/2160 train_time:113209ms step_avg:58.75ms
step:1928/2160 train_time:113297ms step_avg:58.76ms
step:1929/2160 train_time:113387ms step_avg:58.78ms
step:1930/2160 train_time:113475ms step_avg:58.80ms
step:1931/2160 train_time:113565ms step_avg:58.81ms
step:1932/2160 train_time:113653ms step_avg:58.83ms
step:1933/2160 train_time:113743ms step_avg:58.84ms
step:1934/2160 train_time:113831ms step_avg:58.86ms
step:1935/2160 train_time:113921ms step_avg:58.87ms
step:1936/2160 train_time:114009ms step_avg:58.89ms
step:1937/2160 train_time:114099ms step_avg:58.90ms
step:1938/2160 train_time:114187ms step_avg:58.92ms
step:1939/2160 train_time:114276ms step_avg:58.94ms
step:1940/2160 train_time:114364ms step_avg:58.95ms
step:1941/2160 train_time:114453ms step_avg:58.97ms
step:1942/2160 train_time:114541ms step_avg:58.98ms
step:1943/2160 train_time:114631ms step_avg:59.00ms
step:1944/2160 train_time:114720ms step_avg:59.01ms
step:1945/2160 train_time:114810ms step_avg:59.03ms
step:1946/2160 train_time:114898ms step_avg:59.04ms
step:1947/2160 train_time:114988ms step_avg:59.06ms
step:1948/2160 train_time:115076ms step_avg:59.07ms
step:1949/2160 train_time:115166ms step_avg:59.09ms
step:1950/2160 train_time:115254ms step_avg:59.10ms
step:1951/2160 train_time:115344ms step_avg:59.12ms
step:1952/2160 train_time:115431ms step_avg:59.13ms
step:1953/2160 train_time:115522ms step_avg:59.15ms
step:1954/2160 train_time:115610ms step_avg:59.17ms
step:1955/2160 train_time:115700ms step_avg:59.18ms
step:1956/2160 train_time:115789ms step_avg:59.20ms
step:1957/2160 train_time:115877ms step_avg:59.21ms
step:1958/2160 train_time:115965ms step_avg:59.23ms
step:1959/2160 train_time:116054ms step_avg:59.24ms
step:1960/2160 train_time:116143ms step_avg:59.26ms
step:1961/2160 train_time:116233ms step_avg:59.27ms
step:1962/2160 train_time:116321ms step_avg:59.29ms
step:1963/2160 train_time:116410ms step_avg:59.30ms
step:1964/2160 train_time:116498ms step_avg:59.32ms
step:1965/2160 train_time:116588ms step_avg:59.33ms
step:1966/2160 train_time:116676ms step_avg:59.35ms
step:1967/2160 train_time:116767ms step_avg:59.36ms
step:1968/2160 train_time:116855ms step_avg:59.38ms
step:1969/2160 train_time:116944ms step_avg:59.39ms
step:1970/2160 train_time:117031ms step_avg:59.41ms
step:1971/2160 train_time:117121ms step_avg:59.42ms
step:1972/2160 train_time:117209ms step_avg:59.44ms
step:1973/2160 train_time:117299ms step_avg:59.45ms
step:1974/2160 train_time:117386ms step_avg:59.47ms
step:1975/2160 train_time:117475ms step_avg:59.48ms
step:1976/2160 train_time:117563ms step_avg:59.50ms
step:1977/2160 train_time:117653ms step_avg:59.51ms
step:1978/2160 train_time:117740ms step_avg:59.52ms
step:1979/2160 train_time:117831ms step_avg:59.54ms
step:1980/2160 train_time:117919ms step_avg:59.55ms
step:1981/2160 train_time:118008ms step_avg:59.57ms
step:1982/2160 train_time:118097ms step_avg:59.58ms
step:1983/2160 train_time:118187ms step_avg:59.60ms
step:1984/2160 train_time:118274ms step_avg:59.61ms
step:1985/2160 train_time:118364ms step_avg:59.63ms
step:1986/2160 train_time:118451ms step_avg:59.64ms
step:1987/2160 train_time:118541ms step_avg:59.66ms
step:1988/2160 train_time:118631ms step_avg:59.67ms
step:1989/2160 train_time:118720ms step_avg:59.69ms
step:1990/2160 train_time:118808ms step_avg:59.70ms
step:1991/2160 train_time:118897ms step_avg:59.72ms
step:1992/2160 train_time:118984ms step_avg:59.73ms
step:1993/2160 train_time:119074ms step_avg:59.75ms
step:1994/2160 train_time:119162ms step_avg:59.76ms
step:1995/2160 train_time:119252ms step_avg:59.78ms
step:1996/2160 train_time:119340ms step_avg:59.79ms
step:1997/2160 train_time:119430ms step_avg:59.80ms
step:1998/2160 train_time:119519ms step_avg:59.82ms
step:1999/2160 train_time:119608ms step_avg:59.83ms
step:2000/2160 train_time:119696ms step_avg:59.85ms
step:2000/2160 val_loss:3.3093 train_time:119786ms step_avg:59.89ms
step:2001/2160 train_time:119810ms step_avg:59.87ms
step:2002/2160 train_time:119881ms step_avg:59.88ms
step:2003/2160 train_time:119974ms step_avg:59.90ms
step:2004/2160 train_time:120061ms step_avg:59.91ms
step:2005/2160 train_time:120150ms step_avg:59.93ms
step:2006/2160 train_time:120237ms step_avg:59.94ms
step:2007/2160 train_time:120325ms step_avg:59.95ms
step:2008/2160 train_time:120412ms step_avg:59.97ms
step:2009/2160 train_time:120500ms step_avg:59.98ms
step:2010/2160 train_time:120587ms step_avg:59.99ms
step:2011/2160 train_time:120677ms step_avg:60.01ms
step:2012/2160 train_time:120766ms step_avg:60.02ms
step:2013/2160 train_time:120861ms step_avg:60.04ms
step:2014/2160 train_time:120950ms step_avg:60.05ms
step:2015/2160 train_time:121040ms step_avg:60.07ms
step:2016/2160 train_time:121128ms step_avg:60.08ms
step:2017/2160 train_time:121218ms step_avg:60.10ms
step:2018/2160 train_time:121305ms step_avg:60.11ms
step:2019/2160 train_time:121394ms step_avg:60.13ms
step:2020/2160 train_time:121481ms step_avg:60.14ms
step:2021/2160 train_time:121571ms step_avg:60.15ms
step:2022/2160 train_time:121658ms step_avg:60.17ms
step:2023/2160 train_time:121748ms step_avg:60.18ms
step:2024/2160 train_time:121839ms step_avg:60.20ms
step:2025/2160 train_time:121930ms step_avg:60.21ms
step:2026/2160 train_time:122018ms step_avg:60.23ms
step:2027/2160 train_time:122108ms step_avg:60.24ms
step:2028/2160 train_time:122195ms step_avg:60.25ms
step:2029/2160 train_time:122284ms step_avg:60.27ms
step:2030/2160 train_time:122371ms step_avg:60.28ms
step:2031/2160 train_time:122460ms step_avg:60.30ms
step:2032/2160 train_time:122547ms step_avg:60.31ms
step:2033/2160 train_time:122636ms step_avg:60.32ms
step:2034/2160 train_time:122724ms step_avg:60.34ms
step:2035/2160 train_time:122815ms step_avg:60.35ms
step:2036/2160 train_time:122903ms step_avg:60.37ms
step:2037/2160 train_time:122994ms step_avg:60.38ms
step:2038/2160 train_time:123082ms step_avg:60.39ms
step:2039/2160 train_time:123173ms step_avg:60.41ms
step:2040/2160 train_time:123260ms step_avg:60.42ms
step:2041/2160 train_time:123350ms step_avg:60.44ms
step:2042/2160 train_time:123437ms step_avg:60.45ms
step:2043/2160 train_time:123526ms step_avg:60.46ms
step:2044/2160 train_time:123613ms step_avg:60.48ms
step:2045/2160 train_time:123702ms step_avg:60.49ms
step:2046/2160 train_time:123790ms step_avg:60.50ms
step:2047/2160 train_time:123880ms step_avg:60.52ms
step:2048/2160 train_time:123968ms step_avg:60.53ms
step:2049/2160 train_time:124057ms step_avg:60.55ms
step:2050/2160 train_time:124145ms step_avg:60.56ms
step:2051/2160 train_time:124235ms step_avg:60.57ms
step:2052/2160 train_time:124323ms step_avg:60.59ms
step:2053/2160 train_time:124412ms step_avg:60.60ms
step:2054/2160 train_time:124499ms step_avg:60.61ms
step:2055/2160 train_time:124589ms step_avg:60.63ms
step:2056/2160 train_time:124676ms step_avg:60.64ms
step:2057/2160 train_time:124766ms step_avg:60.65ms
step:2058/2160 train_time:124855ms step_avg:60.67ms
step:2059/2160 train_time:124944ms step_avg:60.68ms
step:2060/2160 train_time:125032ms step_avg:60.70ms
step:2061/2160 train_time:125121ms step_avg:60.71ms
step:2062/2160 train_time:125210ms step_avg:60.72ms
step:2063/2160 train_time:125300ms step_avg:60.74ms
step:2064/2160 train_time:125388ms step_avg:60.75ms
step:2065/2160 train_time:125478ms step_avg:60.76ms
step:2066/2160 train_time:125566ms step_avg:60.78ms
step:2067/2160 train_time:125655ms step_avg:60.79ms
step:2068/2160 train_time:125742ms step_avg:60.80ms
step:2069/2160 train_time:125833ms step_avg:60.82ms
step:2070/2160 train_time:125921ms step_avg:60.83ms
step:2071/2160 train_time:126011ms step_avg:60.85ms
step:2072/2160 train_time:126100ms step_avg:60.86ms
step:2073/2160 train_time:126189ms step_avg:60.87ms
step:2074/2160 train_time:126276ms step_avg:60.89ms
step:2075/2160 train_time:126366ms step_avg:60.90ms
step:2076/2160 train_time:126454ms step_avg:60.91ms
step:2077/2160 train_time:126544ms step_avg:60.93ms
step:2078/2160 train_time:126632ms step_avg:60.94ms
step:2079/2160 train_time:126721ms step_avg:60.95ms
step:2080/2160 train_time:126810ms step_avg:60.97ms
step:2081/2160 train_time:126899ms step_avg:60.98ms
step:2082/2160 train_time:126988ms step_avg:60.99ms
step:2083/2160 train_time:127078ms step_avg:61.01ms
step:2084/2160 train_time:127166ms step_avg:61.02ms
step:2085/2160 train_time:127256ms step_avg:61.03ms
step:2086/2160 train_time:127343ms step_avg:61.05ms
step:2087/2160 train_time:127432ms step_avg:61.06ms
step:2088/2160 train_time:127519ms step_avg:61.07ms
step:2089/2160 train_time:127610ms step_avg:61.09ms
step:2090/2160 train_time:127698ms step_avg:61.10ms
step:2091/2160 train_time:127789ms step_avg:61.11ms
step:2092/2160 train_time:127877ms step_avg:61.13ms
step:2093/2160 train_time:127968ms step_avg:61.14ms
step:2094/2160 train_time:128056ms step_avg:61.15ms
step:2095/2160 train_time:128145ms step_avg:61.17ms
step:2096/2160 train_time:128233ms step_avg:61.18ms
step:2097/2160 train_time:128322ms step_avg:61.19ms
step:2098/2160 train_time:128409ms step_avg:61.21ms
step:2099/2160 train_time:128498ms step_avg:61.22ms
step:2100/2160 train_time:128586ms step_avg:61.23ms
step:2101/2160 train_time:128676ms step_avg:61.25ms
step:2102/2160 train_time:128765ms step_avg:61.26ms
step:2103/2160 train_time:128855ms step_avg:61.27ms
step:2104/2160 train_time:128942ms step_avg:61.28ms
step:2105/2160 train_time:129033ms step_avg:61.30ms
step:2106/2160 train_time:129120ms step_avg:61.31ms
step:2107/2160 train_time:129210ms step_avg:61.32ms
step:2108/2160 train_time:129298ms step_avg:61.34ms
step:2109/2160 train_time:129387ms step_avg:61.35ms
step:2110/2160 train_time:129474ms step_avg:61.36ms
step:2111/2160 train_time:129564ms step_avg:61.38ms
step:2112/2160 train_time:129652ms step_avg:61.39ms
step:2113/2160 train_time:129741ms step_avg:61.40ms
step:2114/2160 train_time:129829ms step_avg:61.41ms
step:2115/2160 train_time:129919ms step_avg:61.43ms
step:2116/2160 train_time:130007ms step_avg:61.44ms
step:2117/2160 train_time:130097ms step_avg:61.45ms
step:2118/2160 train_time:130185ms step_avg:61.47ms
step:2119/2160 train_time:130275ms step_avg:61.48ms
step:2120/2160 train_time:130362ms step_avg:61.49ms
step:2121/2160 train_time:130452ms step_avg:61.50ms
step:2122/2160 train_time:130539ms step_avg:61.52ms
step:2123/2160 train_time:130630ms step_avg:61.53ms
step:2124/2160 train_time:130718ms step_avg:61.54ms
step:2125/2160 train_time:130808ms step_avg:61.56ms
step:2126/2160 train_time:130897ms step_avg:61.57ms
step:2127/2160 train_time:130987ms step_avg:61.58ms
step:2128/2160 train_time:131075ms step_avg:61.60ms
step:2129/2160 train_time:131166ms step_avg:61.61ms
step:2130/2160 train_time:131254ms step_avg:61.62ms
step:2131/2160 train_time:131343ms step_avg:61.63ms
step:2132/2160 train_time:131433ms step_avg:61.65ms
step:2133/2160 train_time:131522ms step_avg:61.66ms
step:2134/2160 train_time:131611ms step_avg:61.67ms
step:2135/2160 train_time:131700ms step_avg:61.69ms
step:2136/2160 train_time:131789ms step_avg:61.70ms
step:2137/2160 train_time:131878ms step_avg:61.71ms
step:2138/2160 train_time:131966ms step_avg:61.72ms
step:2139/2160 train_time:132057ms step_avg:61.74ms
step:2140/2160 train_time:132145ms step_avg:61.75ms
step:2141/2160 train_time:132235ms step_avg:61.76ms
step:2142/2160 train_time:132323ms step_avg:61.78ms
step:2143/2160 train_time:132413ms step_avg:61.79ms
step:2144/2160 train_time:132501ms step_avg:61.80ms
step:2145/2160 train_time:132591ms step_avg:61.81ms
step:2146/2160 train_time:132679ms step_avg:61.83ms
step:2147/2160 train_time:132769ms step_avg:61.84ms
step:2148/2160 train_time:132857ms step_avg:61.85ms
step:2149/2160 train_time:132947ms step_avg:61.86ms
step:2150/2160 train_time:133036ms step_avg:61.88ms
step:2151/2160 train_time:133129ms step_avg:61.89ms
step:2152/2160 train_time:133214ms step_avg:61.90ms
step:2153/2160 train_time:133304ms step_avg:61.92ms
step:2154/2160 train_time:133392ms step_avg:61.93ms
step:2155/2160 train_time:133482ms step_avg:61.94ms
step:2156/2160 train_time:133570ms step_avg:61.95ms
step:2157/2160 train_time:133660ms step_avg:61.97ms
step:2158/2160 train_time:133748ms step_avg:61.98ms
step:2159/2160 train_time:133838ms step_avg:61.99ms
step:2160/2160 train_time:133926ms step_avg:62.00ms
step:2160/2160 val_loss:3.2767 train_time:134016ms step_avg:62.04ms
peak memory allocated: 29892 MiB reserved: 44796 MiB

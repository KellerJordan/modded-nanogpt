import uuid
run_id = f"NorMuon Fixes and Optims 8xH100 - 2150 steps - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2110  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 10 2025, 18:15:56) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 19:19:49 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   44C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   44C    P0            131W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   45C    P0            129W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   44C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           32884      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           32885      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           32886      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           32887      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           32888      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           32889      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           32890      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           32891      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           32885      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           32886      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           32887      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           32888      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           32889      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           32890      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           32891      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2150 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2150 train_time:90ms step_avg:90.41ms
step:2/2150 train_time:169ms step_avg:84.38ms
step:3/2150 train_time:191ms step_avg:63.82ms
step:4/2150 train_time:215ms step_avg:53.79ms
step:5/2150 train_time:237ms step_avg:47.40ms
step:6/2150 train_time:337ms step_avg:56.22ms
step:7/2150 train_time:380ms step_avg:54.33ms
step:8/2150 train_time:414ms step_avg:51.70ms
step:9/2150 train_time:446ms step_avg:49.53ms
step:10/2150 train_time:479ms step_avg:47.91ms
step:11/2150 train_time:512ms step_avg:46.53ms
step:12/2150 train_time:546ms step_avg:45.48ms
step:13/2150 train_time:578ms step_avg:44.48ms
step:14/2150 train_time:612ms step_avg:43.70ms
step:15/2150 train_time:645ms step_avg:42.98ms
step:16/2150 train_time:678ms step_avg:42.39ms
step:17/2150 train_time:711ms step_avg:41.84ms
step:18/2150 train_time:745ms step_avg:41.39ms
step:19/2150 train_time:778ms step_avg:40.94ms
step:20/2150 train_time:812ms step_avg:40.58ms
step:21/2150 train_time:844ms step_avg:40.19ms
step:22/2150 train_time:878ms step_avg:39.89ms
step:23/2150 train_time:910ms step_avg:39.58ms
step:24/2150 train_time:944ms step_avg:39.34ms
step:25/2150 train_time:977ms step_avg:39.06ms
step:26/2150 train_time:1010ms step_avg:38.84ms
step:27/2150 train_time:1043ms step_avg:38.63ms
step:28/2150 train_time:1077ms step_avg:38.45ms
step:29/2150 train_time:1109ms step_avg:38.26ms
step:30/2150 train_time:1144ms step_avg:38.14ms
step:31/2150 train_time:1175ms step_avg:37.91ms
step:32/2150 train_time:1209ms step_avg:37.79ms
step:33/2150 train_time:1242ms step_avg:37.63ms
step:34/2150 train_time:1276ms step_avg:37.54ms
step:35/2150 train_time:1311ms step_avg:37.47ms
step:36/2150 train_time:1345ms step_avg:37.37ms
step:37/2150 train_time:1380ms step_avg:37.29ms
step:38/2150 train_time:1414ms step_avg:37.20ms
step:39/2150 train_time:1447ms step_avg:37.11ms
step:40/2150 train_time:1481ms step_avg:37.02ms
step:41/2150 train_time:1514ms step_avg:36.92ms
step:42/2150 train_time:1547ms step_avg:36.84ms
step:43/2150 train_time:1580ms step_avg:36.75ms
step:44/2150 train_time:1614ms step_avg:36.68ms
step:45/2150 train_time:1647ms step_avg:36.60ms
step:46/2150 train_time:1681ms step_avg:36.53ms
step:47/2150 train_time:1713ms step_avg:36.46ms
step:48/2150 train_time:1747ms step_avg:36.39ms
step:49/2150 train_time:1780ms step_avg:36.33ms
step:50/2150 train_time:1814ms step_avg:36.27ms
step:51/2150 train_time:1846ms step_avg:36.20ms
step:52/2150 train_time:1880ms step_avg:36.14ms
step:53/2150 train_time:1913ms step_avg:36.09ms
step:54/2150 train_time:1946ms step_avg:36.04ms
step:55/2150 train_time:1979ms step_avg:35.98ms
step:56/2150 train_time:2013ms step_avg:35.94ms
step:57/2150 train_time:2046ms step_avg:35.89ms
step:58/2150 train_time:2079ms step_avg:35.85ms
step:59/2150 train_time:2112ms step_avg:35.79ms
step:60/2150 train_time:2145ms step_avg:35.76ms
step:61/2150 train_time:2178ms step_avg:35.70ms
step:62/2150 train_time:2211ms step_avg:35.66ms
step:63/2150 train_time:2245ms step_avg:35.63ms
step:64/2150 train_time:2278ms step_avg:35.59ms
step:65/2150 train_time:2311ms step_avg:35.56ms
step:66/2150 train_time:2345ms step_avg:35.53ms
step:67/2150 train_time:2379ms step_avg:35.50ms
step:68/2150 train_time:2412ms step_avg:35.47ms
step:69/2150 train_time:2445ms step_avg:35.44ms
step:70/2150 train_time:2479ms step_avg:35.41ms
step:71/2150 train_time:2512ms step_avg:35.38ms
step:72/2150 train_time:2546ms step_avg:35.35ms
step:73/2150 train_time:2579ms step_avg:35.32ms
step:74/2150 train_time:2612ms step_avg:35.30ms
step:75/2150 train_time:2645ms step_avg:35.27ms
step:76/2150 train_time:2679ms step_avg:35.25ms
step:77/2150 train_time:2712ms step_avg:35.22ms
step:78/2150 train_time:2745ms step_avg:35.20ms
step:79/2150 train_time:2778ms step_avg:35.17ms
step:80/2150 train_time:2812ms step_avg:35.15ms
step:81/2150 train_time:2844ms step_avg:35.12ms
step:82/2150 train_time:2878ms step_avg:35.09ms
step:83/2150 train_time:2911ms step_avg:35.07ms
step:84/2150 train_time:2944ms step_avg:35.05ms
step:85/2150 train_time:2977ms step_avg:35.02ms
step:86/2150 train_time:3010ms step_avg:35.00ms
step:87/2150 train_time:3043ms step_avg:34.98ms
step:88/2150 train_time:3076ms step_avg:34.96ms
step:89/2150 train_time:3110ms step_avg:34.94ms
step:90/2150 train_time:3143ms step_avg:34.92ms
step:91/2150 train_time:3176ms step_avg:34.90ms
step:92/2150 train_time:3209ms step_avg:34.88ms
step:93/2150 train_time:3242ms step_avg:34.86ms
step:94/2150 train_time:3275ms step_avg:34.84ms
step:95/2150 train_time:3308ms step_avg:34.83ms
step:96/2150 train_time:3342ms step_avg:34.81ms
step:97/2150 train_time:3376ms step_avg:34.80ms
step:98/2150 train_time:3409ms step_avg:34.79ms
step:99/2150 train_time:3443ms step_avg:34.78ms
step:100/2150 train_time:3476ms step_avg:34.76ms
step:101/2150 train_time:3509ms step_avg:34.74ms
step:102/2150 train_time:3543ms step_avg:34.73ms
step:103/2150 train_time:3575ms step_avg:34.71ms
step:104/2150 train_time:3609ms step_avg:34.70ms
step:105/2150 train_time:3642ms step_avg:34.68ms
step:106/2150 train_time:3675ms step_avg:34.67ms
step:107/2150 train_time:3708ms step_avg:34.65ms
step:108/2150 train_time:3741ms step_avg:34.64ms
step:109/2150 train_time:3774ms step_avg:34.63ms
step:110/2150 train_time:3808ms step_avg:34.62ms
step:111/2150 train_time:3841ms step_avg:34.60ms
step:112/2150 train_time:3875ms step_avg:34.59ms
step:113/2150 train_time:3907ms step_avg:34.58ms
step:114/2150 train_time:3941ms step_avg:34.57ms
step:115/2150 train_time:3974ms step_avg:34.56ms
step:116/2150 train_time:4008ms step_avg:34.55ms
step:117/2150 train_time:4040ms step_avg:34.53ms
step:118/2150 train_time:4074ms step_avg:34.52ms
step:119/2150 train_time:4106ms step_avg:34.50ms
step:120/2150 train_time:4139ms step_avg:34.49ms
step:121/2150 train_time:4173ms step_avg:34.48ms
step:122/2150 train_time:4206ms step_avg:34.47ms
step:123/2150 train_time:4239ms step_avg:34.46ms
step:124/2150 train_time:4272ms step_avg:34.45ms
step:125/2150 train_time:4305ms step_avg:34.44ms
step:126/2150 train_time:4339ms step_avg:34.44ms
step:127/2150 train_time:4371ms step_avg:34.42ms
step:128/2150 train_time:4405ms step_avg:34.41ms
step:129/2150 train_time:4438ms step_avg:34.41ms
step:130/2150 train_time:4472ms step_avg:34.40ms
step:131/2150 train_time:4505ms step_avg:34.39ms
step:132/2150 train_time:4538ms step_avg:34.38ms
step:133/2150 train_time:4571ms step_avg:34.37ms
step:134/2150 train_time:4604ms step_avg:34.36ms
step:135/2150 train_time:4637ms step_avg:34.35ms
step:136/2150 train_time:4671ms step_avg:34.34ms
step:137/2150 train_time:4704ms step_avg:34.33ms
step:138/2150 train_time:4737ms step_avg:34.33ms
step:139/2150 train_time:4769ms step_avg:34.31ms
step:140/2150 train_time:4803ms step_avg:34.31ms
step:141/2150 train_time:4836ms step_avg:34.30ms
step:142/2150 train_time:4869ms step_avg:34.29ms
step:143/2150 train_time:4902ms step_avg:34.28ms
step:144/2150 train_time:4936ms step_avg:34.28ms
step:145/2150 train_time:4969ms step_avg:34.27ms
step:146/2150 train_time:5002ms step_avg:34.26ms
step:147/2150 train_time:5035ms step_avg:34.25ms
step:148/2150 train_time:5068ms step_avg:34.24ms
step:149/2150 train_time:5100ms step_avg:34.23ms
step:150/2150 train_time:5134ms step_avg:34.23ms
step:151/2150 train_time:5166ms step_avg:34.22ms
step:152/2150 train_time:5200ms step_avg:34.21ms
step:153/2150 train_time:5233ms step_avg:34.20ms
step:154/2150 train_time:5267ms step_avg:34.20ms
step:155/2150 train_time:5300ms step_avg:34.19ms
step:156/2150 train_time:5333ms step_avg:34.19ms
step:157/2150 train_time:5366ms step_avg:34.18ms
step:158/2150 train_time:5399ms step_avg:34.17ms
step:159/2150 train_time:5432ms step_avg:34.16ms
step:160/2150 train_time:5465ms step_avg:34.16ms
step:161/2150 train_time:5498ms step_avg:34.15ms
step:162/2150 train_time:5531ms step_avg:34.14ms
step:163/2150 train_time:5564ms step_avg:34.14ms
step:164/2150 train_time:5597ms step_avg:34.13ms
step:165/2150 train_time:5631ms step_avg:34.12ms
step:166/2150 train_time:5664ms step_avg:34.12ms
step:167/2150 train_time:5697ms step_avg:34.11ms
step:168/2150 train_time:5731ms step_avg:34.11ms
step:169/2150 train_time:5763ms step_avg:34.10ms
step:170/2150 train_time:5797ms step_avg:34.10ms
step:171/2150 train_time:5830ms step_avg:34.09ms
step:172/2150 train_time:5863ms step_avg:34.09ms
step:173/2150 train_time:5896ms step_avg:34.08ms
step:174/2150 train_time:5929ms step_avg:34.08ms
step:175/2150 train_time:5962ms step_avg:34.07ms
step:176/2150 train_time:5996ms step_avg:34.07ms
step:177/2150 train_time:6028ms step_avg:34.06ms
step:178/2150 train_time:6062ms step_avg:34.05ms
step:179/2150 train_time:6095ms step_avg:34.05ms
step:180/2150 train_time:6128ms step_avg:34.05ms
step:181/2150 train_time:6161ms step_avg:34.04ms
step:182/2150 train_time:6194ms step_avg:34.04ms
step:183/2150 train_time:6227ms step_avg:34.03ms
step:184/2150 train_time:6260ms step_avg:34.02ms
step:185/2150 train_time:6293ms step_avg:34.02ms
step:186/2150 train_time:6327ms step_avg:34.01ms
step:187/2150 train_time:6360ms step_avg:34.01ms
step:188/2150 train_time:6393ms step_avg:34.01ms
step:189/2150 train_time:6426ms step_avg:34.00ms
step:190/2150 train_time:6460ms step_avg:34.00ms
step:191/2150 train_time:6493ms step_avg:33.99ms
step:192/2150 train_time:6526ms step_avg:33.99ms
step:193/2150 train_time:6559ms step_avg:33.98ms
step:194/2150 train_time:6592ms step_avg:33.98ms
step:195/2150 train_time:6625ms step_avg:33.98ms
step:196/2150 train_time:6658ms step_avg:33.97ms
step:197/2150 train_time:6691ms step_avg:33.96ms
step:198/2150 train_time:6724ms step_avg:33.96ms
step:199/2150 train_time:6757ms step_avg:33.96ms
step:200/2150 train_time:6790ms step_avg:33.95ms
step:201/2150 train_time:6823ms step_avg:33.95ms
step:202/2150 train_time:6857ms step_avg:33.94ms
step:203/2150 train_time:6889ms step_avg:33.94ms
step:204/2150 train_time:6923ms step_avg:33.93ms
step:205/2150 train_time:6955ms step_avg:33.93ms
step:206/2150 train_time:6989ms step_avg:33.93ms
step:207/2150 train_time:7022ms step_avg:33.92ms
step:208/2150 train_time:7055ms step_avg:33.92ms
step:209/2150 train_time:7088ms step_avg:33.91ms
step:210/2150 train_time:7121ms step_avg:33.91ms
step:211/2150 train_time:7154ms step_avg:33.90ms
step:212/2150 train_time:7187ms step_avg:33.90ms
step:213/2150 train_time:7220ms step_avg:33.90ms
step:214/2150 train_time:7253ms step_avg:33.89ms
step:215/2150 train_time:7286ms step_avg:33.89ms
step:216/2150 train_time:7319ms step_avg:33.89ms
step:217/2150 train_time:7352ms step_avg:33.88ms
step:218/2150 train_time:7385ms step_avg:33.88ms
step:219/2150 train_time:7418ms step_avg:33.87ms
step:220/2150 train_time:7452ms step_avg:33.87ms
step:221/2150 train_time:7484ms step_avg:33.87ms
step:222/2150 train_time:7518ms step_avg:33.86ms
step:223/2150 train_time:7550ms step_avg:33.86ms
step:224/2150 train_time:7584ms step_avg:33.86ms
step:225/2150 train_time:7616ms step_avg:33.85ms
step:226/2150 train_time:7650ms step_avg:33.85ms
step:227/2150 train_time:7683ms step_avg:33.84ms
step:228/2150 train_time:7716ms step_avg:33.84ms
step:229/2150 train_time:7749ms step_avg:33.84ms
step:230/2150 train_time:7782ms step_avg:33.83ms
step:231/2150 train_time:7814ms step_avg:33.83ms
step:232/2150 train_time:7848ms step_avg:33.83ms
step:233/2150 train_time:7880ms step_avg:33.82ms
step:234/2150 train_time:7914ms step_avg:33.82ms
step:235/2150 train_time:7946ms step_avg:33.81ms
step:236/2150 train_time:7980ms step_avg:33.81ms
step:237/2150 train_time:8012ms step_avg:33.81ms
step:238/2150 train_time:8046ms step_avg:33.81ms
step:239/2150 train_time:8078ms step_avg:33.80ms
step:240/2150 train_time:8112ms step_avg:33.80ms
step:241/2150 train_time:8144ms step_avg:33.79ms
step:242/2150 train_time:8178ms step_avg:33.79ms
step:243/2150 train_time:8210ms step_avg:33.79ms
step:244/2150 train_time:8243ms step_avg:33.78ms
step:245/2150 train_time:8276ms step_avg:33.78ms
step:246/2150 train_time:8309ms step_avg:33.78ms
step:247/2150 train_time:8342ms step_avg:33.77ms
step:248/2150 train_time:8376ms step_avg:33.77ms
step:249/2150 train_time:8408ms step_avg:33.77ms
step:250/2150 train_time:8441ms step_avg:33.76ms
step:250/2150 val_loss:4.3069 train_time:8477ms step_avg:33.91ms
step:251/2150 train_time:8500ms step_avg:33.86ms
step:252/2150 train_time:8522ms step_avg:33.82ms
step:253/2150 train_time:8544ms step_avg:33.77ms
step:254/2150 train_time:8579ms step_avg:33.77ms
step:255/2150 train_time:8615ms step_avg:33.79ms
step:256/2150 train_time:8652ms step_avg:33.80ms
step:257/2150 train_time:8688ms step_avg:33.80ms
step:258/2150 train_time:8721ms step_avg:33.80ms
step:259/2150 train_time:8754ms step_avg:33.80ms
step:260/2150 train_time:8788ms step_avg:33.80ms
step:261/2150 train_time:8820ms step_avg:33.79ms
step:262/2150 train_time:8854ms step_avg:33.79ms
step:263/2150 train_time:8886ms step_avg:33.79ms
step:264/2150 train_time:8919ms step_avg:33.79ms
step:265/2150 train_time:8952ms step_avg:33.78ms
step:266/2150 train_time:8985ms step_avg:33.78ms
step:267/2150 train_time:9018ms step_avg:33.77ms
step:268/2150 train_time:9051ms step_avg:33.77ms
step:269/2150 train_time:9083ms step_avg:33.77ms
step:270/2150 train_time:9116ms step_avg:33.76ms
step:271/2150 train_time:9149ms step_avg:33.76ms
step:272/2150 train_time:9182ms step_avg:33.76ms
step:273/2150 train_time:9215ms step_avg:33.75ms
step:274/2150 train_time:9248ms step_avg:33.75ms
step:275/2150 train_time:9281ms step_avg:33.75ms
step:276/2150 train_time:9314ms step_avg:33.75ms
step:277/2150 train_time:9347ms step_avg:33.74ms
step:278/2150 train_time:9380ms step_avg:33.74ms
step:279/2150 train_time:9413ms step_avg:33.74ms
step:280/2150 train_time:9446ms step_avg:33.74ms
step:281/2150 train_time:9479ms step_avg:33.73ms
step:282/2150 train_time:9512ms step_avg:33.73ms
step:283/2150 train_time:9545ms step_avg:33.73ms
step:284/2150 train_time:9579ms step_avg:33.73ms
step:285/2150 train_time:9613ms step_avg:33.73ms
step:286/2150 train_time:9648ms step_avg:33.73ms
step:287/2150 train_time:9681ms step_avg:33.73ms
step:288/2150 train_time:9715ms step_avg:33.73ms
step:289/2150 train_time:9749ms step_avg:33.73ms
step:290/2150 train_time:9782ms step_avg:33.73ms
step:291/2150 train_time:9815ms step_avg:33.73ms
step:292/2150 train_time:9848ms step_avg:33.73ms
step:293/2150 train_time:9882ms step_avg:33.73ms
step:294/2150 train_time:9915ms step_avg:33.73ms
step:295/2150 train_time:9948ms step_avg:33.72ms
step:296/2150 train_time:9981ms step_avg:33.72ms
step:297/2150 train_time:10013ms step_avg:33.72ms
step:298/2150 train_time:10047ms step_avg:33.71ms
step:299/2150 train_time:10079ms step_avg:33.71ms
step:300/2150 train_time:10113ms step_avg:33.71ms
step:301/2150 train_time:10145ms step_avg:33.70ms
step:302/2150 train_time:10178ms step_avg:33.70ms
step:303/2150 train_time:10211ms step_avg:33.70ms
step:304/2150 train_time:10244ms step_avg:33.70ms
step:305/2150 train_time:10277ms step_avg:33.69ms
step:306/2150 train_time:10310ms step_avg:33.69ms
step:307/2150 train_time:10343ms step_avg:33.69ms
step:308/2150 train_time:10376ms step_avg:33.69ms
step:309/2150 train_time:10408ms step_avg:33.68ms
step:310/2150 train_time:10442ms step_avg:33.68ms
step:311/2150 train_time:10474ms step_avg:33.68ms
step:312/2150 train_time:10508ms step_avg:33.68ms
step:313/2150 train_time:10541ms step_avg:33.68ms
step:314/2150 train_time:10574ms step_avg:33.68ms
step:315/2150 train_time:10607ms step_avg:33.67ms
step:316/2150 train_time:10641ms step_avg:33.67ms
step:317/2150 train_time:10674ms step_avg:33.67ms
step:318/2150 train_time:10707ms step_avg:33.67ms
step:319/2150 train_time:10741ms step_avg:33.67ms
step:320/2150 train_time:10774ms step_avg:33.67ms
step:321/2150 train_time:10808ms step_avg:33.67ms
step:322/2150 train_time:10841ms step_avg:33.67ms
step:323/2150 train_time:10874ms step_avg:33.67ms
step:324/2150 train_time:10908ms step_avg:33.67ms
step:325/2150 train_time:10941ms step_avg:33.66ms
step:326/2150 train_time:10975ms step_avg:33.66ms
step:327/2150 train_time:11007ms step_avg:33.66ms
step:328/2150 train_time:11040ms step_avg:33.66ms
step:329/2150 train_time:11073ms step_avg:33.66ms
step:330/2150 train_time:11106ms step_avg:33.66ms
step:331/2150 train_time:11139ms step_avg:33.65ms
step:332/2150 train_time:11173ms step_avg:33.65ms
step:333/2150 train_time:11205ms step_avg:33.65ms
step:334/2150 train_time:11238ms step_avg:33.65ms
step:335/2150 train_time:11271ms step_avg:33.64ms
step:336/2150 train_time:11304ms step_avg:33.64ms
step:337/2150 train_time:11337ms step_avg:33.64ms
step:338/2150 train_time:11371ms step_avg:33.64ms
step:339/2150 train_time:11403ms step_avg:33.64ms
step:340/2150 train_time:11436ms step_avg:33.64ms
step:341/2150 train_time:11469ms step_avg:33.63ms
step:342/2150 train_time:11502ms step_avg:33.63ms
step:343/2150 train_time:11535ms step_avg:33.63ms
step:344/2150 train_time:11569ms step_avg:33.63ms
step:345/2150 train_time:11601ms step_avg:33.63ms
step:346/2150 train_time:11634ms step_avg:33.63ms
step:347/2150 train_time:11668ms step_avg:33.63ms
step:348/2150 train_time:11701ms step_avg:33.62ms
step:349/2150 train_time:11734ms step_avg:33.62ms
step:350/2150 train_time:11768ms step_avg:33.62ms
step:351/2150 train_time:11801ms step_avg:33.62ms
step:352/2150 train_time:11834ms step_avg:33.62ms
step:353/2150 train_time:11868ms step_avg:33.62ms
step:354/2150 train_time:11901ms step_avg:33.62ms
step:355/2150 train_time:11934ms step_avg:33.62ms
step:356/2150 train_time:11967ms step_avg:33.62ms
step:357/2150 train_time:12000ms step_avg:33.61ms
step:358/2150 train_time:12034ms step_avg:33.61ms
step:359/2150 train_time:12066ms step_avg:33.61ms
step:360/2150 train_time:12100ms step_avg:33.61ms
step:361/2150 train_time:12133ms step_avg:33.61ms
step:362/2150 train_time:12166ms step_avg:33.61ms
step:363/2150 train_time:12199ms step_avg:33.61ms
step:364/2150 train_time:12232ms step_avg:33.61ms
step:365/2150 train_time:12265ms step_avg:33.60ms
step:366/2150 train_time:12298ms step_avg:33.60ms
step:367/2150 train_time:12331ms step_avg:33.60ms
step:368/2150 train_time:12364ms step_avg:33.60ms
step:369/2150 train_time:12399ms step_avg:33.60ms
step:370/2150 train_time:12430ms step_avg:33.60ms
step:371/2150 train_time:12463ms step_avg:33.59ms
step:372/2150 train_time:12497ms step_avg:33.59ms
step:373/2150 train_time:12529ms step_avg:33.59ms
step:374/2150 train_time:12562ms step_avg:33.59ms
step:375/2150 train_time:12595ms step_avg:33.59ms
step:376/2150 train_time:12629ms step_avg:33.59ms
step:377/2150 train_time:12661ms step_avg:33.58ms
step:378/2150 train_time:12695ms step_avg:33.58ms
step:379/2150 train_time:12728ms step_avg:33.58ms
step:380/2150 train_time:12761ms step_avg:33.58ms
step:381/2150 train_time:12794ms step_avg:33.58ms
step:382/2150 train_time:12827ms step_avg:33.58ms
step:383/2150 train_time:12860ms step_avg:33.58ms
step:384/2150 train_time:12894ms step_avg:33.58ms
step:385/2150 train_time:12926ms step_avg:33.57ms
step:386/2150 train_time:12960ms step_avg:33.57ms
step:387/2150 train_time:12992ms step_avg:33.57ms
step:388/2150 train_time:13026ms step_avg:33.57ms
step:389/2150 train_time:13059ms step_avg:33.57ms
step:390/2150 train_time:13093ms step_avg:33.57ms
step:391/2150 train_time:13125ms step_avg:33.57ms
step:392/2150 train_time:13159ms step_avg:33.57ms
step:393/2150 train_time:13192ms step_avg:33.57ms
step:394/2150 train_time:13225ms step_avg:33.57ms
step:395/2150 train_time:13258ms step_avg:33.56ms
step:396/2150 train_time:13291ms step_avg:33.56ms
step:397/2150 train_time:13324ms step_avg:33.56ms
step:398/2150 train_time:13357ms step_avg:33.56ms
step:399/2150 train_time:13390ms step_avg:33.56ms
step:400/2150 train_time:13423ms step_avg:33.56ms
step:401/2150 train_time:13456ms step_avg:33.56ms
step:402/2150 train_time:13490ms step_avg:33.56ms
step:403/2150 train_time:13523ms step_avg:33.55ms
step:404/2150 train_time:13556ms step_avg:33.55ms
step:405/2150 train_time:13588ms step_avg:33.55ms
step:406/2150 train_time:13622ms step_avg:33.55ms
step:407/2150 train_time:13655ms step_avg:33.55ms
step:408/2150 train_time:13688ms step_avg:33.55ms
step:409/2150 train_time:13721ms step_avg:33.55ms
step:410/2150 train_time:13755ms step_avg:33.55ms
step:411/2150 train_time:13789ms step_avg:33.55ms
step:412/2150 train_time:13821ms step_avg:33.55ms
step:413/2150 train_time:13854ms step_avg:33.54ms
step:414/2150 train_time:13887ms step_avg:33.54ms
step:415/2150 train_time:13920ms step_avg:33.54ms
step:416/2150 train_time:13954ms step_avg:33.54ms
step:417/2150 train_time:13987ms step_avg:33.54ms
step:418/2150 train_time:14020ms step_avg:33.54ms
step:419/2150 train_time:14053ms step_avg:33.54ms
step:420/2150 train_time:14086ms step_avg:33.54ms
step:421/2150 train_time:14119ms step_avg:33.54ms
step:422/2150 train_time:14152ms step_avg:33.54ms
step:423/2150 train_time:14185ms step_avg:33.54ms
step:424/2150 train_time:14219ms step_avg:33.53ms
step:425/2150 train_time:14251ms step_avg:33.53ms
step:426/2150 train_time:14284ms step_avg:33.53ms
step:427/2150 train_time:14317ms step_avg:33.53ms
step:428/2150 train_time:14350ms step_avg:33.53ms
step:429/2150 train_time:14383ms step_avg:33.53ms
step:430/2150 train_time:14417ms step_avg:33.53ms
step:431/2150 train_time:14449ms step_avg:33.53ms
step:432/2150 train_time:14483ms step_avg:33.52ms
step:433/2150 train_time:14516ms step_avg:33.52ms
step:434/2150 train_time:14549ms step_avg:33.52ms
step:435/2150 train_time:14582ms step_avg:33.52ms
step:436/2150 train_time:14615ms step_avg:33.52ms
step:437/2150 train_time:14648ms step_avg:33.52ms
step:438/2150 train_time:14682ms step_avg:33.52ms
step:439/2150 train_time:14715ms step_avg:33.52ms
step:440/2150 train_time:14748ms step_avg:33.52ms
step:441/2150 train_time:14781ms step_avg:33.52ms
step:442/2150 train_time:14814ms step_avg:33.52ms
step:443/2150 train_time:14848ms step_avg:33.52ms
step:444/2150 train_time:14881ms step_avg:33.52ms
step:445/2150 train_time:14914ms step_avg:33.51ms
step:446/2150 train_time:14947ms step_avg:33.51ms
step:447/2150 train_time:14980ms step_avg:33.51ms
step:448/2150 train_time:15014ms step_avg:33.51ms
step:449/2150 train_time:15047ms step_avg:33.51ms
step:450/2150 train_time:15080ms step_avg:33.51ms
step:451/2150 train_time:15113ms step_avg:33.51ms
step:452/2150 train_time:15146ms step_avg:33.51ms
step:453/2150 train_time:15179ms step_avg:33.51ms
step:454/2150 train_time:15213ms step_avg:33.51ms
step:455/2150 train_time:15245ms step_avg:33.51ms
step:456/2150 train_time:15279ms step_avg:33.51ms
step:457/2150 train_time:15311ms step_avg:33.50ms
step:458/2150 train_time:15345ms step_avg:33.50ms
step:459/2150 train_time:15378ms step_avg:33.50ms
step:460/2150 train_time:15411ms step_avg:33.50ms
step:461/2150 train_time:15444ms step_avg:33.50ms
step:462/2150 train_time:15477ms step_avg:33.50ms
step:463/2150 train_time:15510ms step_avg:33.50ms
step:464/2150 train_time:15543ms step_avg:33.50ms
step:465/2150 train_time:15575ms step_avg:33.50ms
step:466/2150 train_time:15609ms step_avg:33.49ms
step:467/2150 train_time:15641ms step_avg:33.49ms
step:468/2150 train_time:15674ms step_avg:33.49ms
step:469/2150 train_time:15707ms step_avg:33.49ms
step:470/2150 train_time:15741ms step_avg:33.49ms
step:471/2150 train_time:15773ms step_avg:33.49ms
step:472/2150 train_time:15807ms step_avg:33.49ms
step:473/2150 train_time:15839ms step_avg:33.49ms
step:474/2150 train_time:15873ms step_avg:33.49ms
step:475/2150 train_time:15905ms step_avg:33.49ms
step:476/2150 train_time:15939ms step_avg:33.49ms
step:477/2150 train_time:15971ms step_avg:33.48ms
step:478/2150 train_time:16004ms step_avg:33.48ms
step:479/2150 train_time:16038ms step_avg:33.48ms
step:480/2150 train_time:16071ms step_avg:33.48ms
step:481/2150 train_time:16104ms step_avg:33.48ms
step:482/2150 train_time:16138ms step_avg:33.48ms
step:483/2150 train_time:16170ms step_avg:33.48ms
step:484/2150 train_time:16204ms step_avg:33.48ms
step:485/2150 train_time:16236ms step_avg:33.48ms
step:486/2150 train_time:16270ms step_avg:33.48ms
step:487/2150 train_time:16303ms step_avg:33.48ms
step:488/2150 train_time:16336ms step_avg:33.48ms
step:489/2150 train_time:16369ms step_avg:33.47ms
step:490/2150 train_time:16402ms step_avg:33.47ms
step:491/2150 train_time:16435ms step_avg:33.47ms
step:492/2150 train_time:16469ms step_avg:33.47ms
step:493/2150 train_time:16501ms step_avg:33.47ms
step:494/2150 train_time:16535ms step_avg:33.47ms
step:495/2150 train_time:16567ms step_avg:33.47ms
step:496/2150 train_time:16601ms step_avg:33.47ms
step:497/2150 train_time:16633ms step_avg:33.47ms
step:498/2150 train_time:16667ms step_avg:33.47ms
step:499/2150 train_time:16700ms step_avg:33.47ms
step:500/2150 train_time:16733ms step_avg:33.47ms
step:500/2150 val_loss:4.0207 train_time:16769ms step_avg:33.54ms
step:501/2150 train_time:16791ms step_avg:33.52ms
step:502/2150 train_time:16813ms step_avg:33.49ms
step:503/2150 train_time:16835ms step_avg:33.47ms
step:504/2150 train_time:16868ms step_avg:33.47ms
step:505/2150 train_time:16903ms step_avg:33.47ms
step:506/2150 train_time:16937ms step_avg:33.47ms
step:507/2150 train_time:16971ms step_avg:33.47ms
step:508/2150 train_time:17004ms step_avg:33.47ms
step:509/2150 train_time:17037ms step_avg:33.47ms
step:510/2150 train_time:17071ms step_avg:33.47ms
step:511/2150 train_time:17103ms step_avg:33.47ms
step:512/2150 train_time:17136ms step_avg:33.47ms
step:513/2150 train_time:17169ms step_avg:33.47ms
step:514/2150 train_time:17202ms step_avg:33.47ms
step:515/2150 train_time:17235ms step_avg:33.47ms
step:516/2150 train_time:17268ms step_avg:33.47ms
step:517/2150 train_time:17300ms step_avg:33.46ms
step:518/2150 train_time:17333ms step_avg:33.46ms
step:519/2150 train_time:17366ms step_avg:33.46ms
step:520/2150 train_time:17399ms step_avg:33.46ms
step:521/2150 train_time:17432ms step_avg:33.46ms
step:522/2150 train_time:17465ms step_avg:33.46ms
step:523/2150 train_time:17498ms step_avg:33.46ms
step:524/2150 train_time:17532ms step_avg:33.46ms
step:525/2150 train_time:17565ms step_avg:33.46ms
step:526/2150 train_time:17598ms step_avg:33.46ms
step:527/2150 train_time:17631ms step_avg:33.46ms
step:528/2150 train_time:17664ms step_avg:33.45ms
step:529/2150 train_time:17697ms step_avg:33.45ms
step:530/2150 train_time:17731ms step_avg:33.45ms
step:531/2150 train_time:17764ms step_avg:33.45ms
step:532/2150 train_time:17798ms step_avg:33.45ms
step:533/2150 train_time:17831ms step_avg:33.45ms
step:534/2150 train_time:17865ms step_avg:33.45ms
step:535/2150 train_time:17898ms step_avg:33.45ms
step:536/2150 train_time:17932ms step_avg:33.45ms
step:537/2150 train_time:17965ms step_avg:33.45ms
step:538/2150 train_time:17998ms step_avg:33.45ms
step:539/2150 train_time:18031ms step_avg:33.45ms
step:540/2150 train_time:18064ms step_avg:33.45ms
step:541/2150 train_time:18097ms step_avg:33.45ms
step:542/2150 train_time:18131ms step_avg:33.45ms
step:543/2150 train_time:18164ms step_avg:33.45ms
step:544/2150 train_time:18197ms step_avg:33.45ms
step:545/2150 train_time:18230ms step_avg:33.45ms
step:546/2150 train_time:18263ms step_avg:33.45ms
step:547/2150 train_time:18295ms step_avg:33.45ms
step:548/2150 train_time:18329ms step_avg:33.45ms
step:549/2150 train_time:18362ms step_avg:33.45ms
step:550/2150 train_time:18395ms step_avg:33.44ms
step:551/2150 train_time:18428ms step_avg:33.44ms
step:552/2150 train_time:18461ms step_avg:33.44ms
step:553/2150 train_time:18494ms step_avg:33.44ms
step:554/2150 train_time:18527ms step_avg:33.44ms
step:555/2150 train_time:18560ms step_avg:33.44ms
step:556/2150 train_time:18593ms step_avg:33.44ms
step:557/2150 train_time:18626ms step_avg:33.44ms
step:558/2150 train_time:18659ms step_avg:33.44ms
step:559/2150 train_time:18692ms step_avg:33.44ms
step:560/2150 train_time:18725ms step_avg:33.44ms
step:561/2150 train_time:18759ms step_avg:33.44ms
step:562/2150 train_time:18792ms step_avg:33.44ms
step:563/2150 train_time:18825ms step_avg:33.44ms
step:564/2150 train_time:18858ms step_avg:33.44ms
step:565/2150 train_time:18892ms step_avg:33.44ms
step:566/2150 train_time:18925ms step_avg:33.44ms
step:567/2150 train_time:18958ms step_avg:33.44ms
step:568/2150 train_time:18991ms step_avg:33.44ms
step:569/2150 train_time:19025ms step_avg:33.44ms
step:570/2150 train_time:19058ms step_avg:33.44ms
step:571/2150 train_time:19091ms step_avg:33.43ms
step:572/2150 train_time:19124ms step_avg:33.43ms
step:573/2150 train_time:19158ms step_avg:33.43ms
step:574/2150 train_time:19191ms step_avg:33.43ms
step:575/2150 train_time:19224ms step_avg:33.43ms
step:576/2150 train_time:19257ms step_avg:33.43ms
step:577/2150 train_time:19290ms step_avg:33.43ms
step:578/2150 train_time:19323ms step_avg:33.43ms
step:579/2150 train_time:19356ms step_avg:33.43ms
step:580/2150 train_time:19389ms step_avg:33.43ms
step:581/2150 train_time:19422ms step_avg:33.43ms
step:582/2150 train_time:19455ms step_avg:33.43ms
step:583/2150 train_time:19488ms step_avg:33.43ms
step:584/2150 train_time:19522ms step_avg:33.43ms
step:585/2150 train_time:19554ms step_avg:33.43ms
step:586/2150 train_time:19588ms step_avg:33.43ms
step:587/2150 train_time:19620ms step_avg:33.42ms
step:588/2150 train_time:19653ms step_avg:33.42ms
step:589/2150 train_time:19686ms step_avg:33.42ms
step:590/2150 train_time:19719ms step_avg:33.42ms
step:591/2150 train_time:19752ms step_avg:33.42ms
step:592/2150 train_time:19786ms step_avg:33.42ms
step:593/2150 train_time:19819ms step_avg:33.42ms
step:594/2150 train_time:19852ms step_avg:33.42ms
step:595/2150 train_time:19886ms step_avg:33.42ms
step:596/2150 train_time:19919ms step_avg:33.42ms
step:597/2150 train_time:19952ms step_avg:33.42ms
step:598/2150 train_time:19986ms step_avg:33.42ms
step:599/2150 train_time:20019ms step_avg:33.42ms
step:600/2150 train_time:20052ms step_avg:33.42ms
step:601/2150 train_time:20085ms step_avg:33.42ms
step:602/2150 train_time:20118ms step_avg:33.42ms
step:603/2150 train_time:20153ms step_avg:33.42ms
step:604/2150 train_time:20184ms step_avg:33.42ms
step:605/2150 train_time:20216ms step_avg:33.42ms
step:606/2150 train_time:20250ms step_avg:33.42ms
step:607/2150 train_time:20283ms step_avg:33.41ms
step:608/2150 train_time:20316ms step_avg:33.41ms
step:609/2150 train_time:20349ms step_avg:33.41ms
step:610/2150 train_time:20382ms step_avg:33.41ms
step:611/2150 train_time:20415ms step_avg:33.41ms
step:612/2150 train_time:20448ms step_avg:33.41ms
step:613/2150 train_time:20481ms step_avg:33.41ms
step:614/2150 train_time:20515ms step_avg:33.41ms
step:615/2150 train_time:20547ms step_avg:33.41ms
step:616/2150 train_time:20581ms step_avg:33.41ms
step:617/2150 train_time:20613ms step_avg:33.41ms
step:618/2150 train_time:20647ms step_avg:33.41ms
step:619/2150 train_time:20680ms step_avg:33.41ms
step:620/2150 train_time:20713ms step_avg:33.41ms
step:621/2150 train_time:20746ms step_avg:33.41ms
step:622/2150 train_time:20779ms step_avg:33.41ms
step:623/2150 train_time:20812ms step_avg:33.41ms
step:624/2150 train_time:20846ms step_avg:33.41ms
step:625/2150 train_time:20879ms step_avg:33.41ms
step:626/2150 train_time:20912ms step_avg:33.41ms
step:627/2150 train_time:20945ms step_avg:33.41ms
step:628/2150 train_time:20978ms step_avg:33.41ms
step:629/2150 train_time:21011ms step_avg:33.40ms
step:630/2150 train_time:21045ms step_avg:33.40ms
step:631/2150 train_time:21078ms step_avg:33.40ms
step:632/2150 train_time:21111ms step_avg:33.40ms
step:633/2150 train_time:21144ms step_avg:33.40ms
step:634/2150 train_time:21177ms step_avg:33.40ms
step:635/2150 train_time:21210ms step_avg:33.40ms
step:636/2150 train_time:21244ms step_avg:33.40ms
step:637/2150 train_time:21277ms step_avg:33.40ms
step:638/2150 train_time:21310ms step_avg:33.40ms
step:639/2150 train_time:21344ms step_avg:33.40ms
step:640/2150 train_time:21377ms step_avg:33.40ms
step:641/2150 train_time:21410ms step_avg:33.40ms
step:642/2150 train_time:21443ms step_avg:33.40ms
step:643/2150 train_time:21476ms step_avg:33.40ms
step:644/2150 train_time:21509ms step_avg:33.40ms
step:645/2150 train_time:21544ms step_avg:33.40ms
step:646/2150 train_time:21575ms step_avg:33.40ms
step:647/2150 train_time:21608ms step_avg:33.40ms
step:648/2150 train_time:21642ms step_avg:33.40ms
step:649/2150 train_time:21674ms step_avg:33.40ms
step:650/2150 train_time:21708ms step_avg:33.40ms
step:651/2150 train_time:21740ms step_avg:33.40ms
step:652/2150 train_time:21774ms step_avg:33.39ms
step:653/2150 train_time:21807ms step_avg:33.39ms
step:654/2150 train_time:21840ms step_avg:33.39ms
step:655/2150 train_time:21873ms step_avg:33.39ms
step:656/2150 train_time:21906ms step_avg:33.39ms
step:657/2150 train_time:21940ms step_avg:33.39ms
step:658/2150 train_time:21973ms step_avg:33.39ms
step:659/2150 train_time:22006ms step_avg:33.39ms
step:660/2150 train_time:22039ms step_avg:33.39ms
step:661/2150 train_time:22072ms step_avg:33.39ms
step:662/2150 train_time:22105ms step_avg:33.39ms
step:663/2150 train_time:22138ms step_avg:33.39ms
step:664/2150 train_time:22172ms step_avg:33.39ms
step:665/2150 train_time:22204ms step_avg:33.39ms
step:666/2150 train_time:22238ms step_avg:33.39ms
step:667/2150 train_time:22271ms step_avg:33.39ms
step:668/2150 train_time:22304ms step_avg:33.39ms
step:669/2150 train_time:22337ms step_avg:33.39ms
step:670/2150 train_time:22370ms step_avg:33.39ms
step:671/2150 train_time:22403ms step_avg:33.39ms
step:672/2150 train_time:22437ms step_avg:33.39ms
step:673/2150 train_time:22469ms step_avg:33.39ms
step:674/2150 train_time:22503ms step_avg:33.39ms
step:675/2150 train_time:22535ms step_avg:33.39ms
step:676/2150 train_time:22569ms step_avg:33.39ms
step:677/2150 train_time:22601ms step_avg:33.38ms
step:678/2150 train_time:22635ms step_avg:33.38ms
step:679/2150 train_time:22667ms step_avg:33.38ms
step:680/2150 train_time:22701ms step_avg:33.38ms
step:681/2150 train_time:22734ms step_avg:33.38ms
step:682/2150 train_time:22767ms step_avg:33.38ms
step:683/2150 train_time:22800ms step_avg:33.38ms
step:684/2150 train_time:22833ms step_avg:33.38ms
step:685/2150 train_time:22866ms step_avg:33.38ms
step:686/2150 train_time:22900ms step_avg:33.38ms
step:687/2150 train_time:22933ms step_avg:33.38ms
step:688/2150 train_time:22967ms step_avg:33.38ms
step:689/2150 train_time:22999ms step_avg:33.38ms
step:690/2150 train_time:23032ms step_avg:33.38ms
step:691/2150 train_time:23065ms step_avg:33.38ms
step:692/2150 train_time:23098ms step_avg:33.38ms
step:693/2150 train_time:23131ms step_avg:33.38ms
step:694/2150 train_time:23165ms step_avg:33.38ms
step:695/2150 train_time:23197ms step_avg:33.38ms
step:696/2150 train_time:23231ms step_avg:33.38ms
step:697/2150 train_time:23264ms step_avg:33.38ms
step:698/2150 train_time:23297ms step_avg:33.38ms
step:699/2150 train_time:23330ms step_avg:33.38ms
step:700/2150 train_time:23363ms step_avg:33.38ms
step:701/2150 train_time:23396ms step_avg:33.38ms
step:702/2150 train_time:23429ms step_avg:33.38ms
step:703/2150 train_time:23462ms step_avg:33.37ms
step:704/2150 train_time:23496ms step_avg:33.37ms
step:705/2150 train_time:23529ms step_avg:33.38ms
step:706/2150 train_time:23588ms step_avg:33.41ms
step:707/2150 train_time:23649ms step_avg:33.45ms
step:708/2150 train_time:23708ms step_avg:33.49ms
step:709/2150 train_time:23769ms step_avg:33.52ms
step:710/2150 train_time:23829ms step_avg:33.56ms
step:711/2150 train_time:23890ms step_avg:33.60ms
step:712/2150 train_time:23949ms step_avg:33.64ms
step:713/2150 train_time:24010ms step_avg:33.67ms
step:714/2150 train_time:24069ms step_avg:33.71ms
step:715/2150 train_time:24130ms step_avg:33.75ms
step:716/2150 train_time:24190ms step_avg:33.78ms
step:717/2150 train_time:24251ms step_avg:33.82ms
step:718/2150 train_time:24311ms step_avg:33.86ms
step:719/2150 train_time:24372ms step_avg:33.90ms
step:720/2150 train_time:24431ms step_avg:33.93ms
step:721/2150 train_time:24492ms step_avg:33.97ms
step:722/2150 train_time:24552ms step_avg:34.01ms
step:723/2150 train_time:24613ms step_avg:34.04ms
step:724/2150 train_time:24672ms step_avg:34.08ms
step:725/2150 train_time:24733ms step_avg:34.11ms
step:726/2150 train_time:24792ms step_avg:34.15ms
step:727/2150 train_time:24854ms step_avg:34.19ms
step:728/2150 train_time:24914ms step_avg:34.22ms
step:729/2150 train_time:24975ms step_avg:34.26ms
step:730/2150 train_time:25035ms step_avg:34.29ms
step:731/2150 train_time:25096ms step_avg:34.33ms
step:732/2150 train_time:25156ms step_avg:34.37ms
step:733/2150 train_time:25217ms step_avg:34.40ms
step:734/2150 train_time:25276ms step_avg:34.44ms
step:735/2150 train_time:25337ms step_avg:34.47ms
step:736/2150 train_time:25397ms step_avg:34.51ms
step:737/2150 train_time:25458ms step_avg:34.54ms
step:738/2150 train_time:25517ms step_avg:34.58ms
step:739/2150 train_time:25579ms step_avg:34.61ms
step:740/2150 train_time:25638ms step_avg:34.65ms
step:741/2150 train_time:25699ms step_avg:34.68ms
step:742/2150 train_time:25759ms step_avg:34.72ms
step:743/2150 train_time:25821ms step_avg:34.75ms
step:744/2150 train_time:25880ms step_avg:34.79ms
step:745/2150 train_time:25941ms step_avg:34.82ms
step:746/2150 train_time:26001ms step_avg:34.85ms
step:747/2150 train_time:26063ms step_avg:34.89ms
step:748/2150 train_time:26122ms step_avg:34.92ms
step:749/2150 train_time:26184ms step_avg:34.96ms
step:750/2150 train_time:26243ms step_avg:34.99ms
step:750/2150 val_loss:3.8695 train_time:26306ms step_avg:35.08ms
step:751/2150 train_time:26329ms step_avg:35.06ms
step:752/2150 train_time:26364ms step_avg:35.06ms
step:753/2150 train_time:26426ms step_avg:35.09ms
step:754/2150 train_time:26492ms step_avg:35.14ms
step:755/2150 train_time:26554ms step_avg:35.17ms
step:756/2150 train_time:26613ms step_avg:35.20ms
step:757/2150 train_time:26673ms step_avg:35.23ms
step:758/2150 train_time:26731ms step_avg:35.27ms
step:759/2150 train_time:26791ms step_avg:35.30ms
step:760/2150 train_time:26849ms step_avg:35.33ms
step:761/2150 train_time:26910ms step_avg:35.36ms
step:762/2150 train_time:26969ms step_avg:35.39ms
step:763/2150 train_time:27029ms step_avg:35.42ms
step:764/2150 train_time:27087ms step_avg:35.45ms
step:765/2150 train_time:27148ms step_avg:35.49ms
step:766/2150 train_time:27214ms step_avg:35.53ms
step:767/2150 train_time:27281ms step_avg:35.57ms
step:768/2150 train_time:27341ms step_avg:35.60ms
step:769/2150 train_time:27403ms step_avg:35.63ms
step:770/2150 train_time:27463ms step_avg:35.67ms
step:771/2150 train_time:27525ms step_avg:35.70ms
step:772/2150 train_time:27585ms step_avg:35.73ms
step:773/2150 train_time:27645ms step_avg:35.76ms
step:774/2150 train_time:27704ms step_avg:35.79ms
step:775/2150 train_time:27764ms step_avg:35.82ms
step:776/2150 train_time:27823ms step_avg:35.85ms
step:777/2150 train_time:27884ms step_avg:35.89ms
step:778/2150 train_time:27942ms step_avg:35.92ms
step:779/2150 train_time:28003ms step_avg:35.95ms
step:780/2150 train_time:28061ms step_avg:35.98ms
step:781/2150 train_time:28123ms step_avg:36.01ms
step:782/2150 train_time:28185ms step_avg:36.04ms
step:783/2150 train_time:28248ms step_avg:36.08ms
step:784/2150 train_time:28310ms step_avg:36.11ms
step:785/2150 train_time:28372ms step_avg:36.14ms
step:786/2150 train_time:28431ms step_avg:36.17ms
step:787/2150 train_time:28493ms step_avg:36.20ms
step:788/2150 train_time:28553ms step_avg:36.23ms
step:789/2150 train_time:28614ms step_avg:36.27ms
step:790/2150 train_time:28673ms step_avg:36.29ms
step:791/2150 train_time:28734ms step_avg:36.33ms
step:792/2150 train_time:28793ms step_avg:36.36ms
step:793/2150 train_time:28854ms step_avg:36.39ms
step:794/2150 train_time:28913ms step_avg:36.41ms
step:795/2150 train_time:28974ms step_avg:36.45ms
step:796/2150 train_time:29033ms step_avg:36.47ms
step:797/2150 train_time:29094ms step_avg:36.50ms
step:798/2150 train_time:29154ms step_avg:36.53ms
step:799/2150 train_time:29216ms step_avg:36.57ms
step:800/2150 train_time:29276ms step_avg:36.60ms
step:801/2150 train_time:29338ms step_avg:36.63ms
step:802/2150 train_time:29398ms step_avg:36.66ms
step:803/2150 train_time:29459ms step_avg:36.69ms
step:804/2150 train_time:29519ms step_avg:36.71ms
step:805/2150 train_time:29580ms step_avg:36.75ms
step:806/2150 train_time:29640ms step_avg:36.77ms
step:807/2150 train_time:29701ms step_avg:36.80ms
step:808/2150 train_time:29760ms step_avg:36.83ms
step:809/2150 train_time:29822ms step_avg:36.86ms
step:810/2150 train_time:29881ms step_avg:36.89ms
step:811/2150 train_time:29941ms step_avg:36.92ms
step:812/2150 train_time:30000ms step_avg:36.95ms
step:813/2150 train_time:30062ms step_avg:36.98ms
step:814/2150 train_time:30121ms step_avg:37.00ms
step:815/2150 train_time:30182ms step_avg:37.03ms
step:816/2150 train_time:30242ms step_avg:37.06ms
step:817/2150 train_time:30303ms step_avg:37.09ms
step:818/2150 train_time:30362ms step_avg:37.12ms
step:819/2150 train_time:30424ms step_avg:37.15ms
step:820/2150 train_time:30483ms step_avg:37.17ms
step:821/2150 train_time:30546ms step_avg:37.21ms
step:822/2150 train_time:30606ms step_avg:37.23ms
step:823/2150 train_time:30668ms step_avg:37.26ms
step:824/2150 train_time:30727ms step_avg:37.29ms
step:825/2150 train_time:30788ms step_avg:37.32ms
step:826/2150 train_time:30847ms step_avg:37.34ms
step:827/2150 train_time:30908ms step_avg:37.37ms
step:828/2150 train_time:30966ms step_avg:37.40ms
step:829/2150 train_time:31027ms step_avg:37.43ms
step:830/2150 train_time:31086ms step_avg:37.45ms
step:831/2150 train_time:31147ms step_avg:37.48ms
step:832/2150 train_time:31207ms step_avg:37.51ms
step:833/2150 train_time:31268ms step_avg:37.54ms
step:834/2150 train_time:31327ms step_avg:37.56ms
step:835/2150 train_time:31388ms step_avg:37.59ms
step:836/2150 train_time:31448ms step_avg:37.62ms
step:837/2150 train_time:31509ms step_avg:37.65ms
step:838/2150 train_time:31569ms step_avg:37.67ms
step:839/2150 train_time:31631ms step_avg:37.70ms
step:840/2150 train_time:31691ms step_avg:37.73ms
step:841/2150 train_time:31752ms step_avg:37.76ms
step:842/2150 train_time:31811ms step_avg:37.78ms
step:843/2150 train_time:31872ms step_avg:37.81ms
step:844/2150 train_time:31931ms step_avg:37.83ms
step:845/2150 train_time:31992ms step_avg:37.86ms
step:846/2150 train_time:32052ms step_avg:37.89ms
step:847/2150 train_time:32113ms step_avg:37.91ms
step:848/2150 train_time:32172ms step_avg:37.94ms
step:849/2150 train_time:32233ms step_avg:37.97ms
step:850/2150 train_time:32293ms step_avg:37.99ms
step:851/2150 train_time:32354ms step_avg:38.02ms
step:852/2150 train_time:32413ms step_avg:38.04ms
step:853/2150 train_time:32475ms step_avg:38.07ms
step:854/2150 train_time:32534ms step_avg:38.10ms
step:855/2150 train_time:32595ms step_avg:38.12ms
step:856/2150 train_time:32655ms step_avg:38.15ms
step:857/2150 train_time:32715ms step_avg:38.17ms
step:858/2150 train_time:32775ms step_avg:38.20ms
step:859/2150 train_time:32835ms step_avg:38.23ms
step:860/2150 train_time:32895ms step_avg:38.25ms
step:861/2150 train_time:32955ms step_avg:38.28ms
step:862/2150 train_time:33014ms step_avg:38.30ms
step:863/2150 train_time:33075ms step_avg:38.33ms
step:864/2150 train_time:33135ms step_avg:38.35ms
step:865/2150 train_time:33195ms step_avg:38.38ms
step:866/2150 train_time:33255ms step_avg:38.40ms
step:867/2150 train_time:33316ms step_avg:38.43ms
step:868/2150 train_time:33375ms step_avg:38.45ms
step:869/2150 train_time:33436ms step_avg:38.48ms
step:870/2150 train_time:33496ms step_avg:38.50ms
step:871/2150 train_time:33558ms step_avg:38.53ms
step:872/2150 train_time:33618ms step_avg:38.55ms
step:873/2150 train_time:33679ms step_avg:38.58ms
step:874/2150 train_time:33738ms step_avg:38.60ms
step:875/2150 train_time:33799ms step_avg:38.63ms
step:876/2150 train_time:33858ms step_avg:38.65ms
step:877/2150 train_time:33920ms step_avg:38.68ms
step:878/2150 train_time:33980ms step_avg:38.70ms
step:879/2150 train_time:34041ms step_avg:38.73ms
step:880/2150 train_time:34101ms step_avg:38.75ms
step:881/2150 train_time:34162ms step_avg:38.78ms
step:882/2150 train_time:34221ms step_avg:38.80ms
step:883/2150 train_time:34282ms step_avg:38.82ms
step:884/2150 train_time:34342ms step_avg:38.85ms
step:885/2150 train_time:34403ms step_avg:38.87ms
step:886/2150 train_time:34463ms step_avg:38.90ms
step:887/2150 train_time:34526ms step_avg:38.92ms
step:888/2150 train_time:34585ms step_avg:38.95ms
step:889/2150 train_time:34646ms step_avg:38.97ms
step:890/2150 train_time:34706ms step_avg:39.00ms
step:891/2150 train_time:34768ms step_avg:39.02ms
step:892/2150 train_time:34827ms step_avg:39.04ms
step:893/2150 train_time:34888ms step_avg:39.07ms
step:894/2150 train_time:34948ms step_avg:39.09ms
step:895/2150 train_time:35009ms step_avg:39.12ms
step:896/2150 train_time:35069ms step_avg:39.14ms
step:897/2150 train_time:35130ms step_avg:39.16ms
step:898/2150 train_time:35190ms step_avg:39.19ms
step:899/2150 train_time:35251ms step_avg:39.21ms
step:900/2150 train_time:35311ms step_avg:39.23ms
step:901/2150 train_time:35372ms step_avg:39.26ms
step:902/2150 train_time:35432ms step_avg:39.28ms
step:903/2150 train_time:35493ms step_avg:39.31ms
step:904/2150 train_time:35553ms step_avg:39.33ms
step:905/2150 train_time:35614ms step_avg:39.35ms
step:906/2150 train_time:35674ms step_avg:39.37ms
step:907/2150 train_time:35736ms step_avg:39.40ms
step:908/2150 train_time:35795ms step_avg:39.42ms
step:909/2150 train_time:35856ms step_avg:39.45ms
step:910/2150 train_time:35916ms step_avg:39.47ms
step:911/2150 train_time:35977ms step_avg:39.49ms
step:912/2150 train_time:36037ms step_avg:39.51ms
step:913/2150 train_time:36098ms step_avg:39.54ms
step:914/2150 train_time:36157ms step_avg:39.56ms
step:915/2150 train_time:36219ms step_avg:39.58ms
step:916/2150 train_time:36279ms step_avg:39.61ms
step:917/2150 train_time:36341ms step_avg:39.63ms
step:918/2150 train_time:36400ms step_avg:39.65ms
step:919/2150 train_time:36462ms step_avg:39.68ms
step:920/2150 train_time:36521ms step_avg:39.70ms
step:921/2150 train_time:36583ms step_avg:39.72ms
step:922/2150 train_time:36643ms step_avg:39.74ms
step:923/2150 train_time:36705ms step_avg:39.77ms
step:924/2150 train_time:36765ms step_avg:39.79ms
step:925/2150 train_time:36826ms step_avg:39.81ms
step:926/2150 train_time:36885ms step_avg:39.83ms
step:927/2150 train_time:36946ms step_avg:39.86ms
step:928/2150 train_time:37007ms step_avg:39.88ms
step:929/2150 train_time:37068ms step_avg:39.90ms
step:930/2150 train_time:37127ms step_avg:39.92ms
step:931/2150 train_time:37188ms step_avg:39.94ms
step:932/2150 train_time:37248ms step_avg:39.97ms
step:933/2150 train_time:37309ms step_avg:39.99ms
step:934/2150 train_time:37369ms step_avg:40.01ms
step:935/2150 train_time:37430ms step_avg:40.03ms
step:936/2150 train_time:37489ms step_avg:40.05ms
step:937/2150 train_time:37551ms step_avg:40.08ms
step:938/2150 train_time:37610ms step_avg:40.10ms
step:939/2150 train_time:37672ms step_avg:40.12ms
step:940/2150 train_time:37732ms step_avg:40.14ms
step:941/2150 train_time:37793ms step_avg:40.16ms
step:942/2150 train_time:37852ms step_avg:40.18ms
step:943/2150 train_time:37914ms step_avg:40.21ms
step:944/2150 train_time:37973ms step_avg:40.23ms
step:945/2150 train_time:38034ms step_avg:40.25ms
step:946/2150 train_time:38094ms step_avg:40.27ms
step:947/2150 train_time:38155ms step_avg:40.29ms
step:948/2150 train_time:38215ms step_avg:40.31ms
step:949/2150 train_time:38276ms step_avg:40.33ms
step:950/2150 train_time:38335ms step_avg:40.35ms
step:951/2150 train_time:38396ms step_avg:40.37ms
step:952/2150 train_time:38455ms step_avg:40.39ms
step:953/2150 train_time:38517ms step_avg:40.42ms
step:954/2150 train_time:38576ms step_avg:40.44ms
step:955/2150 train_time:38637ms step_avg:40.46ms
step:956/2150 train_time:38697ms step_avg:40.48ms
step:957/2150 train_time:38758ms step_avg:40.50ms
step:958/2150 train_time:38818ms step_avg:40.52ms
step:959/2150 train_time:38880ms step_avg:40.54ms
step:960/2150 train_time:38939ms step_avg:40.56ms
step:961/2150 train_time:39001ms step_avg:40.58ms
step:962/2150 train_time:39061ms step_avg:40.60ms
step:963/2150 train_time:39123ms step_avg:40.63ms
step:964/2150 train_time:39182ms step_avg:40.65ms
step:965/2150 train_time:39244ms step_avg:40.67ms
step:966/2150 train_time:39303ms step_avg:40.69ms
step:967/2150 train_time:39365ms step_avg:40.71ms
step:968/2150 train_time:39424ms step_avg:40.73ms
step:969/2150 train_time:39486ms step_avg:40.75ms
step:970/2150 train_time:39545ms step_avg:40.77ms
step:971/2150 train_time:39606ms step_avg:40.79ms
step:972/2150 train_time:39667ms step_avg:40.81ms
step:973/2150 train_time:39728ms step_avg:40.83ms
step:974/2150 train_time:39788ms step_avg:40.85ms
step:975/2150 train_time:39849ms step_avg:40.87ms
step:976/2150 train_time:39909ms step_avg:40.89ms
step:977/2150 train_time:39970ms step_avg:40.91ms
step:978/2150 train_time:40030ms step_avg:40.93ms
step:979/2150 train_time:40092ms step_avg:40.95ms
step:980/2150 train_time:40151ms step_avg:40.97ms
step:981/2150 train_time:40213ms step_avg:40.99ms
step:982/2150 train_time:40273ms step_avg:41.01ms
step:983/2150 train_time:40334ms step_avg:41.03ms
step:984/2150 train_time:40393ms step_avg:41.05ms
step:985/2150 train_time:40454ms step_avg:41.07ms
step:986/2150 train_time:40514ms step_avg:41.09ms
step:987/2150 train_time:40575ms step_avg:41.11ms
step:988/2150 train_time:40635ms step_avg:41.13ms
step:989/2150 train_time:40695ms step_avg:41.15ms
step:990/2150 train_time:40755ms step_avg:41.17ms
step:991/2150 train_time:40815ms step_avg:41.19ms
step:992/2150 train_time:40875ms step_avg:41.20ms
step:993/2150 train_time:40936ms step_avg:41.22ms
step:994/2150 train_time:40995ms step_avg:41.24ms
step:995/2150 train_time:41056ms step_avg:41.26ms
step:996/2150 train_time:41116ms step_avg:41.28ms
step:997/2150 train_time:41177ms step_avg:41.30ms
step:998/2150 train_time:41237ms step_avg:41.32ms
step:999/2150 train_time:41298ms step_avg:41.34ms
step:1000/2150 train_time:41358ms step_avg:41.36ms
step:1000/2150 val_loss:3.7127 train_time:41422ms step_avg:41.42ms
step:1001/2150 train_time:41445ms step_avg:41.40ms
step:1002/2150 train_time:41480ms step_avg:41.40ms
step:1003/2150 train_time:41545ms step_avg:41.42ms
step:1004/2150 train_time:41607ms step_avg:41.44ms
step:1005/2150 train_time:41668ms step_avg:41.46ms
step:1006/2150 train_time:41727ms step_avg:41.48ms
step:1007/2150 train_time:41789ms step_avg:41.50ms
step:1008/2150 train_time:41848ms step_avg:41.52ms
step:1009/2150 train_time:41908ms step_avg:41.53ms
step:1010/2150 train_time:41966ms step_avg:41.55ms
step:1011/2150 train_time:42027ms step_avg:41.57ms
step:1012/2150 train_time:42086ms step_avg:41.59ms
step:1013/2150 train_time:42148ms step_avg:41.61ms
step:1014/2150 train_time:42207ms step_avg:41.62ms
step:1015/2150 train_time:42268ms step_avg:41.64ms
step:1016/2150 train_time:42327ms step_avg:41.66ms
step:1017/2150 train_time:42391ms step_avg:41.68ms
step:1018/2150 train_time:42454ms step_avg:41.70ms
step:1019/2150 train_time:42516ms step_avg:41.72ms
step:1020/2150 train_time:42577ms step_avg:41.74ms
step:1021/2150 train_time:42639ms step_avg:41.76ms
step:1022/2150 train_time:42698ms step_avg:41.78ms
step:1023/2150 train_time:42760ms step_avg:41.80ms
step:1024/2150 train_time:42819ms step_avg:41.82ms
step:1025/2150 train_time:42880ms step_avg:41.83ms
step:1026/2150 train_time:42939ms step_avg:41.85ms
step:1027/2150 train_time:42999ms step_avg:41.87ms
step:1028/2150 train_time:43058ms step_avg:41.89ms
step:1029/2150 train_time:43119ms step_avg:41.90ms
step:1030/2150 train_time:43178ms step_avg:41.92ms
step:1031/2150 train_time:43240ms step_avg:41.94ms
step:1032/2150 train_time:43300ms step_avg:41.96ms
step:1033/2150 train_time:43363ms step_avg:41.98ms
step:1034/2150 train_time:43425ms step_avg:42.00ms
step:1035/2150 train_time:43488ms step_avg:42.02ms
step:1036/2150 train_time:43548ms step_avg:42.03ms
step:1037/2150 train_time:43610ms step_avg:42.05ms
step:1038/2150 train_time:43670ms step_avg:42.07ms
step:1039/2150 train_time:43730ms step_avg:42.09ms
step:1040/2150 train_time:43789ms step_avg:42.11ms
step:1041/2150 train_time:43853ms step_avg:42.13ms
step:1042/2150 train_time:43911ms step_avg:42.14ms
step:1043/2150 train_time:43971ms step_avg:42.16ms
step:1044/2150 train_time:44030ms step_avg:42.17ms
step:1045/2150 train_time:44091ms step_avg:42.19ms
step:1046/2150 train_time:44151ms step_avg:42.21ms
step:1047/2150 train_time:44212ms step_avg:42.23ms
step:1048/2150 train_time:44272ms step_avg:42.24ms
step:1049/2150 train_time:44334ms step_avg:42.26ms
step:1050/2150 train_time:44394ms step_avg:42.28ms
step:1051/2150 train_time:44456ms step_avg:42.30ms
step:1052/2150 train_time:44517ms step_avg:42.32ms
step:1053/2150 train_time:44579ms step_avg:42.34ms
step:1054/2150 train_time:44639ms step_avg:42.35ms
step:1055/2150 train_time:44700ms step_avg:42.37ms
step:1056/2150 train_time:44759ms step_avg:42.39ms
step:1057/2150 train_time:44820ms step_avg:42.40ms
step:1058/2150 train_time:44879ms step_avg:42.42ms
step:1059/2150 train_time:44940ms step_avg:42.44ms
step:1060/2150 train_time:45000ms step_avg:42.45ms
step:1061/2150 train_time:45061ms step_avg:42.47ms
step:1062/2150 train_time:45120ms step_avg:42.49ms
step:1063/2150 train_time:45181ms step_avg:42.50ms
step:1064/2150 train_time:45242ms step_avg:42.52ms
step:1065/2150 train_time:45304ms step_avg:42.54ms
step:1066/2150 train_time:45364ms step_avg:42.56ms
step:1067/2150 train_time:45426ms step_avg:42.57ms
step:1068/2150 train_time:45486ms step_avg:42.59ms
step:1069/2150 train_time:45548ms step_avg:42.61ms
step:1070/2150 train_time:45607ms step_avg:42.62ms
step:1071/2150 train_time:45668ms step_avg:42.64ms
step:1072/2150 train_time:45728ms step_avg:42.66ms
step:1073/2150 train_time:45789ms step_avg:42.67ms
step:1074/2150 train_time:45848ms step_avg:42.69ms
step:1075/2150 train_time:45909ms step_avg:42.71ms
step:1076/2150 train_time:45969ms step_avg:42.72ms
step:1077/2150 train_time:46029ms step_avg:42.74ms
step:1078/2150 train_time:46088ms step_avg:42.75ms
step:1079/2150 train_time:46149ms step_avg:42.77ms
step:1080/2150 train_time:46209ms step_avg:42.79ms
step:1081/2150 train_time:46269ms step_avg:42.80ms
step:1082/2150 train_time:46329ms step_avg:42.82ms
step:1083/2150 train_time:46390ms step_avg:42.83ms
step:1084/2150 train_time:46450ms step_avg:42.85ms
step:1085/2150 train_time:46512ms step_avg:42.87ms
step:1086/2150 train_time:46572ms step_avg:42.88ms
step:1087/2150 train_time:46634ms step_avg:42.90ms
step:1088/2150 train_time:46694ms step_avg:42.92ms
step:1089/2150 train_time:46755ms step_avg:42.93ms
step:1090/2150 train_time:46814ms step_avg:42.95ms
step:1091/2150 train_time:46875ms step_avg:42.97ms
step:1092/2150 train_time:46935ms step_avg:42.98ms
step:1093/2150 train_time:46996ms step_avg:43.00ms
step:1094/2150 train_time:47056ms step_avg:43.01ms
step:1095/2150 train_time:47118ms step_avg:43.03ms
step:1096/2150 train_time:47178ms step_avg:43.05ms
step:1097/2150 train_time:47239ms step_avg:43.06ms
step:1098/2150 train_time:47299ms step_avg:43.08ms
step:1099/2150 train_time:47361ms step_avg:43.09ms
step:1100/2150 train_time:47421ms step_avg:43.11ms
step:1101/2150 train_time:47483ms step_avg:43.13ms
step:1102/2150 train_time:47543ms step_avg:43.14ms
step:1103/2150 train_time:47606ms step_avg:43.16ms
step:1104/2150 train_time:47666ms step_avg:43.18ms
step:1105/2150 train_time:47728ms step_avg:43.19ms
step:1106/2150 train_time:47787ms step_avg:43.21ms
step:1107/2150 train_time:47848ms step_avg:43.22ms
step:1108/2150 train_time:47907ms step_avg:43.24ms
step:1109/2150 train_time:47968ms step_avg:43.25ms
step:1110/2150 train_time:48028ms step_avg:43.27ms
step:1111/2150 train_time:48090ms step_avg:43.28ms
step:1112/2150 train_time:48150ms step_avg:43.30ms
step:1113/2150 train_time:48210ms step_avg:43.32ms
step:1114/2150 train_time:48270ms step_avg:43.33ms
step:1115/2150 train_time:48331ms step_avg:43.35ms
step:1116/2150 train_time:48392ms step_avg:43.36ms
step:1117/2150 train_time:48453ms step_avg:43.38ms
step:1118/2150 train_time:48513ms step_avg:43.39ms
step:1119/2150 train_time:48575ms step_avg:43.41ms
step:1120/2150 train_time:48635ms step_avg:43.42ms
step:1121/2150 train_time:48697ms step_avg:43.44ms
step:1122/2150 train_time:48757ms step_avg:43.46ms
step:1123/2150 train_time:48818ms step_avg:43.47ms
step:1124/2150 train_time:48878ms step_avg:43.49ms
step:1125/2150 train_time:48939ms step_avg:43.50ms
step:1126/2150 train_time:48999ms step_avg:43.52ms
step:1127/2150 train_time:49059ms step_avg:43.53ms
step:1128/2150 train_time:49119ms step_avg:43.55ms
step:1129/2150 train_time:49180ms step_avg:43.56ms
step:1130/2150 train_time:49239ms step_avg:43.57ms
step:1131/2150 train_time:49300ms step_avg:43.59ms
step:1132/2150 train_time:49360ms step_avg:43.60ms
step:1133/2150 train_time:49422ms step_avg:43.62ms
step:1134/2150 train_time:49482ms step_avg:43.64ms
step:1135/2150 train_time:49546ms step_avg:43.65ms
step:1136/2150 train_time:49605ms step_avg:43.67ms
step:1137/2150 train_time:49666ms step_avg:43.68ms
step:1138/2150 train_time:49726ms step_avg:43.70ms
step:1139/2150 train_time:49787ms step_avg:43.71ms
step:1140/2150 train_time:49846ms step_avg:43.72ms
step:1141/2150 train_time:49907ms step_avg:43.74ms
step:1142/2150 train_time:49966ms step_avg:43.75ms
step:1143/2150 train_time:50028ms step_avg:43.77ms
step:1144/2150 train_time:50087ms step_avg:43.78ms
step:1145/2150 train_time:50148ms step_avg:43.80ms
step:1146/2150 train_time:50208ms step_avg:43.81ms
step:1147/2150 train_time:50269ms step_avg:43.83ms
step:1148/2150 train_time:50329ms step_avg:43.84ms
step:1149/2150 train_time:50390ms step_avg:43.86ms
step:1150/2150 train_time:50450ms step_avg:43.87ms
step:1151/2150 train_time:50511ms step_avg:43.88ms
step:1152/2150 train_time:50571ms step_avg:43.90ms
step:1153/2150 train_time:50634ms step_avg:43.91ms
step:1154/2150 train_time:50694ms step_avg:43.93ms
step:1155/2150 train_time:50755ms step_avg:43.94ms
step:1156/2150 train_time:50815ms step_avg:43.96ms
step:1157/2150 train_time:50877ms step_avg:43.97ms
step:1158/2150 train_time:50937ms step_avg:43.99ms
step:1159/2150 train_time:50999ms step_avg:44.00ms
step:1160/2150 train_time:51058ms step_avg:44.02ms
step:1161/2150 train_time:51120ms step_avg:44.03ms
step:1162/2150 train_time:51179ms step_avg:44.04ms
step:1163/2150 train_time:51241ms step_avg:44.06ms
step:1164/2150 train_time:51301ms step_avg:44.07ms
step:1165/2150 train_time:51362ms step_avg:44.09ms
step:1166/2150 train_time:51423ms step_avg:44.10ms
step:1167/2150 train_time:51485ms step_avg:44.12ms
step:1168/2150 train_time:51546ms step_avg:44.13ms
step:1169/2150 train_time:51608ms step_avg:44.15ms
step:1170/2150 train_time:51667ms step_avg:44.16ms
step:1171/2150 train_time:51728ms step_avg:44.17ms
step:1172/2150 train_time:51788ms step_avg:44.19ms
step:1173/2150 train_time:51849ms step_avg:44.20ms
step:1174/2150 train_time:51908ms step_avg:44.21ms
step:1175/2150 train_time:51969ms step_avg:44.23ms
step:1176/2150 train_time:52029ms step_avg:44.24ms
step:1177/2150 train_time:52090ms step_avg:44.26ms
step:1178/2150 train_time:52150ms step_avg:44.27ms
step:1179/2150 train_time:52210ms step_avg:44.28ms
step:1180/2150 train_time:52270ms step_avg:44.30ms
step:1181/2150 train_time:52332ms step_avg:44.31ms
step:1182/2150 train_time:52392ms step_avg:44.32ms
step:1183/2150 train_time:52454ms step_avg:44.34ms
step:1184/2150 train_time:52514ms step_avg:44.35ms
step:1185/2150 train_time:52576ms step_avg:44.37ms
step:1186/2150 train_time:52636ms step_avg:44.38ms
step:1187/2150 train_time:52699ms step_avg:44.40ms
step:1188/2150 train_time:52758ms step_avg:44.41ms
step:1189/2150 train_time:52819ms step_avg:44.42ms
step:1190/2150 train_time:52878ms step_avg:44.44ms
step:1191/2150 train_time:52940ms step_avg:44.45ms
step:1192/2150 train_time:53000ms step_avg:44.46ms
step:1193/2150 train_time:53061ms step_avg:44.48ms
step:1194/2150 train_time:53120ms step_avg:44.49ms
step:1195/2150 train_time:53182ms step_avg:44.50ms
step:1196/2150 train_time:53241ms step_avg:44.52ms
step:1197/2150 train_time:53303ms step_avg:44.53ms
step:1198/2150 train_time:53363ms step_avg:44.54ms
step:1199/2150 train_time:53426ms step_avg:44.56ms
step:1200/2150 train_time:53486ms step_avg:44.57ms
step:1201/2150 train_time:53548ms step_avg:44.59ms
step:1202/2150 train_time:53607ms step_avg:44.60ms
step:1203/2150 train_time:53668ms step_avg:44.61ms
step:1204/2150 train_time:53728ms step_avg:44.62ms
step:1205/2150 train_time:53789ms step_avg:44.64ms
step:1206/2150 train_time:53849ms step_avg:44.65ms
step:1207/2150 train_time:53910ms step_avg:44.66ms
step:1208/2150 train_time:53969ms step_avg:44.68ms
step:1209/2150 train_time:54030ms step_avg:44.69ms
step:1210/2150 train_time:54090ms step_avg:44.70ms
step:1211/2150 train_time:54150ms step_avg:44.71ms
step:1212/2150 train_time:54209ms step_avg:44.73ms
step:1213/2150 train_time:54270ms step_avg:44.74ms
step:1214/2150 train_time:54330ms step_avg:44.75ms
step:1215/2150 train_time:54392ms step_avg:44.77ms
step:1216/2150 train_time:54452ms step_avg:44.78ms
step:1217/2150 train_time:54514ms step_avg:44.79ms
step:1218/2150 train_time:54574ms step_avg:44.81ms
step:1219/2150 train_time:54637ms step_avg:44.82ms
step:1220/2150 train_time:54697ms step_avg:44.83ms
step:1221/2150 train_time:54758ms step_avg:44.85ms
step:1222/2150 train_time:54818ms step_avg:44.86ms
step:1223/2150 train_time:54879ms step_avg:44.87ms
step:1224/2150 train_time:54939ms step_avg:44.88ms
step:1225/2150 train_time:55000ms step_avg:44.90ms
step:1226/2150 train_time:55059ms step_avg:44.91ms
step:1227/2150 train_time:55120ms step_avg:44.92ms
step:1228/2150 train_time:55179ms step_avg:44.93ms
step:1229/2150 train_time:55241ms step_avg:44.95ms
step:1230/2150 train_time:55301ms step_avg:44.96ms
step:1231/2150 train_time:55363ms step_avg:44.97ms
step:1232/2150 train_time:55423ms step_avg:44.99ms
step:1233/2150 train_time:55484ms step_avg:45.00ms
step:1234/2150 train_time:55545ms step_avg:45.01ms
step:1235/2150 train_time:55606ms step_avg:45.03ms
step:1236/2150 train_time:55666ms step_avg:45.04ms
step:1237/2150 train_time:55727ms step_avg:45.05ms
step:1238/2150 train_time:55786ms step_avg:45.06ms
step:1239/2150 train_time:55847ms step_avg:45.07ms
step:1240/2150 train_time:55907ms step_avg:45.09ms
step:1241/2150 train_time:55969ms step_avg:45.10ms
step:1242/2150 train_time:56029ms step_avg:45.11ms
step:1243/2150 train_time:56089ms step_avg:45.12ms
step:1244/2150 train_time:56149ms step_avg:45.14ms
step:1245/2150 train_time:56210ms step_avg:45.15ms
step:1246/2150 train_time:56270ms step_avg:45.16ms
step:1247/2150 train_time:56331ms step_avg:45.17ms
step:1248/2150 train_time:56391ms step_avg:45.18ms
step:1249/2150 train_time:56452ms step_avg:45.20ms
step:1250/2150 train_time:56512ms step_avg:45.21ms
step:1250/2150 val_loss:3.5925 train_time:56576ms step_avg:45.26ms
step:1251/2150 train_time:56598ms step_avg:45.24ms
step:1252/2150 train_time:56635ms step_avg:45.24ms
step:1253/2150 train_time:56701ms step_avg:45.25ms
step:1254/2150 train_time:56762ms step_avg:45.27ms
step:1255/2150 train_time:56823ms step_avg:45.28ms
step:1256/2150 train_time:56883ms step_avg:45.29ms
step:1257/2150 train_time:56944ms step_avg:45.30ms
step:1258/2150 train_time:57003ms step_avg:45.31ms
step:1259/2150 train_time:57064ms step_avg:45.33ms
step:1260/2150 train_time:57123ms step_avg:45.34ms
step:1261/2150 train_time:57185ms step_avg:45.35ms
step:1262/2150 train_time:57243ms step_avg:45.36ms
step:1263/2150 train_time:57304ms step_avg:45.37ms
step:1264/2150 train_time:57363ms step_avg:45.38ms
step:1265/2150 train_time:57424ms step_avg:45.39ms
step:1266/2150 train_time:57485ms step_avg:45.41ms
step:1267/2150 train_time:57547ms step_avg:45.42ms
step:1268/2150 train_time:57609ms step_avg:45.43ms
step:1269/2150 train_time:57671ms step_avg:45.45ms
step:1270/2150 train_time:57732ms step_avg:45.46ms
step:1271/2150 train_time:57794ms step_avg:45.47ms
step:1272/2150 train_time:57854ms step_avg:45.48ms
step:1273/2150 train_time:57915ms step_avg:45.50ms
step:1274/2150 train_time:57975ms step_avg:45.51ms
step:1275/2150 train_time:58036ms step_avg:45.52ms
step:1276/2150 train_time:58095ms step_avg:45.53ms
step:1277/2150 train_time:58156ms step_avg:45.54ms
step:1278/2150 train_time:58214ms step_avg:45.55ms
step:1279/2150 train_time:58275ms step_avg:45.56ms
step:1280/2150 train_time:58334ms step_avg:45.57ms
step:1281/2150 train_time:58395ms step_avg:45.59ms
step:1282/2150 train_time:58454ms step_avg:45.60ms
step:1283/2150 train_time:58516ms step_avg:45.61ms
step:1284/2150 train_time:58576ms step_avg:45.62ms
step:1285/2150 train_time:58639ms step_avg:45.63ms
step:1286/2150 train_time:58700ms step_avg:45.65ms
step:1287/2150 train_time:58762ms step_avg:45.66ms
step:1288/2150 train_time:58821ms step_avg:45.67ms
step:1289/2150 train_time:58883ms step_avg:45.68ms
step:1290/2150 train_time:58942ms step_avg:45.69ms
step:1291/2150 train_time:59002ms step_avg:45.70ms
step:1292/2150 train_time:59062ms step_avg:45.71ms
step:1293/2150 train_time:59122ms step_avg:45.72ms
step:1294/2150 train_time:59182ms step_avg:45.74ms
step:1295/2150 train_time:59242ms step_avg:45.75ms
step:1296/2150 train_time:59302ms step_avg:45.76ms
step:1297/2150 train_time:59361ms step_avg:45.77ms
step:1298/2150 train_time:59422ms step_avg:45.78ms
step:1299/2150 train_time:59483ms step_avg:45.79ms
step:1300/2150 train_time:59543ms step_avg:45.80ms
step:1301/2150 train_time:59604ms step_avg:45.81ms
step:1302/2150 train_time:59664ms step_avg:45.82ms
step:1303/2150 train_time:59726ms step_avg:45.84ms
step:1304/2150 train_time:59786ms step_avg:45.85ms
step:1305/2150 train_time:59848ms step_avg:45.86ms
step:1306/2150 train_time:59908ms step_avg:45.87ms
step:1307/2150 train_time:59970ms step_avg:45.88ms
step:1308/2150 train_time:60031ms step_avg:45.90ms
step:1309/2150 train_time:60093ms step_avg:45.91ms
step:1310/2150 train_time:60152ms step_avg:45.92ms
step:1311/2150 train_time:60214ms step_avg:45.93ms
step:1312/2150 train_time:60273ms step_avg:45.94ms
step:1313/2150 train_time:60334ms step_avg:45.95ms
step:1314/2150 train_time:60394ms step_avg:45.96ms
step:1315/2150 train_time:60456ms step_avg:45.97ms
step:1316/2150 train_time:60516ms step_avg:45.98ms
step:1317/2150 train_time:60578ms step_avg:46.00ms
step:1318/2150 train_time:60639ms step_avg:46.01ms
step:1319/2150 train_time:60701ms step_avg:46.02ms
step:1320/2150 train_time:60760ms step_avg:46.03ms
step:1321/2150 train_time:60822ms step_avg:46.04ms
step:1322/2150 train_time:60881ms step_avg:46.05ms
step:1323/2150 train_time:60942ms step_avg:46.06ms
step:1324/2150 train_time:61003ms step_avg:46.07ms
step:1325/2150 train_time:61065ms step_avg:46.09ms
step:1326/2150 train_time:61124ms step_avg:46.10ms
step:1327/2150 train_time:61185ms step_avg:46.11ms
step:1328/2150 train_time:61245ms step_avg:46.12ms
step:1329/2150 train_time:61306ms step_avg:46.13ms
step:1330/2150 train_time:61366ms step_avg:46.14ms
step:1331/2150 train_time:61428ms step_avg:46.15ms
step:1332/2150 train_time:61489ms step_avg:46.16ms
step:1333/2150 train_time:61551ms step_avg:46.17ms
step:1334/2150 train_time:61611ms step_avg:46.19ms
step:1335/2150 train_time:61674ms step_avg:46.20ms
step:1336/2150 train_time:61733ms step_avg:46.21ms
step:1337/2150 train_time:61795ms step_avg:46.22ms
step:1338/2150 train_time:61855ms step_avg:46.23ms
step:1339/2150 train_time:61917ms step_avg:46.24ms
step:1340/2150 train_time:61977ms step_avg:46.25ms
step:1341/2150 train_time:62039ms step_avg:46.26ms
step:1342/2150 train_time:62099ms step_avg:46.27ms
step:1343/2150 train_time:62160ms step_avg:46.28ms
step:1344/2150 train_time:62219ms step_avg:46.29ms
step:1345/2150 train_time:62280ms step_avg:46.31ms
step:1346/2150 train_time:62340ms step_avg:46.31ms
step:1347/2150 train_time:62402ms step_avg:46.33ms
step:1348/2150 train_time:62461ms step_avg:46.34ms
step:1349/2150 train_time:62523ms step_avg:46.35ms
step:1350/2150 train_time:62583ms step_avg:46.36ms
step:1351/2150 train_time:62644ms step_avg:46.37ms
step:1352/2150 train_time:62704ms step_avg:46.38ms
step:1353/2150 train_time:62765ms step_avg:46.39ms
step:1354/2150 train_time:62825ms step_avg:46.40ms
step:1355/2150 train_time:62887ms step_avg:46.41ms
step:1356/2150 train_time:62947ms step_avg:46.42ms
step:1357/2150 train_time:63008ms step_avg:46.43ms
step:1358/2150 train_time:63068ms step_avg:46.44ms
step:1359/2150 train_time:63131ms step_avg:46.45ms
step:1360/2150 train_time:63190ms step_avg:46.46ms
step:1361/2150 train_time:63252ms step_avg:46.47ms
step:1362/2150 train_time:63311ms step_avg:46.48ms
step:1363/2150 train_time:63373ms step_avg:46.50ms
step:1364/2150 train_time:63433ms step_avg:46.50ms
step:1365/2150 train_time:63495ms step_avg:46.52ms
step:1366/2150 train_time:63554ms step_avg:46.53ms
step:1367/2150 train_time:63615ms step_avg:46.54ms
step:1368/2150 train_time:63675ms step_avg:46.55ms
step:1369/2150 train_time:63736ms step_avg:46.56ms
step:1370/2150 train_time:63796ms step_avg:46.57ms
step:1371/2150 train_time:63858ms step_avg:46.58ms
step:1372/2150 train_time:63918ms step_avg:46.59ms
step:1373/2150 train_time:63980ms step_avg:46.60ms
step:1374/2150 train_time:64039ms step_avg:46.61ms
step:1375/2150 train_time:64100ms step_avg:46.62ms
step:1376/2150 train_time:64160ms step_avg:46.63ms
step:1377/2150 train_time:64221ms step_avg:46.64ms
step:1378/2150 train_time:64281ms step_avg:46.65ms
step:1379/2150 train_time:64342ms step_avg:46.66ms
step:1380/2150 train_time:64401ms step_avg:46.67ms
step:1381/2150 train_time:64462ms step_avg:46.68ms
step:1382/2150 train_time:64522ms step_avg:46.69ms
step:1383/2150 train_time:64583ms step_avg:46.70ms
step:1384/2150 train_time:64643ms step_avg:46.71ms
step:1385/2150 train_time:64703ms step_avg:46.72ms
step:1386/2150 train_time:64763ms step_avg:46.73ms
step:1387/2150 train_time:64824ms step_avg:46.74ms
step:1388/2150 train_time:64885ms step_avg:46.75ms
step:1389/2150 train_time:64946ms step_avg:46.76ms
step:1390/2150 train_time:65005ms step_avg:46.77ms
step:1391/2150 train_time:65067ms step_avg:46.78ms
step:1392/2150 train_time:65127ms step_avg:46.79ms
step:1393/2150 train_time:65189ms step_avg:46.80ms
step:1394/2150 train_time:65248ms step_avg:46.81ms
step:1395/2150 train_time:65310ms step_avg:46.82ms
step:1396/2150 train_time:65371ms step_avg:46.83ms
step:1397/2150 train_time:65432ms step_avg:46.84ms
step:1398/2150 train_time:65491ms step_avg:46.85ms
step:1399/2150 train_time:65552ms step_avg:46.86ms
step:1400/2150 train_time:65612ms step_avg:46.87ms
step:1401/2150 train_time:65673ms step_avg:46.88ms
step:1402/2150 train_time:65733ms step_avg:46.89ms
step:1403/2150 train_time:65796ms step_avg:46.90ms
step:1404/2150 train_time:65855ms step_avg:46.91ms
step:1405/2150 train_time:65917ms step_avg:46.92ms
step:1406/2150 train_time:65977ms step_avg:46.93ms
step:1407/2150 train_time:66039ms step_avg:46.94ms
step:1408/2150 train_time:66099ms step_avg:46.95ms
step:1409/2150 train_time:66189ms step_avg:46.98ms
step:1410/2150 train_time:66277ms step_avg:47.01ms
step:1411/2150 train_time:66368ms step_avg:47.04ms
step:1412/2150 train_time:66456ms step_avg:47.07ms
step:1413/2150 train_time:66546ms step_avg:47.10ms
step:1414/2150 train_time:66634ms step_avg:47.12ms
step:1415/2150 train_time:66724ms step_avg:47.15ms
step:1416/2150 train_time:66813ms step_avg:47.18ms
step:1417/2150 train_time:66904ms step_avg:47.22ms
step:1418/2150 train_time:66992ms step_avg:47.24ms
step:1419/2150 train_time:67083ms step_avg:47.27ms
step:1420/2150 train_time:67171ms step_avg:47.30ms
step:1421/2150 train_time:67259ms step_avg:47.33ms
step:1422/2150 train_time:67347ms step_avg:47.36ms
step:1423/2150 train_time:67437ms step_avg:47.39ms
step:1424/2150 train_time:67524ms step_avg:47.42ms
step:1425/2150 train_time:67613ms step_avg:47.45ms
step:1426/2150 train_time:67702ms step_avg:47.48ms
step:1427/2150 train_time:67792ms step_avg:47.51ms
step:1428/2150 train_time:67880ms step_avg:47.53ms
step:1429/2150 train_time:67970ms step_avg:47.56ms
step:1430/2150 train_time:68058ms step_avg:47.59ms
step:1431/2150 train_time:68147ms step_avg:47.62ms
step:1432/2150 train_time:68235ms step_avg:47.65ms
step:1433/2150 train_time:68325ms step_avg:47.68ms
step:1434/2150 train_time:68413ms step_avg:47.71ms
step:1435/2150 train_time:68502ms step_avg:47.74ms
step:1436/2150 train_time:68590ms step_avg:47.76ms
step:1437/2150 train_time:68679ms step_avg:47.79ms
step:1438/2150 train_time:68767ms step_avg:47.82ms
step:1439/2150 train_time:68857ms step_avg:47.85ms
step:1440/2150 train_time:68946ms step_avg:47.88ms
step:1441/2150 train_time:69035ms step_avg:47.91ms
step:1442/2150 train_time:69124ms step_avg:47.94ms
step:1443/2150 train_time:69215ms step_avg:47.97ms
step:1444/2150 train_time:69303ms step_avg:47.99ms
step:1445/2150 train_time:69393ms step_avg:48.02ms
step:1446/2150 train_time:69481ms step_avg:48.05ms
step:1447/2150 train_time:69571ms step_avg:48.08ms
step:1448/2150 train_time:69658ms step_avg:48.11ms
step:1449/2150 train_time:69747ms step_avg:48.13ms
step:1450/2150 train_time:69835ms step_avg:48.16ms
step:1451/2150 train_time:69924ms step_avg:48.19ms
step:1452/2150 train_time:70012ms step_avg:48.22ms
step:1453/2150 train_time:70102ms step_avg:48.25ms
step:1454/2150 train_time:70189ms step_avg:48.27ms
step:1455/2150 train_time:70278ms step_avg:48.30ms
step:1456/2150 train_time:70367ms step_avg:48.33ms
step:1457/2150 train_time:70456ms step_avg:48.36ms
step:1458/2150 train_time:70544ms step_avg:48.38ms
step:1459/2150 train_time:70633ms step_avg:48.41ms
step:1460/2150 train_time:70720ms step_avg:48.44ms
step:1461/2150 train_time:70810ms step_avg:48.47ms
step:1462/2150 train_time:70898ms step_avg:48.49ms
step:1463/2150 train_time:70987ms step_avg:48.52ms
step:1464/2150 train_time:71075ms step_avg:48.55ms
step:1465/2150 train_time:71164ms step_avg:48.58ms
step:1466/2150 train_time:71252ms step_avg:48.60ms
step:1467/2150 train_time:71340ms step_avg:48.63ms
step:1468/2150 train_time:71428ms step_avg:48.66ms
step:1469/2150 train_time:71517ms step_avg:48.68ms
step:1470/2150 train_time:71605ms step_avg:48.71ms
step:1471/2150 train_time:71695ms step_avg:48.74ms
step:1472/2150 train_time:71783ms step_avg:48.77ms
step:1473/2150 train_time:71873ms step_avg:48.79ms
step:1474/2150 train_time:71961ms step_avg:48.82ms
step:1475/2150 train_time:72051ms step_avg:48.85ms
step:1476/2150 train_time:72139ms step_avg:48.87ms
step:1477/2150 train_time:72228ms step_avg:48.90ms
step:1478/2150 train_time:72316ms step_avg:48.93ms
step:1479/2150 train_time:72406ms step_avg:48.96ms
step:1480/2150 train_time:72494ms step_avg:48.98ms
step:1481/2150 train_time:72583ms step_avg:49.01ms
step:1482/2150 train_time:72670ms step_avg:49.04ms
step:1483/2150 train_time:72759ms step_avg:49.06ms
step:1484/2150 train_time:72846ms step_avg:49.09ms
step:1485/2150 train_time:72936ms step_avg:49.12ms
step:1486/2150 train_time:73024ms step_avg:49.14ms
step:1487/2150 train_time:73113ms step_avg:49.17ms
step:1488/2150 train_time:73202ms step_avg:49.19ms
step:1489/2150 train_time:73291ms step_avg:49.22ms
step:1490/2150 train_time:73379ms step_avg:49.25ms
step:1491/2150 train_time:73469ms step_avg:49.28ms
step:1492/2150 train_time:73557ms step_avg:49.30ms
step:1493/2150 train_time:73646ms step_avg:49.33ms
step:1494/2150 train_time:73734ms step_avg:49.35ms
step:1495/2150 train_time:73824ms step_avg:49.38ms
step:1496/2150 train_time:73912ms step_avg:49.41ms
step:1497/2150 train_time:74001ms step_avg:49.43ms
step:1498/2150 train_time:74088ms step_avg:49.46ms
step:1499/2150 train_time:74178ms step_avg:49.48ms
step:1500/2150 train_time:74265ms step_avg:49.51ms
step:1500/2150 val_loss:3.4881 train_time:74356ms step_avg:49.57ms
step:1501/2150 train_time:74381ms step_avg:49.55ms
step:1502/2150 train_time:74449ms step_avg:49.57ms
step:1503/2150 train_time:74542ms step_avg:49.60ms
step:1504/2150 train_time:74631ms step_avg:49.62ms
step:1505/2150 train_time:74718ms step_avg:49.65ms
step:1506/2150 train_time:74805ms step_avg:49.67ms
step:1507/2150 train_time:74892ms step_avg:49.70ms
step:1508/2150 train_time:74978ms step_avg:49.72ms
step:1509/2150 train_time:75066ms step_avg:49.75ms
step:1510/2150 train_time:75153ms step_avg:49.77ms
step:1511/2150 train_time:75243ms step_avg:49.80ms
step:1512/2150 train_time:75335ms step_avg:49.82ms
step:1513/2150 train_time:75428ms step_avg:49.85ms
step:1514/2150 train_time:75517ms step_avg:49.88ms
step:1515/2150 train_time:75607ms step_avg:49.91ms
step:1516/2150 train_time:75694ms step_avg:49.93ms
step:1517/2150 train_time:75782ms step_avg:49.96ms
step:1518/2150 train_time:75870ms step_avg:49.98ms
step:1519/2150 train_time:75958ms step_avg:50.01ms
step:1520/2150 train_time:76045ms step_avg:50.03ms
step:1521/2150 train_time:76133ms step_avg:50.05ms
step:1522/2150 train_time:76221ms step_avg:50.08ms
step:1523/2150 train_time:76312ms step_avg:50.11ms
step:1524/2150 train_time:76401ms step_avg:50.13ms
step:1525/2150 train_time:76491ms step_avg:50.16ms
step:1526/2150 train_time:76578ms step_avg:50.18ms
step:1527/2150 train_time:76668ms step_avg:50.21ms
step:1528/2150 train_time:76756ms step_avg:50.23ms
step:1529/2150 train_time:76845ms step_avg:50.26ms
step:1530/2150 train_time:76933ms step_avg:50.28ms
step:1531/2150 train_time:77021ms step_avg:50.31ms
step:1532/2150 train_time:77107ms step_avg:50.33ms
step:1533/2150 train_time:77196ms step_avg:50.36ms
step:1534/2150 train_time:77285ms step_avg:50.38ms
step:1535/2150 train_time:77376ms step_avg:50.41ms
step:1536/2150 train_time:77465ms step_avg:50.43ms
step:1537/2150 train_time:77555ms step_avg:50.46ms
step:1538/2150 train_time:77643ms step_avg:50.48ms
step:1539/2150 train_time:77733ms step_avg:50.51ms
step:1540/2150 train_time:77820ms step_avg:50.53ms
step:1541/2150 train_time:77908ms step_avg:50.56ms
step:1542/2150 train_time:77995ms step_avg:50.58ms
step:1543/2150 train_time:78083ms step_avg:50.60ms
step:1544/2150 train_time:78171ms step_avg:50.63ms
step:1545/2150 train_time:78261ms step_avg:50.65ms
step:1546/2150 train_time:78349ms step_avg:50.68ms
step:1547/2150 train_time:78440ms step_avg:50.70ms
step:1548/2150 train_time:78528ms step_avg:50.73ms
step:1549/2150 train_time:78616ms step_avg:50.75ms
step:1550/2150 train_time:78705ms step_avg:50.78ms
step:1551/2150 train_time:78794ms step_avg:50.80ms
step:1552/2150 train_time:78881ms step_avg:50.83ms
step:1553/2150 train_time:78970ms step_avg:50.85ms
step:1554/2150 train_time:79056ms step_avg:50.87ms
step:1555/2150 train_time:79145ms step_avg:50.90ms
step:1556/2150 train_time:79232ms step_avg:50.92ms
step:1557/2150 train_time:79322ms step_avg:50.95ms
step:1558/2150 train_time:79409ms step_avg:50.97ms
step:1559/2150 train_time:79499ms step_avg:50.99ms
step:1560/2150 train_time:79586ms step_avg:51.02ms
step:1561/2150 train_time:79675ms step_avg:51.04ms
step:1562/2150 train_time:79763ms step_avg:51.06ms
step:1563/2150 train_time:79851ms step_avg:51.09ms
step:1564/2150 train_time:79938ms step_avg:51.11ms
step:1565/2150 train_time:80028ms step_avg:51.14ms
step:1566/2150 train_time:80115ms step_avg:51.16ms
step:1567/2150 train_time:80204ms step_avg:51.18ms
step:1568/2150 train_time:80292ms step_avg:51.21ms
step:1569/2150 train_time:80381ms step_avg:51.23ms
step:1570/2150 train_time:80469ms step_avg:51.25ms
step:1571/2150 train_time:80558ms step_avg:51.28ms
step:1572/2150 train_time:80646ms step_avg:51.30ms
step:1573/2150 train_time:80736ms step_avg:51.33ms
step:1574/2150 train_time:80824ms step_avg:51.35ms
step:1575/2150 train_time:80913ms step_avg:51.37ms
step:1576/2150 train_time:81000ms step_avg:51.40ms
step:1577/2150 train_time:81088ms step_avg:51.42ms
step:1578/2150 train_time:81176ms step_avg:51.44ms
step:1579/2150 train_time:81265ms step_avg:51.47ms
step:1580/2150 train_time:81353ms step_avg:51.49ms
step:1581/2150 train_time:81442ms step_avg:51.51ms
step:1582/2150 train_time:81529ms step_avg:51.54ms
step:1583/2150 train_time:81619ms step_avg:51.56ms
step:1584/2150 train_time:81707ms step_avg:51.58ms
step:1585/2150 train_time:81796ms step_avg:51.61ms
step:1586/2150 train_time:81884ms step_avg:51.63ms
step:1587/2150 train_time:81973ms step_avg:51.65ms
step:1588/2150 train_time:82061ms step_avg:51.68ms
step:1589/2150 train_time:82150ms step_avg:51.70ms
step:1590/2150 train_time:82237ms step_avg:51.72ms
step:1591/2150 train_time:82326ms step_avg:51.74ms
step:1592/2150 train_time:82413ms step_avg:51.77ms
step:1593/2150 train_time:82502ms step_avg:51.79ms
step:1594/2150 train_time:82590ms step_avg:51.81ms
step:1595/2150 train_time:82679ms step_avg:51.84ms
step:1596/2150 train_time:82767ms step_avg:51.86ms
step:1597/2150 train_time:82856ms step_avg:51.88ms
step:1598/2150 train_time:82944ms step_avg:51.91ms
step:1599/2150 train_time:83034ms step_avg:51.93ms
step:1600/2150 train_time:83122ms step_avg:51.95ms
step:1601/2150 train_time:83212ms step_avg:51.97ms
step:1602/2150 train_time:83300ms step_avg:52.00ms
step:1603/2150 train_time:83390ms step_avg:52.02ms
step:1604/2150 train_time:83479ms step_avg:52.04ms
step:1605/2150 train_time:83569ms step_avg:52.07ms
step:1606/2150 train_time:83657ms step_avg:52.09ms
step:1607/2150 train_time:83747ms step_avg:52.11ms
step:1608/2150 train_time:83834ms step_avg:52.14ms
step:1609/2150 train_time:83923ms step_avg:52.16ms
step:1610/2150 train_time:84010ms step_avg:52.18ms
step:1611/2150 train_time:84100ms step_avg:52.20ms
step:1612/2150 train_time:84188ms step_avg:52.23ms
step:1613/2150 train_time:84277ms step_avg:52.25ms
step:1614/2150 train_time:84365ms step_avg:52.27ms
step:1615/2150 train_time:84453ms step_avg:52.29ms
step:1616/2150 train_time:84541ms step_avg:52.32ms
step:1617/2150 train_time:84631ms step_avg:52.34ms
step:1618/2150 train_time:84718ms step_avg:52.36ms
step:1619/2150 train_time:84808ms step_avg:52.38ms
step:1620/2150 train_time:84895ms step_avg:52.40ms
step:1621/2150 train_time:84984ms step_avg:52.43ms
step:1622/2150 train_time:85071ms step_avg:52.45ms
step:1623/2150 train_time:85160ms step_avg:52.47ms
step:1624/2150 train_time:85247ms step_avg:52.49ms
step:1625/2150 train_time:85338ms step_avg:52.52ms
step:1626/2150 train_time:85424ms step_avg:52.54ms
step:1627/2150 train_time:85514ms step_avg:52.56ms
step:1628/2150 train_time:85603ms step_avg:52.58ms
step:1629/2150 train_time:85693ms step_avg:52.60ms
step:1630/2150 train_time:85780ms step_avg:52.63ms
step:1631/2150 train_time:85869ms step_avg:52.65ms
step:1632/2150 train_time:85957ms step_avg:52.67ms
step:1633/2150 train_time:86046ms step_avg:52.69ms
step:1634/2150 train_time:86135ms step_avg:52.71ms
step:1635/2150 train_time:86224ms step_avg:52.74ms
step:1636/2150 train_time:86310ms step_avg:52.76ms
step:1637/2150 train_time:86400ms step_avg:52.78ms
step:1638/2150 train_time:86488ms step_avg:52.80ms
step:1639/2150 train_time:86577ms step_avg:52.82ms
step:1640/2150 train_time:86666ms step_avg:52.84ms
step:1641/2150 train_time:86755ms step_avg:52.87ms
step:1642/2150 train_time:86843ms step_avg:52.89ms
step:1643/2150 train_time:86932ms step_avg:52.91ms
step:1644/2150 train_time:87020ms step_avg:52.93ms
step:1645/2150 train_time:87109ms step_avg:52.95ms
step:1646/2150 train_time:87196ms step_avg:52.97ms
step:1647/2150 train_time:87286ms step_avg:53.00ms
step:1648/2150 train_time:87373ms step_avg:53.02ms
step:1649/2150 train_time:87462ms step_avg:53.04ms
step:1650/2150 train_time:87550ms step_avg:53.06ms
step:1651/2150 train_time:87639ms step_avg:53.08ms
step:1652/2150 train_time:87726ms step_avg:53.10ms
step:1653/2150 train_time:87816ms step_avg:53.12ms
step:1654/2150 train_time:87903ms step_avg:53.15ms
step:1655/2150 train_time:87992ms step_avg:53.17ms
step:1656/2150 train_time:88080ms step_avg:53.19ms
step:1657/2150 train_time:88169ms step_avg:53.21ms
step:1658/2150 train_time:88257ms step_avg:53.23ms
step:1659/2150 train_time:88346ms step_avg:53.25ms
step:1660/2150 train_time:88434ms step_avg:53.27ms
step:1661/2150 train_time:88523ms step_avg:53.30ms
step:1662/2150 train_time:88612ms step_avg:53.32ms
step:1663/2150 train_time:88701ms step_avg:53.34ms
step:1664/2150 train_time:88789ms step_avg:53.36ms
step:1665/2150 train_time:88878ms step_avg:53.38ms
step:1666/2150 train_time:88966ms step_avg:53.40ms
step:1667/2150 train_time:89055ms step_avg:53.42ms
step:1668/2150 train_time:89143ms step_avg:53.44ms
step:1669/2150 train_time:89232ms step_avg:53.46ms
step:1670/2150 train_time:89320ms step_avg:53.48ms
step:1671/2150 train_time:89409ms step_avg:53.51ms
step:1672/2150 train_time:89496ms step_avg:53.53ms
step:1673/2150 train_time:89585ms step_avg:53.55ms
step:1674/2150 train_time:89674ms step_avg:53.57ms
step:1675/2150 train_time:89763ms step_avg:53.59ms
step:1676/2150 train_time:89851ms step_avg:53.61ms
step:1677/2150 train_time:89941ms step_avg:53.63ms
step:1678/2150 train_time:90028ms step_avg:53.65ms
step:1679/2150 train_time:90117ms step_avg:53.67ms
step:1680/2150 train_time:90204ms step_avg:53.69ms
step:1681/2150 train_time:90293ms step_avg:53.71ms
step:1682/2150 train_time:90380ms step_avg:53.73ms
step:1683/2150 train_time:90469ms step_avg:53.75ms
step:1684/2150 train_time:90558ms step_avg:53.78ms
step:1685/2150 train_time:90648ms step_avg:53.80ms
step:1686/2150 train_time:90735ms step_avg:53.82ms
step:1687/2150 train_time:90824ms step_avg:53.84ms
step:1688/2150 train_time:90911ms step_avg:53.86ms
step:1689/2150 train_time:91000ms step_avg:53.88ms
step:1690/2150 train_time:91087ms step_avg:53.90ms
step:1691/2150 train_time:91176ms step_avg:53.92ms
step:1692/2150 train_time:91264ms step_avg:53.94ms
step:1693/2150 train_time:91353ms step_avg:53.96ms
step:1694/2150 train_time:91440ms step_avg:53.98ms
step:1695/2150 train_time:91529ms step_avg:54.00ms
step:1696/2150 train_time:91617ms step_avg:54.02ms
step:1697/2150 train_time:91706ms step_avg:54.04ms
step:1698/2150 train_time:91794ms step_avg:54.06ms
step:1699/2150 train_time:91883ms step_avg:54.08ms
step:1700/2150 train_time:91971ms step_avg:54.10ms
step:1701/2150 train_time:92060ms step_avg:54.12ms
step:1702/2150 train_time:92148ms step_avg:54.14ms
step:1703/2150 train_time:92237ms step_avg:54.16ms
step:1704/2150 train_time:92325ms step_avg:54.18ms
step:1705/2150 train_time:92414ms step_avg:54.20ms
step:1706/2150 train_time:92501ms step_avg:54.22ms
step:1707/2150 train_time:92590ms step_avg:54.24ms
step:1708/2150 train_time:92678ms step_avg:54.26ms
step:1709/2150 train_time:92767ms step_avg:54.28ms
step:1710/2150 train_time:92855ms step_avg:54.30ms
step:1711/2150 train_time:92944ms step_avg:54.32ms
step:1712/2150 train_time:93033ms step_avg:54.34ms
step:1713/2150 train_time:93122ms step_avg:54.36ms
step:1714/2150 train_time:93210ms step_avg:54.38ms
step:1715/2150 train_time:93300ms step_avg:54.40ms
step:1716/2150 train_time:93387ms step_avg:54.42ms
step:1717/2150 train_time:93475ms step_avg:54.44ms
step:1718/2150 train_time:93563ms step_avg:54.46ms
step:1719/2150 train_time:93652ms step_avg:54.48ms
step:1720/2150 train_time:93740ms step_avg:54.50ms
step:1721/2150 train_time:93830ms step_avg:54.52ms
step:1722/2150 train_time:93917ms step_avg:54.54ms
step:1723/2150 train_time:94007ms step_avg:54.56ms
step:1724/2150 train_time:94095ms step_avg:54.58ms
step:1725/2150 train_time:94183ms step_avg:54.60ms
step:1726/2150 train_time:94272ms step_avg:54.62ms
step:1727/2150 train_time:94362ms step_avg:54.64ms
step:1728/2150 train_time:94449ms step_avg:54.66ms
step:1729/2150 train_time:94537ms step_avg:54.68ms
step:1730/2150 train_time:94625ms step_avg:54.70ms
step:1731/2150 train_time:94714ms step_avg:54.72ms
step:1732/2150 train_time:94802ms step_avg:54.74ms
step:1733/2150 train_time:94890ms step_avg:54.76ms
step:1734/2150 train_time:94978ms step_avg:54.77ms
step:1735/2150 train_time:95068ms step_avg:54.79ms
step:1736/2150 train_time:95155ms step_avg:54.81ms
step:1737/2150 train_time:95244ms step_avg:54.83ms
step:1738/2150 train_time:95332ms step_avg:54.85ms
step:1739/2150 train_time:95420ms step_avg:54.87ms
step:1740/2150 train_time:95507ms step_avg:54.89ms
step:1741/2150 train_time:95596ms step_avg:54.91ms
step:1742/2150 train_time:95685ms step_avg:54.93ms
step:1743/2150 train_time:95775ms step_avg:54.95ms
step:1744/2150 train_time:95863ms step_avg:54.97ms
step:1745/2150 train_time:95953ms step_avg:54.99ms
step:1746/2150 train_time:96041ms step_avg:55.01ms
step:1747/2150 train_time:96130ms step_avg:55.03ms
step:1748/2150 train_time:96218ms step_avg:55.04ms
step:1749/2150 train_time:96308ms step_avg:55.06ms
step:1750/2150 train_time:96395ms step_avg:55.08ms
step:1750/2150 val_loss:3.3898 train_time:96487ms step_avg:55.14ms
step:1751/2150 train_time:96511ms step_avg:55.12ms
step:1752/2150 train_time:96577ms step_avg:55.12ms
step:1753/2150 train_time:96669ms step_avg:55.14ms
step:1754/2150 train_time:96757ms step_avg:55.16ms
step:1755/2150 train_time:96846ms step_avg:55.18ms
step:1756/2150 train_time:96934ms step_avg:55.20ms
step:1757/2150 train_time:97022ms step_avg:55.22ms
step:1758/2150 train_time:97109ms step_avg:55.24ms
step:1759/2150 train_time:97197ms step_avg:55.26ms
step:1760/2150 train_time:97284ms step_avg:55.28ms
step:1761/2150 train_time:97372ms step_avg:55.29ms
step:1762/2150 train_time:97461ms step_avg:55.31ms
step:1763/2150 train_time:97552ms step_avg:55.33ms
step:1764/2150 train_time:97641ms step_avg:55.35ms
step:1765/2150 train_time:97730ms step_avg:55.37ms
step:1766/2150 train_time:97817ms step_avg:55.39ms
step:1767/2150 train_time:97906ms step_avg:55.41ms
step:1768/2150 train_time:97992ms step_avg:55.43ms
step:1769/2150 train_time:98080ms step_avg:55.44ms
step:1770/2150 train_time:98167ms step_avg:55.46ms
step:1771/2150 train_time:98255ms step_avg:55.48ms
step:1772/2150 train_time:98343ms step_avg:55.50ms
step:1773/2150 train_time:98432ms step_avg:55.52ms
step:1774/2150 train_time:98520ms step_avg:55.54ms
step:1775/2150 train_time:98611ms step_avg:55.56ms
step:1776/2150 train_time:98699ms step_avg:55.57ms
step:1777/2150 train_time:98788ms step_avg:55.59ms
step:1778/2150 train_time:98876ms step_avg:55.61ms
step:1779/2150 train_time:98966ms step_avg:55.63ms
step:1780/2150 train_time:99051ms step_avg:55.65ms
step:1781/2150 train_time:99140ms step_avg:55.67ms
step:1782/2150 train_time:99226ms step_avg:55.68ms
step:1783/2150 train_time:99315ms step_avg:55.70ms
step:1784/2150 train_time:99402ms step_avg:55.72ms
step:1785/2150 train_time:99492ms step_avg:55.74ms
step:1786/2150 train_time:99580ms step_avg:55.76ms
step:1787/2150 train_time:99670ms step_avg:55.77ms
step:1788/2150 train_time:99758ms step_avg:55.79ms
step:1789/2150 train_time:99848ms step_avg:55.81ms
step:1790/2150 train_time:99936ms step_avg:55.83ms
step:1791/2150 train_time:100024ms step_avg:55.85ms
step:1792/2150 train_time:100112ms step_avg:55.87ms
step:1793/2150 train_time:100201ms step_avg:55.88ms
step:1794/2150 train_time:100287ms step_avg:55.90ms
step:1795/2150 train_time:100376ms step_avg:55.92ms
step:1796/2150 train_time:100466ms step_avg:55.94ms
step:1797/2150 train_time:100555ms step_avg:55.96ms
step:1798/2150 train_time:100643ms step_avg:55.97ms
step:1799/2150 train_time:100734ms step_avg:55.99ms
step:1800/2150 train_time:100821ms step_avg:56.01ms
step:1801/2150 train_time:100910ms step_avg:56.03ms
step:1802/2150 train_time:100998ms step_avg:56.05ms
step:1803/2150 train_time:101086ms step_avg:56.07ms
step:1804/2150 train_time:101173ms step_avg:56.08ms
step:1805/2150 train_time:101262ms step_avg:56.10ms
step:1806/2150 train_time:101348ms step_avg:56.12ms
step:1807/2150 train_time:101438ms step_avg:56.14ms
step:1808/2150 train_time:101527ms step_avg:56.15ms
step:1809/2150 train_time:101616ms step_avg:56.17ms
step:1810/2150 train_time:101704ms step_avg:56.19ms
step:1811/2150 train_time:101793ms step_avg:56.21ms
step:1812/2150 train_time:101880ms step_avg:56.23ms
step:1813/2150 train_time:101968ms step_avg:56.24ms
step:1814/2150 train_time:102055ms step_avg:56.26ms
step:1815/2150 train_time:102144ms step_avg:56.28ms
step:1816/2150 train_time:102233ms step_avg:56.30ms
step:1817/2150 train_time:102321ms step_avg:56.31ms
step:1818/2150 train_time:102408ms step_avg:56.33ms
step:1819/2150 train_time:102497ms step_avg:56.35ms
step:1820/2150 train_time:102585ms step_avg:56.37ms
step:1821/2150 train_time:102674ms step_avg:56.38ms
step:1822/2150 train_time:102763ms step_avg:56.40ms
step:1823/2150 train_time:102851ms step_avg:56.42ms
step:1824/2150 train_time:102939ms step_avg:56.44ms
step:1825/2150 train_time:103028ms step_avg:56.45ms
step:1826/2150 train_time:103115ms step_avg:56.47ms
step:1827/2150 train_time:103204ms step_avg:56.49ms
step:1828/2150 train_time:103291ms step_avg:56.51ms
step:1829/2150 train_time:103379ms step_avg:56.52ms
step:1830/2150 train_time:103466ms step_avg:56.54ms
step:1831/2150 train_time:103555ms step_avg:56.56ms
step:1832/2150 train_time:103643ms step_avg:56.57ms
step:1833/2150 train_time:103732ms step_avg:56.59ms
step:1834/2150 train_time:103820ms step_avg:56.61ms
step:1835/2150 train_time:103908ms step_avg:56.63ms
step:1836/2150 train_time:103996ms step_avg:56.64ms
step:1837/2150 train_time:104085ms step_avg:56.66ms
step:1838/2150 train_time:104173ms step_avg:56.68ms
step:1839/2150 train_time:104262ms step_avg:56.70ms
step:1840/2150 train_time:104349ms step_avg:56.71ms
step:1841/2150 train_time:104439ms step_avg:56.73ms
step:1842/2150 train_time:104527ms step_avg:56.75ms
step:1843/2150 train_time:104616ms step_avg:56.76ms
step:1844/2150 train_time:104705ms step_avg:56.78ms
step:1845/2150 train_time:104794ms step_avg:56.80ms
step:1846/2150 train_time:104881ms step_avg:56.82ms
step:1847/2150 train_time:104969ms step_avg:56.83ms
step:1848/2150 train_time:105056ms step_avg:56.85ms
step:1849/2150 train_time:105145ms step_avg:56.87ms
step:1850/2150 train_time:105233ms step_avg:56.88ms
step:1851/2150 train_time:105321ms step_avg:56.90ms
step:1852/2150 train_time:105409ms step_avg:56.92ms
step:1853/2150 train_time:105498ms step_avg:56.93ms
step:1854/2150 train_time:105587ms step_avg:56.95ms
step:1855/2150 train_time:105676ms step_avg:56.97ms
step:1856/2150 train_time:105764ms step_avg:56.99ms
step:1857/2150 train_time:105853ms step_avg:57.00ms
step:1858/2150 train_time:105940ms step_avg:57.02ms
step:1859/2150 train_time:106029ms step_avg:57.04ms
step:1860/2150 train_time:106116ms step_avg:57.05ms
step:1861/2150 train_time:106205ms step_avg:57.07ms
step:1862/2150 train_time:106293ms step_avg:57.09ms
step:1863/2150 train_time:106382ms step_avg:57.10ms
step:1864/2150 train_time:106469ms step_avg:57.12ms
step:1865/2150 train_time:106558ms step_avg:57.14ms
step:1866/2150 train_time:106646ms step_avg:57.15ms
step:1867/2150 train_time:106736ms step_avg:57.17ms
step:1868/2150 train_time:106823ms step_avg:57.19ms
step:1869/2150 train_time:106911ms step_avg:57.20ms
step:1870/2150 train_time:106998ms step_avg:57.22ms
step:1871/2150 train_time:107087ms step_avg:57.24ms
step:1872/2150 train_time:107175ms step_avg:57.25ms
step:1873/2150 train_time:107264ms step_avg:57.27ms
step:1874/2150 train_time:107351ms step_avg:57.28ms
step:1875/2150 train_time:107440ms step_avg:57.30ms
step:1876/2150 train_time:107527ms step_avg:57.32ms
step:1877/2150 train_time:107616ms step_avg:57.33ms
step:1878/2150 train_time:107704ms step_avg:57.35ms
step:1879/2150 train_time:107793ms step_avg:57.37ms
step:1880/2150 train_time:107881ms step_avg:57.38ms
step:1881/2150 train_time:107969ms step_avg:57.40ms
step:1882/2150 train_time:108057ms step_avg:57.42ms
step:1883/2150 train_time:108146ms step_avg:57.43ms
step:1884/2150 train_time:108233ms step_avg:57.45ms
step:1885/2150 train_time:108322ms step_avg:57.47ms
step:1886/2150 train_time:108410ms step_avg:57.48ms
step:1887/2150 train_time:108499ms step_avg:57.50ms
step:1888/2150 train_time:108587ms step_avg:57.51ms
step:1889/2150 train_time:108677ms step_avg:57.53ms
step:1890/2150 train_time:108765ms step_avg:57.55ms
step:1891/2150 train_time:108855ms step_avg:57.56ms
step:1892/2150 train_time:108942ms step_avg:57.58ms
step:1893/2150 train_time:109030ms step_avg:57.60ms
step:1894/2150 train_time:109117ms step_avg:57.61ms
step:1895/2150 train_time:109207ms step_avg:57.63ms
step:1896/2150 train_time:109294ms step_avg:57.64ms
step:1897/2150 train_time:109382ms step_avg:57.66ms
step:1898/2150 train_time:109469ms step_avg:57.68ms
step:1899/2150 train_time:109559ms step_avg:57.69ms
step:1900/2150 train_time:109646ms step_avg:57.71ms
step:1901/2150 train_time:109735ms step_avg:57.73ms
step:1902/2150 train_time:109822ms step_avg:57.74ms
step:1903/2150 train_time:109911ms step_avg:57.76ms
step:1904/2150 train_time:109999ms step_avg:57.77ms
step:1905/2150 train_time:110087ms step_avg:57.79ms
step:1906/2150 train_time:110175ms step_avg:57.80ms
step:1907/2150 train_time:110264ms step_avg:57.82ms
step:1908/2150 train_time:110351ms step_avg:57.84ms
step:1909/2150 train_time:110440ms step_avg:57.85ms
step:1910/2150 train_time:110527ms step_avg:57.87ms
step:1911/2150 train_time:110616ms step_avg:57.88ms
step:1912/2150 train_time:110704ms step_avg:57.90ms
step:1913/2150 train_time:110793ms step_avg:57.92ms
step:1914/2150 train_time:110880ms step_avg:57.93ms
step:1915/2150 train_time:110968ms step_avg:57.95ms
step:1916/2150 train_time:111056ms step_avg:57.96ms
step:1917/2150 train_time:111145ms step_avg:57.98ms
step:1918/2150 train_time:111232ms step_avg:57.99ms
step:1919/2150 train_time:111321ms step_avg:58.01ms
step:1920/2150 train_time:111408ms step_avg:58.02ms
step:1921/2150 train_time:111498ms step_avg:58.04ms
step:1922/2150 train_time:111584ms step_avg:58.06ms
step:1923/2150 train_time:111673ms step_avg:58.07ms
step:1924/2150 train_time:111761ms step_avg:58.09ms
step:1925/2150 train_time:111849ms step_avg:58.10ms
step:1926/2150 train_time:111938ms step_avg:58.12ms
step:1927/2150 train_time:112027ms step_avg:58.14ms
step:1928/2150 train_time:112114ms step_avg:58.15ms
step:1929/2150 train_time:112203ms step_avg:58.17ms
step:1930/2150 train_time:112291ms step_avg:58.18ms
step:1931/2150 train_time:112380ms step_avg:58.20ms
step:1932/2150 train_time:112468ms step_avg:58.21ms
step:1933/2150 train_time:112557ms step_avg:58.23ms
step:1934/2150 train_time:112645ms step_avg:58.24ms
step:1935/2150 train_time:112733ms step_avg:58.26ms
step:1936/2150 train_time:112821ms step_avg:58.28ms
step:1937/2150 train_time:112910ms step_avg:58.29ms
step:1938/2150 train_time:112997ms step_avg:58.31ms
step:1939/2150 train_time:113087ms step_avg:58.32ms
step:1940/2150 train_time:113175ms step_avg:58.34ms
step:1941/2150 train_time:113263ms step_avg:58.35ms
step:1942/2150 train_time:113350ms step_avg:58.37ms
step:1943/2150 train_time:113439ms step_avg:58.38ms
step:1944/2150 train_time:113526ms step_avg:58.40ms
step:1945/2150 train_time:113614ms step_avg:58.41ms
step:1946/2150 train_time:113701ms step_avg:58.43ms
step:1947/2150 train_time:113791ms step_avg:58.44ms
step:1948/2150 train_time:113880ms step_avg:58.46ms
step:1949/2150 train_time:113969ms step_avg:58.48ms
step:1950/2150 train_time:114057ms step_avg:58.49ms
step:1951/2150 train_time:114145ms step_avg:58.51ms
step:1952/2150 train_time:114232ms step_avg:58.52ms
step:1953/2150 train_time:114321ms step_avg:58.54ms
step:1954/2150 train_time:114408ms step_avg:58.55ms
step:1955/2150 train_time:114497ms step_avg:58.57ms
step:1956/2150 train_time:114585ms step_avg:58.58ms
step:1957/2150 train_time:114674ms step_avg:58.60ms
step:1958/2150 train_time:114761ms step_avg:58.61ms
step:1959/2150 train_time:114850ms step_avg:58.63ms
step:1960/2150 train_time:114938ms step_avg:58.64ms
step:1961/2150 train_time:115028ms step_avg:58.66ms
step:1962/2150 train_time:115115ms step_avg:58.67ms
step:1963/2150 train_time:115204ms step_avg:58.69ms
step:1964/2150 train_time:115292ms step_avg:58.70ms
step:1965/2150 train_time:115381ms step_avg:58.72ms
step:1966/2150 train_time:115469ms step_avg:58.73ms
step:1967/2150 train_time:115558ms step_avg:58.75ms
step:1968/2150 train_time:115645ms step_avg:58.76ms
step:1969/2150 train_time:115734ms step_avg:58.78ms
step:1970/2150 train_time:115822ms step_avg:58.79ms
step:1971/2150 train_time:115911ms step_avg:58.81ms
step:1972/2150 train_time:115998ms step_avg:58.82ms
step:1973/2150 train_time:116088ms step_avg:58.84ms
step:1974/2150 train_time:116176ms step_avg:58.85ms
step:1975/2150 train_time:116265ms step_avg:58.87ms
step:1976/2150 train_time:116352ms step_avg:58.88ms
step:1977/2150 train_time:116441ms step_avg:58.90ms
step:1978/2150 train_time:116530ms step_avg:58.91ms
step:1979/2150 train_time:116620ms step_avg:58.93ms
step:1980/2150 train_time:116707ms step_avg:58.94ms
step:1981/2150 train_time:116796ms step_avg:58.96ms
step:1982/2150 train_time:116884ms step_avg:58.97ms
step:1983/2150 train_time:116972ms step_avg:58.99ms
step:1984/2150 train_time:117060ms step_avg:59.00ms
step:1985/2150 train_time:117149ms step_avg:59.02ms
step:1986/2150 train_time:117237ms step_avg:59.03ms
step:1987/2150 train_time:117326ms step_avg:59.05ms
step:1988/2150 train_time:117413ms step_avg:59.06ms
step:1989/2150 train_time:117501ms step_avg:59.08ms
step:1990/2150 train_time:117588ms step_avg:59.09ms
step:1991/2150 train_time:117677ms step_avg:59.10ms
step:1992/2150 train_time:117766ms step_avg:59.12ms
step:1993/2150 train_time:117855ms step_avg:59.13ms
step:1994/2150 train_time:117942ms step_avg:59.15ms
step:1995/2150 train_time:118032ms step_avg:59.16ms
step:1996/2150 train_time:118119ms step_avg:59.18ms
step:1997/2150 train_time:118208ms step_avg:59.19ms
step:1998/2150 train_time:118296ms step_avg:59.21ms
step:1999/2150 train_time:118385ms step_avg:59.22ms
step:2000/2150 train_time:118472ms step_avg:59.24ms
step:2000/2150 val_loss:3.3134 train_time:118564ms step_avg:59.28ms
step:2001/2150 train_time:118587ms step_avg:59.26ms
step:2002/2150 train_time:118652ms step_avg:59.27ms
step:2003/2150 train_time:118746ms step_avg:59.28ms
step:2004/2150 train_time:118835ms step_avg:59.30ms
step:2005/2150 train_time:118923ms step_avg:59.31ms
step:2006/2150 train_time:119010ms step_avg:59.33ms
step:2007/2150 train_time:119097ms step_avg:59.34ms
step:2008/2150 train_time:119183ms step_avg:59.35ms
step:2009/2150 train_time:119271ms step_avg:59.37ms
step:2010/2150 train_time:119358ms step_avg:59.38ms
step:2011/2150 train_time:119447ms step_avg:59.40ms
step:2012/2150 train_time:119535ms step_avg:59.41ms
step:2013/2150 train_time:119626ms step_avg:59.43ms
step:2014/2150 train_time:119717ms step_avg:59.44ms
step:2015/2150 train_time:119807ms step_avg:59.46ms
step:2016/2150 train_time:119894ms step_avg:59.47ms
step:2017/2150 train_time:119983ms step_avg:59.49ms
step:2018/2150 train_time:120070ms step_avg:59.50ms
step:2019/2150 train_time:120158ms step_avg:59.51ms
step:2020/2150 train_time:120244ms step_avg:59.53ms
step:2021/2150 train_time:120332ms step_avg:59.54ms
step:2022/2150 train_time:120419ms step_avg:59.55ms
step:2023/2150 train_time:120509ms step_avg:59.57ms
step:2024/2150 train_time:120598ms step_avg:59.58ms
step:2025/2150 train_time:120688ms step_avg:59.60ms
step:2026/2150 train_time:120777ms step_avg:59.61ms
step:2027/2150 train_time:120867ms step_avg:59.63ms
step:2028/2150 train_time:120953ms step_avg:59.64ms
step:2029/2150 train_time:121042ms step_avg:59.66ms
step:2030/2150 train_time:121128ms step_avg:59.67ms
step:2031/2150 train_time:121217ms step_avg:59.68ms
step:2032/2150 train_time:121304ms step_avg:59.70ms
step:2033/2150 train_time:121392ms step_avg:59.71ms
step:2034/2150 train_time:121479ms step_avg:59.72ms
step:2035/2150 train_time:121569ms step_avg:59.74ms
step:2036/2150 train_time:121657ms step_avg:59.75ms
step:2037/2150 train_time:121746ms step_avg:59.77ms
step:2038/2150 train_time:121836ms step_avg:59.78ms
step:2039/2150 train_time:121925ms step_avg:59.80ms
step:2040/2150 train_time:122013ms step_avg:59.81ms
step:2041/2150 train_time:122101ms step_avg:59.82ms
step:2042/2150 train_time:122188ms step_avg:59.84ms
step:2043/2150 train_time:122276ms step_avg:59.85ms
step:2044/2150 train_time:122363ms step_avg:59.86ms
step:2045/2150 train_time:122453ms step_avg:59.88ms
step:2046/2150 train_time:122540ms step_avg:59.89ms
step:2047/2150 train_time:122629ms step_avg:59.91ms
step:2048/2150 train_time:122717ms step_avg:59.92ms
step:2049/2150 train_time:122806ms step_avg:59.93ms
step:2050/2150 train_time:122894ms step_avg:59.95ms
step:2051/2150 train_time:122983ms step_avg:59.96ms
step:2052/2150 train_time:123071ms step_avg:59.98ms
step:2053/2150 train_time:123159ms step_avg:59.99ms
step:2054/2150 train_time:123246ms step_avg:60.00ms
step:2055/2150 train_time:123334ms step_avg:60.02ms
step:2056/2150 train_time:123421ms step_avg:60.03ms
step:2057/2150 train_time:123511ms step_avg:60.04ms
step:2058/2150 train_time:123599ms step_avg:60.06ms
step:2059/2150 train_time:123688ms step_avg:60.07ms
step:2060/2150 train_time:123776ms step_avg:60.09ms
step:2061/2150 train_time:123866ms step_avg:60.10ms
step:2062/2150 train_time:123953ms step_avg:60.11ms
step:2063/2150 train_time:124042ms step_avg:60.13ms
step:2064/2150 train_time:124129ms step_avg:60.14ms
step:2065/2150 train_time:124218ms step_avg:60.15ms
step:2066/2150 train_time:124305ms step_avg:60.17ms
step:2067/2150 train_time:124394ms step_avg:60.18ms
step:2068/2150 train_time:124480ms step_avg:60.19ms
step:2069/2150 train_time:124570ms step_avg:60.21ms
step:2070/2150 train_time:124657ms step_avg:60.22ms
step:2071/2150 train_time:124747ms step_avg:60.23ms
step:2072/2150 train_time:124835ms step_avg:60.25ms
step:2073/2150 train_time:124924ms step_avg:60.26ms
step:2074/2150 train_time:125011ms step_avg:60.28ms
step:2075/2150 train_time:125100ms step_avg:60.29ms
step:2076/2150 train_time:125187ms step_avg:60.30ms
step:2077/2150 train_time:125276ms step_avg:60.32ms
step:2078/2150 train_time:125363ms step_avg:60.33ms
step:2079/2150 train_time:125453ms step_avg:60.34ms
step:2080/2150 train_time:125540ms step_avg:60.36ms
step:2081/2150 train_time:125629ms step_avg:60.37ms
step:2082/2150 train_time:125718ms step_avg:60.38ms
step:2083/2150 train_time:125806ms step_avg:60.40ms
step:2084/2150 train_time:125894ms step_avg:60.41ms
step:2085/2150 train_time:125983ms step_avg:60.42ms
step:2086/2150 train_time:126071ms step_avg:60.44ms
step:2087/2150 train_time:126161ms step_avg:60.45ms
step:2088/2150 train_time:126248ms step_avg:60.46ms
step:2089/2150 train_time:126337ms step_avg:60.48ms
step:2090/2150 train_time:126424ms step_avg:60.49ms
step:2091/2150 train_time:126514ms step_avg:60.50ms
step:2092/2150 train_time:126601ms step_avg:60.52ms
step:2093/2150 train_time:126691ms step_avg:60.53ms
step:2094/2150 train_time:126778ms step_avg:60.54ms
step:2095/2150 train_time:126867ms step_avg:60.56ms
step:2096/2150 train_time:126954ms step_avg:60.57ms
step:2097/2150 train_time:127042ms step_avg:60.58ms
step:2098/2150 train_time:127130ms step_avg:60.60ms
step:2099/2150 train_time:127219ms step_avg:60.61ms
step:2100/2150 train_time:127306ms step_avg:60.62ms
step:2101/2150 train_time:127395ms step_avg:60.64ms
step:2102/2150 train_time:127482ms step_avg:60.65ms
step:2103/2150 train_time:127572ms step_avg:60.66ms
step:2104/2150 train_time:127659ms step_avg:60.67ms
step:2105/2150 train_time:127749ms step_avg:60.69ms
step:2106/2150 train_time:127837ms step_avg:60.70ms
step:2107/2150 train_time:127926ms step_avg:60.71ms
step:2108/2150 train_time:128013ms step_avg:60.73ms
step:2109/2150 train_time:128101ms step_avg:60.74ms
step:2110/2150 train_time:128188ms step_avg:60.75ms
step:2111/2150 train_time:128277ms step_avg:60.77ms
step:2112/2150 train_time:128364ms step_avg:60.78ms
step:2113/2150 train_time:128454ms step_avg:60.79ms
step:2114/2150 train_time:128541ms step_avg:60.80ms
step:2115/2150 train_time:128631ms step_avg:60.82ms
step:2116/2150 train_time:128719ms step_avg:60.83ms
step:2117/2150 train_time:128809ms step_avg:60.84ms
step:2118/2150 train_time:128897ms step_avg:60.86ms
step:2119/2150 train_time:128986ms step_avg:60.87ms
step:2120/2150 train_time:129074ms step_avg:60.88ms
step:2121/2150 train_time:129162ms step_avg:60.90ms
step:2122/2150 train_time:129250ms step_avg:60.91ms
step:2123/2150 train_time:129340ms step_avg:60.92ms
step:2124/2150 train_time:129428ms step_avg:60.94ms
step:2125/2150 train_time:129519ms step_avg:60.95ms
step:2126/2150 train_time:129607ms step_avg:60.96ms
step:2127/2150 train_time:129696ms step_avg:60.98ms
step:2128/2150 train_time:129783ms step_avg:60.99ms
step:2129/2150 train_time:129873ms step_avg:61.00ms
step:2130/2150 train_time:129959ms step_avg:61.01ms
step:2131/2150 train_time:130048ms step_avg:61.03ms
step:2132/2150 train_time:130136ms step_avg:61.04ms
step:2133/2150 train_time:130226ms step_avg:61.05ms
step:2134/2150 train_time:130313ms step_avg:61.07ms
step:2135/2150 train_time:130403ms step_avg:61.08ms
step:2136/2150 train_time:130491ms step_avg:61.09ms
step:2137/2150 train_time:130580ms step_avg:61.10ms
step:2138/2150 train_time:130669ms step_avg:61.12ms
step:2139/2150 train_time:130758ms step_avg:61.13ms
step:2140/2150 train_time:130845ms step_avg:61.14ms
step:2141/2150 train_time:130934ms step_avg:61.16ms
step:2142/2150 train_time:131021ms step_avg:61.17ms
step:2143/2150 train_time:131111ms step_avg:61.18ms
step:2144/2150 train_time:131198ms step_avg:61.19ms
step:2145/2150 train_time:131287ms step_avg:61.21ms
step:2146/2150 train_time:131374ms step_avg:61.22ms
step:2147/2150 train_time:131463ms step_avg:61.23ms
step:2148/2150 train_time:131551ms step_avg:61.24ms
step:2149/2150 train_time:131641ms step_avg:61.26ms
step:2150/2150 train_time:131729ms step_avg:61.27ms
step:2150/2150 val_loss:3.2784 train_time:131820ms step_avg:61.31ms
peak memory allocated: 29892 MiB reserved: 44416 MiB

import uuid
run_id = f"NorMuon Fixes and Optims 8xH100 - 2150 steps - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2110  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 10 2025, 18:15:56) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 19:08:15 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   44C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   44C    P0            131W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   44C    P0            127W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   34C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   43C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   34C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           26617      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           26618      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           26619      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           26620      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           26621      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           26622      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           26623      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           26624      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           26618      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           26619      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           26620      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           26621      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           26622      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           26623      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           26624      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2150 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2150 train_time:87ms step_avg:87.20ms
step:2/2150 train_time:164ms step_avg:81.80ms
step:3/2150 train_time:186ms step_avg:62.07ms
step:4/2150 train_time:210ms step_avg:52.53ms
step:5/2150 train_time:232ms step_avg:46.40ms
step:6/2150 train_time:334ms step_avg:55.62ms
step:7/2150 train_time:365ms step_avg:52.17ms
step:8/2150 train_time:398ms step_avg:49.81ms
step:9/2150 train_time:431ms step_avg:47.84ms
step:10/2150 train_time:464ms step_avg:46.39ms
step:11/2150 train_time:497ms step_avg:45.14ms
step:12/2150 train_time:530ms step_avg:44.17ms
step:13/2150 train_time:563ms step_avg:43.30ms
step:14/2150 train_time:596ms step_avg:42.61ms
step:15/2150 train_time:629ms step_avg:41.94ms
step:16/2150 train_time:663ms step_avg:41.41ms
step:17/2150 train_time:695ms step_avg:40.91ms
step:18/2150 train_time:729ms step_avg:40.49ms
step:19/2150 train_time:762ms step_avg:40.09ms
step:20/2150 train_time:795ms step_avg:39.76ms
step:21/2150 train_time:828ms step_avg:39.42ms
step:22/2150 train_time:862ms step_avg:39.16ms
step:23/2150 train_time:894ms step_avg:38.88ms
step:24/2150 train_time:928ms step_avg:38.68ms
step:25/2150 train_time:960ms step_avg:38.41ms
step:26/2150 train_time:994ms step_avg:38.22ms
step:27/2150 train_time:1026ms step_avg:38.01ms
step:28/2150 train_time:1060ms step_avg:37.86ms
step:29/2150 train_time:1092ms step_avg:37.67ms
step:30/2150 train_time:1126ms step_avg:37.53ms
step:31/2150 train_time:1158ms step_avg:37.37ms
step:32/2150 train_time:1192ms step_avg:37.25ms
step:33/2150 train_time:1225ms step_avg:37.11ms
step:34/2150 train_time:1259ms step_avg:37.03ms
step:35/2150 train_time:1294ms step_avg:36.97ms
step:36/2150 train_time:1328ms step_avg:36.90ms
step:37/2150 train_time:1362ms step_avg:36.82ms
step:38/2150 train_time:1396ms step_avg:36.74ms
step:39/2150 train_time:1429ms step_avg:36.65ms
step:40/2150 train_time:1462ms step_avg:36.56ms
step:41/2150 train_time:1496ms step_avg:36.48ms
step:42/2150 train_time:1529ms step_avg:36.41ms
step:43/2150 train_time:1563ms step_avg:36.34ms
step:44/2150 train_time:1596ms step_avg:36.27ms
step:45/2150 train_time:1629ms step_avg:36.19ms
step:46/2150 train_time:1663ms step_avg:36.14ms
step:47/2150 train_time:1695ms step_avg:36.06ms
step:48/2150 train_time:1729ms step_avg:36.01ms
step:49/2150 train_time:1761ms step_avg:35.94ms
step:50/2150 train_time:1795ms step_avg:35.89ms
step:51/2150 train_time:1827ms step_avg:35.83ms
step:52/2150 train_time:1861ms step_avg:35.78ms
step:53/2150 train_time:1894ms step_avg:35.73ms
step:54/2150 train_time:1927ms step_avg:35.69ms
step:55/2150 train_time:1960ms step_avg:35.63ms
step:56/2150 train_time:1993ms step_avg:35.59ms
step:57/2150 train_time:2026ms step_avg:35.54ms
step:58/2150 train_time:2060ms step_avg:35.51ms
step:59/2150 train_time:2092ms step_avg:35.46ms
step:60/2150 train_time:2126ms step_avg:35.43ms
step:61/2150 train_time:2158ms step_avg:35.38ms
step:62/2150 train_time:2192ms step_avg:35.35ms
step:63/2150 train_time:2225ms step_avg:35.32ms
step:64/2150 train_time:2262ms step_avg:35.34ms
step:65/2150 train_time:2292ms step_avg:35.27ms
step:66/2150 train_time:2326ms step_avg:35.24ms
step:67/2150 train_time:2359ms step_avg:35.21ms
step:68/2150 train_time:2393ms step_avg:35.19ms
step:69/2150 train_time:2426ms step_avg:35.16ms
step:70/2150 train_time:2460ms step_avg:35.14ms
step:71/2150 train_time:2493ms step_avg:35.12ms
step:72/2150 train_time:2527ms step_avg:35.10ms
step:73/2150 train_time:2560ms step_avg:35.07ms
step:74/2150 train_time:2593ms step_avg:35.04ms
step:75/2150 train_time:2626ms step_avg:35.02ms
step:76/2150 train_time:2660ms step_avg:35.00ms
step:77/2150 train_time:2693ms step_avg:34.97ms
step:78/2150 train_time:2726ms step_avg:34.95ms
step:79/2150 train_time:2759ms step_avg:34.93ms
step:80/2150 train_time:2792ms step_avg:34.91ms
step:81/2150 train_time:2825ms step_avg:34.88ms
step:82/2150 train_time:2859ms step_avg:34.86ms
step:83/2150 train_time:2891ms step_avg:34.84ms
step:84/2150 train_time:2925ms step_avg:34.82ms
step:85/2150 train_time:2957ms step_avg:34.79ms
step:86/2150 train_time:2991ms step_avg:34.78ms
step:87/2150 train_time:3023ms step_avg:34.75ms
step:88/2150 train_time:3057ms step_avg:34.74ms
step:89/2150 train_time:3090ms step_avg:34.72ms
step:90/2150 train_time:3123ms step_avg:34.70ms
step:91/2150 train_time:3156ms step_avg:34.68ms
step:92/2150 train_time:3190ms step_avg:34.67ms
step:93/2150 train_time:3222ms step_avg:34.65ms
step:94/2150 train_time:3256ms step_avg:34.64ms
step:95/2150 train_time:3290ms step_avg:34.63ms
step:96/2150 train_time:3323ms step_avg:34.62ms
step:97/2150 train_time:3356ms step_avg:34.60ms
step:98/2150 train_time:3390ms step_avg:34.59ms
step:99/2150 train_time:3423ms step_avg:34.58ms
step:100/2150 train_time:3457ms step_avg:34.57ms
step:101/2150 train_time:3490ms step_avg:34.56ms
step:102/2150 train_time:3524ms step_avg:34.55ms
step:103/2150 train_time:3557ms step_avg:34.53ms
step:104/2150 train_time:3590ms step_avg:34.52ms
step:105/2150 train_time:3623ms step_avg:34.51ms
step:106/2150 train_time:3656ms step_avg:34.49ms
step:107/2150 train_time:3689ms step_avg:34.48ms
step:108/2150 train_time:3723ms step_avg:34.47ms
step:109/2150 train_time:3758ms step_avg:34.48ms
step:110/2150 train_time:3789ms step_avg:34.44ms
step:111/2150 train_time:3821ms step_avg:34.42ms
step:112/2150 train_time:3854ms step_avg:34.42ms
step:113/2150 train_time:3888ms step_avg:34.40ms
step:114/2150 train_time:3921ms step_avg:34.39ms
step:115/2150 train_time:3954ms step_avg:34.38ms
step:116/2150 train_time:3987ms step_avg:34.37ms
step:117/2150 train_time:4019ms step_avg:34.35ms
step:118/2150 train_time:4053ms step_avg:34.35ms
step:119/2150 train_time:4086ms step_avg:34.33ms
step:120/2150 train_time:4119ms step_avg:34.33ms
step:121/2150 train_time:4152ms step_avg:34.31ms
step:122/2150 train_time:4185ms step_avg:34.31ms
step:123/2150 train_time:4218ms step_avg:34.29ms
step:124/2150 train_time:4251ms step_avg:34.29ms
step:125/2150 train_time:4284ms step_avg:34.27ms
step:126/2150 train_time:4317ms step_avg:34.27ms
step:127/2150 train_time:4351ms step_avg:34.26ms
step:128/2150 train_time:4384ms step_avg:34.25ms
step:129/2150 train_time:4417ms step_avg:34.24ms
step:130/2150 train_time:4451ms step_avg:34.24ms
step:131/2150 train_time:4484ms step_avg:34.23ms
step:132/2150 train_time:4517ms step_avg:34.22ms
step:133/2150 train_time:4550ms step_avg:34.21ms
step:134/2150 train_time:4583ms step_avg:34.20ms
step:135/2150 train_time:4616ms step_avg:34.20ms
step:136/2150 train_time:4650ms step_avg:34.19ms
step:137/2150 train_time:4683ms step_avg:34.18ms
step:138/2150 train_time:4716ms step_avg:34.18ms
step:139/2150 train_time:4749ms step_avg:34.17ms
step:140/2150 train_time:4782ms step_avg:34.16ms
step:141/2150 train_time:4815ms step_avg:34.15ms
step:142/2150 train_time:4849ms step_avg:34.14ms
step:143/2150 train_time:4881ms step_avg:34.13ms
step:144/2150 train_time:4915ms step_avg:34.13ms
step:145/2150 train_time:4947ms step_avg:34.12ms
step:146/2150 train_time:4981ms step_avg:34.11ms
step:147/2150 train_time:5014ms step_avg:34.11ms
step:148/2150 train_time:5047ms step_avg:34.10ms
step:149/2150 train_time:5080ms step_avg:34.09ms
step:150/2150 train_time:5113ms step_avg:34.09ms
step:151/2150 train_time:5146ms step_avg:34.08ms
step:152/2150 train_time:5180ms step_avg:34.08ms
step:153/2150 train_time:5212ms step_avg:34.07ms
step:154/2150 train_time:5246ms step_avg:34.06ms
step:155/2150 train_time:5279ms step_avg:34.06ms
step:156/2150 train_time:5312ms step_avg:34.05ms
step:157/2150 train_time:5345ms step_avg:34.05ms
step:158/2150 train_time:5379ms step_avg:34.04ms
step:159/2150 train_time:5411ms step_avg:34.03ms
step:160/2150 train_time:5445ms step_avg:34.03ms
step:161/2150 train_time:5478ms step_avg:34.02ms
step:162/2150 train_time:5511ms step_avg:34.02ms
step:163/2150 train_time:5544ms step_avg:34.01ms
step:164/2150 train_time:5577ms step_avg:34.01ms
step:165/2150 train_time:5610ms step_avg:34.00ms
step:166/2150 train_time:5644ms step_avg:34.00ms
step:167/2150 train_time:5677ms step_avg:33.99ms
step:168/2150 train_time:5710ms step_avg:33.99ms
step:169/2150 train_time:5743ms step_avg:33.98ms
step:170/2150 train_time:5777ms step_avg:33.98ms
step:171/2150 train_time:5809ms step_avg:33.97ms
step:172/2150 train_time:5842ms step_avg:33.97ms
step:173/2150 train_time:5875ms step_avg:33.96ms
step:174/2150 train_time:5908ms step_avg:33.95ms
step:175/2150 train_time:5941ms step_avg:33.95ms
step:176/2150 train_time:5974ms step_avg:33.95ms
step:177/2150 train_time:6007ms step_avg:33.94ms
step:178/2150 train_time:6041ms step_avg:33.94ms
step:179/2150 train_time:6073ms step_avg:33.93ms
step:180/2150 train_time:6107ms step_avg:33.93ms
step:181/2150 train_time:6139ms step_avg:33.92ms
step:182/2150 train_time:6173ms step_avg:33.92ms
step:183/2150 train_time:6205ms step_avg:33.91ms
step:184/2150 train_time:6238ms step_avg:33.90ms
step:185/2150 train_time:6271ms step_avg:33.90ms
step:186/2150 train_time:6304ms step_avg:33.89ms
step:187/2150 train_time:6337ms step_avg:33.89ms
step:188/2150 train_time:6370ms step_avg:33.88ms
step:189/2150 train_time:6402ms step_avg:33.87ms
step:190/2150 train_time:6436ms step_avg:33.87ms
step:191/2150 train_time:6469ms step_avg:33.87ms
step:192/2150 train_time:6503ms step_avg:33.87ms
step:193/2150 train_time:6535ms step_avg:33.86ms
step:194/2150 train_time:6569ms step_avg:33.86ms
step:195/2150 train_time:6601ms step_avg:33.85ms
step:196/2150 train_time:6634ms step_avg:33.85ms
step:197/2150 train_time:6668ms step_avg:33.85ms
step:198/2150 train_time:6701ms step_avg:33.84ms
step:199/2150 train_time:6734ms step_avg:33.84ms
step:200/2150 train_time:6767ms step_avg:33.84ms
step:201/2150 train_time:6801ms step_avg:33.83ms
step:202/2150 train_time:6834ms step_avg:33.83ms
step:203/2150 train_time:6866ms step_avg:33.82ms
step:204/2150 train_time:6900ms step_avg:33.82ms
step:205/2150 train_time:6932ms step_avg:33.82ms
step:206/2150 train_time:6965ms step_avg:33.81ms
step:207/2150 train_time:6998ms step_avg:33.81ms
step:208/2150 train_time:7031ms step_avg:33.80ms
step:209/2150 train_time:7064ms step_avg:33.80ms
step:210/2150 train_time:7098ms step_avg:33.80ms
step:211/2150 train_time:7130ms step_avg:33.79ms
step:212/2150 train_time:7163ms step_avg:33.79ms
step:213/2150 train_time:7196ms step_avg:33.79ms
step:214/2150 train_time:7230ms step_avg:33.78ms
step:215/2150 train_time:7263ms step_avg:33.78ms
step:216/2150 train_time:7296ms step_avg:33.78ms
step:217/2150 train_time:7329ms step_avg:33.77ms
step:218/2150 train_time:7362ms step_avg:33.77ms
step:219/2150 train_time:7395ms step_avg:33.77ms
step:220/2150 train_time:7428ms step_avg:33.76ms
step:221/2150 train_time:7461ms step_avg:33.76ms
step:222/2150 train_time:7494ms step_avg:33.76ms
step:223/2150 train_time:7527ms step_avg:33.75ms
step:224/2150 train_time:7560ms step_avg:33.75ms
step:225/2150 train_time:7593ms step_avg:33.75ms
step:226/2150 train_time:7627ms step_avg:33.75ms
step:227/2150 train_time:7660ms step_avg:33.74ms
step:228/2150 train_time:7693ms step_avg:33.74ms
step:229/2150 train_time:7726ms step_avg:33.74ms
step:230/2150 train_time:7759ms step_avg:33.74ms
step:231/2150 train_time:7792ms step_avg:33.73ms
step:232/2150 train_time:7826ms step_avg:33.73ms
step:233/2150 train_time:7858ms step_avg:33.72ms
step:234/2150 train_time:7891ms step_avg:33.72ms
step:235/2150 train_time:7924ms step_avg:33.72ms
step:236/2150 train_time:7957ms step_avg:33.72ms
step:237/2150 train_time:7990ms step_avg:33.71ms
step:238/2150 train_time:8023ms step_avg:33.71ms
step:239/2150 train_time:8057ms step_avg:33.71ms
step:240/2150 train_time:8089ms step_avg:33.70ms
step:241/2150 train_time:8122ms step_avg:33.70ms
step:242/2150 train_time:8155ms step_avg:33.70ms
step:243/2150 train_time:8188ms step_avg:33.69ms
step:244/2150 train_time:8221ms step_avg:33.69ms
step:245/2150 train_time:8253ms step_avg:33.69ms
step:246/2150 train_time:8287ms step_avg:33.69ms
step:247/2150 train_time:8319ms step_avg:33.68ms
step:248/2150 train_time:8352ms step_avg:33.68ms
step:249/2150 train_time:8385ms step_avg:33.68ms
step:250/2150 train_time:8419ms step_avg:33.68ms
step:250/2150 val_loss:4.3051 train_time:8455ms step_avg:33.82ms
step:251/2150 train_time:8477ms step_avg:33.77ms
step:252/2150 train_time:8499ms step_avg:33.73ms
step:253/2150 train_time:8520ms step_avg:33.68ms
step:254/2150 train_time:8557ms step_avg:33.69ms
step:255/2150 train_time:8593ms step_avg:33.70ms
step:256/2150 train_time:8629ms step_avg:33.71ms
step:257/2150 train_time:8663ms step_avg:33.71ms
step:258/2150 train_time:8696ms step_avg:33.71ms
step:259/2150 train_time:8729ms step_avg:33.70ms
step:260/2150 train_time:8762ms step_avg:33.70ms
step:261/2150 train_time:8795ms step_avg:33.70ms
step:262/2150 train_time:8829ms step_avg:33.70ms
step:263/2150 train_time:8861ms step_avg:33.69ms
step:264/2150 train_time:8894ms step_avg:33.69ms
step:265/2150 train_time:8926ms step_avg:33.68ms
step:266/2150 train_time:8960ms step_avg:33.68ms
step:267/2150 train_time:8992ms step_avg:33.68ms
step:268/2150 train_time:9025ms step_avg:33.68ms
step:269/2150 train_time:9058ms step_avg:33.67ms
step:270/2150 train_time:9091ms step_avg:33.67ms
step:271/2150 train_time:9124ms step_avg:33.67ms
step:272/2150 train_time:9157ms step_avg:33.66ms
step:273/2150 train_time:9189ms step_avg:33.66ms
step:274/2150 train_time:9222ms step_avg:33.66ms
step:275/2150 train_time:9255ms step_avg:33.65ms
step:276/2150 train_time:9288ms step_avg:33.65ms
step:277/2150 train_time:9320ms step_avg:33.65ms
step:278/2150 train_time:9354ms step_avg:33.65ms
step:279/2150 train_time:9386ms step_avg:33.64ms
step:280/2150 train_time:9420ms step_avg:33.64ms
step:281/2150 train_time:9452ms step_avg:33.64ms
step:282/2150 train_time:9486ms step_avg:33.64ms
step:283/2150 train_time:9519ms step_avg:33.64ms
step:284/2150 train_time:9553ms step_avg:33.64ms
step:285/2150 train_time:9586ms step_avg:33.64ms
step:286/2150 train_time:9620ms step_avg:33.64ms
step:287/2150 train_time:9653ms step_avg:33.64ms
step:288/2150 train_time:9687ms step_avg:33.63ms
step:289/2150 train_time:9720ms step_avg:33.63ms
step:290/2150 train_time:9753ms step_avg:33.63ms
step:291/2150 train_time:9786ms step_avg:33.63ms
step:292/2150 train_time:9820ms step_avg:33.63ms
step:293/2150 train_time:9852ms step_avg:33.63ms
step:294/2150 train_time:9886ms step_avg:33.62ms
step:295/2150 train_time:9918ms step_avg:33.62ms
step:296/2150 train_time:9951ms step_avg:33.62ms
step:297/2150 train_time:9984ms step_avg:33.62ms
step:298/2150 train_time:10017ms step_avg:33.62ms
step:299/2150 train_time:10050ms step_avg:33.61ms
step:300/2150 train_time:10083ms step_avg:33.61ms
step:301/2150 train_time:10115ms step_avg:33.61ms
step:302/2150 train_time:10149ms step_avg:33.61ms
step:303/2150 train_time:10181ms step_avg:33.60ms
step:304/2150 train_time:10215ms step_avg:33.60ms
step:305/2150 train_time:10247ms step_avg:33.60ms
step:306/2150 train_time:10281ms step_avg:33.60ms
step:307/2150 train_time:10313ms step_avg:33.59ms
step:308/2150 train_time:10346ms step_avg:33.59ms
step:309/2150 train_time:10379ms step_avg:33.59ms
step:310/2150 train_time:10412ms step_avg:33.59ms
step:311/2150 train_time:10445ms step_avg:33.59ms
step:312/2150 train_time:10479ms step_avg:33.59ms
step:313/2150 train_time:10512ms step_avg:33.58ms
step:314/2150 train_time:10545ms step_avg:33.58ms
step:315/2150 train_time:10578ms step_avg:33.58ms
step:316/2150 train_time:10611ms step_avg:33.58ms
step:317/2150 train_time:10644ms step_avg:33.58ms
step:318/2150 train_time:10677ms step_avg:33.58ms
step:319/2150 train_time:10710ms step_avg:33.57ms
step:320/2150 train_time:10744ms step_avg:33.57ms
step:321/2150 train_time:10776ms step_avg:33.57ms
step:322/2150 train_time:10810ms step_avg:33.57ms
step:323/2150 train_time:10843ms step_avg:33.57ms
step:324/2150 train_time:10876ms step_avg:33.57ms
step:325/2150 train_time:10909ms step_avg:33.57ms
step:326/2150 train_time:10942ms step_avg:33.56ms
step:327/2150 train_time:10975ms step_avg:33.56ms
step:328/2150 train_time:11008ms step_avg:33.56ms
step:329/2150 train_time:11041ms step_avg:33.56ms
step:330/2150 train_time:11074ms step_avg:33.56ms
step:331/2150 train_time:11107ms step_avg:33.55ms
step:332/2150 train_time:11140ms step_avg:33.55ms
step:333/2150 train_time:11172ms step_avg:33.55ms
step:334/2150 train_time:11206ms step_avg:33.55ms
step:335/2150 train_time:11238ms step_avg:33.55ms
step:336/2150 train_time:11272ms step_avg:33.55ms
step:337/2150 train_time:11305ms step_avg:33.55ms
step:338/2150 train_time:11337ms step_avg:33.54ms
step:339/2150 train_time:11369ms step_avg:33.54ms
step:340/2150 train_time:11403ms step_avg:33.54ms
step:341/2150 train_time:11436ms step_avg:33.54ms
step:342/2150 train_time:11469ms step_avg:33.53ms
step:343/2150 train_time:11502ms step_avg:33.53ms
step:344/2150 train_time:11535ms step_avg:33.53ms
step:345/2150 train_time:11568ms step_avg:33.53ms
step:346/2150 train_time:11602ms step_avg:33.53ms
step:347/2150 train_time:11635ms step_avg:33.53ms
step:348/2150 train_time:11668ms step_avg:33.53ms
step:349/2150 train_time:11701ms step_avg:33.53ms
step:350/2150 train_time:11735ms step_avg:33.53ms
step:351/2150 train_time:11768ms step_avg:33.53ms
step:352/2150 train_time:11801ms step_avg:33.53ms
step:353/2150 train_time:11834ms step_avg:33.52ms
step:354/2150 train_time:11867ms step_avg:33.52ms
step:355/2150 train_time:11900ms step_avg:33.52ms
step:356/2150 train_time:11933ms step_avg:33.52ms
step:357/2150 train_time:11967ms step_avg:33.52ms
step:358/2150 train_time:12000ms step_avg:33.52ms
step:359/2150 train_time:12033ms step_avg:33.52ms
step:360/2150 train_time:12066ms step_avg:33.52ms
step:361/2150 train_time:12098ms step_avg:33.51ms
step:362/2150 train_time:12132ms step_avg:33.51ms
step:363/2150 train_time:12164ms step_avg:33.51ms
step:364/2150 train_time:12197ms step_avg:33.51ms
step:365/2150 train_time:12230ms step_avg:33.51ms
step:366/2150 train_time:12263ms step_avg:33.51ms
step:367/2150 train_time:12295ms step_avg:33.50ms
step:368/2150 train_time:12329ms step_avg:33.50ms
step:369/2150 train_time:12361ms step_avg:33.50ms
step:370/2150 train_time:12394ms step_avg:33.50ms
step:371/2150 train_time:12427ms step_avg:33.50ms
step:372/2150 train_time:12460ms step_avg:33.50ms
step:373/2150 train_time:12494ms step_avg:33.49ms
step:374/2150 train_time:12527ms step_avg:33.49ms
step:375/2150 train_time:12560ms step_avg:33.49ms
step:376/2150 train_time:12593ms step_avg:33.49ms
step:377/2150 train_time:12627ms step_avg:33.49ms
step:378/2150 train_time:12660ms step_avg:33.49ms
step:379/2150 train_time:12692ms step_avg:33.49ms
step:380/2150 train_time:12726ms step_avg:33.49ms
step:381/2150 train_time:12758ms step_avg:33.49ms
step:382/2150 train_time:12795ms step_avg:33.49ms
step:383/2150 train_time:12825ms step_avg:33.48ms
step:384/2150 train_time:12858ms step_avg:33.48ms
step:385/2150 train_time:12891ms step_avg:33.48ms
step:386/2150 train_time:12924ms step_avg:33.48ms
step:387/2150 train_time:12957ms step_avg:33.48ms
step:388/2150 train_time:12990ms step_avg:33.48ms
step:389/2150 train_time:13023ms step_avg:33.48ms
step:390/2150 train_time:13056ms step_avg:33.48ms
step:391/2150 train_time:13089ms step_avg:33.48ms
step:392/2150 train_time:13123ms step_avg:33.48ms
step:393/2150 train_time:13155ms step_avg:33.47ms
step:394/2150 train_time:13188ms step_avg:33.47ms
step:395/2150 train_time:13220ms step_avg:33.47ms
step:396/2150 train_time:13254ms step_avg:33.47ms
step:397/2150 train_time:13287ms step_avg:33.47ms
step:398/2150 train_time:13320ms step_avg:33.47ms
step:399/2150 train_time:13352ms step_avg:33.46ms
step:400/2150 train_time:13385ms step_avg:33.46ms
step:401/2150 train_time:13418ms step_avg:33.46ms
step:402/2150 train_time:13451ms step_avg:33.46ms
step:403/2150 train_time:13484ms step_avg:33.46ms
step:404/2150 train_time:13517ms step_avg:33.46ms
step:405/2150 train_time:13550ms step_avg:33.46ms
step:406/2150 train_time:13583ms step_avg:33.46ms
step:407/2150 train_time:13616ms step_avg:33.45ms
step:408/2150 train_time:13649ms step_avg:33.45ms
step:409/2150 train_time:13682ms step_avg:33.45ms
step:410/2150 train_time:13716ms step_avg:33.45ms
step:411/2150 train_time:13749ms step_avg:33.45ms
step:412/2150 train_time:13782ms step_avg:33.45ms
step:413/2150 train_time:13815ms step_avg:33.45ms
step:414/2150 train_time:13848ms step_avg:33.45ms
step:415/2150 train_time:13881ms step_avg:33.45ms
step:416/2150 train_time:13914ms step_avg:33.45ms
step:417/2150 train_time:13947ms step_avg:33.45ms
step:418/2150 train_time:13981ms step_avg:33.45ms
step:419/2150 train_time:14013ms step_avg:33.44ms
step:420/2150 train_time:14047ms step_avg:33.44ms
step:421/2150 train_time:14080ms step_avg:33.44ms
step:422/2150 train_time:14113ms step_avg:33.44ms
step:423/2150 train_time:14145ms step_avg:33.44ms
step:424/2150 train_time:14179ms step_avg:33.44ms
step:425/2150 train_time:14211ms step_avg:33.44ms
step:426/2150 train_time:14244ms step_avg:33.44ms
step:427/2150 train_time:14277ms step_avg:33.44ms
step:428/2150 train_time:14311ms step_avg:33.44ms
step:429/2150 train_time:14343ms step_avg:33.43ms
step:430/2150 train_time:14376ms step_avg:33.43ms
step:431/2150 train_time:14409ms step_avg:33.43ms
step:432/2150 train_time:14442ms step_avg:33.43ms
step:433/2150 train_time:14475ms step_avg:33.43ms
step:434/2150 train_time:14508ms step_avg:33.43ms
step:435/2150 train_time:14541ms step_avg:33.43ms
step:436/2150 train_time:14575ms step_avg:33.43ms
step:437/2150 train_time:14608ms step_avg:33.43ms
step:438/2150 train_time:14641ms step_avg:33.43ms
step:439/2150 train_time:14674ms step_avg:33.43ms
step:440/2150 train_time:14708ms step_avg:33.43ms
step:441/2150 train_time:14741ms step_avg:33.43ms
step:442/2150 train_time:14774ms step_avg:33.43ms
step:443/2150 train_time:14807ms step_avg:33.42ms
step:444/2150 train_time:14840ms step_avg:33.42ms
step:445/2150 train_time:14873ms step_avg:33.42ms
step:446/2150 train_time:14906ms step_avg:33.42ms
step:447/2150 train_time:14939ms step_avg:33.42ms
step:448/2150 train_time:14972ms step_avg:33.42ms
step:449/2150 train_time:15005ms step_avg:33.42ms
step:450/2150 train_time:15039ms step_avg:33.42ms
step:451/2150 train_time:15072ms step_avg:33.42ms
step:452/2150 train_time:15105ms step_avg:33.42ms
step:453/2150 train_time:15138ms step_avg:33.42ms
step:454/2150 train_time:15171ms step_avg:33.42ms
step:455/2150 train_time:15204ms step_avg:33.42ms
step:456/2150 train_time:15237ms step_avg:33.42ms
step:457/2150 train_time:15270ms step_avg:33.41ms
step:458/2150 train_time:15303ms step_avg:33.41ms
step:459/2150 train_time:15336ms step_avg:33.41ms
step:460/2150 train_time:15369ms step_avg:33.41ms
step:461/2150 train_time:15402ms step_avg:33.41ms
step:462/2150 train_time:15435ms step_avg:33.41ms
step:463/2150 train_time:15468ms step_avg:33.41ms
step:464/2150 train_time:15501ms step_avg:33.41ms
step:465/2150 train_time:15534ms step_avg:33.41ms
step:466/2150 train_time:15567ms step_avg:33.41ms
step:467/2150 train_time:15600ms step_avg:33.40ms
step:468/2150 train_time:15633ms step_avg:33.40ms
step:469/2150 train_time:15666ms step_avg:33.40ms
step:470/2150 train_time:15700ms step_avg:33.40ms
step:471/2150 train_time:15732ms step_avg:33.40ms
step:472/2150 train_time:15766ms step_avg:33.40ms
step:473/2150 train_time:15801ms step_avg:33.41ms
step:474/2150 train_time:15832ms step_avg:33.40ms
step:475/2150 train_time:15864ms step_avg:33.40ms
step:476/2150 train_time:15898ms step_avg:33.40ms
step:477/2150 train_time:15931ms step_avg:33.40ms
step:478/2150 train_time:15964ms step_avg:33.40ms
step:479/2150 train_time:15997ms step_avg:33.40ms
step:480/2150 train_time:16030ms step_avg:33.40ms
step:481/2150 train_time:16063ms step_avg:33.39ms
step:482/2150 train_time:16096ms step_avg:33.39ms
step:483/2150 train_time:16129ms step_avg:33.39ms
step:484/2150 train_time:16162ms step_avg:33.39ms
step:485/2150 train_time:16196ms step_avg:33.39ms
step:486/2150 train_time:16229ms step_avg:33.39ms
step:487/2150 train_time:16262ms step_avg:33.39ms
step:488/2150 train_time:16295ms step_avg:33.39ms
step:489/2150 train_time:16328ms step_avg:33.39ms
step:490/2150 train_time:16362ms step_avg:33.39ms
step:491/2150 train_time:16394ms step_avg:33.39ms
step:492/2150 train_time:16427ms step_avg:33.39ms
step:493/2150 train_time:16460ms step_avg:33.39ms
step:494/2150 train_time:16493ms step_avg:33.39ms
step:495/2150 train_time:16527ms step_avg:33.39ms
step:496/2150 train_time:16560ms step_avg:33.39ms
step:497/2150 train_time:16593ms step_avg:33.39ms
step:498/2150 train_time:16626ms step_avg:33.39ms
step:499/2150 train_time:16659ms step_avg:33.38ms
step:500/2150 train_time:16692ms step_avg:33.38ms
step:500/2150 val_loss:4.0203 train_time:16729ms step_avg:33.46ms
step:501/2150 train_time:16751ms step_avg:33.43ms
step:502/2150 train_time:16773ms step_avg:33.41ms
step:503/2150 train_time:16794ms step_avg:33.39ms
step:504/2150 train_time:16827ms step_avg:33.39ms
step:505/2150 train_time:16861ms step_avg:33.39ms
step:506/2150 train_time:16895ms step_avg:33.39ms
step:507/2150 train_time:16929ms step_avg:33.39ms
step:508/2150 train_time:16962ms step_avg:33.39ms
step:509/2150 train_time:16995ms step_avg:33.39ms
step:510/2150 train_time:17029ms step_avg:33.39ms
step:511/2150 train_time:17062ms step_avg:33.39ms
step:512/2150 train_time:17095ms step_avg:33.39ms
step:513/2150 train_time:17128ms step_avg:33.39ms
step:514/2150 train_time:17161ms step_avg:33.39ms
step:515/2150 train_time:17194ms step_avg:33.39ms
step:516/2150 train_time:17227ms step_avg:33.39ms
step:517/2150 train_time:17260ms step_avg:33.38ms
step:518/2150 train_time:17293ms step_avg:33.38ms
step:519/2150 train_time:17326ms step_avg:33.38ms
step:520/2150 train_time:17359ms step_avg:33.38ms
step:521/2150 train_time:17392ms step_avg:33.38ms
step:522/2150 train_time:17425ms step_avg:33.38ms
step:523/2150 train_time:17458ms step_avg:33.38ms
step:524/2150 train_time:17491ms step_avg:33.38ms
step:525/2150 train_time:17524ms step_avg:33.38ms
step:526/2150 train_time:17557ms step_avg:33.38ms
step:527/2150 train_time:17589ms step_avg:33.38ms
step:528/2150 train_time:17623ms step_avg:33.38ms
step:529/2150 train_time:17655ms step_avg:33.38ms
step:530/2150 train_time:17689ms step_avg:33.37ms
step:531/2150 train_time:17722ms step_avg:33.37ms
step:532/2150 train_time:17759ms step_avg:33.38ms
step:533/2150 train_time:17789ms step_avg:33.37ms
step:534/2150 train_time:17822ms step_avg:33.37ms
step:535/2150 train_time:17856ms step_avg:33.38ms
step:536/2150 train_time:17889ms step_avg:33.38ms
step:537/2150 train_time:17923ms step_avg:33.38ms
step:538/2150 train_time:17956ms step_avg:33.38ms
step:539/2150 train_time:17989ms step_avg:33.38ms
step:540/2150 train_time:18023ms step_avg:33.38ms
step:541/2150 train_time:18056ms step_avg:33.38ms
step:542/2150 train_time:18089ms step_avg:33.38ms
step:543/2150 train_time:18122ms step_avg:33.37ms
step:544/2150 train_time:18156ms step_avg:33.37ms
step:545/2150 train_time:18188ms step_avg:33.37ms
step:546/2150 train_time:18221ms step_avg:33.37ms
step:547/2150 train_time:18254ms step_avg:33.37ms
step:548/2150 train_time:18288ms step_avg:33.37ms
step:549/2150 train_time:18321ms step_avg:33.37ms
step:550/2150 train_time:18354ms step_avg:33.37ms
step:551/2150 train_time:18386ms step_avg:33.37ms
step:552/2150 train_time:18420ms step_avg:33.37ms
step:553/2150 train_time:18453ms step_avg:33.37ms
step:554/2150 train_time:18486ms step_avg:33.37ms
step:555/2150 train_time:18518ms step_avg:33.37ms
step:556/2150 train_time:18551ms step_avg:33.37ms
step:557/2150 train_time:18584ms step_avg:33.36ms
step:558/2150 train_time:18617ms step_avg:33.36ms
step:559/2150 train_time:18649ms step_avg:33.36ms
step:560/2150 train_time:18683ms step_avg:33.36ms
step:561/2150 train_time:18716ms step_avg:33.36ms
step:562/2150 train_time:18749ms step_avg:33.36ms
step:563/2150 train_time:18782ms step_avg:33.36ms
step:564/2150 train_time:18816ms step_avg:33.36ms
step:565/2150 train_time:18849ms step_avg:33.36ms
step:566/2150 train_time:18883ms step_avg:33.36ms
step:567/2150 train_time:18916ms step_avg:33.36ms
step:568/2150 train_time:18949ms step_avg:33.36ms
step:569/2150 train_time:18983ms step_avg:33.36ms
step:570/2150 train_time:19016ms step_avg:33.36ms
step:571/2150 train_time:19049ms step_avg:33.36ms
step:572/2150 train_time:19082ms step_avg:33.36ms
step:573/2150 train_time:19115ms step_avg:33.36ms
step:574/2150 train_time:19148ms step_avg:33.36ms
step:575/2150 train_time:19181ms step_avg:33.36ms
step:576/2150 train_time:19214ms step_avg:33.36ms
step:577/2150 train_time:19247ms step_avg:33.36ms
step:578/2150 train_time:19280ms step_avg:33.36ms
step:579/2150 train_time:19314ms step_avg:33.36ms
step:580/2150 train_time:19347ms step_avg:33.36ms
step:581/2150 train_time:19380ms step_avg:33.36ms
step:582/2150 train_time:19413ms step_avg:33.36ms
step:583/2150 train_time:19448ms step_avg:33.36ms
step:584/2150 train_time:19479ms step_avg:33.36ms
step:585/2150 train_time:19512ms step_avg:33.35ms
step:586/2150 train_time:19545ms step_avg:33.35ms
step:587/2150 train_time:19578ms step_avg:33.35ms
step:588/2150 train_time:19611ms step_avg:33.35ms
step:589/2150 train_time:19644ms step_avg:33.35ms
step:590/2150 train_time:19678ms step_avg:33.35ms
step:591/2150 train_time:19710ms step_avg:33.35ms
step:592/2150 train_time:19744ms step_avg:33.35ms
step:593/2150 train_time:19777ms step_avg:33.35ms
step:594/2150 train_time:19810ms step_avg:33.35ms
step:595/2150 train_time:19843ms step_avg:33.35ms
step:596/2150 train_time:19877ms step_avg:33.35ms
step:597/2150 train_time:19910ms step_avg:33.35ms
step:598/2150 train_time:19943ms step_avg:33.35ms
step:599/2150 train_time:19976ms step_avg:33.35ms
step:600/2150 train_time:20009ms step_avg:33.35ms
step:601/2150 train_time:20042ms step_avg:33.35ms
step:602/2150 train_time:20075ms step_avg:33.35ms
step:603/2150 train_time:20108ms step_avg:33.35ms
step:604/2150 train_time:20142ms step_avg:33.35ms
step:605/2150 train_time:20175ms step_avg:33.35ms
step:606/2150 train_time:20208ms step_avg:33.35ms
step:607/2150 train_time:20241ms step_avg:33.35ms
step:608/2150 train_time:20275ms step_avg:33.35ms
step:609/2150 train_time:20307ms step_avg:33.35ms
step:610/2150 train_time:20341ms step_avg:33.35ms
step:611/2150 train_time:20374ms step_avg:33.34ms
step:612/2150 train_time:20407ms step_avg:33.35ms
step:613/2150 train_time:20440ms step_avg:33.34ms
step:614/2150 train_time:20474ms step_avg:33.34ms
step:615/2150 train_time:20506ms step_avg:33.34ms
step:616/2150 train_time:20540ms step_avg:33.34ms
step:617/2150 train_time:20573ms step_avg:33.34ms
step:618/2150 train_time:20606ms step_avg:33.34ms
step:619/2150 train_time:20639ms step_avg:33.34ms
step:620/2150 train_time:20672ms step_avg:33.34ms
step:621/2150 train_time:20705ms step_avg:33.34ms
step:622/2150 train_time:20738ms step_avg:33.34ms
step:623/2150 train_time:20771ms step_avg:33.34ms
step:624/2150 train_time:20804ms step_avg:33.34ms
step:625/2150 train_time:20837ms step_avg:33.34ms
step:626/2150 train_time:20870ms step_avg:33.34ms
step:627/2150 train_time:20903ms step_avg:33.34ms
step:628/2150 train_time:20937ms step_avg:33.34ms
step:629/2150 train_time:20970ms step_avg:33.34ms
step:630/2150 train_time:21003ms step_avg:33.34ms
step:631/2150 train_time:21036ms step_avg:33.34ms
step:632/2150 train_time:21069ms step_avg:33.34ms
step:633/2150 train_time:21102ms step_avg:33.34ms
step:634/2150 train_time:21135ms step_avg:33.34ms
step:635/2150 train_time:21168ms step_avg:33.34ms
step:636/2150 train_time:21202ms step_avg:33.34ms
step:637/2150 train_time:21234ms step_avg:33.33ms
step:638/2150 train_time:21268ms step_avg:33.34ms
step:639/2150 train_time:21301ms step_avg:33.33ms
step:640/2150 train_time:21334ms step_avg:33.34ms
step:641/2150 train_time:21367ms step_avg:33.33ms
step:642/2150 train_time:21401ms step_avg:33.33ms
step:643/2150 train_time:21433ms step_avg:33.33ms
step:644/2150 train_time:21467ms step_avg:33.33ms
step:645/2150 train_time:21500ms step_avg:33.33ms
step:646/2150 train_time:21533ms step_avg:33.33ms
step:647/2150 train_time:21566ms step_avg:33.33ms
step:648/2150 train_time:21599ms step_avg:33.33ms
step:649/2150 train_time:21632ms step_avg:33.33ms
step:650/2150 train_time:21665ms step_avg:33.33ms
step:651/2150 train_time:21699ms step_avg:33.33ms
step:652/2150 train_time:21732ms step_avg:33.33ms
step:653/2150 train_time:21765ms step_avg:33.33ms
step:654/2150 train_time:21798ms step_avg:33.33ms
step:655/2150 train_time:21831ms step_avg:33.33ms
step:656/2150 train_time:21864ms step_avg:33.33ms
step:657/2150 train_time:21897ms step_avg:33.33ms
step:658/2150 train_time:21930ms step_avg:33.33ms
step:659/2150 train_time:21963ms step_avg:33.33ms
step:660/2150 train_time:21997ms step_avg:33.33ms
step:661/2150 train_time:22030ms step_avg:33.33ms
step:662/2150 train_time:22063ms step_avg:33.33ms
step:663/2150 train_time:22096ms step_avg:33.33ms
step:664/2150 train_time:22129ms step_avg:33.33ms
step:665/2150 train_time:22162ms step_avg:33.33ms
step:666/2150 train_time:22196ms step_avg:33.33ms
step:667/2150 train_time:22228ms step_avg:33.33ms
step:668/2150 train_time:22263ms step_avg:33.33ms
step:669/2150 train_time:22295ms step_avg:33.33ms
step:670/2150 train_time:22328ms step_avg:33.33ms
step:671/2150 train_time:22361ms step_avg:33.33ms
step:672/2150 train_time:22395ms step_avg:33.33ms
step:673/2150 train_time:22428ms step_avg:33.32ms
step:674/2150 train_time:22461ms step_avg:33.32ms
step:675/2150 train_time:22494ms step_avg:33.32ms
step:676/2150 train_time:22527ms step_avg:33.32ms
step:677/2150 train_time:22561ms step_avg:33.32ms
step:678/2150 train_time:22594ms step_avg:33.33ms
step:679/2150 train_time:22627ms step_avg:33.32ms
step:680/2150 train_time:22660ms step_avg:33.32ms
step:681/2150 train_time:22693ms step_avg:33.32ms
step:682/2150 train_time:22726ms step_avg:33.32ms
step:683/2150 train_time:22759ms step_avg:33.32ms
step:684/2150 train_time:22792ms step_avg:33.32ms
step:685/2150 train_time:22825ms step_avg:33.32ms
step:686/2150 train_time:22858ms step_avg:33.32ms
step:687/2150 train_time:22891ms step_avg:33.32ms
step:688/2150 train_time:22925ms step_avg:33.32ms
step:689/2150 train_time:22958ms step_avg:33.32ms
step:690/2150 train_time:22991ms step_avg:33.32ms
step:691/2150 train_time:23024ms step_avg:33.32ms
step:692/2150 train_time:23057ms step_avg:33.32ms
step:693/2150 train_time:23090ms step_avg:33.32ms
step:694/2150 train_time:23123ms step_avg:33.32ms
step:695/2150 train_time:23156ms step_avg:33.32ms
step:696/2150 train_time:23189ms step_avg:33.32ms
step:697/2150 train_time:23223ms step_avg:33.32ms
step:698/2150 train_time:23256ms step_avg:33.32ms
step:699/2150 train_time:23289ms step_avg:33.32ms
step:700/2150 train_time:23322ms step_avg:33.32ms
step:701/2150 train_time:23355ms step_avg:33.32ms
step:702/2150 train_time:23388ms step_avg:33.32ms
step:703/2150 train_time:23421ms step_avg:33.32ms
step:704/2150 train_time:23455ms step_avg:33.32ms
step:705/2150 train_time:23489ms step_avg:33.32ms
step:706/2150 train_time:23547ms step_avg:33.35ms
step:707/2150 train_time:23609ms step_avg:33.39ms
step:708/2150 train_time:23669ms step_avg:33.43ms
step:709/2150 train_time:23729ms step_avg:33.47ms
step:710/2150 train_time:23789ms step_avg:33.51ms
step:711/2150 train_time:23851ms step_avg:33.55ms
step:712/2150 train_time:23910ms step_avg:33.58ms
step:713/2150 train_time:23972ms step_avg:33.62ms
step:714/2150 train_time:24032ms step_avg:33.66ms
step:715/2150 train_time:24093ms step_avg:33.70ms
step:716/2150 train_time:24153ms step_avg:33.73ms
step:717/2150 train_time:24215ms step_avg:33.77ms
step:718/2150 train_time:24274ms step_avg:33.81ms
step:719/2150 train_time:24335ms step_avg:33.85ms
step:720/2150 train_time:24395ms step_avg:33.88ms
step:721/2150 train_time:24456ms step_avg:33.92ms
step:722/2150 train_time:24514ms step_avg:33.95ms
step:723/2150 train_time:24576ms step_avg:33.99ms
step:724/2150 train_time:24635ms step_avg:34.03ms
step:725/2150 train_time:24696ms step_avg:34.06ms
step:726/2150 train_time:24755ms step_avg:34.10ms
step:727/2150 train_time:24817ms step_avg:34.14ms
step:728/2150 train_time:24877ms step_avg:34.17ms
step:729/2150 train_time:24938ms step_avg:34.21ms
step:730/2150 train_time:24998ms step_avg:34.24ms
step:731/2150 train_time:25060ms step_avg:34.28ms
step:732/2150 train_time:25118ms step_avg:34.31ms
step:733/2150 train_time:25179ms step_avg:34.35ms
step:734/2150 train_time:25238ms step_avg:34.38ms
step:735/2150 train_time:25299ms step_avg:34.42ms
step:736/2150 train_time:25359ms step_avg:34.46ms
step:737/2150 train_time:25420ms step_avg:34.49ms
step:738/2150 train_time:25480ms step_avg:34.53ms
step:739/2150 train_time:25540ms step_avg:34.56ms
step:740/2150 train_time:25599ms step_avg:34.59ms
step:741/2150 train_time:25661ms step_avg:34.63ms
step:742/2150 train_time:25720ms step_avg:34.66ms
step:743/2150 train_time:25781ms step_avg:34.70ms
step:744/2150 train_time:25841ms step_avg:34.73ms
step:745/2150 train_time:25902ms step_avg:34.77ms
step:746/2150 train_time:25961ms step_avg:34.80ms
step:747/2150 train_time:26022ms step_avg:34.84ms
step:748/2150 train_time:26082ms step_avg:34.87ms
step:749/2150 train_time:26143ms step_avg:34.90ms
step:750/2150 train_time:26203ms step_avg:34.94ms
step:750/2150 val_loss:3.8705 train_time:26266ms step_avg:35.02ms
step:751/2150 train_time:26289ms step_avg:35.01ms
step:752/2150 train_time:26326ms step_avg:35.01ms
step:753/2150 train_time:26386ms step_avg:35.04ms
step:754/2150 train_time:26450ms step_avg:35.08ms
step:755/2150 train_time:26512ms step_avg:35.12ms
step:756/2150 train_time:26571ms step_avg:35.15ms
step:757/2150 train_time:26631ms step_avg:35.18ms
step:758/2150 train_time:26690ms step_avg:35.21ms
step:759/2150 train_time:26750ms step_avg:35.24ms
step:760/2150 train_time:26808ms step_avg:35.27ms
step:761/2150 train_time:26868ms step_avg:35.31ms
step:762/2150 train_time:26927ms step_avg:35.34ms
step:763/2150 train_time:26987ms step_avg:35.37ms
step:764/2150 train_time:27045ms step_avg:35.40ms
step:765/2150 train_time:27105ms step_avg:35.43ms
step:766/2150 train_time:27166ms step_avg:35.46ms
step:767/2150 train_time:27232ms step_avg:35.50ms
step:768/2150 train_time:27294ms step_avg:35.54ms
step:769/2150 train_time:27356ms step_avg:35.57ms
step:770/2150 train_time:27416ms step_avg:35.61ms
step:771/2150 train_time:27477ms step_avg:35.64ms
step:772/2150 train_time:27536ms step_avg:35.67ms
step:773/2150 train_time:27597ms step_avg:35.70ms
step:774/2150 train_time:27655ms step_avg:35.73ms
step:775/2150 train_time:27716ms step_avg:35.76ms
step:776/2150 train_time:27774ms step_avg:35.79ms
step:777/2150 train_time:27835ms step_avg:35.82ms
step:778/2150 train_time:27894ms step_avg:35.85ms
step:779/2150 train_time:27954ms step_avg:35.88ms
step:780/2150 train_time:28012ms step_avg:35.91ms
step:781/2150 train_time:28073ms step_avg:35.95ms
step:782/2150 train_time:28134ms step_avg:35.98ms
step:783/2150 train_time:28196ms step_avg:36.01ms
step:784/2150 train_time:28256ms step_avg:36.04ms
step:785/2150 train_time:28319ms step_avg:36.07ms
step:786/2150 train_time:28379ms step_avg:36.11ms
step:787/2150 train_time:28440ms step_avg:36.14ms
step:788/2150 train_time:28500ms step_avg:36.17ms
step:789/2150 train_time:28562ms step_avg:36.20ms
step:790/2150 train_time:28621ms step_avg:36.23ms
step:791/2150 train_time:28683ms step_avg:36.26ms
step:792/2150 train_time:28741ms step_avg:36.29ms
step:793/2150 train_time:28802ms step_avg:36.32ms
step:794/2150 train_time:28861ms step_avg:36.35ms
step:795/2150 train_time:28921ms step_avg:36.38ms
step:796/2150 train_time:28980ms step_avg:36.41ms
step:797/2150 train_time:29041ms step_avg:36.44ms
step:798/2150 train_time:29100ms step_avg:36.47ms
step:799/2150 train_time:29162ms step_avg:36.50ms
step:800/2150 train_time:29221ms step_avg:36.53ms
step:801/2150 train_time:29283ms step_avg:36.56ms
step:802/2150 train_time:29343ms step_avg:36.59ms
step:803/2150 train_time:29406ms step_avg:36.62ms
step:804/2150 train_time:29466ms step_avg:36.65ms
step:805/2150 train_time:29527ms step_avg:36.68ms
step:806/2150 train_time:29586ms step_avg:36.71ms
step:807/2150 train_time:29647ms step_avg:36.74ms
step:808/2150 train_time:29706ms step_avg:36.76ms
step:809/2150 train_time:29767ms step_avg:36.79ms
step:810/2150 train_time:29826ms step_avg:36.82ms
step:811/2150 train_time:29886ms step_avg:36.85ms
step:812/2150 train_time:29945ms step_avg:36.88ms
step:813/2150 train_time:30006ms step_avg:36.91ms
step:814/2150 train_time:30065ms step_avg:36.94ms
step:815/2150 train_time:30127ms step_avg:36.97ms
step:816/2150 train_time:30186ms step_avg:36.99ms
step:817/2150 train_time:30247ms step_avg:37.02ms
step:818/2150 train_time:30307ms step_avg:37.05ms
step:819/2150 train_time:30368ms step_avg:37.08ms
step:820/2150 train_time:30428ms step_avg:37.11ms
step:821/2150 train_time:30489ms step_avg:37.14ms
step:822/2150 train_time:30549ms step_avg:37.16ms
step:823/2150 train_time:30610ms step_avg:37.19ms
step:824/2150 train_time:30669ms step_avg:37.22ms
step:825/2150 train_time:30729ms step_avg:37.25ms
step:826/2150 train_time:30788ms step_avg:37.27ms
step:827/2150 train_time:30851ms step_avg:37.30ms
step:828/2150 train_time:30909ms step_avg:37.33ms
step:829/2150 train_time:30969ms step_avg:37.36ms
step:830/2150 train_time:31029ms step_avg:37.38ms
step:831/2150 train_time:31090ms step_avg:37.41ms
step:832/2150 train_time:31150ms step_avg:37.44ms
step:833/2150 train_time:31211ms step_avg:37.47ms
step:834/2150 train_time:31270ms step_avg:37.49ms
step:835/2150 train_time:31331ms step_avg:37.52ms
step:836/2150 train_time:31391ms step_avg:37.55ms
step:837/2150 train_time:31453ms step_avg:37.58ms
step:838/2150 train_time:31512ms step_avg:37.60ms
step:839/2150 train_time:31574ms step_avg:37.63ms
step:840/2150 train_time:31633ms step_avg:37.66ms
step:841/2150 train_time:31694ms step_avg:37.69ms
step:842/2150 train_time:31753ms step_avg:37.71ms
step:843/2150 train_time:31814ms step_avg:37.74ms
step:844/2150 train_time:31874ms step_avg:37.76ms
step:845/2150 train_time:31934ms step_avg:37.79ms
step:846/2150 train_time:31994ms step_avg:37.82ms
step:847/2150 train_time:32055ms step_avg:37.85ms
step:848/2150 train_time:32115ms step_avg:37.87ms
step:849/2150 train_time:32175ms step_avg:37.90ms
step:850/2150 train_time:32235ms step_avg:37.92ms
step:851/2150 train_time:32296ms step_avg:37.95ms
step:852/2150 train_time:32358ms step_avg:37.98ms
step:853/2150 train_time:32417ms step_avg:38.00ms
step:854/2150 train_time:32477ms step_avg:38.03ms
step:855/2150 train_time:32539ms step_avg:38.06ms
step:856/2150 train_time:32599ms step_avg:38.08ms
step:857/2150 train_time:32660ms step_avg:38.11ms
step:858/2150 train_time:32719ms step_avg:38.13ms
step:859/2150 train_time:32780ms step_avg:38.16ms
step:860/2150 train_time:32840ms step_avg:38.19ms
step:861/2150 train_time:32901ms step_avg:38.21ms
step:862/2150 train_time:32960ms step_avg:38.24ms
step:863/2150 train_time:33022ms step_avg:38.26ms
step:864/2150 train_time:33082ms step_avg:38.29ms
step:865/2150 train_time:33143ms step_avg:38.32ms
step:866/2150 train_time:33202ms step_avg:38.34ms
step:867/2150 train_time:33263ms step_avg:38.37ms
step:868/2150 train_time:33322ms step_avg:38.39ms
step:869/2150 train_time:33383ms step_avg:38.42ms
step:870/2150 train_time:33443ms step_avg:38.44ms
step:871/2150 train_time:33505ms step_avg:38.47ms
step:872/2150 train_time:33565ms step_avg:38.49ms
step:873/2150 train_time:33626ms step_avg:38.52ms
step:874/2150 train_time:33685ms step_avg:38.54ms
step:875/2150 train_time:33747ms step_avg:38.57ms
step:876/2150 train_time:33806ms step_avg:38.59ms
step:877/2150 train_time:33868ms step_avg:38.62ms
step:878/2150 train_time:33927ms step_avg:38.64ms
step:879/2150 train_time:33988ms step_avg:38.67ms
step:880/2150 train_time:34047ms step_avg:38.69ms
step:881/2150 train_time:34108ms step_avg:38.71ms
step:882/2150 train_time:34167ms step_avg:38.74ms
step:883/2150 train_time:34228ms step_avg:38.76ms
step:884/2150 train_time:34287ms step_avg:38.79ms
step:885/2150 train_time:34348ms step_avg:38.81ms
step:886/2150 train_time:34407ms step_avg:38.83ms
step:887/2150 train_time:34468ms step_avg:38.86ms
step:888/2150 train_time:34528ms step_avg:38.88ms
step:889/2150 train_time:34589ms step_avg:38.91ms
step:890/2150 train_time:34648ms step_avg:38.93ms
step:891/2150 train_time:34709ms step_avg:38.95ms
step:892/2150 train_time:34768ms step_avg:38.98ms
step:893/2150 train_time:34829ms step_avg:39.00ms
step:894/2150 train_time:34889ms step_avg:39.03ms
step:895/2150 train_time:34950ms step_avg:39.05ms
step:896/2150 train_time:35009ms step_avg:39.07ms
step:897/2150 train_time:35070ms step_avg:39.10ms
step:898/2150 train_time:35129ms step_avg:39.12ms
step:899/2150 train_time:35189ms step_avg:39.14ms
step:900/2150 train_time:35248ms step_avg:39.16ms
step:901/2150 train_time:35309ms step_avg:39.19ms
step:902/2150 train_time:35369ms step_avg:39.21ms
step:903/2150 train_time:35430ms step_avg:39.24ms
step:904/2150 train_time:35490ms step_avg:39.26ms
step:905/2150 train_time:35550ms step_avg:39.28ms
step:906/2150 train_time:35610ms step_avg:39.30ms
step:907/2150 train_time:35671ms step_avg:39.33ms
step:908/2150 train_time:35731ms step_avg:39.35ms
step:909/2150 train_time:35791ms step_avg:39.37ms
step:910/2150 train_time:35851ms step_avg:39.40ms
step:911/2150 train_time:35912ms step_avg:39.42ms
step:912/2150 train_time:35972ms step_avg:39.44ms
step:913/2150 train_time:36033ms step_avg:39.47ms
step:914/2150 train_time:36093ms step_avg:39.49ms
step:915/2150 train_time:36154ms step_avg:39.51ms
step:916/2150 train_time:36214ms step_avg:39.53ms
step:917/2150 train_time:36275ms step_avg:39.56ms
step:918/2150 train_time:36334ms step_avg:39.58ms
step:919/2150 train_time:36396ms step_avg:39.60ms
step:920/2150 train_time:36456ms step_avg:39.63ms
step:921/2150 train_time:36517ms step_avg:39.65ms
step:922/2150 train_time:36577ms step_avg:39.67ms
step:923/2150 train_time:36638ms step_avg:39.69ms
step:924/2150 train_time:36698ms step_avg:39.72ms
step:925/2150 train_time:36760ms step_avg:39.74ms
step:926/2150 train_time:36819ms step_avg:39.76ms
step:927/2150 train_time:36880ms step_avg:39.78ms
step:928/2150 train_time:36940ms step_avg:39.81ms
step:929/2150 train_time:37002ms step_avg:39.83ms
step:930/2150 train_time:37061ms step_avg:39.85ms
step:931/2150 train_time:37123ms step_avg:39.87ms
step:932/2150 train_time:37182ms step_avg:39.90ms
step:933/2150 train_time:37244ms step_avg:39.92ms
step:934/2150 train_time:37303ms step_avg:39.94ms
step:935/2150 train_time:37364ms step_avg:39.96ms
step:936/2150 train_time:37423ms step_avg:39.98ms
step:937/2150 train_time:37485ms step_avg:40.00ms
step:938/2150 train_time:37544ms step_avg:40.03ms
step:939/2150 train_time:37606ms step_avg:40.05ms
step:940/2150 train_time:37666ms step_avg:40.07ms
step:941/2150 train_time:37727ms step_avg:40.09ms
step:942/2150 train_time:37786ms step_avg:40.11ms
step:943/2150 train_time:37847ms step_avg:40.14ms
step:944/2150 train_time:37907ms step_avg:40.16ms
step:945/2150 train_time:37968ms step_avg:40.18ms
step:946/2150 train_time:38027ms step_avg:40.20ms
step:947/2150 train_time:38089ms step_avg:40.22ms
step:948/2150 train_time:38148ms step_avg:40.24ms
step:949/2150 train_time:38209ms step_avg:40.26ms
step:950/2150 train_time:38269ms step_avg:40.28ms
step:951/2150 train_time:38330ms step_avg:40.30ms
step:952/2150 train_time:38389ms step_avg:40.32ms
step:953/2150 train_time:38450ms step_avg:40.35ms
step:954/2150 train_time:38510ms step_avg:40.37ms
step:955/2150 train_time:38571ms step_avg:40.39ms
step:956/2150 train_time:38630ms step_avg:40.41ms
step:957/2150 train_time:38692ms step_avg:40.43ms
step:958/2150 train_time:38751ms step_avg:40.45ms
step:959/2150 train_time:38812ms step_avg:40.47ms
step:960/2150 train_time:38871ms step_avg:40.49ms
step:961/2150 train_time:38933ms step_avg:40.51ms
step:962/2150 train_time:38993ms step_avg:40.53ms
step:963/2150 train_time:39055ms step_avg:40.56ms
step:964/2150 train_time:39114ms step_avg:40.57ms
step:965/2150 train_time:39175ms step_avg:40.60ms
step:966/2150 train_time:39235ms step_avg:40.62ms
step:967/2150 train_time:39296ms step_avg:40.64ms
step:968/2150 train_time:39356ms step_avg:40.66ms
step:969/2150 train_time:39417ms step_avg:40.68ms
step:970/2150 train_time:39476ms step_avg:40.70ms
step:971/2150 train_time:39537ms step_avg:40.72ms
step:972/2150 train_time:39597ms step_avg:40.74ms
step:973/2150 train_time:39658ms step_avg:40.76ms
step:974/2150 train_time:39718ms step_avg:40.78ms
step:975/2150 train_time:39779ms step_avg:40.80ms
step:976/2150 train_time:39838ms step_avg:40.82ms
step:977/2150 train_time:39900ms step_avg:40.84ms
step:978/2150 train_time:39960ms step_avg:40.86ms
step:979/2150 train_time:40021ms step_avg:40.88ms
step:980/2150 train_time:40081ms step_avg:40.90ms
step:981/2150 train_time:40143ms step_avg:40.92ms
step:982/2150 train_time:40203ms step_avg:40.94ms
step:983/2150 train_time:40265ms step_avg:40.96ms
step:984/2150 train_time:40324ms step_avg:40.98ms
step:985/2150 train_time:40386ms step_avg:41.00ms
step:986/2150 train_time:40445ms step_avg:41.02ms
step:987/2150 train_time:40507ms step_avg:41.04ms
step:988/2150 train_time:40566ms step_avg:41.06ms
step:989/2150 train_time:40627ms step_avg:41.08ms
step:990/2150 train_time:40686ms step_avg:41.10ms
step:991/2150 train_time:40747ms step_avg:41.12ms
step:992/2150 train_time:40806ms step_avg:41.14ms
step:993/2150 train_time:40868ms step_avg:41.16ms
step:994/2150 train_time:40927ms step_avg:41.17ms
step:995/2150 train_time:40988ms step_avg:41.19ms
step:996/2150 train_time:41047ms step_avg:41.21ms
step:997/2150 train_time:41109ms step_avg:41.23ms
step:998/2150 train_time:41168ms step_avg:41.25ms
step:999/2150 train_time:41230ms step_avg:41.27ms
step:1000/2150 train_time:41289ms step_avg:41.29ms
step:1000/2150 val_loss:3.7083 train_time:41352ms step_avg:41.35ms
step:1001/2150 train_time:41375ms step_avg:41.33ms
step:1002/2150 train_time:41413ms step_avg:41.33ms
step:1003/2150 train_time:41478ms step_avg:41.35ms
step:1004/2150 train_time:41540ms step_avg:41.37ms
step:1005/2150 train_time:41603ms step_avg:41.40ms
step:1006/2150 train_time:41662ms step_avg:41.41ms
step:1007/2150 train_time:41722ms step_avg:41.43ms
step:1008/2150 train_time:41781ms step_avg:41.45ms
step:1009/2150 train_time:41842ms step_avg:41.47ms
step:1010/2150 train_time:41900ms step_avg:41.49ms
step:1011/2150 train_time:41961ms step_avg:41.50ms
step:1012/2150 train_time:42020ms step_avg:41.52ms
step:1013/2150 train_time:42080ms step_avg:41.54ms
step:1014/2150 train_time:42139ms step_avg:41.56ms
step:1015/2150 train_time:42201ms step_avg:41.58ms
step:1016/2150 train_time:42261ms step_avg:41.59ms
step:1017/2150 train_time:42324ms step_avg:41.62ms
step:1018/2150 train_time:42383ms step_avg:41.63ms
step:1019/2150 train_time:42446ms step_avg:41.65ms
step:1020/2150 train_time:42506ms step_avg:41.67ms
step:1021/2150 train_time:42569ms step_avg:41.69ms
step:1022/2150 train_time:42629ms step_avg:41.71ms
step:1023/2150 train_time:42690ms step_avg:41.73ms
step:1024/2150 train_time:42750ms step_avg:41.75ms
step:1025/2150 train_time:42811ms step_avg:41.77ms
step:1026/2150 train_time:42870ms step_avg:41.78ms
step:1027/2150 train_time:42931ms step_avg:41.80ms
step:1028/2150 train_time:42990ms step_avg:41.82ms
step:1029/2150 train_time:43051ms step_avg:41.84ms
step:1030/2150 train_time:43110ms step_avg:41.85ms
step:1031/2150 train_time:43170ms step_avg:41.87ms
step:1032/2150 train_time:43230ms step_avg:41.89ms
step:1033/2150 train_time:43291ms step_avg:41.91ms
step:1034/2150 train_time:43351ms step_avg:41.93ms
step:1035/2150 train_time:43414ms step_avg:41.95ms
step:1036/2150 train_time:43474ms step_avg:41.96ms
step:1037/2150 train_time:43537ms step_avg:41.98ms
step:1038/2150 train_time:43597ms step_avg:42.00ms
step:1039/2150 train_time:43659ms step_avg:42.02ms
step:1040/2150 train_time:43719ms step_avg:42.04ms
step:1041/2150 train_time:43780ms step_avg:42.06ms
step:1042/2150 train_time:43839ms step_avg:42.07ms
step:1043/2150 train_time:43901ms step_avg:42.09ms
step:1044/2150 train_time:43959ms step_avg:42.11ms
step:1045/2150 train_time:44020ms step_avg:42.12ms
step:1046/2150 train_time:44079ms step_avg:42.14ms
step:1047/2150 train_time:44140ms step_avg:42.16ms
step:1048/2150 train_time:44200ms step_avg:42.18ms
step:1049/2150 train_time:44262ms step_avg:42.19ms
step:1050/2150 train_time:44322ms step_avg:42.21ms
step:1051/2150 train_time:44383ms step_avg:42.23ms
step:1052/2150 train_time:44443ms step_avg:42.25ms
step:1053/2150 train_time:44505ms step_avg:42.26ms
step:1054/2150 train_time:44564ms step_avg:42.28ms
step:1055/2150 train_time:44626ms step_avg:42.30ms
step:1056/2150 train_time:44685ms step_avg:42.32ms
step:1057/2150 train_time:44746ms step_avg:42.33ms
step:1058/2150 train_time:44806ms step_avg:42.35ms
step:1059/2150 train_time:44867ms step_avg:42.37ms
step:1060/2150 train_time:44927ms step_avg:42.38ms
step:1061/2150 train_time:44988ms step_avg:42.40ms
step:1062/2150 train_time:45047ms step_avg:42.42ms
step:1063/2150 train_time:45109ms step_avg:42.44ms
step:1064/2150 train_time:45168ms step_avg:42.45ms
step:1065/2150 train_time:45229ms step_avg:42.47ms
step:1066/2150 train_time:45292ms step_avg:42.49ms
step:1067/2150 train_time:45350ms step_avg:42.50ms
step:1068/2150 train_time:45410ms step_avg:42.52ms
step:1069/2150 train_time:45472ms step_avg:42.54ms
step:1070/2150 train_time:45532ms step_avg:42.55ms
step:1071/2150 train_time:45595ms step_avg:42.57ms
step:1072/2150 train_time:45655ms step_avg:42.59ms
step:1073/2150 train_time:45717ms step_avg:42.61ms
step:1074/2150 train_time:45776ms step_avg:42.62ms
step:1075/2150 train_time:45838ms step_avg:42.64ms
step:1076/2150 train_time:45898ms step_avg:42.66ms
step:1077/2150 train_time:45959ms step_avg:42.67ms
step:1078/2150 train_time:46018ms step_avg:42.69ms
step:1079/2150 train_time:46079ms step_avg:42.71ms
step:1080/2150 train_time:46139ms step_avg:42.72ms
step:1081/2150 train_time:46201ms step_avg:42.74ms
step:1082/2150 train_time:46261ms step_avg:42.75ms
step:1083/2150 train_time:46322ms step_avg:42.77ms
step:1084/2150 train_time:46382ms step_avg:42.79ms
step:1085/2150 train_time:46443ms step_avg:42.80ms
step:1086/2150 train_time:46502ms step_avg:42.82ms
step:1087/2150 train_time:46564ms step_avg:42.84ms
step:1088/2150 train_time:46623ms step_avg:42.85ms
step:1089/2150 train_time:46684ms step_avg:42.87ms
step:1090/2150 train_time:46744ms step_avg:42.88ms
step:1091/2150 train_time:46805ms step_avg:42.90ms
step:1092/2150 train_time:46865ms step_avg:42.92ms
step:1093/2150 train_time:46926ms step_avg:42.93ms
step:1094/2150 train_time:46985ms step_avg:42.95ms
step:1095/2150 train_time:47047ms step_avg:42.96ms
step:1096/2150 train_time:47107ms step_avg:42.98ms
step:1097/2150 train_time:47168ms step_avg:43.00ms
step:1098/2150 train_time:47227ms step_avg:43.01ms
step:1099/2150 train_time:47289ms step_avg:43.03ms
step:1100/2150 train_time:47349ms step_avg:43.04ms
step:1101/2150 train_time:47411ms step_avg:43.06ms
step:1102/2150 train_time:47471ms step_avg:43.08ms
step:1103/2150 train_time:47534ms step_avg:43.09ms
step:1104/2150 train_time:47593ms step_avg:43.11ms
step:1105/2150 train_time:47656ms step_avg:43.13ms
step:1106/2150 train_time:47716ms step_avg:43.14ms
step:1107/2150 train_time:47777ms step_avg:43.16ms
step:1108/2150 train_time:47836ms step_avg:43.17ms
step:1109/2150 train_time:47897ms step_avg:43.19ms
step:1110/2150 train_time:47956ms step_avg:43.20ms
step:1111/2150 train_time:48018ms step_avg:43.22ms
step:1112/2150 train_time:48077ms step_avg:43.23ms
step:1113/2150 train_time:48139ms step_avg:43.25ms
step:1114/2150 train_time:48200ms step_avg:43.27ms
step:1115/2150 train_time:48261ms step_avg:43.28ms
step:1116/2150 train_time:48320ms step_avg:43.30ms
step:1117/2150 train_time:48382ms step_avg:43.31ms
step:1118/2150 train_time:48441ms step_avg:43.33ms
step:1119/2150 train_time:48504ms step_avg:43.35ms
step:1120/2150 train_time:48562ms step_avg:43.36ms
step:1121/2150 train_time:48623ms step_avg:43.37ms
step:1122/2150 train_time:48682ms step_avg:43.39ms
step:1123/2150 train_time:48743ms step_avg:43.40ms
step:1124/2150 train_time:48803ms step_avg:43.42ms
step:1125/2150 train_time:48864ms step_avg:43.43ms
step:1126/2150 train_time:48923ms step_avg:43.45ms
step:1127/2150 train_time:48984ms step_avg:43.46ms
step:1128/2150 train_time:49044ms step_avg:43.48ms
step:1129/2150 train_time:49105ms step_avg:43.49ms
step:1130/2150 train_time:49166ms step_avg:43.51ms
step:1131/2150 train_time:49226ms step_avg:43.52ms
step:1132/2150 train_time:49286ms step_avg:43.54ms
step:1133/2150 train_time:49346ms step_avg:43.55ms
step:1134/2150 train_time:49406ms step_avg:43.57ms
step:1135/2150 train_time:49467ms step_avg:43.58ms
step:1136/2150 train_time:49526ms step_avg:43.60ms
step:1137/2150 train_time:49589ms step_avg:43.61ms
step:1138/2150 train_time:49646ms step_avg:43.63ms
step:1139/2150 train_time:49708ms step_avg:43.64ms
step:1140/2150 train_time:49767ms step_avg:43.66ms
step:1141/2150 train_time:49829ms step_avg:43.67ms
step:1142/2150 train_time:49889ms step_avg:43.69ms
step:1143/2150 train_time:49951ms step_avg:43.70ms
step:1144/2150 train_time:50010ms step_avg:43.72ms
step:1145/2150 train_time:50072ms step_avg:43.73ms
step:1146/2150 train_time:50133ms step_avg:43.75ms
step:1147/2150 train_time:50194ms step_avg:43.76ms
step:1148/2150 train_time:50254ms step_avg:43.78ms
step:1149/2150 train_time:50316ms step_avg:43.79ms
step:1150/2150 train_time:50375ms step_avg:43.80ms
step:1151/2150 train_time:50437ms step_avg:43.82ms
step:1152/2150 train_time:50496ms step_avg:43.83ms
step:1153/2150 train_time:50558ms step_avg:43.85ms
step:1154/2150 train_time:50618ms step_avg:43.86ms
step:1155/2150 train_time:50680ms step_avg:43.88ms
step:1156/2150 train_time:50739ms step_avg:43.89ms
step:1157/2150 train_time:50801ms step_avg:43.91ms
step:1158/2150 train_time:50861ms step_avg:43.92ms
step:1159/2150 train_time:50922ms step_avg:43.94ms
step:1160/2150 train_time:50982ms step_avg:43.95ms
step:1161/2150 train_time:51043ms step_avg:43.96ms
step:1162/2150 train_time:51102ms step_avg:43.98ms
step:1163/2150 train_time:51164ms step_avg:43.99ms
step:1164/2150 train_time:51223ms step_avg:44.01ms
step:1165/2150 train_time:51284ms step_avg:44.02ms
step:1166/2150 train_time:51345ms step_avg:44.03ms
step:1167/2150 train_time:51405ms step_avg:44.05ms
step:1168/2150 train_time:51466ms step_avg:44.06ms
step:1169/2150 train_time:51526ms step_avg:44.08ms
step:1170/2150 train_time:51586ms step_avg:44.09ms
step:1171/2150 train_time:51646ms step_avg:44.10ms
step:1172/2150 train_time:51706ms step_avg:44.12ms
step:1173/2150 train_time:51768ms step_avg:44.13ms
step:1174/2150 train_time:51827ms step_avg:44.15ms
step:1175/2150 train_time:51889ms step_avg:44.16ms
step:1176/2150 train_time:51949ms step_avg:44.17ms
step:1177/2150 train_time:52011ms step_avg:44.19ms
step:1178/2150 train_time:52071ms step_avg:44.20ms
step:1179/2150 train_time:52133ms step_avg:44.22ms
step:1180/2150 train_time:52193ms step_avg:44.23ms
step:1181/2150 train_time:52255ms step_avg:44.25ms
step:1182/2150 train_time:52314ms step_avg:44.26ms
step:1183/2150 train_time:52376ms step_avg:44.27ms
step:1184/2150 train_time:52436ms step_avg:44.29ms
step:1185/2150 train_time:52497ms step_avg:44.30ms
step:1186/2150 train_time:52557ms step_avg:44.31ms
step:1187/2150 train_time:52619ms step_avg:44.33ms
step:1188/2150 train_time:52679ms step_avg:44.34ms
step:1189/2150 train_time:52741ms step_avg:44.36ms
step:1190/2150 train_time:52801ms step_avg:44.37ms
step:1191/2150 train_time:52863ms step_avg:44.39ms
step:1192/2150 train_time:52922ms step_avg:44.40ms
step:1193/2150 train_time:52984ms step_avg:44.41ms
step:1194/2150 train_time:53043ms step_avg:44.42ms
step:1195/2150 train_time:53104ms step_avg:44.44ms
step:1196/2150 train_time:53163ms step_avg:44.45ms
step:1197/2150 train_time:53224ms step_avg:44.46ms
step:1198/2150 train_time:53283ms step_avg:44.48ms
step:1199/2150 train_time:53345ms step_avg:44.49ms
step:1200/2150 train_time:53405ms step_avg:44.50ms
step:1201/2150 train_time:53466ms step_avg:44.52ms
step:1202/2150 train_time:53525ms step_avg:44.53ms
step:1203/2150 train_time:53588ms step_avg:44.55ms
step:1204/2150 train_time:53646ms step_avg:44.56ms
step:1205/2150 train_time:53708ms step_avg:44.57ms
step:1206/2150 train_time:53767ms step_avg:44.58ms
step:1207/2150 train_time:53829ms step_avg:44.60ms
step:1208/2150 train_time:53889ms step_avg:44.61ms
step:1209/2150 train_time:53951ms step_avg:44.62ms
step:1210/2150 train_time:54010ms step_avg:44.64ms
step:1211/2150 train_time:54072ms step_avg:44.65ms
step:1212/2150 train_time:54132ms step_avg:44.66ms
step:1213/2150 train_time:54194ms step_avg:44.68ms
step:1214/2150 train_time:54254ms step_avg:44.69ms
step:1215/2150 train_time:54316ms step_avg:44.70ms
step:1216/2150 train_time:54375ms step_avg:44.72ms
step:1217/2150 train_time:54437ms step_avg:44.73ms
step:1218/2150 train_time:54497ms step_avg:44.74ms
step:1219/2150 train_time:54559ms step_avg:44.76ms
step:1220/2150 train_time:54618ms step_avg:44.77ms
step:1221/2150 train_time:54679ms step_avg:44.78ms
step:1222/2150 train_time:54740ms step_avg:44.80ms
step:1223/2150 train_time:54803ms step_avg:44.81ms
step:1224/2150 train_time:54863ms step_avg:44.82ms
step:1225/2150 train_time:54924ms step_avg:44.84ms
step:1226/2150 train_time:54984ms step_avg:44.85ms
step:1227/2150 train_time:55045ms step_avg:44.86ms
step:1228/2150 train_time:55103ms step_avg:44.87ms
step:1229/2150 train_time:55164ms step_avg:44.89ms
step:1230/2150 train_time:55223ms step_avg:44.90ms
step:1231/2150 train_time:55285ms step_avg:44.91ms
step:1232/2150 train_time:55344ms step_avg:44.92ms
step:1233/2150 train_time:55405ms step_avg:44.94ms
step:1234/2150 train_time:55466ms step_avg:44.95ms
step:1235/2150 train_time:55527ms step_avg:44.96ms
step:1236/2150 train_time:55587ms step_avg:44.97ms
step:1237/2150 train_time:55648ms step_avg:44.99ms
step:1238/2150 train_time:55708ms step_avg:45.00ms
step:1239/2150 train_time:55770ms step_avg:45.01ms
step:1240/2150 train_time:55830ms step_avg:45.02ms
step:1241/2150 train_time:55891ms step_avg:45.04ms
step:1242/2150 train_time:55951ms step_avg:45.05ms
step:1243/2150 train_time:56012ms step_avg:45.06ms
step:1244/2150 train_time:56071ms step_avg:45.07ms
step:1245/2150 train_time:56133ms step_avg:45.09ms
step:1246/2150 train_time:56192ms step_avg:45.10ms
step:1247/2150 train_time:56254ms step_avg:45.11ms
step:1248/2150 train_time:56314ms step_avg:45.12ms
step:1249/2150 train_time:56376ms step_avg:45.14ms
step:1250/2150 train_time:56435ms step_avg:45.15ms
step:1250/2150 val_loss:3.5934 train_time:56499ms step_avg:45.20ms
step:1251/2150 train_time:56521ms step_avg:45.18ms
step:1252/2150 train_time:56557ms step_avg:45.17ms
step:1253/2150 train_time:56622ms step_avg:45.19ms
step:1254/2150 train_time:56683ms step_avg:45.20ms
step:1255/2150 train_time:56745ms step_avg:45.22ms
step:1256/2150 train_time:56804ms step_avg:45.23ms
step:1257/2150 train_time:56865ms step_avg:45.24ms
step:1258/2150 train_time:56924ms step_avg:45.25ms
step:1259/2150 train_time:56985ms step_avg:45.26ms
step:1260/2150 train_time:57044ms step_avg:45.27ms
step:1261/2150 train_time:57106ms step_avg:45.29ms
step:1262/2150 train_time:57165ms step_avg:45.30ms
step:1263/2150 train_time:57227ms step_avg:45.31ms
step:1264/2150 train_time:57287ms step_avg:45.32ms
step:1265/2150 train_time:57349ms step_avg:45.33ms
step:1266/2150 train_time:57408ms step_avg:45.35ms
step:1267/2150 train_time:57471ms step_avg:45.36ms
step:1268/2150 train_time:57532ms step_avg:45.37ms
step:1269/2150 train_time:57594ms step_avg:45.39ms
step:1270/2150 train_time:57655ms step_avg:45.40ms
step:1271/2150 train_time:57718ms step_avg:45.41ms
step:1272/2150 train_time:57777ms step_avg:45.42ms
step:1273/2150 train_time:57839ms step_avg:45.43ms
step:1274/2150 train_time:57898ms step_avg:45.45ms
step:1275/2150 train_time:57959ms step_avg:45.46ms
step:1276/2150 train_time:58018ms step_avg:45.47ms
step:1277/2150 train_time:58078ms step_avg:45.48ms
step:1278/2150 train_time:58138ms step_avg:45.49ms
step:1279/2150 train_time:58199ms step_avg:45.50ms
step:1280/2150 train_time:58259ms step_avg:45.51ms
step:1281/2150 train_time:58319ms step_avg:45.53ms
step:1282/2150 train_time:58379ms step_avg:45.54ms
step:1283/2150 train_time:58441ms step_avg:45.55ms
step:1284/2150 train_time:58501ms step_avg:45.56ms
step:1285/2150 train_time:58563ms step_avg:45.57ms
step:1286/2150 train_time:58624ms step_avg:45.59ms
step:1287/2150 train_time:58686ms step_avg:45.60ms
step:1288/2150 train_time:58747ms step_avg:45.61ms
step:1289/2150 train_time:58808ms step_avg:45.62ms
step:1290/2150 train_time:58867ms step_avg:45.63ms
step:1291/2150 train_time:58929ms step_avg:45.65ms
step:1292/2150 train_time:58989ms step_avg:45.66ms
step:1293/2150 train_time:59050ms step_avg:45.67ms
step:1294/2150 train_time:59109ms step_avg:45.68ms
step:1295/2150 train_time:59171ms step_avg:45.69ms
step:1296/2150 train_time:59231ms step_avg:45.70ms
step:1297/2150 train_time:59292ms step_avg:45.71ms
step:1298/2150 train_time:59352ms step_avg:45.73ms
step:1299/2150 train_time:59413ms step_avg:45.74ms
step:1300/2150 train_time:59473ms step_avg:45.75ms
step:1301/2150 train_time:59535ms step_avg:45.76ms
step:1302/2150 train_time:59595ms step_avg:45.77ms
step:1303/2150 train_time:59656ms step_avg:45.78ms
step:1304/2150 train_time:59716ms step_avg:45.79ms
step:1305/2150 train_time:59778ms step_avg:45.81ms
step:1306/2150 train_time:59838ms step_avg:45.82ms
step:1307/2150 train_time:59899ms step_avg:45.83ms
step:1308/2150 train_time:59959ms step_avg:45.84ms
step:1309/2150 train_time:60020ms step_avg:45.85ms
step:1310/2150 train_time:60080ms step_avg:45.86ms
step:1311/2150 train_time:60141ms step_avg:45.87ms
step:1312/2150 train_time:60200ms step_avg:45.88ms
step:1313/2150 train_time:60262ms step_avg:45.90ms
step:1314/2150 train_time:60322ms step_avg:45.91ms
step:1315/2150 train_time:60384ms step_avg:45.92ms
step:1316/2150 train_time:60444ms step_avg:45.93ms
step:1317/2150 train_time:60505ms step_avg:45.94ms
step:1318/2150 train_time:60565ms step_avg:45.95ms
step:1319/2150 train_time:60627ms step_avg:45.96ms
step:1320/2150 train_time:60688ms step_avg:45.98ms
step:1321/2150 train_time:60750ms step_avg:45.99ms
step:1322/2150 train_time:60809ms step_avg:46.00ms
step:1323/2150 train_time:60871ms step_avg:46.01ms
step:1324/2150 train_time:60931ms step_avg:46.02ms
step:1325/2150 train_time:60992ms step_avg:46.03ms
step:1326/2150 train_time:61052ms step_avg:46.04ms
step:1327/2150 train_time:61113ms step_avg:46.05ms
step:1328/2150 train_time:61172ms step_avg:46.06ms
step:1329/2150 train_time:61234ms step_avg:46.08ms
step:1330/2150 train_time:61293ms step_avg:46.09ms
step:1331/2150 train_time:61354ms step_avg:46.10ms
step:1332/2150 train_time:61413ms step_avg:46.11ms
step:1333/2150 train_time:61474ms step_avg:46.12ms
step:1334/2150 train_time:61534ms step_avg:46.13ms
step:1335/2150 train_time:61595ms step_avg:46.14ms
step:1336/2150 train_time:61655ms step_avg:46.15ms
step:1337/2150 train_time:61717ms step_avg:46.16ms
step:1338/2150 train_time:61777ms step_avg:46.17ms
step:1339/2150 train_time:61838ms step_avg:46.18ms
step:1340/2150 train_time:61898ms step_avg:46.19ms
step:1341/2150 train_time:61960ms step_avg:46.20ms
step:1342/2150 train_time:62019ms step_avg:46.21ms
step:1343/2150 train_time:62080ms step_avg:46.23ms
step:1344/2150 train_time:62140ms step_avg:46.24ms
step:1345/2150 train_time:62202ms step_avg:46.25ms
step:1346/2150 train_time:62261ms step_avg:46.26ms
step:1347/2150 train_time:62323ms step_avg:46.27ms
step:1348/2150 train_time:62383ms step_avg:46.28ms
step:1349/2150 train_time:62444ms step_avg:46.29ms
step:1350/2150 train_time:62504ms step_avg:46.30ms
step:1351/2150 train_time:62565ms step_avg:46.31ms
step:1352/2150 train_time:62625ms step_avg:46.32ms
step:1353/2150 train_time:62687ms step_avg:46.33ms
step:1354/2150 train_time:62747ms step_avg:46.34ms
step:1355/2150 train_time:62808ms step_avg:46.35ms
step:1356/2150 train_time:62868ms step_avg:46.36ms
step:1357/2150 train_time:62930ms step_avg:46.37ms
step:1358/2150 train_time:62989ms step_avg:46.38ms
step:1359/2150 train_time:63051ms step_avg:46.40ms
step:1360/2150 train_time:63111ms step_avg:46.41ms
step:1361/2150 train_time:63171ms step_avg:46.42ms
step:1362/2150 train_time:63231ms step_avg:46.42ms
step:1363/2150 train_time:63292ms step_avg:46.44ms
step:1364/2150 train_time:63351ms step_avg:46.44ms
step:1365/2150 train_time:63412ms step_avg:46.46ms
step:1366/2150 train_time:63471ms step_avg:46.47ms
step:1367/2150 train_time:63533ms step_avg:46.48ms
step:1368/2150 train_time:63592ms step_avg:46.49ms
step:1369/2150 train_time:63654ms step_avg:46.50ms
step:1370/2150 train_time:63714ms step_avg:46.51ms
step:1371/2150 train_time:63775ms step_avg:46.52ms
step:1372/2150 train_time:63835ms step_avg:46.53ms
step:1373/2150 train_time:63895ms step_avg:46.54ms
step:1374/2150 train_time:63955ms step_avg:46.55ms
step:1375/2150 train_time:64016ms step_avg:46.56ms
step:1376/2150 train_time:64075ms step_avg:46.57ms
step:1377/2150 train_time:64138ms step_avg:46.58ms
step:1378/2150 train_time:64197ms step_avg:46.59ms
step:1379/2150 train_time:64258ms step_avg:46.60ms
step:1380/2150 train_time:64318ms step_avg:46.61ms
step:1381/2150 train_time:64379ms step_avg:46.62ms
step:1382/2150 train_time:64439ms step_avg:46.63ms
step:1383/2150 train_time:64500ms step_avg:46.64ms
step:1384/2150 train_time:64560ms step_avg:46.65ms
step:1385/2150 train_time:64622ms step_avg:46.66ms
step:1386/2150 train_time:64682ms step_avg:46.67ms
step:1387/2150 train_time:64744ms step_avg:46.68ms
step:1388/2150 train_time:64803ms step_avg:46.69ms
step:1389/2150 train_time:64865ms step_avg:46.70ms
step:1390/2150 train_time:64925ms step_avg:46.71ms
step:1391/2150 train_time:64986ms step_avg:46.72ms
step:1392/2150 train_time:65046ms step_avg:46.73ms
step:1393/2150 train_time:65107ms step_avg:46.74ms
step:1394/2150 train_time:65167ms step_avg:46.75ms
step:1395/2150 train_time:65228ms step_avg:46.76ms
step:1396/2150 train_time:65288ms step_avg:46.77ms
step:1397/2150 train_time:65350ms step_avg:46.78ms
step:1398/2150 train_time:65410ms step_avg:46.79ms
step:1399/2150 train_time:65471ms step_avg:46.80ms
step:1400/2150 train_time:65531ms step_avg:46.81ms
step:1401/2150 train_time:65592ms step_avg:46.82ms
step:1402/2150 train_time:65651ms step_avg:46.83ms
step:1403/2150 train_time:65713ms step_avg:46.84ms
step:1404/2150 train_time:65772ms step_avg:46.85ms
step:1405/2150 train_time:65834ms step_avg:46.86ms
step:1406/2150 train_time:65894ms step_avg:46.87ms
step:1407/2150 train_time:65954ms step_avg:46.88ms
step:1408/2150 train_time:66015ms step_avg:46.89ms
step:1409/2150 train_time:66105ms step_avg:46.92ms
step:1410/2150 train_time:66192ms step_avg:46.94ms
step:1411/2150 train_time:66283ms step_avg:46.98ms
step:1412/2150 train_time:66372ms step_avg:47.01ms
step:1413/2150 train_time:66462ms step_avg:47.04ms
step:1414/2150 train_time:66550ms step_avg:47.07ms
step:1415/2150 train_time:66641ms step_avg:47.10ms
step:1416/2150 train_time:66728ms step_avg:47.12ms
step:1417/2150 train_time:66817ms step_avg:47.15ms
step:1418/2150 train_time:66904ms step_avg:47.18ms
step:1419/2150 train_time:66994ms step_avg:47.21ms
step:1420/2150 train_time:67081ms step_avg:47.24ms
step:1421/2150 train_time:67170ms step_avg:47.27ms
step:1422/2150 train_time:67258ms step_avg:47.30ms
step:1423/2150 train_time:67348ms step_avg:47.33ms
step:1424/2150 train_time:67437ms step_avg:47.36ms
step:1425/2150 train_time:67529ms step_avg:47.39ms
step:1426/2150 train_time:67615ms step_avg:47.42ms
step:1427/2150 train_time:67704ms step_avg:47.44ms
step:1428/2150 train_time:67792ms step_avg:47.47ms
step:1429/2150 train_time:67881ms step_avg:47.50ms
step:1430/2150 train_time:67969ms step_avg:47.53ms
step:1431/2150 train_time:68059ms step_avg:47.56ms
step:1432/2150 train_time:68146ms step_avg:47.59ms
step:1433/2150 train_time:68235ms step_avg:47.62ms
step:1434/2150 train_time:68323ms step_avg:47.65ms
step:1435/2150 train_time:68413ms step_avg:47.67ms
step:1436/2150 train_time:68502ms step_avg:47.70ms
step:1437/2150 train_time:68591ms step_avg:47.73ms
step:1438/2150 train_time:68679ms step_avg:47.76ms
step:1439/2150 train_time:68768ms step_avg:47.79ms
step:1440/2150 train_time:68857ms step_avg:47.82ms
step:1441/2150 train_time:68946ms step_avg:47.85ms
step:1442/2150 train_time:69034ms step_avg:47.87ms
step:1443/2150 train_time:69124ms step_avg:47.90ms
step:1444/2150 train_time:69210ms step_avg:47.93ms
step:1445/2150 train_time:69300ms step_avg:47.96ms
step:1446/2150 train_time:69388ms step_avg:47.99ms
step:1447/2150 train_time:69478ms step_avg:48.02ms
step:1448/2150 train_time:69566ms step_avg:48.04ms
step:1449/2150 train_time:69656ms step_avg:48.07ms
step:1450/2150 train_time:69743ms step_avg:48.10ms
step:1451/2150 train_time:69833ms step_avg:48.13ms
step:1452/2150 train_time:69921ms step_avg:48.15ms
step:1453/2150 train_time:70010ms step_avg:48.18ms
step:1454/2150 train_time:70098ms step_avg:48.21ms
step:1455/2150 train_time:70187ms step_avg:48.24ms
step:1456/2150 train_time:70275ms step_avg:48.27ms
step:1457/2150 train_time:70365ms step_avg:48.29ms
step:1458/2150 train_time:70453ms step_avg:48.32ms
step:1459/2150 train_time:70543ms step_avg:48.35ms
step:1460/2150 train_time:70631ms step_avg:48.38ms
step:1461/2150 train_time:70720ms step_avg:48.41ms
step:1462/2150 train_time:70808ms step_avg:48.43ms
step:1463/2150 train_time:70898ms step_avg:48.46ms
step:1464/2150 train_time:70986ms step_avg:48.49ms
step:1465/2150 train_time:71075ms step_avg:48.52ms
step:1466/2150 train_time:71162ms step_avg:48.54ms
step:1467/2150 train_time:71252ms step_avg:48.57ms
step:1468/2150 train_time:71341ms step_avg:48.60ms
step:1469/2150 train_time:71430ms step_avg:48.62ms
step:1470/2150 train_time:71518ms step_avg:48.65ms
step:1471/2150 train_time:71608ms step_avg:48.68ms
step:1472/2150 train_time:71696ms step_avg:48.71ms
step:1473/2150 train_time:71785ms step_avg:48.73ms
step:1474/2150 train_time:71874ms step_avg:48.76ms
step:1475/2150 train_time:71964ms step_avg:48.79ms
step:1476/2150 train_time:72052ms step_avg:48.82ms
step:1477/2150 train_time:72141ms step_avg:48.84ms
step:1478/2150 train_time:72229ms step_avg:48.87ms
step:1479/2150 train_time:72319ms step_avg:48.90ms
step:1480/2150 train_time:72407ms step_avg:48.92ms
step:1481/2150 train_time:72497ms step_avg:48.95ms
step:1482/2150 train_time:72585ms step_avg:48.98ms
step:1483/2150 train_time:72674ms step_avg:49.00ms
step:1484/2150 train_time:72763ms step_avg:49.03ms
step:1485/2150 train_time:72853ms step_avg:49.06ms
step:1486/2150 train_time:72941ms step_avg:49.09ms
step:1487/2150 train_time:73029ms step_avg:49.11ms
step:1488/2150 train_time:73117ms step_avg:49.14ms
step:1489/2150 train_time:73207ms step_avg:49.16ms
step:1490/2150 train_time:73295ms step_avg:49.19ms
step:1491/2150 train_time:73385ms step_avg:49.22ms
step:1492/2150 train_time:73473ms step_avg:49.24ms
step:1493/2150 train_time:73563ms step_avg:49.27ms
step:1494/2150 train_time:73651ms step_avg:49.30ms
step:1495/2150 train_time:73741ms step_avg:49.32ms
step:1496/2150 train_time:73829ms step_avg:49.35ms
step:1497/2150 train_time:73919ms step_avg:49.38ms
step:1498/2150 train_time:74006ms step_avg:49.40ms
step:1499/2150 train_time:74096ms step_avg:49.43ms
step:1500/2150 train_time:74184ms step_avg:49.46ms
step:1500/2150 val_loss:3.4888 train_time:74275ms step_avg:49.52ms
step:1501/2150 train_time:74298ms step_avg:49.50ms
step:1502/2150 train_time:74364ms step_avg:49.51ms
step:1503/2150 train_time:74461ms step_avg:49.54ms
step:1504/2150 train_time:74551ms step_avg:49.57ms
step:1505/2150 train_time:74639ms step_avg:49.59ms
step:1506/2150 train_time:74726ms step_avg:49.62ms
step:1507/2150 train_time:74814ms step_avg:49.64ms
step:1508/2150 train_time:74900ms step_avg:49.67ms
step:1509/2150 train_time:74988ms step_avg:49.69ms
step:1510/2150 train_time:75076ms step_avg:49.72ms
step:1511/2150 train_time:75165ms step_avg:49.75ms
step:1512/2150 train_time:75254ms step_avg:49.77ms
step:1513/2150 train_time:75345ms step_avg:49.80ms
step:1514/2150 train_time:75438ms step_avg:49.83ms
step:1515/2150 train_time:75529ms step_avg:49.85ms
step:1516/2150 train_time:75619ms step_avg:49.88ms
step:1517/2150 train_time:75705ms step_avg:49.90ms
step:1518/2150 train_time:75791ms step_avg:49.93ms
step:1519/2150 train_time:75879ms step_avg:49.95ms
step:1520/2150 train_time:75965ms step_avg:49.98ms
step:1521/2150 train_time:76053ms step_avg:50.00ms
step:1522/2150 train_time:76140ms step_avg:50.03ms
step:1523/2150 train_time:76229ms step_avg:50.05ms
step:1524/2150 train_time:76318ms step_avg:50.08ms
step:1525/2150 train_time:76410ms step_avg:50.11ms
step:1526/2150 train_time:76500ms step_avg:50.13ms
step:1527/2150 train_time:76590ms step_avg:50.16ms
step:1528/2150 train_time:76679ms step_avg:50.18ms
step:1529/2150 train_time:76768ms step_avg:50.21ms
step:1530/2150 train_time:76855ms step_avg:50.23ms
step:1531/2150 train_time:76943ms step_avg:50.26ms
step:1532/2150 train_time:77030ms step_avg:50.28ms
step:1533/2150 train_time:77118ms step_avg:50.31ms
step:1534/2150 train_time:77206ms step_avg:50.33ms
step:1535/2150 train_time:77297ms step_avg:50.36ms
step:1536/2150 train_time:77387ms step_avg:50.38ms
step:1537/2150 train_time:77479ms step_avg:50.41ms
step:1538/2150 train_time:77568ms step_avg:50.43ms
step:1539/2150 train_time:77657ms step_avg:50.46ms
step:1540/2150 train_time:77744ms step_avg:50.48ms
step:1541/2150 train_time:77833ms step_avg:50.51ms
step:1542/2150 train_time:77920ms step_avg:50.53ms
step:1543/2150 train_time:78008ms step_avg:50.56ms
step:1544/2150 train_time:78095ms step_avg:50.58ms
step:1545/2150 train_time:78183ms step_avg:50.60ms
step:1546/2150 train_time:78271ms step_avg:50.63ms
step:1547/2150 train_time:78361ms step_avg:50.65ms
step:1548/2150 train_time:78450ms step_avg:50.68ms
step:1549/2150 train_time:78540ms step_avg:50.70ms
step:1550/2150 train_time:78629ms step_avg:50.73ms
step:1551/2150 train_time:78718ms step_avg:50.75ms
step:1552/2150 train_time:78806ms step_avg:50.78ms
step:1553/2150 train_time:78896ms step_avg:50.80ms
step:1554/2150 train_time:78982ms step_avg:50.82ms
step:1555/2150 train_time:79071ms step_avg:50.85ms
step:1556/2150 train_time:79158ms step_avg:50.87ms
step:1557/2150 train_time:79246ms step_avg:50.90ms
step:1558/2150 train_time:79335ms step_avg:50.92ms
step:1559/2150 train_time:79424ms step_avg:50.95ms
step:1560/2150 train_time:79512ms step_avg:50.97ms
step:1561/2150 train_time:79602ms step_avg:50.99ms
step:1562/2150 train_time:79690ms step_avg:51.02ms
step:1563/2150 train_time:79779ms step_avg:51.04ms
step:1564/2150 train_time:79867ms step_avg:51.07ms
step:1565/2150 train_time:79957ms step_avg:51.09ms
step:1566/2150 train_time:80043ms step_avg:51.11ms
step:1567/2150 train_time:80133ms step_avg:51.14ms
step:1568/2150 train_time:80220ms step_avg:51.16ms
step:1569/2150 train_time:80309ms step_avg:51.19ms
step:1570/2150 train_time:80399ms step_avg:51.21ms
step:1571/2150 train_time:80488ms step_avg:51.23ms
step:1572/2150 train_time:80577ms step_avg:51.26ms
step:1573/2150 train_time:80666ms step_avg:51.28ms
step:1574/2150 train_time:80754ms step_avg:51.30ms
step:1575/2150 train_time:80843ms step_avg:51.33ms
step:1576/2150 train_time:80931ms step_avg:51.35ms
step:1577/2150 train_time:81019ms step_avg:51.38ms
step:1578/2150 train_time:81107ms step_avg:51.40ms
step:1579/2150 train_time:81197ms step_avg:51.42ms
step:1580/2150 train_time:81286ms step_avg:51.45ms
step:1581/2150 train_time:81375ms step_avg:51.47ms
step:1582/2150 train_time:81463ms step_avg:51.49ms
step:1583/2150 train_time:81553ms step_avg:51.52ms
step:1584/2150 train_time:81641ms step_avg:51.54ms
step:1585/2150 train_time:81731ms step_avg:51.57ms
step:1586/2150 train_time:81818ms step_avg:51.59ms
step:1587/2150 train_time:81906ms step_avg:51.61ms
step:1588/2150 train_time:81995ms step_avg:51.63ms
step:1589/2150 train_time:82083ms step_avg:51.66ms
step:1590/2150 train_time:82171ms step_avg:51.68ms
step:1591/2150 train_time:82261ms step_avg:51.70ms
step:1592/2150 train_time:82350ms step_avg:51.73ms
step:1593/2150 train_time:82439ms step_avg:51.75ms
step:1594/2150 train_time:82526ms step_avg:51.77ms
step:1595/2150 train_time:82616ms step_avg:51.80ms
step:1596/2150 train_time:82704ms step_avg:51.82ms
step:1597/2150 train_time:82794ms step_avg:51.84ms
step:1598/2150 train_time:82882ms step_avg:51.87ms
step:1599/2150 train_time:82971ms step_avg:51.89ms
step:1600/2150 train_time:83059ms step_avg:51.91ms
step:1601/2150 train_time:83148ms step_avg:51.94ms
step:1602/2150 train_time:83236ms step_avg:51.96ms
step:1603/2150 train_time:83326ms step_avg:51.98ms
step:1604/2150 train_time:83414ms step_avg:52.00ms
step:1605/2150 train_time:83502ms step_avg:52.03ms
step:1606/2150 train_time:83591ms step_avg:52.05ms
step:1607/2150 train_time:83681ms step_avg:52.07ms
step:1608/2150 train_time:83769ms step_avg:52.10ms
step:1609/2150 train_time:83859ms step_avg:52.12ms
step:1610/2150 train_time:83946ms step_avg:52.14ms
step:1611/2150 train_time:84035ms step_avg:52.16ms
step:1612/2150 train_time:84122ms step_avg:52.18ms
step:1613/2150 train_time:84211ms step_avg:52.21ms
step:1614/2150 train_time:84299ms step_avg:52.23ms
step:1615/2150 train_time:84388ms step_avg:52.25ms
step:1616/2150 train_time:84476ms step_avg:52.27ms
step:1617/2150 train_time:84565ms step_avg:52.30ms
step:1618/2150 train_time:84652ms step_avg:52.32ms
step:1619/2150 train_time:84743ms step_avg:52.34ms
step:1620/2150 train_time:84831ms step_avg:52.36ms
step:1621/2150 train_time:84920ms step_avg:52.39ms
step:1622/2150 train_time:85007ms step_avg:52.41ms
step:1623/2150 train_time:85096ms step_avg:52.43ms
step:1624/2150 train_time:85184ms step_avg:52.45ms
step:1625/2150 train_time:85273ms step_avg:52.48ms
step:1626/2150 train_time:85361ms step_avg:52.50ms
step:1627/2150 train_time:85450ms step_avg:52.52ms
step:1628/2150 train_time:85537ms step_avg:52.54ms
step:1629/2150 train_time:85626ms step_avg:52.56ms
step:1630/2150 train_time:85714ms step_avg:52.59ms
step:1631/2150 train_time:85803ms step_avg:52.61ms
step:1632/2150 train_time:85892ms step_avg:52.63ms
step:1633/2150 train_time:85980ms step_avg:52.65ms
step:1634/2150 train_time:86068ms step_avg:52.67ms
step:1635/2150 train_time:86157ms step_avg:52.70ms
step:1636/2150 train_time:86245ms step_avg:52.72ms
step:1637/2150 train_time:86334ms step_avg:52.74ms
step:1638/2150 train_time:86422ms step_avg:52.76ms
step:1639/2150 train_time:86511ms step_avg:52.78ms
step:1640/2150 train_time:86599ms step_avg:52.80ms
step:1641/2150 train_time:86690ms step_avg:52.83ms
step:1642/2150 train_time:86778ms step_avg:52.85ms
step:1643/2150 train_time:86867ms step_avg:52.87ms
step:1644/2150 train_time:86954ms step_avg:52.89ms
step:1645/2150 train_time:87043ms step_avg:52.91ms
step:1646/2150 train_time:87132ms step_avg:52.94ms
step:1647/2150 train_time:87222ms step_avg:52.96ms
step:1648/2150 train_time:87309ms step_avg:52.98ms
step:1649/2150 train_time:87398ms step_avg:53.00ms
step:1650/2150 train_time:87485ms step_avg:53.02ms
step:1651/2150 train_time:87576ms step_avg:53.04ms
step:1652/2150 train_time:87664ms step_avg:53.07ms
step:1653/2150 train_time:87753ms step_avg:53.09ms
step:1654/2150 train_time:87842ms step_avg:53.11ms
step:1655/2150 train_time:87931ms step_avg:53.13ms
step:1656/2150 train_time:88019ms step_avg:53.15ms
step:1657/2150 train_time:88108ms step_avg:53.17ms
step:1658/2150 train_time:88197ms step_avg:53.19ms
step:1659/2150 train_time:88286ms step_avg:53.22ms
step:1660/2150 train_time:88374ms step_avg:53.24ms
step:1661/2150 train_time:88463ms step_avg:53.26ms
step:1662/2150 train_time:88551ms step_avg:53.28ms
step:1663/2150 train_time:88641ms step_avg:53.30ms
step:1664/2150 train_time:88730ms step_avg:53.32ms
step:1665/2150 train_time:88819ms step_avg:53.35ms
step:1666/2150 train_time:88908ms step_avg:53.37ms
step:1667/2150 train_time:88997ms step_avg:53.39ms
step:1668/2150 train_time:89086ms step_avg:53.41ms
step:1669/2150 train_time:89175ms step_avg:53.43ms
step:1670/2150 train_time:89263ms step_avg:53.45ms
step:1671/2150 train_time:89352ms step_avg:53.47ms
step:1672/2150 train_time:89439ms step_avg:53.49ms
step:1673/2150 train_time:89529ms step_avg:53.51ms
step:1674/2150 train_time:89617ms step_avg:53.53ms
step:1675/2150 train_time:89706ms step_avg:53.56ms
step:1676/2150 train_time:89794ms step_avg:53.58ms
step:1677/2150 train_time:89883ms step_avg:53.60ms
step:1678/2150 train_time:89970ms step_avg:53.62ms
step:1679/2150 train_time:90060ms step_avg:53.64ms
step:1680/2150 train_time:90148ms step_avg:53.66ms
step:1681/2150 train_time:90237ms step_avg:53.68ms
step:1682/2150 train_time:90325ms step_avg:53.70ms
step:1683/2150 train_time:90415ms step_avg:53.72ms
step:1684/2150 train_time:90504ms step_avg:53.74ms
step:1685/2150 train_time:90593ms step_avg:53.76ms
step:1686/2150 train_time:90680ms step_avg:53.78ms
step:1687/2150 train_time:90769ms step_avg:53.81ms
step:1688/2150 train_time:90856ms step_avg:53.82ms
step:1689/2150 train_time:90945ms step_avg:53.85ms
step:1690/2150 train_time:91032ms step_avg:53.87ms
step:1691/2150 train_time:91121ms step_avg:53.89ms
step:1692/2150 train_time:91210ms step_avg:53.91ms
step:1693/2150 train_time:91299ms step_avg:53.93ms
step:1694/2150 train_time:91387ms step_avg:53.95ms
step:1695/2150 train_time:91477ms step_avg:53.97ms
step:1696/2150 train_time:91565ms step_avg:53.99ms
step:1697/2150 train_time:91654ms step_avg:54.01ms
step:1698/2150 train_time:91742ms step_avg:54.03ms
step:1699/2150 train_time:91831ms step_avg:54.05ms
step:1700/2150 train_time:91918ms step_avg:54.07ms
step:1701/2150 train_time:92008ms step_avg:54.09ms
step:1702/2150 train_time:92096ms step_avg:54.11ms
step:1703/2150 train_time:92184ms step_avg:54.13ms
step:1704/2150 train_time:92272ms step_avg:54.15ms
step:1705/2150 train_time:92362ms step_avg:54.17ms
step:1706/2150 train_time:92450ms step_avg:54.19ms
step:1707/2150 train_time:92539ms step_avg:54.21ms
step:1708/2150 train_time:92627ms step_avg:54.23ms
step:1709/2150 train_time:92716ms step_avg:54.25ms
step:1710/2150 train_time:92804ms step_avg:54.27ms
step:1711/2150 train_time:92894ms step_avg:54.29ms
step:1712/2150 train_time:92982ms step_avg:54.31ms
step:1713/2150 train_time:93071ms step_avg:54.33ms
step:1714/2150 train_time:93158ms step_avg:54.35ms
step:1715/2150 train_time:93247ms step_avg:54.37ms
step:1716/2150 train_time:93336ms step_avg:54.39ms
step:1717/2150 train_time:93425ms step_avg:54.41ms
step:1718/2150 train_time:93513ms step_avg:54.43ms
step:1719/2150 train_time:93602ms step_avg:54.45ms
step:1720/2150 train_time:93690ms step_avg:54.47ms
step:1721/2150 train_time:93779ms step_avg:54.49ms
step:1722/2150 train_time:93867ms step_avg:54.51ms
step:1723/2150 train_time:93955ms step_avg:54.53ms
step:1724/2150 train_time:94042ms step_avg:54.55ms
step:1725/2150 train_time:94132ms step_avg:54.57ms
step:1726/2150 train_time:94219ms step_avg:54.59ms
step:1727/2150 train_time:94308ms step_avg:54.61ms
step:1728/2150 train_time:94395ms step_avg:54.63ms
step:1729/2150 train_time:94485ms step_avg:54.65ms
step:1730/2150 train_time:94573ms step_avg:54.67ms
step:1731/2150 train_time:94662ms step_avg:54.69ms
step:1732/2150 train_time:94749ms step_avg:54.71ms
step:1733/2150 train_time:94839ms step_avg:54.73ms
step:1734/2150 train_time:94926ms step_avg:54.74ms
step:1735/2150 train_time:95015ms step_avg:54.76ms
step:1736/2150 train_time:95102ms step_avg:54.78ms
step:1737/2150 train_time:95193ms step_avg:54.80ms
step:1738/2150 train_time:95281ms step_avg:54.82ms
step:1739/2150 train_time:95370ms step_avg:54.84ms
step:1740/2150 train_time:95458ms step_avg:54.86ms
step:1741/2150 train_time:95547ms step_avg:54.88ms
step:1742/2150 train_time:95635ms step_avg:54.90ms
step:1743/2150 train_time:95723ms step_avg:54.92ms
step:1744/2150 train_time:95811ms step_avg:54.94ms
step:1745/2150 train_time:95902ms step_avg:54.96ms
step:1746/2150 train_time:95989ms step_avg:54.98ms
step:1747/2150 train_time:96078ms step_avg:55.00ms
step:1748/2150 train_time:96167ms step_avg:55.02ms
step:1749/2150 train_time:96256ms step_avg:55.04ms
step:1750/2150 train_time:96344ms step_avg:55.05ms
step:1750/2150 val_loss:3.3899 train_time:96436ms step_avg:55.11ms
step:1751/2150 train_time:96459ms step_avg:55.09ms
step:1752/2150 train_time:96526ms step_avg:55.09ms
step:1753/2150 train_time:96618ms step_avg:55.12ms
step:1754/2150 train_time:96708ms step_avg:55.14ms
step:1755/2150 train_time:96797ms step_avg:55.16ms
step:1756/2150 train_time:96885ms step_avg:55.17ms
step:1757/2150 train_time:96973ms step_avg:55.19ms
step:1758/2150 train_time:97061ms step_avg:55.21ms
step:1759/2150 train_time:97149ms step_avg:55.23ms
step:1760/2150 train_time:97237ms step_avg:55.25ms
step:1761/2150 train_time:97326ms step_avg:55.27ms
step:1762/2150 train_time:97415ms step_avg:55.29ms
step:1763/2150 train_time:97506ms step_avg:55.31ms
step:1764/2150 train_time:97596ms step_avg:55.33ms
step:1765/2150 train_time:97685ms step_avg:55.35ms
step:1766/2150 train_time:97773ms step_avg:55.36ms
step:1767/2150 train_time:97862ms step_avg:55.38ms
step:1768/2150 train_time:97949ms step_avg:55.40ms
step:1769/2150 train_time:98037ms step_avg:55.42ms
step:1770/2150 train_time:98125ms step_avg:55.44ms
step:1771/2150 train_time:98213ms step_avg:55.46ms
step:1772/2150 train_time:98301ms step_avg:55.47ms
step:1773/2150 train_time:98391ms step_avg:55.49ms
step:1774/2150 train_time:98480ms step_avg:55.51ms
step:1775/2150 train_time:98571ms step_avg:55.53ms
step:1776/2150 train_time:98660ms step_avg:55.55ms
step:1777/2150 train_time:98753ms step_avg:55.57ms
step:1778/2150 train_time:98837ms step_avg:55.59ms
step:1779/2150 train_time:98926ms step_avg:55.61ms
step:1780/2150 train_time:99014ms step_avg:55.63ms
step:1781/2150 train_time:99102ms step_avg:55.64ms
step:1782/2150 train_time:99189ms step_avg:55.66ms
step:1783/2150 train_time:99277ms step_avg:55.68ms
step:1784/2150 train_time:99365ms step_avg:55.70ms
step:1785/2150 train_time:99455ms step_avg:55.72ms
step:1786/2150 train_time:99543ms step_avg:55.74ms
step:1787/2150 train_time:99633ms step_avg:55.75ms
step:1788/2150 train_time:99721ms step_avg:55.77ms
step:1789/2150 train_time:99810ms step_avg:55.79ms
step:1790/2150 train_time:99897ms step_avg:55.81ms
step:1791/2150 train_time:99987ms step_avg:55.83ms
step:1792/2150 train_time:100074ms step_avg:55.84ms
step:1793/2150 train_time:100163ms step_avg:55.86ms
step:1794/2150 train_time:100249ms step_avg:55.88ms
step:1795/2150 train_time:100338ms step_avg:55.90ms
step:1796/2150 train_time:100427ms step_avg:55.92ms
step:1797/2150 train_time:100518ms step_avg:55.94ms
step:1798/2150 train_time:100605ms step_avg:55.95ms
step:1799/2150 train_time:100694ms step_avg:55.97ms
step:1800/2150 train_time:100782ms step_avg:55.99ms
step:1801/2150 train_time:100871ms step_avg:56.01ms
step:1802/2150 train_time:100959ms step_avg:56.03ms
step:1803/2150 train_time:101048ms step_avg:56.04ms
step:1804/2150 train_time:101135ms step_avg:56.06ms
step:1805/2150 train_time:101224ms step_avg:56.08ms
step:1806/2150 train_time:101311ms step_avg:56.10ms
step:1807/2150 train_time:101401ms step_avg:56.12ms
step:1808/2150 train_time:101489ms step_avg:56.13ms
step:1809/2150 train_time:101579ms step_avg:56.15ms
step:1810/2150 train_time:101668ms step_avg:56.17ms
step:1811/2150 train_time:101758ms step_avg:56.19ms
step:1812/2150 train_time:101845ms step_avg:56.21ms
step:1813/2150 train_time:101934ms step_avg:56.22ms
step:1814/2150 train_time:102021ms step_avg:56.24ms
step:1815/2150 train_time:102110ms step_avg:56.26ms
step:1816/2150 train_time:102197ms step_avg:56.28ms
step:1817/2150 train_time:102286ms step_avg:56.29ms
step:1818/2150 train_time:102374ms step_avg:56.31ms
step:1819/2150 train_time:102464ms step_avg:56.33ms
step:1820/2150 train_time:102552ms step_avg:56.35ms
step:1821/2150 train_time:102641ms step_avg:56.37ms
step:1822/2150 train_time:102730ms step_avg:56.38ms
step:1823/2150 train_time:102819ms step_avg:56.40ms
step:1824/2150 train_time:102907ms step_avg:56.42ms
step:1825/2150 train_time:102995ms step_avg:56.44ms
step:1826/2150 train_time:103083ms step_avg:56.45ms
step:1827/2150 train_time:103171ms step_avg:56.47ms
step:1828/2150 train_time:103259ms step_avg:56.49ms
step:1829/2150 train_time:103348ms step_avg:56.51ms
step:1830/2150 train_time:103436ms step_avg:56.52ms
step:1831/2150 train_time:103525ms step_avg:56.54ms
step:1832/2150 train_time:103613ms step_avg:56.56ms
step:1833/2150 train_time:103703ms step_avg:56.58ms
step:1834/2150 train_time:103791ms step_avg:56.59ms
step:1835/2150 train_time:103881ms step_avg:56.61ms
step:1836/2150 train_time:103969ms step_avg:56.63ms
step:1837/2150 train_time:104058ms step_avg:56.65ms
step:1838/2150 train_time:104146ms step_avg:56.66ms
step:1839/2150 train_time:104234ms step_avg:56.68ms
step:1840/2150 train_time:104322ms step_avg:56.70ms
step:1841/2150 train_time:104411ms step_avg:56.71ms
step:1842/2150 train_time:104500ms step_avg:56.73ms
step:1843/2150 train_time:104590ms step_avg:56.75ms
step:1844/2150 train_time:104678ms step_avg:56.77ms
step:1845/2150 train_time:104768ms step_avg:56.79ms
step:1846/2150 train_time:104855ms step_avg:56.80ms
step:1847/2150 train_time:104944ms step_avg:56.82ms
step:1848/2150 train_time:105032ms step_avg:56.84ms
step:1849/2150 train_time:105120ms step_avg:56.85ms
step:1850/2150 train_time:105208ms step_avg:56.87ms
step:1851/2150 train_time:105297ms step_avg:56.89ms
step:1852/2150 train_time:105386ms step_avg:56.90ms
step:1853/2150 train_time:105475ms step_avg:56.92ms
step:1854/2150 train_time:105563ms step_avg:56.94ms
step:1855/2150 train_time:105653ms step_avg:56.96ms
step:1856/2150 train_time:105740ms step_avg:56.97ms
step:1857/2150 train_time:105831ms step_avg:56.99ms
step:1858/2150 train_time:105919ms step_avg:57.01ms
step:1859/2150 train_time:106008ms step_avg:57.02ms
step:1860/2150 train_time:106094ms step_avg:57.04ms
step:1861/2150 train_time:106184ms step_avg:57.06ms
step:1862/2150 train_time:106271ms step_avg:57.07ms
step:1863/2150 train_time:106361ms step_avg:57.09ms
step:1864/2150 train_time:106449ms step_avg:57.11ms
step:1865/2150 train_time:106537ms step_avg:57.12ms
step:1866/2150 train_time:106626ms step_avg:57.14ms
step:1867/2150 train_time:106715ms step_avg:57.16ms
step:1868/2150 train_time:106803ms step_avg:57.17ms
step:1869/2150 train_time:106892ms step_avg:57.19ms
step:1870/2150 train_time:106980ms step_avg:57.21ms
step:1871/2150 train_time:107068ms step_avg:57.23ms
step:1872/2150 train_time:107156ms step_avg:57.24ms
step:1873/2150 train_time:107245ms step_avg:57.26ms
step:1874/2150 train_time:107332ms step_avg:57.27ms
step:1875/2150 train_time:107423ms step_avg:57.29ms
step:1876/2150 train_time:107511ms step_avg:57.31ms
step:1877/2150 train_time:107599ms step_avg:57.33ms
step:1878/2150 train_time:107687ms step_avg:57.34ms
step:1879/2150 train_time:107776ms step_avg:57.36ms
step:1880/2150 train_time:107864ms step_avg:57.37ms
step:1881/2150 train_time:107952ms step_avg:57.39ms
step:1882/2150 train_time:108040ms step_avg:57.41ms
step:1883/2150 train_time:108130ms step_avg:57.42ms
step:1884/2150 train_time:108217ms step_avg:57.44ms
step:1885/2150 train_time:108306ms step_avg:57.46ms
step:1886/2150 train_time:108394ms step_avg:57.47ms
step:1887/2150 train_time:108484ms step_avg:57.49ms
step:1888/2150 train_time:108572ms step_avg:57.51ms
step:1889/2150 train_time:108662ms step_avg:57.52ms
step:1890/2150 train_time:108749ms step_avg:57.54ms
step:1891/2150 train_time:108839ms step_avg:57.56ms
step:1892/2150 train_time:108927ms step_avg:57.57ms
step:1893/2150 train_time:109016ms step_avg:57.59ms
step:1894/2150 train_time:109103ms step_avg:57.60ms
step:1895/2150 train_time:109192ms step_avg:57.62ms
step:1896/2150 train_time:109280ms step_avg:57.64ms
step:1897/2150 train_time:109370ms step_avg:57.65ms
step:1898/2150 train_time:109458ms step_avg:57.67ms
step:1899/2150 train_time:109548ms step_avg:57.69ms
step:1900/2150 train_time:109636ms step_avg:57.70ms
step:1901/2150 train_time:109725ms step_avg:57.72ms
step:1902/2150 train_time:109813ms step_avg:57.74ms
step:1903/2150 train_time:109902ms step_avg:57.75ms
step:1904/2150 train_time:109990ms step_avg:57.77ms
step:1905/2150 train_time:110079ms step_avg:57.78ms
step:1906/2150 train_time:110166ms step_avg:57.80ms
step:1907/2150 train_time:110255ms step_avg:57.82ms
step:1908/2150 train_time:110342ms step_avg:57.83ms
step:1909/2150 train_time:110432ms step_avg:57.85ms
step:1910/2150 train_time:110520ms step_avg:57.86ms
step:1911/2150 train_time:110609ms step_avg:57.88ms
step:1912/2150 train_time:110697ms step_avg:57.90ms
step:1913/2150 train_time:110786ms step_avg:57.91ms
step:1914/2150 train_time:110874ms step_avg:57.93ms
step:1915/2150 train_time:110962ms step_avg:57.94ms
step:1916/2150 train_time:111050ms step_avg:57.96ms
step:1917/2150 train_time:111140ms step_avg:57.98ms
step:1918/2150 train_time:111228ms step_avg:57.99ms
step:1919/2150 train_time:111317ms step_avg:58.01ms
step:1920/2150 train_time:111406ms step_avg:58.02ms
step:1921/2150 train_time:111495ms step_avg:58.04ms
step:1922/2150 train_time:111584ms step_avg:58.06ms
step:1923/2150 train_time:111672ms step_avg:58.07ms
step:1924/2150 train_time:111761ms step_avg:58.09ms
step:1925/2150 train_time:111850ms step_avg:58.10ms
step:1926/2150 train_time:111938ms step_avg:58.12ms
step:1927/2150 train_time:112027ms step_avg:58.14ms
step:1928/2150 train_time:112114ms step_avg:58.15ms
step:1929/2150 train_time:112204ms step_avg:58.17ms
step:1930/2150 train_time:112291ms step_avg:58.18ms
step:1931/2150 train_time:112381ms step_avg:58.20ms
step:1932/2150 train_time:112468ms step_avg:58.21ms
step:1933/2150 train_time:112557ms step_avg:58.23ms
step:1934/2150 train_time:112646ms step_avg:58.24ms
step:1935/2150 train_time:112735ms step_avg:58.26ms
step:1936/2150 train_time:112823ms step_avg:58.28ms
step:1937/2150 train_time:112912ms step_avg:58.29ms
step:1938/2150 train_time:112999ms step_avg:58.31ms
step:1939/2150 train_time:113088ms step_avg:58.32ms
step:1940/2150 train_time:113176ms step_avg:58.34ms
step:1941/2150 train_time:113265ms step_avg:58.35ms
step:1942/2150 train_time:113353ms step_avg:58.37ms
step:1943/2150 train_time:113442ms step_avg:58.38ms
step:1944/2150 train_time:113530ms step_avg:58.40ms
step:1945/2150 train_time:113620ms step_avg:58.42ms
step:1946/2150 train_time:113707ms step_avg:58.43ms
step:1947/2150 train_time:113797ms step_avg:58.45ms
step:1948/2150 train_time:113884ms step_avg:58.46ms
step:1949/2150 train_time:113972ms step_avg:58.48ms
step:1950/2150 train_time:114063ms step_avg:58.49ms
step:1951/2150 train_time:114149ms step_avg:58.51ms
step:1952/2150 train_time:114237ms step_avg:58.52ms
step:1953/2150 train_time:114326ms step_avg:58.54ms
step:1954/2150 train_time:114414ms step_avg:58.55ms
step:1955/2150 train_time:114503ms step_avg:58.57ms
step:1956/2150 train_time:114591ms step_avg:58.58ms
step:1957/2150 train_time:114681ms step_avg:58.60ms
step:1958/2150 train_time:114768ms step_avg:58.62ms
step:1959/2150 train_time:114858ms step_avg:58.63ms
step:1960/2150 train_time:114945ms step_avg:58.65ms
step:1961/2150 train_time:115034ms step_avg:58.66ms
step:1962/2150 train_time:115122ms step_avg:58.68ms
step:1963/2150 train_time:115211ms step_avg:58.69ms
step:1964/2150 train_time:115299ms step_avg:58.71ms
step:1965/2150 train_time:115388ms step_avg:58.72ms
step:1966/2150 train_time:115476ms step_avg:58.74ms
step:1967/2150 train_time:115566ms step_avg:58.75ms
step:1968/2150 train_time:115654ms step_avg:58.77ms
step:1969/2150 train_time:115744ms step_avg:58.78ms
step:1970/2150 train_time:115831ms step_avg:58.80ms
step:1971/2150 train_time:115920ms step_avg:58.81ms
step:1972/2150 train_time:116007ms step_avg:58.83ms
step:1973/2150 train_time:116096ms step_avg:58.84ms
step:1974/2150 train_time:116183ms step_avg:58.86ms
step:1975/2150 train_time:116272ms step_avg:58.87ms
step:1976/2150 train_time:116360ms step_avg:58.89ms
step:1977/2150 train_time:116449ms step_avg:58.90ms
step:1978/2150 train_time:116537ms step_avg:58.92ms
step:1979/2150 train_time:116627ms step_avg:58.93ms
step:1980/2150 train_time:116715ms step_avg:58.95ms
step:1981/2150 train_time:116804ms step_avg:58.96ms
step:1982/2150 train_time:116891ms step_avg:58.98ms
step:1983/2150 train_time:116981ms step_avg:58.99ms
step:1984/2150 train_time:117069ms step_avg:59.01ms
step:1985/2150 train_time:117159ms step_avg:59.02ms
step:1986/2150 train_time:117246ms step_avg:59.04ms
step:1987/2150 train_time:117335ms step_avg:59.05ms
step:1988/2150 train_time:117422ms step_avg:59.07ms
step:1989/2150 train_time:117511ms step_avg:59.08ms
step:1990/2150 train_time:117599ms step_avg:59.10ms
step:1991/2150 train_time:117689ms step_avg:59.11ms
step:1992/2150 train_time:117777ms step_avg:59.12ms
step:1993/2150 train_time:117866ms step_avg:59.14ms
step:1994/2150 train_time:117954ms step_avg:59.15ms
step:1995/2150 train_time:118042ms step_avg:59.17ms
step:1996/2150 train_time:118130ms step_avg:59.18ms
step:1997/2150 train_time:118220ms step_avg:59.20ms
step:1998/2150 train_time:118308ms step_avg:59.21ms
step:1999/2150 train_time:118396ms step_avg:59.23ms
step:2000/2150 train_time:118484ms step_avg:59.24ms
step:2000/2150 val_loss:3.3135 train_time:118575ms step_avg:59.29ms
step:2001/2150 train_time:118599ms step_avg:59.27ms
step:2002/2150 train_time:118665ms step_avg:59.27ms
step:2003/2150 train_time:118760ms step_avg:59.29ms
step:2004/2150 train_time:118850ms step_avg:59.31ms
step:2005/2150 train_time:118938ms step_avg:59.32ms
step:2006/2150 train_time:119024ms step_avg:59.33ms
step:2007/2150 train_time:119111ms step_avg:59.35ms
step:2008/2150 train_time:119198ms step_avg:59.36ms
step:2009/2150 train_time:119287ms step_avg:59.38ms
step:2010/2150 train_time:119377ms step_avg:59.39ms
step:2011/2150 train_time:119465ms step_avg:59.41ms
step:2012/2150 train_time:119553ms step_avg:59.42ms
step:2013/2150 train_time:119644ms step_avg:59.44ms
step:2014/2150 train_time:119737ms step_avg:59.45ms
step:2015/2150 train_time:119826ms step_avg:59.47ms
step:2016/2150 train_time:119914ms step_avg:59.48ms
step:2017/2150 train_time:120002ms step_avg:59.50ms
step:2018/2150 train_time:120088ms step_avg:59.51ms
step:2019/2150 train_time:120177ms step_avg:59.52ms
step:2020/2150 train_time:120263ms step_avg:59.54ms
step:2021/2150 train_time:120352ms step_avg:59.55ms
step:2022/2150 train_time:120439ms step_avg:59.56ms
step:2023/2150 train_time:120529ms step_avg:59.58ms
step:2024/2150 train_time:120618ms step_avg:59.59ms
step:2025/2150 train_time:120709ms step_avg:59.61ms
step:2026/2150 train_time:120798ms step_avg:59.62ms
step:2027/2150 train_time:120887ms step_avg:59.64ms
step:2028/2150 train_time:120974ms step_avg:59.65ms
step:2029/2150 train_time:121062ms step_avg:59.67ms
step:2030/2150 train_time:121149ms step_avg:59.68ms
step:2031/2150 train_time:121239ms step_avg:59.69ms
step:2032/2150 train_time:121327ms step_avg:59.71ms
step:2033/2150 train_time:121415ms step_avg:59.72ms
step:2034/2150 train_time:121503ms step_avg:59.74ms
step:2035/2150 train_time:121593ms step_avg:59.75ms
step:2036/2150 train_time:121682ms step_avg:59.77ms
step:2037/2150 train_time:121772ms step_avg:59.78ms
step:2038/2150 train_time:121860ms step_avg:59.79ms
step:2039/2150 train_time:121950ms step_avg:59.81ms
step:2040/2150 train_time:122037ms step_avg:59.82ms
step:2041/2150 train_time:122126ms step_avg:59.84ms
step:2042/2150 train_time:122214ms step_avg:59.85ms
step:2043/2150 train_time:122303ms step_avg:59.86ms
step:2044/2150 train_time:122389ms step_avg:59.88ms
step:2045/2150 train_time:122477ms step_avg:59.89ms
step:2046/2150 train_time:122566ms step_avg:59.91ms
step:2047/2150 train_time:122657ms step_avg:59.92ms
step:2048/2150 train_time:122746ms step_avg:59.93ms
step:2049/2150 train_time:122835ms step_avg:59.95ms
step:2050/2150 train_time:122924ms step_avg:59.96ms
step:2051/2150 train_time:123013ms step_avg:59.98ms
step:2052/2150 train_time:123100ms step_avg:59.99ms
step:2053/2150 train_time:123189ms step_avg:60.00ms
step:2054/2150 train_time:123275ms step_avg:60.02ms
step:2055/2150 train_time:123364ms step_avg:60.03ms
step:2056/2150 train_time:123451ms step_avg:60.04ms
step:2057/2150 train_time:123540ms step_avg:60.06ms
step:2058/2150 train_time:123629ms step_avg:60.07ms
step:2059/2150 train_time:123719ms step_avg:60.09ms
step:2060/2150 train_time:123807ms step_avg:60.10ms
step:2061/2150 train_time:123897ms step_avg:60.12ms
step:2062/2150 train_time:123985ms step_avg:60.13ms
step:2063/2150 train_time:124074ms step_avg:60.14ms
step:2064/2150 train_time:124161ms step_avg:60.16ms
step:2065/2150 train_time:124250ms step_avg:60.17ms
step:2066/2150 train_time:124337ms step_avg:60.18ms
step:2067/2150 train_time:124426ms step_avg:60.20ms
step:2068/2150 train_time:124514ms step_avg:60.21ms
step:2069/2150 train_time:124603ms step_avg:60.22ms
step:2070/2150 train_time:124694ms step_avg:60.24ms
step:2071/2150 train_time:124782ms step_avg:60.25ms
step:2072/2150 train_time:124870ms step_avg:60.27ms
step:2073/2150 train_time:124960ms step_avg:60.28ms
step:2074/2150 train_time:125048ms step_avg:60.29ms
step:2075/2150 train_time:125137ms step_avg:60.31ms
step:2076/2150 train_time:125225ms step_avg:60.32ms
step:2077/2150 train_time:125315ms step_avg:60.33ms
step:2078/2150 train_time:125402ms step_avg:60.35ms
step:2079/2150 train_time:125490ms step_avg:60.36ms
step:2080/2150 train_time:125577ms step_avg:60.37ms
step:2081/2150 train_time:125667ms step_avg:60.39ms
step:2082/2150 train_time:125756ms step_avg:60.40ms
step:2083/2150 train_time:125845ms step_avg:60.42ms
step:2084/2150 train_time:125933ms step_avg:60.43ms
step:2085/2150 train_time:126022ms step_avg:60.44ms
step:2086/2150 train_time:126110ms step_avg:60.46ms
step:2087/2150 train_time:126198ms step_avg:60.47ms
step:2088/2150 train_time:126287ms step_avg:60.48ms
step:2089/2150 train_time:126375ms step_avg:60.50ms
step:2090/2150 train_time:126462ms step_avg:60.51ms
step:2091/2150 train_time:126552ms step_avg:60.52ms
step:2092/2150 train_time:126640ms step_avg:60.54ms
step:2093/2150 train_time:126729ms step_avg:60.55ms
step:2094/2150 train_time:126817ms step_avg:60.56ms
step:2095/2150 train_time:126906ms step_avg:60.58ms
step:2096/2150 train_time:126994ms step_avg:60.59ms
step:2097/2150 train_time:127083ms step_avg:60.60ms
step:2098/2150 train_time:127170ms step_avg:60.61ms
step:2099/2150 train_time:127260ms step_avg:60.63ms
step:2100/2150 train_time:127347ms step_avg:60.64ms
step:2101/2150 train_time:127436ms step_avg:60.65ms
step:2102/2150 train_time:127523ms step_avg:60.67ms
step:2103/2150 train_time:127612ms step_avg:60.68ms
step:2104/2150 train_time:127700ms step_avg:60.69ms
step:2105/2150 train_time:127789ms step_avg:60.71ms
step:2106/2150 train_time:127877ms step_avg:60.72ms
step:2107/2150 train_time:127965ms step_avg:60.73ms
step:2108/2150 train_time:128054ms step_avg:60.75ms
step:2109/2150 train_time:128143ms step_avg:60.76ms
step:2110/2150 train_time:128231ms step_avg:60.77ms
step:2111/2150 train_time:128320ms step_avg:60.79ms
step:2112/2150 train_time:128408ms step_avg:60.80ms
step:2113/2150 train_time:128497ms step_avg:60.81ms
step:2114/2150 train_time:128585ms step_avg:60.83ms
step:2115/2150 train_time:128673ms step_avg:60.84ms
step:2116/2150 train_time:128761ms step_avg:60.85ms
step:2117/2150 train_time:128850ms step_avg:60.86ms
step:2118/2150 train_time:128938ms step_avg:60.88ms
step:2119/2150 train_time:129028ms step_avg:60.89ms
step:2120/2150 train_time:129116ms step_avg:60.90ms
step:2121/2150 train_time:129207ms step_avg:60.92ms
step:2122/2150 train_time:129295ms step_avg:60.93ms
step:2123/2150 train_time:129383ms step_avg:60.94ms
step:2124/2150 train_time:129472ms step_avg:60.96ms
step:2125/2150 train_time:129561ms step_avg:60.97ms
step:2126/2150 train_time:129650ms step_avg:60.98ms
step:2127/2150 train_time:129739ms step_avg:61.00ms
step:2128/2150 train_time:129827ms step_avg:61.01ms
step:2129/2150 train_time:129916ms step_avg:61.02ms
step:2130/2150 train_time:130005ms step_avg:61.04ms
step:2131/2150 train_time:130095ms step_avg:61.05ms
step:2132/2150 train_time:130183ms step_avg:61.06ms
step:2133/2150 train_time:130272ms step_avg:61.07ms
step:2134/2150 train_time:130360ms step_avg:61.09ms
step:2135/2150 train_time:130449ms step_avg:61.10ms
step:2136/2150 train_time:130537ms step_avg:61.11ms
step:2137/2150 train_time:130626ms step_avg:61.13ms
step:2138/2150 train_time:130715ms step_avg:61.14ms
step:2139/2150 train_time:130804ms step_avg:61.15ms
step:2140/2150 train_time:130892ms step_avg:61.16ms
step:2141/2150 train_time:130981ms step_avg:61.18ms
step:2142/2150 train_time:131068ms step_avg:61.19ms
step:2143/2150 train_time:131159ms step_avg:61.20ms
step:2144/2150 train_time:131247ms step_avg:61.22ms
step:2145/2150 train_time:131337ms step_avg:61.23ms
step:2146/2150 train_time:131425ms step_avg:61.24ms
step:2147/2150 train_time:131515ms step_avg:61.26ms
step:2148/2150 train_time:131603ms step_avg:61.27ms
step:2149/2150 train_time:131692ms step_avg:61.28ms
step:2150/2150 train_time:131780ms step_avg:61.29ms
step:2150/2150 val_loss:3.2790 train_time:131872ms step_avg:61.34ms
peak memory allocated: 29892 MiB reserved: 44696 MiB

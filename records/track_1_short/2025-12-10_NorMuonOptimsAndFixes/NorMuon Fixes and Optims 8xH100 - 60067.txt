import uuid
run_id = f"NorMuon Fixes and Optims 8xH100 - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2115  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# Profiling configuration
ENABLE_PROFILING = os.environ.get("ENABLE_PROFILING", "0") == "1"
PROFILE_WAIT_STEPS = 10
PROFILE_WARMUP_STEPS = 5
PROFILE_ACTIVE_STEPS = 4
PROFILE_TOTAL_STEPS = PROFILE_WAIT_STEPS + PROFILE_WARMUP_STEPS + PROFILE_ACTIVE_STEPS

if ENABLE_PROFILING:
    print0(f"Profiling enabled: wait={PROFILE_WAIT_STEPS}, warmup={PROFILE_WARMUP_STEPS}, active={PROFILE_ACTIVE_STEPS}", console=True)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations

# Override train_steps if profiling is enabled
if ENABLE_PROFILING:
    train_steps = PROFILE_TOTAL_STEPS
    print0(f"Profiling mode: running {train_steps} steps total", console=True)

# Setup profiler if enabled
profiler = None
if ENABLE_PROFILING:
    from torch.profiler import profile, ProfilerActivity, schedule
    
    profiler_schedule = schedule(
        wait=PROFILE_WAIT_STEPS,
        warmup=PROFILE_WARMUP_STEPS,
        active=PROFILE_ACTIVE_STEPS,
        repeat=1
    )
    
    profile_dir = f"logs/{run_id}/profile"
    if master_process:
        os.makedirs(profile_dir, exist_ok=True)
    
    # Custom handler to export chrome trace format (compatible with perfetto)
    def chrome_trace_handler(dir_name):
        def handler_fn(prof):
            # Export chrome trace for perfetto viewer
            chrome_trace_file = f"{dir_name}/rank{rank}_step{prof.step_num}.json"
            prof.export_chrome_trace(chrome_trace_file)
            if master_process:
                print0(f"Exported chrome trace: {chrome_trace_file}", console=True)
        return handler_fn
    
    profiler = profile(
        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
        schedule=profiler_schedule,
        on_trace_ready=chrome_trace_handler(profile_dir),
        record_shapes=True,
        profile_memory=True,
        with_stack=False,
        with_flops=True,
    )
    profiler.start()
    print0(f"Profiler started, chrome traces will be saved to {profile_dir}", console=True)

ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)
    
    # Step profiler if enabled
    if profiler is not None:
        profiler.step()

# Stop profiler if enabled
if profiler is not None:
    profiler.stop()
    print0(f"Profiling complete. Chrome traces saved to logs/{run_id}/profile", console=True)
    print0(f"View in perfetto: https://ui.perfetto.dev/", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 10 2025, 18:15:56) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 19:46:42 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   36C    P0            119W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   33C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   31C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   37C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   37C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   35C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              75      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A              76      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              77      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              78      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              79      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              80      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              81      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A              76      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A              77      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A              78      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A              79      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A              80      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A              81      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A              82      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2155 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2155 train_time:90ms step_avg:90.40ms
step:2/2155 train_time:170ms step_avg:85.13ms
step:3/2155 train_time:193ms step_avg:64.20ms
step:4/2155 train_time:226ms step_avg:56.41ms
step:5/2155 train_time:258ms step_avg:51.64ms
step:6/2155 train_time:353ms step_avg:58.89ms
step:7/2155 train_time:374ms step_avg:53.42ms
step:8/2155 train_time:407ms step_avg:50.90ms
step:9/2155 train_time:429ms step_avg:47.62ms
step:10/2155 train_time:461ms step_avg:46.10ms
step:11/2155 train_time:494ms step_avg:44.88ms
step:12/2155 train_time:527ms step_avg:43.95ms
step:13/2155 train_time:560ms step_avg:43.06ms
step:14/2155 train_time:594ms step_avg:42.40ms
step:15/2155 train_time:626ms step_avg:41.74ms
step:16/2155 train_time:660ms step_avg:41.22ms
step:17/2155 train_time:692ms step_avg:40.72ms
step:18/2155 train_time:726ms step_avg:40.33ms
step:19/2155 train_time:759ms step_avg:39.92ms
step:20/2155 train_time:792ms step_avg:39.61ms
step:21/2155 train_time:825ms step_avg:39.27ms
step:22/2155 train_time:858ms step_avg:39.01ms
step:23/2155 train_time:891ms step_avg:38.73ms
step:24/2155 train_time:925ms step_avg:38.53ms
step:25/2155 train_time:957ms step_avg:38.28ms
step:26/2155 train_time:991ms step_avg:38.10ms
step:27/2155 train_time:1023ms step_avg:37.89ms
step:28/2155 train_time:1057ms step_avg:37.74ms
step:29/2155 train_time:1089ms step_avg:37.56ms
step:30/2155 train_time:1123ms step_avg:37.43ms
step:31/2155 train_time:1155ms step_avg:37.27ms
step:32/2155 train_time:1189ms step_avg:37.16ms
step:33/2155 train_time:1222ms step_avg:37.04ms
step:34/2155 train_time:1257ms step_avg:36.98ms
step:35/2155 train_time:1292ms step_avg:36.91ms
step:36/2155 train_time:1326ms step_avg:36.84ms
step:37/2155 train_time:1361ms step_avg:36.78ms
step:38/2155 train_time:1395ms step_avg:36.70ms
step:39/2155 train_time:1429ms step_avg:36.63ms
step:40/2155 train_time:1462ms step_avg:36.55ms
step:41/2155 train_time:1495ms step_avg:36.47ms
step:42/2155 train_time:1529ms step_avg:36.40ms
step:43/2155 train_time:1562ms step_avg:36.33ms
step:44/2155 train_time:1596ms step_avg:36.26ms
step:45/2155 train_time:1628ms step_avg:36.19ms
step:46/2155 train_time:1662ms step_avg:36.13ms
step:47/2155 train_time:1695ms step_avg:36.07ms
step:48/2155 train_time:1729ms step_avg:36.02ms
step:49/2155 train_time:1762ms step_avg:35.96ms
step:50/2155 train_time:1796ms step_avg:35.91ms
step:51/2155 train_time:1828ms step_avg:35.85ms
step:52/2155 train_time:1862ms step_avg:35.80ms
step:53/2155 train_time:1895ms step_avg:35.76ms
step:54/2155 train_time:1928ms step_avg:35.71ms
step:55/2155 train_time:1961ms step_avg:35.65ms
step:56/2155 train_time:1994ms step_avg:35.61ms
step:57/2155 train_time:2026ms step_avg:35.55ms
step:58/2155 train_time:2060ms step_avg:35.52ms
step:59/2155 train_time:2092ms step_avg:35.47ms
step:60/2155 train_time:2126ms step_avg:35.43ms
step:61/2155 train_time:2159ms step_avg:35.39ms
step:62/2155 train_time:2192ms step_avg:35.36ms
step:63/2155 train_time:2226ms step_avg:35.33ms
step:64/2155 train_time:2260ms step_avg:35.30ms
step:65/2155 train_time:2293ms step_avg:35.28ms
step:66/2155 train_time:2327ms step_avg:35.26ms
step:67/2155 train_time:2361ms step_avg:35.24ms
step:68/2155 train_time:2394ms step_avg:35.21ms
step:69/2155 train_time:2428ms step_avg:35.18ms
step:70/2155 train_time:2461ms step_avg:35.16ms
step:71/2155 train_time:2494ms step_avg:35.13ms
step:72/2155 train_time:2528ms step_avg:35.11ms
step:73/2155 train_time:2561ms step_avg:35.08ms
step:74/2155 train_time:2596ms step_avg:35.08ms
step:75/2155 train_time:2627ms step_avg:35.03ms
step:76/2155 train_time:2661ms step_avg:35.01ms
step:77/2155 train_time:2694ms step_avg:34.99ms
step:78/2155 train_time:2728ms step_avg:34.97ms
step:79/2155 train_time:2761ms step_avg:34.95ms
step:80/2155 train_time:2794ms step_avg:34.93ms
step:81/2155 train_time:2827ms step_avg:34.90ms
step:82/2155 train_time:2861ms step_avg:34.88ms
step:83/2155 train_time:2893ms step_avg:34.86ms
step:84/2155 train_time:2927ms step_avg:34.84ms
step:85/2155 train_time:2959ms step_avg:34.82ms
step:86/2155 train_time:2993ms step_avg:34.80ms
step:87/2155 train_time:3026ms step_avg:34.78ms
step:88/2155 train_time:3059ms step_avg:34.76ms
step:89/2155 train_time:3092ms step_avg:34.74ms
step:90/2155 train_time:3126ms step_avg:34.73ms
step:91/2155 train_time:3158ms step_avg:34.71ms
step:92/2155 train_time:3192ms step_avg:34.69ms
step:93/2155 train_time:3225ms step_avg:34.67ms
step:94/2155 train_time:3258ms step_avg:34.66ms
step:95/2155 train_time:3292ms step_avg:34.65ms
step:96/2155 train_time:3325ms step_avg:34.64ms
step:97/2155 train_time:3358ms step_avg:34.62ms
step:98/2155 train_time:3396ms step_avg:34.65ms
step:99/2155 train_time:3424ms step_avg:34.59ms
step:100/2155 train_time:3458ms step_avg:34.58ms
step:101/2155 train_time:3491ms step_avg:34.56ms
step:102/2155 train_time:3524ms step_avg:34.55ms
step:103/2155 train_time:3557ms step_avg:34.53ms
step:104/2155 train_time:3590ms step_avg:34.52ms
step:105/2155 train_time:3623ms step_avg:34.50ms
step:106/2155 train_time:3656ms step_avg:34.49ms
step:107/2155 train_time:3689ms step_avg:34.48ms
step:108/2155 train_time:3723ms step_avg:34.47ms
step:109/2155 train_time:3756ms step_avg:34.46ms
step:110/2155 train_time:3789ms step_avg:34.45ms
step:111/2155 train_time:3822ms step_avg:34.43ms
step:112/2155 train_time:3855ms step_avg:34.42ms
step:113/2155 train_time:3888ms step_avg:34.40ms
step:114/2155 train_time:3921ms step_avg:34.40ms
step:115/2155 train_time:3953ms step_avg:34.38ms
step:116/2155 train_time:3987ms step_avg:34.37ms
step:117/2155 train_time:4019ms step_avg:34.35ms
step:118/2155 train_time:4053ms step_avg:34.35ms
step:119/2155 train_time:4085ms step_avg:34.33ms
step:120/2155 train_time:4119ms step_avg:34.32ms
step:121/2155 train_time:4151ms step_avg:34.31ms
step:122/2155 train_time:4185ms step_avg:34.30ms
step:123/2155 train_time:4217ms step_avg:34.29ms
step:124/2155 train_time:4251ms step_avg:34.28ms
step:125/2155 train_time:4283ms step_avg:34.27ms
step:126/2155 train_time:4316ms step_avg:34.26ms
step:127/2155 train_time:4349ms step_avg:34.25ms
step:128/2155 train_time:4383ms step_avg:34.24ms
step:129/2155 train_time:4415ms step_avg:34.23ms
step:130/2155 train_time:4449ms step_avg:34.22ms
step:131/2155 train_time:4482ms step_avg:34.21ms
step:132/2155 train_time:4515ms step_avg:34.20ms
step:133/2155 train_time:4548ms step_avg:34.19ms
step:134/2155 train_time:4581ms step_avg:34.19ms
step:135/2155 train_time:4615ms step_avg:34.18ms
step:136/2155 train_time:4648ms step_avg:34.18ms
step:137/2155 train_time:4681ms step_avg:34.17ms
step:138/2155 train_time:4714ms step_avg:34.16ms
step:139/2155 train_time:4747ms step_avg:34.15ms
step:140/2155 train_time:4780ms step_avg:34.15ms
step:141/2155 train_time:4813ms step_avg:34.13ms
step:142/2155 train_time:4846ms step_avg:34.13ms
step:143/2155 train_time:4879ms step_avg:34.12ms
step:144/2155 train_time:4912ms step_avg:34.11ms
step:145/2155 train_time:4945ms step_avg:34.10ms
step:146/2155 train_time:4983ms step_avg:34.13ms
step:147/2155 train_time:5011ms step_avg:34.09ms
step:148/2155 train_time:5045ms step_avg:34.09ms
step:149/2155 train_time:5077ms step_avg:34.08ms
step:150/2155 train_time:5111ms step_avg:34.07ms
step:151/2155 train_time:5143ms step_avg:34.06ms
step:152/2155 train_time:5177ms step_avg:34.06ms
step:153/2155 train_time:5209ms step_avg:34.05ms
step:154/2155 train_time:5243ms step_avg:34.04ms
step:155/2155 train_time:5275ms step_avg:34.03ms
step:156/2155 train_time:5309ms step_avg:34.03ms
step:157/2155 train_time:5342ms step_avg:34.02ms
step:158/2155 train_time:5375ms step_avg:34.02ms
step:159/2155 train_time:5408ms step_avg:34.01ms
step:160/2155 train_time:5441ms step_avg:34.01ms
step:161/2155 train_time:5474ms step_avg:34.00ms
step:162/2155 train_time:5507ms step_avg:33.99ms
step:163/2155 train_time:5540ms step_avg:33.99ms
step:164/2155 train_time:5573ms step_avg:33.98ms
step:165/2155 train_time:5606ms step_avg:33.98ms
step:166/2155 train_time:5640ms step_avg:33.98ms
step:167/2155 train_time:5673ms step_avg:33.97ms
step:168/2155 train_time:5706ms step_avg:33.97ms
step:169/2155 train_time:5739ms step_avg:33.96ms
step:170/2155 train_time:5772ms step_avg:33.95ms
step:171/2155 train_time:5805ms step_avg:33.94ms
step:172/2155 train_time:5838ms step_avg:33.94ms
step:173/2155 train_time:5871ms step_avg:33.93ms
step:174/2155 train_time:5904ms step_avg:33.93ms
step:175/2155 train_time:5937ms step_avg:33.92ms
step:176/2155 train_time:5970ms step_avg:33.92ms
step:177/2155 train_time:6003ms step_avg:33.92ms
step:178/2155 train_time:6036ms step_avg:33.91ms
step:179/2155 train_time:6069ms step_avg:33.91ms
step:180/2155 train_time:6103ms step_avg:33.90ms
step:181/2155 train_time:6136ms step_avg:33.90ms
step:182/2155 train_time:6169ms step_avg:33.90ms
step:183/2155 train_time:6202ms step_avg:33.89ms
step:184/2155 train_time:6235ms step_avg:33.89ms
step:185/2155 train_time:6268ms step_avg:33.88ms
step:186/2155 train_time:6301ms step_avg:33.88ms
step:187/2155 train_time:6334ms step_avg:33.87ms
step:188/2155 train_time:6367ms step_avg:33.87ms
step:189/2155 train_time:6400ms step_avg:33.86ms
step:190/2155 train_time:6433ms step_avg:33.86ms
step:191/2155 train_time:6466ms step_avg:33.85ms
step:192/2155 train_time:6501ms step_avg:33.86ms
step:193/2155 train_time:6532ms step_avg:33.85ms
step:194/2155 train_time:6566ms step_avg:33.84ms
step:195/2155 train_time:6598ms step_avg:33.84ms
step:196/2155 train_time:6632ms step_avg:33.84ms
step:197/2155 train_time:6664ms step_avg:33.83ms
step:198/2155 train_time:6698ms step_avg:33.83ms
step:199/2155 train_time:6730ms step_avg:33.82ms
step:200/2155 train_time:6764ms step_avg:33.82ms
step:201/2155 train_time:6797ms step_avg:33.81ms
step:202/2155 train_time:6830ms step_avg:33.81ms
step:203/2155 train_time:6863ms step_avg:33.81ms
step:204/2155 train_time:6897ms step_avg:33.81ms
step:205/2155 train_time:6929ms step_avg:33.80ms
step:206/2155 train_time:6962ms step_avg:33.80ms
step:207/2155 train_time:6995ms step_avg:33.79ms
step:208/2155 train_time:7029ms step_avg:33.79ms
step:209/2155 train_time:7062ms step_avg:33.79ms
step:210/2155 train_time:7095ms step_avg:33.79ms
step:211/2155 train_time:7128ms step_avg:33.78ms
step:212/2155 train_time:7161ms step_avg:33.78ms
step:213/2155 train_time:7194ms step_avg:33.77ms
step:214/2155 train_time:7227ms step_avg:33.77ms
step:215/2155 train_time:7260ms step_avg:33.77ms
step:216/2155 train_time:7293ms step_avg:33.77ms
step:217/2155 train_time:7326ms step_avg:33.76ms
step:218/2155 train_time:7360ms step_avg:33.76ms
step:219/2155 train_time:7392ms step_avg:33.75ms
step:220/2155 train_time:7426ms step_avg:33.75ms
step:221/2155 train_time:7458ms step_avg:33.75ms
step:222/2155 train_time:7492ms step_avg:33.75ms
step:223/2155 train_time:7524ms step_avg:33.74ms
step:224/2155 train_time:7558ms step_avg:33.74ms
step:225/2155 train_time:7590ms step_avg:33.73ms
step:226/2155 train_time:7624ms step_avg:33.73ms
step:227/2155 train_time:7656ms step_avg:33.73ms
step:228/2155 train_time:7689ms step_avg:33.73ms
step:229/2155 train_time:7722ms step_avg:33.72ms
step:230/2155 train_time:7755ms step_avg:33.72ms
step:231/2155 train_time:7788ms step_avg:33.71ms
step:232/2155 train_time:7822ms step_avg:33.71ms
step:233/2155 train_time:7855ms step_avg:33.71ms
step:234/2155 train_time:7893ms step_avg:33.73ms
step:235/2155 train_time:7920ms step_avg:33.70ms
step:236/2155 train_time:7954ms step_avg:33.70ms
step:237/2155 train_time:7987ms step_avg:33.70ms
step:238/2155 train_time:8020ms step_avg:33.70ms
step:239/2155 train_time:8053ms step_avg:33.69ms
step:240/2155 train_time:8086ms step_avg:33.69ms
step:241/2155 train_time:8119ms step_avg:33.69ms
step:242/2155 train_time:8153ms step_avg:33.69ms
step:243/2155 train_time:8185ms step_avg:33.68ms
step:244/2155 train_time:8218ms step_avg:33.68ms
step:245/2155 train_time:8251ms step_avg:33.68ms
step:246/2155 train_time:8284ms step_avg:33.68ms
step:247/2155 train_time:8317ms step_avg:33.67ms
step:248/2155 train_time:8350ms step_avg:33.67ms
step:249/2155 train_time:8383ms step_avg:33.67ms
step:250/2155 train_time:8416ms step_avg:33.66ms
step:250/2155 val_loss:4.3238 train_time:8453ms step_avg:33.81ms
step:251/2155 train_time:8475ms step_avg:33.76ms
step:252/2155 train_time:8497ms step_avg:33.72ms
step:253/2155 train_time:8519ms step_avg:33.67ms
step:254/2155 train_time:8554ms step_avg:33.68ms
step:255/2155 train_time:8589ms step_avg:33.68ms
step:256/2155 train_time:8624ms step_avg:33.69ms
step:257/2155 train_time:8658ms step_avg:33.69ms
step:258/2155 train_time:8691ms step_avg:33.69ms
step:259/2155 train_time:8725ms step_avg:33.69ms
step:260/2155 train_time:8758ms step_avg:33.68ms
step:261/2155 train_time:8791ms step_avg:33.68ms
step:262/2155 train_time:8824ms step_avg:33.68ms
step:263/2155 train_time:8857ms step_avg:33.68ms
step:264/2155 train_time:8890ms step_avg:33.67ms
step:265/2155 train_time:8923ms step_avg:33.67ms
step:266/2155 train_time:8956ms step_avg:33.67ms
step:267/2155 train_time:8988ms step_avg:33.66ms
step:268/2155 train_time:9022ms step_avg:33.66ms
step:269/2155 train_time:9054ms step_avg:33.66ms
step:270/2155 train_time:9087ms step_avg:33.66ms
step:271/2155 train_time:9120ms step_avg:33.65ms
step:272/2155 train_time:9153ms step_avg:33.65ms
step:273/2155 train_time:9185ms step_avg:33.64ms
step:274/2155 train_time:9219ms step_avg:33.64ms
step:275/2155 train_time:9251ms step_avg:33.64ms
step:276/2155 train_time:9284ms step_avg:33.64ms
step:277/2155 train_time:9316ms step_avg:33.63ms
step:278/2155 train_time:9350ms step_avg:33.63ms
step:279/2155 train_time:9382ms step_avg:33.63ms
step:280/2155 train_time:9415ms step_avg:33.63ms
step:281/2155 train_time:9448ms step_avg:33.62ms
step:282/2155 train_time:9481ms step_avg:33.62ms
step:283/2155 train_time:9514ms step_avg:33.62ms
step:284/2155 train_time:9548ms step_avg:33.62ms
step:285/2155 train_time:9581ms step_avg:33.62ms
step:286/2155 train_time:9615ms step_avg:33.62ms
step:287/2155 train_time:9648ms step_avg:33.62ms
step:288/2155 train_time:9682ms step_avg:33.62ms
step:289/2155 train_time:9714ms step_avg:33.61ms
step:290/2155 train_time:9748ms step_avg:33.61ms
step:291/2155 train_time:9781ms step_avg:33.61ms
step:292/2155 train_time:9814ms step_avg:33.61ms
step:293/2155 train_time:9846ms step_avg:33.61ms
step:294/2155 train_time:9880ms step_avg:33.61ms
step:295/2155 train_time:9913ms step_avg:33.60ms
step:296/2155 train_time:9946ms step_avg:33.60ms
step:297/2155 train_time:9979ms step_avg:33.60ms
step:298/2155 train_time:10012ms step_avg:33.60ms
step:299/2155 train_time:10045ms step_avg:33.59ms
step:300/2155 train_time:10078ms step_avg:33.59ms
step:301/2155 train_time:10111ms step_avg:33.59ms
step:302/2155 train_time:10144ms step_avg:33.59ms
step:303/2155 train_time:10176ms step_avg:33.59ms
step:304/2155 train_time:10210ms step_avg:33.58ms
step:305/2155 train_time:10242ms step_avg:33.58ms
step:306/2155 train_time:10276ms step_avg:33.58ms
step:307/2155 train_time:10308ms step_avg:33.58ms
step:308/2155 train_time:10341ms step_avg:33.58ms
step:309/2155 train_time:10374ms step_avg:33.57ms
step:310/2155 train_time:10407ms step_avg:33.57ms
step:311/2155 train_time:10440ms step_avg:33.57ms
step:312/2155 train_time:10474ms step_avg:33.57ms
step:313/2155 train_time:10506ms step_avg:33.57ms
step:314/2155 train_time:10540ms step_avg:33.57ms
step:315/2155 train_time:10573ms step_avg:33.56ms
step:316/2155 train_time:10606ms step_avg:33.56ms
step:317/2155 train_time:10639ms step_avg:33.56ms
step:318/2155 train_time:10673ms step_avg:33.56ms
step:319/2155 train_time:10705ms step_avg:33.56ms
step:320/2155 train_time:10739ms step_avg:33.56ms
step:321/2155 train_time:10771ms step_avg:33.56ms
step:322/2155 train_time:10805ms step_avg:33.56ms
step:323/2155 train_time:10837ms step_avg:33.55ms
step:324/2155 train_time:10871ms step_avg:33.55ms
step:325/2155 train_time:10904ms step_avg:33.55ms
step:326/2155 train_time:10937ms step_avg:33.55ms
step:327/2155 train_time:10970ms step_avg:33.55ms
step:328/2155 train_time:11003ms step_avg:33.55ms
step:329/2155 train_time:11037ms step_avg:33.55ms
step:330/2155 train_time:11070ms step_avg:33.55ms
step:331/2155 train_time:11103ms step_avg:33.54ms
step:332/2155 train_time:11137ms step_avg:33.54ms
step:333/2155 train_time:11169ms step_avg:33.54ms
step:334/2155 train_time:11202ms step_avg:33.54ms
step:335/2155 train_time:11234ms step_avg:33.54ms
step:336/2155 train_time:11268ms step_avg:33.53ms
step:337/2155 train_time:11300ms step_avg:33.53ms
step:338/2155 train_time:11333ms step_avg:33.53ms
step:339/2155 train_time:11366ms step_avg:33.53ms
step:340/2155 train_time:11399ms step_avg:33.53ms
step:341/2155 train_time:11435ms step_avg:33.53ms
step:342/2155 train_time:11465ms step_avg:33.52ms
step:343/2155 train_time:11498ms step_avg:33.52ms
step:344/2155 train_time:11531ms step_avg:33.52ms
step:345/2155 train_time:11564ms step_avg:33.52ms
step:346/2155 train_time:11597ms step_avg:33.52ms
step:347/2155 train_time:11630ms step_avg:33.52ms
step:348/2155 train_time:11663ms step_avg:33.52ms
step:349/2155 train_time:11696ms step_avg:33.51ms
step:350/2155 train_time:11729ms step_avg:33.51ms
step:351/2155 train_time:11762ms step_avg:33.51ms
step:352/2155 train_time:11796ms step_avg:33.51ms
step:353/2155 train_time:11829ms step_avg:33.51ms
step:354/2155 train_time:11862ms step_avg:33.51ms
step:355/2155 train_time:11895ms step_avg:33.51ms
step:356/2155 train_time:11928ms step_avg:33.51ms
step:357/2155 train_time:11961ms step_avg:33.51ms
step:358/2155 train_time:11995ms step_avg:33.50ms
step:359/2155 train_time:12027ms step_avg:33.50ms
step:360/2155 train_time:12061ms step_avg:33.50ms
step:361/2155 train_time:12093ms step_avg:33.50ms
step:362/2155 train_time:12126ms step_avg:33.50ms
step:363/2155 train_time:12159ms step_avg:33.50ms
step:364/2155 train_time:12193ms step_avg:33.50ms
step:365/2155 train_time:12225ms step_avg:33.49ms
step:366/2155 train_time:12259ms step_avg:33.49ms
step:367/2155 train_time:12291ms step_avg:33.49ms
step:368/2155 train_time:12325ms step_avg:33.49ms
step:369/2155 train_time:12357ms step_avg:33.49ms
step:370/2155 train_time:12390ms step_avg:33.49ms
step:371/2155 train_time:12423ms step_avg:33.48ms
step:372/2155 train_time:12456ms step_avg:33.48ms
step:373/2155 train_time:12489ms step_avg:33.48ms
step:374/2155 train_time:12522ms step_avg:33.48ms
step:375/2155 train_time:12555ms step_avg:33.48ms
step:376/2155 train_time:12588ms step_avg:33.48ms
step:377/2155 train_time:12621ms step_avg:33.48ms
step:378/2155 train_time:12655ms step_avg:33.48ms
step:379/2155 train_time:12688ms step_avg:33.48ms
step:380/2155 train_time:12721ms step_avg:33.48ms
step:381/2155 train_time:12753ms step_avg:33.47ms
step:382/2155 train_time:12787ms step_avg:33.47ms
step:383/2155 train_time:12819ms step_avg:33.47ms
step:384/2155 train_time:12853ms step_avg:33.47ms
step:385/2155 train_time:12885ms step_avg:33.47ms
step:386/2155 train_time:12919ms step_avg:33.47ms
step:387/2155 train_time:12952ms step_avg:33.47ms
step:388/2155 train_time:12985ms step_avg:33.47ms
step:389/2155 train_time:13018ms step_avg:33.46ms
step:390/2155 train_time:13051ms step_avg:33.46ms
step:391/2155 train_time:13084ms step_avg:33.46ms
step:392/2155 train_time:13117ms step_avg:33.46ms
step:393/2155 train_time:13149ms step_avg:33.46ms
step:394/2155 train_time:13183ms step_avg:33.46ms
step:395/2155 train_time:13215ms step_avg:33.46ms
step:396/2155 train_time:13248ms step_avg:33.46ms
step:397/2155 train_time:13281ms step_avg:33.45ms
step:398/2155 train_time:13314ms step_avg:33.45ms
step:399/2155 train_time:13347ms step_avg:33.45ms
step:400/2155 train_time:13380ms step_avg:33.45ms
step:401/2155 train_time:13413ms step_avg:33.45ms
step:402/2155 train_time:13446ms step_avg:33.45ms
step:403/2155 train_time:13479ms step_avg:33.45ms
step:404/2155 train_time:13512ms step_avg:33.45ms
step:405/2155 train_time:13545ms step_avg:33.44ms
step:406/2155 train_time:13579ms step_avg:33.44ms
step:407/2155 train_time:13611ms step_avg:33.44ms
step:408/2155 train_time:13644ms step_avg:33.44ms
step:409/2155 train_time:13677ms step_avg:33.44ms
step:410/2155 train_time:13710ms step_avg:33.44ms
step:411/2155 train_time:13743ms step_avg:33.44ms
step:412/2155 train_time:13776ms step_avg:33.44ms
step:413/2155 train_time:13809ms step_avg:33.44ms
step:414/2155 train_time:13843ms step_avg:33.44ms
step:415/2155 train_time:13876ms step_avg:33.44ms
step:416/2155 train_time:13909ms step_avg:33.43ms
step:417/2155 train_time:13942ms step_avg:33.43ms
step:418/2155 train_time:13975ms step_avg:33.43ms
step:419/2155 train_time:14008ms step_avg:33.43ms
step:420/2155 train_time:14041ms step_avg:33.43ms
step:421/2155 train_time:14075ms step_avg:33.43ms
step:422/2155 train_time:14108ms step_avg:33.43ms
step:423/2155 train_time:14140ms step_avg:33.43ms
step:424/2155 train_time:14174ms step_avg:33.43ms
step:425/2155 train_time:14206ms step_avg:33.43ms
step:426/2155 train_time:14240ms step_avg:33.43ms
step:427/2155 train_time:14273ms step_avg:33.43ms
step:428/2155 train_time:14306ms step_avg:33.42ms
step:429/2155 train_time:14338ms step_avg:33.42ms
step:430/2155 train_time:14372ms step_avg:33.42ms
step:431/2155 train_time:14404ms step_avg:33.42ms
step:432/2155 train_time:14437ms step_avg:33.42ms
step:433/2155 train_time:14470ms step_avg:33.42ms
step:434/2155 train_time:14503ms step_avg:33.42ms
step:435/2155 train_time:14536ms step_avg:33.42ms
step:436/2155 train_time:14570ms step_avg:33.42ms
step:437/2155 train_time:14603ms step_avg:33.42ms
step:438/2155 train_time:14636ms step_avg:33.42ms
step:439/2155 train_time:14668ms step_avg:33.41ms
step:440/2155 train_time:14702ms step_avg:33.41ms
step:441/2155 train_time:14734ms step_avg:33.41ms
step:442/2155 train_time:14767ms step_avg:33.41ms
step:443/2155 train_time:14800ms step_avg:33.41ms
step:444/2155 train_time:14834ms step_avg:33.41ms
step:445/2155 train_time:14866ms step_avg:33.41ms
step:446/2155 train_time:14900ms step_avg:33.41ms
step:447/2155 train_time:14933ms step_avg:33.41ms
step:448/2155 train_time:14966ms step_avg:33.41ms
step:449/2155 train_time:14999ms step_avg:33.40ms
step:450/2155 train_time:15032ms step_avg:33.40ms
step:451/2155 train_time:15065ms step_avg:33.40ms
step:452/2155 train_time:15098ms step_avg:33.40ms
step:453/2155 train_time:15131ms step_avg:33.40ms
step:454/2155 train_time:15164ms step_avg:33.40ms
step:455/2155 train_time:15197ms step_avg:33.40ms
step:456/2155 train_time:15231ms step_avg:33.40ms
step:457/2155 train_time:15263ms step_avg:33.40ms
step:458/2155 train_time:15297ms step_avg:33.40ms
step:459/2155 train_time:15329ms step_avg:33.40ms
step:460/2155 train_time:15363ms step_avg:33.40ms
step:461/2155 train_time:15396ms step_avg:33.40ms
step:462/2155 train_time:15429ms step_avg:33.40ms
step:463/2155 train_time:15462ms step_avg:33.39ms
step:464/2155 train_time:15495ms step_avg:33.39ms
step:465/2155 train_time:15527ms step_avg:33.39ms
step:466/2155 train_time:15561ms step_avg:33.39ms
step:467/2155 train_time:15593ms step_avg:33.39ms
step:468/2155 train_time:15631ms step_avg:33.40ms
step:469/2155 train_time:15660ms step_avg:33.39ms
step:470/2155 train_time:15693ms step_avg:33.39ms
step:471/2155 train_time:15726ms step_avg:33.39ms
step:472/2155 train_time:15759ms step_avg:33.39ms
step:473/2155 train_time:15791ms step_avg:33.39ms
step:474/2155 train_time:15825ms step_avg:33.39ms
step:475/2155 train_time:15858ms step_avg:33.38ms
step:476/2155 train_time:15891ms step_avg:33.38ms
step:477/2155 train_time:15924ms step_avg:33.38ms
step:478/2155 train_time:15957ms step_avg:33.38ms
step:479/2155 train_time:15990ms step_avg:33.38ms
step:480/2155 train_time:16023ms step_avg:33.38ms
step:481/2155 train_time:16056ms step_avg:33.38ms
step:482/2155 train_time:16089ms step_avg:33.38ms
step:483/2155 train_time:16122ms step_avg:33.38ms
step:484/2155 train_time:16156ms step_avg:33.38ms
step:485/2155 train_time:16188ms step_avg:33.38ms
step:486/2155 train_time:16222ms step_avg:33.38ms
step:487/2155 train_time:16254ms step_avg:33.38ms
step:488/2155 train_time:16288ms step_avg:33.38ms
step:489/2155 train_time:16320ms step_avg:33.37ms
step:490/2155 train_time:16354ms step_avg:33.37ms
step:491/2155 train_time:16386ms step_avg:33.37ms
step:492/2155 train_time:16419ms step_avg:33.37ms
step:493/2155 train_time:16452ms step_avg:33.37ms
step:494/2155 train_time:16485ms step_avg:33.37ms
step:495/2155 train_time:16518ms step_avg:33.37ms
step:496/2155 train_time:16551ms step_avg:33.37ms
step:497/2155 train_time:16584ms step_avg:33.37ms
step:498/2155 train_time:16618ms step_avg:33.37ms
step:499/2155 train_time:16650ms step_avg:33.37ms
step:500/2155 train_time:16684ms step_avg:33.37ms
step:500/2155 val_loss:4.0259 train_time:16720ms step_avg:33.44ms
step:501/2155 train_time:16742ms step_avg:33.42ms
step:502/2155 train_time:16764ms step_avg:33.40ms
step:503/2155 train_time:16785ms step_avg:33.37ms
step:504/2155 train_time:16819ms step_avg:33.37ms
step:505/2155 train_time:16853ms step_avg:33.37ms
step:506/2155 train_time:16888ms step_avg:33.38ms
step:507/2155 train_time:16920ms step_avg:33.37ms
step:508/2155 train_time:16953ms step_avg:33.37ms
step:509/2155 train_time:16986ms step_avg:33.37ms
step:510/2155 train_time:17019ms step_avg:33.37ms
step:511/2155 train_time:17052ms step_avg:33.37ms
step:512/2155 train_time:17085ms step_avg:33.37ms
step:513/2155 train_time:17118ms step_avg:33.37ms
step:514/2155 train_time:17151ms step_avg:33.37ms
step:515/2155 train_time:17183ms step_avg:33.37ms
step:516/2155 train_time:17217ms step_avg:33.37ms
step:517/2155 train_time:17249ms step_avg:33.36ms
step:518/2155 train_time:17282ms step_avg:33.36ms
step:519/2155 train_time:17315ms step_avg:33.36ms
step:520/2155 train_time:17348ms step_avg:33.36ms
step:521/2155 train_time:17380ms step_avg:33.36ms
step:522/2155 train_time:17414ms step_avg:33.36ms
step:523/2155 train_time:17446ms step_avg:33.36ms
step:524/2155 train_time:17479ms step_avg:33.36ms
step:525/2155 train_time:17512ms step_avg:33.36ms
step:526/2155 train_time:17545ms step_avg:33.36ms
step:527/2155 train_time:17578ms step_avg:33.35ms
step:528/2155 train_time:17611ms step_avg:33.35ms
step:529/2155 train_time:17644ms step_avg:33.35ms
step:530/2155 train_time:17683ms step_avg:33.36ms
step:531/2155 train_time:17711ms step_avg:33.35ms
step:532/2155 train_time:17744ms step_avg:33.35ms
step:533/2155 train_time:17778ms step_avg:33.35ms
step:534/2155 train_time:17811ms step_avg:33.35ms
step:535/2155 train_time:17845ms step_avg:33.35ms
step:536/2155 train_time:17878ms step_avg:33.35ms
step:537/2155 train_time:17911ms step_avg:33.35ms
step:538/2155 train_time:17945ms step_avg:33.35ms
step:539/2155 train_time:17977ms step_avg:33.35ms
step:540/2155 train_time:18011ms step_avg:33.35ms
step:541/2155 train_time:18044ms step_avg:33.35ms
step:542/2155 train_time:18077ms step_avg:33.35ms
step:543/2155 train_time:18110ms step_avg:33.35ms
step:544/2155 train_time:18143ms step_avg:33.35ms
step:545/2155 train_time:18175ms step_avg:33.35ms
step:546/2155 train_time:18209ms step_avg:33.35ms
step:547/2155 train_time:18242ms step_avg:33.35ms
step:548/2155 train_time:18275ms step_avg:33.35ms
step:549/2155 train_time:18308ms step_avg:33.35ms
step:550/2155 train_time:18341ms step_avg:33.35ms
step:551/2155 train_time:18373ms step_avg:33.35ms
step:552/2155 train_time:18407ms step_avg:33.35ms
step:553/2155 train_time:18439ms step_avg:33.34ms
step:554/2155 train_time:18472ms step_avg:33.34ms
step:555/2155 train_time:18505ms step_avg:33.34ms
step:556/2155 train_time:18538ms step_avg:33.34ms
step:557/2155 train_time:18571ms step_avg:33.34ms
step:558/2155 train_time:18604ms step_avg:33.34ms
step:559/2155 train_time:18637ms step_avg:33.34ms
step:560/2155 train_time:18671ms step_avg:33.34ms
step:561/2155 train_time:18704ms step_avg:33.34ms
step:562/2155 train_time:18737ms step_avg:33.34ms
step:563/2155 train_time:18771ms step_avg:33.34ms
step:564/2155 train_time:18804ms step_avg:33.34ms
step:565/2155 train_time:18837ms step_avg:33.34ms
step:566/2155 train_time:18871ms step_avg:33.34ms
step:567/2155 train_time:18904ms step_avg:33.34ms
step:568/2155 train_time:18937ms step_avg:33.34ms
step:569/2155 train_time:18970ms step_avg:33.34ms
step:570/2155 train_time:19004ms step_avg:33.34ms
step:571/2155 train_time:19037ms step_avg:33.34ms
step:572/2155 train_time:19070ms step_avg:33.34ms
step:573/2155 train_time:19103ms step_avg:33.34ms
step:574/2155 train_time:19137ms step_avg:33.34ms
step:575/2155 train_time:19169ms step_avg:33.34ms
step:576/2155 train_time:19202ms step_avg:33.34ms
step:577/2155 train_time:19235ms step_avg:33.34ms
step:578/2155 train_time:19268ms step_avg:33.34ms
step:579/2155 train_time:19301ms step_avg:33.33ms
step:580/2155 train_time:19334ms step_avg:33.34ms
step:581/2155 train_time:19367ms step_avg:33.33ms
step:582/2155 train_time:19400ms step_avg:33.33ms
step:583/2155 train_time:19433ms step_avg:33.33ms
step:584/2155 train_time:19466ms step_avg:33.33ms
step:585/2155 train_time:19499ms step_avg:33.33ms
step:586/2155 train_time:19532ms step_avg:33.33ms
step:587/2155 train_time:19565ms step_avg:33.33ms
step:588/2155 train_time:19598ms step_avg:33.33ms
step:589/2155 train_time:19631ms step_avg:33.33ms
step:590/2155 train_time:19664ms step_avg:33.33ms
step:591/2155 train_time:19698ms step_avg:33.33ms
step:592/2155 train_time:19731ms step_avg:33.33ms
step:593/2155 train_time:19764ms step_avg:33.33ms
step:594/2155 train_time:19797ms step_avg:33.33ms
step:595/2155 train_time:19830ms step_avg:33.33ms
step:596/2155 train_time:19863ms step_avg:33.33ms
step:597/2155 train_time:19897ms step_avg:33.33ms
step:598/2155 train_time:19930ms step_avg:33.33ms
step:599/2155 train_time:19963ms step_avg:33.33ms
step:600/2155 train_time:19996ms step_avg:33.33ms
step:601/2155 train_time:20029ms step_avg:33.33ms
step:602/2155 train_time:20062ms step_avg:33.33ms
step:603/2155 train_time:20095ms step_avg:33.33ms
step:604/2155 train_time:20129ms step_avg:33.33ms
step:605/2155 train_time:20162ms step_avg:33.33ms
step:606/2155 train_time:20195ms step_avg:33.33ms
step:607/2155 train_time:20228ms step_avg:33.32ms
step:608/2155 train_time:20261ms step_avg:33.32ms
step:609/2155 train_time:20294ms step_avg:33.32ms
step:610/2155 train_time:20327ms step_avg:33.32ms
step:611/2155 train_time:20360ms step_avg:33.32ms
step:612/2155 train_time:20393ms step_avg:33.32ms
step:613/2155 train_time:20426ms step_avg:33.32ms
step:614/2155 train_time:20459ms step_avg:33.32ms
step:615/2155 train_time:20492ms step_avg:33.32ms
step:616/2155 train_time:20526ms step_avg:33.32ms
step:617/2155 train_time:20558ms step_avg:33.32ms
step:618/2155 train_time:20592ms step_avg:33.32ms
step:619/2155 train_time:20625ms step_avg:33.32ms
step:620/2155 train_time:20658ms step_avg:33.32ms
step:621/2155 train_time:20691ms step_avg:33.32ms
step:622/2155 train_time:20724ms step_avg:33.32ms
step:623/2155 train_time:20757ms step_avg:33.32ms
step:624/2155 train_time:20791ms step_avg:33.32ms
step:625/2155 train_time:20823ms step_avg:33.32ms
step:626/2155 train_time:20857ms step_avg:33.32ms
step:627/2155 train_time:20890ms step_avg:33.32ms
step:628/2155 train_time:20923ms step_avg:33.32ms
step:629/2155 train_time:20956ms step_avg:33.32ms
step:630/2155 train_time:20989ms step_avg:33.32ms
step:631/2155 train_time:21022ms step_avg:33.32ms
step:632/2155 train_time:21056ms step_avg:33.32ms
step:633/2155 train_time:21089ms step_avg:33.32ms
step:634/2155 train_time:21122ms step_avg:33.32ms
step:635/2155 train_time:21155ms step_avg:33.32ms
step:636/2155 train_time:21188ms step_avg:33.32ms
step:637/2155 train_time:21221ms step_avg:33.31ms
step:638/2155 train_time:21255ms step_avg:33.31ms
step:639/2155 train_time:21287ms step_avg:33.31ms
step:640/2155 train_time:21320ms step_avg:33.31ms
step:641/2155 train_time:21353ms step_avg:33.31ms
step:642/2155 train_time:21386ms step_avg:33.31ms
step:643/2155 train_time:21419ms step_avg:33.31ms
step:644/2155 train_time:21452ms step_avg:33.31ms
step:645/2155 train_time:21486ms step_avg:33.31ms
step:646/2155 train_time:21519ms step_avg:33.31ms
step:647/2155 train_time:21552ms step_avg:33.31ms
step:648/2155 train_time:21585ms step_avg:33.31ms
step:649/2155 train_time:21619ms step_avg:33.31ms
step:650/2155 train_time:21652ms step_avg:33.31ms
step:651/2155 train_time:21684ms step_avg:33.31ms
step:652/2155 train_time:21717ms step_avg:33.31ms
step:653/2155 train_time:21750ms step_avg:33.31ms
step:654/2155 train_time:21784ms step_avg:33.31ms
step:655/2155 train_time:21817ms step_avg:33.31ms
step:656/2155 train_time:21850ms step_avg:33.31ms
step:657/2155 train_time:21883ms step_avg:33.31ms
step:658/2155 train_time:21917ms step_avg:33.31ms
step:659/2155 train_time:21949ms step_avg:33.31ms
step:660/2155 train_time:21982ms step_avg:33.31ms
step:661/2155 train_time:22016ms step_avg:33.31ms
step:662/2155 train_time:22049ms step_avg:33.31ms
step:663/2155 train_time:22083ms step_avg:33.31ms
step:664/2155 train_time:22116ms step_avg:33.31ms
step:665/2155 train_time:22149ms step_avg:33.31ms
step:666/2155 train_time:22182ms step_avg:33.31ms
step:667/2155 train_time:22215ms step_avg:33.31ms
step:668/2155 train_time:22248ms step_avg:33.31ms
step:669/2155 train_time:22281ms step_avg:33.30ms
step:670/2155 train_time:22314ms step_avg:33.30ms
step:671/2155 train_time:22347ms step_avg:33.30ms
step:672/2155 train_time:22381ms step_avg:33.30ms
step:673/2155 train_time:22413ms step_avg:33.30ms
step:674/2155 train_time:22447ms step_avg:33.30ms
step:675/2155 train_time:22479ms step_avg:33.30ms
step:676/2155 train_time:22512ms step_avg:33.30ms
step:677/2155 train_time:22545ms step_avg:33.30ms
step:678/2155 train_time:22578ms step_avg:33.30ms
step:679/2155 train_time:22611ms step_avg:33.30ms
step:680/2155 train_time:22645ms step_avg:33.30ms
step:681/2155 train_time:22677ms step_avg:33.30ms
step:682/2155 train_time:22710ms step_avg:33.30ms
step:683/2155 train_time:22743ms step_avg:33.30ms
step:684/2155 train_time:22776ms step_avg:33.30ms
step:685/2155 train_time:22810ms step_avg:33.30ms
step:686/2155 train_time:22843ms step_avg:33.30ms
step:687/2155 train_time:22876ms step_avg:33.30ms
step:688/2155 train_time:22910ms step_avg:33.30ms
step:689/2155 train_time:22943ms step_avg:33.30ms
step:690/2155 train_time:22976ms step_avg:33.30ms
step:691/2155 train_time:23009ms step_avg:33.30ms
step:692/2155 train_time:23042ms step_avg:33.30ms
step:693/2155 train_time:23075ms step_avg:33.30ms
step:694/2155 train_time:23109ms step_avg:33.30ms
step:695/2155 train_time:23141ms step_avg:33.30ms
step:696/2155 train_time:23175ms step_avg:33.30ms
step:697/2155 train_time:23207ms step_avg:33.30ms
step:698/2155 train_time:23240ms step_avg:33.30ms
step:699/2155 train_time:23273ms step_avg:33.29ms
step:700/2155 train_time:23307ms step_avg:33.30ms
step:701/2155 train_time:23339ms step_avg:33.29ms
step:702/2155 train_time:23378ms step_avg:33.30ms
step:703/2155 train_time:23406ms step_avg:33.29ms
step:704/2155 train_time:23439ms step_avg:33.29ms
step:705/2155 train_time:23472ms step_avg:33.29ms
step:706/2155 train_time:23506ms step_avg:33.30ms
step:707/2155 train_time:23566ms step_avg:33.33ms
step:708/2155 train_time:23625ms step_avg:33.37ms
step:709/2155 train_time:23686ms step_avg:33.41ms
step:710/2155 train_time:23745ms step_avg:33.44ms
step:711/2155 train_time:23806ms step_avg:33.48ms
step:712/2155 train_time:23866ms step_avg:33.52ms
step:713/2155 train_time:23927ms step_avg:33.56ms
step:714/2155 train_time:23987ms step_avg:33.59ms
step:715/2155 train_time:24048ms step_avg:33.63ms
step:716/2155 train_time:24108ms step_avg:33.67ms
step:717/2155 train_time:24169ms step_avg:33.71ms
step:718/2155 train_time:24229ms step_avg:33.74ms
step:719/2155 train_time:24290ms step_avg:33.78ms
step:720/2155 train_time:24350ms step_avg:33.82ms
step:721/2155 train_time:24412ms step_avg:33.86ms
step:722/2155 train_time:24471ms step_avg:33.89ms
step:723/2155 train_time:24531ms step_avg:33.93ms
step:724/2155 train_time:24591ms step_avg:33.96ms
step:725/2155 train_time:24651ms step_avg:34.00ms
step:726/2155 train_time:24711ms step_avg:34.04ms
step:727/2155 train_time:24775ms step_avg:34.08ms
step:728/2155 train_time:24832ms step_avg:34.11ms
step:729/2155 train_time:24893ms step_avg:34.15ms
step:730/2155 train_time:24953ms step_avg:34.18ms
step:731/2155 train_time:25014ms step_avg:34.22ms
step:732/2155 train_time:25073ms step_avg:34.25ms
step:733/2155 train_time:25135ms step_avg:34.29ms
step:734/2155 train_time:25194ms step_avg:34.32ms
step:735/2155 train_time:25255ms step_avg:34.36ms
step:736/2155 train_time:25314ms step_avg:34.39ms
step:737/2155 train_time:25375ms step_avg:34.43ms
step:738/2155 train_time:25434ms step_avg:34.46ms
step:739/2155 train_time:25496ms step_avg:34.50ms
step:740/2155 train_time:25555ms step_avg:34.53ms
step:741/2155 train_time:25616ms step_avg:34.57ms
step:742/2155 train_time:25676ms step_avg:34.60ms
step:743/2155 train_time:25737ms step_avg:34.64ms
step:744/2155 train_time:25797ms step_avg:34.67ms
step:745/2155 train_time:25858ms step_avg:34.71ms
step:746/2155 train_time:25917ms step_avg:34.74ms
step:747/2155 train_time:25979ms step_avg:34.78ms
step:748/2155 train_time:26038ms step_avg:34.81ms
step:749/2155 train_time:26099ms step_avg:34.85ms
step:750/2155 train_time:26158ms step_avg:34.88ms
step:750/2155 val_loss:3.8690 train_time:26222ms step_avg:34.96ms
step:751/2155 train_time:26246ms step_avg:34.95ms
step:752/2155 train_time:26280ms step_avg:34.95ms
step:753/2155 train_time:26345ms step_avg:34.99ms
step:754/2155 train_time:26408ms step_avg:35.02ms
step:755/2155 train_time:26469ms step_avg:35.06ms
step:756/2155 train_time:26527ms step_avg:35.09ms
step:757/2155 train_time:26586ms step_avg:35.12ms
step:758/2155 train_time:26645ms step_avg:35.15ms
step:759/2155 train_time:26705ms step_avg:35.18ms
step:760/2155 train_time:26764ms step_avg:35.22ms
step:761/2155 train_time:26822ms step_avg:35.25ms
step:762/2155 train_time:26881ms step_avg:35.28ms
step:763/2155 train_time:26941ms step_avg:35.31ms
step:764/2155 train_time:26999ms step_avg:35.34ms
step:765/2155 train_time:27059ms step_avg:35.37ms
step:766/2155 train_time:27119ms step_avg:35.40ms
step:767/2155 train_time:27188ms step_avg:35.45ms
step:768/2155 train_time:27250ms step_avg:35.48ms
step:769/2155 train_time:27311ms step_avg:35.52ms
step:770/2155 train_time:27372ms step_avg:35.55ms
step:771/2155 train_time:27433ms step_avg:35.58ms
step:772/2155 train_time:27492ms step_avg:35.61ms
step:773/2155 train_time:27553ms step_avg:35.64ms
step:774/2155 train_time:27611ms step_avg:35.67ms
step:775/2155 train_time:27671ms step_avg:35.70ms
step:776/2155 train_time:27730ms step_avg:35.73ms
step:777/2155 train_time:27790ms step_avg:35.77ms
step:778/2155 train_time:27849ms step_avg:35.80ms
step:779/2155 train_time:27909ms step_avg:35.83ms
step:780/2155 train_time:27968ms step_avg:35.86ms
step:781/2155 train_time:28029ms step_avg:35.89ms
step:782/2155 train_time:28089ms step_avg:35.92ms
step:783/2155 train_time:28151ms step_avg:35.95ms
step:784/2155 train_time:28212ms step_avg:35.98ms
step:785/2155 train_time:28275ms step_avg:36.02ms
step:786/2155 train_time:28335ms step_avg:36.05ms
step:787/2155 train_time:28397ms step_avg:36.08ms
step:788/2155 train_time:28456ms step_avg:36.11ms
step:789/2155 train_time:28516ms step_avg:36.14ms
step:790/2155 train_time:28575ms step_avg:36.17ms
step:791/2155 train_time:28636ms step_avg:36.20ms
step:792/2155 train_time:28695ms step_avg:36.23ms
step:793/2155 train_time:28755ms step_avg:36.26ms
step:794/2155 train_time:28814ms step_avg:36.29ms
step:795/2155 train_time:28875ms step_avg:36.32ms
step:796/2155 train_time:28934ms step_avg:36.35ms
step:797/2155 train_time:28995ms step_avg:36.38ms
step:798/2155 train_time:29053ms step_avg:36.41ms
step:799/2155 train_time:29115ms step_avg:36.44ms
step:800/2155 train_time:29174ms step_avg:36.47ms
step:801/2155 train_time:29236ms step_avg:36.50ms
step:802/2155 train_time:29295ms step_avg:36.53ms
step:803/2155 train_time:29357ms step_avg:36.56ms
step:804/2155 train_time:29416ms step_avg:36.59ms
step:805/2155 train_time:29477ms step_avg:36.62ms
step:806/2155 train_time:29537ms step_avg:36.65ms
step:807/2155 train_time:29598ms step_avg:36.68ms
step:808/2155 train_time:29657ms step_avg:36.70ms
step:809/2155 train_time:29717ms step_avg:36.73ms
step:810/2155 train_time:29776ms step_avg:36.76ms
step:811/2155 train_time:29837ms step_avg:36.79ms
step:812/2155 train_time:29895ms step_avg:36.82ms
step:813/2155 train_time:29956ms step_avg:36.85ms
step:814/2155 train_time:30015ms step_avg:36.87ms
step:815/2155 train_time:30076ms step_avg:36.90ms
step:816/2155 train_time:30135ms step_avg:36.93ms
step:817/2155 train_time:30197ms step_avg:36.96ms
step:818/2155 train_time:30256ms step_avg:36.99ms
step:819/2155 train_time:30319ms step_avg:37.02ms
step:820/2155 train_time:30378ms step_avg:37.05ms
step:821/2155 train_time:30440ms step_avg:37.08ms
step:822/2155 train_time:30499ms step_avg:37.10ms
step:823/2155 train_time:30560ms step_avg:37.13ms
step:824/2155 train_time:30619ms step_avg:37.16ms
step:825/2155 train_time:30680ms step_avg:37.19ms
step:826/2155 train_time:30739ms step_avg:37.21ms
step:827/2155 train_time:30800ms step_avg:37.24ms
step:828/2155 train_time:30859ms step_avg:37.27ms
step:829/2155 train_time:30919ms step_avg:37.30ms
step:830/2155 train_time:30979ms step_avg:37.32ms
step:831/2155 train_time:31039ms step_avg:37.35ms
step:832/2155 train_time:31099ms step_avg:37.38ms
step:833/2155 train_time:31161ms step_avg:37.41ms
step:834/2155 train_time:31220ms step_avg:37.43ms
step:835/2155 train_time:31281ms step_avg:37.46ms
step:836/2155 train_time:31341ms step_avg:37.49ms
step:837/2155 train_time:31402ms step_avg:37.52ms
step:838/2155 train_time:31461ms step_avg:37.54ms
step:839/2155 train_time:31523ms step_avg:37.57ms
step:840/2155 train_time:31582ms step_avg:37.60ms
step:841/2155 train_time:31643ms step_avg:37.63ms
step:842/2155 train_time:31703ms step_avg:37.65ms
step:843/2155 train_time:31764ms step_avg:37.68ms
step:844/2155 train_time:31823ms step_avg:37.70ms
step:845/2155 train_time:31884ms step_avg:37.73ms
step:846/2155 train_time:31943ms step_avg:37.76ms
step:847/2155 train_time:32004ms step_avg:37.79ms
step:848/2155 train_time:32064ms step_avg:37.81ms
step:849/2155 train_time:32125ms step_avg:37.84ms
step:850/2155 train_time:32184ms step_avg:37.86ms
step:851/2155 train_time:32245ms step_avg:37.89ms
step:852/2155 train_time:32305ms step_avg:37.92ms
step:853/2155 train_time:32366ms step_avg:37.94ms
step:854/2155 train_time:32425ms step_avg:37.97ms
step:855/2155 train_time:32486ms step_avg:38.00ms
step:856/2155 train_time:32546ms step_avg:38.02ms
step:857/2155 train_time:32607ms step_avg:38.05ms
step:858/2155 train_time:32667ms step_avg:38.07ms
step:859/2155 train_time:32728ms step_avg:38.10ms
step:860/2155 train_time:32787ms step_avg:38.12ms
step:861/2155 train_time:32848ms step_avg:38.15ms
step:862/2155 train_time:32908ms step_avg:38.18ms
step:863/2155 train_time:32969ms step_avg:38.20ms
step:864/2155 train_time:33029ms step_avg:38.23ms
step:865/2155 train_time:33090ms step_avg:38.25ms
step:866/2155 train_time:33149ms step_avg:38.28ms
step:867/2155 train_time:33211ms step_avg:38.31ms
step:868/2155 train_time:33270ms step_avg:38.33ms
step:869/2155 train_time:33332ms step_avg:38.36ms
step:870/2155 train_time:33391ms step_avg:38.38ms
step:871/2155 train_time:33452ms step_avg:38.41ms
step:872/2155 train_time:33511ms step_avg:38.43ms
step:873/2155 train_time:33573ms step_avg:38.46ms
step:874/2155 train_time:33632ms step_avg:38.48ms
step:875/2155 train_time:33693ms step_avg:38.51ms
step:876/2155 train_time:33753ms step_avg:38.53ms
step:877/2155 train_time:33814ms step_avg:38.56ms
step:878/2155 train_time:33872ms step_avg:38.58ms
step:879/2155 train_time:33934ms step_avg:38.61ms
step:880/2155 train_time:33993ms step_avg:38.63ms
step:881/2155 train_time:34054ms step_avg:38.65ms
step:882/2155 train_time:34114ms step_avg:38.68ms
step:883/2155 train_time:34175ms step_avg:38.70ms
step:884/2155 train_time:34235ms step_avg:38.73ms
step:885/2155 train_time:34296ms step_avg:38.75ms
step:886/2155 train_time:34355ms step_avg:38.78ms
step:887/2155 train_time:34417ms step_avg:38.80ms
step:888/2155 train_time:34476ms step_avg:38.82ms
step:889/2155 train_time:34537ms step_avg:38.85ms
step:890/2155 train_time:34597ms step_avg:38.87ms
step:891/2155 train_time:34658ms step_avg:38.90ms
step:892/2155 train_time:34718ms step_avg:38.92ms
step:893/2155 train_time:34780ms step_avg:38.95ms
step:894/2155 train_time:34838ms step_avg:38.97ms
step:895/2155 train_time:34900ms step_avg:38.99ms
step:896/2155 train_time:34959ms step_avg:39.02ms
step:897/2155 train_time:35020ms step_avg:39.04ms
step:898/2155 train_time:35080ms step_avg:39.06ms
step:899/2155 train_time:35141ms step_avg:39.09ms
step:900/2155 train_time:35200ms step_avg:39.11ms
step:901/2155 train_time:35261ms step_avg:39.14ms
step:902/2155 train_time:35321ms step_avg:39.16ms
step:903/2155 train_time:35382ms step_avg:39.18ms
step:904/2155 train_time:35442ms step_avg:39.21ms
step:905/2155 train_time:35503ms step_avg:39.23ms
step:906/2155 train_time:35562ms step_avg:39.25ms
step:907/2155 train_time:35623ms step_avg:39.28ms
step:908/2155 train_time:35682ms step_avg:39.30ms
step:909/2155 train_time:35743ms step_avg:39.32ms
step:910/2155 train_time:35803ms step_avg:39.34ms
step:911/2155 train_time:35865ms step_avg:39.37ms
step:912/2155 train_time:35924ms step_avg:39.39ms
step:913/2155 train_time:35985ms step_avg:39.41ms
step:914/2155 train_time:36045ms step_avg:39.44ms
step:915/2155 train_time:36105ms step_avg:39.46ms
step:916/2155 train_time:36165ms step_avg:39.48ms
step:917/2155 train_time:36226ms step_avg:39.51ms
step:918/2155 train_time:36286ms step_avg:39.53ms
step:919/2155 train_time:36348ms step_avg:39.55ms
step:920/2155 train_time:36408ms step_avg:39.57ms
step:921/2155 train_time:36469ms step_avg:39.60ms
step:922/2155 train_time:36529ms step_avg:39.62ms
step:923/2155 train_time:36590ms step_avg:39.64ms
step:924/2155 train_time:36650ms step_avg:39.66ms
step:925/2155 train_time:36711ms step_avg:39.69ms
step:926/2155 train_time:36771ms step_avg:39.71ms
step:927/2155 train_time:36832ms step_avg:39.73ms
step:928/2155 train_time:36892ms step_avg:39.75ms
step:929/2155 train_time:36952ms step_avg:39.78ms
step:930/2155 train_time:37012ms step_avg:39.80ms
step:931/2155 train_time:37072ms step_avg:39.82ms
step:932/2155 train_time:37132ms step_avg:39.84ms
step:933/2155 train_time:37193ms step_avg:39.86ms
step:934/2155 train_time:37253ms step_avg:39.89ms
step:935/2155 train_time:37314ms step_avg:39.91ms
step:936/2155 train_time:37373ms step_avg:39.93ms
step:937/2155 train_time:37436ms step_avg:39.95ms
step:938/2155 train_time:37495ms step_avg:39.97ms
step:939/2155 train_time:37556ms step_avg:40.00ms
step:940/2155 train_time:37615ms step_avg:40.02ms
step:941/2155 train_time:37676ms step_avg:40.04ms
step:942/2155 train_time:37736ms step_avg:40.06ms
step:943/2155 train_time:37797ms step_avg:40.08ms
step:944/2155 train_time:37856ms step_avg:40.10ms
step:945/2155 train_time:37918ms step_avg:40.12ms
step:946/2155 train_time:37977ms step_avg:40.14ms
step:947/2155 train_time:38038ms step_avg:40.17ms
step:948/2155 train_time:38097ms step_avg:40.19ms
step:949/2155 train_time:38159ms step_avg:40.21ms
step:950/2155 train_time:38218ms step_avg:40.23ms
step:951/2155 train_time:38280ms step_avg:40.25ms
step:952/2155 train_time:38340ms step_avg:40.27ms
step:953/2155 train_time:38402ms step_avg:40.30ms
step:954/2155 train_time:38461ms step_avg:40.32ms
step:955/2155 train_time:38521ms step_avg:40.34ms
step:956/2155 train_time:38581ms step_avg:40.36ms
step:957/2155 train_time:38643ms step_avg:40.38ms
step:958/2155 train_time:38703ms step_avg:40.40ms
step:959/2155 train_time:38765ms step_avg:40.42ms
step:960/2155 train_time:38824ms step_avg:40.44ms
step:961/2155 train_time:38885ms step_avg:40.46ms
step:962/2155 train_time:38944ms step_avg:40.48ms
step:963/2155 train_time:39006ms step_avg:40.50ms
step:964/2155 train_time:39065ms step_avg:40.52ms
step:965/2155 train_time:39127ms step_avg:40.55ms
step:966/2155 train_time:39186ms step_avg:40.56ms
step:967/2155 train_time:39247ms step_avg:40.59ms
step:968/2155 train_time:39307ms step_avg:40.61ms
step:969/2155 train_time:39368ms step_avg:40.63ms
step:970/2155 train_time:39428ms step_avg:40.65ms
step:971/2155 train_time:39490ms step_avg:40.67ms
step:972/2155 train_time:39550ms step_avg:40.69ms
step:973/2155 train_time:39611ms step_avg:40.71ms
step:974/2155 train_time:39671ms step_avg:40.73ms
step:975/2155 train_time:39732ms step_avg:40.75ms
step:976/2155 train_time:39791ms step_avg:40.77ms
step:977/2155 train_time:39853ms step_avg:40.79ms
step:978/2155 train_time:39912ms step_avg:40.81ms
step:979/2155 train_time:39973ms step_avg:40.83ms
step:980/2155 train_time:40032ms step_avg:40.85ms
step:981/2155 train_time:40093ms step_avg:40.87ms
step:982/2155 train_time:40152ms step_avg:40.89ms
step:983/2155 train_time:40214ms step_avg:40.91ms
step:984/2155 train_time:40273ms step_avg:40.93ms
step:985/2155 train_time:40334ms step_avg:40.95ms
step:986/2155 train_time:40394ms step_avg:40.97ms
step:987/2155 train_time:40455ms step_avg:40.99ms
step:988/2155 train_time:40515ms step_avg:41.01ms
step:989/2155 train_time:40577ms step_avg:41.03ms
step:990/2155 train_time:40637ms step_avg:41.05ms
step:991/2155 train_time:40698ms step_avg:41.07ms
step:992/2155 train_time:40757ms step_avg:41.09ms
step:993/2155 train_time:40818ms step_avg:41.11ms
step:994/2155 train_time:40877ms step_avg:41.12ms
step:995/2155 train_time:40938ms step_avg:41.14ms
step:996/2155 train_time:40998ms step_avg:41.16ms
step:997/2155 train_time:41060ms step_avg:41.18ms
step:998/2155 train_time:41119ms step_avg:41.20ms
step:999/2155 train_time:41181ms step_avg:41.22ms
step:1000/2155 train_time:41240ms step_avg:41.24ms
step:1000/2155 val_loss:3.7212 train_time:41304ms step_avg:41.30ms
step:1001/2155 train_time:41327ms step_avg:41.29ms
step:1002/2155 train_time:41367ms step_avg:41.28ms
step:1003/2155 train_time:41431ms step_avg:41.31ms
step:1004/2155 train_time:41492ms step_avg:41.33ms
step:1005/2155 train_time:41553ms step_avg:41.35ms
step:1006/2155 train_time:41613ms step_avg:41.36ms
step:1007/2155 train_time:41673ms step_avg:41.38ms
step:1008/2155 train_time:41732ms step_avg:41.40ms
step:1009/2155 train_time:41793ms step_avg:41.42ms
step:1010/2155 train_time:41852ms step_avg:41.44ms
step:1011/2155 train_time:41916ms step_avg:41.46ms
step:1012/2155 train_time:41972ms step_avg:41.47ms
step:1013/2155 train_time:42032ms step_avg:41.49ms
step:1014/2155 train_time:42091ms step_avg:41.51ms
step:1015/2155 train_time:42151ms step_avg:41.53ms
step:1016/2155 train_time:42212ms step_avg:41.55ms
step:1017/2155 train_time:42276ms step_avg:41.57ms
step:1018/2155 train_time:42338ms step_avg:41.59ms
step:1019/2155 train_time:42401ms step_avg:41.61ms
step:1020/2155 train_time:42462ms step_avg:41.63ms
step:1021/2155 train_time:42523ms step_avg:41.65ms
step:1022/2155 train_time:42583ms step_avg:41.67ms
step:1023/2155 train_time:42643ms step_avg:41.68ms
step:1024/2155 train_time:42703ms step_avg:41.70ms
step:1025/2155 train_time:42764ms step_avg:41.72ms
step:1026/2155 train_time:42823ms step_avg:41.74ms
step:1027/2155 train_time:42883ms step_avg:41.76ms
step:1028/2155 train_time:42942ms step_avg:41.77ms
step:1029/2155 train_time:43003ms step_avg:41.79ms
step:1030/2155 train_time:43062ms step_avg:41.81ms
step:1031/2155 train_time:43122ms step_avg:41.83ms
step:1032/2155 train_time:43183ms step_avg:41.84ms
step:1033/2155 train_time:43244ms step_avg:41.86ms
step:1034/2155 train_time:43304ms step_avg:41.88ms
step:1035/2155 train_time:43366ms step_avg:41.90ms
step:1036/2155 train_time:43427ms step_avg:41.92ms
step:1037/2155 train_time:43489ms step_avg:41.94ms
step:1038/2155 train_time:43549ms step_avg:41.95ms
step:1039/2155 train_time:43610ms step_avg:41.97ms
step:1040/2155 train_time:43669ms step_avg:41.99ms
step:1041/2155 train_time:43730ms step_avg:42.01ms
step:1042/2155 train_time:43789ms step_avg:42.02ms
step:1043/2155 train_time:43850ms step_avg:42.04ms
step:1044/2155 train_time:43909ms step_avg:42.06ms
step:1045/2155 train_time:43969ms step_avg:42.08ms
step:1046/2155 train_time:44028ms step_avg:42.09ms
step:1047/2155 train_time:44090ms step_avg:42.11ms
step:1048/2155 train_time:44149ms step_avg:42.13ms
step:1049/2155 train_time:44210ms step_avg:42.15ms
step:1050/2155 train_time:44270ms step_avg:42.16ms
step:1051/2155 train_time:44331ms step_avg:42.18ms
step:1052/2155 train_time:44391ms step_avg:42.20ms
step:1053/2155 train_time:44453ms step_avg:42.22ms
step:1054/2155 train_time:44513ms step_avg:42.23ms
step:1055/2155 train_time:44575ms step_avg:42.25ms
step:1056/2155 train_time:44635ms step_avg:42.27ms
step:1057/2155 train_time:44695ms step_avg:42.29ms
step:1058/2155 train_time:44754ms step_avg:42.30ms
step:1059/2155 train_time:44815ms step_avg:42.32ms
step:1060/2155 train_time:44875ms step_avg:42.33ms
step:1061/2155 train_time:44935ms step_avg:42.35ms
step:1062/2155 train_time:44994ms step_avg:42.37ms
step:1063/2155 train_time:45056ms step_avg:42.39ms
step:1064/2155 train_time:45115ms step_avg:42.40ms
step:1065/2155 train_time:45177ms step_avg:42.42ms
step:1066/2155 train_time:45237ms step_avg:42.44ms
step:1067/2155 train_time:45299ms step_avg:42.45ms
step:1068/2155 train_time:45359ms step_avg:42.47ms
step:1069/2155 train_time:45420ms step_avg:42.49ms
step:1070/2155 train_time:45480ms step_avg:42.50ms
step:1071/2155 train_time:45542ms step_avg:42.52ms
step:1072/2155 train_time:45601ms step_avg:42.54ms
step:1073/2155 train_time:45662ms step_avg:42.56ms
step:1074/2155 train_time:45721ms step_avg:42.57ms
step:1075/2155 train_time:45782ms step_avg:42.59ms
step:1076/2155 train_time:45842ms step_avg:42.60ms
step:1077/2155 train_time:45902ms step_avg:42.62ms
step:1078/2155 train_time:45961ms step_avg:42.64ms
step:1079/2155 train_time:46022ms step_avg:42.65ms
step:1080/2155 train_time:46082ms step_avg:42.67ms
step:1081/2155 train_time:46143ms step_avg:42.69ms
step:1082/2155 train_time:46202ms step_avg:42.70ms
step:1083/2155 train_time:46264ms step_avg:42.72ms
step:1084/2155 train_time:46325ms step_avg:42.73ms
step:1085/2155 train_time:46386ms step_avg:42.75ms
step:1086/2155 train_time:46446ms step_avg:42.77ms
step:1087/2155 train_time:46507ms step_avg:42.78ms
step:1088/2155 train_time:46567ms step_avg:42.80ms
step:1089/2155 train_time:46628ms step_avg:42.82ms
step:1090/2155 train_time:46687ms step_avg:42.83ms
step:1091/2155 train_time:46748ms step_avg:42.85ms
step:1092/2155 train_time:46808ms step_avg:42.86ms
step:1093/2155 train_time:46870ms step_avg:42.88ms
step:1094/2155 train_time:46929ms step_avg:42.90ms
step:1095/2155 train_time:46991ms step_avg:42.91ms
step:1096/2155 train_time:47050ms step_avg:42.93ms
step:1097/2155 train_time:47111ms step_avg:42.95ms
step:1098/2155 train_time:47170ms step_avg:42.96ms
step:1099/2155 train_time:47232ms step_avg:42.98ms
step:1100/2155 train_time:47291ms step_avg:42.99ms
step:1101/2155 train_time:47353ms step_avg:43.01ms
step:1102/2155 train_time:47412ms step_avg:43.02ms
step:1103/2155 train_time:47474ms step_avg:43.04ms
step:1104/2155 train_time:47533ms step_avg:43.06ms
step:1105/2155 train_time:47595ms step_avg:43.07ms
step:1106/2155 train_time:47654ms step_avg:43.09ms
step:1107/2155 train_time:47715ms step_avg:43.10ms
step:1108/2155 train_time:47775ms step_avg:43.12ms
step:1109/2155 train_time:47837ms step_avg:43.14ms
step:1110/2155 train_time:47896ms step_avg:43.15ms
step:1111/2155 train_time:47956ms step_avg:43.17ms
step:1112/2155 train_time:48016ms step_avg:43.18ms
step:1113/2155 train_time:48077ms step_avg:43.20ms
step:1114/2155 train_time:48138ms step_avg:43.21ms
step:1115/2155 train_time:48199ms step_avg:43.23ms
step:1116/2155 train_time:48258ms step_avg:43.24ms
step:1117/2155 train_time:48320ms step_avg:43.26ms
step:1118/2155 train_time:48379ms step_avg:43.27ms
step:1119/2155 train_time:48440ms step_avg:43.29ms
step:1120/2155 train_time:48499ms step_avg:43.30ms
step:1121/2155 train_time:48560ms step_avg:43.32ms
step:1122/2155 train_time:48620ms step_avg:43.33ms
step:1123/2155 train_time:48681ms step_avg:43.35ms
step:1124/2155 train_time:48741ms step_avg:43.36ms
step:1125/2155 train_time:48802ms step_avg:43.38ms
step:1126/2155 train_time:48861ms step_avg:43.39ms
step:1127/2155 train_time:48922ms step_avg:43.41ms
step:1128/2155 train_time:48982ms step_avg:43.42ms
step:1129/2155 train_time:49043ms step_avg:43.44ms
step:1130/2155 train_time:49103ms step_avg:43.45ms
step:1131/2155 train_time:49164ms step_avg:43.47ms
step:1132/2155 train_time:49224ms step_avg:43.48ms
step:1133/2155 train_time:49285ms step_avg:43.50ms
step:1134/2155 train_time:49344ms step_avg:43.51ms
step:1135/2155 train_time:49406ms step_avg:43.53ms
step:1136/2155 train_time:49467ms step_avg:43.54ms
step:1137/2155 train_time:49528ms step_avg:43.56ms
step:1138/2155 train_time:49587ms step_avg:43.57ms
step:1139/2155 train_time:49649ms step_avg:43.59ms
step:1140/2155 train_time:49709ms step_avg:43.60ms
step:1141/2155 train_time:49770ms step_avg:43.62ms
step:1142/2155 train_time:49829ms step_avg:43.63ms
step:1143/2155 train_time:49890ms step_avg:43.65ms
step:1144/2155 train_time:49949ms step_avg:43.66ms
step:1145/2155 train_time:50011ms step_avg:43.68ms
step:1146/2155 train_time:50070ms step_avg:43.69ms
step:1147/2155 train_time:50132ms step_avg:43.71ms
step:1148/2155 train_time:50191ms step_avg:43.72ms
step:1149/2155 train_time:50252ms step_avg:43.74ms
step:1150/2155 train_time:50312ms step_avg:43.75ms
step:1151/2155 train_time:50374ms step_avg:43.77ms
step:1152/2155 train_time:50433ms step_avg:43.78ms
step:1153/2155 train_time:50495ms step_avg:43.79ms
step:1154/2155 train_time:50555ms step_avg:43.81ms
step:1155/2155 train_time:50616ms step_avg:43.82ms
step:1156/2155 train_time:50675ms step_avg:43.84ms
step:1157/2155 train_time:50737ms step_avg:43.85ms
step:1158/2155 train_time:50797ms step_avg:43.87ms
step:1159/2155 train_time:50858ms step_avg:43.88ms
step:1160/2155 train_time:50918ms step_avg:43.89ms
step:1161/2155 train_time:50979ms step_avg:43.91ms
step:1162/2155 train_time:51039ms step_avg:43.92ms
step:1163/2155 train_time:51101ms step_avg:43.94ms
step:1164/2155 train_time:51160ms step_avg:43.95ms
step:1165/2155 train_time:51222ms step_avg:43.97ms
step:1166/2155 train_time:51282ms step_avg:43.98ms
step:1167/2155 train_time:51343ms step_avg:44.00ms
step:1168/2155 train_time:51403ms step_avg:44.01ms
step:1169/2155 train_time:51465ms step_avg:44.03ms
step:1170/2155 train_time:51525ms step_avg:44.04ms
step:1171/2155 train_time:51587ms step_avg:44.05ms
step:1172/2155 train_time:51647ms step_avg:44.07ms
step:1173/2155 train_time:51708ms step_avg:44.08ms
step:1174/2155 train_time:51768ms step_avg:44.10ms
step:1175/2155 train_time:51829ms step_avg:44.11ms
step:1176/2155 train_time:51888ms step_avg:44.12ms
step:1177/2155 train_time:51950ms step_avg:44.14ms
step:1178/2155 train_time:52009ms step_avg:44.15ms
step:1179/2155 train_time:52071ms step_avg:44.17ms
step:1180/2155 train_time:52130ms step_avg:44.18ms
step:1181/2155 train_time:52192ms step_avg:44.19ms
step:1182/2155 train_time:52251ms step_avg:44.21ms
step:1183/2155 train_time:52315ms step_avg:44.22ms
step:1184/2155 train_time:52371ms step_avg:44.23ms
step:1185/2155 train_time:52433ms step_avg:44.25ms
step:1186/2155 train_time:52493ms step_avg:44.26ms
step:1187/2155 train_time:52555ms step_avg:44.28ms
step:1188/2155 train_time:52615ms step_avg:44.29ms
step:1189/2155 train_time:52676ms step_avg:44.30ms
step:1190/2155 train_time:52735ms step_avg:44.32ms
step:1191/2155 train_time:52797ms step_avg:44.33ms
step:1192/2155 train_time:52857ms step_avg:44.34ms
step:1193/2155 train_time:52918ms step_avg:44.36ms
step:1194/2155 train_time:52977ms step_avg:44.37ms
step:1195/2155 train_time:53039ms step_avg:44.38ms
step:1196/2155 train_time:53099ms step_avg:44.40ms
step:1197/2155 train_time:53160ms step_avg:44.41ms
step:1198/2155 train_time:53220ms step_avg:44.42ms
step:1199/2155 train_time:53282ms step_avg:44.44ms
step:1200/2155 train_time:53341ms step_avg:44.45ms
step:1201/2155 train_time:53403ms step_avg:44.47ms
step:1202/2155 train_time:53462ms step_avg:44.48ms
step:1203/2155 train_time:53524ms step_avg:44.49ms
step:1204/2155 train_time:53584ms step_avg:44.51ms
step:1205/2155 train_time:53646ms step_avg:44.52ms
step:1206/2155 train_time:53706ms step_avg:44.53ms
step:1207/2155 train_time:53769ms step_avg:44.55ms
step:1208/2155 train_time:53828ms step_avg:44.56ms
step:1209/2155 train_time:53889ms step_avg:44.57ms
step:1210/2155 train_time:53949ms step_avg:44.59ms
step:1211/2155 train_time:54010ms step_avg:44.60ms
step:1212/2155 train_time:54069ms step_avg:44.61ms
step:1213/2155 train_time:54131ms step_avg:44.63ms
step:1214/2155 train_time:54190ms step_avg:44.64ms
step:1215/2155 train_time:54251ms step_avg:44.65ms
step:1216/2155 train_time:54310ms step_avg:44.66ms
step:1217/2155 train_time:54372ms step_avg:44.68ms
step:1218/2155 train_time:54431ms step_avg:44.69ms
step:1219/2155 train_time:54493ms step_avg:44.70ms
step:1220/2155 train_time:54553ms step_avg:44.72ms
step:1221/2155 train_time:54615ms step_avg:44.73ms
step:1222/2155 train_time:54675ms step_avg:44.74ms
step:1223/2155 train_time:54736ms step_avg:44.76ms
step:1224/2155 train_time:54796ms step_avg:44.77ms
step:1225/2155 train_time:54857ms step_avg:44.78ms
step:1226/2155 train_time:54916ms step_avg:44.79ms
step:1227/2155 train_time:54978ms step_avg:44.81ms
step:1228/2155 train_time:55037ms step_avg:44.82ms
step:1229/2155 train_time:55098ms step_avg:44.83ms
step:1230/2155 train_time:55158ms step_avg:44.84ms
step:1231/2155 train_time:55219ms step_avg:44.86ms
step:1232/2155 train_time:55279ms step_avg:44.87ms
step:1233/2155 train_time:55340ms step_avg:44.88ms
step:1234/2155 train_time:55400ms step_avg:44.89ms
step:1235/2155 train_time:55461ms step_avg:44.91ms
step:1236/2155 train_time:55521ms step_avg:44.92ms
step:1237/2155 train_time:55582ms step_avg:44.93ms
step:1238/2155 train_time:55642ms step_avg:44.95ms
step:1239/2155 train_time:55704ms step_avg:44.96ms
step:1240/2155 train_time:55763ms step_avg:44.97ms
step:1241/2155 train_time:55825ms step_avg:44.98ms
step:1242/2155 train_time:55884ms step_avg:45.00ms
step:1243/2155 train_time:55946ms step_avg:45.01ms
step:1244/2155 train_time:56006ms step_avg:45.02ms
step:1245/2155 train_time:56067ms step_avg:45.03ms
step:1246/2155 train_time:56127ms step_avg:45.05ms
step:1247/2155 train_time:56188ms step_avg:45.06ms
step:1248/2155 train_time:56248ms step_avg:45.07ms
step:1249/2155 train_time:56309ms step_avg:45.08ms
step:1250/2155 train_time:56368ms step_avg:45.09ms
step:1250/2155 val_loss:3.5990 train_time:56432ms step_avg:45.15ms
step:1251/2155 train_time:56454ms step_avg:45.13ms
step:1252/2155 train_time:56490ms step_avg:45.12ms
step:1253/2155 train_time:56554ms step_avg:45.13ms
step:1254/2155 train_time:56616ms step_avg:45.15ms
step:1255/2155 train_time:56678ms step_avg:45.16ms
step:1256/2155 train_time:56737ms step_avg:45.17ms
step:1257/2155 train_time:56797ms step_avg:45.18ms
step:1258/2155 train_time:56857ms step_avg:45.20ms
step:1259/2155 train_time:56918ms step_avg:45.21ms
step:1260/2155 train_time:56977ms step_avg:45.22ms
step:1261/2155 train_time:57038ms step_avg:45.23ms
step:1262/2155 train_time:57097ms step_avg:45.24ms
step:1263/2155 train_time:57158ms step_avg:45.26ms
step:1264/2155 train_time:57217ms step_avg:45.27ms
step:1265/2155 train_time:57278ms step_avg:45.28ms
step:1266/2155 train_time:57337ms step_avg:45.29ms
step:1267/2155 train_time:57400ms step_avg:45.30ms
step:1268/2155 train_time:57460ms step_avg:45.32ms
step:1269/2155 train_time:57523ms step_avg:45.33ms
step:1270/2155 train_time:57584ms step_avg:45.34ms
step:1271/2155 train_time:57647ms step_avg:45.36ms
step:1272/2155 train_time:57707ms step_avg:45.37ms
step:1273/2155 train_time:57768ms step_avg:45.38ms
step:1274/2155 train_time:57827ms step_avg:45.39ms
step:1275/2155 train_time:57889ms step_avg:45.40ms
step:1276/2155 train_time:57947ms step_avg:45.41ms
step:1277/2155 train_time:58007ms step_avg:45.42ms
step:1278/2155 train_time:58066ms step_avg:45.44ms
step:1279/2155 train_time:58127ms step_avg:45.45ms
step:1280/2155 train_time:58186ms step_avg:45.46ms
step:1281/2155 train_time:58247ms step_avg:45.47ms
step:1282/2155 train_time:58306ms step_avg:45.48ms
step:1283/2155 train_time:58368ms step_avg:45.49ms
step:1284/2155 train_time:58428ms step_avg:45.50ms
step:1285/2155 train_time:58491ms step_avg:45.52ms
step:1286/2155 train_time:58553ms step_avg:45.53ms
step:1287/2155 train_time:58613ms step_avg:45.54ms
step:1288/2155 train_time:58673ms step_avg:45.55ms
step:1289/2155 train_time:58735ms step_avg:45.57ms
step:1290/2155 train_time:58795ms step_avg:45.58ms
step:1291/2155 train_time:58856ms step_avg:45.59ms
step:1292/2155 train_time:58915ms step_avg:45.60ms
step:1293/2155 train_time:58976ms step_avg:45.61ms
step:1294/2155 train_time:59035ms step_avg:45.62ms
step:1295/2155 train_time:59096ms step_avg:45.63ms
step:1296/2155 train_time:59155ms step_avg:45.64ms
step:1297/2155 train_time:59217ms step_avg:45.66ms
step:1298/2155 train_time:59277ms step_avg:45.67ms
step:1299/2155 train_time:59338ms step_avg:45.68ms
step:1300/2155 train_time:59398ms step_avg:45.69ms
step:1301/2155 train_time:59460ms step_avg:45.70ms
step:1302/2155 train_time:59520ms step_avg:45.71ms
step:1303/2155 train_time:59582ms step_avg:45.73ms
step:1304/2155 train_time:59642ms step_avg:45.74ms
step:1305/2155 train_time:59705ms step_avg:45.75ms
step:1306/2155 train_time:59765ms step_avg:45.76ms
step:1307/2155 train_time:59827ms step_avg:45.77ms
step:1308/2155 train_time:59887ms step_avg:45.78ms
step:1309/2155 train_time:59948ms step_avg:45.80ms
step:1310/2155 train_time:60007ms step_avg:45.81ms
step:1311/2155 train_time:60068ms step_avg:45.82ms
step:1312/2155 train_time:60127ms step_avg:45.83ms
step:1313/2155 train_time:60188ms step_avg:45.84ms
step:1314/2155 train_time:60247ms step_avg:45.85ms
step:1315/2155 train_time:60309ms step_avg:45.86ms
step:1316/2155 train_time:60368ms step_avg:45.87ms
step:1317/2155 train_time:60430ms step_avg:45.88ms
step:1318/2155 train_time:60489ms step_avg:45.89ms
step:1319/2155 train_time:60551ms step_avg:45.91ms
step:1320/2155 train_time:60611ms step_avg:45.92ms
step:1321/2155 train_time:60674ms step_avg:45.93ms
step:1322/2155 train_time:60734ms step_avg:45.94ms
step:1323/2155 train_time:60795ms step_avg:45.95ms
step:1324/2155 train_time:60854ms step_avg:45.96ms
step:1325/2155 train_time:60915ms step_avg:45.97ms
step:1326/2155 train_time:60975ms step_avg:45.98ms
step:1327/2155 train_time:61037ms step_avg:46.00ms
step:1328/2155 train_time:61096ms step_avg:46.01ms
step:1329/2155 train_time:61156ms step_avg:46.02ms
step:1330/2155 train_time:61216ms step_avg:46.03ms
step:1331/2155 train_time:61277ms step_avg:46.04ms
step:1332/2155 train_time:61337ms step_avg:46.05ms
step:1333/2155 train_time:61398ms step_avg:46.06ms
step:1334/2155 train_time:61459ms step_avg:46.07ms
step:1335/2155 train_time:61520ms step_avg:46.08ms
step:1336/2155 train_time:61580ms step_avg:46.09ms
step:1337/2155 train_time:61641ms step_avg:46.10ms
step:1338/2155 train_time:61701ms step_avg:46.11ms
step:1339/2155 train_time:61764ms step_avg:46.13ms
step:1340/2155 train_time:61824ms step_avg:46.14ms
step:1341/2155 train_time:61886ms step_avg:46.15ms
step:1342/2155 train_time:61946ms step_avg:46.16ms
step:1343/2155 train_time:62007ms step_avg:46.17ms
step:1344/2155 train_time:62066ms step_avg:46.18ms
step:1345/2155 train_time:62127ms step_avg:46.19ms
step:1346/2155 train_time:62187ms step_avg:46.20ms
step:1347/2155 train_time:62248ms step_avg:46.21ms
step:1348/2155 train_time:62308ms step_avg:46.22ms
step:1349/2155 train_time:62369ms step_avg:46.23ms
step:1350/2155 train_time:62427ms step_avg:46.24ms
step:1351/2155 train_time:62488ms step_avg:46.25ms
step:1352/2155 train_time:62548ms step_avg:46.26ms
step:1353/2155 train_time:62610ms step_avg:46.27ms
step:1354/2155 train_time:62670ms step_avg:46.29ms
step:1355/2155 train_time:62732ms step_avg:46.30ms
step:1356/2155 train_time:62792ms step_avg:46.31ms
step:1357/2155 train_time:62853ms step_avg:46.32ms
step:1358/2155 train_time:62913ms step_avg:46.33ms
step:1359/2155 train_time:62975ms step_avg:46.34ms
step:1360/2155 train_time:63034ms step_avg:46.35ms
step:1361/2155 train_time:63095ms step_avg:46.36ms
step:1362/2155 train_time:63154ms step_avg:46.37ms
step:1363/2155 train_time:63215ms step_avg:46.38ms
step:1364/2155 train_time:63275ms step_avg:46.39ms
step:1365/2155 train_time:63336ms step_avg:46.40ms
step:1366/2155 train_time:63396ms step_avg:46.41ms
step:1367/2155 train_time:63456ms step_avg:46.42ms
step:1368/2155 train_time:63516ms step_avg:46.43ms
step:1369/2155 train_time:63577ms step_avg:46.44ms
step:1370/2155 train_time:63638ms step_avg:46.45ms
step:1371/2155 train_time:63700ms step_avg:46.46ms
step:1372/2155 train_time:63759ms step_avg:46.47ms
step:1373/2155 train_time:63821ms step_avg:46.48ms
step:1374/2155 train_time:63880ms step_avg:46.49ms
step:1375/2155 train_time:63942ms step_avg:46.50ms
step:1376/2155 train_time:64001ms step_avg:46.51ms
step:1377/2155 train_time:64064ms step_avg:46.52ms
step:1378/2155 train_time:64124ms step_avg:46.53ms
step:1379/2155 train_time:64186ms step_avg:46.55ms
step:1380/2155 train_time:64245ms step_avg:46.55ms
step:1381/2155 train_time:64307ms step_avg:46.57ms
step:1382/2155 train_time:64367ms step_avg:46.57ms
step:1383/2155 train_time:64428ms step_avg:46.59ms
step:1384/2155 train_time:64487ms step_avg:46.59ms
step:1385/2155 train_time:64548ms step_avg:46.61ms
step:1386/2155 train_time:64608ms step_avg:46.61ms
step:1387/2155 train_time:64670ms step_avg:46.63ms
step:1388/2155 train_time:64730ms step_avg:46.64ms
step:1389/2155 train_time:64792ms step_avg:46.65ms
step:1390/2155 train_time:64852ms step_avg:46.66ms
step:1391/2155 train_time:64914ms step_avg:46.67ms
step:1392/2155 train_time:64974ms step_avg:46.68ms
step:1393/2155 train_time:65035ms step_avg:46.69ms
step:1394/2155 train_time:65095ms step_avg:46.70ms
step:1395/2155 train_time:65156ms step_avg:46.71ms
step:1396/2155 train_time:65216ms step_avg:46.72ms
step:1397/2155 train_time:65278ms step_avg:46.73ms
step:1398/2155 train_time:65337ms step_avg:46.74ms
step:1399/2155 train_time:65398ms step_avg:46.75ms
step:1400/2155 train_time:65458ms step_avg:46.76ms
step:1401/2155 train_time:65519ms step_avg:46.77ms
step:1402/2155 train_time:65579ms step_avg:46.78ms
step:1403/2155 train_time:65640ms step_avg:46.79ms
step:1404/2155 train_time:65700ms step_avg:46.80ms
step:1405/2155 train_time:65764ms step_avg:46.81ms
step:1406/2155 train_time:65823ms step_avg:46.82ms
step:1407/2155 train_time:65885ms step_avg:46.83ms
step:1408/2155 train_time:65944ms step_avg:46.84ms
step:1409/2155 train_time:66006ms step_avg:46.85ms
step:1410/2155 train_time:66065ms step_avg:46.85ms
step:1411/2155 train_time:66128ms step_avg:46.87ms
step:1412/2155 train_time:66217ms step_avg:46.90ms
step:1413/2155 train_time:66307ms step_avg:46.93ms
step:1414/2155 train_time:66394ms step_avg:46.95ms
step:1415/2155 train_time:66483ms step_avg:46.98ms
step:1416/2155 train_time:66570ms step_avg:47.01ms
step:1417/2155 train_time:66660ms step_avg:47.04ms
step:1418/2155 train_time:66749ms step_avg:47.07ms
step:1419/2155 train_time:66839ms step_avg:47.10ms
step:1420/2155 train_time:66927ms step_avg:47.13ms
step:1421/2155 train_time:67016ms step_avg:47.16ms
step:1422/2155 train_time:67103ms step_avg:47.19ms
step:1423/2155 train_time:67193ms step_avg:47.22ms
step:1424/2155 train_time:67280ms step_avg:47.25ms
step:1425/2155 train_time:67370ms step_avg:47.28ms
step:1426/2155 train_time:67458ms step_avg:47.31ms
step:1427/2155 train_time:67547ms step_avg:47.34ms
step:1428/2155 train_time:67635ms step_avg:47.36ms
step:1429/2155 train_time:67725ms step_avg:47.39ms
step:1430/2155 train_time:67813ms step_avg:47.42ms
step:1431/2155 train_time:67903ms step_avg:47.45ms
step:1432/2155 train_time:67992ms step_avg:47.48ms
step:1433/2155 train_time:68081ms step_avg:47.51ms
step:1434/2155 train_time:68170ms step_avg:47.54ms
step:1435/2155 train_time:68259ms step_avg:47.57ms
step:1436/2155 train_time:68347ms step_avg:47.60ms
step:1437/2155 train_time:68436ms step_avg:47.62ms
step:1438/2155 train_time:68524ms step_avg:47.65ms
step:1439/2155 train_time:68614ms step_avg:47.68ms
step:1440/2155 train_time:68702ms step_avg:47.71ms
step:1441/2155 train_time:68791ms step_avg:47.74ms
step:1442/2155 train_time:68880ms step_avg:47.77ms
step:1443/2155 train_time:68969ms step_avg:47.80ms
step:1444/2155 train_time:69057ms step_avg:47.82ms
step:1445/2155 train_time:69148ms step_avg:47.85ms
step:1446/2155 train_time:69236ms step_avg:47.88ms
step:1447/2155 train_time:69325ms step_avg:47.91ms
step:1448/2155 train_time:69413ms step_avg:47.94ms
step:1449/2155 train_time:69502ms step_avg:47.97ms
step:1450/2155 train_time:69589ms step_avg:47.99ms
step:1451/2155 train_time:69679ms step_avg:48.02ms
step:1452/2155 train_time:69768ms step_avg:48.05ms
step:1453/2155 train_time:69857ms step_avg:48.08ms
step:1454/2155 train_time:69945ms step_avg:48.11ms
step:1455/2155 train_time:70034ms step_avg:48.13ms
step:1456/2155 train_time:70122ms step_avg:48.16ms
step:1457/2155 train_time:70212ms step_avg:48.19ms
step:1458/2155 train_time:70299ms step_avg:48.22ms
step:1459/2155 train_time:70388ms step_avg:48.24ms
step:1460/2155 train_time:70477ms step_avg:48.27ms
step:1461/2155 train_time:70566ms step_avg:48.30ms
step:1462/2155 train_time:70654ms step_avg:48.33ms
step:1463/2155 train_time:70743ms step_avg:48.35ms
step:1464/2155 train_time:70830ms step_avg:48.38ms
step:1465/2155 train_time:70920ms step_avg:48.41ms
step:1466/2155 train_time:71008ms step_avg:48.44ms
step:1467/2155 train_time:71097ms step_avg:48.46ms
step:1468/2155 train_time:71185ms step_avg:48.49ms
step:1469/2155 train_time:71275ms step_avg:48.52ms
step:1470/2155 train_time:71362ms step_avg:48.55ms
step:1471/2155 train_time:71455ms step_avg:48.58ms
step:1472/2155 train_time:71540ms step_avg:48.60ms
step:1473/2155 train_time:71629ms step_avg:48.63ms
step:1474/2155 train_time:71717ms step_avg:48.65ms
step:1475/2155 train_time:71807ms step_avg:48.68ms
step:1476/2155 train_time:71894ms step_avg:48.71ms
step:1477/2155 train_time:71983ms step_avg:48.74ms
step:1478/2155 train_time:72071ms step_avg:48.76ms
step:1479/2155 train_time:72160ms step_avg:48.79ms
step:1480/2155 train_time:72249ms step_avg:48.82ms
step:1481/2155 train_time:72338ms step_avg:48.84ms
step:1482/2155 train_time:72426ms step_avg:48.87ms
step:1483/2155 train_time:72515ms step_avg:48.90ms
step:1484/2155 train_time:72605ms step_avg:48.93ms
step:1485/2155 train_time:72694ms step_avg:48.95ms
step:1486/2155 train_time:72782ms step_avg:48.98ms
step:1487/2155 train_time:72872ms step_avg:49.01ms
step:1488/2155 train_time:72960ms step_avg:49.03ms
step:1489/2155 train_time:73049ms step_avg:49.06ms
step:1490/2155 train_time:73137ms step_avg:49.09ms
step:1491/2155 train_time:73225ms step_avg:49.11ms
step:1492/2155 train_time:73314ms step_avg:49.14ms
step:1493/2155 train_time:73403ms step_avg:49.16ms
step:1494/2155 train_time:73491ms step_avg:49.19ms
step:1495/2155 train_time:73581ms step_avg:49.22ms
step:1496/2155 train_time:73669ms step_avg:49.24ms
step:1497/2155 train_time:73759ms step_avg:49.27ms
step:1498/2155 train_time:73846ms step_avg:49.30ms
step:1499/2155 train_time:73935ms step_avg:49.32ms
step:1500/2155 train_time:74023ms step_avg:49.35ms
step:1500/2155 val_loss:3.4949 train_time:74115ms step_avg:49.41ms
step:1501/2155 train_time:74138ms step_avg:49.39ms
step:1502/2155 train_time:74202ms step_avg:49.40ms
step:1503/2155 train_time:74296ms step_avg:49.43ms
step:1504/2155 train_time:74383ms step_avg:49.46ms
step:1505/2155 train_time:74470ms step_avg:49.48ms
step:1506/2155 train_time:74556ms step_avg:49.51ms
step:1507/2155 train_time:74643ms step_avg:49.53ms
step:1508/2155 train_time:74729ms step_avg:49.55ms
step:1509/2155 train_time:74816ms step_avg:49.58ms
step:1510/2155 train_time:74901ms step_avg:49.60ms
step:1511/2155 train_time:74993ms step_avg:49.63ms
step:1512/2155 train_time:75087ms step_avg:49.66ms
step:1513/2155 train_time:75180ms step_avg:49.69ms
step:1514/2155 train_time:75268ms step_avg:49.71ms
step:1515/2155 train_time:75356ms step_avg:49.74ms
step:1516/2155 train_time:75443ms step_avg:49.76ms
step:1517/2155 train_time:75531ms step_avg:49.79ms
step:1518/2155 train_time:75618ms step_avg:49.81ms
step:1519/2155 train_time:75705ms step_avg:49.84ms
step:1520/2155 train_time:75791ms step_avg:49.86ms
step:1521/2155 train_time:75878ms step_avg:49.89ms
step:1522/2155 train_time:75965ms step_avg:49.91ms
step:1523/2155 train_time:76056ms step_avg:49.94ms
step:1524/2155 train_time:76146ms step_avg:49.96ms
step:1525/2155 train_time:76235ms step_avg:49.99ms
step:1526/2155 train_time:76323ms step_avg:50.01ms
step:1527/2155 train_time:76411ms step_avg:50.04ms
step:1528/2155 train_time:76498ms step_avg:50.06ms
step:1529/2155 train_time:76586ms step_avg:50.09ms
step:1530/2155 train_time:76674ms step_avg:50.11ms
step:1531/2155 train_time:76761ms step_avg:50.14ms
step:1532/2155 train_time:76847ms step_avg:50.16ms
step:1533/2155 train_time:76936ms step_avg:50.19ms
step:1534/2155 train_time:77024ms step_avg:50.21ms
step:1535/2155 train_time:77114ms step_avg:50.24ms
step:1536/2155 train_time:77203ms step_avg:50.26ms
step:1537/2155 train_time:77293ms step_avg:50.29ms
step:1538/2155 train_time:77381ms step_avg:50.31ms
step:1539/2155 train_time:77470ms step_avg:50.34ms
step:1540/2155 train_time:77557ms step_avg:50.36ms
step:1541/2155 train_time:77645ms step_avg:50.39ms
step:1542/2155 train_time:77731ms step_avg:50.41ms
step:1543/2155 train_time:77819ms step_avg:50.43ms
step:1544/2155 train_time:77907ms step_avg:50.46ms
step:1545/2155 train_time:77997ms step_avg:50.48ms
step:1546/2155 train_time:78084ms step_avg:50.51ms
step:1547/2155 train_time:78174ms step_avg:50.53ms
step:1548/2155 train_time:78262ms step_avg:50.56ms
step:1549/2155 train_time:78350ms step_avg:50.58ms
step:1550/2155 train_time:78437ms step_avg:50.60ms
step:1551/2155 train_time:78526ms step_avg:50.63ms
step:1552/2155 train_time:78613ms step_avg:50.65ms
step:1553/2155 train_time:78701ms step_avg:50.68ms
step:1554/2155 train_time:78788ms step_avg:50.70ms
step:1555/2155 train_time:78876ms step_avg:50.72ms
step:1556/2155 train_time:78963ms step_avg:50.75ms
step:1557/2155 train_time:79053ms step_avg:50.77ms
step:1558/2155 train_time:79141ms step_avg:50.80ms
step:1559/2155 train_time:79230ms step_avg:50.82ms
step:1560/2155 train_time:79318ms step_avg:50.84ms
step:1561/2155 train_time:79407ms step_avg:50.87ms
step:1562/2155 train_time:79494ms step_avg:50.89ms
step:1563/2155 train_time:79582ms step_avg:50.92ms
step:1564/2155 train_time:79670ms step_avg:50.94ms
step:1565/2155 train_time:79761ms step_avg:50.97ms
step:1566/2155 train_time:79845ms step_avg:50.99ms
step:1567/2155 train_time:79934ms step_avg:51.01ms
step:1568/2155 train_time:80021ms step_avg:51.03ms
step:1569/2155 train_time:80110ms step_avg:51.06ms
step:1570/2155 train_time:80198ms step_avg:51.08ms
step:1571/2155 train_time:80287ms step_avg:51.11ms
step:1572/2155 train_time:80375ms step_avg:51.13ms
step:1573/2155 train_time:80463ms step_avg:51.15ms
step:1574/2155 train_time:80550ms step_avg:51.18ms
step:1575/2155 train_time:80639ms step_avg:51.20ms
step:1576/2155 train_time:80726ms step_avg:51.22ms
step:1577/2155 train_time:80813ms step_avg:51.25ms
step:1578/2155 train_time:80900ms step_avg:51.27ms
step:1579/2155 train_time:80989ms step_avg:51.29ms
step:1580/2155 train_time:81077ms step_avg:51.31ms
step:1581/2155 train_time:81168ms step_avg:51.34ms
step:1582/2155 train_time:81253ms step_avg:51.36ms
step:1583/2155 train_time:81342ms step_avg:51.38ms
step:1584/2155 train_time:81430ms step_avg:51.41ms
step:1585/2155 train_time:81519ms step_avg:51.43ms
step:1586/2155 train_time:81607ms step_avg:51.45ms
step:1587/2155 train_time:81694ms step_avg:51.48ms
step:1588/2155 train_time:81782ms step_avg:51.50ms
step:1589/2155 train_time:81870ms step_avg:51.52ms
step:1590/2155 train_time:81958ms step_avg:51.55ms
step:1591/2155 train_time:82047ms step_avg:51.57ms
step:1592/2155 train_time:82134ms step_avg:51.59ms
step:1593/2155 train_time:82223ms step_avg:51.62ms
step:1594/2155 train_time:82311ms step_avg:51.64ms
step:1595/2155 train_time:82400ms step_avg:51.66ms
step:1596/2155 train_time:82487ms step_avg:51.68ms
step:1597/2155 train_time:82576ms step_avg:51.71ms
step:1598/2155 train_time:82662ms step_avg:51.73ms
step:1599/2155 train_time:82751ms step_avg:51.75ms
step:1600/2155 train_time:82839ms step_avg:51.77ms
step:1601/2155 train_time:82929ms step_avg:51.80ms
step:1602/2155 train_time:83016ms step_avg:51.82ms
step:1603/2155 train_time:83104ms step_avg:51.84ms
step:1604/2155 train_time:83192ms step_avg:51.87ms
step:1605/2155 train_time:83281ms step_avg:51.89ms
step:1606/2155 train_time:83368ms step_avg:51.91ms
step:1607/2155 train_time:83457ms step_avg:51.93ms
step:1608/2155 train_time:83545ms step_avg:51.96ms
step:1609/2155 train_time:83633ms step_avg:51.98ms
step:1610/2155 train_time:83720ms step_avg:52.00ms
step:1611/2155 train_time:83810ms step_avg:52.02ms
step:1612/2155 train_time:83897ms step_avg:52.05ms
step:1613/2155 train_time:83986ms step_avg:52.07ms
step:1614/2155 train_time:84076ms step_avg:52.09ms
step:1615/2155 train_time:84161ms step_avg:52.11ms
step:1616/2155 train_time:84248ms step_avg:52.13ms
step:1617/2155 train_time:84337ms step_avg:52.16ms
step:1618/2155 train_time:84424ms step_avg:52.18ms
step:1619/2155 train_time:84512ms step_avg:52.20ms
step:1620/2155 train_time:84599ms step_avg:52.22ms
step:1621/2155 train_time:84688ms step_avg:52.24ms
step:1622/2155 train_time:84776ms step_avg:52.27ms
step:1623/2155 train_time:84864ms step_avg:52.29ms
step:1624/2155 train_time:84951ms step_avg:52.31ms
step:1625/2155 train_time:85040ms step_avg:52.33ms
step:1626/2155 train_time:85128ms step_avg:52.35ms
step:1627/2155 train_time:85217ms step_avg:52.38ms
step:1628/2155 train_time:85304ms step_avg:52.40ms
step:1629/2155 train_time:85393ms step_avg:52.42ms
step:1630/2155 train_time:85480ms step_avg:52.44ms
step:1631/2155 train_time:85569ms step_avg:52.46ms
step:1632/2155 train_time:85656ms step_avg:52.49ms
step:1633/2155 train_time:85745ms step_avg:52.51ms
step:1634/2155 train_time:85833ms step_avg:52.53ms
step:1635/2155 train_time:85921ms step_avg:52.55ms
step:1636/2155 train_time:86009ms step_avg:52.57ms
step:1637/2155 train_time:86098ms step_avg:52.59ms
step:1638/2155 train_time:86185ms step_avg:52.62ms
step:1639/2155 train_time:86273ms step_avg:52.64ms
step:1640/2155 train_time:86360ms step_avg:52.66ms
step:1641/2155 train_time:86449ms step_avg:52.68ms
step:1642/2155 train_time:86536ms step_avg:52.70ms
step:1643/2155 train_time:86625ms step_avg:52.72ms
step:1644/2155 train_time:86713ms step_avg:52.74ms
step:1645/2155 train_time:86801ms step_avg:52.77ms
step:1646/2155 train_time:86890ms step_avg:52.79ms
step:1647/2155 train_time:86978ms step_avg:52.81ms
step:1648/2155 train_time:87066ms step_avg:52.83ms
step:1649/2155 train_time:87155ms step_avg:52.85ms
step:1650/2155 train_time:87242ms step_avg:52.87ms
step:1651/2155 train_time:87331ms step_avg:52.90ms
step:1652/2155 train_time:87419ms step_avg:52.92ms
step:1653/2155 train_time:87507ms step_avg:52.94ms
step:1654/2155 train_time:87594ms step_avg:52.96ms
step:1655/2155 train_time:87683ms step_avg:52.98ms
step:1656/2155 train_time:87771ms step_avg:53.00ms
step:1657/2155 train_time:87860ms step_avg:53.02ms
step:1658/2155 train_time:87948ms step_avg:53.04ms
step:1659/2155 train_time:88038ms step_avg:53.07ms
step:1660/2155 train_time:88125ms step_avg:53.09ms
step:1661/2155 train_time:88214ms step_avg:53.11ms
step:1662/2155 train_time:88302ms step_avg:53.13ms
step:1663/2155 train_time:88391ms step_avg:53.15ms
step:1664/2155 train_time:88479ms step_avg:53.17ms
step:1665/2155 train_time:88570ms step_avg:53.20ms
step:1666/2155 train_time:88656ms step_avg:53.21ms
step:1667/2155 train_time:88744ms step_avg:53.24ms
step:1668/2155 train_time:88832ms step_avg:53.26ms
step:1669/2155 train_time:88921ms step_avg:53.28ms
step:1670/2155 train_time:89010ms step_avg:53.30ms
step:1671/2155 train_time:89099ms step_avg:53.32ms
step:1672/2155 train_time:89186ms step_avg:53.34ms
step:1673/2155 train_time:89275ms step_avg:53.36ms
step:1674/2155 train_time:89363ms step_avg:53.38ms
step:1675/2155 train_time:89452ms step_avg:53.40ms
step:1676/2155 train_time:89540ms step_avg:53.43ms
step:1677/2155 train_time:89629ms step_avg:53.45ms
step:1678/2155 train_time:89717ms step_avg:53.47ms
step:1679/2155 train_time:89805ms step_avg:53.49ms
step:1680/2155 train_time:89892ms step_avg:53.51ms
step:1681/2155 train_time:89982ms step_avg:53.53ms
step:1682/2155 train_time:90072ms step_avg:53.55ms
step:1683/2155 train_time:90158ms step_avg:53.57ms
step:1684/2155 train_time:90245ms step_avg:53.59ms
step:1685/2155 train_time:90335ms step_avg:53.61ms
step:1686/2155 train_time:90423ms step_avg:53.63ms
step:1687/2155 train_time:90513ms step_avg:53.65ms
step:1688/2155 train_time:90600ms step_avg:53.67ms
step:1689/2155 train_time:90689ms step_avg:53.69ms
step:1690/2155 train_time:90777ms step_avg:53.71ms
step:1691/2155 train_time:90865ms step_avg:53.73ms
step:1692/2155 train_time:90953ms step_avg:53.75ms
step:1693/2155 train_time:91042ms step_avg:53.78ms
step:1694/2155 train_time:91130ms step_avg:53.80ms
step:1695/2155 train_time:91220ms step_avg:53.82ms
step:1696/2155 train_time:91308ms step_avg:53.84ms
step:1697/2155 train_time:91397ms step_avg:53.86ms
step:1698/2155 train_time:91485ms step_avg:53.88ms
step:1699/2155 train_time:91574ms step_avg:53.90ms
step:1700/2155 train_time:91661ms step_avg:53.92ms
step:1701/2155 train_time:91750ms step_avg:53.94ms
step:1702/2155 train_time:91838ms step_avg:53.96ms
step:1703/2155 train_time:91927ms step_avg:53.98ms
step:1704/2155 train_time:92014ms step_avg:54.00ms
step:1705/2155 train_time:92102ms step_avg:54.02ms
step:1706/2155 train_time:92189ms step_avg:54.04ms
step:1707/2155 train_time:92279ms step_avg:54.06ms
step:1708/2155 train_time:92368ms step_avg:54.08ms
step:1709/2155 train_time:92456ms step_avg:54.10ms
step:1710/2155 train_time:92544ms step_avg:54.12ms
step:1711/2155 train_time:92632ms step_avg:54.14ms
step:1712/2155 train_time:92720ms step_avg:54.16ms
step:1713/2155 train_time:92809ms step_avg:54.18ms
step:1714/2155 train_time:92896ms step_avg:54.20ms
step:1715/2155 train_time:92985ms step_avg:54.22ms
step:1716/2155 train_time:93074ms step_avg:54.24ms
step:1717/2155 train_time:93161ms step_avg:54.26ms
step:1718/2155 train_time:93250ms step_avg:54.28ms
step:1719/2155 train_time:93339ms step_avg:54.30ms
step:1720/2155 train_time:93426ms step_avg:54.32ms
step:1721/2155 train_time:93516ms step_avg:54.34ms
step:1722/2155 train_time:93604ms step_avg:54.36ms
step:1723/2155 train_time:93692ms step_avg:54.38ms
step:1724/2155 train_time:93780ms step_avg:54.40ms
step:1725/2155 train_time:93869ms step_avg:54.42ms
step:1726/2155 train_time:93956ms step_avg:54.44ms
step:1727/2155 train_time:94045ms step_avg:54.46ms
step:1728/2155 train_time:94132ms step_avg:54.47ms
step:1729/2155 train_time:94221ms step_avg:54.49ms
step:1730/2155 train_time:94309ms step_avg:54.51ms
step:1731/2155 train_time:94398ms step_avg:54.53ms
step:1732/2155 train_time:94486ms step_avg:54.55ms
step:1733/2155 train_time:94574ms step_avg:54.57ms
step:1734/2155 train_time:94661ms step_avg:54.59ms
step:1735/2155 train_time:94750ms step_avg:54.61ms
step:1736/2155 train_time:94837ms step_avg:54.63ms
step:1737/2155 train_time:94926ms step_avg:54.65ms
step:1738/2155 train_time:95013ms step_avg:54.67ms
step:1739/2155 train_time:95101ms step_avg:54.69ms
step:1740/2155 train_time:95189ms step_avg:54.71ms
step:1741/2155 train_time:95279ms step_avg:54.73ms
step:1742/2155 train_time:95367ms step_avg:54.75ms
step:1743/2155 train_time:95456ms step_avg:54.77ms
step:1744/2155 train_time:95543ms step_avg:54.78ms
step:1745/2155 train_time:95633ms step_avg:54.80ms
step:1746/2155 train_time:95720ms step_avg:54.82ms
step:1747/2155 train_time:95810ms step_avg:54.84ms
step:1748/2155 train_time:95897ms step_avg:54.86ms
step:1749/2155 train_time:95986ms step_avg:54.88ms
step:1750/2155 train_time:96074ms step_avg:54.90ms
step:1750/2155 val_loss:3.3944 train_time:96165ms step_avg:54.95ms
step:1751/2155 train_time:96188ms step_avg:54.93ms
step:1752/2155 train_time:96256ms step_avg:54.94ms
step:1753/2155 train_time:96349ms step_avg:54.96ms
step:1754/2155 train_time:96437ms step_avg:54.98ms
step:1755/2155 train_time:96526ms step_avg:55.00ms
step:1756/2155 train_time:96612ms step_avg:55.02ms
step:1757/2155 train_time:96700ms step_avg:55.04ms
step:1758/2155 train_time:96786ms step_avg:55.05ms
step:1759/2155 train_time:96874ms step_avg:55.07ms
step:1760/2155 train_time:96961ms step_avg:55.09ms
step:1761/2155 train_time:97049ms step_avg:55.11ms
step:1762/2155 train_time:97138ms step_avg:55.13ms
step:1763/2155 train_time:97232ms step_avg:55.15ms
step:1764/2155 train_time:97322ms step_avg:55.17ms
step:1765/2155 train_time:97412ms step_avg:55.19ms
step:1766/2155 train_time:97500ms step_avg:55.21ms
step:1767/2155 train_time:97589ms step_avg:55.23ms
step:1768/2155 train_time:97676ms step_avg:55.25ms
step:1769/2155 train_time:97765ms step_avg:55.27ms
step:1770/2155 train_time:97851ms step_avg:55.28ms
step:1771/2155 train_time:97939ms step_avg:55.30ms
step:1772/2155 train_time:98027ms step_avg:55.32ms
step:1773/2155 train_time:98119ms step_avg:55.34ms
step:1774/2155 train_time:98208ms step_avg:55.36ms
step:1775/2155 train_time:98298ms step_avg:55.38ms
step:1776/2155 train_time:98387ms step_avg:55.40ms
step:1777/2155 train_time:98476ms step_avg:55.42ms
step:1778/2155 train_time:98564ms step_avg:55.44ms
step:1779/2155 train_time:98652ms step_avg:55.45ms
step:1780/2155 train_time:98739ms step_avg:55.47ms
step:1781/2155 train_time:98827ms step_avg:55.49ms
step:1782/2155 train_time:98914ms step_avg:55.51ms
step:1783/2155 train_time:99002ms step_avg:55.53ms
step:1784/2155 train_time:99090ms step_avg:55.54ms
step:1785/2155 train_time:99183ms step_avg:55.56ms
step:1786/2155 train_time:99271ms step_avg:55.58ms
step:1787/2155 train_time:99360ms step_avg:55.60ms
step:1788/2155 train_time:99449ms step_avg:55.62ms
step:1789/2155 train_time:99539ms step_avg:55.64ms
step:1790/2155 train_time:99626ms step_avg:55.66ms
step:1791/2155 train_time:99715ms step_avg:55.68ms
step:1792/2155 train_time:99802ms step_avg:55.69ms
step:1793/2155 train_time:99889ms step_avg:55.71ms
step:1794/2155 train_time:99977ms step_avg:55.73ms
step:1795/2155 train_time:100065ms step_avg:55.75ms
step:1796/2155 train_time:100154ms step_avg:55.76ms
step:1797/2155 train_time:100244ms step_avg:55.78ms
step:1798/2155 train_time:100332ms step_avg:55.80ms
step:1799/2155 train_time:100421ms step_avg:55.82ms
step:1800/2155 train_time:100509ms step_avg:55.84ms
step:1801/2155 train_time:100598ms step_avg:55.86ms
step:1802/2155 train_time:100685ms step_avg:55.87ms
step:1803/2155 train_time:100773ms step_avg:55.89ms
step:1804/2155 train_time:100860ms step_avg:55.91ms
step:1805/2155 train_time:100948ms step_avg:55.93ms
step:1806/2155 train_time:101036ms step_avg:55.94ms
step:1807/2155 train_time:101125ms step_avg:55.96ms
step:1808/2155 train_time:101213ms step_avg:55.98ms
step:1809/2155 train_time:101302ms step_avg:56.00ms
step:1810/2155 train_time:101391ms step_avg:56.02ms
step:1811/2155 train_time:101482ms step_avg:56.04ms
step:1812/2155 train_time:101570ms step_avg:56.05ms
step:1813/2155 train_time:101659ms step_avg:56.07ms
step:1814/2155 train_time:101746ms step_avg:56.09ms
step:1815/2155 train_time:101835ms step_avg:56.11ms
step:1816/2155 train_time:101922ms step_avg:56.12ms
step:1817/2155 train_time:102011ms step_avg:56.14ms
step:1818/2155 train_time:102098ms step_avg:56.16ms
step:1819/2155 train_time:102188ms step_avg:56.18ms
step:1820/2155 train_time:102275ms step_avg:56.19ms
step:1821/2155 train_time:102365ms step_avg:56.21ms
step:1822/2155 train_time:102452ms step_avg:56.23ms
step:1823/2155 train_time:102542ms step_avg:56.25ms
step:1824/2155 train_time:102629ms step_avg:56.27ms
step:1825/2155 train_time:102718ms step_avg:56.28ms
step:1826/2155 train_time:102806ms step_avg:56.30ms
step:1827/2155 train_time:102894ms step_avg:56.32ms
step:1828/2155 train_time:102981ms step_avg:56.34ms
step:1829/2155 train_time:103070ms step_avg:56.35ms
step:1830/2155 train_time:103158ms step_avg:56.37ms
step:1831/2155 train_time:103247ms step_avg:56.39ms
step:1832/2155 train_time:103335ms step_avg:56.41ms
step:1833/2155 train_time:103425ms step_avg:56.42ms
step:1834/2155 train_time:103511ms step_avg:56.44ms
step:1835/2155 train_time:103600ms step_avg:56.46ms
step:1836/2155 train_time:103688ms step_avg:56.48ms
step:1837/2155 train_time:103778ms step_avg:56.49ms
step:1838/2155 train_time:103865ms step_avg:56.51ms
step:1839/2155 train_time:103954ms step_avg:56.53ms
step:1840/2155 train_time:104041ms step_avg:56.54ms
step:1841/2155 train_time:104130ms step_avg:56.56ms
step:1842/2155 train_time:104218ms step_avg:56.58ms
step:1843/2155 train_time:104308ms step_avg:56.60ms
step:1844/2155 train_time:104395ms step_avg:56.61ms
step:1845/2155 train_time:104484ms step_avg:56.63ms
step:1846/2155 train_time:104573ms step_avg:56.65ms
step:1847/2155 train_time:104662ms step_avg:56.67ms
step:1848/2155 train_time:104750ms step_avg:56.68ms
step:1849/2155 train_time:104839ms step_avg:56.70ms
step:1850/2155 train_time:104927ms step_avg:56.72ms
step:1851/2155 train_time:105016ms step_avg:56.73ms
step:1852/2155 train_time:105103ms step_avg:56.75ms
step:1853/2155 train_time:105192ms step_avg:56.77ms
step:1854/2155 train_time:105280ms step_avg:56.79ms
step:1855/2155 train_time:105370ms step_avg:56.80ms
step:1856/2155 train_time:105458ms step_avg:56.82ms
step:1857/2155 train_time:105546ms step_avg:56.84ms
step:1858/2155 train_time:105635ms step_avg:56.85ms
step:1859/2155 train_time:105723ms step_avg:56.87ms
step:1860/2155 train_time:105810ms step_avg:56.89ms
step:1861/2155 train_time:105899ms step_avg:56.90ms
step:1862/2155 train_time:105987ms step_avg:56.92ms
step:1863/2155 train_time:106076ms step_avg:56.94ms
step:1864/2155 train_time:106164ms step_avg:56.95ms
step:1865/2155 train_time:106253ms step_avg:56.97ms
step:1866/2155 train_time:106341ms step_avg:56.99ms
step:1867/2155 train_time:106429ms step_avg:57.01ms
step:1868/2155 train_time:106518ms step_avg:57.02ms
step:1869/2155 train_time:106608ms step_avg:57.04ms
step:1870/2155 train_time:106694ms step_avg:57.06ms
step:1871/2155 train_time:106784ms step_avg:57.07ms
step:1872/2155 train_time:106871ms step_avg:57.09ms
step:1873/2155 train_time:106960ms step_avg:57.11ms
step:1874/2155 train_time:107048ms step_avg:57.12ms
step:1875/2155 train_time:107138ms step_avg:57.14ms
step:1876/2155 train_time:107226ms step_avg:57.16ms
step:1877/2155 train_time:107315ms step_avg:57.17ms
step:1878/2155 train_time:107402ms step_avg:57.19ms
step:1879/2155 train_time:107492ms step_avg:57.21ms
step:1880/2155 train_time:107579ms step_avg:57.22ms
step:1881/2155 train_time:107668ms step_avg:57.24ms
step:1882/2155 train_time:107756ms step_avg:57.26ms
step:1883/2155 train_time:107845ms step_avg:57.27ms
step:1884/2155 train_time:107932ms step_avg:57.29ms
step:1885/2155 train_time:108021ms step_avg:57.31ms
step:1886/2155 train_time:108109ms step_avg:57.32ms
step:1887/2155 train_time:108199ms step_avg:57.34ms
step:1888/2155 train_time:108286ms step_avg:57.36ms
step:1889/2155 train_time:108376ms step_avg:57.37ms
step:1890/2155 train_time:108464ms step_avg:57.39ms
step:1891/2155 train_time:108553ms step_avg:57.41ms
step:1892/2155 train_time:108641ms step_avg:57.42ms
step:1893/2155 train_time:108730ms step_avg:57.44ms
step:1894/2155 train_time:108818ms step_avg:57.45ms
step:1895/2155 train_time:108907ms step_avg:57.47ms
step:1896/2155 train_time:108995ms step_avg:57.49ms
step:1897/2155 train_time:109085ms step_avg:57.50ms
step:1898/2155 train_time:109173ms step_avg:57.52ms
step:1899/2155 train_time:109262ms step_avg:57.54ms
step:1900/2155 train_time:109349ms step_avg:57.55ms
step:1901/2155 train_time:109438ms step_avg:57.57ms
step:1902/2155 train_time:109525ms step_avg:57.58ms
step:1903/2155 train_time:109614ms step_avg:57.60ms
step:1904/2155 train_time:109701ms step_avg:57.62ms
step:1905/2155 train_time:109791ms step_avg:57.63ms
step:1906/2155 train_time:109879ms step_avg:57.65ms
step:1907/2155 train_time:109968ms step_avg:57.67ms
step:1908/2155 train_time:110055ms step_avg:57.68ms
step:1909/2155 train_time:110144ms step_avg:57.70ms
step:1910/2155 train_time:110232ms step_avg:57.71ms
step:1911/2155 train_time:110320ms step_avg:57.73ms
step:1912/2155 train_time:110408ms step_avg:57.74ms
step:1913/2155 train_time:110497ms step_avg:57.76ms
step:1914/2155 train_time:110584ms step_avg:57.78ms
step:1915/2155 train_time:110673ms step_avg:57.79ms
step:1916/2155 train_time:110760ms step_avg:57.81ms
step:1917/2155 train_time:110850ms step_avg:57.82ms
step:1918/2155 train_time:110938ms step_avg:57.84ms
step:1919/2155 train_time:111028ms step_avg:57.86ms
step:1920/2155 train_time:111115ms step_avg:57.87ms
step:1921/2155 train_time:111204ms step_avg:57.89ms
step:1922/2155 train_time:111291ms step_avg:57.90ms
step:1923/2155 train_time:111380ms step_avg:57.92ms
step:1924/2155 train_time:111468ms step_avg:57.94ms
step:1925/2155 train_time:111557ms step_avg:57.95ms
step:1926/2155 train_time:111644ms step_avg:57.97ms
step:1927/2155 train_time:111733ms step_avg:57.98ms
step:1928/2155 train_time:111820ms step_avg:58.00ms
step:1929/2155 train_time:111911ms step_avg:58.01ms
step:1930/2155 train_time:111998ms step_avg:58.03ms
step:1931/2155 train_time:112087ms step_avg:58.05ms
step:1932/2155 train_time:112175ms step_avg:58.06ms
step:1933/2155 train_time:112264ms step_avg:58.08ms
step:1934/2155 train_time:112352ms step_avg:58.09ms
step:1935/2155 train_time:112442ms step_avg:58.11ms
step:1936/2155 train_time:112528ms step_avg:58.12ms
step:1937/2155 train_time:112617ms step_avg:58.14ms
step:1938/2155 train_time:112705ms step_avg:58.16ms
step:1939/2155 train_time:112794ms step_avg:58.17ms
step:1940/2155 train_time:112883ms step_avg:58.19ms
step:1941/2155 train_time:112973ms step_avg:58.20ms
step:1942/2155 train_time:113060ms step_avg:58.22ms
step:1943/2155 train_time:113150ms step_avg:58.23ms
step:1944/2155 train_time:113237ms step_avg:58.25ms
step:1945/2155 train_time:113326ms step_avg:58.27ms
step:1946/2155 train_time:113413ms step_avg:58.28ms
step:1947/2155 train_time:113502ms step_avg:58.30ms
step:1948/2155 train_time:113589ms step_avg:58.31ms
step:1949/2155 train_time:113678ms step_avg:58.33ms
step:1950/2155 train_time:113766ms step_avg:58.34ms
step:1951/2155 train_time:113856ms step_avg:58.36ms
step:1952/2155 train_time:113943ms step_avg:58.37ms
step:1953/2155 train_time:114032ms step_avg:58.39ms
step:1954/2155 train_time:114119ms step_avg:58.40ms
step:1955/2155 train_time:114209ms step_avg:58.42ms
step:1956/2155 train_time:114296ms step_avg:58.43ms
step:1957/2155 train_time:114385ms step_avg:58.45ms
step:1958/2155 train_time:114473ms step_avg:58.46ms
step:1959/2155 train_time:114562ms step_avg:58.48ms
step:1960/2155 train_time:114649ms step_avg:58.49ms
step:1961/2155 train_time:114739ms step_avg:58.51ms
step:1962/2155 train_time:114826ms step_avg:58.53ms
step:1963/2155 train_time:114916ms step_avg:58.54ms
step:1964/2155 train_time:115003ms step_avg:58.56ms
step:1965/2155 train_time:115093ms step_avg:58.57ms
step:1966/2155 train_time:115181ms step_avg:58.59ms
step:1967/2155 train_time:115271ms step_avg:58.60ms
step:1968/2155 train_time:115359ms step_avg:58.62ms
step:1969/2155 train_time:115448ms step_avg:58.63ms
step:1970/2155 train_time:115536ms step_avg:58.65ms
step:1971/2155 train_time:115625ms step_avg:58.66ms
step:1972/2155 train_time:115712ms step_avg:58.68ms
step:1973/2155 train_time:115801ms step_avg:58.69ms
step:1974/2155 train_time:115889ms step_avg:58.71ms
step:1975/2155 train_time:115978ms step_avg:58.72ms
step:1976/2155 train_time:116066ms step_avg:58.74ms
step:1977/2155 train_time:116154ms step_avg:58.75ms
step:1978/2155 train_time:116242ms step_avg:58.77ms
step:1979/2155 train_time:116331ms step_avg:58.78ms
step:1980/2155 train_time:116418ms step_avg:58.80ms
step:1981/2155 train_time:116508ms step_avg:58.81ms
step:1982/2155 train_time:116596ms step_avg:58.83ms
step:1983/2155 train_time:116685ms step_avg:58.84ms
step:1984/2155 train_time:116772ms step_avg:58.86ms
step:1985/2155 train_time:116861ms step_avg:58.87ms
step:1986/2155 train_time:116949ms step_avg:58.89ms
step:1987/2155 train_time:117038ms step_avg:58.90ms
step:1988/2155 train_time:117126ms step_avg:58.92ms
step:1989/2155 train_time:117214ms step_avg:58.93ms
step:1990/2155 train_time:117301ms step_avg:58.95ms
step:1991/2155 train_time:117391ms step_avg:58.96ms
step:1992/2155 train_time:117479ms step_avg:58.98ms
step:1993/2155 train_time:117568ms step_avg:58.99ms
step:1994/2155 train_time:117657ms step_avg:59.01ms
step:1995/2155 train_time:117745ms step_avg:59.02ms
step:1996/2155 train_time:117832ms step_avg:59.03ms
step:1997/2155 train_time:117922ms step_avg:59.05ms
step:1998/2155 train_time:118010ms step_avg:59.06ms
step:1999/2155 train_time:118100ms step_avg:59.08ms
step:2000/2155 train_time:118187ms step_avg:59.09ms
step:2000/2155 val_loss:3.3158 train_time:118278ms step_avg:59.14ms
step:2001/2155 train_time:118301ms step_avg:59.12ms
step:2002/2155 train_time:118368ms step_avg:59.12ms
step:2003/2155 train_time:118462ms step_avg:59.14ms
step:2004/2155 train_time:118549ms step_avg:59.16ms
step:2005/2155 train_time:118638ms step_avg:59.17ms
step:2006/2155 train_time:118724ms step_avg:59.18ms
step:2007/2155 train_time:118811ms step_avg:59.20ms
step:2008/2155 train_time:118897ms step_avg:59.21ms
step:2009/2155 train_time:118986ms step_avg:59.23ms
step:2010/2155 train_time:119072ms step_avg:59.24ms
step:2011/2155 train_time:119161ms step_avg:59.25ms
step:2012/2155 train_time:119253ms step_avg:59.27ms
step:2013/2155 train_time:119344ms step_avg:59.29ms
step:2014/2155 train_time:119434ms step_avg:59.30ms
step:2015/2155 train_time:119525ms step_avg:59.32ms
step:2016/2155 train_time:119612ms step_avg:59.33ms
step:2017/2155 train_time:119701ms step_avg:59.35ms
step:2018/2155 train_time:119788ms step_avg:59.36ms
step:2019/2155 train_time:119876ms step_avg:59.37ms
step:2020/2155 train_time:119963ms step_avg:59.39ms
step:2021/2155 train_time:120051ms step_avg:59.40ms
step:2022/2155 train_time:120138ms step_avg:59.42ms
step:2023/2155 train_time:120227ms step_avg:59.43ms
step:2024/2155 train_time:120318ms step_avg:59.45ms
step:2025/2155 train_time:120408ms step_avg:59.46ms
step:2026/2155 train_time:120497ms step_avg:59.48ms
step:2027/2155 train_time:120585ms step_avg:59.49ms
step:2028/2155 train_time:120673ms step_avg:59.50ms
step:2029/2155 train_time:120761ms step_avg:59.52ms
step:2030/2155 train_time:120847ms step_avg:59.53ms
step:2031/2155 train_time:120935ms step_avg:59.54ms
step:2032/2155 train_time:121022ms step_avg:59.56ms
step:2033/2155 train_time:121111ms step_avg:59.57ms
step:2034/2155 train_time:121198ms step_avg:59.59ms
step:2035/2155 train_time:121288ms step_avg:59.60ms
step:2036/2155 train_time:121377ms step_avg:59.62ms
step:2037/2155 train_time:121467ms step_avg:59.63ms
step:2038/2155 train_time:121555ms step_avg:59.64ms
step:2039/2155 train_time:121644ms step_avg:59.66ms
step:2040/2155 train_time:121731ms step_avg:59.67ms
step:2041/2155 train_time:121819ms step_avg:59.69ms
step:2042/2155 train_time:121907ms step_avg:59.70ms
step:2043/2155 train_time:121995ms step_avg:59.71ms
step:2044/2155 train_time:122082ms step_avg:59.73ms
step:2045/2155 train_time:122171ms step_avg:59.74ms
step:2046/2155 train_time:122258ms step_avg:59.75ms
step:2047/2155 train_time:122348ms step_avg:59.77ms
step:2048/2155 train_time:122436ms step_avg:59.78ms
step:2049/2155 train_time:122526ms step_avg:59.80ms
step:2050/2155 train_time:122613ms step_avg:59.81ms
step:2051/2155 train_time:122702ms step_avg:59.83ms
step:2052/2155 train_time:122790ms step_avg:59.84ms
step:2053/2155 train_time:122878ms step_avg:59.85ms
step:2054/2155 train_time:122964ms step_avg:59.87ms
step:2055/2155 train_time:123052ms step_avg:59.88ms
step:2056/2155 train_time:123140ms step_avg:59.89ms
step:2057/2155 train_time:123229ms step_avg:59.91ms
step:2058/2155 train_time:123317ms step_avg:59.92ms
step:2059/2155 train_time:123406ms step_avg:59.94ms
step:2060/2155 train_time:123495ms step_avg:59.95ms
step:2061/2155 train_time:123584ms step_avg:59.96ms
step:2062/2155 train_time:123671ms step_avg:59.98ms
step:2063/2155 train_time:123760ms step_avg:59.99ms
step:2064/2155 train_time:123847ms step_avg:60.00ms
step:2065/2155 train_time:123936ms step_avg:60.02ms
step:2066/2155 train_time:124023ms step_avg:60.03ms
step:2067/2155 train_time:124113ms step_avg:60.04ms
step:2068/2155 train_time:124201ms step_avg:60.06ms
step:2069/2155 train_time:124290ms step_avg:60.07ms
step:2070/2155 train_time:124378ms step_avg:60.09ms
step:2071/2155 train_time:124468ms step_avg:60.10ms
step:2072/2155 train_time:124556ms step_avg:60.11ms
step:2073/2155 train_time:124644ms step_avg:60.13ms
step:2074/2155 train_time:124732ms step_avg:60.14ms
step:2075/2155 train_time:124820ms step_avg:60.15ms
step:2076/2155 train_time:124908ms step_avg:60.17ms
step:2077/2155 train_time:124997ms step_avg:60.18ms
step:2078/2155 train_time:125085ms step_avg:60.19ms
step:2079/2155 train_time:125173ms step_avg:60.21ms
step:2080/2155 train_time:125261ms step_avg:60.22ms
step:2081/2155 train_time:125351ms step_avg:60.24ms
step:2082/2155 train_time:125439ms step_avg:60.25ms
step:2083/2155 train_time:125529ms step_avg:60.26ms
step:2084/2155 train_time:125616ms step_avg:60.28ms
step:2085/2155 train_time:125704ms step_avg:60.29ms
step:2086/2155 train_time:125792ms step_avg:60.30ms
step:2087/2155 train_time:125880ms step_avg:60.32ms
step:2088/2155 train_time:125969ms step_avg:60.33ms
step:2089/2155 train_time:126058ms step_avg:60.34ms
step:2090/2155 train_time:126146ms step_avg:60.36ms
step:2091/2155 train_time:126236ms step_avg:60.37ms
step:2092/2155 train_time:126323ms step_avg:60.38ms
step:2093/2155 train_time:126412ms step_avg:60.40ms
step:2094/2155 train_time:126500ms step_avg:60.41ms
step:2095/2155 train_time:126590ms step_avg:60.42ms
step:2096/2155 train_time:126677ms step_avg:60.44ms
step:2097/2155 train_time:126765ms step_avg:60.45ms
step:2098/2155 train_time:126852ms step_avg:60.46ms
step:2099/2155 train_time:126941ms step_avg:60.48ms
step:2100/2155 train_time:127028ms step_avg:60.49ms
step:2101/2155 train_time:127117ms step_avg:60.50ms
step:2102/2155 train_time:127205ms step_avg:60.52ms
step:2103/2155 train_time:127294ms step_avg:60.53ms
step:2104/2155 train_time:127382ms step_avg:60.54ms
step:2105/2155 train_time:127472ms step_avg:60.56ms
step:2106/2155 train_time:127560ms step_avg:60.57ms
step:2107/2155 train_time:127649ms step_avg:60.58ms
step:2108/2155 train_time:127737ms step_avg:60.60ms
step:2109/2155 train_time:127826ms step_avg:60.61ms
step:2110/2155 train_time:127913ms step_avg:60.62ms
step:2111/2155 train_time:128002ms step_avg:60.64ms
step:2112/2155 train_time:128090ms step_avg:60.65ms
step:2113/2155 train_time:128179ms step_avg:60.66ms
step:2114/2155 train_time:128267ms step_avg:60.68ms
step:2115/2155 train_time:128356ms step_avg:60.69ms
step:2116/2155 train_time:128444ms step_avg:60.70ms
step:2117/2155 train_time:128533ms step_avg:60.71ms
step:2118/2155 train_time:128620ms step_avg:60.73ms
step:2119/2155 train_time:128710ms step_avg:60.74ms
step:2120/2155 train_time:128798ms step_avg:60.75ms
step:2121/2155 train_time:128888ms step_avg:60.77ms
step:2122/2155 train_time:128976ms step_avg:60.78ms
step:2123/2155 train_time:129065ms step_avg:60.79ms
step:2124/2155 train_time:129152ms step_avg:60.81ms
step:2125/2155 train_time:129241ms step_avg:60.82ms
step:2126/2155 train_time:129329ms step_avg:60.83ms
step:2127/2155 train_time:129419ms step_avg:60.85ms
step:2128/2155 train_time:129507ms step_avg:60.86ms
step:2129/2155 train_time:129598ms step_avg:60.87ms
step:2130/2155 train_time:129685ms step_avg:60.89ms
step:2131/2155 train_time:129774ms step_avg:60.90ms
step:2132/2155 train_time:129862ms step_avg:60.91ms
step:2133/2155 train_time:129951ms step_avg:60.92ms
step:2134/2155 train_time:130039ms step_avg:60.94ms
step:2135/2155 train_time:130128ms step_avg:60.95ms
step:2136/2155 train_time:130215ms step_avg:60.96ms
step:2137/2155 train_time:130305ms step_avg:60.98ms
step:2138/2155 train_time:130392ms step_avg:60.99ms
step:2139/2155 train_time:130481ms step_avg:61.00ms
step:2140/2155 train_time:130570ms step_avg:61.01ms
step:2141/2155 train_time:130659ms step_avg:61.03ms
step:2142/2155 train_time:130747ms step_avg:61.04ms
step:2143/2155 train_time:130837ms step_avg:61.05ms
step:2144/2155 train_time:130925ms step_avg:61.07ms
step:2145/2155 train_time:131015ms step_avg:61.08ms
step:2146/2155 train_time:131103ms step_avg:61.09ms
step:2147/2155 train_time:131192ms step_avg:61.10ms
step:2148/2155 train_time:131280ms step_avg:61.12ms
step:2149/2155 train_time:131369ms step_avg:61.13ms
step:2150/2155 train_time:131457ms step_avg:61.14ms
step:2151/2155 train_time:131546ms step_avg:61.16ms
step:2152/2155 train_time:131635ms step_avg:61.17ms
step:2153/2155 train_time:131724ms step_avg:61.18ms
step:2154/2155 train_time:131812ms step_avg:61.19ms
step:2155/2155 train_time:131901ms step_avg:61.21ms
step:2155/2155 val_loss:3.2801 train_time:131990ms step_avg:61.25ms
peak memory allocated: 29892 MiB reserved: 44956 MiB

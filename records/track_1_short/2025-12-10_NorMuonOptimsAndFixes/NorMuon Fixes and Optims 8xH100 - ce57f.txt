import uuid
run_id = f"NorMuon Fixes and Optims 8xH100 - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2115  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 10 2025, 18:15:56) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 18:48:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   43C    P0            124W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   35C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   43C    P0            130W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   44C    P0            127W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   34C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   43C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   34C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           16164      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           16165      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           16166      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           16167      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           16168      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           16169      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           16170      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           16171      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           16165      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           16166      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           16167      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           16168      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           16169      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           16170      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           16171      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2155 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2155 train_time:97ms step_avg:96.78ms
step:2/2155 train_time:180ms step_avg:90.14ms
step:3/2155 train_time:206ms step_avg:68.61ms
step:4/2155 train_time:232ms step_avg:58.05ms
step:5/2155 train_time:256ms step_avg:51.23ms
step:6/2155 train_time:352ms step_avg:58.70ms
step:7/2155 train_time:376ms step_avg:53.68ms
step:8/2155 train_time:400ms step_avg:49.94ms
step:9/2155 train_time:424ms step_avg:47.08ms
step:10/2155 train_time:457ms step_avg:45.72ms
step:11/2155 train_time:490ms step_avg:44.56ms
step:12/2155 train_time:524ms step_avg:43.65ms
step:13/2155 train_time:556ms step_avg:42.78ms
step:14/2155 train_time:590ms step_avg:42.14ms
step:15/2155 train_time:622ms step_avg:41.49ms
step:16/2155 train_time:656ms step_avg:41.00ms
step:17/2155 train_time:689ms step_avg:40.53ms
step:18/2155 train_time:723ms step_avg:40.15ms
step:19/2155 train_time:755ms step_avg:39.76ms
step:20/2155 train_time:789ms step_avg:39.44ms
step:21/2155 train_time:822ms step_avg:39.14ms
step:22/2155 train_time:856ms step_avg:38.89ms
step:23/2155 train_time:889ms step_avg:38.63ms
step:24/2155 train_time:922ms step_avg:38.42ms
step:25/2155 train_time:955ms step_avg:38.19ms
step:26/2155 train_time:988ms step_avg:38.01ms
step:27/2155 train_time:1021ms step_avg:37.83ms
step:28/2155 train_time:1055ms step_avg:37.67ms
step:29/2155 train_time:1088ms step_avg:37.51ms
step:30/2155 train_time:1121ms step_avg:37.38ms
step:31/2155 train_time:1154ms step_avg:37.22ms
step:32/2155 train_time:1188ms step_avg:37.11ms
step:33/2155 train_time:1220ms step_avg:36.98ms
step:34/2155 train_time:1255ms step_avg:36.90ms
step:35/2155 train_time:1290ms step_avg:36.85ms
step:36/2155 train_time:1324ms step_avg:36.78ms
step:37/2155 train_time:1358ms step_avg:36.72ms
step:38/2155 train_time:1392ms step_avg:36.64ms
step:39/2155 train_time:1426ms step_avg:36.56ms
step:40/2155 train_time:1460ms step_avg:36.49ms
step:41/2155 train_time:1493ms step_avg:36.40ms
step:42/2155 train_time:1526ms step_avg:36.34ms
step:43/2155 train_time:1559ms step_avg:36.27ms
step:44/2155 train_time:1593ms step_avg:36.21ms
step:45/2155 train_time:1626ms step_avg:36.14ms
step:46/2155 train_time:1660ms step_avg:36.08ms
step:47/2155 train_time:1693ms step_avg:36.02ms
step:48/2155 train_time:1727ms step_avg:35.98ms
step:49/2155 train_time:1760ms step_avg:35.91ms
step:50/2155 train_time:1793ms step_avg:35.86ms
step:51/2155 train_time:1828ms step_avg:35.85ms
step:52/2155 train_time:1860ms step_avg:35.77ms
step:53/2155 train_time:1893ms step_avg:35.71ms
step:54/2155 train_time:1926ms step_avg:35.67ms
step:55/2155 train_time:1959ms step_avg:35.61ms
step:56/2155 train_time:1992ms step_avg:35.57ms
step:57/2155 train_time:2025ms step_avg:35.53ms
step:58/2155 train_time:2059ms step_avg:35.50ms
step:59/2155 train_time:2091ms step_avg:35.45ms
step:60/2155 train_time:2125ms step_avg:35.42ms
step:61/2155 train_time:2158ms step_avg:35.37ms
step:62/2155 train_time:2192ms step_avg:35.35ms
step:63/2155 train_time:2225ms step_avg:35.32ms
step:64/2155 train_time:2259ms step_avg:35.29ms
step:65/2155 train_time:2292ms step_avg:35.26ms
step:66/2155 train_time:2326ms step_avg:35.24ms
step:67/2155 train_time:2360ms step_avg:35.22ms
step:68/2155 train_time:2393ms step_avg:35.19ms
step:69/2155 train_time:2426ms step_avg:35.16ms
step:70/2155 train_time:2460ms step_avg:35.14ms
step:71/2155 train_time:2493ms step_avg:35.12ms
step:72/2155 train_time:2527ms step_avg:35.10ms
step:73/2155 train_time:2560ms step_avg:35.07ms
step:74/2155 train_time:2594ms step_avg:35.05ms
step:75/2155 train_time:2627ms step_avg:35.02ms
step:76/2155 train_time:2660ms step_avg:35.00ms
step:77/2155 train_time:2693ms step_avg:34.98ms
step:78/2155 train_time:2727ms step_avg:34.96ms
step:79/2155 train_time:2760ms step_avg:34.94ms
step:80/2155 train_time:2794ms step_avg:34.92ms
step:81/2155 train_time:2827ms step_avg:34.90ms
step:82/2155 train_time:2860ms step_avg:34.88ms
step:83/2155 train_time:2893ms step_avg:34.85ms
step:84/2155 train_time:2926ms step_avg:34.84ms
step:85/2155 train_time:2959ms step_avg:34.81ms
step:86/2155 train_time:2992ms step_avg:34.80ms
step:87/2155 train_time:3025ms step_avg:34.77ms
step:88/2155 train_time:3059ms step_avg:34.76ms
step:89/2155 train_time:3091ms step_avg:34.73ms
step:90/2155 train_time:3125ms step_avg:34.72ms
step:91/2155 train_time:3157ms step_avg:34.70ms
step:92/2155 train_time:3191ms step_avg:34.68ms
step:93/2155 train_time:3224ms step_avg:34.66ms
step:94/2155 train_time:3257ms step_avg:34.65ms
step:95/2155 train_time:3290ms step_avg:34.64ms
step:96/2155 train_time:3324ms step_avg:34.62ms
step:97/2155 train_time:3357ms step_avg:34.61ms
step:98/2155 train_time:3390ms step_avg:34.60ms
step:99/2155 train_time:3423ms step_avg:34.58ms
step:100/2155 train_time:3457ms step_avg:34.57ms
step:101/2155 train_time:3491ms step_avg:34.56ms
step:102/2155 train_time:3525ms step_avg:34.55ms
step:103/2155 train_time:3557ms step_avg:34.54ms
step:104/2155 train_time:3591ms step_avg:34.53ms
step:105/2155 train_time:3624ms step_avg:34.52ms
step:106/2155 train_time:3657ms step_avg:34.50ms
step:107/2155 train_time:3690ms step_avg:34.49ms
step:108/2155 train_time:3724ms step_avg:34.48ms
step:109/2155 train_time:3757ms step_avg:34.46ms
step:110/2155 train_time:3790ms step_avg:34.45ms
step:111/2155 train_time:3823ms step_avg:34.44ms
step:112/2155 train_time:3857ms step_avg:34.43ms
step:113/2155 train_time:3889ms step_avg:34.42ms
step:114/2155 train_time:3922ms step_avg:34.41ms
step:115/2155 train_time:3955ms step_avg:34.39ms
step:116/2155 train_time:3988ms step_avg:34.38ms
step:117/2155 train_time:4021ms step_avg:34.37ms
step:118/2155 train_time:4055ms step_avg:34.36ms
step:119/2155 train_time:4087ms step_avg:34.35ms
step:120/2155 train_time:4121ms step_avg:34.34ms
step:121/2155 train_time:4153ms step_avg:34.33ms
step:122/2155 train_time:4187ms step_avg:34.32ms
step:123/2155 train_time:4220ms step_avg:34.31ms
step:124/2155 train_time:4253ms step_avg:34.30ms
step:125/2155 train_time:4287ms step_avg:34.29ms
step:126/2155 train_time:4320ms step_avg:34.28ms
step:127/2155 train_time:4353ms step_avg:34.28ms
step:128/2155 train_time:4387ms step_avg:34.27ms
step:129/2155 train_time:4420ms step_avg:34.26ms
step:130/2155 train_time:4453ms step_avg:34.26ms
step:131/2155 train_time:4487ms step_avg:34.25ms
step:132/2155 train_time:4520ms step_avg:34.24ms
step:133/2155 train_time:4553ms step_avg:34.23ms
step:134/2155 train_time:4586ms step_avg:34.23ms
step:135/2155 train_time:4619ms step_avg:34.22ms
step:136/2155 train_time:4653ms step_avg:34.21ms
step:137/2155 train_time:4686ms step_avg:34.20ms
step:138/2155 train_time:4719ms step_avg:34.20ms
step:139/2155 train_time:4752ms step_avg:34.19ms
step:140/2155 train_time:4785ms step_avg:34.18ms
step:141/2155 train_time:4818ms step_avg:34.17ms
step:142/2155 train_time:4851ms step_avg:34.16ms
step:143/2155 train_time:4884ms step_avg:34.15ms
step:144/2155 train_time:4917ms step_avg:34.15ms
step:145/2155 train_time:4950ms step_avg:34.14ms
step:146/2155 train_time:4983ms step_avg:34.13ms
step:147/2155 train_time:5016ms step_avg:34.12ms
step:148/2155 train_time:5049ms step_avg:34.12ms
step:149/2155 train_time:5082ms step_avg:34.11ms
step:150/2155 train_time:5116ms step_avg:34.10ms
step:151/2155 train_time:5148ms step_avg:34.09ms
step:152/2155 train_time:5182ms step_avg:34.09ms
step:153/2155 train_time:5214ms step_avg:34.08ms
step:154/2155 train_time:5248ms step_avg:34.08ms
step:155/2155 train_time:5281ms step_avg:34.07ms
step:156/2155 train_time:5314ms step_avg:34.06ms
step:157/2155 train_time:5348ms step_avg:34.06ms
step:158/2155 train_time:5382ms step_avg:34.06ms
step:159/2155 train_time:5415ms step_avg:34.05ms
step:160/2155 train_time:5448ms step_avg:34.05ms
step:161/2155 train_time:5481ms step_avg:34.05ms
step:162/2155 train_time:5515ms step_avg:34.04ms
step:163/2155 train_time:5548ms step_avg:34.04ms
step:164/2155 train_time:5581ms step_avg:34.03ms
step:165/2155 train_time:5614ms step_avg:34.02ms
step:166/2155 train_time:5647ms step_avg:34.02ms
step:167/2155 train_time:5680ms step_avg:34.01ms
step:168/2155 train_time:5714ms step_avg:34.01ms
step:169/2155 train_time:5747ms step_avg:34.00ms
step:170/2155 train_time:5780ms step_avg:34.00ms
step:171/2155 train_time:5813ms step_avg:33.99ms
step:172/2155 train_time:5846ms step_avg:33.99ms
step:173/2155 train_time:5879ms step_avg:33.99ms
step:174/2155 train_time:5913ms step_avg:33.98ms
step:175/2155 train_time:5946ms step_avg:33.98ms
step:176/2155 train_time:5980ms step_avg:33.98ms
step:177/2155 train_time:6012ms step_avg:33.97ms
step:178/2155 train_time:6046ms step_avg:33.96ms
step:179/2155 train_time:6078ms step_avg:33.96ms
step:180/2155 train_time:6112ms step_avg:33.95ms
step:181/2155 train_time:6144ms step_avg:33.95ms
step:182/2155 train_time:6178ms step_avg:33.94ms
step:183/2155 train_time:6210ms step_avg:33.94ms
step:184/2155 train_time:6244ms step_avg:33.93ms
step:185/2155 train_time:6277ms step_avg:33.93ms
step:186/2155 train_time:6310ms step_avg:33.93ms
step:187/2155 train_time:6343ms step_avg:33.92ms
step:188/2155 train_time:6377ms step_avg:33.92ms
step:189/2155 train_time:6410ms step_avg:33.91ms
step:190/2155 train_time:6443ms step_avg:33.91ms
step:191/2155 train_time:6476ms step_avg:33.90ms
step:192/2155 train_time:6509ms step_avg:33.90ms
step:193/2155 train_time:6543ms step_avg:33.90ms
step:194/2155 train_time:6576ms step_avg:33.90ms
step:195/2155 train_time:6609ms step_avg:33.89ms
step:196/2155 train_time:6643ms step_avg:33.89ms
step:197/2155 train_time:6675ms step_avg:33.88ms
step:198/2155 train_time:6709ms step_avg:33.88ms
step:199/2155 train_time:6741ms step_avg:33.87ms
step:200/2155 train_time:6774ms step_avg:33.87ms
step:201/2155 train_time:6807ms step_avg:33.87ms
step:202/2155 train_time:6840ms step_avg:33.86ms
step:203/2155 train_time:6873ms step_avg:33.86ms
step:204/2155 train_time:6906ms step_avg:33.85ms
step:205/2155 train_time:6939ms step_avg:33.85ms
step:206/2155 train_time:6973ms step_avg:33.85ms
step:207/2155 train_time:7005ms step_avg:33.84ms
step:208/2155 train_time:7039ms step_avg:33.84ms
step:209/2155 train_time:7071ms step_avg:33.83ms
step:210/2155 train_time:7105ms step_avg:33.83ms
step:211/2155 train_time:7138ms step_avg:33.83ms
step:212/2155 train_time:7171ms step_avg:33.82ms
step:213/2155 train_time:7204ms step_avg:33.82ms
step:214/2155 train_time:7237ms step_avg:33.82ms
step:215/2155 train_time:7270ms step_avg:33.81ms
step:216/2155 train_time:7303ms step_avg:33.81ms
step:217/2155 train_time:7336ms step_avg:33.81ms
step:218/2155 train_time:7369ms step_avg:33.80ms
step:219/2155 train_time:7402ms step_avg:33.80ms
step:220/2155 train_time:7436ms step_avg:33.80ms
step:221/2155 train_time:7469ms step_avg:33.79ms
step:222/2155 train_time:7502ms step_avg:33.79ms
step:223/2155 train_time:7535ms step_avg:33.79ms
step:224/2155 train_time:7568ms step_avg:33.79ms
step:225/2155 train_time:7601ms step_avg:33.78ms
step:226/2155 train_time:7634ms step_avg:33.78ms
step:227/2155 train_time:7667ms step_avg:33.78ms
step:228/2155 train_time:7701ms step_avg:33.78ms
step:229/2155 train_time:7733ms step_avg:33.77ms
step:230/2155 train_time:7767ms step_avg:33.77ms
step:231/2155 train_time:7800ms step_avg:33.76ms
step:232/2155 train_time:7833ms step_avg:33.76ms
step:233/2155 train_time:7866ms step_avg:33.76ms
step:234/2155 train_time:7899ms step_avg:33.76ms
step:235/2155 train_time:7932ms step_avg:33.75ms
step:236/2155 train_time:7965ms step_avg:33.75ms
step:237/2155 train_time:7998ms step_avg:33.75ms
step:238/2155 train_time:8031ms step_avg:33.74ms
step:239/2155 train_time:8064ms step_avg:33.74ms
step:240/2155 train_time:8097ms step_avg:33.74ms
step:241/2155 train_time:8130ms step_avg:33.73ms
step:242/2155 train_time:8163ms step_avg:33.73ms
step:243/2155 train_time:8196ms step_avg:33.73ms
step:244/2155 train_time:8229ms step_avg:33.73ms
step:245/2155 train_time:8262ms step_avg:33.72ms
step:246/2155 train_time:8296ms step_avg:33.72ms
step:247/2155 train_time:8329ms step_avg:33.72ms
step:248/2155 train_time:8362ms step_avg:33.72ms
step:249/2155 train_time:8395ms step_avg:33.71ms
step:250/2155 train_time:8428ms step_avg:33.71ms
step:250/2155 val_loss:4.3199 train_time:8464ms step_avg:33.86ms
step:251/2155 train_time:8485ms step_avg:33.81ms
step:252/2155 train_time:8506ms step_avg:33.76ms
step:253/2155 train_time:8532ms step_avg:33.72ms
step:254/2155 train_time:8568ms step_avg:33.73ms
step:255/2155 train_time:8606ms step_avg:33.75ms
step:256/2155 train_time:8642ms step_avg:33.76ms
step:257/2155 train_time:8676ms step_avg:33.76ms
step:258/2155 train_time:8710ms step_avg:33.76ms
step:259/2155 train_time:8743ms step_avg:33.76ms
step:260/2155 train_time:8777ms step_avg:33.76ms
step:261/2155 train_time:8809ms step_avg:33.75ms
step:262/2155 train_time:8842ms step_avg:33.75ms
step:263/2155 train_time:8875ms step_avg:33.74ms
step:264/2155 train_time:8908ms step_avg:33.74ms
step:265/2155 train_time:8940ms step_avg:33.74ms
step:266/2155 train_time:8974ms step_avg:33.74ms
step:267/2155 train_time:9006ms step_avg:33.73ms
step:268/2155 train_time:9040ms step_avg:33.73ms
step:269/2155 train_time:9072ms step_avg:33.73ms
step:270/2155 train_time:9106ms step_avg:33.72ms
step:271/2155 train_time:9138ms step_avg:33.72ms
step:272/2155 train_time:9171ms step_avg:33.72ms
step:273/2155 train_time:9203ms step_avg:33.71ms
step:274/2155 train_time:9237ms step_avg:33.71ms
step:275/2155 train_time:9269ms step_avg:33.71ms
step:276/2155 train_time:9303ms step_avg:33.71ms
step:277/2155 train_time:9335ms step_avg:33.70ms
step:278/2155 train_time:9368ms step_avg:33.70ms
step:279/2155 train_time:9401ms step_avg:33.69ms
step:280/2155 train_time:9434ms step_avg:33.69ms
step:281/2155 train_time:9466ms step_avg:33.69ms
step:282/2155 train_time:9500ms step_avg:33.69ms
step:283/2155 train_time:9533ms step_avg:33.69ms
step:284/2155 train_time:9567ms step_avg:33.69ms
step:285/2155 train_time:9600ms step_avg:33.69ms
step:286/2155 train_time:9634ms step_avg:33.69ms
step:287/2155 train_time:9668ms step_avg:33.69ms
step:288/2155 train_time:9701ms step_avg:33.68ms
step:289/2155 train_time:9735ms step_avg:33.69ms
step:290/2155 train_time:9769ms step_avg:33.68ms
step:291/2155 train_time:9801ms step_avg:33.68ms
step:292/2155 train_time:9835ms step_avg:33.68ms
step:293/2155 train_time:9868ms step_avg:33.68ms
step:294/2155 train_time:9901ms step_avg:33.68ms
step:295/2155 train_time:9934ms step_avg:33.67ms
step:296/2155 train_time:9967ms step_avg:33.67ms
step:297/2155 train_time:9999ms step_avg:33.67ms
step:298/2155 train_time:10033ms step_avg:33.67ms
step:299/2155 train_time:10066ms step_avg:33.67ms
step:300/2155 train_time:10099ms step_avg:33.66ms
step:301/2155 train_time:10132ms step_avg:33.66ms
step:302/2155 train_time:10166ms step_avg:33.66ms
step:303/2155 train_time:10198ms step_avg:33.66ms
step:304/2155 train_time:10231ms step_avg:33.66ms
step:305/2155 train_time:10264ms step_avg:33.65ms
step:306/2155 train_time:10297ms step_avg:33.65ms
step:307/2155 train_time:10330ms step_avg:33.65ms
step:308/2155 train_time:10363ms step_avg:33.65ms
step:309/2155 train_time:10396ms step_avg:33.64ms
step:310/2155 train_time:10429ms step_avg:33.64ms
step:311/2155 train_time:10461ms step_avg:33.64ms
step:312/2155 train_time:10495ms step_avg:33.64ms
step:313/2155 train_time:10528ms step_avg:33.63ms
step:314/2155 train_time:10561ms step_avg:33.63ms
step:315/2155 train_time:10595ms step_avg:33.64ms
step:316/2155 train_time:10628ms step_avg:33.63ms
step:317/2155 train_time:10661ms step_avg:33.63ms
step:318/2155 train_time:10695ms step_avg:33.63ms
step:319/2155 train_time:10728ms step_avg:33.63ms
step:320/2155 train_time:10761ms step_avg:33.63ms
step:321/2155 train_time:10795ms step_avg:33.63ms
step:322/2155 train_time:10829ms step_avg:33.63ms
step:323/2155 train_time:10862ms step_avg:33.63ms
step:324/2155 train_time:10895ms step_avg:33.63ms
step:325/2155 train_time:10928ms step_avg:33.62ms
step:326/2155 train_time:10961ms step_avg:33.62ms
step:327/2155 train_time:10994ms step_avg:33.62ms
step:328/2155 train_time:11027ms step_avg:33.62ms
step:329/2155 train_time:11060ms step_avg:33.62ms
step:330/2155 train_time:11093ms step_avg:33.62ms
step:331/2155 train_time:11126ms step_avg:33.61ms
step:332/2155 train_time:11159ms step_avg:33.61ms
step:333/2155 train_time:11192ms step_avg:33.61ms
step:334/2155 train_time:11225ms step_avg:33.61ms
step:335/2155 train_time:11258ms step_avg:33.61ms
step:336/2155 train_time:11291ms step_avg:33.60ms
step:337/2155 train_time:11324ms step_avg:33.60ms
step:338/2155 train_time:11357ms step_avg:33.60ms
step:339/2155 train_time:11390ms step_avg:33.60ms
step:340/2155 train_time:11423ms step_avg:33.60ms
step:341/2155 train_time:11456ms step_avg:33.60ms
step:342/2155 train_time:11490ms step_avg:33.60ms
step:343/2155 train_time:11522ms step_avg:33.59ms
step:344/2155 train_time:11556ms step_avg:33.59ms
step:345/2155 train_time:11588ms step_avg:33.59ms
step:346/2155 train_time:11622ms step_avg:33.59ms
step:347/2155 train_time:11655ms step_avg:33.59ms
step:348/2155 train_time:11688ms step_avg:33.59ms
step:349/2155 train_time:11721ms step_avg:33.58ms
step:350/2155 train_time:11755ms step_avg:33.58ms
step:351/2155 train_time:11788ms step_avg:33.58ms
step:352/2155 train_time:11821ms step_avg:33.58ms
step:353/2155 train_time:11854ms step_avg:33.58ms
step:354/2155 train_time:11888ms step_avg:33.58ms
step:355/2155 train_time:11920ms step_avg:33.58ms
step:356/2155 train_time:11954ms step_avg:33.58ms
step:357/2155 train_time:11987ms step_avg:33.58ms
step:358/2155 train_time:12020ms step_avg:33.58ms
step:359/2155 train_time:12053ms step_avg:33.57ms
step:360/2155 train_time:12087ms step_avg:33.57ms
step:361/2155 train_time:12119ms step_avg:33.57ms
step:362/2155 train_time:12153ms step_avg:33.57ms
step:363/2155 train_time:12186ms step_avg:33.57ms
step:364/2155 train_time:12219ms step_avg:33.57ms
step:365/2155 train_time:12252ms step_avg:33.57ms
step:366/2155 train_time:12289ms step_avg:33.58ms
step:367/2155 train_time:12318ms step_avg:33.56ms
step:368/2155 train_time:12352ms step_avg:33.56ms
step:369/2155 train_time:12384ms step_avg:33.56ms
step:370/2155 train_time:12417ms step_avg:33.56ms
step:371/2155 train_time:12450ms step_avg:33.56ms
step:372/2155 train_time:12483ms step_avg:33.56ms
step:373/2155 train_time:12516ms step_avg:33.55ms
step:374/2155 train_time:12549ms step_avg:33.55ms
step:375/2155 train_time:12582ms step_avg:33.55ms
step:376/2155 train_time:12615ms step_avg:33.55ms
step:377/2155 train_time:12647ms step_avg:33.55ms
step:378/2155 train_time:12681ms step_avg:33.55ms
step:379/2155 train_time:12714ms step_avg:33.55ms
step:380/2155 train_time:12748ms step_avg:33.55ms
step:381/2155 train_time:12781ms step_avg:33.55ms
step:382/2155 train_time:12814ms step_avg:33.54ms
step:383/2155 train_time:12847ms step_avg:33.54ms
step:384/2155 train_time:12880ms step_avg:33.54ms
step:385/2155 train_time:12913ms step_avg:33.54ms
step:386/2155 train_time:12946ms step_avg:33.54ms
step:387/2155 train_time:12979ms step_avg:33.54ms
step:388/2155 train_time:13012ms step_avg:33.54ms
step:389/2155 train_time:13045ms step_avg:33.54ms
step:390/2155 train_time:13079ms step_avg:33.53ms
step:391/2155 train_time:13111ms step_avg:33.53ms
step:392/2155 train_time:13145ms step_avg:33.53ms
step:393/2155 train_time:13178ms step_avg:33.53ms
step:394/2155 train_time:13211ms step_avg:33.53ms
step:395/2155 train_time:13244ms step_avg:33.53ms
step:396/2155 train_time:13277ms step_avg:33.53ms
step:397/2155 train_time:13310ms step_avg:33.53ms
step:398/2155 train_time:13343ms step_avg:33.53ms
step:399/2155 train_time:13376ms step_avg:33.52ms
step:400/2155 train_time:13410ms step_avg:33.52ms
step:401/2155 train_time:13442ms step_avg:33.52ms
step:402/2155 train_time:13476ms step_avg:33.52ms
step:403/2155 train_time:13509ms step_avg:33.52ms
step:404/2155 train_time:13542ms step_avg:33.52ms
step:405/2155 train_time:13575ms step_avg:33.52ms
step:406/2155 train_time:13609ms step_avg:33.52ms
step:407/2155 train_time:13642ms step_avg:33.52ms
step:408/2155 train_time:13675ms step_avg:33.52ms
step:409/2155 train_time:13708ms step_avg:33.52ms
step:410/2155 train_time:13741ms step_avg:33.51ms
step:411/2155 train_time:13774ms step_avg:33.51ms
step:412/2155 train_time:13807ms step_avg:33.51ms
step:413/2155 train_time:13840ms step_avg:33.51ms
step:414/2155 train_time:13873ms step_avg:33.51ms
step:415/2155 train_time:13906ms step_avg:33.51ms
step:416/2155 train_time:13940ms step_avg:33.51ms
step:417/2155 train_time:13973ms step_avg:33.51ms
step:418/2155 train_time:14006ms step_avg:33.51ms
step:419/2155 train_time:14039ms step_avg:33.50ms
step:420/2155 train_time:14072ms step_avg:33.50ms
step:421/2155 train_time:14105ms step_avg:33.50ms
step:422/2155 train_time:14138ms step_avg:33.50ms
step:423/2155 train_time:14171ms step_avg:33.50ms
step:424/2155 train_time:14204ms step_avg:33.50ms
step:425/2155 train_time:14237ms step_avg:33.50ms
step:426/2155 train_time:14271ms step_avg:33.50ms
step:427/2155 train_time:14303ms step_avg:33.50ms
step:428/2155 train_time:14337ms step_avg:33.50ms
step:429/2155 train_time:14369ms step_avg:33.49ms
step:430/2155 train_time:14402ms step_avg:33.49ms
step:431/2155 train_time:14435ms step_avg:33.49ms
step:432/2155 train_time:14469ms step_avg:33.49ms
step:433/2155 train_time:14501ms step_avg:33.49ms
step:434/2155 train_time:14534ms step_avg:33.49ms
step:435/2155 train_time:14567ms step_avg:33.49ms
step:436/2155 train_time:14600ms step_avg:33.49ms
step:437/2155 train_time:14633ms step_avg:33.48ms
step:438/2155 train_time:14666ms step_avg:33.48ms
step:439/2155 train_time:14699ms step_avg:33.48ms
step:440/2155 train_time:14732ms step_avg:33.48ms
step:441/2155 train_time:14765ms step_avg:33.48ms
step:442/2155 train_time:14798ms step_avg:33.48ms
step:443/2155 train_time:14832ms step_avg:33.48ms
step:444/2155 train_time:14865ms step_avg:33.48ms
step:445/2155 train_time:14898ms step_avg:33.48ms
step:446/2155 train_time:14932ms step_avg:33.48ms
step:447/2155 train_time:14965ms step_avg:33.48ms
step:448/2155 train_time:14998ms step_avg:33.48ms
step:449/2155 train_time:15031ms step_avg:33.48ms
step:450/2155 train_time:15064ms step_avg:33.48ms
step:451/2155 train_time:15099ms step_avg:33.48ms
step:452/2155 train_time:15130ms step_avg:33.47ms
step:453/2155 train_time:15163ms step_avg:33.47ms
step:454/2155 train_time:15197ms step_avg:33.47ms
step:455/2155 train_time:15230ms step_avg:33.47ms
step:456/2155 train_time:15263ms step_avg:33.47ms
step:457/2155 train_time:15296ms step_avg:33.47ms
step:458/2155 train_time:15329ms step_avg:33.47ms
step:459/2155 train_time:15362ms step_avg:33.47ms
step:460/2155 train_time:15395ms step_avg:33.47ms
step:461/2155 train_time:15429ms step_avg:33.47ms
step:462/2155 train_time:15462ms step_avg:33.47ms
step:463/2155 train_time:15495ms step_avg:33.47ms
step:464/2155 train_time:15528ms step_avg:33.47ms
step:465/2155 train_time:15561ms step_avg:33.46ms
step:466/2155 train_time:15594ms step_avg:33.46ms
step:467/2155 train_time:15626ms step_avg:33.46ms
step:468/2155 train_time:15660ms step_avg:33.46ms
step:469/2155 train_time:15693ms step_avg:33.46ms
step:470/2155 train_time:15726ms step_avg:33.46ms
step:471/2155 train_time:15759ms step_avg:33.46ms
step:472/2155 train_time:15792ms step_avg:33.46ms
step:473/2155 train_time:15825ms step_avg:33.46ms
step:474/2155 train_time:15859ms step_avg:33.46ms
step:475/2155 train_time:15891ms step_avg:33.46ms
step:476/2155 train_time:15925ms step_avg:33.46ms
step:477/2155 train_time:15958ms step_avg:33.45ms
step:478/2155 train_time:15991ms step_avg:33.45ms
step:479/2155 train_time:16024ms step_avg:33.45ms
step:480/2155 train_time:16057ms step_avg:33.45ms
step:481/2155 train_time:16090ms step_avg:33.45ms
step:482/2155 train_time:16123ms step_avg:33.45ms
step:483/2155 train_time:16156ms step_avg:33.45ms
step:484/2155 train_time:16189ms step_avg:33.45ms
step:485/2155 train_time:16222ms step_avg:33.45ms
step:486/2155 train_time:16255ms step_avg:33.45ms
step:487/2155 train_time:16288ms step_avg:33.45ms
step:488/2155 train_time:16321ms step_avg:33.45ms
step:489/2155 train_time:16354ms step_avg:33.44ms
step:490/2155 train_time:16388ms step_avg:33.44ms
step:491/2155 train_time:16420ms step_avg:33.44ms
step:492/2155 train_time:16454ms step_avg:33.44ms
step:493/2155 train_time:16486ms step_avg:33.44ms
step:494/2155 train_time:16520ms step_avg:33.44ms
step:495/2155 train_time:16552ms step_avg:33.44ms
step:496/2155 train_time:16589ms step_avg:33.44ms
step:497/2155 train_time:16618ms step_avg:33.44ms
step:498/2155 train_time:16652ms step_avg:33.44ms
step:499/2155 train_time:16684ms step_avg:33.44ms
step:500/2155 train_time:16718ms step_avg:33.44ms
step:500/2155 val_loss:4.0303 train_time:16753ms step_avg:33.51ms
step:501/2155 train_time:16775ms step_avg:33.48ms
step:502/2155 train_time:16796ms step_avg:33.46ms
step:503/2155 train_time:16821ms step_avg:33.44ms
step:504/2155 train_time:16855ms step_avg:33.44ms
step:505/2155 train_time:16889ms step_avg:33.44ms
step:506/2155 train_time:16923ms step_avg:33.44ms
step:507/2155 train_time:16956ms step_avg:33.44ms
step:508/2155 train_time:16990ms step_avg:33.44ms
step:509/2155 train_time:17022ms step_avg:33.44ms
step:510/2155 train_time:17056ms step_avg:33.44ms
step:511/2155 train_time:17088ms step_avg:33.44ms
step:512/2155 train_time:17122ms step_avg:33.44ms
step:513/2155 train_time:17154ms step_avg:33.44ms
step:514/2155 train_time:17187ms step_avg:33.44ms
step:515/2155 train_time:17220ms step_avg:33.44ms
step:516/2155 train_time:17253ms step_avg:33.44ms
step:517/2155 train_time:17285ms step_avg:33.43ms
step:518/2155 train_time:17319ms step_avg:33.43ms
step:519/2155 train_time:17351ms step_avg:33.43ms
step:520/2155 train_time:17385ms step_avg:33.43ms
step:521/2155 train_time:17417ms step_avg:33.43ms
step:522/2155 train_time:17455ms step_avg:33.44ms
step:523/2155 train_time:17483ms step_avg:33.43ms
step:524/2155 train_time:17516ms step_avg:33.43ms
step:525/2155 train_time:17549ms step_avg:33.43ms
step:526/2155 train_time:17582ms step_avg:33.43ms
step:527/2155 train_time:17615ms step_avg:33.42ms
step:528/2155 train_time:17648ms step_avg:33.42ms
step:529/2155 train_time:17681ms step_avg:33.42ms
step:530/2155 train_time:17714ms step_avg:33.42ms
step:531/2155 train_time:17748ms step_avg:33.42ms
step:532/2155 train_time:17781ms step_avg:33.42ms
step:533/2155 train_time:17815ms step_avg:33.42ms
step:534/2155 train_time:17848ms step_avg:33.42ms
step:535/2155 train_time:17882ms step_avg:33.42ms
step:536/2155 train_time:17915ms step_avg:33.42ms
step:537/2155 train_time:17948ms step_avg:33.42ms
step:538/2155 train_time:17982ms step_avg:33.42ms
step:539/2155 train_time:18015ms step_avg:33.42ms
step:540/2155 train_time:18048ms step_avg:33.42ms
step:541/2155 train_time:18081ms step_avg:33.42ms
step:542/2155 train_time:18114ms step_avg:33.42ms
step:543/2155 train_time:18147ms step_avg:33.42ms
step:544/2155 train_time:18180ms step_avg:33.42ms
step:545/2155 train_time:18213ms step_avg:33.42ms
step:546/2155 train_time:18246ms step_avg:33.42ms
step:547/2155 train_time:18279ms step_avg:33.42ms
step:548/2155 train_time:18312ms step_avg:33.42ms
step:549/2155 train_time:18345ms step_avg:33.42ms
step:550/2155 train_time:18379ms step_avg:33.42ms
step:551/2155 train_time:18411ms step_avg:33.41ms
step:552/2155 train_time:18444ms step_avg:33.41ms
step:553/2155 train_time:18477ms step_avg:33.41ms
step:554/2155 train_time:18510ms step_avg:33.41ms
step:555/2155 train_time:18543ms step_avg:33.41ms
step:556/2155 train_time:18577ms step_avg:33.41ms
step:557/2155 train_time:18609ms step_avg:33.41ms
step:558/2155 train_time:18643ms step_avg:33.41ms
step:559/2155 train_time:18675ms step_avg:33.41ms
step:560/2155 train_time:18709ms step_avg:33.41ms
step:561/2155 train_time:18742ms step_avg:33.41ms
step:562/2155 train_time:18775ms step_avg:33.41ms
step:563/2155 train_time:18808ms step_avg:33.41ms
step:564/2155 train_time:18842ms step_avg:33.41ms
step:565/2155 train_time:18875ms step_avg:33.41ms
step:566/2155 train_time:18909ms step_avg:33.41ms
step:567/2155 train_time:18942ms step_avg:33.41ms
step:568/2155 train_time:18975ms step_avg:33.41ms
step:569/2155 train_time:19009ms step_avg:33.41ms
step:570/2155 train_time:19045ms step_avg:33.41ms
step:571/2155 train_time:19075ms step_avg:33.41ms
step:572/2155 train_time:19109ms step_avg:33.41ms
step:573/2155 train_time:19142ms step_avg:33.41ms
step:574/2155 train_time:19175ms step_avg:33.41ms
step:575/2155 train_time:19208ms step_avg:33.40ms
step:576/2155 train_time:19241ms step_avg:33.40ms
step:577/2155 train_time:19274ms step_avg:33.40ms
step:578/2155 train_time:19307ms step_avg:33.40ms
step:579/2155 train_time:19340ms step_avg:33.40ms
step:580/2155 train_time:19373ms step_avg:33.40ms
step:581/2155 train_time:19406ms step_avg:33.40ms
step:582/2155 train_time:19440ms step_avg:33.40ms
step:583/2155 train_time:19472ms step_avg:33.40ms
step:584/2155 train_time:19505ms step_avg:33.40ms
step:585/2155 train_time:19538ms step_avg:33.40ms
step:586/2155 train_time:19571ms step_avg:33.40ms
step:587/2155 train_time:19604ms step_avg:33.40ms
step:588/2155 train_time:19637ms step_avg:33.40ms
step:589/2155 train_time:19670ms step_avg:33.40ms
step:590/2155 train_time:19704ms step_avg:33.40ms
step:591/2155 train_time:19737ms step_avg:33.40ms
step:592/2155 train_time:19770ms step_avg:33.40ms
step:593/2155 train_time:19803ms step_avg:33.39ms
step:594/2155 train_time:19836ms step_avg:33.39ms
step:595/2155 train_time:19869ms step_avg:33.39ms
step:596/2155 train_time:19903ms step_avg:33.39ms
step:597/2155 train_time:19936ms step_avg:33.39ms
step:598/2155 train_time:19969ms step_avg:33.39ms
step:599/2155 train_time:20002ms step_avg:33.39ms
step:600/2155 train_time:20035ms step_avg:33.39ms
step:601/2155 train_time:20069ms step_avg:33.39ms
step:602/2155 train_time:20102ms step_avg:33.39ms
step:603/2155 train_time:20135ms step_avg:33.39ms
step:604/2155 train_time:20168ms step_avg:33.39ms
step:605/2155 train_time:20201ms step_avg:33.39ms
step:606/2155 train_time:20234ms step_avg:33.39ms
step:607/2155 train_time:20267ms step_avg:33.39ms
step:608/2155 train_time:20301ms step_avg:33.39ms
step:609/2155 train_time:20333ms step_avg:33.39ms
step:610/2155 train_time:20367ms step_avg:33.39ms
step:611/2155 train_time:20399ms step_avg:33.39ms
step:612/2155 train_time:20432ms step_avg:33.39ms
step:613/2155 train_time:20465ms step_avg:33.39ms
step:614/2155 train_time:20499ms step_avg:33.39ms
step:615/2155 train_time:20531ms step_avg:33.38ms
step:616/2155 train_time:20565ms step_avg:33.38ms
step:617/2155 train_time:20598ms step_avg:33.38ms
step:618/2155 train_time:20631ms step_avg:33.38ms
step:619/2155 train_time:20664ms step_avg:33.38ms
step:620/2155 train_time:20697ms step_avg:33.38ms
step:621/2155 train_time:20731ms step_avg:33.38ms
step:622/2155 train_time:20764ms step_avg:33.38ms
step:623/2155 train_time:20797ms step_avg:33.38ms
step:624/2155 train_time:20831ms step_avg:33.38ms
step:625/2155 train_time:20864ms step_avg:33.38ms
step:626/2155 train_time:20897ms step_avg:33.38ms
step:627/2155 train_time:20930ms step_avg:33.38ms
step:628/2155 train_time:20963ms step_avg:33.38ms
step:629/2155 train_time:20996ms step_avg:33.38ms
step:630/2155 train_time:21030ms step_avg:33.38ms
step:631/2155 train_time:21063ms step_avg:33.38ms
step:632/2155 train_time:21096ms step_avg:33.38ms
step:633/2155 train_time:21130ms step_avg:33.38ms
step:634/2155 train_time:21163ms step_avg:33.38ms
step:635/2155 train_time:21196ms step_avg:33.38ms
step:636/2155 train_time:21229ms step_avg:33.38ms
step:637/2155 train_time:21262ms step_avg:33.38ms
step:638/2155 train_time:21295ms step_avg:33.38ms
step:639/2155 train_time:21329ms step_avg:33.38ms
step:640/2155 train_time:21362ms step_avg:33.38ms
step:641/2155 train_time:21395ms step_avg:33.38ms
step:642/2155 train_time:21428ms step_avg:33.38ms
step:643/2155 train_time:21461ms step_avg:33.38ms
step:644/2155 train_time:21494ms step_avg:33.38ms
step:645/2155 train_time:21527ms step_avg:33.38ms
step:646/2155 train_time:21560ms step_avg:33.38ms
step:647/2155 train_time:21593ms step_avg:33.37ms
step:648/2155 train_time:21627ms step_avg:33.37ms
step:649/2155 train_time:21659ms step_avg:33.37ms
step:650/2155 train_time:21693ms step_avg:33.37ms
step:651/2155 train_time:21726ms step_avg:33.37ms
step:652/2155 train_time:21759ms step_avg:33.37ms
step:653/2155 train_time:21793ms step_avg:33.37ms
step:654/2155 train_time:21826ms step_avg:33.37ms
step:655/2155 train_time:21859ms step_avg:33.37ms
step:656/2155 train_time:21892ms step_avg:33.37ms
step:657/2155 train_time:21926ms step_avg:33.37ms
step:658/2155 train_time:21960ms step_avg:33.37ms
step:659/2155 train_time:21992ms step_avg:33.37ms
step:660/2155 train_time:22026ms step_avg:33.37ms
step:661/2155 train_time:22059ms step_avg:33.37ms
step:662/2155 train_time:22092ms step_avg:33.37ms
step:663/2155 train_time:22125ms step_avg:33.37ms
step:664/2155 train_time:22159ms step_avg:33.37ms
step:665/2155 train_time:22192ms step_avg:33.37ms
step:666/2155 train_time:22225ms step_avg:33.37ms
step:667/2155 train_time:22258ms step_avg:33.37ms
step:668/2155 train_time:22291ms step_avg:33.37ms
step:669/2155 train_time:22324ms step_avg:33.37ms
step:670/2155 train_time:22358ms step_avg:33.37ms
step:671/2155 train_time:22391ms step_avg:33.37ms
step:672/2155 train_time:22424ms step_avg:33.37ms
step:673/2155 train_time:22457ms step_avg:33.37ms
step:674/2155 train_time:22490ms step_avg:33.37ms
step:675/2155 train_time:22523ms step_avg:33.37ms
step:676/2155 train_time:22556ms step_avg:33.37ms
step:677/2155 train_time:22589ms step_avg:33.37ms
step:678/2155 train_time:22622ms step_avg:33.37ms
step:679/2155 train_time:22655ms step_avg:33.36ms
step:680/2155 train_time:22688ms step_avg:33.37ms
step:681/2155 train_time:22721ms step_avg:33.36ms
step:682/2155 train_time:22755ms step_avg:33.36ms
step:683/2155 train_time:22788ms step_avg:33.36ms
step:684/2155 train_time:22821ms step_avg:33.36ms
step:685/2155 train_time:22854ms step_avg:33.36ms
step:686/2155 train_time:22888ms step_avg:33.36ms
step:687/2155 train_time:22921ms step_avg:33.36ms
step:688/2155 train_time:22954ms step_avg:33.36ms
step:689/2155 train_time:22987ms step_avg:33.36ms
step:690/2155 train_time:23021ms step_avg:33.36ms
step:691/2155 train_time:23054ms step_avg:33.36ms
step:692/2155 train_time:23087ms step_avg:33.36ms
step:693/2155 train_time:23120ms step_avg:33.36ms
step:694/2155 train_time:23153ms step_avg:33.36ms
step:695/2155 train_time:23186ms step_avg:33.36ms
step:696/2155 train_time:23220ms step_avg:33.36ms
step:697/2155 train_time:23252ms step_avg:33.36ms
step:698/2155 train_time:23286ms step_avg:33.36ms
step:699/2155 train_time:23319ms step_avg:33.36ms
step:700/2155 train_time:23352ms step_avg:33.36ms
step:701/2155 train_time:23385ms step_avg:33.36ms
step:702/2155 train_time:23418ms step_avg:33.36ms
step:703/2155 train_time:23451ms step_avg:33.36ms
step:704/2155 train_time:23485ms step_avg:33.36ms
step:705/2155 train_time:23518ms step_avg:33.36ms
step:706/2155 train_time:23552ms step_avg:33.36ms
step:707/2155 train_time:23612ms step_avg:33.40ms
step:708/2155 train_time:23670ms step_avg:33.43ms
step:709/2155 train_time:23732ms step_avg:33.47ms
step:710/2155 train_time:23791ms step_avg:33.51ms
step:711/2155 train_time:23853ms step_avg:33.55ms
step:712/2155 train_time:23913ms step_avg:33.59ms
step:713/2155 train_time:23974ms step_avg:33.62ms
step:714/2155 train_time:24034ms step_avg:33.66ms
step:715/2155 train_time:24096ms step_avg:33.70ms
step:716/2155 train_time:24155ms step_avg:33.74ms
step:717/2155 train_time:24217ms step_avg:33.78ms
step:718/2155 train_time:24276ms step_avg:33.81ms
step:719/2155 train_time:24338ms step_avg:33.85ms
step:720/2155 train_time:24398ms step_avg:33.89ms
step:721/2155 train_time:24459ms step_avg:33.92ms
step:722/2155 train_time:24518ms step_avg:33.96ms
step:723/2155 train_time:24578ms step_avg:33.99ms
step:724/2155 train_time:24637ms step_avg:34.03ms
step:725/2155 train_time:24698ms step_avg:34.07ms
step:726/2155 train_time:24758ms step_avg:34.10ms
step:727/2155 train_time:24819ms step_avg:34.14ms
step:728/2155 train_time:24879ms step_avg:34.17ms
step:729/2155 train_time:24941ms step_avg:34.21ms
step:730/2155 train_time:25000ms step_avg:34.25ms
step:731/2155 train_time:25061ms step_avg:34.28ms
step:732/2155 train_time:25121ms step_avg:34.32ms
step:733/2155 train_time:25182ms step_avg:34.35ms
step:734/2155 train_time:25241ms step_avg:34.39ms
step:735/2155 train_time:25302ms step_avg:34.42ms
step:736/2155 train_time:25361ms step_avg:34.46ms
step:737/2155 train_time:25422ms step_avg:34.49ms
step:738/2155 train_time:25481ms step_avg:34.53ms
step:739/2155 train_time:25542ms step_avg:34.56ms
step:740/2155 train_time:25601ms step_avg:34.60ms
step:741/2155 train_time:25662ms step_avg:34.63ms
step:742/2155 train_time:25721ms step_avg:34.66ms
step:743/2155 train_time:25783ms step_avg:34.70ms
step:744/2155 train_time:25842ms step_avg:34.73ms
step:745/2155 train_time:25903ms step_avg:34.77ms
step:746/2155 train_time:25962ms step_avg:34.80ms
step:747/2155 train_time:26023ms step_avg:34.84ms
step:748/2155 train_time:26083ms step_avg:34.87ms
step:749/2155 train_time:26144ms step_avg:34.91ms
step:750/2155 train_time:26203ms step_avg:34.94ms
step:750/2155 val_loss:3.8840 train_time:26266ms step_avg:35.02ms
step:751/2155 train_time:26288ms step_avg:35.00ms
step:752/2155 train_time:26324ms step_avg:35.00ms
step:753/2155 train_time:26385ms step_avg:35.04ms
step:754/2155 train_time:26447ms step_avg:35.08ms
step:755/2155 train_time:26509ms step_avg:35.11ms
step:756/2155 train_time:26567ms step_avg:35.14ms
step:757/2155 train_time:26627ms step_avg:35.17ms
step:758/2155 train_time:26685ms step_avg:35.20ms
step:759/2155 train_time:26746ms step_avg:35.24ms
step:760/2155 train_time:26804ms step_avg:35.27ms
step:761/2155 train_time:26864ms step_avg:35.30ms
step:762/2155 train_time:26925ms step_avg:35.33ms
step:763/2155 train_time:26983ms step_avg:35.36ms
step:764/2155 train_time:27041ms step_avg:35.39ms
step:765/2155 train_time:27102ms step_avg:35.43ms
step:766/2155 train_time:27167ms step_avg:35.47ms
step:767/2155 train_time:27235ms step_avg:35.51ms
step:768/2155 train_time:27295ms step_avg:35.54ms
step:769/2155 train_time:27357ms step_avg:35.57ms
step:770/2155 train_time:27417ms step_avg:35.61ms
step:771/2155 train_time:27478ms step_avg:35.64ms
step:772/2155 train_time:27537ms step_avg:35.67ms
step:773/2155 train_time:27598ms step_avg:35.70ms
step:774/2155 train_time:27658ms step_avg:35.73ms
step:775/2155 train_time:27718ms step_avg:35.77ms
step:776/2155 train_time:27777ms step_avg:35.80ms
step:777/2155 train_time:27838ms step_avg:35.83ms
step:778/2155 train_time:27897ms step_avg:35.86ms
step:779/2155 train_time:27957ms step_avg:35.89ms
step:780/2155 train_time:28016ms step_avg:35.92ms
step:781/2155 train_time:28076ms step_avg:35.95ms
step:782/2155 train_time:28137ms step_avg:35.98ms
step:783/2155 train_time:28200ms step_avg:36.02ms
step:784/2155 train_time:28262ms step_avg:36.05ms
step:785/2155 train_time:28324ms step_avg:36.08ms
step:786/2155 train_time:28385ms step_avg:36.11ms
step:787/2155 train_time:28447ms step_avg:36.15ms
step:788/2155 train_time:28506ms step_avg:36.17ms
step:789/2155 train_time:28567ms step_avg:36.21ms
step:790/2155 train_time:28626ms step_avg:36.24ms
step:791/2155 train_time:28686ms step_avg:36.27ms
step:792/2155 train_time:28745ms step_avg:36.29ms
step:793/2155 train_time:28805ms step_avg:36.32ms
step:794/2155 train_time:28864ms step_avg:36.35ms
step:795/2155 train_time:28925ms step_avg:36.38ms
step:796/2155 train_time:28983ms step_avg:36.41ms
step:797/2155 train_time:29044ms step_avg:36.44ms
step:798/2155 train_time:29104ms step_avg:36.47ms
step:799/2155 train_time:29167ms step_avg:36.50ms
step:800/2155 train_time:29227ms step_avg:36.53ms
step:801/2155 train_time:29289ms step_avg:36.57ms
step:802/2155 train_time:29349ms step_avg:36.59ms
step:803/2155 train_time:29411ms step_avg:36.63ms
step:804/2155 train_time:29470ms step_avg:36.65ms
step:805/2155 train_time:29531ms step_avg:36.68ms
step:806/2155 train_time:29590ms step_avg:36.71ms
step:807/2155 train_time:29651ms step_avg:36.74ms
step:808/2155 train_time:29710ms step_avg:36.77ms
step:809/2155 train_time:29772ms step_avg:36.80ms
step:810/2155 train_time:29830ms step_avg:36.83ms
step:811/2155 train_time:29891ms step_avg:36.86ms
step:812/2155 train_time:29951ms step_avg:36.88ms
step:813/2155 train_time:30012ms step_avg:36.91ms
step:814/2155 train_time:30071ms step_avg:36.94ms
step:815/2155 train_time:30132ms step_avg:36.97ms
step:816/2155 train_time:30191ms step_avg:37.00ms
step:817/2155 train_time:30253ms step_avg:37.03ms
step:818/2155 train_time:30313ms step_avg:37.06ms
step:819/2155 train_time:30374ms step_avg:37.09ms
step:820/2155 train_time:30434ms step_avg:37.11ms
step:821/2155 train_time:30493ms step_avg:37.14ms
step:822/2155 train_time:30553ms step_avg:37.17ms
step:823/2155 train_time:30614ms step_avg:37.20ms
step:824/2155 train_time:30673ms step_avg:37.22ms
step:825/2155 train_time:30733ms step_avg:37.25ms
step:826/2155 train_time:30792ms step_avg:37.28ms
step:827/2155 train_time:30854ms step_avg:37.31ms
step:828/2155 train_time:30913ms step_avg:37.33ms
step:829/2155 train_time:30973ms step_avg:37.36ms
step:830/2155 train_time:31032ms step_avg:37.39ms
step:831/2155 train_time:31093ms step_avg:37.42ms
step:832/2155 train_time:31153ms step_avg:37.44ms
step:833/2155 train_time:31214ms step_avg:37.47ms
step:834/2155 train_time:31273ms step_avg:37.50ms
step:835/2155 train_time:31334ms step_avg:37.53ms
step:836/2155 train_time:31393ms step_avg:37.55ms
step:837/2155 train_time:31455ms step_avg:37.58ms
step:838/2155 train_time:31514ms step_avg:37.61ms
step:839/2155 train_time:31574ms step_avg:37.63ms
step:840/2155 train_time:31633ms step_avg:37.66ms
step:841/2155 train_time:31694ms step_avg:37.69ms
step:842/2155 train_time:31753ms step_avg:37.71ms
step:843/2155 train_time:31815ms step_avg:37.74ms
step:844/2155 train_time:31874ms step_avg:37.77ms
step:845/2155 train_time:31935ms step_avg:37.79ms
step:846/2155 train_time:31995ms step_avg:37.82ms
step:847/2155 train_time:32056ms step_avg:37.85ms
step:848/2155 train_time:32115ms step_avg:37.87ms
step:849/2155 train_time:32175ms step_avg:37.90ms
step:850/2155 train_time:32235ms step_avg:37.92ms
step:851/2155 train_time:32295ms step_avg:37.95ms
step:852/2155 train_time:32354ms step_avg:37.97ms
step:853/2155 train_time:32415ms step_avg:38.00ms
step:854/2155 train_time:32475ms step_avg:38.03ms
step:855/2155 train_time:32536ms step_avg:38.05ms
step:856/2155 train_time:32596ms step_avg:38.08ms
step:857/2155 train_time:32656ms step_avg:38.11ms
step:858/2155 train_time:32716ms step_avg:38.13ms
step:859/2155 train_time:32777ms step_avg:38.16ms
step:860/2155 train_time:32836ms step_avg:38.18ms
step:861/2155 train_time:32898ms step_avg:38.21ms
step:862/2155 train_time:32957ms step_avg:38.23ms
step:863/2155 train_time:33018ms step_avg:38.26ms
step:864/2155 train_time:33077ms step_avg:38.28ms
step:865/2155 train_time:33139ms step_avg:38.31ms
step:866/2155 train_time:33198ms step_avg:38.34ms
step:867/2155 train_time:33260ms step_avg:38.36ms
step:868/2155 train_time:33320ms step_avg:38.39ms
step:869/2155 train_time:33382ms step_avg:38.41ms
step:870/2155 train_time:33441ms step_avg:38.44ms
step:871/2155 train_time:33503ms step_avg:38.47ms
step:872/2155 train_time:33563ms step_avg:38.49ms
step:873/2155 train_time:33625ms step_avg:38.52ms
step:874/2155 train_time:33684ms step_avg:38.54ms
step:875/2155 train_time:33746ms step_avg:38.57ms
step:876/2155 train_time:33806ms step_avg:38.59ms
step:877/2155 train_time:33867ms step_avg:38.62ms
step:878/2155 train_time:33926ms step_avg:38.64ms
step:879/2155 train_time:33988ms step_avg:38.67ms
step:880/2155 train_time:34047ms step_avg:38.69ms
step:881/2155 train_time:34109ms step_avg:38.72ms
step:882/2155 train_time:34168ms step_avg:38.74ms
step:883/2155 train_time:34230ms step_avg:38.77ms
step:884/2155 train_time:34289ms step_avg:38.79ms
step:885/2155 train_time:34350ms step_avg:38.81ms
step:886/2155 train_time:34408ms step_avg:38.84ms
step:887/2155 train_time:34469ms step_avg:38.86ms
step:888/2155 train_time:34528ms step_avg:38.88ms
step:889/2155 train_time:34590ms step_avg:38.91ms
step:890/2155 train_time:34650ms step_avg:38.93ms
step:891/2155 train_time:34712ms step_avg:38.96ms
step:892/2155 train_time:34770ms step_avg:38.98ms
step:893/2155 train_time:34832ms step_avg:39.01ms
step:894/2155 train_time:34891ms step_avg:39.03ms
step:895/2155 train_time:34952ms step_avg:39.05ms
step:896/2155 train_time:35012ms step_avg:39.08ms
step:897/2155 train_time:35073ms step_avg:39.10ms
step:898/2155 train_time:35132ms step_avg:39.12ms
step:899/2155 train_time:35193ms step_avg:39.15ms
step:900/2155 train_time:35252ms step_avg:39.17ms
step:901/2155 train_time:35313ms step_avg:39.19ms
step:902/2155 train_time:35372ms step_avg:39.21ms
step:903/2155 train_time:35432ms step_avg:39.24ms
step:904/2155 train_time:35492ms step_avg:39.26ms
step:905/2155 train_time:35553ms step_avg:39.28ms
step:906/2155 train_time:35612ms step_avg:39.31ms
step:907/2155 train_time:35673ms step_avg:39.33ms
step:908/2155 train_time:35733ms step_avg:39.35ms
step:909/2155 train_time:35794ms step_avg:39.38ms
step:910/2155 train_time:35853ms step_avg:39.40ms
step:911/2155 train_time:35914ms step_avg:39.42ms
step:912/2155 train_time:35973ms step_avg:39.44ms
step:913/2155 train_time:36034ms step_avg:39.47ms
step:914/2155 train_time:36094ms step_avg:39.49ms
step:915/2155 train_time:36155ms step_avg:39.51ms
step:916/2155 train_time:36214ms step_avg:39.53ms
step:917/2155 train_time:36274ms step_avg:39.56ms
step:918/2155 train_time:36334ms step_avg:39.58ms
step:919/2155 train_time:36394ms step_avg:39.60ms
step:920/2155 train_time:36454ms step_avg:39.62ms
step:921/2155 train_time:36515ms step_avg:39.65ms
step:922/2155 train_time:36574ms step_avg:39.67ms
step:923/2155 train_time:36635ms step_avg:39.69ms
step:924/2155 train_time:36694ms step_avg:39.71ms
step:925/2155 train_time:36755ms step_avg:39.74ms
step:926/2155 train_time:36815ms step_avg:39.76ms
step:927/2155 train_time:36875ms step_avg:39.78ms
step:928/2155 train_time:36934ms step_avg:39.80ms
step:929/2155 train_time:36995ms step_avg:39.82ms
step:930/2155 train_time:37055ms step_avg:39.84ms
step:931/2155 train_time:37116ms step_avg:39.87ms
step:932/2155 train_time:37175ms step_avg:39.89ms
step:933/2155 train_time:37236ms step_avg:39.91ms
step:934/2155 train_time:37295ms step_avg:39.93ms
step:935/2155 train_time:37357ms step_avg:39.95ms
step:936/2155 train_time:37416ms step_avg:39.97ms
step:937/2155 train_time:37477ms step_avg:40.00ms
step:938/2155 train_time:37537ms step_avg:40.02ms
step:939/2155 train_time:37598ms step_avg:40.04ms
step:940/2155 train_time:37658ms step_avg:40.06ms
step:941/2155 train_time:37719ms step_avg:40.08ms
step:942/2155 train_time:37779ms step_avg:40.10ms
step:943/2155 train_time:37840ms step_avg:40.13ms
step:944/2155 train_time:37900ms step_avg:40.15ms
step:945/2155 train_time:37961ms step_avg:40.17ms
step:946/2155 train_time:38021ms step_avg:40.19ms
step:947/2155 train_time:38084ms step_avg:40.22ms
step:948/2155 train_time:38143ms step_avg:40.24ms
step:949/2155 train_time:38204ms step_avg:40.26ms
step:950/2155 train_time:38264ms step_avg:40.28ms
step:951/2155 train_time:38325ms step_avg:40.30ms
step:952/2155 train_time:38385ms step_avg:40.32ms
step:953/2155 train_time:38446ms step_avg:40.34ms
step:954/2155 train_time:38507ms step_avg:40.36ms
step:955/2155 train_time:38568ms step_avg:40.39ms
step:956/2155 train_time:38629ms step_avg:40.41ms
step:957/2155 train_time:38691ms step_avg:40.43ms
step:958/2155 train_time:38750ms step_avg:40.45ms
step:959/2155 train_time:38811ms step_avg:40.47ms
step:960/2155 train_time:38870ms step_avg:40.49ms
step:961/2155 train_time:38931ms step_avg:40.51ms
step:962/2155 train_time:38990ms step_avg:40.53ms
step:963/2155 train_time:39052ms step_avg:40.55ms
step:964/2155 train_time:39111ms step_avg:40.57ms
step:965/2155 train_time:39172ms step_avg:40.59ms
step:966/2155 train_time:39231ms step_avg:40.61ms
step:967/2155 train_time:39292ms step_avg:40.63ms
step:968/2155 train_time:39351ms step_avg:40.65ms
step:969/2155 train_time:39413ms step_avg:40.67ms
step:970/2155 train_time:39472ms step_avg:40.69ms
step:971/2155 train_time:39533ms step_avg:40.71ms
step:972/2155 train_time:39593ms step_avg:40.73ms
step:973/2155 train_time:39654ms step_avg:40.75ms
step:974/2155 train_time:39713ms step_avg:40.77ms
step:975/2155 train_time:39774ms step_avg:40.79ms
step:976/2155 train_time:39833ms step_avg:40.81ms
step:977/2155 train_time:39894ms step_avg:40.83ms
step:978/2155 train_time:39954ms step_avg:40.85ms
step:979/2155 train_time:40014ms step_avg:40.87ms
step:980/2155 train_time:40074ms step_avg:40.89ms
step:981/2155 train_time:40134ms step_avg:40.91ms
step:982/2155 train_time:40194ms step_avg:40.93ms
step:983/2155 train_time:40255ms step_avg:40.95ms
step:984/2155 train_time:40314ms step_avg:40.97ms
step:985/2155 train_time:40375ms step_avg:40.99ms
step:986/2155 train_time:40434ms step_avg:41.01ms
step:987/2155 train_time:40495ms step_avg:41.03ms
step:988/2155 train_time:40555ms step_avg:41.05ms
step:989/2155 train_time:40615ms step_avg:41.07ms
step:990/2155 train_time:40675ms step_avg:41.09ms
step:991/2155 train_time:40736ms step_avg:41.11ms
step:992/2155 train_time:40795ms step_avg:41.12ms
step:993/2155 train_time:40856ms step_avg:41.14ms
step:994/2155 train_time:40915ms step_avg:41.16ms
step:995/2155 train_time:40976ms step_avg:41.18ms
step:996/2155 train_time:41035ms step_avg:41.20ms
step:997/2155 train_time:41096ms step_avg:41.22ms
step:998/2155 train_time:41156ms step_avg:41.24ms
step:999/2155 train_time:41217ms step_avg:41.26ms
step:1000/2155 train_time:41276ms step_avg:41.28ms
step:1000/2155 val_loss:3.7169 train_time:41340ms step_avg:41.34ms
step:1001/2155 train_time:41373ms step_avg:41.33ms
step:1002/2155 train_time:41402ms step_avg:41.32ms
step:1003/2155 train_time:41466ms step_avg:41.34ms
step:1004/2155 train_time:41525ms step_avg:41.36ms
step:1005/2155 train_time:41586ms step_avg:41.38ms
step:1006/2155 train_time:41645ms step_avg:41.40ms
step:1007/2155 train_time:41705ms step_avg:41.42ms
step:1008/2155 train_time:41764ms step_avg:41.43ms
step:1009/2155 train_time:41825ms step_avg:41.45ms
step:1010/2155 train_time:41883ms step_avg:41.47ms
step:1011/2155 train_time:41944ms step_avg:41.49ms
step:1012/2155 train_time:42003ms step_avg:41.50ms
step:1013/2155 train_time:42063ms step_avg:41.52ms
step:1014/2155 train_time:42122ms step_avg:41.54ms
step:1015/2155 train_time:42184ms step_avg:41.56ms
step:1016/2155 train_time:42243ms step_avg:41.58ms
step:1017/2155 train_time:42306ms step_avg:41.60ms
step:1018/2155 train_time:42368ms step_avg:41.62ms
step:1019/2155 train_time:42431ms step_avg:41.64ms
step:1020/2155 train_time:42490ms step_avg:41.66ms
step:1021/2155 train_time:42552ms step_avg:41.68ms
step:1022/2155 train_time:42612ms step_avg:41.69ms
step:1023/2155 train_time:42673ms step_avg:41.71ms
step:1024/2155 train_time:42733ms step_avg:41.73ms
step:1025/2155 train_time:42794ms step_avg:41.75ms
step:1026/2155 train_time:42853ms step_avg:41.77ms
step:1027/2155 train_time:42914ms step_avg:41.79ms
step:1028/2155 train_time:42973ms step_avg:41.80ms
step:1029/2155 train_time:43033ms step_avg:41.82ms
step:1030/2155 train_time:43093ms step_avg:41.84ms
step:1031/2155 train_time:43154ms step_avg:41.86ms
step:1032/2155 train_time:43215ms step_avg:41.88ms
step:1033/2155 train_time:43277ms step_avg:41.89ms
step:1034/2155 train_time:43337ms step_avg:41.91ms
step:1035/2155 train_time:43399ms step_avg:41.93ms
step:1036/2155 train_time:43460ms step_avg:41.95ms
step:1037/2155 train_time:43523ms step_avg:41.97ms
step:1038/2155 train_time:43582ms step_avg:41.99ms
step:1039/2155 train_time:43643ms step_avg:42.00ms
step:1040/2155 train_time:43703ms step_avg:42.02ms
step:1041/2155 train_time:43764ms step_avg:42.04ms
step:1042/2155 train_time:43822ms step_avg:42.06ms
step:1043/2155 train_time:43884ms step_avg:42.07ms
step:1044/2155 train_time:43942ms step_avg:42.09ms
step:1045/2155 train_time:44003ms step_avg:42.11ms
step:1046/2155 train_time:44062ms step_avg:42.12ms
step:1047/2155 train_time:44124ms step_avg:42.14ms
step:1048/2155 train_time:44183ms step_avg:42.16ms
step:1049/2155 train_time:44244ms step_avg:42.18ms
step:1050/2155 train_time:44304ms step_avg:42.19ms
step:1051/2155 train_time:44366ms step_avg:42.21ms
step:1052/2155 train_time:44426ms step_avg:42.23ms
step:1053/2155 train_time:44488ms step_avg:42.25ms
step:1054/2155 train_time:44547ms step_avg:42.26ms
step:1055/2155 train_time:44609ms step_avg:42.28ms
step:1056/2155 train_time:44668ms step_avg:42.30ms
step:1057/2155 train_time:44729ms step_avg:42.32ms
step:1058/2155 train_time:44788ms step_avg:42.33ms
step:1059/2155 train_time:44849ms step_avg:42.35ms
step:1060/2155 train_time:44909ms step_avg:42.37ms
step:1061/2155 train_time:44969ms step_avg:42.38ms
step:1062/2155 train_time:45030ms step_avg:42.40ms
step:1063/2155 train_time:45091ms step_avg:42.42ms
step:1064/2155 train_time:45151ms step_avg:42.44ms
step:1065/2155 train_time:45213ms step_avg:42.45ms
step:1066/2155 train_time:45272ms step_avg:42.47ms
step:1067/2155 train_time:45334ms step_avg:42.49ms
step:1068/2155 train_time:45394ms step_avg:42.50ms
step:1069/2155 train_time:45456ms step_avg:42.52ms
step:1070/2155 train_time:45516ms step_avg:42.54ms
step:1071/2155 train_time:45578ms step_avg:42.56ms
step:1072/2155 train_time:45638ms step_avg:42.57ms
step:1073/2155 train_time:45699ms step_avg:42.59ms
step:1074/2155 train_time:45759ms step_avg:42.61ms
step:1075/2155 train_time:45820ms step_avg:42.62ms
step:1076/2155 train_time:45881ms step_avg:42.64ms
step:1077/2155 train_time:45942ms step_avg:42.66ms
step:1078/2155 train_time:46001ms step_avg:42.67ms
step:1079/2155 train_time:46062ms step_avg:42.69ms
step:1080/2155 train_time:46122ms step_avg:42.71ms
step:1081/2155 train_time:46183ms step_avg:42.72ms
step:1082/2155 train_time:46243ms step_avg:42.74ms
step:1083/2155 train_time:46304ms step_avg:42.76ms
step:1084/2155 train_time:46363ms step_avg:42.77ms
step:1085/2155 train_time:46424ms step_avg:42.79ms
step:1086/2155 train_time:46484ms step_avg:42.80ms
step:1087/2155 train_time:46545ms step_avg:42.82ms
step:1088/2155 train_time:46605ms step_avg:42.84ms
step:1089/2155 train_time:46666ms step_avg:42.85ms
step:1090/2155 train_time:46726ms step_avg:42.87ms
step:1091/2155 train_time:46787ms step_avg:42.88ms
step:1092/2155 train_time:46846ms step_avg:42.90ms
step:1093/2155 train_time:46907ms step_avg:42.92ms
step:1094/2155 train_time:46966ms step_avg:42.93ms
step:1095/2155 train_time:47027ms step_avg:42.95ms
step:1096/2155 train_time:47086ms step_avg:42.96ms
step:1097/2155 train_time:47147ms step_avg:42.98ms
step:1098/2155 train_time:47207ms step_avg:42.99ms
step:1099/2155 train_time:47268ms step_avg:43.01ms
step:1100/2155 train_time:47328ms step_avg:43.03ms
step:1101/2155 train_time:47389ms step_avg:43.04ms
step:1102/2155 train_time:47449ms step_avg:43.06ms
step:1103/2155 train_time:47509ms step_avg:43.07ms
step:1104/2155 train_time:47569ms step_avg:43.09ms
step:1105/2155 train_time:47631ms step_avg:43.10ms
step:1106/2155 train_time:47690ms step_avg:43.12ms
step:1107/2155 train_time:47752ms step_avg:43.14ms
step:1108/2155 train_time:47812ms step_avg:43.15ms
step:1109/2155 train_time:47874ms step_avg:43.17ms
step:1110/2155 train_time:47934ms step_avg:43.18ms
step:1111/2155 train_time:47994ms step_avg:43.20ms
step:1112/2155 train_time:48054ms step_avg:43.21ms
step:1113/2155 train_time:48115ms step_avg:43.23ms
step:1114/2155 train_time:48175ms step_avg:43.25ms
step:1115/2155 train_time:48237ms step_avg:43.26ms
step:1116/2155 train_time:48297ms step_avg:43.28ms
step:1117/2155 train_time:48359ms step_avg:43.29ms
step:1118/2155 train_time:48419ms step_avg:43.31ms
step:1119/2155 train_time:48481ms step_avg:43.33ms
step:1120/2155 train_time:48541ms step_avg:43.34ms
step:1121/2155 train_time:48603ms step_avg:43.36ms
step:1122/2155 train_time:48662ms step_avg:43.37ms
step:1123/2155 train_time:48724ms step_avg:43.39ms
step:1124/2155 train_time:48784ms step_avg:43.40ms
step:1125/2155 train_time:48845ms step_avg:43.42ms
step:1126/2155 train_time:48904ms step_avg:43.43ms
step:1127/2155 train_time:48965ms step_avg:43.45ms
step:1128/2155 train_time:49024ms step_avg:43.46ms
step:1129/2155 train_time:49085ms step_avg:43.48ms
step:1130/2155 train_time:49144ms step_avg:43.49ms
step:1131/2155 train_time:49205ms step_avg:43.51ms
step:1132/2155 train_time:49265ms step_avg:43.52ms
step:1133/2155 train_time:49326ms step_avg:43.54ms
step:1134/2155 train_time:49386ms step_avg:43.55ms
step:1135/2155 train_time:49447ms step_avg:43.57ms
step:1136/2155 train_time:49506ms step_avg:43.58ms
step:1137/2155 train_time:49567ms step_avg:43.59ms
step:1138/2155 train_time:49627ms step_avg:43.61ms
step:1139/2155 train_time:49688ms step_avg:43.62ms
step:1140/2155 train_time:49748ms step_avg:43.64ms
step:1141/2155 train_time:49808ms step_avg:43.65ms
step:1142/2155 train_time:49867ms step_avg:43.67ms
step:1143/2155 train_time:49928ms step_avg:43.68ms
step:1144/2155 train_time:49987ms step_avg:43.70ms
step:1145/2155 train_time:50049ms step_avg:43.71ms
step:1146/2155 train_time:50108ms step_avg:43.72ms
step:1147/2155 train_time:50169ms step_avg:43.74ms
step:1148/2155 train_time:50229ms step_avg:43.75ms
step:1149/2155 train_time:50291ms step_avg:43.77ms
step:1150/2155 train_time:50351ms step_avg:43.78ms
step:1151/2155 train_time:50413ms step_avg:43.80ms
step:1152/2155 train_time:50473ms step_avg:43.81ms
step:1153/2155 train_time:50534ms step_avg:43.83ms
step:1154/2155 train_time:50594ms step_avg:43.84ms
step:1155/2155 train_time:50656ms step_avg:43.86ms
step:1156/2155 train_time:50715ms step_avg:43.87ms
step:1157/2155 train_time:50777ms step_avg:43.89ms
step:1158/2155 train_time:50837ms step_avg:43.90ms
step:1159/2155 train_time:50899ms step_avg:43.92ms
step:1160/2155 train_time:50959ms step_avg:43.93ms
step:1161/2155 train_time:51021ms step_avg:43.95ms
step:1162/2155 train_time:51080ms step_avg:43.96ms
step:1163/2155 train_time:51142ms step_avg:43.97ms
step:1164/2155 train_time:51202ms step_avg:43.99ms
step:1165/2155 train_time:51263ms step_avg:44.00ms
step:1166/2155 train_time:51323ms step_avg:44.02ms
step:1167/2155 train_time:51384ms step_avg:44.03ms
step:1168/2155 train_time:51444ms step_avg:44.04ms
step:1169/2155 train_time:51505ms step_avg:44.06ms
step:1170/2155 train_time:51565ms step_avg:44.07ms
step:1171/2155 train_time:51627ms step_avg:44.09ms
step:1172/2155 train_time:51687ms step_avg:44.10ms
step:1173/2155 train_time:51748ms step_avg:44.12ms
step:1174/2155 train_time:51808ms step_avg:44.13ms
step:1175/2155 train_time:51870ms step_avg:44.14ms
step:1176/2155 train_time:51930ms step_avg:44.16ms
step:1177/2155 train_time:51990ms step_avg:44.17ms
step:1178/2155 train_time:52050ms step_avg:44.19ms
step:1179/2155 train_time:52111ms step_avg:44.20ms
step:1180/2155 train_time:52170ms step_avg:44.21ms
step:1181/2155 train_time:52232ms step_avg:44.23ms
step:1182/2155 train_time:52291ms step_avg:44.24ms
step:1183/2155 train_time:52353ms step_avg:44.25ms
step:1184/2155 train_time:52413ms step_avg:44.27ms
step:1185/2155 train_time:52474ms step_avg:44.28ms
step:1186/2155 train_time:52534ms step_avg:44.30ms
step:1187/2155 train_time:52596ms step_avg:44.31ms
step:1188/2155 train_time:52657ms step_avg:44.32ms
step:1189/2155 train_time:52719ms step_avg:44.34ms
step:1190/2155 train_time:52779ms step_avg:44.35ms
step:1191/2155 train_time:52841ms step_avg:44.37ms
step:1192/2155 train_time:52901ms step_avg:44.38ms
step:1193/2155 train_time:52963ms step_avg:44.39ms
step:1194/2155 train_time:53022ms step_avg:44.41ms
step:1195/2155 train_time:53084ms step_avg:44.42ms
step:1196/2155 train_time:53144ms step_avg:44.43ms
step:1197/2155 train_time:53205ms step_avg:44.45ms
step:1198/2155 train_time:53264ms step_avg:44.46ms
step:1199/2155 train_time:53325ms step_avg:44.47ms
step:1200/2155 train_time:53385ms step_avg:44.49ms
step:1201/2155 train_time:53446ms step_avg:44.50ms
step:1202/2155 train_time:53505ms step_avg:44.51ms
step:1203/2155 train_time:53566ms step_avg:44.53ms
step:1204/2155 train_time:53626ms step_avg:44.54ms
step:1205/2155 train_time:53688ms step_avg:44.55ms
step:1206/2155 train_time:53748ms step_avg:44.57ms
step:1207/2155 train_time:53809ms step_avg:44.58ms
step:1208/2155 train_time:53868ms step_avg:44.59ms
step:1209/2155 train_time:53929ms step_avg:44.61ms
step:1210/2155 train_time:53989ms step_avg:44.62ms
step:1211/2155 train_time:54051ms step_avg:44.63ms
step:1212/2155 train_time:54111ms step_avg:44.65ms
step:1213/2155 train_time:54172ms step_avg:44.66ms
step:1214/2155 train_time:54232ms step_avg:44.67ms
step:1215/2155 train_time:54294ms step_avg:44.69ms
step:1216/2155 train_time:54354ms step_avg:44.70ms
step:1217/2155 train_time:54416ms step_avg:44.71ms
step:1218/2155 train_time:54476ms step_avg:44.73ms
step:1219/2155 train_time:54538ms step_avg:44.74ms
step:1220/2155 train_time:54598ms step_avg:44.75ms
step:1221/2155 train_time:54660ms step_avg:44.77ms
step:1222/2155 train_time:54720ms step_avg:44.78ms
step:1223/2155 train_time:54782ms step_avg:44.79ms
step:1224/2155 train_time:54842ms step_avg:44.81ms
step:1225/2155 train_time:54904ms step_avg:44.82ms
step:1226/2155 train_time:54963ms step_avg:44.83ms
step:1227/2155 train_time:55025ms step_avg:44.85ms
step:1228/2155 train_time:55084ms step_avg:44.86ms
step:1229/2155 train_time:55145ms step_avg:44.87ms
step:1230/2155 train_time:55204ms step_avg:44.88ms
step:1231/2155 train_time:55265ms step_avg:44.89ms
step:1232/2155 train_time:55324ms step_avg:44.91ms
step:1233/2155 train_time:55386ms step_avg:44.92ms
step:1234/2155 train_time:55445ms step_avg:44.93ms
step:1235/2155 train_time:55506ms step_avg:44.94ms
step:1236/2155 train_time:55566ms step_avg:44.96ms
step:1237/2155 train_time:55628ms step_avg:44.97ms
step:1238/2155 train_time:55687ms step_avg:44.98ms
step:1239/2155 train_time:55748ms step_avg:44.99ms
step:1240/2155 train_time:55808ms step_avg:45.01ms
step:1241/2155 train_time:55869ms step_avg:45.02ms
step:1242/2155 train_time:55929ms step_avg:45.03ms
step:1243/2155 train_time:55990ms step_avg:45.04ms
step:1244/2155 train_time:56050ms step_avg:45.06ms
step:1245/2155 train_time:56111ms step_avg:45.07ms
step:1246/2155 train_time:56171ms step_avg:45.08ms
step:1247/2155 train_time:56232ms step_avg:45.09ms
step:1248/2155 train_time:56292ms step_avg:45.11ms
step:1249/2155 train_time:56353ms step_avg:45.12ms
step:1250/2155 train_time:56413ms step_avg:45.13ms
step:1250/2155 val_loss:3.5974 train_time:56477ms step_avg:45.18ms
step:1251/2155 train_time:56499ms step_avg:45.16ms
step:1252/2155 train_time:56537ms step_avg:45.16ms
step:1253/2155 train_time:56601ms step_avg:45.17ms
step:1254/2155 train_time:56663ms step_avg:45.19ms
step:1255/2155 train_time:56725ms step_avg:45.20ms
step:1256/2155 train_time:56783ms step_avg:45.21ms
step:1257/2155 train_time:56844ms step_avg:45.22ms
step:1258/2155 train_time:56903ms step_avg:45.23ms
step:1259/2155 train_time:56964ms step_avg:45.25ms
step:1260/2155 train_time:57023ms step_avg:45.26ms
step:1261/2155 train_time:57084ms step_avg:45.27ms
step:1262/2155 train_time:57143ms step_avg:45.28ms
step:1263/2155 train_time:57204ms step_avg:45.29ms
step:1264/2155 train_time:57264ms step_avg:45.30ms
step:1265/2155 train_time:57326ms step_avg:45.32ms
step:1266/2155 train_time:57385ms step_avg:45.33ms
step:1267/2155 train_time:57449ms step_avg:45.34ms
step:1268/2155 train_time:57510ms step_avg:45.36ms
step:1269/2155 train_time:57574ms step_avg:45.37ms
step:1270/2155 train_time:57634ms step_avg:45.38ms
step:1271/2155 train_time:57696ms step_avg:45.39ms
step:1272/2155 train_time:57755ms step_avg:45.40ms
step:1273/2155 train_time:57816ms step_avg:45.42ms
step:1274/2155 train_time:57875ms step_avg:45.43ms
step:1275/2155 train_time:57936ms step_avg:45.44ms
step:1276/2155 train_time:57994ms step_avg:45.45ms
step:1277/2155 train_time:58055ms step_avg:45.46ms
step:1278/2155 train_time:58115ms step_avg:45.47ms
step:1279/2155 train_time:58176ms step_avg:45.49ms
step:1280/2155 train_time:58235ms step_avg:45.50ms
step:1281/2155 train_time:58295ms step_avg:45.51ms
step:1282/2155 train_time:58355ms step_avg:45.52ms
step:1283/2155 train_time:58417ms step_avg:45.53ms
step:1284/2155 train_time:58477ms step_avg:45.54ms
step:1285/2155 train_time:58539ms step_avg:45.56ms
step:1286/2155 train_time:58600ms step_avg:45.57ms
step:1287/2155 train_time:58661ms step_avg:45.58ms
step:1288/2155 train_time:58720ms step_avg:45.59ms
step:1289/2155 train_time:58781ms step_avg:45.60ms
step:1290/2155 train_time:58841ms step_avg:45.61ms
step:1291/2155 train_time:58901ms step_avg:45.62ms
step:1292/2155 train_time:58961ms step_avg:45.64ms
step:1293/2155 train_time:59022ms step_avg:45.65ms
step:1294/2155 train_time:59081ms step_avg:45.66ms
step:1295/2155 train_time:59142ms step_avg:45.67ms
step:1296/2155 train_time:59201ms step_avg:45.68ms
step:1297/2155 train_time:59262ms step_avg:45.69ms
step:1298/2155 train_time:59322ms step_avg:45.70ms
step:1299/2155 train_time:59383ms step_avg:45.71ms
step:1300/2155 train_time:59443ms step_avg:45.73ms
step:1301/2155 train_time:59506ms step_avg:45.74ms
step:1302/2155 train_time:59567ms step_avg:45.75ms
step:1303/2155 train_time:59629ms step_avg:45.76ms
step:1304/2155 train_time:59689ms step_avg:45.77ms
step:1305/2155 train_time:59751ms step_avg:45.79ms
step:1306/2155 train_time:59811ms step_avg:45.80ms
step:1307/2155 train_time:59873ms step_avg:45.81ms
step:1308/2155 train_time:59933ms step_avg:45.82ms
step:1309/2155 train_time:59994ms step_avg:45.83ms
step:1310/2155 train_time:60054ms step_avg:45.84ms
step:1311/2155 train_time:60116ms step_avg:45.85ms
step:1312/2155 train_time:60175ms step_avg:45.86ms
step:1313/2155 train_time:60236ms step_avg:45.88ms
step:1314/2155 train_time:60294ms step_avg:45.89ms
step:1315/2155 train_time:60356ms step_avg:45.90ms
step:1316/2155 train_time:60415ms step_avg:45.91ms
step:1317/2155 train_time:60477ms step_avg:45.92ms
step:1318/2155 train_time:60537ms step_avg:45.93ms
step:1319/2155 train_time:60598ms step_avg:45.94ms
step:1320/2155 train_time:60658ms step_avg:45.95ms
step:1321/2155 train_time:60719ms step_avg:45.96ms
step:1322/2155 train_time:60778ms step_avg:45.97ms
step:1323/2155 train_time:60839ms step_avg:45.99ms
step:1324/2155 train_time:60899ms step_avg:46.00ms
step:1325/2155 train_time:60959ms step_avg:46.01ms
step:1326/2155 train_time:61019ms step_avg:46.02ms
step:1327/2155 train_time:61079ms step_avg:46.03ms
step:1328/2155 train_time:61139ms step_avg:46.04ms
step:1329/2155 train_time:61199ms step_avg:46.05ms
step:1330/2155 train_time:61259ms step_avg:46.06ms
step:1331/2155 train_time:61320ms step_avg:46.07ms
step:1332/2155 train_time:61379ms step_avg:46.08ms
step:1333/2155 train_time:61440ms step_avg:46.09ms
step:1334/2155 train_time:61501ms step_avg:46.10ms
step:1335/2155 train_time:61561ms step_avg:46.11ms
step:1336/2155 train_time:61621ms step_avg:46.12ms
step:1337/2155 train_time:61683ms step_avg:46.14ms
step:1338/2155 train_time:61743ms step_avg:46.15ms
step:1339/2155 train_time:61806ms step_avg:46.16ms
step:1340/2155 train_time:61866ms step_avg:46.17ms
step:1341/2155 train_time:61927ms step_avg:46.18ms
step:1342/2155 train_time:61987ms step_avg:46.19ms
step:1343/2155 train_time:62049ms step_avg:46.20ms
step:1344/2155 train_time:62109ms step_avg:46.21ms
step:1345/2155 train_time:62171ms step_avg:46.22ms
step:1346/2155 train_time:62230ms step_avg:46.23ms
step:1347/2155 train_time:62292ms step_avg:46.24ms
step:1348/2155 train_time:62352ms step_avg:46.25ms
step:1349/2155 train_time:62414ms step_avg:46.27ms
step:1350/2155 train_time:62474ms step_avg:46.28ms
step:1351/2155 train_time:62536ms step_avg:46.29ms
step:1352/2155 train_time:62595ms step_avg:46.30ms
step:1353/2155 train_time:62656ms step_avg:46.31ms
step:1354/2155 train_time:62716ms step_avg:46.32ms
step:1355/2155 train_time:62778ms step_avg:46.33ms
step:1356/2155 train_time:62837ms step_avg:46.34ms
step:1357/2155 train_time:62898ms step_avg:46.35ms
step:1358/2155 train_time:62958ms step_avg:46.36ms
step:1359/2155 train_time:63019ms step_avg:46.37ms
step:1360/2155 train_time:63078ms step_avg:46.38ms
step:1361/2155 train_time:63139ms step_avg:46.39ms
step:1362/2155 train_time:63199ms step_avg:46.40ms
step:1363/2155 train_time:63260ms step_avg:46.41ms
step:1364/2155 train_time:63319ms step_avg:46.42ms
step:1365/2155 train_time:63380ms step_avg:46.43ms
step:1366/2155 train_time:63440ms step_avg:46.44ms
step:1367/2155 train_time:63500ms step_avg:46.45ms
step:1368/2155 train_time:63560ms step_avg:46.46ms
step:1369/2155 train_time:63622ms step_avg:46.47ms
step:1370/2155 train_time:63682ms step_avg:46.48ms
step:1371/2155 train_time:63742ms step_avg:46.49ms
step:1372/2155 train_time:63802ms step_avg:46.50ms
step:1373/2155 train_time:63863ms step_avg:46.51ms
step:1374/2155 train_time:63923ms step_avg:46.52ms
step:1375/2155 train_time:63985ms step_avg:46.53ms
step:1376/2155 train_time:64045ms step_avg:46.54ms
step:1377/2155 train_time:64107ms step_avg:46.56ms
step:1378/2155 train_time:64168ms step_avg:46.57ms
step:1379/2155 train_time:64230ms step_avg:46.58ms
step:1380/2155 train_time:64289ms step_avg:46.59ms
step:1381/2155 train_time:64351ms step_avg:46.60ms
step:1382/2155 train_time:64411ms step_avg:46.61ms
step:1383/2155 train_time:64473ms step_avg:46.62ms
step:1384/2155 train_time:64533ms step_avg:46.63ms
step:1385/2155 train_time:64594ms step_avg:46.64ms
step:1386/2155 train_time:64654ms step_avg:46.65ms
step:1387/2155 train_time:64716ms step_avg:46.66ms
step:1388/2155 train_time:64776ms step_avg:46.67ms
step:1389/2155 train_time:64837ms step_avg:46.68ms
step:1390/2155 train_time:64896ms step_avg:46.69ms
step:1391/2155 train_time:64958ms step_avg:46.70ms
step:1392/2155 train_time:65017ms step_avg:46.71ms
step:1393/2155 train_time:65079ms step_avg:46.72ms
step:1394/2155 train_time:65139ms step_avg:46.73ms
step:1395/2155 train_time:65201ms step_avg:46.74ms
step:1396/2155 train_time:65261ms step_avg:46.75ms
step:1397/2155 train_time:65322ms step_avg:46.76ms
step:1398/2155 train_time:65382ms step_avg:46.77ms
step:1399/2155 train_time:65443ms step_avg:46.78ms
step:1400/2155 train_time:65503ms step_avg:46.79ms
step:1401/2155 train_time:65564ms step_avg:46.80ms
step:1402/2155 train_time:65624ms step_avg:46.81ms
step:1403/2155 train_time:65686ms step_avg:46.82ms
step:1404/2155 train_time:65746ms step_avg:46.83ms
step:1405/2155 train_time:65807ms step_avg:46.84ms
step:1406/2155 train_time:65867ms step_avg:46.85ms
step:1407/2155 train_time:65929ms step_avg:46.86ms
step:1408/2155 train_time:65989ms step_avg:46.87ms
step:1409/2155 train_time:66051ms step_avg:46.88ms
step:1410/2155 train_time:66111ms step_avg:46.89ms
step:1411/2155 train_time:66172ms step_avg:46.90ms
step:1412/2155 train_time:66260ms step_avg:46.93ms
step:1413/2155 train_time:66351ms step_avg:46.96ms
step:1414/2155 train_time:66439ms step_avg:46.99ms
step:1415/2155 train_time:66529ms step_avg:47.02ms
step:1416/2155 train_time:66617ms step_avg:47.05ms
step:1417/2155 train_time:66707ms step_avg:47.08ms
step:1418/2155 train_time:66795ms step_avg:47.10ms
step:1419/2155 train_time:66885ms step_avg:47.14ms
step:1420/2155 train_time:66972ms step_avg:47.16ms
step:1421/2155 train_time:67062ms step_avg:47.19ms
step:1422/2155 train_time:67149ms step_avg:47.22ms
step:1423/2155 train_time:67239ms step_avg:47.25ms
step:1424/2155 train_time:67328ms step_avg:47.28ms
step:1425/2155 train_time:67417ms step_avg:47.31ms
step:1426/2155 train_time:67506ms step_avg:47.34ms
step:1427/2155 train_time:67595ms step_avg:47.37ms
step:1428/2155 train_time:67683ms step_avg:47.40ms
step:1429/2155 train_time:67773ms step_avg:47.43ms
step:1430/2155 train_time:67861ms step_avg:47.46ms
step:1431/2155 train_time:67951ms step_avg:47.48ms
step:1432/2155 train_time:68039ms step_avg:47.51ms
step:1433/2155 train_time:68128ms step_avg:47.54ms
step:1434/2155 train_time:68215ms step_avg:47.57ms
step:1435/2155 train_time:68306ms step_avg:47.60ms
step:1436/2155 train_time:68394ms step_avg:47.63ms
step:1437/2155 train_time:68485ms step_avg:47.66ms
step:1438/2155 train_time:68573ms step_avg:47.69ms
step:1439/2155 train_time:68663ms step_avg:47.72ms
step:1440/2155 train_time:68751ms step_avg:47.74ms
step:1441/2155 train_time:68841ms step_avg:47.77ms
step:1442/2155 train_time:68928ms step_avg:47.80ms
step:1443/2155 train_time:69018ms step_avg:47.83ms
step:1444/2155 train_time:69106ms step_avg:47.86ms
step:1445/2155 train_time:69195ms step_avg:47.89ms
step:1446/2155 train_time:69284ms step_avg:47.91ms
step:1447/2155 train_time:69373ms step_avg:47.94ms
step:1448/2155 train_time:69462ms step_avg:47.97ms
step:1449/2155 train_time:69551ms step_avg:48.00ms
step:1450/2155 train_time:69640ms step_avg:48.03ms
step:1451/2155 train_time:69729ms step_avg:48.06ms
step:1452/2155 train_time:69817ms step_avg:48.08ms
step:1453/2155 train_time:69906ms step_avg:48.11ms
step:1454/2155 train_time:69995ms step_avg:48.14ms
step:1455/2155 train_time:70085ms step_avg:48.17ms
step:1456/2155 train_time:70172ms step_avg:48.20ms
step:1457/2155 train_time:70261ms step_avg:48.22ms
step:1458/2155 train_time:70349ms step_avg:48.25ms
step:1459/2155 train_time:70439ms step_avg:48.28ms
step:1460/2155 train_time:70527ms step_avg:48.31ms
step:1461/2155 train_time:70616ms step_avg:48.33ms
step:1462/2155 train_time:70704ms step_avg:48.36ms
step:1463/2155 train_time:70794ms step_avg:48.39ms
step:1464/2155 train_time:70882ms step_avg:48.42ms
step:1465/2155 train_time:70971ms step_avg:48.44ms
step:1466/2155 train_time:71060ms step_avg:48.47ms
step:1467/2155 train_time:71150ms step_avg:48.50ms
step:1468/2155 train_time:71238ms step_avg:48.53ms
step:1469/2155 train_time:71328ms step_avg:48.56ms
step:1470/2155 train_time:71416ms step_avg:48.58ms
step:1471/2155 train_time:71506ms step_avg:48.61ms
step:1472/2155 train_time:71593ms step_avg:48.64ms
step:1473/2155 train_time:71683ms step_avg:48.66ms
step:1474/2155 train_time:71770ms step_avg:48.69ms
step:1475/2155 train_time:71860ms step_avg:48.72ms
step:1476/2155 train_time:71948ms step_avg:48.75ms
step:1477/2155 train_time:72037ms step_avg:48.77ms
step:1478/2155 train_time:72126ms step_avg:48.80ms
step:1479/2155 train_time:72214ms step_avg:48.83ms
step:1480/2155 train_time:72302ms step_avg:48.85ms
step:1481/2155 train_time:72391ms step_avg:48.88ms
step:1482/2155 train_time:72480ms step_avg:48.91ms
step:1483/2155 train_time:72569ms step_avg:48.93ms
step:1484/2155 train_time:72658ms step_avg:48.96ms
step:1485/2155 train_time:72747ms step_avg:48.99ms
step:1486/2155 train_time:72835ms step_avg:49.01ms
step:1487/2155 train_time:72924ms step_avg:49.04ms
step:1488/2155 train_time:73012ms step_avg:49.07ms
step:1489/2155 train_time:73103ms step_avg:49.10ms
step:1490/2155 train_time:73190ms step_avg:49.12ms
step:1491/2155 train_time:73280ms step_avg:49.15ms
step:1492/2155 train_time:73367ms step_avg:49.17ms
step:1493/2155 train_time:73457ms step_avg:49.20ms
step:1494/2155 train_time:73545ms step_avg:49.23ms
step:1495/2155 train_time:73634ms step_avg:49.25ms
step:1496/2155 train_time:73722ms step_avg:49.28ms
step:1497/2155 train_time:73811ms step_avg:49.31ms
step:1498/2155 train_time:73899ms step_avg:49.33ms
step:1499/2155 train_time:73990ms step_avg:49.36ms
step:1500/2155 train_time:74079ms step_avg:49.39ms
step:1500/2155 val_loss:3.4929 train_time:74170ms step_avg:49.45ms
step:1501/2155 train_time:74192ms step_avg:49.43ms
step:1502/2155 train_time:74256ms step_avg:49.44ms
step:1503/2155 train_time:74346ms step_avg:49.47ms
step:1504/2155 train_time:74438ms step_avg:49.49ms
step:1505/2155 train_time:74525ms step_avg:49.52ms
step:1506/2155 train_time:74611ms step_avg:49.54ms
step:1507/2155 train_time:74699ms step_avg:49.57ms
step:1508/2155 train_time:74785ms step_avg:49.59ms
step:1509/2155 train_time:74873ms step_avg:49.62ms
step:1510/2155 train_time:74959ms step_avg:49.64ms
step:1511/2155 train_time:75048ms step_avg:49.67ms
step:1512/2155 train_time:75144ms step_avg:49.70ms
step:1513/2155 train_time:75235ms step_avg:49.73ms
step:1514/2155 train_time:75324ms step_avg:49.75ms
step:1515/2155 train_time:75414ms step_avg:49.78ms
step:1516/2155 train_time:75501ms step_avg:49.80ms
step:1517/2155 train_time:75590ms step_avg:49.83ms
step:1518/2155 train_time:75676ms step_avg:49.85ms
step:1519/2155 train_time:75764ms step_avg:49.88ms
step:1520/2155 train_time:75850ms step_avg:49.90ms
step:1521/2155 train_time:75938ms step_avg:49.93ms
step:1522/2155 train_time:76025ms step_avg:49.95ms
step:1523/2155 train_time:76117ms step_avg:49.98ms
step:1524/2155 train_time:76206ms step_avg:50.00ms
step:1525/2155 train_time:76299ms step_avg:50.03ms
step:1526/2155 train_time:76387ms step_avg:50.06ms
step:1527/2155 train_time:76476ms step_avg:50.08ms
step:1528/2155 train_time:76562ms step_avg:50.11ms
step:1529/2155 train_time:76650ms step_avg:50.13ms
step:1530/2155 train_time:76737ms step_avg:50.15ms
step:1531/2155 train_time:76825ms step_avg:50.18ms
step:1532/2155 train_time:76912ms step_avg:50.20ms
step:1533/2155 train_time:77001ms step_avg:50.23ms
step:1534/2155 train_time:77089ms step_avg:50.25ms
step:1535/2155 train_time:77180ms step_avg:50.28ms
step:1536/2155 train_time:77269ms step_avg:50.31ms
step:1537/2155 train_time:77359ms step_avg:50.33ms
step:1538/2155 train_time:77447ms step_avg:50.36ms
step:1539/2155 train_time:77536ms step_avg:50.38ms
step:1540/2155 train_time:77623ms step_avg:50.40ms
step:1541/2155 train_time:77711ms step_avg:50.43ms
step:1542/2155 train_time:77798ms step_avg:50.45ms
step:1543/2155 train_time:77886ms step_avg:50.48ms
step:1544/2155 train_time:77974ms step_avg:50.50ms
step:1545/2155 train_time:78063ms step_avg:50.53ms
step:1546/2155 train_time:78152ms step_avg:50.55ms
step:1547/2155 train_time:78242ms step_avg:50.58ms
step:1548/2155 train_time:78330ms step_avg:50.60ms
step:1549/2155 train_time:78419ms step_avg:50.63ms
step:1550/2155 train_time:78507ms step_avg:50.65ms
step:1551/2155 train_time:78596ms step_avg:50.67ms
step:1552/2155 train_time:78683ms step_avg:50.70ms
step:1553/2155 train_time:78771ms step_avg:50.72ms
step:1554/2155 train_time:78859ms step_avg:50.75ms
step:1555/2155 train_time:78948ms step_avg:50.77ms
step:1556/2155 train_time:79036ms step_avg:50.79ms
step:1557/2155 train_time:79126ms step_avg:50.82ms
step:1558/2155 train_time:79214ms step_avg:50.84ms
step:1559/2155 train_time:79303ms step_avg:50.87ms
step:1560/2155 train_time:79391ms step_avg:50.89ms
step:1561/2155 train_time:79480ms step_avg:50.92ms
step:1562/2155 train_time:79569ms step_avg:50.94ms
step:1563/2155 train_time:79658ms step_avg:50.96ms
step:1564/2155 train_time:79745ms step_avg:50.99ms
step:1565/2155 train_time:79835ms step_avg:51.01ms
step:1566/2155 train_time:79922ms step_avg:51.04ms
step:1567/2155 train_time:80011ms step_avg:51.06ms
step:1568/2155 train_time:80099ms step_avg:51.08ms
step:1569/2155 train_time:80189ms step_avg:51.11ms
step:1570/2155 train_time:80277ms step_avg:51.13ms
step:1571/2155 train_time:80367ms step_avg:51.16ms
step:1572/2155 train_time:80456ms step_avg:51.18ms
step:1573/2155 train_time:80544ms step_avg:51.20ms
step:1574/2155 train_time:80632ms step_avg:51.23ms
step:1575/2155 train_time:80722ms step_avg:51.25ms
step:1576/2155 train_time:80809ms step_avg:51.27ms
step:1577/2155 train_time:80899ms step_avg:51.30ms
step:1578/2155 train_time:80986ms step_avg:51.32ms
step:1579/2155 train_time:81075ms step_avg:51.35ms
step:1580/2155 train_time:81164ms step_avg:51.37ms
step:1581/2155 train_time:81252ms step_avg:51.39ms
step:1582/2155 train_time:81341ms step_avg:51.42ms
step:1583/2155 train_time:81431ms step_avg:51.44ms
step:1584/2155 train_time:81518ms step_avg:51.46ms
step:1585/2155 train_time:81607ms step_avg:51.49ms
step:1586/2155 train_time:81695ms step_avg:51.51ms
step:1587/2155 train_time:81784ms step_avg:51.53ms
step:1588/2155 train_time:81873ms step_avg:51.56ms
step:1589/2155 train_time:81961ms step_avg:51.58ms
step:1590/2155 train_time:82049ms step_avg:51.60ms
step:1591/2155 train_time:82139ms step_avg:51.63ms
step:1592/2155 train_time:82226ms step_avg:51.65ms
step:1593/2155 train_time:82316ms step_avg:51.67ms
step:1594/2155 train_time:82405ms step_avg:51.70ms
step:1595/2155 train_time:82493ms step_avg:51.72ms
step:1596/2155 train_time:82581ms step_avg:51.74ms
step:1597/2155 train_time:82670ms step_avg:51.77ms
step:1598/2155 train_time:82758ms step_avg:51.79ms
step:1599/2155 train_time:82846ms step_avg:51.81ms
step:1600/2155 train_time:82935ms step_avg:51.83ms
step:1601/2155 train_time:83024ms step_avg:51.86ms
step:1602/2155 train_time:83113ms step_avg:51.88ms
step:1603/2155 train_time:83202ms step_avg:51.90ms
step:1604/2155 train_time:83290ms step_avg:51.93ms
step:1605/2155 train_time:83381ms step_avg:51.95ms
step:1606/2155 train_time:83468ms step_avg:51.97ms
step:1607/2155 train_time:83558ms step_avg:52.00ms
step:1608/2155 train_time:83645ms step_avg:52.02ms
step:1609/2155 train_time:83734ms step_avg:52.04ms
step:1610/2155 train_time:83822ms step_avg:52.06ms
step:1611/2155 train_time:83911ms step_avg:52.09ms
step:1612/2155 train_time:83999ms step_avg:52.11ms
step:1613/2155 train_time:84088ms step_avg:52.13ms
step:1614/2155 train_time:84176ms step_avg:52.15ms
step:1615/2155 train_time:84266ms step_avg:52.18ms
step:1616/2155 train_time:84354ms step_avg:52.20ms
step:1617/2155 train_time:84443ms step_avg:52.22ms
step:1618/2155 train_time:84532ms step_avg:52.24ms
step:1619/2155 train_time:84621ms step_avg:52.27ms
step:1620/2155 train_time:84710ms step_avg:52.29ms
step:1621/2155 train_time:84799ms step_avg:52.31ms
step:1622/2155 train_time:84886ms step_avg:52.33ms
step:1623/2155 train_time:84975ms step_avg:52.36ms
step:1624/2155 train_time:85062ms step_avg:52.38ms
step:1625/2155 train_time:85151ms step_avg:52.40ms
step:1626/2155 train_time:85239ms step_avg:52.42ms
step:1627/2155 train_time:85329ms step_avg:52.45ms
step:1628/2155 train_time:85417ms step_avg:52.47ms
step:1629/2155 train_time:85506ms step_avg:52.49ms
step:1630/2155 train_time:85594ms step_avg:52.51ms
step:1631/2155 train_time:85684ms step_avg:52.53ms
step:1632/2155 train_time:85771ms step_avg:52.56ms
step:1633/2155 train_time:85860ms step_avg:52.58ms
step:1634/2155 train_time:85948ms step_avg:52.60ms
step:1635/2155 train_time:86037ms step_avg:52.62ms
step:1636/2155 train_time:86124ms step_avg:52.64ms
step:1637/2155 train_time:86214ms step_avg:52.67ms
step:1638/2155 train_time:86302ms step_avg:52.69ms
step:1639/2155 train_time:86392ms step_avg:52.71ms
step:1640/2155 train_time:86480ms step_avg:52.73ms
step:1641/2155 train_time:86568ms step_avg:52.75ms
step:1642/2155 train_time:86657ms step_avg:52.78ms
step:1643/2155 train_time:86745ms step_avg:52.80ms
step:1644/2155 train_time:86833ms step_avg:52.82ms
step:1645/2155 train_time:86922ms step_avg:52.84ms
step:1646/2155 train_time:87011ms step_avg:52.86ms
step:1647/2155 train_time:87100ms step_avg:52.88ms
step:1648/2155 train_time:87189ms step_avg:52.91ms
step:1649/2155 train_time:87278ms step_avg:52.93ms
step:1650/2155 train_time:87365ms step_avg:52.95ms
step:1651/2155 train_time:87456ms step_avg:52.97ms
step:1652/2155 train_time:87544ms step_avg:52.99ms
step:1653/2155 train_time:87632ms step_avg:53.01ms
step:1654/2155 train_time:87722ms step_avg:53.04ms
step:1655/2155 train_time:87810ms step_avg:53.06ms
step:1656/2155 train_time:87898ms step_avg:53.08ms
step:1657/2155 train_time:87987ms step_avg:53.10ms
step:1658/2155 train_time:88076ms step_avg:53.12ms
step:1659/2155 train_time:88165ms step_avg:53.14ms
step:1660/2155 train_time:88254ms step_avg:53.16ms
step:1661/2155 train_time:88344ms step_avg:53.19ms
step:1662/2155 train_time:88433ms step_avg:53.21ms
step:1663/2155 train_time:88524ms step_avg:53.23ms
step:1664/2155 train_time:88613ms step_avg:53.25ms
step:1665/2155 train_time:88703ms step_avg:53.28ms
step:1666/2155 train_time:88791ms step_avg:53.30ms
step:1667/2155 train_time:88880ms step_avg:53.32ms
step:1668/2155 train_time:88968ms step_avg:53.34ms
step:1669/2155 train_time:89059ms step_avg:53.36ms
step:1670/2155 train_time:89146ms step_avg:53.38ms
step:1671/2155 train_time:89236ms step_avg:53.40ms
step:1672/2155 train_time:89324ms step_avg:53.42ms
step:1673/2155 train_time:89413ms step_avg:53.44ms
step:1674/2155 train_time:89502ms step_avg:53.47ms
step:1675/2155 train_time:89592ms step_avg:53.49ms
step:1676/2155 train_time:89680ms step_avg:53.51ms
step:1677/2155 train_time:89769ms step_avg:53.53ms
step:1678/2155 train_time:89857ms step_avg:53.55ms
step:1679/2155 train_time:89947ms step_avg:53.57ms
step:1680/2155 train_time:90035ms step_avg:53.59ms
step:1681/2155 train_time:90125ms step_avg:53.61ms
step:1682/2155 train_time:90212ms step_avg:53.63ms
step:1683/2155 train_time:90302ms step_avg:53.66ms
step:1684/2155 train_time:90390ms step_avg:53.68ms
step:1685/2155 train_time:90480ms step_avg:53.70ms
step:1686/2155 train_time:90567ms step_avg:53.72ms
step:1687/2155 train_time:90656ms step_avg:53.74ms
step:1688/2155 train_time:90744ms step_avg:53.76ms
step:1689/2155 train_time:90834ms step_avg:53.78ms
step:1690/2155 train_time:90922ms step_avg:53.80ms
step:1691/2155 train_time:91012ms step_avg:53.82ms
step:1692/2155 train_time:91100ms step_avg:53.84ms
step:1693/2155 train_time:91190ms step_avg:53.86ms
step:1694/2155 train_time:91277ms step_avg:53.88ms
step:1695/2155 train_time:91366ms step_avg:53.90ms
step:1696/2155 train_time:91455ms step_avg:53.92ms
step:1697/2155 train_time:91544ms step_avg:53.94ms
step:1698/2155 train_time:91631ms step_avg:53.96ms
step:1699/2155 train_time:91721ms step_avg:53.99ms
step:1700/2155 train_time:91809ms step_avg:54.01ms
step:1701/2155 train_time:91898ms step_avg:54.03ms
step:1702/2155 train_time:91989ms step_avg:54.05ms
step:1703/2155 train_time:92074ms step_avg:54.07ms
step:1704/2155 train_time:92162ms step_avg:54.09ms
step:1705/2155 train_time:92251ms step_avg:54.11ms
step:1706/2155 train_time:92340ms step_avg:54.13ms
step:1707/2155 train_time:92429ms step_avg:54.15ms
step:1708/2155 train_time:92517ms step_avg:54.17ms
step:1709/2155 train_time:92606ms step_avg:54.19ms
step:1710/2155 train_time:92693ms step_avg:54.21ms
step:1711/2155 train_time:92782ms step_avg:54.23ms
step:1712/2155 train_time:92869ms step_avg:54.25ms
step:1713/2155 train_time:92959ms step_avg:54.27ms
step:1714/2155 train_time:93046ms step_avg:54.29ms
step:1715/2155 train_time:93136ms step_avg:54.31ms
step:1716/2155 train_time:93223ms step_avg:54.33ms
step:1717/2155 train_time:93313ms step_avg:54.35ms
step:1718/2155 train_time:93401ms step_avg:54.37ms
step:1719/2155 train_time:93490ms step_avg:54.39ms
step:1720/2155 train_time:93578ms step_avg:54.41ms
step:1721/2155 train_time:93667ms step_avg:54.43ms
step:1722/2155 train_time:93756ms step_avg:54.45ms
step:1723/2155 train_time:93845ms step_avg:54.47ms
step:1724/2155 train_time:93932ms step_avg:54.49ms
step:1725/2155 train_time:94022ms step_avg:54.51ms
step:1726/2155 train_time:94109ms step_avg:54.52ms
step:1727/2155 train_time:94199ms step_avg:54.54ms
step:1728/2155 train_time:94287ms step_avg:54.56ms
step:1729/2155 train_time:94376ms step_avg:54.58ms
step:1730/2155 train_time:94464ms step_avg:54.60ms
step:1731/2155 train_time:94553ms step_avg:54.62ms
step:1732/2155 train_time:94641ms step_avg:54.64ms
step:1733/2155 train_time:94730ms step_avg:54.66ms
step:1734/2155 train_time:94818ms step_avg:54.68ms
step:1735/2155 train_time:94906ms step_avg:54.70ms
step:1736/2155 train_time:94994ms step_avg:54.72ms
step:1737/2155 train_time:95084ms step_avg:54.74ms
step:1738/2155 train_time:95172ms step_avg:54.76ms
step:1739/2155 train_time:95261ms step_avg:54.78ms
step:1740/2155 train_time:95350ms step_avg:54.80ms
step:1741/2155 train_time:95440ms step_avg:54.82ms
step:1742/2155 train_time:95527ms step_avg:54.84ms
step:1743/2155 train_time:95618ms step_avg:54.86ms
step:1744/2155 train_time:95705ms step_avg:54.88ms
step:1745/2155 train_time:95794ms step_avg:54.90ms
step:1746/2155 train_time:95883ms step_avg:54.92ms
step:1747/2155 train_time:95971ms step_avg:54.93ms
step:1748/2155 train_time:96059ms step_avg:54.95ms
step:1749/2155 train_time:96148ms step_avg:54.97ms
step:1750/2155 train_time:96236ms step_avg:54.99ms
step:1750/2155 val_loss:3.3954 train_time:96327ms step_avg:55.04ms
step:1751/2155 train_time:96350ms step_avg:55.03ms
step:1752/2155 train_time:96418ms step_avg:55.03ms
step:1753/2155 train_time:96513ms step_avg:55.06ms
step:1754/2155 train_time:96603ms step_avg:55.08ms
step:1755/2155 train_time:96692ms step_avg:55.09ms
step:1756/2155 train_time:96779ms step_avg:55.11ms
step:1757/2155 train_time:96868ms step_avg:55.13ms
step:1758/2155 train_time:96955ms step_avg:55.15ms
step:1759/2155 train_time:97043ms step_avg:55.17ms
step:1760/2155 train_time:97131ms step_avg:55.19ms
step:1761/2155 train_time:97219ms step_avg:55.21ms
step:1762/2155 train_time:97308ms step_avg:55.23ms
step:1763/2155 train_time:97399ms step_avg:55.25ms
step:1764/2155 train_time:97489ms step_avg:55.27ms
step:1765/2155 train_time:97580ms step_avg:55.29ms
step:1766/2155 train_time:97668ms step_avg:55.30ms
step:1767/2155 train_time:97757ms step_avg:55.32ms
step:1768/2155 train_time:97844ms step_avg:55.34ms
step:1769/2155 train_time:97932ms step_avg:55.36ms
step:1770/2155 train_time:98019ms step_avg:55.38ms
step:1771/2155 train_time:98108ms step_avg:55.40ms
step:1772/2155 train_time:98195ms step_avg:55.41ms
step:1773/2155 train_time:98285ms step_avg:55.43ms
step:1774/2155 train_time:98373ms step_avg:55.45ms
step:1775/2155 train_time:98464ms step_avg:55.47ms
step:1776/2155 train_time:98553ms step_avg:55.49ms
step:1777/2155 train_time:98643ms step_avg:55.51ms
step:1778/2155 train_time:98731ms step_avg:55.53ms
step:1779/2155 train_time:98821ms step_avg:55.55ms
step:1780/2155 train_time:98908ms step_avg:55.57ms
step:1781/2155 train_time:98996ms step_avg:55.58ms
step:1782/2155 train_time:99084ms step_avg:55.60ms
step:1783/2155 train_time:99172ms step_avg:55.62ms
step:1784/2155 train_time:99260ms step_avg:55.64ms
step:1785/2155 train_time:99351ms step_avg:55.66ms
step:1786/2155 train_time:99439ms step_avg:55.68ms
step:1787/2155 train_time:99529ms step_avg:55.70ms
step:1788/2155 train_time:99618ms step_avg:55.71ms
step:1789/2155 train_time:99707ms step_avg:55.73ms
step:1790/2155 train_time:99795ms step_avg:55.75ms
step:1791/2155 train_time:99884ms step_avg:55.77ms
step:1792/2155 train_time:99971ms step_avg:55.79ms
step:1793/2155 train_time:100060ms step_avg:55.81ms
step:1794/2155 train_time:100148ms step_avg:55.82ms
step:1795/2155 train_time:100236ms step_avg:55.84ms
step:1796/2155 train_time:100324ms step_avg:55.86ms
step:1797/2155 train_time:100413ms step_avg:55.88ms
step:1798/2155 train_time:100502ms step_avg:55.90ms
step:1799/2155 train_time:100591ms step_avg:55.92ms
step:1800/2155 train_time:100680ms step_avg:55.93ms
step:1801/2155 train_time:100769ms step_avg:55.95ms
step:1802/2155 train_time:100857ms step_avg:55.97ms
step:1803/2155 train_time:100946ms step_avg:55.99ms
step:1804/2155 train_time:101033ms step_avg:56.01ms
step:1805/2155 train_time:101122ms step_avg:56.02ms
step:1806/2155 train_time:101209ms step_avg:56.04ms
step:1807/2155 train_time:101299ms step_avg:56.06ms
step:1808/2155 train_time:101388ms step_avg:56.08ms
step:1809/2155 train_time:101478ms step_avg:56.10ms
step:1810/2155 train_time:101568ms step_avg:56.12ms
step:1811/2155 train_time:101658ms step_avg:56.13ms
step:1812/2155 train_time:101746ms step_avg:56.15ms
step:1813/2155 train_time:101835ms step_avg:56.17ms
step:1814/2155 train_time:101922ms step_avg:56.19ms
step:1815/2155 train_time:102011ms step_avg:56.20ms
step:1816/2155 train_time:102098ms step_avg:56.22ms
step:1817/2155 train_time:102187ms step_avg:56.24ms
step:1818/2155 train_time:102275ms step_avg:56.26ms
step:1819/2155 train_time:102365ms step_avg:56.28ms
step:1820/2155 train_time:102453ms step_avg:56.29ms
step:1821/2155 train_time:102542ms step_avg:56.31ms
step:1822/2155 train_time:102630ms step_avg:56.33ms
step:1823/2155 train_time:102719ms step_avg:56.35ms
step:1824/2155 train_time:102806ms step_avg:56.36ms
step:1825/2155 train_time:102896ms step_avg:56.38ms
step:1826/2155 train_time:102984ms step_avg:56.40ms
step:1827/2155 train_time:103073ms step_avg:56.42ms
step:1828/2155 train_time:103160ms step_avg:56.43ms
step:1829/2155 train_time:103249ms step_avg:56.45ms
step:1830/2155 train_time:103337ms step_avg:56.47ms
step:1831/2155 train_time:103428ms step_avg:56.49ms
step:1832/2155 train_time:103516ms step_avg:56.50ms
step:1833/2155 train_time:103606ms step_avg:56.52ms
step:1834/2155 train_time:103694ms step_avg:56.54ms
step:1835/2155 train_time:103784ms step_avg:56.56ms
step:1836/2155 train_time:103871ms step_avg:56.57ms
step:1837/2155 train_time:103961ms step_avg:56.59ms
step:1838/2155 train_time:104048ms step_avg:56.61ms
step:1839/2155 train_time:104137ms step_avg:56.63ms
step:1840/2155 train_time:104224ms step_avg:56.64ms
step:1841/2155 train_time:104313ms step_avg:56.66ms
step:1842/2155 train_time:104401ms step_avg:56.68ms
step:1843/2155 train_time:104491ms step_avg:56.70ms
step:1844/2155 train_time:104579ms step_avg:56.71ms
step:1845/2155 train_time:104669ms step_avg:56.73ms
step:1846/2155 train_time:104757ms step_avg:56.75ms
step:1847/2155 train_time:104846ms step_avg:56.77ms
step:1848/2155 train_time:104934ms step_avg:56.78ms
step:1849/2155 train_time:105023ms step_avg:56.80ms
step:1850/2155 train_time:105110ms step_avg:56.82ms
step:1851/2155 train_time:105199ms step_avg:56.83ms
step:1852/2155 train_time:105287ms step_avg:56.85ms
step:1853/2155 train_time:105376ms step_avg:56.87ms
step:1854/2155 train_time:105464ms step_avg:56.88ms
step:1855/2155 train_time:105554ms step_avg:56.90ms
step:1856/2155 train_time:105642ms step_avg:56.92ms
step:1857/2155 train_time:105731ms step_avg:56.94ms
step:1858/2155 train_time:105820ms step_avg:56.95ms
step:1859/2155 train_time:105908ms step_avg:56.97ms
step:1860/2155 train_time:105996ms step_avg:56.99ms
step:1861/2155 train_time:106085ms step_avg:57.00ms
step:1862/2155 train_time:106173ms step_avg:57.02ms
step:1863/2155 train_time:106263ms step_avg:57.04ms
step:1864/2155 train_time:106350ms step_avg:57.05ms
step:1865/2155 train_time:106439ms step_avg:57.07ms
step:1866/2155 train_time:106527ms step_avg:57.09ms
step:1867/2155 train_time:106616ms step_avg:57.11ms
step:1868/2155 train_time:106704ms step_avg:57.12ms
step:1869/2155 train_time:106794ms step_avg:57.14ms
step:1870/2155 train_time:106882ms step_avg:57.16ms
step:1871/2155 train_time:106971ms step_avg:57.17ms
step:1872/2155 train_time:107058ms step_avg:57.19ms
step:1873/2155 train_time:107148ms step_avg:57.21ms
step:1874/2155 train_time:107236ms step_avg:57.22ms
step:1875/2155 train_time:107326ms step_avg:57.24ms
step:1876/2155 train_time:107415ms step_avg:57.26ms
step:1877/2155 train_time:107504ms step_avg:57.27ms
step:1878/2155 train_time:107591ms step_avg:57.29ms
step:1879/2155 train_time:107681ms step_avg:57.31ms
step:1880/2155 train_time:107769ms step_avg:57.32ms
step:1881/2155 train_time:107859ms step_avg:57.34ms
step:1882/2155 train_time:107947ms step_avg:57.36ms
step:1883/2155 train_time:108036ms step_avg:57.37ms
step:1884/2155 train_time:108124ms step_avg:57.39ms
step:1885/2155 train_time:108213ms step_avg:57.41ms
step:1886/2155 train_time:108302ms step_avg:57.42ms
step:1887/2155 train_time:108392ms step_avg:57.44ms
step:1888/2155 train_time:108480ms step_avg:57.46ms
step:1889/2155 train_time:108569ms step_avg:57.47ms
step:1890/2155 train_time:108657ms step_avg:57.49ms
step:1891/2155 train_time:108745ms step_avg:57.51ms
step:1892/2155 train_time:108833ms step_avg:57.52ms
step:1893/2155 train_time:108922ms step_avg:57.54ms
step:1894/2155 train_time:109010ms step_avg:57.56ms
step:1895/2155 train_time:109099ms step_avg:57.57ms
step:1896/2155 train_time:109188ms step_avg:57.59ms
step:1897/2155 train_time:109276ms step_avg:57.60ms
step:1898/2155 train_time:109364ms step_avg:57.62ms
step:1899/2155 train_time:109454ms step_avg:57.64ms
step:1900/2155 train_time:109542ms step_avg:57.65ms
step:1901/2155 train_time:109630ms step_avg:57.67ms
step:1902/2155 train_time:109718ms step_avg:57.69ms
step:1903/2155 train_time:109808ms step_avg:57.70ms
step:1904/2155 train_time:109895ms step_avg:57.72ms
step:1905/2155 train_time:109985ms step_avg:57.73ms
step:1906/2155 train_time:110072ms step_avg:57.75ms
step:1907/2155 train_time:110162ms step_avg:57.77ms
step:1908/2155 train_time:110250ms step_avg:57.78ms
step:1909/2155 train_time:110340ms step_avg:57.80ms
step:1910/2155 train_time:110428ms step_avg:57.82ms
step:1911/2155 train_time:110518ms step_avg:57.83ms
step:1912/2155 train_time:110605ms step_avg:57.85ms
step:1913/2155 train_time:110694ms step_avg:57.86ms
step:1914/2155 train_time:110783ms step_avg:57.88ms
step:1915/2155 train_time:110872ms step_avg:57.90ms
step:1916/2155 train_time:110959ms step_avg:57.91ms
step:1917/2155 train_time:111048ms step_avg:57.93ms
step:1918/2155 train_time:111135ms step_avg:57.94ms
step:1919/2155 train_time:111225ms step_avg:57.96ms
step:1920/2155 train_time:111313ms step_avg:57.98ms
step:1921/2155 train_time:111402ms step_avg:57.99ms
step:1922/2155 train_time:111490ms step_avg:58.01ms
step:1923/2155 train_time:111579ms step_avg:58.02ms
step:1924/2155 train_time:111668ms step_avg:58.04ms
step:1925/2155 train_time:111757ms step_avg:58.06ms
step:1926/2155 train_time:111844ms step_avg:58.07ms
step:1927/2155 train_time:111933ms step_avg:58.09ms
step:1928/2155 train_time:112020ms step_avg:58.10ms
step:1929/2155 train_time:112110ms step_avg:58.12ms
step:1930/2155 train_time:112197ms step_avg:58.13ms
step:1931/2155 train_time:112288ms step_avg:58.15ms
step:1932/2155 train_time:112376ms step_avg:58.17ms
step:1933/2155 train_time:112466ms step_avg:58.18ms
step:1934/2155 train_time:112553ms step_avg:58.20ms
step:1935/2155 train_time:112643ms step_avg:58.21ms
step:1936/2155 train_time:112730ms step_avg:58.23ms
step:1937/2155 train_time:112821ms step_avg:58.25ms
step:1938/2155 train_time:112908ms step_avg:58.26ms
step:1939/2155 train_time:112996ms step_avg:58.28ms
step:1940/2155 train_time:113084ms step_avg:58.29ms
step:1941/2155 train_time:113173ms step_avg:58.31ms
step:1942/2155 train_time:113262ms step_avg:58.32ms
step:1943/2155 train_time:113351ms step_avg:58.34ms
step:1944/2155 train_time:113438ms step_avg:58.35ms
step:1945/2155 train_time:113527ms step_avg:58.37ms
step:1946/2155 train_time:113615ms step_avg:58.38ms
step:1947/2155 train_time:113704ms step_avg:58.40ms
step:1948/2155 train_time:113792ms step_avg:58.41ms
step:1949/2155 train_time:113882ms step_avg:58.43ms
step:1950/2155 train_time:113969ms step_avg:58.45ms
step:1951/2155 train_time:114059ms step_avg:58.46ms
step:1952/2155 train_time:114147ms step_avg:58.48ms
step:1953/2155 train_time:114236ms step_avg:58.49ms
step:1954/2155 train_time:114324ms step_avg:58.51ms
step:1955/2155 train_time:114412ms step_avg:58.52ms
step:1956/2155 train_time:114500ms step_avg:58.54ms
step:1957/2155 train_time:114590ms step_avg:58.55ms
step:1958/2155 train_time:114679ms step_avg:58.57ms
step:1959/2155 train_time:114768ms step_avg:58.59ms
step:1960/2155 train_time:114857ms step_avg:58.60ms
step:1961/2155 train_time:114946ms step_avg:58.62ms
step:1962/2155 train_time:115033ms step_avg:58.63ms
step:1963/2155 train_time:115122ms step_avg:58.65ms
step:1964/2155 train_time:115210ms step_avg:58.66ms
step:1965/2155 train_time:115300ms step_avg:58.68ms
step:1966/2155 train_time:115387ms step_avg:58.69ms
step:1967/2155 train_time:115476ms step_avg:58.71ms
step:1968/2155 train_time:115563ms step_avg:58.72ms
step:1969/2155 train_time:115653ms step_avg:58.74ms
step:1970/2155 train_time:115741ms step_avg:58.75ms
step:1971/2155 train_time:115829ms step_avg:58.77ms
step:1972/2155 train_time:115917ms step_avg:58.78ms
step:1973/2155 train_time:116006ms step_avg:58.80ms
step:1974/2155 train_time:116093ms step_avg:58.81ms
step:1975/2155 train_time:116183ms step_avg:58.83ms
step:1976/2155 train_time:116271ms step_avg:58.84ms
step:1977/2155 train_time:116361ms step_avg:58.86ms
step:1978/2155 train_time:116448ms step_avg:58.87ms
step:1979/2155 train_time:116537ms step_avg:58.89ms
step:1980/2155 train_time:116628ms step_avg:58.90ms
step:1981/2155 train_time:116715ms step_avg:58.92ms
step:1982/2155 train_time:116803ms step_avg:58.93ms
step:1983/2155 train_time:116893ms step_avg:58.95ms
step:1984/2155 train_time:116981ms step_avg:58.96ms
step:1985/2155 train_time:117071ms step_avg:58.98ms
step:1986/2155 train_time:117159ms step_avg:58.99ms
step:1987/2155 train_time:117248ms step_avg:59.01ms
step:1988/2155 train_time:117336ms step_avg:59.02ms
step:1989/2155 train_time:117426ms step_avg:59.04ms
step:1990/2155 train_time:117513ms step_avg:59.05ms
step:1991/2155 train_time:117602ms step_avg:59.07ms
step:1992/2155 train_time:117690ms step_avg:59.08ms
step:1993/2155 train_time:117779ms step_avg:59.10ms
step:1994/2155 train_time:117868ms step_avg:59.11ms
step:1995/2155 train_time:117957ms step_avg:59.13ms
step:1996/2155 train_time:118046ms step_avg:59.14ms
step:1997/2155 train_time:118134ms step_avg:59.16ms
step:1998/2155 train_time:118222ms step_avg:59.17ms
step:1999/2155 train_time:118312ms step_avg:59.19ms
step:2000/2155 train_time:118399ms step_avg:59.20ms
step:2000/2155 val_loss:3.3158 train_time:118490ms step_avg:59.25ms
step:2001/2155 train_time:118513ms step_avg:59.23ms
step:2002/2155 train_time:118580ms step_avg:59.23ms
step:2003/2155 train_time:118676ms step_avg:59.25ms
step:2004/2155 train_time:118765ms step_avg:59.26ms
step:2005/2155 train_time:118853ms step_avg:59.28ms
step:2006/2155 train_time:118939ms step_avg:59.29ms
step:2007/2155 train_time:119027ms step_avg:59.31ms
step:2008/2155 train_time:119114ms step_avg:59.32ms
step:2009/2155 train_time:119202ms step_avg:59.33ms
step:2010/2155 train_time:119289ms step_avg:59.35ms
step:2011/2155 train_time:119378ms step_avg:59.36ms
step:2012/2155 train_time:119467ms step_avg:59.38ms
step:2013/2155 train_time:119559ms step_avg:59.39ms
step:2014/2155 train_time:119648ms step_avg:59.41ms
step:2015/2155 train_time:119739ms step_avg:59.42ms
step:2016/2155 train_time:119828ms step_avg:59.44ms
step:2017/2155 train_time:119916ms step_avg:59.45ms
step:2018/2155 train_time:120003ms step_avg:59.47ms
step:2019/2155 train_time:120091ms step_avg:59.48ms
step:2020/2155 train_time:120179ms step_avg:59.49ms
step:2021/2155 train_time:120268ms step_avg:59.51ms
step:2022/2155 train_time:120355ms step_avg:59.52ms
step:2023/2155 train_time:120444ms step_avg:59.54ms
step:2024/2155 train_time:120533ms step_avg:59.55ms
step:2025/2155 train_time:120624ms step_avg:59.57ms
step:2026/2155 train_time:120713ms step_avg:59.58ms
step:2027/2155 train_time:120803ms step_avg:59.60ms
step:2028/2155 train_time:120891ms step_avg:59.61ms
step:2029/2155 train_time:120980ms step_avg:59.63ms
step:2030/2155 train_time:121067ms step_avg:59.64ms
step:2031/2155 train_time:121156ms step_avg:59.65ms
step:2032/2155 train_time:121243ms step_avg:59.67ms
step:2033/2155 train_time:121332ms step_avg:59.68ms
step:2034/2155 train_time:121419ms step_avg:59.69ms
step:2035/2155 train_time:121510ms step_avg:59.71ms
step:2036/2155 train_time:121599ms step_avg:59.72ms
step:2037/2155 train_time:121690ms step_avg:59.74ms
step:2038/2155 train_time:121779ms step_avg:59.75ms
step:2039/2155 train_time:121868ms step_avg:59.77ms
step:2040/2155 train_time:121955ms step_avg:59.78ms
step:2041/2155 train_time:122044ms step_avg:59.80ms
step:2042/2155 train_time:122132ms step_avg:59.81ms
step:2043/2155 train_time:122221ms step_avg:59.82ms
step:2044/2155 train_time:122308ms step_avg:59.84ms
step:2045/2155 train_time:122397ms step_avg:59.85ms
step:2046/2155 train_time:122485ms step_avg:59.87ms
step:2047/2155 train_time:122575ms step_avg:59.88ms
step:2048/2155 train_time:122663ms step_avg:59.89ms
step:2049/2155 train_time:122752ms step_avg:59.91ms
step:2050/2155 train_time:122840ms step_avg:59.92ms
step:2051/2155 train_time:122929ms step_avg:59.94ms
step:2052/2155 train_time:123017ms step_avg:59.95ms
step:2053/2155 train_time:123105ms step_avg:59.96ms
step:2054/2155 train_time:123192ms step_avg:59.98ms
step:2055/2155 train_time:123281ms step_avg:59.99ms
step:2056/2155 train_time:123368ms step_avg:60.00ms
step:2057/2155 train_time:123457ms step_avg:60.02ms
step:2058/2155 train_time:123545ms step_avg:60.03ms
step:2059/2155 train_time:123635ms step_avg:60.05ms
step:2060/2155 train_time:123722ms step_avg:60.06ms
step:2061/2155 train_time:123811ms step_avg:60.07ms
step:2062/2155 train_time:123899ms step_avg:60.09ms
step:2063/2155 train_time:123988ms step_avg:60.10ms
step:2064/2155 train_time:124076ms step_avg:60.11ms
step:2065/2155 train_time:124165ms step_avg:60.13ms
step:2066/2155 train_time:124253ms step_avg:60.14ms
step:2067/2155 train_time:124342ms step_avg:60.16ms
step:2068/2155 train_time:124430ms step_avg:60.17ms
step:2069/2155 train_time:124520ms step_avg:60.18ms
step:2070/2155 train_time:124608ms step_avg:60.20ms
step:2071/2155 train_time:124697ms step_avg:60.21ms
step:2072/2155 train_time:124785ms step_avg:60.22ms
step:2073/2155 train_time:124875ms step_avg:60.24ms
step:2074/2155 train_time:124962ms step_avg:60.25ms
step:2075/2155 train_time:125051ms step_avg:60.27ms
step:2076/2155 train_time:125139ms step_avg:60.28ms
step:2077/2155 train_time:125228ms step_avg:60.29ms
step:2078/2155 train_time:125316ms step_avg:60.31ms
step:2079/2155 train_time:125404ms step_avg:60.32ms
step:2080/2155 train_time:125493ms step_avg:60.33ms
step:2081/2155 train_time:125583ms step_avg:60.35ms
step:2082/2155 train_time:125671ms step_avg:60.36ms
step:2083/2155 train_time:125759ms step_avg:60.37ms
step:2084/2155 train_time:125848ms step_avg:60.39ms
step:2085/2155 train_time:125937ms step_avg:60.40ms
step:2086/2155 train_time:126025ms step_avg:60.41ms
step:2087/2155 train_time:126114ms step_avg:60.43ms
step:2088/2155 train_time:126202ms step_avg:60.44ms
step:2089/2155 train_time:126292ms step_avg:60.46ms
step:2090/2155 train_time:126379ms step_avg:60.47ms
step:2091/2155 train_time:126469ms step_avg:60.48ms
step:2092/2155 train_time:126558ms step_avg:60.50ms
step:2093/2155 train_time:126645ms step_avg:60.51ms
step:2094/2155 train_time:126733ms step_avg:60.52ms
step:2095/2155 train_time:126822ms step_avg:60.54ms
step:2096/2155 train_time:126910ms step_avg:60.55ms
step:2097/2155 train_time:127000ms step_avg:60.56ms
step:2098/2155 train_time:127088ms step_avg:60.58ms
step:2099/2155 train_time:127176ms step_avg:60.59ms
step:2100/2155 train_time:127264ms step_avg:60.60ms
step:2101/2155 train_time:127353ms step_avg:60.62ms
step:2102/2155 train_time:127441ms step_avg:60.63ms
step:2103/2155 train_time:127531ms step_avg:60.64ms
step:2104/2155 train_time:127619ms step_avg:60.66ms
step:2105/2155 train_time:127708ms step_avg:60.67ms
step:2106/2155 train_time:127796ms step_avg:60.68ms
step:2107/2155 train_time:127885ms step_avg:60.70ms
step:2108/2155 train_time:127973ms step_avg:60.71ms
step:2109/2155 train_time:128062ms step_avg:60.72ms
step:2110/2155 train_time:128150ms step_avg:60.73ms
step:2111/2155 train_time:128239ms step_avg:60.75ms
step:2112/2155 train_time:128326ms step_avg:60.76ms
step:2113/2155 train_time:128415ms step_avg:60.77ms
step:2114/2155 train_time:128503ms step_avg:60.79ms
step:2115/2155 train_time:128593ms step_avg:60.80ms
step:2116/2155 train_time:128680ms step_avg:60.81ms
step:2117/2155 train_time:128770ms step_avg:60.83ms
step:2118/2155 train_time:128858ms step_avg:60.84ms
step:2119/2155 train_time:128947ms step_avg:60.85ms
step:2120/2155 train_time:129034ms step_avg:60.87ms
step:2121/2155 train_time:129123ms step_avg:60.88ms
step:2122/2155 train_time:129212ms step_avg:60.89ms
step:2123/2155 train_time:129303ms step_avg:60.91ms
step:2124/2155 train_time:129391ms step_avg:60.92ms
step:2125/2155 train_time:129480ms step_avg:60.93ms
step:2126/2155 train_time:129569ms step_avg:60.94ms
step:2127/2155 train_time:129659ms step_avg:60.96ms
step:2128/2155 train_time:129746ms step_avg:60.97ms
step:2129/2155 train_time:129836ms step_avg:60.98ms
step:2130/2155 train_time:129924ms step_avg:61.00ms
step:2131/2155 train_time:130014ms step_avg:61.01ms
step:2132/2155 train_time:130101ms step_avg:61.02ms
step:2133/2155 train_time:130190ms step_avg:61.04ms
step:2134/2155 train_time:130279ms step_avg:61.05ms
step:2135/2155 train_time:130368ms step_avg:61.06ms
step:2136/2155 train_time:130457ms step_avg:61.08ms
step:2137/2155 train_time:130546ms step_avg:61.09ms
step:2138/2155 train_time:130634ms step_avg:61.10ms
step:2139/2155 train_time:130723ms step_avg:61.11ms
step:2140/2155 train_time:130811ms step_avg:61.13ms
step:2141/2155 train_time:130900ms step_avg:61.14ms
step:2142/2155 train_time:130989ms step_avg:61.15ms
step:2143/2155 train_time:131078ms step_avg:61.17ms
step:2144/2155 train_time:131166ms step_avg:61.18ms
step:2145/2155 train_time:131256ms step_avg:61.19ms
step:2146/2155 train_time:131344ms step_avg:61.20ms
step:2147/2155 train_time:131433ms step_avg:61.22ms
step:2148/2155 train_time:131521ms step_avg:61.23ms
step:2149/2155 train_time:131610ms step_avg:61.24ms
step:2150/2155 train_time:131697ms step_avg:61.25ms
step:2151/2155 train_time:131786ms step_avg:61.27ms
step:2152/2155 train_time:131874ms step_avg:61.28ms
step:2153/2155 train_time:131963ms step_avg:61.29ms
step:2154/2155 train_time:132051ms step_avg:61.30ms
step:2155/2155 train_time:132141ms step_avg:61.32ms
step:2155/2155 val_loss:3.2802 train_time:132230ms step_avg:61.36ms
peak memory allocated: 29707 MiB reserved: 44636 MiB

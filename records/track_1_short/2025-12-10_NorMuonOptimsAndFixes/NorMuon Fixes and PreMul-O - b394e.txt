import uuid
run_id = f"NorMuon Fixes and PreMul-O - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
#from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977 (sa_lambdas[1] moved to O projection)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 1.0]) for _ in range(num_layers)
                    ],  # SA lambdas (sa_lambdas[1] init to 1.0 since it's now pre-multiplied to O)
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Dec 11 2025, 08:13:04) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 11 11:23:55 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   42C    P0            121W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   34C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   33C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   41C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   41C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   33C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   43C    P0            128W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           45519      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           45520      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           45521      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           45522      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           45523      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           45524      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           45525      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           45526      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           45520      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           45521      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           45522      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           45523      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           45524      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           45525      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           45526      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:87ms step_avg:87.30ms
step:2/2160 train_time:113ms step_avg:56.28ms
step:3/2160 train_time:135ms step_avg:44.92ms
step:4/2160 train_time:165ms step_avg:41.26ms
step:5/2160 train_time:198ms step_avg:39.57ms
step:6/2160 train_time:372ms step_avg:61.96ms
step:7/2160 train_time:392ms step_avg:55.94ms
step:8/2160 train_time:414ms step_avg:51.75ms
step:9/2160 train_time:447ms step_avg:49.66ms
step:10/2160 train_time:479ms step_avg:47.92ms
step:11/2160 train_time:512ms step_avg:46.56ms
step:12/2160 train_time:544ms step_avg:45.37ms
step:13/2160 train_time:578ms step_avg:44.46ms
step:14/2160 train_time:611ms step_avg:43.61ms
step:15/2160 train_time:644ms step_avg:42.92ms
step:16/2160 train_time:676ms step_avg:42.27ms
step:17/2160 train_time:709ms step_avg:41.73ms
step:18/2160 train_time:742ms step_avg:41.22ms
step:19/2160 train_time:775ms step_avg:40.79ms
step:20/2160 train_time:807ms step_avg:40.37ms
step:21/2160 train_time:841ms step_avg:40.05ms
step:22/2160 train_time:873ms step_avg:39.70ms
step:23/2160 train_time:907ms step_avg:39.42ms
step:24/2160 train_time:939ms step_avg:39.13ms
step:25/2160 train_time:972ms step_avg:38.89ms
step:26/2160 train_time:1005ms step_avg:38.64ms
step:27/2160 train_time:1038ms step_avg:38.44ms
step:28/2160 train_time:1070ms step_avg:38.23ms
step:29/2160 train_time:1104ms step_avg:38.05ms
step:30/2160 train_time:1136ms step_avg:37.87ms
step:31/2160 train_time:1169ms step_avg:37.71ms
step:32/2160 train_time:1201ms step_avg:37.54ms
step:33/2160 train_time:1235ms step_avg:37.41ms
step:34/2160 train_time:1267ms step_avg:37.27ms
step:35/2160 train_time:1302ms step_avg:37.20ms
step:36/2160 train_time:1334ms step_avg:37.07ms
step:37/2160 train_time:1369ms step_avg:37.00ms
step:38/2160 train_time:1402ms step_avg:36.89ms
step:39/2160 train_time:1435ms step_avg:36.80ms
step:40/2160 train_time:1468ms step_avg:36.70ms
step:41/2160 train_time:1501ms step_avg:36.62ms
step:42/2160 train_time:1534ms step_avg:36.53ms
step:43/2160 train_time:1568ms step_avg:36.46ms
step:44/2160 train_time:1600ms step_avg:36.37ms
step:45/2160 train_time:1633ms step_avg:36.30ms
step:46/2160 train_time:1666ms step_avg:36.22ms
step:47/2160 train_time:1699ms step_avg:36.16ms
step:48/2160 train_time:1732ms step_avg:36.08ms
step:49/2160 train_time:1765ms step_avg:36.03ms
step:50/2160 train_time:1798ms step_avg:35.96ms
step:51/2160 train_time:1831ms step_avg:35.90ms
step:52/2160 train_time:1863ms step_avg:35.83ms
step:53/2160 train_time:1896ms step_avg:35.78ms
step:54/2160 train_time:1929ms step_avg:35.72ms
step:55/2160 train_time:1962ms step_avg:35.68ms
step:56/2160 train_time:1995ms step_avg:35.62ms
step:57/2160 train_time:2028ms step_avg:35.58ms
step:58/2160 train_time:2060ms step_avg:35.52ms
step:59/2160 train_time:2094ms step_avg:35.49ms
step:60/2160 train_time:2126ms step_avg:35.44ms
step:61/2160 train_time:2159ms step_avg:35.40ms
step:62/2160 train_time:2192ms step_avg:35.35ms
step:63/2160 train_time:2225ms step_avg:35.32ms
step:64/2160 train_time:2258ms step_avg:35.28ms
step:65/2160 train_time:2292ms step_avg:35.26ms
step:66/2160 train_time:2324ms step_avg:35.21ms
step:67/2160 train_time:2358ms step_avg:35.19ms
step:68/2160 train_time:2390ms step_avg:35.15ms
step:69/2160 train_time:2424ms step_avg:35.13ms
step:70/2160 train_time:2457ms step_avg:35.09ms
step:71/2160 train_time:2490ms step_avg:35.07ms
step:72/2160 train_time:2523ms step_avg:35.04ms
step:73/2160 train_time:2556ms step_avg:35.01ms
step:74/2160 train_time:2588ms step_avg:34.98ms
step:75/2160 train_time:2622ms step_avg:34.96ms
step:76/2160 train_time:2654ms step_avg:34.93ms
step:77/2160 train_time:2688ms step_avg:34.91ms
step:78/2160 train_time:2720ms step_avg:34.88ms
step:79/2160 train_time:2754ms step_avg:34.86ms
step:80/2160 train_time:2786ms step_avg:34.83ms
step:81/2160 train_time:2820ms step_avg:34.81ms
step:82/2160 train_time:2852ms step_avg:34.78ms
step:83/2160 train_time:2885ms step_avg:34.76ms
step:84/2160 train_time:2918ms step_avg:34.73ms
step:85/2160 train_time:2951ms step_avg:34.72ms
step:86/2160 train_time:2983ms step_avg:34.69ms
step:87/2160 train_time:3016ms step_avg:34.67ms
step:88/2160 train_time:3049ms step_avg:34.64ms
step:89/2160 train_time:3082ms step_avg:34.63ms
step:90/2160 train_time:3114ms step_avg:34.60ms
step:91/2160 train_time:3147ms step_avg:34.59ms
step:92/2160 train_time:3180ms step_avg:34.56ms
step:93/2160 train_time:3213ms step_avg:34.55ms
step:94/2160 train_time:3245ms step_avg:34.53ms
step:95/2160 train_time:3279ms step_avg:34.51ms
step:96/2160 train_time:3312ms step_avg:34.50ms
step:97/2160 train_time:3345ms step_avg:34.49ms
step:98/2160 train_time:3378ms step_avg:34.47ms
step:99/2160 train_time:3411ms step_avg:34.46ms
step:100/2160 train_time:3444ms step_avg:34.44ms
step:101/2160 train_time:3477ms step_avg:34.43ms
step:102/2160 train_time:3510ms step_avg:34.41ms
step:103/2160 train_time:3544ms step_avg:34.40ms
step:104/2160 train_time:3576ms step_avg:34.39ms
step:105/2160 train_time:3610ms step_avg:34.38ms
step:106/2160 train_time:3642ms step_avg:34.36ms
step:107/2160 train_time:3675ms step_avg:34.35ms
step:108/2160 train_time:3707ms step_avg:34.33ms
step:109/2160 train_time:3741ms step_avg:34.32ms
step:110/2160 train_time:3773ms step_avg:34.30ms
step:111/2160 train_time:3806ms step_avg:34.29ms
step:112/2160 train_time:3839ms step_avg:34.28ms
step:113/2160 train_time:3872ms step_avg:34.27ms
step:114/2160 train_time:3905ms step_avg:34.25ms
step:115/2160 train_time:3938ms step_avg:34.24ms
step:116/2160 train_time:3970ms step_avg:34.22ms
step:117/2160 train_time:4003ms step_avg:34.22ms
step:118/2160 train_time:4036ms step_avg:34.20ms
step:119/2160 train_time:4069ms step_avg:34.19ms
step:120/2160 train_time:4101ms step_avg:34.18ms
step:121/2160 train_time:4134ms step_avg:34.17ms
step:122/2160 train_time:4167ms step_avg:34.15ms
step:123/2160 train_time:4200ms step_avg:34.14ms
step:124/2160 train_time:4232ms step_avg:34.13ms
step:125/2160 train_time:4266ms step_avg:34.13ms
step:126/2160 train_time:4298ms step_avg:34.11ms
step:127/2160 train_time:4332ms step_avg:34.11ms
step:128/2160 train_time:4364ms step_avg:34.09ms
step:129/2160 train_time:4397ms step_avg:34.09ms
step:130/2160 train_time:4430ms step_avg:34.08ms
step:131/2160 train_time:4463ms step_avg:34.07ms
step:132/2160 train_time:4496ms step_avg:34.06ms
step:133/2160 train_time:4529ms step_avg:34.05ms
step:134/2160 train_time:4562ms step_avg:34.04ms
step:135/2160 train_time:4595ms step_avg:34.04ms
step:136/2160 train_time:4627ms step_avg:34.02ms
step:137/2160 train_time:4661ms step_avg:34.02ms
step:138/2160 train_time:4693ms step_avg:34.01ms
step:139/2160 train_time:4727ms step_avg:34.00ms
step:140/2160 train_time:4759ms step_avg:33.99ms
step:141/2160 train_time:4793ms step_avg:33.99ms
step:142/2160 train_time:4825ms step_avg:33.98ms
step:143/2160 train_time:4858ms step_avg:33.97ms
step:144/2160 train_time:4890ms step_avg:33.96ms
step:145/2160 train_time:4924ms step_avg:33.96ms
step:146/2160 train_time:4956ms step_avg:33.95ms
step:147/2160 train_time:4990ms step_avg:33.94ms
step:148/2160 train_time:5022ms step_avg:33.93ms
step:149/2160 train_time:5055ms step_avg:33.93ms
step:150/2160 train_time:5087ms step_avg:33.92ms
step:151/2160 train_time:5120ms step_avg:33.91ms
step:152/2160 train_time:5153ms step_avg:33.90ms
step:153/2160 train_time:5186ms step_avg:33.89ms
step:154/2160 train_time:5218ms step_avg:33.89ms
step:155/2160 train_time:5252ms step_avg:33.88ms
step:156/2160 train_time:5284ms step_avg:33.87ms
step:157/2160 train_time:5317ms step_avg:33.87ms
step:158/2160 train_time:5349ms step_avg:33.85ms
step:159/2160 train_time:5382ms step_avg:33.85ms
step:160/2160 train_time:5415ms step_avg:33.84ms
step:161/2160 train_time:5449ms step_avg:33.84ms
step:162/2160 train_time:5481ms step_avg:33.83ms
step:163/2160 train_time:5514ms step_avg:33.83ms
step:164/2160 train_time:5547ms step_avg:33.82ms
step:165/2160 train_time:5580ms step_avg:33.82ms
step:166/2160 train_time:5612ms step_avg:33.81ms
step:167/2160 train_time:5645ms step_avg:33.80ms
step:168/2160 train_time:5678ms step_avg:33.80ms
step:169/2160 train_time:5711ms step_avg:33.79ms
step:170/2160 train_time:5744ms step_avg:33.79ms
step:171/2160 train_time:5777ms step_avg:33.78ms
step:172/2160 train_time:5809ms step_avg:33.77ms
step:173/2160 train_time:5842ms step_avg:33.77ms
step:174/2160 train_time:5875ms step_avg:33.76ms
step:175/2160 train_time:5908ms step_avg:33.76ms
step:176/2160 train_time:5940ms step_avg:33.75ms
step:177/2160 train_time:5973ms step_avg:33.75ms
step:178/2160 train_time:6005ms step_avg:33.74ms
step:179/2160 train_time:6039ms step_avg:33.74ms
step:180/2160 train_time:6071ms step_avg:33.73ms
step:181/2160 train_time:6105ms step_avg:33.73ms
step:182/2160 train_time:6137ms step_avg:33.72ms
step:183/2160 train_time:6170ms step_avg:33.72ms
step:184/2160 train_time:6203ms step_avg:33.71ms
step:185/2160 train_time:6236ms step_avg:33.71ms
step:186/2160 train_time:6269ms step_avg:33.70ms
step:187/2160 train_time:6302ms step_avg:33.70ms
step:188/2160 train_time:6334ms step_avg:33.69ms
step:189/2160 train_time:6367ms step_avg:33.69ms
step:190/2160 train_time:6400ms step_avg:33.68ms
step:191/2160 train_time:6432ms step_avg:33.68ms
step:192/2160 train_time:6465ms step_avg:33.67ms
step:193/2160 train_time:6498ms step_avg:33.67ms
step:194/2160 train_time:6530ms step_avg:33.66ms
step:195/2160 train_time:6563ms step_avg:33.66ms
step:196/2160 train_time:6596ms step_avg:33.65ms
step:197/2160 train_time:6629ms step_avg:33.65ms
step:198/2160 train_time:6662ms step_avg:33.64ms
step:199/2160 train_time:6695ms step_avg:33.64ms
step:200/2160 train_time:6727ms step_avg:33.64ms
step:201/2160 train_time:6760ms step_avg:33.63ms
step:202/2160 train_time:6793ms step_avg:33.63ms
step:203/2160 train_time:6826ms step_avg:33.63ms
step:204/2160 train_time:6859ms step_avg:33.62ms
step:205/2160 train_time:6892ms step_avg:33.62ms
step:206/2160 train_time:6924ms step_avg:33.61ms
step:207/2160 train_time:6957ms step_avg:33.61ms
step:208/2160 train_time:6990ms step_avg:33.60ms
step:209/2160 train_time:7023ms step_avg:33.60ms
step:210/2160 train_time:7055ms step_avg:33.59ms
step:211/2160 train_time:7088ms step_avg:33.59ms
step:212/2160 train_time:7121ms step_avg:33.59ms
step:213/2160 train_time:7154ms step_avg:33.59ms
step:214/2160 train_time:7186ms step_avg:33.58ms
step:215/2160 train_time:7219ms step_avg:33.58ms
step:216/2160 train_time:7252ms step_avg:33.57ms
step:217/2160 train_time:7285ms step_avg:33.57ms
step:218/2160 train_time:7317ms step_avg:33.57ms
step:219/2160 train_time:7351ms step_avg:33.56ms
step:220/2160 train_time:7383ms step_avg:33.56ms
step:221/2160 train_time:7416ms step_avg:33.56ms
step:222/2160 train_time:7448ms step_avg:33.55ms
step:223/2160 train_time:7481ms step_avg:33.55ms
step:224/2160 train_time:7513ms step_avg:33.54ms
step:225/2160 train_time:7547ms step_avg:33.54ms
step:226/2160 train_time:7579ms step_avg:33.54ms
step:227/2160 train_time:7612ms step_avg:33.53ms
step:228/2160 train_time:7644ms step_avg:33.53ms
step:229/2160 train_time:7678ms step_avg:33.53ms
step:230/2160 train_time:7710ms step_avg:33.52ms
step:231/2160 train_time:7743ms step_avg:33.52ms
step:232/2160 train_time:7776ms step_avg:33.52ms
step:233/2160 train_time:7809ms step_avg:33.52ms
step:234/2160 train_time:7842ms step_avg:33.51ms
step:235/2160 train_time:7875ms step_avg:33.51ms
step:236/2160 train_time:7907ms step_avg:33.50ms
step:237/2160 train_time:7940ms step_avg:33.50ms
step:238/2160 train_time:7973ms step_avg:33.50ms
step:239/2160 train_time:8006ms step_avg:33.50ms
step:240/2160 train_time:8038ms step_avg:33.49ms
step:241/2160 train_time:8071ms step_avg:33.49ms
step:242/2160 train_time:8103ms step_avg:33.49ms
step:243/2160 train_time:8136ms step_avg:33.48ms
step:244/2160 train_time:8169ms step_avg:33.48ms
step:245/2160 train_time:8202ms step_avg:33.48ms
step:246/2160 train_time:8235ms step_avg:33.47ms
step:247/2160 train_time:8268ms step_avg:33.47ms
step:248/2160 train_time:8300ms step_avg:33.47ms
step:249/2160 train_time:8333ms step_avg:33.47ms
step:250/2160 train_time:8366ms step_avg:33.46ms
step:250/2160 val_loss:4.3447 train_time:8401ms step_avg:33.60ms
step:251/2160 train_time:8422ms step_avg:33.55ms
step:252/2160 train_time:8443ms step_avg:33.50ms
step:253/2160 train_time:8467ms step_avg:33.47ms
step:254/2160 train_time:8499ms step_avg:33.46ms
step:255/2160 train_time:8534ms step_avg:33.47ms
step:256/2160 train_time:8567ms step_avg:33.47ms
step:257/2160 train_time:8602ms step_avg:33.47ms
step:258/2160 train_time:8635ms step_avg:33.47ms
step:259/2160 train_time:8669ms step_avg:33.47ms
step:260/2160 train_time:8701ms step_avg:33.47ms
step:261/2160 train_time:8734ms step_avg:33.46ms
step:262/2160 train_time:8767ms step_avg:33.46ms
step:263/2160 train_time:8800ms step_avg:33.46ms
step:264/2160 train_time:8832ms step_avg:33.45ms
step:265/2160 train_time:8865ms step_avg:33.45ms
step:266/2160 train_time:8898ms step_avg:33.45ms
step:267/2160 train_time:8930ms step_avg:33.45ms
step:268/2160 train_time:8963ms step_avg:33.44ms
step:269/2160 train_time:8996ms step_avg:33.44ms
step:270/2160 train_time:9028ms step_avg:33.44ms
step:271/2160 train_time:9061ms step_avg:33.43ms
step:272/2160 train_time:9093ms step_avg:33.43ms
step:273/2160 train_time:9126ms step_avg:33.43ms
step:274/2160 train_time:9158ms step_avg:33.42ms
step:275/2160 train_time:9191ms step_avg:33.42ms
step:276/2160 train_time:9223ms step_avg:33.42ms
step:277/2160 train_time:9256ms step_avg:33.41ms
step:278/2160 train_time:9288ms step_avg:33.41ms
step:279/2160 train_time:9321ms step_avg:33.41ms
step:280/2160 train_time:9353ms step_avg:33.40ms
step:281/2160 train_time:9386ms step_avg:33.40ms
step:282/2160 train_time:9418ms step_avg:33.40ms
step:283/2160 train_time:9452ms step_avg:33.40ms
step:284/2160 train_time:9485ms step_avg:33.40ms
step:285/2160 train_time:9518ms step_avg:33.40ms
step:286/2160 train_time:9551ms step_avg:33.39ms
step:287/2160 train_time:9584ms step_avg:33.39ms
step:288/2160 train_time:9616ms step_avg:33.39ms
step:289/2160 train_time:9650ms step_avg:33.39ms
step:290/2160 train_time:9682ms step_avg:33.39ms
step:291/2160 train_time:9715ms step_avg:33.38ms
step:292/2160 train_time:9747ms step_avg:33.38ms
step:293/2160 train_time:9781ms step_avg:33.38ms
step:294/2160 train_time:9816ms step_avg:33.39ms
step:295/2160 train_time:9847ms step_avg:33.38ms
step:296/2160 train_time:9879ms step_avg:33.37ms
step:297/2160 train_time:9912ms step_avg:33.37ms
step:298/2160 train_time:9944ms step_avg:33.37ms
step:299/2160 train_time:9977ms step_avg:33.37ms
step:300/2160 train_time:10009ms step_avg:33.36ms
step:301/2160 train_time:10043ms step_avg:33.36ms
step:302/2160 train_time:10075ms step_avg:33.36ms
step:303/2160 train_time:10108ms step_avg:33.36ms
step:304/2160 train_time:10140ms step_avg:33.35ms
step:305/2160 train_time:10173ms step_avg:33.35ms
step:306/2160 train_time:10205ms step_avg:33.35ms
step:307/2160 train_time:10238ms step_avg:33.35ms
step:308/2160 train_time:10270ms step_avg:33.35ms
step:309/2160 train_time:10303ms step_avg:33.34ms
step:310/2160 train_time:10336ms step_avg:33.34ms
step:311/2160 train_time:10369ms step_avg:33.34ms
step:312/2160 train_time:10401ms step_avg:33.34ms
step:313/2160 train_time:10435ms step_avg:33.34ms
step:314/2160 train_time:10467ms step_avg:33.33ms
step:315/2160 train_time:10500ms step_avg:33.33ms
step:316/2160 train_time:10532ms step_avg:33.33ms
step:317/2160 train_time:10566ms step_avg:33.33ms
step:318/2160 train_time:10598ms step_avg:33.33ms
step:319/2160 train_time:10631ms step_avg:33.33ms
step:320/2160 train_time:10663ms step_avg:33.32ms
step:321/2160 train_time:10696ms step_avg:33.32ms
step:322/2160 train_time:10729ms step_avg:33.32ms
step:323/2160 train_time:10762ms step_avg:33.32ms
step:324/2160 train_time:10794ms step_avg:33.32ms
step:325/2160 train_time:10827ms step_avg:33.32ms
step:326/2160 train_time:10860ms step_avg:33.31ms
step:327/2160 train_time:10893ms step_avg:33.31ms
step:328/2160 train_time:10925ms step_avg:33.31ms
step:329/2160 train_time:10958ms step_avg:33.31ms
step:330/2160 train_time:10990ms step_avg:33.30ms
step:331/2160 train_time:11023ms step_avg:33.30ms
step:332/2160 train_time:11056ms step_avg:33.30ms
step:333/2160 train_time:11089ms step_avg:33.30ms
step:334/2160 train_time:11121ms step_avg:33.30ms
step:335/2160 train_time:11154ms step_avg:33.30ms
step:336/2160 train_time:11187ms step_avg:33.29ms
step:337/2160 train_time:11220ms step_avg:33.29ms
step:338/2160 train_time:11252ms step_avg:33.29ms
step:339/2160 train_time:11285ms step_avg:33.29ms
step:340/2160 train_time:11317ms step_avg:33.29ms
step:341/2160 train_time:11350ms step_avg:33.28ms
step:342/2160 train_time:11382ms step_avg:33.28ms
step:343/2160 train_time:11415ms step_avg:33.28ms
step:344/2160 train_time:11448ms step_avg:33.28ms
step:345/2160 train_time:11481ms step_avg:33.28ms
step:346/2160 train_time:11513ms step_avg:33.27ms
step:347/2160 train_time:11547ms step_avg:33.28ms
step:348/2160 train_time:11579ms step_avg:33.27ms
step:349/2160 train_time:11612ms step_avg:33.27ms
step:350/2160 train_time:11644ms step_avg:33.27ms
step:351/2160 train_time:11677ms step_avg:33.27ms
step:352/2160 train_time:11710ms step_avg:33.27ms
step:353/2160 train_time:11743ms step_avg:33.27ms
step:354/2160 train_time:11775ms step_avg:33.26ms
step:355/2160 train_time:11809ms step_avg:33.27ms
step:356/2160 train_time:11841ms step_avg:33.26ms
step:357/2160 train_time:11875ms step_avg:33.26ms
step:358/2160 train_time:11907ms step_avg:33.26ms
step:359/2160 train_time:11940ms step_avg:33.26ms
step:360/2160 train_time:11973ms step_avg:33.26ms
step:361/2160 train_time:12006ms step_avg:33.26ms
step:362/2160 train_time:12038ms step_avg:33.26ms
step:363/2160 train_time:12072ms step_avg:33.26ms
step:364/2160 train_time:12104ms step_avg:33.25ms
step:365/2160 train_time:12137ms step_avg:33.25ms
step:366/2160 train_time:12169ms step_avg:33.25ms
step:367/2160 train_time:12202ms step_avg:33.25ms
step:368/2160 train_time:12234ms step_avg:33.25ms
step:369/2160 train_time:12268ms step_avg:33.25ms
step:370/2160 train_time:12300ms step_avg:33.24ms
step:371/2160 train_time:12333ms step_avg:33.24ms
step:372/2160 train_time:12365ms step_avg:33.24ms
step:373/2160 train_time:12399ms step_avg:33.24ms
step:374/2160 train_time:12431ms step_avg:33.24ms
step:375/2160 train_time:12464ms step_avg:33.24ms
step:376/2160 train_time:12496ms step_avg:33.24ms
step:377/2160 train_time:12530ms step_avg:33.23ms
step:378/2160 train_time:12562ms step_avg:33.23ms
step:379/2160 train_time:12595ms step_avg:33.23ms
step:380/2160 train_time:12627ms step_avg:33.23ms
step:381/2160 train_time:12660ms step_avg:33.23ms
step:382/2160 train_time:12693ms step_avg:33.23ms
step:383/2160 train_time:12726ms step_avg:33.23ms
step:384/2160 train_time:12758ms step_avg:33.23ms
step:385/2160 train_time:12792ms step_avg:33.22ms
step:386/2160 train_time:12824ms step_avg:33.22ms
step:387/2160 train_time:12857ms step_avg:33.22ms
step:388/2160 train_time:12889ms step_avg:33.22ms
step:389/2160 train_time:12922ms step_avg:33.22ms
step:390/2160 train_time:12954ms step_avg:33.22ms
step:391/2160 train_time:12988ms step_avg:33.22ms
step:392/2160 train_time:13020ms step_avg:33.22ms
step:393/2160 train_time:13054ms step_avg:33.22ms
step:394/2160 train_time:13086ms step_avg:33.21ms
step:395/2160 train_time:13119ms step_avg:33.21ms
step:396/2160 train_time:13151ms step_avg:33.21ms
step:397/2160 train_time:13184ms step_avg:33.21ms
step:398/2160 train_time:13217ms step_avg:33.21ms
step:399/2160 train_time:13250ms step_avg:33.21ms
step:400/2160 train_time:13283ms step_avg:33.21ms
step:401/2160 train_time:13315ms step_avg:33.21ms
step:402/2160 train_time:13348ms step_avg:33.20ms
step:403/2160 train_time:13381ms step_avg:33.20ms
step:404/2160 train_time:13413ms step_avg:33.20ms
step:405/2160 train_time:13447ms step_avg:33.20ms
step:406/2160 train_time:13479ms step_avg:33.20ms
step:407/2160 train_time:13512ms step_avg:33.20ms
step:408/2160 train_time:13544ms step_avg:33.20ms
step:409/2160 train_time:13577ms step_avg:33.20ms
step:410/2160 train_time:13610ms step_avg:33.19ms
step:411/2160 train_time:13643ms step_avg:33.19ms
step:412/2160 train_time:13675ms step_avg:33.19ms
step:413/2160 train_time:13709ms step_avg:33.19ms
step:414/2160 train_time:13741ms step_avg:33.19ms
step:415/2160 train_time:13774ms step_avg:33.19ms
step:416/2160 train_time:13806ms step_avg:33.19ms
step:417/2160 train_time:13840ms step_avg:33.19ms
step:418/2160 train_time:13872ms step_avg:33.19ms
step:419/2160 train_time:13905ms step_avg:33.19ms
step:420/2160 train_time:13938ms step_avg:33.18ms
step:421/2160 train_time:13971ms step_avg:33.18ms
step:422/2160 train_time:14003ms step_avg:33.18ms
step:423/2160 train_time:14036ms step_avg:33.18ms
step:424/2160 train_time:14068ms step_avg:33.18ms
step:425/2160 train_time:14101ms step_avg:33.18ms
step:426/2160 train_time:14133ms step_avg:33.18ms
step:427/2160 train_time:14167ms step_avg:33.18ms
step:428/2160 train_time:14199ms step_avg:33.18ms
step:429/2160 train_time:14232ms step_avg:33.18ms
step:430/2160 train_time:14264ms step_avg:33.17ms
step:431/2160 train_time:14298ms step_avg:33.17ms
step:432/2160 train_time:14330ms step_avg:33.17ms
step:433/2160 train_time:14364ms step_avg:33.17ms
step:434/2160 train_time:14396ms step_avg:33.17ms
step:435/2160 train_time:14430ms step_avg:33.17ms
step:436/2160 train_time:14462ms step_avg:33.17ms
step:437/2160 train_time:14495ms step_avg:33.17ms
step:438/2160 train_time:14527ms step_avg:33.17ms
step:439/2160 train_time:14561ms step_avg:33.17ms
step:440/2160 train_time:14593ms step_avg:33.17ms
step:441/2160 train_time:14626ms step_avg:33.17ms
step:442/2160 train_time:14659ms step_avg:33.16ms
step:443/2160 train_time:14692ms step_avg:33.16ms
step:444/2160 train_time:14724ms step_avg:33.16ms
step:445/2160 train_time:14757ms step_avg:33.16ms
step:446/2160 train_time:14789ms step_avg:33.16ms
step:447/2160 train_time:14823ms step_avg:33.16ms
step:448/2160 train_time:14855ms step_avg:33.16ms
step:449/2160 train_time:14889ms step_avg:33.16ms
step:450/2160 train_time:14921ms step_avg:33.16ms
step:451/2160 train_time:14954ms step_avg:33.16ms
step:452/2160 train_time:14986ms step_avg:33.16ms
step:453/2160 train_time:15020ms step_avg:33.16ms
step:454/2160 train_time:15052ms step_avg:33.15ms
step:455/2160 train_time:15085ms step_avg:33.15ms
step:456/2160 train_time:15117ms step_avg:33.15ms
step:457/2160 train_time:15150ms step_avg:33.15ms
step:458/2160 train_time:15183ms step_avg:33.15ms
step:459/2160 train_time:15216ms step_avg:33.15ms
step:460/2160 train_time:15248ms step_avg:33.15ms
step:461/2160 train_time:15281ms step_avg:33.15ms
step:462/2160 train_time:15313ms step_avg:33.15ms
step:463/2160 train_time:15346ms step_avg:33.15ms
step:464/2160 train_time:15379ms step_avg:33.14ms
step:465/2160 train_time:15412ms step_avg:33.14ms
step:466/2160 train_time:15445ms step_avg:33.14ms
step:467/2160 train_time:15478ms step_avg:33.14ms
step:468/2160 train_time:15510ms step_avg:33.14ms
step:469/2160 train_time:15544ms step_avg:33.14ms
step:470/2160 train_time:15576ms step_avg:33.14ms
step:471/2160 train_time:15609ms step_avg:33.14ms
step:472/2160 train_time:15642ms step_avg:33.14ms
step:473/2160 train_time:15675ms step_avg:33.14ms
step:474/2160 train_time:15707ms step_avg:33.14ms
step:475/2160 train_time:15740ms step_avg:33.14ms
step:476/2160 train_time:15772ms step_avg:33.14ms
step:477/2160 train_time:15806ms step_avg:33.14ms
step:478/2160 train_time:15838ms step_avg:33.13ms
step:479/2160 train_time:15871ms step_avg:33.13ms
step:480/2160 train_time:15904ms step_avg:33.13ms
step:481/2160 train_time:15937ms step_avg:33.13ms
step:482/2160 train_time:15969ms step_avg:33.13ms
step:483/2160 train_time:16002ms step_avg:33.13ms
step:484/2160 train_time:16035ms step_avg:33.13ms
step:485/2160 train_time:16068ms step_avg:33.13ms
step:486/2160 train_time:16101ms step_avg:33.13ms
step:487/2160 train_time:16134ms step_avg:33.13ms
step:488/2160 train_time:16166ms step_avg:33.13ms
step:489/2160 train_time:16199ms step_avg:33.13ms
step:490/2160 train_time:16232ms step_avg:33.13ms
step:491/2160 train_time:16265ms step_avg:33.13ms
step:492/2160 train_time:16297ms step_avg:33.12ms
step:493/2160 train_time:16330ms step_avg:33.12ms
step:494/2160 train_time:16363ms step_avg:33.12ms
step:495/2160 train_time:16396ms step_avg:33.12ms
step:496/2160 train_time:16428ms step_avg:33.12ms
step:497/2160 train_time:16461ms step_avg:33.12ms
step:498/2160 train_time:16493ms step_avg:33.12ms
step:499/2160 train_time:16527ms step_avg:33.12ms
step:500/2160 train_time:16559ms step_avg:33.12ms
step:500/2160 val_loss:4.0313 train_time:16595ms step_avg:33.19ms
step:501/2160 train_time:16616ms step_avg:33.17ms
step:502/2160 train_time:16638ms step_avg:33.14ms
step:503/2160 train_time:16663ms step_avg:33.13ms
step:504/2160 train_time:16695ms step_avg:33.13ms
step:505/2160 train_time:16731ms step_avg:33.13ms
step:506/2160 train_time:16764ms step_avg:33.13ms
step:507/2160 train_time:16799ms step_avg:33.13ms
step:508/2160 train_time:16831ms step_avg:33.13ms
step:509/2160 train_time:16865ms step_avg:33.13ms
step:510/2160 train_time:16897ms step_avg:33.13ms
step:511/2160 train_time:16931ms step_avg:33.13ms
step:512/2160 train_time:16963ms step_avg:33.13ms
step:513/2160 train_time:16996ms step_avg:33.13ms
step:514/2160 train_time:17028ms step_avg:33.13ms
step:515/2160 train_time:17061ms step_avg:33.13ms
step:516/2160 train_time:17093ms step_avg:33.13ms
step:517/2160 train_time:17126ms step_avg:33.13ms
step:518/2160 train_time:17158ms step_avg:33.12ms
step:519/2160 train_time:17191ms step_avg:33.12ms
step:520/2160 train_time:17223ms step_avg:33.12ms
step:521/2160 train_time:17256ms step_avg:33.12ms
step:522/2160 train_time:17288ms step_avg:33.12ms
step:523/2160 train_time:17321ms step_avg:33.12ms
step:524/2160 train_time:17353ms step_avg:33.12ms
step:525/2160 train_time:17386ms step_avg:33.12ms
step:526/2160 train_time:17418ms step_avg:33.11ms
step:527/2160 train_time:17451ms step_avg:33.11ms
step:528/2160 train_time:17483ms step_avg:33.11ms
step:529/2160 train_time:17516ms step_avg:33.11ms
step:530/2160 train_time:17548ms step_avg:33.11ms
step:531/2160 train_time:17582ms step_avg:33.11ms
step:532/2160 train_time:17614ms step_avg:33.11ms
step:533/2160 train_time:17648ms step_avg:33.11ms
step:534/2160 train_time:17681ms step_avg:33.11ms
step:535/2160 train_time:17714ms step_avg:33.11ms
step:536/2160 train_time:17747ms step_avg:33.11ms
step:537/2160 train_time:17781ms step_avg:33.11ms
step:538/2160 train_time:17813ms step_avg:33.11ms
step:539/2160 train_time:17847ms step_avg:33.11ms
step:540/2160 train_time:17879ms step_avg:33.11ms
step:541/2160 train_time:17913ms step_avg:33.11ms
step:542/2160 train_time:17945ms step_avg:33.11ms
step:543/2160 train_time:17978ms step_avg:33.11ms
step:544/2160 train_time:18010ms step_avg:33.11ms
step:545/2160 train_time:18044ms step_avg:33.11ms
step:546/2160 train_time:18076ms step_avg:33.11ms
step:547/2160 train_time:18109ms step_avg:33.11ms
step:548/2160 train_time:18141ms step_avg:33.10ms
step:549/2160 train_time:18174ms step_avg:33.10ms
step:550/2160 train_time:18206ms step_avg:33.10ms
step:551/2160 train_time:18240ms step_avg:33.10ms
step:552/2160 train_time:18272ms step_avg:33.10ms
step:553/2160 train_time:18305ms step_avg:33.10ms
step:554/2160 train_time:18338ms step_avg:33.10ms
step:555/2160 train_time:18371ms step_avg:33.10ms
step:556/2160 train_time:18403ms step_avg:33.10ms
step:557/2160 train_time:18436ms step_avg:33.10ms
step:558/2160 train_time:18468ms step_avg:33.10ms
step:559/2160 train_time:18501ms step_avg:33.10ms
step:560/2160 train_time:18533ms step_avg:33.10ms
step:561/2160 train_time:18567ms step_avg:33.10ms
step:562/2160 train_time:18600ms step_avg:33.10ms
step:563/2160 train_time:18633ms step_avg:33.10ms
step:564/2160 train_time:18665ms step_avg:33.09ms
step:565/2160 train_time:18699ms step_avg:33.10ms
step:566/2160 train_time:18732ms step_avg:33.10ms
step:567/2160 train_time:18766ms step_avg:33.10ms
step:568/2160 train_time:18798ms step_avg:33.09ms
step:569/2160 train_time:18831ms step_avg:33.10ms
step:570/2160 train_time:18864ms step_avg:33.09ms
step:571/2160 train_time:18897ms step_avg:33.10ms
step:572/2160 train_time:18930ms step_avg:33.09ms
step:573/2160 train_time:18963ms step_avg:33.09ms
step:574/2160 train_time:18996ms step_avg:33.09ms
step:575/2160 train_time:19029ms step_avg:33.09ms
step:576/2160 train_time:19061ms step_avg:33.09ms
step:577/2160 train_time:19094ms step_avg:33.09ms
step:578/2160 train_time:19127ms step_avg:33.09ms
step:579/2160 train_time:19160ms step_avg:33.09ms
step:580/2160 train_time:19193ms step_avg:33.09ms
step:581/2160 train_time:19226ms step_avg:33.09ms
step:582/2160 train_time:19258ms step_avg:33.09ms
step:583/2160 train_time:19291ms step_avg:33.09ms
step:584/2160 train_time:19323ms step_avg:33.09ms
step:585/2160 train_time:19356ms step_avg:33.09ms
step:586/2160 train_time:19388ms step_avg:33.09ms
step:587/2160 train_time:19421ms step_avg:33.09ms
step:588/2160 train_time:19454ms step_avg:33.08ms
step:589/2160 train_time:19487ms step_avg:33.08ms
step:590/2160 train_time:19519ms step_avg:33.08ms
step:591/2160 train_time:19552ms step_avg:33.08ms
step:592/2160 train_time:19584ms step_avg:33.08ms
step:593/2160 train_time:19618ms step_avg:33.08ms
step:594/2160 train_time:19650ms step_avg:33.08ms
step:595/2160 train_time:19684ms step_avg:33.08ms
step:596/2160 train_time:19716ms step_avg:33.08ms
step:597/2160 train_time:19750ms step_avg:33.08ms
step:598/2160 train_time:19782ms step_avg:33.08ms
step:599/2160 train_time:19815ms step_avg:33.08ms
step:600/2160 train_time:19848ms step_avg:33.08ms
step:601/2160 train_time:19882ms step_avg:33.08ms
step:602/2160 train_time:19914ms step_avg:33.08ms
step:603/2160 train_time:19947ms step_avg:33.08ms
step:604/2160 train_time:19980ms step_avg:33.08ms
step:605/2160 train_time:20013ms step_avg:33.08ms
step:606/2160 train_time:20046ms step_avg:33.08ms
step:607/2160 train_time:20078ms step_avg:33.08ms
step:608/2160 train_time:20111ms step_avg:33.08ms
step:609/2160 train_time:20145ms step_avg:33.08ms
step:610/2160 train_time:20177ms step_avg:33.08ms
step:611/2160 train_time:20210ms step_avg:33.08ms
step:612/2160 train_time:20243ms step_avg:33.08ms
step:613/2160 train_time:20275ms step_avg:33.08ms
step:614/2160 train_time:20308ms step_avg:33.07ms
step:615/2160 train_time:20341ms step_avg:33.07ms
step:616/2160 train_time:20373ms step_avg:33.07ms
step:617/2160 train_time:20407ms step_avg:33.07ms
step:618/2160 train_time:20439ms step_avg:33.07ms
step:619/2160 train_time:20472ms step_avg:33.07ms
step:620/2160 train_time:20504ms step_avg:33.07ms
step:621/2160 train_time:20537ms step_avg:33.07ms
step:622/2160 train_time:20569ms step_avg:33.07ms
step:623/2160 train_time:20603ms step_avg:33.07ms
step:624/2160 train_time:20636ms step_avg:33.07ms
step:625/2160 train_time:20669ms step_avg:33.07ms
step:626/2160 train_time:20701ms step_avg:33.07ms
step:627/2160 train_time:20734ms step_avg:33.07ms
step:628/2160 train_time:20767ms step_avg:33.07ms
step:629/2160 train_time:20800ms step_avg:33.07ms
step:630/2160 train_time:20832ms step_avg:33.07ms
step:631/2160 train_time:20866ms step_avg:33.07ms
step:632/2160 train_time:20898ms step_avg:33.07ms
step:633/2160 train_time:20932ms step_avg:33.07ms
step:634/2160 train_time:20964ms step_avg:33.07ms
step:635/2160 train_time:20998ms step_avg:33.07ms
step:636/2160 train_time:21030ms step_avg:33.07ms
step:637/2160 train_time:21063ms step_avg:33.07ms
step:638/2160 train_time:21096ms step_avg:33.07ms
step:639/2160 train_time:21129ms step_avg:33.07ms
step:640/2160 train_time:21162ms step_avg:33.06ms
step:641/2160 train_time:21195ms step_avg:33.06ms
step:642/2160 train_time:21227ms step_avg:33.06ms
step:643/2160 train_time:21260ms step_avg:33.06ms
step:644/2160 train_time:21293ms step_avg:33.06ms
step:645/2160 train_time:21326ms step_avg:33.06ms
step:646/2160 train_time:21358ms step_avg:33.06ms
step:647/2160 train_time:21391ms step_avg:33.06ms
step:648/2160 train_time:21423ms step_avg:33.06ms
step:649/2160 train_time:21457ms step_avg:33.06ms
step:650/2160 train_time:21489ms step_avg:33.06ms
step:651/2160 train_time:21522ms step_avg:33.06ms
step:652/2160 train_time:21554ms step_avg:33.06ms
step:653/2160 train_time:21588ms step_avg:33.06ms
step:654/2160 train_time:21620ms step_avg:33.06ms
step:655/2160 train_time:21654ms step_avg:33.06ms
step:656/2160 train_time:21686ms step_avg:33.06ms
step:657/2160 train_time:21719ms step_avg:33.06ms
step:658/2160 train_time:21752ms step_avg:33.06ms
step:659/2160 train_time:21785ms step_avg:33.06ms
step:660/2160 train_time:21817ms step_avg:33.06ms
step:661/2160 train_time:21851ms step_avg:33.06ms
step:662/2160 train_time:21883ms step_avg:33.06ms
step:663/2160 train_time:21916ms step_avg:33.06ms
step:664/2160 train_time:21948ms step_avg:33.05ms
step:665/2160 train_time:21982ms step_avg:33.06ms
step:666/2160 train_time:22015ms step_avg:33.06ms
step:667/2160 train_time:22048ms step_avg:33.06ms
step:668/2160 train_time:22081ms step_avg:33.05ms
step:669/2160 train_time:22114ms step_avg:33.05ms
step:670/2160 train_time:22146ms step_avg:33.05ms
step:671/2160 train_time:22179ms step_avg:33.05ms
step:672/2160 train_time:22212ms step_avg:33.05ms
step:673/2160 train_time:22245ms step_avg:33.05ms
step:674/2160 train_time:22278ms step_avg:33.05ms
step:675/2160 train_time:22311ms step_avg:33.05ms
step:676/2160 train_time:22343ms step_avg:33.05ms
step:677/2160 train_time:22376ms step_avg:33.05ms
step:678/2160 train_time:22408ms step_avg:33.05ms
step:679/2160 train_time:22441ms step_avg:33.05ms
step:680/2160 train_time:22474ms step_avg:33.05ms
step:681/2160 train_time:22507ms step_avg:33.05ms
step:682/2160 train_time:22540ms step_avg:33.05ms
step:683/2160 train_time:22573ms step_avg:33.05ms
step:684/2160 train_time:22605ms step_avg:33.05ms
step:685/2160 train_time:22638ms step_avg:33.05ms
step:686/2160 train_time:22671ms step_avg:33.05ms
step:687/2160 train_time:22704ms step_avg:33.05ms
step:688/2160 train_time:22736ms step_avg:33.05ms
step:689/2160 train_time:22769ms step_avg:33.05ms
step:690/2160 train_time:22802ms step_avg:33.05ms
step:691/2160 train_time:22835ms step_avg:33.05ms
step:692/2160 train_time:22867ms step_avg:33.04ms
step:693/2160 train_time:22900ms step_avg:33.05ms
step:694/2160 train_time:22933ms step_avg:33.04ms
step:695/2160 train_time:22966ms step_avg:33.04ms
step:696/2160 train_time:22999ms step_avg:33.04ms
step:697/2160 train_time:23032ms step_avg:33.04ms
step:698/2160 train_time:23064ms step_avg:33.04ms
step:699/2160 train_time:23098ms step_avg:33.04ms
step:700/2160 train_time:23130ms step_avg:33.04ms
step:701/2160 train_time:23163ms step_avg:33.04ms
step:702/2160 train_time:23195ms step_avg:33.04ms
step:703/2160 train_time:23229ms step_avg:33.04ms
step:704/2160 train_time:23261ms step_avg:33.04ms
step:705/2160 train_time:23294ms step_avg:33.04ms
step:706/2160 train_time:23326ms step_avg:33.04ms
step:707/2160 train_time:23360ms step_avg:33.04ms
step:708/2160 train_time:23393ms step_avg:33.04ms
step:709/2160 train_time:23452ms step_avg:33.08ms
step:710/2160 train_time:23511ms step_avg:33.11ms
step:711/2160 train_time:23571ms step_avg:33.15ms
step:712/2160 train_time:23630ms step_avg:33.19ms
step:713/2160 train_time:23691ms step_avg:33.23ms
step:714/2160 train_time:23750ms step_avg:33.26ms
step:715/2160 train_time:23811ms step_avg:33.30ms
step:716/2160 train_time:23870ms step_avg:33.34ms
step:717/2160 train_time:23930ms step_avg:33.38ms
step:718/2160 train_time:23989ms step_avg:33.41ms
step:719/2160 train_time:24049ms step_avg:33.45ms
step:720/2160 train_time:24108ms step_avg:33.48ms
step:721/2160 train_time:24168ms step_avg:33.52ms
step:722/2160 train_time:24227ms step_avg:33.56ms
step:723/2160 train_time:24287ms step_avg:33.59ms
step:724/2160 train_time:24345ms step_avg:33.63ms
step:725/2160 train_time:24405ms step_avg:33.66ms
step:726/2160 train_time:24463ms step_avg:33.70ms
step:727/2160 train_time:24523ms step_avg:33.73ms
step:728/2160 train_time:24582ms step_avg:33.77ms
step:729/2160 train_time:24642ms step_avg:33.80ms
step:730/2160 train_time:24701ms step_avg:33.84ms
step:731/2160 train_time:24762ms step_avg:33.87ms
step:732/2160 train_time:24821ms step_avg:33.91ms
step:733/2160 train_time:24881ms step_avg:33.94ms
step:734/2160 train_time:24941ms step_avg:33.98ms
step:735/2160 train_time:25001ms step_avg:34.02ms
step:736/2160 train_time:25061ms step_avg:34.05ms
step:737/2160 train_time:25121ms step_avg:34.09ms
step:738/2160 train_time:25180ms step_avg:34.12ms
step:739/2160 train_time:25240ms step_avg:34.15ms
step:740/2160 train_time:25299ms step_avg:34.19ms
step:741/2160 train_time:25359ms step_avg:34.22ms
step:742/2160 train_time:25418ms step_avg:34.26ms
step:743/2160 train_time:25479ms step_avg:34.29ms
step:744/2160 train_time:25538ms step_avg:34.33ms
step:745/2160 train_time:25599ms step_avg:34.36ms
step:746/2160 train_time:25658ms step_avg:34.39ms
step:747/2160 train_time:25719ms step_avg:34.43ms
step:748/2160 train_time:25778ms step_avg:34.46ms
step:749/2160 train_time:25839ms step_avg:34.50ms
step:750/2160 train_time:25898ms step_avg:34.53ms
step:750/2160 val_loss:3.8703 train_time:25961ms step_avg:34.61ms
step:751/2160 train_time:25983ms step_avg:34.60ms
step:752/2160 train_time:26019ms step_avg:34.60ms
step:753/2160 train_time:26084ms step_avg:34.64ms
step:754/2160 train_time:26147ms step_avg:34.68ms
step:755/2160 train_time:26208ms step_avg:34.71ms
step:756/2160 train_time:26266ms step_avg:34.74ms
step:757/2160 train_time:26326ms step_avg:34.78ms
step:758/2160 train_time:26383ms step_avg:34.81ms
step:759/2160 train_time:26443ms step_avg:34.84ms
step:760/2160 train_time:26501ms step_avg:34.87ms
step:761/2160 train_time:26560ms step_avg:34.90ms
step:762/2160 train_time:26618ms step_avg:34.93ms
step:763/2160 train_time:26677ms step_avg:34.96ms
step:764/2160 train_time:26736ms step_avg:34.99ms
step:765/2160 train_time:26796ms step_avg:35.03ms
step:766/2160 train_time:26855ms step_avg:35.06ms
step:767/2160 train_time:26916ms step_avg:35.09ms
step:768/2160 train_time:26977ms step_avg:35.13ms
step:769/2160 train_time:27040ms step_avg:35.16ms
step:770/2160 train_time:27101ms step_avg:35.20ms
step:771/2160 train_time:27163ms step_avg:35.23ms
step:772/2160 train_time:27222ms step_avg:35.26ms
step:773/2160 train_time:27283ms step_avg:35.29ms
step:774/2160 train_time:27342ms step_avg:35.32ms
step:775/2160 train_time:27401ms step_avg:35.36ms
step:776/2160 train_time:27460ms step_avg:35.39ms
step:777/2160 train_time:27520ms step_avg:35.42ms
step:778/2160 train_time:27578ms step_avg:35.45ms
step:779/2160 train_time:27637ms step_avg:35.48ms
step:780/2160 train_time:27696ms step_avg:35.51ms
step:781/2160 train_time:27755ms step_avg:35.54ms
step:782/2160 train_time:27815ms step_avg:35.57ms
step:783/2160 train_time:27877ms step_avg:35.60ms
step:784/2160 train_time:27937ms step_avg:35.63ms
step:785/2160 train_time:28000ms step_avg:35.67ms
step:786/2160 train_time:28061ms step_avg:35.70ms
step:787/2160 train_time:28122ms step_avg:35.73ms
step:788/2160 train_time:28182ms step_avg:35.76ms
step:789/2160 train_time:28243ms step_avg:35.80ms
step:790/2160 train_time:28302ms step_avg:35.83ms
step:791/2160 train_time:28361ms step_avg:35.86ms
step:792/2160 train_time:28420ms step_avg:35.88ms
step:793/2160 train_time:28480ms step_avg:35.91ms
step:794/2160 train_time:28538ms step_avg:35.94ms
step:795/2160 train_time:28598ms step_avg:35.97ms
step:796/2160 train_time:28657ms step_avg:36.00ms
step:797/2160 train_time:28716ms step_avg:36.03ms
step:798/2160 train_time:28774ms step_avg:36.06ms
step:799/2160 train_time:28834ms step_avg:36.09ms
step:800/2160 train_time:28894ms step_avg:36.12ms
step:801/2160 train_time:28956ms step_avg:36.15ms
step:802/2160 train_time:29017ms step_avg:36.18ms
step:803/2160 train_time:29081ms step_avg:36.22ms
step:804/2160 train_time:29141ms step_avg:36.24ms
step:805/2160 train_time:29202ms step_avg:36.28ms
step:806/2160 train_time:29261ms step_avg:36.30ms
step:807/2160 train_time:29322ms step_avg:36.33ms
step:808/2160 train_time:29381ms step_avg:36.36ms
step:809/2160 train_time:29441ms step_avg:36.39ms
step:810/2160 train_time:29500ms step_avg:36.42ms
step:811/2160 train_time:29559ms step_avg:36.45ms
step:812/2160 train_time:29618ms step_avg:36.47ms
step:813/2160 train_time:29678ms step_avg:36.50ms
step:814/2160 train_time:29736ms step_avg:36.53ms
step:815/2160 train_time:29796ms step_avg:36.56ms
step:816/2160 train_time:29855ms step_avg:36.59ms
step:817/2160 train_time:29916ms step_avg:36.62ms
step:818/2160 train_time:29975ms step_avg:36.64ms
step:819/2160 train_time:30038ms step_avg:36.68ms
step:820/2160 train_time:30097ms step_avg:36.70ms
step:821/2160 train_time:30159ms step_avg:36.73ms
step:822/2160 train_time:30219ms step_avg:36.76ms
step:823/2160 train_time:30280ms step_avg:36.79ms
step:824/2160 train_time:30340ms step_avg:36.82ms
step:825/2160 train_time:30400ms step_avg:36.85ms
step:826/2160 train_time:30459ms step_avg:36.88ms
step:827/2160 train_time:30519ms step_avg:36.90ms
step:828/2160 train_time:30578ms step_avg:36.93ms
step:829/2160 train_time:30638ms step_avg:36.96ms
step:830/2160 train_time:30697ms step_avg:36.98ms
step:831/2160 train_time:30757ms step_avg:37.01ms
step:832/2160 train_time:30816ms step_avg:37.04ms
step:833/2160 train_time:30877ms step_avg:37.07ms
step:834/2160 train_time:30937ms step_avg:37.09ms
step:835/2160 train_time:30998ms step_avg:37.12ms
step:836/2160 train_time:31058ms step_avg:37.15ms
step:837/2160 train_time:31120ms step_avg:37.18ms
step:838/2160 train_time:31180ms step_avg:37.21ms
step:839/2160 train_time:31241ms step_avg:37.24ms
step:840/2160 train_time:31300ms step_avg:37.26ms
step:841/2160 train_time:31360ms step_avg:37.29ms
step:842/2160 train_time:31419ms step_avg:37.31ms
step:843/2160 train_time:31480ms step_avg:37.34ms
step:844/2160 train_time:31539ms step_avg:37.37ms
step:845/2160 train_time:31599ms step_avg:37.39ms
step:846/2160 train_time:31657ms step_avg:37.42ms
step:847/2160 train_time:31717ms step_avg:37.45ms
step:848/2160 train_time:31776ms step_avg:37.47ms
step:849/2160 train_time:31836ms step_avg:37.50ms
step:850/2160 train_time:31895ms step_avg:37.52ms
step:851/2160 train_time:31956ms step_avg:37.55ms
step:852/2160 train_time:32015ms step_avg:37.58ms
step:853/2160 train_time:32077ms step_avg:37.60ms
step:854/2160 train_time:32137ms step_avg:37.63ms
step:855/2160 train_time:32199ms step_avg:37.66ms
step:856/2160 train_time:32258ms step_avg:37.68ms
step:857/2160 train_time:32319ms step_avg:37.71ms
step:858/2160 train_time:32378ms step_avg:37.74ms
step:859/2160 train_time:32439ms step_avg:37.76ms
step:860/2160 train_time:32498ms step_avg:37.79ms
step:861/2160 train_time:32558ms step_avg:37.81ms
step:862/2160 train_time:32616ms step_avg:37.84ms
step:863/2160 train_time:32677ms step_avg:37.86ms
step:864/2160 train_time:32737ms step_avg:37.89ms
step:865/2160 train_time:32798ms step_avg:37.92ms
step:866/2160 train_time:32856ms step_avg:37.94ms
step:867/2160 train_time:32917ms step_avg:37.97ms
step:868/2160 train_time:32976ms step_avg:37.99ms
step:869/2160 train_time:33037ms step_avg:38.02ms
step:870/2160 train_time:33097ms step_avg:38.04ms
step:871/2160 train_time:33158ms step_avg:38.07ms
step:872/2160 train_time:33217ms step_avg:38.09ms
step:873/2160 train_time:33279ms step_avg:38.12ms
step:874/2160 train_time:33338ms step_avg:38.14ms
step:875/2160 train_time:33399ms step_avg:38.17ms
step:876/2160 train_time:33458ms step_avg:38.19ms
step:877/2160 train_time:33518ms step_avg:38.22ms
step:878/2160 train_time:33577ms step_avg:38.24ms
step:879/2160 train_time:33638ms step_avg:38.27ms
step:880/2160 train_time:33697ms step_avg:38.29ms
step:881/2160 train_time:33758ms step_avg:38.32ms
step:882/2160 train_time:33817ms step_avg:38.34ms
step:883/2160 train_time:33879ms step_avg:38.37ms
step:884/2160 train_time:33938ms step_avg:38.39ms
step:885/2160 train_time:33998ms step_avg:38.42ms
step:886/2160 train_time:34058ms step_avg:38.44ms
step:887/2160 train_time:34118ms step_avg:38.46ms
step:888/2160 train_time:34178ms step_avg:38.49ms
step:889/2160 train_time:34240ms step_avg:38.51ms
step:890/2160 train_time:34299ms step_avg:38.54ms
step:891/2160 train_time:34360ms step_avg:38.56ms
step:892/2160 train_time:34419ms step_avg:38.59ms
step:893/2160 train_time:34479ms step_avg:38.61ms
step:894/2160 train_time:34538ms step_avg:38.63ms
step:895/2160 train_time:34598ms step_avg:38.66ms
step:896/2160 train_time:34657ms step_avg:38.68ms
step:897/2160 train_time:34718ms step_avg:38.70ms
step:898/2160 train_time:34777ms step_avg:38.73ms
step:899/2160 train_time:34838ms step_avg:38.75ms
step:900/2160 train_time:34897ms step_avg:38.77ms
step:901/2160 train_time:34958ms step_avg:38.80ms
step:902/2160 train_time:35018ms step_avg:38.82ms
step:903/2160 train_time:35078ms step_avg:38.85ms
step:904/2160 train_time:35138ms step_avg:38.87ms
step:905/2160 train_time:35199ms step_avg:38.89ms
step:906/2160 train_time:35257ms step_avg:38.92ms
step:907/2160 train_time:35318ms step_avg:38.94ms
step:908/2160 train_time:35378ms step_avg:38.96ms
step:909/2160 train_time:35440ms step_avg:38.99ms
step:910/2160 train_time:35499ms step_avg:39.01ms
step:911/2160 train_time:35559ms step_avg:39.03ms
step:912/2160 train_time:35619ms step_avg:39.06ms
step:913/2160 train_time:35679ms step_avg:39.08ms
step:914/2160 train_time:35739ms step_avg:39.10ms
step:915/2160 train_time:35800ms step_avg:39.13ms
step:916/2160 train_time:35859ms step_avg:39.15ms
step:917/2160 train_time:35919ms step_avg:39.17ms
step:918/2160 train_time:35978ms step_avg:39.19ms
step:919/2160 train_time:36039ms step_avg:39.22ms
step:920/2160 train_time:36099ms step_avg:39.24ms
step:921/2160 train_time:36160ms step_avg:39.26ms
step:922/2160 train_time:36220ms step_avg:39.28ms
step:923/2160 train_time:36282ms step_avg:39.31ms
step:924/2160 train_time:36340ms step_avg:39.33ms
step:925/2160 train_time:36401ms step_avg:39.35ms
step:926/2160 train_time:36461ms step_avg:39.37ms
step:927/2160 train_time:36520ms step_avg:39.40ms
step:928/2160 train_time:36579ms step_avg:39.42ms
step:929/2160 train_time:36640ms step_avg:39.44ms
step:930/2160 train_time:36699ms step_avg:39.46ms
step:931/2160 train_time:36759ms step_avg:39.48ms
step:932/2160 train_time:36818ms step_avg:39.50ms
step:933/2160 train_time:36878ms step_avg:39.53ms
step:934/2160 train_time:36938ms step_avg:39.55ms
step:935/2160 train_time:36998ms step_avg:39.57ms
step:936/2160 train_time:37057ms step_avg:39.59ms
step:937/2160 train_time:37117ms step_avg:39.61ms
step:938/2160 train_time:37177ms step_avg:39.63ms
step:939/2160 train_time:37238ms step_avg:39.66ms
step:940/2160 train_time:37299ms step_avg:39.68ms
step:941/2160 train_time:37358ms step_avg:39.70ms
step:942/2160 train_time:37418ms step_avg:39.72ms
step:943/2160 train_time:37479ms step_avg:39.74ms
step:944/2160 train_time:37539ms step_avg:39.77ms
step:945/2160 train_time:37599ms step_avg:39.79ms
step:946/2160 train_time:37658ms step_avg:39.81ms
step:947/2160 train_time:37719ms step_avg:39.83ms
step:948/2160 train_time:37778ms step_avg:39.85ms
step:949/2160 train_time:37839ms step_avg:39.87ms
step:950/2160 train_time:37899ms step_avg:39.89ms
step:951/2160 train_time:37959ms step_avg:39.92ms
step:952/2160 train_time:38019ms step_avg:39.94ms
step:953/2160 train_time:38080ms step_avg:39.96ms
step:954/2160 train_time:38139ms step_avg:39.98ms
step:955/2160 train_time:38201ms step_avg:40.00ms
step:956/2160 train_time:38260ms step_avg:40.02ms
step:957/2160 train_time:38321ms step_avg:40.04ms
step:958/2160 train_time:38379ms step_avg:40.06ms
step:959/2160 train_time:38440ms step_avg:40.08ms
step:960/2160 train_time:38500ms step_avg:40.10ms
step:961/2160 train_time:38559ms step_avg:40.12ms
step:962/2160 train_time:38619ms step_avg:40.14ms
step:963/2160 train_time:38680ms step_avg:40.17ms
step:964/2160 train_time:38739ms step_avg:40.19ms
step:965/2160 train_time:38800ms step_avg:40.21ms
step:966/2160 train_time:38860ms step_avg:40.23ms
step:967/2160 train_time:38920ms step_avg:40.25ms
step:968/2160 train_time:38979ms step_avg:40.27ms
step:969/2160 train_time:39040ms step_avg:40.29ms
step:970/2160 train_time:39100ms step_avg:40.31ms
step:971/2160 train_time:39160ms step_avg:40.33ms
step:972/2160 train_time:39219ms step_avg:40.35ms
step:973/2160 train_time:39280ms step_avg:40.37ms
step:974/2160 train_time:39339ms step_avg:40.39ms
step:975/2160 train_time:39400ms step_avg:40.41ms
step:976/2160 train_time:39460ms step_avg:40.43ms
step:977/2160 train_time:39520ms step_avg:40.45ms
step:978/2160 train_time:39580ms step_avg:40.47ms
step:979/2160 train_time:39641ms step_avg:40.49ms
step:980/2160 train_time:39700ms step_avg:40.51ms
step:981/2160 train_time:39760ms step_avg:40.53ms
step:982/2160 train_time:39820ms step_avg:40.55ms
step:983/2160 train_time:39881ms step_avg:40.57ms
step:984/2160 train_time:39940ms step_avg:40.59ms
step:985/2160 train_time:40001ms step_avg:40.61ms
step:986/2160 train_time:40061ms step_avg:40.63ms
step:987/2160 train_time:40120ms step_avg:40.65ms
step:988/2160 train_time:40180ms step_avg:40.67ms
step:989/2160 train_time:40241ms step_avg:40.69ms
step:990/2160 train_time:40299ms step_avg:40.71ms
step:991/2160 train_time:40360ms step_avg:40.73ms
step:992/2160 train_time:40419ms step_avg:40.75ms
step:993/2160 train_time:40480ms step_avg:40.77ms
step:994/2160 train_time:40540ms step_avg:40.78ms
step:995/2160 train_time:40600ms step_avg:40.80ms
step:996/2160 train_time:40659ms step_avg:40.82ms
step:997/2160 train_time:40719ms step_avg:40.84ms
step:998/2160 train_time:40778ms step_avg:40.86ms
step:999/2160 train_time:40839ms step_avg:40.88ms
step:1000/2160 train_time:40899ms step_avg:40.90ms
step:1000/2160 val_loss:3.7175 train_time:40962ms step_avg:40.96ms
step:1001/2160 train_time:40984ms step_avg:40.94ms
step:1002/2160 train_time:41021ms step_avg:40.94ms
step:1003/2160 train_time:41085ms step_avg:40.96ms
step:1004/2160 train_time:41146ms step_avg:40.98ms
step:1005/2160 train_time:41207ms step_avg:41.00ms
step:1006/2160 train_time:41266ms step_avg:41.02ms
step:1007/2160 train_time:41326ms step_avg:41.04ms
step:1008/2160 train_time:41384ms step_avg:41.06ms
step:1009/2160 train_time:41443ms step_avg:41.07ms
step:1010/2160 train_time:41501ms step_avg:41.09ms
step:1011/2160 train_time:41561ms step_avg:41.11ms
step:1012/2160 train_time:41619ms step_avg:41.13ms
step:1013/2160 train_time:41679ms step_avg:41.14ms
step:1014/2160 train_time:41737ms step_avg:41.16ms
step:1015/2160 train_time:41797ms step_avg:41.18ms
step:1016/2160 train_time:41855ms step_avg:41.20ms
step:1017/2160 train_time:41917ms step_avg:41.22ms
step:1018/2160 train_time:41978ms step_avg:41.24ms
step:1019/2160 train_time:42040ms step_avg:41.26ms
step:1020/2160 train_time:42101ms step_avg:41.28ms
step:1021/2160 train_time:42162ms step_avg:41.29ms
step:1022/2160 train_time:42221ms step_avg:41.31ms
step:1023/2160 train_time:42281ms step_avg:41.33ms
step:1024/2160 train_time:42340ms step_avg:41.35ms
step:1025/2160 train_time:42399ms step_avg:41.37ms
step:1026/2160 train_time:42458ms step_avg:41.38ms
step:1027/2160 train_time:42518ms step_avg:41.40ms
step:1028/2160 train_time:42577ms step_avg:41.42ms
step:1029/2160 train_time:42636ms step_avg:41.43ms
step:1030/2160 train_time:42695ms step_avg:41.45ms
step:1031/2160 train_time:42754ms step_avg:41.47ms
step:1032/2160 train_time:42814ms step_avg:41.49ms
step:1033/2160 train_time:42875ms step_avg:41.51ms
step:1034/2160 train_time:42935ms step_avg:41.52ms
step:1035/2160 train_time:42997ms step_avg:41.54ms
step:1036/2160 train_time:43057ms step_avg:41.56ms
step:1037/2160 train_time:43120ms step_avg:41.58ms
step:1038/2160 train_time:43180ms step_avg:41.60ms
step:1039/2160 train_time:43241ms step_avg:41.62ms
step:1040/2160 train_time:43300ms step_avg:41.63ms
step:1041/2160 train_time:43360ms step_avg:41.65ms
step:1042/2160 train_time:43418ms step_avg:41.67ms
step:1043/2160 train_time:43478ms step_avg:41.69ms
step:1044/2160 train_time:43537ms step_avg:41.70ms
step:1045/2160 train_time:43597ms step_avg:41.72ms
step:1046/2160 train_time:43655ms step_avg:41.74ms
step:1047/2160 train_time:43715ms step_avg:41.75ms
step:1048/2160 train_time:43774ms step_avg:41.77ms
step:1049/2160 train_time:43835ms step_avg:41.79ms
step:1050/2160 train_time:43894ms step_avg:41.80ms
step:1051/2160 train_time:43956ms step_avg:41.82ms
step:1052/2160 train_time:44016ms step_avg:41.84ms
step:1053/2160 train_time:44078ms step_avg:41.86ms
step:1054/2160 train_time:44138ms step_avg:41.88ms
step:1055/2160 train_time:44199ms step_avg:41.89ms
step:1056/2160 train_time:44258ms step_avg:41.91ms
step:1057/2160 train_time:44319ms step_avg:41.93ms
step:1058/2160 train_time:44379ms step_avg:41.95ms
step:1059/2160 train_time:44439ms step_avg:41.96ms
step:1060/2160 train_time:44498ms step_avg:41.98ms
step:1061/2160 train_time:44558ms step_avg:42.00ms
step:1062/2160 train_time:44617ms step_avg:42.01ms
step:1063/2160 train_time:44676ms step_avg:42.03ms
step:1064/2160 train_time:44735ms step_avg:42.04ms
step:1065/2160 train_time:44796ms step_avg:42.06ms
step:1066/2160 train_time:44856ms step_avg:42.08ms
step:1067/2160 train_time:44917ms step_avg:42.10ms
step:1068/2160 train_time:44977ms step_avg:42.11ms
step:1069/2160 train_time:45038ms step_avg:42.13ms
step:1070/2160 train_time:45098ms step_avg:42.15ms
step:1071/2160 train_time:45159ms step_avg:42.17ms
step:1072/2160 train_time:45219ms step_avg:42.18ms
step:1073/2160 train_time:45280ms step_avg:42.20ms
step:1074/2160 train_time:45340ms step_avg:42.22ms
step:1075/2160 train_time:45400ms step_avg:42.23ms
step:1076/2160 train_time:45459ms step_avg:42.25ms
step:1077/2160 train_time:45519ms step_avg:42.26ms
step:1078/2160 train_time:45578ms step_avg:42.28ms
step:1079/2160 train_time:45638ms step_avg:42.30ms
step:1080/2160 train_time:45696ms step_avg:42.31ms
step:1081/2160 train_time:45757ms step_avg:42.33ms
step:1082/2160 train_time:45816ms step_avg:42.34ms
step:1083/2160 train_time:45877ms step_avg:42.36ms
step:1084/2160 train_time:45938ms step_avg:42.38ms
step:1085/2160 train_time:45999ms step_avg:42.40ms
step:1086/2160 train_time:46059ms step_avg:42.41ms
step:1087/2160 train_time:46120ms step_avg:42.43ms
step:1088/2160 train_time:46179ms step_avg:42.44ms
step:1089/2160 train_time:46240ms step_avg:42.46ms
step:1090/2160 train_time:46299ms step_avg:42.48ms
step:1091/2160 train_time:46359ms step_avg:42.49ms
step:1092/2160 train_time:46418ms step_avg:42.51ms
step:1093/2160 train_time:46479ms step_avg:42.52ms
step:1094/2160 train_time:46538ms step_avg:42.54ms
step:1095/2160 train_time:46598ms step_avg:42.56ms
step:1096/2160 train_time:46657ms step_avg:42.57ms
step:1097/2160 train_time:46718ms step_avg:42.59ms
step:1098/2160 train_time:46776ms step_avg:42.60ms
step:1099/2160 train_time:46837ms step_avg:42.62ms
step:1100/2160 train_time:46896ms step_avg:42.63ms
step:1101/2160 train_time:46958ms step_avg:42.65ms
step:1102/2160 train_time:47018ms step_avg:42.67ms
step:1103/2160 train_time:47079ms step_avg:42.68ms
step:1104/2160 train_time:47138ms step_avg:42.70ms
step:1105/2160 train_time:47200ms step_avg:42.71ms
step:1106/2160 train_time:47259ms step_avg:42.73ms
step:1107/2160 train_time:47320ms step_avg:42.75ms
step:1108/2160 train_time:47379ms step_avg:42.76ms
step:1109/2160 train_time:47439ms step_avg:42.78ms
step:1110/2160 train_time:47499ms step_avg:42.79ms
step:1111/2160 train_time:47559ms step_avg:42.81ms
step:1112/2160 train_time:47618ms step_avg:42.82ms
step:1113/2160 train_time:47678ms step_avg:42.84ms
step:1114/2160 train_time:47737ms step_avg:42.85ms
step:1115/2160 train_time:47798ms step_avg:42.87ms
step:1116/2160 train_time:47857ms step_avg:42.88ms
step:1117/2160 train_time:47918ms step_avg:42.90ms
step:1118/2160 train_time:47977ms step_avg:42.91ms
step:1119/2160 train_time:48038ms step_avg:42.93ms
step:1120/2160 train_time:48098ms step_avg:42.94ms
step:1121/2160 train_time:48159ms step_avg:42.96ms
step:1122/2160 train_time:48218ms step_avg:42.98ms
step:1123/2160 train_time:48279ms step_avg:42.99ms
step:1124/2160 train_time:48338ms step_avg:43.01ms
step:1125/2160 train_time:48398ms step_avg:43.02ms
step:1126/2160 train_time:48458ms step_avg:43.04ms
step:1127/2160 train_time:48519ms step_avg:43.05ms
step:1128/2160 train_time:48578ms step_avg:43.07ms
step:1129/2160 train_time:48639ms step_avg:43.08ms
step:1130/2160 train_time:48698ms step_avg:43.10ms
step:1131/2160 train_time:48759ms step_avg:43.11ms
step:1132/2160 train_time:48818ms step_avg:43.13ms
step:1133/2160 train_time:48879ms step_avg:43.14ms
step:1134/2160 train_time:48939ms step_avg:43.16ms
step:1135/2160 train_time:48999ms step_avg:43.17ms
step:1136/2160 train_time:49058ms step_avg:43.18ms
step:1137/2160 train_time:49119ms step_avg:43.20ms
step:1138/2160 train_time:49179ms step_avg:43.21ms
step:1139/2160 train_time:49239ms step_avg:43.23ms
step:1140/2160 train_time:49299ms step_avg:43.24ms
step:1141/2160 train_time:49360ms step_avg:43.26ms
step:1142/2160 train_time:49419ms step_avg:43.27ms
step:1143/2160 train_time:49480ms step_avg:43.29ms
step:1144/2160 train_time:49539ms step_avg:43.30ms
step:1145/2160 train_time:49599ms step_avg:43.32ms
step:1146/2160 train_time:49658ms step_avg:43.33ms
step:1147/2160 train_time:49720ms step_avg:43.35ms
step:1148/2160 train_time:49778ms step_avg:43.36ms
step:1149/2160 train_time:49838ms step_avg:43.38ms
step:1150/2160 train_time:49898ms step_avg:43.39ms
step:1151/2160 train_time:49959ms step_avg:43.40ms
step:1152/2160 train_time:50018ms step_avg:43.42ms
step:1153/2160 train_time:50079ms step_avg:43.43ms
step:1154/2160 train_time:50139ms step_avg:43.45ms
step:1155/2160 train_time:50200ms step_avg:43.46ms
step:1156/2160 train_time:50259ms step_avg:43.48ms
step:1157/2160 train_time:50319ms step_avg:43.49ms
step:1158/2160 train_time:50378ms step_avg:43.50ms
step:1159/2160 train_time:50439ms step_avg:43.52ms
step:1160/2160 train_time:50499ms step_avg:43.53ms
step:1161/2160 train_time:50559ms step_avg:43.55ms
step:1162/2160 train_time:50619ms step_avg:43.56ms
step:1163/2160 train_time:50680ms step_avg:43.58ms
step:1164/2160 train_time:50738ms step_avg:43.59ms
step:1165/2160 train_time:50799ms step_avg:43.60ms
step:1166/2160 train_time:50858ms step_avg:43.62ms
step:1167/2160 train_time:50920ms step_avg:43.63ms
step:1168/2160 train_time:50978ms step_avg:43.65ms
step:1169/2160 train_time:51039ms step_avg:43.66ms
step:1170/2160 train_time:51099ms step_avg:43.67ms
step:1171/2160 train_time:51160ms step_avg:43.69ms
step:1172/2160 train_time:51219ms step_avg:43.70ms
step:1173/2160 train_time:51279ms step_avg:43.72ms
step:1174/2160 train_time:51339ms step_avg:43.73ms
step:1175/2160 train_time:51399ms step_avg:43.74ms
step:1176/2160 train_time:51458ms step_avg:43.76ms
step:1177/2160 train_time:51519ms step_avg:43.77ms
step:1178/2160 train_time:51579ms step_avg:43.78ms
step:1179/2160 train_time:51639ms step_avg:43.80ms
step:1180/2160 train_time:51698ms step_avg:43.81ms
step:1181/2160 train_time:51759ms step_avg:43.83ms
step:1182/2160 train_time:51819ms step_avg:43.84ms
step:1183/2160 train_time:51879ms step_avg:43.85ms
step:1184/2160 train_time:51938ms step_avg:43.87ms
step:1185/2160 train_time:51999ms step_avg:43.88ms
step:1186/2160 train_time:52058ms step_avg:43.89ms
step:1187/2160 train_time:52120ms step_avg:43.91ms
step:1188/2160 train_time:52179ms step_avg:43.92ms
step:1189/2160 train_time:52239ms step_avg:43.93ms
step:1190/2160 train_time:52298ms step_avg:43.95ms
step:1191/2160 train_time:52359ms step_avg:43.96ms
step:1192/2160 train_time:52419ms step_avg:43.98ms
step:1193/2160 train_time:52480ms step_avg:43.99ms
step:1194/2160 train_time:52541ms step_avg:44.00ms
step:1195/2160 train_time:52601ms step_avg:44.02ms
step:1196/2160 train_time:52660ms step_avg:44.03ms
step:1197/2160 train_time:52720ms step_avg:44.04ms
step:1198/2160 train_time:52779ms step_avg:44.06ms
step:1199/2160 train_time:52839ms step_avg:44.07ms
step:1200/2160 train_time:52898ms step_avg:44.08ms
step:1201/2160 train_time:52959ms step_avg:44.10ms
step:1202/2160 train_time:53018ms step_avg:44.11ms
step:1203/2160 train_time:53079ms step_avg:44.12ms
step:1204/2160 train_time:53139ms step_avg:44.14ms
step:1205/2160 train_time:53199ms step_avg:44.15ms
step:1206/2160 train_time:53259ms step_avg:44.16ms
step:1207/2160 train_time:53320ms step_avg:44.18ms
step:1208/2160 train_time:53379ms step_avg:44.19ms
step:1209/2160 train_time:53440ms step_avg:44.20ms
step:1210/2160 train_time:53500ms step_avg:44.21ms
step:1211/2160 train_time:53560ms step_avg:44.23ms
step:1212/2160 train_time:53619ms step_avg:44.24ms
step:1213/2160 train_time:53680ms step_avg:44.25ms
step:1214/2160 train_time:53740ms step_avg:44.27ms
step:1215/2160 train_time:53800ms step_avg:44.28ms
step:1216/2160 train_time:53859ms step_avg:44.29ms
step:1217/2160 train_time:53920ms step_avg:44.31ms
step:1218/2160 train_time:53979ms step_avg:44.32ms
step:1219/2160 train_time:54039ms step_avg:44.33ms
step:1220/2160 train_time:54099ms step_avg:44.34ms
step:1221/2160 train_time:54159ms step_avg:44.36ms
step:1222/2160 train_time:54219ms step_avg:44.37ms
step:1223/2160 train_time:54280ms step_avg:44.38ms
step:1224/2160 train_time:54340ms step_avg:44.40ms
step:1225/2160 train_time:54400ms step_avg:44.41ms
step:1226/2160 train_time:54459ms step_avg:44.42ms
step:1227/2160 train_time:54521ms step_avg:44.43ms
step:1228/2160 train_time:54580ms step_avg:44.45ms
step:1229/2160 train_time:54640ms step_avg:44.46ms
step:1230/2160 train_time:54700ms step_avg:44.47ms
step:1231/2160 train_time:54761ms step_avg:44.48ms
step:1232/2160 train_time:54820ms step_avg:44.50ms
step:1233/2160 train_time:54880ms step_avg:44.51ms
step:1234/2160 train_time:54940ms step_avg:44.52ms
step:1235/2160 train_time:55001ms step_avg:44.53ms
step:1236/2160 train_time:55060ms step_avg:44.55ms
step:1237/2160 train_time:55120ms step_avg:44.56ms
step:1238/2160 train_time:55179ms step_avg:44.57ms
step:1239/2160 train_time:55239ms step_avg:44.58ms
step:1240/2160 train_time:55298ms step_avg:44.60ms
step:1241/2160 train_time:55359ms step_avg:44.61ms
step:1242/2160 train_time:55418ms step_avg:44.62ms
step:1243/2160 train_time:55479ms step_avg:44.63ms
step:1244/2160 train_time:55539ms step_avg:44.65ms
step:1245/2160 train_time:55600ms step_avg:44.66ms
step:1246/2160 train_time:55659ms step_avg:44.67ms
step:1247/2160 train_time:55720ms step_avg:44.68ms
step:1248/2160 train_time:55779ms step_avg:44.69ms
step:1249/2160 train_time:55839ms step_avg:44.71ms
step:1250/2160 train_time:55898ms step_avg:44.72ms
step:1250/2160 val_loss:3.5996 train_time:55960ms step_avg:44.77ms
step:1251/2160 train_time:55983ms step_avg:44.75ms
step:1252/2160 train_time:56022ms step_avg:44.75ms
step:1253/2160 train_time:56084ms step_avg:44.76ms
step:1254/2160 train_time:56144ms step_avg:44.77ms
step:1255/2160 train_time:56205ms step_avg:44.78ms
step:1256/2160 train_time:56265ms step_avg:44.80ms
step:1257/2160 train_time:56325ms step_avg:44.81ms
step:1258/2160 train_time:56383ms step_avg:44.82ms
step:1259/2160 train_time:56443ms step_avg:44.83ms
step:1260/2160 train_time:56502ms step_avg:44.84ms
step:1261/2160 train_time:56561ms step_avg:44.85ms
step:1262/2160 train_time:56620ms step_avg:44.86ms
step:1263/2160 train_time:56679ms step_avg:44.88ms
step:1264/2160 train_time:56738ms step_avg:44.89ms
step:1265/2160 train_time:56797ms step_avg:44.90ms
step:1266/2160 train_time:56856ms step_avg:44.91ms
step:1267/2160 train_time:56917ms step_avg:44.92ms
step:1268/2160 train_time:56977ms step_avg:44.93ms
step:1269/2160 train_time:57038ms step_avg:44.95ms
step:1270/2160 train_time:57098ms step_avg:44.96ms
step:1271/2160 train_time:57159ms step_avg:44.97ms
step:1272/2160 train_time:57218ms step_avg:44.98ms
step:1273/2160 train_time:57278ms step_avg:44.99ms
step:1274/2160 train_time:57337ms step_avg:45.01ms
step:1275/2160 train_time:57397ms step_avg:45.02ms
step:1276/2160 train_time:57456ms step_avg:45.03ms
step:1277/2160 train_time:57515ms step_avg:45.04ms
step:1278/2160 train_time:57574ms step_avg:45.05ms
step:1279/2160 train_time:57634ms step_avg:45.06ms
step:1280/2160 train_time:57693ms step_avg:45.07ms
step:1281/2160 train_time:57754ms step_avg:45.08ms
step:1282/2160 train_time:57813ms step_avg:45.10ms
step:1283/2160 train_time:57874ms step_avg:45.11ms
step:1284/2160 train_time:57934ms step_avg:45.12ms
step:1285/2160 train_time:57995ms step_avg:45.13ms
step:1286/2160 train_time:58056ms step_avg:45.14ms
step:1287/2160 train_time:58116ms step_avg:45.16ms
step:1288/2160 train_time:58176ms step_avg:45.17ms
step:1289/2160 train_time:58236ms step_avg:45.18ms
step:1290/2160 train_time:58295ms step_avg:45.19ms
step:1291/2160 train_time:58356ms step_avg:45.20ms
step:1292/2160 train_time:58415ms step_avg:45.21ms
step:1293/2160 train_time:58475ms step_avg:45.22ms
step:1294/2160 train_time:58534ms step_avg:45.23ms
step:1295/2160 train_time:58594ms step_avg:45.25ms
step:1296/2160 train_time:58653ms step_avg:45.26ms
step:1297/2160 train_time:58713ms step_avg:45.27ms
step:1298/2160 train_time:58772ms step_avg:45.28ms
step:1299/2160 train_time:58833ms step_avg:45.29ms
step:1300/2160 train_time:58893ms step_avg:45.30ms
step:1301/2160 train_time:58954ms step_avg:45.31ms
step:1302/2160 train_time:59014ms step_avg:45.33ms
step:1303/2160 train_time:59076ms step_avg:45.34ms
step:1304/2160 train_time:59136ms step_avg:45.35ms
step:1305/2160 train_time:59196ms step_avg:45.36ms
step:1306/2160 train_time:59255ms step_avg:45.37ms
step:1307/2160 train_time:59315ms step_avg:45.38ms
step:1308/2160 train_time:59375ms step_avg:45.39ms
step:1309/2160 train_time:59435ms step_avg:45.40ms
step:1310/2160 train_time:59494ms step_avg:45.42ms
step:1311/2160 train_time:59554ms step_avg:45.43ms
step:1312/2160 train_time:59613ms step_avg:45.44ms
step:1313/2160 train_time:59674ms step_avg:45.45ms
step:1314/2160 train_time:59733ms step_avg:45.46ms
step:1315/2160 train_time:59793ms step_avg:45.47ms
step:1316/2160 train_time:59853ms step_avg:45.48ms
step:1317/2160 train_time:59914ms step_avg:45.49ms
step:1318/2160 train_time:59975ms step_avg:45.50ms
step:1319/2160 train_time:60035ms step_avg:45.52ms
step:1320/2160 train_time:60095ms step_avg:45.53ms
step:1321/2160 train_time:60156ms step_avg:45.54ms
step:1322/2160 train_time:60216ms step_avg:45.55ms
step:1323/2160 train_time:60276ms step_avg:45.56ms
step:1324/2160 train_time:60335ms step_avg:45.57ms
step:1325/2160 train_time:60395ms step_avg:45.58ms
step:1326/2160 train_time:60455ms step_avg:45.59ms
step:1327/2160 train_time:60515ms step_avg:45.60ms
step:1328/2160 train_time:60574ms step_avg:45.61ms
step:1329/2160 train_time:60634ms step_avg:45.62ms
step:1330/2160 train_time:60693ms step_avg:45.63ms
step:1331/2160 train_time:60754ms step_avg:45.65ms
step:1332/2160 train_time:60813ms step_avg:45.66ms
step:1333/2160 train_time:60874ms step_avg:45.67ms
step:1334/2160 train_time:60933ms step_avg:45.68ms
step:1335/2160 train_time:60994ms step_avg:45.69ms
step:1336/2160 train_time:61054ms step_avg:45.70ms
step:1337/2160 train_time:61115ms step_avg:45.71ms
step:1338/2160 train_time:61174ms step_avg:45.72ms
step:1339/2160 train_time:61235ms step_avg:45.73ms
step:1340/2160 train_time:61295ms step_avg:45.74ms
step:1341/2160 train_time:61355ms step_avg:45.75ms
step:1342/2160 train_time:61414ms step_avg:45.76ms
step:1343/2160 train_time:61474ms step_avg:45.77ms
step:1344/2160 train_time:61533ms step_avg:45.78ms
step:1345/2160 train_time:61594ms step_avg:45.79ms
step:1346/2160 train_time:61653ms step_avg:45.80ms
step:1347/2160 train_time:61713ms step_avg:45.82ms
step:1348/2160 train_time:61773ms step_avg:45.83ms
step:1349/2160 train_time:61833ms step_avg:45.84ms
step:1350/2160 train_time:61893ms step_avg:45.85ms
step:1351/2160 train_time:61954ms step_avg:45.86ms
step:1352/2160 train_time:62013ms step_avg:45.87ms
step:1353/2160 train_time:62075ms step_avg:45.88ms
step:1354/2160 train_time:62134ms step_avg:45.89ms
step:1355/2160 train_time:62195ms step_avg:45.90ms
step:1356/2160 train_time:62255ms step_avg:45.91ms
step:1357/2160 train_time:62315ms step_avg:45.92ms
step:1358/2160 train_time:62375ms step_avg:45.93ms
step:1359/2160 train_time:62435ms step_avg:45.94ms
step:1360/2160 train_time:62494ms step_avg:45.95ms
step:1361/2160 train_time:62554ms step_avg:45.96ms
step:1362/2160 train_time:62613ms step_avg:45.97ms
step:1363/2160 train_time:62673ms step_avg:45.98ms
step:1364/2160 train_time:62732ms step_avg:45.99ms
step:1365/2160 train_time:62793ms step_avg:46.00ms
step:1366/2160 train_time:62852ms step_avg:46.01ms
step:1367/2160 train_time:62914ms step_avg:46.02ms
step:1368/2160 train_time:62974ms step_avg:46.03ms
step:1369/2160 train_time:63034ms step_avg:46.04ms
step:1370/2160 train_time:63094ms step_avg:46.05ms
step:1371/2160 train_time:63155ms step_avg:46.06ms
step:1372/2160 train_time:63214ms step_avg:46.07ms
step:1373/2160 train_time:63275ms step_avg:46.09ms
step:1374/2160 train_time:63334ms step_avg:46.09ms
step:1375/2160 train_time:63394ms step_avg:46.10ms
step:1376/2160 train_time:63454ms step_avg:46.11ms
step:1377/2160 train_time:63514ms step_avg:46.13ms
step:1378/2160 train_time:63575ms step_avg:46.14ms
step:1379/2160 train_time:63635ms step_avg:46.15ms
step:1380/2160 train_time:63694ms step_avg:46.16ms
step:1381/2160 train_time:63754ms step_avg:46.17ms
step:1382/2160 train_time:63814ms step_avg:46.17ms
step:1383/2160 train_time:63874ms step_avg:46.18ms
step:1384/2160 train_time:63933ms step_avg:46.19ms
step:1385/2160 train_time:63994ms step_avg:46.20ms
step:1386/2160 train_time:64054ms step_avg:46.21ms
step:1387/2160 train_time:64114ms step_avg:46.23ms
step:1388/2160 train_time:64173ms step_avg:46.23ms
step:1389/2160 train_time:64233ms step_avg:46.24ms
step:1390/2160 train_time:64293ms step_avg:46.25ms
step:1391/2160 train_time:64355ms step_avg:46.27ms
step:1392/2160 train_time:64414ms step_avg:46.27ms
step:1393/2160 train_time:64474ms step_avg:46.28ms
step:1394/2160 train_time:64534ms step_avg:46.29ms
step:1395/2160 train_time:64594ms step_avg:46.30ms
step:1396/2160 train_time:64654ms step_avg:46.31ms
step:1397/2160 train_time:64714ms step_avg:46.32ms
step:1398/2160 train_time:64774ms step_avg:46.33ms
step:1399/2160 train_time:64834ms step_avg:46.34ms
step:1400/2160 train_time:64894ms step_avg:46.35ms
step:1401/2160 train_time:64955ms step_avg:46.36ms
step:1402/2160 train_time:65014ms step_avg:46.37ms
step:1403/2160 train_time:65075ms step_avg:46.38ms
step:1404/2160 train_time:65134ms step_avg:46.39ms
step:1405/2160 train_time:65194ms step_avg:46.40ms
step:1406/2160 train_time:65253ms step_avg:46.41ms
step:1407/2160 train_time:65315ms step_avg:46.42ms
step:1408/2160 train_time:65374ms step_avg:46.43ms
step:1409/2160 train_time:65434ms step_avg:46.44ms
step:1410/2160 train_time:65493ms step_avg:46.45ms
step:1411/2160 train_time:65554ms step_avg:46.46ms
step:1412/2160 train_time:65613ms step_avg:46.47ms
step:1413/2160 train_time:65674ms step_avg:46.48ms
step:1414/2160 train_time:65733ms step_avg:46.49ms
step:1415/2160 train_time:65794ms step_avg:46.50ms
step:1416/2160 train_time:65883ms step_avg:46.53ms
step:1417/2160 train_time:65972ms step_avg:46.56ms
step:1418/2160 train_time:66058ms step_avg:46.59ms
step:1419/2160 train_time:66146ms step_avg:46.61ms
step:1420/2160 train_time:66232ms step_avg:46.64ms
step:1421/2160 train_time:66322ms step_avg:46.67ms
step:1422/2160 train_time:66409ms step_avg:46.70ms
step:1423/2160 train_time:66497ms step_avg:46.73ms
step:1424/2160 train_time:66584ms step_avg:46.76ms
step:1425/2160 train_time:66671ms step_avg:46.79ms
step:1426/2160 train_time:66758ms step_avg:46.82ms
step:1427/2160 train_time:66848ms step_avg:46.84ms
step:1428/2160 train_time:66934ms step_avg:46.87ms
step:1429/2160 train_time:67023ms step_avg:46.90ms
step:1430/2160 train_time:67109ms step_avg:46.93ms
step:1431/2160 train_time:67198ms step_avg:46.96ms
step:1432/2160 train_time:67286ms step_avg:46.99ms
step:1433/2160 train_time:67374ms step_avg:47.02ms
step:1434/2160 train_time:67460ms step_avg:47.04ms
step:1435/2160 train_time:67549ms step_avg:47.07ms
step:1436/2160 train_time:67635ms step_avg:47.10ms
step:1437/2160 train_time:67724ms step_avg:47.13ms
step:1438/2160 train_time:67811ms step_avg:47.16ms
step:1439/2160 train_time:67899ms step_avg:47.18ms
step:1440/2160 train_time:67986ms step_avg:47.21ms
step:1441/2160 train_time:68074ms step_avg:47.24ms
step:1442/2160 train_time:68161ms step_avg:47.27ms
step:1443/2160 train_time:68250ms step_avg:47.30ms
step:1444/2160 train_time:68337ms step_avg:47.32ms
step:1445/2160 train_time:68427ms step_avg:47.35ms
step:1446/2160 train_time:68514ms step_avg:47.38ms
step:1447/2160 train_time:68602ms step_avg:47.41ms
step:1448/2160 train_time:68689ms step_avg:47.44ms
step:1449/2160 train_time:68777ms step_avg:47.47ms
step:1450/2160 train_time:68864ms step_avg:47.49ms
step:1451/2160 train_time:68953ms step_avg:47.52ms
step:1452/2160 train_time:69040ms step_avg:47.55ms
step:1453/2160 train_time:69129ms step_avg:47.58ms
step:1454/2160 train_time:69216ms step_avg:47.60ms
step:1455/2160 train_time:69305ms step_avg:47.63ms
step:1456/2160 train_time:69391ms step_avg:47.66ms
step:1457/2160 train_time:69480ms step_avg:47.69ms
step:1458/2160 train_time:69567ms step_avg:47.71ms
step:1459/2160 train_time:69655ms step_avg:47.74ms
step:1460/2160 train_time:69742ms step_avg:47.77ms
step:1461/2160 train_time:69830ms step_avg:47.80ms
step:1462/2160 train_time:69916ms step_avg:47.82ms
step:1463/2160 train_time:70005ms step_avg:47.85ms
step:1464/2160 train_time:70091ms step_avg:47.88ms
step:1465/2160 train_time:70180ms step_avg:47.90ms
step:1466/2160 train_time:70267ms step_avg:47.93ms
step:1467/2160 train_time:70355ms step_avg:47.96ms
step:1468/2160 train_time:70443ms step_avg:47.99ms
step:1469/2160 train_time:70532ms step_avg:48.01ms
step:1470/2160 train_time:70619ms step_avg:48.04ms
step:1471/2160 train_time:70708ms step_avg:48.07ms
step:1472/2160 train_time:70795ms step_avg:48.09ms
step:1473/2160 train_time:70882ms step_avg:48.12ms
step:1474/2160 train_time:70968ms step_avg:48.15ms
step:1475/2160 train_time:71056ms step_avg:48.17ms
step:1476/2160 train_time:71143ms step_avg:48.20ms
step:1477/2160 train_time:71232ms step_avg:48.23ms
step:1478/2160 train_time:71318ms step_avg:48.25ms
step:1479/2160 train_time:71408ms step_avg:48.28ms
step:1480/2160 train_time:71495ms step_avg:48.31ms
step:1481/2160 train_time:71584ms step_avg:48.34ms
step:1482/2160 train_time:71671ms step_avg:48.36ms
step:1483/2160 train_time:71759ms step_avg:48.39ms
step:1484/2160 train_time:71846ms step_avg:48.41ms
step:1485/2160 train_time:71933ms step_avg:48.44ms
step:1486/2160 train_time:72021ms step_avg:48.47ms
step:1487/2160 train_time:72110ms step_avg:48.49ms
step:1488/2160 train_time:72196ms step_avg:48.52ms
step:1489/2160 train_time:72285ms step_avg:48.55ms
step:1490/2160 train_time:72371ms step_avg:48.57ms
step:1491/2160 train_time:72459ms step_avg:48.60ms
step:1492/2160 train_time:72546ms step_avg:48.62ms
step:1493/2160 train_time:72633ms step_avg:48.65ms
step:1494/2160 train_time:72720ms step_avg:48.67ms
step:1495/2160 train_time:72809ms step_avg:48.70ms
step:1496/2160 train_time:72895ms step_avg:48.73ms
step:1497/2160 train_time:72984ms step_avg:48.75ms
step:1498/2160 train_time:73070ms step_avg:48.78ms
step:1499/2160 train_time:73159ms step_avg:48.81ms
step:1500/2160 train_time:73247ms step_avg:48.83ms
step:1500/2160 val_loss:3.4966 train_time:73336ms step_avg:48.89ms
step:1501/2160 train_time:73359ms step_avg:48.87ms
step:1502/2160 train_time:73426ms step_avg:48.89ms
step:1503/2160 train_time:73518ms step_avg:48.91ms
step:1504/2160 train_time:73605ms step_avg:48.94ms
step:1505/2160 train_time:73693ms step_avg:48.97ms
step:1506/2160 train_time:73779ms step_avg:48.99ms
step:1507/2160 train_time:73866ms step_avg:49.02ms
step:1508/2160 train_time:73953ms step_avg:49.04ms
step:1509/2160 train_time:74040ms step_avg:49.07ms
step:1510/2160 train_time:74126ms step_avg:49.09ms
step:1511/2160 train_time:74215ms step_avg:49.12ms
step:1512/2160 train_time:74303ms step_avg:49.14ms
step:1513/2160 train_time:74395ms step_avg:49.17ms
step:1514/2160 train_time:74483ms step_avg:49.20ms
step:1515/2160 train_time:74572ms step_avg:49.22ms
step:1516/2160 train_time:74659ms step_avg:49.25ms
step:1517/2160 train_time:74747ms step_avg:49.27ms
step:1518/2160 train_time:74833ms step_avg:49.30ms
step:1519/2160 train_time:74919ms step_avg:49.32ms
step:1520/2160 train_time:75005ms step_avg:49.35ms
step:1521/2160 train_time:75093ms step_avg:49.37ms
step:1522/2160 train_time:75179ms step_avg:49.40ms
step:1523/2160 train_time:75269ms step_avg:49.42ms
step:1524/2160 train_time:75357ms step_avg:49.45ms
step:1525/2160 train_time:75447ms step_avg:49.47ms
step:1526/2160 train_time:75535ms step_avg:49.50ms
step:1527/2160 train_time:75624ms step_avg:49.52ms
step:1528/2160 train_time:75710ms step_avg:49.55ms
step:1529/2160 train_time:75798ms step_avg:49.57ms
step:1530/2160 train_time:75884ms step_avg:49.60ms
step:1531/2160 train_time:75972ms step_avg:49.62ms
step:1532/2160 train_time:76057ms step_avg:49.65ms
step:1533/2160 train_time:76145ms step_avg:49.67ms
step:1534/2160 train_time:76232ms step_avg:49.69ms
step:1535/2160 train_time:76320ms step_avg:49.72ms
step:1536/2160 train_time:76408ms step_avg:49.74ms
step:1537/2160 train_time:76498ms step_avg:49.77ms
step:1538/2160 train_time:76585ms step_avg:49.80ms
step:1539/2160 train_time:76674ms step_avg:49.82ms
step:1540/2160 train_time:76760ms step_avg:49.84ms
step:1541/2160 train_time:76848ms step_avg:49.87ms
step:1542/2160 train_time:76935ms step_avg:49.89ms
step:1543/2160 train_time:77021ms step_avg:49.92ms
step:1544/2160 train_time:77108ms step_avg:49.94ms
step:1545/2160 train_time:77196ms step_avg:49.96ms
step:1546/2160 train_time:77284ms step_avg:49.99ms
step:1547/2160 train_time:77374ms step_avg:50.02ms
step:1548/2160 train_time:77460ms step_avg:50.04ms
step:1549/2160 train_time:77549ms step_avg:50.06ms
step:1550/2160 train_time:77637ms step_avg:50.09ms
step:1551/2160 train_time:77724ms step_avg:50.11ms
step:1552/2160 train_time:77811ms step_avg:50.14ms
step:1553/2160 train_time:77898ms step_avg:50.16ms
step:1554/2160 train_time:77984ms step_avg:50.18ms
step:1555/2160 train_time:78073ms step_avg:50.21ms
step:1556/2160 train_time:78159ms step_avg:50.23ms
step:1557/2160 train_time:78248ms step_avg:50.26ms
step:1558/2160 train_time:78336ms step_avg:50.28ms
step:1559/2160 train_time:78426ms step_avg:50.31ms
step:1560/2160 train_time:78513ms step_avg:50.33ms
step:1561/2160 train_time:78600ms step_avg:50.35ms
step:1562/2160 train_time:78687ms step_avg:50.38ms
step:1563/2160 train_time:78775ms step_avg:50.40ms
step:1564/2160 train_time:78861ms step_avg:50.42ms
step:1565/2160 train_time:78950ms step_avg:50.45ms
step:1566/2160 train_time:79036ms step_avg:50.47ms
step:1567/2160 train_time:79123ms step_avg:50.49ms
step:1568/2160 train_time:79210ms step_avg:50.52ms
step:1569/2160 train_time:79298ms step_avg:50.54ms
step:1570/2160 train_time:79384ms step_avg:50.56ms
step:1571/2160 train_time:79475ms step_avg:50.59ms
step:1572/2160 train_time:79562ms step_avg:50.61ms
step:1573/2160 train_time:79651ms step_avg:50.64ms
step:1574/2160 train_time:79737ms step_avg:50.66ms
step:1575/2160 train_time:79825ms step_avg:50.68ms
step:1576/2160 train_time:79913ms step_avg:50.71ms
step:1577/2160 train_time:80001ms step_avg:50.73ms
step:1578/2160 train_time:80087ms step_avg:50.75ms
step:1579/2160 train_time:80175ms step_avg:50.78ms
step:1580/2160 train_time:80262ms step_avg:50.80ms
step:1581/2160 train_time:80351ms step_avg:50.82ms
step:1582/2160 train_time:80438ms step_avg:50.85ms
step:1583/2160 train_time:80526ms step_avg:50.87ms
step:1584/2160 train_time:80614ms step_avg:50.89ms
step:1585/2160 train_time:80703ms step_avg:50.92ms
step:1586/2160 train_time:80789ms step_avg:50.94ms
step:1587/2160 train_time:80878ms step_avg:50.96ms
step:1588/2160 train_time:80965ms step_avg:50.99ms
step:1589/2160 train_time:81054ms step_avg:51.01ms
step:1590/2160 train_time:81141ms step_avg:51.03ms
step:1591/2160 train_time:81230ms step_avg:51.06ms
step:1592/2160 train_time:81317ms step_avg:51.08ms
step:1593/2160 train_time:81405ms step_avg:51.10ms
step:1594/2160 train_time:81493ms step_avg:51.12ms
step:1595/2160 train_time:81581ms step_avg:51.15ms
step:1596/2160 train_time:81668ms step_avg:51.17ms
step:1597/2160 train_time:81757ms step_avg:51.19ms
step:1598/2160 train_time:81844ms step_avg:51.22ms
step:1599/2160 train_time:81933ms step_avg:51.24ms
step:1600/2160 train_time:82019ms step_avg:51.26ms
step:1601/2160 train_time:82108ms step_avg:51.29ms
step:1602/2160 train_time:82195ms step_avg:51.31ms
step:1603/2160 train_time:82283ms step_avg:51.33ms
step:1604/2160 train_time:82371ms step_avg:51.35ms
step:1605/2160 train_time:82459ms step_avg:51.38ms
step:1606/2160 train_time:82546ms step_avg:51.40ms
step:1607/2160 train_time:82634ms step_avg:51.42ms
step:1608/2160 train_time:82720ms step_avg:51.44ms
step:1609/2160 train_time:82809ms step_avg:51.47ms
step:1610/2160 train_time:82895ms step_avg:51.49ms
step:1611/2160 train_time:82983ms step_avg:51.51ms
step:1612/2160 train_time:83070ms step_avg:51.53ms
step:1613/2160 train_time:83158ms step_avg:51.55ms
step:1614/2160 train_time:83245ms step_avg:51.58ms
step:1615/2160 train_time:83334ms step_avg:51.60ms
step:1616/2160 train_time:83422ms step_avg:51.62ms
step:1617/2160 train_time:83511ms step_avg:51.65ms
step:1618/2160 train_time:83597ms step_avg:51.67ms
step:1619/2160 train_time:83685ms step_avg:51.69ms
step:1620/2160 train_time:83772ms step_avg:51.71ms
step:1621/2160 train_time:83860ms step_avg:51.73ms
step:1622/2160 train_time:83947ms step_avg:51.76ms
step:1623/2160 train_time:84036ms step_avg:51.78ms
step:1624/2160 train_time:84122ms step_avg:51.80ms
step:1625/2160 train_time:84210ms step_avg:51.82ms
step:1626/2160 train_time:84296ms step_avg:51.84ms
step:1627/2160 train_time:84385ms step_avg:51.87ms
step:1628/2160 train_time:84473ms step_avg:51.89ms
step:1629/2160 train_time:84561ms step_avg:51.91ms
step:1630/2160 train_time:84649ms step_avg:51.93ms
step:1631/2160 train_time:84737ms step_avg:51.95ms
step:1632/2160 train_time:84824ms step_avg:51.98ms
step:1633/2160 train_time:84915ms step_avg:52.00ms
step:1634/2160 train_time:85001ms step_avg:52.02ms
step:1635/2160 train_time:85090ms step_avg:52.04ms
step:1636/2160 train_time:85176ms step_avg:52.06ms
step:1637/2160 train_time:85263ms step_avg:52.09ms
step:1638/2160 train_time:85350ms step_avg:52.11ms
step:1639/2160 train_time:85438ms step_avg:52.13ms
step:1640/2160 train_time:85525ms step_avg:52.15ms
step:1641/2160 train_time:85614ms step_avg:52.17ms
step:1642/2160 train_time:85700ms step_avg:52.19ms
step:1643/2160 train_time:85788ms step_avg:52.21ms
step:1644/2160 train_time:85875ms step_avg:52.24ms
step:1645/2160 train_time:85962ms step_avg:52.26ms
step:1646/2160 train_time:86050ms step_avg:52.28ms
step:1647/2160 train_time:86137ms step_avg:52.30ms
step:1648/2160 train_time:86225ms step_avg:52.32ms
step:1649/2160 train_time:86314ms step_avg:52.34ms
step:1650/2160 train_time:86400ms step_avg:52.36ms
step:1651/2160 train_time:86489ms step_avg:52.39ms
step:1652/2160 train_time:86575ms step_avg:52.41ms
step:1653/2160 train_time:86663ms step_avg:52.43ms
step:1654/2160 train_time:86750ms step_avg:52.45ms
step:1655/2160 train_time:86838ms step_avg:52.47ms
step:1656/2160 train_time:86925ms step_avg:52.49ms
step:1657/2160 train_time:87014ms step_avg:52.51ms
step:1658/2160 train_time:87101ms step_avg:52.53ms
step:1659/2160 train_time:87191ms step_avg:52.56ms
step:1660/2160 train_time:87277ms step_avg:52.58ms
step:1661/2160 train_time:87365ms step_avg:52.60ms
step:1662/2160 train_time:87452ms step_avg:52.62ms
step:1663/2160 train_time:87540ms step_avg:52.64ms
step:1664/2160 train_time:87627ms step_avg:52.66ms
step:1665/2160 train_time:87716ms step_avg:52.68ms
step:1666/2160 train_time:87802ms step_avg:52.70ms
step:1667/2160 train_time:87890ms step_avg:52.72ms
step:1668/2160 train_time:87977ms step_avg:52.74ms
step:1669/2160 train_time:88065ms step_avg:52.77ms
step:1670/2160 train_time:88152ms step_avg:52.79ms
step:1671/2160 train_time:88241ms step_avg:52.81ms
step:1672/2160 train_time:88327ms step_avg:52.83ms
step:1673/2160 train_time:88416ms step_avg:52.85ms
step:1674/2160 train_time:88502ms step_avg:52.87ms
step:1675/2160 train_time:88591ms step_avg:52.89ms
step:1676/2160 train_time:88677ms step_avg:52.91ms
step:1677/2160 train_time:88765ms step_avg:52.93ms
step:1678/2160 train_time:88853ms step_avg:52.95ms
step:1679/2160 train_time:88941ms step_avg:52.97ms
step:1680/2160 train_time:89028ms step_avg:52.99ms
step:1681/2160 train_time:89117ms step_avg:53.01ms
step:1682/2160 train_time:89204ms step_avg:53.03ms
step:1683/2160 train_time:89294ms step_avg:53.06ms
step:1684/2160 train_time:89379ms step_avg:53.08ms
step:1685/2160 train_time:89468ms step_avg:53.10ms
step:1686/2160 train_time:89555ms step_avg:53.12ms
step:1687/2160 train_time:89644ms step_avg:53.14ms
step:1688/2160 train_time:89731ms step_avg:53.16ms
step:1689/2160 train_time:89820ms step_avg:53.18ms
step:1690/2160 train_time:89907ms step_avg:53.20ms
step:1691/2160 train_time:89996ms step_avg:53.22ms
step:1692/2160 train_time:90082ms step_avg:53.24ms
step:1693/2160 train_time:90171ms step_avg:53.26ms
step:1694/2160 train_time:90258ms step_avg:53.28ms
step:1695/2160 train_time:90345ms step_avg:53.30ms
step:1696/2160 train_time:90432ms step_avg:53.32ms
step:1697/2160 train_time:90520ms step_avg:53.34ms
step:1698/2160 train_time:90607ms step_avg:53.36ms
step:1699/2160 train_time:90695ms step_avg:53.38ms
step:1700/2160 train_time:90781ms step_avg:53.40ms
step:1701/2160 train_time:90871ms step_avg:53.42ms
step:1702/2160 train_time:90957ms step_avg:53.44ms
step:1703/2160 train_time:91046ms step_avg:53.46ms
step:1704/2160 train_time:91134ms step_avg:53.48ms
step:1705/2160 train_time:91222ms step_avg:53.50ms
step:1706/2160 train_time:91308ms step_avg:53.52ms
step:1707/2160 train_time:91397ms step_avg:53.54ms
step:1708/2160 train_time:91484ms step_avg:53.56ms
step:1709/2160 train_time:91574ms step_avg:53.58ms
step:1710/2160 train_time:91659ms step_avg:53.60ms
step:1711/2160 train_time:91748ms step_avg:53.62ms
step:1712/2160 train_time:91835ms step_avg:53.64ms
step:1713/2160 train_time:91923ms step_avg:53.66ms
step:1714/2160 train_time:92011ms step_avg:53.68ms
step:1715/2160 train_time:92099ms step_avg:53.70ms
step:1716/2160 train_time:92187ms step_avg:53.72ms
step:1717/2160 train_time:92275ms step_avg:53.74ms
step:1718/2160 train_time:92361ms step_avg:53.76ms
step:1719/2160 train_time:92450ms step_avg:53.78ms
step:1720/2160 train_time:92537ms step_avg:53.80ms
step:1721/2160 train_time:92625ms step_avg:53.82ms
step:1722/2160 train_time:92713ms step_avg:53.84ms
step:1723/2160 train_time:92801ms step_avg:53.86ms
step:1724/2160 train_time:92888ms step_avg:53.88ms
step:1725/2160 train_time:92976ms step_avg:53.90ms
step:1726/2160 train_time:93063ms step_avg:53.92ms
step:1727/2160 train_time:93151ms step_avg:53.94ms
step:1728/2160 train_time:93237ms step_avg:53.96ms
step:1729/2160 train_time:93326ms step_avg:53.98ms
step:1730/2160 train_time:93412ms step_avg:54.00ms
step:1731/2160 train_time:93500ms step_avg:54.02ms
step:1732/2160 train_time:93587ms step_avg:54.03ms
step:1733/2160 train_time:93677ms step_avg:54.05ms
step:1734/2160 train_time:93763ms step_avg:54.07ms
step:1735/2160 train_time:93852ms step_avg:54.09ms
step:1736/2160 train_time:93938ms step_avg:54.11ms
step:1737/2160 train_time:94027ms step_avg:54.13ms
step:1738/2160 train_time:94114ms step_avg:54.15ms
step:1739/2160 train_time:94203ms step_avg:54.17ms
step:1740/2160 train_time:94290ms step_avg:54.19ms
step:1741/2160 train_time:94378ms step_avg:54.21ms
step:1742/2160 train_time:94464ms step_avg:54.23ms
step:1743/2160 train_time:94553ms step_avg:54.25ms
step:1744/2160 train_time:94639ms step_avg:54.27ms
step:1745/2160 train_time:94727ms step_avg:54.29ms
step:1746/2160 train_time:94815ms step_avg:54.30ms
step:1747/2160 train_time:94902ms step_avg:54.32ms
step:1748/2160 train_time:94989ms step_avg:54.34ms
step:1749/2160 train_time:95077ms step_avg:54.36ms
step:1750/2160 train_time:95164ms step_avg:54.38ms
step:1750/2160 val_loss:3.3943 train_time:95254ms step_avg:54.43ms
step:1751/2160 train_time:95277ms step_avg:54.41ms
step:1752/2160 train_time:95345ms step_avg:54.42ms
step:1753/2160 train_time:95438ms step_avg:54.44ms
step:1754/2160 train_time:95525ms step_avg:54.46ms
step:1755/2160 train_time:95612ms step_avg:54.48ms
step:1756/2160 train_time:95698ms step_avg:54.50ms
step:1757/2160 train_time:95785ms step_avg:54.52ms
step:1758/2160 train_time:95871ms step_avg:54.53ms
step:1759/2160 train_time:95957ms step_avg:54.55ms
step:1760/2160 train_time:96043ms step_avg:54.57ms
step:1761/2160 train_time:96130ms step_avg:54.59ms
step:1762/2160 train_time:96218ms step_avg:54.61ms
step:1763/2160 train_time:96310ms step_avg:54.63ms
step:1764/2160 train_time:96399ms step_avg:54.65ms
step:1765/2160 train_time:96491ms step_avg:54.67ms
step:1766/2160 train_time:96577ms step_avg:54.69ms
step:1767/2160 train_time:96665ms step_avg:54.71ms
step:1768/2160 train_time:96751ms step_avg:54.72ms
step:1769/2160 train_time:96838ms step_avg:54.74ms
step:1770/2160 train_time:96924ms step_avg:54.76ms
step:1771/2160 train_time:97011ms step_avg:54.78ms
step:1772/2160 train_time:97097ms step_avg:54.80ms
step:1773/2160 train_time:97186ms step_avg:54.81ms
step:1774/2160 train_time:97274ms step_avg:54.83ms
step:1775/2160 train_time:97364ms step_avg:54.85ms
step:1776/2160 train_time:97452ms step_avg:54.87ms
step:1777/2160 train_time:97541ms step_avg:54.89ms
step:1778/2160 train_time:97627ms step_avg:54.91ms
step:1779/2160 train_time:97715ms step_avg:54.93ms
step:1780/2160 train_time:97801ms step_avg:54.94ms
step:1781/2160 train_time:97889ms step_avg:54.96ms
step:1782/2160 train_time:97974ms step_avg:54.98ms
step:1783/2160 train_time:98062ms step_avg:55.00ms
step:1784/2160 train_time:98148ms step_avg:55.02ms
step:1785/2160 train_time:98237ms step_avg:55.03ms
step:1786/2160 train_time:98326ms step_avg:55.05ms
step:1787/2160 train_time:98414ms step_avg:55.07ms
step:1788/2160 train_time:98502ms step_avg:55.09ms
step:1789/2160 train_time:98592ms step_avg:55.11ms
step:1790/2160 train_time:98678ms step_avg:55.13ms
step:1791/2160 train_time:98766ms step_avg:55.15ms
step:1792/2160 train_time:98851ms step_avg:55.16ms
step:1793/2160 train_time:98939ms step_avg:55.18ms
step:1794/2160 train_time:99026ms step_avg:55.20ms
step:1795/2160 train_time:99113ms step_avg:55.22ms
step:1796/2160 train_time:99200ms step_avg:55.23ms
step:1797/2160 train_time:99292ms step_avg:55.25ms
step:1798/2160 train_time:99380ms step_avg:55.27ms
step:1799/2160 train_time:99469ms step_avg:55.29ms
step:1800/2160 train_time:99556ms step_avg:55.31ms
step:1801/2160 train_time:99645ms step_avg:55.33ms
step:1802/2160 train_time:99731ms step_avg:55.34ms
step:1803/2160 train_time:99818ms step_avg:55.36ms
step:1804/2160 train_time:99905ms step_avg:55.38ms
step:1805/2160 train_time:99994ms step_avg:55.40ms
step:1806/2160 train_time:100081ms step_avg:55.42ms
step:1807/2160 train_time:100171ms step_avg:55.43ms
step:1808/2160 train_time:100258ms step_avg:55.45ms
step:1809/2160 train_time:100347ms step_avg:55.47ms
step:1810/2160 train_time:100433ms step_avg:55.49ms
step:1811/2160 train_time:100522ms step_avg:55.51ms
step:1812/2160 train_time:100609ms step_avg:55.52ms
step:1813/2160 train_time:100696ms step_avg:55.54ms
step:1814/2160 train_time:100783ms step_avg:55.56ms
step:1815/2160 train_time:100871ms step_avg:55.58ms
step:1816/2160 train_time:100958ms step_avg:55.59ms
step:1817/2160 train_time:101046ms step_avg:55.61ms
step:1818/2160 train_time:101132ms step_avg:55.63ms
step:1819/2160 train_time:101221ms step_avg:55.65ms
step:1820/2160 train_time:101308ms step_avg:55.66ms
step:1821/2160 train_time:101396ms step_avg:55.68ms
step:1822/2160 train_time:101484ms step_avg:55.70ms
step:1823/2160 train_time:101573ms step_avg:55.72ms
step:1824/2160 train_time:101659ms step_avg:55.73ms
step:1825/2160 train_time:101747ms step_avg:55.75ms
step:1826/2160 train_time:101832ms step_avg:55.77ms
step:1827/2160 train_time:101920ms step_avg:55.79ms
step:1828/2160 train_time:102007ms step_avg:55.80ms
step:1829/2160 train_time:102094ms step_avg:55.82ms
step:1830/2160 train_time:102181ms step_avg:55.84ms
step:1831/2160 train_time:102271ms step_avg:55.86ms
step:1832/2160 train_time:102358ms step_avg:55.87ms
step:1833/2160 train_time:102448ms step_avg:55.89ms
step:1834/2160 train_time:102534ms step_avg:55.91ms
step:1835/2160 train_time:102623ms step_avg:55.93ms
step:1836/2160 train_time:102710ms step_avg:55.94ms
step:1837/2160 train_time:102797ms step_avg:55.96ms
step:1838/2160 train_time:102884ms step_avg:55.98ms
step:1839/2160 train_time:102972ms step_avg:55.99ms
step:1840/2160 train_time:103058ms step_avg:56.01ms
step:1841/2160 train_time:103146ms step_avg:56.03ms
step:1842/2160 train_time:103232ms step_avg:56.04ms
step:1843/2160 train_time:103321ms step_avg:56.06ms
step:1844/2160 train_time:103409ms step_avg:56.08ms
step:1845/2160 train_time:103498ms step_avg:56.10ms
step:1846/2160 train_time:103585ms step_avg:56.11ms
step:1847/2160 train_time:103673ms step_avg:56.13ms
step:1848/2160 train_time:103758ms step_avg:56.15ms
step:1849/2160 train_time:103847ms step_avg:56.16ms
step:1850/2160 train_time:103932ms step_avg:56.18ms
step:1851/2160 train_time:104020ms step_avg:56.20ms
step:1852/2160 train_time:104106ms step_avg:56.21ms
step:1853/2160 train_time:104195ms step_avg:56.23ms
step:1854/2160 train_time:104282ms step_avg:56.25ms
step:1855/2160 train_time:104371ms step_avg:56.26ms
step:1856/2160 train_time:104459ms step_avg:56.28ms
step:1857/2160 train_time:104549ms step_avg:56.30ms
step:1858/2160 train_time:104635ms step_avg:56.32ms
step:1859/2160 train_time:104723ms step_avg:56.33ms
step:1860/2160 train_time:104809ms step_avg:56.35ms
step:1861/2160 train_time:104897ms step_avg:56.37ms
step:1862/2160 train_time:104983ms step_avg:56.38ms
step:1863/2160 train_time:105072ms step_avg:56.40ms
step:1864/2160 train_time:105158ms step_avg:56.42ms
step:1865/2160 train_time:105247ms step_avg:56.43ms
step:1866/2160 train_time:105333ms step_avg:56.45ms
step:1867/2160 train_time:105423ms step_avg:56.47ms
step:1868/2160 train_time:105511ms step_avg:56.48ms
step:1869/2160 train_time:105599ms step_avg:56.50ms
step:1870/2160 train_time:105686ms step_avg:56.52ms
step:1871/2160 train_time:105774ms step_avg:56.53ms
step:1872/2160 train_time:105860ms step_avg:56.55ms
step:1873/2160 train_time:105949ms step_avg:56.57ms
step:1874/2160 train_time:106035ms step_avg:56.58ms
step:1875/2160 train_time:106123ms step_avg:56.60ms
step:1876/2160 train_time:106210ms step_avg:56.61ms
step:1877/2160 train_time:106297ms step_avg:56.63ms
step:1878/2160 train_time:106385ms step_avg:56.65ms
step:1879/2160 train_time:106473ms step_avg:56.66ms
step:1880/2160 train_time:106560ms step_avg:56.68ms
step:1881/2160 train_time:106650ms step_avg:56.70ms
step:1882/2160 train_time:106737ms step_avg:56.71ms
step:1883/2160 train_time:106825ms step_avg:56.73ms
step:1884/2160 train_time:106911ms step_avg:56.75ms
step:1885/2160 train_time:106999ms step_avg:56.76ms
step:1886/2160 train_time:107086ms step_avg:56.78ms
step:1887/2160 train_time:107174ms step_avg:56.80ms
step:1888/2160 train_time:107262ms step_avg:56.81ms
step:1889/2160 train_time:107352ms step_avg:56.83ms
step:1890/2160 train_time:107439ms step_avg:56.85ms
step:1891/2160 train_time:107528ms step_avg:56.86ms
step:1892/2160 train_time:107614ms step_avg:56.88ms
step:1893/2160 train_time:107702ms step_avg:56.90ms
step:1894/2160 train_time:107789ms step_avg:56.91ms
step:1895/2160 train_time:107877ms step_avg:56.93ms
step:1896/2160 train_time:107963ms step_avg:56.94ms
step:1897/2160 train_time:108052ms step_avg:56.96ms
step:1898/2160 train_time:108137ms step_avg:56.97ms
step:1899/2160 train_time:108226ms step_avg:56.99ms
step:1900/2160 train_time:108314ms step_avg:57.01ms
step:1901/2160 train_time:108401ms step_avg:57.02ms
step:1902/2160 train_time:108489ms step_avg:57.04ms
step:1903/2160 train_time:108578ms step_avg:57.06ms
step:1904/2160 train_time:108665ms step_avg:57.07ms
step:1905/2160 train_time:108753ms step_avg:57.09ms
step:1906/2160 train_time:108840ms step_avg:57.10ms
step:1907/2160 train_time:108928ms step_avg:57.12ms
step:1908/2160 train_time:109014ms step_avg:57.14ms
step:1909/2160 train_time:109102ms step_avg:57.15ms
step:1910/2160 train_time:109189ms step_avg:57.17ms
step:1911/2160 train_time:109277ms step_avg:57.18ms
step:1912/2160 train_time:109364ms step_avg:57.20ms
step:1913/2160 train_time:109453ms step_avg:57.22ms
step:1914/2160 train_time:109540ms step_avg:57.23ms
step:1915/2160 train_time:109629ms step_avg:57.25ms
step:1916/2160 train_time:109714ms step_avg:57.26ms
step:1917/2160 train_time:109804ms step_avg:57.28ms
step:1918/2160 train_time:109890ms step_avg:57.29ms
step:1919/2160 train_time:109978ms step_avg:57.31ms
step:1920/2160 train_time:110065ms step_avg:57.33ms
step:1921/2160 train_time:110153ms step_avg:57.34ms
step:1922/2160 train_time:110240ms step_avg:57.36ms
step:1923/2160 train_time:110329ms step_avg:57.37ms
step:1924/2160 train_time:110415ms step_avg:57.39ms
step:1925/2160 train_time:110503ms step_avg:57.40ms
step:1926/2160 train_time:110591ms step_avg:57.42ms
step:1927/2160 train_time:110679ms step_avg:57.44ms
step:1928/2160 train_time:110766ms step_avg:57.45ms
step:1929/2160 train_time:110853ms step_avg:57.47ms
step:1930/2160 train_time:110940ms step_avg:57.48ms
step:1931/2160 train_time:111028ms step_avg:57.50ms
step:1932/2160 train_time:111114ms step_avg:57.51ms
step:1933/2160 train_time:111202ms step_avg:57.53ms
step:1934/2160 train_time:111289ms step_avg:57.54ms
step:1935/2160 train_time:111376ms step_avg:57.56ms
step:1936/2160 train_time:111464ms step_avg:57.57ms
step:1937/2160 train_time:111552ms step_avg:57.59ms
step:1938/2160 train_time:111639ms step_avg:57.61ms
step:1939/2160 train_time:111728ms step_avg:57.62ms
step:1940/2160 train_time:111813ms step_avg:57.64ms
step:1941/2160 train_time:111901ms step_avg:57.65ms
step:1942/2160 train_time:111988ms step_avg:57.67ms
step:1943/2160 train_time:112076ms step_avg:57.68ms
step:1944/2160 train_time:112162ms step_avg:57.70ms
step:1945/2160 train_time:112251ms step_avg:57.71ms
step:1946/2160 train_time:112337ms step_avg:57.73ms
step:1947/2160 train_time:112426ms step_avg:57.74ms
step:1948/2160 train_time:112512ms step_avg:57.76ms
step:1949/2160 train_time:112599ms step_avg:57.77ms
step:1950/2160 train_time:112686ms step_avg:57.79ms
step:1951/2160 train_time:112774ms step_avg:57.80ms
step:1952/2160 train_time:112861ms step_avg:57.82ms
step:1953/2160 train_time:112950ms step_avg:57.83ms
step:1954/2160 train_time:113036ms step_avg:57.85ms
step:1955/2160 train_time:113125ms step_avg:57.86ms
step:1956/2160 train_time:113211ms step_avg:57.88ms
step:1957/2160 train_time:113299ms step_avg:57.89ms
step:1958/2160 train_time:113387ms step_avg:57.91ms
step:1959/2160 train_time:113475ms step_avg:57.93ms
step:1960/2160 train_time:113562ms step_avg:57.94ms
step:1961/2160 train_time:113650ms step_avg:57.96ms
step:1962/2160 train_time:113736ms step_avg:57.97ms
step:1963/2160 train_time:113825ms step_avg:57.99ms
step:1964/2160 train_time:113912ms step_avg:58.00ms
step:1965/2160 train_time:113999ms step_avg:58.01ms
step:1966/2160 train_time:114086ms step_avg:58.03ms
step:1967/2160 train_time:114174ms step_avg:58.04ms
step:1968/2160 train_time:114262ms step_avg:58.06ms
step:1969/2160 train_time:114352ms step_avg:58.08ms
step:1970/2160 train_time:114439ms step_avg:58.09ms
step:1971/2160 train_time:114528ms step_avg:58.11ms
step:1972/2160 train_time:114615ms step_avg:58.12ms
step:1973/2160 train_time:114702ms step_avg:58.14ms
step:1974/2160 train_time:114790ms step_avg:58.15ms
step:1975/2160 train_time:114878ms step_avg:58.17ms
step:1976/2160 train_time:114965ms step_avg:58.18ms
step:1977/2160 train_time:115053ms step_avg:58.20ms
step:1978/2160 train_time:115138ms step_avg:58.21ms
step:1979/2160 train_time:115227ms step_avg:58.23ms
step:1980/2160 train_time:115313ms step_avg:58.24ms
step:1981/2160 train_time:115402ms step_avg:58.25ms
step:1982/2160 train_time:115489ms step_avg:58.27ms
step:1983/2160 train_time:115577ms step_avg:58.28ms
step:1984/2160 train_time:115664ms step_avg:58.30ms
step:1985/2160 train_time:115752ms step_avg:58.31ms
step:1986/2160 train_time:115839ms step_avg:58.33ms
step:1987/2160 train_time:115928ms step_avg:58.34ms
step:1988/2160 train_time:116014ms step_avg:58.36ms
step:1989/2160 train_time:116101ms step_avg:58.37ms
step:1990/2160 train_time:116188ms step_avg:58.39ms
step:1991/2160 train_time:116276ms step_avg:58.40ms
step:1992/2160 train_time:116364ms step_avg:58.42ms
step:1993/2160 train_time:116453ms step_avg:58.43ms
step:1994/2160 train_time:116540ms step_avg:58.45ms
step:1995/2160 train_time:116629ms step_avg:58.46ms
step:1996/2160 train_time:116716ms step_avg:58.47ms
step:1997/2160 train_time:116804ms step_avg:58.49ms
step:1998/2160 train_time:116891ms step_avg:58.50ms
step:1999/2160 train_time:116978ms step_avg:58.52ms
step:2000/2160 train_time:117065ms step_avg:58.53ms
step:2000/2160 val_loss:3.3170 train_time:117154ms step_avg:58.58ms
step:2001/2160 train_time:117177ms step_avg:58.56ms
step:2002/2160 train_time:117243ms step_avg:58.56ms
step:2003/2160 train_time:117334ms step_avg:58.58ms
step:2004/2160 train_time:117423ms step_avg:58.59ms
step:2005/2160 train_time:117511ms step_avg:58.61ms
step:2006/2160 train_time:117597ms step_avg:58.62ms
step:2007/2160 train_time:117685ms step_avg:58.64ms
step:2008/2160 train_time:117769ms step_avg:58.65ms
step:2009/2160 train_time:117856ms step_avg:58.66ms
step:2010/2160 train_time:117942ms step_avg:58.68ms
step:2011/2160 train_time:118028ms step_avg:58.69ms
step:2012/2160 train_time:118116ms step_avg:58.71ms
step:2013/2160 train_time:118207ms step_avg:58.72ms
step:2014/2160 train_time:118295ms step_avg:58.74ms
step:2015/2160 train_time:118386ms step_avg:58.75ms
step:2016/2160 train_time:118473ms step_avg:58.77ms
step:2017/2160 train_time:118562ms step_avg:58.78ms
step:2018/2160 train_time:118647ms step_avg:58.79ms
step:2019/2160 train_time:118735ms step_avg:58.81ms
step:2020/2160 train_time:118820ms step_avg:58.82ms
step:2021/2160 train_time:118908ms step_avg:58.84ms
step:2022/2160 train_time:118994ms step_avg:58.85ms
step:2023/2160 train_time:119082ms step_avg:58.86ms
step:2024/2160 train_time:119169ms step_avg:58.88ms
step:2025/2160 train_time:119260ms step_avg:58.89ms
step:2026/2160 train_time:119347ms step_avg:58.91ms
step:2027/2160 train_time:119435ms step_avg:58.92ms
step:2028/2160 train_time:119523ms step_avg:58.94ms
step:2029/2160 train_time:119611ms step_avg:58.95ms
step:2030/2160 train_time:119697ms step_avg:58.96ms
step:2031/2160 train_time:119785ms step_avg:58.98ms
step:2032/2160 train_time:119869ms step_avg:58.99ms
step:2033/2160 train_time:119957ms step_avg:59.01ms
step:2034/2160 train_time:120043ms step_avg:59.02ms
step:2035/2160 train_time:120131ms step_avg:59.03ms
step:2036/2160 train_time:120219ms step_avg:59.05ms
step:2037/2160 train_time:120308ms step_avg:59.06ms
step:2038/2160 train_time:120396ms step_avg:59.08ms
step:2039/2160 train_time:120485ms step_avg:59.09ms
step:2040/2160 train_time:120572ms step_avg:59.10ms
step:2041/2160 train_time:120660ms step_avg:59.12ms
step:2042/2160 train_time:120745ms step_avg:59.13ms
step:2043/2160 train_time:120832ms step_avg:59.14ms
step:2044/2160 train_time:120918ms step_avg:59.16ms
step:2045/2160 train_time:121006ms step_avg:59.17ms
step:2046/2160 train_time:121093ms step_avg:59.19ms
step:2047/2160 train_time:121182ms step_avg:59.20ms
step:2048/2160 train_time:121269ms step_avg:59.21ms
step:2049/2160 train_time:121358ms step_avg:59.23ms
step:2050/2160 train_time:121446ms step_avg:59.24ms
step:2051/2160 train_time:121534ms step_avg:59.26ms
step:2052/2160 train_time:121622ms step_avg:59.27ms
step:2053/2160 train_time:121710ms step_avg:59.28ms
step:2054/2160 train_time:121797ms step_avg:59.30ms
step:2055/2160 train_time:121884ms step_avg:59.31ms
step:2056/2160 train_time:121971ms step_avg:59.32ms
step:2057/2160 train_time:122059ms step_avg:59.34ms
step:2058/2160 train_time:122146ms step_avg:59.35ms
step:2059/2160 train_time:122233ms step_avg:59.37ms
step:2060/2160 train_time:122320ms step_avg:59.38ms
step:2061/2160 train_time:122408ms step_avg:59.39ms
step:2062/2160 train_time:122496ms step_avg:59.41ms
step:2063/2160 train_time:122586ms step_avg:59.42ms
step:2064/2160 train_time:122672ms step_avg:59.43ms
step:2065/2160 train_time:122761ms step_avg:59.45ms
step:2066/2160 train_time:122846ms step_avg:59.46ms
step:2067/2160 train_time:122934ms step_avg:59.47ms
step:2068/2160 train_time:123021ms step_avg:59.49ms
step:2069/2160 train_time:123108ms step_avg:59.50ms
step:2070/2160 train_time:123194ms step_avg:59.51ms
step:2071/2160 train_time:123283ms step_avg:59.53ms
step:2072/2160 train_time:123370ms step_avg:59.54ms
step:2073/2160 train_time:123459ms step_avg:59.56ms
step:2074/2160 train_time:123546ms step_avg:59.57ms
step:2075/2160 train_time:123633ms step_avg:59.58ms
step:2076/2160 train_time:123720ms step_avg:59.60ms
step:2077/2160 train_time:123807ms step_avg:59.61ms
step:2078/2160 train_time:123894ms step_avg:59.62ms
step:2079/2160 train_time:123983ms step_avg:59.64ms
step:2080/2160 train_time:124068ms step_avg:59.65ms
step:2081/2160 train_time:124156ms step_avg:59.66ms
step:2082/2160 train_time:124243ms step_avg:59.67ms
step:2083/2160 train_time:124330ms step_avg:59.69ms
step:2084/2160 train_time:124417ms step_avg:59.70ms
step:2085/2160 train_time:124506ms step_avg:59.71ms
step:2086/2160 train_time:124592ms step_avg:59.73ms
step:2087/2160 train_time:124681ms step_avg:59.74ms
step:2088/2160 train_time:124769ms step_avg:59.76ms
step:2089/2160 train_time:124858ms step_avg:59.77ms
step:2090/2160 train_time:124944ms step_avg:59.78ms
step:2091/2160 train_time:125032ms step_avg:59.80ms
step:2092/2160 train_time:125119ms step_avg:59.81ms
step:2093/2160 train_time:125207ms step_avg:59.82ms
step:2094/2160 train_time:125293ms step_avg:59.83ms
step:2095/2160 train_time:125381ms step_avg:59.85ms
step:2096/2160 train_time:125467ms step_avg:59.86ms
step:2097/2160 train_time:125555ms step_avg:59.87ms
step:2098/2160 train_time:125642ms step_avg:59.89ms
step:2099/2160 train_time:125729ms step_avg:59.90ms
step:2100/2160 train_time:125815ms step_avg:59.91ms
step:2101/2160 train_time:125904ms step_avg:59.93ms
step:2102/2160 train_time:125990ms step_avg:59.94ms
step:2103/2160 train_time:126079ms step_avg:59.95ms
step:2104/2160 train_time:126165ms step_avg:59.96ms
step:2105/2160 train_time:126252ms step_avg:59.98ms
step:2106/2160 train_time:126339ms step_avg:59.99ms
step:2107/2160 train_time:126428ms step_avg:60.00ms
step:2108/2160 train_time:126515ms step_avg:60.02ms
step:2109/2160 train_time:126603ms step_avg:60.03ms
step:2110/2160 train_time:126689ms step_avg:60.04ms
step:2111/2160 train_time:126778ms step_avg:60.06ms
step:2112/2160 train_time:126865ms step_avg:60.07ms
step:2113/2160 train_time:126953ms step_avg:60.08ms
step:2114/2160 train_time:127040ms step_avg:60.09ms
step:2115/2160 train_time:127127ms step_avg:60.11ms
step:2116/2160 train_time:127213ms step_avg:60.12ms
step:2117/2160 train_time:127301ms step_avg:60.13ms
step:2118/2160 train_time:127387ms step_avg:60.14ms
step:2119/2160 train_time:127475ms step_avg:60.16ms
step:2120/2160 train_time:127562ms step_avg:60.17ms
step:2121/2160 train_time:127650ms step_avg:60.18ms
step:2122/2160 train_time:127738ms step_avg:60.20ms
step:2123/2160 train_time:127826ms step_avg:60.21ms
step:2124/2160 train_time:127913ms step_avg:60.22ms
step:2125/2160 train_time:128002ms step_avg:60.24ms
step:2126/2160 train_time:128088ms step_avg:60.25ms
step:2127/2160 train_time:128176ms step_avg:60.26ms
step:2128/2160 train_time:128264ms step_avg:60.27ms
step:2129/2160 train_time:128352ms step_avg:60.29ms
step:2130/2160 train_time:128439ms step_avg:60.30ms
step:2131/2160 train_time:128527ms step_avg:60.31ms
step:2132/2160 train_time:128614ms step_avg:60.33ms
step:2133/2160 train_time:128702ms step_avg:60.34ms
step:2134/2160 train_time:128789ms step_avg:60.35ms
step:2135/2160 train_time:128878ms step_avg:60.36ms
step:2136/2160 train_time:128964ms step_avg:60.38ms
step:2137/2160 train_time:129052ms step_avg:60.39ms
step:2138/2160 train_time:129138ms step_avg:60.40ms
step:2139/2160 train_time:129226ms step_avg:60.41ms
step:2140/2160 train_time:129313ms step_avg:60.43ms
step:2141/2160 train_time:129403ms step_avg:60.44ms
step:2142/2160 train_time:129488ms step_avg:60.45ms
step:2143/2160 train_time:129577ms step_avg:60.47ms
step:2144/2160 train_time:129663ms step_avg:60.48ms
step:2145/2160 train_time:129752ms step_avg:60.49ms
step:2146/2160 train_time:129839ms step_avg:60.50ms
step:2147/2160 train_time:129927ms step_avg:60.52ms
step:2148/2160 train_time:130014ms step_avg:60.53ms
step:2149/2160 train_time:130102ms step_avg:60.54ms
step:2150/2160 train_time:130188ms step_avg:60.55ms
step:2151/2160 train_time:130277ms step_avg:60.57ms
step:2152/2160 train_time:130365ms step_avg:60.58ms
step:2153/2160 train_time:130453ms step_avg:60.59ms
step:2154/2160 train_time:130540ms step_avg:60.60ms
step:2155/2160 train_time:130628ms step_avg:60.62ms
step:2156/2160 train_time:130715ms step_avg:60.63ms
step:2157/2160 train_time:130803ms step_avg:60.64ms
step:2158/2160 train_time:130891ms step_avg:60.65ms
step:2159/2160 train_time:130979ms step_avg:60.67ms
step:2160/2160 train_time:131065ms step_avg:60.68ms
step:2160/2160 val_loss:3.2795 train_time:131155ms step_avg:60.72ms
peak memory allocated: 29862 MiB reserved: 45136 MiB

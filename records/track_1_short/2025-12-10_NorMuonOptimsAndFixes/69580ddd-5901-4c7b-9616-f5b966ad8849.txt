import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Dec 10 2025, 18:15:56) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 18:19:15 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   34C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   35C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   35C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   34C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A              75      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A              76      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              77      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              78      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              79      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              80      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              81      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A              82      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A              76      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A              77      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A              78      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A              79      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A              80      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A              81      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A              82      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:136ms step_avg:135.83ms
step:2/2160 train_time:182ms step_avg:90.83ms
step:3/2160 train_time:203ms step_avg:67.67ms
step:4/2160 train_time:236ms step_avg:58.88ms
step:5/2160 train_time:270ms step_avg:54.03ms
step:6/2160 train_time:363ms step_avg:60.52ms
step:7/2160 train_time:382ms step_avg:54.64ms
step:8/2160 train_time:411ms step_avg:51.42ms
step:9/2160 train_time:445ms step_avg:49.45ms
step:10/2160 train_time:479ms step_avg:47.86ms
step:11/2160 train_time:513ms step_avg:46.60ms
step:12/2160 train_time:546ms step_avg:45.48ms
step:13/2160 train_time:580ms step_avg:44.63ms
step:14/2160 train_time:613ms step_avg:43.82ms
step:15/2160 train_time:647ms step_avg:43.16ms
step:16/2160 train_time:681ms step_avg:42.54ms
step:17/2160 train_time:715ms step_avg:42.05ms
step:18/2160 train_time:748ms step_avg:41.57ms
step:19/2160 train_time:782ms step_avg:41.17ms
step:20/2160 train_time:816ms step_avg:40.78ms
step:21/2160 train_time:850ms step_avg:40.46ms
step:22/2160 train_time:883ms step_avg:40.15ms
step:23/2160 train_time:917ms step_avg:39.87ms
step:24/2160 train_time:950ms step_avg:39.60ms
step:25/2160 train_time:985ms step_avg:39.39ms
step:26/2160 train_time:1018ms step_avg:39.14ms
step:27/2160 train_time:1052ms step_avg:38.97ms
step:28/2160 train_time:1086ms step_avg:38.79ms
step:29/2160 train_time:1120ms step_avg:38.62ms
step:30/2160 train_time:1153ms step_avg:38.44ms
step:31/2160 train_time:1187ms step_avg:38.30ms
step:32/2160 train_time:1221ms step_avg:38.15ms
step:33/2160 train_time:1256ms step_avg:38.05ms
step:34/2160 train_time:1290ms step_avg:37.93ms
step:35/2160 train_time:1325ms step_avg:37.86ms
step:36/2160 train_time:1358ms step_avg:37.73ms
step:37/2160 train_time:1393ms step_avg:37.66ms
step:38/2160 train_time:1427ms step_avg:37.55ms
step:39/2160 train_time:1461ms step_avg:37.47ms
step:40/2160 train_time:1495ms step_avg:37.38ms
step:41/2160 train_time:1529ms step_avg:37.30ms
step:42/2160 train_time:1563ms step_avg:37.20ms
step:43/2160 train_time:1597ms step_avg:37.14ms
step:44/2160 train_time:1630ms step_avg:37.05ms
step:45/2160 train_time:1665ms step_avg:36.99ms
step:46/2160 train_time:1698ms step_avg:36.91ms
step:47/2160 train_time:1732ms step_avg:36.85ms
step:48/2160 train_time:1765ms step_avg:36.77ms
step:49/2160 train_time:1800ms step_avg:36.73ms
step:50/2160 train_time:1833ms step_avg:36.66ms
step:51/2160 train_time:1867ms step_avg:36.61ms
step:52/2160 train_time:1901ms step_avg:36.55ms
step:53/2160 train_time:1935ms step_avg:36.51ms
step:54/2160 train_time:1968ms step_avg:36.44ms
step:55/2160 train_time:2003ms step_avg:36.42ms
step:56/2160 train_time:2036ms step_avg:36.36ms
step:57/2160 train_time:2071ms step_avg:36.33ms
step:58/2160 train_time:2104ms step_avg:36.28ms
step:59/2160 train_time:2138ms step_avg:36.24ms
step:60/2160 train_time:2171ms step_avg:36.19ms
step:61/2160 train_time:2206ms step_avg:36.16ms
step:62/2160 train_time:2239ms step_avg:36.11ms
step:63/2160 train_time:2274ms step_avg:36.09ms
step:64/2160 train_time:2307ms step_avg:36.05ms
step:65/2160 train_time:2342ms step_avg:36.03ms
step:66/2160 train_time:2375ms step_avg:35.99ms
step:67/2160 train_time:2410ms step_avg:35.97ms
step:68/2160 train_time:2444ms step_avg:35.93ms
step:69/2160 train_time:2478ms step_avg:35.91ms
step:70/2160 train_time:2511ms step_avg:35.88ms
step:71/2160 train_time:2546ms step_avg:35.85ms
step:72/2160 train_time:2579ms step_avg:35.82ms
step:73/2160 train_time:2613ms step_avg:35.80ms
step:74/2160 train_time:2647ms step_avg:35.77ms
step:75/2160 train_time:2681ms step_avg:35.74ms
step:76/2160 train_time:2714ms step_avg:35.71ms
step:77/2160 train_time:2749ms step_avg:35.70ms
step:78/2160 train_time:2782ms step_avg:35.67ms
step:79/2160 train_time:2816ms step_avg:35.65ms
step:80/2160 train_time:2849ms step_avg:35.62ms
step:81/2160 train_time:2883ms step_avg:35.60ms
step:82/2160 train_time:2917ms step_avg:35.57ms
step:83/2160 train_time:2951ms step_avg:35.55ms
step:84/2160 train_time:2984ms step_avg:35.52ms
step:85/2160 train_time:3018ms step_avg:35.51ms
step:86/2160 train_time:3051ms step_avg:35.48ms
step:87/2160 train_time:3086ms step_avg:35.47ms
step:88/2160 train_time:3119ms step_avg:35.45ms
step:89/2160 train_time:3153ms step_avg:35.43ms
step:90/2160 train_time:3187ms step_avg:35.41ms
step:91/2160 train_time:3221ms step_avg:35.40ms
step:92/2160 train_time:3254ms step_avg:35.37ms
step:93/2160 train_time:3289ms step_avg:35.36ms
step:94/2160 train_time:3322ms step_avg:35.34ms
step:95/2160 train_time:3356ms step_avg:35.33ms
step:96/2160 train_time:3389ms step_avg:35.31ms
step:97/2160 train_time:3425ms step_avg:35.31ms
step:98/2160 train_time:3458ms step_avg:35.28ms
step:99/2160 train_time:3492ms step_avg:35.28ms
step:100/2160 train_time:3526ms step_avg:35.26ms
step:101/2160 train_time:3560ms step_avg:35.25ms
step:102/2160 train_time:3593ms step_avg:35.23ms
step:103/2160 train_time:3628ms step_avg:35.22ms
step:104/2160 train_time:3661ms step_avg:35.20ms
step:105/2160 train_time:3695ms step_avg:35.19ms
step:106/2160 train_time:3728ms step_avg:35.17ms
step:107/2160 train_time:3763ms step_avg:35.17ms
step:108/2160 train_time:3796ms step_avg:35.15ms
step:109/2160 train_time:3830ms step_avg:35.14ms
step:110/2160 train_time:3864ms step_avg:35.12ms
step:111/2160 train_time:3898ms step_avg:35.12ms
step:112/2160 train_time:3931ms step_avg:35.10ms
step:113/2160 train_time:3966ms step_avg:35.09ms
step:114/2160 train_time:3999ms step_avg:35.08ms
step:115/2160 train_time:4033ms step_avg:35.07ms
step:116/2160 train_time:4066ms step_avg:35.05ms
step:117/2160 train_time:4100ms step_avg:35.05ms
step:118/2160 train_time:4134ms step_avg:35.03ms
step:119/2160 train_time:4168ms step_avg:35.02ms
step:120/2160 train_time:4201ms step_avg:35.01ms
step:121/2160 train_time:4235ms step_avg:35.00ms
step:122/2160 train_time:4268ms step_avg:34.99ms
step:123/2160 train_time:4303ms step_avg:34.99ms
step:124/2160 train_time:4337ms step_avg:34.97ms
step:125/2160 train_time:4371ms step_avg:34.97ms
step:126/2160 train_time:4405ms step_avg:34.96ms
step:127/2160 train_time:4439ms step_avg:34.95ms
step:128/2160 train_time:4472ms step_avg:34.94ms
step:129/2160 train_time:4507ms step_avg:34.93ms
step:130/2160 train_time:4540ms step_avg:34.92ms
step:131/2160 train_time:4574ms step_avg:34.92ms
step:132/2160 train_time:4608ms step_avg:34.91ms
step:133/2160 train_time:4642ms step_avg:34.90ms
step:134/2160 train_time:4675ms step_avg:34.89ms
step:135/2160 train_time:4709ms step_avg:34.88ms
step:136/2160 train_time:4742ms step_avg:34.87ms
step:137/2160 train_time:4777ms step_avg:34.87ms
step:138/2160 train_time:4810ms step_avg:34.85ms
step:139/2160 train_time:4844ms step_avg:34.85ms
step:140/2160 train_time:4877ms step_avg:34.84ms
step:141/2160 train_time:4911ms step_avg:34.83ms
step:142/2160 train_time:4945ms step_avg:34.82ms
step:143/2160 train_time:4979ms step_avg:34.82ms
step:144/2160 train_time:5012ms step_avg:34.81ms
step:145/2160 train_time:5047ms step_avg:34.80ms
step:146/2160 train_time:5080ms step_avg:34.79ms
step:147/2160 train_time:5114ms step_avg:34.79ms
step:148/2160 train_time:5147ms step_avg:34.78ms
step:149/2160 train_time:5181ms step_avg:34.77ms
step:150/2160 train_time:5215ms step_avg:34.77ms
step:151/2160 train_time:5249ms step_avg:34.76ms
step:152/2160 train_time:5282ms step_avg:34.75ms
step:153/2160 train_time:5316ms step_avg:34.75ms
step:154/2160 train_time:5350ms step_avg:34.74ms
step:155/2160 train_time:5384ms step_avg:34.73ms
step:156/2160 train_time:5417ms step_avg:34.72ms
step:157/2160 train_time:5451ms step_avg:34.72ms
step:158/2160 train_time:5484ms step_avg:34.71ms
step:159/2160 train_time:5519ms step_avg:34.71ms
step:160/2160 train_time:5552ms step_avg:34.70ms
step:161/2160 train_time:5587ms step_avg:34.70ms
step:162/2160 train_time:5620ms step_avg:34.69ms
step:163/2160 train_time:5654ms step_avg:34.69ms
step:164/2160 train_time:5687ms step_avg:34.68ms
step:165/2160 train_time:5721ms step_avg:34.67ms
step:166/2160 train_time:5755ms step_avg:34.67ms
step:167/2160 train_time:5789ms step_avg:34.66ms
step:168/2160 train_time:5822ms step_avg:34.65ms
step:169/2160 train_time:5856ms step_avg:34.65ms
step:170/2160 train_time:5889ms step_avg:34.64ms
step:171/2160 train_time:5924ms step_avg:34.64ms
step:172/2160 train_time:5957ms step_avg:34.63ms
step:173/2160 train_time:5991ms step_avg:34.63ms
step:174/2160 train_time:6024ms step_avg:34.62ms
step:175/2160 train_time:6059ms step_avg:34.62ms
step:176/2160 train_time:6092ms step_avg:34.61ms
step:177/2160 train_time:6126ms step_avg:34.61ms
step:178/2160 train_time:6159ms step_avg:34.60ms
step:179/2160 train_time:6194ms step_avg:34.60ms
step:180/2160 train_time:6227ms step_avg:34.60ms
step:181/2160 train_time:6261ms step_avg:34.59ms
step:182/2160 train_time:6295ms step_avg:34.59ms
step:183/2160 train_time:6329ms step_avg:34.59ms
step:184/2160 train_time:6362ms step_avg:34.58ms
step:185/2160 train_time:6397ms step_avg:34.58ms
step:186/2160 train_time:6430ms step_avg:34.57ms
step:187/2160 train_time:6465ms step_avg:34.57ms
step:188/2160 train_time:6498ms step_avg:34.56ms
step:189/2160 train_time:6532ms step_avg:34.56ms
step:190/2160 train_time:6565ms step_avg:34.55ms
step:191/2160 train_time:6600ms step_avg:34.55ms
step:192/2160 train_time:6633ms step_avg:34.55ms
step:193/2160 train_time:6668ms step_avg:34.55ms
step:194/2160 train_time:6701ms step_avg:34.54ms
step:195/2160 train_time:6735ms step_avg:34.54ms
step:196/2160 train_time:6768ms step_avg:34.53ms
step:197/2160 train_time:6803ms step_avg:34.53ms
step:198/2160 train_time:6836ms step_avg:34.52ms
step:199/2160 train_time:6870ms step_avg:34.52ms
step:200/2160 train_time:6903ms step_avg:34.51ms
step:201/2160 train_time:6937ms step_avg:34.51ms
step:202/2160 train_time:6970ms step_avg:34.50ms
step:203/2160 train_time:7005ms step_avg:34.51ms
step:204/2160 train_time:7038ms step_avg:34.50ms
step:205/2160 train_time:7072ms step_avg:34.50ms
step:206/2160 train_time:7105ms step_avg:34.49ms
step:207/2160 train_time:7139ms step_avg:34.49ms
step:208/2160 train_time:7173ms step_avg:34.48ms
step:209/2160 train_time:7207ms step_avg:34.48ms
step:210/2160 train_time:7240ms step_avg:34.48ms
step:211/2160 train_time:7274ms step_avg:34.47ms
step:212/2160 train_time:7307ms step_avg:34.47ms
step:213/2160 train_time:7342ms step_avg:34.47ms
step:214/2160 train_time:7375ms step_avg:34.46ms
step:215/2160 train_time:7410ms step_avg:34.46ms
step:216/2160 train_time:7443ms step_avg:34.46ms
step:217/2160 train_time:7477ms step_avg:34.45ms
step:218/2160 train_time:7510ms step_avg:34.45ms
step:219/2160 train_time:7545ms step_avg:34.45ms
step:220/2160 train_time:7578ms step_avg:34.44ms
step:221/2160 train_time:7612ms step_avg:34.44ms
step:222/2160 train_time:7645ms step_avg:34.44ms
step:223/2160 train_time:7679ms step_avg:34.44ms
step:224/2160 train_time:7712ms step_avg:34.43ms
step:225/2160 train_time:7747ms step_avg:34.43ms
step:226/2160 train_time:7780ms step_avg:34.43ms
step:227/2160 train_time:7814ms step_avg:34.43ms
step:228/2160 train_time:7848ms step_avg:34.42ms
step:229/2160 train_time:7882ms step_avg:34.42ms
step:230/2160 train_time:7915ms step_avg:34.41ms
step:231/2160 train_time:7949ms step_avg:34.41ms
step:232/2160 train_time:7982ms step_avg:34.41ms
step:233/2160 train_time:8017ms step_avg:34.41ms
step:234/2160 train_time:8050ms step_avg:34.40ms
step:235/2160 train_time:8084ms step_avg:34.40ms
step:236/2160 train_time:8118ms step_avg:34.40ms
step:237/2160 train_time:8151ms step_avg:34.39ms
step:238/2160 train_time:8184ms step_avg:34.39ms
step:239/2160 train_time:8219ms step_avg:34.39ms
step:240/2160 train_time:8252ms step_avg:34.38ms
step:241/2160 train_time:8286ms step_avg:34.38ms
step:242/2160 train_time:8319ms step_avg:34.38ms
step:243/2160 train_time:8353ms step_avg:34.38ms
step:244/2160 train_time:8387ms step_avg:34.37ms
step:245/2160 train_time:8421ms step_avg:34.37ms
step:246/2160 train_time:8454ms step_avg:34.37ms
step:247/2160 train_time:8489ms step_avg:34.37ms
step:248/2160 train_time:8522ms step_avg:34.36ms
step:249/2160 train_time:8556ms step_avg:34.36ms
step:250/2160 train_time:8589ms step_avg:34.36ms
step:250/2160 val_loss:4.2988 train_time:8625ms step_avg:34.50ms
step:251/2160 train_time:8646ms step_avg:34.45ms
step:252/2160 train_time:8667ms step_avg:34.39ms
step:253/2160 train_time:8696ms step_avg:34.37ms
step:254/2160 train_time:8730ms step_avg:34.37ms
step:255/2160 train_time:8771ms step_avg:34.39ms
step:256/2160 train_time:8808ms step_avg:34.41ms
step:257/2160 train_time:8844ms step_avg:34.41ms
step:258/2160 train_time:8877ms step_avg:34.41ms
step:259/2160 train_time:8912ms step_avg:34.41ms
step:260/2160 train_time:8945ms step_avg:34.40ms
step:261/2160 train_time:8979ms step_avg:34.40ms
step:262/2160 train_time:9012ms step_avg:34.40ms
step:263/2160 train_time:9046ms step_avg:34.40ms
step:264/2160 train_time:9079ms step_avg:34.39ms
step:265/2160 train_time:9113ms step_avg:34.39ms
step:266/2160 train_time:9146ms step_avg:34.38ms
step:267/2160 train_time:9180ms step_avg:34.38ms
step:268/2160 train_time:9213ms step_avg:34.38ms
step:269/2160 train_time:9247ms step_avg:34.37ms
step:270/2160 train_time:9280ms step_avg:34.37ms
step:271/2160 train_time:9313ms step_avg:34.37ms
step:272/2160 train_time:9346ms step_avg:34.36ms
step:273/2160 train_time:9380ms step_avg:34.36ms
step:274/2160 train_time:9413ms step_avg:34.35ms
step:275/2160 train_time:9447ms step_avg:34.35ms
step:276/2160 train_time:9481ms step_avg:34.35ms
step:277/2160 train_time:9514ms step_avg:34.35ms
step:278/2160 train_time:9548ms step_avg:34.34ms
step:279/2160 train_time:9581ms step_avg:34.34ms
step:280/2160 train_time:9614ms step_avg:34.34ms
step:281/2160 train_time:9648ms step_avg:34.33ms
step:282/2160 train_time:9682ms step_avg:34.33ms
step:283/2160 train_time:9716ms step_avg:34.33ms
step:284/2160 train_time:9749ms step_avg:34.33ms
step:285/2160 train_time:9784ms step_avg:34.33ms
step:286/2160 train_time:9818ms step_avg:34.33ms
step:287/2160 train_time:9852ms step_avg:34.33ms
step:288/2160 train_time:9885ms step_avg:34.32ms
step:289/2160 train_time:9920ms step_avg:34.32ms
step:290/2160 train_time:9953ms step_avg:34.32ms
step:291/2160 train_time:9987ms step_avg:34.32ms
step:292/2160 train_time:10020ms step_avg:34.32ms
step:293/2160 train_time:10055ms step_avg:34.32ms
step:294/2160 train_time:10087ms step_avg:34.31ms
step:295/2160 train_time:10122ms step_avg:34.31ms
step:296/2160 train_time:10155ms step_avg:34.31ms
step:297/2160 train_time:10189ms step_avg:34.31ms
step:298/2160 train_time:10222ms step_avg:34.30ms
step:299/2160 train_time:10257ms step_avg:34.30ms
step:300/2160 train_time:10290ms step_avg:34.30ms
step:301/2160 train_time:10324ms step_avg:34.30ms
step:302/2160 train_time:10357ms step_avg:34.29ms
step:303/2160 train_time:10391ms step_avg:34.29ms
step:304/2160 train_time:10424ms step_avg:34.29ms
step:305/2160 train_time:10458ms step_avg:34.29ms
step:306/2160 train_time:10491ms step_avg:34.28ms
step:307/2160 train_time:10525ms step_avg:34.28ms
step:308/2160 train_time:10558ms step_avg:34.28ms
step:309/2160 train_time:10592ms step_avg:34.28ms
step:310/2160 train_time:10625ms step_avg:34.27ms
step:311/2160 train_time:10659ms step_avg:34.27ms
step:312/2160 train_time:10692ms step_avg:34.27ms
step:313/2160 train_time:10727ms step_avg:34.27ms
step:314/2160 train_time:10760ms step_avg:34.27ms
step:315/2160 train_time:10795ms step_avg:34.27ms
step:316/2160 train_time:10828ms step_avg:34.26ms
step:317/2160 train_time:10863ms step_avg:34.27ms
step:318/2160 train_time:10896ms step_avg:34.26ms
step:319/2160 train_time:10929ms step_avg:34.26ms
step:320/2160 train_time:10962ms step_avg:34.26ms
step:321/2160 train_time:10997ms step_avg:34.26ms
step:322/2160 train_time:11030ms step_avg:34.25ms
step:323/2160 train_time:11064ms step_avg:34.25ms
step:324/2160 train_time:11097ms step_avg:34.25ms
step:325/2160 train_time:11132ms step_avg:34.25ms
step:326/2160 train_time:11165ms step_avg:34.25ms
step:327/2160 train_time:11200ms step_avg:34.25ms
step:328/2160 train_time:11233ms step_avg:34.25ms
step:329/2160 train_time:11267ms step_avg:34.25ms
step:330/2160 train_time:11300ms step_avg:34.24ms
step:331/2160 train_time:11335ms step_avg:34.24ms
step:332/2160 train_time:11368ms step_avg:34.24ms
step:333/2160 train_time:11402ms step_avg:34.24ms
step:334/2160 train_time:11435ms step_avg:34.24ms
step:335/2160 train_time:11469ms step_avg:34.24ms
step:336/2160 train_time:11502ms step_avg:34.23ms
step:337/2160 train_time:11537ms step_avg:34.23ms
step:338/2160 train_time:11569ms step_avg:34.23ms
step:339/2160 train_time:11604ms step_avg:34.23ms
step:340/2160 train_time:11637ms step_avg:34.23ms
step:341/2160 train_time:11671ms step_avg:34.23ms
step:342/2160 train_time:11704ms step_avg:34.22ms
step:343/2160 train_time:11739ms step_avg:34.22ms
step:344/2160 train_time:11772ms step_avg:34.22ms
step:345/2160 train_time:11806ms step_avg:34.22ms
step:346/2160 train_time:11840ms step_avg:34.22ms
step:347/2160 train_time:11874ms step_avg:34.22ms
step:348/2160 train_time:11907ms step_avg:34.21ms
step:349/2160 train_time:11941ms step_avg:34.22ms
step:350/2160 train_time:11974ms step_avg:34.21ms
step:351/2160 train_time:12009ms step_avg:34.21ms
step:352/2160 train_time:12042ms step_avg:34.21ms
step:353/2160 train_time:12077ms step_avg:34.21ms
step:354/2160 train_time:12110ms step_avg:34.21ms
step:355/2160 train_time:12144ms step_avg:34.21ms
step:356/2160 train_time:12177ms step_avg:34.21ms
step:357/2160 train_time:12211ms step_avg:34.20ms
step:358/2160 train_time:12244ms step_avg:34.20ms
step:359/2160 train_time:12278ms step_avg:34.20ms
step:360/2160 train_time:12311ms step_avg:34.20ms
step:361/2160 train_time:12346ms step_avg:34.20ms
step:362/2160 train_time:12379ms step_avg:34.20ms
step:363/2160 train_time:12413ms step_avg:34.20ms
step:364/2160 train_time:12446ms step_avg:34.19ms
step:365/2160 train_time:12481ms step_avg:34.19ms
step:366/2160 train_time:12514ms step_avg:34.19ms
step:367/2160 train_time:12548ms step_avg:34.19ms
step:368/2160 train_time:12581ms step_avg:34.19ms
step:369/2160 train_time:12615ms step_avg:34.19ms
step:370/2160 train_time:12648ms step_avg:34.18ms
step:371/2160 train_time:12682ms step_avg:34.18ms
step:372/2160 train_time:12715ms step_avg:34.18ms
step:373/2160 train_time:12749ms step_avg:34.18ms
step:374/2160 train_time:12782ms step_avg:34.18ms
step:375/2160 train_time:12817ms step_avg:34.18ms
step:376/2160 train_time:12850ms step_avg:34.18ms
step:377/2160 train_time:12885ms step_avg:34.18ms
step:378/2160 train_time:12918ms step_avg:34.17ms
step:379/2160 train_time:12952ms step_avg:34.17ms
step:380/2160 train_time:12985ms step_avg:34.17ms
step:381/2160 train_time:13019ms step_avg:34.17ms
step:382/2160 train_time:13052ms step_avg:34.17ms
step:383/2160 train_time:13087ms step_avg:34.17ms
step:384/2160 train_time:13120ms step_avg:34.17ms
step:385/2160 train_time:13154ms step_avg:34.17ms
step:386/2160 train_time:13187ms step_avg:34.16ms
step:387/2160 train_time:13222ms step_avg:34.16ms
step:388/2160 train_time:13255ms step_avg:34.16ms
step:389/2160 train_time:13289ms step_avg:34.16ms
step:390/2160 train_time:13322ms step_avg:34.16ms
step:391/2160 train_time:13357ms step_avg:34.16ms
step:392/2160 train_time:13390ms step_avg:34.16ms
step:393/2160 train_time:13424ms step_avg:34.16ms
step:394/2160 train_time:13457ms step_avg:34.16ms
step:395/2160 train_time:13491ms step_avg:34.16ms
step:396/2160 train_time:13524ms step_avg:34.15ms
step:397/2160 train_time:13558ms step_avg:34.15ms
step:398/2160 train_time:13591ms step_avg:34.15ms
step:399/2160 train_time:13625ms step_avg:34.15ms
step:400/2160 train_time:13658ms step_avg:34.15ms
step:401/2160 train_time:13693ms step_avg:34.15ms
step:402/2160 train_time:13725ms step_avg:34.14ms
step:403/2160 train_time:13760ms step_avg:34.14ms
step:404/2160 train_time:13793ms step_avg:34.14ms
step:405/2160 train_time:13827ms step_avg:34.14ms
step:406/2160 train_time:13860ms step_avg:34.14ms
step:407/2160 train_time:13895ms step_avg:34.14ms
step:408/2160 train_time:13927ms step_avg:34.14ms
step:409/2160 train_time:13962ms step_avg:34.14ms
step:410/2160 train_time:13995ms step_avg:34.13ms
step:411/2160 train_time:14029ms step_avg:34.13ms
step:412/2160 train_time:14063ms step_avg:34.13ms
step:413/2160 train_time:14097ms step_avg:34.13ms
step:414/2160 train_time:14130ms step_avg:34.13ms
step:415/2160 train_time:14164ms step_avg:34.13ms
step:416/2160 train_time:14198ms step_avg:34.13ms
step:417/2160 train_time:14232ms step_avg:34.13ms
step:418/2160 train_time:14265ms step_avg:34.13ms
step:419/2160 train_time:14300ms step_avg:34.13ms
step:420/2160 train_time:14332ms step_avg:34.12ms
step:421/2160 train_time:14366ms step_avg:34.12ms
step:422/2160 train_time:14400ms step_avg:34.12ms
step:423/2160 train_time:14434ms step_avg:34.12ms
step:424/2160 train_time:14467ms step_avg:34.12ms
step:425/2160 train_time:14501ms step_avg:34.12ms
step:426/2160 train_time:14534ms step_avg:34.12ms
step:427/2160 train_time:14568ms step_avg:34.12ms
step:428/2160 train_time:14601ms step_avg:34.11ms
step:429/2160 train_time:14635ms step_avg:34.12ms
step:430/2160 train_time:14668ms step_avg:34.11ms
step:431/2160 train_time:14703ms step_avg:34.11ms
step:432/2160 train_time:14736ms step_avg:34.11ms
step:433/2160 train_time:14770ms step_avg:34.11ms
step:434/2160 train_time:14803ms step_avg:34.11ms
step:435/2160 train_time:14837ms step_avg:34.11ms
step:436/2160 train_time:14870ms step_avg:34.11ms
step:437/2160 train_time:14905ms step_avg:34.11ms
step:438/2160 train_time:14938ms step_avg:34.10ms
step:439/2160 train_time:14972ms step_avg:34.11ms
step:440/2160 train_time:15005ms step_avg:34.10ms
step:441/2160 train_time:15040ms step_avg:34.10ms
step:442/2160 train_time:15073ms step_avg:34.10ms
step:443/2160 train_time:15108ms step_avg:34.10ms
step:444/2160 train_time:15141ms step_avg:34.10ms
step:445/2160 train_time:15175ms step_avg:34.10ms
step:446/2160 train_time:15208ms step_avg:34.10ms
step:447/2160 train_time:15243ms step_avg:34.10ms
step:448/2160 train_time:15276ms step_avg:34.10ms
step:449/2160 train_time:15310ms step_avg:34.10ms
step:450/2160 train_time:15343ms step_avg:34.10ms
step:451/2160 train_time:15378ms step_avg:34.10ms
step:452/2160 train_time:15411ms step_avg:34.10ms
step:453/2160 train_time:15446ms step_avg:34.10ms
step:454/2160 train_time:15479ms step_avg:34.09ms
step:455/2160 train_time:15513ms step_avg:34.09ms
step:456/2160 train_time:15546ms step_avg:34.09ms
step:457/2160 train_time:15580ms step_avg:34.09ms
step:458/2160 train_time:15613ms step_avg:34.09ms
step:459/2160 train_time:15648ms step_avg:34.09ms
step:460/2160 train_time:15681ms step_avg:34.09ms
step:461/2160 train_time:15715ms step_avg:34.09ms
step:462/2160 train_time:15748ms step_avg:34.09ms
step:463/2160 train_time:15783ms step_avg:34.09ms
step:464/2160 train_time:15816ms step_avg:34.09ms
step:465/2160 train_time:15850ms step_avg:34.09ms
step:466/2160 train_time:15883ms step_avg:34.08ms
step:467/2160 train_time:15917ms step_avg:34.08ms
step:468/2160 train_time:15950ms step_avg:34.08ms
step:469/2160 train_time:15985ms step_avg:34.08ms
step:470/2160 train_time:16018ms step_avg:34.08ms
step:471/2160 train_time:16052ms step_avg:34.08ms
step:472/2160 train_time:16085ms step_avg:34.08ms
step:473/2160 train_time:16120ms step_avg:34.08ms
step:474/2160 train_time:16153ms step_avg:34.08ms
step:475/2160 train_time:16188ms step_avg:34.08ms
step:476/2160 train_time:16221ms step_avg:34.08ms
step:477/2160 train_time:16255ms step_avg:34.08ms
step:478/2160 train_time:16288ms step_avg:34.08ms
step:479/2160 train_time:16323ms step_avg:34.08ms
step:480/2160 train_time:16356ms step_avg:34.07ms
step:481/2160 train_time:16390ms step_avg:34.07ms
step:482/2160 train_time:16423ms step_avg:34.07ms
step:483/2160 train_time:16457ms step_avg:34.07ms
step:484/2160 train_time:16490ms step_avg:34.07ms
step:485/2160 train_time:16524ms step_avg:34.07ms
step:486/2160 train_time:16557ms step_avg:34.07ms
step:487/2160 train_time:16592ms step_avg:34.07ms
step:488/2160 train_time:16625ms step_avg:34.07ms
step:489/2160 train_time:16659ms step_avg:34.07ms
step:490/2160 train_time:16692ms step_avg:34.07ms
step:491/2160 train_time:16726ms step_avg:34.07ms
step:492/2160 train_time:16759ms step_avg:34.06ms
step:493/2160 train_time:16794ms step_avg:34.06ms
step:494/2160 train_time:16826ms step_avg:34.06ms
step:495/2160 train_time:16861ms step_avg:34.06ms
step:496/2160 train_time:16894ms step_avg:34.06ms
step:497/2160 train_time:16928ms step_avg:34.06ms
step:498/2160 train_time:16961ms step_avg:34.06ms
step:499/2160 train_time:16995ms step_avg:34.06ms
step:500/2160 train_time:17028ms step_avg:34.06ms
step:500/2160 val_loss:4.0062 train_time:17064ms step_avg:34.13ms
step:501/2160 train_time:17085ms step_avg:34.10ms
step:502/2160 train_time:17106ms step_avg:34.08ms
step:503/2160 train_time:17134ms step_avg:34.06ms
step:504/2160 train_time:17168ms step_avg:34.06ms
step:505/2160 train_time:17204ms step_avg:34.07ms
step:506/2160 train_time:17237ms step_avg:34.07ms
step:507/2160 train_time:17272ms step_avg:34.07ms
step:508/2160 train_time:17305ms step_avg:34.06ms
step:509/2160 train_time:17339ms step_avg:34.07ms
step:510/2160 train_time:17372ms step_avg:34.06ms
step:511/2160 train_time:17407ms step_avg:34.06ms
step:512/2160 train_time:17440ms step_avg:34.06ms
step:513/2160 train_time:17474ms step_avg:34.06ms
step:514/2160 train_time:17507ms step_avg:34.06ms
step:515/2160 train_time:17541ms step_avg:34.06ms
step:516/2160 train_time:17574ms step_avg:34.06ms
step:517/2160 train_time:17608ms step_avg:34.06ms
step:518/2160 train_time:17641ms step_avg:34.06ms
step:519/2160 train_time:17675ms step_avg:34.06ms
step:520/2160 train_time:17708ms step_avg:34.05ms
step:521/2160 train_time:17742ms step_avg:34.05ms
step:522/2160 train_time:17775ms step_avg:34.05ms
step:523/2160 train_time:17809ms step_avg:34.05ms
step:524/2160 train_time:17842ms step_avg:34.05ms
step:525/2160 train_time:17876ms step_avg:34.05ms
step:526/2160 train_time:17909ms step_avg:34.05ms
step:527/2160 train_time:17943ms step_avg:34.05ms
step:528/2160 train_time:17976ms step_avg:34.05ms
step:529/2160 train_time:18010ms step_avg:34.05ms
step:530/2160 train_time:18044ms step_avg:34.05ms
step:531/2160 train_time:18078ms step_avg:34.05ms
step:532/2160 train_time:18112ms step_avg:34.04ms
step:533/2160 train_time:18147ms step_avg:34.05ms
step:534/2160 train_time:18180ms step_avg:34.05ms
step:535/2160 train_time:18215ms step_avg:34.05ms
step:536/2160 train_time:18248ms step_avg:34.05ms
step:537/2160 train_time:18283ms step_avg:34.05ms
step:538/2160 train_time:18316ms step_avg:34.04ms
step:539/2160 train_time:18351ms step_avg:34.05ms
step:540/2160 train_time:18384ms step_avg:34.04ms
step:541/2160 train_time:18418ms step_avg:34.05ms
step:542/2160 train_time:18452ms step_avg:34.04ms
step:543/2160 train_time:18486ms step_avg:34.04ms
step:544/2160 train_time:18519ms step_avg:34.04ms
step:545/2160 train_time:18553ms step_avg:34.04ms
step:546/2160 train_time:18586ms step_avg:34.04ms
step:547/2160 train_time:18620ms step_avg:34.04ms
step:548/2160 train_time:18653ms step_avg:34.04ms
step:549/2160 train_time:18687ms step_avg:34.04ms
step:550/2160 train_time:18720ms step_avg:34.04ms
step:551/2160 train_time:18754ms step_avg:34.04ms
step:552/2160 train_time:18787ms step_avg:34.04ms
step:553/2160 train_time:18821ms step_avg:34.04ms
step:554/2160 train_time:18855ms step_avg:34.03ms
step:555/2160 train_time:18889ms step_avg:34.03ms
step:556/2160 train_time:18922ms step_avg:34.03ms
step:557/2160 train_time:18956ms step_avg:34.03ms
step:558/2160 train_time:18989ms step_avg:34.03ms
step:559/2160 train_time:19024ms step_avg:34.03ms
step:560/2160 train_time:19057ms step_avg:34.03ms
step:561/2160 train_time:19091ms step_avg:34.03ms
step:562/2160 train_time:19125ms step_avg:34.03ms
step:563/2160 train_time:19159ms step_avg:34.03ms
step:564/2160 train_time:19192ms step_avg:34.03ms
step:565/2160 train_time:19227ms step_avg:34.03ms
step:566/2160 train_time:19260ms step_avg:34.03ms
step:567/2160 train_time:19294ms step_avg:34.03ms
step:568/2160 train_time:19327ms step_avg:34.03ms
step:569/2160 train_time:19362ms step_avg:34.03ms
step:570/2160 train_time:19395ms step_avg:34.03ms
step:571/2160 train_time:19429ms step_avg:34.03ms
step:572/2160 train_time:19463ms step_avg:34.03ms
step:573/2160 train_time:19497ms step_avg:34.03ms
step:574/2160 train_time:19530ms step_avg:34.02ms
step:575/2160 train_time:19564ms step_avg:34.02ms
step:576/2160 train_time:19597ms step_avg:34.02ms
step:577/2160 train_time:19631ms step_avg:34.02ms
step:578/2160 train_time:19664ms step_avg:34.02ms
step:579/2160 train_time:19699ms step_avg:34.02ms
step:580/2160 train_time:19732ms step_avg:34.02ms
step:581/2160 train_time:19766ms step_avg:34.02ms
step:582/2160 train_time:19799ms step_avg:34.02ms
step:583/2160 train_time:19834ms step_avg:34.02ms
step:584/2160 train_time:19867ms step_avg:34.02ms
step:585/2160 train_time:19901ms step_avg:34.02ms
step:586/2160 train_time:19934ms step_avg:34.02ms
step:587/2160 train_time:19968ms step_avg:34.02ms
step:588/2160 train_time:20001ms step_avg:34.02ms
step:589/2160 train_time:20035ms step_avg:34.02ms
step:590/2160 train_time:20068ms step_avg:34.01ms
step:591/2160 train_time:20103ms step_avg:34.02ms
step:592/2160 train_time:20136ms step_avg:34.01ms
step:593/2160 train_time:20171ms step_avg:34.01ms
step:594/2160 train_time:20204ms step_avg:34.01ms
step:595/2160 train_time:20238ms step_avg:34.01ms
step:596/2160 train_time:20271ms step_avg:34.01ms
step:597/2160 train_time:20306ms step_avg:34.01ms
step:598/2160 train_time:20339ms step_avg:34.01ms
step:599/2160 train_time:20373ms step_avg:34.01ms
step:600/2160 train_time:20407ms step_avg:34.01ms
step:601/2160 train_time:20441ms step_avg:34.01ms
step:602/2160 train_time:20474ms step_avg:34.01ms
step:603/2160 train_time:20508ms step_avg:34.01ms
step:604/2160 train_time:20542ms step_avg:34.01ms
step:605/2160 train_time:20576ms step_avg:34.01ms
step:606/2160 train_time:20609ms step_avg:34.01ms
step:607/2160 train_time:20643ms step_avg:34.01ms
step:608/2160 train_time:20676ms step_avg:34.01ms
step:609/2160 train_time:20711ms step_avg:34.01ms
step:610/2160 train_time:20744ms step_avg:34.01ms
step:611/2160 train_time:20778ms step_avg:34.01ms
step:612/2160 train_time:20811ms step_avg:34.00ms
step:613/2160 train_time:20845ms step_avg:34.01ms
step:614/2160 train_time:20878ms step_avg:34.00ms
step:615/2160 train_time:20913ms step_avg:34.00ms
step:616/2160 train_time:20946ms step_avg:34.00ms
step:617/2160 train_time:20980ms step_avg:34.00ms
step:618/2160 train_time:21013ms step_avg:34.00ms
step:619/2160 train_time:21048ms step_avg:34.00ms
step:620/2160 train_time:21081ms step_avg:34.00ms
step:621/2160 train_time:21115ms step_avg:34.00ms
step:622/2160 train_time:21149ms step_avg:34.00ms
step:623/2160 train_time:21183ms step_avg:34.00ms
step:624/2160 train_time:21216ms step_avg:34.00ms
step:625/2160 train_time:21251ms step_avg:34.00ms
step:626/2160 train_time:21284ms step_avg:34.00ms
step:627/2160 train_time:21319ms step_avg:34.00ms
step:628/2160 train_time:21352ms step_avg:34.00ms
step:629/2160 train_time:21387ms step_avg:34.00ms
step:630/2160 train_time:21420ms step_avg:34.00ms
step:631/2160 train_time:21454ms step_avg:34.00ms
step:632/2160 train_time:21487ms step_avg:34.00ms
step:633/2160 train_time:21522ms step_avg:34.00ms
step:634/2160 train_time:21555ms step_avg:34.00ms
step:635/2160 train_time:21589ms step_avg:34.00ms
step:636/2160 train_time:21622ms step_avg:34.00ms
step:637/2160 train_time:21656ms step_avg:34.00ms
step:638/2160 train_time:21690ms step_avg:34.00ms
step:639/2160 train_time:21724ms step_avg:34.00ms
step:640/2160 train_time:21757ms step_avg:34.00ms
step:641/2160 train_time:21792ms step_avg:34.00ms
step:642/2160 train_time:21825ms step_avg:34.00ms
step:643/2160 train_time:21859ms step_avg:34.00ms
step:644/2160 train_time:21892ms step_avg:33.99ms
step:645/2160 train_time:21927ms step_avg:33.99ms
step:646/2160 train_time:21960ms step_avg:33.99ms
step:647/2160 train_time:21994ms step_avg:33.99ms
step:648/2160 train_time:22027ms step_avg:33.99ms
step:649/2160 train_time:22061ms step_avg:33.99ms
step:650/2160 train_time:22094ms step_avg:33.99ms
step:651/2160 train_time:22129ms step_avg:33.99ms
step:652/2160 train_time:22162ms step_avg:33.99ms
step:653/2160 train_time:22196ms step_avg:33.99ms
step:654/2160 train_time:22229ms step_avg:33.99ms
step:655/2160 train_time:22263ms step_avg:33.99ms
step:656/2160 train_time:22296ms step_avg:33.99ms
step:657/2160 train_time:22331ms step_avg:33.99ms
step:658/2160 train_time:22365ms step_avg:33.99ms
step:659/2160 train_time:22399ms step_avg:33.99ms
step:660/2160 train_time:22432ms step_avg:33.99ms
step:661/2160 train_time:22467ms step_avg:33.99ms
step:662/2160 train_time:22500ms step_avg:33.99ms
step:663/2160 train_time:22535ms step_avg:33.99ms
step:664/2160 train_time:22568ms step_avg:33.99ms
step:665/2160 train_time:22602ms step_avg:33.99ms
step:666/2160 train_time:22636ms step_avg:33.99ms
step:667/2160 train_time:22670ms step_avg:33.99ms
step:668/2160 train_time:22704ms step_avg:33.99ms
step:669/2160 train_time:22738ms step_avg:33.99ms
step:670/2160 train_time:22771ms step_avg:33.99ms
step:671/2160 train_time:22806ms step_avg:33.99ms
step:672/2160 train_time:22839ms step_avg:33.99ms
step:673/2160 train_time:22873ms step_avg:33.99ms
step:674/2160 train_time:22907ms step_avg:33.99ms
step:675/2160 train_time:22941ms step_avg:33.99ms
step:676/2160 train_time:22974ms step_avg:33.99ms
step:677/2160 train_time:23008ms step_avg:33.99ms
step:678/2160 train_time:23042ms step_avg:33.98ms
step:679/2160 train_time:23076ms step_avg:33.98ms
step:680/2160 train_time:23109ms step_avg:33.98ms
step:681/2160 train_time:23144ms step_avg:33.99ms
step:682/2160 train_time:23177ms step_avg:33.98ms
step:683/2160 train_time:23212ms step_avg:33.98ms
step:684/2160 train_time:23245ms step_avg:33.98ms
step:685/2160 train_time:23279ms step_avg:33.98ms
step:686/2160 train_time:23312ms step_avg:33.98ms
step:687/2160 train_time:23347ms step_avg:33.98ms
step:688/2160 train_time:23380ms step_avg:33.98ms
step:689/2160 train_time:23414ms step_avg:33.98ms
step:690/2160 train_time:23448ms step_avg:33.98ms
step:691/2160 train_time:23483ms step_avg:33.98ms
step:692/2160 train_time:23516ms step_avg:33.98ms
step:693/2160 train_time:23550ms step_avg:33.98ms
step:694/2160 train_time:23583ms step_avg:33.98ms
step:695/2160 train_time:23618ms step_avg:33.98ms
step:696/2160 train_time:23651ms step_avg:33.98ms
step:697/2160 train_time:23686ms step_avg:33.98ms
step:698/2160 train_time:23719ms step_avg:33.98ms
step:699/2160 train_time:23753ms step_avg:33.98ms
step:700/2160 train_time:23786ms step_avg:33.98ms
step:701/2160 train_time:23820ms step_avg:33.98ms
step:702/2160 train_time:23853ms step_avg:33.98ms
step:703/2160 train_time:23888ms step_avg:33.98ms
step:704/2160 train_time:23921ms step_avg:33.98ms
step:705/2160 train_time:23955ms step_avg:33.98ms
step:706/2160 train_time:23988ms step_avg:33.98ms
step:707/2160 train_time:24023ms step_avg:33.98ms
step:708/2160 train_time:24057ms step_avg:33.98ms
step:709/2160 train_time:24118ms step_avg:34.02ms
step:710/2160 train_time:24178ms step_avg:34.05ms
step:711/2160 train_time:24239ms step_avg:34.09ms
step:712/2160 train_time:24300ms step_avg:34.13ms
step:713/2160 train_time:24361ms step_avg:34.17ms
step:714/2160 train_time:24420ms step_avg:34.20ms
step:715/2160 train_time:24482ms step_avg:34.24ms
step:716/2160 train_time:24542ms step_avg:34.28ms
step:717/2160 train_time:24604ms step_avg:34.31ms
step:718/2160 train_time:24663ms step_avg:34.35ms
step:719/2160 train_time:24725ms step_avg:34.39ms
step:720/2160 train_time:24784ms step_avg:34.42ms
step:721/2160 train_time:24846ms step_avg:34.46ms
step:722/2160 train_time:24906ms step_avg:34.50ms
step:723/2160 train_time:24968ms step_avg:34.53ms
step:724/2160 train_time:25028ms step_avg:34.57ms
step:725/2160 train_time:25089ms step_avg:34.61ms
step:726/2160 train_time:25148ms step_avg:34.64ms
step:727/2160 train_time:25210ms step_avg:34.68ms
step:728/2160 train_time:25270ms step_avg:34.71ms
step:729/2160 train_time:25332ms step_avg:34.75ms
step:730/2160 train_time:25392ms step_avg:34.78ms
step:731/2160 train_time:25453ms step_avg:34.82ms
step:732/2160 train_time:25514ms step_avg:34.85ms
step:733/2160 train_time:25576ms step_avg:34.89ms
step:734/2160 train_time:25636ms step_avg:34.93ms
step:735/2160 train_time:25698ms step_avg:34.96ms
step:736/2160 train_time:25759ms step_avg:35.00ms
step:737/2160 train_time:25821ms step_avg:35.04ms
step:738/2160 train_time:25881ms step_avg:35.07ms
step:739/2160 train_time:25942ms step_avg:35.10ms
step:740/2160 train_time:26002ms step_avg:35.14ms
step:741/2160 train_time:26064ms step_avg:35.17ms
step:742/2160 train_time:26124ms step_avg:35.21ms
step:743/2160 train_time:26186ms step_avg:35.24ms
step:744/2160 train_time:26246ms step_avg:35.28ms
step:745/2160 train_time:26308ms step_avg:35.31ms
step:746/2160 train_time:26368ms step_avg:35.35ms
step:747/2160 train_time:26430ms step_avg:35.38ms
step:748/2160 train_time:26490ms step_avg:35.41ms
step:749/2160 train_time:26553ms step_avg:35.45ms
step:750/2160 train_time:26613ms step_avg:35.48ms
step:750/2160 val_loss:3.8505 train_time:26675ms step_avg:35.57ms
step:751/2160 train_time:26698ms step_avg:35.55ms
step:752/2160 train_time:26737ms step_avg:35.55ms
step:753/2160 train_time:26803ms step_avg:35.59ms
step:754/2160 train_time:26866ms step_avg:35.63ms
step:755/2160 train_time:26927ms step_avg:35.67ms
step:756/2160 train_time:26987ms step_avg:35.70ms
step:757/2160 train_time:27048ms step_avg:35.73ms
step:758/2160 train_time:27107ms step_avg:35.76ms
step:759/2160 train_time:27168ms step_avg:35.79ms
step:760/2160 train_time:27227ms step_avg:35.82ms
step:761/2160 train_time:27288ms step_avg:35.86ms
step:762/2160 train_time:27346ms step_avg:35.89ms
step:763/2160 train_time:27406ms step_avg:35.92ms
step:764/2160 train_time:27465ms step_avg:35.95ms
step:765/2160 train_time:27526ms step_avg:35.98ms
step:766/2160 train_time:27589ms step_avg:36.02ms
step:767/2160 train_time:27655ms step_avg:36.06ms
step:768/2160 train_time:27718ms step_avg:36.09ms
step:769/2160 train_time:27781ms step_avg:36.13ms
step:770/2160 train_time:27840ms step_avg:36.16ms
step:771/2160 train_time:27902ms step_avg:36.19ms
step:772/2160 train_time:27961ms step_avg:36.22ms
step:773/2160 train_time:28023ms step_avg:36.25ms
step:774/2160 train_time:28082ms step_avg:36.28ms
step:775/2160 train_time:28144ms step_avg:36.31ms
step:776/2160 train_time:28203ms step_avg:36.34ms
step:777/2160 train_time:28264ms step_avg:36.38ms
step:778/2160 train_time:28323ms step_avg:36.41ms
step:779/2160 train_time:28384ms step_avg:36.44ms
step:780/2160 train_time:28444ms step_avg:36.47ms
step:781/2160 train_time:28505ms step_avg:36.50ms
step:782/2160 train_time:28565ms step_avg:36.53ms
step:783/2160 train_time:28629ms step_avg:36.56ms
step:784/2160 train_time:28690ms step_avg:36.59ms
step:785/2160 train_time:28751ms step_avg:36.63ms
step:786/2160 train_time:28811ms step_avg:36.66ms
step:787/2160 train_time:28873ms step_avg:36.69ms
step:788/2160 train_time:28934ms step_avg:36.72ms
step:789/2160 train_time:28996ms step_avg:36.75ms
step:790/2160 train_time:29055ms step_avg:36.78ms
step:791/2160 train_time:29116ms step_avg:36.81ms
step:792/2160 train_time:29176ms step_avg:36.84ms
step:793/2160 train_time:29238ms step_avg:36.87ms
step:794/2160 train_time:29297ms step_avg:36.90ms
step:795/2160 train_time:29358ms step_avg:36.93ms
step:796/2160 train_time:29418ms step_avg:36.96ms
step:797/2160 train_time:29479ms step_avg:36.99ms
step:798/2160 train_time:29540ms step_avg:37.02ms
step:799/2160 train_time:29602ms step_avg:37.05ms
step:800/2160 train_time:29663ms step_avg:37.08ms
step:801/2160 train_time:29726ms step_avg:37.11ms
step:802/2160 train_time:29786ms step_avg:37.14ms
step:803/2160 train_time:29848ms step_avg:37.17ms
step:804/2160 train_time:29907ms step_avg:37.20ms
step:805/2160 train_time:29968ms step_avg:37.23ms
step:806/2160 train_time:30028ms step_avg:37.26ms
step:807/2160 train_time:30090ms step_avg:37.29ms
step:808/2160 train_time:30150ms step_avg:37.31ms
step:809/2160 train_time:30211ms step_avg:37.34ms
step:810/2160 train_time:30271ms step_avg:37.37ms
step:811/2160 train_time:30333ms step_avg:37.40ms
step:812/2160 train_time:30393ms step_avg:37.43ms
step:813/2160 train_time:30455ms step_avg:37.46ms
step:814/2160 train_time:30514ms step_avg:37.49ms
step:815/2160 train_time:30576ms step_avg:37.52ms
step:816/2160 train_time:30637ms step_avg:37.55ms
step:817/2160 train_time:30700ms step_avg:37.58ms
step:818/2160 train_time:30760ms step_avg:37.60ms
step:819/2160 train_time:30822ms step_avg:37.63ms
step:820/2160 train_time:30883ms step_avg:37.66ms
step:821/2160 train_time:30945ms step_avg:37.69ms
step:822/2160 train_time:31004ms step_avg:37.72ms
step:823/2160 train_time:31066ms step_avg:37.75ms
step:824/2160 train_time:31126ms step_avg:37.77ms
step:825/2160 train_time:31187ms step_avg:37.80ms
step:826/2160 train_time:31246ms step_avg:37.83ms
step:827/2160 train_time:31308ms step_avg:37.86ms
step:828/2160 train_time:31367ms step_avg:37.88ms
step:829/2160 train_time:31428ms step_avg:37.91ms
step:830/2160 train_time:31488ms step_avg:37.94ms
step:831/2160 train_time:31549ms step_avg:37.97ms
step:832/2160 train_time:31609ms step_avg:37.99ms
step:833/2160 train_time:31671ms step_avg:38.02ms
step:834/2160 train_time:31731ms step_avg:38.05ms
step:835/2160 train_time:31793ms step_avg:38.08ms
step:836/2160 train_time:31854ms step_avg:38.10ms
step:837/2160 train_time:31916ms step_avg:38.13ms
step:838/2160 train_time:31975ms step_avg:38.16ms
step:839/2160 train_time:32037ms step_avg:38.18ms
step:840/2160 train_time:32096ms step_avg:38.21ms
step:841/2160 train_time:32158ms step_avg:38.24ms
step:842/2160 train_time:32217ms step_avg:38.26ms
step:843/2160 train_time:32279ms step_avg:38.29ms
step:844/2160 train_time:32339ms step_avg:38.32ms
step:845/2160 train_time:32401ms step_avg:38.34ms
step:846/2160 train_time:32461ms step_avg:38.37ms
step:847/2160 train_time:32523ms step_avg:38.40ms
step:848/2160 train_time:32584ms step_avg:38.42ms
step:849/2160 train_time:32646ms step_avg:38.45ms
step:850/2160 train_time:32706ms step_avg:38.48ms
step:851/2160 train_time:32768ms step_avg:38.50ms
step:852/2160 train_time:32827ms step_avg:38.53ms
step:853/2160 train_time:32889ms step_avg:38.56ms
step:854/2160 train_time:32949ms step_avg:38.58ms
step:855/2160 train_time:33010ms step_avg:38.61ms
step:856/2160 train_time:33070ms step_avg:38.63ms
step:857/2160 train_time:33131ms step_avg:38.66ms
step:858/2160 train_time:33191ms step_avg:38.68ms
step:859/2160 train_time:33253ms step_avg:38.71ms
step:860/2160 train_time:33312ms step_avg:38.73ms
step:861/2160 train_time:33374ms step_avg:38.76ms
step:862/2160 train_time:33433ms step_avg:38.79ms
step:863/2160 train_time:33495ms step_avg:38.81ms
step:864/2160 train_time:33554ms step_avg:38.84ms
step:865/2160 train_time:33616ms step_avg:38.86ms
step:866/2160 train_time:33676ms step_avg:38.89ms
step:867/2160 train_time:33738ms step_avg:38.91ms
step:868/2160 train_time:33799ms step_avg:38.94ms
step:869/2160 train_time:33860ms step_avg:38.96ms
step:870/2160 train_time:33921ms step_avg:38.99ms
step:871/2160 train_time:33983ms step_avg:39.02ms
step:872/2160 train_time:34044ms step_avg:39.04ms
step:873/2160 train_time:34106ms step_avg:39.07ms
step:874/2160 train_time:34165ms step_avg:39.09ms
step:875/2160 train_time:34226ms step_avg:39.12ms
step:876/2160 train_time:34286ms step_avg:39.14ms
step:877/2160 train_time:34348ms step_avg:39.17ms
step:878/2160 train_time:34407ms step_avg:39.19ms
step:879/2160 train_time:34469ms step_avg:39.21ms
step:880/2160 train_time:34527ms step_avg:39.24ms
step:881/2160 train_time:34589ms step_avg:39.26ms
step:882/2160 train_time:34650ms step_avg:39.29ms
step:883/2160 train_time:34712ms step_avg:39.31ms
step:884/2160 train_time:34772ms step_avg:39.33ms
step:885/2160 train_time:34833ms step_avg:39.36ms
step:886/2160 train_time:34893ms step_avg:39.38ms
step:887/2160 train_time:34955ms step_avg:39.41ms
step:888/2160 train_time:35015ms step_avg:39.43ms
step:889/2160 train_time:35077ms step_avg:39.46ms
step:890/2160 train_time:35136ms step_avg:39.48ms
step:891/2160 train_time:35198ms step_avg:39.50ms
step:892/2160 train_time:35257ms step_avg:39.53ms
step:893/2160 train_time:35320ms step_avg:39.55ms
step:894/2160 train_time:35380ms step_avg:39.57ms
step:895/2160 train_time:35442ms step_avg:39.60ms
step:896/2160 train_time:35501ms step_avg:39.62ms
step:897/2160 train_time:35563ms step_avg:39.65ms
step:898/2160 train_time:35622ms step_avg:39.67ms
step:899/2160 train_time:35685ms step_avg:39.69ms
step:900/2160 train_time:35746ms step_avg:39.72ms
step:901/2160 train_time:35808ms step_avg:39.74ms
step:902/2160 train_time:35868ms step_avg:39.76ms
step:903/2160 train_time:35929ms step_avg:39.79ms
step:904/2160 train_time:35988ms step_avg:39.81ms
step:905/2160 train_time:36050ms step_avg:39.83ms
step:906/2160 train_time:36111ms step_avg:39.86ms
step:907/2160 train_time:36173ms step_avg:39.88ms
step:908/2160 train_time:36233ms step_avg:39.90ms
step:909/2160 train_time:36295ms step_avg:39.93ms
step:910/2160 train_time:36355ms step_avg:39.95ms
step:911/2160 train_time:36417ms step_avg:39.97ms
step:912/2160 train_time:36477ms step_avg:40.00ms
step:913/2160 train_time:36538ms step_avg:40.02ms
step:914/2160 train_time:36598ms step_avg:40.04ms
step:915/2160 train_time:36660ms step_avg:40.07ms
step:916/2160 train_time:36721ms step_avg:40.09ms
step:917/2160 train_time:36783ms step_avg:40.11ms
step:918/2160 train_time:36844ms step_avg:40.13ms
step:919/2160 train_time:36906ms step_avg:40.16ms
step:920/2160 train_time:36966ms step_avg:40.18ms
step:921/2160 train_time:37028ms step_avg:40.20ms
step:922/2160 train_time:37087ms step_avg:40.22ms
step:923/2160 train_time:37149ms step_avg:40.25ms
step:924/2160 train_time:37208ms step_avg:40.27ms
step:925/2160 train_time:37269ms step_avg:40.29ms
step:926/2160 train_time:37330ms step_avg:40.31ms
step:927/2160 train_time:37392ms step_avg:40.34ms
step:928/2160 train_time:37452ms step_avg:40.36ms
step:929/2160 train_time:37514ms step_avg:40.38ms
step:930/2160 train_time:37574ms step_avg:40.40ms
step:931/2160 train_time:37636ms step_avg:40.43ms
step:932/2160 train_time:37695ms step_avg:40.45ms
step:933/2160 train_time:37757ms step_avg:40.47ms
step:934/2160 train_time:37817ms step_avg:40.49ms
step:935/2160 train_time:37880ms step_avg:40.51ms
step:936/2160 train_time:37940ms step_avg:40.53ms
step:937/2160 train_time:38002ms step_avg:40.56ms
step:938/2160 train_time:38063ms step_avg:40.58ms
step:939/2160 train_time:38125ms step_avg:40.60ms
step:940/2160 train_time:38185ms step_avg:40.62ms
step:941/2160 train_time:38247ms step_avg:40.64ms
step:942/2160 train_time:38306ms step_avg:40.66ms
step:943/2160 train_time:38368ms step_avg:40.69ms
step:944/2160 train_time:38427ms step_avg:40.71ms
step:945/2160 train_time:38488ms step_avg:40.73ms
step:946/2160 train_time:38548ms step_avg:40.75ms
step:947/2160 train_time:38610ms step_avg:40.77ms
step:948/2160 train_time:38670ms step_avg:40.79ms
step:949/2160 train_time:38733ms step_avg:40.81ms
step:950/2160 train_time:38792ms step_avg:40.83ms
step:951/2160 train_time:38854ms step_avg:40.86ms
step:952/2160 train_time:38914ms step_avg:40.88ms
step:953/2160 train_time:38976ms step_avg:40.90ms
step:954/2160 train_time:39036ms step_avg:40.92ms
step:955/2160 train_time:39098ms step_avg:40.94ms
step:956/2160 train_time:39158ms step_avg:40.96ms
step:957/2160 train_time:39220ms step_avg:40.98ms
step:958/2160 train_time:39281ms step_avg:41.00ms
step:959/2160 train_time:39343ms step_avg:41.02ms
step:960/2160 train_time:39403ms step_avg:41.04ms
step:961/2160 train_time:39465ms step_avg:41.07ms
step:962/2160 train_time:39525ms step_avg:41.09ms
step:963/2160 train_time:39587ms step_avg:41.11ms
step:964/2160 train_time:39647ms step_avg:41.13ms
step:965/2160 train_time:39708ms step_avg:41.15ms
step:966/2160 train_time:39768ms step_avg:41.17ms
step:967/2160 train_time:39829ms step_avg:41.19ms
step:968/2160 train_time:39889ms step_avg:41.21ms
step:969/2160 train_time:39951ms step_avg:41.23ms
step:970/2160 train_time:40010ms step_avg:41.25ms
step:971/2160 train_time:40072ms step_avg:41.27ms
step:972/2160 train_time:40133ms step_avg:41.29ms
step:973/2160 train_time:40194ms step_avg:41.31ms
step:974/2160 train_time:40254ms step_avg:41.33ms
step:975/2160 train_time:40316ms step_avg:41.35ms
step:976/2160 train_time:40376ms step_avg:41.37ms
step:977/2160 train_time:40438ms step_avg:41.39ms
step:978/2160 train_time:40498ms step_avg:41.41ms
step:979/2160 train_time:40560ms step_avg:41.43ms
step:980/2160 train_time:40620ms step_avg:41.45ms
step:981/2160 train_time:40683ms step_avg:41.47ms
step:982/2160 train_time:40743ms step_avg:41.49ms
step:983/2160 train_time:40805ms step_avg:41.51ms
step:984/2160 train_time:40865ms step_avg:41.53ms
step:985/2160 train_time:40927ms step_avg:41.55ms
step:986/2160 train_time:40986ms step_avg:41.57ms
step:987/2160 train_time:41048ms step_avg:41.59ms
step:988/2160 train_time:41108ms step_avg:41.61ms
step:989/2160 train_time:41169ms step_avg:41.63ms
step:990/2160 train_time:41229ms step_avg:41.65ms
step:991/2160 train_time:41292ms step_avg:41.67ms
step:992/2160 train_time:41351ms step_avg:41.68ms
step:993/2160 train_time:41413ms step_avg:41.71ms
step:994/2160 train_time:41473ms step_avg:41.72ms
step:995/2160 train_time:41536ms step_avg:41.74ms
step:996/2160 train_time:41596ms step_avg:41.76ms
step:997/2160 train_time:41657ms step_avg:41.78ms
step:998/2160 train_time:41717ms step_avg:41.80ms
step:999/2160 train_time:41779ms step_avg:41.82ms
step:1000/2160 train_time:41839ms step_avg:41.84ms
step:1000/2160 val_loss:3.6886 train_time:41902ms step_avg:41.90ms
step:1001/2160 train_time:41924ms step_avg:41.88ms
step:1002/2160 train_time:41963ms step_avg:41.88ms
step:1003/2160 train_time:42027ms step_avg:41.90ms
step:1004/2160 train_time:42089ms step_avg:41.92ms
step:1005/2160 train_time:42150ms step_avg:41.94ms
step:1006/2160 train_time:42210ms step_avg:41.96ms
step:1007/2160 train_time:42271ms step_avg:41.98ms
step:1008/2160 train_time:42330ms step_avg:41.99ms
step:1009/2160 train_time:42391ms step_avg:42.01ms
step:1010/2160 train_time:42450ms step_avg:42.03ms
step:1011/2160 train_time:42511ms step_avg:42.05ms
step:1012/2160 train_time:42570ms step_avg:42.07ms
step:1013/2160 train_time:42632ms step_avg:42.08ms
step:1014/2160 train_time:42692ms step_avg:42.10ms
step:1015/2160 train_time:42753ms step_avg:42.12ms
step:1016/2160 train_time:42812ms step_avg:42.14ms
step:1017/2160 train_time:42875ms step_avg:42.16ms
step:1018/2160 train_time:42937ms step_avg:42.18ms
step:1019/2160 train_time:43000ms step_avg:42.20ms
step:1020/2160 train_time:43061ms step_avg:42.22ms
step:1021/2160 train_time:43125ms step_avg:42.24ms
step:1022/2160 train_time:43185ms step_avg:42.26ms
step:1023/2160 train_time:43247ms step_avg:42.27ms
step:1024/2160 train_time:43306ms step_avg:42.29ms
step:1025/2160 train_time:43367ms step_avg:42.31ms
step:1026/2160 train_time:43426ms step_avg:42.33ms
step:1027/2160 train_time:43487ms step_avg:42.34ms
step:1028/2160 train_time:43546ms step_avg:42.36ms
step:1029/2160 train_time:43607ms step_avg:42.38ms
step:1030/2160 train_time:43667ms step_avg:42.39ms
step:1031/2160 train_time:43729ms step_avg:42.41ms
step:1032/2160 train_time:43789ms step_avg:42.43ms
step:1033/2160 train_time:43851ms step_avg:42.45ms
step:1034/2160 train_time:43912ms step_avg:42.47ms
step:1035/2160 train_time:43975ms step_avg:42.49ms
step:1036/2160 train_time:44036ms step_avg:42.51ms
step:1037/2160 train_time:44098ms step_avg:42.52ms
step:1038/2160 train_time:44159ms step_avg:42.54ms
step:1039/2160 train_time:44221ms step_avg:42.56ms
step:1040/2160 train_time:44281ms step_avg:42.58ms
step:1041/2160 train_time:44342ms step_avg:42.60ms
step:1042/2160 train_time:44403ms step_avg:42.61ms
step:1043/2160 train_time:44465ms step_avg:42.63ms
step:1044/2160 train_time:44524ms step_avg:42.65ms
step:1045/2160 train_time:44586ms step_avg:42.67ms
step:1046/2160 train_time:44645ms step_avg:42.68ms
step:1047/2160 train_time:44706ms step_avg:42.70ms
step:1048/2160 train_time:44766ms step_avg:42.72ms
step:1049/2160 train_time:44828ms step_avg:42.73ms
step:1050/2160 train_time:44888ms step_avg:42.75ms
step:1051/2160 train_time:44950ms step_avg:42.77ms
step:1052/2160 train_time:45011ms step_avg:42.79ms
step:1053/2160 train_time:45074ms step_avg:42.81ms
step:1054/2160 train_time:45134ms step_avg:42.82ms
step:1055/2160 train_time:45196ms step_avg:42.84ms
step:1056/2160 train_time:45256ms step_avg:42.86ms
step:1057/2160 train_time:45317ms step_avg:42.87ms
step:1058/2160 train_time:45378ms step_avg:42.89ms
step:1059/2160 train_time:45439ms step_avg:42.91ms
step:1060/2160 train_time:45499ms step_avg:42.92ms
step:1061/2160 train_time:45562ms step_avg:42.94ms
step:1062/2160 train_time:45621ms step_avg:42.96ms
step:1063/2160 train_time:45683ms step_avg:42.98ms
step:1064/2160 train_time:45743ms step_avg:42.99ms
step:1065/2160 train_time:45806ms step_avg:43.01ms
step:1066/2160 train_time:45866ms step_avg:43.03ms
step:1067/2160 train_time:45928ms step_avg:43.04ms
step:1068/2160 train_time:45988ms step_avg:43.06ms
step:1069/2160 train_time:46049ms step_avg:43.08ms
step:1070/2160 train_time:46109ms step_avg:43.09ms
step:1071/2160 train_time:46170ms step_avg:43.11ms
step:1072/2160 train_time:46230ms step_avg:43.13ms
step:1073/2160 train_time:46292ms step_avg:43.14ms
step:1074/2160 train_time:46352ms step_avg:43.16ms
step:1075/2160 train_time:46413ms step_avg:43.18ms
step:1076/2160 train_time:46473ms step_avg:43.19ms
step:1077/2160 train_time:46536ms step_avg:43.21ms
step:1078/2160 train_time:46596ms step_avg:43.22ms
step:1079/2160 train_time:46658ms step_avg:43.24ms
step:1080/2160 train_time:46718ms step_avg:43.26ms
step:1081/2160 train_time:46781ms step_avg:43.28ms
step:1082/2160 train_time:46841ms step_avg:43.29ms
step:1083/2160 train_time:46904ms step_avg:43.31ms
step:1084/2160 train_time:46964ms step_avg:43.33ms
step:1085/2160 train_time:47027ms step_avg:43.34ms
step:1086/2160 train_time:47086ms step_avg:43.36ms
step:1087/2160 train_time:47148ms step_avg:43.37ms
step:1088/2160 train_time:47207ms step_avg:43.39ms
step:1089/2160 train_time:47269ms step_avg:43.41ms
step:1090/2160 train_time:47329ms step_avg:43.42ms
step:1091/2160 train_time:47391ms step_avg:43.44ms
step:1092/2160 train_time:47451ms step_avg:43.45ms
step:1093/2160 train_time:47512ms step_avg:43.47ms
step:1094/2160 train_time:47572ms step_avg:43.48ms
step:1095/2160 train_time:47634ms step_avg:43.50ms
step:1096/2160 train_time:47694ms step_avg:43.52ms
step:1097/2160 train_time:47757ms step_avg:43.53ms
step:1098/2160 train_time:47817ms step_avg:43.55ms
step:1099/2160 train_time:47880ms step_avg:43.57ms
step:1100/2160 train_time:47940ms step_avg:43.58ms
step:1101/2160 train_time:48003ms step_avg:43.60ms
step:1102/2160 train_time:48063ms step_avg:43.61ms
step:1103/2160 train_time:48125ms step_avg:43.63ms
step:1104/2160 train_time:48184ms step_avg:43.65ms
step:1105/2160 train_time:48246ms step_avg:43.66ms
step:1106/2160 train_time:48306ms step_avg:43.68ms
step:1107/2160 train_time:48368ms step_avg:43.69ms
step:1108/2160 train_time:48428ms step_avg:43.71ms
step:1109/2160 train_time:48490ms step_avg:43.72ms
step:1110/2160 train_time:48550ms step_avg:43.74ms
step:1111/2160 train_time:48612ms step_avg:43.76ms
step:1112/2160 train_time:48673ms step_avg:43.77ms
step:1113/2160 train_time:48735ms step_avg:43.79ms
step:1114/2160 train_time:48795ms step_avg:43.80ms
step:1115/2160 train_time:48857ms step_avg:43.82ms
step:1116/2160 train_time:48917ms step_avg:43.83ms
step:1117/2160 train_time:48980ms step_avg:43.85ms
step:1118/2160 train_time:49041ms step_avg:43.86ms
step:1119/2160 train_time:49103ms step_avg:43.88ms
step:1120/2160 train_time:49164ms step_avg:43.90ms
step:1121/2160 train_time:49227ms step_avg:43.91ms
step:1122/2160 train_time:49287ms step_avg:43.93ms
step:1123/2160 train_time:49348ms step_avg:43.94ms
step:1124/2160 train_time:49407ms step_avg:43.96ms
step:1125/2160 train_time:49469ms step_avg:43.97ms
step:1126/2160 train_time:49528ms step_avg:43.99ms
step:1127/2160 train_time:49590ms step_avg:44.00ms
step:1128/2160 train_time:49649ms step_avg:44.02ms
step:1129/2160 train_time:49711ms step_avg:44.03ms
step:1130/2160 train_time:49771ms step_avg:44.05ms
step:1131/2160 train_time:49834ms step_avg:44.06ms
step:1132/2160 train_time:49894ms step_avg:44.08ms
step:1133/2160 train_time:49955ms step_avg:44.09ms
step:1134/2160 train_time:50015ms step_avg:44.10ms
step:1135/2160 train_time:50077ms step_avg:44.12ms
step:1136/2160 train_time:50137ms step_avg:44.13ms
step:1137/2160 train_time:50200ms step_avg:44.15ms
step:1138/2160 train_time:50261ms step_avg:44.17ms
step:1139/2160 train_time:50323ms step_avg:44.18ms
step:1140/2160 train_time:50383ms step_avg:44.20ms
step:1141/2160 train_time:50445ms step_avg:44.21ms
step:1142/2160 train_time:50505ms step_avg:44.22ms
step:1143/2160 train_time:50567ms step_avg:44.24ms
step:1144/2160 train_time:50627ms step_avg:44.25ms
step:1145/2160 train_time:50689ms step_avg:44.27ms
step:1146/2160 train_time:50748ms step_avg:44.28ms
step:1147/2160 train_time:50810ms step_avg:44.30ms
step:1148/2160 train_time:50870ms step_avg:44.31ms
step:1149/2160 train_time:50932ms step_avg:44.33ms
step:1150/2160 train_time:50992ms step_avg:44.34ms
step:1151/2160 train_time:51054ms step_avg:44.36ms
step:1152/2160 train_time:51114ms step_avg:44.37ms
step:1153/2160 train_time:51177ms step_avg:44.39ms
step:1154/2160 train_time:51237ms step_avg:44.40ms
step:1155/2160 train_time:51300ms step_avg:44.42ms
step:1156/2160 train_time:51360ms step_avg:44.43ms
step:1157/2160 train_time:51423ms step_avg:44.45ms
step:1158/2160 train_time:51484ms step_avg:44.46ms
step:1159/2160 train_time:51546ms step_avg:44.47ms
step:1160/2160 train_time:51606ms step_avg:44.49ms
step:1161/2160 train_time:51667ms step_avg:44.50ms
step:1162/2160 train_time:51727ms step_avg:44.52ms
step:1163/2160 train_time:51788ms step_avg:44.53ms
step:1164/2160 train_time:51848ms step_avg:44.54ms
step:1165/2160 train_time:51909ms step_avg:44.56ms
step:1166/2160 train_time:51969ms step_avg:44.57ms
step:1167/2160 train_time:52031ms step_avg:44.58ms
step:1168/2160 train_time:52091ms step_avg:44.60ms
step:1169/2160 train_time:52153ms step_avg:44.61ms
step:1170/2160 train_time:52213ms step_avg:44.63ms
step:1171/2160 train_time:52276ms step_avg:44.64ms
step:1172/2160 train_time:52336ms step_avg:44.66ms
step:1173/2160 train_time:52399ms step_avg:44.67ms
step:1174/2160 train_time:52459ms step_avg:44.68ms
step:1175/2160 train_time:52522ms step_avg:44.70ms
step:1176/2160 train_time:52582ms step_avg:44.71ms
step:1177/2160 train_time:52644ms step_avg:44.73ms
step:1178/2160 train_time:52704ms step_avg:44.74ms
step:1179/2160 train_time:52767ms step_avg:44.76ms
step:1180/2160 train_time:52826ms step_avg:44.77ms
step:1181/2160 train_time:52888ms step_avg:44.78ms
step:1182/2160 train_time:52947ms step_avg:44.79ms
step:1183/2160 train_time:53008ms step_avg:44.81ms
step:1184/2160 train_time:53068ms step_avg:44.82ms
step:1185/2160 train_time:53130ms step_avg:44.84ms
step:1186/2160 train_time:53190ms step_avg:44.85ms
step:1187/2160 train_time:53252ms step_avg:44.86ms
step:1188/2160 train_time:53313ms step_avg:44.88ms
step:1189/2160 train_time:53375ms step_avg:44.89ms
step:1190/2160 train_time:53435ms step_avg:44.90ms
step:1191/2160 train_time:53497ms step_avg:44.92ms
step:1192/2160 train_time:53557ms step_avg:44.93ms
step:1193/2160 train_time:53620ms step_avg:44.95ms
step:1194/2160 train_time:53681ms step_avg:44.96ms
step:1195/2160 train_time:53743ms step_avg:44.97ms
step:1196/2160 train_time:53803ms step_avg:44.99ms
step:1197/2160 train_time:53866ms step_avg:45.00ms
step:1198/2160 train_time:53926ms step_avg:45.01ms
step:1199/2160 train_time:53988ms step_avg:45.03ms
step:1200/2160 train_time:54047ms step_avg:45.04ms
step:1201/2160 train_time:54109ms step_avg:45.05ms
step:1202/2160 train_time:54169ms step_avg:45.07ms
step:1203/2160 train_time:54231ms step_avg:45.08ms
step:1204/2160 train_time:54291ms step_avg:45.09ms
step:1205/2160 train_time:54354ms step_avg:45.11ms
step:1206/2160 train_time:54414ms step_avg:45.12ms
step:1207/2160 train_time:54475ms step_avg:45.13ms
step:1208/2160 train_time:54535ms step_avg:45.14ms
step:1209/2160 train_time:54597ms step_avg:45.16ms
step:1210/2160 train_time:54657ms step_avg:45.17ms
step:1211/2160 train_time:54720ms step_avg:45.19ms
step:1212/2160 train_time:54780ms step_avg:45.20ms
step:1213/2160 train_time:54843ms step_avg:45.21ms
step:1214/2160 train_time:54903ms step_avg:45.23ms
step:1215/2160 train_time:54966ms step_avg:45.24ms
step:1216/2160 train_time:55026ms step_avg:45.25ms
step:1217/2160 train_time:55088ms step_avg:45.27ms
step:1218/2160 train_time:55147ms step_avg:45.28ms
step:1219/2160 train_time:55209ms step_avg:45.29ms
step:1220/2160 train_time:55268ms step_avg:45.30ms
step:1221/2160 train_time:55330ms step_avg:45.32ms
step:1222/2160 train_time:55391ms step_avg:45.33ms
step:1223/2160 train_time:55453ms step_avg:45.34ms
step:1224/2160 train_time:55513ms step_avg:45.35ms
step:1225/2160 train_time:55575ms step_avg:45.37ms
step:1226/2160 train_time:55635ms step_avg:45.38ms
step:1227/2160 train_time:55696ms step_avg:45.39ms
step:1228/2160 train_time:55756ms step_avg:45.40ms
step:1229/2160 train_time:55819ms step_avg:45.42ms
step:1230/2160 train_time:55880ms step_avg:45.43ms
step:1231/2160 train_time:55942ms step_avg:45.44ms
step:1232/2160 train_time:56003ms step_avg:45.46ms
step:1233/2160 train_time:56065ms step_avg:45.47ms
step:1234/2160 train_time:56125ms step_avg:45.48ms
step:1235/2160 train_time:56187ms step_avg:45.50ms
step:1236/2160 train_time:56246ms step_avg:45.51ms
step:1237/2160 train_time:56308ms step_avg:45.52ms
step:1238/2160 train_time:56368ms step_avg:45.53ms
step:1239/2160 train_time:56429ms step_avg:45.54ms
step:1240/2160 train_time:56489ms step_avg:45.56ms
step:1241/2160 train_time:56551ms step_avg:45.57ms
step:1242/2160 train_time:56611ms step_avg:45.58ms
step:1243/2160 train_time:56673ms step_avg:45.59ms
step:1244/2160 train_time:56733ms step_avg:45.61ms
step:1245/2160 train_time:56795ms step_avg:45.62ms
step:1246/2160 train_time:56855ms step_avg:45.63ms
step:1247/2160 train_time:56917ms step_avg:45.64ms
step:1248/2160 train_time:56978ms step_avg:45.66ms
step:1249/2160 train_time:57040ms step_avg:45.67ms
step:1250/2160 train_time:57101ms step_avg:45.68ms
step:1250/2160 val_loss:3.5704 train_time:57164ms step_avg:45.73ms
step:1251/2160 train_time:57186ms step_avg:45.71ms
step:1252/2160 train_time:57229ms step_avg:45.71ms
step:1253/2160 train_time:57292ms step_avg:45.72ms
step:1254/2160 train_time:57355ms step_avg:45.74ms
step:1255/2160 train_time:57418ms step_avg:45.75ms
step:1256/2160 train_time:57478ms step_avg:45.76ms
step:1257/2160 train_time:57539ms step_avg:45.77ms
step:1258/2160 train_time:57598ms step_avg:45.79ms
step:1259/2160 train_time:57659ms step_avg:45.80ms
step:1260/2160 train_time:57719ms step_avg:45.81ms
step:1261/2160 train_time:57781ms step_avg:45.82ms
step:1262/2160 train_time:57840ms step_avg:45.83ms
step:1263/2160 train_time:57902ms step_avg:45.84ms
step:1264/2160 train_time:57961ms step_avg:45.86ms
step:1265/2160 train_time:58022ms step_avg:45.87ms
step:1266/2160 train_time:58082ms step_avg:45.88ms
step:1267/2160 train_time:58145ms step_avg:45.89ms
step:1268/2160 train_time:58206ms step_avg:45.90ms
step:1269/2160 train_time:58270ms step_avg:45.92ms
step:1270/2160 train_time:58330ms step_avg:45.93ms
step:1271/2160 train_time:58393ms step_avg:45.94ms
step:1272/2160 train_time:58454ms step_avg:45.95ms
step:1273/2160 train_time:58516ms step_avg:45.97ms
step:1274/2160 train_time:58576ms step_avg:45.98ms
step:1275/2160 train_time:58638ms step_avg:45.99ms
step:1276/2160 train_time:58699ms step_avg:46.00ms
step:1277/2160 train_time:58761ms step_avg:46.01ms
step:1278/2160 train_time:58820ms step_avg:46.02ms
step:1279/2160 train_time:58881ms step_avg:46.04ms
step:1280/2160 train_time:58941ms step_avg:46.05ms
step:1281/2160 train_time:59001ms step_avg:46.06ms
step:1282/2160 train_time:59061ms step_avg:46.07ms
step:1283/2160 train_time:59123ms step_avg:46.08ms
step:1284/2160 train_time:59183ms step_avg:46.09ms
step:1285/2160 train_time:59246ms step_avg:46.11ms
step:1286/2160 train_time:59306ms step_avg:46.12ms
step:1287/2160 train_time:59369ms step_avg:46.13ms
step:1288/2160 train_time:59429ms step_avg:46.14ms
step:1289/2160 train_time:59491ms step_avg:46.15ms
step:1290/2160 train_time:59551ms step_avg:46.16ms
step:1291/2160 train_time:59613ms step_avg:46.18ms
step:1292/2160 train_time:59673ms step_avg:46.19ms
step:1293/2160 train_time:59735ms step_avg:46.20ms
step:1294/2160 train_time:59795ms step_avg:46.21ms
step:1295/2160 train_time:59857ms step_avg:46.22ms
step:1296/2160 train_time:59916ms step_avg:46.23ms
step:1297/2160 train_time:59978ms step_avg:46.24ms
step:1298/2160 train_time:60038ms step_avg:46.25ms
step:1299/2160 train_time:60101ms step_avg:46.27ms
step:1300/2160 train_time:60162ms step_avg:46.28ms
step:1301/2160 train_time:60224ms step_avg:46.29ms
step:1302/2160 train_time:60284ms step_avg:46.30ms
step:1303/2160 train_time:60346ms step_avg:46.31ms
step:1304/2160 train_time:60406ms step_avg:46.32ms
step:1305/2160 train_time:60469ms step_avg:46.34ms
step:1306/2160 train_time:60529ms step_avg:46.35ms
step:1307/2160 train_time:60591ms step_avg:46.36ms
step:1308/2160 train_time:60650ms step_avg:46.37ms
step:1309/2160 train_time:60712ms step_avg:46.38ms
step:1310/2160 train_time:60772ms step_avg:46.39ms
step:1311/2160 train_time:60834ms step_avg:46.40ms
step:1312/2160 train_time:60894ms step_avg:46.41ms
step:1313/2160 train_time:60956ms step_avg:46.42ms
step:1314/2160 train_time:61016ms step_avg:46.44ms
step:1315/2160 train_time:61078ms step_avg:46.45ms
step:1316/2160 train_time:61139ms step_avg:46.46ms
step:1317/2160 train_time:61202ms step_avg:46.47ms
step:1318/2160 train_time:61262ms step_avg:46.48ms
step:1319/2160 train_time:61325ms step_avg:46.49ms
step:1320/2160 train_time:61384ms step_avg:46.50ms
step:1321/2160 train_time:61446ms step_avg:46.51ms
step:1322/2160 train_time:61506ms step_avg:46.52ms
step:1323/2160 train_time:61568ms step_avg:46.54ms
step:1324/2160 train_time:61628ms step_avg:46.55ms
step:1325/2160 train_time:61690ms step_avg:46.56ms
step:1326/2160 train_time:61749ms step_avg:46.57ms
step:1327/2160 train_time:61812ms step_avg:46.58ms
step:1328/2160 train_time:61871ms step_avg:46.59ms
step:1329/2160 train_time:61933ms step_avg:46.60ms
step:1330/2160 train_time:61993ms step_avg:46.61ms
step:1331/2160 train_time:62056ms step_avg:46.62ms
step:1332/2160 train_time:62116ms step_avg:46.63ms
step:1333/2160 train_time:62180ms step_avg:46.65ms
step:1334/2160 train_time:62241ms step_avg:46.66ms
step:1335/2160 train_time:62303ms step_avg:46.67ms
step:1336/2160 train_time:62363ms step_avg:46.68ms
step:1337/2160 train_time:62425ms step_avg:46.69ms
step:1338/2160 train_time:62485ms step_avg:46.70ms
step:1339/2160 train_time:62546ms step_avg:46.71ms
step:1340/2160 train_time:62607ms step_avg:46.72ms
step:1341/2160 train_time:62668ms step_avg:46.73ms
step:1342/2160 train_time:62728ms step_avg:46.74ms
step:1343/2160 train_time:62790ms step_avg:46.75ms
step:1344/2160 train_time:62849ms step_avg:46.76ms
step:1345/2160 train_time:62911ms step_avg:46.77ms
step:1346/2160 train_time:62971ms step_avg:46.78ms
step:1347/2160 train_time:63033ms step_avg:46.80ms
step:1348/2160 train_time:63094ms step_avg:46.81ms
step:1349/2160 train_time:63157ms step_avg:46.82ms
step:1350/2160 train_time:63218ms step_avg:46.83ms
step:1351/2160 train_time:63281ms step_avg:46.84ms
step:1352/2160 train_time:63341ms step_avg:46.85ms
step:1353/2160 train_time:63404ms step_avg:46.86ms
step:1354/2160 train_time:63463ms step_avg:46.87ms
step:1355/2160 train_time:63525ms step_avg:46.88ms
step:1356/2160 train_time:63584ms step_avg:46.89ms
step:1357/2160 train_time:63646ms step_avg:46.90ms
step:1358/2160 train_time:63706ms step_avg:46.91ms
step:1359/2160 train_time:63768ms step_avg:46.92ms
step:1360/2160 train_time:63828ms step_avg:46.93ms
step:1361/2160 train_time:63890ms step_avg:46.94ms
step:1362/2160 train_time:63949ms step_avg:46.95ms
step:1363/2160 train_time:64012ms step_avg:46.96ms
step:1364/2160 train_time:64072ms step_avg:46.97ms
step:1365/2160 train_time:64134ms step_avg:46.98ms
step:1366/2160 train_time:64194ms step_avg:46.99ms
step:1367/2160 train_time:64257ms step_avg:47.01ms
step:1368/2160 train_time:64318ms step_avg:47.02ms
step:1369/2160 train_time:64380ms step_avg:47.03ms
step:1370/2160 train_time:64441ms step_avg:47.04ms
step:1371/2160 train_time:64503ms step_avg:47.05ms
step:1372/2160 train_time:64563ms step_avg:47.06ms
step:1373/2160 train_time:64625ms step_avg:47.07ms
step:1374/2160 train_time:64684ms step_avg:47.08ms
step:1375/2160 train_time:64746ms step_avg:47.09ms
step:1376/2160 train_time:64805ms step_avg:47.10ms
step:1377/2160 train_time:64867ms step_avg:47.11ms
step:1378/2160 train_time:64927ms step_avg:47.12ms
step:1379/2160 train_time:64989ms step_avg:47.13ms
step:1380/2160 train_time:65049ms step_avg:47.14ms
step:1381/2160 train_time:65111ms step_avg:47.15ms
step:1382/2160 train_time:65171ms step_avg:47.16ms
step:1383/2160 train_time:65233ms step_avg:47.17ms
step:1384/2160 train_time:65294ms step_avg:47.18ms
step:1385/2160 train_time:65357ms step_avg:47.19ms
step:1386/2160 train_time:65418ms step_avg:47.20ms
step:1387/2160 train_time:65480ms step_avg:47.21ms
step:1388/2160 train_time:65540ms step_avg:47.22ms
step:1389/2160 train_time:65603ms step_avg:47.23ms
step:1390/2160 train_time:65663ms step_avg:47.24ms
step:1391/2160 train_time:65724ms step_avg:47.25ms
step:1392/2160 train_time:65784ms step_avg:47.26ms
step:1393/2160 train_time:65845ms step_avg:47.27ms
step:1394/2160 train_time:65905ms step_avg:47.28ms
step:1395/2160 train_time:65968ms step_avg:47.29ms
step:1396/2160 train_time:66028ms step_avg:47.30ms
step:1397/2160 train_time:66089ms step_avg:47.31ms
step:1398/2160 train_time:66149ms step_avg:47.32ms
step:1399/2160 train_time:66211ms step_avg:47.33ms
step:1400/2160 train_time:66272ms step_avg:47.34ms
step:1401/2160 train_time:66334ms step_avg:47.35ms
step:1402/2160 train_time:66395ms step_avg:47.36ms
step:1403/2160 train_time:66458ms step_avg:47.37ms
step:1404/2160 train_time:66519ms step_avg:47.38ms
step:1405/2160 train_time:66582ms step_avg:47.39ms
step:1406/2160 train_time:66642ms step_avg:47.40ms
step:1407/2160 train_time:66703ms step_avg:47.41ms
step:1408/2160 train_time:66763ms step_avg:47.42ms
step:1409/2160 train_time:66824ms step_avg:47.43ms
step:1410/2160 train_time:66884ms step_avg:47.44ms
step:1411/2160 train_time:66946ms step_avg:47.45ms
step:1412/2160 train_time:67006ms step_avg:47.45ms
step:1413/2160 train_time:67068ms step_avg:47.46ms
step:1414/2160 train_time:67128ms step_avg:47.47ms
step:1415/2160 train_time:67191ms step_avg:47.48ms
step:1416/2160 train_time:67280ms step_avg:47.51ms
step:1417/2160 train_time:67370ms step_avg:47.54ms
step:1418/2160 train_time:67458ms step_avg:47.57ms
step:1419/2160 train_time:67549ms step_avg:47.60ms
step:1420/2160 train_time:67637ms step_avg:47.63ms
step:1421/2160 train_time:67728ms step_avg:47.66ms
step:1422/2160 train_time:67817ms step_avg:47.69ms
step:1423/2160 train_time:67909ms step_avg:47.72ms
step:1424/2160 train_time:67996ms step_avg:47.75ms
step:1425/2160 train_time:68086ms step_avg:47.78ms
step:1426/2160 train_time:68173ms step_avg:47.81ms
step:1427/2160 train_time:68264ms step_avg:47.84ms
step:1428/2160 train_time:68352ms step_avg:47.87ms
step:1429/2160 train_time:68443ms step_avg:47.90ms
step:1430/2160 train_time:68531ms step_avg:47.92ms
step:1431/2160 train_time:68622ms step_avg:47.95ms
step:1432/2160 train_time:68711ms step_avg:47.98ms
step:1433/2160 train_time:68801ms step_avg:48.01ms
step:1434/2160 train_time:68890ms step_avg:48.04ms
step:1435/2160 train_time:68979ms step_avg:48.07ms
step:1436/2160 train_time:69067ms step_avg:48.10ms
step:1437/2160 train_time:69157ms step_avg:48.13ms
step:1438/2160 train_time:69245ms step_avg:48.15ms
step:1439/2160 train_time:69336ms step_avg:48.18ms
step:1440/2160 train_time:69424ms step_avg:48.21ms
step:1441/2160 train_time:69515ms step_avg:48.24ms
step:1442/2160 train_time:69604ms step_avg:48.27ms
step:1443/2160 train_time:69695ms step_avg:48.30ms
step:1444/2160 train_time:69783ms step_avg:48.33ms
step:1445/2160 train_time:69873ms step_avg:48.36ms
step:1446/2160 train_time:69961ms step_avg:48.38ms
step:1447/2160 train_time:70052ms step_avg:48.41ms
step:1448/2160 train_time:70139ms step_avg:48.44ms
step:1449/2160 train_time:70229ms step_avg:48.47ms
step:1450/2160 train_time:70317ms step_avg:48.49ms
step:1451/2160 train_time:70407ms step_avg:48.52ms
step:1452/2160 train_time:70495ms step_avg:48.55ms
step:1453/2160 train_time:70585ms step_avg:48.58ms
step:1454/2160 train_time:70673ms step_avg:48.61ms
step:1455/2160 train_time:70763ms step_avg:48.63ms
step:1456/2160 train_time:70852ms step_avg:48.66ms
step:1457/2160 train_time:70942ms step_avg:48.69ms
step:1458/2160 train_time:71030ms step_avg:48.72ms
step:1459/2160 train_time:71119ms step_avg:48.75ms
step:1460/2160 train_time:71208ms step_avg:48.77ms
step:1461/2160 train_time:71298ms step_avg:48.80ms
step:1462/2160 train_time:71385ms step_avg:48.83ms
step:1463/2160 train_time:71476ms step_avg:48.86ms
step:1464/2160 train_time:71564ms step_avg:48.88ms
step:1465/2160 train_time:71655ms step_avg:48.91ms
step:1466/2160 train_time:71742ms step_avg:48.94ms
step:1467/2160 train_time:71833ms step_avg:48.97ms
step:1468/2160 train_time:71921ms step_avg:48.99ms
step:1469/2160 train_time:72012ms step_avg:49.02ms
step:1470/2160 train_time:72101ms step_avg:49.05ms
step:1471/2160 train_time:72191ms step_avg:49.08ms
step:1472/2160 train_time:72278ms step_avg:49.10ms
step:1473/2160 train_time:72368ms step_avg:49.13ms
step:1474/2160 train_time:72455ms step_avg:49.16ms
step:1475/2160 train_time:72546ms step_avg:49.18ms
step:1476/2160 train_time:72633ms step_avg:49.21ms
step:1477/2160 train_time:72723ms step_avg:49.24ms
step:1478/2160 train_time:72813ms step_avg:49.26ms
step:1479/2160 train_time:72903ms step_avg:49.29ms
step:1480/2160 train_time:72992ms step_avg:49.32ms
step:1481/2160 train_time:73081ms step_avg:49.35ms
step:1482/2160 train_time:73170ms step_avg:49.37ms
step:1483/2160 train_time:73260ms step_avg:49.40ms
step:1484/2160 train_time:73348ms step_avg:49.43ms
step:1485/2160 train_time:73438ms step_avg:49.45ms
step:1486/2160 train_time:73527ms step_avg:49.48ms
step:1487/2160 train_time:73618ms step_avg:49.51ms
step:1488/2160 train_time:73706ms step_avg:49.53ms
step:1489/2160 train_time:73796ms step_avg:49.56ms
step:1490/2160 train_time:73885ms step_avg:49.59ms
step:1491/2160 train_time:73975ms step_avg:49.61ms
step:1492/2160 train_time:74063ms step_avg:49.64ms
step:1493/2160 train_time:74154ms step_avg:49.67ms
step:1494/2160 train_time:74242ms step_avg:49.69ms
step:1495/2160 train_time:74333ms step_avg:49.72ms
step:1496/2160 train_time:74421ms step_avg:49.75ms
step:1497/2160 train_time:74512ms step_avg:49.77ms
step:1498/2160 train_time:74599ms step_avg:49.80ms
step:1499/2160 train_time:74689ms step_avg:49.83ms
step:1500/2160 train_time:74777ms step_avg:49.85ms
step:1500/2160 val_loss:3.4694 train_time:74868ms step_avg:49.91ms
step:1501/2160 train_time:74891ms step_avg:49.89ms
step:1502/2160 train_time:74955ms step_avg:49.90ms
step:1503/2160 train_time:75051ms step_avg:49.93ms
step:1504/2160 train_time:75138ms step_avg:49.96ms
step:1505/2160 train_time:75226ms step_avg:49.98ms
step:1506/2160 train_time:75312ms step_avg:50.01ms
step:1507/2160 train_time:75400ms step_avg:50.03ms
step:1508/2160 train_time:75486ms step_avg:50.06ms
step:1509/2160 train_time:75574ms step_avg:50.08ms
step:1510/2160 train_time:75660ms step_avg:50.11ms
step:1511/2160 train_time:75751ms step_avg:50.13ms
step:1512/2160 train_time:75846ms step_avg:50.16ms
step:1513/2160 train_time:75938ms step_avg:50.19ms
step:1514/2160 train_time:76026ms step_avg:50.22ms
step:1515/2160 train_time:76115ms step_avg:50.24ms
step:1516/2160 train_time:76202ms step_avg:50.27ms
step:1517/2160 train_time:76290ms step_avg:50.29ms
step:1518/2160 train_time:76376ms step_avg:50.31ms
step:1519/2160 train_time:76465ms step_avg:50.34ms
step:1520/2160 train_time:76551ms step_avg:50.36ms
step:1521/2160 train_time:76639ms step_avg:50.39ms
step:1522/2160 train_time:76728ms step_avg:50.41ms
step:1523/2160 train_time:76819ms step_avg:50.44ms
step:1524/2160 train_time:76908ms step_avg:50.46ms
step:1525/2160 train_time:76998ms step_avg:50.49ms
step:1526/2160 train_time:77086ms step_avg:50.51ms
step:1527/2160 train_time:77175ms step_avg:50.54ms
step:1528/2160 train_time:77262ms step_avg:50.56ms
step:1529/2160 train_time:77350ms step_avg:50.59ms
step:1530/2160 train_time:77437ms step_avg:50.61ms
step:1531/2160 train_time:77527ms step_avg:50.64ms
step:1532/2160 train_time:77613ms step_avg:50.66ms
step:1533/2160 train_time:77703ms step_avg:50.69ms
step:1534/2160 train_time:77790ms step_avg:50.71ms
step:1535/2160 train_time:77880ms step_avg:50.74ms
step:1536/2160 train_time:77968ms step_avg:50.76ms
step:1537/2160 train_time:78058ms step_avg:50.79ms
step:1538/2160 train_time:78146ms step_avg:50.81ms
step:1539/2160 train_time:78235ms step_avg:50.84ms
step:1540/2160 train_time:78322ms step_avg:50.86ms
step:1541/2160 train_time:78411ms step_avg:50.88ms
step:1542/2160 train_time:78499ms step_avg:50.91ms
step:1543/2160 train_time:78587ms step_avg:50.93ms
step:1544/2160 train_time:78676ms step_avg:50.96ms
step:1545/2160 train_time:78766ms step_avg:50.98ms
step:1546/2160 train_time:78854ms step_avg:51.01ms
step:1547/2160 train_time:78944ms step_avg:51.03ms
step:1548/2160 train_time:79031ms step_avg:51.05ms
step:1549/2160 train_time:79121ms step_avg:51.08ms
step:1550/2160 train_time:79208ms step_avg:51.10ms
step:1551/2160 train_time:79297ms step_avg:51.13ms
step:1552/2160 train_time:79384ms step_avg:51.15ms
step:1553/2160 train_time:79473ms step_avg:51.17ms
step:1554/2160 train_time:79561ms step_avg:51.20ms
step:1555/2160 train_time:79650ms step_avg:51.22ms
step:1556/2160 train_time:79738ms step_avg:51.25ms
step:1557/2160 train_time:79828ms step_avg:51.27ms
step:1558/2160 train_time:79916ms step_avg:51.29ms
step:1559/2160 train_time:80007ms step_avg:51.32ms
step:1560/2160 train_time:80096ms step_avg:51.34ms
step:1561/2160 train_time:80186ms step_avg:51.37ms
step:1562/2160 train_time:80274ms step_avg:51.39ms
step:1563/2160 train_time:80363ms step_avg:51.42ms
step:1564/2160 train_time:80450ms step_avg:51.44ms
step:1565/2160 train_time:80539ms step_avg:51.46ms
step:1566/2160 train_time:80626ms step_avg:51.49ms
step:1567/2160 train_time:80716ms step_avg:51.51ms
step:1568/2160 train_time:80803ms step_avg:51.53ms
step:1569/2160 train_time:80893ms step_avg:51.56ms
step:1570/2160 train_time:80980ms step_avg:51.58ms
step:1571/2160 train_time:81071ms step_avg:51.60ms
step:1572/2160 train_time:81158ms step_avg:51.63ms
step:1573/2160 train_time:81248ms step_avg:51.65ms
step:1574/2160 train_time:81335ms step_avg:51.67ms
step:1575/2160 train_time:81425ms step_avg:51.70ms
step:1576/2160 train_time:81511ms step_avg:51.72ms
step:1577/2160 train_time:81601ms step_avg:51.74ms
step:1578/2160 train_time:81688ms step_avg:51.77ms
step:1579/2160 train_time:81777ms step_avg:51.79ms
step:1580/2160 train_time:81865ms step_avg:51.81ms
step:1581/2160 train_time:81954ms step_avg:51.84ms
step:1582/2160 train_time:82042ms step_avg:51.86ms
step:1583/2160 train_time:82130ms step_avg:51.88ms
step:1584/2160 train_time:82218ms step_avg:51.91ms
step:1585/2160 train_time:82307ms step_avg:51.93ms
step:1586/2160 train_time:82395ms step_avg:51.95ms
step:1587/2160 train_time:82486ms step_avg:51.98ms
step:1588/2160 train_time:82574ms step_avg:52.00ms
step:1589/2160 train_time:82664ms step_avg:52.02ms
step:1590/2160 train_time:82751ms step_avg:52.04ms
step:1591/2160 train_time:82841ms step_avg:52.07ms
step:1592/2160 train_time:82928ms step_avg:52.09ms
step:1593/2160 train_time:83018ms step_avg:52.11ms
step:1594/2160 train_time:83105ms step_avg:52.14ms
step:1595/2160 train_time:83194ms step_avg:52.16ms
step:1596/2160 train_time:83282ms step_avg:52.18ms
step:1597/2160 train_time:83371ms step_avg:52.20ms
step:1598/2160 train_time:83459ms step_avg:52.23ms
step:1599/2160 train_time:83548ms step_avg:52.25ms
step:1600/2160 train_time:83636ms step_avg:52.27ms
step:1601/2160 train_time:83725ms step_avg:52.30ms
step:1602/2160 train_time:83812ms step_avg:52.32ms
step:1603/2160 train_time:83902ms step_avg:52.34ms
step:1604/2160 train_time:83990ms step_avg:52.36ms
step:1605/2160 train_time:84080ms step_avg:52.39ms
step:1606/2160 train_time:84167ms step_avg:52.41ms
step:1607/2160 train_time:84257ms step_avg:52.43ms
step:1608/2160 train_time:84344ms step_avg:52.45ms
step:1609/2160 train_time:84434ms step_avg:52.48ms
step:1610/2160 train_time:84522ms step_avg:52.50ms
step:1611/2160 train_time:84610ms step_avg:52.52ms
step:1612/2160 train_time:84697ms step_avg:52.54ms
step:1613/2160 train_time:84787ms step_avg:52.57ms
step:1614/2160 train_time:84875ms step_avg:52.59ms
step:1615/2160 train_time:84966ms step_avg:52.61ms
step:1616/2160 train_time:85053ms step_avg:52.63ms
step:1617/2160 train_time:85143ms step_avg:52.65ms
step:1618/2160 train_time:85230ms step_avg:52.68ms
step:1619/2160 train_time:85321ms step_avg:52.70ms
step:1620/2160 train_time:85408ms step_avg:52.72ms
step:1621/2160 train_time:85497ms step_avg:52.74ms
step:1622/2160 train_time:85585ms step_avg:52.76ms
step:1623/2160 train_time:85674ms step_avg:52.79ms
step:1624/2160 train_time:85763ms step_avg:52.81ms
step:1625/2160 train_time:85853ms step_avg:52.83ms
step:1626/2160 train_time:85940ms step_avg:52.85ms
step:1627/2160 train_time:86030ms step_avg:52.88ms
step:1628/2160 train_time:86117ms step_avg:52.90ms
step:1629/2160 train_time:86207ms step_avg:52.92ms
step:1630/2160 train_time:86295ms step_avg:52.94ms
step:1631/2160 train_time:86385ms step_avg:52.96ms
step:1632/2160 train_time:86472ms step_avg:52.99ms
step:1633/2160 train_time:86563ms step_avg:53.01ms
step:1634/2160 train_time:86649ms step_avg:53.03ms
step:1635/2160 train_time:86740ms step_avg:53.05ms
step:1636/2160 train_time:86827ms step_avg:53.07ms
step:1637/2160 train_time:86916ms step_avg:53.09ms
step:1638/2160 train_time:87004ms step_avg:53.12ms
step:1639/2160 train_time:87094ms step_avg:53.14ms
step:1640/2160 train_time:87182ms step_avg:53.16ms
step:1641/2160 train_time:87271ms step_avg:53.18ms
step:1642/2160 train_time:87359ms step_avg:53.20ms
step:1643/2160 train_time:87448ms step_avg:53.22ms
step:1644/2160 train_time:87536ms step_avg:53.25ms
step:1645/2160 train_time:87626ms step_avg:53.27ms
step:1646/2160 train_time:87714ms step_avg:53.29ms
step:1647/2160 train_time:87803ms step_avg:53.31ms
step:1648/2160 train_time:87891ms step_avg:53.33ms
step:1649/2160 train_time:87981ms step_avg:53.35ms
step:1650/2160 train_time:88069ms step_avg:53.37ms
step:1651/2160 train_time:88157ms step_avg:53.40ms
step:1652/2160 train_time:88245ms step_avg:53.42ms
step:1653/2160 train_time:88334ms step_avg:53.44ms
step:1654/2160 train_time:88421ms step_avg:53.46ms
step:1655/2160 train_time:88510ms step_avg:53.48ms
step:1656/2160 train_time:88599ms step_avg:53.50ms
step:1657/2160 train_time:88688ms step_avg:53.52ms
step:1658/2160 train_time:88776ms step_avg:53.54ms
step:1659/2160 train_time:88866ms step_avg:53.57ms
step:1660/2160 train_time:88954ms step_avg:53.59ms
step:1661/2160 train_time:89044ms step_avg:53.61ms
step:1662/2160 train_time:89132ms step_avg:53.63ms
step:1663/2160 train_time:89223ms step_avg:53.65ms
step:1664/2160 train_time:89310ms step_avg:53.67ms
step:1665/2160 train_time:89399ms step_avg:53.69ms
step:1666/2160 train_time:89486ms step_avg:53.71ms
step:1667/2160 train_time:89576ms step_avg:53.74ms
step:1668/2160 train_time:89665ms step_avg:53.76ms
step:1669/2160 train_time:89756ms step_avg:53.78ms
step:1670/2160 train_time:89843ms step_avg:53.80ms
step:1671/2160 train_time:89933ms step_avg:53.82ms
step:1672/2160 train_time:90020ms step_avg:53.84ms
step:1673/2160 train_time:90109ms step_avg:53.86ms
step:1674/2160 train_time:90198ms step_avg:53.88ms
step:1675/2160 train_time:90287ms step_avg:53.90ms
step:1676/2160 train_time:90375ms step_avg:53.92ms
step:1677/2160 train_time:90465ms step_avg:53.94ms
step:1678/2160 train_time:90552ms step_avg:53.96ms
step:1679/2160 train_time:90643ms step_avg:53.99ms
step:1680/2160 train_time:90730ms step_avg:54.01ms
step:1681/2160 train_time:90820ms step_avg:54.03ms
step:1682/2160 train_time:90907ms step_avg:54.05ms
step:1683/2160 train_time:90996ms step_avg:54.07ms
step:1684/2160 train_time:91084ms step_avg:54.09ms
step:1685/2160 train_time:91173ms step_avg:54.11ms
step:1686/2160 train_time:91262ms step_avg:54.13ms
step:1687/2160 train_time:91350ms step_avg:54.15ms
step:1688/2160 train_time:91438ms step_avg:54.17ms
step:1689/2160 train_time:91528ms step_avg:54.19ms
step:1690/2160 train_time:91616ms step_avg:54.21ms
step:1691/2160 train_time:91705ms step_avg:54.23ms
step:1692/2160 train_time:91793ms step_avg:54.25ms
step:1693/2160 train_time:91882ms step_avg:54.27ms
step:1694/2160 train_time:91969ms step_avg:54.29ms
step:1695/2160 train_time:92059ms step_avg:54.31ms
step:1696/2160 train_time:92146ms step_avg:54.33ms
step:1697/2160 train_time:92236ms step_avg:54.35ms
step:1698/2160 train_time:92324ms step_avg:54.37ms
step:1699/2160 train_time:92413ms step_avg:54.39ms
step:1700/2160 train_time:92500ms step_avg:54.41ms
step:1701/2160 train_time:92589ms step_avg:54.43ms
step:1702/2160 train_time:92678ms step_avg:54.45ms
step:1703/2160 train_time:92769ms step_avg:54.47ms
step:1704/2160 train_time:92858ms step_avg:54.49ms
step:1705/2160 train_time:92947ms step_avg:54.51ms
step:1706/2160 train_time:93035ms step_avg:54.53ms
step:1707/2160 train_time:93124ms step_avg:54.55ms
step:1708/2160 train_time:93212ms step_avg:54.57ms
step:1709/2160 train_time:93302ms step_avg:54.59ms
step:1710/2160 train_time:93389ms step_avg:54.61ms
step:1711/2160 train_time:93478ms step_avg:54.63ms
step:1712/2160 train_time:93566ms step_avg:54.65ms
step:1713/2160 train_time:93656ms step_avg:54.67ms
step:1714/2160 train_time:93744ms step_avg:54.69ms
step:1715/2160 train_time:93834ms step_avg:54.71ms
step:1716/2160 train_time:93921ms step_avg:54.73ms
step:1717/2160 train_time:94011ms step_avg:54.75ms
step:1718/2160 train_time:94099ms step_avg:54.77ms
step:1719/2160 train_time:94188ms step_avg:54.79ms
step:1720/2160 train_time:94275ms step_avg:54.81ms
step:1721/2160 train_time:94366ms step_avg:54.83ms
step:1722/2160 train_time:94453ms step_avg:54.85ms
step:1723/2160 train_time:94543ms step_avg:54.87ms
step:1724/2160 train_time:94630ms step_avg:54.89ms
step:1725/2160 train_time:94720ms step_avg:54.91ms
step:1726/2160 train_time:94807ms step_avg:54.93ms
step:1727/2160 train_time:94897ms step_avg:54.95ms
step:1728/2160 train_time:94984ms step_avg:54.97ms
step:1729/2160 train_time:95074ms step_avg:54.99ms
step:1730/2160 train_time:95161ms step_avg:55.01ms
step:1731/2160 train_time:95250ms step_avg:55.03ms
step:1732/2160 train_time:95338ms step_avg:55.04ms
step:1733/2160 train_time:95428ms step_avg:55.07ms
step:1734/2160 train_time:95515ms step_avg:55.08ms
step:1735/2160 train_time:95605ms step_avg:55.10ms
step:1736/2160 train_time:95693ms step_avg:55.12ms
step:1737/2160 train_time:95783ms step_avg:55.14ms
step:1738/2160 train_time:95871ms step_avg:55.16ms
step:1739/2160 train_time:95961ms step_avg:55.18ms
step:1740/2160 train_time:96048ms step_avg:55.20ms
step:1741/2160 train_time:96138ms step_avg:55.22ms
step:1742/2160 train_time:96227ms step_avg:55.24ms
step:1743/2160 train_time:96316ms step_avg:55.26ms
step:1744/2160 train_time:96403ms step_avg:55.28ms
step:1745/2160 train_time:96493ms step_avg:55.30ms
step:1746/2160 train_time:96581ms step_avg:55.32ms
step:1747/2160 train_time:96670ms step_avg:55.33ms
step:1748/2160 train_time:96759ms step_avg:55.35ms
step:1749/2160 train_time:96849ms step_avg:55.37ms
step:1750/2160 train_time:96936ms step_avg:55.39ms
step:1750/2160 val_loss:3.3789 train_time:97027ms step_avg:55.44ms
step:1751/2160 train_time:97049ms step_avg:55.43ms
step:1752/2160 train_time:97119ms step_avg:55.43ms
step:1753/2160 train_time:97212ms step_avg:55.45ms
step:1754/2160 train_time:97300ms step_avg:55.47ms
step:1755/2160 train_time:97389ms step_avg:55.49ms
step:1756/2160 train_time:97475ms step_avg:55.51ms
step:1757/2160 train_time:97563ms step_avg:55.53ms
step:1758/2160 train_time:97650ms step_avg:55.55ms
step:1759/2160 train_time:97738ms step_avg:55.56ms
step:1760/2160 train_time:97826ms step_avg:55.58ms
step:1761/2160 train_time:97916ms step_avg:55.60ms
step:1762/2160 train_time:98004ms step_avg:55.62ms
step:1763/2160 train_time:98097ms step_avg:55.64ms
step:1764/2160 train_time:98187ms step_avg:55.66ms
step:1765/2160 train_time:98277ms step_avg:55.68ms
step:1766/2160 train_time:98367ms step_avg:55.70ms
step:1767/2160 train_time:98456ms step_avg:55.72ms
step:1768/2160 train_time:98542ms step_avg:55.74ms
step:1769/2160 train_time:98630ms step_avg:55.75ms
step:1770/2160 train_time:98716ms step_avg:55.77ms
step:1771/2160 train_time:98805ms step_avg:55.79ms
step:1772/2160 train_time:98893ms step_avg:55.81ms
step:1773/2160 train_time:98982ms step_avg:55.83ms
step:1774/2160 train_time:99070ms step_avg:55.85ms
step:1775/2160 train_time:99161ms step_avg:55.87ms
step:1776/2160 train_time:99250ms step_avg:55.88ms
step:1777/2160 train_time:99340ms step_avg:55.90ms
step:1778/2160 train_time:99428ms step_avg:55.92ms
step:1779/2160 train_time:99517ms step_avg:55.94ms
step:1780/2160 train_time:99604ms step_avg:55.96ms
step:1781/2160 train_time:99693ms step_avg:55.98ms
step:1782/2160 train_time:99779ms step_avg:55.99ms
step:1783/2160 train_time:99868ms step_avg:56.01ms
step:1784/2160 train_time:99955ms step_avg:56.03ms
step:1785/2160 train_time:100046ms step_avg:56.05ms
step:1786/2160 train_time:100134ms step_avg:56.07ms
step:1787/2160 train_time:100225ms step_avg:56.09ms
step:1788/2160 train_time:100314ms step_avg:56.10ms
step:1789/2160 train_time:100403ms step_avg:56.12ms
step:1790/2160 train_time:100491ms step_avg:56.14ms
step:1791/2160 train_time:100580ms step_avg:56.16ms
step:1792/2160 train_time:100668ms step_avg:56.18ms
step:1793/2160 train_time:100757ms step_avg:56.19ms
step:1794/2160 train_time:100844ms step_avg:56.21ms
step:1795/2160 train_time:100934ms step_avg:56.23ms
step:1796/2160 train_time:101022ms step_avg:56.25ms
step:1797/2160 train_time:101112ms step_avg:56.27ms
step:1798/2160 train_time:101200ms step_avg:56.28ms
step:1799/2160 train_time:101290ms step_avg:56.30ms
step:1800/2160 train_time:101378ms step_avg:56.32ms
step:1801/2160 train_time:101468ms step_avg:56.34ms
step:1802/2160 train_time:101556ms step_avg:56.36ms
step:1803/2160 train_time:101644ms step_avg:56.38ms
step:1804/2160 train_time:101733ms step_avg:56.39ms
step:1805/2160 train_time:101821ms step_avg:56.41ms
step:1806/2160 train_time:101908ms step_avg:56.43ms
step:1807/2160 train_time:101998ms step_avg:56.45ms
step:1808/2160 train_time:102087ms step_avg:56.46ms
step:1809/2160 train_time:102177ms step_avg:56.48ms
step:1810/2160 train_time:102264ms step_avg:56.50ms
step:1811/2160 train_time:102354ms step_avg:56.52ms
step:1812/2160 train_time:102441ms step_avg:56.53ms
step:1813/2160 train_time:102531ms step_avg:56.55ms
step:1814/2160 train_time:102618ms step_avg:56.57ms
step:1815/2160 train_time:102707ms step_avg:56.59ms
step:1816/2160 train_time:102795ms step_avg:56.61ms
step:1817/2160 train_time:102884ms step_avg:56.62ms
step:1818/2160 train_time:102972ms step_avg:56.64ms
step:1819/2160 train_time:103062ms step_avg:56.66ms
step:1820/2160 train_time:103151ms step_avg:56.68ms
step:1821/2160 train_time:103241ms step_avg:56.69ms
step:1822/2160 train_time:103328ms step_avg:56.71ms
step:1823/2160 train_time:103418ms step_avg:56.73ms
step:1824/2160 train_time:103506ms step_avg:56.75ms
step:1825/2160 train_time:103596ms step_avg:56.76ms
step:1826/2160 train_time:103683ms step_avg:56.78ms
step:1827/2160 train_time:103773ms step_avg:56.80ms
step:1828/2160 train_time:103860ms step_avg:56.82ms
step:1829/2160 train_time:103950ms step_avg:56.83ms
step:1830/2160 train_time:104038ms step_avg:56.85ms
step:1831/2160 train_time:104128ms step_avg:56.87ms
step:1832/2160 train_time:104215ms step_avg:56.89ms
step:1833/2160 train_time:104305ms step_avg:56.90ms
step:1834/2160 train_time:104394ms step_avg:56.92ms
step:1835/2160 train_time:104484ms step_avg:56.94ms
step:1836/2160 train_time:104571ms step_avg:56.96ms
step:1837/2160 train_time:104661ms step_avg:56.97ms
step:1838/2160 train_time:104748ms step_avg:56.99ms
step:1839/2160 train_time:104839ms step_avg:57.01ms
step:1840/2160 train_time:104926ms step_avg:57.02ms
step:1841/2160 train_time:105015ms step_avg:57.04ms
step:1842/2160 train_time:105103ms step_avg:57.06ms
step:1843/2160 train_time:105193ms step_avg:57.08ms
step:1844/2160 train_time:105281ms step_avg:57.09ms
step:1845/2160 train_time:105371ms step_avg:57.11ms
step:1846/2160 train_time:105459ms step_avg:57.13ms
step:1847/2160 train_time:105548ms step_avg:57.15ms
step:1848/2160 train_time:105635ms step_avg:57.16ms
step:1849/2160 train_time:105725ms step_avg:57.18ms
step:1850/2160 train_time:105814ms step_avg:57.20ms
step:1851/2160 train_time:105903ms step_avg:57.21ms
step:1852/2160 train_time:105992ms step_avg:57.23ms
step:1853/2160 train_time:106080ms step_avg:57.25ms
step:1854/2160 train_time:106168ms step_avg:57.26ms
step:1855/2160 train_time:106258ms step_avg:57.28ms
step:1856/2160 train_time:106346ms step_avg:57.30ms
step:1857/2160 train_time:106437ms step_avg:57.32ms
step:1858/2160 train_time:106524ms step_avg:57.33ms
step:1859/2160 train_time:106613ms step_avg:57.35ms
step:1860/2160 train_time:106700ms step_avg:57.37ms
step:1861/2160 train_time:106790ms step_avg:57.38ms
step:1862/2160 train_time:106877ms step_avg:57.40ms
step:1863/2160 train_time:106967ms step_avg:57.42ms
step:1864/2160 train_time:107054ms step_avg:57.43ms
step:1865/2160 train_time:107143ms step_avg:57.45ms
step:1866/2160 train_time:107231ms step_avg:57.47ms
step:1867/2160 train_time:107321ms step_avg:57.48ms
step:1868/2160 train_time:107409ms step_avg:57.50ms
step:1869/2160 train_time:107499ms step_avg:57.52ms
step:1870/2160 train_time:107587ms step_avg:57.53ms
step:1871/2160 train_time:107677ms step_avg:57.55ms
step:1872/2160 train_time:107764ms step_avg:57.57ms
step:1873/2160 train_time:107855ms step_avg:57.58ms
step:1874/2160 train_time:107942ms step_avg:57.60ms
step:1875/2160 train_time:108032ms step_avg:57.62ms
step:1876/2160 train_time:108119ms step_avg:57.63ms
step:1877/2160 train_time:108208ms step_avg:57.65ms
step:1878/2160 train_time:108296ms step_avg:57.67ms
step:1879/2160 train_time:108386ms step_avg:57.68ms
step:1880/2160 train_time:108473ms step_avg:57.70ms
step:1881/2160 train_time:108563ms step_avg:57.72ms
step:1882/2160 train_time:108651ms step_avg:57.73ms
step:1883/2160 train_time:108740ms step_avg:57.75ms
step:1884/2160 train_time:108829ms step_avg:57.76ms
step:1885/2160 train_time:108919ms step_avg:57.78ms
step:1886/2160 train_time:109006ms step_avg:57.80ms
step:1887/2160 train_time:109096ms step_avg:57.81ms
step:1888/2160 train_time:109183ms step_avg:57.83ms
step:1889/2160 train_time:109273ms step_avg:57.85ms
step:1890/2160 train_time:109361ms step_avg:57.86ms
step:1891/2160 train_time:109451ms step_avg:57.88ms
step:1892/2160 train_time:109538ms step_avg:57.90ms
step:1893/2160 train_time:109627ms step_avg:57.91ms
step:1894/2160 train_time:109716ms step_avg:57.93ms
step:1895/2160 train_time:109806ms step_avg:57.95ms
step:1896/2160 train_time:109895ms step_avg:57.96ms
step:1897/2160 train_time:109985ms step_avg:57.98ms
step:1898/2160 train_time:110072ms step_avg:57.99ms
step:1899/2160 train_time:110162ms step_avg:58.01ms
step:1900/2160 train_time:110250ms step_avg:58.03ms
step:1901/2160 train_time:110340ms step_avg:58.04ms
step:1902/2160 train_time:110427ms step_avg:58.06ms
step:1903/2160 train_time:110517ms step_avg:58.08ms
step:1904/2160 train_time:110604ms step_avg:58.09ms
step:1905/2160 train_time:110694ms step_avg:58.11ms
step:1906/2160 train_time:110782ms step_avg:58.12ms
step:1907/2160 train_time:110871ms step_avg:58.14ms
step:1908/2160 train_time:110959ms step_avg:58.15ms
step:1909/2160 train_time:111048ms step_avg:58.17ms
step:1910/2160 train_time:111136ms step_avg:58.19ms
step:1911/2160 train_time:111225ms step_avg:58.20ms
step:1912/2160 train_time:111313ms step_avg:58.22ms
step:1913/2160 train_time:111403ms step_avg:58.23ms
step:1914/2160 train_time:111491ms step_avg:58.25ms
step:1915/2160 train_time:111580ms step_avg:58.27ms
step:1916/2160 train_time:111667ms step_avg:58.28ms
step:1917/2160 train_time:111757ms step_avg:58.30ms
step:1918/2160 train_time:111844ms step_avg:58.31ms
step:1919/2160 train_time:111934ms step_avg:58.33ms
step:1920/2160 train_time:112022ms step_avg:58.34ms
step:1921/2160 train_time:112112ms step_avg:58.36ms
step:1922/2160 train_time:112200ms step_avg:58.38ms
step:1923/2160 train_time:112289ms step_avg:58.39ms
step:1924/2160 train_time:112377ms step_avg:58.41ms
step:1925/2160 train_time:112467ms step_avg:58.42ms
step:1926/2160 train_time:112556ms step_avg:58.44ms
step:1927/2160 train_time:112645ms step_avg:58.46ms
step:1928/2160 train_time:112733ms step_avg:58.47ms
step:1929/2160 train_time:112823ms step_avg:58.49ms
step:1930/2160 train_time:112910ms step_avg:58.50ms
step:1931/2160 train_time:113000ms step_avg:58.52ms
step:1932/2160 train_time:113087ms step_avg:58.53ms
step:1933/2160 train_time:113177ms step_avg:58.55ms
step:1934/2160 train_time:113265ms step_avg:58.57ms
step:1935/2160 train_time:113355ms step_avg:58.58ms
step:1936/2160 train_time:113443ms step_avg:58.60ms
step:1937/2160 train_time:113532ms step_avg:58.61ms
step:1938/2160 train_time:113618ms step_avg:58.63ms
step:1939/2160 train_time:113709ms step_avg:58.64ms
step:1940/2160 train_time:113796ms step_avg:58.66ms
step:1941/2160 train_time:113886ms step_avg:58.67ms
step:1942/2160 train_time:113973ms step_avg:58.69ms
step:1943/2160 train_time:114063ms step_avg:58.70ms
step:1944/2160 train_time:114151ms step_avg:58.72ms
step:1945/2160 train_time:114240ms step_avg:58.74ms
step:1946/2160 train_time:114328ms step_avg:58.75ms
step:1947/2160 train_time:114418ms step_avg:58.77ms
step:1948/2160 train_time:114506ms step_avg:58.78ms
step:1949/2160 train_time:114596ms step_avg:58.80ms
step:1950/2160 train_time:114684ms step_avg:58.81ms
step:1951/2160 train_time:114774ms step_avg:58.83ms
step:1952/2160 train_time:114861ms step_avg:58.84ms
step:1953/2160 train_time:114951ms step_avg:58.86ms
step:1954/2160 train_time:115039ms step_avg:58.87ms
step:1955/2160 train_time:115128ms step_avg:58.89ms
step:1956/2160 train_time:115216ms step_avg:58.90ms
step:1957/2160 train_time:115305ms step_avg:58.92ms
step:1958/2160 train_time:115394ms step_avg:58.93ms
step:1959/2160 train_time:115483ms step_avg:58.95ms
step:1960/2160 train_time:115572ms step_avg:58.97ms
step:1961/2160 train_time:115661ms step_avg:58.98ms
step:1962/2160 train_time:115749ms step_avg:59.00ms
step:1963/2160 train_time:115839ms step_avg:59.01ms
step:1964/2160 train_time:115928ms step_avg:59.03ms
step:1965/2160 train_time:116018ms step_avg:59.04ms
step:1966/2160 train_time:116106ms step_avg:59.06ms
step:1967/2160 train_time:116197ms step_avg:59.07ms
step:1968/2160 train_time:116284ms step_avg:59.09ms
step:1969/2160 train_time:116373ms step_avg:59.10ms
step:1970/2160 train_time:116460ms step_avg:59.12ms
step:1971/2160 train_time:116550ms step_avg:59.13ms
step:1972/2160 train_time:116638ms step_avg:59.15ms
step:1973/2160 train_time:116727ms step_avg:59.16ms
step:1974/2160 train_time:116816ms step_avg:59.18ms
step:1975/2160 train_time:116905ms step_avg:59.19ms
step:1976/2160 train_time:116994ms step_avg:59.21ms
step:1977/2160 train_time:117082ms step_avg:59.22ms
step:1978/2160 train_time:117170ms step_avg:59.24ms
step:1979/2160 train_time:117259ms step_avg:59.25ms
step:1980/2160 train_time:117347ms step_avg:59.27ms
step:1981/2160 train_time:117437ms step_avg:59.28ms
step:1982/2160 train_time:117526ms step_avg:59.30ms
step:1983/2160 train_time:117616ms step_avg:59.31ms
step:1984/2160 train_time:117704ms step_avg:59.33ms
step:1985/2160 train_time:117794ms step_avg:59.34ms
step:1986/2160 train_time:117882ms step_avg:59.36ms
step:1987/2160 train_time:117971ms step_avg:59.37ms
step:1988/2160 train_time:118059ms step_avg:59.39ms
step:1989/2160 train_time:118149ms step_avg:59.40ms
step:1990/2160 train_time:118236ms step_avg:59.42ms
step:1991/2160 train_time:118326ms step_avg:59.43ms
step:1992/2160 train_time:118413ms step_avg:59.44ms
step:1993/2160 train_time:118503ms step_avg:59.46ms
step:1994/2160 train_time:118591ms step_avg:59.47ms
step:1995/2160 train_time:118681ms step_avg:59.49ms
step:1996/2160 train_time:118769ms step_avg:59.50ms
step:1997/2160 train_time:118859ms step_avg:59.52ms
step:1998/2160 train_time:118947ms step_avg:59.53ms
step:1999/2160 train_time:119038ms step_avg:59.55ms
step:2000/2160 train_time:119125ms step_avg:59.56ms
step:2000/2160 val_loss:3.3103 train_time:119216ms step_avg:59.61ms
step:2001/2160 train_time:119238ms step_avg:59.59ms
step:2002/2160 train_time:119308ms step_avg:59.59ms
step:2003/2160 train_time:119401ms step_avg:59.61ms
step:2004/2160 train_time:119489ms step_avg:59.63ms
step:2005/2160 train_time:119578ms step_avg:59.64ms
step:2006/2160 train_time:119665ms step_avg:59.65ms
step:2007/2160 train_time:119753ms step_avg:59.67ms
step:2008/2160 train_time:119840ms step_avg:59.68ms
step:2009/2160 train_time:119928ms step_avg:59.70ms
step:2010/2160 train_time:120014ms step_avg:59.71ms
step:2011/2160 train_time:120103ms step_avg:59.72ms
step:2012/2160 train_time:120193ms step_avg:59.74ms
step:2013/2160 train_time:120284ms step_avg:59.75ms
step:2014/2160 train_time:120374ms step_avg:59.77ms
step:2015/2160 train_time:120464ms step_avg:59.78ms
step:2016/2160 train_time:120552ms step_avg:59.80ms
step:2017/2160 train_time:120641ms step_avg:59.81ms
step:2018/2160 train_time:120729ms step_avg:59.83ms
step:2019/2160 train_time:120817ms step_avg:59.84ms
step:2020/2160 train_time:120904ms step_avg:59.85ms
step:2021/2160 train_time:120993ms step_avg:59.87ms
step:2022/2160 train_time:121080ms step_avg:59.88ms
step:2023/2160 train_time:121170ms step_avg:59.90ms
step:2024/2160 train_time:121259ms step_avg:59.91ms
step:2025/2160 train_time:121350ms step_avg:59.93ms
step:2026/2160 train_time:121439ms step_avg:59.94ms
step:2027/2160 train_time:121530ms step_avg:59.96ms
step:2028/2160 train_time:121617ms step_avg:59.97ms
step:2029/2160 train_time:121706ms step_avg:59.98ms
step:2030/2160 train_time:121793ms step_avg:60.00ms
step:2031/2160 train_time:121881ms step_avg:60.01ms
step:2032/2160 train_time:121968ms step_avg:60.02ms
step:2033/2160 train_time:122057ms step_avg:60.04ms
step:2034/2160 train_time:122146ms step_avg:60.05ms
step:2035/2160 train_time:122235ms step_avg:60.07ms
step:2036/2160 train_time:122325ms step_avg:60.08ms
step:2037/2160 train_time:122414ms step_avg:60.10ms
step:2038/2160 train_time:122503ms step_avg:60.11ms
step:2039/2160 train_time:122593ms step_avg:60.12ms
step:2040/2160 train_time:122681ms step_avg:60.14ms
step:2041/2160 train_time:122770ms step_avg:60.15ms
step:2042/2160 train_time:122857ms step_avg:60.17ms
step:2043/2160 train_time:122946ms step_avg:60.18ms
step:2044/2160 train_time:123033ms step_avg:60.19ms
step:2045/2160 train_time:123122ms step_avg:60.21ms
step:2046/2160 train_time:123210ms step_avg:60.22ms
step:2047/2160 train_time:123300ms step_avg:60.23ms
step:2048/2160 train_time:123388ms step_avg:60.25ms
step:2049/2160 train_time:123478ms step_avg:60.26ms
step:2050/2160 train_time:123567ms step_avg:60.28ms
step:2051/2160 train_time:123657ms step_avg:60.29ms
step:2052/2160 train_time:123746ms step_avg:60.30ms
step:2053/2160 train_time:123834ms step_avg:60.32ms
step:2054/2160 train_time:123923ms step_avg:60.33ms
step:2055/2160 train_time:124011ms step_avg:60.35ms
step:2056/2160 train_time:124099ms step_avg:60.36ms
step:2057/2160 train_time:124188ms step_avg:60.37ms
step:2058/2160 train_time:124276ms step_avg:60.39ms
step:2059/2160 train_time:124367ms step_avg:60.40ms
step:2060/2160 train_time:124455ms step_avg:60.42ms
step:2061/2160 train_time:124545ms step_avg:60.43ms
step:2062/2160 train_time:124632ms step_avg:60.44ms
step:2063/2160 train_time:124722ms step_avg:60.46ms
step:2064/2160 train_time:124810ms step_avg:60.47ms
step:2065/2160 train_time:124899ms step_avg:60.48ms
step:2066/2160 train_time:124987ms step_avg:60.50ms
step:2067/2160 train_time:125076ms step_avg:60.51ms
step:2068/2160 train_time:125164ms step_avg:60.52ms
step:2069/2160 train_time:125254ms step_avg:60.54ms
step:2070/2160 train_time:125342ms step_avg:60.55ms
step:2071/2160 train_time:125432ms step_avg:60.57ms
step:2072/2160 train_time:125520ms step_avg:60.58ms
step:2073/2160 train_time:125610ms step_avg:60.59ms
step:2074/2160 train_time:125698ms step_avg:60.61ms
step:2075/2160 train_time:125789ms step_avg:60.62ms
step:2076/2160 train_time:125876ms step_avg:60.63ms
step:2077/2160 train_time:125965ms step_avg:60.65ms
step:2078/2160 train_time:126052ms step_avg:60.66ms
step:2079/2160 train_time:126142ms step_avg:60.67ms
step:2080/2160 train_time:126230ms step_avg:60.69ms
step:2081/2160 train_time:126320ms step_avg:60.70ms
step:2082/2160 train_time:126408ms step_avg:60.71ms
step:2083/2160 train_time:126497ms step_avg:60.73ms
step:2084/2160 train_time:126586ms step_avg:60.74ms
step:2085/2160 train_time:126675ms step_avg:60.76ms
step:2086/2160 train_time:126762ms step_avg:60.77ms
step:2087/2160 train_time:126851ms step_avg:60.78ms
step:2088/2160 train_time:126939ms step_avg:60.79ms
step:2089/2160 train_time:127029ms step_avg:60.81ms
step:2090/2160 train_time:127117ms step_avg:60.82ms
step:2091/2160 train_time:127207ms step_avg:60.84ms
step:2092/2160 train_time:127295ms step_avg:60.85ms
step:2093/2160 train_time:127385ms step_avg:60.86ms
step:2094/2160 train_time:127472ms step_avg:60.87ms
step:2095/2160 train_time:127563ms step_avg:60.89ms
step:2096/2160 train_time:127650ms step_avg:60.90ms
step:2097/2160 train_time:127740ms step_avg:60.92ms
step:2098/2160 train_time:127828ms step_avg:60.93ms
step:2099/2160 train_time:127918ms step_avg:60.94ms
step:2100/2160 train_time:128006ms step_avg:60.96ms
step:2101/2160 train_time:128097ms step_avg:60.97ms
step:2102/2160 train_time:128185ms step_avg:60.98ms
step:2103/2160 train_time:128274ms step_avg:61.00ms
step:2104/2160 train_time:128362ms step_avg:61.01ms
step:2105/2160 train_time:128452ms step_avg:61.02ms
step:2106/2160 train_time:128541ms step_avg:61.04ms
step:2107/2160 train_time:128631ms step_avg:61.05ms
step:2108/2160 train_time:128719ms step_avg:61.06ms
step:2109/2160 train_time:128808ms step_avg:61.08ms
step:2110/2160 train_time:128896ms step_avg:61.09ms
step:2111/2160 train_time:128985ms step_avg:61.10ms
step:2112/2160 train_time:129073ms step_avg:61.11ms
step:2113/2160 train_time:129163ms step_avg:61.13ms
step:2114/2160 train_time:129250ms step_avg:61.14ms
step:2115/2160 train_time:129341ms step_avg:61.15ms
step:2116/2160 train_time:129428ms step_avg:61.17ms
step:2117/2160 train_time:129517ms step_avg:61.18ms
step:2118/2160 train_time:129605ms step_avg:61.19ms
step:2119/2160 train_time:129694ms step_avg:61.21ms
step:2120/2160 train_time:129782ms step_avg:61.22ms
step:2121/2160 train_time:129871ms step_avg:61.23ms
step:2122/2160 train_time:129959ms step_avg:61.24ms
step:2123/2160 train_time:130050ms step_avg:61.26ms
step:2124/2160 train_time:130138ms step_avg:61.27ms
step:2125/2160 train_time:130228ms step_avg:61.28ms
step:2126/2160 train_time:130316ms step_avg:61.30ms
step:2127/2160 train_time:130406ms step_avg:61.31ms
step:2128/2160 train_time:130494ms step_avg:61.32ms
step:2129/2160 train_time:130585ms step_avg:61.34ms
step:2130/2160 train_time:130673ms step_avg:61.35ms
step:2131/2160 train_time:130763ms step_avg:61.36ms
step:2132/2160 train_time:130850ms step_avg:61.37ms
step:2133/2160 train_time:130940ms step_avg:61.39ms
step:2134/2160 train_time:131027ms step_avg:61.40ms
step:2135/2160 train_time:131117ms step_avg:61.41ms
step:2136/2160 train_time:131205ms step_avg:61.43ms
step:2137/2160 train_time:131295ms step_avg:61.44ms
step:2138/2160 train_time:131383ms step_avg:61.45ms
step:2139/2160 train_time:131473ms step_avg:61.46ms
step:2140/2160 train_time:131560ms step_avg:61.48ms
step:2141/2160 train_time:131651ms step_avg:61.49ms
step:2142/2160 train_time:131738ms step_avg:61.50ms
step:2143/2160 train_time:131828ms step_avg:61.52ms
step:2144/2160 train_time:131916ms step_avg:61.53ms
step:2145/2160 train_time:132006ms step_avg:61.54ms
step:2146/2160 train_time:132093ms step_avg:61.55ms
step:2147/2160 train_time:132183ms step_avg:61.57ms
step:2148/2160 train_time:132271ms step_avg:61.58ms
step:2149/2160 train_time:132361ms step_avg:61.59ms
step:2150/2160 train_time:132449ms step_avg:61.60ms
step:2151/2160 train_time:132539ms step_avg:61.62ms
step:2152/2160 train_time:132627ms step_avg:61.63ms
step:2153/2160 train_time:132717ms step_avg:61.64ms
step:2154/2160 train_time:132804ms step_avg:61.65ms
step:2155/2160 train_time:132894ms step_avg:61.67ms
step:2156/2160 train_time:132982ms step_avg:61.68ms
step:2157/2160 train_time:133071ms step_avg:61.69ms
step:2158/2160 train_time:133159ms step_avg:61.70ms
step:2159/2160 train_time:133249ms step_avg:61.72ms
step:2160/2160 train_time:133336ms step_avg:61.73ms
step:2160/2160 val_loss:3.2782 train_time:133427ms step_avg:61.77ms
peak memory allocated: 29892 MiB reserved: 44836 MiB

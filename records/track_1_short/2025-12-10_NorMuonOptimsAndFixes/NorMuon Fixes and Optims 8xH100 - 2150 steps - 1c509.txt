import uuid
run_id = f"NorMuon Fixes and Optims 8xH100 - 2150 steps - {str(uuid.uuid4())[0:5]}"

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled Helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer
# -----------------------------------------------------------------------------

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal
        # Compiled helpers by @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # Corrected variance calculation for gates and attn output heads by @chrisjmccormick
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest, so it should be processed first.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

# If we're on a GH200, just use the existing flash attention installed.
# Otherwise, if we're on an H100, we'll use the official flash attention for the speedrun.
gpu_name = torch.cuda.get_device_properties(0).name
if "H100" in gpu_name:  # H100
    from kernels import get_kernel
    flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface
else:  # GH200 or other
    from flash_attn import flash_attn_interface


class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        #
        # Stacked layout stores all QKVO heads horizontally, allowing us to 
        # correctly calculate variance for output heads in NorMuon without
        # splitting off O. @chrisjmccormick  
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label module to enable explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        # label modules to enable explicit optimizer grouping
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # label module to enable explicit optimizer grouping
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # label module to enable explicit optimizer grouping
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2110  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = run_id #f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.7 (main, Dec 10 2025, 18:15:56) [GCC 13.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Dec 10 19:04:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   43C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   35C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   33C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   43C    P0            129W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   44C    P0            127W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   34C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   43C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   34C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           24526      C   /usr/local/bin/python                  1510MiB |
|    0   N/A  N/A           24527      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           24528      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           24529      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           24530      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           24531      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           24532      C   /usr/local/bin/python                   614MiB |
|    0   N/A  N/A           24533      C   /usr/local/bin/python                   614MiB |
|    1   N/A  N/A           24527      C   /usr/local/bin/python                  1510MiB |
|    2   N/A  N/A           24528      C   /usr/local/bin/python                  1510MiB |
|    3   N/A  N/A           24529      C   /usr/local/bin/python                  1510MiB |
|    4   N/A  N/A           24530      C   /usr/local/bin/python                  1510MiB |
|    5   N/A  N/A           24531      C   /usr/local/bin/python                  1510MiB |
|    6   N/A  N/A           24532      C   /usr/local/bin/python                  1510MiB |
|    7   N/A  N/A           24533      C   /usr/local/bin/python                  1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2150 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2150 train_time:88ms step_avg:87.93ms
step:2/2150 train_time:165ms step_avg:82.64ms
step:3/2150 train_time:188ms step_avg:62.73ms
step:4/2150 train_time:214ms step_avg:53.54ms
step:5/2150 train_time:247ms step_avg:49.37ms
step:6/2150 train_time:350ms step_avg:58.41ms
step:7/2150 train_time:375ms step_avg:53.54ms
step:8/2150 train_time:402ms step_avg:50.20ms
step:9/2150 train_time:427ms step_avg:47.40ms
step:10/2150 train_time:458ms step_avg:45.77ms
step:11/2150 train_time:490ms step_avg:44.57ms
step:12/2150 train_time:524ms step_avg:43.64ms
step:13/2150 train_time:556ms step_avg:42.80ms
step:14/2150 train_time:590ms step_avg:42.13ms
step:15/2150 train_time:623ms step_avg:41.51ms
step:16/2150 train_time:656ms step_avg:41.01ms
step:17/2150 train_time:689ms step_avg:40.53ms
step:18/2150 train_time:723ms step_avg:40.15ms
step:19/2150 train_time:755ms step_avg:39.76ms
step:20/2150 train_time:789ms step_avg:39.45ms
step:21/2150 train_time:822ms step_avg:39.13ms
step:22/2150 train_time:855ms step_avg:38.87ms
step:23/2150 train_time:888ms step_avg:38.60ms
step:24/2150 train_time:922ms step_avg:38.40ms
step:25/2150 train_time:954ms step_avg:38.17ms
step:26/2150 train_time:988ms step_avg:37.99ms
step:27/2150 train_time:1020ms step_avg:37.79ms
step:28/2150 train_time:1054ms step_avg:37.64ms
step:29/2150 train_time:1086ms step_avg:37.46ms
step:30/2150 train_time:1120ms step_avg:37.33ms
step:31/2150 train_time:1152ms step_avg:37.17ms
step:32/2150 train_time:1186ms step_avg:37.06ms
step:33/2150 train_time:1220ms step_avg:36.96ms
step:34/2150 train_time:1254ms step_avg:36.89ms
step:35/2150 train_time:1289ms step_avg:36.82ms
step:36/2150 train_time:1323ms step_avg:36.74ms
step:37/2150 train_time:1356ms step_avg:36.66ms
step:38/2150 train_time:1390ms step_avg:36.58ms
step:39/2150 train_time:1424ms step_avg:36.51ms
step:40/2150 train_time:1461ms step_avg:36.52ms
step:41/2150 train_time:1491ms step_avg:36.36ms
step:42/2150 train_time:1524ms step_avg:36.29ms
step:43/2150 train_time:1557ms step_avg:36.21ms
step:44/2150 train_time:1590ms step_avg:36.15ms
step:45/2150 train_time:1624ms step_avg:36.09ms
step:46/2150 train_time:1657ms step_avg:36.03ms
step:47/2150 train_time:1690ms step_avg:35.96ms
step:48/2150 train_time:1723ms step_avg:35.91ms
step:49/2150 train_time:1756ms step_avg:35.84ms
step:50/2150 train_time:1790ms step_avg:35.80ms
step:51/2150 train_time:1823ms step_avg:35.74ms
step:52/2150 train_time:1856ms step_avg:35.69ms
step:53/2150 train_time:1889ms step_avg:35.64ms
step:54/2150 train_time:1923ms step_avg:35.61ms
step:55/2150 train_time:1955ms step_avg:35.55ms
step:56/2150 train_time:1989ms step_avg:35.52ms
step:57/2150 train_time:2022ms step_avg:35.47ms
step:58/2150 train_time:2055ms step_avg:35.43ms
step:59/2150 train_time:2088ms step_avg:35.39ms
step:60/2150 train_time:2121ms step_avg:35.35ms
step:61/2150 train_time:2154ms step_avg:35.31ms
step:62/2150 train_time:2188ms step_avg:35.29ms
step:63/2150 train_time:2221ms step_avg:35.26ms
step:64/2150 train_time:2255ms step_avg:35.23ms
step:65/2150 train_time:2288ms step_avg:35.20ms
step:66/2150 train_time:2322ms step_avg:35.18ms
step:67/2150 train_time:2355ms step_avg:35.15ms
step:68/2150 train_time:2389ms step_avg:35.13ms
step:69/2150 train_time:2422ms step_avg:35.11ms
step:70/2150 train_time:2456ms step_avg:35.09ms
step:71/2150 train_time:2489ms step_avg:35.06ms
step:72/2150 train_time:2523ms step_avg:35.04ms
step:73/2150 train_time:2556ms step_avg:35.01ms
step:74/2150 train_time:2589ms step_avg:34.99ms
step:75/2150 train_time:2623ms step_avg:34.97ms
step:76/2150 train_time:2656ms step_avg:34.95ms
step:77/2150 train_time:2689ms step_avg:34.92ms
step:78/2150 train_time:2723ms step_avg:34.91ms
step:79/2150 train_time:2756ms step_avg:34.89ms
step:80/2150 train_time:2789ms step_avg:34.87ms
step:81/2150 train_time:2822ms step_avg:34.84ms
step:82/2150 train_time:2855ms step_avg:34.82ms
step:83/2150 train_time:2888ms step_avg:34.80ms
step:84/2150 train_time:2922ms step_avg:34.79ms
step:85/2150 train_time:2957ms step_avg:34.79ms
step:86/2150 train_time:2988ms step_avg:34.74ms
step:87/2150 train_time:3021ms step_avg:34.72ms
step:88/2150 train_time:3054ms step_avg:34.70ms
step:89/2150 train_time:3087ms step_avg:34.68ms
step:90/2150 train_time:3120ms step_avg:34.67ms
step:91/2150 train_time:3153ms step_avg:34.65ms
step:92/2150 train_time:3186ms step_avg:34.64ms
step:93/2150 train_time:3219ms step_avg:34.61ms
step:94/2150 train_time:3253ms step_avg:34.60ms
step:95/2150 train_time:3286ms step_avg:34.58ms
step:96/2150 train_time:3319ms step_avg:34.57ms
step:97/2150 train_time:3353ms step_avg:34.56ms
step:98/2150 train_time:3386ms step_avg:34.55ms
step:99/2150 train_time:3420ms step_avg:34.54ms
step:100/2150 train_time:3453ms step_avg:34.53ms
step:101/2150 train_time:3486ms step_avg:34.52ms
step:102/2150 train_time:3519ms step_avg:34.50ms
step:103/2150 train_time:3552ms step_avg:34.49ms
step:104/2150 train_time:3586ms step_avg:34.48ms
step:105/2150 train_time:3618ms step_avg:34.46ms
step:106/2150 train_time:3652ms step_avg:34.45ms
step:107/2150 train_time:3685ms step_avg:34.44ms
step:108/2150 train_time:3719ms step_avg:34.43ms
step:109/2150 train_time:3751ms step_avg:34.41ms
step:110/2150 train_time:3784ms step_avg:34.40ms
step:111/2150 train_time:3817ms step_avg:34.39ms
step:112/2150 train_time:3851ms step_avg:34.38ms
step:113/2150 train_time:3883ms step_avg:34.36ms
step:114/2150 train_time:3917ms step_avg:34.36ms
step:115/2150 train_time:3949ms step_avg:34.34ms
step:116/2150 train_time:3983ms step_avg:34.34ms
step:117/2150 train_time:4015ms step_avg:34.32ms
step:118/2150 train_time:4049ms step_avg:34.31ms
step:119/2150 train_time:4081ms step_avg:34.30ms
step:120/2150 train_time:4115ms step_avg:34.29ms
step:121/2150 train_time:4147ms step_avg:34.27ms
step:122/2150 train_time:4181ms step_avg:34.27ms
step:123/2150 train_time:4214ms step_avg:34.26ms
step:124/2150 train_time:4247ms step_avg:34.25ms
step:125/2150 train_time:4281ms step_avg:34.24ms
step:126/2150 train_time:4314ms step_avg:34.24ms
step:127/2150 train_time:4347ms step_avg:34.23ms
step:128/2150 train_time:4380ms step_avg:34.22ms
step:129/2150 train_time:4413ms step_avg:34.21ms
step:130/2150 train_time:4447ms step_avg:34.21ms
step:131/2150 train_time:4481ms step_avg:34.20ms
step:132/2150 train_time:4514ms step_avg:34.20ms
step:133/2150 train_time:4547ms step_avg:34.19ms
step:134/2150 train_time:4580ms step_avg:34.18ms
step:135/2150 train_time:4613ms step_avg:34.17ms
step:136/2150 train_time:4647ms step_avg:34.17ms
step:137/2150 train_time:4680ms step_avg:34.16ms
step:138/2150 train_time:4713ms step_avg:34.15ms
step:139/2150 train_time:4748ms step_avg:34.16ms
step:140/2150 train_time:4779ms step_avg:34.14ms
step:141/2150 train_time:4812ms step_avg:34.13ms
step:142/2150 train_time:4845ms step_avg:34.12ms
step:143/2150 train_time:4878ms step_avg:34.11ms
step:144/2150 train_time:4911ms step_avg:34.11ms
step:145/2150 train_time:4944ms step_avg:34.10ms
step:146/2150 train_time:4977ms step_avg:34.09ms
step:147/2150 train_time:5010ms step_avg:34.08ms
step:148/2150 train_time:5044ms step_avg:34.08ms
step:149/2150 train_time:5076ms step_avg:34.07ms
step:150/2150 train_time:5110ms step_avg:34.07ms
step:151/2150 train_time:5142ms step_avg:34.05ms
step:152/2150 train_time:5175ms step_avg:34.05ms
step:153/2150 train_time:5209ms step_avg:34.04ms
step:154/2150 train_time:5242ms step_avg:34.04ms
step:155/2150 train_time:5275ms step_avg:34.03ms
step:156/2150 train_time:5308ms step_avg:34.03ms
step:157/2150 train_time:5341ms step_avg:34.02ms
step:158/2150 train_time:5374ms step_avg:34.02ms
step:159/2150 train_time:5408ms step_avg:34.01ms
step:160/2150 train_time:5441ms step_avg:34.01ms
step:161/2150 train_time:5474ms step_avg:34.00ms
step:162/2150 train_time:5508ms step_avg:34.00ms
step:163/2150 train_time:5541ms step_avg:33.99ms
step:164/2150 train_time:5574ms step_avg:33.99ms
step:165/2150 train_time:5607ms step_avg:33.98ms
step:166/2150 train_time:5641ms step_avg:33.98ms
step:167/2150 train_time:5674ms step_avg:33.98ms
step:168/2150 train_time:5708ms step_avg:33.97ms
step:169/2150 train_time:5740ms step_avg:33.97ms
step:170/2150 train_time:5774ms step_avg:33.96ms
step:171/2150 train_time:5807ms step_avg:33.96ms
step:172/2150 train_time:5840ms step_avg:33.95ms
step:173/2150 train_time:5873ms step_avg:33.95ms
step:174/2150 train_time:5906ms step_avg:33.94ms
step:175/2150 train_time:5939ms step_avg:33.94ms
step:176/2150 train_time:5972ms step_avg:33.93ms
step:177/2150 train_time:6005ms step_avg:33.93ms
step:178/2150 train_time:6039ms step_avg:33.93ms
step:179/2150 train_time:6071ms step_avg:33.92ms
step:180/2150 train_time:6105ms step_avg:33.92ms
step:181/2150 train_time:6138ms step_avg:33.91ms
step:182/2150 train_time:6171ms step_avg:33.91ms
step:183/2150 train_time:6204ms step_avg:33.90ms
step:184/2150 train_time:6237ms step_avg:33.90ms
step:185/2150 train_time:6270ms step_avg:33.89ms
step:186/2150 train_time:6303ms step_avg:33.89ms
step:187/2150 train_time:6335ms step_avg:33.88ms
step:188/2150 train_time:6369ms step_avg:33.88ms
step:189/2150 train_time:6402ms step_avg:33.87ms
step:190/2150 train_time:6435ms step_avg:33.87ms
step:191/2150 train_time:6468ms step_avg:33.86ms
step:192/2150 train_time:6501ms step_avg:33.86ms
step:193/2150 train_time:6534ms step_avg:33.86ms
step:194/2150 train_time:6568ms step_avg:33.85ms
step:195/2150 train_time:6601ms step_avg:33.85ms
step:196/2150 train_time:6634ms step_avg:33.85ms
step:197/2150 train_time:6667ms step_avg:33.84ms
step:198/2150 train_time:6700ms step_avg:33.84ms
step:199/2150 train_time:6733ms step_avg:33.83ms
step:200/2150 train_time:6766ms step_avg:33.83ms
step:201/2150 train_time:6799ms step_avg:33.83ms
step:202/2150 train_time:6832ms step_avg:33.82ms
step:203/2150 train_time:6865ms step_avg:33.82ms
step:204/2150 train_time:6899ms step_avg:33.82ms
step:205/2150 train_time:6931ms step_avg:33.81ms
step:206/2150 train_time:6965ms step_avg:33.81ms
step:207/2150 train_time:6997ms step_avg:33.80ms
step:208/2150 train_time:7030ms step_avg:33.80ms
step:209/2150 train_time:7063ms step_avg:33.79ms
step:210/2150 train_time:7096ms step_avg:33.79ms
step:211/2150 train_time:7129ms step_avg:33.79ms
step:212/2150 train_time:7162ms step_avg:33.78ms
step:213/2150 train_time:7195ms step_avg:33.78ms
step:214/2150 train_time:7228ms step_avg:33.78ms
step:215/2150 train_time:7261ms step_avg:33.77ms
step:216/2150 train_time:7295ms step_avg:33.77ms
step:217/2150 train_time:7327ms step_avg:33.77ms
step:218/2150 train_time:7361ms step_avg:33.77ms
step:219/2150 train_time:7393ms step_avg:33.76ms
step:220/2150 train_time:7427ms step_avg:33.76ms
step:221/2150 train_time:7459ms step_avg:33.75ms
step:222/2150 train_time:7493ms step_avg:33.75ms
step:223/2150 train_time:7525ms step_avg:33.75ms
step:224/2150 train_time:7559ms step_avg:33.74ms
step:225/2150 train_time:7592ms step_avg:33.74ms
step:226/2150 train_time:7625ms step_avg:33.74ms
step:227/2150 train_time:7658ms step_avg:33.74ms
step:228/2150 train_time:7691ms step_avg:33.73ms
step:229/2150 train_time:7724ms step_avg:33.73ms
step:230/2150 train_time:7760ms step_avg:33.74ms
step:231/2150 train_time:7790ms step_avg:33.72ms
step:232/2150 train_time:7823ms step_avg:33.72ms
step:233/2150 train_time:7856ms step_avg:33.72ms
step:234/2150 train_time:7889ms step_avg:33.71ms
step:235/2150 train_time:7922ms step_avg:33.71ms
step:236/2150 train_time:7955ms step_avg:33.71ms
step:237/2150 train_time:7988ms step_avg:33.70ms
step:238/2150 train_time:8021ms step_avg:33.70ms
step:239/2150 train_time:8054ms step_avg:33.70ms
step:240/2150 train_time:8087ms step_avg:33.70ms
step:241/2150 train_time:8120ms step_avg:33.69ms
step:242/2150 train_time:8154ms step_avg:33.69ms
step:243/2150 train_time:8186ms step_avg:33.69ms
step:244/2150 train_time:8219ms step_avg:33.69ms
step:245/2150 train_time:8252ms step_avg:33.68ms
step:246/2150 train_time:8285ms step_avg:33.68ms
step:247/2150 train_time:8318ms step_avg:33.68ms
step:248/2150 train_time:8351ms step_avg:33.67ms
step:249/2150 train_time:8384ms step_avg:33.67ms
step:250/2150 train_time:8417ms step_avg:33.67ms
step:250/2150 val_loss:4.3053 train_time:8454ms step_avg:33.81ms
step:251/2150 train_time:8476ms step_avg:33.77ms
step:252/2150 train_time:8498ms step_avg:33.72ms
step:253/2150 train_time:8520ms step_avg:33.68ms
step:254/2150 train_time:8555ms step_avg:33.68ms
step:255/2150 train_time:8592ms step_avg:33.70ms
step:256/2150 train_time:8628ms step_avg:33.70ms
step:257/2150 train_time:8662ms step_avg:33.70ms
step:258/2150 train_time:8695ms step_avg:33.70ms
step:259/2150 train_time:8728ms step_avg:33.70ms
step:260/2150 train_time:8762ms step_avg:33.70ms
step:261/2150 train_time:8794ms step_avg:33.69ms
step:262/2150 train_time:8828ms step_avg:33.69ms
step:263/2150 train_time:8860ms step_avg:33.69ms
step:264/2150 train_time:8893ms step_avg:33.69ms
step:265/2150 train_time:8926ms step_avg:33.68ms
step:266/2150 train_time:8959ms step_avg:33.68ms
step:267/2150 train_time:8991ms step_avg:33.68ms
step:268/2150 train_time:9025ms step_avg:33.67ms
step:269/2150 train_time:9057ms step_avg:33.67ms
step:270/2150 train_time:9090ms step_avg:33.67ms
step:271/2150 train_time:9122ms step_avg:33.66ms
step:272/2150 train_time:9156ms step_avg:33.66ms
step:273/2150 train_time:9188ms step_avg:33.65ms
step:274/2150 train_time:9221ms step_avg:33.65ms
step:275/2150 train_time:9253ms step_avg:33.65ms
step:276/2150 train_time:9287ms step_avg:33.65ms
step:277/2150 train_time:9319ms step_avg:33.64ms
step:278/2150 train_time:9352ms step_avg:33.64ms
step:279/2150 train_time:9385ms step_avg:33.64ms
step:280/2150 train_time:9418ms step_avg:33.64ms
step:281/2150 train_time:9450ms step_avg:33.63ms
step:282/2150 train_time:9484ms step_avg:33.63ms
step:283/2150 train_time:9517ms step_avg:33.63ms
step:284/2150 train_time:9550ms step_avg:33.63ms
step:285/2150 train_time:9584ms step_avg:33.63ms
step:286/2150 train_time:9619ms step_avg:33.63ms
step:287/2150 train_time:9653ms step_avg:33.63ms
step:288/2150 train_time:9686ms step_avg:33.63ms
step:289/2150 train_time:9720ms step_avg:33.63ms
step:290/2150 train_time:9753ms step_avg:33.63ms
step:291/2150 train_time:9786ms step_avg:33.63ms
step:292/2150 train_time:9819ms step_avg:33.63ms
step:293/2150 train_time:9852ms step_avg:33.63ms
step:294/2150 train_time:9886ms step_avg:33.62ms
step:295/2150 train_time:9919ms step_avg:33.62ms
step:296/2150 train_time:9952ms step_avg:33.62ms
step:297/2150 train_time:9984ms step_avg:33.62ms
step:298/2150 train_time:10017ms step_avg:33.62ms
step:299/2150 train_time:10050ms step_avg:33.61ms
step:300/2150 train_time:10083ms step_avg:33.61ms
step:301/2150 train_time:10116ms step_avg:33.61ms
step:302/2150 train_time:10149ms step_avg:33.61ms
step:303/2150 train_time:10182ms step_avg:33.60ms
step:304/2150 train_time:10215ms step_avg:33.60ms
step:305/2150 train_time:10247ms step_avg:33.60ms
step:306/2150 train_time:10281ms step_avg:33.60ms
step:307/2150 train_time:10313ms step_avg:33.59ms
step:308/2150 train_time:10346ms step_avg:33.59ms
step:309/2150 train_time:10379ms step_avg:33.59ms
step:310/2150 train_time:10412ms step_avg:33.59ms
step:311/2150 train_time:10445ms step_avg:33.58ms
step:312/2150 train_time:10478ms step_avg:33.58ms
step:313/2150 train_time:10512ms step_avg:33.58ms
step:314/2150 train_time:10545ms step_avg:33.58ms
step:315/2150 train_time:10578ms step_avg:33.58ms
step:316/2150 train_time:10612ms step_avg:33.58ms
step:317/2150 train_time:10645ms step_avg:33.58ms
step:318/2150 train_time:10679ms step_avg:33.58ms
step:319/2150 train_time:10711ms step_avg:33.58ms
step:320/2150 train_time:10745ms step_avg:33.58ms
step:321/2150 train_time:10778ms step_avg:33.58ms
step:322/2150 train_time:10812ms step_avg:33.58ms
step:323/2150 train_time:10844ms step_avg:33.57ms
step:324/2150 train_time:10878ms step_avg:33.57ms
step:325/2150 train_time:10911ms step_avg:33.57ms
step:326/2150 train_time:10945ms step_avg:33.57ms
step:327/2150 train_time:10977ms step_avg:33.57ms
step:328/2150 train_time:11010ms step_avg:33.57ms
step:329/2150 train_time:11043ms step_avg:33.57ms
step:330/2150 train_time:11076ms step_avg:33.56ms
step:331/2150 train_time:11109ms step_avg:33.56ms
step:332/2150 train_time:11142ms step_avg:33.56ms
step:333/2150 train_time:11175ms step_avg:33.56ms
step:334/2150 train_time:11208ms step_avg:33.56ms
step:335/2150 train_time:11240ms step_avg:33.55ms
step:336/2150 train_time:11273ms step_avg:33.55ms
step:337/2150 train_time:11306ms step_avg:33.55ms
step:338/2150 train_time:11339ms step_avg:33.55ms
step:339/2150 train_time:11372ms step_avg:33.55ms
step:340/2150 train_time:11405ms step_avg:33.54ms
step:341/2150 train_time:11438ms step_avg:33.54ms
step:342/2150 train_time:11471ms step_avg:33.54ms
step:343/2150 train_time:11504ms step_avg:33.54ms
step:344/2150 train_time:11537ms step_avg:33.54ms
step:345/2150 train_time:11570ms step_avg:33.54ms
step:346/2150 train_time:11604ms step_avg:33.54ms
step:347/2150 train_time:11637ms step_avg:33.54ms
step:348/2150 train_time:11671ms step_avg:33.54ms
step:349/2150 train_time:11704ms step_avg:33.54ms
step:350/2150 train_time:11737ms step_avg:33.53ms
step:351/2150 train_time:11770ms step_avg:33.53ms
step:352/2150 train_time:11807ms step_avg:33.54ms
step:353/2150 train_time:11836ms step_avg:33.53ms
step:354/2150 train_time:11869ms step_avg:33.53ms
step:355/2150 train_time:11902ms step_avg:33.53ms
step:356/2150 train_time:11936ms step_avg:33.53ms
step:357/2150 train_time:11968ms step_avg:33.52ms
step:358/2150 train_time:12002ms step_avg:33.52ms
step:359/2150 train_time:12035ms step_avg:33.52ms
step:360/2150 train_time:12068ms step_avg:33.52ms
step:361/2150 train_time:12101ms step_avg:33.52ms
step:362/2150 train_time:12134ms step_avg:33.52ms
step:363/2150 train_time:12167ms step_avg:33.52ms
step:364/2150 train_time:12200ms step_avg:33.52ms
step:365/2150 train_time:12233ms step_avg:33.51ms
step:366/2150 train_time:12266ms step_avg:33.51ms
step:367/2150 train_time:12298ms step_avg:33.51ms
step:368/2150 train_time:12332ms step_avg:33.51ms
step:369/2150 train_time:12364ms step_avg:33.51ms
step:370/2150 train_time:12398ms step_avg:33.51ms
step:371/2150 train_time:12430ms step_avg:33.50ms
step:372/2150 train_time:12463ms step_avg:33.50ms
step:373/2150 train_time:12496ms step_avg:33.50ms
step:374/2150 train_time:12529ms step_avg:33.50ms
step:375/2150 train_time:12562ms step_avg:33.50ms
step:376/2150 train_time:12596ms step_avg:33.50ms
step:377/2150 train_time:12629ms step_avg:33.50ms
step:378/2150 train_time:12662ms step_avg:33.50ms
step:379/2150 train_time:12695ms step_avg:33.50ms
step:380/2150 train_time:12728ms step_avg:33.50ms
step:381/2150 train_time:12761ms step_avg:33.49ms
step:382/2150 train_time:12794ms step_avg:33.49ms
step:383/2150 train_time:12827ms step_avg:33.49ms
step:384/2150 train_time:12860ms step_avg:33.49ms
step:385/2150 train_time:12893ms step_avg:33.49ms
step:386/2150 train_time:12926ms step_avg:33.49ms
step:387/2150 train_time:12959ms step_avg:33.49ms
step:388/2150 train_time:12992ms step_avg:33.48ms
step:389/2150 train_time:13025ms step_avg:33.48ms
step:390/2150 train_time:13058ms step_avg:33.48ms
step:391/2150 train_time:13091ms step_avg:33.48ms
step:392/2150 train_time:13124ms step_avg:33.48ms
step:393/2150 train_time:13156ms step_avg:33.48ms
step:394/2150 train_time:13190ms step_avg:33.48ms
step:395/2150 train_time:13222ms step_avg:33.47ms
step:396/2150 train_time:13256ms step_avg:33.47ms
step:397/2150 train_time:13288ms step_avg:33.47ms
step:398/2150 train_time:13321ms step_avg:33.47ms
step:399/2150 train_time:13354ms step_avg:33.47ms
step:400/2150 train_time:13387ms step_avg:33.47ms
step:401/2150 train_time:13420ms step_avg:33.47ms
step:402/2150 train_time:13453ms step_avg:33.47ms
step:403/2150 train_time:13486ms step_avg:33.46ms
step:404/2150 train_time:13519ms step_avg:33.46ms
step:405/2150 train_time:13552ms step_avg:33.46ms
step:406/2150 train_time:13586ms step_avg:33.46ms
step:407/2150 train_time:13618ms step_avg:33.46ms
step:408/2150 train_time:13652ms step_avg:33.46ms
step:409/2150 train_time:13685ms step_avg:33.46ms
step:410/2150 train_time:13718ms step_avg:33.46ms
step:411/2150 train_time:13751ms step_avg:33.46ms
step:412/2150 train_time:13784ms step_avg:33.46ms
step:413/2150 train_time:13817ms step_avg:33.45ms
step:414/2150 train_time:13850ms step_avg:33.45ms
step:415/2150 train_time:13883ms step_avg:33.45ms
step:416/2150 train_time:13917ms step_avg:33.45ms
step:417/2150 train_time:13949ms step_avg:33.45ms
step:418/2150 train_time:13982ms step_avg:33.45ms
step:419/2150 train_time:14015ms step_avg:33.45ms
step:420/2150 train_time:14049ms step_avg:33.45ms
step:421/2150 train_time:14082ms step_avg:33.45ms
step:422/2150 train_time:14115ms step_avg:33.45ms
step:423/2150 train_time:14148ms step_avg:33.45ms
step:424/2150 train_time:14181ms step_avg:33.45ms
step:425/2150 train_time:14214ms step_avg:33.45ms
step:426/2150 train_time:14247ms step_avg:33.44ms
step:427/2150 train_time:14280ms step_avg:33.44ms
step:428/2150 train_time:14313ms step_avg:33.44ms
step:429/2150 train_time:14346ms step_avg:33.44ms
step:430/2150 train_time:14379ms step_avg:33.44ms
step:431/2150 train_time:14412ms step_avg:33.44ms
step:432/2150 train_time:14445ms step_avg:33.44ms
step:433/2150 train_time:14478ms step_avg:33.44ms
step:434/2150 train_time:14511ms step_avg:33.44ms
step:435/2150 train_time:14544ms step_avg:33.44ms
step:436/2150 train_time:14578ms step_avg:33.43ms
step:437/2150 train_time:14610ms step_avg:33.43ms
step:438/2150 train_time:14644ms step_avg:33.43ms
step:439/2150 train_time:14677ms step_avg:33.43ms
step:440/2150 train_time:14710ms step_avg:33.43ms
step:441/2150 train_time:14743ms step_avg:33.43ms
step:442/2150 train_time:14776ms step_avg:33.43ms
step:443/2150 train_time:14810ms step_avg:33.43ms
step:444/2150 train_time:14843ms step_avg:33.43ms
step:445/2150 train_time:14876ms step_avg:33.43ms
step:446/2150 train_time:14909ms step_avg:33.43ms
step:447/2150 train_time:14942ms step_avg:33.43ms
step:448/2150 train_time:14975ms step_avg:33.43ms
step:449/2150 train_time:15008ms step_avg:33.43ms
step:450/2150 train_time:15041ms step_avg:33.42ms
step:451/2150 train_time:15074ms step_avg:33.42ms
step:452/2150 train_time:15107ms step_avg:33.42ms
step:453/2150 train_time:15140ms step_avg:33.42ms
step:454/2150 train_time:15173ms step_avg:33.42ms
step:455/2150 train_time:15206ms step_avg:33.42ms
step:456/2150 train_time:15240ms step_avg:33.42ms
step:457/2150 train_time:15272ms step_avg:33.42ms
step:458/2150 train_time:15305ms step_avg:33.42ms
step:459/2150 train_time:15338ms step_avg:33.42ms
step:460/2150 train_time:15371ms step_avg:33.42ms
step:461/2150 train_time:15404ms step_avg:33.42ms
step:462/2150 train_time:15438ms step_avg:33.41ms
step:463/2150 train_time:15470ms step_avg:33.41ms
step:464/2150 train_time:15504ms step_avg:33.41ms
step:465/2150 train_time:15536ms step_avg:33.41ms
step:466/2150 train_time:15570ms step_avg:33.41ms
step:467/2150 train_time:15602ms step_avg:33.41ms
step:468/2150 train_time:15636ms step_avg:33.41ms
step:469/2150 train_time:15668ms step_avg:33.41ms
step:470/2150 train_time:15702ms step_avg:33.41ms
step:471/2150 train_time:15735ms step_avg:33.41ms
step:472/2150 train_time:15768ms step_avg:33.41ms
step:473/2150 train_time:15801ms step_avg:33.41ms
step:474/2150 train_time:15834ms step_avg:33.41ms
step:475/2150 train_time:15867ms step_avg:33.40ms
step:476/2150 train_time:15900ms step_avg:33.40ms
step:477/2150 train_time:15933ms step_avg:33.40ms
step:478/2150 train_time:15966ms step_avg:33.40ms
step:479/2150 train_time:15999ms step_avg:33.40ms
step:480/2150 train_time:16033ms step_avg:33.40ms
step:481/2150 train_time:16066ms step_avg:33.40ms
step:482/2150 train_time:16099ms step_avg:33.40ms
step:483/2150 train_time:16132ms step_avg:33.40ms
step:484/2150 train_time:16165ms step_avg:33.40ms
step:485/2150 train_time:16200ms step_avg:33.40ms
step:486/2150 train_time:16231ms step_avg:33.40ms
step:487/2150 train_time:16264ms step_avg:33.40ms
step:488/2150 train_time:16297ms step_avg:33.40ms
step:489/2150 train_time:16330ms step_avg:33.39ms
step:490/2150 train_time:16363ms step_avg:33.39ms
step:491/2150 train_time:16396ms step_avg:33.39ms
step:492/2150 train_time:16429ms step_avg:33.39ms
step:493/2150 train_time:16462ms step_avg:33.39ms
step:494/2150 train_time:16495ms step_avg:33.39ms
step:495/2150 train_time:16528ms step_avg:33.39ms
step:496/2150 train_time:16561ms step_avg:33.39ms
step:497/2150 train_time:16594ms step_avg:33.39ms
step:498/2150 train_time:16628ms step_avg:33.39ms
step:499/2150 train_time:16660ms step_avg:33.39ms
step:500/2150 train_time:16694ms step_avg:33.39ms
step:500/2150 val_loss:4.0203 train_time:16730ms step_avg:33.46ms
step:501/2150 train_time:16752ms step_avg:33.44ms
step:502/2150 train_time:16774ms step_avg:33.41ms
step:503/2150 train_time:16796ms step_avg:33.39ms
step:504/2150 train_time:16830ms step_avg:33.39ms
step:505/2150 train_time:16866ms step_avg:33.40ms
step:506/2150 train_time:16900ms step_avg:33.40ms
step:507/2150 train_time:16934ms step_avg:33.40ms
step:508/2150 train_time:16967ms step_avg:33.40ms
step:509/2150 train_time:17000ms step_avg:33.40ms
step:510/2150 train_time:17034ms step_avg:33.40ms
step:511/2150 train_time:17067ms step_avg:33.40ms
step:512/2150 train_time:17100ms step_avg:33.40ms
step:513/2150 train_time:17132ms step_avg:33.40ms
step:514/2150 train_time:17165ms step_avg:33.40ms
step:515/2150 train_time:17198ms step_avg:33.39ms
step:516/2150 train_time:17231ms step_avg:33.39ms
step:517/2150 train_time:17263ms step_avg:33.39ms
step:518/2150 train_time:17297ms step_avg:33.39ms
step:519/2150 train_time:17329ms step_avg:33.39ms
step:520/2150 train_time:17362ms step_avg:33.39ms
step:521/2150 train_time:17395ms step_avg:33.39ms
step:522/2150 train_time:17428ms step_avg:33.39ms
step:523/2150 train_time:17461ms step_avg:33.39ms
step:524/2150 train_time:17494ms step_avg:33.39ms
step:525/2150 train_time:17526ms step_avg:33.38ms
step:526/2150 train_time:17560ms step_avg:33.38ms
step:527/2150 train_time:17592ms step_avg:33.38ms
step:528/2150 train_time:17626ms step_avg:33.38ms
step:529/2150 train_time:17658ms step_avg:33.38ms
step:530/2150 train_time:17691ms step_avg:33.38ms
step:531/2150 train_time:17724ms step_avg:33.38ms
step:532/2150 train_time:17758ms step_avg:33.38ms
step:533/2150 train_time:17791ms step_avg:33.38ms
step:534/2150 train_time:17825ms step_avg:33.38ms
step:535/2150 train_time:17858ms step_avg:33.38ms
step:536/2150 train_time:17892ms step_avg:33.38ms
step:537/2150 train_time:17926ms step_avg:33.38ms
step:538/2150 train_time:17959ms step_avg:33.38ms
step:539/2150 train_time:17993ms step_avg:33.38ms
step:540/2150 train_time:18026ms step_avg:33.38ms
step:541/2150 train_time:18059ms step_avg:33.38ms
step:542/2150 train_time:18092ms step_avg:33.38ms
step:543/2150 train_time:18125ms step_avg:33.38ms
step:544/2150 train_time:18158ms step_avg:33.38ms
step:545/2150 train_time:18191ms step_avg:33.38ms
step:546/2150 train_time:18224ms step_avg:33.38ms
step:547/2150 train_time:18257ms step_avg:33.38ms
step:548/2150 train_time:18290ms step_avg:33.38ms
step:549/2150 train_time:18323ms step_avg:33.38ms
step:550/2150 train_time:18356ms step_avg:33.38ms
step:551/2150 train_time:18389ms step_avg:33.37ms
step:552/2150 train_time:18422ms step_avg:33.37ms
step:553/2150 train_time:18457ms step_avg:33.38ms
step:554/2150 train_time:18489ms step_avg:33.37ms
step:555/2150 train_time:18521ms step_avg:33.37ms
step:556/2150 train_time:18555ms step_avg:33.37ms
step:557/2150 train_time:18587ms step_avg:33.37ms
step:558/2150 train_time:18620ms step_avg:33.37ms
step:559/2150 train_time:18653ms step_avg:33.37ms
step:560/2150 train_time:18686ms step_avg:33.37ms
step:561/2150 train_time:18719ms step_avg:33.37ms
step:562/2150 train_time:18752ms step_avg:33.37ms
step:563/2150 train_time:18786ms step_avg:33.37ms
step:564/2150 train_time:18819ms step_avg:33.37ms
step:565/2150 train_time:18852ms step_avg:33.37ms
step:566/2150 train_time:18885ms step_avg:33.37ms
step:567/2150 train_time:18919ms step_avg:33.37ms
step:568/2150 train_time:18952ms step_avg:33.37ms
step:569/2150 train_time:18986ms step_avg:33.37ms
step:570/2150 train_time:19019ms step_avg:33.37ms
step:571/2150 train_time:19052ms step_avg:33.37ms
step:572/2150 train_time:19086ms step_avg:33.37ms
step:573/2150 train_time:19119ms step_avg:33.37ms
step:574/2150 train_time:19152ms step_avg:33.37ms
step:575/2150 train_time:19185ms step_avg:33.36ms
step:576/2150 train_time:19218ms step_avg:33.36ms
step:577/2150 train_time:19251ms step_avg:33.36ms
step:578/2150 train_time:19284ms step_avg:33.36ms
step:579/2150 train_time:19317ms step_avg:33.36ms
step:580/2150 train_time:19350ms step_avg:33.36ms
step:581/2150 train_time:19383ms step_avg:33.36ms
step:582/2150 train_time:19416ms step_avg:33.36ms
step:583/2150 train_time:19449ms step_avg:33.36ms
step:584/2150 train_time:19482ms step_avg:33.36ms
step:585/2150 train_time:19515ms step_avg:33.36ms
step:586/2150 train_time:19549ms step_avg:33.36ms
step:587/2150 train_time:19581ms step_avg:33.36ms
step:588/2150 train_time:19615ms step_avg:33.36ms
step:589/2150 train_time:19648ms step_avg:33.36ms
step:590/2150 train_time:19681ms step_avg:33.36ms
step:591/2150 train_time:19714ms step_avg:33.36ms
step:592/2150 train_time:19747ms step_avg:33.36ms
step:593/2150 train_time:19780ms step_avg:33.36ms
step:594/2150 train_time:19813ms step_avg:33.36ms
step:595/2150 train_time:19846ms step_avg:33.35ms
step:596/2150 train_time:19879ms step_avg:33.35ms
step:597/2150 train_time:19913ms step_avg:33.35ms
step:598/2150 train_time:19946ms step_avg:33.35ms
step:599/2150 train_time:19979ms step_avg:33.35ms
step:600/2150 train_time:20013ms step_avg:33.35ms
step:601/2150 train_time:20046ms step_avg:33.35ms
step:602/2150 train_time:20079ms step_avg:33.35ms
step:603/2150 train_time:20112ms step_avg:33.35ms
step:604/2150 train_time:20145ms step_avg:33.35ms
step:605/2150 train_time:20178ms step_avg:33.35ms
step:606/2150 train_time:20212ms step_avg:33.35ms
step:607/2150 train_time:20244ms step_avg:33.35ms
step:608/2150 train_time:20278ms step_avg:33.35ms
step:609/2150 train_time:20311ms step_avg:33.35ms
step:610/2150 train_time:20344ms step_avg:33.35ms
step:611/2150 train_time:20376ms step_avg:33.35ms
step:612/2150 train_time:20410ms step_avg:33.35ms
step:613/2150 train_time:20442ms step_avg:33.35ms
step:614/2150 train_time:20476ms step_avg:33.35ms
step:615/2150 train_time:20509ms step_avg:33.35ms
step:616/2150 train_time:20542ms step_avg:33.35ms
step:617/2150 train_time:20575ms step_avg:33.35ms
step:618/2150 train_time:20608ms step_avg:33.35ms
step:619/2150 train_time:20641ms step_avg:33.35ms
step:620/2150 train_time:20674ms step_avg:33.35ms
step:621/2150 train_time:20707ms step_avg:33.34ms
step:622/2150 train_time:20741ms step_avg:33.34ms
step:623/2150 train_time:20774ms step_avg:33.35ms
step:624/2150 train_time:20807ms step_avg:33.34ms
step:625/2150 train_time:20840ms step_avg:33.34ms
step:626/2150 train_time:20873ms step_avg:33.34ms
step:627/2150 train_time:20907ms step_avg:33.34ms
step:628/2150 train_time:20940ms step_avg:33.34ms
step:629/2150 train_time:20973ms step_avg:33.34ms
step:630/2150 train_time:21006ms step_avg:33.34ms
step:631/2150 train_time:21039ms step_avg:33.34ms
step:632/2150 train_time:21073ms step_avg:33.34ms
step:633/2150 train_time:21106ms step_avg:33.34ms
step:634/2150 train_time:21139ms step_avg:33.34ms
step:635/2150 train_time:21172ms step_avg:33.34ms
step:636/2150 train_time:21205ms step_avg:33.34ms
step:637/2150 train_time:21238ms step_avg:33.34ms
step:638/2150 train_time:21271ms step_avg:33.34ms
step:639/2150 train_time:21304ms step_avg:33.34ms
step:640/2150 train_time:21337ms step_avg:33.34ms
step:641/2150 train_time:21370ms step_avg:33.34ms
step:642/2150 train_time:21404ms step_avg:33.34ms
step:643/2150 train_time:21436ms step_avg:33.34ms
step:644/2150 train_time:21470ms step_avg:33.34ms
step:645/2150 train_time:21502ms step_avg:33.34ms
step:646/2150 train_time:21536ms step_avg:33.34ms
step:647/2150 train_time:21569ms step_avg:33.34ms
step:648/2150 train_time:21602ms step_avg:33.34ms
step:649/2150 train_time:21635ms step_avg:33.34ms
step:650/2150 train_time:21668ms step_avg:33.34ms
step:651/2150 train_time:21702ms step_avg:33.34ms
step:652/2150 train_time:21735ms step_avg:33.34ms
step:653/2150 train_time:21768ms step_avg:33.34ms
step:654/2150 train_time:21801ms step_avg:33.34ms
step:655/2150 train_time:21834ms step_avg:33.33ms
step:656/2150 train_time:21867ms step_avg:33.33ms
step:657/2150 train_time:21900ms step_avg:33.33ms
step:658/2150 train_time:21934ms step_avg:33.33ms
step:659/2150 train_time:21967ms step_avg:33.33ms
step:660/2150 train_time:22000ms step_avg:33.33ms
step:661/2150 train_time:22033ms step_avg:33.33ms
step:662/2150 train_time:22067ms step_avg:33.33ms
step:663/2150 train_time:22100ms step_avg:33.33ms
step:664/2150 train_time:22133ms step_avg:33.33ms
step:665/2150 train_time:22166ms step_avg:33.33ms
step:666/2150 train_time:22199ms step_avg:33.33ms
step:667/2150 train_time:22232ms step_avg:33.33ms
step:668/2150 train_time:22266ms step_avg:33.33ms
step:669/2150 train_time:22299ms step_avg:33.33ms
step:670/2150 train_time:22332ms step_avg:33.33ms
step:671/2150 train_time:22365ms step_avg:33.33ms
step:672/2150 train_time:22398ms step_avg:33.33ms
step:673/2150 train_time:22431ms step_avg:33.33ms
step:674/2150 train_time:22464ms step_avg:33.33ms
step:675/2150 train_time:22498ms step_avg:33.33ms
step:676/2150 train_time:22531ms step_avg:33.33ms
step:677/2150 train_time:22564ms step_avg:33.33ms
step:678/2150 train_time:22597ms step_avg:33.33ms
step:679/2150 train_time:22630ms step_avg:33.33ms
step:680/2150 train_time:22664ms step_avg:33.33ms
step:681/2150 train_time:22697ms step_avg:33.33ms
step:682/2150 train_time:22730ms step_avg:33.33ms
step:683/2150 train_time:22763ms step_avg:33.33ms
step:684/2150 train_time:22797ms step_avg:33.33ms
step:685/2150 train_time:22829ms step_avg:33.33ms
step:686/2150 train_time:22867ms step_avg:33.33ms
step:687/2150 train_time:22896ms step_avg:33.33ms
step:688/2150 train_time:22929ms step_avg:33.33ms
step:689/2150 train_time:22962ms step_avg:33.33ms
step:690/2150 train_time:22996ms step_avg:33.33ms
step:691/2150 train_time:23028ms step_avg:33.33ms
step:692/2150 train_time:23062ms step_avg:33.33ms
step:693/2150 train_time:23095ms step_avg:33.33ms
step:694/2150 train_time:23128ms step_avg:33.33ms
step:695/2150 train_time:23161ms step_avg:33.33ms
step:696/2150 train_time:23195ms step_avg:33.33ms
step:697/2150 train_time:23227ms step_avg:33.32ms
step:698/2150 train_time:23261ms step_avg:33.32ms
step:699/2150 train_time:23294ms step_avg:33.32ms
step:700/2150 train_time:23327ms step_avg:33.32ms
step:701/2150 train_time:23360ms step_avg:33.32ms
step:702/2150 train_time:23393ms step_avg:33.32ms
step:703/2150 train_time:23426ms step_avg:33.32ms
step:704/2150 train_time:23459ms step_avg:33.32ms
step:705/2150 train_time:23493ms step_avg:33.32ms
step:706/2150 train_time:23552ms step_avg:33.36ms
step:707/2150 train_time:23613ms step_avg:33.40ms
step:708/2150 train_time:23673ms step_avg:33.44ms
step:709/2150 train_time:23734ms step_avg:33.48ms
step:710/2150 train_time:23793ms step_avg:33.51ms
step:711/2150 train_time:23855ms step_avg:33.55ms
step:712/2150 train_time:23915ms step_avg:33.59ms
step:713/2150 train_time:23976ms step_avg:33.63ms
step:714/2150 train_time:24035ms step_avg:33.66ms
step:715/2150 train_time:24097ms step_avg:33.70ms
step:716/2150 train_time:24159ms step_avg:33.74ms
step:717/2150 train_time:24218ms step_avg:33.78ms
step:718/2150 train_time:24277ms step_avg:33.81ms
step:719/2150 train_time:24338ms step_avg:33.85ms
step:720/2150 train_time:24398ms step_avg:33.89ms
step:721/2150 train_time:24459ms step_avg:33.92ms
step:722/2150 train_time:24517ms step_avg:33.96ms
step:723/2150 train_time:24579ms step_avg:34.00ms
step:724/2150 train_time:24638ms step_avg:34.03ms
step:725/2150 train_time:24700ms step_avg:34.07ms
step:726/2150 train_time:24759ms step_avg:34.10ms
step:727/2150 train_time:24821ms step_avg:34.14ms
step:728/2150 train_time:24880ms step_avg:34.18ms
step:729/2150 train_time:24941ms step_avg:34.21ms
step:730/2150 train_time:25000ms step_avg:34.25ms
step:731/2150 train_time:25062ms step_avg:34.28ms
step:732/2150 train_time:25121ms step_avg:34.32ms
step:733/2150 train_time:25182ms step_avg:34.35ms
step:734/2150 train_time:25241ms step_avg:34.39ms
step:735/2150 train_time:25302ms step_avg:34.42ms
step:736/2150 train_time:25361ms step_avg:34.46ms
step:737/2150 train_time:25422ms step_avg:34.49ms
step:738/2150 train_time:25481ms step_avg:34.53ms
step:739/2150 train_time:25542ms step_avg:34.56ms
step:740/2150 train_time:25602ms step_avg:34.60ms
step:741/2150 train_time:25663ms step_avg:34.63ms
step:742/2150 train_time:25723ms step_avg:34.67ms
step:743/2150 train_time:25783ms step_avg:34.70ms
step:744/2150 train_time:25843ms step_avg:34.73ms
step:745/2150 train_time:25904ms step_avg:34.77ms
step:746/2150 train_time:25964ms step_avg:34.80ms
step:747/2150 train_time:26025ms step_avg:34.84ms
step:748/2150 train_time:26085ms step_avg:34.87ms
step:749/2150 train_time:26145ms step_avg:34.91ms
step:750/2150 train_time:26205ms step_avg:34.94ms
step:750/2150 val_loss:3.8703 train_time:26269ms step_avg:35.03ms
step:751/2150 train_time:26293ms step_avg:35.01ms
step:752/2150 train_time:26327ms step_avg:35.01ms
step:753/2150 train_time:26393ms step_avg:35.05ms
step:754/2150 train_time:26457ms step_avg:35.09ms
step:755/2150 train_time:26518ms step_avg:35.12ms
step:756/2150 train_time:26576ms step_avg:35.15ms
step:757/2150 train_time:26636ms step_avg:35.19ms
step:758/2150 train_time:26694ms step_avg:35.22ms
step:759/2150 train_time:26754ms step_avg:35.25ms
step:760/2150 train_time:26812ms step_avg:35.28ms
step:761/2150 train_time:26872ms step_avg:35.31ms
step:762/2150 train_time:26930ms step_avg:35.34ms
step:763/2150 train_time:26990ms step_avg:35.37ms
step:764/2150 train_time:27048ms step_avg:35.40ms
step:765/2150 train_time:27108ms step_avg:35.44ms
step:766/2150 train_time:27169ms step_avg:35.47ms
step:767/2150 train_time:27235ms step_avg:35.51ms
step:768/2150 train_time:27298ms step_avg:35.54ms
step:769/2150 train_time:27361ms step_avg:35.58ms
step:770/2150 train_time:27421ms step_avg:35.61ms
step:771/2150 train_time:27482ms step_avg:35.65ms
step:772/2150 train_time:27541ms step_avg:35.68ms
step:773/2150 train_time:27602ms step_avg:35.71ms
step:774/2150 train_time:27663ms step_avg:35.74ms
step:775/2150 train_time:27722ms step_avg:35.77ms
step:776/2150 train_time:27780ms step_avg:35.80ms
step:777/2150 train_time:27841ms step_avg:35.83ms
step:778/2150 train_time:27900ms step_avg:35.86ms
step:779/2150 train_time:27960ms step_avg:35.89ms
step:780/2150 train_time:28019ms step_avg:35.92ms
step:781/2150 train_time:28081ms step_avg:35.95ms
step:782/2150 train_time:28141ms step_avg:35.99ms
step:783/2150 train_time:28206ms step_avg:36.02ms
step:784/2150 train_time:28267ms step_avg:36.05ms
step:785/2150 train_time:28328ms step_avg:36.09ms
step:786/2150 train_time:28387ms step_avg:36.12ms
step:787/2150 train_time:28449ms step_avg:36.15ms
step:788/2150 train_time:28508ms step_avg:36.18ms
step:789/2150 train_time:28570ms step_avg:36.21ms
step:790/2150 train_time:28630ms step_avg:36.24ms
step:791/2150 train_time:28691ms step_avg:36.27ms
step:792/2150 train_time:28750ms step_avg:36.30ms
step:793/2150 train_time:28811ms step_avg:36.33ms
step:794/2150 train_time:28870ms step_avg:36.36ms
step:795/2150 train_time:28930ms step_avg:36.39ms
step:796/2150 train_time:28990ms step_avg:36.42ms
step:797/2150 train_time:29051ms step_avg:36.45ms
step:798/2150 train_time:29111ms step_avg:36.48ms
step:799/2150 train_time:29172ms step_avg:36.51ms
step:800/2150 train_time:29232ms step_avg:36.54ms
step:801/2150 train_time:29293ms step_avg:36.57ms
step:802/2150 train_time:29353ms step_avg:36.60ms
step:803/2150 train_time:29415ms step_avg:36.63ms
step:804/2150 train_time:29475ms step_avg:36.66ms
step:805/2150 train_time:29537ms step_avg:36.69ms
step:806/2150 train_time:29596ms step_avg:36.72ms
step:807/2150 train_time:29658ms step_avg:36.75ms
step:808/2150 train_time:29717ms step_avg:36.78ms
step:809/2150 train_time:29777ms step_avg:36.81ms
step:810/2150 train_time:29836ms step_avg:36.83ms
step:811/2150 train_time:29897ms step_avg:36.86ms
step:812/2150 train_time:29956ms step_avg:36.89ms
step:813/2150 train_time:30017ms step_avg:36.92ms
step:814/2150 train_time:30076ms step_avg:36.95ms
step:815/2150 train_time:30138ms step_avg:36.98ms
step:816/2150 train_time:30197ms step_avg:37.01ms
step:817/2150 train_time:30259ms step_avg:37.04ms
step:818/2150 train_time:30319ms step_avg:37.06ms
step:819/2150 train_time:30380ms step_avg:37.09ms
step:820/2150 train_time:30440ms step_avg:37.12ms
step:821/2150 train_time:30502ms step_avg:37.15ms
step:822/2150 train_time:30562ms step_avg:37.18ms
step:823/2150 train_time:30623ms step_avg:37.21ms
step:824/2150 train_time:30683ms step_avg:37.24ms
step:825/2150 train_time:30744ms step_avg:37.27ms
step:826/2150 train_time:30803ms step_avg:37.29ms
step:827/2150 train_time:30864ms step_avg:37.32ms
step:828/2150 train_time:30923ms step_avg:37.35ms
step:829/2150 train_time:30984ms step_avg:37.38ms
step:830/2150 train_time:31043ms step_avg:37.40ms
step:831/2150 train_time:31104ms step_avg:37.43ms
step:832/2150 train_time:31165ms step_avg:37.46ms
step:833/2150 train_time:31226ms step_avg:37.49ms
step:834/2150 train_time:31286ms step_avg:37.51ms
step:835/2150 train_time:31348ms step_avg:37.54ms
step:836/2150 train_time:31408ms step_avg:37.57ms
step:837/2150 train_time:31469ms step_avg:37.60ms
step:838/2150 train_time:31529ms step_avg:37.62ms
step:839/2150 train_time:31590ms step_avg:37.65ms
step:840/2150 train_time:31651ms step_avg:37.68ms
step:841/2150 train_time:31712ms step_avg:37.71ms
step:842/2150 train_time:31771ms step_avg:37.73ms
step:843/2150 train_time:31832ms step_avg:37.76ms
step:844/2150 train_time:31892ms step_avg:37.79ms
step:845/2150 train_time:31953ms step_avg:37.81ms
step:846/2150 train_time:32012ms step_avg:37.84ms
step:847/2150 train_time:32073ms step_avg:37.87ms
step:848/2150 train_time:32133ms step_avg:37.89ms
step:849/2150 train_time:32194ms step_avg:37.92ms
step:850/2150 train_time:32253ms step_avg:37.95ms
step:851/2150 train_time:32315ms step_avg:37.97ms
step:852/2150 train_time:32375ms step_avg:38.00ms
step:853/2150 train_time:32437ms step_avg:38.03ms
step:854/2150 train_time:32496ms step_avg:38.05ms
step:855/2150 train_time:32559ms step_avg:38.08ms
step:856/2150 train_time:32618ms step_avg:38.11ms
step:857/2150 train_time:32680ms step_avg:38.13ms
step:858/2150 train_time:32739ms step_avg:38.16ms
step:859/2150 train_time:32800ms step_avg:38.18ms
step:860/2150 train_time:32859ms step_avg:38.21ms
step:861/2150 train_time:32920ms step_avg:38.23ms
step:862/2150 train_time:32980ms step_avg:38.26ms
step:863/2150 train_time:33042ms step_avg:38.29ms
step:864/2150 train_time:33102ms step_avg:38.31ms
step:865/2150 train_time:33163ms step_avg:38.34ms
step:866/2150 train_time:33222ms step_avg:38.36ms
step:867/2150 train_time:33284ms step_avg:38.39ms
step:868/2150 train_time:33343ms step_avg:38.41ms
step:869/2150 train_time:33404ms step_avg:38.44ms
step:870/2150 train_time:33464ms step_avg:38.46ms
step:871/2150 train_time:33525ms step_avg:38.49ms
step:872/2150 train_time:33585ms step_avg:38.51ms
step:873/2150 train_time:33646ms step_avg:38.54ms
step:874/2150 train_time:33705ms step_avg:38.56ms
step:875/2150 train_time:33766ms step_avg:38.59ms
step:876/2150 train_time:33826ms step_avg:38.61ms
step:877/2150 train_time:33887ms step_avg:38.64ms
step:878/2150 train_time:33947ms step_avg:38.66ms
step:879/2150 train_time:34007ms step_avg:38.69ms
step:880/2150 train_time:34067ms step_avg:38.71ms
step:881/2150 train_time:34128ms step_avg:38.74ms
step:882/2150 train_time:34187ms step_avg:38.76ms
step:883/2150 train_time:34248ms step_avg:38.79ms
step:884/2150 train_time:34308ms step_avg:38.81ms
step:885/2150 train_time:34372ms step_avg:38.84ms
step:886/2150 train_time:34429ms step_avg:38.86ms
step:887/2150 train_time:34490ms step_avg:38.88ms
step:888/2150 train_time:34550ms step_avg:38.91ms
step:889/2150 train_time:34612ms step_avg:38.93ms
step:890/2150 train_time:34671ms step_avg:38.96ms
step:891/2150 train_time:34733ms step_avg:38.98ms
step:892/2150 train_time:34792ms step_avg:39.00ms
step:893/2150 train_time:34854ms step_avg:39.03ms
step:894/2150 train_time:34913ms step_avg:39.05ms
step:895/2150 train_time:34975ms step_avg:39.08ms
step:896/2150 train_time:35034ms step_avg:39.10ms
step:897/2150 train_time:35096ms step_avg:39.13ms
step:898/2150 train_time:35155ms step_avg:39.15ms
step:899/2150 train_time:35217ms step_avg:39.17ms
step:900/2150 train_time:35276ms step_avg:39.20ms
step:901/2150 train_time:35338ms step_avg:39.22ms
step:902/2150 train_time:35398ms step_avg:39.24ms
step:903/2150 train_time:35459ms step_avg:39.27ms
step:904/2150 train_time:35519ms step_avg:39.29ms
step:905/2150 train_time:35580ms step_avg:39.32ms
step:906/2150 train_time:35640ms step_avg:39.34ms
step:907/2150 train_time:35702ms step_avg:39.36ms
step:908/2150 train_time:35762ms step_avg:39.39ms
step:909/2150 train_time:35823ms step_avg:39.41ms
step:910/2150 train_time:35882ms step_avg:39.43ms
step:911/2150 train_time:35943ms step_avg:39.45ms
step:912/2150 train_time:36003ms step_avg:39.48ms
step:913/2150 train_time:36064ms step_avg:39.50ms
step:914/2150 train_time:36123ms step_avg:39.52ms
step:915/2150 train_time:36185ms step_avg:39.55ms
step:916/2150 train_time:36244ms step_avg:39.57ms
step:917/2150 train_time:36305ms step_avg:39.59ms
step:918/2150 train_time:36365ms step_avg:39.61ms
step:919/2150 train_time:36426ms step_avg:39.64ms
step:920/2150 train_time:36486ms step_avg:39.66ms
step:921/2150 train_time:36547ms step_avg:39.68ms
step:922/2150 train_time:36606ms step_avg:39.70ms
step:923/2150 train_time:36668ms step_avg:39.73ms
step:924/2150 train_time:36728ms step_avg:39.75ms
step:925/2150 train_time:36788ms step_avg:39.77ms
step:926/2150 train_time:36848ms step_avg:39.79ms
step:927/2150 train_time:36910ms step_avg:39.82ms
step:928/2150 train_time:36969ms step_avg:39.84ms
step:929/2150 train_time:37030ms step_avg:39.86ms
step:930/2150 train_time:37090ms step_avg:39.88ms
step:931/2150 train_time:37151ms step_avg:39.90ms
step:932/2150 train_time:37211ms step_avg:39.93ms
step:933/2150 train_time:37273ms step_avg:39.95ms
step:934/2150 train_time:37332ms step_avg:39.97ms
step:935/2150 train_time:37394ms step_avg:39.99ms
step:936/2150 train_time:37453ms step_avg:40.01ms
step:937/2150 train_time:37515ms step_avg:40.04ms
step:938/2150 train_time:37574ms step_avg:40.06ms
step:939/2150 train_time:37636ms step_avg:40.08ms
step:940/2150 train_time:37695ms step_avg:40.10ms
step:941/2150 train_time:37757ms step_avg:40.12ms
step:942/2150 train_time:37817ms step_avg:40.15ms
step:943/2150 train_time:37879ms step_avg:40.17ms
step:944/2150 train_time:37938ms step_avg:40.19ms
step:945/2150 train_time:38000ms step_avg:40.21ms
step:946/2150 train_time:38059ms step_avg:40.23ms
step:947/2150 train_time:38121ms step_avg:40.25ms
step:948/2150 train_time:38182ms step_avg:40.28ms
step:949/2150 train_time:38243ms step_avg:40.30ms
step:950/2150 train_time:38303ms step_avg:40.32ms
step:951/2150 train_time:38364ms step_avg:40.34ms
step:952/2150 train_time:38423ms step_avg:40.36ms
step:953/2150 train_time:38484ms step_avg:40.38ms
step:954/2150 train_time:38544ms step_avg:40.40ms
step:955/2150 train_time:38605ms step_avg:40.42ms
step:956/2150 train_time:38664ms step_avg:40.44ms
step:957/2150 train_time:38725ms step_avg:40.47ms
step:958/2150 train_time:38785ms step_avg:40.49ms
step:959/2150 train_time:38846ms step_avg:40.51ms
step:960/2150 train_time:38906ms step_avg:40.53ms
step:961/2150 train_time:38967ms step_avg:40.55ms
step:962/2150 train_time:39027ms step_avg:40.57ms
step:963/2150 train_time:39088ms step_avg:40.59ms
step:964/2150 train_time:39147ms step_avg:40.61ms
step:965/2150 train_time:39208ms step_avg:40.63ms
step:966/2150 train_time:39268ms step_avg:40.65ms
step:967/2150 train_time:39329ms step_avg:40.67ms
step:968/2150 train_time:39389ms step_avg:40.69ms
step:969/2150 train_time:39450ms step_avg:40.71ms
step:970/2150 train_time:39510ms step_avg:40.73ms
step:971/2150 train_time:39573ms step_avg:40.75ms
step:972/2150 train_time:39632ms step_avg:40.77ms
step:973/2150 train_time:39693ms step_avg:40.79ms
step:974/2150 train_time:39753ms step_avg:40.81ms
step:975/2150 train_time:39815ms step_avg:40.84ms
step:976/2150 train_time:39877ms step_avg:40.86ms
step:977/2150 train_time:39936ms step_avg:40.88ms
step:978/2150 train_time:39997ms step_avg:40.90ms
step:979/2150 train_time:40059ms step_avg:40.92ms
step:980/2150 train_time:40118ms step_avg:40.94ms
step:981/2150 train_time:40179ms step_avg:40.96ms
step:982/2150 train_time:40239ms step_avg:40.98ms
step:983/2150 train_time:40300ms step_avg:41.00ms
step:984/2150 train_time:40360ms step_avg:41.02ms
step:985/2150 train_time:40421ms step_avg:41.04ms
step:986/2150 train_time:40480ms step_avg:41.05ms
step:987/2150 train_time:40541ms step_avg:41.08ms
step:988/2150 train_time:40601ms step_avg:41.09ms
step:989/2150 train_time:40663ms step_avg:41.12ms
step:990/2150 train_time:40722ms step_avg:41.13ms
step:991/2150 train_time:40783ms step_avg:41.15ms
step:992/2150 train_time:40843ms step_avg:41.17ms
step:993/2150 train_time:40904ms step_avg:41.19ms
step:994/2150 train_time:40964ms step_avg:41.21ms
step:995/2150 train_time:41025ms step_avg:41.23ms
step:996/2150 train_time:41084ms step_avg:41.25ms
step:997/2150 train_time:41145ms step_avg:41.27ms
step:998/2150 train_time:41205ms step_avg:41.29ms
step:999/2150 train_time:41266ms step_avg:41.31ms
step:1000/2150 train_time:41325ms step_avg:41.33ms
step:1000/2150 val_loss:3.7136 train_time:41388ms step_avg:41.39ms
step:1001/2150 train_time:41411ms step_avg:41.37ms
step:1002/2150 train_time:41447ms step_avg:41.36ms
step:1003/2150 train_time:41513ms step_avg:41.39ms
step:1004/2150 train_time:41578ms step_avg:41.41ms
step:1005/2150 train_time:41639ms step_avg:41.43ms
step:1006/2150 train_time:41699ms step_avg:41.45ms
step:1007/2150 train_time:41760ms step_avg:41.47ms
step:1008/2150 train_time:41818ms step_avg:41.49ms
step:1009/2150 train_time:41879ms step_avg:41.51ms
step:1010/2150 train_time:41938ms step_avg:41.52ms
step:1011/2150 train_time:41998ms step_avg:41.54ms
step:1012/2150 train_time:42057ms step_avg:41.56ms
step:1013/2150 train_time:42117ms step_avg:41.58ms
step:1014/2150 train_time:42177ms step_avg:41.59ms
step:1015/2150 train_time:42238ms step_avg:41.61ms
step:1016/2150 train_time:42298ms step_avg:41.63ms
step:1017/2150 train_time:42360ms step_avg:41.65ms
step:1018/2150 train_time:42420ms step_avg:41.67ms
step:1019/2150 train_time:42484ms step_avg:41.69ms
step:1020/2150 train_time:42545ms step_avg:41.71ms
step:1021/2150 train_time:42607ms step_avg:41.73ms
step:1022/2150 train_time:42666ms step_avg:41.75ms
step:1023/2150 train_time:42727ms step_avg:41.77ms
step:1024/2150 train_time:42786ms step_avg:41.78ms
step:1025/2150 train_time:42847ms step_avg:41.80ms
step:1026/2150 train_time:42906ms step_avg:41.82ms
step:1027/2150 train_time:42968ms step_avg:41.84ms
step:1028/2150 train_time:43027ms step_avg:41.86ms
step:1029/2150 train_time:43089ms step_avg:41.87ms
step:1030/2150 train_time:43148ms step_avg:41.89ms
step:1031/2150 train_time:43208ms step_avg:41.91ms
step:1032/2150 train_time:43267ms step_avg:41.93ms
step:1033/2150 train_time:43329ms step_avg:41.95ms
step:1034/2150 train_time:43389ms step_avg:41.96ms
step:1035/2150 train_time:43452ms step_avg:41.98ms
step:1036/2150 train_time:43514ms step_avg:42.00ms
step:1037/2150 train_time:43577ms step_avg:42.02ms
step:1038/2150 train_time:43637ms step_avg:42.04ms
step:1039/2150 train_time:43699ms step_avg:42.06ms
step:1040/2150 train_time:43758ms step_avg:42.08ms
step:1041/2150 train_time:43820ms step_avg:42.09ms
step:1042/2150 train_time:43878ms step_avg:42.11ms
step:1043/2150 train_time:43940ms step_avg:42.13ms
step:1044/2150 train_time:43999ms step_avg:42.14ms
step:1045/2150 train_time:44060ms step_avg:42.16ms
step:1046/2150 train_time:44119ms step_avg:42.18ms
step:1047/2150 train_time:44179ms step_avg:42.20ms
step:1048/2150 train_time:44238ms step_avg:42.21ms
step:1049/2150 train_time:44299ms step_avg:42.23ms
step:1050/2150 train_time:44359ms step_avg:42.25ms
step:1051/2150 train_time:44421ms step_avg:42.27ms
step:1052/2150 train_time:44481ms step_avg:42.28ms
step:1053/2150 train_time:44543ms step_avg:42.30ms
step:1054/2150 train_time:44603ms step_avg:42.32ms
step:1055/2150 train_time:44665ms step_avg:42.34ms
step:1056/2150 train_time:44724ms step_avg:42.35ms
step:1057/2150 train_time:44786ms step_avg:42.37ms
step:1058/2150 train_time:44845ms step_avg:42.39ms
step:1059/2150 train_time:44906ms step_avg:42.40ms
step:1060/2150 train_time:44966ms step_avg:42.42ms
step:1061/2150 train_time:45027ms step_avg:42.44ms
step:1062/2150 train_time:45087ms step_avg:42.45ms
step:1063/2150 train_time:45148ms step_avg:42.47ms
step:1064/2150 train_time:45207ms step_avg:42.49ms
step:1065/2150 train_time:45268ms step_avg:42.51ms
step:1066/2150 train_time:45328ms step_avg:42.52ms
step:1067/2150 train_time:45390ms step_avg:42.54ms
step:1068/2150 train_time:45450ms step_avg:42.56ms
step:1069/2150 train_time:45512ms step_avg:42.57ms
step:1070/2150 train_time:45571ms step_avg:42.59ms
step:1071/2150 train_time:45633ms step_avg:42.61ms
step:1072/2150 train_time:45694ms step_avg:42.62ms
step:1073/2150 train_time:45756ms step_avg:42.64ms
step:1074/2150 train_time:45816ms step_avg:42.66ms
step:1075/2150 train_time:45877ms step_avg:42.68ms
step:1076/2150 train_time:45937ms step_avg:42.69ms
step:1077/2150 train_time:45998ms step_avg:42.71ms
step:1078/2150 train_time:46058ms step_avg:42.73ms
step:1079/2150 train_time:46119ms step_avg:42.74ms
step:1080/2150 train_time:46178ms step_avg:42.76ms
step:1081/2150 train_time:46239ms step_avg:42.77ms
step:1082/2150 train_time:46298ms step_avg:42.79ms
step:1083/2150 train_time:46360ms step_avg:42.81ms
step:1084/2150 train_time:46419ms step_avg:42.82ms
step:1085/2150 train_time:46481ms step_avg:42.84ms
step:1086/2150 train_time:46540ms step_avg:42.85ms
step:1087/2150 train_time:46602ms step_avg:42.87ms
step:1088/2150 train_time:46662ms step_avg:42.89ms
step:1089/2150 train_time:46723ms step_avg:42.90ms
step:1090/2150 train_time:46783ms step_avg:42.92ms
step:1091/2150 train_time:46844ms step_avg:42.94ms
step:1092/2150 train_time:46904ms step_avg:42.95ms
step:1093/2150 train_time:46965ms step_avg:42.97ms
step:1094/2150 train_time:47024ms step_avg:42.98ms
step:1095/2150 train_time:47085ms step_avg:43.00ms
step:1096/2150 train_time:47145ms step_avg:43.02ms
step:1097/2150 train_time:47206ms step_avg:43.03ms
step:1098/2150 train_time:47266ms step_avg:43.05ms
step:1099/2150 train_time:47328ms step_avg:43.06ms
step:1100/2150 train_time:47387ms step_avg:43.08ms
step:1101/2150 train_time:47449ms step_avg:43.10ms
step:1102/2150 train_time:47508ms step_avg:43.11ms
step:1103/2150 train_time:47570ms step_avg:43.13ms
step:1104/2150 train_time:47630ms step_avg:43.14ms
step:1105/2150 train_time:47692ms step_avg:43.16ms
step:1106/2150 train_time:47752ms step_avg:43.18ms
step:1107/2150 train_time:47814ms step_avg:43.19ms
step:1108/2150 train_time:47873ms step_avg:43.21ms
step:1109/2150 train_time:47935ms step_avg:43.22ms
step:1110/2150 train_time:47994ms step_avg:43.24ms
step:1111/2150 train_time:48056ms step_avg:43.25ms
step:1112/2150 train_time:48116ms step_avg:43.27ms
step:1113/2150 train_time:48178ms step_avg:43.29ms
step:1114/2150 train_time:48238ms step_avg:43.30ms
step:1115/2150 train_time:48299ms step_avg:43.32ms
step:1116/2150 train_time:48358ms step_avg:43.33ms
step:1117/2150 train_time:48420ms step_avg:43.35ms
step:1118/2150 train_time:48481ms step_avg:43.36ms
step:1119/2150 train_time:48542ms step_avg:43.38ms
step:1120/2150 train_time:48602ms step_avg:43.39ms
step:1121/2150 train_time:48663ms step_avg:43.41ms
step:1122/2150 train_time:48722ms step_avg:43.42ms
step:1123/2150 train_time:48783ms step_avg:43.44ms
step:1124/2150 train_time:48844ms step_avg:43.46ms
step:1125/2150 train_time:48905ms step_avg:43.47ms
step:1126/2150 train_time:48965ms step_avg:43.49ms
step:1127/2150 train_time:49026ms step_avg:43.50ms
step:1128/2150 train_time:49086ms step_avg:43.52ms
step:1129/2150 train_time:49147ms step_avg:43.53ms
step:1130/2150 train_time:49206ms step_avg:43.55ms
step:1131/2150 train_time:49268ms step_avg:43.56ms
step:1132/2150 train_time:49328ms step_avg:43.58ms
step:1133/2150 train_time:49389ms step_avg:43.59ms
step:1134/2150 train_time:49448ms step_avg:43.61ms
step:1135/2150 train_time:49510ms step_avg:43.62ms
step:1136/2150 train_time:49570ms step_avg:43.64ms
step:1137/2150 train_time:49632ms step_avg:43.65ms
step:1138/2150 train_time:49692ms step_avg:43.67ms
step:1139/2150 train_time:49754ms step_avg:43.68ms
step:1140/2150 train_time:49814ms step_avg:43.70ms
step:1141/2150 train_time:49876ms step_avg:43.71ms
step:1142/2150 train_time:49935ms step_avg:43.73ms
step:1143/2150 train_time:49997ms step_avg:43.74ms
step:1144/2150 train_time:50057ms step_avg:43.76ms
step:1145/2150 train_time:50119ms step_avg:43.77ms
step:1146/2150 train_time:50178ms step_avg:43.79ms
step:1147/2150 train_time:50240ms step_avg:43.80ms
step:1148/2150 train_time:50299ms step_avg:43.81ms
step:1149/2150 train_time:50360ms step_avg:43.83ms
step:1150/2150 train_time:50419ms step_avg:43.84ms
step:1151/2150 train_time:50481ms step_avg:43.86ms
step:1152/2150 train_time:50540ms step_avg:43.87ms
step:1153/2150 train_time:50602ms step_avg:43.89ms
step:1154/2150 train_time:50662ms step_avg:43.90ms
step:1155/2150 train_time:50723ms step_avg:43.92ms
step:1156/2150 train_time:50783ms step_avg:43.93ms
step:1157/2150 train_time:50844ms step_avg:43.94ms
step:1158/2150 train_time:50904ms step_avg:43.96ms
step:1159/2150 train_time:50965ms step_avg:43.97ms
step:1160/2150 train_time:51025ms step_avg:43.99ms
step:1161/2150 train_time:51087ms step_avg:44.00ms
step:1162/2150 train_time:51147ms step_avg:44.02ms
step:1163/2150 train_time:51208ms step_avg:44.03ms
step:1164/2150 train_time:51268ms step_avg:44.04ms
step:1165/2150 train_time:51330ms step_avg:44.06ms
step:1166/2150 train_time:51390ms step_avg:44.07ms
step:1167/2150 train_time:51451ms step_avg:44.09ms
step:1168/2150 train_time:51510ms step_avg:44.10ms
step:1169/2150 train_time:51572ms step_avg:44.12ms
step:1170/2150 train_time:51632ms step_avg:44.13ms
step:1171/2150 train_time:51693ms step_avg:44.14ms
step:1172/2150 train_time:51753ms step_avg:44.16ms
step:1173/2150 train_time:51816ms step_avg:44.17ms
step:1174/2150 train_time:51876ms step_avg:44.19ms
step:1175/2150 train_time:51937ms step_avg:44.20ms
step:1176/2150 train_time:51997ms step_avg:44.21ms
step:1177/2150 train_time:52059ms step_avg:44.23ms
step:1178/2150 train_time:52118ms step_avg:44.24ms
step:1179/2150 train_time:52179ms step_avg:44.26ms
step:1180/2150 train_time:52238ms step_avg:44.27ms
step:1181/2150 train_time:52300ms step_avg:44.28ms
step:1182/2150 train_time:52359ms step_avg:44.30ms
step:1183/2150 train_time:52421ms step_avg:44.31ms
step:1184/2150 train_time:52479ms step_avg:44.32ms
step:1185/2150 train_time:52540ms step_avg:44.34ms
step:1186/2150 train_time:52600ms step_avg:44.35ms
step:1187/2150 train_time:52661ms step_avg:44.37ms
step:1188/2150 train_time:52721ms step_avg:44.38ms
step:1189/2150 train_time:52783ms step_avg:44.39ms
step:1190/2150 train_time:52843ms step_avg:44.41ms
step:1191/2150 train_time:52905ms step_avg:44.42ms
step:1192/2150 train_time:52964ms step_avg:44.43ms
step:1193/2150 train_time:53026ms step_avg:44.45ms
step:1194/2150 train_time:53085ms step_avg:44.46ms
step:1195/2150 train_time:53146ms step_avg:44.47ms
step:1196/2150 train_time:53206ms step_avg:44.49ms
step:1197/2150 train_time:53268ms step_avg:44.50ms
step:1198/2150 train_time:53328ms step_avg:44.51ms
step:1199/2150 train_time:53390ms step_avg:44.53ms
step:1200/2150 train_time:53450ms step_avg:44.54ms
step:1201/2150 train_time:53512ms step_avg:44.56ms
step:1202/2150 train_time:53571ms step_avg:44.57ms
step:1203/2150 train_time:53633ms step_avg:44.58ms
step:1204/2150 train_time:53693ms step_avg:44.60ms
step:1205/2150 train_time:53755ms step_avg:44.61ms
step:1206/2150 train_time:53815ms step_avg:44.62ms
step:1207/2150 train_time:53876ms step_avg:44.64ms
step:1208/2150 train_time:53935ms step_avg:44.65ms
step:1209/2150 train_time:53998ms step_avg:44.66ms
step:1210/2150 train_time:54057ms step_avg:44.68ms
step:1211/2150 train_time:54119ms step_avg:44.69ms
step:1212/2150 train_time:54179ms step_avg:44.70ms
step:1213/2150 train_time:54240ms step_avg:44.72ms
step:1214/2150 train_time:54299ms step_avg:44.73ms
step:1215/2150 train_time:54361ms step_avg:44.74ms
step:1216/2150 train_time:54420ms step_avg:44.75ms
step:1217/2150 train_time:54482ms step_avg:44.77ms
step:1218/2150 train_time:54541ms step_avg:44.78ms
step:1219/2150 train_time:54603ms step_avg:44.79ms
step:1220/2150 train_time:54663ms step_avg:44.81ms
step:1221/2150 train_time:54724ms step_avg:44.82ms
step:1222/2150 train_time:54784ms step_avg:44.83ms
step:1223/2150 train_time:54846ms step_avg:44.85ms
step:1224/2150 train_time:54906ms step_avg:44.86ms
step:1225/2150 train_time:54967ms step_avg:44.87ms
step:1226/2150 train_time:55027ms step_avg:44.88ms
step:1227/2150 train_time:55089ms step_avg:44.90ms
step:1228/2150 train_time:55150ms step_avg:44.91ms
step:1229/2150 train_time:55212ms step_avg:44.92ms
step:1230/2150 train_time:55271ms step_avg:44.94ms
step:1231/2150 train_time:55334ms step_avg:44.95ms
step:1232/2150 train_time:55393ms step_avg:44.96ms
step:1233/2150 train_time:55455ms step_avg:44.98ms
step:1234/2150 train_time:55515ms step_avg:44.99ms
step:1235/2150 train_time:55577ms step_avg:45.00ms
step:1236/2150 train_time:55637ms step_avg:45.01ms
step:1237/2150 train_time:55699ms step_avg:45.03ms
step:1238/2150 train_time:55758ms step_avg:45.04ms
step:1239/2150 train_time:55820ms step_avg:45.05ms
step:1240/2150 train_time:55880ms step_avg:45.06ms
step:1241/2150 train_time:55941ms step_avg:45.08ms
step:1242/2150 train_time:56001ms step_avg:45.09ms
step:1243/2150 train_time:56062ms step_avg:45.10ms
step:1244/2150 train_time:56122ms step_avg:45.11ms
step:1245/2150 train_time:56183ms step_avg:45.13ms
step:1246/2150 train_time:56243ms step_avg:45.14ms
step:1247/2150 train_time:56304ms step_avg:45.15ms
step:1248/2150 train_time:56364ms step_avg:45.16ms
step:1249/2150 train_time:56426ms step_avg:45.18ms
step:1250/2150 train_time:56485ms step_avg:45.19ms
step:1250/2150 val_loss:3.5980 train_time:56549ms step_avg:45.24ms
step:1251/2150 train_time:56571ms step_avg:45.22ms
step:1252/2150 train_time:56608ms step_avg:45.21ms
step:1253/2150 train_time:56674ms step_avg:45.23ms
step:1254/2150 train_time:56734ms step_avg:45.24ms
step:1255/2150 train_time:56795ms step_avg:45.25ms
step:1256/2150 train_time:56855ms step_avg:45.27ms
step:1257/2150 train_time:56915ms step_avg:45.28ms
step:1258/2150 train_time:56975ms step_avg:45.29ms
step:1259/2150 train_time:57035ms step_avg:45.30ms
step:1260/2150 train_time:57095ms step_avg:45.31ms
step:1261/2150 train_time:57155ms step_avg:45.32ms
step:1262/2150 train_time:57214ms step_avg:45.34ms
step:1263/2150 train_time:57274ms step_avg:45.35ms
step:1264/2150 train_time:57334ms step_avg:45.36ms
step:1265/2150 train_time:57394ms step_avg:45.37ms
step:1266/2150 train_time:57455ms step_avg:45.38ms
step:1267/2150 train_time:57518ms step_avg:45.40ms
step:1268/2150 train_time:57579ms step_avg:45.41ms
step:1269/2150 train_time:57643ms step_avg:45.42ms
step:1270/2150 train_time:57703ms step_avg:45.44ms
step:1271/2150 train_time:57765ms step_avg:45.45ms
step:1272/2150 train_time:57825ms step_avg:45.46ms
step:1273/2150 train_time:57887ms step_avg:45.47ms
step:1274/2150 train_time:57946ms step_avg:45.48ms
step:1275/2150 train_time:58008ms step_avg:45.50ms
step:1276/2150 train_time:58066ms step_avg:45.51ms
step:1277/2150 train_time:58126ms step_avg:45.52ms
step:1278/2150 train_time:58185ms step_avg:45.53ms
step:1279/2150 train_time:58246ms step_avg:45.54ms
step:1280/2150 train_time:58305ms step_avg:45.55ms
step:1281/2150 train_time:58366ms step_avg:45.56ms
step:1282/2150 train_time:58425ms step_avg:45.57ms
step:1283/2150 train_time:58487ms step_avg:45.59ms
step:1284/2150 train_time:58547ms step_avg:45.60ms
step:1285/2150 train_time:58610ms step_avg:45.61ms
step:1286/2150 train_time:58670ms step_avg:45.62ms
step:1287/2150 train_time:58732ms step_avg:45.63ms
step:1288/2150 train_time:58791ms step_avg:45.65ms
step:1289/2150 train_time:58853ms step_avg:45.66ms
step:1290/2150 train_time:58912ms step_avg:45.67ms
step:1291/2150 train_time:58973ms step_avg:45.68ms
step:1292/2150 train_time:59033ms step_avg:45.69ms
step:1293/2150 train_time:59093ms step_avg:45.70ms
step:1294/2150 train_time:59153ms step_avg:45.71ms
step:1295/2150 train_time:59213ms step_avg:45.72ms
step:1296/2150 train_time:59272ms step_avg:45.73ms
step:1297/2150 train_time:59333ms step_avg:45.75ms
step:1298/2150 train_time:59393ms step_avg:45.76ms
step:1299/2150 train_time:59454ms step_avg:45.77ms
step:1300/2150 train_time:59514ms step_avg:45.78ms
step:1301/2150 train_time:59576ms step_avg:45.79ms
step:1302/2150 train_time:59636ms step_avg:45.80ms
step:1303/2150 train_time:59698ms step_avg:45.82ms
step:1304/2150 train_time:59759ms step_avg:45.83ms
step:1305/2150 train_time:59821ms step_avg:45.84ms
step:1306/2150 train_time:59881ms step_avg:45.85ms
step:1307/2150 train_time:59943ms step_avg:45.86ms
step:1308/2150 train_time:60002ms step_avg:45.87ms
step:1309/2150 train_time:60063ms step_avg:45.88ms
step:1310/2150 train_time:60123ms step_avg:45.90ms
step:1311/2150 train_time:60184ms step_avg:45.91ms
step:1312/2150 train_time:60244ms step_avg:45.92ms
step:1313/2150 train_time:60305ms step_avg:45.93ms
step:1314/2150 train_time:60365ms step_avg:45.94ms
step:1315/2150 train_time:60426ms step_avg:45.95ms
step:1316/2150 train_time:60486ms step_avg:45.96ms
step:1317/2150 train_time:60548ms step_avg:45.97ms
step:1318/2150 train_time:60608ms step_avg:45.98ms
step:1319/2150 train_time:60670ms step_avg:46.00ms
step:1320/2150 train_time:60730ms step_avg:46.01ms
step:1321/2150 train_time:60791ms step_avg:46.02ms
step:1322/2150 train_time:60850ms step_avg:46.03ms
step:1323/2150 train_time:60911ms step_avg:46.04ms
step:1324/2150 train_time:60970ms step_avg:46.05ms
step:1325/2150 train_time:61032ms step_avg:46.06ms
step:1326/2150 train_time:61092ms step_avg:46.07ms
step:1327/2150 train_time:61153ms step_avg:46.08ms
step:1328/2150 train_time:61213ms step_avg:46.09ms
step:1329/2150 train_time:61273ms step_avg:46.10ms
step:1330/2150 train_time:61333ms step_avg:46.11ms
step:1331/2150 train_time:61393ms step_avg:46.13ms
step:1332/2150 train_time:61453ms step_avg:46.14ms
step:1333/2150 train_time:61514ms step_avg:46.15ms
step:1334/2150 train_time:61574ms step_avg:46.16ms
step:1335/2150 train_time:61635ms step_avg:46.17ms
step:1336/2150 train_time:61695ms step_avg:46.18ms
step:1337/2150 train_time:61757ms step_avg:46.19ms
step:1338/2150 train_time:61816ms step_avg:46.20ms
step:1339/2150 train_time:61878ms step_avg:46.21ms
step:1340/2150 train_time:61938ms step_avg:46.22ms
step:1341/2150 train_time:62000ms step_avg:46.23ms
step:1342/2150 train_time:62060ms step_avg:46.24ms
step:1343/2150 train_time:62121ms step_avg:46.26ms
step:1344/2150 train_time:62181ms step_avg:46.27ms
step:1345/2150 train_time:62241ms step_avg:46.28ms
step:1346/2150 train_time:62301ms step_avg:46.29ms
step:1347/2150 train_time:62363ms step_avg:46.30ms
step:1348/2150 train_time:62423ms step_avg:46.31ms
step:1349/2150 train_time:62485ms step_avg:46.32ms
step:1350/2150 train_time:62544ms step_avg:46.33ms
step:1351/2150 train_time:62606ms step_avg:46.34ms
step:1352/2150 train_time:62665ms step_avg:46.35ms
step:1353/2150 train_time:62727ms step_avg:46.36ms
step:1354/2150 train_time:62788ms step_avg:46.37ms
step:1355/2150 train_time:62849ms step_avg:46.38ms
step:1356/2150 train_time:62909ms step_avg:46.39ms
step:1357/2150 train_time:62970ms step_avg:46.40ms
step:1358/2150 train_time:63030ms step_avg:46.41ms
step:1359/2150 train_time:63091ms step_avg:46.42ms
step:1360/2150 train_time:63150ms step_avg:46.43ms
step:1361/2150 train_time:63211ms step_avg:46.44ms
step:1362/2150 train_time:63271ms step_avg:46.45ms
step:1363/2150 train_time:63332ms step_avg:46.47ms
step:1364/2150 train_time:63392ms step_avg:46.47ms
step:1365/2150 train_time:63454ms step_avg:46.49ms
step:1366/2150 train_time:63514ms step_avg:46.50ms
step:1367/2150 train_time:63574ms step_avg:46.51ms
step:1368/2150 train_time:63634ms step_avg:46.52ms
step:1369/2150 train_time:63695ms step_avg:46.53ms
step:1370/2150 train_time:63755ms step_avg:46.54ms
step:1371/2150 train_time:63817ms step_avg:46.55ms
step:1372/2150 train_time:63877ms step_avg:46.56ms
step:1373/2150 train_time:63939ms step_avg:46.57ms
step:1374/2150 train_time:63999ms step_avg:46.58ms
step:1375/2150 train_time:64061ms step_avg:46.59ms
step:1376/2150 train_time:64121ms step_avg:46.60ms
step:1377/2150 train_time:64182ms step_avg:46.61ms
step:1378/2150 train_time:64241ms step_avg:46.62ms
step:1379/2150 train_time:64303ms step_avg:46.63ms
step:1380/2150 train_time:64363ms step_avg:46.64ms
step:1381/2150 train_time:64425ms step_avg:46.65ms
step:1382/2150 train_time:64486ms step_avg:46.66ms
step:1383/2150 train_time:64548ms step_avg:46.67ms
step:1384/2150 train_time:64607ms step_avg:46.68ms
step:1385/2150 train_time:64669ms step_avg:46.69ms
step:1386/2150 train_time:64729ms step_avg:46.70ms
step:1387/2150 train_time:64791ms step_avg:46.71ms
step:1388/2150 train_time:64850ms step_avg:46.72ms
step:1389/2150 train_time:64912ms step_avg:46.73ms
step:1390/2150 train_time:64972ms step_avg:46.74ms
step:1391/2150 train_time:65033ms step_avg:46.75ms
step:1392/2150 train_time:65093ms step_avg:46.76ms
step:1393/2150 train_time:65154ms step_avg:46.77ms
step:1394/2150 train_time:65213ms step_avg:46.78ms
step:1395/2150 train_time:65274ms step_avg:46.79ms
step:1396/2150 train_time:65335ms step_avg:46.80ms
step:1397/2150 train_time:65396ms step_avg:46.81ms
step:1398/2150 train_time:65456ms step_avg:46.82ms
step:1399/2150 train_time:65518ms step_avg:46.83ms
step:1400/2150 train_time:65577ms step_avg:46.84ms
step:1401/2150 train_time:65638ms step_avg:46.85ms
step:1402/2150 train_time:65698ms step_avg:46.86ms
step:1403/2150 train_time:65760ms step_avg:46.87ms
step:1404/2150 train_time:65820ms step_avg:46.88ms
step:1405/2150 train_time:65882ms step_avg:46.89ms
step:1406/2150 train_time:65942ms step_avg:46.90ms
step:1407/2150 train_time:66004ms step_avg:46.91ms
step:1408/2150 train_time:66064ms step_avg:46.92ms
step:1409/2150 train_time:66156ms step_avg:46.95ms
step:1410/2150 train_time:66241ms step_avg:46.98ms
step:1411/2150 train_time:66331ms step_avg:47.01ms
step:1412/2150 train_time:66419ms step_avg:47.04ms
step:1413/2150 train_time:66509ms step_avg:47.07ms
step:1414/2150 train_time:66595ms step_avg:47.10ms
step:1415/2150 train_time:66685ms step_avg:47.13ms
step:1416/2150 train_time:66773ms step_avg:47.16ms
step:1417/2150 train_time:66864ms step_avg:47.19ms
step:1418/2150 train_time:66952ms step_avg:47.22ms
step:1419/2150 train_time:67041ms step_avg:47.25ms
step:1420/2150 train_time:67129ms step_avg:47.27ms
step:1421/2150 train_time:67219ms step_avg:47.30ms
step:1422/2150 train_time:67307ms step_avg:47.33ms
step:1423/2150 train_time:67397ms step_avg:47.36ms
step:1424/2150 train_time:67485ms step_avg:47.39ms
step:1425/2150 train_time:67574ms step_avg:47.42ms
step:1426/2150 train_time:67664ms step_avg:47.45ms
step:1427/2150 train_time:67752ms step_avg:47.48ms
step:1428/2150 train_time:67841ms step_avg:47.51ms
step:1429/2150 train_time:67932ms step_avg:47.54ms
step:1430/2150 train_time:68019ms step_avg:47.57ms
step:1431/2150 train_time:68109ms step_avg:47.60ms
step:1432/2150 train_time:68197ms step_avg:47.62ms
step:1433/2150 train_time:68286ms step_avg:47.65ms
step:1434/2150 train_time:68374ms step_avg:47.68ms
step:1435/2150 train_time:68465ms step_avg:47.71ms
step:1436/2150 train_time:68553ms step_avg:47.74ms
step:1437/2150 train_time:68643ms step_avg:47.77ms
step:1438/2150 train_time:68731ms step_avg:47.80ms
step:1439/2150 train_time:68821ms step_avg:47.83ms
step:1440/2150 train_time:68908ms step_avg:47.85ms
step:1441/2150 train_time:68998ms step_avg:47.88ms
step:1442/2150 train_time:69085ms step_avg:47.91ms
step:1443/2150 train_time:69175ms step_avg:47.94ms
step:1444/2150 train_time:69263ms step_avg:47.97ms
step:1445/2150 train_time:69352ms step_avg:47.99ms
step:1446/2150 train_time:69440ms step_avg:48.02ms
step:1447/2150 train_time:69530ms step_avg:48.05ms
step:1448/2150 train_time:69618ms step_avg:48.08ms
step:1449/2150 train_time:69706ms step_avg:48.11ms
step:1450/2150 train_time:69794ms step_avg:48.13ms
step:1451/2150 train_time:69885ms step_avg:48.16ms
step:1452/2150 train_time:69974ms step_avg:48.19ms
step:1453/2150 train_time:70063ms step_avg:48.22ms
step:1454/2150 train_time:70151ms step_avg:48.25ms
step:1455/2150 train_time:70241ms step_avg:48.28ms
step:1456/2150 train_time:70328ms step_avg:48.30ms
step:1457/2150 train_time:70418ms step_avg:48.33ms
step:1458/2150 train_time:70505ms step_avg:48.36ms
step:1459/2150 train_time:70595ms step_avg:48.39ms
step:1460/2150 train_time:70683ms step_avg:48.41ms
step:1461/2150 train_time:70773ms step_avg:48.44ms
step:1462/2150 train_time:70861ms step_avg:48.47ms
step:1463/2150 train_time:70949ms step_avg:48.50ms
step:1464/2150 train_time:71037ms step_avg:48.52ms
step:1465/2150 train_time:71127ms step_avg:48.55ms
step:1466/2150 train_time:71215ms step_avg:48.58ms
step:1467/2150 train_time:71305ms step_avg:48.61ms
step:1468/2150 train_time:71393ms step_avg:48.63ms
step:1469/2150 train_time:71482ms step_avg:48.66ms
step:1470/2150 train_time:71570ms step_avg:48.69ms
step:1471/2150 train_time:71660ms step_avg:48.72ms
step:1472/2150 train_time:71747ms step_avg:48.74ms
step:1473/2150 train_time:71837ms step_avg:48.77ms
step:1474/2150 train_time:71925ms step_avg:48.80ms
step:1475/2150 train_time:72015ms step_avg:48.82ms
step:1476/2150 train_time:72103ms step_avg:48.85ms
step:1477/2150 train_time:72193ms step_avg:48.88ms
step:1478/2150 train_time:72281ms step_avg:48.90ms
step:1479/2150 train_time:72370ms step_avg:48.93ms
step:1480/2150 train_time:72457ms step_avg:48.96ms
step:1481/2150 train_time:72546ms step_avg:48.98ms
step:1482/2150 train_time:72633ms step_avg:49.01ms
step:1483/2150 train_time:72723ms step_avg:49.04ms
step:1484/2150 train_time:72811ms step_avg:49.06ms
step:1485/2150 train_time:72901ms step_avg:49.09ms
step:1486/2150 train_time:72988ms step_avg:49.12ms
step:1487/2150 train_time:73077ms step_avg:49.14ms
step:1488/2150 train_time:73166ms step_avg:49.17ms
step:1489/2150 train_time:73255ms step_avg:49.20ms
step:1490/2150 train_time:73343ms step_avg:49.22ms
step:1491/2150 train_time:73433ms step_avg:49.25ms
step:1492/2150 train_time:73519ms step_avg:49.28ms
step:1493/2150 train_time:73608ms step_avg:49.30ms
step:1494/2150 train_time:73696ms step_avg:49.33ms
step:1495/2150 train_time:73786ms step_avg:49.35ms
step:1496/2150 train_time:73873ms step_avg:49.38ms
step:1497/2150 train_time:73963ms step_avg:49.41ms
step:1498/2150 train_time:74051ms step_avg:49.43ms
step:1499/2150 train_time:74140ms step_avg:49.46ms
step:1500/2150 train_time:74228ms step_avg:49.49ms
step:1500/2150 val_loss:3.4939 train_time:74320ms step_avg:49.55ms
step:1501/2150 train_time:74343ms step_avg:49.53ms
step:1502/2150 train_time:74409ms step_avg:49.54ms
step:1503/2150 train_time:74505ms step_avg:49.57ms
step:1504/2150 train_time:74593ms step_avg:49.60ms
step:1505/2150 train_time:74680ms step_avg:49.62ms
step:1506/2150 train_time:74766ms step_avg:49.65ms
step:1507/2150 train_time:74854ms step_avg:49.67ms
step:1508/2150 train_time:74939ms step_avg:49.69ms
step:1509/2150 train_time:75027ms step_avg:49.72ms
step:1510/2150 train_time:75113ms step_avg:49.74ms
step:1511/2150 train_time:75204ms step_avg:49.77ms
step:1512/2150 train_time:75298ms step_avg:49.80ms
step:1513/2150 train_time:75391ms step_avg:49.83ms
step:1514/2150 train_time:75482ms step_avg:49.86ms
step:1515/2150 train_time:75571ms step_avg:49.88ms
step:1516/2150 train_time:75659ms step_avg:49.91ms
step:1517/2150 train_time:75746ms step_avg:49.93ms
step:1518/2150 train_time:75832ms step_avg:49.96ms
step:1519/2150 train_time:75919ms step_avg:49.98ms
step:1520/2150 train_time:76005ms step_avg:50.00ms
step:1521/2150 train_time:76092ms step_avg:50.03ms
step:1522/2150 train_time:76181ms step_avg:50.05ms
step:1523/2150 train_time:76274ms step_avg:50.08ms
step:1524/2150 train_time:76364ms step_avg:50.11ms
step:1525/2150 train_time:76454ms step_avg:50.13ms
step:1526/2150 train_time:76542ms step_avg:50.16ms
step:1527/2150 train_time:76631ms step_avg:50.18ms
step:1528/2150 train_time:76719ms step_avg:50.21ms
step:1529/2150 train_time:76807ms step_avg:50.23ms
step:1530/2150 train_time:76893ms step_avg:50.26ms
step:1531/2150 train_time:76981ms step_avg:50.28ms
step:1532/2150 train_time:77069ms step_avg:50.31ms
step:1533/2150 train_time:77158ms step_avg:50.33ms
step:1534/2150 train_time:77246ms step_avg:50.36ms
step:1535/2150 train_time:77337ms step_avg:50.38ms
step:1536/2150 train_time:77427ms step_avg:50.41ms
step:1537/2150 train_time:77517ms step_avg:50.43ms
step:1538/2150 train_time:77605ms step_avg:50.46ms
step:1539/2150 train_time:77694ms step_avg:50.48ms
step:1540/2150 train_time:77782ms step_avg:50.51ms
step:1541/2150 train_time:77870ms step_avg:50.53ms
step:1542/2150 train_time:77956ms step_avg:50.56ms
step:1543/2150 train_time:78046ms step_avg:50.58ms
step:1544/2150 train_time:78133ms step_avg:50.60ms
step:1545/2150 train_time:78222ms step_avg:50.63ms
step:1546/2150 train_time:78312ms step_avg:50.65ms
step:1547/2150 train_time:78402ms step_avg:50.68ms
step:1548/2150 train_time:78491ms step_avg:50.70ms
step:1549/2150 train_time:78579ms step_avg:50.73ms
step:1550/2150 train_time:78667ms step_avg:50.75ms
step:1551/2150 train_time:78756ms step_avg:50.78ms
step:1552/2150 train_time:78843ms step_avg:50.80ms
step:1553/2150 train_time:78931ms step_avg:50.82ms
step:1554/2150 train_time:79018ms step_avg:50.85ms
step:1555/2150 train_time:79107ms step_avg:50.87ms
step:1556/2150 train_time:79195ms step_avg:50.90ms
step:1557/2150 train_time:79285ms step_avg:50.92ms
step:1558/2150 train_time:79373ms step_avg:50.95ms
step:1559/2150 train_time:79463ms step_avg:50.97ms
step:1560/2150 train_time:79551ms step_avg:50.99ms
step:1561/2150 train_time:79640ms step_avg:51.02ms
step:1562/2150 train_time:79727ms step_avg:51.04ms
step:1563/2150 train_time:79815ms step_avg:51.07ms
step:1564/2150 train_time:79903ms step_avg:51.09ms
step:1565/2150 train_time:79991ms step_avg:51.11ms
step:1566/2150 train_time:80079ms step_avg:51.14ms
step:1567/2150 train_time:80168ms step_avg:51.16ms
step:1568/2150 train_time:80256ms step_avg:51.18ms
step:1569/2150 train_time:80345ms step_avg:51.21ms
step:1570/2150 train_time:80434ms step_avg:51.23ms
step:1571/2150 train_time:80523ms step_avg:51.26ms
step:1572/2150 train_time:80611ms step_avg:51.28ms
step:1573/2150 train_time:80700ms step_avg:51.30ms
step:1574/2150 train_time:80787ms step_avg:51.33ms
step:1575/2150 train_time:80876ms step_avg:51.35ms
step:1576/2150 train_time:80963ms step_avg:51.37ms
step:1577/2150 train_time:81053ms step_avg:51.40ms
step:1578/2150 train_time:81140ms step_avg:51.42ms
step:1579/2150 train_time:81230ms step_avg:51.44ms
step:1580/2150 train_time:81319ms step_avg:51.47ms
step:1581/2150 train_time:81409ms step_avg:51.49ms
step:1582/2150 train_time:81497ms step_avg:51.51ms
step:1583/2150 train_time:81587ms step_avg:51.54ms
step:1584/2150 train_time:81675ms step_avg:51.56ms
step:1585/2150 train_time:81764ms step_avg:51.59ms
step:1586/2150 train_time:81851ms step_avg:51.61ms
step:1587/2150 train_time:81939ms step_avg:51.63ms
step:1588/2150 train_time:82026ms step_avg:51.65ms
step:1589/2150 train_time:82116ms step_avg:51.68ms
step:1590/2150 train_time:82204ms step_avg:51.70ms
step:1591/2150 train_time:82293ms step_avg:51.72ms
step:1592/2150 train_time:82381ms step_avg:51.75ms
step:1593/2150 train_time:82472ms step_avg:51.77ms
step:1594/2150 train_time:82559ms step_avg:51.79ms
step:1595/2150 train_time:82648ms step_avg:51.82ms
step:1596/2150 train_time:82736ms step_avg:51.84ms
step:1597/2150 train_time:82824ms step_avg:51.86ms
step:1598/2150 train_time:82913ms step_avg:51.89ms
step:1599/2150 train_time:83002ms step_avg:51.91ms
step:1600/2150 train_time:83089ms step_avg:51.93ms
step:1601/2150 train_time:83179ms step_avg:51.95ms
step:1602/2150 train_time:83267ms step_avg:51.98ms
step:1603/2150 train_time:83356ms step_avg:52.00ms
step:1604/2150 train_time:83444ms step_avg:52.02ms
step:1605/2150 train_time:83533ms step_avg:52.05ms
step:1606/2150 train_time:83620ms step_avg:52.07ms
step:1607/2150 train_time:83710ms step_avg:52.09ms
step:1608/2150 train_time:83798ms step_avg:52.11ms
step:1609/2150 train_time:83888ms step_avg:52.14ms
step:1610/2150 train_time:83976ms step_avg:52.16ms
step:1611/2150 train_time:84065ms step_avg:52.18ms
step:1612/2150 train_time:84152ms step_avg:52.20ms
step:1613/2150 train_time:84240ms step_avg:52.23ms
step:1614/2150 train_time:84329ms step_avg:52.25ms
step:1615/2150 train_time:84418ms step_avg:52.27ms
step:1616/2150 train_time:84507ms step_avg:52.29ms
step:1617/2150 train_time:84596ms step_avg:52.32ms
step:1618/2150 train_time:84684ms step_avg:52.34ms
step:1619/2150 train_time:84773ms step_avg:52.36ms
step:1620/2150 train_time:84860ms step_avg:52.38ms
step:1621/2150 train_time:84951ms step_avg:52.41ms
step:1622/2150 train_time:85038ms step_avg:52.43ms
step:1623/2150 train_time:85127ms step_avg:52.45ms
step:1624/2150 train_time:85214ms step_avg:52.47ms
step:1625/2150 train_time:85302ms step_avg:52.49ms
step:1626/2150 train_time:85391ms step_avg:52.52ms
step:1627/2150 train_time:85480ms step_avg:52.54ms
step:1628/2150 train_time:85568ms step_avg:52.56ms
step:1629/2150 train_time:85657ms step_avg:52.58ms
step:1630/2150 train_time:85745ms step_avg:52.60ms
step:1631/2150 train_time:85834ms step_avg:52.63ms
step:1632/2150 train_time:85922ms step_avg:52.65ms
step:1633/2150 train_time:86012ms step_avg:52.67ms
step:1634/2150 train_time:86099ms step_avg:52.69ms
step:1635/2150 train_time:86189ms step_avg:52.71ms
step:1636/2150 train_time:86276ms step_avg:52.74ms
step:1637/2150 train_time:86366ms step_avg:52.76ms
step:1638/2150 train_time:86453ms step_avg:52.78ms
step:1639/2150 train_time:86544ms step_avg:52.80ms
step:1640/2150 train_time:86631ms step_avg:52.82ms
step:1641/2150 train_time:86720ms step_avg:52.85ms
step:1642/2150 train_time:86808ms step_avg:52.87ms
step:1643/2150 train_time:86898ms step_avg:52.89ms
step:1644/2150 train_time:86985ms step_avg:52.91ms
step:1645/2150 train_time:87075ms step_avg:52.93ms
step:1646/2150 train_time:87163ms step_avg:52.95ms
step:1647/2150 train_time:87253ms step_avg:52.98ms
step:1648/2150 train_time:87341ms step_avg:53.00ms
step:1649/2150 train_time:87429ms step_avg:53.02ms
step:1650/2150 train_time:87517ms step_avg:53.04ms
step:1651/2150 train_time:87606ms step_avg:53.06ms
step:1652/2150 train_time:87694ms step_avg:53.08ms
step:1653/2150 train_time:87783ms step_avg:53.11ms
step:1654/2150 train_time:87871ms step_avg:53.13ms
step:1655/2150 train_time:87961ms step_avg:53.15ms
step:1656/2150 train_time:88048ms step_avg:53.17ms
step:1657/2150 train_time:88137ms step_avg:53.19ms
step:1658/2150 train_time:88224ms step_avg:53.21ms
step:1659/2150 train_time:88313ms step_avg:53.23ms
step:1660/2150 train_time:88401ms step_avg:53.25ms
step:1661/2150 train_time:88491ms step_avg:53.28ms
step:1662/2150 train_time:88579ms step_avg:53.30ms
step:1663/2150 train_time:88668ms step_avg:53.32ms
step:1664/2150 train_time:88756ms step_avg:53.34ms
step:1665/2150 train_time:88846ms step_avg:53.36ms
step:1666/2150 train_time:88933ms step_avg:53.38ms
step:1667/2150 train_time:89022ms step_avg:53.40ms
step:1668/2150 train_time:89111ms step_avg:53.42ms
step:1669/2150 train_time:89200ms step_avg:53.45ms
step:1670/2150 train_time:89287ms step_avg:53.47ms
step:1671/2150 train_time:89375ms step_avg:53.49ms
step:1672/2150 train_time:89463ms step_avg:53.51ms
step:1673/2150 train_time:89553ms step_avg:53.53ms
step:1674/2150 train_time:89641ms step_avg:53.55ms
step:1675/2150 train_time:89732ms step_avg:53.57ms
step:1676/2150 train_time:89819ms step_avg:53.59ms
step:1677/2150 train_time:89909ms step_avg:53.61ms
step:1678/2150 train_time:89996ms step_avg:53.63ms
step:1679/2150 train_time:90086ms step_avg:53.65ms
step:1680/2150 train_time:90173ms step_avg:53.67ms
step:1681/2150 train_time:90261ms step_avg:53.70ms
step:1682/2150 train_time:90349ms step_avg:53.72ms
step:1683/2150 train_time:90439ms step_avg:53.74ms
step:1684/2150 train_time:90526ms step_avg:53.76ms
step:1685/2150 train_time:90615ms step_avg:53.78ms
step:1686/2150 train_time:90704ms step_avg:53.80ms
step:1687/2150 train_time:90794ms step_avg:53.82ms
step:1688/2150 train_time:90881ms step_avg:53.84ms
step:1689/2150 train_time:90970ms step_avg:53.86ms
step:1690/2150 train_time:91058ms step_avg:53.88ms
step:1691/2150 train_time:91148ms step_avg:53.90ms
step:1692/2150 train_time:91236ms step_avg:53.92ms
step:1693/2150 train_time:91325ms step_avg:53.94ms
step:1694/2150 train_time:91413ms step_avg:53.96ms
step:1695/2150 train_time:91502ms step_avg:53.98ms
step:1696/2150 train_time:91590ms step_avg:54.00ms
step:1697/2150 train_time:91679ms step_avg:54.02ms
step:1698/2150 train_time:91766ms step_avg:54.04ms
step:1699/2150 train_time:91855ms step_avg:54.06ms
step:1700/2150 train_time:91943ms step_avg:54.08ms
step:1701/2150 train_time:92033ms step_avg:54.11ms
step:1702/2150 train_time:92122ms step_avg:54.13ms
step:1703/2150 train_time:92210ms step_avg:54.15ms
step:1704/2150 train_time:92298ms step_avg:54.17ms
step:1705/2150 train_time:92388ms step_avg:54.19ms
step:1706/2150 train_time:92478ms step_avg:54.21ms
step:1707/2150 train_time:92566ms step_avg:54.23ms
step:1708/2150 train_time:92653ms step_avg:54.25ms
step:1709/2150 train_time:92742ms step_avg:54.27ms
step:1710/2150 train_time:92830ms step_avg:54.29ms
step:1711/2150 train_time:92918ms step_avg:54.31ms
step:1712/2150 train_time:93007ms step_avg:54.33ms
step:1713/2150 train_time:93096ms step_avg:54.35ms
step:1714/2150 train_time:93184ms step_avg:54.37ms
step:1715/2150 train_time:93273ms step_avg:54.39ms
step:1716/2150 train_time:93361ms step_avg:54.41ms
step:1717/2150 train_time:93450ms step_avg:54.43ms
step:1718/2150 train_time:93538ms step_avg:54.45ms
step:1719/2150 train_time:93627ms step_avg:54.47ms
step:1720/2150 train_time:93714ms step_avg:54.49ms
step:1721/2150 train_time:93803ms step_avg:54.51ms
step:1722/2150 train_time:93891ms step_avg:54.52ms
step:1723/2150 train_time:93979ms step_avg:54.54ms
step:1724/2150 train_time:94067ms step_avg:54.56ms
step:1725/2150 train_time:94155ms step_avg:54.58ms
step:1726/2150 train_time:94243ms step_avg:54.60ms
step:1727/2150 train_time:94333ms step_avg:54.62ms
step:1728/2150 train_time:94421ms step_avg:54.64ms
step:1729/2150 train_time:94510ms step_avg:54.66ms
step:1730/2150 train_time:94598ms step_avg:54.68ms
step:1731/2150 train_time:94687ms step_avg:54.70ms
step:1732/2150 train_time:94775ms step_avg:54.72ms
step:1733/2150 train_time:94864ms step_avg:54.74ms
step:1734/2150 train_time:94951ms step_avg:54.76ms
step:1735/2150 train_time:95041ms step_avg:54.78ms
step:1736/2150 train_time:95129ms step_avg:54.80ms
step:1737/2150 train_time:95218ms step_avg:54.82ms
step:1738/2150 train_time:95305ms step_avg:54.84ms
step:1739/2150 train_time:95395ms step_avg:54.86ms
step:1740/2150 train_time:95484ms step_avg:54.88ms
step:1741/2150 train_time:95573ms step_avg:54.90ms
step:1742/2150 train_time:95661ms step_avg:54.91ms
step:1743/2150 train_time:95750ms step_avg:54.93ms
step:1744/2150 train_time:95838ms step_avg:54.95ms
step:1745/2150 train_time:95928ms step_avg:54.97ms
step:1746/2150 train_time:96015ms step_avg:54.99ms
step:1747/2150 train_time:96105ms step_avg:55.01ms
step:1748/2150 train_time:96192ms step_avg:55.03ms
step:1749/2150 train_time:96281ms step_avg:55.05ms
step:1750/2150 train_time:96370ms step_avg:55.07ms
step:1750/2150 val_loss:3.3929 train_time:96461ms step_avg:55.12ms
step:1751/2150 train_time:96484ms step_avg:55.10ms
step:1752/2150 train_time:96551ms step_avg:55.11ms
step:1753/2150 train_time:96646ms step_avg:55.13ms
step:1754/2150 train_time:96735ms step_avg:55.15ms
step:1755/2150 train_time:96824ms step_avg:55.17ms
step:1756/2150 train_time:96910ms step_avg:55.19ms
step:1757/2150 train_time:96998ms step_avg:55.21ms
step:1758/2150 train_time:97084ms step_avg:55.22ms
step:1759/2150 train_time:97172ms step_avg:55.24ms
step:1760/2150 train_time:97259ms step_avg:55.26ms
step:1761/2150 train_time:97347ms step_avg:55.28ms
step:1762/2150 train_time:97436ms step_avg:55.30ms
step:1763/2150 train_time:97528ms step_avg:55.32ms
step:1764/2150 train_time:97619ms step_avg:55.34ms
step:1765/2150 train_time:97710ms step_avg:55.36ms
step:1766/2150 train_time:97797ms step_avg:55.38ms
step:1767/2150 train_time:97885ms step_avg:55.40ms
step:1768/2150 train_time:97972ms step_avg:55.41ms
step:1769/2150 train_time:98060ms step_avg:55.43ms
step:1770/2150 train_time:98147ms step_avg:55.45ms
step:1771/2150 train_time:98235ms step_avg:55.47ms
step:1772/2150 train_time:98322ms step_avg:55.49ms
step:1773/2150 train_time:98411ms step_avg:55.51ms
step:1774/2150 train_time:98499ms step_avg:55.52ms
step:1775/2150 train_time:98589ms step_avg:55.54ms
step:1776/2150 train_time:98678ms step_avg:55.56ms
step:1777/2150 train_time:98768ms step_avg:55.58ms
step:1778/2150 train_time:98856ms step_avg:55.60ms
step:1779/2150 train_time:98945ms step_avg:55.62ms
step:1780/2150 train_time:99032ms step_avg:55.64ms
step:1781/2150 train_time:99120ms step_avg:55.65ms
step:1782/2150 train_time:99206ms step_avg:55.67ms
step:1783/2150 train_time:99295ms step_avg:55.69ms
step:1784/2150 train_time:99382ms step_avg:55.71ms
step:1785/2150 train_time:99473ms step_avg:55.73ms
step:1786/2150 train_time:99561ms step_avg:55.75ms
step:1787/2150 train_time:99651ms step_avg:55.76ms
step:1788/2150 train_time:99739ms step_avg:55.78ms
step:1789/2150 train_time:99828ms step_avg:55.80ms
step:1790/2150 train_time:99916ms step_avg:55.82ms
step:1791/2150 train_time:100005ms step_avg:55.84ms
step:1792/2150 train_time:100093ms step_avg:55.86ms
step:1793/2150 train_time:100182ms step_avg:55.87ms
step:1794/2150 train_time:100268ms step_avg:55.89ms
step:1795/2150 train_time:100357ms step_avg:55.91ms
step:1796/2150 train_time:100445ms step_avg:55.93ms
step:1797/2150 train_time:100535ms step_avg:55.95ms
step:1798/2150 train_time:100624ms step_avg:55.96ms
step:1799/2150 train_time:100713ms step_avg:55.98ms
step:1800/2150 train_time:100801ms step_avg:56.00ms
step:1801/2150 train_time:100890ms step_avg:56.02ms
step:1802/2150 train_time:100977ms step_avg:56.04ms
step:1803/2150 train_time:101066ms step_avg:56.05ms
step:1804/2150 train_time:101153ms step_avg:56.07ms
step:1805/2150 train_time:101242ms step_avg:56.09ms
step:1806/2150 train_time:101330ms step_avg:56.11ms
step:1807/2150 train_time:101419ms step_avg:56.13ms
step:1808/2150 train_time:101507ms step_avg:56.14ms
step:1809/2150 train_time:101597ms step_avg:56.16ms
step:1810/2150 train_time:101684ms step_avg:56.18ms
step:1811/2150 train_time:101774ms step_avg:56.20ms
step:1812/2150 train_time:101861ms step_avg:56.21ms
step:1813/2150 train_time:101950ms step_avg:56.23ms
step:1814/2150 train_time:102038ms step_avg:56.25ms
step:1815/2150 train_time:102127ms step_avg:56.27ms
step:1816/2150 train_time:102214ms step_avg:56.29ms
step:1817/2150 train_time:102304ms step_avg:56.30ms
step:1818/2150 train_time:102391ms step_avg:56.32ms
step:1819/2150 train_time:102480ms step_avg:56.34ms
step:1820/2150 train_time:102568ms step_avg:56.36ms
step:1821/2150 train_time:102658ms step_avg:56.37ms
step:1822/2150 train_time:102746ms step_avg:56.39ms
step:1823/2150 train_time:102835ms step_avg:56.41ms
step:1824/2150 train_time:102923ms step_avg:56.43ms
step:1825/2150 train_time:103012ms step_avg:56.45ms
step:1826/2150 train_time:103101ms step_avg:56.46ms
step:1827/2150 train_time:103189ms step_avg:56.48ms
step:1828/2150 train_time:103277ms step_avg:56.50ms
step:1829/2150 train_time:103366ms step_avg:56.52ms
step:1830/2150 train_time:103454ms step_avg:56.53ms
step:1831/2150 train_time:103543ms step_avg:56.55ms
step:1832/2150 train_time:103631ms step_avg:56.57ms
step:1833/2150 train_time:103720ms step_avg:56.58ms
step:1834/2150 train_time:103807ms step_avg:56.60ms
step:1835/2150 train_time:103897ms step_avg:56.62ms
step:1836/2150 train_time:103984ms step_avg:56.64ms
step:1837/2150 train_time:104074ms step_avg:56.65ms
step:1838/2150 train_time:104162ms step_avg:56.67ms
step:1839/2150 train_time:104251ms step_avg:56.69ms
step:1840/2150 train_time:104338ms step_avg:56.71ms
step:1841/2150 train_time:104427ms step_avg:56.72ms
step:1842/2150 train_time:104515ms step_avg:56.74ms
step:1843/2150 train_time:104605ms step_avg:56.76ms
step:1844/2150 train_time:104693ms step_avg:56.78ms
step:1845/2150 train_time:104783ms step_avg:56.79ms
step:1846/2150 train_time:104870ms step_avg:56.81ms
step:1847/2150 train_time:104958ms step_avg:56.83ms
step:1848/2150 train_time:105045ms step_avg:56.84ms
step:1849/2150 train_time:105134ms step_avg:56.86ms
step:1850/2150 train_time:105222ms step_avg:56.88ms
step:1851/2150 train_time:105311ms step_avg:56.89ms
step:1852/2150 train_time:105399ms step_avg:56.91ms
step:1853/2150 train_time:105488ms step_avg:56.93ms
step:1854/2150 train_time:105576ms step_avg:56.94ms
step:1855/2150 train_time:105665ms step_avg:56.96ms
step:1856/2150 train_time:105752ms step_avg:56.98ms
step:1857/2150 train_time:105841ms step_avg:57.00ms
step:1858/2150 train_time:105929ms step_avg:57.01ms
step:1859/2150 train_time:106018ms step_avg:57.03ms
step:1860/2150 train_time:106108ms step_avg:57.05ms
step:1861/2150 train_time:106194ms step_avg:57.06ms
step:1862/2150 train_time:106282ms step_avg:57.08ms
step:1863/2150 train_time:106372ms step_avg:57.10ms
step:1864/2150 train_time:106459ms step_avg:57.11ms
step:1865/2150 train_time:106548ms step_avg:57.13ms
step:1866/2150 train_time:106635ms step_avg:57.15ms
step:1867/2150 train_time:106725ms step_avg:57.16ms
step:1868/2150 train_time:106813ms step_avg:57.18ms
step:1869/2150 train_time:106902ms step_avg:57.20ms
step:1870/2150 train_time:106990ms step_avg:57.21ms
step:1871/2150 train_time:107079ms step_avg:57.23ms
step:1872/2150 train_time:107168ms step_avg:57.25ms
step:1873/2150 train_time:107257ms step_avg:57.26ms
step:1874/2150 train_time:107345ms step_avg:57.28ms
step:1875/2150 train_time:107434ms step_avg:57.30ms
step:1876/2150 train_time:107521ms step_avg:57.31ms
step:1877/2150 train_time:107610ms step_avg:57.33ms
step:1878/2150 train_time:107698ms step_avg:57.35ms
step:1879/2150 train_time:107787ms step_avg:57.36ms
step:1880/2150 train_time:107874ms step_avg:57.38ms
step:1881/2150 train_time:107964ms step_avg:57.40ms
step:1882/2150 train_time:108052ms step_avg:57.41ms
step:1883/2150 train_time:108142ms step_avg:57.43ms
step:1884/2150 train_time:108230ms step_avg:57.45ms
step:1885/2150 train_time:108319ms step_avg:57.46ms
step:1886/2150 train_time:108408ms step_avg:57.48ms
step:1887/2150 train_time:108497ms step_avg:57.50ms
step:1888/2150 train_time:108585ms step_avg:57.51ms
step:1889/2150 train_time:108674ms step_avg:57.53ms
step:1890/2150 train_time:108762ms step_avg:57.55ms
step:1891/2150 train_time:108851ms step_avg:57.56ms
step:1892/2150 train_time:108939ms step_avg:57.58ms
step:1893/2150 train_time:109029ms step_avg:57.60ms
step:1894/2150 train_time:109117ms step_avg:57.61ms
step:1895/2150 train_time:109206ms step_avg:57.63ms
step:1896/2150 train_time:109293ms step_avg:57.64ms
step:1897/2150 train_time:109382ms step_avg:57.66ms
step:1898/2150 train_time:109469ms step_avg:57.68ms
step:1899/2150 train_time:109558ms step_avg:57.69ms
step:1900/2150 train_time:109646ms step_avg:57.71ms
step:1901/2150 train_time:109735ms step_avg:57.72ms
step:1902/2150 train_time:109822ms step_avg:57.74ms
step:1903/2150 train_time:109911ms step_avg:57.76ms
step:1904/2150 train_time:109999ms step_avg:57.77ms
step:1905/2150 train_time:110089ms step_avg:57.79ms
step:1906/2150 train_time:110176ms step_avg:57.80ms
step:1907/2150 train_time:110266ms step_avg:57.82ms
step:1908/2150 train_time:110353ms step_avg:57.84ms
step:1909/2150 train_time:110442ms step_avg:57.85ms
step:1910/2150 train_time:110529ms step_avg:57.87ms
step:1911/2150 train_time:110619ms step_avg:57.89ms
step:1912/2150 train_time:110706ms step_avg:57.90ms
step:1913/2150 train_time:110795ms step_avg:57.92ms
step:1914/2150 train_time:110882ms step_avg:57.93ms
step:1915/2150 train_time:110971ms step_avg:57.95ms
step:1916/2150 train_time:111058ms step_avg:57.96ms
step:1917/2150 train_time:111148ms step_avg:57.98ms
step:1918/2150 train_time:111236ms step_avg:58.00ms
step:1919/2150 train_time:111325ms step_avg:58.01ms
step:1920/2150 train_time:111412ms step_avg:58.03ms
step:1921/2150 train_time:111501ms step_avg:58.04ms
step:1922/2150 train_time:111589ms step_avg:58.06ms
step:1923/2150 train_time:111679ms step_avg:58.08ms
step:1924/2150 train_time:111766ms step_avg:58.09ms
step:1925/2150 train_time:111855ms step_avg:58.11ms
step:1926/2150 train_time:111943ms step_avg:58.12ms
step:1927/2150 train_time:112032ms step_avg:58.14ms
step:1928/2150 train_time:112119ms step_avg:58.15ms
step:1929/2150 train_time:112208ms step_avg:58.17ms
step:1930/2150 train_time:112296ms step_avg:58.18ms
step:1931/2150 train_time:112385ms step_avg:58.20ms
step:1932/2150 train_time:112473ms step_avg:58.22ms
step:1933/2150 train_time:112563ms step_avg:58.23ms
step:1934/2150 train_time:112650ms step_avg:58.25ms
step:1935/2150 train_time:112739ms step_avg:58.26ms
step:1936/2150 train_time:112827ms step_avg:58.28ms
step:1937/2150 train_time:112916ms step_avg:58.29ms
step:1938/2150 train_time:113003ms step_avg:58.31ms
step:1939/2150 train_time:113092ms step_avg:58.33ms
step:1940/2150 train_time:113180ms step_avg:58.34ms
step:1941/2150 train_time:113269ms step_avg:58.36ms
step:1942/2150 train_time:113356ms step_avg:58.37ms
step:1943/2150 train_time:113446ms step_avg:58.39ms
step:1944/2150 train_time:113534ms step_avg:58.40ms
step:1945/2150 train_time:113624ms step_avg:58.42ms
step:1946/2150 train_time:113712ms step_avg:58.43ms
step:1947/2150 train_time:113802ms step_avg:58.45ms
step:1948/2150 train_time:113889ms step_avg:58.46ms
step:1949/2150 train_time:113978ms step_avg:58.48ms
step:1950/2150 train_time:114066ms step_avg:58.50ms
step:1951/2150 train_time:114155ms step_avg:58.51ms
step:1952/2150 train_time:114242ms step_avg:58.53ms
step:1953/2150 train_time:114331ms step_avg:58.54ms
step:1954/2150 train_time:114418ms step_avg:58.56ms
step:1955/2150 train_time:114507ms step_avg:58.57ms
step:1956/2150 train_time:114595ms step_avg:58.59ms
step:1957/2150 train_time:114685ms step_avg:58.60ms
step:1958/2150 train_time:114773ms step_avg:58.62ms
step:1959/2150 train_time:114862ms step_avg:58.63ms
step:1960/2150 train_time:114950ms step_avg:58.65ms
step:1961/2150 train_time:115039ms step_avg:58.66ms
step:1962/2150 train_time:115128ms step_avg:58.68ms
step:1963/2150 train_time:115216ms step_avg:58.69ms
step:1964/2150 train_time:115304ms step_avg:58.71ms
step:1965/2150 train_time:115393ms step_avg:58.72ms
step:1966/2150 train_time:115480ms step_avg:58.74ms
step:1967/2150 train_time:115569ms step_avg:58.75ms
step:1968/2150 train_time:115657ms step_avg:58.77ms
step:1969/2150 train_time:115747ms step_avg:58.78ms
step:1970/2150 train_time:115835ms step_avg:58.80ms
step:1971/2150 train_time:115924ms step_avg:58.81ms
step:1972/2150 train_time:116012ms step_avg:58.83ms
step:1973/2150 train_time:116101ms step_avg:58.85ms
step:1974/2150 train_time:116189ms step_avg:58.86ms
step:1975/2150 train_time:116278ms step_avg:58.88ms
step:1976/2150 train_time:116366ms step_avg:58.89ms
step:1977/2150 train_time:116456ms step_avg:58.91ms
step:1978/2150 train_time:116544ms step_avg:58.92ms
step:1979/2150 train_time:116633ms step_avg:58.94ms
step:1980/2150 train_time:116721ms step_avg:58.95ms
step:1981/2150 train_time:116810ms step_avg:58.97ms
step:1982/2150 train_time:116897ms step_avg:58.98ms
step:1983/2150 train_time:116987ms step_avg:58.99ms
step:1984/2150 train_time:117075ms step_avg:59.01ms
step:1985/2150 train_time:117164ms step_avg:59.02ms
step:1986/2150 train_time:117251ms step_avg:59.04ms
step:1987/2150 train_time:117339ms step_avg:59.05ms
step:1988/2150 train_time:117427ms step_avg:59.07ms
step:1989/2150 train_time:117516ms step_avg:59.08ms
step:1990/2150 train_time:117605ms step_avg:59.10ms
step:1991/2150 train_time:117694ms step_avg:59.11ms
step:1992/2150 train_time:117782ms step_avg:59.13ms
step:1993/2150 train_time:117871ms step_avg:59.14ms
step:1994/2150 train_time:117958ms step_avg:59.16ms
step:1995/2150 train_time:118047ms step_avg:59.17ms
step:1996/2150 train_time:118135ms step_avg:59.19ms
step:1997/2150 train_time:118224ms step_avg:59.20ms
step:1998/2150 train_time:118312ms step_avg:59.22ms
step:1999/2150 train_time:118401ms step_avg:59.23ms
step:2000/2150 train_time:118488ms step_avg:59.24ms
step:2000/2150 val_loss:3.3149 train_time:118579ms step_avg:59.29ms
step:2001/2150 train_time:118602ms step_avg:59.27ms
step:2002/2150 train_time:118669ms step_avg:59.27ms
step:2003/2150 train_time:118762ms step_avg:59.29ms
step:2004/2150 train_time:118849ms step_avg:59.31ms
step:2005/2150 train_time:118937ms step_avg:59.32ms
step:2006/2150 train_time:119024ms step_avg:59.33ms
step:2007/2150 train_time:119111ms step_avg:59.35ms
step:2008/2150 train_time:119198ms step_avg:59.36ms
step:2009/2150 train_time:119288ms step_avg:59.38ms
step:2010/2150 train_time:119375ms step_avg:59.39ms
step:2011/2150 train_time:119463ms step_avg:59.40ms
step:2012/2150 train_time:119552ms step_avg:59.42ms
step:2013/2150 train_time:119644ms step_avg:59.44ms
step:2014/2150 train_time:119734ms step_avg:59.45ms
step:2015/2150 train_time:119823ms step_avg:59.47ms
step:2016/2150 train_time:119910ms step_avg:59.48ms
step:2017/2150 train_time:119998ms step_avg:59.49ms
step:2018/2150 train_time:120085ms step_avg:59.51ms
step:2019/2150 train_time:120173ms step_avg:59.52ms
step:2020/2150 train_time:120261ms step_avg:59.54ms
step:2021/2150 train_time:120351ms step_avg:59.55ms
step:2022/2150 train_time:120437ms step_avg:59.56ms
step:2023/2150 train_time:120526ms step_avg:59.58ms
step:2024/2150 train_time:120614ms step_avg:59.59ms
step:2025/2150 train_time:120705ms step_avg:59.61ms
step:2026/2150 train_time:120793ms step_avg:59.62ms
step:2027/2150 train_time:120883ms step_avg:59.64ms
step:2028/2150 train_time:120969ms step_avg:59.65ms
step:2029/2150 train_time:121058ms step_avg:59.66ms
step:2030/2150 train_time:121145ms step_avg:59.68ms
step:2031/2150 train_time:121233ms step_avg:59.69ms
step:2032/2150 train_time:121321ms step_avg:59.71ms
step:2033/2150 train_time:121409ms step_avg:59.72ms
step:2034/2150 train_time:121496ms step_avg:59.73ms
step:2035/2150 train_time:121587ms step_avg:59.75ms
step:2036/2150 train_time:121675ms step_avg:59.76ms
step:2037/2150 train_time:121765ms step_avg:59.78ms
step:2038/2150 train_time:121853ms step_avg:59.79ms
step:2039/2150 train_time:121943ms step_avg:59.81ms
step:2040/2150 train_time:122030ms step_avg:59.82ms
step:2041/2150 train_time:122119ms step_avg:59.83ms
step:2042/2150 train_time:122206ms step_avg:59.85ms
step:2043/2150 train_time:122294ms step_avg:59.86ms
step:2044/2150 train_time:122382ms step_avg:59.87ms
step:2045/2150 train_time:122471ms step_avg:59.89ms
step:2046/2150 train_time:122559ms step_avg:59.90ms
step:2047/2150 train_time:122648ms step_avg:59.92ms
step:2048/2150 train_time:122736ms step_avg:59.93ms
step:2049/2150 train_time:122826ms step_avg:59.94ms
step:2050/2150 train_time:122914ms step_avg:59.96ms
step:2051/2150 train_time:123003ms step_avg:59.97ms
step:2052/2150 train_time:123090ms step_avg:59.99ms
step:2053/2150 train_time:123179ms step_avg:60.00ms
step:2054/2150 train_time:123267ms step_avg:60.01ms
step:2055/2150 train_time:123356ms step_avg:60.03ms
step:2056/2150 train_time:123444ms step_avg:60.04ms
step:2057/2150 train_time:123534ms step_avg:60.06ms
step:2058/2150 train_time:123622ms step_avg:60.07ms
step:2059/2150 train_time:123711ms step_avg:60.08ms
step:2060/2150 train_time:123799ms step_avg:60.10ms
step:2061/2150 train_time:123888ms step_avg:60.11ms
step:2062/2150 train_time:123976ms step_avg:60.12ms
step:2063/2150 train_time:124065ms step_avg:60.14ms
step:2064/2150 train_time:124153ms step_avg:60.15ms
step:2065/2150 train_time:124241ms step_avg:60.17ms
step:2066/2150 train_time:124330ms step_avg:60.18ms
step:2067/2150 train_time:124419ms step_avg:60.19ms
step:2068/2150 train_time:124507ms step_avg:60.21ms
step:2069/2150 train_time:124596ms step_avg:60.22ms
step:2070/2150 train_time:124685ms step_avg:60.23ms
step:2071/2150 train_time:124774ms step_avg:60.25ms
step:2072/2150 train_time:124862ms step_avg:60.26ms
step:2073/2150 train_time:124951ms step_avg:60.28ms
step:2074/2150 train_time:125039ms step_avg:60.29ms
step:2075/2150 train_time:125127ms step_avg:60.30ms
step:2076/2150 train_time:125215ms step_avg:60.32ms
step:2077/2150 train_time:125303ms step_avg:60.33ms
step:2078/2150 train_time:125391ms step_avg:60.34ms
step:2079/2150 train_time:125479ms step_avg:60.36ms
step:2080/2150 train_time:125567ms step_avg:60.37ms
step:2081/2150 train_time:125657ms step_avg:60.38ms
step:2082/2150 train_time:125746ms step_avg:60.40ms
step:2083/2150 train_time:125835ms step_avg:60.41ms
step:2084/2150 train_time:125923ms step_avg:60.42ms
step:2085/2150 train_time:126012ms step_avg:60.44ms
step:2086/2150 train_time:126100ms step_avg:60.45ms
step:2087/2150 train_time:126189ms step_avg:60.46ms
step:2088/2150 train_time:126276ms step_avg:60.48ms
step:2089/2150 train_time:126364ms step_avg:60.49ms
step:2090/2150 train_time:126451ms step_avg:60.50ms
step:2091/2150 train_time:126540ms step_avg:60.52ms
step:2092/2150 train_time:126628ms step_avg:60.53ms
step:2093/2150 train_time:126717ms step_avg:60.54ms
step:2094/2150 train_time:126805ms step_avg:60.56ms
step:2095/2150 train_time:126894ms step_avg:60.57ms
step:2096/2150 train_time:126981ms step_avg:60.58ms
step:2097/2150 train_time:127070ms step_avg:60.60ms
step:2098/2150 train_time:127157ms step_avg:60.61ms
step:2099/2150 train_time:127247ms step_avg:60.62ms
step:2100/2150 train_time:127333ms step_avg:60.63ms
step:2101/2150 train_time:127422ms step_avg:60.65ms
step:2102/2150 train_time:127511ms step_avg:60.66ms
step:2103/2150 train_time:127600ms step_avg:60.68ms
step:2104/2150 train_time:127688ms step_avg:60.69ms
step:2105/2150 train_time:127777ms step_avg:60.70ms
step:2106/2150 train_time:127866ms step_avg:60.71ms
step:2107/2150 train_time:127954ms step_avg:60.73ms
step:2108/2150 train_time:128042ms step_avg:60.74ms
step:2109/2150 train_time:128131ms step_avg:60.75ms
step:2110/2150 train_time:128218ms step_avg:60.77ms
step:2111/2150 train_time:128308ms step_avg:60.78ms
step:2112/2150 train_time:128396ms step_avg:60.79ms
step:2113/2150 train_time:128485ms step_avg:60.81ms
step:2114/2150 train_time:128573ms step_avg:60.82ms
step:2115/2150 train_time:128663ms step_avg:60.83ms
step:2116/2150 train_time:128750ms step_avg:60.85ms
step:2117/2150 train_time:128840ms step_avg:60.86ms
step:2118/2150 train_time:128928ms step_avg:60.87ms
step:2119/2150 train_time:129017ms step_avg:60.89ms
step:2120/2150 train_time:129105ms step_avg:60.90ms
step:2121/2150 train_time:129193ms step_avg:60.91ms
step:2122/2150 train_time:129281ms step_avg:60.92ms
step:2123/2150 train_time:129371ms step_avg:60.94ms
step:2124/2150 train_time:129459ms step_avg:60.95ms
step:2125/2150 train_time:129548ms step_avg:60.96ms
step:2126/2150 train_time:129636ms step_avg:60.98ms
step:2127/2150 train_time:129725ms step_avg:60.99ms
step:2128/2150 train_time:129812ms step_avg:61.00ms
step:2129/2150 train_time:129902ms step_avg:61.02ms
step:2130/2150 train_time:129991ms step_avg:61.03ms
step:2131/2150 train_time:130079ms step_avg:61.04ms
step:2132/2150 train_time:130167ms step_avg:61.05ms
step:2133/2150 train_time:130257ms step_avg:61.07ms
step:2134/2150 train_time:130344ms step_avg:61.08ms
step:2135/2150 train_time:130433ms step_avg:61.09ms
step:2136/2150 train_time:130521ms step_avg:61.11ms
step:2137/2150 train_time:130611ms step_avg:61.12ms
step:2138/2150 train_time:130698ms step_avg:61.13ms
step:2139/2150 train_time:130787ms step_avg:61.14ms
step:2140/2150 train_time:130876ms step_avg:61.16ms
step:2141/2150 train_time:130965ms step_avg:61.17ms
step:2142/2150 train_time:131055ms step_avg:61.18ms
step:2143/2150 train_time:131143ms step_avg:61.20ms
step:2144/2150 train_time:131231ms step_avg:61.21ms
step:2145/2150 train_time:131320ms step_avg:61.22ms
step:2146/2150 train_time:131407ms step_avg:61.23ms
step:2147/2150 train_time:131496ms step_avg:61.25ms
step:2148/2150 train_time:131584ms step_avg:61.26ms
step:2149/2150 train_time:131674ms step_avg:61.27ms
step:2150/2150 train_time:131762ms step_avg:61.28ms
step:2150/2150 val_loss:3.2807 train_time:131853ms step_avg:61.33ms
peak memory allocated: 30540 MiB reserved: 44816 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc
os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        group_sizes = [15, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

        # only include gates on layers with value embeds used on forward pass
        if layer_idx in [0,1,8,9,10]:
            self.value_embed_gate = CastedLinear(12, num_heads)
            self.value_embed_gate.weight.label = 'value_embed_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(self.value_embed_gate(x[..., :self.value_embed_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        #self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig, skew = None):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if skew is None:
            logits = 23 * torch.sigmoid((logits+5) / 7.5)
        else:
            logits = 23 * torch.sigmoid((logits+skew) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
def log_parameter_norms(model):
    norms = {}
    for name, param in model.named_parameters():
        if param.requires_grad:
            norms[f"param_norm/{name}"] = param.norm().item()
            norms[f"param_max/{name}"] = param.max().item()
            norms[f"param_min/{name}"] = param.min().item()
            if param.grad is not None:
                norms[f"grad_norm/{name}"] = param.grad.norm().item()
                norms[f"grad_max/{name}"] = param.grad.max().item()
    
    # Total norms
    total_param_norm = sum(p.norm().item() ** 2 for p in model.parameters()) ** 0.5
    total_grad_norm = sum(p.grad.norm().item() ** 2 for p in model.parameters() if p.grad is not None) ** 0.5
    
    norms["param_norm/total"] = total_param_norm
    norms["grad_norm/total"] = total_grad_norm
    
    return norms

train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

# import wandb
# if master_process:
#     wandb.init(
#         project="nano4",
#         name="baseline"
#     )

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    # if master_process and step%2==1:
    #     wandb.log({
    #         "loss": loss.item(),
    #         **log_parameter_norms(model)  # Add all norms
    #     })
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 29 05:55:42 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   30C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0            109W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            107W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8307 train_time:0ms step_avg:0.03ms
step:1/1845 train_time:77ms step_avg:76.84ms
step:2/1845 train_time:100ms step_avg:50.24ms
step:3/1845 train_time:124ms step_avg:41.41ms
step:4/1845 train_time:158ms step_avg:39.53ms
step:5/1845 train_time:193ms step_avg:38.56ms
step:6/1845 train_time:279ms step_avg:46.55ms
step:7/1845 train_time:298ms step_avg:42.57ms
step:8/1845 train_time:331ms step_avg:41.42ms
step:9/1845 train_time:366ms step_avg:40.63ms
step:10/1845 train_time:400ms step_avg:39.96ms
step:11/1845 train_time:434ms step_avg:39.47ms
step:12/1845 train_time:468ms step_avg:39.02ms
step:13/1845 train_time:503ms step_avg:38.69ms
step:14/1845 train_time:537ms step_avg:38.36ms
step:15/1845 train_time:572ms step_avg:38.11ms
step:16/1845 train_time:606ms step_avg:37.87ms
step:17/1845 train_time:640ms step_avg:37.68ms
step:18/1845 train_time:675ms step_avg:37.48ms
step:19/1845 train_time:709ms step_avg:37.33ms
step:20/1845 train_time:744ms step_avg:37.18ms
step:21/1845 train_time:778ms step_avg:37.04ms
step:22/1845 train_time:812ms step_avg:36.91ms
step:23/1845 train_time:847ms step_avg:36.82ms
step:24/1845 train_time:881ms step_avg:36.71ms
step:25/1845 train_time:915ms step_avg:36.62ms
step:26/1845 train_time:949ms step_avg:36.52ms
step:27/1845 train_time:984ms step_avg:36.45ms
step:28/1845 train_time:1018ms step_avg:36.37ms
step:29/1845 train_time:1053ms step_avg:36.31ms
step:30/1845 train_time:1087ms step_avg:36.24ms
step:31/1845 train_time:1122ms step_avg:36.18ms
step:32/1845 train_time:1156ms step_avg:36.11ms
step:33/1845 train_time:1191ms step_avg:36.08ms
step:34/1845 train_time:1225ms step_avg:36.03ms
step:35/1845 train_time:1260ms step_avg:36.00ms
step:36/1845 train_time:1294ms step_avg:35.95ms
step:37/1845 train_time:1329ms step_avg:35.92ms
step:38/1845 train_time:1363ms step_avg:35.87ms
step:39/1845 train_time:1398ms step_avg:35.84ms
step:40/1845 train_time:1432ms step_avg:35.80ms
step:41/1845 train_time:1467ms step_avg:35.77ms
step:42/1845 train_time:1501ms step_avg:35.74ms
step:43/1845 train_time:1536ms step_avg:35.71ms
step:44/1845 train_time:1570ms step_avg:35.68ms
step:45/1845 train_time:1605ms step_avg:35.66ms
step:46/1845 train_time:1639ms step_avg:35.62ms
step:47/1845 train_time:1673ms step_avg:35.60ms
step:48/1845 train_time:1708ms step_avg:35.58ms
step:49/1845 train_time:1742ms step_avg:35.55ms
step:50/1845 train_time:1776ms step_avg:35.52ms
step:51/1845 train_time:1811ms step_avg:35.51ms
step:52/1845 train_time:1845ms step_avg:35.48ms
step:53/1845 train_time:1879ms step_avg:35.46ms
step:54/1845 train_time:1914ms step_avg:35.44ms
step:55/1845 train_time:1948ms step_avg:35.43ms
step:56/1845 train_time:1982ms step_avg:35.40ms
step:57/1845 train_time:2017ms step_avg:35.39ms
step:58/1845 train_time:2051ms step_avg:35.37ms
step:59/1845 train_time:2086ms step_avg:35.35ms
step:60/1845 train_time:2120ms step_avg:35.33ms
step:61/1845 train_time:2154ms step_avg:35.32ms
step:62/1845 train_time:2189ms step_avg:35.31ms
step:63/1845 train_time:2224ms step_avg:35.30ms
step:64/1845 train_time:2258ms step_avg:35.28ms
step:65/1845 train_time:2293ms step_avg:35.28ms
step:66/1845 train_time:2327ms step_avg:35.26ms
step:67/1845 train_time:2363ms step_avg:35.26ms
step:68/1845 train_time:2396ms step_avg:35.24ms
step:69/1845 train_time:2431ms step_avg:35.23ms
step:70/1845 train_time:2465ms step_avg:35.22ms
step:71/1845 train_time:2500ms step_avg:35.21ms
step:72/1845 train_time:2534ms step_avg:35.20ms
step:73/1845 train_time:2569ms step_avg:35.19ms
step:74/1845 train_time:2603ms step_avg:35.17ms
step:75/1845 train_time:2638ms step_avg:35.17ms
step:76/1845 train_time:2672ms step_avg:35.15ms
step:77/1845 train_time:2707ms step_avg:35.15ms
step:78/1845 train_time:2741ms step_avg:35.14ms
step:79/1845 train_time:2775ms step_avg:35.13ms
step:80/1845 train_time:2809ms step_avg:35.12ms
step:81/1845 train_time:2845ms step_avg:35.12ms
step:82/1845 train_time:2879ms step_avg:35.11ms
step:83/1845 train_time:2914ms step_avg:35.11ms
step:84/1845 train_time:2948ms step_avg:35.09ms
step:85/1845 train_time:2983ms step_avg:35.09ms
step:86/1845 train_time:3017ms step_avg:35.08ms
step:87/1845 train_time:3051ms step_avg:35.07ms
step:88/1845 train_time:3086ms step_avg:35.06ms
step:89/1845 train_time:3120ms step_avg:35.06ms
step:90/1845 train_time:3154ms step_avg:35.05ms
step:91/1845 train_time:3189ms step_avg:35.04ms
step:92/1845 train_time:3223ms step_avg:35.03ms
step:93/1845 train_time:3257ms step_avg:35.03ms
step:94/1845 train_time:3292ms step_avg:35.02ms
step:95/1845 train_time:3327ms step_avg:35.02ms
step:96/1845 train_time:3361ms step_avg:35.01ms
step:97/1845 train_time:3396ms step_avg:35.01ms
step:98/1845 train_time:3430ms step_avg:35.00ms
step:99/1845 train_time:3465ms step_avg:35.00ms
step:100/1845 train_time:3500ms step_avg:35.00ms
step:101/1845 train_time:3534ms step_avg:34.99ms
step:102/1845 train_time:3568ms step_avg:34.98ms
step:103/1845 train_time:3603ms step_avg:34.98ms
step:104/1845 train_time:3637ms step_avg:34.97ms
step:105/1845 train_time:3672ms step_avg:34.97ms
step:106/1845 train_time:3706ms step_avg:34.96ms
step:107/1845 train_time:3740ms step_avg:34.96ms
step:108/1845 train_time:3774ms step_avg:34.95ms
step:109/1845 train_time:3809ms step_avg:34.95ms
step:110/1845 train_time:3843ms step_avg:34.94ms
step:111/1845 train_time:3878ms step_avg:34.93ms
step:112/1845 train_time:3912ms step_avg:34.93ms
step:113/1845 train_time:3947ms step_avg:34.93ms
step:114/1845 train_time:3981ms step_avg:34.92ms
step:115/1845 train_time:4016ms step_avg:34.92ms
step:116/1845 train_time:4050ms step_avg:34.91ms
step:117/1845 train_time:4085ms step_avg:34.92ms
step:118/1845 train_time:4119ms step_avg:34.91ms
step:119/1845 train_time:4154ms step_avg:34.91ms
step:120/1845 train_time:4188ms step_avg:34.90ms
step:121/1845 train_time:4223ms step_avg:34.90ms
step:122/1845 train_time:4257ms step_avg:34.89ms
step:123/1845 train_time:4291ms step_avg:34.89ms
step:124/1845 train_time:4326ms step_avg:34.88ms
step:125/1845 train_time:4360ms step_avg:34.88ms
step:126/1845 train_time:4394ms step_avg:34.87ms
step:127/1845 train_time:4429ms step_avg:34.87ms
step:128/1845 train_time:4463ms step_avg:34.87ms
step:129/1845 train_time:4497ms step_avg:34.86ms
step:130/1845 train_time:4531ms step_avg:34.86ms
step:131/1845 train_time:4566ms step_avg:34.85ms
step:132/1845 train_time:4600ms step_avg:34.85ms
step:133/1845 train_time:4635ms step_avg:34.85ms
step:134/1845 train_time:4669ms step_avg:34.84ms
step:135/1845 train_time:4703ms step_avg:34.84ms
step:136/1845 train_time:4737ms step_avg:34.83ms
step:137/1845 train_time:4772ms step_avg:34.83ms
step:138/1845 train_time:4806ms step_avg:34.83ms
step:139/1845 train_time:4841ms step_avg:34.83ms
step:140/1845 train_time:4875ms step_avg:34.82ms
step:141/1845 train_time:4909ms step_avg:34.82ms
step:142/1845 train_time:4944ms step_avg:34.81ms
step:143/1845 train_time:4978ms step_avg:34.81ms
step:144/1845 train_time:5013ms step_avg:34.81ms
step:145/1845 train_time:5048ms step_avg:34.81ms
step:146/1845 train_time:5082ms step_avg:34.81ms
step:147/1845 train_time:5116ms step_avg:34.81ms
step:148/1845 train_time:5150ms step_avg:34.80ms
step:149/1845 train_time:5185ms step_avg:34.80ms
step:150/1845 train_time:5219ms step_avg:34.79ms
step:151/1845 train_time:5254ms step_avg:34.79ms
step:152/1845 train_time:5288ms step_avg:34.79ms
step:153/1845 train_time:5323ms step_avg:34.79ms
step:154/1845 train_time:5357ms step_avg:34.78ms
step:155/1845 train_time:5392ms step_avg:34.79ms
step:156/1845 train_time:5426ms step_avg:34.78ms
step:157/1845 train_time:5460ms step_avg:34.78ms
step:158/1845 train_time:5495ms step_avg:34.78ms
step:159/1845 train_time:5530ms step_avg:34.78ms
step:160/1845 train_time:5564ms step_avg:34.77ms
step:161/1845 train_time:5598ms step_avg:34.77ms
step:162/1845 train_time:5632ms step_avg:34.77ms
step:163/1845 train_time:5667ms step_avg:34.77ms
step:164/1845 train_time:5701ms step_avg:34.76ms
step:165/1845 train_time:5735ms step_avg:34.76ms
step:166/1845 train_time:5769ms step_avg:34.76ms
step:167/1845 train_time:5804ms step_avg:34.75ms
step:168/1845 train_time:5838ms step_avg:34.75ms
step:169/1845 train_time:5872ms step_avg:34.75ms
step:170/1845 train_time:5907ms step_avg:34.74ms
step:171/1845 train_time:5941ms step_avg:34.74ms
step:172/1845 train_time:5975ms step_avg:34.74ms
step:173/1845 train_time:6010ms step_avg:34.74ms
step:174/1845 train_time:6044ms step_avg:34.74ms
step:175/1845 train_time:6079ms step_avg:34.74ms
step:176/1845 train_time:6113ms step_avg:34.73ms
step:177/1845 train_time:6148ms step_avg:34.73ms
step:178/1845 train_time:6182ms step_avg:34.73ms
step:179/1845 train_time:6216ms step_avg:34.73ms
step:180/1845 train_time:6251ms step_avg:34.73ms
step:181/1845 train_time:6286ms step_avg:34.73ms
step:182/1845 train_time:6320ms step_avg:34.72ms
step:183/1845 train_time:6354ms step_avg:34.72ms
step:184/1845 train_time:6388ms step_avg:34.72ms
step:185/1845 train_time:6423ms step_avg:34.72ms
step:186/1845 train_time:6457ms step_avg:34.72ms
step:187/1845 train_time:6492ms step_avg:34.72ms
step:188/1845 train_time:6526ms step_avg:34.71ms
step:189/1845 train_time:6561ms step_avg:34.72ms
step:190/1845 train_time:6595ms step_avg:34.71ms
step:191/1845 train_time:6630ms step_avg:34.71ms
step:192/1845 train_time:6664ms step_avg:34.71ms
step:193/1845 train_time:6699ms step_avg:34.71ms
step:194/1845 train_time:6733ms step_avg:34.70ms
step:195/1845 train_time:6767ms step_avg:34.70ms
step:196/1845 train_time:6801ms step_avg:34.70ms
step:197/1845 train_time:6835ms step_avg:34.70ms
step:198/1845 train_time:6869ms step_avg:34.69ms
step:199/1845 train_time:6904ms step_avg:34.69ms
step:200/1845 train_time:6938ms step_avg:34.69ms
step:201/1845 train_time:6973ms step_avg:34.69ms
step:202/1845 train_time:7007ms step_avg:34.69ms
step:203/1845 train_time:7041ms step_avg:34.69ms
step:204/1845 train_time:7075ms step_avg:34.68ms
step:205/1845 train_time:7110ms step_avg:34.68ms
step:206/1845 train_time:7144ms step_avg:34.68ms
step:207/1845 train_time:7178ms step_avg:34.68ms
step:208/1845 train_time:7212ms step_avg:34.68ms
step:209/1845 train_time:7247ms step_avg:34.68ms
step:210/1845 train_time:7281ms step_avg:34.67ms
step:211/1845 train_time:7316ms step_avg:34.67ms
step:212/1845 train_time:7350ms step_avg:34.67ms
step:213/1845 train_time:7385ms step_avg:34.67ms
step:214/1845 train_time:7419ms step_avg:34.67ms
step:215/1845 train_time:7454ms step_avg:34.67ms
step:216/1845 train_time:7488ms step_avg:34.67ms
step:217/1845 train_time:7522ms step_avg:34.67ms
step:218/1845 train_time:7557ms step_avg:34.66ms
step:219/1845 train_time:7591ms step_avg:34.66ms
step:220/1845 train_time:7625ms step_avg:34.66ms
step:221/1845 train_time:7659ms step_avg:34.66ms
step:222/1845 train_time:7694ms step_avg:34.66ms
step:223/1845 train_time:7728ms step_avg:34.65ms
step:224/1845 train_time:7762ms step_avg:34.65ms
step:225/1845 train_time:7796ms step_avg:34.65ms
step:226/1845 train_time:7830ms step_avg:34.65ms
step:227/1845 train_time:7865ms step_avg:34.65ms
step:228/1845 train_time:7899ms step_avg:34.64ms
step:229/1845 train_time:7934ms step_avg:34.64ms
step:230/1845 train_time:7968ms step_avg:34.64ms
step:231/1845 train_time:8002ms step_avg:34.64ms
step:232/1845 train_time:8036ms step_avg:34.64ms
step:233/1845 train_time:8070ms step_avg:34.64ms
step:234/1845 train_time:8104ms step_avg:34.63ms
step:235/1845 train_time:8139ms step_avg:34.63ms
step:236/1845 train_time:8173ms step_avg:34.63ms
step:237/1845 train_time:8208ms step_avg:34.63ms
step:238/1845 train_time:8242ms step_avg:34.63ms
step:239/1845 train_time:8276ms step_avg:34.63ms
step:240/1845 train_time:8310ms step_avg:34.63ms
step:241/1845 train_time:8345ms step_avg:34.63ms
step:242/1845 train_time:8379ms step_avg:34.62ms
step:243/1845 train_time:8414ms step_avg:34.62ms
step:244/1845 train_time:8448ms step_avg:34.62ms
step:245/1845 train_time:8482ms step_avg:34.62ms
step:246/1845 train_time:8516ms step_avg:34.62ms
step:247/1845 train_time:8551ms step_avg:34.62ms
step:248/1845 train_time:8585ms step_avg:34.62ms
step:249/1845 train_time:8619ms step_avg:34.62ms
step:250/1845 train_time:8654ms step_avg:34.61ms
step:250/1845 val_loss:4.6164 train_time:8690ms step_avg:34.76ms
step:251/1845 train_time:8710ms step_avg:34.70ms
step:252/1845 train_time:8729ms step_avg:34.64ms
step:253/1845 train_time:8760ms step_avg:34.62ms
step:254/1845 train_time:8794ms step_avg:34.62ms
step:255/1845 train_time:8830ms step_avg:34.63ms
step:256/1845 train_time:8865ms step_avg:34.63ms
step:257/1845 train_time:8899ms step_avg:34.63ms
step:258/1845 train_time:8934ms step_avg:34.63ms
step:259/1845 train_time:8968ms step_avg:34.63ms
step:260/1845 train_time:9002ms step_avg:34.62ms
step:261/1845 train_time:9037ms step_avg:34.62ms
step:262/1845 train_time:9071ms step_avg:34.62ms
step:263/1845 train_time:9105ms step_avg:34.62ms
step:264/1845 train_time:9139ms step_avg:34.62ms
step:265/1845 train_time:9173ms step_avg:34.62ms
step:266/1845 train_time:9208ms step_avg:34.62ms
step:267/1845 train_time:9242ms step_avg:34.61ms
step:268/1845 train_time:9276ms step_avg:34.61ms
step:269/1845 train_time:9310ms step_avg:34.61ms
step:270/1845 train_time:9344ms step_avg:34.61ms
step:271/1845 train_time:9379ms step_avg:34.61ms
step:272/1845 train_time:9413ms step_avg:34.61ms
step:273/1845 train_time:9447ms step_avg:34.61ms
step:274/1845 train_time:9482ms step_avg:34.60ms
step:275/1845 train_time:9516ms step_avg:34.60ms
step:276/1845 train_time:9550ms step_avg:34.60ms
step:277/1845 train_time:9584ms step_avg:34.60ms
step:278/1845 train_time:9618ms step_avg:34.60ms
step:279/1845 train_time:9652ms step_avg:34.60ms
step:280/1845 train_time:9687ms step_avg:34.59ms
step:281/1845 train_time:9721ms step_avg:34.59ms
step:282/1845 train_time:9755ms step_avg:34.59ms
step:283/1845 train_time:9790ms step_avg:34.59ms
step:284/1845 train_time:9824ms step_avg:34.59ms
step:285/1845 train_time:9858ms step_avg:34.59ms
step:286/1845 train_time:9892ms step_avg:34.59ms
step:287/1845 train_time:9927ms step_avg:34.59ms
step:288/1845 train_time:9962ms step_avg:34.59ms
step:289/1845 train_time:9996ms step_avg:34.59ms
step:290/1845 train_time:10030ms step_avg:34.59ms
step:291/1845 train_time:10065ms step_avg:34.59ms
step:292/1845 train_time:10099ms step_avg:34.58ms
step:293/1845 train_time:10133ms step_avg:34.59ms
step:294/1845 train_time:10168ms step_avg:34.58ms
step:295/1845 train_time:10202ms step_avg:34.58ms
step:296/1845 train_time:10236ms step_avg:34.58ms
step:297/1845 train_time:10271ms step_avg:34.58ms
step:298/1845 train_time:10305ms step_avg:34.58ms
step:299/1845 train_time:10340ms step_avg:34.58ms
step:300/1845 train_time:10374ms step_avg:34.58ms
step:301/1845 train_time:10408ms step_avg:34.58ms
step:302/1845 train_time:10442ms step_avg:34.58ms
step:303/1845 train_time:10477ms step_avg:34.58ms
step:304/1845 train_time:10511ms step_avg:34.57ms
step:305/1845 train_time:10545ms step_avg:34.57ms
step:306/1845 train_time:10579ms step_avg:34.57ms
step:307/1845 train_time:10613ms step_avg:34.57ms
step:308/1845 train_time:10647ms step_avg:34.57ms
step:309/1845 train_time:10682ms step_avg:34.57ms
step:310/1845 train_time:10716ms step_avg:34.57ms
step:311/1845 train_time:10750ms step_avg:34.57ms
step:312/1845 train_time:10784ms step_avg:34.57ms
step:313/1845 train_time:10819ms step_avg:34.57ms
step:314/1845 train_time:10853ms step_avg:34.56ms
step:315/1845 train_time:10887ms step_avg:34.56ms
step:316/1845 train_time:10921ms step_avg:34.56ms
step:317/1845 train_time:10956ms step_avg:34.56ms
step:318/1845 train_time:10990ms step_avg:34.56ms
step:319/1845 train_time:11024ms step_avg:34.56ms
step:320/1845 train_time:11058ms step_avg:34.56ms
step:321/1845 train_time:11093ms step_avg:34.56ms
step:322/1845 train_time:11127ms step_avg:34.56ms
step:323/1845 train_time:11161ms step_avg:34.56ms
step:324/1845 train_time:11196ms step_avg:34.55ms
step:325/1845 train_time:11230ms step_avg:34.55ms
step:326/1845 train_time:11264ms step_avg:34.55ms
step:327/1845 train_time:11299ms step_avg:34.55ms
step:328/1845 train_time:11333ms step_avg:34.55ms
step:329/1845 train_time:11368ms step_avg:34.55ms
step:330/1845 train_time:11402ms step_avg:34.55ms
step:331/1845 train_time:11436ms step_avg:34.55ms
step:332/1845 train_time:11470ms step_avg:34.55ms
step:333/1845 train_time:11505ms step_avg:34.55ms
step:334/1845 train_time:11539ms step_avg:34.55ms
step:335/1845 train_time:11573ms step_avg:34.55ms
step:336/1845 train_time:11607ms step_avg:34.54ms
step:337/1845 train_time:11642ms step_avg:34.55ms
step:338/1845 train_time:11676ms step_avg:34.54ms
step:339/1845 train_time:11710ms step_avg:34.54ms
step:340/1845 train_time:11744ms step_avg:34.54ms
step:341/1845 train_time:11779ms step_avg:34.54ms
step:342/1845 train_time:11813ms step_avg:34.54ms
step:343/1845 train_time:11847ms step_avg:34.54ms
step:344/1845 train_time:11882ms step_avg:34.54ms
step:345/1845 train_time:11916ms step_avg:34.54ms
step:346/1845 train_time:11950ms step_avg:34.54ms
step:347/1845 train_time:11985ms step_avg:34.54ms
step:348/1845 train_time:12019ms step_avg:34.54ms
step:349/1845 train_time:12053ms step_avg:34.54ms
step:350/1845 train_time:12087ms step_avg:34.53ms
step:351/1845 train_time:12122ms step_avg:34.54ms
step:352/1845 train_time:12156ms step_avg:34.53ms
step:353/1845 train_time:12190ms step_avg:34.53ms
step:354/1845 train_time:12224ms step_avg:34.53ms
step:355/1845 train_time:12259ms step_avg:34.53ms
step:356/1845 train_time:12293ms step_avg:34.53ms
step:357/1845 train_time:12328ms step_avg:34.53ms
step:358/1845 train_time:12362ms step_avg:34.53ms
step:359/1845 train_time:12396ms step_avg:34.53ms
step:360/1845 train_time:12430ms step_avg:34.53ms
step:361/1845 train_time:12465ms step_avg:34.53ms
step:362/1845 train_time:12499ms step_avg:34.53ms
step:363/1845 train_time:12533ms step_avg:34.53ms
step:364/1845 train_time:12567ms step_avg:34.53ms
step:365/1845 train_time:12602ms step_avg:34.53ms
step:366/1845 train_time:12636ms step_avg:34.52ms
step:367/1845 train_time:12671ms step_avg:34.52ms
step:368/1845 train_time:12705ms step_avg:34.52ms
step:369/1845 train_time:12740ms step_avg:34.53ms
step:370/1845 train_time:12774ms step_avg:34.52ms
step:371/1845 train_time:12808ms step_avg:34.52ms
step:372/1845 train_time:12842ms step_avg:34.52ms
step:373/1845 train_time:12877ms step_avg:34.52ms
step:374/1845 train_time:12911ms step_avg:34.52ms
step:375/1845 train_time:12945ms step_avg:34.52ms
step:376/1845 train_time:12980ms step_avg:34.52ms
step:377/1845 train_time:13014ms step_avg:34.52ms
step:378/1845 train_time:13048ms step_avg:34.52ms
step:379/1845 train_time:13083ms step_avg:34.52ms
step:380/1845 train_time:13117ms step_avg:34.52ms
step:381/1845 train_time:13151ms step_avg:34.52ms
step:382/1845 train_time:13185ms step_avg:34.52ms
step:383/1845 train_time:13219ms step_avg:34.52ms
step:384/1845 train_time:13253ms step_avg:34.51ms
step:385/1845 train_time:13288ms step_avg:34.51ms
step:386/1845 train_time:13322ms step_avg:34.51ms
step:387/1845 train_time:13357ms step_avg:34.51ms
step:388/1845 train_time:13391ms step_avg:34.51ms
step:389/1845 train_time:13425ms step_avg:34.51ms
step:390/1845 train_time:13459ms step_avg:34.51ms
step:391/1845 train_time:13494ms step_avg:34.51ms
step:392/1845 train_time:13528ms step_avg:34.51ms
step:393/1845 train_time:13562ms step_avg:34.51ms
step:394/1845 train_time:13596ms step_avg:34.51ms
step:395/1845 train_time:13631ms step_avg:34.51ms
step:396/1845 train_time:13665ms step_avg:34.51ms
step:397/1845 train_time:13699ms step_avg:34.51ms
step:398/1845 train_time:13734ms step_avg:34.51ms
step:399/1845 train_time:13768ms step_avg:34.51ms
step:400/1845 train_time:13802ms step_avg:34.51ms
step:401/1845 train_time:13837ms step_avg:34.51ms
step:402/1845 train_time:13871ms step_avg:34.51ms
step:403/1845 train_time:13906ms step_avg:34.51ms
step:404/1845 train_time:13940ms step_avg:34.50ms
step:405/1845 train_time:13975ms step_avg:34.51ms
step:406/1845 train_time:14009ms step_avg:34.50ms
step:407/1845 train_time:14043ms step_avg:34.50ms
step:408/1845 train_time:14078ms step_avg:34.50ms
step:409/1845 train_time:14112ms step_avg:34.50ms
step:410/1845 train_time:14146ms step_avg:34.50ms
step:411/1845 train_time:14180ms step_avg:34.50ms
step:412/1845 train_time:14215ms step_avg:34.50ms
step:413/1845 train_time:14249ms step_avg:34.50ms
step:414/1845 train_time:14283ms step_avg:34.50ms
step:415/1845 train_time:14318ms step_avg:34.50ms
step:416/1845 train_time:14352ms step_avg:34.50ms
step:417/1845 train_time:14386ms step_avg:34.50ms
step:418/1845 train_time:14421ms step_avg:34.50ms
step:419/1845 train_time:14455ms step_avg:34.50ms
step:420/1845 train_time:14489ms step_avg:34.50ms
step:421/1845 train_time:14523ms step_avg:34.50ms
step:422/1845 train_time:14557ms step_avg:34.50ms
step:423/1845 train_time:14592ms step_avg:34.50ms
step:424/1845 train_time:14626ms step_avg:34.49ms
step:425/1845 train_time:14660ms step_avg:34.49ms
step:426/1845 train_time:14694ms step_avg:34.49ms
step:427/1845 train_time:14729ms step_avg:34.49ms
step:428/1845 train_time:14763ms step_avg:34.49ms
step:429/1845 train_time:14798ms step_avg:34.49ms
step:430/1845 train_time:14832ms step_avg:34.49ms
step:431/1845 train_time:14866ms step_avg:34.49ms
step:432/1845 train_time:14900ms step_avg:34.49ms
step:433/1845 train_time:14934ms step_avg:34.49ms
step:434/1845 train_time:14968ms step_avg:34.49ms
step:435/1845 train_time:15003ms step_avg:34.49ms
step:436/1845 train_time:15037ms step_avg:34.49ms
step:437/1845 train_time:15071ms step_avg:34.49ms
step:438/1845 train_time:15105ms step_avg:34.49ms
step:439/1845 train_time:15140ms step_avg:34.49ms
step:440/1845 train_time:15174ms step_avg:34.49ms
step:441/1845 train_time:15208ms step_avg:34.49ms
step:442/1845 train_time:15242ms step_avg:34.49ms
step:443/1845 train_time:15277ms step_avg:34.49ms
step:444/1845 train_time:15311ms step_avg:34.48ms
step:445/1845 train_time:15346ms step_avg:34.49ms
step:446/1845 train_time:15380ms step_avg:34.48ms
step:447/1845 train_time:15415ms step_avg:34.48ms
step:448/1845 train_time:15449ms step_avg:34.48ms
step:449/1845 train_time:15483ms step_avg:34.48ms
step:450/1845 train_time:15517ms step_avg:34.48ms
step:451/1845 train_time:15552ms step_avg:34.48ms
step:452/1845 train_time:15586ms step_avg:34.48ms
step:453/1845 train_time:15620ms step_avg:34.48ms
step:454/1845 train_time:15654ms step_avg:34.48ms
step:455/1845 train_time:15689ms step_avg:34.48ms
step:456/1845 train_time:15723ms step_avg:34.48ms
step:457/1845 train_time:15757ms step_avg:34.48ms
step:458/1845 train_time:15791ms step_avg:34.48ms
step:459/1845 train_time:15826ms step_avg:34.48ms
step:460/1845 train_time:15860ms step_avg:34.48ms
step:461/1845 train_time:15894ms step_avg:34.48ms
step:462/1845 train_time:15928ms step_avg:34.48ms
step:463/1845 train_time:15963ms step_avg:34.48ms
step:464/1845 train_time:15997ms step_avg:34.48ms
step:465/1845 train_time:16031ms step_avg:34.48ms
step:466/1845 train_time:16066ms step_avg:34.48ms
step:467/1845 train_time:16100ms step_avg:34.48ms
step:468/1845 train_time:16134ms step_avg:34.48ms
step:469/1845 train_time:16169ms step_avg:34.48ms
step:470/1845 train_time:16203ms step_avg:34.47ms
step:471/1845 train_time:16238ms step_avg:34.47ms
step:472/1845 train_time:16272ms step_avg:34.47ms
step:473/1845 train_time:16306ms step_avg:34.47ms
step:474/1845 train_time:16340ms step_avg:34.47ms
step:475/1845 train_time:16375ms step_avg:34.47ms
step:476/1845 train_time:16409ms step_avg:34.47ms
step:477/1845 train_time:16444ms step_avg:34.47ms
step:478/1845 train_time:16478ms step_avg:34.47ms
step:479/1845 train_time:16512ms step_avg:34.47ms
step:480/1845 train_time:16546ms step_avg:34.47ms
step:481/1845 train_time:16581ms step_avg:34.47ms
step:482/1845 train_time:16615ms step_avg:34.47ms
step:483/1845 train_time:16649ms step_avg:34.47ms
step:484/1845 train_time:16683ms step_avg:34.47ms
step:485/1845 train_time:16717ms step_avg:34.47ms
step:486/1845 train_time:16751ms step_avg:34.47ms
step:487/1845 train_time:16786ms step_avg:34.47ms
step:488/1845 train_time:16820ms step_avg:34.47ms
step:489/1845 train_time:16855ms step_avg:34.47ms
step:490/1845 train_time:16889ms step_avg:34.47ms
step:491/1845 train_time:16923ms step_avg:34.47ms
step:492/1845 train_time:16957ms step_avg:34.47ms
step:493/1845 train_time:16992ms step_avg:34.47ms
step:494/1845 train_time:17026ms step_avg:34.46ms
step:495/1845 train_time:17060ms step_avg:34.47ms
step:496/1845 train_time:17094ms step_avg:34.46ms
step:497/1845 train_time:17129ms step_avg:34.46ms
step:498/1845 train_time:17163ms step_avg:34.46ms
step:499/1845 train_time:17197ms step_avg:34.46ms
step:500/1845 train_time:17231ms step_avg:34.46ms
step:500/1845 val_loss:4.2912 train_time:17268ms step_avg:34.54ms
step:501/1845 train_time:17287ms step_avg:34.50ms
step:502/1845 train_time:17306ms step_avg:34.47ms
step:503/1845 train_time:17336ms step_avg:34.47ms
step:504/1845 train_time:17371ms step_avg:34.47ms
step:505/1845 train_time:17406ms step_avg:34.47ms
step:506/1845 train_time:17440ms step_avg:34.47ms
step:507/1845 train_time:17475ms step_avg:34.47ms
step:508/1845 train_time:17509ms step_avg:34.47ms
step:509/1845 train_time:17543ms step_avg:34.47ms
step:510/1845 train_time:17578ms step_avg:34.47ms
step:511/1845 train_time:17612ms step_avg:34.46ms
step:512/1845 train_time:17646ms step_avg:34.46ms
step:513/1845 train_time:17680ms step_avg:34.46ms
step:514/1845 train_time:17714ms step_avg:34.46ms
step:515/1845 train_time:17748ms step_avg:34.46ms
step:516/1845 train_time:17782ms step_avg:34.46ms
step:517/1845 train_time:17816ms step_avg:34.46ms
step:518/1845 train_time:17850ms step_avg:34.46ms
step:519/1845 train_time:17885ms step_avg:34.46ms
step:520/1845 train_time:17918ms step_avg:34.46ms
step:521/1845 train_time:17953ms step_avg:34.46ms
step:522/1845 train_time:17987ms step_avg:34.46ms
step:523/1845 train_time:18021ms step_avg:34.46ms
step:524/1845 train_time:18055ms step_avg:34.46ms
step:525/1845 train_time:18089ms step_avg:34.46ms
step:526/1845 train_time:18123ms step_avg:34.45ms
step:527/1845 train_time:18157ms step_avg:34.45ms
step:528/1845 train_time:18191ms step_avg:34.45ms
step:529/1845 train_time:18226ms step_avg:34.45ms
step:530/1845 train_time:18260ms step_avg:34.45ms
step:531/1845 train_time:18295ms step_avg:34.45ms
step:532/1845 train_time:18329ms step_avg:34.45ms
step:533/1845 train_time:18363ms step_avg:34.45ms
step:534/1845 train_time:18397ms step_avg:34.45ms
step:535/1845 train_time:18432ms step_avg:34.45ms
step:536/1845 train_time:18466ms step_avg:34.45ms
step:537/1845 train_time:18500ms step_avg:34.45ms
step:538/1845 train_time:18535ms step_avg:34.45ms
step:539/1845 train_time:18569ms step_avg:34.45ms
step:540/1845 train_time:18603ms step_avg:34.45ms
step:541/1845 train_time:18637ms step_avg:34.45ms
step:542/1845 train_time:18671ms step_avg:34.45ms
step:543/1845 train_time:18706ms step_avg:34.45ms
step:544/1845 train_time:18740ms step_avg:34.45ms
step:545/1845 train_time:18774ms step_avg:34.45ms
step:546/1845 train_time:18808ms step_avg:34.45ms
step:547/1845 train_time:18842ms step_avg:34.45ms
step:548/1845 train_time:18876ms step_avg:34.45ms
step:549/1845 train_time:18911ms step_avg:34.45ms
step:550/1845 train_time:18945ms step_avg:34.45ms
step:551/1845 train_time:18979ms step_avg:34.45ms
step:552/1845 train_time:19013ms step_avg:34.44ms
step:553/1845 train_time:19048ms step_avg:34.44ms
step:554/1845 train_time:19082ms step_avg:34.44ms
step:555/1845 train_time:19116ms step_avg:34.44ms
step:556/1845 train_time:19150ms step_avg:34.44ms
step:557/1845 train_time:19184ms step_avg:34.44ms
step:558/1845 train_time:19218ms step_avg:34.44ms
step:559/1845 train_time:19253ms step_avg:34.44ms
step:560/1845 train_time:19286ms step_avg:34.44ms
step:561/1845 train_time:19320ms step_avg:34.44ms
step:562/1845 train_time:19355ms step_avg:34.44ms
step:563/1845 train_time:19389ms step_avg:34.44ms
step:564/1845 train_time:19423ms step_avg:34.44ms
step:565/1845 train_time:19457ms step_avg:34.44ms
step:566/1845 train_time:19492ms step_avg:34.44ms
step:567/1845 train_time:19526ms step_avg:34.44ms
step:568/1845 train_time:19560ms step_avg:34.44ms
step:569/1845 train_time:19594ms step_avg:34.44ms
step:570/1845 train_time:19628ms step_avg:34.43ms
step:571/1845 train_time:19662ms step_avg:34.44ms
step:572/1845 train_time:19696ms step_avg:34.43ms
step:573/1845 train_time:19731ms step_avg:34.43ms
step:574/1845 train_time:19765ms step_avg:34.43ms
step:575/1845 train_time:19799ms step_avg:34.43ms
step:576/1845 train_time:19833ms step_avg:34.43ms
step:577/1845 train_time:19868ms step_avg:34.43ms
step:578/1845 train_time:19902ms step_avg:34.43ms
step:579/1845 train_time:19937ms step_avg:34.43ms
step:580/1845 train_time:19971ms step_avg:34.43ms
step:581/1845 train_time:20005ms step_avg:34.43ms
step:582/1845 train_time:20039ms step_avg:34.43ms
step:583/1845 train_time:20074ms step_avg:34.43ms
step:584/1845 train_time:20108ms step_avg:34.43ms
step:585/1845 train_time:20142ms step_avg:34.43ms
step:586/1845 train_time:20176ms step_avg:34.43ms
step:587/1845 train_time:20211ms step_avg:34.43ms
step:588/1845 train_time:20245ms step_avg:34.43ms
step:589/1845 train_time:20279ms step_avg:34.43ms
step:590/1845 train_time:20313ms step_avg:34.43ms
step:591/1845 train_time:20347ms step_avg:34.43ms
step:592/1845 train_time:20381ms step_avg:34.43ms
step:593/1845 train_time:20416ms step_avg:34.43ms
step:594/1845 train_time:20450ms step_avg:34.43ms
step:595/1845 train_time:20484ms step_avg:34.43ms
step:596/1845 train_time:20518ms step_avg:34.43ms
step:597/1845 train_time:20552ms step_avg:34.43ms
step:598/1845 train_time:20586ms step_avg:34.43ms
step:599/1845 train_time:20621ms step_avg:34.42ms
step:600/1845 train_time:20655ms step_avg:34.42ms
step:601/1845 train_time:20689ms step_avg:34.42ms
step:602/1845 train_time:20723ms step_avg:34.42ms
step:603/1845 train_time:20758ms step_avg:34.42ms
step:604/1845 train_time:20818ms step_avg:34.47ms
step:605/1845 train_time:20880ms step_avg:34.51ms
step:606/1845 train_time:20941ms step_avg:34.56ms
step:607/1845 train_time:21004ms step_avg:34.60ms
step:608/1845 train_time:21065ms step_avg:34.65ms
step:609/1845 train_time:21127ms step_avg:34.69ms
step:610/1845 train_time:21188ms step_avg:34.73ms
step:611/1845 train_time:21250ms step_avg:34.78ms
step:612/1845 train_time:21312ms step_avg:34.82ms
step:613/1845 train_time:21374ms step_avg:34.87ms
step:614/1845 train_time:21435ms step_avg:34.91ms
step:615/1845 train_time:21498ms step_avg:34.96ms
step:616/1845 train_time:21559ms step_avg:35.00ms
step:617/1845 train_time:21621ms step_avg:35.04ms
step:618/1845 train_time:21683ms step_avg:35.09ms
step:619/1845 train_time:21745ms step_avg:35.13ms
step:620/1845 train_time:21805ms step_avg:35.17ms
step:621/1845 train_time:21868ms step_avg:35.21ms
step:622/1845 train_time:21929ms step_avg:35.26ms
step:623/1845 train_time:21992ms step_avg:35.30ms
step:624/1845 train_time:22053ms step_avg:35.34ms
step:625/1845 train_time:22115ms step_avg:35.38ms
step:626/1845 train_time:22176ms step_avg:35.43ms
step:627/1845 train_time:22239ms step_avg:35.47ms
step:628/1845 train_time:22300ms step_avg:35.51ms
step:629/1845 train_time:22363ms step_avg:35.55ms
step:630/1845 train_time:22424ms step_avg:35.59ms
step:631/1845 train_time:22486ms step_avg:35.64ms
step:632/1845 train_time:22547ms step_avg:35.68ms
step:633/1845 train_time:22609ms step_avg:35.72ms
step:634/1845 train_time:22671ms step_avg:35.76ms
step:635/1845 train_time:22734ms step_avg:35.80ms
step:636/1845 train_time:22795ms step_avg:35.84ms
step:637/1845 train_time:22857ms step_avg:35.88ms
step:638/1845 train_time:22918ms step_avg:35.92ms
step:639/1845 train_time:22982ms step_avg:35.97ms
step:640/1845 train_time:23043ms step_avg:36.00ms
step:641/1845 train_time:23105ms step_avg:36.05ms
step:642/1845 train_time:23166ms step_avg:36.08ms
step:643/1845 train_time:23228ms step_avg:36.12ms
step:644/1845 train_time:23289ms step_avg:36.16ms
step:645/1845 train_time:23352ms step_avg:36.20ms
step:646/1845 train_time:23413ms step_avg:36.24ms
step:647/1845 train_time:23475ms step_avg:36.28ms
step:648/1845 train_time:23536ms step_avg:36.32ms
step:649/1845 train_time:23598ms step_avg:36.36ms
step:650/1845 train_time:23659ms step_avg:36.40ms
step:651/1845 train_time:23723ms step_avg:36.44ms
step:652/1845 train_time:23784ms step_avg:36.48ms
step:653/1845 train_time:23846ms step_avg:36.52ms
step:654/1845 train_time:23907ms step_avg:36.55ms
step:655/1845 train_time:23969ms step_avg:36.59ms
step:656/1845 train_time:24030ms step_avg:36.63ms
step:657/1845 train_time:24093ms step_avg:36.67ms
step:658/1845 train_time:24153ms step_avg:36.71ms
step:659/1845 train_time:24216ms step_avg:36.75ms
step:660/1845 train_time:24278ms step_avg:36.78ms
step:661/1845 train_time:24340ms step_avg:36.82ms
step:662/1845 train_time:24401ms step_avg:36.86ms
step:663/1845 train_time:24464ms step_avg:36.90ms
step:664/1845 train_time:24525ms step_avg:36.93ms
step:665/1845 train_time:24587ms step_avg:36.97ms
step:666/1845 train_time:24647ms step_avg:37.01ms
step:667/1845 train_time:24710ms step_avg:37.05ms
step:668/1845 train_time:24770ms step_avg:37.08ms
step:669/1845 train_time:24833ms step_avg:37.12ms
step:670/1845 train_time:24894ms step_avg:37.15ms
step:671/1845 train_time:24956ms step_avg:37.19ms
step:672/1845 train_time:25017ms step_avg:37.23ms
step:673/1845 train_time:25080ms step_avg:37.27ms
step:674/1845 train_time:25140ms step_avg:37.30ms
step:675/1845 train_time:25203ms step_avg:37.34ms
step:676/1845 train_time:25264ms step_avg:37.37ms
step:677/1845 train_time:25326ms step_avg:37.41ms
step:678/1845 train_time:25387ms step_avg:37.44ms
step:679/1845 train_time:25450ms step_avg:37.48ms
step:680/1845 train_time:25511ms step_avg:37.52ms
step:681/1845 train_time:25573ms step_avg:37.55ms
step:682/1845 train_time:25634ms step_avg:37.59ms
step:683/1845 train_time:25696ms step_avg:37.62ms
step:684/1845 train_time:25757ms step_avg:37.66ms
step:685/1845 train_time:25820ms step_avg:37.69ms
step:686/1845 train_time:25881ms step_avg:37.73ms
step:687/1845 train_time:25943ms step_avg:37.76ms
step:688/1845 train_time:26004ms step_avg:37.80ms
step:689/1845 train_time:26066ms step_avg:37.83ms
step:690/1845 train_time:26127ms step_avg:37.86ms
step:691/1845 train_time:26189ms step_avg:37.90ms
step:692/1845 train_time:26250ms step_avg:37.93ms
step:693/1845 train_time:26312ms step_avg:37.97ms
step:694/1845 train_time:26373ms step_avg:38.00ms
step:695/1845 train_time:26436ms step_avg:38.04ms
step:696/1845 train_time:26497ms step_avg:38.07ms
step:697/1845 train_time:26559ms step_avg:38.11ms
step:698/1845 train_time:26620ms step_avg:38.14ms
step:699/1845 train_time:26683ms step_avg:38.17ms
step:700/1845 train_time:26744ms step_avg:38.21ms
step:701/1845 train_time:26806ms step_avg:38.24ms
step:702/1845 train_time:26867ms step_avg:38.27ms
step:703/1845 train_time:26929ms step_avg:38.31ms
step:704/1845 train_time:26990ms step_avg:38.34ms
step:705/1845 train_time:27053ms step_avg:38.37ms
step:706/1845 train_time:27114ms step_avg:38.40ms
step:707/1845 train_time:27176ms step_avg:38.44ms
step:708/1845 train_time:27238ms step_avg:38.47ms
step:709/1845 train_time:27300ms step_avg:38.51ms
step:710/1845 train_time:27361ms step_avg:38.54ms
step:711/1845 train_time:27424ms step_avg:38.57ms
step:712/1845 train_time:27485ms step_avg:38.60ms
step:713/1845 train_time:27547ms step_avg:38.64ms
step:714/1845 train_time:27608ms step_avg:38.67ms
step:715/1845 train_time:27670ms step_avg:38.70ms
step:716/1845 train_time:27731ms step_avg:38.73ms
step:717/1845 train_time:27794ms step_avg:38.76ms
step:718/1845 train_time:27855ms step_avg:38.79ms
step:719/1845 train_time:27917ms step_avg:38.83ms
step:720/1845 train_time:27979ms step_avg:38.86ms
step:721/1845 train_time:28041ms step_avg:38.89ms
step:722/1845 train_time:28102ms step_avg:38.92ms
step:723/1845 train_time:28165ms step_avg:38.96ms
step:724/1845 train_time:28226ms step_avg:38.99ms
step:725/1845 train_time:28288ms step_avg:39.02ms
step:726/1845 train_time:28349ms step_avg:39.05ms
step:727/1845 train_time:28411ms step_avg:39.08ms
step:728/1845 train_time:28472ms step_avg:39.11ms
step:729/1845 train_time:28534ms step_avg:39.14ms
step:730/1845 train_time:28596ms step_avg:39.17ms
step:731/1845 train_time:28659ms step_avg:39.20ms
step:732/1845 train_time:28720ms step_avg:39.24ms
step:733/1845 train_time:28782ms step_avg:39.27ms
step:734/1845 train_time:28844ms step_avg:39.30ms
step:735/1845 train_time:28906ms step_avg:39.33ms
step:736/1845 train_time:28967ms step_avg:39.36ms
step:737/1845 train_time:29029ms step_avg:39.39ms
step:738/1845 train_time:29090ms step_avg:39.42ms
step:739/1845 train_time:29152ms step_avg:39.45ms
step:740/1845 train_time:29212ms step_avg:39.48ms
step:741/1845 train_time:29276ms step_avg:39.51ms
step:742/1845 train_time:29337ms step_avg:39.54ms
step:743/1845 train_time:29399ms step_avg:39.57ms
step:744/1845 train_time:29460ms step_avg:39.60ms
step:745/1845 train_time:29522ms step_avg:39.63ms
step:746/1845 train_time:29583ms step_avg:39.66ms
step:747/1845 train_time:29645ms step_avg:39.69ms
step:748/1845 train_time:29706ms step_avg:39.71ms
step:749/1845 train_time:29769ms step_avg:39.75ms
step:750/1845 train_time:29830ms step_avg:39.77ms
step:750/1845 val_loss:4.0336 train_time:29893ms step_avg:39.86ms
step:751/1845 train_time:29913ms step_avg:39.83ms
step:752/1845 train_time:29956ms step_avg:39.84ms
step:753/1845 train_time:30020ms step_avg:39.87ms
step:754/1845 train_time:30082ms step_avg:39.90ms
step:755/1845 train_time:30144ms step_avg:39.93ms
step:756/1845 train_time:30206ms step_avg:39.96ms
step:757/1845 train_time:30268ms step_avg:39.98ms
step:758/1845 train_time:30328ms step_avg:40.01ms
step:759/1845 train_time:30390ms step_avg:40.04ms
step:760/1845 train_time:30450ms step_avg:40.07ms
step:761/1845 train_time:30512ms step_avg:40.09ms
step:762/1845 train_time:30572ms step_avg:40.12ms
step:763/1845 train_time:30634ms step_avg:40.15ms
step:764/1845 train_time:30694ms step_avg:40.18ms
step:765/1845 train_time:30756ms step_avg:40.20ms
step:766/1845 train_time:30817ms step_avg:40.23ms
step:767/1845 train_time:30880ms step_avg:40.26ms
step:768/1845 train_time:30941ms step_avg:40.29ms
step:769/1845 train_time:31004ms step_avg:40.32ms
step:770/1845 train_time:31066ms step_avg:40.35ms
step:771/1845 train_time:31129ms step_avg:40.37ms
step:772/1845 train_time:31190ms step_avg:40.40ms
step:773/1845 train_time:31252ms step_avg:40.43ms
step:774/1845 train_time:31313ms step_avg:40.46ms
step:775/1845 train_time:31375ms step_avg:40.48ms
step:776/1845 train_time:31436ms step_avg:40.51ms
step:777/1845 train_time:31498ms step_avg:40.54ms
step:778/1845 train_time:31558ms step_avg:40.56ms
step:779/1845 train_time:31620ms step_avg:40.59ms
step:780/1845 train_time:31681ms step_avg:40.62ms
step:781/1845 train_time:31744ms step_avg:40.65ms
step:782/1845 train_time:31805ms step_avg:40.67ms
step:783/1845 train_time:31868ms step_avg:40.70ms
step:784/1845 train_time:31929ms step_avg:40.73ms
step:785/1845 train_time:31992ms step_avg:40.75ms
step:786/1845 train_time:32053ms step_avg:40.78ms
step:787/1845 train_time:32116ms step_avg:40.81ms
step:788/1845 train_time:32178ms step_avg:40.83ms
step:789/1845 train_time:32240ms step_avg:40.86ms
step:790/1845 train_time:32301ms step_avg:40.89ms
step:791/1845 train_time:32364ms step_avg:40.92ms
step:792/1845 train_time:32425ms step_avg:40.94ms
step:793/1845 train_time:32487ms step_avg:40.97ms
step:794/1845 train_time:32548ms step_avg:40.99ms
step:795/1845 train_time:32610ms step_avg:41.02ms
step:796/1845 train_time:32671ms step_avg:41.04ms
step:797/1845 train_time:32733ms step_avg:41.07ms
step:798/1845 train_time:32793ms step_avg:41.09ms
step:799/1845 train_time:32856ms step_avg:41.12ms
step:800/1845 train_time:32917ms step_avg:41.15ms
step:801/1845 train_time:32980ms step_avg:41.17ms
step:802/1845 train_time:33041ms step_avg:41.20ms
step:803/1845 train_time:33104ms step_avg:41.23ms
step:804/1845 train_time:33165ms step_avg:41.25ms
step:805/1845 train_time:33228ms step_avg:41.28ms
step:806/1845 train_time:33288ms step_avg:41.30ms
step:807/1845 train_time:33351ms step_avg:41.33ms
step:808/1845 train_time:33412ms step_avg:41.35ms
step:809/1845 train_time:33475ms step_avg:41.38ms
step:810/1845 train_time:33535ms step_avg:41.40ms
step:811/1845 train_time:33597ms step_avg:41.43ms
step:812/1845 train_time:33658ms step_avg:41.45ms
step:813/1845 train_time:33720ms step_avg:41.48ms
step:814/1845 train_time:33781ms step_avg:41.50ms
step:815/1845 train_time:33843ms step_avg:41.53ms
step:816/1845 train_time:33904ms step_avg:41.55ms
step:817/1845 train_time:33967ms step_avg:41.57ms
step:818/1845 train_time:34028ms step_avg:41.60ms
step:819/1845 train_time:34090ms step_avg:41.62ms
step:820/1845 train_time:34151ms step_avg:41.65ms
step:821/1845 train_time:34215ms step_avg:41.67ms
step:822/1845 train_time:34276ms step_avg:41.70ms
step:823/1845 train_time:34338ms step_avg:41.72ms
step:824/1845 train_time:34399ms step_avg:41.75ms
step:825/1845 train_time:34462ms step_avg:41.77ms
step:826/1845 train_time:34523ms step_avg:41.80ms
step:827/1845 train_time:34586ms step_avg:41.82ms
step:828/1845 train_time:34647ms step_avg:41.84ms
step:829/1845 train_time:34710ms step_avg:41.87ms
step:830/1845 train_time:34770ms step_avg:41.89ms
step:831/1845 train_time:34832ms step_avg:41.92ms
step:832/1845 train_time:34892ms step_avg:41.94ms
step:833/1845 train_time:34954ms step_avg:41.96ms
step:834/1845 train_time:35015ms step_avg:41.98ms
step:835/1845 train_time:35077ms step_avg:42.01ms
step:836/1845 train_time:35138ms step_avg:42.03ms
step:837/1845 train_time:35201ms step_avg:42.06ms
step:838/1845 train_time:35262ms step_avg:42.08ms
step:839/1845 train_time:35324ms step_avg:42.10ms
step:840/1845 train_time:35385ms step_avg:42.13ms
step:841/1845 train_time:35448ms step_avg:42.15ms
step:842/1845 train_time:35509ms step_avg:42.17ms
step:843/1845 train_time:35571ms step_avg:42.20ms
step:844/1845 train_time:35632ms step_avg:42.22ms
step:845/1845 train_time:35695ms step_avg:42.24ms
step:846/1845 train_time:35756ms step_avg:42.26ms
step:847/1845 train_time:35818ms step_avg:42.29ms
step:848/1845 train_time:35879ms step_avg:42.31ms
step:849/1845 train_time:35942ms step_avg:42.33ms
step:850/1845 train_time:36002ms step_avg:42.36ms
step:851/1845 train_time:36065ms step_avg:42.38ms
step:852/1845 train_time:36127ms step_avg:42.40ms
step:853/1845 train_time:36189ms step_avg:42.43ms
step:854/1845 train_time:36250ms step_avg:42.45ms
step:855/1845 train_time:36312ms step_avg:42.47ms
step:856/1845 train_time:36373ms step_avg:42.49ms
step:857/1845 train_time:36435ms step_avg:42.52ms
step:858/1845 train_time:36497ms step_avg:42.54ms
step:859/1845 train_time:36558ms step_avg:42.56ms
step:860/1845 train_time:36620ms step_avg:42.58ms
step:861/1845 train_time:36682ms step_avg:42.60ms
step:862/1845 train_time:36744ms step_avg:42.63ms
step:863/1845 train_time:36807ms step_avg:42.65ms
step:864/1845 train_time:36868ms step_avg:42.67ms
step:865/1845 train_time:36930ms step_avg:42.69ms
step:866/1845 train_time:36990ms step_avg:42.71ms
step:867/1845 train_time:37053ms step_avg:42.74ms
step:868/1845 train_time:37114ms step_avg:42.76ms
step:869/1845 train_time:37177ms step_avg:42.78ms
step:870/1845 train_time:37237ms step_avg:42.80ms
step:871/1845 train_time:37300ms step_avg:42.82ms
step:872/1845 train_time:37361ms step_avg:42.85ms
step:873/1845 train_time:37423ms step_avg:42.87ms
step:874/1845 train_time:37484ms step_avg:42.89ms
step:875/1845 train_time:37547ms step_avg:42.91ms
step:876/1845 train_time:37608ms step_avg:42.93ms
step:877/1845 train_time:37670ms step_avg:42.95ms
step:878/1845 train_time:37731ms step_avg:42.97ms
step:879/1845 train_time:37794ms step_avg:43.00ms
step:880/1845 train_time:37855ms step_avg:43.02ms
step:881/1845 train_time:37917ms step_avg:43.04ms
step:882/1845 train_time:37978ms step_avg:43.06ms
step:883/1845 train_time:38041ms step_avg:43.08ms
step:884/1845 train_time:38102ms step_avg:43.10ms
step:885/1845 train_time:38164ms step_avg:43.12ms
step:886/1845 train_time:38225ms step_avg:43.14ms
step:887/1845 train_time:38287ms step_avg:43.16ms
step:888/1845 train_time:38348ms step_avg:43.18ms
step:889/1845 train_time:38410ms step_avg:43.21ms
step:890/1845 train_time:38471ms step_avg:43.23ms
step:891/1845 train_time:38534ms step_avg:43.25ms
step:892/1845 train_time:38595ms step_avg:43.27ms
step:893/1845 train_time:38657ms step_avg:43.29ms
step:894/1845 train_time:38717ms step_avg:43.31ms
step:895/1845 train_time:38780ms step_avg:43.33ms
step:896/1845 train_time:38840ms step_avg:43.35ms
step:897/1845 train_time:38903ms step_avg:43.37ms
step:898/1845 train_time:38964ms step_avg:43.39ms
step:899/1845 train_time:39027ms step_avg:43.41ms
step:900/1845 train_time:39088ms step_avg:43.43ms
step:901/1845 train_time:39150ms step_avg:43.45ms
step:902/1845 train_time:39211ms step_avg:43.47ms
step:903/1845 train_time:39273ms step_avg:43.49ms
step:904/1845 train_time:39334ms step_avg:43.51ms
step:905/1845 train_time:39396ms step_avg:43.53ms
step:906/1845 train_time:39458ms step_avg:43.55ms
step:907/1845 train_time:39520ms step_avg:43.57ms
step:908/1845 train_time:39581ms step_avg:43.59ms
step:909/1845 train_time:39644ms step_avg:43.61ms
step:910/1845 train_time:39705ms step_avg:43.63ms
step:911/1845 train_time:39767ms step_avg:43.65ms
step:912/1845 train_time:39828ms step_avg:43.67ms
step:913/1845 train_time:39890ms step_avg:43.69ms
step:914/1845 train_time:39951ms step_avg:43.71ms
step:915/1845 train_time:40014ms step_avg:43.73ms
step:916/1845 train_time:40075ms step_avg:43.75ms
step:917/1845 train_time:40138ms step_avg:43.77ms
step:918/1845 train_time:40198ms step_avg:43.79ms
step:919/1845 train_time:40261ms step_avg:43.81ms
step:920/1845 train_time:40323ms step_avg:43.83ms
step:921/1845 train_time:40385ms step_avg:43.85ms
step:922/1845 train_time:40446ms step_avg:43.87ms
step:923/1845 train_time:40509ms step_avg:43.89ms
step:924/1845 train_time:40570ms step_avg:43.91ms
step:925/1845 train_time:40632ms step_avg:43.93ms
step:926/1845 train_time:40693ms step_avg:43.94ms
step:927/1845 train_time:40756ms step_avg:43.96ms
step:928/1845 train_time:40817ms step_avg:43.98ms
step:929/1845 train_time:40879ms step_avg:44.00ms
step:930/1845 train_time:40940ms step_avg:44.02ms
step:931/1845 train_time:41003ms step_avg:44.04ms
step:932/1845 train_time:41064ms step_avg:44.06ms
step:933/1845 train_time:41127ms step_avg:44.08ms
step:934/1845 train_time:41187ms step_avg:44.10ms
step:935/1845 train_time:41249ms step_avg:44.12ms
step:936/1845 train_time:41310ms step_avg:44.13ms
step:937/1845 train_time:41372ms step_avg:44.15ms
step:938/1845 train_time:41432ms step_avg:44.17ms
step:939/1845 train_time:41495ms step_avg:44.19ms
step:940/1845 train_time:41555ms step_avg:44.21ms
step:941/1845 train_time:41618ms step_avg:44.23ms
step:942/1845 train_time:41679ms step_avg:44.24ms
step:943/1845 train_time:41741ms step_avg:44.26ms
step:944/1845 train_time:41802ms step_avg:44.28ms
step:945/1845 train_time:41864ms step_avg:44.30ms
step:946/1845 train_time:41925ms step_avg:44.32ms
step:947/1845 train_time:41987ms step_avg:44.34ms
step:948/1845 train_time:42048ms step_avg:44.35ms
step:949/1845 train_time:42111ms step_avg:44.37ms
step:950/1845 train_time:42172ms step_avg:44.39ms
step:951/1845 train_time:42235ms step_avg:44.41ms
step:952/1845 train_time:42296ms step_avg:44.43ms
step:953/1845 train_time:42358ms step_avg:44.45ms
step:954/1845 train_time:42419ms step_avg:44.46ms
step:955/1845 train_time:42482ms step_avg:44.48ms
step:956/1845 train_time:42543ms step_avg:44.50ms
step:957/1845 train_time:42606ms step_avg:44.52ms
step:958/1845 train_time:42667ms step_avg:44.54ms
step:959/1845 train_time:42730ms step_avg:44.56ms
step:960/1845 train_time:42790ms step_avg:44.57ms
step:961/1845 train_time:42852ms step_avg:44.59ms
step:962/1845 train_time:42913ms step_avg:44.61ms
step:963/1845 train_time:42976ms step_avg:44.63ms
step:964/1845 train_time:43036ms step_avg:44.64ms
step:965/1845 train_time:43099ms step_avg:44.66ms
step:966/1845 train_time:43160ms step_avg:44.68ms
step:967/1845 train_time:43222ms step_avg:44.70ms
step:968/1845 train_time:43284ms step_avg:44.71ms
step:969/1845 train_time:43346ms step_avg:44.73ms
step:970/1845 train_time:43407ms step_avg:44.75ms
step:971/1845 train_time:43469ms step_avg:44.77ms
step:972/1845 train_time:43530ms step_avg:44.78ms
step:973/1845 train_time:43592ms step_avg:44.80ms
step:974/1845 train_time:43653ms step_avg:44.82ms
step:975/1845 train_time:43716ms step_avg:44.84ms
step:976/1845 train_time:43777ms step_avg:44.85ms
step:977/1845 train_time:43839ms step_avg:44.87ms
step:978/1845 train_time:43901ms step_avg:44.89ms
step:979/1845 train_time:43963ms step_avg:44.91ms
step:980/1845 train_time:44024ms step_avg:44.92ms
step:981/1845 train_time:44087ms step_avg:44.94ms
step:982/1845 train_time:44148ms step_avg:44.96ms
step:983/1845 train_time:44210ms step_avg:44.97ms
step:984/1845 train_time:44271ms step_avg:44.99ms
step:985/1845 train_time:44333ms step_avg:45.01ms
step:986/1845 train_time:44394ms step_avg:45.02ms
step:987/1845 train_time:44457ms step_avg:45.04ms
step:988/1845 train_time:44517ms step_avg:45.06ms
step:989/1845 train_time:44580ms step_avg:45.08ms
step:990/1845 train_time:44641ms step_avg:45.09ms
step:991/1845 train_time:44703ms step_avg:45.11ms
step:992/1845 train_time:44764ms step_avg:45.13ms
step:993/1845 train_time:44827ms step_avg:45.14ms
step:994/1845 train_time:44888ms step_avg:45.16ms
step:995/1845 train_time:44951ms step_avg:45.18ms
step:996/1845 train_time:45011ms step_avg:45.19ms
step:997/1845 train_time:45073ms step_avg:45.21ms
step:998/1845 train_time:45135ms step_avg:45.22ms
step:999/1845 train_time:45197ms step_avg:45.24ms
step:1000/1845 train_time:45258ms step_avg:45.26ms
step:1000/1845 val_loss:3.7719 train_time:45322ms step_avg:45.32ms
step:1001/1845 train_time:45342ms step_avg:45.30ms
step:1002/1845 train_time:45383ms step_avg:45.29ms
step:1003/1845 train_time:45448ms step_avg:45.31ms
step:1004/1845 train_time:45512ms step_avg:45.33ms
step:1005/1845 train_time:45575ms step_avg:45.35ms
step:1006/1845 train_time:45635ms step_avg:45.36ms
step:1007/1845 train_time:45697ms step_avg:45.38ms
step:1008/1845 train_time:45757ms step_avg:45.39ms
step:1009/1845 train_time:45819ms step_avg:45.41ms
step:1010/1845 train_time:45880ms step_avg:45.43ms
step:1011/1845 train_time:45943ms step_avg:45.44ms
step:1012/1845 train_time:46003ms step_avg:45.46ms
step:1013/1845 train_time:46065ms step_avg:45.47ms
step:1014/1845 train_time:46126ms step_avg:45.49ms
step:1015/1845 train_time:46188ms step_avg:45.51ms
step:1016/1845 train_time:46249ms step_avg:45.52ms
step:1017/1845 train_time:46312ms step_avg:45.54ms
step:1018/1845 train_time:46373ms step_avg:45.55ms
step:1019/1845 train_time:46437ms step_avg:45.57ms
step:1020/1845 train_time:46498ms step_avg:45.59ms
step:1021/1845 train_time:46561ms step_avg:45.60ms
step:1022/1845 train_time:46622ms step_avg:45.62ms
step:1023/1845 train_time:46685ms step_avg:45.64ms
step:1024/1845 train_time:46746ms step_avg:45.65ms
step:1025/1845 train_time:46809ms step_avg:45.67ms
step:1026/1845 train_time:46871ms step_avg:45.68ms
step:1027/1845 train_time:46933ms step_avg:45.70ms
step:1028/1845 train_time:46994ms step_avg:45.71ms
step:1029/1845 train_time:47055ms step_avg:45.73ms
step:1030/1845 train_time:47115ms step_avg:45.74ms
step:1031/1845 train_time:47177ms step_avg:45.76ms
step:1032/1845 train_time:47238ms step_avg:45.77ms
step:1033/1845 train_time:47300ms step_avg:45.79ms
step:1034/1845 train_time:47361ms step_avg:45.80ms
step:1035/1845 train_time:47424ms step_avg:45.82ms
step:1036/1845 train_time:47485ms step_avg:45.84ms
step:1037/1845 train_time:47548ms step_avg:45.85ms
step:1038/1845 train_time:47610ms step_avg:45.87ms
step:1039/1845 train_time:47672ms step_avg:45.88ms
step:1040/1845 train_time:47733ms step_avg:45.90ms
step:1041/1845 train_time:47795ms step_avg:45.91ms
step:1042/1845 train_time:47856ms step_avg:45.93ms
step:1043/1845 train_time:47918ms step_avg:45.94ms
step:1044/1845 train_time:47979ms step_avg:45.96ms
step:1045/1845 train_time:48042ms step_avg:45.97ms
step:1046/1845 train_time:48102ms step_avg:45.99ms
step:1047/1845 train_time:48165ms step_avg:46.00ms
step:1048/1845 train_time:48226ms step_avg:46.02ms
step:1049/1845 train_time:48289ms step_avg:46.03ms
step:1050/1845 train_time:48349ms step_avg:46.05ms
step:1051/1845 train_time:48411ms step_avg:46.06ms
step:1052/1845 train_time:48472ms step_avg:46.08ms
step:1053/1845 train_time:48535ms step_avg:46.09ms
step:1054/1845 train_time:48596ms step_avg:46.11ms
step:1055/1845 train_time:48659ms step_avg:46.12ms
step:1056/1845 train_time:48720ms step_avg:46.14ms
step:1057/1845 train_time:48782ms step_avg:46.15ms
step:1058/1845 train_time:48843ms step_avg:46.17ms
step:1059/1845 train_time:48906ms step_avg:46.18ms
step:1060/1845 train_time:48968ms step_avg:46.20ms
step:1061/1845 train_time:49031ms step_avg:46.21ms
step:1062/1845 train_time:49091ms step_avg:46.23ms
step:1063/1845 train_time:49153ms step_avg:46.24ms
step:1064/1845 train_time:49214ms step_avg:46.25ms
step:1065/1845 train_time:49276ms step_avg:46.27ms
step:1066/1845 train_time:49336ms step_avg:46.28ms
step:1067/1845 train_time:49398ms step_avg:46.30ms
step:1068/1845 train_time:49459ms step_avg:46.31ms
step:1069/1845 train_time:49521ms step_avg:46.32ms
step:1070/1845 train_time:49583ms step_avg:46.34ms
step:1071/1845 train_time:49645ms step_avg:46.35ms
step:1072/1845 train_time:49707ms step_avg:46.37ms
step:1073/1845 train_time:49769ms step_avg:46.38ms
step:1074/1845 train_time:49831ms step_avg:46.40ms
step:1075/1845 train_time:49893ms step_avg:46.41ms
step:1076/1845 train_time:49955ms step_avg:46.43ms
step:1077/1845 train_time:50017ms step_avg:46.44ms
step:1078/1845 train_time:50078ms step_avg:46.45ms
step:1079/1845 train_time:50140ms step_avg:46.47ms
step:1080/1845 train_time:50201ms step_avg:46.48ms
step:1081/1845 train_time:50263ms step_avg:46.50ms
step:1082/1845 train_time:50324ms step_avg:46.51ms
step:1083/1845 train_time:50386ms step_avg:46.52ms
step:1084/1845 train_time:50447ms step_avg:46.54ms
step:1085/1845 train_time:50509ms step_avg:46.55ms
step:1086/1845 train_time:50570ms step_avg:46.57ms
step:1087/1845 train_time:50633ms step_avg:46.58ms
step:1088/1845 train_time:50693ms step_avg:46.59ms
step:1089/1845 train_time:50756ms step_avg:46.61ms
step:1090/1845 train_time:50817ms step_avg:46.62ms
step:1091/1845 train_time:50879ms step_avg:46.64ms
step:1092/1845 train_time:50940ms step_avg:46.65ms
step:1093/1845 train_time:51003ms step_avg:46.66ms
step:1094/1845 train_time:51064ms step_avg:46.68ms
step:1095/1845 train_time:51127ms step_avg:46.69ms
step:1096/1845 train_time:51188ms step_avg:46.70ms
step:1097/1845 train_time:51250ms step_avg:46.72ms
step:1098/1845 train_time:51311ms step_avg:46.73ms
step:1099/1845 train_time:51373ms step_avg:46.75ms
step:1100/1845 train_time:51434ms step_avg:46.76ms
step:1101/1845 train_time:51497ms step_avg:46.77ms
step:1102/1845 train_time:51557ms step_avg:46.79ms
step:1103/1845 train_time:51621ms step_avg:46.80ms
step:1104/1845 train_time:51682ms step_avg:46.81ms
step:1105/1845 train_time:51744ms step_avg:46.83ms
step:1106/1845 train_time:51806ms step_avg:46.84ms
step:1107/1845 train_time:51868ms step_avg:46.85ms
step:1108/1845 train_time:51929ms step_avg:46.87ms
step:1109/1845 train_time:51992ms step_avg:46.88ms
step:1110/1845 train_time:52052ms step_avg:46.89ms
step:1111/1845 train_time:52115ms step_avg:46.91ms
step:1112/1845 train_time:52176ms step_avg:46.92ms
step:1113/1845 train_time:52239ms step_avg:46.94ms
step:1114/1845 train_time:52300ms step_avg:46.95ms
step:1115/1845 train_time:52362ms step_avg:46.96ms
step:1116/1845 train_time:52423ms step_avg:46.97ms
step:1117/1845 train_time:52486ms step_avg:46.99ms
step:1118/1845 train_time:52547ms step_avg:47.00ms
step:1119/1845 train_time:52609ms step_avg:47.01ms
step:1120/1845 train_time:52670ms step_avg:47.03ms
step:1121/1845 train_time:52733ms step_avg:47.04ms
step:1122/1845 train_time:52793ms step_avg:47.05ms
step:1123/1845 train_time:52855ms step_avg:47.07ms
step:1124/1845 train_time:52916ms step_avg:47.08ms
step:1125/1845 train_time:52978ms step_avg:47.09ms
step:1126/1845 train_time:53039ms step_avg:47.10ms
step:1127/1845 train_time:53102ms step_avg:47.12ms
step:1128/1845 train_time:53162ms step_avg:47.13ms
step:1129/1845 train_time:53225ms step_avg:47.14ms
step:1130/1845 train_time:53286ms step_avg:47.16ms
step:1131/1845 train_time:53349ms step_avg:47.17ms
step:1132/1845 train_time:53410ms step_avg:47.18ms
step:1133/1845 train_time:53473ms step_avg:47.20ms
step:1134/1845 train_time:53533ms step_avg:47.21ms
step:1135/1845 train_time:53596ms step_avg:47.22ms
step:1136/1845 train_time:53656ms step_avg:47.23ms
step:1137/1845 train_time:53719ms step_avg:47.25ms
step:1138/1845 train_time:53780ms step_avg:47.26ms
step:1139/1845 train_time:53842ms step_avg:47.27ms
step:1140/1845 train_time:53903ms step_avg:47.28ms
step:1141/1845 train_time:53966ms step_avg:47.30ms
step:1142/1845 train_time:54027ms step_avg:47.31ms
step:1143/1845 train_time:54090ms step_avg:47.32ms
step:1144/1845 train_time:54151ms step_avg:47.34ms
step:1145/1845 train_time:54213ms step_avg:47.35ms
step:1146/1845 train_time:54274ms step_avg:47.36ms
step:1147/1845 train_time:54337ms step_avg:47.37ms
step:1148/1845 train_time:54397ms step_avg:47.38ms
step:1149/1845 train_time:54459ms step_avg:47.40ms
step:1150/1845 train_time:54520ms step_avg:47.41ms
step:1151/1845 train_time:54583ms step_avg:47.42ms
step:1152/1845 train_time:54644ms step_avg:47.43ms
step:1153/1845 train_time:54706ms step_avg:47.45ms
step:1154/1845 train_time:54767ms step_avg:47.46ms
step:1155/1845 train_time:54830ms step_avg:47.47ms
step:1156/1845 train_time:54891ms step_avg:47.48ms
step:1157/1845 train_time:54953ms step_avg:47.50ms
step:1158/1845 train_time:55014ms step_avg:47.51ms
step:1159/1845 train_time:55077ms step_avg:47.52ms
step:1160/1845 train_time:55137ms step_avg:47.53ms
step:1161/1845 train_time:55199ms step_avg:47.54ms
step:1162/1845 train_time:55260ms step_avg:47.56ms
step:1163/1845 train_time:55322ms step_avg:47.57ms
step:1164/1845 train_time:55383ms step_avg:47.58ms
step:1165/1845 train_time:55445ms step_avg:47.59ms
step:1166/1845 train_time:55506ms step_avg:47.60ms
step:1167/1845 train_time:55569ms step_avg:47.62ms
step:1168/1845 train_time:55630ms step_avg:47.63ms
step:1169/1845 train_time:55693ms step_avg:47.64ms
step:1170/1845 train_time:55753ms step_avg:47.65ms
step:1171/1845 train_time:55815ms step_avg:47.66ms
step:1172/1845 train_time:55876ms step_avg:47.68ms
step:1173/1845 train_time:55938ms step_avg:47.69ms
step:1174/1845 train_time:55998ms step_avg:47.70ms
step:1175/1845 train_time:56061ms step_avg:47.71ms
step:1176/1845 train_time:56122ms step_avg:47.72ms
step:1177/1845 train_time:56184ms step_avg:47.74ms
step:1178/1845 train_time:56245ms step_avg:47.75ms
step:1179/1845 train_time:56307ms step_avg:47.76ms
step:1180/1845 train_time:56368ms step_avg:47.77ms
step:1181/1845 train_time:56430ms step_avg:47.78ms
step:1182/1845 train_time:56491ms step_avg:47.79ms
step:1183/1845 train_time:56553ms step_avg:47.81ms
step:1184/1845 train_time:56614ms step_avg:47.82ms
step:1185/1845 train_time:56676ms step_avg:47.83ms
step:1186/1845 train_time:56737ms step_avg:47.84ms
step:1187/1845 train_time:56799ms step_avg:47.85ms
step:1188/1845 train_time:56860ms step_avg:47.86ms
step:1189/1845 train_time:56923ms step_avg:47.87ms
step:1190/1845 train_time:56984ms step_avg:47.89ms
step:1191/1845 train_time:57047ms step_avg:47.90ms
step:1192/1845 train_time:57108ms step_avg:47.91ms
step:1193/1845 train_time:57171ms step_avg:47.92ms
step:1194/1845 train_time:57232ms step_avg:47.93ms
step:1195/1845 train_time:57294ms step_avg:47.95ms
step:1196/1845 train_time:57355ms step_avg:47.96ms
step:1197/1845 train_time:57418ms step_avg:47.97ms
step:1198/1845 train_time:57479ms step_avg:47.98ms
step:1199/1845 train_time:57542ms step_avg:47.99ms
step:1200/1845 train_time:57602ms step_avg:48.00ms
step:1201/1845 train_time:57665ms step_avg:48.01ms
step:1202/1845 train_time:57726ms step_avg:48.03ms
step:1203/1845 train_time:57788ms step_avg:48.04ms
step:1204/1845 train_time:57849ms step_avg:48.05ms
step:1205/1845 train_time:57912ms step_avg:48.06ms
step:1206/1845 train_time:58000ms step_avg:48.09ms
step:1207/1845 train_time:58088ms step_avg:48.13ms
step:1208/1845 train_time:58176ms step_avg:48.16ms
step:1209/1845 train_time:58265ms step_avg:48.19ms
step:1210/1845 train_time:58352ms step_avg:48.23ms
step:1211/1845 train_time:58441ms step_avg:48.26ms
step:1212/1845 train_time:58527ms step_avg:48.29ms
step:1213/1845 train_time:58616ms step_avg:48.32ms
step:1214/1845 train_time:58704ms step_avg:48.36ms
step:1215/1845 train_time:58792ms step_avg:48.39ms
step:1216/1845 train_time:58879ms step_avg:48.42ms
step:1217/1845 train_time:58967ms step_avg:48.45ms
step:1218/1845 train_time:59055ms step_avg:48.48ms
step:1219/1845 train_time:59143ms step_avg:48.52ms
step:1220/1845 train_time:59230ms step_avg:48.55ms
step:1221/1845 train_time:59319ms step_avg:48.58ms
step:1222/1845 train_time:59408ms step_avg:48.62ms
step:1223/1845 train_time:59496ms step_avg:48.65ms
step:1224/1845 train_time:59583ms step_avg:48.68ms
step:1225/1845 train_time:59672ms step_avg:48.71ms
step:1226/1845 train_time:59760ms step_avg:48.74ms
step:1227/1845 train_time:59848ms step_avg:48.78ms
step:1228/1845 train_time:59934ms step_avg:48.81ms
step:1229/1845 train_time:60023ms step_avg:48.84ms
step:1230/1845 train_time:60110ms step_avg:48.87ms
step:1231/1845 train_time:60199ms step_avg:48.90ms
step:1232/1845 train_time:60286ms step_avg:48.93ms
step:1233/1845 train_time:60374ms step_avg:48.97ms
step:1234/1845 train_time:60462ms step_avg:49.00ms
step:1235/1845 train_time:60551ms step_avg:49.03ms
step:1236/1845 train_time:60638ms step_avg:49.06ms
step:1237/1845 train_time:60727ms step_avg:49.09ms
step:1238/1845 train_time:60814ms step_avg:49.12ms
step:1239/1845 train_time:60903ms step_avg:49.15ms
step:1240/1845 train_time:60990ms step_avg:49.19ms
step:1241/1845 train_time:61078ms step_avg:49.22ms
step:1242/1845 train_time:61166ms step_avg:49.25ms
step:1243/1845 train_time:61254ms step_avg:49.28ms
step:1244/1845 train_time:61343ms step_avg:49.31ms
step:1245/1845 train_time:61431ms step_avg:49.34ms
step:1246/1845 train_time:61519ms step_avg:49.37ms
step:1247/1845 train_time:61607ms step_avg:49.40ms
step:1248/1845 train_time:61694ms step_avg:49.43ms
step:1249/1845 train_time:61782ms step_avg:49.47ms
step:1250/1845 train_time:61869ms step_avg:49.49ms
step:1250/1845 val_loss:3.5390 train_time:61959ms step_avg:49.57ms
step:1251/1845 train_time:61979ms step_avg:49.54ms
step:1252/1845 train_time:62049ms step_avg:49.56ms
step:1253/1845 train_time:62142ms step_avg:49.59ms
step:1254/1845 train_time:62231ms step_avg:49.63ms
step:1255/1845 train_time:62320ms step_avg:49.66ms
step:1256/1845 train_time:62406ms step_avg:49.69ms
step:1257/1845 train_time:62493ms step_avg:49.72ms
step:1258/1845 train_time:62579ms step_avg:49.75ms
step:1259/1845 train_time:62666ms step_avg:49.77ms
step:1260/1845 train_time:62752ms step_avg:49.80ms
step:1261/1845 train_time:62840ms step_avg:49.83ms
step:1262/1845 train_time:62927ms step_avg:49.86ms
step:1263/1845 train_time:63020ms step_avg:49.90ms
step:1264/1845 train_time:63110ms step_avg:49.93ms
step:1265/1845 train_time:63198ms step_avg:49.96ms
step:1266/1845 train_time:63286ms step_avg:49.99ms
step:1267/1845 train_time:63375ms step_avg:50.02ms
step:1268/1845 train_time:63462ms step_avg:50.05ms
step:1269/1845 train_time:63550ms step_avg:50.08ms
step:1270/1845 train_time:63636ms step_avg:50.11ms
step:1271/1845 train_time:63723ms step_avg:50.14ms
step:1272/1845 train_time:63810ms step_avg:50.17ms
step:1273/1845 train_time:63899ms step_avg:50.20ms
step:1274/1845 train_time:63988ms step_avg:50.23ms
step:1275/1845 train_time:64077ms step_avg:50.26ms
step:1276/1845 train_time:64167ms step_avg:50.29ms
step:1277/1845 train_time:64257ms step_avg:50.32ms
step:1278/1845 train_time:64345ms step_avg:50.35ms
step:1279/1845 train_time:64433ms step_avg:50.38ms
step:1280/1845 train_time:64520ms step_avg:50.41ms
step:1281/1845 train_time:64607ms step_avg:50.44ms
step:1282/1845 train_time:64694ms step_avg:50.46ms
step:1283/1845 train_time:64782ms step_avg:50.49ms
step:1284/1845 train_time:64869ms step_avg:50.52ms
step:1285/1845 train_time:64958ms step_avg:50.55ms
step:1286/1845 train_time:65046ms step_avg:50.58ms
step:1287/1845 train_time:65135ms step_avg:50.61ms
step:1288/1845 train_time:65223ms step_avg:50.64ms
step:1289/1845 train_time:65312ms step_avg:50.67ms
step:1290/1845 train_time:65399ms step_avg:50.70ms
step:1291/1845 train_time:65487ms step_avg:50.73ms
step:1292/1845 train_time:65574ms step_avg:50.75ms
step:1293/1845 train_time:65662ms step_avg:50.78ms
step:1294/1845 train_time:65749ms step_avg:50.81ms
step:1295/1845 train_time:65837ms step_avg:50.84ms
step:1296/1845 train_time:65924ms step_avg:50.87ms
step:1297/1845 train_time:66013ms step_avg:50.90ms
step:1298/1845 train_time:66101ms step_avg:50.92ms
step:1299/1845 train_time:66190ms step_avg:50.95ms
step:1300/1845 train_time:66279ms step_avg:50.98ms
step:1301/1845 train_time:66367ms step_avg:51.01ms
step:1302/1845 train_time:66454ms step_avg:51.04ms
step:1303/1845 train_time:66542ms step_avg:51.07ms
step:1304/1845 train_time:66629ms step_avg:51.10ms
step:1305/1845 train_time:66717ms step_avg:51.12ms
step:1306/1845 train_time:66803ms step_avg:51.15ms
step:1307/1845 train_time:66892ms step_avg:51.18ms
step:1308/1845 train_time:66979ms step_avg:51.21ms
step:1309/1845 train_time:67067ms step_avg:51.24ms
step:1310/1845 train_time:67156ms step_avg:51.26ms
step:1311/1845 train_time:67244ms step_avg:51.29ms
step:1312/1845 train_time:67332ms step_avg:51.32ms
step:1313/1845 train_time:67420ms step_avg:51.35ms
step:1314/1845 train_time:67507ms step_avg:51.37ms
step:1315/1845 train_time:67595ms step_avg:51.40ms
step:1316/1845 train_time:67682ms step_avg:51.43ms
step:1317/1845 train_time:67770ms step_avg:51.46ms
step:1318/1845 train_time:67858ms step_avg:51.49ms
step:1319/1845 train_time:67947ms step_avg:51.51ms
step:1320/1845 train_time:68035ms step_avg:51.54ms
step:1321/1845 train_time:68124ms step_avg:51.57ms
step:1322/1845 train_time:68211ms step_avg:51.60ms
step:1323/1845 train_time:68300ms step_avg:51.62ms
step:1324/1845 train_time:68387ms step_avg:51.65ms
step:1325/1845 train_time:68475ms step_avg:51.68ms
step:1326/1845 train_time:68563ms step_avg:51.71ms
step:1327/1845 train_time:68651ms step_avg:51.73ms
step:1328/1845 train_time:68739ms step_avg:51.76ms
step:1329/1845 train_time:68827ms step_avg:51.79ms
step:1330/1845 train_time:68914ms step_avg:51.81ms
step:1331/1845 train_time:69003ms step_avg:51.84ms
step:1332/1845 train_time:69090ms step_avg:51.87ms
step:1333/1845 train_time:69181ms step_avg:51.90ms
step:1334/1845 train_time:69267ms step_avg:51.92ms
step:1335/1845 train_time:69357ms step_avg:51.95ms
step:1336/1845 train_time:69443ms step_avg:51.98ms
step:1337/1845 train_time:69532ms step_avg:52.01ms
step:1338/1845 train_time:69618ms step_avg:52.03ms
step:1339/1845 train_time:69707ms step_avg:52.06ms
step:1340/1845 train_time:69795ms step_avg:52.09ms
step:1341/1845 train_time:69884ms step_avg:52.11ms
step:1342/1845 train_time:69972ms step_avg:52.14ms
step:1343/1845 train_time:70060ms step_avg:52.17ms
step:1344/1845 train_time:70147ms step_avg:52.19ms
step:1345/1845 train_time:70237ms step_avg:52.22ms
step:1346/1845 train_time:70324ms step_avg:52.25ms
step:1347/1845 train_time:70412ms step_avg:52.27ms
step:1348/1845 train_time:70499ms step_avg:52.30ms
step:1349/1845 train_time:70587ms step_avg:52.33ms
step:1350/1845 train_time:70674ms step_avg:52.35ms
step:1351/1845 train_time:70762ms step_avg:52.38ms
step:1352/1845 train_time:70850ms step_avg:52.40ms
step:1353/1845 train_time:70938ms step_avg:52.43ms
step:1354/1845 train_time:71025ms step_avg:52.46ms
step:1355/1845 train_time:71114ms step_avg:52.48ms
step:1356/1845 train_time:71201ms step_avg:52.51ms
step:1357/1845 train_time:71290ms step_avg:52.54ms
step:1358/1845 train_time:71378ms step_avg:52.56ms
step:1359/1845 train_time:71467ms step_avg:52.59ms
step:1360/1845 train_time:71554ms step_avg:52.61ms
step:1361/1845 train_time:71642ms step_avg:52.64ms
step:1362/1845 train_time:71730ms step_avg:52.67ms
step:1363/1845 train_time:71819ms step_avg:52.69ms
step:1364/1845 train_time:71905ms step_avg:52.72ms
step:1365/1845 train_time:71993ms step_avg:52.74ms
step:1366/1845 train_time:72081ms step_avg:52.77ms
step:1367/1845 train_time:72169ms step_avg:52.79ms
step:1368/1845 train_time:72257ms step_avg:52.82ms
step:1369/1845 train_time:72346ms step_avg:52.85ms
step:1370/1845 train_time:72434ms step_avg:52.87ms
step:1371/1845 train_time:72522ms step_avg:52.90ms
step:1372/1845 train_time:72609ms step_avg:52.92ms
step:1373/1845 train_time:72697ms step_avg:52.95ms
step:1374/1845 train_time:72784ms step_avg:52.97ms
step:1375/1845 train_time:72873ms step_avg:53.00ms
step:1376/1845 train_time:72960ms step_avg:53.02ms
step:1377/1845 train_time:73048ms step_avg:53.05ms
step:1378/1845 train_time:73137ms step_avg:53.07ms
step:1379/1845 train_time:73225ms step_avg:53.10ms
step:1380/1845 train_time:73312ms step_avg:53.12ms
step:1381/1845 train_time:73401ms step_avg:53.15ms
step:1382/1845 train_time:73488ms step_avg:53.18ms
step:1383/1845 train_time:73576ms step_avg:53.20ms
step:1384/1845 train_time:73664ms step_avg:53.23ms
step:1385/1845 train_time:73752ms step_avg:53.25ms
step:1386/1845 train_time:73839ms step_avg:53.27ms
step:1387/1845 train_time:73928ms step_avg:53.30ms
step:1388/1845 train_time:74016ms step_avg:53.33ms
step:1389/1845 train_time:74104ms step_avg:53.35ms
step:1390/1845 train_time:74192ms step_avg:53.38ms
step:1391/1845 train_time:74280ms step_avg:53.40ms
step:1392/1845 train_time:74367ms step_avg:53.42ms
step:1393/1845 train_time:74455ms step_avg:53.45ms
step:1394/1845 train_time:74542ms step_avg:53.47ms
step:1395/1845 train_time:74630ms step_avg:53.50ms
step:1396/1845 train_time:74717ms step_avg:53.52ms
step:1397/1845 train_time:74805ms step_avg:53.55ms
step:1398/1845 train_time:74892ms step_avg:53.57ms
step:1399/1845 train_time:74981ms step_avg:53.60ms
step:1400/1845 train_time:75068ms step_avg:53.62ms
step:1401/1845 train_time:75156ms step_avg:53.64ms
step:1402/1845 train_time:75243ms step_avg:53.67ms
step:1403/1845 train_time:75331ms step_avg:53.69ms
step:1404/1845 train_time:75419ms step_avg:53.72ms
step:1405/1845 train_time:75507ms step_avg:53.74ms
step:1406/1845 train_time:75594ms step_avg:53.77ms
step:1407/1845 train_time:75683ms step_avg:53.79ms
step:1408/1845 train_time:75770ms step_avg:53.81ms
step:1409/1845 train_time:75859ms step_avg:53.84ms
step:1410/1845 train_time:75946ms step_avg:53.86ms
step:1411/1845 train_time:76034ms step_avg:53.89ms
step:1412/1845 train_time:76122ms step_avg:53.91ms
step:1413/1845 train_time:76211ms step_avg:53.94ms
step:1414/1845 train_time:76298ms step_avg:53.96ms
step:1415/1845 train_time:76386ms step_avg:53.98ms
step:1416/1845 train_time:76474ms step_avg:54.01ms
step:1417/1845 train_time:76562ms step_avg:54.03ms
step:1418/1845 train_time:76650ms step_avg:54.05ms
step:1419/1845 train_time:76738ms step_avg:54.08ms
step:1420/1845 train_time:76825ms step_avg:54.10ms
step:1421/1845 train_time:76914ms step_avg:54.13ms
step:1422/1845 train_time:77001ms step_avg:54.15ms
step:1423/1845 train_time:77089ms step_avg:54.17ms
step:1424/1845 train_time:77177ms step_avg:54.20ms
step:1425/1845 train_time:77265ms step_avg:54.22ms
step:1426/1845 train_time:77352ms step_avg:54.24ms
step:1427/1845 train_time:77442ms step_avg:54.27ms
step:1428/1845 train_time:77530ms step_avg:54.29ms
step:1429/1845 train_time:77619ms step_avg:54.32ms
step:1430/1845 train_time:77705ms step_avg:54.34ms
step:1431/1845 train_time:77794ms step_avg:54.36ms
step:1432/1845 train_time:77881ms step_avg:54.39ms
step:1433/1845 train_time:77969ms step_avg:54.41ms
step:1434/1845 train_time:78058ms step_avg:54.43ms
step:1435/1845 train_time:78145ms step_avg:54.46ms
step:1436/1845 train_time:78232ms step_avg:54.48ms
step:1437/1845 train_time:78321ms step_avg:54.50ms
step:1438/1845 train_time:78409ms step_avg:54.53ms
step:1439/1845 train_time:78498ms step_avg:54.55ms
step:1440/1845 train_time:78585ms step_avg:54.57ms
step:1441/1845 train_time:78674ms step_avg:54.60ms
step:1442/1845 train_time:78761ms step_avg:54.62ms
step:1443/1845 train_time:78849ms step_avg:54.64ms
step:1444/1845 train_time:78937ms step_avg:54.67ms
step:1445/1845 train_time:79025ms step_avg:54.69ms
step:1446/1845 train_time:79112ms step_avg:54.71ms
step:1447/1845 train_time:79201ms step_avg:54.73ms
step:1448/1845 train_time:79288ms step_avg:54.76ms
step:1449/1845 train_time:79377ms step_avg:54.78ms
step:1450/1845 train_time:79464ms step_avg:54.80ms
step:1451/1845 train_time:79553ms step_avg:54.83ms
step:1452/1845 train_time:79640ms step_avg:54.85ms
step:1453/1845 train_time:79729ms step_avg:54.87ms
step:1454/1845 train_time:79816ms step_avg:54.89ms
step:1455/1845 train_time:79904ms step_avg:54.92ms
step:1456/1845 train_time:79991ms step_avg:54.94ms
step:1457/1845 train_time:80080ms step_avg:54.96ms
step:1458/1845 train_time:80167ms step_avg:54.98ms
step:1459/1845 train_time:80255ms step_avg:55.01ms
step:1460/1845 train_time:80343ms step_avg:55.03ms
step:1461/1845 train_time:80431ms step_avg:55.05ms
step:1462/1845 train_time:80519ms step_avg:55.07ms
step:1463/1845 train_time:80608ms step_avg:55.10ms
step:1464/1845 train_time:80695ms step_avg:55.12ms
step:1465/1845 train_time:80784ms step_avg:55.14ms
step:1466/1845 train_time:80871ms step_avg:55.16ms
step:1467/1845 train_time:80960ms step_avg:55.19ms
step:1468/1845 train_time:81047ms step_avg:55.21ms
step:1469/1845 train_time:81136ms step_avg:55.23ms
step:1470/1845 train_time:81223ms step_avg:55.25ms
step:1471/1845 train_time:81312ms step_avg:55.28ms
step:1472/1845 train_time:81399ms step_avg:55.30ms
step:1473/1845 train_time:81487ms step_avg:55.32ms
step:1474/1845 train_time:81575ms step_avg:55.34ms
step:1475/1845 train_time:81663ms step_avg:55.36ms
step:1476/1845 train_time:81751ms step_avg:55.39ms
step:1477/1845 train_time:81838ms step_avg:55.41ms
step:1478/1845 train_time:81925ms step_avg:55.43ms
step:1479/1845 train_time:82014ms step_avg:55.45ms
step:1480/1845 train_time:82102ms step_avg:55.47ms
step:1481/1845 train_time:82191ms step_avg:55.50ms
step:1482/1845 train_time:82279ms step_avg:55.52ms
step:1483/1845 train_time:82367ms step_avg:55.54ms
step:1484/1845 train_time:82455ms step_avg:55.56ms
step:1485/1845 train_time:82543ms step_avg:55.58ms
step:1486/1845 train_time:82630ms step_avg:55.61ms
step:1487/1845 train_time:82720ms step_avg:55.63ms
step:1488/1845 train_time:82806ms step_avg:55.65ms
step:1489/1845 train_time:82895ms step_avg:55.67ms
step:1490/1845 train_time:82982ms step_avg:55.69ms
step:1491/1845 train_time:83071ms step_avg:55.72ms
step:1492/1845 train_time:83159ms step_avg:55.74ms
step:1493/1845 train_time:83247ms step_avg:55.76ms
step:1494/1845 train_time:83335ms step_avg:55.78ms
step:1495/1845 train_time:83423ms step_avg:55.80ms
step:1496/1845 train_time:83510ms step_avg:55.82ms
step:1497/1845 train_time:83598ms step_avg:55.84ms
step:1498/1845 train_time:83684ms step_avg:55.86ms
step:1499/1845 train_time:83773ms step_avg:55.89ms
step:1500/1845 train_time:83861ms step_avg:55.91ms
step:1500/1845 val_loss:3.4054 train_time:83951ms step_avg:55.97ms
step:1501/1845 train_time:83971ms step_avg:55.94ms
step:1502/1845 train_time:84042ms step_avg:55.95ms
step:1503/1845 train_time:84136ms step_avg:55.98ms
step:1504/1845 train_time:84224ms step_avg:56.00ms
step:1505/1845 train_time:84312ms step_avg:56.02ms
step:1506/1845 train_time:84399ms step_avg:56.04ms
step:1507/1845 train_time:84487ms step_avg:56.06ms
step:1508/1845 train_time:84574ms step_avg:56.08ms
step:1509/1845 train_time:84662ms step_avg:56.10ms
step:1510/1845 train_time:84748ms step_avg:56.12ms
step:1511/1845 train_time:84836ms step_avg:56.15ms
step:1512/1845 train_time:84923ms step_avg:56.17ms
step:1513/1845 train_time:85015ms step_avg:56.19ms
step:1514/1845 train_time:85105ms step_avg:56.21ms
step:1515/1845 train_time:85195ms step_avg:56.23ms
step:1516/1845 train_time:85283ms step_avg:56.26ms
step:1517/1845 train_time:85371ms step_avg:56.28ms
step:1518/1845 train_time:85458ms step_avg:56.30ms
step:1519/1845 train_time:85545ms step_avg:56.32ms
step:1520/1845 train_time:85632ms step_avg:56.34ms
step:1521/1845 train_time:85720ms step_avg:56.36ms
step:1522/1845 train_time:85807ms step_avg:56.38ms
step:1523/1845 train_time:85896ms step_avg:56.40ms
step:1524/1845 train_time:85984ms step_avg:56.42ms
step:1525/1845 train_time:86075ms step_avg:56.44ms
step:1526/1845 train_time:86164ms step_avg:56.46ms
step:1527/1845 train_time:86254ms step_avg:56.49ms
step:1528/1845 train_time:86342ms step_avg:56.51ms
step:1529/1845 train_time:86430ms step_avg:56.53ms
step:1530/1845 train_time:86518ms step_avg:56.55ms
step:1531/1845 train_time:86606ms step_avg:56.57ms
step:1532/1845 train_time:86693ms step_avg:56.59ms
step:1533/1845 train_time:86781ms step_avg:56.61ms
step:1534/1845 train_time:86868ms step_avg:56.63ms
step:1535/1845 train_time:86958ms step_avg:56.65ms
step:1536/1845 train_time:87047ms step_avg:56.67ms
step:1537/1845 train_time:87138ms step_avg:56.69ms
step:1538/1845 train_time:87226ms step_avg:56.71ms
step:1539/1845 train_time:87316ms step_avg:56.74ms
step:1540/1845 train_time:87403ms step_avg:56.76ms
step:1541/1845 train_time:87491ms step_avg:56.78ms
step:1542/1845 train_time:87579ms step_avg:56.80ms
step:1543/1845 train_time:87667ms step_avg:56.82ms
step:1544/1845 train_time:87754ms step_avg:56.84ms
step:1545/1845 train_time:87843ms step_avg:56.86ms
step:1546/1845 train_time:87930ms step_avg:56.88ms
step:1547/1845 train_time:88019ms step_avg:56.90ms
step:1548/1845 train_time:88107ms step_avg:56.92ms
step:1549/1845 train_time:88198ms step_avg:56.94ms
step:1550/1845 train_time:88286ms step_avg:56.96ms
step:1551/1845 train_time:88375ms step_avg:56.98ms
step:1552/1845 train_time:88462ms step_avg:57.00ms
step:1553/1845 train_time:88551ms step_avg:57.02ms
step:1554/1845 train_time:88638ms step_avg:57.04ms
step:1555/1845 train_time:88726ms step_avg:57.06ms
step:1556/1845 train_time:88813ms step_avg:57.08ms
step:1557/1845 train_time:88901ms step_avg:57.10ms
step:1558/1845 train_time:88989ms step_avg:57.12ms
step:1559/1845 train_time:89078ms step_avg:57.14ms
step:1560/1845 train_time:89168ms step_avg:57.16ms
step:1561/1845 train_time:89258ms step_avg:57.18ms
step:1562/1845 train_time:89346ms step_avg:57.20ms
step:1563/1845 train_time:89435ms step_avg:57.22ms
step:1564/1845 train_time:89522ms step_avg:57.24ms
step:1565/1845 train_time:89611ms step_avg:57.26ms
step:1566/1845 train_time:89698ms step_avg:57.28ms
step:1567/1845 train_time:89787ms step_avg:57.30ms
step:1568/1845 train_time:89874ms step_avg:57.32ms
step:1569/1845 train_time:89962ms step_avg:57.34ms
step:1570/1845 train_time:90051ms step_avg:57.36ms
step:1571/1845 train_time:90140ms step_avg:57.38ms
step:1572/1845 train_time:90227ms step_avg:57.40ms
step:1573/1845 train_time:90317ms step_avg:57.42ms
step:1574/1845 train_time:90405ms step_avg:57.44ms
step:1575/1845 train_time:90494ms step_avg:57.46ms
step:1576/1845 train_time:90581ms step_avg:57.48ms
step:1577/1845 train_time:90670ms step_avg:57.50ms
step:1578/1845 train_time:90758ms step_avg:57.51ms
step:1579/1845 train_time:90847ms step_avg:57.53ms
step:1580/1845 train_time:90933ms step_avg:57.55ms
step:1581/1845 train_time:91022ms step_avg:57.57ms
step:1582/1845 train_time:91111ms step_avg:57.59ms
step:1583/1845 train_time:91201ms step_avg:57.61ms
step:1584/1845 train_time:91290ms step_avg:57.63ms
step:1585/1845 train_time:91379ms step_avg:57.65ms
step:1586/1845 train_time:91467ms step_avg:57.67ms
step:1587/1845 train_time:91556ms step_avg:57.69ms
step:1588/1845 train_time:91643ms step_avg:57.71ms
step:1589/1845 train_time:91731ms step_avg:57.73ms
step:1590/1845 train_time:91818ms step_avg:57.75ms
step:1591/1845 train_time:91906ms step_avg:57.77ms
step:1592/1845 train_time:91994ms step_avg:57.79ms
step:1593/1845 train_time:92083ms step_avg:57.80ms
step:1594/1845 train_time:92170ms step_avg:57.82ms
step:1595/1845 train_time:92259ms step_avg:57.84ms
step:1596/1845 train_time:92347ms step_avg:57.86ms
step:1597/1845 train_time:92436ms step_avg:57.88ms
step:1598/1845 train_time:92523ms step_avg:57.90ms
step:1599/1845 train_time:92612ms step_avg:57.92ms
step:1600/1845 train_time:92699ms step_avg:57.94ms
step:1601/1845 train_time:92788ms step_avg:57.96ms
step:1602/1845 train_time:92877ms step_avg:57.98ms
step:1603/1845 train_time:92965ms step_avg:57.99ms
step:1604/1845 train_time:93052ms step_avg:58.01ms
step:1605/1845 train_time:93140ms step_avg:58.03ms
step:1606/1845 train_time:93227ms step_avg:58.05ms
step:1607/1845 train_time:93318ms step_avg:58.07ms
step:1608/1845 train_time:93406ms step_avg:58.09ms
step:1609/1845 train_time:93495ms step_avg:58.11ms
step:1610/1845 train_time:93582ms step_avg:58.13ms
step:1611/1845 train_time:93671ms step_avg:58.14ms
step:1612/1845 train_time:93758ms step_avg:58.16ms
step:1613/1845 train_time:93847ms step_avg:58.18ms
step:1614/1845 train_time:93935ms step_avg:58.20ms
step:1615/1845 train_time:94023ms step_avg:58.22ms
step:1616/1845 train_time:94110ms step_avg:58.24ms
step:1617/1845 train_time:94200ms step_avg:58.26ms
step:1618/1845 train_time:94288ms step_avg:58.27ms
step:1619/1845 train_time:94377ms step_avg:58.29ms
step:1620/1845 train_time:94465ms step_avg:58.31ms
step:1621/1845 train_time:94555ms step_avg:58.33ms
step:1622/1845 train_time:94643ms step_avg:58.35ms
step:1623/1845 train_time:94732ms step_avg:58.37ms
step:1624/1845 train_time:94819ms step_avg:58.39ms
step:1625/1845 train_time:94908ms step_avg:58.40ms
step:1626/1845 train_time:94994ms step_avg:58.42ms
step:1627/1845 train_time:95083ms step_avg:58.44ms
step:1628/1845 train_time:95171ms step_avg:58.46ms
step:1629/1845 train_time:95260ms step_avg:58.48ms
step:1630/1845 train_time:95348ms step_avg:58.50ms
step:1631/1845 train_time:95437ms step_avg:58.51ms
step:1632/1845 train_time:95524ms step_avg:58.53ms
step:1633/1845 train_time:95614ms step_avg:58.55ms
step:1634/1845 train_time:95701ms step_avg:58.57ms
step:1635/1845 train_time:95790ms step_avg:58.59ms
step:1636/1845 train_time:95877ms step_avg:58.60ms
step:1637/1845 train_time:95966ms step_avg:58.62ms
step:1638/1845 train_time:96054ms step_avg:58.64ms
step:1639/1845 train_time:96142ms step_avg:58.66ms
step:1640/1845 train_time:96229ms step_avg:58.68ms
step:1641/1845 train_time:96319ms step_avg:58.70ms
step:1642/1845 train_time:96406ms step_avg:58.71ms
step:1643/1845 train_time:96495ms step_avg:58.73ms
step:1644/1845 train_time:96582ms step_avg:58.75ms
step:1645/1845 train_time:96672ms step_avg:58.77ms
step:1646/1845 train_time:96759ms step_avg:58.78ms
step:1647/1845 train_time:96848ms step_avg:58.80ms
step:1648/1845 train_time:96936ms step_avg:58.82ms
step:1649/1845 train_time:97024ms step_avg:58.84ms
step:1650/1845 train_time:97112ms step_avg:58.86ms
step:1651/1845 train_time:97201ms step_avg:58.87ms
step:1652/1845 train_time:97288ms step_avg:58.89ms
step:1653/1845 train_time:97377ms step_avg:58.91ms
step:1654/1845 train_time:97466ms step_avg:58.93ms
step:1655/1845 train_time:97554ms step_avg:58.94ms
step:1656/1845 train_time:97640ms step_avg:58.96ms
step:1657/1845 train_time:97729ms step_avg:58.98ms
step:1658/1845 train_time:97818ms step_avg:59.00ms
step:1659/1845 train_time:97906ms step_avg:59.02ms
step:1660/1845 train_time:97993ms step_avg:59.03ms
step:1661/1845 train_time:98082ms step_avg:59.05ms
step:1662/1845 train_time:98170ms step_avg:59.07ms
step:1663/1845 train_time:98258ms step_avg:59.08ms
step:1664/1845 train_time:98347ms step_avg:59.10ms
step:1665/1845 train_time:98435ms step_avg:59.12ms
step:1666/1845 train_time:98522ms step_avg:59.14ms
step:1667/1845 train_time:98612ms step_avg:59.16ms
step:1668/1845 train_time:98698ms step_avg:59.17ms
step:1669/1845 train_time:98787ms step_avg:59.19ms
step:1670/1845 train_time:98875ms step_avg:59.21ms
step:1671/1845 train_time:98963ms step_avg:59.22ms
step:1672/1845 train_time:99051ms step_avg:59.24ms
step:1673/1845 train_time:99139ms step_avg:59.26ms
step:1674/1845 train_time:99226ms step_avg:59.27ms
step:1675/1845 train_time:99315ms step_avg:59.29ms
step:1676/1845 train_time:99402ms step_avg:59.31ms
step:1677/1845 train_time:99491ms step_avg:59.33ms
step:1678/1845 train_time:99578ms step_avg:59.34ms
step:1679/1845 train_time:99668ms step_avg:59.36ms
step:1680/1845 train_time:99755ms step_avg:59.38ms
step:1681/1845 train_time:99843ms step_avg:59.40ms
step:1682/1845 train_time:99931ms step_avg:59.41ms
step:1683/1845 train_time:100020ms step_avg:59.43ms
step:1684/1845 train_time:100107ms step_avg:59.45ms
step:1685/1845 train_time:100197ms step_avg:59.46ms
step:1686/1845 train_time:100285ms step_avg:59.48ms
step:1687/1845 train_time:100375ms step_avg:59.50ms
step:1688/1845 train_time:100462ms step_avg:59.52ms
step:1689/1845 train_time:100550ms step_avg:59.53ms
step:1690/1845 train_time:100638ms step_avg:59.55ms
step:1691/1845 train_time:100727ms step_avg:59.57ms
step:1692/1845 train_time:100815ms step_avg:59.58ms
step:1693/1845 train_time:100903ms step_avg:59.60ms
step:1694/1845 train_time:100992ms step_avg:59.62ms
step:1695/1845 train_time:101081ms step_avg:59.63ms
step:1696/1845 train_time:101170ms step_avg:59.65ms
step:1697/1845 train_time:101260ms step_avg:59.67ms
step:1698/1845 train_time:101349ms step_avg:59.69ms
step:1699/1845 train_time:101439ms step_avg:59.70ms
step:1700/1845 train_time:101526ms step_avg:59.72ms
step:1701/1845 train_time:101615ms step_avg:59.74ms
step:1702/1845 train_time:101703ms step_avg:59.75ms
step:1703/1845 train_time:101791ms step_avg:59.77ms
step:1704/1845 train_time:101878ms step_avg:59.79ms
step:1705/1845 train_time:101967ms step_avg:59.80ms
step:1706/1845 train_time:102054ms step_avg:59.82ms
step:1707/1845 train_time:102142ms step_avg:59.84ms
step:1708/1845 train_time:102229ms step_avg:59.85ms
step:1709/1845 train_time:102319ms step_avg:59.87ms
step:1710/1845 train_time:102408ms step_avg:59.89ms
step:1711/1845 train_time:102497ms step_avg:59.90ms
step:1712/1845 train_time:102584ms step_avg:59.92ms
step:1713/1845 train_time:102674ms step_avg:59.94ms
step:1714/1845 train_time:102760ms step_avg:59.95ms
step:1715/1845 train_time:102849ms step_avg:59.97ms
step:1716/1845 train_time:102936ms step_avg:59.99ms
step:1717/1845 train_time:103024ms step_avg:60.00ms
step:1718/1845 train_time:103112ms step_avg:60.02ms
step:1719/1845 train_time:103200ms step_avg:60.04ms
step:1720/1845 train_time:103288ms step_avg:60.05ms
step:1721/1845 train_time:103377ms step_avg:60.07ms
step:1722/1845 train_time:103464ms step_avg:60.08ms
step:1723/1845 train_time:103553ms step_avg:60.10ms
step:1724/1845 train_time:103640ms step_avg:60.12ms
step:1725/1845 train_time:103729ms step_avg:60.13ms
step:1726/1845 train_time:103817ms step_avg:60.15ms
step:1727/1845 train_time:103905ms step_avg:60.16ms
step:1728/1845 train_time:103992ms step_avg:60.18ms
step:1729/1845 train_time:104081ms step_avg:60.20ms
step:1730/1845 train_time:104168ms step_avg:60.21ms
step:1731/1845 train_time:104257ms step_avg:60.23ms
step:1732/1845 train_time:104345ms step_avg:60.25ms
step:1733/1845 train_time:104434ms step_avg:60.26ms
step:1734/1845 train_time:104521ms step_avg:60.28ms
step:1735/1845 train_time:104611ms step_avg:60.29ms
step:1736/1845 train_time:104698ms step_avg:60.31ms
step:1737/1845 train_time:104787ms step_avg:60.33ms
step:1738/1845 train_time:104875ms step_avg:60.34ms
step:1739/1845 train_time:104963ms step_avg:60.36ms
step:1740/1845 train_time:105052ms step_avg:60.37ms
step:1741/1845 train_time:105140ms step_avg:60.39ms
step:1742/1845 train_time:105227ms step_avg:60.41ms
step:1743/1845 train_time:105318ms step_avg:60.42ms
step:1744/1845 train_time:105406ms step_avg:60.44ms
step:1745/1845 train_time:105496ms step_avg:60.46ms
step:1746/1845 train_time:105583ms step_avg:60.47ms
step:1747/1845 train_time:105671ms step_avg:60.49ms
step:1748/1845 train_time:105758ms step_avg:60.50ms
step:1749/1845 train_time:105848ms step_avg:60.52ms
step:1750/1845 train_time:105934ms step_avg:60.53ms
step:1750/1845 val_loss:3.3053 train_time:106024ms step_avg:60.59ms
step:1751/1845 train_time:106044ms step_avg:60.56ms
step:1752/1845 train_time:106116ms step_avg:60.57ms
step:1753/1845 train_time:106211ms step_avg:60.59ms
step:1754/1845 train_time:106299ms step_avg:60.60ms
step:1755/1845 train_time:106386ms step_avg:60.62ms
step:1756/1845 train_time:106472ms step_avg:60.63ms
step:1757/1845 train_time:106560ms step_avg:60.65ms
step:1758/1845 train_time:106646ms step_avg:60.66ms
step:1759/1845 train_time:106733ms step_avg:60.68ms
step:1760/1845 train_time:106819ms step_avg:60.69ms
step:1761/1845 train_time:106908ms step_avg:60.71ms
step:1762/1845 train_time:106997ms step_avg:60.72ms
step:1763/1845 train_time:107088ms step_avg:60.74ms
step:1764/1845 train_time:107178ms step_avg:60.76ms
step:1765/1845 train_time:107268ms step_avg:60.77ms
step:1766/1845 train_time:107354ms step_avg:60.79ms
step:1767/1845 train_time:107442ms step_avg:60.80ms
step:1768/1845 train_time:107529ms step_avg:60.82ms
step:1769/1845 train_time:107616ms step_avg:60.83ms
step:1770/1845 train_time:107702ms step_avg:60.85ms
step:1771/1845 train_time:107790ms step_avg:60.86ms
step:1772/1845 train_time:107876ms step_avg:60.88ms
step:1773/1845 train_time:107964ms step_avg:60.89ms
step:1774/1845 train_time:108053ms step_avg:60.91ms
step:1775/1845 train_time:108144ms step_avg:60.93ms
step:1776/1845 train_time:108232ms step_avg:60.94ms
step:1777/1845 train_time:108321ms step_avg:60.96ms
step:1778/1845 train_time:108409ms step_avg:60.97ms
step:1779/1845 train_time:108497ms step_avg:60.99ms
step:1780/1845 train_time:108583ms step_avg:61.00ms
step:1781/1845 train_time:108671ms step_avg:61.02ms
step:1782/1845 train_time:108757ms step_avg:61.03ms
step:1783/1845 train_time:108845ms step_avg:61.05ms
step:1784/1845 train_time:108933ms step_avg:61.06ms
step:1785/1845 train_time:109023ms step_avg:61.08ms
step:1786/1845 train_time:109111ms step_avg:61.09ms
step:1787/1845 train_time:109201ms step_avg:61.11ms
step:1788/1845 train_time:109289ms step_avg:61.12ms
step:1789/1845 train_time:109378ms step_avg:61.14ms
step:1790/1845 train_time:109465ms step_avg:61.15ms
step:1791/1845 train_time:109553ms step_avg:61.17ms
step:1792/1845 train_time:109640ms step_avg:61.18ms
step:1793/1845 train_time:109728ms step_avg:61.20ms
step:1794/1845 train_time:109814ms step_avg:61.21ms
step:1795/1845 train_time:109903ms step_avg:61.23ms
step:1796/1845 train_time:109991ms step_avg:61.24ms
step:1797/1845 train_time:110080ms step_avg:61.26ms
step:1798/1845 train_time:110168ms step_avg:61.27ms
step:1799/1845 train_time:110257ms step_avg:61.29ms
step:1800/1845 train_time:110346ms step_avg:61.30ms
step:1801/1845 train_time:110435ms step_avg:61.32ms
step:1802/1845 train_time:110523ms step_avg:61.33ms
step:1803/1845 train_time:110611ms step_avg:61.35ms
step:1804/1845 train_time:110698ms step_avg:61.36ms
step:1805/1845 train_time:110786ms step_avg:61.38ms
step:1806/1845 train_time:110873ms step_avg:61.39ms
step:1807/1845 train_time:110961ms step_avg:61.41ms
step:1808/1845 train_time:111050ms step_avg:61.42ms
step:1809/1845 train_time:111138ms step_avg:61.44ms
step:1810/1845 train_time:111227ms step_avg:61.45ms
step:1811/1845 train_time:111315ms step_avg:61.47ms
step:1812/1845 train_time:111404ms step_avg:61.48ms
step:1813/1845 train_time:111493ms step_avg:61.50ms
step:1814/1845 train_time:111579ms step_avg:61.51ms
step:1815/1845 train_time:111668ms step_avg:61.53ms
step:1816/1845 train_time:111755ms step_avg:61.54ms
step:1817/1845 train_time:111845ms step_avg:61.55ms
step:1818/1845 train_time:111932ms step_avg:61.57ms
step:1819/1845 train_time:112022ms step_avg:61.58ms
step:1820/1845 train_time:112110ms step_avg:61.60ms
step:1821/1845 train_time:112198ms step_avg:61.61ms
step:1822/1845 train_time:112285ms step_avg:61.63ms
step:1823/1845 train_time:112375ms step_avg:61.64ms
step:1824/1845 train_time:112462ms step_avg:61.66ms
step:1825/1845 train_time:112552ms step_avg:61.67ms
step:1826/1845 train_time:112639ms step_avg:61.69ms
step:1827/1845 train_time:112728ms step_avg:61.70ms
step:1828/1845 train_time:112815ms step_avg:61.72ms
step:1829/1845 train_time:112904ms step_avg:61.73ms
step:1830/1845 train_time:112992ms step_avg:61.74ms
step:1831/1845 train_time:113081ms step_avg:61.76ms
step:1832/1845 train_time:113169ms step_avg:61.77ms
step:1833/1845 train_time:113257ms step_avg:61.79ms
step:1834/1845 train_time:113346ms step_avg:61.80ms
step:1835/1845 train_time:113434ms step_avg:61.82ms
step:1836/1845 train_time:113522ms step_avg:61.83ms
step:1837/1845 train_time:113610ms step_avg:61.85ms
step:1838/1845 train_time:113697ms step_avg:61.86ms
step:1839/1845 train_time:113786ms step_avg:61.87ms
step:1840/1845 train_time:113873ms step_avg:61.89ms
step:1841/1845 train_time:113962ms step_avg:61.90ms
step:1842/1845 train_time:114049ms step_avg:61.92ms
step:1843/1845 train_time:114137ms step_avg:61.93ms
step:1844/1845 train_time:114224ms step_avg:61.94ms
step:1845/1845 train_time:114313ms step_avg:61.96ms
step:1845/1845 val_loss:3.2792 train_time:114401ms step_avg:62.01ms
peak memory allocated: 29405 MiB reserved: 44238 MiB

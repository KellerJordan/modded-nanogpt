import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc
os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        group_sizes = [15, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

        # only include gates on layers with value embeds used on forward pass
        if layer_idx in [0,1,8,9,10]:
            self.value_embed_gate = CastedLinear(12, num_heads)
            self.value_embed_gate.weight.label = 'value_embed_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(self.value_embed_gate(x[..., :self.value_embed_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        #self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig, skew = None):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if skew is None:
            logits = 23 * torch.sigmoid((logits+5) / 7.5)
        else:
            logits = 23 * torch.sigmoid((logits+skew) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
def log_parameter_norms(model):
    norms = {}
    for name, param in model.named_parameters():
        if param.requires_grad:
            norms[f"param_norm/{name}"] = param.norm().item()
            norms[f"param_max/{name}"] = param.max().item()
            norms[f"param_min/{name}"] = param.min().item()
            if param.grad is not None:
                norms[f"grad_norm/{name}"] = param.grad.norm().item()
                norms[f"grad_max/{name}"] = param.grad.max().item()
    
    # Total norms
    total_param_norm = sum(p.norm().item() ** 2 for p in model.parameters()) ** 0.5
    total_grad_norm = sum(p.grad.norm().item() ** 2 for p in model.parameters() if p.grad is not None) ** 0.5
    
    norms["param_norm/total"] = total_param_norm
    norms["grad_norm/total"] = total_grad_norm
    
    return norms

train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

# import wandb
# if master_process:
#     wandb.init(
#         project="nano4",
#         name="baseline"
#     )

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    # if master_process and step%2==1:
    #     wandb.log({
    #         "loss": loss.item(),
    #         **log_parameter_norms(model)  # Add all norms
    #     })
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 29 05:51:48 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            108W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8284 train_time:0ms step_avg:0.03ms
step:1/1845 train_time:72ms step_avg:72.24ms
step:2/1845 train_time:96ms step_avg:48.13ms
step:3/1845 train_time:118ms step_avg:39.20ms
step:4/1845 train_time:152ms step_avg:37.88ms
step:5/1845 train_time:186ms step_avg:37.17ms
step:6/1845 train_time:289ms step_avg:48.18ms
step:7/1845 train_time:307ms step_avg:43.88ms
step:8/1845 train_time:336ms step_avg:42.04ms
step:9/1845 train_time:371ms step_avg:41.18ms
step:10/1845 train_time:405ms step_avg:40.46ms
step:11/1845 train_time:439ms step_avg:39.94ms
step:12/1845 train_time:473ms step_avg:39.44ms
step:13/1845 train_time:508ms step_avg:39.07ms
step:14/1845 train_time:542ms step_avg:38.72ms
step:15/1845 train_time:576ms step_avg:38.42ms
step:16/1845 train_time:610ms step_avg:38.15ms
step:17/1845 train_time:645ms step_avg:37.93ms
step:18/1845 train_time:679ms step_avg:37.72ms
step:19/1845 train_time:714ms step_avg:37.56ms
step:20/1845 train_time:748ms step_avg:37.38ms
step:21/1845 train_time:782ms step_avg:37.25ms
step:22/1845 train_time:816ms step_avg:37.10ms
step:23/1845 train_time:851ms step_avg:36.99ms
step:24/1845 train_time:885ms step_avg:36.87ms
step:25/1845 train_time:919ms step_avg:36.78ms
step:26/1845 train_time:954ms step_avg:36.68ms
step:27/1845 train_time:988ms step_avg:36.59ms
step:28/1845 train_time:1022ms step_avg:36.51ms
step:29/1845 train_time:1057ms step_avg:36.45ms
step:30/1845 train_time:1091ms step_avg:36.36ms
step:31/1845 train_time:1126ms step_avg:36.31ms
step:32/1845 train_time:1160ms step_avg:36.24ms
step:33/1845 train_time:1194ms step_avg:36.19ms
step:34/1845 train_time:1228ms step_avg:36.13ms
step:35/1845 train_time:1263ms step_avg:36.10ms
step:36/1845 train_time:1297ms step_avg:36.04ms
step:37/1845 train_time:1332ms step_avg:36.00ms
step:38/1845 train_time:1367ms step_avg:35.97ms
step:39/1845 train_time:1401ms step_avg:35.93ms
step:40/1845 train_time:1435ms step_avg:35.88ms
step:41/1845 train_time:1470ms step_avg:35.85ms
step:42/1845 train_time:1504ms step_avg:35.81ms
step:43/1845 train_time:1539ms step_avg:35.79ms
step:44/1845 train_time:1573ms step_avg:35.75ms
step:45/1845 train_time:1607ms step_avg:35.72ms
step:46/1845 train_time:1641ms step_avg:35.68ms
step:47/1845 train_time:1676ms step_avg:35.67ms
step:48/1845 train_time:1710ms step_avg:35.63ms
step:49/1845 train_time:1745ms step_avg:35.61ms
step:50/1845 train_time:1779ms step_avg:35.58ms
step:51/1845 train_time:1814ms step_avg:35.57ms
step:52/1845 train_time:1848ms step_avg:35.55ms
step:53/1845 train_time:1883ms step_avg:35.53ms
step:54/1845 train_time:1917ms step_avg:35.50ms
step:55/1845 train_time:1952ms step_avg:35.48ms
step:56/1845 train_time:1986ms step_avg:35.46ms
step:57/1845 train_time:2020ms step_avg:35.45ms
step:58/1845 train_time:2054ms step_avg:35.42ms
step:59/1845 train_time:2089ms step_avg:35.41ms
step:60/1845 train_time:2123ms step_avg:35.38ms
step:61/1845 train_time:2158ms step_avg:35.38ms
step:62/1845 train_time:2192ms step_avg:35.35ms
step:63/1845 train_time:2227ms step_avg:35.34ms
step:64/1845 train_time:2261ms step_avg:35.32ms
step:65/1845 train_time:2295ms step_avg:35.31ms
step:66/1845 train_time:2329ms step_avg:35.29ms
step:67/1845 train_time:2364ms step_avg:35.29ms
step:68/1845 train_time:2398ms step_avg:35.27ms
step:69/1845 train_time:2433ms step_avg:35.26ms
step:70/1845 train_time:2467ms step_avg:35.25ms
step:71/1845 train_time:2502ms step_avg:35.24ms
step:72/1845 train_time:2536ms step_avg:35.22ms
step:73/1845 train_time:2571ms step_avg:35.21ms
step:74/1845 train_time:2605ms step_avg:35.20ms
step:75/1845 train_time:2639ms step_avg:35.19ms
step:76/1845 train_time:2673ms step_avg:35.17ms
step:77/1845 train_time:2708ms step_avg:35.17ms
step:78/1845 train_time:2742ms step_avg:35.15ms
step:79/1845 train_time:2776ms step_avg:35.14ms
step:80/1845 train_time:2810ms step_avg:35.13ms
step:81/1845 train_time:2845ms step_avg:35.12ms
step:82/1845 train_time:2879ms step_avg:35.11ms
step:83/1845 train_time:2914ms step_avg:35.10ms
step:84/1845 train_time:2948ms step_avg:35.09ms
step:85/1845 train_time:2982ms step_avg:35.09ms
step:86/1845 train_time:3016ms step_avg:35.07ms
step:87/1845 train_time:3051ms step_avg:35.07ms
step:88/1845 train_time:3085ms step_avg:35.06ms
step:89/1845 train_time:3120ms step_avg:35.06ms
step:90/1845 train_time:3154ms step_avg:35.04ms
step:91/1845 train_time:3189ms step_avg:35.04ms
step:92/1845 train_time:3223ms step_avg:35.03ms
step:93/1845 train_time:3257ms step_avg:35.02ms
step:94/1845 train_time:3291ms step_avg:35.01ms
step:95/1845 train_time:3326ms step_avg:35.01ms
step:96/1845 train_time:3360ms step_avg:35.00ms
step:97/1845 train_time:3395ms step_avg:35.00ms
step:98/1845 train_time:3429ms step_avg:34.98ms
step:99/1845 train_time:3463ms step_avg:34.98ms
step:100/1845 train_time:3497ms step_avg:34.97ms
step:101/1845 train_time:3532ms step_avg:34.97ms
step:102/1845 train_time:3566ms step_avg:34.96ms
step:103/1845 train_time:3600ms step_avg:34.96ms
step:104/1845 train_time:3634ms step_avg:34.95ms
step:105/1845 train_time:3669ms step_avg:34.94ms
step:106/1845 train_time:3703ms step_avg:34.93ms
step:107/1845 train_time:3737ms step_avg:34.93ms
step:108/1845 train_time:3771ms step_avg:34.92ms
step:109/1845 train_time:3806ms step_avg:34.92ms
step:110/1845 train_time:3840ms step_avg:34.91ms
step:111/1845 train_time:3874ms step_avg:34.90ms
step:112/1845 train_time:3908ms step_avg:34.90ms
step:113/1845 train_time:3943ms step_avg:34.89ms
step:114/1845 train_time:3977ms step_avg:34.89ms
step:115/1845 train_time:4012ms step_avg:34.88ms
step:116/1845 train_time:4045ms step_avg:34.87ms
step:117/1845 train_time:4080ms step_avg:34.87ms
step:118/1845 train_time:4114ms step_avg:34.87ms
step:119/1845 train_time:4149ms step_avg:34.86ms
step:120/1845 train_time:4183ms step_avg:34.86ms
step:121/1845 train_time:4218ms step_avg:34.86ms
step:122/1845 train_time:4252ms step_avg:34.85ms
step:123/1845 train_time:4286ms step_avg:34.85ms
step:124/1845 train_time:4320ms step_avg:34.84ms
step:125/1845 train_time:4355ms step_avg:34.84ms
step:126/1845 train_time:4389ms step_avg:34.84ms
step:127/1845 train_time:4424ms step_avg:34.83ms
step:128/1845 train_time:4458ms step_avg:34.83ms
step:129/1845 train_time:4492ms step_avg:34.82ms
step:130/1845 train_time:4526ms step_avg:34.82ms
step:131/1845 train_time:4561ms step_avg:34.82ms
step:132/1845 train_time:4595ms step_avg:34.81ms
step:133/1845 train_time:4629ms step_avg:34.81ms
step:134/1845 train_time:4664ms step_avg:34.80ms
step:135/1845 train_time:4698ms step_avg:34.80ms
step:136/1845 train_time:4732ms step_avg:34.80ms
step:137/1845 train_time:4767ms step_avg:34.80ms
step:138/1845 train_time:4801ms step_avg:34.79ms
step:139/1845 train_time:4836ms step_avg:34.79ms
step:140/1845 train_time:4870ms step_avg:34.78ms
step:141/1845 train_time:4904ms step_avg:34.78ms
step:142/1845 train_time:4938ms step_avg:34.78ms
step:143/1845 train_time:4973ms step_avg:34.77ms
step:144/1845 train_time:5007ms step_avg:34.77ms
step:145/1845 train_time:5041ms step_avg:34.77ms
step:146/1845 train_time:5075ms step_avg:34.76ms
step:147/1845 train_time:5110ms step_avg:34.76ms
step:148/1845 train_time:5143ms step_avg:34.75ms
step:149/1845 train_time:5178ms step_avg:34.75ms
step:150/1845 train_time:5212ms step_avg:34.75ms
step:151/1845 train_time:5247ms step_avg:34.75ms
step:152/1845 train_time:5281ms step_avg:34.74ms
step:153/1845 train_time:5316ms step_avg:34.74ms
step:154/1845 train_time:5350ms step_avg:34.74ms
step:155/1845 train_time:5384ms step_avg:34.74ms
step:156/1845 train_time:5418ms step_avg:34.73ms
step:157/1845 train_time:5453ms step_avg:34.73ms
step:158/1845 train_time:5487ms step_avg:34.73ms
step:159/1845 train_time:5522ms step_avg:34.73ms
step:160/1845 train_time:5556ms step_avg:34.72ms
step:161/1845 train_time:5590ms step_avg:34.72ms
step:162/1845 train_time:5624ms step_avg:34.72ms
step:163/1845 train_time:5659ms step_avg:34.72ms
step:164/1845 train_time:5693ms step_avg:34.71ms
step:165/1845 train_time:5727ms step_avg:34.71ms
step:166/1845 train_time:5762ms step_avg:34.71ms
step:167/1845 train_time:5796ms step_avg:34.71ms
step:168/1845 train_time:5830ms step_avg:34.70ms
step:169/1845 train_time:5865ms step_avg:34.70ms
step:170/1845 train_time:5899ms step_avg:34.70ms
step:171/1845 train_time:5933ms step_avg:34.70ms
step:172/1845 train_time:5967ms step_avg:34.69ms
step:173/1845 train_time:6002ms step_avg:34.69ms
step:174/1845 train_time:6035ms step_avg:34.69ms
step:175/1845 train_time:6070ms step_avg:34.68ms
step:176/1845 train_time:6104ms step_avg:34.68ms
step:177/1845 train_time:6138ms step_avg:34.68ms
step:178/1845 train_time:6172ms step_avg:34.68ms
step:179/1845 train_time:6207ms step_avg:34.68ms
step:180/1845 train_time:6241ms step_avg:34.67ms
step:181/1845 train_time:6276ms step_avg:34.67ms
step:182/1845 train_time:6309ms step_avg:34.67ms
step:183/1845 train_time:6344ms step_avg:34.67ms
step:184/1845 train_time:6378ms step_avg:34.66ms
step:185/1845 train_time:6413ms step_avg:34.66ms
step:186/1845 train_time:6447ms step_avg:34.66ms
step:187/1845 train_time:6481ms step_avg:34.66ms
step:188/1845 train_time:6515ms step_avg:34.66ms
step:189/1845 train_time:6550ms step_avg:34.66ms
step:190/1845 train_time:6584ms step_avg:34.65ms
step:191/1845 train_time:6619ms step_avg:34.66ms
step:192/1845 train_time:6653ms step_avg:34.65ms
step:193/1845 train_time:6688ms step_avg:34.65ms
step:194/1845 train_time:6722ms step_avg:34.65ms
step:195/1845 train_time:6756ms step_avg:34.65ms
step:196/1845 train_time:6790ms step_avg:34.64ms
step:197/1845 train_time:6825ms step_avg:34.65ms
step:198/1845 train_time:6859ms step_avg:34.64ms
step:199/1845 train_time:6893ms step_avg:34.64ms
step:200/1845 train_time:6927ms step_avg:34.64ms
step:201/1845 train_time:6962ms step_avg:34.64ms
step:202/1845 train_time:6996ms step_avg:34.63ms
step:203/1845 train_time:7030ms step_avg:34.63ms
step:204/1845 train_time:7064ms step_avg:34.63ms
step:205/1845 train_time:7099ms step_avg:34.63ms
step:206/1845 train_time:7133ms step_avg:34.63ms
step:207/1845 train_time:7167ms step_avg:34.62ms
step:208/1845 train_time:7201ms step_avg:34.62ms
step:209/1845 train_time:7236ms step_avg:34.62ms
step:210/1845 train_time:7270ms step_avg:34.62ms
step:211/1845 train_time:7304ms step_avg:34.62ms
step:212/1845 train_time:7338ms step_avg:34.61ms
step:213/1845 train_time:7373ms step_avg:34.61ms
step:214/1845 train_time:7407ms step_avg:34.61ms
step:215/1845 train_time:7441ms step_avg:34.61ms
step:216/1845 train_time:7475ms step_avg:34.61ms
step:217/1845 train_time:7510ms step_avg:34.61ms
step:218/1845 train_time:7544ms step_avg:34.60ms
step:219/1845 train_time:7579ms step_avg:34.61ms
step:220/1845 train_time:7613ms step_avg:34.60ms
step:221/1845 train_time:7647ms step_avg:34.60ms
step:222/1845 train_time:7681ms step_avg:34.60ms
step:223/1845 train_time:7716ms step_avg:34.60ms
step:224/1845 train_time:7750ms step_avg:34.60ms
step:225/1845 train_time:7785ms step_avg:34.60ms
step:226/1845 train_time:7819ms step_avg:34.60ms
step:227/1845 train_time:7853ms step_avg:34.59ms
step:228/1845 train_time:7887ms step_avg:34.59ms
step:229/1845 train_time:7922ms step_avg:34.59ms
step:230/1845 train_time:7955ms step_avg:34.59ms
step:231/1845 train_time:7990ms step_avg:34.59ms
step:232/1845 train_time:8024ms step_avg:34.58ms
step:233/1845 train_time:8058ms step_avg:34.59ms
step:234/1845 train_time:8092ms step_avg:34.58ms
step:235/1845 train_time:8127ms step_avg:34.58ms
step:236/1845 train_time:8161ms step_avg:34.58ms
step:237/1845 train_time:8195ms step_avg:34.58ms
step:238/1845 train_time:8229ms step_avg:34.58ms
step:239/1845 train_time:8264ms step_avg:34.58ms
step:240/1845 train_time:8298ms step_avg:34.57ms
step:241/1845 train_time:8332ms step_avg:34.57ms
step:242/1845 train_time:8366ms step_avg:34.57ms
step:243/1845 train_time:8400ms step_avg:34.57ms
step:244/1845 train_time:8434ms step_avg:34.57ms
step:245/1845 train_time:8469ms step_avg:34.57ms
step:246/1845 train_time:8503ms step_avg:34.56ms
step:247/1845 train_time:8537ms step_avg:34.56ms
step:248/1845 train_time:8571ms step_avg:34.56ms
step:249/1845 train_time:8606ms step_avg:34.56ms
step:250/1845 train_time:8640ms step_avg:34.56ms
step:250/1845 val_loss:4.6111 train_time:8676ms step_avg:34.70ms
step:251/1845 train_time:8696ms step_avg:34.64ms
step:252/1845 train_time:8716ms step_avg:34.59ms
step:253/1845 train_time:8745ms step_avg:34.57ms
step:254/1845 train_time:8780ms step_avg:34.56ms
step:255/1845 train_time:8815ms step_avg:34.57ms
step:256/1845 train_time:8849ms step_avg:34.57ms
step:257/1845 train_time:8884ms step_avg:34.57ms
step:258/1845 train_time:8918ms step_avg:34.57ms
step:259/1845 train_time:8953ms step_avg:34.57ms
step:260/1845 train_time:8987ms step_avg:34.56ms
step:261/1845 train_time:9021ms step_avg:34.56ms
step:262/1845 train_time:9055ms step_avg:34.56ms
step:263/1845 train_time:9089ms step_avg:34.56ms
step:264/1845 train_time:9123ms step_avg:34.56ms
step:265/1845 train_time:9157ms step_avg:34.56ms
step:266/1845 train_time:9191ms step_avg:34.55ms
step:267/1845 train_time:9226ms step_avg:34.55ms
step:268/1845 train_time:9260ms step_avg:34.55ms
step:269/1845 train_time:9294ms step_avg:34.55ms
step:270/1845 train_time:9328ms step_avg:34.55ms
step:271/1845 train_time:9362ms step_avg:34.55ms
step:272/1845 train_time:9396ms step_avg:34.54ms
step:273/1845 train_time:9430ms step_avg:34.54ms
step:274/1845 train_time:9464ms step_avg:34.54ms
step:275/1845 train_time:9498ms step_avg:34.54ms
step:276/1845 train_time:9532ms step_avg:34.54ms
step:277/1845 train_time:9566ms step_avg:34.54ms
step:278/1845 train_time:9600ms step_avg:34.53ms
step:279/1845 train_time:9635ms step_avg:34.53ms
step:280/1845 train_time:9669ms step_avg:34.53ms
step:281/1845 train_time:9703ms step_avg:34.53ms
step:282/1845 train_time:9738ms step_avg:34.53ms
step:283/1845 train_time:9772ms step_avg:34.53ms
step:284/1845 train_time:9806ms step_avg:34.53ms
step:285/1845 train_time:9841ms step_avg:34.53ms
step:286/1845 train_time:9875ms step_avg:34.53ms
step:287/1845 train_time:9909ms step_avg:34.53ms
step:288/1845 train_time:9944ms step_avg:34.53ms
step:289/1845 train_time:9978ms step_avg:34.53ms
step:290/1845 train_time:10012ms step_avg:34.52ms
step:291/1845 train_time:10046ms step_avg:34.52ms
step:292/1845 train_time:10080ms step_avg:34.52ms
step:293/1845 train_time:10115ms step_avg:34.52ms
step:294/1845 train_time:10149ms step_avg:34.52ms
step:295/1845 train_time:10183ms step_avg:34.52ms
step:296/1845 train_time:10218ms step_avg:34.52ms
step:297/1845 train_time:10252ms step_avg:34.52ms
step:298/1845 train_time:10285ms step_avg:34.52ms
step:299/1845 train_time:10320ms step_avg:34.51ms
step:300/1845 train_time:10354ms step_avg:34.51ms
step:301/1845 train_time:10388ms step_avg:34.51ms
step:302/1845 train_time:10422ms step_avg:34.51ms
step:303/1845 train_time:10456ms step_avg:34.51ms
step:304/1845 train_time:10490ms step_avg:34.51ms
step:305/1845 train_time:10525ms step_avg:34.51ms
step:306/1845 train_time:10559ms step_avg:34.51ms
step:307/1845 train_time:10593ms step_avg:34.51ms
step:308/1845 train_time:10627ms step_avg:34.50ms
step:309/1845 train_time:10661ms step_avg:34.50ms
step:310/1845 train_time:10695ms step_avg:34.50ms
step:311/1845 train_time:10730ms step_avg:34.50ms
step:312/1845 train_time:10764ms step_avg:34.50ms
step:313/1845 train_time:10798ms step_avg:34.50ms
step:314/1845 train_time:10832ms step_avg:34.50ms
step:315/1845 train_time:10866ms step_avg:34.50ms
step:316/1845 train_time:10900ms step_avg:34.49ms
step:317/1845 train_time:10935ms step_avg:34.49ms
step:318/1845 train_time:10968ms step_avg:34.49ms
step:319/1845 train_time:11003ms step_avg:34.49ms
step:320/1845 train_time:11037ms step_avg:34.49ms
step:321/1845 train_time:11072ms step_avg:34.49ms
step:322/1845 train_time:11106ms step_avg:34.49ms
step:323/1845 train_time:11140ms step_avg:34.49ms
step:324/1845 train_time:11174ms step_avg:34.49ms
step:325/1845 train_time:11209ms step_avg:34.49ms
step:326/1845 train_time:11243ms step_avg:34.49ms
step:327/1845 train_time:11278ms step_avg:34.49ms
step:328/1845 train_time:11311ms step_avg:34.49ms
step:329/1845 train_time:11346ms step_avg:34.49ms
step:330/1845 train_time:11380ms step_avg:34.48ms
step:331/1845 train_time:11414ms step_avg:34.48ms
step:332/1845 train_time:11448ms step_avg:34.48ms
step:333/1845 train_time:11482ms step_avg:34.48ms
step:334/1845 train_time:11516ms step_avg:34.48ms
step:335/1845 train_time:11550ms step_avg:34.48ms
step:336/1845 train_time:11584ms step_avg:34.48ms
step:337/1845 train_time:11619ms step_avg:34.48ms
step:338/1845 train_time:11653ms step_avg:34.48ms
step:339/1845 train_time:11687ms step_avg:34.48ms
step:340/1845 train_time:11721ms step_avg:34.47ms
step:341/1845 train_time:11756ms step_avg:34.48ms
step:342/1845 train_time:11790ms step_avg:34.47ms
step:343/1845 train_time:11824ms step_avg:34.47ms
step:344/1845 train_time:11859ms step_avg:34.47ms
step:345/1845 train_time:11893ms step_avg:34.47ms
step:346/1845 train_time:11927ms step_avg:34.47ms
step:347/1845 train_time:11962ms step_avg:34.47ms
step:348/1845 train_time:11996ms step_avg:34.47ms
step:349/1845 train_time:12030ms step_avg:34.47ms
step:350/1845 train_time:12063ms step_avg:34.47ms
step:351/1845 train_time:12098ms step_avg:34.47ms
step:352/1845 train_time:12132ms step_avg:34.47ms
step:353/1845 train_time:12166ms step_avg:34.47ms
step:354/1845 train_time:12200ms step_avg:34.46ms
step:355/1845 train_time:12234ms step_avg:34.46ms
step:356/1845 train_time:12268ms step_avg:34.46ms
step:357/1845 train_time:12303ms step_avg:34.46ms
step:358/1845 train_time:12337ms step_avg:34.46ms
step:359/1845 train_time:12371ms step_avg:34.46ms
step:360/1845 train_time:12405ms step_avg:34.46ms
step:361/1845 train_time:12440ms step_avg:34.46ms
step:362/1845 train_time:12474ms step_avg:34.46ms
step:363/1845 train_time:12508ms step_avg:34.46ms
step:364/1845 train_time:12542ms step_avg:34.46ms
step:365/1845 train_time:12576ms step_avg:34.45ms
step:366/1845 train_time:12610ms step_avg:34.45ms
step:367/1845 train_time:12644ms step_avg:34.45ms
step:368/1845 train_time:12678ms step_avg:34.45ms
step:369/1845 train_time:12712ms step_avg:34.45ms
step:370/1845 train_time:12746ms step_avg:34.45ms
step:371/1845 train_time:12780ms step_avg:34.45ms
step:372/1845 train_time:12814ms step_avg:34.45ms
step:373/1845 train_time:12849ms step_avg:34.45ms
step:374/1845 train_time:12883ms step_avg:34.45ms
step:375/1845 train_time:12917ms step_avg:34.45ms
step:376/1845 train_time:12951ms step_avg:34.44ms
step:377/1845 train_time:12985ms step_avg:34.44ms
step:378/1845 train_time:13019ms step_avg:34.44ms
step:379/1845 train_time:13054ms step_avg:34.44ms
step:380/1845 train_time:13088ms step_avg:34.44ms
step:381/1845 train_time:13122ms step_avg:34.44ms
step:382/1845 train_time:13156ms step_avg:34.44ms
step:383/1845 train_time:13190ms step_avg:34.44ms
step:384/1845 train_time:13224ms step_avg:34.44ms
step:385/1845 train_time:13259ms step_avg:34.44ms
step:386/1845 train_time:13293ms step_avg:34.44ms
step:387/1845 train_time:13327ms step_avg:34.44ms
step:388/1845 train_time:13361ms step_avg:34.44ms
step:389/1845 train_time:13395ms step_avg:34.44ms
step:390/1845 train_time:13429ms step_avg:34.43ms
step:391/1845 train_time:13464ms step_avg:34.43ms
step:392/1845 train_time:13498ms step_avg:34.43ms
step:393/1845 train_time:13532ms step_avg:34.43ms
step:394/1845 train_time:13566ms step_avg:34.43ms
step:395/1845 train_time:13600ms step_avg:34.43ms
step:396/1845 train_time:13634ms step_avg:34.43ms
step:397/1845 train_time:13668ms step_avg:34.43ms
step:398/1845 train_time:13702ms step_avg:34.43ms
step:399/1845 train_time:13736ms step_avg:34.43ms
step:400/1845 train_time:13770ms step_avg:34.43ms
step:401/1845 train_time:13804ms step_avg:34.43ms
step:402/1845 train_time:13838ms step_avg:34.42ms
step:403/1845 train_time:13873ms step_avg:34.42ms
step:404/1845 train_time:13907ms step_avg:34.42ms
step:405/1845 train_time:13942ms step_avg:34.42ms
step:406/1845 train_time:13976ms step_avg:34.42ms
step:407/1845 train_time:14010ms step_avg:34.42ms
step:408/1845 train_time:14044ms step_avg:34.42ms
step:409/1845 train_time:14078ms step_avg:34.42ms
step:410/1845 train_time:14112ms step_avg:34.42ms
step:411/1845 train_time:14146ms step_avg:34.42ms
step:412/1845 train_time:14180ms step_avg:34.42ms
step:413/1845 train_time:14215ms step_avg:34.42ms
step:414/1845 train_time:14248ms step_avg:34.42ms
step:415/1845 train_time:14283ms step_avg:34.42ms
step:416/1845 train_time:14317ms step_avg:34.42ms
step:417/1845 train_time:14351ms step_avg:34.41ms
step:418/1845 train_time:14385ms step_avg:34.41ms
step:419/1845 train_time:14419ms step_avg:34.41ms
step:420/1845 train_time:14453ms step_avg:34.41ms
step:421/1845 train_time:14488ms step_avg:34.41ms
step:422/1845 train_time:14521ms step_avg:34.41ms
step:423/1845 train_time:14556ms step_avg:34.41ms
step:424/1845 train_time:14590ms step_avg:34.41ms
step:425/1845 train_time:14624ms step_avg:34.41ms
step:426/1845 train_time:14658ms step_avg:34.41ms
step:427/1845 train_time:14693ms step_avg:34.41ms
step:428/1845 train_time:14727ms step_avg:34.41ms
step:429/1845 train_time:14761ms step_avg:34.41ms
step:430/1845 train_time:14795ms step_avg:34.41ms
step:431/1845 train_time:14829ms step_avg:34.41ms
step:432/1845 train_time:14863ms step_avg:34.41ms
step:433/1845 train_time:14897ms step_avg:34.41ms
step:434/1845 train_time:14931ms step_avg:34.40ms
step:435/1845 train_time:14965ms step_avg:34.40ms
step:436/1845 train_time:14999ms step_avg:34.40ms
step:437/1845 train_time:15034ms step_avg:34.40ms
step:438/1845 train_time:15068ms step_avg:34.40ms
step:439/1845 train_time:15102ms step_avg:34.40ms
step:440/1845 train_time:15136ms step_avg:34.40ms
step:441/1845 train_time:15170ms step_avg:34.40ms
step:442/1845 train_time:15204ms step_avg:34.40ms
step:443/1845 train_time:15238ms step_avg:34.40ms
step:444/1845 train_time:15272ms step_avg:34.40ms
step:445/1845 train_time:15307ms step_avg:34.40ms
step:446/1845 train_time:15341ms step_avg:34.40ms
step:447/1845 train_time:15375ms step_avg:34.40ms
step:448/1845 train_time:15409ms step_avg:34.40ms
step:449/1845 train_time:15444ms step_avg:34.40ms
step:450/1845 train_time:15478ms step_avg:34.40ms
step:451/1845 train_time:15512ms step_avg:34.39ms
step:452/1845 train_time:15546ms step_avg:34.39ms
step:453/1845 train_time:15581ms step_avg:34.39ms
step:454/1845 train_time:15614ms step_avg:34.39ms
step:455/1845 train_time:15649ms step_avg:34.39ms
step:456/1845 train_time:15683ms step_avg:34.39ms
step:457/1845 train_time:15717ms step_avg:34.39ms
step:458/1845 train_time:15752ms step_avg:34.39ms
step:459/1845 train_time:15786ms step_avg:34.39ms
step:460/1845 train_time:15820ms step_avg:34.39ms
step:461/1845 train_time:15854ms step_avg:34.39ms
step:462/1845 train_time:15888ms step_avg:34.39ms
step:463/1845 train_time:15922ms step_avg:34.39ms
step:464/1845 train_time:15956ms step_avg:34.39ms
step:465/1845 train_time:15990ms step_avg:34.39ms
step:466/1845 train_time:16024ms step_avg:34.39ms
step:467/1845 train_time:16058ms step_avg:34.39ms
step:468/1845 train_time:16092ms step_avg:34.39ms
step:469/1845 train_time:16127ms step_avg:34.39ms
step:470/1845 train_time:16161ms step_avg:34.38ms
step:471/1845 train_time:16195ms step_avg:34.38ms
step:472/1845 train_time:16229ms step_avg:34.38ms
step:473/1845 train_time:16263ms step_avg:34.38ms
step:474/1845 train_time:16298ms step_avg:34.38ms
step:475/1845 train_time:16332ms step_avg:34.38ms
step:476/1845 train_time:16366ms step_avg:34.38ms
step:477/1845 train_time:16400ms step_avg:34.38ms
step:478/1845 train_time:16434ms step_avg:34.38ms
step:479/1845 train_time:16469ms step_avg:34.38ms
step:480/1845 train_time:16503ms step_avg:34.38ms
step:481/1845 train_time:16537ms step_avg:34.38ms
step:482/1845 train_time:16571ms step_avg:34.38ms
step:483/1845 train_time:16606ms step_avg:34.38ms
step:484/1845 train_time:16640ms step_avg:34.38ms
step:485/1845 train_time:16674ms step_avg:34.38ms
step:486/1845 train_time:16708ms step_avg:34.38ms
step:487/1845 train_time:16743ms step_avg:34.38ms
step:488/1845 train_time:16777ms step_avg:34.38ms
step:489/1845 train_time:16811ms step_avg:34.38ms
step:490/1845 train_time:16845ms step_avg:34.38ms
step:491/1845 train_time:16879ms step_avg:34.38ms
step:492/1845 train_time:16913ms step_avg:34.38ms
step:493/1845 train_time:16947ms step_avg:34.38ms
step:494/1845 train_time:16981ms step_avg:34.38ms
step:495/1845 train_time:17016ms step_avg:34.38ms
step:496/1845 train_time:17050ms step_avg:34.37ms
step:497/1845 train_time:17084ms step_avg:34.37ms
step:498/1845 train_time:17118ms step_avg:34.37ms
step:499/1845 train_time:17152ms step_avg:34.37ms
step:500/1845 train_time:17186ms step_avg:34.37ms
step:500/1845 val_loss:4.2847 train_time:17223ms step_avg:34.45ms
step:501/1845 train_time:17242ms step_avg:34.42ms
step:502/1845 train_time:17263ms step_avg:34.39ms
step:503/1845 train_time:17291ms step_avg:34.38ms
step:504/1845 train_time:17326ms step_avg:34.38ms
step:505/1845 train_time:17362ms step_avg:34.38ms
step:506/1845 train_time:17396ms step_avg:34.38ms
step:507/1845 train_time:17431ms step_avg:34.38ms
step:508/1845 train_time:17465ms step_avg:34.38ms
step:509/1845 train_time:17499ms step_avg:34.38ms
step:510/1845 train_time:17533ms step_avg:34.38ms
step:511/1845 train_time:17567ms step_avg:34.38ms
step:512/1845 train_time:17601ms step_avg:34.38ms
step:513/1845 train_time:17635ms step_avg:34.38ms
step:514/1845 train_time:17669ms step_avg:34.38ms
step:515/1845 train_time:17704ms step_avg:34.38ms
step:516/1845 train_time:17738ms step_avg:34.38ms
step:517/1845 train_time:17772ms step_avg:34.37ms
step:518/1845 train_time:17806ms step_avg:34.37ms
step:519/1845 train_time:17840ms step_avg:34.37ms
step:520/1845 train_time:17874ms step_avg:34.37ms
step:521/1845 train_time:17908ms step_avg:34.37ms
step:522/1845 train_time:17942ms step_avg:34.37ms
step:523/1845 train_time:17976ms step_avg:34.37ms
step:524/1845 train_time:18010ms step_avg:34.37ms
step:525/1845 train_time:18045ms step_avg:34.37ms
step:526/1845 train_time:18079ms step_avg:34.37ms
step:527/1845 train_time:18113ms step_avg:34.37ms
step:528/1845 train_time:18147ms step_avg:34.37ms
step:529/1845 train_time:18181ms step_avg:34.37ms
step:530/1845 train_time:18215ms step_avg:34.37ms
step:531/1845 train_time:18250ms step_avg:34.37ms
step:532/1845 train_time:18284ms step_avg:34.37ms
step:533/1845 train_time:18318ms step_avg:34.37ms
step:534/1845 train_time:18352ms step_avg:34.37ms
step:535/1845 train_time:18387ms step_avg:34.37ms
step:536/1845 train_time:18421ms step_avg:34.37ms
step:537/1845 train_time:18455ms step_avg:34.37ms
step:538/1845 train_time:18489ms step_avg:34.37ms
step:539/1845 train_time:18524ms step_avg:34.37ms
step:540/1845 train_time:18557ms step_avg:34.37ms
step:541/1845 train_time:18592ms step_avg:34.37ms
step:542/1845 train_time:18625ms step_avg:34.36ms
step:543/1845 train_time:18660ms step_avg:34.36ms
step:544/1845 train_time:18694ms step_avg:34.36ms
step:545/1845 train_time:18728ms step_avg:34.36ms
step:546/1845 train_time:18762ms step_avg:34.36ms
step:547/1845 train_time:18796ms step_avg:34.36ms
step:548/1845 train_time:18830ms step_avg:34.36ms
step:549/1845 train_time:18865ms step_avg:34.36ms
step:550/1845 train_time:18899ms step_avg:34.36ms
step:551/1845 train_time:18933ms step_avg:34.36ms
step:552/1845 train_time:18967ms step_avg:34.36ms
step:553/1845 train_time:19001ms step_avg:34.36ms
step:554/1845 train_time:19035ms step_avg:34.36ms
step:555/1845 train_time:19070ms step_avg:34.36ms
step:556/1845 train_time:19104ms step_avg:34.36ms
step:557/1845 train_time:19138ms step_avg:34.36ms
step:558/1845 train_time:19172ms step_avg:34.36ms
step:559/1845 train_time:19206ms step_avg:34.36ms
step:560/1845 train_time:19240ms step_avg:34.36ms
step:561/1845 train_time:19275ms step_avg:34.36ms
step:562/1845 train_time:19309ms step_avg:34.36ms
step:563/1845 train_time:19344ms step_avg:34.36ms
step:564/1845 train_time:19378ms step_avg:34.36ms
step:565/1845 train_time:19412ms step_avg:34.36ms
step:566/1845 train_time:19446ms step_avg:34.36ms
step:567/1845 train_time:19481ms step_avg:34.36ms
step:568/1845 train_time:19515ms step_avg:34.36ms
step:569/1845 train_time:19549ms step_avg:34.36ms
step:570/1845 train_time:19583ms step_avg:34.36ms
step:571/1845 train_time:19617ms step_avg:34.36ms
step:572/1845 train_time:19651ms step_avg:34.36ms
step:573/1845 train_time:19686ms step_avg:34.36ms
step:574/1845 train_time:19720ms step_avg:34.35ms
step:575/1845 train_time:19754ms step_avg:34.35ms
step:576/1845 train_time:19788ms step_avg:34.35ms
step:577/1845 train_time:19823ms step_avg:34.35ms
step:578/1845 train_time:19856ms step_avg:34.35ms
step:579/1845 train_time:19891ms step_avg:34.35ms
step:580/1845 train_time:19925ms step_avg:34.35ms
step:581/1845 train_time:19959ms step_avg:34.35ms
step:582/1845 train_time:19993ms step_avg:34.35ms
step:583/1845 train_time:20027ms step_avg:34.35ms
step:584/1845 train_time:20061ms step_avg:34.35ms
step:585/1845 train_time:20095ms step_avg:34.35ms
step:586/1845 train_time:20129ms step_avg:34.35ms
step:587/1845 train_time:20164ms step_avg:34.35ms
step:588/1845 train_time:20198ms step_avg:34.35ms
step:589/1845 train_time:20232ms step_avg:34.35ms
step:590/1845 train_time:20266ms step_avg:34.35ms
step:591/1845 train_time:20300ms step_avg:34.35ms
step:592/1845 train_time:20334ms step_avg:34.35ms
step:593/1845 train_time:20369ms step_avg:34.35ms
step:594/1845 train_time:20403ms step_avg:34.35ms
step:595/1845 train_time:20437ms step_avg:34.35ms
step:596/1845 train_time:20471ms step_avg:34.35ms
step:597/1845 train_time:20506ms step_avg:34.35ms
step:598/1845 train_time:20540ms step_avg:34.35ms
step:599/1845 train_time:20574ms step_avg:34.35ms
step:600/1845 train_time:20608ms step_avg:34.35ms
step:601/1845 train_time:20642ms step_avg:34.35ms
step:602/1845 train_time:20676ms step_avg:34.35ms
step:603/1845 train_time:20712ms step_avg:34.35ms
step:604/1845 train_time:20772ms step_avg:34.39ms
step:605/1845 train_time:20834ms step_avg:34.44ms
step:606/1845 train_time:20895ms step_avg:34.48ms
step:607/1845 train_time:20957ms step_avg:34.53ms
step:608/1845 train_time:21017ms step_avg:34.57ms
step:609/1845 train_time:21080ms step_avg:34.61ms
step:610/1845 train_time:21140ms step_avg:34.66ms
step:611/1845 train_time:21203ms step_avg:34.70ms
step:612/1845 train_time:21264ms step_avg:34.74ms
step:613/1845 train_time:21327ms step_avg:34.79ms
step:614/1845 train_time:21387ms step_avg:34.83ms
step:615/1845 train_time:21450ms step_avg:34.88ms
step:616/1845 train_time:21511ms step_avg:34.92ms
step:617/1845 train_time:21573ms step_avg:34.96ms
step:618/1845 train_time:21634ms step_avg:35.01ms
step:619/1845 train_time:21696ms step_avg:35.05ms
step:620/1845 train_time:21757ms step_avg:35.09ms
step:621/1845 train_time:21819ms step_avg:35.14ms
step:622/1845 train_time:21880ms step_avg:35.18ms
step:623/1845 train_time:21944ms step_avg:35.22ms
step:624/1845 train_time:22004ms step_avg:35.26ms
step:625/1845 train_time:22066ms step_avg:35.31ms
step:626/1845 train_time:22127ms step_avg:35.35ms
step:627/1845 train_time:22190ms step_avg:35.39ms
step:628/1845 train_time:22251ms step_avg:35.43ms
step:629/1845 train_time:22314ms step_avg:35.48ms
step:630/1845 train_time:22375ms step_avg:35.52ms
step:631/1845 train_time:22438ms step_avg:35.56ms
step:632/1845 train_time:22498ms step_avg:35.60ms
step:633/1845 train_time:22561ms step_avg:35.64ms
step:634/1845 train_time:22622ms step_avg:35.68ms
step:635/1845 train_time:22685ms step_avg:35.72ms
step:636/1845 train_time:22746ms step_avg:35.76ms
step:637/1845 train_time:22808ms step_avg:35.81ms
step:638/1845 train_time:22870ms step_avg:35.85ms
step:639/1845 train_time:22933ms step_avg:35.89ms
step:640/1845 train_time:22994ms step_avg:35.93ms
step:641/1845 train_time:23056ms step_avg:35.97ms
step:642/1845 train_time:23117ms step_avg:36.01ms
step:643/1845 train_time:23179ms step_avg:36.05ms
step:644/1845 train_time:23240ms step_avg:36.09ms
step:645/1845 train_time:23302ms step_avg:36.13ms
step:646/1845 train_time:23363ms step_avg:36.17ms
step:647/1845 train_time:23425ms step_avg:36.21ms
step:648/1845 train_time:23487ms step_avg:36.25ms
step:649/1845 train_time:23549ms step_avg:36.29ms
step:650/1845 train_time:23610ms step_avg:36.32ms
step:651/1845 train_time:23673ms step_avg:36.36ms
step:652/1845 train_time:23734ms step_avg:36.40ms
step:653/1845 train_time:23797ms step_avg:36.44ms
step:654/1845 train_time:23857ms step_avg:36.48ms
step:655/1845 train_time:23920ms step_avg:36.52ms
step:656/1845 train_time:23980ms step_avg:36.55ms
step:657/1845 train_time:24042ms step_avg:36.59ms
step:658/1845 train_time:24103ms step_avg:36.63ms
step:659/1845 train_time:24166ms step_avg:36.67ms
step:660/1845 train_time:24226ms step_avg:36.71ms
step:661/1845 train_time:24290ms step_avg:36.75ms
step:662/1845 train_time:24351ms step_avg:36.78ms
step:663/1845 train_time:24414ms step_avg:36.82ms
step:664/1845 train_time:24475ms step_avg:36.86ms
step:665/1845 train_time:24537ms step_avg:36.90ms
step:666/1845 train_time:24598ms step_avg:36.93ms
step:667/1845 train_time:24661ms step_avg:36.97ms
step:668/1845 train_time:24721ms step_avg:37.01ms
step:669/1845 train_time:24784ms step_avg:37.05ms
step:670/1845 train_time:24846ms step_avg:37.08ms
step:671/1845 train_time:24908ms step_avg:37.12ms
step:672/1845 train_time:24969ms step_avg:37.16ms
step:673/1845 train_time:25031ms step_avg:37.19ms
step:674/1845 train_time:25092ms step_avg:37.23ms
step:675/1845 train_time:25155ms step_avg:37.27ms
step:676/1845 train_time:25215ms step_avg:37.30ms
step:677/1845 train_time:25278ms step_avg:37.34ms
step:678/1845 train_time:25338ms step_avg:37.37ms
step:679/1845 train_time:25400ms step_avg:37.41ms
step:680/1845 train_time:25461ms step_avg:37.44ms
step:681/1845 train_time:25523ms step_avg:37.48ms
step:682/1845 train_time:25584ms step_avg:37.51ms
step:683/1845 train_time:25647ms step_avg:37.55ms
step:684/1845 train_time:25708ms step_avg:37.58ms
step:685/1845 train_time:25770ms step_avg:37.62ms
step:686/1845 train_time:25831ms step_avg:37.66ms
step:687/1845 train_time:25894ms step_avg:37.69ms
step:688/1845 train_time:25955ms step_avg:37.72ms
step:689/1845 train_time:26017ms step_avg:37.76ms
step:690/1845 train_time:26077ms step_avg:37.79ms
step:691/1845 train_time:26139ms step_avg:37.83ms
step:692/1845 train_time:26200ms step_avg:37.86ms
step:693/1845 train_time:26262ms step_avg:37.90ms
step:694/1845 train_time:26323ms step_avg:37.93ms
step:695/1845 train_time:26385ms step_avg:37.96ms
step:696/1845 train_time:26446ms step_avg:38.00ms
step:697/1845 train_time:26509ms step_avg:38.03ms
step:698/1845 train_time:26570ms step_avg:38.07ms
step:699/1845 train_time:26633ms step_avg:38.10ms
step:700/1845 train_time:26693ms step_avg:38.13ms
step:701/1845 train_time:26756ms step_avg:38.17ms
step:702/1845 train_time:26817ms step_avg:38.20ms
step:703/1845 train_time:26879ms step_avg:38.23ms
step:704/1845 train_time:26940ms step_avg:38.27ms
step:705/1845 train_time:27002ms step_avg:38.30ms
step:706/1845 train_time:27063ms step_avg:38.33ms
step:707/1845 train_time:27125ms step_avg:38.37ms
step:708/1845 train_time:27186ms step_avg:38.40ms
step:709/1845 train_time:27249ms step_avg:38.43ms
step:710/1845 train_time:27310ms step_avg:38.46ms
step:711/1845 train_time:27373ms step_avg:38.50ms
step:712/1845 train_time:27434ms step_avg:38.53ms
step:713/1845 train_time:27496ms step_avg:38.56ms
step:714/1845 train_time:27556ms step_avg:38.59ms
step:715/1845 train_time:27619ms step_avg:38.63ms
step:716/1845 train_time:27679ms step_avg:38.66ms
step:717/1845 train_time:27741ms step_avg:38.69ms
step:718/1845 train_time:27802ms step_avg:38.72ms
step:719/1845 train_time:27864ms step_avg:38.75ms
step:720/1845 train_time:27925ms step_avg:38.78ms
step:721/1845 train_time:27988ms step_avg:38.82ms
step:722/1845 train_time:28050ms step_avg:38.85ms
step:723/1845 train_time:28113ms step_avg:38.88ms
step:724/1845 train_time:28174ms step_avg:38.91ms
step:725/1845 train_time:28236ms step_avg:38.95ms
step:726/1845 train_time:28297ms step_avg:38.98ms
step:727/1845 train_time:28360ms step_avg:39.01ms
step:728/1845 train_time:28421ms step_avg:39.04ms
step:729/1845 train_time:28483ms step_avg:39.07ms
step:730/1845 train_time:28544ms step_avg:39.10ms
step:731/1845 train_time:28607ms step_avg:39.13ms
step:732/1845 train_time:28668ms step_avg:39.16ms
step:733/1845 train_time:28730ms step_avg:39.20ms
step:734/1845 train_time:28791ms step_avg:39.23ms
step:735/1845 train_time:28854ms step_avg:39.26ms
step:736/1845 train_time:28915ms step_avg:39.29ms
step:737/1845 train_time:28977ms step_avg:39.32ms
step:738/1845 train_time:29038ms step_avg:39.35ms
step:739/1845 train_time:29100ms step_avg:39.38ms
step:740/1845 train_time:29161ms step_avg:39.41ms
step:741/1845 train_time:29223ms step_avg:39.44ms
step:742/1845 train_time:29284ms step_avg:39.47ms
step:743/1845 train_time:29347ms step_avg:39.50ms
step:744/1845 train_time:29408ms step_avg:39.53ms
step:745/1845 train_time:29470ms step_avg:39.56ms
step:746/1845 train_time:29531ms step_avg:39.59ms
step:747/1845 train_time:29594ms step_avg:39.62ms
step:748/1845 train_time:29654ms step_avg:39.64ms
step:749/1845 train_time:29717ms step_avg:39.68ms
step:750/1845 train_time:29777ms step_avg:39.70ms
step:750/1845 val_loss:4.0301 train_time:29840ms step_avg:39.79ms
step:751/1845 train_time:29860ms step_avg:39.76ms
step:752/1845 train_time:29902ms step_avg:39.76ms
step:753/1845 train_time:29968ms step_avg:39.80ms
step:754/1845 train_time:30029ms step_avg:39.83ms
step:755/1845 train_time:30092ms step_avg:39.86ms
step:756/1845 train_time:30153ms step_avg:39.88ms
step:757/1845 train_time:30214ms step_avg:39.91ms
step:758/1845 train_time:30275ms step_avg:39.94ms
step:759/1845 train_time:30337ms step_avg:39.97ms
step:760/1845 train_time:30397ms step_avg:40.00ms
step:761/1845 train_time:30459ms step_avg:40.03ms
step:762/1845 train_time:30520ms step_avg:40.05ms
step:763/1845 train_time:30582ms step_avg:40.08ms
step:764/1845 train_time:30642ms step_avg:40.11ms
step:765/1845 train_time:30704ms step_avg:40.14ms
step:766/1845 train_time:30766ms step_avg:40.16ms
step:767/1845 train_time:30829ms step_avg:40.19ms
step:768/1845 train_time:30891ms step_avg:40.22ms
step:769/1845 train_time:30955ms step_avg:40.25ms
step:770/1845 train_time:31017ms step_avg:40.28ms
step:771/1845 train_time:31080ms step_avg:40.31ms
step:772/1845 train_time:31141ms step_avg:40.34ms
step:773/1845 train_time:31203ms step_avg:40.37ms
step:774/1845 train_time:31263ms step_avg:40.39ms
step:775/1845 train_time:31325ms step_avg:40.42ms
step:776/1845 train_time:31386ms step_avg:40.45ms
step:777/1845 train_time:31447ms step_avg:40.47ms
step:778/1845 train_time:31508ms step_avg:40.50ms
step:779/1845 train_time:31570ms step_avg:40.53ms
step:780/1845 train_time:31631ms step_avg:40.55ms
step:781/1845 train_time:31694ms step_avg:40.58ms
step:782/1845 train_time:31755ms step_avg:40.61ms
step:783/1845 train_time:31818ms step_avg:40.64ms
step:784/1845 train_time:31879ms step_avg:40.66ms
step:785/1845 train_time:31942ms step_avg:40.69ms
step:786/1845 train_time:32003ms step_avg:40.72ms
step:787/1845 train_time:32066ms step_avg:40.74ms
step:788/1845 train_time:32126ms step_avg:40.77ms
step:789/1845 train_time:32188ms step_avg:40.80ms
step:790/1845 train_time:32249ms step_avg:40.82ms
step:791/1845 train_time:32312ms step_avg:40.85ms
step:792/1845 train_time:32373ms step_avg:40.87ms
step:793/1845 train_time:32435ms step_avg:40.90ms
step:794/1845 train_time:32495ms step_avg:40.93ms
step:795/1845 train_time:32558ms step_avg:40.95ms
step:796/1845 train_time:32618ms step_avg:40.98ms
step:797/1845 train_time:32681ms step_avg:41.00ms
step:798/1845 train_time:32741ms step_avg:41.03ms
step:799/1845 train_time:32804ms step_avg:41.06ms
step:800/1845 train_time:32865ms step_avg:41.08ms
step:801/1845 train_time:32928ms step_avg:41.11ms
step:802/1845 train_time:32989ms step_avg:41.13ms
step:803/1845 train_time:33052ms step_avg:41.16ms
step:804/1845 train_time:33113ms step_avg:41.18ms
step:805/1845 train_time:33176ms step_avg:41.21ms
step:806/1845 train_time:33237ms step_avg:41.24ms
step:807/1845 train_time:33300ms step_avg:41.26ms
step:808/1845 train_time:33361ms step_avg:41.29ms
step:809/1845 train_time:33423ms step_avg:41.31ms
step:810/1845 train_time:33483ms step_avg:41.34ms
step:811/1845 train_time:33545ms step_avg:41.36ms
step:812/1845 train_time:33605ms step_avg:41.39ms
step:813/1845 train_time:33668ms step_avg:41.41ms
step:814/1845 train_time:33729ms step_avg:41.44ms
step:815/1845 train_time:33791ms step_avg:41.46ms
step:816/1845 train_time:33853ms step_avg:41.49ms
step:817/1845 train_time:33915ms step_avg:41.51ms
step:818/1845 train_time:33976ms step_avg:41.54ms
step:819/1845 train_time:34039ms step_avg:41.56ms
step:820/1845 train_time:34100ms step_avg:41.59ms
step:821/1845 train_time:34163ms step_avg:41.61ms
step:822/1845 train_time:34224ms step_avg:41.63ms
step:823/1845 train_time:34287ms step_avg:41.66ms
step:824/1845 train_time:34347ms step_avg:41.68ms
step:825/1845 train_time:34410ms step_avg:41.71ms
step:826/1845 train_time:34470ms step_avg:41.73ms
step:827/1845 train_time:34533ms step_avg:41.76ms
step:828/1845 train_time:34594ms step_avg:41.78ms
step:829/1845 train_time:34657ms step_avg:41.81ms
step:830/1845 train_time:34718ms step_avg:41.83ms
step:831/1845 train_time:34781ms step_avg:41.85ms
step:832/1845 train_time:34841ms step_avg:41.88ms
step:833/1845 train_time:34904ms step_avg:41.90ms
step:834/1845 train_time:34964ms step_avg:41.92ms
step:835/1845 train_time:35026ms step_avg:41.95ms
step:836/1845 train_time:35087ms step_avg:41.97ms
step:837/1845 train_time:35150ms step_avg:41.99ms
step:838/1845 train_time:35211ms step_avg:42.02ms
step:839/1845 train_time:35273ms step_avg:42.04ms
step:840/1845 train_time:35334ms step_avg:42.06ms
step:841/1845 train_time:35396ms step_avg:42.09ms
step:842/1845 train_time:35458ms step_avg:42.11ms
step:843/1845 train_time:35520ms step_avg:42.14ms
step:844/1845 train_time:35581ms step_avg:42.16ms
step:845/1845 train_time:35643ms step_avg:42.18ms
step:846/1845 train_time:35704ms step_avg:42.20ms
step:847/1845 train_time:35766ms step_avg:42.23ms
step:848/1845 train_time:35827ms step_avg:42.25ms
step:849/1845 train_time:35890ms step_avg:42.27ms
step:850/1845 train_time:35950ms step_avg:42.29ms
step:851/1845 train_time:36012ms step_avg:42.32ms
step:852/1845 train_time:36074ms step_avg:42.34ms
step:853/1845 train_time:36136ms step_avg:42.36ms
step:854/1845 train_time:36197ms step_avg:42.39ms
step:855/1845 train_time:36260ms step_avg:42.41ms
step:856/1845 train_time:36320ms step_avg:42.43ms
step:857/1845 train_time:36384ms step_avg:42.45ms
step:858/1845 train_time:36443ms step_avg:42.47ms
step:859/1845 train_time:36505ms step_avg:42.50ms
step:860/1845 train_time:36566ms step_avg:42.52ms
step:861/1845 train_time:36629ms step_avg:42.54ms
step:862/1845 train_time:36690ms step_avg:42.56ms
step:863/1845 train_time:36751ms step_avg:42.59ms
step:864/1845 train_time:36812ms step_avg:42.61ms
step:865/1845 train_time:36875ms step_avg:42.63ms
step:866/1845 train_time:36935ms step_avg:42.65ms
step:867/1845 train_time:36998ms step_avg:42.67ms
step:868/1845 train_time:37059ms step_avg:42.69ms
step:869/1845 train_time:37121ms step_avg:42.72ms
step:870/1845 train_time:37182ms step_avg:42.74ms
step:871/1845 train_time:37244ms step_avg:42.76ms
step:872/1845 train_time:37305ms step_avg:42.78ms
step:873/1845 train_time:37368ms step_avg:42.80ms
step:874/1845 train_time:37428ms step_avg:42.82ms
step:875/1845 train_time:37491ms step_avg:42.85ms
step:876/1845 train_time:37552ms step_avg:42.87ms
step:877/1845 train_time:37614ms step_avg:42.89ms
step:878/1845 train_time:37675ms step_avg:42.91ms
step:879/1845 train_time:37738ms step_avg:42.93ms
step:880/1845 train_time:37798ms step_avg:42.95ms
step:881/1845 train_time:37861ms step_avg:42.98ms
step:882/1845 train_time:37922ms step_avg:43.00ms
step:883/1845 train_time:37984ms step_avg:43.02ms
step:884/1845 train_time:38044ms step_avg:43.04ms
step:885/1845 train_time:38106ms step_avg:43.06ms
step:886/1845 train_time:38167ms step_avg:43.08ms
step:887/1845 train_time:38229ms step_avg:43.10ms
step:888/1845 train_time:38290ms step_avg:43.12ms
step:889/1845 train_time:38352ms step_avg:43.14ms
step:890/1845 train_time:38413ms step_avg:43.16ms
step:891/1845 train_time:38475ms step_avg:43.18ms
step:892/1845 train_time:38537ms step_avg:43.20ms
step:893/1845 train_time:38599ms step_avg:43.22ms
step:894/1845 train_time:38660ms step_avg:43.24ms
step:895/1845 train_time:38722ms step_avg:43.27ms
step:896/1845 train_time:38783ms step_avg:43.28ms
step:897/1845 train_time:38845ms step_avg:43.31ms
step:898/1845 train_time:38906ms step_avg:43.32ms
step:899/1845 train_time:38968ms step_avg:43.35ms
step:900/1845 train_time:39029ms step_avg:43.37ms
step:901/1845 train_time:39091ms step_avg:43.39ms
step:902/1845 train_time:39152ms step_avg:43.41ms
step:903/1845 train_time:39215ms step_avg:43.43ms
step:904/1845 train_time:39275ms step_avg:43.45ms
step:905/1845 train_time:39338ms step_avg:43.47ms
step:906/1845 train_time:39399ms step_avg:43.49ms
step:907/1845 train_time:39462ms step_avg:43.51ms
step:908/1845 train_time:39523ms step_avg:43.53ms
step:909/1845 train_time:39585ms step_avg:43.55ms
step:910/1845 train_time:39645ms step_avg:43.57ms
step:911/1845 train_time:39708ms step_avg:43.59ms
step:912/1845 train_time:39770ms step_avg:43.61ms
step:913/1845 train_time:39832ms step_avg:43.63ms
step:914/1845 train_time:39893ms step_avg:43.65ms
step:915/1845 train_time:39956ms step_avg:43.67ms
step:916/1845 train_time:40017ms step_avg:43.69ms
step:917/1845 train_time:40079ms step_avg:43.71ms
step:918/1845 train_time:40140ms step_avg:43.73ms
step:919/1845 train_time:40202ms step_avg:43.75ms
step:920/1845 train_time:40263ms step_avg:43.76ms
step:921/1845 train_time:40325ms step_avg:43.78ms
step:922/1845 train_time:40386ms step_avg:43.80ms
step:923/1845 train_time:40449ms step_avg:43.82ms
step:924/1845 train_time:40509ms step_avg:43.84ms
step:925/1845 train_time:40572ms step_avg:43.86ms
step:926/1845 train_time:40633ms step_avg:43.88ms
step:927/1845 train_time:40696ms step_avg:43.90ms
step:928/1845 train_time:40757ms step_avg:43.92ms
step:929/1845 train_time:40820ms step_avg:43.94ms
step:930/1845 train_time:40881ms step_avg:43.96ms
step:931/1845 train_time:40944ms step_avg:43.98ms
step:932/1845 train_time:41004ms step_avg:44.00ms
step:933/1845 train_time:41066ms step_avg:44.02ms
step:934/1845 train_time:41127ms step_avg:44.03ms
step:935/1845 train_time:41190ms step_avg:44.05ms
step:936/1845 train_time:41251ms step_avg:44.07ms
step:937/1845 train_time:41313ms step_avg:44.09ms
step:938/1845 train_time:41374ms step_avg:44.11ms
step:939/1845 train_time:41437ms step_avg:44.13ms
step:940/1845 train_time:41498ms step_avg:44.15ms
step:941/1845 train_time:41561ms step_avg:44.17ms
step:942/1845 train_time:41621ms step_avg:44.18ms
step:943/1845 train_time:41684ms step_avg:44.20ms
step:944/1845 train_time:41744ms step_avg:44.22ms
step:945/1845 train_time:41806ms step_avg:44.24ms
step:946/1845 train_time:41868ms step_avg:44.26ms
step:947/1845 train_time:41930ms step_avg:44.28ms
step:948/1845 train_time:41991ms step_avg:44.29ms
step:949/1845 train_time:42053ms step_avg:44.31ms
step:950/1845 train_time:42114ms step_avg:44.33ms
step:951/1845 train_time:42177ms step_avg:44.35ms
step:952/1845 train_time:42238ms step_avg:44.37ms
step:953/1845 train_time:42300ms step_avg:44.39ms
step:954/1845 train_time:42361ms step_avg:44.40ms
step:955/1845 train_time:42423ms step_avg:44.42ms
step:956/1845 train_time:42484ms step_avg:44.44ms
step:957/1845 train_time:42546ms step_avg:44.46ms
step:958/1845 train_time:42607ms step_avg:44.47ms
step:959/1845 train_time:42670ms step_avg:44.49ms
step:960/1845 train_time:42731ms step_avg:44.51ms
step:961/1845 train_time:42793ms step_avg:44.53ms
step:962/1845 train_time:42854ms step_avg:44.55ms
step:963/1845 train_time:42916ms step_avg:44.57ms
step:964/1845 train_time:42978ms step_avg:44.58ms
step:965/1845 train_time:43040ms step_avg:44.60ms
step:966/1845 train_time:43101ms step_avg:44.62ms
step:967/1845 train_time:43164ms step_avg:44.64ms
step:968/1845 train_time:43224ms step_avg:44.65ms
step:969/1845 train_time:43287ms step_avg:44.67ms
step:970/1845 train_time:43348ms step_avg:44.69ms
step:971/1845 train_time:43411ms step_avg:44.71ms
step:972/1845 train_time:43471ms step_avg:44.72ms
step:973/1845 train_time:43533ms step_avg:44.74ms
step:974/1845 train_time:43594ms step_avg:44.76ms
step:975/1845 train_time:43657ms step_avg:44.78ms
step:976/1845 train_time:43719ms step_avg:44.79ms
step:977/1845 train_time:43781ms step_avg:44.81ms
step:978/1845 train_time:43842ms step_avg:44.83ms
step:979/1845 train_time:43904ms step_avg:44.85ms
step:980/1845 train_time:43964ms step_avg:44.86ms
step:981/1845 train_time:44026ms step_avg:44.88ms
step:982/1845 train_time:44087ms step_avg:44.90ms
step:983/1845 train_time:44149ms step_avg:44.91ms
step:984/1845 train_time:44211ms step_avg:44.93ms
step:985/1845 train_time:44273ms step_avg:44.95ms
step:986/1845 train_time:44334ms step_avg:44.96ms
step:987/1845 train_time:44396ms step_avg:44.98ms
step:988/1845 train_time:44457ms step_avg:45.00ms
step:989/1845 train_time:44519ms step_avg:45.01ms
step:990/1845 train_time:44581ms step_avg:45.03ms
step:991/1845 train_time:44643ms step_avg:45.05ms
step:992/1845 train_time:44704ms step_avg:45.06ms
step:993/1845 train_time:44766ms step_avg:45.08ms
step:994/1845 train_time:44827ms step_avg:45.10ms
step:995/1845 train_time:44890ms step_avg:45.12ms
step:996/1845 train_time:44950ms step_avg:45.13ms
step:997/1845 train_time:45013ms step_avg:45.15ms
step:998/1845 train_time:45073ms step_avg:45.16ms
step:999/1845 train_time:45136ms step_avg:45.18ms
step:1000/1845 train_time:45197ms step_avg:45.20ms
step:1000/1845 val_loss:3.7747 train_time:45261ms step_avg:45.26ms
step:1001/1845 train_time:45281ms step_avg:45.24ms
step:1002/1845 train_time:45324ms step_avg:45.23ms
step:1003/1845 train_time:45388ms step_avg:45.25ms
step:1004/1845 train_time:45450ms step_avg:45.27ms
step:1005/1845 train_time:45512ms step_avg:45.29ms
step:1006/1845 train_time:45572ms step_avg:45.30ms
step:1007/1845 train_time:45634ms step_avg:45.32ms
step:1008/1845 train_time:45694ms step_avg:45.33ms
step:1009/1845 train_time:45756ms step_avg:45.35ms
step:1010/1845 train_time:45816ms step_avg:45.36ms
step:1011/1845 train_time:45878ms step_avg:45.38ms
step:1012/1845 train_time:45938ms step_avg:45.39ms
step:1013/1845 train_time:46000ms step_avg:45.41ms
step:1014/1845 train_time:46061ms step_avg:45.42ms
step:1015/1845 train_time:46122ms step_avg:45.44ms
step:1016/1845 train_time:46183ms step_avg:45.46ms
step:1017/1845 train_time:46246ms step_avg:45.47ms
step:1018/1845 train_time:46308ms step_avg:45.49ms
step:1019/1845 train_time:46371ms step_avg:45.51ms
step:1020/1845 train_time:46433ms step_avg:45.52ms
step:1021/1845 train_time:46495ms step_avg:45.54ms
step:1022/1845 train_time:46556ms step_avg:45.55ms
step:1023/1845 train_time:46617ms step_avg:45.57ms
step:1024/1845 train_time:46678ms step_avg:45.58ms
step:1025/1845 train_time:46740ms step_avg:45.60ms
step:1026/1845 train_time:46800ms step_avg:45.61ms
step:1027/1845 train_time:46862ms step_avg:45.63ms
step:1028/1845 train_time:46923ms step_avg:45.64ms
step:1029/1845 train_time:46985ms step_avg:45.66ms
step:1030/1845 train_time:47045ms step_avg:45.67ms
step:1031/1845 train_time:47107ms step_avg:45.69ms
step:1032/1845 train_time:47168ms step_avg:45.71ms
step:1033/1845 train_time:47231ms step_avg:45.72ms
step:1034/1845 train_time:47292ms step_avg:45.74ms
step:1035/1845 train_time:47354ms step_avg:45.75ms
step:1036/1845 train_time:47415ms step_avg:45.77ms
step:1037/1845 train_time:47479ms step_avg:45.78ms
step:1038/1845 train_time:47540ms step_avg:45.80ms
step:1039/1845 train_time:47602ms step_avg:45.82ms
step:1040/1845 train_time:47663ms step_avg:45.83ms
step:1041/1845 train_time:47726ms step_avg:45.85ms
step:1042/1845 train_time:47786ms step_avg:45.86ms
step:1043/1845 train_time:47849ms step_avg:45.88ms
step:1044/1845 train_time:47910ms step_avg:45.89ms
step:1045/1845 train_time:47972ms step_avg:45.91ms
step:1046/1845 train_time:48032ms step_avg:45.92ms
step:1047/1845 train_time:48094ms step_avg:45.94ms
step:1048/1845 train_time:48155ms step_avg:45.95ms
step:1049/1845 train_time:48217ms step_avg:45.96ms
step:1050/1845 train_time:48278ms step_avg:45.98ms
step:1051/1845 train_time:48341ms step_avg:46.00ms
step:1052/1845 train_time:48402ms step_avg:46.01ms
step:1053/1845 train_time:48466ms step_avg:46.03ms
step:1054/1845 train_time:48526ms step_avg:46.04ms
step:1055/1845 train_time:48589ms step_avg:46.06ms
step:1056/1845 train_time:48650ms step_avg:46.07ms
step:1057/1845 train_time:48712ms step_avg:46.08ms
step:1058/1845 train_time:48772ms step_avg:46.10ms
step:1059/1845 train_time:48834ms step_avg:46.11ms
step:1060/1845 train_time:48895ms step_avg:46.13ms
step:1061/1845 train_time:48957ms step_avg:46.14ms
step:1062/1845 train_time:49018ms step_avg:46.16ms
step:1063/1845 train_time:49080ms step_avg:46.17ms
step:1064/1845 train_time:49141ms step_avg:46.19ms
step:1065/1845 train_time:49204ms step_avg:46.20ms
step:1066/1845 train_time:49264ms step_avg:46.21ms
step:1067/1845 train_time:49326ms step_avg:46.23ms
step:1068/1845 train_time:49387ms step_avg:46.24ms
step:1069/1845 train_time:49450ms step_avg:46.26ms
step:1070/1845 train_time:49511ms step_avg:46.27ms
step:1071/1845 train_time:49574ms step_avg:46.29ms
step:1072/1845 train_time:49635ms step_avg:46.30ms
step:1073/1845 train_time:49697ms step_avg:46.32ms
step:1074/1845 train_time:49758ms step_avg:46.33ms
step:1075/1845 train_time:49820ms step_avg:46.34ms
step:1076/1845 train_time:49881ms step_avg:46.36ms
step:1077/1845 train_time:49942ms step_avg:46.37ms
step:1078/1845 train_time:50003ms step_avg:46.39ms
step:1079/1845 train_time:50065ms step_avg:46.40ms
step:1080/1845 train_time:50126ms step_avg:46.41ms
step:1081/1845 train_time:50188ms step_avg:46.43ms
step:1082/1845 train_time:50249ms step_avg:46.44ms
step:1083/1845 train_time:50312ms step_avg:46.46ms
step:1084/1845 train_time:50372ms step_avg:46.47ms
step:1085/1845 train_time:50434ms step_avg:46.48ms
step:1086/1845 train_time:50495ms step_avg:46.50ms
step:1087/1845 train_time:50557ms step_avg:46.51ms
step:1088/1845 train_time:50618ms step_avg:46.52ms
step:1089/1845 train_time:50680ms step_avg:46.54ms
step:1090/1845 train_time:50741ms step_avg:46.55ms
step:1091/1845 train_time:50804ms step_avg:46.57ms
step:1092/1845 train_time:50866ms step_avg:46.58ms
step:1093/1845 train_time:50928ms step_avg:46.59ms
step:1094/1845 train_time:50989ms step_avg:46.61ms
step:1095/1845 train_time:51051ms step_avg:46.62ms
step:1096/1845 train_time:51111ms step_avg:46.63ms
step:1097/1845 train_time:51173ms step_avg:46.65ms
step:1098/1845 train_time:51233ms step_avg:46.66ms
step:1099/1845 train_time:51296ms step_avg:46.68ms
step:1100/1845 train_time:51357ms step_avg:46.69ms
step:1101/1845 train_time:51419ms step_avg:46.70ms
step:1102/1845 train_time:51480ms step_avg:46.72ms
step:1103/1845 train_time:51543ms step_avg:46.73ms
step:1104/1845 train_time:51604ms step_avg:46.74ms
step:1105/1845 train_time:51666ms step_avg:46.76ms
step:1106/1845 train_time:51727ms step_avg:46.77ms
step:1107/1845 train_time:51790ms step_avg:46.78ms
step:1108/1845 train_time:51851ms step_avg:46.80ms
step:1109/1845 train_time:51913ms step_avg:46.81ms
step:1110/1845 train_time:51973ms step_avg:46.82ms
step:1111/1845 train_time:52035ms step_avg:46.84ms
step:1112/1845 train_time:52096ms step_avg:46.85ms
step:1113/1845 train_time:52158ms step_avg:46.86ms
step:1114/1845 train_time:52219ms step_avg:46.88ms
step:1115/1845 train_time:52282ms step_avg:46.89ms
step:1116/1845 train_time:52342ms step_avg:46.90ms
step:1117/1845 train_time:52404ms step_avg:46.92ms
step:1118/1845 train_time:52466ms step_avg:46.93ms
step:1119/1845 train_time:52528ms step_avg:46.94ms
step:1120/1845 train_time:52589ms step_avg:46.95ms
step:1121/1845 train_time:52651ms step_avg:46.97ms
step:1122/1845 train_time:52711ms step_avg:46.98ms
step:1123/1845 train_time:52774ms step_avg:46.99ms
step:1124/1845 train_time:52834ms step_avg:47.01ms
step:1125/1845 train_time:52897ms step_avg:47.02ms
step:1126/1845 train_time:52957ms step_avg:47.03ms
step:1127/1845 train_time:53021ms step_avg:47.05ms
step:1128/1845 train_time:53081ms step_avg:47.06ms
step:1129/1845 train_time:53143ms step_avg:47.07ms
step:1130/1845 train_time:53203ms step_avg:47.08ms
step:1131/1845 train_time:53266ms step_avg:47.10ms
step:1132/1845 train_time:53327ms step_avg:47.11ms
step:1133/1845 train_time:53389ms step_avg:47.12ms
step:1134/1845 train_time:53450ms step_avg:47.13ms
step:1135/1845 train_time:53513ms step_avg:47.15ms
step:1136/1845 train_time:53574ms step_avg:47.16ms
step:1137/1845 train_time:53636ms step_avg:47.17ms
step:1138/1845 train_time:53697ms step_avg:47.19ms
step:1139/1845 train_time:53759ms step_avg:47.20ms
step:1140/1845 train_time:53820ms step_avg:47.21ms
step:1141/1845 train_time:53883ms step_avg:47.22ms
step:1142/1845 train_time:53944ms step_avg:47.24ms
step:1143/1845 train_time:54007ms step_avg:47.25ms
step:1144/1845 train_time:54068ms step_avg:47.26ms
step:1145/1845 train_time:54130ms step_avg:47.28ms
step:1146/1845 train_time:54190ms step_avg:47.29ms
step:1147/1845 train_time:54252ms step_avg:47.30ms
step:1148/1845 train_time:54313ms step_avg:47.31ms
step:1149/1845 train_time:54376ms step_avg:47.32ms
step:1150/1845 train_time:54437ms step_avg:47.34ms
step:1151/1845 train_time:54500ms step_avg:47.35ms
step:1152/1845 train_time:54561ms step_avg:47.36ms
step:1153/1845 train_time:54624ms step_avg:47.38ms
step:1154/1845 train_time:54685ms step_avg:47.39ms
step:1155/1845 train_time:54748ms step_avg:47.40ms
step:1156/1845 train_time:54809ms step_avg:47.41ms
step:1157/1845 train_time:54872ms step_avg:47.43ms
step:1158/1845 train_time:54933ms step_avg:47.44ms
step:1159/1845 train_time:54995ms step_avg:47.45ms
step:1160/1845 train_time:55056ms step_avg:47.46ms
step:1161/1845 train_time:55119ms step_avg:47.48ms
step:1162/1845 train_time:55180ms step_avg:47.49ms
step:1163/1845 train_time:55242ms step_avg:47.50ms
step:1164/1845 train_time:55303ms step_avg:47.51ms
step:1165/1845 train_time:55365ms step_avg:47.52ms
step:1166/1845 train_time:55427ms step_avg:47.54ms
step:1167/1845 train_time:55489ms step_avg:47.55ms
step:1168/1845 train_time:55550ms step_avg:47.56ms
step:1169/1845 train_time:55613ms step_avg:47.57ms
step:1170/1845 train_time:55673ms step_avg:47.58ms
step:1171/1845 train_time:55735ms step_avg:47.60ms
step:1172/1845 train_time:55795ms step_avg:47.61ms
step:1173/1845 train_time:55857ms step_avg:47.62ms
step:1174/1845 train_time:55918ms step_avg:47.63ms
step:1175/1845 train_time:55980ms step_avg:47.64ms
step:1176/1845 train_time:56041ms step_avg:47.65ms
step:1177/1845 train_time:56104ms step_avg:47.67ms
step:1178/1845 train_time:56165ms step_avg:47.68ms
step:1179/1845 train_time:56226ms step_avg:47.69ms
step:1180/1845 train_time:56287ms step_avg:47.70ms
step:1181/1845 train_time:56351ms step_avg:47.71ms
step:1182/1845 train_time:56411ms step_avg:47.72ms
step:1183/1845 train_time:56473ms step_avg:47.74ms
step:1184/1845 train_time:56533ms step_avg:47.75ms
step:1185/1845 train_time:56596ms step_avg:47.76ms
step:1186/1845 train_time:56657ms step_avg:47.77ms
step:1187/1845 train_time:56720ms step_avg:47.78ms
step:1188/1845 train_time:56781ms step_avg:47.80ms
step:1189/1845 train_time:56842ms step_avg:47.81ms
step:1190/1845 train_time:56903ms step_avg:47.82ms
step:1191/1845 train_time:56965ms step_avg:47.83ms
step:1192/1845 train_time:57026ms step_avg:47.84ms
step:1193/1845 train_time:57089ms step_avg:47.85ms
step:1194/1845 train_time:57150ms step_avg:47.86ms
step:1195/1845 train_time:57212ms step_avg:47.88ms
step:1196/1845 train_time:57273ms step_avg:47.89ms
step:1197/1845 train_time:57335ms step_avg:47.90ms
step:1198/1845 train_time:57395ms step_avg:47.91ms
step:1199/1845 train_time:57458ms step_avg:47.92ms
step:1200/1845 train_time:57518ms step_avg:47.93ms
step:1201/1845 train_time:57580ms step_avg:47.94ms
step:1202/1845 train_time:57641ms step_avg:47.95ms
step:1203/1845 train_time:57704ms step_avg:47.97ms
step:1204/1845 train_time:57764ms step_avg:47.98ms
step:1205/1845 train_time:57827ms step_avg:47.99ms
step:1206/1845 train_time:57914ms step_avg:48.02ms
step:1207/1845 train_time:58004ms step_avg:48.06ms
step:1208/1845 train_time:58091ms step_avg:48.09ms
step:1209/1845 train_time:58181ms step_avg:48.12ms
step:1210/1845 train_time:58269ms step_avg:48.16ms
step:1211/1845 train_time:58358ms step_avg:48.19ms
step:1212/1845 train_time:58444ms step_avg:48.22ms
step:1213/1845 train_time:58533ms step_avg:48.25ms
step:1214/1845 train_time:58620ms step_avg:48.29ms
step:1215/1845 train_time:58708ms step_avg:48.32ms
step:1216/1845 train_time:58796ms step_avg:48.35ms
step:1217/1845 train_time:58884ms step_avg:48.38ms
step:1218/1845 train_time:58971ms step_avg:48.42ms
step:1219/1845 train_time:59060ms step_avg:48.45ms
step:1220/1845 train_time:59147ms step_avg:48.48ms
step:1221/1845 train_time:59236ms step_avg:48.51ms
step:1222/1845 train_time:59323ms step_avg:48.55ms
step:1223/1845 train_time:59412ms step_avg:48.58ms
step:1224/1845 train_time:59501ms step_avg:48.61ms
step:1225/1845 train_time:59589ms step_avg:48.64ms
step:1226/1845 train_time:59676ms step_avg:48.68ms
step:1227/1845 train_time:59764ms step_avg:48.71ms
step:1228/1845 train_time:59851ms step_avg:48.74ms
step:1229/1845 train_time:59939ms step_avg:48.77ms
step:1230/1845 train_time:60027ms step_avg:48.80ms
step:1231/1845 train_time:60115ms step_avg:48.83ms
step:1232/1845 train_time:60203ms step_avg:48.87ms
step:1233/1845 train_time:60291ms step_avg:48.90ms
step:1234/1845 train_time:60380ms step_avg:48.93ms
step:1235/1845 train_time:60468ms step_avg:48.96ms
step:1236/1845 train_time:60555ms step_avg:48.99ms
step:1237/1845 train_time:60643ms step_avg:49.02ms
step:1238/1845 train_time:60731ms step_avg:49.06ms
step:1239/1845 train_time:60818ms step_avg:49.09ms
step:1240/1845 train_time:60905ms step_avg:49.12ms
step:1241/1845 train_time:60993ms step_avg:49.15ms
step:1242/1845 train_time:61082ms step_avg:49.18ms
step:1243/1845 train_time:61170ms step_avg:49.21ms
step:1244/1845 train_time:61258ms step_avg:49.24ms
step:1245/1845 train_time:61346ms step_avg:49.27ms
step:1246/1845 train_time:61433ms step_avg:49.30ms
step:1247/1845 train_time:61522ms step_avg:49.34ms
step:1248/1845 train_time:61608ms step_avg:49.37ms
step:1249/1845 train_time:61697ms step_avg:49.40ms
step:1250/1845 train_time:61784ms step_avg:49.43ms
step:1250/1845 val_loss:3.5363 train_time:61874ms step_avg:49.50ms
step:1251/1845 train_time:61894ms step_avg:49.48ms
step:1252/1845 train_time:61961ms step_avg:49.49ms
step:1253/1845 train_time:62056ms step_avg:49.53ms
step:1254/1845 train_time:62144ms step_avg:49.56ms
step:1255/1845 train_time:62233ms step_avg:49.59ms
step:1256/1845 train_time:62319ms step_avg:49.62ms
step:1257/1845 train_time:62406ms step_avg:49.65ms
step:1258/1845 train_time:62492ms step_avg:49.68ms
step:1259/1845 train_time:62580ms step_avg:49.71ms
step:1260/1845 train_time:62666ms step_avg:49.73ms
step:1261/1845 train_time:62754ms step_avg:49.77ms
step:1262/1845 train_time:62843ms step_avg:49.80ms
step:1263/1845 train_time:62933ms step_avg:49.83ms
step:1264/1845 train_time:63024ms step_avg:49.86ms
step:1265/1845 train_time:63114ms step_avg:49.89ms
step:1266/1845 train_time:63202ms step_avg:49.92ms
step:1267/1845 train_time:63290ms step_avg:49.95ms
step:1268/1845 train_time:63376ms step_avg:49.98ms
step:1269/1845 train_time:63464ms step_avg:50.01ms
step:1270/1845 train_time:63550ms step_avg:50.04ms
step:1271/1845 train_time:63637ms step_avg:50.07ms
step:1272/1845 train_time:63724ms step_avg:50.10ms
step:1273/1845 train_time:63812ms step_avg:50.13ms
step:1274/1845 train_time:63900ms step_avg:50.16ms
step:1275/1845 train_time:63989ms step_avg:50.19ms
step:1276/1845 train_time:64078ms step_avg:50.22ms
step:1277/1845 train_time:64168ms step_avg:50.25ms
step:1278/1845 train_time:64256ms step_avg:50.28ms
step:1279/1845 train_time:64343ms step_avg:50.31ms
step:1280/1845 train_time:64429ms step_avg:50.34ms
step:1281/1845 train_time:64517ms step_avg:50.36ms
step:1282/1845 train_time:64603ms step_avg:50.39ms
step:1283/1845 train_time:64691ms step_avg:50.42ms
step:1284/1845 train_time:64778ms step_avg:50.45ms
step:1285/1845 train_time:64867ms step_avg:50.48ms
step:1286/1845 train_time:64955ms step_avg:50.51ms
step:1287/1845 train_time:65045ms step_avg:50.54ms
step:1288/1845 train_time:65134ms step_avg:50.57ms
step:1289/1845 train_time:65223ms step_avg:50.60ms
step:1290/1845 train_time:65310ms step_avg:50.63ms
step:1291/1845 train_time:65400ms step_avg:50.66ms
step:1292/1845 train_time:65486ms step_avg:50.69ms
step:1293/1845 train_time:65574ms step_avg:50.71ms
step:1294/1845 train_time:65660ms step_avg:50.74ms
step:1295/1845 train_time:65748ms step_avg:50.77ms
step:1296/1845 train_time:65836ms step_avg:50.80ms
step:1297/1845 train_time:65925ms step_avg:50.83ms
step:1298/1845 train_time:66013ms step_avg:50.86ms
step:1299/1845 train_time:66103ms step_avg:50.89ms
step:1300/1845 train_time:66192ms step_avg:50.92ms
step:1301/1845 train_time:66280ms step_avg:50.95ms
step:1302/1845 train_time:66368ms step_avg:50.97ms
step:1303/1845 train_time:66457ms step_avg:51.00ms
step:1304/1845 train_time:66544ms step_avg:51.03ms
step:1305/1845 train_time:66632ms step_avg:51.06ms
step:1306/1845 train_time:66718ms step_avg:51.09ms
step:1307/1845 train_time:66807ms step_avg:51.11ms
step:1308/1845 train_time:66894ms step_avg:51.14ms
step:1309/1845 train_time:66983ms step_avg:51.17ms
step:1310/1845 train_time:67070ms step_avg:51.20ms
step:1311/1845 train_time:67160ms step_avg:51.23ms
step:1312/1845 train_time:67247ms step_avg:51.26ms
step:1313/1845 train_time:67335ms step_avg:51.28ms
step:1314/1845 train_time:67424ms step_avg:51.31ms
step:1315/1845 train_time:67511ms step_avg:51.34ms
step:1316/1845 train_time:67599ms step_avg:51.37ms
step:1317/1845 train_time:67686ms step_avg:51.39ms
step:1318/1845 train_time:67773ms step_avg:51.42ms
step:1319/1845 train_time:67862ms step_avg:51.45ms
step:1320/1845 train_time:67948ms step_avg:51.48ms
step:1321/1845 train_time:68037ms step_avg:51.50ms
step:1322/1845 train_time:68125ms step_avg:51.53ms
step:1323/1845 train_time:68213ms step_avg:51.56ms
step:1324/1845 train_time:68302ms step_avg:51.59ms
step:1325/1845 train_time:68390ms step_avg:51.62ms
step:1326/1845 train_time:68477ms step_avg:51.64ms
step:1327/1845 train_time:68565ms step_avg:51.67ms
step:1328/1845 train_time:68652ms step_avg:51.70ms
step:1329/1845 train_time:68741ms step_avg:51.72ms
step:1330/1845 train_time:68828ms step_avg:51.75ms
step:1331/1845 train_time:68916ms step_avg:51.78ms
step:1332/1845 train_time:69004ms step_avg:51.80ms
step:1333/1845 train_time:69093ms step_avg:51.83ms
step:1334/1845 train_time:69181ms step_avg:51.86ms
step:1335/1845 train_time:69269ms step_avg:51.89ms
step:1336/1845 train_time:69357ms step_avg:51.91ms
step:1337/1845 train_time:69446ms step_avg:51.94ms
step:1338/1845 train_time:69534ms step_avg:51.97ms
step:1339/1845 train_time:69623ms step_avg:52.00ms
step:1340/1845 train_time:69710ms step_avg:52.02ms
step:1341/1845 train_time:69799ms step_avg:52.05ms
step:1342/1845 train_time:69886ms step_avg:52.08ms
step:1343/1845 train_time:69975ms step_avg:52.10ms
step:1344/1845 train_time:70062ms step_avg:52.13ms
step:1345/1845 train_time:70150ms step_avg:52.16ms
step:1346/1845 train_time:70238ms step_avg:52.18ms
step:1347/1845 train_time:70326ms step_avg:52.21ms
step:1348/1845 train_time:70414ms step_avg:52.24ms
step:1349/1845 train_time:70502ms step_avg:52.26ms
step:1350/1845 train_time:70589ms step_avg:52.29ms
step:1351/1845 train_time:70678ms step_avg:52.32ms
step:1352/1845 train_time:70765ms step_avg:52.34ms
step:1353/1845 train_time:70852ms step_avg:52.37ms
step:1354/1845 train_time:70940ms step_avg:52.39ms
step:1355/1845 train_time:71027ms step_avg:52.42ms
step:1356/1845 train_time:71114ms step_avg:52.44ms
step:1357/1845 train_time:71204ms step_avg:52.47ms
step:1358/1845 train_time:71292ms step_avg:52.50ms
step:1359/1845 train_time:71381ms step_avg:52.52ms
step:1360/1845 train_time:71469ms step_avg:52.55ms
step:1361/1845 train_time:71557ms step_avg:52.58ms
step:1362/1845 train_time:71644ms step_avg:52.60ms
step:1363/1845 train_time:71733ms step_avg:52.63ms
step:1364/1845 train_time:71820ms step_avg:52.65ms
step:1365/1845 train_time:71908ms step_avg:52.68ms
step:1366/1845 train_time:71997ms step_avg:52.71ms
step:1367/1845 train_time:72086ms step_avg:52.73ms
step:1368/1845 train_time:72173ms step_avg:52.76ms
step:1369/1845 train_time:72262ms step_avg:52.78ms
step:1370/1845 train_time:72349ms step_avg:52.81ms
step:1371/1845 train_time:72438ms step_avg:52.84ms
step:1372/1845 train_time:72524ms step_avg:52.86ms
step:1373/1845 train_time:72612ms step_avg:52.89ms
step:1374/1845 train_time:72700ms step_avg:52.91ms
step:1375/1845 train_time:72789ms step_avg:52.94ms
step:1376/1845 train_time:72876ms step_avg:52.96ms
step:1377/1845 train_time:72964ms step_avg:52.99ms
step:1378/1845 train_time:73051ms step_avg:53.01ms
step:1379/1845 train_time:73140ms step_avg:53.04ms
step:1380/1845 train_time:73227ms step_avg:53.06ms
step:1381/1845 train_time:73317ms step_avg:53.09ms
step:1382/1845 train_time:73405ms step_avg:53.11ms
step:1383/1845 train_time:73493ms step_avg:53.14ms
step:1384/1845 train_time:73581ms step_avg:53.17ms
step:1385/1845 train_time:73668ms step_avg:53.19ms
step:1386/1845 train_time:73756ms step_avg:53.22ms
step:1387/1845 train_time:73845ms step_avg:53.24ms
step:1388/1845 train_time:73932ms step_avg:53.26ms
step:1389/1845 train_time:74020ms step_avg:53.29ms
step:1390/1845 train_time:74107ms step_avg:53.31ms
step:1391/1845 train_time:74196ms step_avg:53.34ms
step:1392/1845 train_time:74282ms step_avg:53.36ms
step:1393/1845 train_time:74371ms step_avg:53.39ms
step:1394/1845 train_time:74459ms step_avg:53.41ms
step:1395/1845 train_time:74546ms step_avg:53.44ms
step:1396/1845 train_time:74634ms step_avg:53.46ms
step:1397/1845 train_time:74723ms step_avg:53.49ms
step:1398/1845 train_time:74810ms step_avg:53.51ms
step:1399/1845 train_time:74899ms step_avg:53.54ms
step:1400/1845 train_time:74985ms step_avg:53.56ms
step:1401/1845 train_time:75073ms step_avg:53.59ms
step:1402/1845 train_time:75160ms step_avg:53.61ms
step:1403/1845 train_time:75249ms step_avg:53.63ms
step:1404/1845 train_time:75337ms step_avg:53.66ms
step:1405/1845 train_time:75426ms step_avg:53.68ms
step:1406/1845 train_time:75512ms step_avg:53.71ms
step:1407/1845 train_time:75602ms step_avg:53.73ms
step:1408/1845 train_time:75689ms step_avg:53.76ms
step:1409/1845 train_time:75778ms step_avg:53.78ms
step:1410/1845 train_time:75864ms step_avg:53.80ms
step:1411/1845 train_time:75952ms step_avg:53.83ms
step:1412/1845 train_time:76039ms step_avg:53.85ms
step:1413/1845 train_time:76127ms step_avg:53.88ms
step:1414/1845 train_time:76215ms step_avg:53.90ms
step:1415/1845 train_time:76304ms step_avg:53.93ms
step:1416/1845 train_time:76392ms step_avg:53.95ms
step:1417/1845 train_time:76480ms step_avg:53.97ms
step:1418/1845 train_time:76567ms step_avg:54.00ms
step:1419/1845 train_time:76656ms step_avg:54.02ms
step:1420/1845 train_time:76744ms step_avg:54.04ms
step:1421/1845 train_time:76832ms step_avg:54.07ms
step:1422/1845 train_time:76919ms step_avg:54.09ms
step:1423/1845 train_time:77008ms step_avg:54.12ms
step:1424/1845 train_time:77096ms step_avg:54.14ms
step:1425/1845 train_time:77184ms step_avg:54.16ms
step:1426/1845 train_time:77271ms step_avg:54.19ms
step:1427/1845 train_time:77360ms step_avg:54.21ms
step:1428/1845 train_time:77447ms step_avg:54.23ms
step:1429/1845 train_time:77535ms step_avg:54.26ms
step:1430/1845 train_time:77623ms step_avg:54.28ms
step:1431/1845 train_time:77711ms step_avg:54.31ms
step:1432/1845 train_time:77799ms step_avg:54.33ms
step:1433/1845 train_time:77886ms step_avg:54.35ms
step:1434/1845 train_time:77974ms step_avg:54.38ms
step:1435/1845 train_time:78062ms step_avg:54.40ms
step:1436/1845 train_time:78149ms step_avg:54.42ms
step:1437/1845 train_time:78237ms step_avg:54.44ms
step:1438/1845 train_time:78326ms step_avg:54.47ms
step:1439/1845 train_time:78413ms step_avg:54.49ms
step:1440/1845 train_time:78500ms step_avg:54.51ms
step:1441/1845 train_time:78588ms step_avg:54.54ms
step:1442/1845 train_time:78676ms step_avg:54.56ms
step:1443/1845 train_time:78764ms step_avg:54.58ms
step:1444/1845 train_time:78851ms step_avg:54.61ms
step:1445/1845 train_time:78940ms step_avg:54.63ms
step:1446/1845 train_time:79027ms step_avg:54.65ms
step:1447/1845 train_time:79116ms step_avg:54.68ms
step:1448/1845 train_time:79203ms step_avg:54.70ms
step:1449/1845 train_time:79290ms step_avg:54.72ms
step:1450/1845 train_time:79378ms step_avg:54.74ms
step:1451/1845 train_time:79466ms step_avg:54.77ms
step:1452/1845 train_time:79553ms step_avg:54.79ms
step:1453/1845 train_time:79643ms step_avg:54.81ms
step:1454/1845 train_time:79729ms step_avg:54.83ms
step:1455/1845 train_time:79818ms step_avg:54.86ms
step:1456/1845 train_time:79905ms step_avg:54.88ms
step:1457/1845 train_time:79993ms step_avg:54.90ms
step:1458/1845 train_time:80080ms step_avg:54.92ms
step:1459/1845 train_time:80168ms step_avg:54.95ms
step:1460/1845 train_time:80256ms step_avg:54.97ms
step:1461/1845 train_time:80345ms step_avg:54.99ms
step:1462/1845 train_time:80432ms step_avg:55.02ms
step:1463/1845 train_time:80521ms step_avg:55.04ms
step:1464/1845 train_time:80609ms step_avg:55.06ms
step:1465/1845 train_time:80697ms step_avg:55.08ms
step:1466/1845 train_time:80785ms step_avg:55.11ms
step:1467/1845 train_time:80873ms step_avg:55.13ms
step:1468/1845 train_time:80960ms step_avg:55.15ms
step:1469/1845 train_time:81048ms step_avg:55.17ms
step:1470/1845 train_time:81136ms step_avg:55.19ms
step:1471/1845 train_time:81224ms step_avg:55.22ms
step:1472/1845 train_time:81311ms step_avg:55.24ms
step:1473/1845 train_time:81400ms step_avg:55.26ms
step:1474/1845 train_time:81487ms step_avg:55.28ms
step:1475/1845 train_time:81575ms step_avg:55.31ms
step:1476/1845 train_time:81662ms step_avg:55.33ms
step:1477/1845 train_time:81751ms step_avg:55.35ms
step:1478/1845 train_time:81839ms step_avg:55.37ms
step:1479/1845 train_time:81926ms step_avg:55.39ms
step:1480/1845 train_time:82014ms step_avg:55.41ms
step:1481/1845 train_time:82102ms step_avg:55.44ms
step:1482/1845 train_time:82189ms step_avg:55.46ms
step:1483/1845 train_time:82278ms step_avg:55.48ms
step:1484/1845 train_time:82365ms step_avg:55.50ms
step:1485/1845 train_time:82453ms step_avg:55.52ms
step:1486/1845 train_time:82540ms step_avg:55.55ms
step:1487/1845 train_time:82629ms step_avg:55.57ms
step:1488/1845 train_time:82717ms step_avg:55.59ms
step:1489/1845 train_time:82805ms step_avg:55.61ms
step:1490/1845 train_time:82893ms step_avg:55.63ms
step:1491/1845 train_time:82981ms step_avg:55.65ms
step:1492/1845 train_time:83069ms step_avg:55.68ms
step:1493/1845 train_time:83157ms step_avg:55.70ms
step:1494/1845 train_time:83244ms step_avg:55.72ms
step:1495/1845 train_time:83332ms step_avg:55.74ms
step:1496/1845 train_time:83419ms step_avg:55.76ms
step:1497/1845 train_time:83507ms step_avg:55.78ms
step:1498/1845 train_time:83595ms step_avg:55.80ms
step:1499/1845 train_time:83683ms step_avg:55.83ms
step:1500/1845 train_time:83770ms step_avg:55.85ms
step:1500/1845 val_loss:3.4038 train_time:83860ms step_avg:55.91ms
step:1501/1845 train_time:83880ms step_avg:55.88ms
step:1502/1845 train_time:83951ms step_avg:55.89ms
step:1503/1845 train_time:84044ms step_avg:55.92ms
step:1504/1845 train_time:84132ms step_avg:55.94ms
step:1505/1845 train_time:84220ms step_avg:55.96ms
step:1506/1845 train_time:84306ms step_avg:55.98ms
step:1507/1845 train_time:84394ms step_avg:56.00ms
step:1508/1845 train_time:84480ms step_avg:56.02ms
step:1509/1845 train_time:84567ms step_avg:56.04ms
step:1510/1845 train_time:84654ms step_avg:56.06ms
step:1511/1845 train_time:84742ms step_avg:56.08ms
step:1512/1845 train_time:84830ms step_avg:56.10ms
step:1513/1845 train_time:84921ms step_avg:56.13ms
step:1514/1845 train_time:85010ms step_avg:56.15ms
step:1515/1845 train_time:85100ms step_avg:56.17ms
step:1516/1845 train_time:85187ms step_avg:56.19ms
step:1517/1845 train_time:85275ms step_avg:56.21ms
step:1518/1845 train_time:85361ms step_avg:56.23ms
step:1519/1845 train_time:85449ms step_avg:56.25ms
step:1520/1845 train_time:85536ms step_avg:56.27ms
step:1521/1845 train_time:85623ms step_avg:56.29ms
step:1522/1845 train_time:85709ms step_avg:56.31ms
step:1523/1845 train_time:85799ms step_avg:56.34ms
step:1524/1845 train_time:85886ms step_avg:56.36ms
step:1525/1845 train_time:85977ms step_avg:56.38ms
step:1526/1845 train_time:86065ms step_avg:56.40ms
step:1527/1845 train_time:86155ms step_avg:56.42ms
step:1528/1845 train_time:86242ms step_avg:56.44ms
step:1529/1845 train_time:86330ms step_avg:56.46ms
step:1530/1845 train_time:86417ms step_avg:56.48ms
step:1531/1845 train_time:86504ms step_avg:56.50ms
step:1532/1845 train_time:86591ms step_avg:56.52ms
step:1533/1845 train_time:86679ms step_avg:56.54ms
step:1534/1845 train_time:86767ms step_avg:56.56ms
step:1535/1845 train_time:86856ms step_avg:56.58ms
step:1536/1845 train_time:86944ms step_avg:56.60ms
step:1537/1845 train_time:87033ms step_avg:56.63ms
step:1538/1845 train_time:87121ms step_avg:56.65ms
step:1539/1845 train_time:87209ms step_avg:56.67ms
step:1540/1845 train_time:87296ms step_avg:56.69ms
step:1541/1845 train_time:87384ms step_avg:56.71ms
step:1542/1845 train_time:87471ms step_avg:56.73ms
step:1543/1845 train_time:87559ms step_avg:56.75ms
step:1544/1845 train_time:87647ms step_avg:56.77ms
step:1545/1845 train_time:87734ms step_avg:56.79ms
step:1546/1845 train_time:87821ms step_avg:56.81ms
step:1547/1845 train_time:87910ms step_avg:56.83ms
step:1548/1845 train_time:87998ms step_avg:56.85ms
step:1549/1845 train_time:88087ms step_avg:56.87ms
step:1550/1845 train_time:88174ms step_avg:56.89ms
step:1551/1845 train_time:88263ms step_avg:56.91ms
step:1552/1845 train_time:88349ms step_avg:56.93ms
step:1553/1845 train_time:88437ms step_avg:56.95ms
step:1554/1845 train_time:88524ms step_avg:56.97ms
step:1555/1845 train_time:88612ms step_avg:56.99ms
step:1556/1845 train_time:88699ms step_avg:57.00ms
step:1557/1845 train_time:88787ms step_avg:57.02ms
step:1558/1845 train_time:88876ms step_avg:57.04ms
step:1559/1845 train_time:88965ms step_avg:57.07ms
step:1560/1845 train_time:89053ms step_avg:57.09ms
step:1561/1845 train_time:89142ms step_avg:57.11ms
step:1562/1845 train_time:89229ms step_avg:57.13ms
step:1563/1845 train_time:89317ms step_avg:57.14ms
step:1564/1845 train_time:89404ms step_avg:57.16ms
step:1565/1845 train_time:89491ms step_avg:57.18ms
step:1566/1845 train_time:89579ms step_avg:57.20ms
step:1567/1845 train_time:89667ms step_avg:57.22ms
step:1568/1845 train_time:89754ms step_avg:57.24ms
step:1569/1845 train_time:89843ms step_avg:57.26ms
step:1570/1845 train_time:89931ms step_avg:57.28ms
step:1571/1845 train_time:90019ms step_avg:57.30ms
step:1572/1845 train_time:90106ms step_avg:57.32ms
step:1573/1845 train_time:90194ms step_avg:57.34ms
step:1574/1845 train_time:90281ms step_avg:57.36ms
step:1575/1845 train_time:90370ms step_avg:57.38ms
step:1576/1845 train_time:90458ms step_avg:57.40ms
step:1577/1845 train_time:90546ms step_avg:57.42ms
step:1578/1845 train_time:90633ms step_avg:57.44ms
step:1579/1845 train_time:90722ms step_avg:57.46ms
step:1580/1845 train_time:90809ms step_avg:57.47ms
step:1581/1845 train_time:90897ms step_avg:57.49ms
step:1582/1845 train_time:90985ms step_avg:57.51ms
step:1583/1845 train_time:91073ms step_avg:57.53ms
step:1584/1845 train_time:91160ms step_avg:57.55ms
step:1585/1845 train_time:91248ms step_avg:57.57ms
step:1586/1845 train_time:91337ms step_avg:57.59ms
step:1587/1845 train_time:91426ms step_avg:57.61ms
step:1588/1845 train_time:91512ms step_avg:57.63ms
step:1589/1845 train_time:91601ms step_avg:57.65ms
step:1590/1845 train_time:91689ms step_avg:57.67ms
step:1591/1845 train_time:91778ms step_avg:57.69ms
step:1592/1845 train_time:91865ms step_avg:57.70ms
step:1593/1845 train_time:91953ms step_avg:57.72ms
step:1594/1845 train_time:92040ms step_avg:57.74ms
step:1595/1845 train_time:92128ms step_avg:57.76ms
step:1596/1845 train_time:92215ms step_avg:57.78ms
step:1597/1845 train_time:92304ms step_avg:57.80ms
step:1598/1845 train_time:92391ms step_avg:57.82ms
step:1599/1845 train_time:92480ms step_avg:57.84ms
step:1600/1845 train_time:92567ms step_avg:57.85ms
step:1601/1845 train_time:92655ms step_avg:57.87ms
step:1602/1845 train_time:92743ms step_avg:57.89ms
step:1603/1845 train_time:92831ms step_avg:57.91ms
step:1604/1845 train_time:92918ms step_avg:57.93ms
step:1605/1845 train_time:93007ms step_avg:57.95ms
step:1606/1845 train_time:93094ms step_avg:57.97ms
step:1607/1845 train_time:93183ms step_avg:57.99ms
step:1608/1845 train_time:93270ms step_avg:58.00ms
step:1609/1845 train_time:93360ms step_avg:58.02ms
step:1610/1845 train_time:93447ms step_avg:58.04ms
step:1611/1845 train_time:93536ms step_avg:58.06ms
step:1612/1845 train_time:93623ms step_avg:58.08ms
step:1613/1845 train_time:93711ms step_avg:58.10ms
step:1614/1845 train_time:93799ms step_avg:58.12ms
step:1615/1845 train_time:93887ms step_avg:58.13ms
step:1616/1845 train_time:93975ms step_avg:58.15ms
step:1617/1845 train_time:94063ms step_avg:58.17ms
step:1618/1845 train_time:94150ms step_avg:58.19ms
step:1619/1845 train_time:94239ms step_avg:58.21ms
step:1620/1845 train_time:94326ms step_avg:58.23ms
step:1621/1845 train_time:94416ms step_avg:58.25ms
step:1622/1845 train_time:94503ms step_avg:58.26ms
step:1623/1845 train_time:94591ms step_avg:58.28ms
step:1624/1845 train_time:94679ms step_avg:58.30ms
step:1625/1845 train_time:94767ms step_avg:58.32ms
step:1626/1845 train_time:94854ms step_avg:58.34ms
step:1627/1845 train_time:94943ms step_avg:58.35ms
step:1628/1845 train_time:95031ms step_avg:58.37ms
step:1629/1845 train_time:95118ms step_avg:58.39ms
step:1630/1845 train_time:95206ms step_avg:58.41ms
step:1631/1845 train_time:95294ms step_avg:58.43ms
step:1632/1845 train_time:95381ms step_avg:58.44ms
step:1633/1845 train_time:95470ms step_avg:58.46ms
step:1634/1845 train_time:95558ms step_avg:58.48ms
step:1635/1845 train_time:95646ms step_avg:58.50ms
step:1636/1845 train_time:95733ms step_avg:58.52ms
step:1637/1845 train_time:95821ms step_avg:58.53ms
step:1638/1845 train_time:95908ms step_avg:58.55ms
step:1639/1845 train_time:95996ms step_avg:58.57ms
step:1640/1845 train_time:96083ms step_avg:58.59ms
step:1641/1845 train_time:96172ms step_avg:58.61ms
step:1642/1845 train_time:96259ms step_avg:58.62ms
step:1643/1845 train_time:96347ms step_avg:58.64ms
step:1644/1845 train_time:96435ms step_avg:58.66ms
step:1645/1845 train_time:96523ms step_avg:58.68ms
step:1646/1845 train_time:96611ms step_avg:58.69ms
step:1647/1845 train_time:96700ms step_avg:58.71ms
step:1648/1845 train_time:96786ms step_avg:58.73ms
step:1649/1845 train_time:96875ms step_avg:58.75ms
step:1650/1845 train_time:96962ms step_avg:58.77ms
step:1651/1845 train_time:97051ms step_avg:58.78ms
step:1652/1845 train_time:97140ms step_avg:58.80ms
step:1653/1845 train_time:97227ms step_avg:58.82ms
step:1654/1845 train_time:97315ms step_avg:58.84ms
step:1655/1845 train_time:97402ms step_avg:58.85ms
step:1656/1845 train_time:97488ms step_avg:58.87ms
step:1657/1845 train_time:97578ms step_avg:58.89ms
step:1658/1845 train_time:97665ms step_avg:58.91ms
step:1659/1845 train_time:97754ms step_avg:58.92ms
step:1660/1845 train_time:97841ms step_avg:58.94ms
step:1661/1845 train_time:97929ms step_avg:58.96ms
step:1662/1845 train_time:98017ms step_avg:58.98ms
step:1663/1845 train_time:98106ms step_avg:58.99ms
step:1664/1845 train_time:98193ms step_avg:59.01ms
step:1665/1845 train_time:98282ms step_avg:59.03ms
step:1666/1845 train_time:98369ms step_avg:59.04ms
step:1667/1845 train_time:98458ms step_avg:59.06ms
step:1668/1845 train_time:98545ms step_avg:59.08ms
step:1669/1845 train_time:98633ms step_avg:59.10ms
step:1670/1845 train_time:98720ms step_avg:59.11ms
step:1671/1845 train_time:98808ms step_avg:59.13ms
step:1672/1845 train_time:98895ms step_avg:59.15ms
step:1673/1845 train_time:98984ms step_avg:59.17ms
step:1674/1845 train_time:99072ms step_avg:59.18ms
step:1675/1845 train_time:99160ms step_avg:59.20ms
step:1676/1845 train_time:99247ms step_avg:59.22ms
step:1677/1845 train_time:99335ms step_avg:59.23ms
step:1678/1845 train_time:99423ms step_avg:59.25ms
step:1679/1845 train_time:99512ms step_avg:59.27ms
step:1680/1845 train_time:99600ms step_avg:59.29ms
step:1681/1845 train_time:99688ms step_avg:59.30ms
step:1682/1845 train_time:99776ms step_avg:59.32ms
step:1683/1845 train_time:99865ms step_avg:59.34ms
step:1684/1845 train_time:99952ms step_avg:59.35ms
step:1685/1845 train_time:100041ms step_avg:59.37ms
step:1686/1845 train_time:100129ms step_avg:59.39ms
step:1687/1845 train_time:100217ms step_avg:59.41ms
step:1688/1845 train_time:100305ms step_avg:59.42ms
step:1689/1845 train_time:100394ms step_avg:59.44ms
step:1690/1845 train_time:100482ms step_avg:59.46ms
step:1691/1845 train_time:100570ms step_avg:59.47ms
step:1692/1845 train_time:100657ms step_avg:59.49ms
step:1693/1845 train_time:100745ms step_avg:59.51ms
step:1694/1845 train_time:100833ms step_avg:59.52ms
step:1695/1845 train_time:100921ms step_avg:59.54ms
step:1696/1845 train_time:101008ms step_avg:59.56ms
step:1697/1845 train_time:101098ms step_avg:59.57ms
step:1698/1845 train_time:101185ms step_avg:59.59ms
step:1699/1845 train_time:101274ms step_avg:59.61ms
step:1700/1845 train_time:101361ms step_avg:59.62ms
step:1701/1845 train_time:101449ms step_avg:59.64ms
step:1702/1845 train_time:101536ms step_avg:59.66ms
step:1703/1845 train_time:101624ms step_avg:59.67ms
step:1704/1845 train_time:101712ms step_avg:59.69ms
step:1705/1845 train_time:101800ms step_avg:59.71ms
step:1706/1845 train_time:101887ms step_avg:59.72ms
step:1707/1845 train_time:101975ms step_avg:59.74ms
step:1708/1845 train_time:102063ms step_avg:59.76ms
step:1709/1845 train_time:102151ms step_avg:59.77ms
step:1710/1845 train_time:102239ms step_avg:59.79ms
step:1711/1845 train_time:102327ms step_avg:59.81ms
step:1712/1845 train_time:102415ms step_avg:59.82ms
step:1713/1845 train_time:102502ms step_avg:59.84ms
step:1714/1845 train_time:102590ms step_avg:59.85ms
step:1715/1845 train_time:102679ms step_avg:59.87ms
step:1716/1845 train_time:102766ms step_avg:59.89ms
step:1717/1845 train_time:102854ms step_avg:59.90ms
step:1718/1845 train_time:102942ms step_avg:59.92ms
step:1719/1845 train_time:103030ms step_avg:59.94ms
step:1720/1845 train_time:103118ms step_avg:59.95ms
step:1721/1845 train_time:103206ms step_avg:59.97ms
step:1722/1845 train_time:103293ms step_avg:59.98ms
step:1723/1845 train_time:103383ms step_avg:60.00ms
step:1724/1845 train_time:103471ms step_avg:60.02ms
step:1725/1845 train_time:103558ms step_avg:60.03ms
step:1726/1845 train_time:103645ms step_avg:60.05ms
step:1727/1845 train_time:103734ms step_avg:60.07ms
step:1728/1845 train_time:103821ms step_avg:60.08ms
step:1729/1845 train_time:103910ms step_avg:60.10ms
step:1730/1845 train_time:103997ms step_avg:60.11ms
step:1731/1845 train_time:104086ms step_avg:60.13ms
step:1732/1845 train_time:104172ms step_avg:60.15ms
step:1733/1845 train_time:104261ms step_avg:60.16ms
step:1734/1845 train_time:104348ms step_avg:60.18ms
step:1735/1845 train_time:104437ms step_avg:60.19ms
step:1736/1845 train_time:104523ms step_avg:60.21ms
step:1737/1845 train_time:104612ms step_avg:60.23ms
step:1738/1845 train_time:104699ms step_avg:60.24ms
step:1739/1845 train_time:104787ms step_avg:60.26ms
step:1740/1845 train_time:104875ms step_avg:60.27ms
step:1741/1845 train_time:104964ms step_avg:60.29ms
step:1742/1845 train_time:105052ms step_avg:60.31ms
step:1743/1845 train_time:105141ms step_avg:60.32ms
step:1744/1845 train_time:105227ms step_avg:60.34ms
step:1745/1845 train_time:105316ms step_avg:60.35ms
step:1746/1845 train_time:105403ms step_avg:60.37ms
step:1747/1845 train_time:105491ms step_avg:60.38ms
step:1748/1845 train_time:105580ms step_avg:60.40ms
step:1749/1845 train_time:105668ms step_avg:60.42ms
step:1750/1845 train_time:105755ms step_avg:60.43ms
step:1750/1845 val_loss:3.3048 train_time:105845ms step_avg:60.48ms
step:1751/1845 train_time:105865ms step_avg:60.46ms
step:1752/1845 train_time:105935ms step_avg:60.46ms
step:1753/1845 train_time:106027ms step_avg:60.48ms
step:1754/1845 train_time:106115ms step_avg:60.50ms
step:1755/1845 train_time:106203ms step_avg:60.51ms
step:1756/1845 train_time:106290ms step_avg:60.53ms
step:1757/1845 train_time:106378ms step_avg:60.55ms
step:1758/1845 train_time:106464ms step_avg:60.56ms
step:1759/1845 train_time:106553ms step_avg:60.58ms
step:1760/1845 train_time:106640ms step_avg:60.59ms
step:1761/1845 train_time:106728ms step_avg:60.61ms
step:1762/1845 train_time:106816ms step_avg:60.62ms
step:1763/1845 train_time:106906ms step_avg:60.64ms
step:1764/1845 train_time:106996ms step_avg:60.66ms
step:1765/1845 train_time:107084ms step_avg:60.67ms
step:1766/1845 train_time:107173ms step_avg:60.69ms
step:1767/1845 train_time:107261ms step_avg:60.70ms
step:1768/1845 train_time:107347ms step_avg:60.72ms
step:1769/1845 train_time:107435ms step_avg:60.73ms
step:1770/1845 train_time:107522ms step_avg:60.75ms
step:1771/1845 train_time:107609ms step_avg:60.76ms
step:1772/1845 train_time:107696ms step_avg:60.78ms
step:1773/1845 train_time:107784ms step_avg:60.79ms
step:1774/1845 train_time:107873ms step_avg:60.81ms
step:1775/1845 train_time:107962ms step_avg:60.82ms
step:1776/1845 train_time:108051ms step_avg:60.84ms
step:1777/1845 train_time:108140ms step_avg:60.86ms
step:1778/1845 train_time:108227ms step_avg:60.87ms
step:1779/1845 train_time:108315ms step_avg:60.89ms
step:1780/1845 train_time:108401ms step_avg:60.90ms
step:1781/1845 train_time:108490ms step_avg:60.92ms
step:1782/1845 train_time:108578ms step_avg:60.93ms
step:1783/1845 train_time:108666ms step_avg:60.95ms
step:1784/1845 train_time:108753ms step_avg:60.96ms
step:1785/1845 train_time:108843ms step_avg:60.98ms
step:1786/1845 train_time:108931ms step_avg:60.99ms
step:1787/1845 train_time:109020ms step_avg:61.01ms
step:1788/1845 train_time:109108ms step_avg:61.02ms
step:1789/1845 train_time:109197ms step_avg:61.04ms
step:1790/1845 train_time:109285ms step_avg:61.05ms
step:1791/1845 train_time:109372ms step_avg:61.07ms
step:1792/1845 train_time:109459ms step_avg:61.08ms
step:1793/1845 train_time:109547ms step_avg:61.10ms
step:1794/1845 train_time:109634ms step_avg:61.11ms
step:1795/1845 train_time:109723ms step_avg:61.13ms
step:1796/1845 train_time:109811ms step_avg:61.14ms
step:1797/1845 train_time:109900ms step_avg:61.16ms
step:1798/1845 train_time:109988ms step_avg:61.17ms
step:1799/1845 train_time:110077ms step_avg:61.19ms
step:1800/1845 train_time:110164ms step_avg:61.20ms
step:1801/1845 train_time:110253ms step_avg:61.22ms
step:1802/1845 train_time:110341ms step_avg:61.23ms
step:1803/1845 train_time:110429ms step_avg:61.25ms
step:1804/1845 train_time:110517ms step_avg:61.26ms
step:1805/1845 train_time:110605ms step_avg:61.28ms
step:1806/1845 train_time:110693ms step_avg:61.29ms
step:1807/1845 train_time:110783ms step_avg:61.31ms
step:1808/1845 train_time:110870ms step_avg:61.32ms
step:1809/1845 train_time:110960ms step_avg:61.34ms
step:1810/1845 train_time:111048ms step_avg:61.35ms
step:1811/1845 train_time:111137ms step_avg:61.37ms
step:1812/1845 train_time:111224ms step_avg:61.38ms
step:1813/1845 train_time:111313ms step_avg:61.40ms
step:1814/1845 train_time:111400ms step_avg:61.41ms
step:1815/1845 train_time:111488ms step_avg:61.43ms
step:1816/1845 train_time:111576ms step_avg:61.44ms
step:1817/1845 train_time:111664ms step_avg:61.46ms
step:1818/1845 train_time:111752ms step_avg:61.47ms
step:1819/1845 train_time:111841ms step_avg:61.48ms
step:1820/1845 train_time:111927ms step_avg:61.50ms
step:1821/1845 train_time:112016ms step_avg:61.51ms
step:1822/1845 train_time:112104ms step_avg:61.53ms
step:1823/1845 train_time:112193ms step_avg:61.54ms
step:1824/1845 train_time:112280ms step_avg:61.56ms
step:1825/1845 train_time:112369ms step_avg:61.57ms
step:1826/1845 train_time:112456ms step_avg:61.59ms
step:1827/1845 train_time:112544ms step_avg:61.60ms
step:1828/1845 train_time:112632ms step_avg:61.61ms
step:1829/1845 train_time:112721ms step_avg:61.63ms
step:1830/1845 train_time:112809ms step_avg:61.64ms
step:1831/1845 train_time:112897ms step_avg:61.66ms
step:1832/1845 train_time:112984ms step_avg:61.67ms
step:1833/1845 train_time:113073ms step_avg:61.69ms
step:1834/1845 train_time:113160ms step_avg:61.70ms
step:1835/1845 train_time:113249ms step_avg:61.72ms
step:1836/1845 train_time:113337ms step_avg:61.73ms
step:1837/1845 train_time:113425ms step_avg:61.74ms
step:1838/1845 train_time:113512ms step_avg:61.76ms
step:1839/1845 train_time:113602ms step_avg:61.77ms
step:1840/1845 train_time:113690ms step_avg:61.79ms
step:1841/1845 train_time:113779ms step_avg:61.80ms
step:1842/1845 train_time:113866ms step_avg:61.82ms
step:1843/1845 train_time:113956ms step_avg:61.83ms
step:1844/1845 train_time:114043ms step_avg:61.85ms
step:1845/1845 train_time:114132ms step_avg:61.86ms
step:1845/1845 val_loss:3.2784 train_time:114220ms step_avg:61.91ms
peak memory allocated: 29405 MiB reserved: 44398 MiB

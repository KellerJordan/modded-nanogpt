import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc
os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        group_sizes = [15, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

        # only include gates on layers with value embeds used on forward pass
        if layer_idx in [0,1,8,9,10]:
            self.value_embed_gate = CastedLinear(12, num_heads)
            self.value_embed_gate.weight.label = 'value_embed_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(self.value_embed_gate(x[..., :self.value_embed_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        #self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig, skew = None):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if skew is None:
            logits = 23 * torch.sigmoid((logits+5) / 7.5)
        else:
            logits = 23 * torch.sigmoid((logits+skew) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
def log_parameter_norms(model):
    norms = {}
    for name, param in model.named_parameters():
        if param.requires_grad:
            norms[f"param_norm/{name}"] = param.norm().item()
            norms[f"param_max/{name}"] = param.max().item()
            norms[f"param_min/{name}"] = param.min().item()
            if param.grad is not None:
                norms[f"grad_norm/{name}"] = param.grad.norm().item()
                norms[f"grad_max/{name}"] = param.grad.max().item()
    
    # Total norms
    total_param_norm = sum(p.norm().item() ** 2 for p in model.parameters()) ** 0.5
    total_grad_norm = sum(p.grad.norm().item() ** 2 for p in model.parameters() if p.grad is not None) ** 0.5
    
    norms["param_norm/total"] = total_param_norm
    norms["grad_norm/total"] = total_grad_norm
    
    return norms

train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

# import wandb
# if master_process:
#     wandb.init(
#         project="nano4",
#         name="baseline"
#     )

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    # if master_process and step%2==1:
    #     wandb.log({
    #         "loss": loss.item(),
    #         **log_parameter_norms(model)  # Add all norms
    #     })
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 29 05:59:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0            110W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            107W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8309 train_time:0ms step_avg:0.03ms
step:1/1845 train_time:86ms step_avg:86.15ms
step:2/1845 train_time:110ms step_avg:55.04ms
step:3/1845 train_time:131ms step_avg:43.65ms
step:4/1845 train_time:164ms step_avg:41.07ms
step:5/1845 train_time:199ms step_avg:39.83ms
step:6/1845 train_time:286ms step_avg:47.73ms
step:7/1845 train_time:429ms step_avg:61.25ms
step:8/1845 train_time:463ms step_avg:57.84ms
step:9/1845 train_time:497ms step_avg:55.23ms
step:10/1845 train_time:531ms step_avg:53.10ms
step:11/1845 train_time:566ms step_avg:51.41ms
step:12/1845 train_time:600ms step_avg:49.97ms
step:13/1845 train_time:634ms step_avg:48.80ms
step:14/1845 train_time:669ms step_avg:47.80ms
step:15/1845 train_time:703ms step_avg:46.88ms
step:16/1845 train_time:737ms step_avg:46.08ms
step:17/1845 train_time:772ms step_avg:45.42ms
step:18/1845 train_time:806ms step_avg:44.79ms
step:19/1845 train_time:841ms step_avg:44.26ms
step:20/1845 train_time:875ms step_avg:43.75ms
step:21/1845 train_time:910ms step_avg:43.32ms
step:22/1845 train_time:944ms step_avg:42.90ms
step:23/1845 train_time:979ms step_avg:42.54ms
step:24/1845 train_time:1013ms step_avg:42.20ms
step:25/1845 train_time:1047ms step_avg:41.88ms
step:26/1845 train_time:1081ms step_avg:41.58ms
step:27/1845 train_time:1116ms step_avg:41.33ms
step:28/1845 train_time:1150ms step_avg:41.07ms
step:29/1845 train_time:1185ms step_avg:40.85ms
step:30/1845 train_time:1219ms step_avg:40.62ms
step:31/1845 train_time:1253ms step_avg:40.43ms
step:32/1845 train_time:1288ms step_avg:40.24ms
step:33/1845 train_time:1323ms step_avg:40.09ms
step:34/1845 train_time:1357ms step_avg:39.92ms
step:35/1845 train_time:1393ms step_avg:39.79ms
step:36/1845 train_time:1427ms step_avg:39.63ms
step:37/1845 train_time:1462ms step_avg:39.51ms
step:38/1845 train_time:1496ms step_avg:39.38ms
step:39/1845 train_time:1531ms step_avg:39.27ms
step:40/1845 train_time:1565ms step_avg:39.14ms
step:41/1845 train_time:1601ms step_avg:39.04ms
step:42/1845 train_time:1635ms step_avg:38.93ms
step:43/1845 train_time:1670ms step_avg:38.83ms
step:44/1845 train_time:1704ms step_avg:38.73ms
step:45/1845 train_time:1739ms step_avg:38.64ms
step:46/1845 train_time:1773ms step_avg:38.54ms
step:47/1845 train_time:1808ms step_avg:38.46ms
step:48/1845 train_time:1842ms step_avg:38.37ms
step:49/1845 train_time:1877ms step_avg:38.30ms
step:50/1845 train_time:1911ms step_avg:38.21ms
step:51/1845 train_time:1945ms step_avg:38.14ms
step:52/1845 train_time:1979ms step_avg:38.06ms
step:53/1845 train_time:2014ms step_avg:38.00ms
step:54/1845 train_time:2048ms step_avg:37.93ms
step:55/1845 train_time:2083ms step_avg:37.87ms
step:56/1845 train_time:2117ms step_avg:37.80ms
step:57/1845 train_time:2151ms step_avg:37.74ms
step:58/1845 train_time:2186ms step_avg:37.68ms
step:59/1845 train_time:2220ms step_avg:37.63ms
step:60/1845 train_time:2255ms step_avg:37.58ms
step:61/1845 train_time:2289ms step_avg:37.53ms
step:62/1845 train_time:2324ms step_avg:37.48ms
step:63/1845 train_time:2359ms step_avg:37.44ms
step:64/1845 train_time:2393ms step_avg:37.39ms
step:65/1845 train_time:2427ms step_avg:37.34ms
step:66/1845 train_time:2462ms step_avg:37.30ms
step:67/1845 train_time:2497ms step_avg:37.26ms
step:68/1845 train_time:2531ms step_avg:37.21ms
step:69/1845 train_time:2565ms step_avg:37.18ms
step:70/1845 train_time:2600ms step_avg:37.14ms
step:71/1845 train_time:2635ms step_avg:37.11ms
step:72/1845 train_time:2669ms step_avg:37.07ms
step:73/1845 train_time:2704ms step_avg:37.04ms
step:74/1845 train_time:2738ms step_avg:37.00ms
step:75/1845 train_time:2773ms step_avg:36.98ms
step:76/1845 train_time:2807ms step_avg:36.94ms
step:77/1845 train_time:2842ms step_avg:36.91ms
step:78/1845 train_time:2876ms step_avg:36.87ms
step:79/1845 train_time:2911ms step_avg:36.85ms
step:80/1845 train_time:2945ms step_avg:36.81ms
step:81/1845 train_time:2980ms step_avg:36.79ms
step:82/1845 train_time:3014ms step_avg:36.75ms
step:83/1845 train_time:3049ms step_avg:36.73ms
step:84/1845 train_time:3083ms step_avg:36.70ms
step:85/1845 train_time:3118ms step_avg:36.68ms
step:86/1845 train_time:3152ms step_avg:36.65ms
step:87/1845 train_time:3186ms step_avg:36.62ms
step:88/1845 train_time:3220ms step_avg:36.60ms
step:89/1845 train_time:3255ms step_avg:36.58ms
step:90/1845 train_time:3290ms step_avg:36.55ms
step:91/1845 train_time:3324ms step_avg:36.53ms
step:92/1845 train_time:3358ms step_avg:36.50ms
step:93/1845 train_time:3393ms step_avg:36.49ms
step:94/1845 train_time:3427ms step_avg:36.46ms
step:95/1845 train_time:3462ms step_avg:36.45ms
step:96/1845 train_time:3496ms step_avg:36.42ms
step:97/1845 train_time:3531ms step_avg:36.41ms
step:98/1845 train_time:3565ms step_avg:36.38ms
step:99/1845 train_time:3600ms step_avg:36.36ms
step:100/1845 train_time:3634ms step_avg:36.34ms
step:101/1845 train_time:3669ms step_avg:36.32ms
step:102/1845 train_time:3703ms step_avg:36.30ms
step:103/1845 train_time:3737ms step_avg:36.28ms
step:104/1845 train_time:3771ms step_avg:36.26ms
step:105/1845 train_time:3806ms step_avg:36.25ms
step:106/1845 train_time:3840ms step_avg:36.23ms
step:107/1845 train_time:3875ms step_avg:36.21ms
step:108/1845 train_time:3909ms step_avg:36.19ms
step:109/1845 train_time:3944ms step_avg:36.18ms
step:110/1845 train_time:3978ms step_avg:36.16ms
step:111/1845 train_time:4012ms step_avg:36.15ms
step:112/1845 train_time:4046ms step_avg:36.13ms
step:113/1845 train_time:4081ms step_avg:36.11ms
step:114/1845 train_time:4115ms step_avg:36.10ms
step:115/1845 train_time:4150ms step_avg:36.08ms
step:116/1845 train_time:4184ms step_avg:36.07ms
step:117/1845 train_time:4219ms step_avg:36.06ms
step:118/1845 train_time:4253ms step_avg:36.04ms
step:119/1845 train_time:4288ms step_avg:36.03ms
step:120/1845 train_time:4322ms step_avg:36.01ms
step:121/1845 train_time:4357ms step_avg:36.01ms
step:122/1845 train_time:4391ms step_avg:35.99ms
step:123/1845 train_time:4426ms step_avg:35.98ms
step:124/1845 train_time:4460ms step_avg:35.96ms
step:125/1845 train_time:4495ms step_avg:35.96ms
step:126/1845 train_time:4529ms step_avg:35.94ms
step:127/1845 train_time:4564ms step_avg:35.93ms
step:128/1845 train_time:4598ms step_avg:35.92ms
step:129/1845 train_time:4632ms step_avg:35.91ms
step:130/1845 train_time:4666ms step_avg:35.90ms
step:131/1845 train_time:4701ms step_avg:35.89ms
step:132/1845 train_time:4735ms step_avg:35.87ms
step:133/1845 train_time:4770ms step_avg:35.86ms
step:134/1845 train_time:4804ms step_avg:35.85ms
step:135/1845 train_time:4839ms step_avg:35.84ms
step:136/1845 train_time:4873ms step_avg:35.83ms
step:137/1845 train_time:4907ms step_avg:35.82ms
step:138/1845 train_time:4941ms step_avg:35.81ms
step:139/1845 train_time:4976ms step_avg:35.80ms
step:140/1845 train_time:5010ms step_avg:35.79ms
step:141/1845 train_time:5045ms step_avg:35.78ms
step:142/1845 train_time:5079ms step_avg:35.77ms
step:143/1845 train_time:5114ms step_avg:35.76ms
step:144/1845 train_time:5148ms step_avg:35.75ms
step:145/1845 train_time:5183ms step_avg:35.74ms
step:146/1845 train_time:5217ms step_avg:35.73ms
step:147/1845 train_time:5251ms step_avg:35.72ms
step:148/1845 train_time:5286ms step_avg:35.71ms
step:149/1845 train_time:5320ms step_avg:35.71ms
step:150/1845 train_time:5354ms step_avg:35.70ms
step:151/1845 train_time:5389ms step_avg:35.69ms
step:152/1845 train_time:5423ms step_avg:35.68ms
step:153/1845 train_time:5458ms step_avg:35.67ms
step:154/1845 train_time:5492ms step_avg:35.66ms
step:155/1845 train_time:5526ms step_avg:35.65ms
step:156/1845 train_time:5561ms step_avg:35.65ms
step:157/1845 train_time:5596ms step_avg:35.64ms
step:158/1845 train_time:5630ms step_avg:35.63ms
step:159/1845 train_time:5665ms step_avg:35.63ms
step:160/1845 train_time:5698ms step_avg:35.62ms
step:161/1845 train_time:5733ms step_avg:35.61ms
step:162/1845 train_time:5767ms step_avg:35.60ms
step:163/1845 train_time:5802ms step_avg:35.59ms
step:164/1845 train_time:5836ms step_avg:35.59ms
step:165/1845 train_time:5871ms step_avg:35.58ms
step:166/1845 train_time:5905ms step_avg:35.57ms
step:167/1845 train_time:5939ms step_avg:35.56ms
step:168/1845 train_time:5973ms step_avg:35.56ms
step:169/1845 train_time:6008ms step_avg:35.55ms
step:170/1845 train_time:6042ms step_avg:35.54ms
step:171/1845 train_time:6076ms step_avg:35.53ms
step:172/1845 train_time:6110ms step_avg:35.52ms
step:173/1845 train_time:6145ms step_avg:35.52ms
step:174/1845 train_time:6179ms step_avg:35.51ms
step:175/1845 train_time:6214ms step_avg:35.51ms
step:176/1845 train_time:6248ms step_avg:35.50ms
step:177/1845 train_time:6282ms step_avg:35.49ms
step:178/1845 train_time:6316ms step_avg:35.48ms
step:179/1845 train_time:6351ms step_avg:35.48ms
step:180/1845 train_time:6385ms step_avg:35.47ms
step:181/1845 train_time:6420ms step_avg:35.47ms
step:182/1845 train_time:6454ms step_avg:35.46ms
step:183/1845 train_time:6488ms step_avg:35.45ms
step:184/1845 train_time:6522ms step_avg:35.45ms
step:185/1845 train_time:6557ms step_avg:35.44ms
step:186/1845 train_time:6591ms step_avg:35.44ms
step:187/1845 train_time:6626ms step_avg:35.43ms
step:188/1845 train_time:6660ms step_avg:35.42ms
step:189/1845 train_time:6694ms step_avg:35.42ms
step:190/1845 train_time:6729ms step_avg:35.41ms
step:191/1845 train_time:6763ms step_avg:35.41ms
step:192/1845 train_time:6797ms step_avg:35.40ms
step:193/1845 train_time:6832ms step_avg:35.40ms
step:194/1845 train_time:6866ms step_avg:35.39ms
step:195/1845 train_time:6901ms step_avg:35.39ms
step:196/1845 train_time:6935ms step_avg:35.38ms
step:197/1845 train_time:6970ms step_avg:35.38ms
step:198/1845 train_time:7003ms step_avg:35.37ms
step:199/1845 train_time:7038ms step_avg:35.37ms
step:200/1845 train_time:7072ms step_avg:35.36ms
step:201/1845 train_time:7106ms step_avg:35.35ms
step:202/1845 train_time:7140ms step_avg:35.35ms
step:203/1845 train_time:7175ms step_avg:35.34ms
step:204/1845 train_time:7209ms step_avg:35.34ms
step:205/1845 train_time:7243ms step_avg:35.33ms
step:206/1845 train_time:7277ms step_avg:35.33ms
step:207/1845 train_time:7312ms step_avg:35.32ms
step:208/1845 train_time:7346ms step_avg:35.32ms
step:209/1845 train_time:7381ms step_avg:35.31ms
step:210/1845 train_time:7415ms step_avg:35.31ms
step:211/1845 train_time:7450ms step_avg:35.31ms
step:212/1845 train_time:7484ms step_avg:35.30ms
step:213/1845 train_time:7518ms step_avg:35.30ms
step:214/1845 train_time:7552ms step_avg:35.29ms
step:215/1845 train_time:7587ms step_avg:35.29ms
step:216/1845 train_time:7621ms step_avg:35.28ms
step:217/1845 train_time:7656ms step_avg:35.28ms
step:218/1845 train_time:7690ms step_avg:35.28ms
step:219/1845 train_time:7724ms step_avg:35.27ms
step:220/1845 train_time:7759ms step_avg:35.27ms
step:221/1845 train_time:7793ms step_avg:35.26ms
step:222/1845 train_time:7828ms step_avg:35.26ms
step:223/1845 train_time:7862ms step_avg:35.26ms
step:224/1845 train_time:7896ms step_avg:35.25ms
step:225/1845 train_time:7931ms step_avg:35.25ms
step:226/1845 train_time:7965ms step_avg:35.24ms
step:227/1845 train_time:7999ms step_avg:35.24ms
step:228/1845 train_time:8033ms step_avg:35.23ms
step:229/1845 train_time:8068ms step_avg:35.23ms
step:230/1845 train_time:8102ms step_avg:35.22ms
step:231/1845 train_time:8136ms step_avg:35.22ms
step:232/1845 train_time:8170ms step_avg:35.22ms
step:233/1845 train_time:8204ms step_avg:35.21ms
step:234/1845 train_time:8238ms step_avg:35.21ms
step:235/1845 train_time:8273ms step_avg:35.20ms
step:236/1845 train_time:8307ms step_avg:35.20ms
step:237/1845 train_time:8341ms step_avg:35.20ms
step:238/1845 train_time:8375ms step_avg:35.19ms
step:239/1845 train_time:8410ms step_avg:35.19ms
step:240/1845 train_time:8444ms step_avg:35.18ms
step:241/1845 train_time:8478ms step_avg:35.18ms
step:242/1845 train_time:8512ms step_avg:35.18ms
step:243/1845 train_time:8547ms step_avg:35.17ms
step:244/1845 train_time:8581ms step_avg:35.17ms
step:245/1845 train_time:8616ms step_avg:35.17ms
step:246/1845 train_time:8650ms step_avg:35.16ms
step:247/1845 train_time:8684ms step_avg:35.16ms
step:248/1845 train_time:8718ms step_avg:35.15ms
step:249/1845 train_time:8753ms step_avg:35.15ms
step:250/1845 train_time:8787ms step_avg:35.15ms
step:250/1845 val_loss:4.6200 train_time:8824ms step_avg:35.29ms
step:251/1845 train_time:8843ms step_avg:35.23ms
step:252/1845 train_time:8864ms step_avg:35.17ms
step:253/1845 train_time:8894ms step_avg:35.15ms
step:254/1845 train_time:8928ms step_avg:35.15ms
step:255/1845 train_time:8963ms step_avg:35.15ms
step:256/1845 train_time:8998ms step_avg:35.15ms
step:257/1845 train_time:9032ms step_avg:35.15ms
step:258/1845 train_time:9066ms step_avg:35.14ms
step:259/1845 train_time:9101ms step_avg:35.14ms
step:260/1845 train_time:9135ms step_avg:35.14ms
step:261/1845 train_time:9170ms step_avg:35.13ms
step:262/1845 train_time:9204ms step_avg:35.13ms
step:263/1845 train_time:9238ms step_avg:35.13ms
step:264/1845 train_time:9272ms step_avg:35.12ms
step:265/1845 train_time:9306ms step_avg:35.12ms
step:266/1845 train_time:9340ms step_avg:35.11ms
step:267/1845 train_time:9375ms step_avg:35.11ms
step:268/1845 train_time:9408ms step_avg:35.11ms
step:269/1845 train_time:9443ms step_avg:35.10ms
step:270/1845 train_time:9477ms step_avg:35.10ms
step:271/1845 train_time:9511ms step_avg:35.10ms
step:272/1845 train_time:9545ms step_avg:35.09ms
step:273/1845 train_time:9579ms step_avg:35.09ms
step:274/1845 train_time:9613ms step_avg:35.08ms
step:275/1845 train_time:9647ms step_avg:35.08ms
step:276/1845 train_time:9681ms step_avg:35.08ms
step:277/1845 train_time:9716ms step_avg:35.07ms
step:278/1845 train_time:9750ms step_avg:35.07ms
step:279/1845 train_time:9784ms step_avg:35.07ms
step:280/1845 train_time:9818ms step_avg:35.06ms
step:281/1845 train_time:9852ms step_avg:35.06ms
step:282/1845 train_time:9886ms step_avg:35.06ms
step:283/1845 train_time:9921ms step_avg:35.06ms
step:284/1845 train_time:9955ms step_avg:35.05ms
step:285/1845 train_time:9989ms step_avg:35.05ms
step:286/1845 train_time:10023ms step_avg:35.05ms
step:287/1845 train_time:10058ms step_avg:35.05ms
step:288/1845 train_time:10092ms step_avg:35.04ms
step:289/1845 train_time:10127ms step_avg:35.04ms
step:290/1845 train_time:10161ms step_avg:35.04ms
step:291/1845 train_time:10195ms step_avg:35.03ms
step:292/1845 train_time:10229ms step_avg:35.03ms
step:293/1845 train_time:10263ms step_avg:35.03ms
step:294/1845 train_time:10297ms step_avg:35.03ms
step:295/1845 train_time:10332ms step_avg:35.02ms
step:296/1845 train_time:10366ms step_avg:35.02ms
step:297/1845 train_time:10400ms step_avg:35.02ms
step:298/1845 train_time:10434ms step_avg:35.01ms
step:299/1845 train_time:10468ms step_avg:35.01ms
step:300/1845 train_time:10502ms step_avg:35.01ms
step:301/1845 train_time:10537ms step_avg:35.01ms
step:302/1845 train_time:10571ms step_avg:35.00ms
step:303/1845 train_time:10605ms step_avg:35.00ms
step:304/1845 train_time:10639ms step_avg:35.00ms
step:305/1845 train_time:10674ms step_avg:35.00ms
step:306/1845 train_time:10708ms step_avg:34.99ms
step:307/1845 train_time:10742ms step_avg:34.99ms
step:308/1845 train_time:10776ms step_avg:34.99ms
step:309/1845 train_time:10811ms step_avg:34.99ms
step:310/1845 train_time:10845ms step_avg:34.98ms
step:311/1845 train_time:10879ms step_avg:34.98ms
step:312/1845 train_time:10913ms step_avg:34.98ms
step:313/1845 train_time:10947ms step_avg:34.98ms
step:314/1845 train_time:10981ms step_avg:34.97ms
step:315/1845 train_time:11016ms step_avg:34.97ms
step:316/1845 train_time:11050ms step_avg:34.97ms
step:317/1845 train_time:11085ms step_avg:34.97ms
step:318/1845 train_time:11119ms step_avg:34.96ms
step:319/1845 train_time:11153ms step_avg:34.96ms
step:320/1845 train_time:11187ms step_avg:34.96ms
step:321/1845 train_time:11221ms step_avg:34.96ms
step:322/1845 train_time:11256ms step_avg:34.96ms
step:323/1845 train_time:11289ms step_avg:34.95ms
step:324/1845 train_time:11324ms step_avg:34.95ms
step:325/1845 train_time:11358ms step_avg:34.95ms
step:326/1845 train_time:11392ms step_avg:34.94ms
step:327/1845 train_time:11426ms step_avg:34.94ms
step:328/1845 train_time:11460ms step_avg:34.94ms
step:329/1845 train_time:11495ms step_avg:34.94ms
step:330/1845 train_time:11529ms step_avg:34.94ms
step:331/1845 train_time:11563ms step_avg:34.93ms
step:332/1845 train_time:11597ms step_avg:34.93ms
step:333/1845 train_time:11631ms step_avg:34.93ms
step:334/1845 train_time:11665ms step_avg:34.93ms
step:335/1845 train_time:11700ms step_avg:34.92ms
step:336/1845 train_time:11734ms step_avg:34.92ms
step:337/1845 train_time:11768ms step_avg:34.92ms
step:338/1845 train_time:11802ms step_avg:34.92ms
step:339/1845 train_time:11837ms step_avg:34.92ms
step:340/1845 train_time:11871ms step_avg:34.91ms
step:341/1845 train_time:11905ms step_avg:34.91ms
step:342/1845 train_time:11939ms step_avg:34.91ms
step:343/1845 train_time:11974ms step_avg:34.91ms
step:344/1845 train_time:12008ms step_avg:34.91ms
step:345/1845 train_time:12042ms step_avg:34.90ms
step:346/1845 train_time:12076ms step_avg:34.90ms
step:347/1845 train_time:12110ms step_avg:34.90ms
step:348/1845 train_time:12144ms step_avg:34.90ms
step:349/1845 train_time:12179ms step_avg:34.90ms
step:350/1845 train_time:12213ms step_avg:34.89ms
step:351/1845 train_time:12248ms step_avg:34.89ms
step:352/1845 train_time:12281ms step_avg:34.89ms
step:353/1845 train_time:12316ms step_avg:34.89ms
step:354/1845 train_time:12350ms step_avg:34.89ms
step:355/1845 train_time:12385ms step_avg:34.89ms
step:356/1845 train_time:12419ms step_avg:34.88ms
step:357/1845 train_time:12453ms step_avg:34.88ms
step:358/1845 train_time:12487ms step_avg:34.88ms
step:359/1845 train_time:12521ms step_avg:34.88ms
step:360/1845 train_time:12555ms step_avg:34.88ms
step:361/1845 train_time:12590ms step_avg:34.88ms
step:362/1845 train_time:12624ms step_avg:34.87ms
step:363/1845 train_time:12659ms step_avg:34.87ms
step:364/1845 train_time:12692ms step_avg:34.87ms
step:365/1845 train_time:12727ms step_avg:34.87ms
step:366/1845 train_time:12761ms step_avg:34.87ms
step:367/1845 train_time:12795ms step_avg:34.86ms
step:368/1845 train_time:12829ms step_avg:34.86ms
step:369/1845 train_time:12863ms step_avg:34.86ms
step:370/1845 train_time:12897ms step_avg:34.86ms
step:371/1845 train_time:12932ms step_avg:34.86ms
step:372/1845 train_time:12966ms step_avg:34.85ms
step:373/1845 train_time:13000ms step_avg:34.85ms
step:374/1845 train_time:13034ms step_avg:34.85ms
step:375/1845 train_time:13069ms step_avg:34.85ms
step:376/1845 train_time:13103ms step_avg:34.85ms
step:377/1845 train_time:13137ms step_avg:34.85ms
step:378/1845 train_time:13171ms step_avg:34.84ms
step:379/1845 train_time:13206ms step_avg:34.84ms
step:380/1845 train_time:13239ms step_avg:34.84ms
step:381/1845 train_time:13274ms step_avg:34.84ms
step:382/1845 train_time:13308ms step_avg:34.84ms
step:383/1845 train_time:13342ms step_avg:34.84ms
step:384/1845 train_time:13376ms step_avg:34.83ms
step:385/1845 train_time:13410ms step_avg:34.83ms
step:386/1845 train_time:13444ms step_avg:34.83ms
step:387/1845 train_time:13479ms step_avg:34.83ms
step:388/1845 train_time:13513ms step_avg:34.83ms
step:389/1845 train_time:13547ms step_avg:34.83ms
step:390/1845 train_time:13581ms step_avg:34.82ms
step:391/1845 train_time:13616ms step_avg:34.82ms
step:392/1845 train_time:13650ms step_avg:34.82ms
step:393/1845 train_time:13684ms step_avg:34.82ms
step:394/1845 train_time:13718ms step_avg:34.82ms
step:395/1845 train_time:13753ms step_avg:34.82ms
step:396/1845 train_time:13787ms step_avg:34.82ms
step:397/1845 train_time:13821ms step_avg:34.81ms
step:398/1845 train_time:13855ms step_avg:34.81ms
step:399/1845 train_time:13890ms step_avg:34.81ms
step:400/1845 train_time:13924ms step_avg:34.81ms
step:401/1845 train_time:13958ms step_avg:34.81ms
step:402/1845 train_time:13992ms step_avg:34.81ms
step:403/1845 train_time:14026ms step_avg:34.81ms
step:404/1845 train_time:14060ms step_avg:34.80ms
step:405/1845 train_time:14095ms step_avg:34.80ms
step:406/1845 train_time:14129ms step_avg:34.80ms
step:407/1845 train_time:14163ms step_avg:34.80ms
step:408/1845 train_time:14197ms step_avg:34.80ms
step:409/1845 train_time:14231ms step_avg:34.80ms
step:410/1845 train_time:14265ms step_avg:34.79ms
step:411/1845 train_time:14299ms step_avg:34.79ms
step:412/1845 train_time:14333ms step_avg:34.79ms
step:413/1845 train_time:14368ms step_avg:34.79ms
step:414/1845 train_time:14402ms step_avg:34.79ms
step:415/1845 train_time:14436ms step_avg:34.79ms
step:416/1845 train_time:14470ms step_avg:34.78ms
step:417/1845 train_time:14504ms step_avg:34.78ms
step:418/1845 train_time:14538ms step_avg:34.78ms
step:419/1845 train_time:14573ms step_avg:34.78ms
step:420/1845 train_time:14607ms step_avg:34.78ms
step:421/1845 train_time:14641ms step_avg:34.78ms
step:422/1845 train_time:14675ms step_avg:34.77ms
step:423/1845 train_time:14709ms step_avg:34.77ms
step:424/1845 train_time:14743ms step_avg:34.77ms
step:425/1845 train_time:14777ms step_avg:34.77ms
step:426/1845 train_time:14811ms step_avg:34.77ms
step:427/1845 train_time:14846ms step_avg:34.77ms
step:428/1845 train_time:14880ms step_avg:34.77ms
step:429/1845 train_time:14914ms step_avg:34.77ms
step:430/1845 train_time:14948ms step_avg:34.76ms
step:431/1845 train_time:14983ms step_avg:34.76ms
step:432/1845 train_time:15016ms step_avg:34.76ms
step:433/1845 train_time:15051ms step_avg:34.76ms
step:434/1845 train_time:15085ms step_avg:34.76ms
step:435/1845 train_time:15119ms step_avg:34.76ms
step:436/1845 train_time:15153ms step_avg:34.76ms
step:437/1845 train_time:15188ms step_avg:34.75ms
step:438/1845 train_time:15222ms step_avg:34.75ms
step:439/1845 train_time:15256ms step_avg:34.75ms
step:440/1845 train_time:15290ms step_avg:34.75ms
step:441/1845 train_time:15325ms step_avg:34.75ms
step:442/1845 train_time:15359ms step_avg:34.75ms
step:443/1845 train_time:15393ms step_avg:34.75ms
step:444/1845 train_time:15427ms step_avg:34.75ms
step:445/1845 train_time:15462ms step_avg:34.75ms
step:446/1845 train_time:15496ms step_avg:34.74ms
step:447/1845 train_time:15530ms step_avg:34.74ms
step:448/1845 train_time:15564ms step_avg:34.74ms
step:449/1845 train_time:15599ms step_avg:34.74ms
step:450/1845 train_time:15632ms step_avg:34.74ms
step:451/1845 train_time:15667ms step_avg:34.74ms
step:452/1845 train_time:15701ms step_avg:34.74ms
step:453/1845 train_time:15735ms step_avg:34.74ms
step:454/1845 train_time:15769ms step_avg:34.73ms
step:455/1845 train_time:15804ms step_avg:34.73ms
step:456/1845 train_time:15838ms step_avg:34.73ms
step:457/1845 train_time:15872ms step_avg:34.73ms
step:458/1845 train_time:15906ms step_avg:34.73ms
step:459/1845 train_time:15940ms step_avg:34.73ms
step:460/1845 train_time:15974ms step_avg:34.73ms
step:461/1845 train_time:16008ms step_avg:34.72ms
step:462/1845 train_time:16042ms step_avg:34.72ms
step:463/1845 train_time:16076ms step_avg:34.72ms
step:464/1845 train_time:16110ms step_avg:34.72ms
step:465/1845 train_time:16145ms step_avg:34.72ms
step:466/1845 train_time:16179ms step_avg:34.72ms
step:467/1845 train_time:16213ms step_avg:34.72ms
step:468/1845 train_time:16247ms step_avg:34.72ms
step:469/1845 train_time:16282ms step_avg:34.72ms
step:470/1845 train_time:16316ms step_avg:34.71ms
step:471/1845 train_time:16350ms step_avg:34.71ms
step:472/1845 train_time:16384ms step_avg:34.71ms
step:473/1845 train_time:16419ms step_avg:34.71ms
step:474/1845 train_time:16453ms step_avg:34.71ms
step:475/1845 train_time:16487ms step_avg:34.71ms
step:476/1845 train_time:16521ms step_avg:34.71ms
step:477/1845 train_time:16555ms step_avg:34.71ms
step:478/1845 train_time:16589ms step_avg:34.71ms
step:479/1845 train_time:16624ms step_avg:34.70ms
step:480/1845 train_time:16658ms step_avg:34.70ms
step:481/1845 train_time:16692ms step_avg:34.70ms
step:482/1845 train_time:16726ms step_avg:34.70ms
step:483/1845 train_time:16760ms step_avg:34.70ms
step:484/1845 train_time:16794ms step_avg:34.70ms
step:485/1845 train_time:16829ms step_avg:34.70ms
step:486/1845 train_time:16863ms step_avg:34.70ms
step:487/1845 train_time:16897ms step_avg:34.70ms
step:488/1845 train_time:16931ms step_avg:34.69ms
step:489/1845 train_time:16965ms step_avg:34.69ms
step:490/1845 train_time:16999ms step_avg:34.69ms
step:491/1845 train_time:17034ms step_avg:34.69ms
step:492/1845 train_time:17068ms step_avg:34.69ms
step:493/1845 train_time:17102ms step_avg:34.69ms
step:494/1845 train_time:17136ms step_avg:34.69ms
step:495/1845 train_time:17170ms step_avg:34.69ms
step:496/1845 train_time:17204ms step_avg:34.69ms
step:497/1845 train_time:17238ms step_avg:34.69ms
step:498/1845 train_time:17273ms step_avg:34.68ms
step:499/1845 train_time:17307ms step_avg:34.68ms
step:500/1845 train_time:17341ms step_avg:34.68ms
step:500/1845 val_loss:4.2807 train_time:17377ms step_avg:34.75ms
step:501/1845 train_time:17401ms step_avg:34.73ms
step:502/1845 train_time:17421ms step_avg:34.70ms
step:503/1845 train_time:17448ms step_avg:34.69ms
step:504/1845 train_time:17482ms step_avg:34.69ms
step:505/1845 train_time:17519ms step_avg:34.69ms
step:506/1845 train_time:17553ms step_avg:34.69ms
step:507/1845 train_time:17588ms step_avg:34.69ms
step:508/1845 train_time:17622ms step_avg:34.69ms
step:509/1845 train_time:17657ms step_avg:34.69ms
step:510/1845 train_time:17691ms step_avg:34.69ms
step:511/1845 train_time:17725ms step_avg:34.69ms
step:512/1845 train_time:17759ms step_avg:34.69ms
step:513/1845 train_time:17793ms step_avg:34.68ms
step:514/1845 train_time:17827ms step_avg:34.68ms
step:515/1845 train_time:17861ms step_avg:34.68ms
step:516/1845 train_time:17895ms step_avg:34.68ms
step:517/1845 train_time:17930ms step_avg:34.68ms
step:518/1845 train_time:17964ms step_avg:34.68ms
step:519/1845 train_time:17999ms step_avg:34.68ms
step:520/1845 train_time:18032ms step_avg:34.68ms
step:521/1845 train_time:18067ms step_avg:34.68ms
step:522/1845 train_time:18101ms step_avg:34.68ms
step:523/1845 train_time:18135ms step_avg:34.67ms
step:524/1845 train_time:18169ms step_avg:34.67ms
step:525/1845 train_time:18203ms step_avg:34.67ms
step:526/1845 train_time:18237ms step_avg:34.67ms
step:527/1845 train_time:18272ms step_avg:34.67ms
step:528/1845 train_time:18306ms step_avg:34.67ms
step:529/1845 train_time:18340ms step_avg:34.67ms
step:530/1845 train_time:18374ms step_avg:34.67ms
step:531/1845 train_time:18409ms step_avg:34.67ms
step:532/1845 train_time:18443ms step_avg:34.67ms
step:533/1845 train_time:18478ms step_avg:34.67ms
step:534/1845 train_time:18512ms step_avg:34.67ms
step:535/1845 train_time:18547ms step_avg:34.67ms
step:536/1845 train_time:18581ms step_avg:34.67ms
step:537/1845 train_time:18616ms step_avg:34.67ms
step:538/1845 train_time:18650ms step_avg:34.67ms
step:539/1845 train_time:18685ms step_avg:34.67ms
step:540/1845 train_time:18719ms step_avg:34.67ms
step:541/1845 train_time:18754ms step_avg:34.67ms
step:542/1845 train_time:18788ms step_avg:34.66ms
step:543/1845 train_time:18822ms step_avg:34.66ms
step:544/1845 train_time:18856ms step_avg:34.66ms
step:545/1845 train_time:18891ms step_avg:34.66ms
step:546/1845 train_time:18925ms step_avg:34.66ms
step:547/1845 train_time:18959ms step_avg:34.66ms
step:548/1845 train_time:18993ms step_avg:34.66ms
step:549/1845 train_time:19028ms step_avg:34.66ms
step:550/1845 train_time:19062ms step_avg:34.66ms
step:551/1845 train_time:19096ms step_avg:34.66ms
step:552/1845 train_time:19130ms step_avg:34.66ms
step:553/1845 train_time:19164ms step_avg:34.66ms
step:554/1845 train_time:19198ms step_avg:34.65ms
step:555/1845 train_time:19233ms step_avg:34.65ms
step:556/1845 train_time:19267ms step_avg:34.65ms
step:557/1845 train_time:19301ms step_avg:34.65ms
step:558/1845 train_time:19335ms step_avg:34.65ms
step:559/1845 train_time:19370ms step_avg:34.65ms
step:560/1845 train_time:19404ms step_avg:34.65ms
step:561/1845 train_time:19438ms step_avg:34.65ms
step:562/1845 train_time:19472ms step_avg:34.65ms
step:563/1845 train_time:19507ms step_avg:34.65ms
step:564/1845 train_time:19541ms step_avg:34.65ms
step:565/1845 train_time:19576ms step_avg:34.65ms
step:566/1845 train_time:19610ms step_avg:34.65ms
step:567/1845 train_time:19644ms step_avg:34.65ms
step:568/1845 train_time:19678ms step_avg:34.65ms
step:569/1845 train_time:19713ms step_avg:34.65ms
step:570/1845 train_time:19747ms step_avg:34.64ms
step:571/1845 train_time:19782ms step_avg:34.64ms
step:572/1845 train_time:19816ms step_avg:34.64ms
step:573/1845 train_time:19850ms step_avg:34.64ms
step:574/1845 train_time:19884ms step_avg:34.64ms
step:575/1845 train_time:19919ms step_avg:34.64ms
step:576/1845 train_time:19953ms step_avg:34.64ms
step:577/1845 train_time:19987ms step_avg:34.64ms
step:578/1845 train_time:20021ms step_avg:34.64ms
step:579/1845 train_time:20056ms step_avg:34.64ms
step:580/1845 train_time:20090ms step_avg:34.64ms
step:581/1845 train_time:20124ms step_avg:34.64ms
step:582/1845 train_time:20158ms step_avg:34.64ms
step:583/1845 train_time:20193ms step_avg:34.64ms
step:584/1845 train_time:20227ms step_avg:34.63ms
step:585/1845 train_time:20262ms step_avg:34.64ms
step:586/1845 train_time:20295ms step_avg:34.63ms
step:587/1845 train_time:20330ms step_avg:34.63ms
step:588/1845 train_time:20364ms step_avg:34.63ms
step:589/1845 train_time:20398ms step_avg:34.63ms
step:590/1845 train_time:20432ms step_avg:34.63ms
step:591/1845 train_time:20467ms step_avg:34.63ms
step:592/1845 train_time:20501ms step_avg:34.63ms
step:593/1845 train_time:20535ms step_avg:34.63ms
step:594/1845 train_time:20569ms step_avg:34.63ms
step:595/1845 train_time:20604ms step_avg:34.63ms
step:596/1845 train_time:20638ms step_avg:34.63ms
step:597/1845 train_time:20673ms step_avg:34.63ms
step:598/1845 train_time:20707ms step_avg:34.63ms
step:599/1845 train_time:20741ms step_avg:34.63ms
step:600/1845 train_time:20775ms step_avg:34.63ms
step:601/1845 train_time:20810ms step_avg:34.63ms
step:602/1845 train_time:20844ms step_avg:34.62ms
step:603/1845 train_time:20880ms step_avg:34.63ms
step:604/1845 train_time:20940ms step_avg:34.67ms
step:605/1845 train_time:21002ms step_avg:34.71ms
step:606/1845 train_time:21063ms step_avg:34.76ms
step:607/1845 train_time:21125ms step_avg:34.80ms
step:608/1845 train_time:21186ms step_avg:34.85ms
step:609/1845 train_time:21250ms step_avg:34.89ms
step:610/1845 train_time:21311ms step_avg:34.94ms
step:611/1845 train_time:21373ms step_avg:34.98ms
step:612/1845 train_time:21434ms step_avg:35.02ms
step:613/1845 train_time:21498ms step_avg:35.07ms
step:614/1845 train_time:21559ms step_avg:35.11ms
step:615/1845 train_time:21622ms step_avg:35.16ms
step:616/1845 train_time:21683ms step_avg:35.20ms
step:617/1845 train_time:21746ms step_avg:35.24ms
step:618/1845 train_time:21807ms step_avg:35.29ms
step:619/1845 train_time:21869ms step_avg:35.33ms
step:620/1845 train_time:21930ms step_avg:35.37ms
step:621/1845 train_time:21992ms step_avg:35.41ms
step:622/1845 train_time:22053ms step_avg:35.45ms
step:623/1845 train_time:22115ms step_avg:35.50ms
step:624/1845 train_time:22177ms step_avg:35.54ms
step:625/1845 train_time:22240ms step_avg:35.58ms
step:626/1845 train_time:22301ms step_avg:35.62ms
step:627/1845 train_time:22364ms step_avg:35.67ms
step:628/1845 train_time:22425ms step_avg:35.71ms
step:629/1845 train_time:22488ms step_avg:35.75ms
step:630/1845 train_time:22548ms step_avg:35.79ms
step:631/1845 train_time:22612ms step_avg:35.83ms
step:632/1845 train_time:22671ms step_avg:35.87ms
step:633/1845 train_time:22734ms step_avg:35.92ms
step:634/1845 train_time:22796ms step_avg:35.96ms
step:635/1845 train_time:22859ms step_avg:36.00ms
step:636/1845 train_time:22920ms step_avg:36.04ms
step:637/1845 train_time:22982ms step_avg:36.08ms
step:638/1845 train_time:23043ms step_avg:36.12ms
step:639/1845 train_time:23105ms step_avg:36.16ms
step:640/1845 train_time:23166ms step_avg:36.20ms
step:641/1845 train_time:23228ms step_avg:36.24ms
step:642/1845 train_time:23289ms step_avg:36.28ms
step:643/1845 train_time:23352ms step_avg:36.32ms
step:644/1845 train_time:23413ms step_avg:36.36ms
step:645/1845 train_time:23476ms step_avg:36.40ms
step:646/1845 train_time:23538ms step_avg:36.44ms
step:647/1845 train_time:23601ms step_avg:36.48ms
step:648/1845 train_time:23662ms step_avg:36.51ms
step:649/1845 train_time:23724ms step_avg:36.55ms
step:650/1845 train_time:23785ms step_avg:36.59ms
step:651/1845 train_time:23848ms step_avg:36.63ms
step:652/1845 train_time:23909ms step_avg:36.67ms
step:653/1845 train_time:23971ms step_avg:36.71ms
step:654/1845 train_time:24031ms step_avg:36.75ms
step:655/1845 train_time:24094ms step_avg:36.79ms
step:656/1845 train_time:24156ms step_avg:36.82ms
step:657/1845 train_time:24218ms step_avg:36.86ms
step:658/1845 train_time:24279ms step_avg:36.90ms
step:659/1845 train_time:24343ms step_avg:36.94ms
step:660/1845 train_time:24404ms step_avg:36.98ms
step:661/1845 train_time:24466ms step_avg:37.01ms
step:662/1845 train_time:24527ms step_avg:37.05ms
step:663/1845 train_time:24590ms step_avg:37.09ms
step:664/1845 train_time:24650ms step_avg:37.12ms
step:665/1845 train_time:24713ms step_avg:37.16ms
step:666/1845 train_time:24774ms step_avg:37.20ms
step:667/1845 train_time:24838ms step_avg:37.24ms
step:668/1845 train_time:24899ms step_avg:37.27ms
step:669/1845 train_time:24961ms step_avg:37.31ms
step:670/1845 train_time:25022ms step_avg:37.35ms
step:671/1845 train_time:25084ms step_avg:37.38ms
step:672/1845 train_time:25145ms step_avg:37.42ms
step:673/1845 train_time:25207ms step_avg:37.46ms
step:674/1845 train_time:25268ms step_avg:37.49ms
step:675/1845 train_time:25330ms step_avg:37.53ms
step:676/1845 train_time:25391ms step_avg:37.56ms
step:677/1845 train_time:25455ms step_avg:37.60ms
step:678/1845 train_time:25517ms step_avg:37.64ms
step:679/1845 train_time:25580ms step_avg:37.67ms
step:680/1845 train_time:25641ms step_avg:37.71ms
step:681/1845 train_time:25704ms step_avg:37.74ms
step:682/1845 train_time:25764ms step_avg:37.78ms
step:683/1845 train_time:25827ms step_avg:37.81ms
step:684/1845 train_time:25888ms step_avg:37.85ms
step:685/1845 train_time:25951ms step_avg:37.89ms
step:686/1845 train_time:26012ms step_avg:37.92ms
step:687/1845 train_time:26075ms step_avg:37.96ms
step:688/1845 train_time:26136ms step_avg:37.99ms
step:689/1845 train_time:26199ms step_avg:38.03ms
step:690/1845 train_time:26260ms step_avg:38.06ms
step:691/1845 train_time:26322ms step_avg:38.09ms
step:692/1845 train_time:26383ms step_avg:38.13ms
step:693/1845 train_time:26446ms step_avg:38.16ms
step:694/1845 train_time:26507ms step_avg:38.19ms
step:695/1845 train_time:26570ms step_avg:38.23ms
step:696/1845 train_time:26631ms step_avg:38.26ms
step:697/1845 train_time:26694ms step_avg:38.30ms
step:698/1845 train_time:26755ms step_avg:38.33ms
step:699/1845 train_time:26818ms step_avg:38.37ms
step:700/1845 train_time:26879ms step_avg:38.40ms
step:701/1845 train_time:26942ms step_avg:38.43ms
step:702/1845 train_time:27003ms step_avg:38.47ms
step:703/1845 train_time:27065ms step_avg:38.50ms
step:704/1845 train_time:27126ms step_avg:38.53ms
step:705/1845 train_time:27189ms step_avg:38.57ms
step:706/1845 train_time:27249ms step_avg:38.60ms
step:707/1845 train_time:27312ms step_avg:38.63ms
step:708/1845 train_time:27373ms step_avg:38.66ms
step:709/1845 train_time:27436ms step_avg:38.70ms
step:710/1845 train_time:27497ms step_avg:38.73ms
step:711/1845 train_time:27560ms step_avg:38.76ms
step:712/1845 train_time:27621ms step_avg:38.79ms
step:713/1845 train_time:27684ms step_avg:38.83ms
step:714/1845 train_time:27743ms step_avg:38.86ms
step:715/1845 train_time:27806ms step_avg:38.89ms
step:716/1845 train_time:27867ms step_avg:38.92ms
step:717/1845 train_time:27929ms step_avg:38.95ms
step:718/1845 train_time:27990ms step_avg:38.98ms
step:719/1845 train_time:28053ms step_avg:39.02ms
step:720/1845 train_time:28114ms step_avg:39.05ms
step:721/1845 train_time:28178ms step_avg:39.08ms
step:722/1845 train_time:28239ms step_avg:39.11ms
step:723/1845 train_time:28302ms step_avg:39.15ms
step:724/1845 train_time:28363ms step_avg:39.18ms
step:725/1845 train_time:28425ms step_avg:39.21ms
step:726/1845 train_time:28486ms step_avg:39.24ms
step:727/1845 train_time:28549ms step_avg:39.27ms
step:728/1845 train_time:28610ms step_avg:39.30ms
step:729/1845 train_time:28673ms step_avg:39.33ms
step:730/1845 train_time:28734ms step_avg:39.36ms
step:731/1845 train_time:28797ms step_avg:39.39ms
step:732/1845 train_time:28858ms step_avg:39.42ms
step:733/1845 train_time:28921ms step_avg:39.46ms
step:734/1845 train_time:28982ms step_avg:39.48ms
step:735/1845 train_time:29044ms step_avg:39.52ms
step:736/1845 train_time:29105ms step_avg:39.54ms
step:737/1845 train_time:29167ms step_avg:39.58ms
step:738/1845 train_time:29228ms step_avg:39.60ms
step:739/1845 train_time:29291ms step_avg:39.64ms
step:740/1845 train_time:29352ms step_avg:39.66ms
step:741/1845 train_time:29415ms step_avg:39.70ms
step:742/1845 train_time:29476ms step_avg:39.72ms
step:743/1845 train_time:29539ms step_avg:39.76ms
step:744/1845 train_time:29599ms step_avg:39.78ms
step:745/1845 train_time:29662ms step_avg:39.81ms
step:746/1845 train_time:29723ms step_avg:39.84ms
step:747/1845 train_time:29786ms step_avg:39.87ms
step:748/1845 train_time:29846ms step_avg:39.90ms
step:749/1845 train_time:29909ms step_avg:39.93ms
step:750/1845 train_time:29970ms step_avg:39.96ms
step:750/1845 val_loss:4.0266 train_time:30034ms step_avg:40.05ms
step:751/1845 train_time:30059ms step_avg:40.02ms
step:752/1845 train_time:30096ms step_avg:40.02ms
step:753/1845 train_time:30160ms step_avg:40.05ms
step:754/1845 train_time:30223ms step_avg:40.08ms
step:755/1845 train_time:30285ms step_avg:40.11ms
step:756/1845 train_time:30347ms step_avg:40.14ms
step:757/1845 train_time:30408ms step_avg:40.17ms
step:758/1845 train_time:30468ms step_avg:40.20ms
step:759/1845 train_time:30530ms step_avg:40.22ms
step:760/1845 train_time:30591ms step_avg:40.25ms
step:761/1845 train_time:30653ms step_avg:40.28ms
step:762/1845 train_time:30713ms step_avg:40.31ms
step:763/1845 train_time:30776ms step_avg:40.34ms
step:764/1845 train_time:30836ms step_avg:40.36ms
step:765/1845 train_time:30898ms step_avg:40.39ms
step:766/1845 train_time:30960ms step_avg:40.42ms
step:767/1845 train_time:31024ms step_avg:40.45ms
step:768/1845 train_time:31086ms step_avg:40.48ms
step:769/1845 train_time:31150ms step_avg:40.51ms
step:770/1845 train_time:31212ms step_avg:40.53ms
step:771/1845 train_time:31275ms step_avg:40.56ms
step:772/1845 train_time:31337ms step_avg:40.59ms
step:773/1845 train_time:31400ms step_avg:40.62ms
step:774/1845 train_time:31461ms step_avg:40.65ms
step:775/1845 train_time:31524ms step_avg:40.68ms
step:776/1845 train_time:31584ms step_avg:40.70ms
step:777/1845 train_time:31647ms step_avg:40.73ms
step:778/1845 train_time:31707ms step_avg:40.75ms
step:779/1845 train_time:31770ms step_avg:40.78ms
step:780/1845 train_time:31830ms step_avg:40.81ms
step:781/1845 train_time:31893ms step_avg:40.84ms
step:782/1845 train_time:31955ms step_avg:40.86ms
step:783/1845 train_time:32017ms step_avg:40.89ms
step:784/1845 train_time:32078ms step_avg:40.92ms
step:785/1845 train_time:32142ms step_avg:40.94ms
step:786/1845 train_time:32203ms step_avg:40.97ms
step:787/1845 train_time:32266ms step_avg:41.00ms
step:788/1845 train_time:32327ms step_avg:41.02ms
step:789/1845 train_time:32390ms step_avg:41.05ms
step:790/1845 train_time:32451ms step_avg:41.08ms
step:791/1845 train_time:32514ms step_avg:41.10ms
step:792/1845 train_time:32574ms step_avg:41.13ms
step:793/1845 train_time:32637ms step_avg:41.16ms
step:794/1845 train_time:32698ms step_avg:41.18ms
step:795/1845 train_time:32762ms step_avg:41.21ms
step:796/1845 train_time:32823ms step_avg:41.23ms
step:797/1845 train_time:32885ms step_avg:41.26ms
step:798/1845 train_time:32946ms step_avg:41.29ms
step:799/1845 train_time:33008ms step_avg:41.31ms
step:800/1845 train_time:33069ms step_avg:41.34ms
step:801/1845 train_time:33131ms step_avg:41.36ms
step:802/1845 train_time:33192ms step_avg:41.39ms
step:803/1845 train_time:33255ms step_avg:41.41ms
step:804/1845 train_time:33316ms step_avg:41.44ms
step:805/1845 train_time:33379ms step_avg:41.46ms
step:806/1845 train_time:33441ms step_avg:41.49ms
step:807/1845 train_time:33504ms step_avg:41.52ms
step:808/1845 train_time:33565ms step_avg:41.54ms
step:809/1845 train_time:33627ms step_avg:41.57ms
step:810/1845 train_time:33688ms step_avg:41.59ms
step:811/1845 train_time:33750ms step_avg:41.62ms
step:812/1845 train_time:33811ms step_avg:41.64ms
step:813/1845 train_time:33873ms step_avg:41.66ms
step:814/1845 train_time:33934ms step_avg:41.69ms
step:815/1845 train_time:33997ms step_avg:41.71ms
step:816/1845 train_time:34058ms step_avg:41.74ms
step:817/1845 train_time:34120ms step_avg:41.76ms
step:818/1845 train_time:34182ms step_avg:41.79ms
step:819/1845 train_time:34245ms step_avg:41.81ms
step:820/1845 train_time:34306ms step_avg:41.84ms
step:821/1845 train_time:34368ms step_avg:41.86ms
step:822/1845 train_time:34429ms step_avg:41.88ms
step:823/1845 train_time:34492ms step_avg:41.91ms
step:824/1845 train_time:34553ms step_avg:41.93ms
step:825/1845 train_time:34616ms step_avg:41.96ms
step:826/1845 train_time:34677ms step_avg:41.98ms
step:827/1845 train_time:34740ms step_avg:42.01ms
step:828/1845 train_time:34801ms step_avg:42.03ms
step:829/1845 train_time:34864ms step_avg:42.06ms
step:830/1845 train_time:34925ms step_avg:42.08ms
step:831/1845 train_time:34986ms step_avg:42.10ms
step:832/1845 train_time:35047ms step_avg:42.12ms
step:833/1845 train_time:35110ms step_avg:42.15ms
step:834/1845 train_time:35171ms step_avg:42.17ms
step:835/1845 train_time:35233ms step_avg:42.20ms
step:836/1845 train_time:35294ms step_avg:42.22ms
step:837/1845 train_time:35356ms step_avg:42.24ms
step:838/1845 train_time:35418ms step_avg:42.26ms
step:839/1845 train_time:35481ms step_avg:42.29ms
step:840/1845 train_time:35542ms step_avg:42.31ms
step:841/1845 train_time:35606ms step_avg:42.34ms
step:842/1845 train_time:35666ms step_avg:42.36ms
step:843/1845 train_time:35729ms step_avg:42.38ms
step:844/1845 train_time:35791ms step_avg:42.41ms
step:845/1845 train_time:35853ms step_avg:42.43ms
step:846/1845 train_time:35914ms step_avg:42.45ms
step:847/1845 train_time:35976ms step_avg:42.48ms
step:848/1845 train_time:36038ms step_avg:42.50ms
step:849/1845 train_time:36101ms step_avg:42.52ms
step:850/1845 train_time:36162ms step_avg:42.54ms
step:851/1845 train_time:36225ms step_avg:42.57ms
step:852/1845 train_time:36286ms step_avg:42.59ms
step:853/1845 train_time:36348ms step_avg:42.61ms
step:854/1845 train_time:36409ms step_avg:42.63ms
step:855/1845 train_time:36472ms step_avg:42.66ms
step:856/1845 train_time:36532ms step_avg:42.68ms
step:857/1845 train_time:36595ms step_avg:42.70ms
step:858/1845 train_time:36656ms step_avg:42.72ms
step:859/1845 train_time:36719ms step_avg:42.75ms
step:860/1845 train_time:36781ms step_avg:42.77ms
step:861/1845 train_time:36844ms step_avg:42.79ms
step:862/1845 train_time:36905ms step_avg:42.81ms
step:863/1845 train_time:36967ms step_avg:42.84ms
step:864/1845 train_time:37028ms step_avg:42.86ms
step:865/1845 train_time:37090ms step_avg:42.88ms
step:866/1845 train_time:37151ms step_avg:42.90ms
step:867/1845 train_time:37213ms step_avg:42.92ms
step:868/1845 train_time:37274ms step_avg:42.94ms
step:869/1845 train_time:37336ms step_avg:42.96ms
step:870/1845 train_time:37398ms step_avg:42.99ms
step:871/1845 train_time:37460ms step_avg:43.01ms
step:872/1845 train_time:37521ms step_avg:43.03ms
step:873/1845 train_time:37584ms step_avg:43.05ms
step:874/1845 train_time:37645ms step_avg:43.07ms
step:875/1845 train_time:37707ms step_avg:43.09ms
step:876/1845 train_time:37768ms step_avg:43.11ms
step:877/1845 train_time:37831ms step_avg:43.14ms
step:878/1845 train_time:37892ms step_avg:43.16ms
step:879/1845 train_time:37955ms step_avg:43.18ms
step:880/1845 train_time:38016ms step_avg:43.20ms
step:881/1845 train_time:38080ms step_avg:43.22ms
step:882/1845 train_time:38142ms step_avg:43.24ms
step:883/1845 train_time:38205ms step_avg:43.27ms
step:884/1845 train_time:38266ms step_avg:43.29ms
step:885/1845 train_time:38328ms step_avg:43.31ms
step:886/1845 train_time:38389ms step_avg:43.33ms
step:887/1845 train_time:38452ms step_avg:43.35ms
step:888/1845 train_time:38513ms step_avg:43.37ms
step:889/1845 train_time:38575ms step_avg:43.39ms
step:890/1845 train_time:38636ms step_avg:43.41ms
step:891/1845 train_time:38698ms step_avg:43.43ms
step:892/1845 train_time:38760ms step_avg:43.45ms
step:893/1845 train_time:38822ms step_avg:43.47ms
step:894/1845 train_time:38883ms step_avg:43.49ms
step:895/1845 train_time:38946ms step_avg:43.52ms
step:896/1845 train_time:39006ms step_avg:43.53ms
step:897/1845 train_time:39069ms step_avg:43.55ms
step:898/1845 train_time:39129ms step_avg:43.57ms
step:899/1845 train_time:39192ms step_avg:43.60ms
step:900/1845 train_time:39253ms step_avg:43.61ms
step:901/1845 train_time:39316ms step_avg:43.64ms
step:902/1845 train_time:39377ms step_avg:43.66ms
step:903/1845 train_time:39440ms step_avg:43.68ms
step:904/1845 train_time:39501ms step_avg:43.70ms
step:905/1845 train_time:39563ms step_avg:43.72ms
step:906/1845 train_time:39624ms step_avg:43.74ms
step:907/1845 train_time:39686ms step_avg:43.76ms
step:908/1845 train_time:39747ms step_avg:43.77ms
step:909/1845 train_time:39810ms step_avg:43.80ms
step:910/1845 train_time:39871ms step_avg:43.81ms
step:911/1845 train_time:39933ms step_avg:43.83ms
step:912/1845 train_time:39994ms step_avg:43.85ms
step:913/1845 train_time:40057ms step_avg:43.87ms
step:914/1845 train_time:40118ms step_avg:43.89ms
step:915/1845 train_time:40181ms step_avg:43.91ms
step:916/1845 train_time:40242ms step_avg:43.93ms
step:917/1845 train_time:40305ms step_avg:43.95ms
step:918/1845 train_time:40365ms step_avg:43.97ms
step:919/1845 train_time:40428ms step_avg:43.99ms
step:920/1845 train_time:40489ms step_avg:44.01ms
step:921/1845 train_time:40551ms step_avg:44.03ms
step:922/1845 train_time:40611ms step_avg:44.05ms
step:923/1845 train_time:40674ms step_avg:44.07ms
step:924/1845 train_time:40735ms step_avg:44.09ms
step:925/1845 train_time:40798ms step_avg:44.11ms
step:926/1845 train_time:40859ms step_avg:44.12ms
step:927/1845 train_time:40923ms step_avg:44.15ms
step:928/1845 train_time:40984ms step_avg:44.16ms
step:929/1845 train_time:41046ms step_avg:44.18ms
step:930/1845 train_time:41107ms step_avg:44.20ms
step:931/1845 train_time:41170ms step_avg:44.22ms
step:932/1845 train_time:41230ms step_avg:44.24ms
step:933/1845 train_time:41293ms step_avg:44.26ms
step:934/1845 train_time:41353ms step_avg:44.28ms
step:935/1845 train_time:41416ms step_avg:44.30ms
step:936/1845 train_time:41478ms step_avg:44.31ms
step:937/1845 train_time:41542ms step_avg:44.34ms
step:938/1845 train_time:41603ms step_avg:44.35ms
step:939/1845 train_time:41665ms step_avg:44.37ms
step:940/1845 train_time:41726ms step_avg:44.39ms
step:941/1845 train_time:41789ms step_avg:44.41ms
step:942/1845 train_time:41849ms step_avg:44.43ms
step:943/1845 train_time:41912ms step_avg:44.45ms
step:944/1845 train_time:41972ms step_avg:44.46ms
step:945/1845 train_time:42035ms step_avg:44.48ms
step:946/1845 train_time:42096ms step_avg:44.50ms
step:947/1845 train_time:42158ms step_avg:44.52ms
step:948/1845 train_time:42220ms step_avg:44.54ms
step:949/1845 train_time:42283ms step_avg:44.55ms
step:950/1845 train_time:42344ms step_avg:44.57ms
step:951/1845 train_time:42407ms step_avg:44.59ms
step:952/1845 train_time:42467ms step_avg:44.61ms
step:953/1845 train_time:42530ms step_avg:44.63ms
step:954/1845 train_time:42591ms step_avg:44.64ms
step:955/1845 train_time:42654ms step_avg:44.66ms
step:956/1845 train_time:42715ms step_avg:44.68ms
step:957/1845 train_time:42779ms step_avg:44.70ms
step:958/1845 train_time:42840ms step_avg:44.72ms
step:959/1845 train_time:42903ms step_avg:44.74ms
step:960/1845 train_time:42964ms step_avg:44.75ms
step:961/1845 train_time:43026ms step_avg:44.77ms
step:962/1845 train_time:43087ms step_avg:44.79ms
step:963/1845 train_time:43150ms step_avg:44.81ms
step:964/1845 train_time:43211ms step_avg:44.82ms
step:965/1845 train_time:43273ms step_avg:44.84ms
step:966/1845 train_time:43333ms step_avg:44.86ms
step:967/1845 train_time:43396ms step_avg:44.88ms
step:968/1845 train_time:43458ms step_avg:44.89ms
step:969/1845 train_time:43520ms step_avg:44.91ms
step:970/1845 train_time:43582ms step_avg:44.93ms
step:971/1845 train_time:43645ms step_avg:44.95ms
step:972/1845 train_time:43706ms step_avg:44.96ms
step:973/1845 train_time:43768ms step_avg:44.98ms
step:974/1845 train_time:43829ms step_avg:45.00ms
step:975/1845 train_time:43891ms step_avg:45.02ms
step:976/1845 train_time:43951ms step_avg:45.03ms
step:977/1845 train_time:44014ms step_avg:45.05ms
step:978/1845 train_time:44075ms step_avg:45.07ms
step:979/1845 train_time:44138ms step_avg:45.08ms
step:980/1845 train_time:44199ms step_avg:45.10ms
step:981/1845 train_time:44262ms step_avg:45.12ms
step:982/1845 train_time:44323ms step_avg:45.14ms
step:983/1845 train_time:44385ms step_avg:45.15ms
step:984/1845 train_time:44446ms step_avg:45.17ms
step:985/1845 train_time:44509ms step_avg:45.19ms
step:986/1845 train_time:44570ms step_avg:45.20ms
step:987/1845 train_time:44633ms step_avg:45.22ms
step:988/1845 train_time:44693ms step_avg:45.24ms
step:989/1845 train_time:44756ms step_avg:45.25ms
step:990/1845 train_time:44817ms step_avg:45.27ms
step:991/1845 train_time:44880ms step_avg:45.29ms
step:992/1845 train_time:44941ms step_avg:45.30ms
step:993/1845 train_time:45005ms step_avg:45.32ms
step:994/1845 train_time:45065ms step_avg:45.34ms
step:995/1845 train_time:45128ms step_avg:45.35ms
step:996/1845 train_time:45188ms step_avg:45.37ms
step:997/1845 train_time:45251ms step_avg:45.39ms
step:998/1845 train_time:45311ms step_avg:45.40ms
step:999/1845 train_time:45374ms step_avg:45.42ms
step:1000/1845 train_time:45435ms step_avg:45.43ms
step:1000/1845 val_loss:3.7712 train_time:45500ms step_avg:45.50ms
step:1001/1845 train_time:45521ms step_avg:45.48ms
step:1002/1845 train_time:45561ms step_avg:45.47ms
step:1003/1845 train_time:45625ms step_avg:45.49ms
step:1004/1845 train_time:45690ms step_avg:45.51ms
step:1005/1845 train_time:45751ms step_avg:45.52ms
step:1006/1845 train_time:45812ms step_avg:45.54ms
step:1007/1845 train_time:45874ms step_avg:45.56ms
step:1008/1845 train_time:45935ms step_avg:45.57ms
step:1009/1845 train_time:45997ms step_avg:45.59ms
step:1010/1845 train_time:46057ms step_avg:45.60ms
step:1011/1845 train_time:46119ms step_avg:45.62ms
step:1012/1845 train_time:46179ms step_avg:45.63ms
step:1013/1845 train_time:46242ms step_avg:45.65ms
step:1014/1845 train_time:46304ms step_avg:45.66ms
step:1015/1845 train_time:46366ms step_avg:45.68ms
step:1016/1845 train_time:46427ms step_avg:45.70ms
step:1017/1845 train_time:46490ms step_avg:45.71ms
step:1018/1845 train_time:46551ms step_avg:45.73ms
step:1019/1845 train_time:46614ms step_avg:45.74ms
step:1020/1845 train_time:46675ms step_avg:45.76ms
step:1021/1845 train_time:46738ms step_avg:45.78ms
step:1022/1845 train_time:46800ms step_avg:45.79ms
step:1023/1845 train_time:46862ms step_avg:45.81ms
step:1024/1845 train_time:46924ms step_avg:45.82ms
step:1025/1845 train_time:46986ms step_avg:45.84ms
step:1026/1845 train_time:47047ms step_avg:45.85ms
step:1027/1845 train_time:47109ms step_avg:45.87ms
step:1028/1845 train_time:47169ms step_avg:45.88ms
step:1029/1845 train_time:47231ms step_avg:45.90ms
step:1030/1845 train_time:47292ms step_avg:45.92ms
step:1031/1845 train_time:47355ms step_avg:45.93ms
step:1032/1845 train_time:47416ms step_avg:45.95ms
step:1033/1845 train_time:47478ms step_avg:45.96ms
step:1034/1845 train_time:47539ms step_avg:45.98ms
step:1035/1845 train_time:47603ms step_avg:45.99ms
step:1036/1845 train_time:47664ms step_avg:46.01ms
step:1037/1845 train_time:47727ms step_avg:46.02ms
step:1038/1845 train_time:47789ms step_avg:46.04ms
step:1039/1845 train_time:47852ms step_avg:46.06ms
step:1040/1845 train_time:47913ms step_avg:46.07ms
step:1041/1845 train_time:47975ms step_avg:46.09ms
step:1042/1845 train_time:48035ms step_avg:46.10ms
step:1043/1845 train_time:48098ms step_avg:46.11ms
step:1044/1845 train_time:48158ms step_avg:46.13ms
step:1045/1845 train_time:48221ms step_avg:46.14ms
step:1046/1845 train_time:48282ms step_avg:46.16ms
step:1047/1845 train_time:48345ms step_avg:46.17ms
step:1048/1845 train_time:48406ms step_avg:46.19ms
step:1049/1845 train_time:48469ms step_avg:46.20ms
step:1050/1845 train_time:48529ms step_avg:46.22ms
step:1051/1845 train_time:48592ms step_avg:46.23ms
step:1052/1845 train_time:48653ms step_avg:46.25ms
step:1053/1845 train_time:48715ms step_avg:46.26ms
step:1054/1845 train_time:48776ms step_avg:46.28ms
step:1055/1845 train_time:48839ms step_avg:46.29ms
step:1056/1845 train_time:48901ms step_avg:46.31ms
step:1057/1845 train_time:48963ms step_avg:46.32ms
step:1058/1845 train_time:49024ms step_avg:46.34ms
step:1059/1845 train_time:49086ms step_avg:46.35ms
step:1060/1845 train_time:49147ms step_avg:46.36ms
step:1061/1845 train_time:49209ms step_avg:46.38ms
step:1062/1845 train_time:49269ms step_avg:46.39ms
step:1063/1845 train_time:49332ms step_avg:46.41ms
step:1064/1845 train_time:49393ms step_avg:46.42ms
step:1065/1845 train_time:49456ms step_avg:46.44ms
step:1066/1845 train_time:49516ms step_avg:46.45ms
step:1067/1845 train_time:49579ms step_avg:46.47ms
step:1068/1845 train_time:49640ms step_avg:46.48ms
step:1069/1845 train_time:49702ms step_avg:46.49ms
step:1070/1845 train_time:49763ms step_avg:46.51ms
step:1071/1845 train_time:49826ms step_avg:46.52ms
step:1072/1845 train_time:49887ms step_avg:46.54ms
step:1073/1845 train_time:49950ms step_avg:46.55ms
step:1074/1845 train_time:50011ms step_avg:46.57ms
step:1075/1845 train_time:50074ms step_avg:46.58ms
step:1076/1845 train_time:50135ms step_avg:46.59ms
step:1077/1845 train_time:50196ms step_avg:46.61ms
step:1078/1845 train_time:50257ms step_avg:46.62ms
step:1079/1845 train_time:50319ms step_avg:46.63ms
step:1080/1845 train_time:50380ms step_avg:46.65ms
step:1081/1845 train_time:50442ms step_avg:46.66ms
step:1082/1845 train_time:50503ms step_avg:46.68ms
step:1083/1845 train_time:50565ms step_avg:46.69ms
step:1084/1845 train_time:50626ms step_avg:46.70ms
step:1085/1845 train_time:50688ms step_avg:46.72ms
step:1086/1845 train_time:50749ms step_avg:46.73ms
step:1087/1845 train_time:50811ms step_avg:46.74ms
step:1088/1845 train_time:50872ms step_avg:46.76ms
step:1089/1845 train_time:50935ms step_avg:46.77ms
step:1090/1845 train_time:50996ms step_avg:46.78ms
step:1091/1845 train_time:51058ms step_avg:46.80ms
step:1092/1845 train_time:51119ms step_avg:46.81ms
step:1093/1845 train_time:51182ms step_avg:46.83ms
step:1094/1845 train_time:51242ms step_avg:46.84ms
step:1095/1845 train_time:51305ms step_avg:46.85ms
step:1096/1845 train_time:51367ms step_avg:46.87ms
step:1097/1845 train_time:51429ms step_avg:46.88ms
step:1098/1845 train_time:51490ms step_avg:46.89ms
step:1099/1845 train_time:51553ms step_avg:46.91ms
step:1100/1845 train_time:51613ms step_avg:46.92ms
step:1101/1845 train_time:51675ms step_avg:46.93ms
step:1102/1845 train_time:51736ms step_avg:46.95ms
step:1103/1845 train_time:51799ms step_avg:46.96ms
step:1104/1845 train_time:51860ms step_avg:46.97ms
step:1105/1845 train_time:51922ms step_avg:46.99ms
step:1106/1845 train_time:51984ms step_avg:47.00ms
step:1107/1845 train_time:52046ms step_avg:47.02ms
step:1108/1845 train_time:52107ms step_avg:47.03ms
step:1109/1845 train_time:52170ms step_avg:47.04ms
step:1110/1845 train_time:52230ms step_avg:47.05ms
step:1111/1845 train_time:52293ms step_avg:47.07ms
step:1112/1845 train_time:52354ms step_avg:47.08ms
step:1113/1845 train_time:52416ms step_avg:47.09ms
step:1114/1845 train_time:52477ms step_avg:47.11ms
step:1115/1845 train_time:52539ms step_avg:47.12ms
step:1116/1845 train_time:52601ms step_avg:47.13ms
step:1117/1845 train_time:52663ms step_avg:47.15ms
step:1118/1845 train_time:52724ms step_avg:47.16ms
step:1119/1845 train_time:52787ms step_avg:47.17ms
step:1120/1845 train_time:52848ms step_avg:47.19ms
step:1121/1845 train_time:52910ms step_avg:47.20ms
step:1122/1845 train_time:52971ms step_avg:47.21ms
step:1123/1845 train_time:53033ms step_avg:47.22ms
step:1124/1845 train_time:53094ms step_avg:47.24ms
step:1125/1845 train_time:53156ms step_avg:47.25ms
step:1126/1845 train_time:53217ms step_avg:47.26ms
step:1127/1845 train_time:53279ms step_avg:47.28ms
step:1128/1845 train_time:53341ms step_avg:47.29ms
step:1129/1845 train_time:53403ms step_avg:47.30ms
step:1130/1845 train_time:53464ms step_avg:47.31ms
step:1131/1845 train_time:53527ms step_avg:47.33ms
step:1132/1845 train_time:53587ms step_avg:47.34ms
step:1133/1845 train_time:53649ms step_avg:47.35ms
step:1134/1845 train_time:53710ms step_avg:47.36ms
step:1135/1845 train_time:53772ms step_avg:47.38ms
step:1136/1845 train_time:53833ms step_avg:47.39ms
step:1137/1845 train_time:53895ms step_avg:47.40ms
step:1138/1845 train_time:53956ms step_avg:47.41ms
step:1139/1845 train_time:54018ms step_avg:47.43ms
step:1140/1845 train_time:54079ms step_avg:47.44ms
step:1141/1845 train_time:54142ms step_avg:47.45ms
step:1142/1845 train_time:54203ms step_avg:47.46ms
step:1143/1845 train_time:54265ms step_avg:47.48ms
step:1144/1845 train_time:54326ms step_avg:47.49ms
step:1145/1845 train_time:54389ms step_avg:47.50ms
step:1146/1845 train_time:54449ms step_avg:47.51ms
step:1147/1845 train_time:54512ms step_avg:47.53ms
step:1148/1845 train_time:54573ms step_avg:47.54ms
step:1149/1845 train_time:54636ms step_avg:47.55ms
step:1150/1845 train_time:54696ms step_avg:47.56ms
step:1151/1845 train_time:54759ms step_avg:47.57ms
step:1152/1845 train_time:54820ms step_avg:47.59ms
step:1153/1845 train_time:54882ms step_avg:47.60ms
step:1154/1845 train_time:54944ms step_avg:47.61ms
step:1155/1845 train_time:55007ms step_avg:47.62ms
step:1156/1845 train_time:55068ms step_avg:47.64ms
step:1157/1845 train_time:55130ms step_avg:47.65ms
step:1158/1845 train_time:55190ms step_avg:47.66ms
step:1159/1845 train_time:55253ms step_avg:47.67ms
step:1160/1845 train_time:55314ms step_avg:47.68ms
step:1161/1845 train_time:55376ms step_avg:47.70ms
step:1162/1845 train_time:55437ms step_avg:47.71ms
step:1163/1845 train_time:55499ms step_avg:47.72ms
step:1164/1845 train_time:55560ms step_avg:47.73ms
step:1165/1845 train_time:55623ms step_avg:47.75ms
step:1166/1845 train_time:55684ms step_avg:47.76ms
step:1167/1845 train_time:55747ms step_avg:47.77ms
step:1168/1845 train_time:55808ms step_avg:47.78ms
step:1169/1845 train_time:55870ms step_avg:47.79ms
step:1170/1845 train_time:55931ms step_avg:47.80ms
step:1171/1845 train_time:55994ms step_avg:47.82ms
step:1172/1845 train_time:56054ms step_avg:47.83ms
step:1173/1845 train_time:56116ms step_avg:47.84ms
step:1174/1845 train_time:56177ms step_avg:47.85ms
step:1175/1845 train_time:56239ms step_avg:47.86ms
step:1176/1845 train_time:56300ms step_avg:47.87ms
step:1177/1845 train_time:56363ms step_avg:47.89ms
step:1178/1845 train_time:56424ms step_avg:47.90ms
step:1179/1845 train_time:56487ms step_avg:47.91ms
step:1180/1845 train_time:56547ms step_avg:47.92ms
step:1181/1845 train_time:56610ms step_avg:47.93ms
step:1182/1845 train_time:56670ms step_avg:47.94ms
step:1183/1845 train_time:56733ms step_avg:47.96ms
step:1184/1845 train_time:56794ms step_avg:47.97ms
step:1185/1845 train_time:56856ms step_avg:47.98ms
step:1186/1845 train_time:56917ms step_avg:47.99ms
step:1187/1845 train_time:56979ms step_avg:48.00ms
step:1188/1845 train_time:57041ms step_avg:48.01ms
step:1189/1845 train_time:57103ms step_avg:48.03ms
step:1190/1845 train_time:57164ms step_avg:48.04ms
step:1191/1845 train_time:57226ms step_avg:48.05ms
step:1192/1845 train_time:57287ms step_avg:48.06ms
step:1193/1845 train_time:57350ms step_avg:48.07ms
step:1194/1845 train_time:57410ms step_avg:48.08ms
step:1195/1845 train_time:57473ms step_avg:48.09ms
step:1196/1845 train_time:57533ms step_avg:48.10ms
step:1197/1845 train_time:57596ms step_avg:48.12ms
step:1198/1845 train_time:57656ms step_avg:48.13ms
step:1199/1845 train_time:57718ms step_avg:48.14ms
step:1200/1845 train_time:57780ms step_avg:48.15ms
step:1201/1845 train_time:57842ms step_avg:48.16ms
step:1202/1845 train_time:57903ms step_avg:48.17ms
step:1203/1845 train_time:57966ms step_avg:48.18ms
step:1204/1845 train_time:58027ms step_avg:48.20ms
step:1205/1845 train_time:58090ms step_avg:48.21ms
step:1206/1845 train_time:58177ms step_avg:48.24ms
step:1207/1845 train_time:58265ms step_avg:48.27ms
step:1208/1845 train_time:58354ms step_avg:48.31ms
step:1209/1845 train_time:58443ms step_avg:48.34ms
step:1210/1845 train_time:58531ms step_avg:48.37ms
step:1211/1845 train_time:58620ms step_avg:48.41ms
step:1212/1845 train_time:58707ms step_avg:48.44ms
step:1213/1845 train_time:58796ms step_avg:48.47ms
step:1214/1845 train_time:58883ms step_avg:48.50ms
step:1215/1845 train_time:58973ms step_avg:48.54ms
step:1216/1845 train_time:59060ms step_avg:48.57ms
step:1217/1845 train_time:59147ms step_avg:48.60ms
step:1218/1845 train_time:59235ms step_avg:48.63ms
step:1219/1845 train_time:59323ms step_avg:48.67ms
step:1220/1845 train_time:59410ms step_avg:48.70ms
step:1221/1845 train_time:59500ms step_avg:48.73ms
step:1222/1845 train_time:59588ms step_avg:48.76ms
step:1223/1845 train_time:59677ms step_avg:48.80ms
step:1224/1845 train_time:59764ms step_avg:48.83ms
step:1225/1845 train_time:59853ms step_avg:48.86ms
step:1226/1845 train_time:59940ms step_avg:48.89ms
step:1227/1845 train_time:60029ms step_avg:48.92ms
step:1228/1845 train_time:60115ms step_avg:48.95ms
step:1229/1845 train_time:60203ms step_avg:48.99ms
step:1230/1845 train_time:60290ms step_avg:49.02ms
step:1231/1845 train_time:60379ms step_avg:49.05ms
step:1232/1845 train_time:60466ms step_avg:49.08ms
step:1233/1845 train_time:60556ms step_avg:49.11ms
step:1234/1845 train_time:60643ms step_avg:49.14ms
step:1235/1845 train_time:60732ms step_avg:49.18ms
step:1236/1845 train_time:60819ms step_avg:49.21ms
step:1237/1845 train_time:60908ms step_avg:49.24ms
step:1238/1845 train_time:60997ms step_avg:49.27ms
step:1239/1845 train_time:61085ms step_avg:49.30ms
step:1240/1845 train_time:61172ms step_avg:49.33ms
step:1241/1845 train_time:61262ms step_avg:49.36ms
step:1242/1845 train_time:61349ms step_avg:49.40ms
step:1243/1845 train_time:61439ms step_avg:49.43ms
step:1244/1845 train_time:61527ms step_avg:49.46ms
step:1245/1845 train_time:61616ms step_avg:49.49ms
step:1246/1845 train_time:61702ms step_avg:49.52ms
step:1247/1845 train_time:61791ms step_avg:49.55ms
step:1248/1845 train_time:61879ms step_avg:49.58ms
step:1249/1845 train_time:61968ms step_avg:49.61ms
step:1250/1845 train_time:62055ms step_avg:49.64ms
step:1250/1845 val_loss:3.5330 train_time:62145ms step_avg:49.72ms
step:1251/1845 train_time:62166ms step_avg:49.69ms
step:1252/1845 train_time:62235ms step_avg:49.71ms
step:1253/1845 train_time:62325ms step_avg:49.74ms
step:1254/1845 train_time:62413ms step_avg:49.77ms
step:1255/1845 train_time:62501ms step_avg:49.80ms
step:1256/1845 train_time:62587ms step_avg:49.83ms
step:1257/1845 train_time:62675ms step_avg:49.86ms
step:1258/1845 train_time:62762ms step_avg:49.89ms
step:1259/1845 train_time:62850ms step_avg:49.92ms
step:1260/1845 train_time:62936ms step_avg:49.95ms
step:1261/1845 train_time:63024ms step_avg:49.98ms
step:1262/1845 train_time:63113ms step_avg:50.01ms
step:1263/1845 train_time:63204ms step_avg:50.04ms
step:1264/1845 train_time:63293ms step_avg:50.07ms
step:1265/1845 train_time:63383ms step_avg:50.11ms
step:1266/1845 train_time:63471ms step_avg:50.13ms
step:1267/1845 train_time:63559ms step_avg:50.16ms
step:1268/1845 train_time:63646ms step_avg:50.19ms
step:1269/1845 train_time:63734ms step_avg:50.22ms
step:1270/1845 train_time:63819ms step_avg:50.25ms
step:1271/1845 train_time:63907ms step_avg:50.28ms
step:1272/1845 train_time:63993ms step_avg:50.31ms
step:1273/1845 train_time:64083ms step_avg:50.34ms
step:1274/1845 train_time:64171ms step_avg:50.37ms
step:1275/1845 train_time:64261ms step_avg:50.40ms
step:1276/1845 train_time:64349ms step_avg:50.43ms
step:1277/1845 train_time:64438ms step_avg:50.46ms
step:1278/1845 train_time:64525ms step_avg:50.49ms
step:1279/1845 train_time:64613ms step_avg:50.52ms
step:1280/1845 train_time:64699ms step_avg:50.55ms
step:1281/1845 train_time:64788ms step_avg:50.58ms
step:1282/1845 train_time:64875ms step_avg:50.60ms
step:1283/1845 train_time:64963ms step_avg:50.63ms
step:1284/1845 train_time:65051ms step_avg:50.66ms
step:1285/1845 train_time:65140ms step_avg:50.69ms
step:1286/1845 train_time:65228ms step_avg:50.72ms
step:1287/1845 train_time:65318ms step_avg:50.75ms
step:1288/1845 train_time:65405ms step_avg:50.78ms
step:1289/1845 train_time:65494ms step_avg:50.81ms
step:1290/1845 train_time:65581ms step_avg:50.84ms
step:1291/1845 train_time:65669ms step_avg:50.87ms
step:1292/1845 train_time:65755ms step_avg:50.89ms
step:1293/1845 train_time:65843ms step_avg:50.92ms
step:1294/1845 train_time:65930ms step_avg:50.95ms
step:1295/1845 train_time:66019ms step_avg:50.98ms
step:1296/1845 train_time:66107ms step_avg:51.01ms
step:1297/1845 train_time:66196ms step_avg:51.04ms
step:1298/1845 train_time:66283ms step_avg:51.07ms
step:1299/1845 train_time:66372ms step_avg:51.09ms
step:1300/1845 train_time:66459ms step_avg:51.12ms
step:1301/1845 train_time:66548ms step_avg:51.15ms
step:1302/1845 train_time:66636ms step_avg:51.18ms
step:1303/1845 train_time:66725ms step_avg:51.21ms
step:1304/1845 train_time:66812ms step_avg:51.24ms
step:1305/1845 train_time:66900ms step_avg:51.26ms
step:1306/1845 train_time:66987ms step_avg:51.29ms
step:1307/1845 train_time:67076ms step_avg:51.32ms
step:1308/1845 train_time:67164ms step_avg:51.35ms
step:1309/1845 train_time:67252ms step_avg:51.38ms
step:1310/1845 train_time:67339ms step_avg:51.40ms
step:1311/1845 train_time:67428ms step_avg:51.43ms
step:1312/1845 train_time:67516ms step_avg:51.46ms
step:1313/1845 train_time:67604ms step_avg:51.49ms
step:1314/1845 train_time:67692ms step_avg:51.52ms
step:1315/1845 train_time:67780ms step_avg:51.54ms
step:1316/1845 train_time:67867ms step_avg:51.57ms
step:1317/1845 train_time:67955ms step_avg:51.60ms
step:1318/1845 train_time:68044ms step_avg:51.63ms
step:1319/1845 train_time:68133ms step_avg:51.66ms
step:1320/1845 train_time:68219ms step_avg:51.68ms
step:1321/1845 train_time:68308ms step_avg:51.71ms
step:1322/1845 train_time:68396ms step_avg:51.74ms
step:1323/1845 train_time:68485ms step_avg:51.77ms
step:1324/1845 train_time:68572ms step_avg:51.79ms
step:1325/1845 train_time:68660ms step_avg:51.82ms
step:1326/1845 train_time:68747ms step_avg:51.85ms
step:1327/1845 train_time:68836ms step_avg:51.87ms
step:1328/1845 train_time:68924ms step_avg:51.90ms
step:1329/1845 train_time:69012ms step_avg:51.93ms
step:1330/1845 train_time:69098ms step_avg:51.95ms
step:1331/1845 train_time:69188ms step_avg:51.98ms
step:1332/1845 train_time:69275ms step_avg:52.01ms
step:1333/1845 train_time:69363ms step_avg:52.04ms
step:1334/1845 train_time:69452ms step_avg:52.06ms
step:1335/1845 train_time:69539ms step_avg:52.09ms
step:1336/1845 train_time:69627ms step_avg:52.12ms
step:1337/1845 train_time:69716ms step_avg:52.14ms
step:1338/1845 train_time:69803ms step_avg:52.17ms
step:1339/1845 train_time:69893ms step_avg:52.20ms
step:1340/1845 train_time:69981ms step_avg:52.22ms
step:1341/1845 train_time:70068ms step_avg:52.25ms
step:1342/1845 train_time:70156ms step_avg:52.28ms
step:1343/1845 train_time:70244ms step_avg:52.30ms
step:1344/1845 train_time:70332ms step_avg:52.33ms
step:1345/1845 train_time:70420ms step_avg:52.36ms
step:1346/1845 train_time:70507ms step_avg:52.38ms
step:1347/1845 train_time:70596ms step_avg:52.41ms
step:1348/1845 train_time:70683ms step_avg:52.44ms
step:1349/1845 train_time:70771ms step_avg:52.46ms
step:1350/1845 train_time:70858ms step_avg:52.49ms
step:1351/1845 train_time:70946ms step_avg:52.51ms
step:1352/1845 train_time:71034ms step_avg:52.54ms
step:1353/1845 train_time:71122ms step_avg:52.57ms
step:1354/1845 train_time:71209ms step_avg:52.59ms
step:1355/1845 train_time:71298ms step_avg:52.62ms
step:1356/1845 train_time:71385ms step_avg:52.64ms
step:1357/1845 train_time:71474ms step_avg:52.67ms
step:1358/1845 train_time:71561ms step_avg:52.70ms
step:1359/1845 train_time:71650ms step_avg:52.72ms
step:1360/1845 train_time:71737ms step_avg:52.75ms
step:1361/1845 train_time:71826ms step_avg:52.77ms
step:1362/1845 train_time:71913ms step_avg:52.80ms
step:1363/1845 train_time:72001ms step_avg:52.83ms
step:1364/1845 train_time:72089ms step_avg:52.85ms
step:1365/1845 train_time:72177ms step_avg:52.88ms
step:1366/1845 train_time:72266ms step_avg:52.90ms
step:1367/1845 train_time:72357ms step_avg:52.93ms
step:1368/1845 train_time:72445ms step_avg:52.96ms
step:1369/1845 train_time:72533ms step_avg:52.98ms
step:1370/1845 train_time:72620ms step_avg:53.01ms
step:1371/1845 train_time:72708ms step_avg:53.03ms
step:1372/1845 train_time:72795ms step_avg:53.06ms
step:1373/1845 train_time:72883ms step_avg:53.08ms
step:1374/1845 train_time:72970ms step_avg:53.11ms
step:1375/1845 train_time:73059ms step_avg:53.13ms
step:1376/1845 train_time:73146ms step_avg:53.16ms
step:1377/1845 train_time:73235ms step_avg:53.18ms
step:1378/1845 train_time:73322ms step_avg:53.21ms
step:1379/1845 train_time:73411ms step_avg:53.23ms
step:1380/1845 train_time:73497ms step_avg:53.26ms
step:1381/1845 train_time:73586ms step_avg:53.28ms
step:1382/1845 train_time:73674ms step_avg:53.31ms
step:1383/1845 train_time:73763ms step_avg:53.34ms
step:1384/1845 train_time:73851ms step_avg:53.36ms
step:1385/1845 train_time:73938ms step_avg:53.39ms
step:1386/1845 train_time:74026ms step_avg:53.41ms
step:1387/1845 train_time:74114ms step_avg:53.43ms
step:1388/1845 train_time:74200ms step_avg:53.46ms
step:1389/1845 train_time:74289ms step_avg:53.48ms
step:1390/1845 train_time:74377ms step_avg:53.51ms
step:1391/1845 train_time:74466ms step_avg:53.53ms
step:1392/1845 train_time:74553ms step_avg:53.56ms
step:1393/1845 train_time:74642ms step_avg:53.58ms
step:1394/1845 train_time:74730ms step_avg:53.61ms
step:1395/1845 train_time:74818ms step_avg:53.63ms
step:1396/1845 train_time:74906ms step_avg:53.66ms
step:1397/1845 train_time:74996ms step_avg:53.68ms
step:1398/1845 train_time:75083ms step_avg:53.71ms
step:1399/1845 train_time:75171ms step_avg:53.73ms
step:1400/1845 train_time:75258ms step_avg:53.76ms
step:1401/1845 train_time:75347ms step_avg:53.78ms
step:1402/1845 train_time:75434ms step_avg:53.80ms
step:1403/1845 train_time:75523ms step_avg:53.83ms
step:1404/1845 train_time:75611ms step_avg:53.85ms
step:1405/1845 train_time:75700ms step_avg:53.88ms
step:1406/1845 train_time:75786ms step_avg:53.90ms
step:1407/1845 train_time:75875ms step_avg:53.93ms
step:1408/1845 train_time:75962ms step_avg:53.95ms
step:1409/1845 train_time:76052ms step_avg:53.98ms
step:1410/1845 train_time:76138ms step_avg:54.00ms
step:1411/1845 train_time:76227ms step_avg:54.02ms
step:1412/1845 train_time:76314ms step_avg:54.05ms
step:1413/1845 train_time:76402ms step_avg:54.07ms
step:1414/1845 train_time:76489ms step_avg:54.09ms
step:1415/1845 train_time:76579ms step_avg:54.12ms
step:1416/1845 train_time:76667ms step_avg:54.14ms
step:1417/1845 train_time:76755ms step_avg:54.17ms
step:1418/1845 train_time:76843ms step_avg:54.19ms
step:1419/1845 train_time:76931ms step_avg:54.21ms
step:1420/1845 train_time:77018ms step_avg:54.24ms
step:1421/1845 train_time:77106ms step_avg:54.26ms
step:1422/1845 train_time:77194ms step_avg:54.29ms
step:1423/1845 train_time:77282ms step_avg:54.31ms
step:1424/1845 train_time:77370ms step_avg:54.33ms
step:1425/1845 train_time:77459ms step_avg:54.36ms
step:1426/1845 train_time:77546ms step_avg:54.38ms
step:1427/1845 train_time:77635ms step_avg:54.40ms
step:1428/1845 train_time:77722ms step_avg:54.43ms
step:1429/1845 train_time:77811ms step_avg:54.45ms
step:1430/1845 train_time:77897ms step_avg:54.47ms
step:1431/1845 train_time:77986ms step_avg:54.50ms
step:1432/1845 train_time:78074ms step_avg:54.52ms
step:1433/1845 train_time:78162ms step_avg:54.54ms
step:1434/1845 train_time:78250ms step_avg:54.57ms
step:1435/1845 train_time:78338ms step_avg:54.59ms
step:1436/1845 train_time:78425ms step_avg:54.61ms
step:1437/1845 train_time:78513ms step_avg:54.64ms
step:1438/1845 train_time:78600ms step_avg:54.66ms
step:1439/1845 train_time:78690ms step_avg:54.68ms
step:1440/1845 train_time:78777ms step_avg:54.71ms
step:1441/1845 train_time:78865ms step_avg:54.73ms
step:1442/1845 train_time:78954ms step_avg:54.75ms
step:1443/1845 train_time:79041ms step_avg:54.78ms
step:1444/1845 train_time:79128ms step_avg:54.80ms
step:1445/1845 train_time:79217ms step_avg:54.82ms
step:1446/1845 train_time:79306ms step_avg:54.84ms
step:1447/1845 train_time:79395ms step_avg:54.87ms
step:1448/1845 train_time:79482ms step_avg:54.89ms
step:1449/1845 train_time:79569ms step_avg:54.91ms
step:1450/1845 train_time:79657ms step_avg:54.94ms
step:1451/1845 train_time:79745ms step_avg:54.96ms
step:1452/1845 train_time:79832ms step_avg:54.98ms
step:1453/1845 train_time:79921ms step_avg:55.00ms
step:1454/1845 train_time:80008ms step_avg:55.03ms
step:1455/1845 train_time:80098ms step_avg:55.05ms
step:1456/1845 train_time:80185ms step_avg:55.07ms
step:1457/1845 train_time:80274ms step_avg:55.10ms
step:1458/1845 train_time:80361ms step_avg:55.12ms
step:1459/1845 train_time:80449ms step_avg:55.14ms
step:1460/1845 train_time:80536ms step_avg:55.16ms
step:1461/1845 train_time:80624ms step_avg:55.18ms
step:1462/1845 train_time:80711ms step_avg:55.21ms
step:1463/1845 train_time:80799ms step_avg:55.23ms
step:1464/1845 train_time:80886ms step_avg:55.25ms
step:1465/1845 train_time:80975ms step_avg:55.27ms
step:1466/1845 train_time:81063ms step_avg:55.30ms
step:1467/1845 train_time:81151ms step_avg:55.32ms
step:1468/1845 train_time:81239ms step_avg:55.34ms
step:1469/1845 train_time:81328ms step_avg:55.36ms
step:1470/1845 train_time:81415ms step_avg:55.38ms
step:1471/1845 train_time:81504ms step_avg:55.41ms
step:1472/1845 train_time:81591ms step_avg:55.43ms
step:1473/1845 train_time:81680ms step_avg:55.45ms
step:1474/1845 train_time:81768ms step_avg:55.47ms
step:1475/1845 train_time:81856ms step_avg:55.50ms
step:1476/1845 train_time:81943ms step_avg:55.52ms
step:1477/1845 train_time:82031ms step_avg:55.54ms
step:1478/1845 train_time:82119ms step_avg:55.56ms
step:1479/1845 train_time:82207ms step_avg:55.58ms
step:1480/1845 train_time:82295ms step_avg:55.60ms
step:1481/1845 train_time:82383ms step_avg:55.63ms
step:1482/1845 train_time:82470ms step_avg:55.65ms
step:1483/1845 train_time:82559ms step_avg:55.67ms
step:1484/1845 train_time:82648ms step_avg:55.69ms
step:1485/1845 train_time:82736ms step_avg:55.71ms
step:1486/1845 train_time:82824ms step_avg:55.74ms
step:1487/1845 train_time:82912ms step_avg:55.76ms
step:1488/1845 train_time:82998ms step_avg:55.78ms
step:1489/1845 train_time:83087ms step_avg:55.80ms
step:1490/1845 train_time:83174ms step_avg:55.82ms
step:1491/1845 train_time:83263ms step_avg:55.84ms
step:1492/1845 train_time:83351ms step_avg:55.87ms
step:1493/1845 train_time:83439ms step_avg:55.89ms
step:1494/1845 train_time:83526ms step_avg:55.91ms
step:1495/1845 train_time:83614ms step_avg:55.93ms
step:1496/1845 train_time:83702ms step_avg:55.95ms
step:1497/1845 train_time:83790ms step_avg:55.97ms
step:1498/1845 train_time:83876ms step_avg:55.99ms
step:1499/1845 train_time:83965ms step_avg:56.01ms
step:1500/1845 train_time:84053ms step_avg:56.04ms
step:1500/1845 val_loss:3.4024 train_time:84142ms step_avg:56.09ms
step:1501/1845 train_time:84164ms step_avg:56.07ms
step:1502/1845 train_time:84234ms step_avg:56.08ms
step:1503/1845 train_time:84325ms step_avg:56.10ms
step:1504/1845 train_time:84414ms step_avg:56.13ms
step:1505/1845 train_time:84501ms step_avg:56.15ms
step:1506/1845 train_time:84588ms step_avg:56.17ms
step:1507/1845 train_time:84675ms step_avg:56.19ms
step:1508/1845 train_time:84761ms step_avg:56.21ms
step:1509/1845 train_time:84849ms step_avg:56.23ms
step:1510/1845 train_time:84936ms step_avg:56.25ms
step:1511/1845 train_time:85023ms step_avg:56.27ms
step:1512/1845 train_time:85113ms step_avg:56.29ms
step:1513/1845 train_time:85202ms step_avg:56.31ms
step:1514/1845 train_time:85292ms step_avg:56.34ms
step:1515/1845 train_time:85382ms step_avg:56.36ms
step:1516/1845 train_time:85470ms step_avg:56.38ms
step:1517/1845 train_time:85558ms step_avg:56.40ms
step:1518/1845 train_time:85644ms step_avg:56.42ms
step:1519/1845 train_time:85732ms step_avg:56.44ms
step:1520/1845 train_time:85818ms step_avg:56.46ms
step:1521/1845 train_time:85906ms step_avg:56.48ms
step:1522/1845 train_time:85993ms step_avg:56.50ms
step:1523/1845 train_time:86082ms step_avg:56.52ms
step:1524/1845 train_time:86171ms step_avg:56.54ms
step:1525/1845 train_time:86261ms step_avg:56.56ms
step:1526/1845 train_time:86350ms step_avg:56.59ms
step:1527/1845 train_time:86439ms step_avg:56.61ms
step:1528/1845 train_time:86528ms step_avg:56.63ms
step:1529/1845 train_time:86616ms step_avg:56.65ms
step:1530/1845 train_time:86702ms step_avg:56.67ms
step:1531/1845 train_time:86790ms step_avg:56.69ms
step:1532/1845 train_time:86876ms step_avg:56.71ms
step:1533/1845 train_time:86964ms step_avg:56.73ms
step:1534/1845 train_time:87051ms step_avg:56.75ms
step:1535/1845 train_time:87139ms step_avg:56.77ms
step:1536/1845 train_time:87228ms step_avg:56.79ms
step:1537/1845 train_time:87318ms step_avg:56.81ms
step:1538/1845 train_time:87406ms step_avg:56.83ms
step:1539/1845 train_time:87494ms step_avg:56.85ms
step:1540/1845 train_time:87581ms step_avg:56.87ms
step:1541/1845 train_time:87670ms step_avg:56.89ms
step:1542/1845 train_time:87757ms step_avg:56.91ms
step:1543/1845 train_time:87844ms step_avg:56.93ms
step:1544/1845 train_time:87932ms step_avg:56.95ms
step:1545/1845 train_time:88019ms step_avg:56.97ms
step:1546/1845 train_time:88107ms step_avg:56.99ms
step:1547/1845 train_time:88196ms step_avg:57.01ms
step:1548/1845 train_time:88284ms step_avg:57.03ms
step:1549/1845 train_time:88373ms step_avg:57.05ms
step:1550/1845 train_time:88461ms step_avg:57.07ms
step:1551/1845 train_time:88549ms step_avg:57.09ms
step:1552/1845 train_time:88637ms step_avg:57.11ms
step:1553/1845 train_time:88725ms step_avg:57.13ms
step:1554/1845 train_time:88812ms step_avg:57.15ms
step:1555/1845 train_time:88900ms step_avg:57.17ms
step:1556/1845 train_time:88987ms step_avg:57.19ms
step:1557/1845 train_time:89077ms step_avg:57.21ms
step:1558/1845 train_time:89164ms step_avg:57.23ms
step:1559/1845 train_time:89252ms step_avg:57.25ms
step:1560/1845 train_time:89341ms step_avg:57.27ms
step:1561/1845 train_time:89430ms step_avg:57.29ms
step:1562/1845 train_time:89517ms step_avg:57.31ms
step:1563/1845 train_time:89606ms step_avg:57.33ms
step:1564/1845 train_time:89695ms step_avg:57.35ms
step:1565/1845 train_time:89782ms step_avg:57.37ms
step:1566/1845 train_time:89869ms step_avg:57.39ms
step:1567/1845 train_time:89957ms step_avg:57.41ms
step:1568/1845 train_time:90044ms step_avg:57.43ms
step:1569/1845 train_time:90133ms step_avg:57.45ms
step:1570/1845 train_time:90219ms step_avg:57.46ms
step:1571/1845 train_time:90308ms step_avg:57.48ms
step:1572/1845 train_time:90396ms step_avg:57.50ms
step:1573/1845 train_time:90485ms step_avg:57.52ms
step:1574/1845 train_time:90573ms step_avg:57.54ms
step:1575/1845 train_time:90661ms step_avg:57.56ms
step:1576/1845 train_time:90749ms step_avg:57.58ms
step:1577/1845 train_time:90838ms step_avg:57.60ms
step:1578/1845 train_time:90925ms step_avg:57.62ms
step:1579/1845 train_time:91014ms step_avg:57.64ms
step:1580/1845 train_time:91101ms step_avg:57.66ms
step:1581/1845 train_time:91189ms step_avg:57.68ms
step:1582/1845 train_time:91277ms step_avg:57.70ms
step:1583/1845 train_time:91366ms step_avg:57.72ms
step:1584/1845 train_time:91453ms step_avg:57.74ms
step:1585/1845 train_time:91542ms step_avg:57.76ms
step:1586/1845 train_time:91629ms step_avg:57.77ms
step:1587/1845 train_time:91718ms step_avg:57.79ms
step:1588/1845 train_time:91806ms step_avg:57.81ms
step:1589/1845 train_time:91896ms step_avg:57.83ms
step:1590/1845 train_time:91983ms step_avg:57.85ms
step:1591/1845 train_time:92072ms step_avg:57.87ms
step:1592/1845 train_time:92158ms step_avg:57.89ms
step:1593/1845 train_time:92248ms step_avg:57.91ms
step:1594/1845 train_time:92336ms step_avg:57.93ms
step:1595/1845 train_time:92425ms step_avg:57.95ms
step:1596/1845 train_time:92513ms step_avg:57.97ms
step:1597/1845 train_time:92600ms step_avg:57.98ms
step:1598/1845 train_time:92687ms step_avg:58.00ms
step:1599/1845 train_time:92775ms step_avg:58.02ms
step:1600/1845 train_time:92862ms step_avg:58.04ms
step:1601/1845 train_time:92950ms step_avg:58.06ms
step:1602/1845 train_time:93037ms step_avg:58.08ms
step:1603/1845 train_time:93125ms step_avg:58.09ms
step:1604/1845 train_time:93212ms step_avg:58.11ms
step:1605/1845 train_time:93301ms step_avg:58.13ms
step:1606/1845 train_time:93388ms step_avg:58.15ms
step:1607/1845 train_time:93477ms step_avg:58.17ms
step:1608/1845 train_time:93564ms step_avg:58.19ms
step:1609/1845 train_time:93654ms step_avg:58.21ms
step:1610/1845 train_time:93741ms step_avg:58.22ms
step:1611/1845 train_time:93830ms step_avg:58.24ms
step:1612/1845 train_time:93917ms step_avg:58.26ms
step:1613/1845 train_time:94005ms step_avg:58.28ms
step:1614/1845 train_time:94094ms step_avg:58.30ms
step:1615/1845 train_time:94182ms step_avg:58.32ms
step:1616/1845 train_time:94269ms step_avg:58.33ms
step:1617/1845 train_time:94358ms step_avg:58.35ms
step:1618/1845 train_time:94446ms step_avg:58.37ms
step:1619/1845 train_time:94535ms step_avg:58.39ms
step:1620/1845 train_time:94623ms step_avg:58.41ms
step:1621/1845 train_time:94710ms step_avg:58.43ms
step:1622/1845 train_time:94797ms step_avg:58.44ms
step:1623/1845 train_time:94886ms step_avg:58.46ms
step:1624/1845 train_time:94972ms step_avg:58.48ms
step:1625/1845 train_time:95060ms step_avg:58.50ms
step:1626/1845 train_time:95147ms step_avg:58.52ms
step:1627/1845 train_time:95236ms step_avg:58.53ms
step:1628/1845 train_time:95323ms step_avg:58.55ms
step:1629/1845 train_time:95412ms step_avg:58.57ms
step:1630/1845 train_time:95499ms step_avg:58.59ms
step:1631/1845 train_time:95588ms step_avg:58.61ms
step:1632/1845 train_time:95675ms step_avg:58.62ms
step:1633/1845 train_time:95763ms step_avg:58.64ms
step:1634/1845 train_time:95851ms step_avg:58.66ms
step:1635/1845 train_time:95939ms step_avg:58.68ms
step:1636/1845 train_time:96026ms step_avg:58.70ms
step:1637/1845 train_time:96116ms step_avg:58.71ms
step:1638/1845 train_time:96203ms step_avg:58.73ms
step:1639/1845 train_time:96291ms step_avg:58.75ms
step:1640/1845 train_time:96378ms step_avg:58.77ms
step:1641/1845 train_time:96467ms step_avg:58.79ms
step:1642/1845 train_time:96555ms step_avg:58.80ms
step:1643/1845 train_time:96642ms step_avg:58.82ms
step:1644/1845 train_time:96730ms step_avg:58.84ms
step:1645/1845 train_time:96820ms step_avg:58.86ms
step:1646/1845 train_time:96906ms step_avg:58.87ms
step:1647/1845 train_time:96995ms step_avg:58.89ms
step:1648/1845 train_time:97081ms step_avg:58.91ms
step:1649/1845 train_time:97170ms step_avg:58.93ms
step:1650/1845 train_time:97257ms step_avg:58.94ms
step:1651/1845 train_time:97347ms step_avg:58.96ms
step:1652/1845 train_time:97434ms step_avg:58.98ms
step:1653/1845 train_time:97522ms step_avg:59.00ms
step:1654/1845 train_time:97610ms step_avg:59.01ms
step:1655/1845 train_time:97698ms step_avg:59.03ms
step:1656/1845 train_time:97785ms step_avg:59.05ms
step:1657/1845 train_time:97873ms step_avg:59.07ms
step:1658/1845 train_time:97960ms step_avg:59.08ms
step:1659/1845 train_time:98048ms step_avg:59.10ms
step:1660/1845 train_time:98136ms step_avg:59.12ms
step:1661/1845 train_time:98224ms step_avg:59.14ms
step:1662/1845 train_time:98311ms step_avg:59.15ms
step:1663/1845 train_time:98400ms step_avg:59.17ms
step:1664/1845 train_time:98487ms step_avg:59.19ms
step:1665/1845 train_time:98575ms step_avg:59.20ms
step:1666/1845 train_time:98662ms step_avg:59.22ms
step:1667/1845 train_time:98751ms step_avg:59.24ms
step:1668/1845 train_time:98838ms step_avg:59.26ms
step:1669/1845 train_time:98926ms step_avg:59.27ms
step:1670/1845 train_time:99014ms step_avg:59.29ms
step:1671/1845 train_time:99102ms step_avg:59.31ms
step:1672/1845 train_time:99190ms step_avg:59.32ms
step:1673/1845 train_time:99278ms step_avg:59.34ms
step:1674/1845 train_time:99365ms step_avg:59.36ms
step:1675/1845 train_time:99453ms step_avg:59.38ms
step:1676/1845 train_time:99540ms step_avg:59.39ms
step:1677/1845 train_time:99629ms step_avg:59.41ms
step:1678/1845 train_time:99716ms step_avg:59.43ms
step:1679/1845 train_time:99804ms step_avg:59.44ms
step:1680/1845 train_time:99891ms step_avg:59.46ms
step:1681/1845 train_time:99979ms step_avg:59.48ms
step:1682/1845 train_time:100067ms step_avg:59.49ms
step:1683/1845 train_time:100156ms step_avg:59.51ms
step:1684/1845 train_time:100243ms step_avg:59.53ms
step:1685/1845 train_time:100332ms step_avg:59.54ms
step:1686/1845 train_time:100419ms step_avg:59.56ms
step:1687/1845 train_time:100507ms step_avg:59.58ms
step:1688/1845 train_time:100595ms step_avg:59.59ms
step:1689/1845 train_time:100683ms step_avg:59.61ms
step:1690/1845 train_time:100770ms step_avg:59.63ms
step:1691/1845 train_time:100858ms step_avg:59.64ms
step:1692/1845 train_time:100945ms step_avg:59.66ms
step:1693/1845 train_time:101034ms step_avg:59.68ms
step:1694/1845 train_time:101121ms step_avg:59.69ms
step:1695/1845 train_time:101210ms step_avg:59.71ms
step:1696/1845 train_time:101298ms step_avg:59.73ms
step:1697/1845 train_time:101387ms step_avg:59.74ms
step:1698/1845 train_time:101474ms step_avg:59.76ms
step:1699/1845 train_time:101562ms step_avg:59.78ms
step:1700/1845 train_time:101650ms step_avg:59.79ms
step:1701/1845 train_time:101739ms step_avg:59.81ms
step:1702/1845 train_time:101826ms step_avg:59.83ms
step:1703/1845 train_time:101914ms step_avg:59.84ms
step:1704/1845 train_time:102001ms step_avg:59.86ms
step:1705/1845 train_time:102089ms step_avg:59.88ms
step:1706/1845 train_time:102177ms step_avg:59.89ms
step:1707/1845 train_time:102266ms step_avg:59.91ms
step:1708/1845 train_time:102353ms step_avg:59.93ms
step:1709/1845 train_time:102441ms step_avg:59.94ms
step:1710/1845 train_time:102529ms step_avg:59.96ms
step:1711/1845 train_time:102618ms step_avg:59.98ms
step:1712/1845 train_time:102705ms step_avg:59.99ms
step:1713/1845 train_time:102793ms step_avg:60.01ms
step:1714/1845 train_time:102880ms step_avg:60.02ms
step:1715/1845 train_time:102969ms step_avg:60.04ms
step:1716/1845 train_time:103056ms step_avg:60.06ms
step:1717/1845 train_time:103145ms step_avg:60.07ms
step:1718/1845 train_time:103232ms step_avg:60.09ms
step:1719/1845 train_time:103320ms step_avg:60.10ms
step:1720/1845 train_time:103408ms step_avg:60.12ms
step:1721/1845 train_time:103496ms step_avg:60.14ms
step:1722/1845 train_time:103583ms step_avg:60.15ms
step:1723/1845 train_time:103673ms step_avg:60.17ms
step:1724/1845 train_time:103759ms step_avg:60.19ms
step:1725/1845 train_time:103848ms step_avg:60.20ms
step:1726/1845 train_time:103935ms step_avg:60.22ms
step:1727/1845 train_time:104024ms step_avg:60.23ms
step:1728/1845 train_time:104112ms step_avg:60.25ms
step:1729/1845 train_time:104199ms step_avg:60.27ms
step:1730/1845 train_time:104287ms step_avg:60.28ms
step:1731/1845 train_time:104375ms step_avg:60.30ms
step:1732/1845 train_time:104462ms step_avg:60.31ms
step:1733/1845 train_time:104550ms step_avg:60.33ms
step:1734/1845 train_time:104637ms step_avg:60.34ms
step:1735/1845 train_time:104726ms step_avg:60.36ms
step:1736/1845 train_time:104813ms step_avg:60.38ms
step:1737/1845 train_time:104901ms step_avg:60.39ms
step:1738/1845 train_time:104989ms step_avg:60.41ms
step:1739/1845 train_time:105079ms step_avg:60.42ms
step:1740/1845 train_time:105166ms step_avg:60.44ms
step:1741/1845 train_time:105254ms step_avg:60.46ms
step:1742/1845 train_time:105341ms step_avg:60.47ms
step:1743/1845 train_time:105429ms step_avg:60.49ms
step:1744/1845 train_time:105517ms step_avg:60.50ms
step:1745/1845 train_time:105606ms step_avg:60.52ms
step:1746/1845 train_time:105693ms step_avg:60.53ms
step:1747/1845 train_time:105780ms step_avg:60.55ms
step:1748/1845 train_time:105867ms step_avg:60.56ms
step:1749/1845 train_time:105957ms step_avg:60.58ms
step:1750/1845 train_time:106044ms step_avg:60.60ms
step:1750/1845 val_loss:3.3037 train_time:106135ms step_avg:60.65ms
step:1751/1845 train_time:106155ms step_avg:60.63ms
step:1752/1845 train_time:106226ms step_avg:60.63ms
step:1753/1845 train_time:106316ms step_avg:60.65ms
step:1754/1845 train_time:106405ms step_avg:60.66ms
step:1755/1845 train_time:106494ms step_avg:60.68ms
step:1756/1845 train_time:106580ms step_avg:60.69ms
step:1757/1845 train_time:106667ms step_avg:60.71ms
step:1758/1845 train_time:106753ms step_avg:60.72ms
step:1759/1845 train_time:106840ms step_avg:60.74ms
step:1760/1845 train_time:106928ms step_avg:60.75ms
step:1761/1845 train_time:107015ms step_avg:60.77ms
step:1762/1845 train_time:107103ms step_avg:60.78ms
step:1763/1845 train_time:107194ms step_avg:60.80ms
step:1764/1845 train_time:107284ms step_avg:60.82ms
step:1765/1845 train_time:107373ms step_avg:60.83ms
step:1766/1845 train_time:107460ms step_avg:60.85ms
step:1767/1845 train_time:107549ms step_avg:60.87ms
step:1768/1845 train_time:107635ms step_avg:60.88ms
step:1769/1845 train_time:107723ms step_avg:60.89ms
step:1770/1845 train_time:107809ms step_avg:60.91ms
step:1771/1845 train_time:107897ms step_avg:60.92ms
step:1772/1845 train_time:107984ms step_avg:60.94ms
step:1773/1845 train_time:108072ms step_avg:60.95ms
step:1774/1845 train_time:108161ms step_avg:60.97ms
step:1775/1845 train_time:108250ms step_avg:60.99ms
step:1776/1845 train_time:108339ms step_avg:61.00ms
step:1777/1845 train_time:108428ms step_avg:61.02ms
step:1778/1845 train_time:108515ms step_avg:61.03ms
step:1779/1845 train_time:108603ms step_avg:61.05ms
step:1780/1845 train_time:108690ms step_avg:61.06ms
step:1781/1845 train_time:108777ms step_avg:61.08ms
step:1782/1845 train_time:108864ms step_avg:61.09ms
step:1783/1845 train_time:108952ms step_avg:61.11ms
step:1784/1845 train_time:109039ms step_avg:61.12ms
step:1785/1845 train_time:109129ms step_avg:61.14ms
step:1786/1845 train_time:109217ms step_avg:61.15ms
step:1787/1845 train_time:109306ms step_avg:61.17ms
step:1788/1845 train_time:109395ms step_avg:61.18ms
step:1789/1845 train_time:109483ms step_avg:61.20ms
step:1790/1845 train_time:109571ms step_avg:61.21ms
step:1791/1845 train_time:109658ms step_avg:61.23ms
step:1792/1845 train_time:109746ms step_avg:61.24ms
step:1793/1845 train_time:109833ms step_avg:61.26ms
step:1794/1845 train_time:109920ms step_avg:61.27ms
step:1795/1845 train_time:110008ms step_avg:61.29ms
step:1796/1845 train_time:110095ms step_avg:61.30ms
step:1797/1845 train_time:110184ms step_avg:61.32ms
step:1798/1845 train_time:110272ms step_avg:61.33ms
step:1799/1845 train_time:110361ms step_avg:61.35ms
step:1800/1845 train_time:110448ms step_avg:61.36ms
step:1801/1845 train_time:110536ms step_avg:61.37ms
step:1802/1845 train_time:110623ms step_avg:61.39ms
step:1803/1845 train_time:110712ms step_avg:61.40ms
step:1804/1845 train_time:110799ms step_avg:61.42ms
step:1805/1845 train_time:110887ms step_avg:61.43ms
step:1806/1845 train_time:110974ms step_avg:61.45ms
step:1807/1845 train_time:111063ms step_avg:61.46ms
step:1808/1845 train_time:111152ms step_avg:61.48ms
step:1809/1845 train_time:111242ms step_avg:61.49ms
step:1810/1845 train_time:111330ms step_avg:61.51ms
step:1811/1845 train_time:111419ms step_avg:61.52ms
step:1812/1845 train_time:111506ms step_avg:61.54ms
step:1813/1845 train_time:111595ms step_avg:61.55ms
step:1814/1845 train_time:111682ms step_avg:61.57ms
step:1815/1845 train_time:111772ms step_avg:61.58ms
step:1816/1845 train_time:111859ms step_avg:61.60ms
step:1817/1845 train_time:111947ms step_avg:61.61ms
step:1818/1845 train_time:112035ms step_avg:61.63ms
step:1819/1845 train_time:112123ms step_avg:61.64ms
step:1820/1845 train_time:112211ms step_avg:61.65ms
step:1821/1845 train_time:112300ms step_avg:61.67ms
step:1822/1845 train_time:112387ms step_avg:61.68ms
step:1823/1845 train_time:112476ms step_avg:61.70ms
step:1824/1845 train_time:112563ms step_avg:61.71ms
step:1825/1845 train_time:112653ms step_avg:61.73ms
step:1826/1845 train_time:112740ms step_avg:61.74ms
step:1827/1845 train_time:112829ms step_avg:61.76ms
step:1828/1845 train_time:112915ms step_avg:61.77ms
step:1829/1845 train_time:113005ms step_avg:61.78ms
step:1830/1845 train_time:113092ms step_avg:61.80ms
step:1831/1845 train_time:113182ms step_avg:61.81ms
step:1832/1845 train_time:113270ms step_avg:61.83ms
step:1833/1845 train_time:113358ms step_avg:61.84ms
step:1834/1845 train_time:113446ms step_avg:61.86ms
step:1835/1845 train_time:113535ms step_avg:61.87ms
step:1836/1845 train_time:113625ms step_avg:61.89ms
step:1837/1845 train_time:113714ms step_avg:61.90ms
step:1838/1845 train_time:113801ms step_avg:61.92ms
step:1839/1845 train_time:113890ms step_avg:61.93ms
step:1840/1845 train_time:113977ms step_avg:61.94ms
step:1841/1845 train_time:114068ms step_avg:61.96ms
step:1842/1845 train_time:114154ms step_avg:61.97ms
step:1843/1845 train_time:114243ms step_avg:61.99ms
step:1844/1845 train_time:114331ms step_avg:62.00ms
step:1845/1845 train_time:114420ms step_avg:62.02ms
step:1845/1845 val_loss:3.2790 train_time:114508ms step_avg:62.06ms
peak memory allocated: 29405 MiB reserved: 45118 MiB

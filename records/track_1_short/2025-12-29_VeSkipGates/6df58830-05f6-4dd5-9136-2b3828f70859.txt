import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc
os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        group_sizes = [15, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

        # only include gates on layers with value embeds used on forward pass
        if layer_idx in [0,1,8,9,10]:
            self.value_embed_gate = CastedLinear(12, num_heads)
            self.value_embed_gate.weight.label = 'value_embed_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(self.value_embed_gate(x[..., :self.value_embed_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        #self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig, skew = None):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if skew is None:
            logits = 23 * torch.sigmoid((logits+5) / 7.5)
        else:
            logits = 23 * torch.sigmoid((logits+skew) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 29 06:03:30 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   30C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0            110W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            107W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8300 train_time:0ms step_avg:0.04ms
step:1/1845 train_time:89ms step_avg:89.27ms
step:2/1845 train_time:112ms step_avg:56.20ms
step:3/1845 train_time:135ms step_avg:45.11ms
step:4/1845 train_time:169ms step_avg:42.34ms
step:5/1845 train_time:204ms step_avg:40.72ms
step:6/1845 train_time:286ms step_avg:47.65ms
step:7/1845 train_time:431ms step_avg:61.55ms
step:8/1845 train_time:465ms step_avg:58.10ms
step:9/1845 train_time:499ms step_avg:55.45ms
step:10/1845 train_time:533ms step_avg:53.29ms
step:11/1845 train_time:568ms step_avg:51.60ms
step:12/1845 train_time:601ms step_avg:50.12ms
step:13/1845 train_time:636ms step_avg:48.93ms
step:14/1845 train_time:670ms step_avg:47.89ms
step:15/1845 train_time:705ms step_avg:46.98ms
step:16/1845 train_time:739ms step_avg:46.17ms
step:17/1845 train_time:773ms step_avg:45.49ms
step:18/1845 train_time:808ms step_avg:44.86ms
step:19/1845 train_time:842ms step_avg:44.34ms
step:20/1845 train_time:876ms step_avg:43.82ms
step:21/1845 train_time:911ms step_avg:43.39ms
step:22/1845 train_time:945ms step_avg:42.96ms
step:23/1845 train_time:980ms step_avg:42.61ms
step:24/1845 train_time:1014ms step_avg:42.26ms
step:25/1845 train_time:1049ms step_avg:41.95ms
step:26/1845 train_time:1083ms step_avg:41.65ms
step:27/1845 train_time:1117ms step_avg:41.39ms
step:28/1845 train_time:1152ms step_avg:41.13ms
step:29/1845 train_time:1186ms step_avg:40.90ms
step:30/1845 train_time:1220ms step_avg:40.67ms
step:31/1845 train_time:1255ms step_avg:40.48ms
step:32/1845 train_time:1289ms step_avg:40.28ms
step:33/1845 train_time:1324ms step_avg:40.11ms
step:34/1845 train_time:1358ms step_avg:39.93ms
step:35/1845 train_time:1393ms step_avg:39.79ms
step:36/1845 train_time:1427ms step_avg:39.64ms
step:37/1845 train_time:1462ms step_avg:39.51ms
step:38/1845 train_time:1496ms step_avg:39.37ms
step:39/1845 train_time:1531ms step_avg:39.25ms
step:40/1845 train_time:1565ms step_avg:39.13ms
step:41/1845 train_time:1601ms step_avg:39.04ms
step:42/1845 train_time:1635ms step_avg:38.92ms
step:43/1845 train_time:1670ms step_avg:38.84ms
step:44/1845 train_time:1704ms step_avg:38.73ms
step:45/1845 train_time:1739ms step_avg:38.64ms
step:46/1845 train_time:1773ms step_avg:38.55ms
step:47/1845 train_time:1808ms step_avg:38.47ms
step:48/1845 train_time:1842ms step_avg:38.38ms
step:49/1845 train_time:1877ms step_avg:38.30ms
step:50/1845 train_time:1911ms step_avg:38.22ms
step:51/1845 train_time:1946ms step_avg:38.15ms
step:52/1845 train_time:1980ms step_avg:38.07ms
step:53/1845 train_time:2015ms step_avg:38.01ms
step:54/1845 train_time:2049ms step_avg:37.94ms
step:55/1845 train_time:2083ms step_avg:37.87ms
step:56/1845 train_time:2117ms step_avg:37.81ms
step:57/1845 train_time:2152ms step_avg:37.75ms
step:58/1845 train_time:2186ms step_avg:37.69ms
step:59/1845 train_time:2220ms step_avg:37.63ms
step:60/1845 train_time:2254ms step_avg:37.57ms
step:61/1845 train_time:2289ms step_avg:37.52ms
step:62/1845 train_time:2323ms step_avg:37.47ms
step:63/1845 train_time:2358ms step_avg:37.43ms
step:64/1845 train_time:2392ms step_avg:37.38ms
step:65/1845 train_time:2427ms step_avg:37.34ms
step:66/1845 train_time:2461ms step_avg:37.29ms
step:67/1845 train_time:2496ms step_avg:37.25ms
step:68/1845 train_time:2530ms step_avg:37.20ms
step:69/1845 train_time:2564ms step_avg:37.16ms
step:70/1845 train_time:2598ms step_avg:37.12ms
step:71/1845 train_time:2633ms step_avg:37.08ms
step:72/1845 train_time:2667ms step_avg:37.04ms
step:73/1845 train_time:2702ms step_avg:37.01ms
step:74/1845 train_time:2736ms step_avg:36.97ms
step:75/1845 train_time:2771ms step_avg:36.94ms
step:76/1845 train_time:2805ms step_avg:36.90ms
step:77/1845 train_time:2839ms step_avg:36.88ms
step:78/1845 train_time:2873ms step_avg:36.84ms
step:79/1845 train_time:2908ms step_avg:36.81ms
step:80/1845 train_time:2942ms step_avg:36.78ms
step:81/1845 train_time:2977ms step_avg:36.75ms
step:82/1845 train_time:3011ms step_avg:36.72ms
step:83/1845 train_time:3046ms step_avg:36.69ms
step:84/1845 train_time:3080ms step_avg:36.66ms
step:85/1845 train_time:3114ms step_avg:36.64ms
step:86/1845 train_time:3149ms step_avg:36.61ms
step:87/1845 train_time:3184ms step_avg:36.59ms
step:88/1845 train_time:3218ms step_avg:36.56ms
step:89/1845 train_time:3252ms step_avg:36.54ms
step:90/1845 train_time:3286ms step_avg:36.51ms
step:91/1845 train_time:3321ms step_avg:36.50ms
step:92/1845 train_time:3355ms step_avg:36.47ms
step:93/1845 train_time:3390ms step_avg:36.45ms
step:94/1845 train_time:3424ms step_avg:36.43ms
step:95/1845 train_time:3459ms step_avg:36.41ms
step:96/1845 train_time:3493ms step_avg:36.39ms
step:97/1845 train_time:3528ms step_avg:36.37ms
step:98/1845 train_time:3562ms step_avg:36.35ms
step:99/1845 train_time:3597ms step_avg:36.33ms
step:100/1845 train_time:3631ms step_avg:36.31ms
step:101/1845 train_time:3666ms step_avg:36.29ms
step:102/1845 train_time:3700ms step_avg:36.27ms
step:103/1845 train_time:3734ms step_avg:36.25ms
step:104/1845 train_time:3769ms step_avg:36.24ms
step:105/1845 train_time:3803ms step_avg:36.22ms
step:106/1845 train_time:3838ms step_avg:36.20ms
step:107/1845 train_time:3872ms step_avg:36.19ms
step:108/1845 train_time:3906ms step_avg:36.17ms
step:109/1845 train_time:3941ms step_avg:36.16ms
step:110/1845 train_time:3975ms step_avg:36.14ms
step:111/1845 train_time:4010ms step_avg:36.12ms
step:112/1845 train_time:4044ms step_avg:36.11ms
step:113/1845 train_time:4079ms step_avg:36.10ms
step:114/1845 train_time:4113ms step_avg:36.08ms
step:115/1845 train_time:4148ms step_avg:36.07ms
step:116/1845 train_time:4182ms step_avg:36.05ms
step:117/1845 train_time:4217ms step_avg:36.04ms
step:118/1845 train_time:4251ms step_avg:36.02ms
step:119/1845 train_time:4286ms step_avg:36.01ms
step:120/1845 train_time:4320ms step_avg:36.00ms
step:121/1845 train_time:4354ms step_avg:35.99ms
step:122/1845 train_time:4389ms step_avg:35.97ms
step:123/1845 train_time:4423ms step_avg:35.96ms
step:124/1845 train_time:4457ms step_avg:35.95ms
step:125/1845 train_time:4492ms step_avg:35.94ms
step:126/1845 train_time:4527ms step_avg:35.92ms
step:127/1845 train_time:4562ms step_avg:35.92ms
step:128/1845 train_time:4596ms step_avg:35.91ms
step:129/1845 train_time:4631ms step_avg:35.90ms
step:130/1845 train_time:4665ms step_avg:35.88ms
step:131/1845 train_time:4700ms step_avg:35.88ms
step:132/1845 train_time:4734ms step_avg:35.86ms
step:133/1845 train_time:4769ms step_avg:35.85ms
step:134/1845 train_time:4803ms step_avg:35.84ms
step:135/1845 train_time:4838ms step_avg:35.83ms
step:136/1845 train_time:4872ms step_avg:35.82ms
step:137/1845 train_time:4906ms step_avg:35.81ms
step:138/1845 train_time:4940ms step_avg:35.80ms
step:139/1845 train_time:4975ms step_avg:35.79ms
step:140/1845 train_time:5009ms step_avg:35.78ms
step:141/1845 train_time:5044ms step_avg:35.77ms
step:142/1845 train_time:5078ms step_avg:35.76ms
step:143/1845 train_time:5112ms step_avg:35.75ms
step:144/1845 train_time:5146ms step_avg:35.74ms
step:145/1845 train_time:5181ms step_avg:35.73ms
step:146/1845 train_time:5215ms step_avg:35.72ms
step:147/1845 train_time:5250ms step_avg:35.71ms
step:148/1845 train_time:5284ms step_avg:35.70ms
step:149/1845 train_time:5318ms step_avg:35.69ms
step:150/1845 train_time:5352ms step_avg:35.68ms
step:151/1845 train_time:5387ms step_avg:35.68ms
step:152/1845 train_time:5421ms step_avg:35.66ms
step:153/1845 train_time:5456ms step_avg:35.66ms
step:154/1845 train_time:5490ms step_avg:35.65ms
step:155/1845 train_time:5524ms step_avg:35.64ms
step:156/1845 train_time:5558ms step_avg:35.63ms
step:157/1845 train_time:5593ms step_avg:35.62ms
step:158/1845 train_time:5627ms step_avg:35.61ms
step:159/1845 train_time:5662ms step_avg:35.61ms
step:160/1845 train_time:5696ms step_avg:35.60ms
step:161/1845 train_time:5730ms step_avg:35.59ms
step:162/1845 train_time:5764ms step_avg:35.58ms
step:163/1845 train_time:5799ms step_avg:35.58ms
step:164/1845 train_time:5833ms step_avg:35.57ms
step:165/1845 train_time:5868ms step_avg:35.56ms
step:166/1845 train_time:5902ms step_avg:35.55ms
step:167/1845 train_time:5937ms step_avg:35.55ms
step:168/1845 train_time:5971ms step_avg:35.54ms
step:169/1845 train_time:6006ms step_avg:35.54ms
step:170/1845 train_time:6040ms step_avg:35.53ms
step:171/1845 train_time:6075ms step_avg:35.52ms
step:172/1845 train_time:6109ms step_avg:35.52ms
step:173/1845 train_time:6144ms step_avg:35.51ms
step:174/1845 train_time:6177ms step_avg:35.50ms
step:175/1845 train_time:6212ms step_avg:35.50ms
step:176/1845 train_time:6246ms step_avg:35.49ms
step:177/1845 train_time:6281ms step_avg:35.49ms
step:178/1845 train_time:6315ms step_avg:35.48ms
step:179/1845 train_time:6350ms step_avg:35.47ms
step:180/1845 train_time:6384ms step_avg:35.47ms
step:181/1845 train_time:6419ms step_avg:35.46ms
step:182/1845 train_time:6453ms step_avg:35.46ms
step:183/1845 train_time:6488ms step_avg:35.45ms
step:184/1845 train_time:6522ms step_avg:35.44ms
step:185/1845 train_time:6556ms step_avg:35.44ms
step:186/1845 train_time:6590ms step_avg:35.43ms
step:187/1845 train_time:6625ms step_avg:35.43ms
step:188/1845 train_time:6659ms step_avg:35.42ms
step:189/1845 train_time:6694ms step_avg:35.42ms
step:190/1845 train_time:6728ms step_avg:35.41ms
step:191/1845 train_time:6763ms step_avg:35.41ms
step:192/1845 train_time:6797ms step_avg:35.40ms
step:193/1845 train_time:6831ms step_avg:35.39ms
step:194/1845 train_time:6865ms step_avg:35.39ms
step:195/1845 train_time:6900ms step_avg:35.38ms
step:196/1845 train_time:6934ms step_avg:35.38ms
step:197/1845 train_time:6969ms step_avg:35.37ms
step:198/1845 train_time:7003ms step_avg:35.37ms
step:199/1845 train_time:7037ms step_avg:35.36ms
step:200/1845 train_time:7071ms step_avg:35.36ms
step:201/1845 train_time:7106ms step_avg:35.35ms
step:202/1845 train_time:7140ms step_avg:35.35ms
step:203/1845 train_time:7175ms step_avg:35.34ms
step:204/1845 train_time:7210ms step_avg:35.34ms
step:205/1845 train_time:7243ms step_avg:35.33ms
step:206/1845 train_time:7277ms step_avg:35.33ms
step:207/1845 train_time:7312ms step_avg:35.32ms
step:208/1845 train_time:7346ms step_avg:35.32ms
step:209/1845 train_time:7381ms step_avg:35.32ms
step:210/1845 train_time:7415ms step_avg:35.31ms
step:211/1845 train_time:7450ms step_avg:35.31ms
step:212/1845 train_time:7484ms step_avg:35.30ms
step:213/1845 train_time:7518ms step_avg:35.30ms
step:214/1845 train_time:7552ms step_avg:35.29ms
step:215/1845 train_time:7587ms step_avg:35.29ms
step:216/1845 train_time:7621ms step_avg:35.28ms
step:217/1845 train_time:7656ms step_avg:35.28ms
step:218/1845 train_time:7690ms step_avg:35.27ms
step:219/1845 train_time:7724ms step_avg:35.27ms
step:220/1845 train_time:7758ms step_avg:35.27ms
step:221/1845 train_time:7793ms step_avg:35.26ms
step:222/1845 train_time:7827ms step_avg:35.26ms
step:223/1845 train_time:7861ms step_avg:35.25ms
step:224/1845 train_time:7895ms step_avg:35.25ms
step:225/1845 train_time:7930ms step_avg:35.24ms
step:226/1845 train_time:7964ms step_avg:35.24ms
step:227/1845 train_time:7998ms step_avg:35.23ms
step:228/1845 train_time:8032ms step_avg:35.23ms
step:229/1845 train_time:8067ms step_avg:35.23ms
step:230/1845 train_time:8101ms step_avg:35.22ms
step:231/1845 train_time:8136ms step_avg:35.22ms
step:232/1845 train_time:8169ms step_avg:35.21ms
step:233/1845 train_time:8204ms step_avg:35.21ms
step:234/1845 train_time:8238ms step_avg:35.21ms
step:235/1845 train_time:8273ms step_avg:35.20ms
step:236/1845 train_time:8307ms step_avg:35.20ms
step:237/1845 train_time:8341ms step_avg:35.20ms
step:238/1845 train_time:8376ms step_avg:35.19ms
step:239/1845 train_time:8410ms step_avg:35.19ms
step:240/1845 train_time:8444ms step_avg:35.18ms
step:241/1845 train_time:8479ms step_avg:35.18ms
step:242/1845 train_time:8513ms step_avg:35.18ms
step:243/1845 train_time:8547ms step_avg:35.17ms
step:244/1845 train_time:8581ms step_avg:35.17ms
step:245/1845 train_time:8616ms step_avg:35.17ms
step:246/1845 train_time:8650ms step_avg:35.16ms
step:247/1845 train_time:8685ms step_avg:35.16ms
step:248/1845 train_time:8718ms step_avg:35.16ms
step:249/1845 train_time:8753ms step_avg:35.15ms
step:250/1845 train_time:8787ms step_avg:35.15ms
step:250/1845 val_loss:4.6095 train_time:8823ms step_avg:35.29ms
step:251/1845 train_time:8844ms step_avg:35.24ms
step:252/1845 train_time:8864ms step_avg:35.17ms
step:253/1845 train_time:8893ms step_avg:35.15ms
step:254/1845 train_time:8927ms step_avg:35.15ms
step:255/1845 train_time:8963ms step_avg:35.15ms
step:256/1845 train_time:8998ms step_avg:35.15ms
step:257/1845 train_time:9032ms step_avg:35.15ms
step:258/1845 train_time:9067ms step_avg:35.14ms
step:259/1845 train_time:9101ms step_avg:35.14ms
step:260/1845 train_time:9135ms step_avg:35.13ms
step:261/1845 train_time:9170ms step_avg:35.13ms
step:262/1845 train_time:9204ms step_avg:35.13ms
step:263/1845 train_time:9238ms step_avg:35.13ms
step:264/1845 train_time:9272ms step_avg:35.12ms
step:265/1845 train_time:9306ms step_avg:35.12ms
step:266/1845 train_time:9340ms step_avg:35.11ms
step:267/1845 train_time:9374ms step_avg:35.11ms
step:268/1845 train_time:9408ms step_avg:35.11ms
step:269/1845 train_time:9442ms step_avg:35.10ms
step:270/1845 train_time:9476ms step_avg:35.10ms
step:271/1845 train_time:9511ms step_avg:35.10ms
step:272/1845 train_time:9545ms step_avg:35.09ms
step:273/1845 train_time:9579ms step_avg:35.09ms
step:274/1845 train_time:9613ms step_avg:35.08ms
step:275/1845 train_time:9647ms step_avg:35.08ms
step:276/1845 train_time:9681ms step_avg:35.08ms
step:277/1845 train_time:9716ms step_avg:35.07ms
step:278/1845 train_time:9750ms step_avg:35.07ms
step:279/1845 train_time:9784ms step_avg:35.07ms
step:280/1845 train_time:9818ms step_avg:35.06ms
step:281/1845 train_time:9853ms step_avg:35.06ms
step:282/1845 train_time:9887ms step_avg:35.06ms
step:283/1845 train_time:9922ms step_avg:35.06ms
step:284/1845 train_time:9956ms step_avg:35.06ms
step:285/1845 train_time:9991ms step_avg:35.06ms
step:286/1845 train_time:10025ms step_avg:35.05ms
step:287/1845 train_time:10060ms step_avg:35.05ms
step:288/1845 train_time:10094ms step_avg:35.05ms
step:289/1845 train_time:10129ms step_avg:35.05ms
step:290/1845 train_time:10163ms step_avg:35.04ms
step:291/1845 train_time:10197ms step_avg:35.04ms
step:292/1845 train_time:10231ms step_avg:35.04ms
step:293/1845 train_time:10266ms step_avg:35.04ms
step:294/1845 train_time:10300ms step_avg:35.03ms
step:295/1845 train_time:10334ms step_avg:35.03ms
step:296/1845 train_time:10368ms step_avg:35.03ms
step:297/1845 train_time:10402ms step_avg:35.03ms
step:298/1845 train_time:10436ms step_avg:35.02ms
step:299/1845 train_time:10471ms step_avg:35.02ms
step:300/1845 train_time:10505ms step_avg:35.02ms
step:301/1845 train_time:10539ms step_avg:35.01ms
step:302/1845 train_time:10573ms step_avg:35.01ms
step:303/1845 train_time:10608ms step_avg:35.01ms
step:304/1845 train_time:10642ms step_avg:35.01ms
step:305/1845 train_time:10676ms step_avg:35.00ms
step:306/1845 train_time:10710ms step_avg:35.00ms
step:307/1845 train_time:10744ms step_avg:35.00ms
step:308/1845 train_time:10778ms step_avg:34.99ms
step:309/1845 train_time:10813ms step_avg:34.99ms
step:310/1845 train_time:10847ms step_avg:34.99ms
step:311/1845 train_time:10881ms step_avg:34.99ms
step:312/1845 train_time:10915ms step_avg:34.98ms
step:313/1845 train_time:10950ms step_avg:34.98ms
step:314/1845 train_time:10984ms step_avg:34.98ms
step:315/1845 train_time:11018ms step_avg:34.98ms
step:316/1845 train_time:11052ms step_avg:34.97ms
step:317/1845 train_time:11087ms step_avg:34.97ms
step:318/1845 train_time:11121ms step_avg:34.97ms
step:319/1845 train_time:11155ms step_avg:34.97ms
step:320/1845 train_time:11189ms step_avg:34.97ms
step:321/1845 train_time:11224ms step_avg:34.97ms
step:322/1845 train_time:11258ms step_avg:34.96ms
step:323/1845 train_time:11292ms step_avg:34.96ms
step:324/1845 train_time:11326ms step_avg:34.96ms
step:325/1845 train_time:11361ms step_avg:34.96ms
step:326/1845 train_time:11395ms step_avg:34.95ms
step:327/1845 train_time:11430ms step_avg:34.95ms
step:328/1845 train_time:11464ms step_avg:34.95ms
step:329/1845 train_time:11498ms step_avg:34.95ms
step:330/1845 train_time:11532ms step_avg:34.94ms
step:331/1845 train_time:11566ms step_avg:34.94ms
step:332/1845 train_time:11600ms step_avg:34.94ms
step:333/1845 train_time:11635ms step_avg:34.94ms
step:334/1845 train_time:11669ms step_avg:34.94ms
step:335/1845 train_time:11703ms step_avg:34.94ms
step:336/1845 train_time:11737ms step_avg:34.93ms
step:337/1845 train_time:11772ms step_avg:34.93ms
step:338/1845 train_time:11805ms step_avg:34.93ms
step:339/1845 train_time:11840ms step_avg:34.93ms
step:340/1845 train_time:11874ms step_avg:34.92ms
step:341/1845 train_time:11908ms step_avg:34.92ms
step:342/1845 train_time:11942ms step_avg:34.92ms
step:343/1845 train_time:11976ms step_avg:34.92ms
step:344/1845 train_time:12010ms step_avg:34.91ms
step:345/1845 train_time:12045ms step_avg:34.91ms
step:346/1845 train_time:12079ms step_avg:34.91ms
step:347/1845 train_time:12114ms step_avg:34.91ms
step:348/1845 train_time:12148ms step_avg:34.91ms
step:349/1845 train_time:12182ms step_avg:34.91ms
step:350/1845 train_time:12216ms step_avg:34.90ms
step:351/1845 train_time:12250ms step_avg:34.90ms
step:352/1845 train_time:12284ms step_avg:34.90ms
step:353/1845 train_time:12319ms step_avg:34.90ms
step:354/1845 train_time:12353ms step_avg:34.89ms
step:355/1845 train_time:12388ms step_avg:34.89ms
step:356/1845 train_time:12421ms step_avg:34.89ms
step:357/1845 train_time:12456ms step_avg:34.89ms
step:358/1845 train_time:12490ms step_avg:34.89ms
step:359/1845 train_time:12524ms step_avg:34.89ms
step:360/1845 train_time:12558ms step_avg:34.88ms
step:361/1845 train_time:12593ms step_avg:34.88ms
step:362/1845 train_time:12627ms step_avg:34.88ms
step:363/1845 train_time:12661ms step_avg:34.88ms
step:364/1845 train_time:12695ms step_avg:34.88ms
step:365/1845 train_time:12730ms step_avg:34.88ms
step:366/1845 train_time:12764ms step_avg:34.87ms
step:367/1845 train_time:12798ms step_avg:34.87ms
step:368/1845 train_time:12832ms step_avg:34.87ms
step:369/1845 train_time:12866ms step_avg:34.87ms
step:370/1845 train_time:12900ms step_avg:34.87ms
step:371/1845 train_time:12935ms step_avg:34.86ms
step:372/1845 train_time:12969ms step_avg:34.86ms
step:373/1845 train_time:13003ms step_avg:34.86ms
step:374/1845 train_time:13037ms step_avg:34.86ms
step:375/1845 train_time:13071ms step_avg:34.86ms
step:376/1845 train_time:13105ms step_avg:34.85ms
step:377/1845 train_time:13139ms step_avg:34.85ms
step:378/1845 train_time:13173ms step_avg:34.85ms
step:379/1845 train_time:13208ms step_avg:34.85ms
step:380/1845 train_time:13242ms step_avg:34.85ms
step:381/1845 train_time:13276ms step_avg:34.85ms
step:382/1845 train_time:13310ms step_avg:34.84ms
step:383/1845 train_time:13345ms step_avg:34.84ms
step:384/1845 train_time:13379ms step_avg:34.84ms
step:385/1845 train_time:13413ms step_avg:34.84ms
step:386/1845 train_time:13447ms step_avg:34.84ms
step:387/1845 train_time:13482ms step_avg:34.84ms
step:388/1845 train_time:13515ms step_avg:34.83ms
step:389/1845 train_time:13550ms step_avg:34.83ms
step:390/1845 train_time:13584ms step_avg:34.83ms
step:391/1845 train_time:13618ms step_avg:34.83ms
step:392/1845 train_time:13652ms step_avg:34.83ms
step:393/1845 train_time:13687ms step_avg:34.83ms
step:394/1845 train_time:13721ms step_avg:34.82ms
step:395/1845 train_time:13755ms step_avg:34.82ms
step:396/1845 train_time:13789ms step_avg:34.82ms
step:397/1845 train_time:13824ms step_avg:34.82ms
step:398/1845 train_time:13858ms step_avg:34.82ms
step:399/1845 train_time:13892ms step_avg:34.82ms
step:400/1845 train_time:13926ms step_avg:34.82ms
step:401/1845 train_time:13961ms step_avg:34.82ms
step:402/1845 train_time:13995ms step_avg:34.81ms
step:403/1845 train_time:14029ms step_avg:34.81ms
step:404/1845 train_time:14063ms step_avg:34.81ms
step:405/1845 train_time:14097ms step_avg:34.81ms
step:406/1845 train_time:14132ms step_avg:34.81ms
step:407/1845 train_time:14166ms step_avg:34.81ms
step:408/1845 train_time:14200ms step_avg:34.80ms
step:409/1845 train_time:14234ms step_avg:34.80ms
step:410/1845 train_time:14268ms step_avg:34.80ms
step:411/1845 train_time:14302ms step_avg:34.80ms
step:412/1845 train_time:14337ms step_avg:34.80ms
step:413/1845 train_time:14371ms step_avg:34.80ms
step:414/1845 train_time:14405ms step_avg:34.80ms
step:415/1845 train_time:14440ms step_avg:34.80ms
step:416/1845 train_time:14474ms step_avg:34.79ms
step:417/1845 train_time:14509ms step_avg:34.79ms
step:418/1845 train_time:14543ms step_avg:34.79ms
step:419/1845 train_time:14577ms step_avg:34.79ms
step:420/1845 train_time:14611ms step_avg:34.79ms
step:421/1845 train_time:14646ms step_avg:34.79ms
step:422/1845 train_time:14680ms step_avg:34.79ms
step:423/1845 train_time:14714ms step_avg:34.79ms
step:424/1845 train_time:14748ms step_avg:34.78ms
step:425/1845 train_time:14782ms step_avg:34.78ms
step:426/1845 train_time:14816ms step_avg:34.78ms
step:427/1845 train_time:14851ms step_avg:34.78ms
step:428/1845 train_time:14885ms step_avg:34.78ms
step:429/1845 train_time:14919ms step_avg:34.78ms
step:430/1845 train_time:14953ms step_avg:34.77ms
step:431/1845 train_time:14987ms step_avg:34.77ms
step:432/1845 train_time:15021ms step_avg:34.77ms
step:433/1845 train_time:15056ms step_avg:34.77ms
step:434/1845 train_time:15090ms step_avg:34.77ms
step:435/1845 train_time:15124ms step_avg:34.77ms
step:436/1845 train_time:15158ms step_avg:34.77ms
step:437/1845 train_time:15193ms step_avg:34.77ms
step:438/1845 train_time:15227ms step_avg:34.76ms
step:439/1845 train_time:15261ms step_avg:34.76ms
step:440/1845 train_time:15295ms step_avg:34.76ms
step:441/1845 train_time:15330ms step_avg:34.76ms
step:442/1845 train_time:15364ms step_avg:34.76ms
step:443/1845 train_time:15398ms step_avg:34.76ms
step:444/1845 train_time:15432ms step_avg:34.76ms
step:445/1845 train_time:15466ms step_avg:34.76ms
step:446/1845 train_time:15500ms step_avg:34.75ms
step:447/1845 train_time:15535ms step_avg:34.75ms
step:448/1845 train_time:15569ms step_avg:34.75ms
step:449/1845 train_time:15603ms step_avg:34.75ms
step:450/1845 train_time:15637ms step_avg:34.75ms
step:451/1845 train_time:15672ms step_avg:34.75ms
step:452/1845 train_time:15706ms step_avg:34.75ms
step:453/1845 train_time:15740ms step_avg:34.75ms
step:454/1845 train_time:15774ms step_avg:34.75ms
step:455/1845 train_time:15809ms step_avg:34.74ms
step:456/1845 train_time:15843ms step_avg:34.74ms
step:457/1845 train_time:15877ms step_avg:34.74ms
step:458/1845 train_time:15911ms step_avg:34.74ms
step:459/1845 train_time:15945ms step_avg:34.74ms
step:460/1845 train_time:15979ms step_avg:34.74ms
step:461/1845 train_time:16014ms step_avg:34.74ms
step:462/1845 train_time:16048ms step_avg:34.73ms
step:463/1845 train_time:16082ms step_avg:34.73ms
step:464/1845 train_time:16116ms step_avg:34.73ms
step:465/1845 train_time:16150ms step_avg:34.73ms
step:466/1845 train_time:16184ms step_avg:34.73ms
step:467/1845 train_time:16218ms step_avg:34.73ms
step:468/1845 train_time:16252ms step_avg:34.73ms
step:469/1845 train_time:16287ms step_avg:34.73ms
step:470/1845 train_time:16321ms step_avg:34.72ms
step:471/1845 train_time:16355ms step_avg:34.72ms
step:472/1845 train_time:16389ms step_avg:34.72ms
step:473/1845 train_time:16424ms step_avg:34.72ms
step:474/1845 train_time:16458ms step_avg:34.72ms
step:475/1845 train_time:16492ms step_avg:34.72ms
step:476/1845 train_time:16526ms step_avg:34.72ms
step:477/1845 train_time:16560ms step_avg:34.72ms
step:478/1845 train_time:16595ms step_avg:34.72ms
step:479/1845 train_time:16629ms step_avg:34.72ms
step:480/1845 train_time:16663ms step_avg:34.71ms
step:481/1845 train_time:16698ms step_avg:34.71ms
step:482/1845 train_time:16732ms step_avg:34.71ms
step:483/1845 train_time:16766ms step_avg:34.71ms
step:484/1845 train_time:16800ms step_avg:34.71ms
step:485/1845 train_time:16835ms step_avg:34.71ms
step:486/1845 train_time:16869ms step_avg:34.71ms
step:487/1845 train_time:16903ms step_avg:34.71ms
step:488/1845 train_time:16937ms step_avg:34.71ms
step:489/1845 train_time:16972ms step_avg:34.71ms
step:490/1845 train_time:17006ms step_avg:34.71ms
step:491/1845 train_time:17040ms step_avg:34.70ms
step:492/1845 train_time:17074ms step_avg:34.70ms
step:493/1845 train_time:17108ms step_avg:34.70ms
step:494/1845 train_time:17142ms step_avg:34.70ms
step:495/1845 train_time:17177ms step_avg:34.70ms
step:496/1845 train_time:17211ms step_avg:34.70ms
step:497/1845 train_time:17245ms step_avg:34.70ms
step:498/1845 train_time:17279ms step_avg:34.70ms
step:499/1845 train_time:17313ms step_avg:34.70ms
step:500/1845 train_time:17347ms step_avg:34.69ms
step:500/1845 val_loss:4.2937 train_time:17384ms step_avg:34.77ms
step:501/1845 train_time:17403ms step_avg:34.74ms
step:502/1845 train_time:17422ms step_avg:34.71ms
step:503/1845 train_time:17453ms step_avg:34.70ms
step:504/1845 train_time:17487ms step_avg:34.70ms
step:505/1845 train_time:17522ms step_avg:34.70ms
step:506/1845 train_time:17556ms step_avg:34.70ms
step:507/1845 train_time:17591ms step_avg:34.70ms
step:508/1845 train_time:17625ms step_avg:34.69ms
step:509/1845 train_time:17659ms step_avg:34.69ms
step:510/1845 train_time:17693ms step_avg:34.69ms
step:511/1845 train_time:17727ms step_avg:34.69ms
step:512/1845 train_time:17761ms step_avg:34.69ms
step:513/1845 train_time:17796ms step_avg:34.69ms
step:514/1845 train_time:17830ms step_avg:34.69ms
step:515/1845 train_time:17864ms step_avg:34.69ms
step:516/1845 train_time:17898ms step_avg:34.69ms
step:517/1845 train_time:17932ms step_avg:34.69ms
step:518/1845 train_time:17966ms step_avg:34.68ms
step:519/1845 train_time:18000ms step_avg:34.68ms
step:520/1845 train_time:18034ms step_avg:34.68ms
step:521/1845 train_time:18069ms step_avg:34.68ms
step:522/1845 train_time:18103ms step_avg:34.68ms
step:523/1845 train_time:18137ms step_avg:34.68ms
step:524/1845 train_time:18171ms step_avg:34.68ms
step:525/1845 train_time:18206ms step_avg:34.68ms
step:526/1845 train_time:18239ms step_avg:34.68ms
step:527/1845 train_time:18274ms step_avg:34.68ms
step:528/1845 train_time:18308ms step_avg:34.67ms
step:529/1845 train_time:18342ms step_avg:34.67ms
step:530/1845 train_time:18376ms step_avg:34.67ms
step:531/1845 train_time:18411ms step_avg:34.67ms
step:532/1845 train_time:18445ms step_avg:34.67ms
step:533/1845 train_time:18480ms step_avg:34.67ms
step:534/1845 train_time:18514ms step_avg:34.67ms
step:535/1845 train_time:18548ms step_avg:34.67ms
step:536/1845 train_time:18582ms step_avg:34.67ms
step:537/1845 train_time:18616ms step_avg:34.67ms
step:538/1845 train_time:18650ms step_avg:34.67ms
step:539/1845 train_time:18685ms step_avg:34.67ms
step:540/1845 train_time:18719ms step_avg:34.66ms
step:541/1845 train_time:18753ms step_avg:34.66ms
step:542/1845 train_time:18787ms step_avg:34.66ms
step:543/1845 train_time:18822ms step_avg:34.66ms
step:544/1845 train_time:18856ms step_avg:34.66ms
step:545/1845 train_time:18890ms step_avg:34.66ms
step:546/1845 train_time:18924ms step_avg:34.66ms
step:547/1845 train_time:18958ms step_avg:34.66ms
step:548/1845 train_time:18992ms step_avg:34.66ms
step:549/1845 train_time:19027ms step_avg:34.66ms
step:550/1845 train_time:19061ms step_avg:34.66ms
step:551/1845 train_time:19095ms step_avg:34.66ms
step:552/1845 train_time:19129ms step_avg:34.65ms
step:553/1845 train_time:19163ms step_avg:34.65ms
step:554/1845 train_time:19197ms step_avg:34.65ms
step:555/1845 train_time:19232ms step_avg:34.65ms
step:556/1845 train_time:19266ms step_avg:34.65ms
step:557/1845 train_time:19300ms step_avg:34.65ms
step:558/1845 train_time:19334ms step_avg:34.65ms
step:559/1845 train_time:19368ms step_avg:34.65ms
step:560/1845 train_time:19402ms step_avg:34.65ms
step:561/1845 train_time:19437ms step_avg:34.65ms
step:562/1845 train_time:19471ms step_avg:34.65ms
step:563/1845 train_time:19506ms step_avg:34.65ms
step:564/1845 train_time:19540ms step_avg:34.65ms
step:565/1845 train_time:19574ms step_avg:34.64ms
step:566/1845 train_time:19608ms step_avg:34.64ms
step:567/1845 train_time:19642ms step_avg:34.64ms
step:568/1845 train_time:19676ms step_avg:34.64ms
step:569/1845 train_time:19711ms step_avg:34.64ms
step:570/1845 train_time:19745ms step_avg:34.64ms
step:571/1845 train_time:19779ms step_avg:34.64ms
step:572/1845 train_time:19813ms step_avg:34.64ms
step:573/1845 train_time:19848ms step_avg:34.64ms
step:574/1845 train_time:19882ms step_avg:34.64ms
step:575/1845 train_time:19916ms step_avg:34.64ms
step:576/1845 train_time:19950ms step_avg:34.64ms
step:577/1845 train_time:19984ms step_avg:34.64ms
step:578/1845 train_time:20018ms step_avg:34.63ms
step:579/1845 train_time:20053ms step_avg:34.63ms
step:580/1845 train_time:20087ms step_avg:34.63ms
step:581/1845 train_time:20121ms step_avg:34.63ms
step:582/1845 train_time:20155ms step_avg:34.63ms
step:583/1845 train_time:20190ms step_avg:34.63ms
step:584/1845 train_time:20224ms step_avg:34.63ms
step:585/1845 train_time:20258ms step_avg:34.63ms
step:586/1845 train_time:20292ms step_avg:34.63ms
step:587/1845 train_time:20327ms step_avg:34.63ms
step:588/1845 train_time:20361ms step_avg:34.63ms
step:589/1845 train_time:20395ms step_avg:34.63ms
step:590/1845 train_time:20429ms step_avg:34.63ms
step:591/1845 train_time:20463ms step_avg:34.62ms
step:592/1845 train_time:20497ms step_avg:34.62ms
step:593/1845 train_time:20532ms step_avg:34.62ms
step:594/1845 train_time:20566ms step_avg:34.62ms
step:595/1845 train_time:20600ms step_avg:34.62ms
step:596/1845 train_time:20634ms step_avg:34.62ms
step:597/1845 train_time:20668ms step_avg:34.62ms
step:598/1845 train_time:20702ms step_avg:34.62ms
step:599/1845 train_time:20737ms step_avg:34.62ms
step:600/1845 train_time:20771ms step_avg:34.62ms
step:601/1845 train_time:20805ms step_avg:34.62ms
step:602/1845 train_time:20839ms step_avg:34.62ms
step:603/1845 train_time:20874ms step_avg:34.62ms
step:604/1845 train_time:20935ms step_avg:34.66ms
step:605/1845 train_time:20997ms step_avg:34.71ms
step:606/1845 train_time:21058ms step_avg:34.75ms
step:607/1845 train_time:21120ms step_avg:34.79ms
step:608/1845 train_time:21181ms step_avg:34.84ms
step:609/1845 train_time:21244ms step_avg:34.88ms
step:610/1845 train_time:21305ms step_avg:34.93ms
step:611/1845 train_time:21368ms step_avg:34.97ms
step:612/1845 train_time:21429ms step_avg:35.01ms
step:613/1845 train_time:21491ms step_avg:35.06ms
step:614/1845 train_time:21552ms step_avg:35.10ms
step:615/1845 train_time:21615ms step_avg:35.15ms
step:616/1845 train_time:21677ms step_avg:35.19ms
step:617/1845 train_time:21739ms step_avg:35.23ms
step:618/1845 train_time:21799ms step_avg:35.27ms
step:619/1845 train_time:21862ms step_avg:35.32ms
step:620/1845 train_time:21923ms step_avg:35.36ms
step:621/1845 train_time:21985ms step_avg:35.40ms
step:622/1845 train_time:22046ms step_avg:35.44ms
step:623/1845 train_time:22108ms step_avg:35.49ms
step:624/1845 train_time:22169ms step_avg:35.53ms
step:625/1845 train_time:22232ms step_avg:35.57ms
step:626/1845 train_time:22293ms step_avg:35.61ms
step:627/1845 train_time:22356ms step_avg:35.66ms
step:628/1845 train_time:22418ms step_avg:35.70ms
step:629/1845 train_time:22480ms step_avg:35.74ms
step:630/1845 train_time:22541ms step_avg:35.78ms
step:631/1845 train_time:22605ms step_avg:35.82ms
step:632/1845 train_time:22666ms step_avg:35.86ms
step:633/1845 train_time:22728ms step_avg:35.91ms
step:634/1845 train_time:22789ms step_avg:35.94ms
step:635/1845 train_time:22851ms step_avg:35.99ms
step:636/1845 train_time:22912ms step_avg:36.03ms
step:637/1845 train_time:22975ms step_avg:36.07ms
step:638/1845 train_time:23036ms step_avg:36.11ms
step:639/1845 train_time:23099ms step_avg:36.15ms
step:640/1845 train_time:23160ms step_avg:36.19ms
step:641/1845 train_time:23222ms step_avg:36.23ms
step:642/1845 train_time:23283ms step_avg:36.27ms
step:643/1845 train_time:23346ms step_avg:36.31ms
step:644/1845 train_time:23406ms step_avg:36.35ms
step:645/1845 train_time:23469ms step_avg:36.39ms
step:646/1845 train_time:23529ms step_avg:36.42ms
step:647/1845 train_time:23593ms step_avg:36.47ms
step:648/1845 train_time:23654ms step_avg:36.50ms
step:649/1845 train_time:23717ms step_avg:36.54ms
step:650/1845 train_time:23778ms step_avg:36.58ms
step:651/1845 train_time:23840ms step_avg:36.62ms
step:652/1845 train_time:23901ms step_avg:36.66ms
step:653/1845 train_time:23964ms step_avg:36.70ms
step:654/1845 train_time:24025ms step_avg:36.74ms
step:655/1845 train_time:24087ms step_avg:36.77ms
step:656/1845 train_time:24147ms step_avg:36.81ms
step:657/1845 train_time:24209ms step_avg:36.85ms
step:658/1845 train_time:24270ms step_avg:36.88ms
step:659/1845 train_time:24333ms step_avg:36.92ms
step:660/1845 train_time:24394ms step_avg:36.96ms
step:661/1845 train_time:24456ms step_avg:37.00ms
step:662/1845 train_time:24517ms step_avg:37.04ms
step:663/1845 train_time:24580ms step_avg:37.07ms
step:664/1845 train_time:24641ms step_avg:37.11ms
step:665/1845 train_time:24704ms step_avg:37.15ms
step:666/1845 train_time:24764ms step_avg:37.18ms
step:667/1845 train_time:24827ms step_avg:37.22ms
step:668/1845 train_time:24888ms step_avg:37.26ms
step:669/1845 train_time:24950ms step_avg:37.29ms
step:670/1845 train_time:25011ms step_avg:37.33ms
step:671/1845 train_time:25073ms step_avg:37.37ms
step:672/1845 train_time:25134ms step_avg:37.40ms
step:673/1845 train_time:25196ms step_avg:37.44ms
step:674/1845 train_time:25257ms step_avg:37.47ms
step:675/1845 train_time:25320ms step_avg:37.51ms
step:676/1845 train_time:25380ms step_avg:37.54ms
step:677/1845 train_time:25443ms step_avg:37.58ms
step:678/1845 train_time:25504ms step_avg:37.62ms
step:679/1845 train_time:25566ms step_avg:37.65ms
step:680/1845 train_time:25626ms step_avg:37.69ms
step:681/1845 train_time:25689ms step_avg:37.72ms
step:682/1845 train_time:25749ms step_avg:37.76ms
step:683/1845 train_time:25812ms step_avg:37.79ms
step:684/1845 train_time:25873ms step_avg:37.83ms
step:685/1845 train_time:25935ms step_avg:37.86ms
step:686/1845 train_time:25996ms step_avg:37.90ms
step:687/1845 train_time:26059ms step_avg:37.93ms
step:688/1845 train_time:26120ms step_avg:37.96ms
step:689/1845 train_time:26183ms step_avg:38.00ms
step:690/1845 train_time:26244ms step_avg:38.03ms
step:691/1845 train_time:26306ms step_avg:38.07ms
step:692/1845 train_time:26366ms step_avg:38.10ms
step:693/1845 train_time:26429ms step_avg:38.14ms
step:694/1845 train_time:26490ms step_avg:38.17ms
step:695/1845 train_time:26553ms step_avg:38.21ms
step:696/1845 train_time:26614ms step_avg:38.24ms
step:697/1845 train_time:26676ms step_avg:38.27ms
step:698/1845 train_time:26737ms step_avg:38.31ms
step:699/1845 train_time:26800ms step_avg:38.34ms
step:700/1845 train_time:26861ms step_avg:38.37ms
step:701/1845 train_time:26923ms step_avg:38.41ms
step:702/1845 train_time:26984ms step_avg:38.44ms
step:703/1845 train_time:27047ms step_avg:38.47ms
step:704/1845 train_time:27108ms step_avg:38.51ms
step:705/1845 train_time:27170ms step_avg:38.54ms
step:706/1845 train_time:27230ms step_avg:38.57ms
step:707/1845 train_time:27293ms step_avg:38.60ms
step:708/1845 train_time:27355ms step_avg:38.64ms
step:709/1845 train_time:27417ms step_avg:38.67ms
step:710/1845 train_time:27479ms step_avg:38.70ms
step:711/1845 train_time:27541ms step_avg:38.74ms
step:712/1845 train_time:27602ms step_avg:38.77ms
step:713/1845 train_time:27665ms step_avg:38.80ms
step:714/1845 train_time:27726ms step_avg:38.83ms
step:715/1845 train_time:27789ms step_avg:38.87ms
step:716/1845 train_time:27849ms step_avg:38.90ms
step:717/1845 train_time:27912ms step_avg:38.93ms
step:718/1845 train_time:27974ms step_avg:38.96ms
step:719/1845 train_time:28036ms step_avg:38.99ms
step:720/1845 train_time:28097ms step_avg:39.02ms
step:721/1845 train_time:28159ms step_avg:39.06ms
step:722/1845 train_time:28220ms step_avg:39.09ms
step:723/1845 train_time:28283ms step_avg:39.12ms
step:724/1845 train_time:28344ms step_avg:39.15ms
step:725/1845 train_time:28406ms step_avg:39.18ms
step:726/1845 train_time:28467ms step_avg:39.21ms
step:727/1845 train_time:28529ms step_avg:39.24ms
step:728/1845 train_time:28591ms step_avg:39.27ms
step:729/1845 train_time:28653ms step_avg:39.31ms
step:730/1845 train_time:28715ms step_avg:39.34ms
step:731/1845 train_time:28777ms step_avg:39.37ms
step:732/1845 train_time:28838ms step_avg:39.40ms
step:733/1845 train_time:28901ms step_avg:39.43ms
step:734/1845 train_time:28962ms step_avg:39.46ms
step:735/1845 train_time:29025ms step_avg:39.49ms
step:736/1845 train_time:29085ms step_avg:39.52ms
step:737/1845 train_time:29147ms step_avg:39.55ms
step:738/1845 train_time:29208ms step_avg:39.58ms
step:739/1845 train_time:29270ms step_avg:39.61ms
step:740/1845 train_time:29331ms step_avg:39.64ms
step:741/1845 train_time:29394ms step_avg:39.67ms
step:742/1845 train_time:29455ms step_avg:39.70ms
step:743/1845 train_time:29517ms step_avg:39.73ms
step:744/1845 train_time:29578ms step_avg:39.76ms
step:745/1845 train_time:29641ms step_avg:39.79ms
step:746/1845 train_time:29702ms step_avg:39.81ms
step:747/1845 train_time:29764ms step_avg:39.85ms
step:748/1845 train_time:29825ms step_avg:39.87ms
step:749/1845 train_time:29888ms step_avg:39.90ms
step:750/1845 train_time:29948ms step_avg:39.93ms
step:750/1845 val_loss:4.0267 train_time:30012ms step_avg:40.02ms
step:751/1845 train_time:30039ms step_avg:40.00ms
step:752/1845 train_time:30074ms step_avg:39.99ms
step:753/1845 train_time:30138ms step_avg:40.02ms
step:754/1845 train_time:30200ms step_avg:40.05ms
step:755/1845 train_time:30262ms step_avg:40.08ms
step:756/1845 train_time:30323ms step_avg:40.11ms
step:757/1845 train_time:30385ms step_avg:40.14ms
step:758/1845 train_time:30445ms step_avg:40.16ms
step:759/1845 train_time:30507ms step_avg:40.19ms
step:760/1845 train_time:30567ms step_avg:40.22ms
step:761/1845 train_time:30630ms step_avg:40.25ms
step:762/1845 train_time:30689ms step_avg:40.27ms
step:763/1845 train_time:30751ms step_avg:40.30ms
step:764/1845 train_time:30812ms step_avg:40.33ms
step:765/1845 train_time:30873ms step_avg:40.36ms
step:766/1845 train_time:30934ms step_avg:40.38ms
step:767/1845 train_time:30998ms step_avg:40.41ms
step:768/1845 train_time:31060ms step_avg:40.44ms
step:769/1845 train_time:31123ms step_avg:40.47ms
step:770/1845 train_time:31184ms step_avg:40.50ms
step:771/1845 train_time:31247ms step_avg:40.53ms
step:772/1845 train_time:31308ms step_avg:40.55ms
step:773/1845 train_time:31370ms step_avg:40.58ms
step:774/1845 train_time:31430ms step_avg:40.61ms
step:775/1845 train_time:31492ms step_avg:40.63ms
step:776/1845 train_time:31552ms step_avg:40.66ms
step:777/1845 train_time:31614ms step_avg:40.69ms
step:778/1845 train_time:31675ms step_avg:40.71ms
step:779/1845 train_time:31738ms step_avg:40.74ms
step:780/1845 train_time:31799ms step_avg:40.77ms
step:781/1845 train_time:31861ms step_avg:40.80ms
step:782/1845 train_time:31923ms step_avg:40.82ms
step:783/1845 train_time:31985ms step_avg:40.85ms
step:784/1845 train_time:32046ms step_avg:40.88ms
step:785/1845 train_time:32109ms step_avg:40.90ms
step:786/1845 train_time:32170ms step_avg:40.93ms
step:787/1845 train_time:32234ms step_avg:40.96ms
step:788/1845 train_time:32294ms step_avg:40.98ms
step:789/1845 train_time:32357ms step_avg:41.01ms
step:790/1845 train_time:32417ms step_avg:41.03ms
step:791/1845 train_time:32480ms step_avg:41.06ms
step:792/1845 train_time:32541ms step_avg:41.09ms
step:793/1845 train_time:32603ms step_avg:41.11ms
step:794/1845 train_time:32663ms step_avg:41.14ms
step:795/1845 train_time:32726ms step_avg:41.16ms
step:796/1845 train_time:32786ms step_avg:41.19ms
step:797/1845 train_time:32848ms step_avg:41.21ms
step:798/1845 train_time:32909ms step_avg:41.24ms
step:799/1845 train_time:32972ms step_avg:41.27ms
step:800/1845 train_time:33032ms step_avg:41.29ms
step:801/1845 train_time:33095ms step_avg:41.32ms
step:802/1845 train_time:33156ms step_avg:41.34ms
step:803/1845 train_time:33219ms step_avg:41.37ms
step:804/1845 train_time:33280ms step_avg:41.39ms
step:805/1845 train_time:33343ms step_avg:41.42ms
step:806/1845 train_time:33404ms step_avg:41.44ms
step:807/1845 train_time:33466ms step_avg:41.47ms
step:808/1845 train_time:33527ms step_avg:41.49ms
step:809/1845 train_time:33589ms step_avg:41.52ms
step:810/1845 train_time:33650ms step_avg:41.54ms
step:811/1845 train_time:33712ms step_avg:41.57ms
step:812/1845 train_time:33772ms step_avg:41.59ms
step:813/1845 train_time:33835ms step_avg:41.62ms
step:814/1845 train_time:33895ms step_avg:41.64ms
step:815/1845 train_time:33957ms step_avg:41.67ms
step:816/1845 train_time:34019ms step_avg:41.69ms
step:817/1845 train_time:34081ms step_avg:41.71ms
step:818/1845 train_time:34143ms step_avg:41.74ms
step:819/1845 train_time:34205ms step_avg:41.76ms
step:820/1845 train_time:34266ms step_avg:41.79ms
step:821/1845 train_time:34329ms step_avg:41.81ms
step:822/1845 train_time:34390ms step_avg:41.84ms
step:823/1845 train_time:34452ms step_avg:41.86ms
step:824/1845 train_time:34513ms step_avg:41.89ms
step:825/1845 train_time:34576ms step_avg:41.91ms
step:826/1845 train_time:34637ms step_avg:41.93ms
step:827/1845 train_time:34699ms step_avg:41.96ms
step:828/1845 train_time:34761ms step_avg:41.98ms
step:829/1845 train_time:34824ms step_avg:42.01ms
step:830/1845 train_time:34884ms step_avg:42.03ms
step:831/1845 train_time:34947ms step_avg:42.05ms
step:832/1845 train_time:35007ms step_avg:42.08ms
step:833/1845 train_time:35070ms step_avg:42.10ms
step:834/1845 train_time:35132ms step_avg:42.12ms
step:835/1845 train_time:35194ms step_avg:42.15ms
step:836/1845 train_time:35255ms step_avg:42.17ms
step:837/1845 train_time:35317ms step_avg:42.19ms
step:838/1845 train_time:35377ms step_avg:42.22ms
step:839/1845 train_time:35440ms step_avg:42.24ms
step:840/1845 train_time:35501ms step_avg:42.26ms
step:841/1845 train_time:35563ms step_avg:42.29ms
step:842/1845 train_time:35624ms step_avg:42.31ms
step:843/1845 train_time:35687ms step_avg:42.33ms
step:844/1845 train_time:35747ms step_avg:42.35ms
step:845/1845 train_time:35810ms step_avg:42.38ms
step:846/1845 train_time:35870ms step_avg:42.40ms
step:847/1845 train_time:35933ms step_avg:42.42ms
step:848/1845 train_time:35993ms step_avg:42.44ms
step:849/1845 train_time:36056ms step_avg:42.47ms
step:850/1845 train_time:36116ms step_avg:42.49ms
step:851/1845 train_time:36179ms step_avg:42.51ms
step:852/1845 train_time:36240ms step_avg:42.54ms
step:853/1845 train_time:36303ms step_avg:42.56ms
step:854/1845 train_time:36364ms step_avg:42.58ms
step:855/1845 train_time:36426ms step_avg:42.60ms
step:856/1845 train_time:36487ms step_avg:42.63ms
step:857/1845 train_time:36549ms step_avg:42.65ms
step:858/1845 train_time:36610ms step_avg:42.67ms
step:859/1845 train_time:36672ms step_avg:42.69ms
step:860/1845 train_time:36733ms step_avg:42.71ms
step:861/1845 train_time:36796ms step_avg:42.74ms
step:862/1845 train_time:36857ms step_avg:42.76ms
step:863/1845 train_time:36920ms step_avg:42.78ms
step:864/1845 train_time:36981ms step_avg:42.80ms
step:865/1845 train_time:37044ms step_avg:42.83ms
step:866/1845 train_time:37105ms step_avg:42.85ms
step:867/1845 train_time:37167ms step_avg:42.87ms
step:868/1845 train_time:37229ms step_avg:42.89ms
step:869/1845 train_time:37292ms step_avg:42.91ms
step:870/1845 train_time:37352ms step_avg:42.93ms
step:871/1845 train_time:37415ms step_avg:42.96ms
step:872/1845 train_time:37475ms step_avg:42.98ms
step:873/1845 train_time:37538ms step_avg:43.00ms
step:874/1845 train_time:37600ms step_avg:43.02ms
step:875/1845 train_time:37663ms step_avg:43.04ms
step:876/1845 train_time:37724ms step_avg:43.06ms
step:877/1845 train_time:37786ms step_avg:43.09ms
step:878/1845 train_time:37846ms step_avg:43.11ms
step:879/1845 train_time:37909ms step_avg:43.13ms
step:880/1845 train_time:37970ms step_avg:43.15ms
step:881/1845 train_time:38033ms step_avg:43.17ms
step:882/1845 train_time:38093ms step_avg:43.19ms
step:883/1845 train_time:38156ms step_avg:43.21ms
step:884/1845 train_time:38217ms step_avg:43.23ms
step:885/1845 train_time:38280ms step_avg:43.25ms
step:886/1845 train_time:38341ms step_avg:43.27ms
step:887/1845 train_time:38404ms step_avg:43.30ms
step:888/1845 train_time:38464ms step_avg:43.32ms
step:889/1845 train_time:38527ms step_avg:43.34ms
step:890/1845 train_time:38588ms step_avg:43.36ms
step:891/1845 train_time:38650ms step_avg:43.38ms
step:892/1845 train_time:38711ms step_avg:43.40ms
step:893/1845 train_time:38773ms step_avg:43.42ms
step:894/1845 train_time:38833ms step_avg:43.44ms
step:895/1845 train_time:38895ms step_avg:43.46ms
step:896/1845 train_time:38956ms step_avg:43.48ms
step:897/1845 train_time:39018ms step_avg:43.50ms
step:898/1845 train_time:39080ms step_avg:43.52ms
step:899/1845 train_time:39142ms step_avg:43.54ms
step:900/1845 train_time:39203ms step_avg:43.56ms
step:901/1845 train_time:39266ms step_avg:43.58ms
step:902/1845 train_time:39326ms step_avg:43.60ms
step:903/1845 train_time:39389ms step_avg:43.62ms
step:904/1845 train_time:39450ms step_avg:43.64ms
step:905/1845 train_time:39512ms step_avg:43.66ms
step:906/1845 train_time:39573ms step_avg:43.68ms
step:907/1845 train_time:39636ms step_avg:43.70ms
step:908/1845 train_time:39696ms step_avg:43.72ms
step:909/1845 train_time:39759ms step_avg:43.74ms
step:910/1845 train_time:39820ms step_avg:43.76ms
step:911/1845 train_time:39883ms step_avg:43.78ms
step:912/1845 train_time:39944ms step_avg:43.80ms
step:913/1845 train_time:40006ms step_avg:43.82ms
step:914/1845 train_time:40067ms step_avg:43.84ms
step:915/1845 train_time:40129ms step_avg:43.86ms
step:916/1845 train_time:40190ms step_avg:43.88ms
step:917/1845 train_time:40252ms step_avg:43.90ms
step:918/1845 train_time:40313ms step_avg:43.91ms
step:919/1845 train_time:40375ms step_avg:43.93ms
step:920/1845 train_time:40436ms step_avg:43.95ms
step:921/1845 train_time:40499ms step_avg:43.97ms
step:922/1845 train_time:40560ms step_avg:43.99ms
step:923/1845 train_time:40622ms step_avg:44.01ms
step:924/1845 train_time:40684ms step_avg:44.03ms
step:925/1845 train_time:40746ms step_avg:44.05ms
step:926/1845 train_time:40807ms step_avg:44.07ms
step:927/1845 train_time:40869ms step_avg:44.09ms
step:928/1845 train_time:40930ms step_avg:44.11ms
step:929/1845 train_time:40992ms step_avg:44.13ms
step:930/1845 train_time:41053ms step_avg:44.14ms
step:931/1845 train_time:41116ms step_avg:44.16ms
step:932/1845 train_time:41176ms step_avg:44.18ms
step:933/1845 train_time:41239ms step_avg:44.20ms
step:934/1845 train_time:41300ms step_avg:44.22ms
step:935/1845 train_time:41363ms step_avg:44.24ms
step:936/1845 train_time:41423ms step_avg:44.26ms
step:937/1845 train_time:41486ms step_avg:44.28ms
step:938/1845 train_time:41547ms step_avg:44.29ms
step:939/1845 train_time:41609ms step_avg:44.31ms
step:940/1845 train_time:41670ms step_avg:44.33ms
step:941/1845 train_time:41732ms step_avg:44.35ms
step:942/1845 train_time:41793ms step_avg:44.37ms
step:943/1845 train_time:41855ms step_avg:44.39ms
step:944/1845 train_time:41916ms step_avg:44.40ms
step:945/1845 train_time:41979ms step_avg:44.42ms
step:946/1845 train_time:42040ms step_avg:44.44ms
step:947/1845 train_time:42103ms step_avg:44.46ms
step:948/1845 train_time:42164ms step_avg:44.48ms
step:949/1845 train_time:42227ms step_avg:44.50ms
step:950/1845 train_time:42287ms step_avg:44.51ms
step:951/1845 train_time:42351ms step_avg:44.53ms
step:952/1845 train_time:42411ms step_avg:44.55ms
step:953/1845 train_time:42474ms step_avg:44.57ms
step:954/1845 train_time:42534ms step_avg:44.58ms
step:955/1845 train_time:42596ms step_avg:44.60ms
step:956/1845 train_time:42657ms step_avg:44.62ms
step:957/1845 train_time:42719ms step_avg:44.64ms
step:958/1845 train_time:42781ms step_avg:44.66ms
step:959/1845 train_time:42843ms step_avg:44.67ms
step:960/1845 train_time:42904ms step_avg:44.69ms
step:961/1845 train_time:42966ms step_avg:44.71ms
step:962/1845 train_time:43027ms step_avg:44.73ms
step:963/1845 train_time:43090ms step_avg:44.75ms
step:964/1845 train_time:43150ms step_avg:44.76ms
step:965/1845 train_time:43212ms step_avg:44.78ms
step:966/1845 train_time:43273ms step_avg:44.80ms
step:967/1845 train_time:43335ms step_avg:44.81ms
step:968/1845 train_time:43396ms step_avg:44.83ms
step:969/1845 train_time:43458ms step_avg:44.85ms
step:970/1845 train_time:43519ms step_avg:44.87ms
step:971/1845 train_time:43582ms step_avg:44.88ms
step:972/1845 train_time:43643ms step_avg:44.90ms
step:973/1845 train_time:43705ms step_avg:44.92ms
step:974/1845 train_time:43766ms step_avg:44.93ms
step:975/1845 train_time:43828ms step_avg:44.95ms
step:976/1845 train_time:43889ms step_avg:44.97ms
step:977/1845 train_time:43951ms step_avg:44.99ms
step:978/1845 train_time:44012ms step_avg:45.00ms
step:979/1845 train_time:44075ms step_avg:45.02ms
step:980/1845 train_time:44136ms step_avg:45.04ms
step:981/1845 train_time:44198ms step_avg:45.05ms
step:982/1845 train_time:44259ms step_avg:45.07ms
step:983/1845 train_time:44321ms step_avg:45.09ms
step:984/1845 train_time:44382ms step_avg:45.10ms
step:985/1845 train_time:44445ms step_avg:45.12ms
step:986/1845 train_time:44506ms step_avg:45.14ms
step:987/1845 train_time:44568ms step_avg:45.15ms
step:988/1845 train_time:44628ms step_avg:45.17ms
step:989/1845 train_time:44691ms step_avg:45.19ms
step:990/1845 train_time:44752ms step_avg:45.20ms
step:991/1845 train_time:44814ms step_avg:45.22ms
step:992/1845 train_time:44875ms step_avg:45.24ms
step:993/1845 train_time:44938ms step_avg:45.25ms
step:994/1845 train_time:44999ms step_avg:45.27ms
step:995/1845 train_time:45062ms step_avg:45.29ms
step:996/1845 train_time:45123ms step_avg:45.30ms
step:997/1845 train_time:45186ms step_avg:45.32ms
step:998/1845 train_time:45246ms step_avg:45.34ms
step:999/1845 train_time:45309ms step_avg:45.35ms
step:1000/1845 train_time:45370ms step_avg:45.37ms
step:1000/1845 val_loss:3.7850 train_time:45434ms step_avg:45.43ms
step:1001/1845 train_time:45457ms step_avg:45.41ms
step:1002/1845 train_time:45496ms step_avg:45.40ms
step:1003/1845 train_time:45558ms step_avg:45.42ms
step:1004/1845 train_time:45621ms step_avg:45.44ms
step:1005/1845 train_time:45684ms step_avg:45.46ms
step:1006/1845 train_time:45744ms step_avg:45.47ms
step:1007/1845 train_time:45806ms step_avg:45.49ms
step:1008/1845 train_time:45866ms step_avg:45.50ms
step:1009/1845 train_time:45928ms step_avg:45.52ms
step:1010/1845 train_time:45988ms step_avg:45.53ms
step:1011/1845 train_time:46050ms step_avg:45.55ms
step:1012/1845 train_time:46111ms step_avg:45.56ms
step:1013/1845 train_time:46173ms step_avg:45.58ms
step:1014/1845 train_time:46234ms step_avg:45.60ms
step:1015/1845 train_time:46296ms step_avg:45.61ms
step:1016/1845 train_time:46357ms step_avg:45.63ms
step:1017/1845 train_time:46420ms step_avg:45.64ms
step:1018/1845 train_time:46482ms step_avg:45.66ms
step:1019/1845 train_time:46545ms step_avg:45.68ms
step:1020/1845 train_time:46607ms step_avg:45.69ms
step:1021/1845 train_time:46671ms step_avg:45.71ms
step:1022/1845 train_time:46732ms step_avg:45.73ms
step:1023/1845 train_time:46795ms step_avg:45.74ms
step:1024/1845 train_time:46855ms step_avg:45.76ms
step:1025/1845 train_time:46918ms step_avg:45.77ms
step:1026/1845 train_time:46978ms step_avg:45.79ms
step:1027/1845 train_time:47041ms step_avg:45.80ms
step:1028/1845 train_time:47101ms step_avg:45.82ms
step:1029/1845 train_time:47164ms step_avg:45.84ms
step:1030/1845 train_time:47224ms step_avg:45.85ms
step:1031/1845 train_time:47286ms step_avg:45.86ms
step:1032/1845 train_time:47346ms step_avg:45.88ms
step:1033/1845 train_time:47409ms step_avg:45.89ms
step:1034/1845 train_time:47471ms step_avg:45.91ms
step:1035/1845 train_time:47534ms step_avg:45.93ms
step:1036/1845 train_time:47596ms step_avg:45.94ms
step:1037/1845 train_time:47658ms step_avg:45.96ms
step:1038/1845 train_time:47719ms step_avg:45.97ms
step:1039/1845 train_time:47782ms step_avg:45.99ms
step:1040/1845 train_time:47842ms step_avg:46.00ms
step:1041/1845 train_time:47904ms step_avg:46.02ms
step:1042/1845 train_time:47965ms step_avg:46.03ms
step:1043/1845 train_time:48027ms step_avg:46.05ms
step:1044/1845 train_time:48088ms step_avg:46.06ms
step:1045/1845 train_time:48151ms step_avg:46.08ms
step:1046/1845 train_time:48212ms step_avg:46.09ms
step:1047/1845 train_time:48274ms step_avg:46.11ms
step:1048/1845 train_time:48335ms step_avg:46.12ms
step:1049/1845 train_time:48397ms step_avg:46.14ms
step:1050/1845 train_time:48458ms step_avg:46.15ms
step:1051/1845 train_time:48520ms step_avg:46.17ms
step:1052/1845 train_time:48582ms step_avg:46.18ms
step:1053/1845 train_time:48644ms step_avg:46.20ms
step:1054/1845 train_time:48705ms step_avg:46.21ms
step:1055/1845 train_time:48767ms step_avg:46.22ms
step:1056/1845 train_time:48829ms step_avg:46.24ms
step:1057/1845 train_time:48891ms step_avg:46.25ms
step:1058/1845 train_time:48952ms step_avg:46.27ms
step:1059/1845 train_time:49015ms step_avg:46.28ms
step:1060/1845 train_time:49075ms step_avg:46.30ms
step:1061/1845 train_time:49137ms step_avg:46.31ms
step:1062/1845 train_time:49198ms step_avg:46.33ms
step:1063/1845 train_time:49261ms step_avg:46.34ms
step:1064/1845 train_time:49322ms step_avg:46.35ms
step:1065/1845 train_time:49384ms step_avg:46.37ms
step:1066/1845 train_time:49444ms step_avg:46.38ms
step:1067/1845 train_time:49507ms step_avg:46.40ms
step:1068/1845 train_time:49567ms step_avg:46.41ms
step:1069/1845 train_time:49630ms step_avg:46.43ms
step:1070/1845 train_time:49692ms step_avg:46.44ms
step:1071/1845 train_time:49754ms step_avg:46.46ms
step:1072/1845 train_time:49815ms step_avg:46.47ms
step:1073/1845 train_time:49877ms step_avg:46.48ms
step:1074/1845 train_time:49938ms step_avg:46.50ms
step:1075/1845 train_time:50001ms step_avg:46.51ms
step:1076/1845 train_time:50061ms step_avg:46.53ms
step:1077/1845 train_time:50124ms step_avg:46.54ms
step:1078/1845 train_time:50185ms step_avg:46.55ms
step:1079/1845 train_time:50247ms step_avg:46.57ms
step:1080/1845 train_time:50308ms step_avg:46.58ms
step:1081/1845 train_time:50370ms step_avg:46.60ms
step:1082/1845 train_time:50431ms step_avg:46.61ms
step:1083/1845 train_time:50494ms step_avg:46.62ms
step:1084/1845 train_time:50555ms step_avg:46.64ms
step:1085/1845 train_time:50617ms step_avg:46.65ms
step:1086/1845 train_time:50678ms step_avg:46.66ms
step:1087/1845 train_time:50740ms step_avg:46.68ms
step:1088/1845 train_time:50801ms step_avg:46.69ms
step:1089/1845 train_time:50864ms step_avg:46.71ms
step:1090/1845 train_time:50924ms step_avg:46.72ms
step:1091/1845 train_time:50987ms step_avg:46.73ms
step:1092/1845 train_time:51048ms step_avg:46.75ms
step:1093/1845 train_time:51110ms step_avg:46.76ms
step:1094/1845 train_time:51171ms step_avg:46.77ms
step:1095/1845 train_time:51234ms step_avg:46.79ms
step:1096/1845 train_time:51295ms step_avg:46.80ms
step:1097/1845 train_time:51357ms step_avg:46.82ms
step:1098/1845 train_time:51418ms step_avg:46.83ms
step:1099/1845 train_time:51481ms step_avg:46.84ms
step:1100/1845 train_time:51542ms step_avg:46.86ms
step:1101/1845 train_time:51604ms step_avg:46.87ms
step:1102/1845 train_time:51665ms step_avg:46.88ms
step:1103/1845 train_time:51728ms step_avg:46.90ms
step:1104/1845 train_time:51789ms step_avg:46.91ms
step:1105/1845 train_time:51851ms step_avg:46.92ms
step:1106/1845 train_time:51913ms step_avg:46.94ms
step:1107/1845 train_time:51975ms step_avg:46.95ms
step:1108/1845 train_time:52036ms step_avg:46.96ms
step:1109/1845 train_time:52099ms step_avg:46.98ms
step:1110/1845 train_time:52160ms step_avg:46.99ms
step:1111/1845 train_time:52222ms step_avg:47.00ms
step:1112/1845 train_time:52283ms step_avg:47.02ms
step:1113/1845 train_time:52345ms step_avg:47.03ms
step:1114/1845 train_time:52406ms step_avg:47.04ms
step:1115/1845 train_time:52469ms step_avg:47.06ms
step:1116/1845 train_time:52530ms step_avg:47.07ms
step:1117/1845 train_time:52592ms step_avg:47.08ms
step:1118/1845 train_time:52653ms step_avg:47.10ms
step:1119/1845 train_time:52716ms step_avg:47.11ms
step:1120/1845 train_time:52777ms step_avg:47.12ms
step:1121/1845 train_time:52840ms step_avg:47.14ms
step:1122/1845 train_time:52901ms step_avg:47.15ms
step:1123/1845 train_time:52963ms step_avg:47.16ms
step:1124/1845 train_time:53024ms step_avg:47.17ms
step:1125/1845 train_time:53086ms step_avg:47.19ms
step:1126/1845 train_time:53147ms step_avg:47.20ms
step:1127/1845 train_time:53210ms step_avg:47.21ms
step:1128/1845 train_time:53272ms step_avg:47.23ms
step:1129/1845 train_time:53334ms step_avg:47.24ms
step:1130/1845 train_time:53394ms step_avg:47.25ms
step:1131/1845 train_time:53457ms step_avg:47.27ms
step:1132/1845 train_time:53517ms step_avg:47.28ms
step:1133/1845 train_time:53580ms step_avg:47.29ms
step:1134/1845 train_time:53641ms step_avg:47.30ms
step:1135/1845 train_time:53703ms step_avg:47.32ms
step:1136/1845 train_time:53764ms step_avg:47.33ms
step:1137/1845 train_time:53826ms step_avg:47.34ms
step:1138/1845 train_time:53887ms step_avg:47.35ms
step:1139/1845 train_time:53949ms step_avg:47.37ms
step:1140/1845 train_time:54010ms step_avg:47.38ms
step:1141/1845 train_time:54073ms step_avg:47.39ms
step:1142/1845 train_time:54134ms step_avg:47.40ms
step:1143/1845 train_time:54196ms step_avg:47.42ms
step:1144/1845 train_time:54258ms step_avg:47.43ms
step:1145/1845 train_time:54320ms step_avg:47.44ms
step:1146/1845 train_time:54380ms step_avg:47.45ms
step:1147/1845 train_time:54443ms step_avg:47.47ms
step:1148/1845 train_time:54504ms step_avg:47.48ms
step:1149/1845 train_time:54566ms step_avg:47.49ms
step:1150/1845 train_time:54627ms step_avg:47.50ms
step:1151/1845 train_time:54690ms step_avg:47.51ms
step:1152/1845 train_time:54751ms step_avg:47.53ms
step:1153/1845 train_time:54813ms step_avg:47.54ms
step:1154/1845 train_time:54874ms step_avg:47.55ms
step:1155/1845 train_time:54937ms step_avg:47.56ms
step:1156/1845 train_time:54998ms step_avg:47.58ms
step:1157/1845 train_time:55060ms step_avg:47.59ms
step:1158/1845 train_time:55120ms step_avg:47.60ms
step:1159/1845 train_time:55184ms step_avg:47.61ms
step:1160/1845 train_time:55244ms step_avg:47.62ms
step:1161/1845 train_time:55306ms step_avg:47.64ms
step:1162/1845 train_time:55367ms step_avg:47.65ms
step:1163/1845 train_time:55429ms step_avg:47.66ms
step:1164/1845 train_time:55490ms step_avg:47.67ms
step:1165/1845 train_time:55553ms step_avg:47.69ms
step:1166/1845 train_time:55614ms step_avg:47.70ms
step:1167/1845 train_time:55677ms step_avg:47.71ms
step:1168/1845 train_time:55738ms step_avg:47.72ms
step:1169/1845 train_time:55800ms step_avg:47.73ms
step:1170/1845 train_time:55861ms step_avg:47.74ms
step:1171/1845 train_time:55924ms step_avg:47.76ms
step:1172/1845 train_time:55984ms step_avg:47.77ms
step:1173/1845 train_time:56047ms step_avg:47.78ms
step:1174/1845 train_time:56108ms step_avg:47.79ms
step:1175/1845 train_time:56170ms step_avg:47.80ms
step:1176/1845 train_time:56232ms step_avg:47.82ms
step:1177/1845 train_time:56294ms step_avg:47.83ms
step:1178/1845 train_time:56355ms step_avg:47.84ms
step:1179/1845 train_time:56417ms step_avg:47.85ms
step:1180/1845 train_time:56478ms step_avg:47.86ms
step:1181/1845 train_time:56540ms step_avg:47.88ms
step:1182/1845 train_time:56601ms step_avg:47.89ms
step:1183/1845 train_time:56664ms step_avg:47.90ms
step:1184/1845 train_time:56725ms step_avg:47.91ms
step:1185/1845 train_time:56787ms step_avg:47.92ms
step:1186/1845 train_time:56848ms step_avg:47.93ms
step:1187/1845 train_time:56911ms step_avg:47.94ms
step:1188/1845 train_time:56972ms step_avg:47.96ms
step:1189/1845 train_time:57034ms step_avg:47.97ms
step:1190/1845 train_time:57095ms step_avg:47.98ms
step:1191/1845 train_time:57157ms step_avg:47.99ms
step:1192/1845 train_time:57218ms step_avg:48.00ms
step:1193/1845 train_time:57280ms step_avg:48.01ms
step:1194/1845 train_time:57341ms step_avg:48.02ms
step:1195/1845 train_time:57404ms step_avg:48.04ms
step:1196/1845 train_time:57464ms step_avg:48.05ms
step:1197/1845 train_time:57526ms step_avg:48.06ms
step:1198/1845 train_time:57587ms step_avg:48.07ms
step:1199/1845 train_time:57650ms step_avg:48.08ms
step:1200/1845 train_time:57711ms step_avg:48.09ms
step:1201/1845 train_time:57773ms step_avg:48.10ms
step:1202/1845 train_time:57834ms step_avg:48.11ms
step:1203/1845 train_time:57896ms step_avg:48.13ms
step:1204/1845 train_time:57956ms step_avg:48.14ms
step:1205/1845 train_time:58019ms step_avg:48.15ms
step:1206/1845 train_time:58107ms step_avg:48.18ms
step:1207/1845 train_time:58196ms step_avg:48.22ms
step:1208/1845 train_time:58283ms step_avg:48.25ms
step:1209/1845 train_time:58372ms step_avg:48.28ms
step:1210/1845 train_time:58458ms step_avg:48.31ms
step:1211/1845 train_time:58547ms step_avg:48.35ms
step:1212/1845 train_time:58634ms step_avg:48.38ms
step:1213/1845 train_time:58723ms step_avg:48.41ms
step:1214/1845 train_time:58810ms step_avg:48.44ms
step:1215/1845 train_time:58898ms step_avg:48.48ms
step:1216/1845 train_time:58985ms step_avg:48.51ms
step:1217/1845 train_time:59074ms step_avg:48.54ms
step:1218/1845 train_time:59161ms step_avg:48.57ms
step:1219/1845 train_time:59251ms step_avg:48.61ms
step:1220/1845 train_time:59339ms step_avg:48.64ms
step:1221/1845 train_time:59428ms step_avg:48.67ms
step:1222/1845 train_time:59515ms step_avg:48.70ms
step:1223/1845 train_time:59604ms step_avg:48.74ms
step:1224/1845 train_time:59690ms step_avg:48.77ms
step:1225/1845 train_time:59779ms step_avg:48.80ms
step:1226/1845 train_time:59866ms step_avg:48.83ms
step:1227/1845 train_time:59955ms step_avg:48.86ms
step:1228/1845 train_time:60042ms step_avg:48.89ms
step:1229/1845 train_time:60130ms step_avg:48.93ms
step:1230/1845 train_time:60217ms step_avg:48.96ms
step:1231/1845 train_time:60306ms step_avg:48.99ms
step:1232/1845 train_time:60393ms step_avg:49.02ms
step:1233/1845 train_time:60481ms step_avg:49.05ms
step:1234/1845 train_time:60569ms step_avg:49.08ms
step:1235/1845 train_time:60658ms step_avg:49.12ms
step:1236/1845 train_time:60746ms step_avg:49.15ms
step:1237/1845 train_time:60833ms step_avg:49.18ms
step:1238/1845 train_time:60920ms step_avg:49.21ms
step:1239/1845 train_time:61009ms step_avg:49.24ms
step:1240/1845 train_time:61098ms step_avg:49.27ms
step:1241/1845 train_time:61187ms step_avg:49.30ms
step:1242/1845 train_time:61274ms step_avg:49.34ms
step:1243/1845 train_time:61363ms step_avg:49.37ms
step:1244/1845 train_time:61451ms step_avg:49.40ms
step:1245/1845 train_time:61538ms step_avg:49.43ms
step:1246/1845 train_time:61626ms step_avg:49.46ms
step:1247/1845 train_time:61715ms step_avg:49.49ms
step:1248/1845 train_time:61803ms step_avg:49.52ms
step:1249/1845 train_time:61892ms step_avg:49.55ms
step:1250/1845 train_time:61979ms step_avg:49.58ms
step:1250/1845 val_loss:3.5358 train_time:62069ms step_avg:49.66ms
step:1251/1845 train_time:62093ms step_avg:49.63ms
step:1252/1845 train_time:62155ms step_avg:49.64ms
step:1253/1845 train_time:62242ms step_avg:49.67ms
step:1254/1845 train_time:62333ms step_avg:49.71ms
step:1255/1845 train_time:62424ms step_avg:49.74ms
step:1256/1845 train_time:62510ms step_avg:49.77ms
step:1257/1845 train_time:62598ms step_avg:49.80ms
step:1258/1845 train_time:62684ms step_avg:49.83ms
step:1259/1845 train_time:62772ms step_avg:49.86ms
step:1260/1845 train_time:62858ms step_avg:49.89ms
step:1261/1845 train_time:62947ms step_avg:49.92ms
step:1262/1845 train_time:63040ms step_avg:49.95ms
step:1263/1845 train_time:63131ms step_avg:49.98ms
step:1264/1845 train_time:63219ms step_avg:50.01ms
step:1265/1845 train_time:63308ms step_avg:50.05ms
step:1266/1845 train_time:63395ms step_avg:50.07ms
step:1267/1845 train_time:63483ms step_avg:50.11ms
step:1268/1845 train_time:63570ms step_avg:50.13ms
step:1269/1845 train_time:63657ms step_avg:50.16ms
step:1270/1845 train_time:63744ms step_avg:50.19ms
step:1271/1845 train_time:63831ms step_avg:50.22ms
step:1272/1845 train_time:63919ms step_avg:50.25ms
step:1273/1845 train_time:64010ms step_avg:50.28ms
step:1274/1845 train_time:64099ms step_avg:50.31ms
step:1275/1845 train_time:64187ms step_avg:50.34ms
step:1276/1845 train_time:64275ms step_avg:50.37ms
step:1277/1845 train_time:64364ms step_avg:50.40ms
step:1278/1845 train_time:64451ms step_avg:50.43ms
step:1279/1845 train_time:64538ms step_avg:50.46ms
step:1280/1845 train_time:64625ms step_avg:50.49ms
step:1281/1845 train_time:64713ms step_avg:50.52ms
step:1282/1845 train_time:64800ms step_avg:50.55ms
step:1283/1845 train_time:64889ms step_avg:50.58ms
step:1284/1845 train_time:64977ms step_avg:50.60ms
step:1285/1845 train_time:65067ms step_avg:50.64ms
step:1286/1845 train_time:65154ms step_avg:50.66ms
step:1287/1845 train_time:65244ms step_avg:50.69ms
step:1288/1845 train_time:65331ms step_avg:50.72ms
step:1289/1845 train_time:65420ms step_avg:50.75ms
step:1290/1845 train_time:65507ms step_avg:50.78ms
step:1291/1845 train_time:65595ms step_avg:50.81ms
step:1292/1845 train_time:65682ms step_avg:50.84ms
step:1293/1845 train_time:65770ms step_avg:50.87ms
step:1294/1845 train_time:65857ms step_avg:50.89ms
step:1295/1845 train_time:65947ms step_avg:50.92ms
step:1296/1845 train_time:66034ms step_avg:50.95ms
step:1297/1845 train_time:66123ms step_avg:50.98ms
step:1298/1845 train_time:66212ms step_avg:51.01ms
step:1299/1845 train_time:66302ms step_avg:51.04ms
step:1300/1845 train_time:66389ms step_avg:51.07ms
step:1301/1845 train_time:66476ms step_avg:51.10ms
step:1302/1845 train_time:66564ms step_avg:51.12ms
step:1303/1845 train_time:66652ms step_avg:51.15ms
step:1304/1845 train_time:66740ms step_avg:51.18ms
step:1305/1845 train_time:66828ms step_avg:51.21ms
step:1306/1845 train_time:66914ms step_avg:51.24ms
step:1307/1845 train_time:67003ms step_avg:51.27ms
step:1308/1845 train_time:67091ms step_avg:51.29ms
step:1309/1845 train_time:67180ms step_avg:51.32ms
step:1310/1845 train_time:67268ms step_avg:51.35ms
step:1311/1845 train_time:67357ms step_avg:51.38ms
step:1312/1845 train_time:67446ms step_avg:51.41ms
step:1313/1845 train_time:67533ms step_avg:51.43ms
step:1314/1845 train_time:67619ms step_avg:51.46ms
step:1315/1845 train_time:67708ms step_avg:51.49ms
step:1316/1845 train_time:67795ms step_avg:51.52ms
step:1317/1845 train_time:67883ms step_avg:51.54ms
step:1318/1845 train_time:67970ms step_avg:51.57ms
step:1319/1845 train_time:68059ms step_avg:51.60ms
step:1320/1845 train_time:68148ms step_avg:51.63ms
step:1321/1845 train_time:68237ms step_avg:51.66ms
step:1322/1845 train_time:68325ms step_avg:51.68ms
step:1323/1845 train_time:68414ms step_avg:51.71ms
step:1324/1845 train_time:68500ms step_avg:51.74ms
step:1325/1845 train_time:68588ms step_avg:51.76ms
step:1326/1845 train_time:68675ms step_avg:51.79ms
step:1327/1845 train_time:68764ms step_avg:51.82ms
step:1328/1845 train_time:68851ms step_avg:51.85ms
step:1329/1845 train_time:68939ms step_avg:51.87ms
step:1330/1845 train_time:69027ms step_avg:51.90ms
step:1331/1845 train_time:69116ms step_avg:51.93ms
step:1332/1845 train_time:69205ms step_avg:51.96ms
step:1333/1845 train_time:69293ms step_avg:51.98ms
step:1334/1845 train_time:69381ms step_avg:52.01ms
step:1335/1845 train_time:69470ms step_avg:52.04ms
step:1336/1845 train_time:69557ms step_avg:52.06ms
step:1337/1845 train_time:69645ms step_avg:52.09ms
step:1338/1845 train_time:69732ms step_avg:52.12ms
step:1339/1845 train_time:69822ms step_avg:52.14ms
step:1340/1845 train_time:69909ms step_avg:52.17ms
step:1341/1845 train_time:69997ms step_avg:52.20ms
step:1342/1845 train_time:70086ms step_avg:52.23ms
step:1343/1845 train_time:70176ms step_avg:52.25ms
step:1344/1845 train_time:70262ms step_avg:52.28ms
step:1345/1845 train_time:70352ms step_avg:52.31ms
step:1346/1845 train_time:70439ms step_avg:52.33ms
step:1347/1845 train_time:70529ms step_avg:52.36ms
step:1348/1845 train_time:70617ms step_avg:52.39ms
step:1349/1845 train_time:70704ms step_avg:52.41ms
step:1350/1845 train_time:70791ms step_avg:52.44ms
step:1351/1845 train_time:70880ms step_avg:52.46ms
step:1352/1845 train_time:70968ms step_avg:52.49ms
step:1353/1845 train_time:71055ms step_avg:52.52ms
step:1354/1845 train_time:71143ms step_avg:52.54ms
step:1355/1845 train_time:71231ms step_avg:52.57ms
step:1356/1845 train_time:71319ms step_avg:52.59ms
step:1357/1845 train_time:71408ms step_avg:52.62ms
step:1358/1845 train_time:71495ms step_avg:52.65ms
step:1359/1845 train_time:71584ms step_avg:52.67ms
step:1360/1845 train_time:71670ms step_avg:52.70ms
step:1361/1845 train_time:71759ms step_avg:52.73ms
step:1362/1845 train_time:71846ms step_avg:52.75ms
step:1363/1845 train_time:71934ms step_avg:52.78ms
step:1364/1845 train_time:72022ms step_avg:52.80ms
step:1365/1845 train_time:72111ms step_avg:52.83ms
step:1366/1845 train_time:72198ms step_avg:52.85ms
step:1367/1845 train_time:72287ms step_avg:52.88ms
step:1368/1845 train_time:72374ms step_avg:52.90ms
step:1369/1845 train_time:72462ms step_avg:52.93ms
step:1370/1845 train_time:72550ms step_avg:52.96ms
step:1371/1845 train_time:72639ms step_avg:52.98ms
step:1372/1845 train_time:72727ms step_avg:53.01ms
step:1373/1845 train_time:72815ms step_avg:53.03ms
step:1374/1845 train_time:72902ms step_avg:53.06ms
step:1375/1845 train_time:72990ms step_avg:53.08ms
step:1376/1845 train_time:73078ms step_avg:53.11ms
step:1377/1845 train_time:73166ms step_avg:53.13ms
step:1378/1845 train_time:73254ms step_avg:53.16ms
step:1379/1845 train_time:73342ms step_avg:53.19ms
step:1380/1845 train_time:73429ms step_avg:53.21ms
step:1381/1845 train_time:73518ms step_avg:53.24ms
step:1382/1845 train_time:73606ms step_avg:53.26ms
step:1383/1845 train_time:73695ms step_avg:53.29ms
step:1384/1845 train_time:73782ms step_avg:53.31ms
step:1385/1845 train_time:73870ms step_avg:53.34ms
step:1386/1845 train_time:73957ms step_avg:53.36ms
step:1387/1845 train_time:74046ms step_avg:53.39ms
step:1388/1845 train_time:74133ms step_avg:53.41ms
step:1389/1845 train_time:74222ms step_avg:53.44ms
step:1390/1845 train_time:74309ms step_avg:53.46ms
step:1391/1845 train_time:74398ms step_avg:53.49ms
step:1392/1845 train_time:74485ms step_avg:53.51ms
step:1393/1845 train_time:74575ms step_avg:53.54ms
step:1394/1845 train_time:74661ms step_avg:53.56ms
step:1395/1845 train_time:74750ms step_avg:53.58ms
step:1396/1845 train_time:74838ms step_avg:53.61ms
step:1397/1845 train_time:74926ms step_avg:53.63ms
step:1398/1845 train_time:75013ms step_avg:53.66ms
step:1399/1845 train_time:75102ms step_avg:53.68ms
step:1400/1845 train_time:75189ms step_avg:53.71ms
step:1401/1845 train_time:75278ms step_avg:53.73ms
step:1402/1845 train_time:75365ms step_avg:53.76ms
step:1403/1845 train_time:75453ms step_avg:53.78ms
step:1404/1845 train_time:75541ms step_avg:53.80ms
step:1405/1845 train_time:75629ms step_avg:53.83ms
step:1406/1845 train_time:75717ms step_avg:53.85ms
step:1407/1845 train_time:75806ms step_avg:53.88ms
step:1408/1845 train_time:75893ms step_avg:53.90ms
step:1409/1845 train_time:75982ms step_avg:53.93ms
step:1410/1845 train_time:76069ms step_avg:53.95ms
step:1411/1845 train_time:76157ms step_avg:53.97ms
step:1412/1845 train_time:76245ms step_avg:54.00ms
step:1413/1845 train_time:76334ms step_avg:54.02ms
step:1414/1845 train_time:76421ms step_avg:54.05ms
step:1415/1845 train_time:76511ms step_avg:54.07ms
step:1416/1845 train_time:76598ms step_avg:54.09ms
step:1417/1845 train_time:76685ms step_avg:54.12ms
step:1418/1845 train_time:76772ms step_avg:54.14ms
step:1419/1845 train_time:76860ms step_avg:54.17ms
step:1420/1845 train_time:76948ms step_avg:54.19ms
step:1421/1845 train_time:77036ms step_avg:54.21ms
step:1422/1845 train_time:77124ms step_avg:54.24ms
step:1423/1845 train_time:77211ms step_avg:54.26ms
step:1424/1845 train_time:77299ms step_avg:54.28ms
step:1425/1845 train_time:77387ms step_avg:54.31ms
step:1426/1845 train_time:77474ms step_avg:54.33ms
step:1427/1845 train_time:77562ms step_avg:54.35ms
step:1428/1845 train_time:77650ms step_avg:54.38ms
step:1429/1845 train_time:77738ms step_avg:54.40ms
step:1430/1845 train_time:77825ms step_avg:54.42ms
step:1431/1845 train_time:77914ms step_avg:54.45ms
step:1432/1845 train_time:78001ms step_avg:54.47ms
step:1433/1845 train_time:78089ms step_avg:54.49ms
step:1434/1845 train_time:78177ms step_avg:54.52ms
step:1435/1845 train_time:78266ms step_avg:54.54ms
step:1436/1845 train_time:78353ms step_avg:54.56ms
step:1437/1845 train_time:78441ms step_avg:54.59ms
step:1438/1845 train_time:78529ms step_avg:54.61ms
step:1439/1845 train_time:78617ms step_avg:54.63ms
step:1440/1845 train_time:78705ms step_avg:54.66ms
step:1441/1845 train_time:78793ms step_avg:54.68ms
step:1442/1845 train_time:78880ms step_avg:54.70ms
step:1443/1845 train_time:78969ms step_avg:54.73ms
step:1444/1845 train_time:79056ms step_avg:54.75ms
step:1445/1845 train_time:79144ms step_avg:54.77ms
step:1446/1845 train_time:79232ms step_avg:54.79ms
step:1447/1845 train_time:79321ms step_avg:54.82ms
step:1448/1845 train_time:79409ms step_avg:54.84ms
step:1449/1845 train_time:79496ms step_avg:54.86ms
step:1450/1845 train_time:79583ms step_avg:54.88ms
step:1451/1845 train_time:79672ms step_avg:54.91ms
step:1452/1845 train_time:79760ms step_avg:54.93ms
step:1453/1845 train_time:79848ms step_avg:54.95ms
step:1454/1845 train_time:79936ms step_avg:54.98ms
step:1455/1845 train_time:80024ms step_avg:55.00ms
step:1456/1845 train_time:80111ms step_avg:55.02ms
step:1457/1845 train_time:80199ms step_avg:55.04ms
step:1458/1845 train_time:80287ms step_avg:55.07ms
step:1459/1845 train_time:80375ms step_avg:55.09ms
step:1460/1845 train_time:80463ms step_avg:55.11ms
step:1461/1845 train_time:80552ms step_avg:55.13ms
step:1462/1845 train_time:80639ms step_avg:55.16ms
step:1463/1845 train_time:80728ms step_avg:55.18ms
step:1464/1845 train_time:80814ms step_avg:55.20ms
step:1465/1845 train_time:80902ms step_avg:55.22ms
step:1466/1845 train_time:80989ms step_avg:55.24ms
step:1467/1845 train_time:81078ms step_avg:55.27ms
step:1468/1845 train_time:81165ms step_avg:55.29ms
step:1469/1845 train_time:81253ms step_avg:55.31ms
step:1470/1845 train_time:81340ms step_avg:55.33ms
step:1471/1845 train_time:81429ms step_avg:55.36ms
step:1472/1845 train_time:81517ms step_avg:55.38ms
step:1473/1845 train_time:81605ms step_avg:55.40ms
step:1474/1845 train_time:81693ms step_avg:55.42ms
step:1475/1845 train_time:81782ms step_avg:55.45ms
step:1476/1845 train_time:81868ms step_avg:55.47ms
step:1477/1845 train_time:81957ms step_avg:55.49ms
step:1478/1845 train_time:82045ms step_avg:55.51ms
step:1479/1845 train_time:82132ms step_avg:55.53ms
step:1480/1845 train_time:82219ms step_avg:55.55ms
step:1481/1845 train_time:82308ms step_avg:55.58ms
step:1482/1845 train_time:82395ms step_avg:55.60ms
step:1483/1845 train_time:82484ms step_avg:55.62ms
step:1484/1845 train_time:82571ms step_avg:55.64ms
step:1485/1845 train_time:82660ms step_avg:55.66ms
step:1486/1845 train_time:82747ms step_avg:55.68ms
step:1487/1845 train_time:82836ms step_avg:55.71ms
step:1488/1845 train_time:82923ms step_avg:55.73ms
step:1489/1845 train_time:83012ms step_avg:55.75ms
step:1490/1845 train_time:83099ms step_avg:55.77ms
step:1491/1845 train_time:83188ms step_avg:55.79ms
step:1492/1845 train_time:83275ms step_avg:55.81ms
step:1493/1845 train_time:83363ms step_avg:55.84ms
step:1494/1845 train_time:83450ms step_avg:55.86ms
step:1495/1845 train_time:83538ms step_avg:55.88ms
step:1496/1845 train_time:83625ms step_avg:55.90ms
step:1497/1845 train_time:83714ms step_avg:55.92ms
step:1498/1845 train_time:83801ms step_avg:55.94ms
step:1499/1845 train_time:83889ms step_avg:55.96ms
step:1500/1845 train_time:83977ms step_avg:55.98ms
step:1500/1845 val_loss:3.4051 train_time:84066ms step_avg:56.04ms
step:1501/1845 train_time:84090ms step_avg:56.02ms
step:1502/1845 train_time:84155ms step_avg:56.03ms
step:1503/1845 train_time:84247ms step_avg:56.05ms
step:1504/1845 train_time:84335ms step_avg:56.07ms
step:1505/1845 train_time:84423ms step_avg:56.09ms
step:1506/1845 train_time:84510ms step_avg:56.12ms
step:1507/1845 train_time:84598ms step_avg:56.14ms
step:1508/1845 train_time:84684ms step_avg:56.16ms
step:1509/1845 train_time:84772ms step_avg:56.18ms
step:1510/1845 train_time:84859ms step_avg:56.20ms
step:1511/1845 train_time:84947ms step_avg:56.22ms
step:1512/1845 train_time:85035ms step_avg:56.24ms
step:1513/1845 train_time:85125ms step_avg:56.26ms
step:1514/1845 train_time:85214ms step_avg:56.28ms
step:1515/1845 train_time:85303ms step_avg:56.31ms
step:1516/1845 train_time:85392ms step_avg:56.33ms
step:1517/1845 train_time:85480ms step_avg:56.35ms
step:1518/1845 train_time:85568ms step_avg:56.37ms
step:1519/1845 train_time:85655ms step_avg:56.39ms
step:1520/1845 train_time:85742ms step_avg:56.41ms
step:1521/1845 train_time:85830ms step_avg:56.43ms
step:1522/1845 train_time:85916ms step_avg:56.45ms
step:1523/1845 train_time:86004ms step_avg:56.47ms
step:1524/1845 train_time:86092ms step_avg:56.49ms
step:1525/1845 train_time:86182ms step_avg:56.51ms
step:1526/1845 train_time:86269ms step_avg:56.53ms
step:1527/1845 train_time:86358ms step_avg:56.55ms
step:1528/1845 train_time:86446ms step_avg:56.57ms
step:1529/1845 train_time:86535ms step_avg:56.60ms
step:1530/1845 train_time:86621ms step_avg:56.62ms
step:1531/1845 train_time:86710ms step_avg:56.64ms
step:1532/1845 train_time:86796ms step_avg:56.66ms
step:1533/1845 train_time:86884ms step_avg:56.68ms
step:1534/1845 train_time:86972ms step_avg:56.70ms
step:1535/1845 train_time:87060ms step_avg:56.72ms
step:1536/1845 train_time:87150ms step_avg:56.74ms
step:1537/1845 train_time:87238ms step_avg:56.76ms
step:1538/1845 train_time:87326ms step_avg:56.78ms
step:1539/1845 train_time:87415ms step_avg:56.80ms
step:1540/1845 train_time:87502ms step_avg:56.82ms
step:1541/1845 train_time:87591ms step_avg:56.84ms
step:1542/1845 train_time:87679ms step_avg:56.86ms
step:1543/1845 train_time:87767ms step_avg:56.88ms
step:1544/1845 train_time:87853ms step_avg:56.90ms
step:1545/1845 train_time:87940ms step_avg:56.92ms
step:1546/1845 train_time:88029ms step_avg:56.94ms
step:1547/1845 train_time:88117ms step_avg:56.96ms
step:1548/1845 train_time:88205ms step_avg:56.98ms
step:1549/1845 train_time:88295ms step_avg:57.00ms
step:1550/1845 train_time:88383ms step_avg:57.02ms
step:1551/1845 train_time:88472ms step_avg:57.04ms
step:1552/1845 train_time:88559ms step_avg:57.06ms
step:1553/1845 train_time:88648ms step_avg:57.08ms
step:1554/1845 train_time:88735ms step_avg:57.10ms
step:1555/1845 train_time:88823ms step_avg:57.12ms
step:1556/1845 train_time:88910ms step_avg:57.14ms
step:1557/1845 train_time:89000ms step_avg:57.16ms
step:1558/1845 train_time:89088ms step_avg:57.18ms
step:1559/1845 train_time:89177ms step_avg:57.20ms
step:1560/1845 train_time:89265ms step_avg:57.22ms
step:1561/1845 train_time:89355ms step_avg:57.24ms
step:1562/1845 train_time:89443ms step_avg:57.26ms
step:1563/1845 train_time:89532ms step_avg:57.28ms
step:1564/1845 train_time:89620ms step_avg:57.30ms
step:1565/1845 train_time:89709ms step_avg:57.32ms
step:1566/1845 train_time:89796ms step_avg:57.34ms
step:1567/1845 train_time:89886ms step_avg:57.36ms
step:1568/1845 train_time:89973ms step_avg:57.38ms
step:1569/1845 train_time:90063ms step_avg:57.40ms
step:1570/1845 train_time:90150ms step_avg:57.42ms
step:1571/1845 train_time:90239ms step_avg:57.44ms
step:1572/1845 train_time:90327ms step_avg:57.46ms
step:1573/1845 train_time:90416ms step_avg:57.48ms
step:1574/1845 train_time:90504ms step_avg:57.50ms
step:1575/1845 train_time:90593ms step_avg:57.52ms
step:1576/1845 train_time:90680ms step_avg:57.54ms
step:1577/1845 train_time:90769ms step_avg:57.56ms
step:1578/1845 train_time:90856ms step_avg:57.58ms
step:1579/1845 train_time:90944ms step_avg:57.60ms
step:1580/1845 train_time:91031ms step_avg:57.61ms
step:1581/1845 train_time:91119ms step_avg:57.63ms
step:1582/1845 train_time:91207ms step_avg:57.65ms
step:1583/1845 train_time:91296ms step_avg:57.67ms
step:1584/1845 train_time:91383ms step_avg:57.69ms
step:1585/1845 train_time:91472ms step_avg:57.71ms
step:1586/1845 train_time:91559ms step_avg:57.73ms
step:1587/1845 train_time:91647ms step_avg:57.75ms
step:1588/1845 train_time:91734ms step_avg:57.77ms
step:1589/1845 train_time:91823ms step_avg:57.79ms
step:1590/1845 train_time:91911ms step_avg:57.81ms
step:1591/1845 train_time:91999ms step_avg:57.82ms
step:1592/1845 train_time:92087ms step_avg:57.84ms
step:1593/1845 train_time:92176ms step_avg:57.86ms
step:1594/1845 train_time:92263ms step_avg:57.88ms
step:1595/1845 train_time:92352ms step_avg:57.90ms
step:1596/1845 train_time:92440ms step_avg:57.92ms
step:1597/1845 train_time:92529ms step_avg:57.94ms
step:1598/1845 train_time:92616ms step_avg:57.96ms
step:1599/1845 train_time:92705ms step_avg:57.98ms
step:1600/1845 train_time:92792ms step_avg:57.99ms
step:1601/1845 train_time:92880ms step_avg:58.01ms
step:1602/1845 train_time:92967ms step_avg:58.03ms
step:1603/1845 train_time:93055ms step_avg:58.05ms
step:1604/1845 train_time:93143ms step_avg:58.07ms
step:1605/1845 train_time:93232ms step_avg:58.09ms
step:1606/1845 train_time:93320ms step_avg:58.11ms
step:1607/1845 train_time:93409ms step_avg:58.13ms
step:1608/1845 train_time:93496ms step_avg:58.14ms
step:1609/1845 train_time:93587ms step_avg:58.16ms
step:1610/1845 train_time:93674ms step_avg:58.18ms
step:1611/1845 train_time:93762ms step_avg:58.20ms
step:1612/1845 train_time:93850ms step_avg:58.22ms
step:1613/1845 train_time:93938ms step_avg:58.24ms
step:1614/1845 train_time:94025ms step_avg:58.26ms
step:1615/1845 train_time:94114ms step_avg:58.27ms
step:1616/1845 train_time:94201ms step_avg:58.29ms
step:1617/1845 train_time:94291ms step_avg:58.31ms
step:1618/1845 train_time:94378ms step_avg:58.33ms
step:1619/1845 train_time:94468ms step_avg:58.35ms
step:1620/1845 train_time:94554ms step_avg:58.37ms
step:1621/1845 train_time:94644ms step_avg:58.39ms
step:1622/1845 train_time:94731ms step_avg:58.40ms
step:1623/1845 train_time:94819ms step_avg:58.42ms
step:1624/1845 train_time:94906ms step_avg:58.44ms
step:1625/1845 train_time:94994ms step_avg:58.46ms
step:1626/1845 train_time:95081ms step_avg:58.48ms
step:1627/1845 train_time:95170ms step_avg:58.49ms
step:1628/1845 train_time:95257ms step_avg:58.51ms
step:1629/1845 train_time:95345ms step_avg:58.53ms
step:1630/1845 train_time:95432ms step_avg:58.55ms
step:1631/1845 train_time:95521ms step_avg:58.57ms
step:1632/1845 train_time:95609ms step_avg:58.58ms
step:1633/1845 train_time:95697ms step_avg:58.60ms
step:1634/1845 train_time:95785ms step_avg:58.62ms
step:1635/1845 train_time:95873ms step_avg:58.64ms
step:1636/1845 train_time:95961ms step_avg:58.66ms
step:1637/1845 train_time:96049ms step_avg:58.67ms
step:1638/1845 train_time:96135ms step_avg:58.69ms
step:1639/1845 train_time:96223ms step_avg:58.71ms
step:1640/1845 train_time:96311ms step_avg:58.73ms
step:1641/1845 train_time:96401ms step_avg:58.75ms
step:1642/1845 train_time:96489ms step_avg:58.76ms
step:1643/1845 train_time:96578ms step_avg:58.78ms
step:1644/1845 train_time:96665ms step_avg:58.80ms
step:1645/1845 train_time:96754ms step_avg:58.82ms
step:1646/1845 train_time:96842ms step_avg:58.83ms
step:1647/1845 train_time:96930ms step_avg:58.85ms
step:1648/1845 train_time:97017ms step_avg:58.87ms
step:1649/1845 train_time:97105ms step_avg:58.89ms
step:1650/1845 train_time:97193ms step_avg:58.90ms
step:1651/1845 train_time:97282ms step_avg:58.92ms
step:1652/1845 train_time:97369ms step_avg:58.94ms
step:1653/1845 train_time:97458ms step_avg:58.96ms
step:1654/1845 train_time:97547ms step_avg:58.98ms
step:1655/1845 train_time:97634ms step_avg:58.99ms
step:1656/1845 train_time:97722ms step_avg:59.01ms
step:1657/1845 train_time:97811ms step_avg:59.03ms
step:1658/1845 train_time:97898ms step_avg:59.05ms
step:1659/1845 train_time:97987ms step_avg:59.06ms
step:1660/1845 train_time:98075ms step_avg:59.08ms
step:1661/1845 train_time:98163ms step_avg:59.10ms
step:1662/1845 train_time:98252ms step_avg:59.12ms
step:1663/1845 train_time:98339ms step_avg:59.13ms
step:1664/1845 train_time:98427ms step_avg:59.15ms
step:1665/1845 train_time:98514ms step_avg:59.17ms
step:1666/1845 train_time:98601ms step_avg:59.18ms
step:1667/1845 train_time:98690ms step_avg:59.20ms
step:1668/1845 train_time:98778ms step_avg:59.22ms
step:1669/1845 train_time:98866ms step_avg:59.24ms
step:1670/1845 train_time:98953ms step_avg:59.25ms
step:1671/1845 train_time:99042ms step_avg:59.27ms
step:1672/1845 train_time:99129ms step_avg:59.29ms
step:1673/1845 train_time:99218ms step_avg:59.31ms
step:1674/1845 train_time:99305ms step_avg:59.32ms
step:1675/1845 train_time:99393ms step_avg:59.34ms
step:1676/1845 train_time:99480ms step_avg:59.36ms
step:1677/1845 train_time:99570ms step_avg:59.37ms
step:1678/1845 train_time:99656ms step_avg:59.39ms
step:1679/1845 train_time:99745ms step_avg:59.41ms
step:1680/1845 train_time:99833ms step_avg:59.42ms
step:1681/1845 train_time:99921ms step_avg:59.44ms
step:1682/1845 train_time:100008ms step_avg:59.46ms
step:1683/1845 train_time:100097ms step_avg:59.48ms
step:1684/1845 train_time:100184ms step_avg:59.49ms
step:1685/1845 train_time:100272ms step_avg:59.51ms
step:1686/1845 train_time:100360ms step_avg:59.53ms
step:1687/1845 train_time:100450ms step_avg:59.54ms
step:1688/1845 train_time:100536ms step_avg:59.56ms
step:1689/1845 train_time:100626ms step_avg:59.58ms
step:1690/1845 train_time:100713ms step_avg:59.59ms
step:1691/1845 train_time:100802ms step_avg:59.61ms
step:1692/1845 train_time:100889ms step_avg:59.63ms
step:1693/1845 train_time:100977ms step_avg:59.64ms
step:1694/1845 train_time:101064ms step_avg:59.66ms
step:1695/1845 train_time:101153ms step_avg:59.68ms
step:1696/1845 train_time:101240ms step_avg:59.69ms
step:1697/1845 train_time:101329ms step_avg:59.71ms
step:1698/1845 train_time:101417ms step_avg:59.73ms
step:1699/1845 train_time:101505ms step_avg:59.74ms
step:1700/1845 train_time:101593ms step_avg:59.76ms
step:1701/1845 train_time:101681ms step_avg:59.78ms
step:1702/1845 train_time:101768ms step_avg:59.79ms
step:1703/1845 train_time:101857ms step_avg:59.81ms
step:1704/1845 train_time:101944ms step_avg:59.83ms
step:1705/1845 train_time:102033ms step_avg:59.84ms
step:1706/1845 train_time:102120ms step_avg:59.86ms
step:1707/1845 train_time:102210ms step_avg:59.88ms
step:1708/1845 train_time:102298ms step_avg:59.89ms
step:1709/1845 train_time:102387ms step_avg:59.91ms
step:1710/1845 train_time:102475ms step_avg:59.93ms
step:1711/1845 train_time:102563ms step_avg:59.94ms
step:1712/1845 train_time:102650ms step_avg:59.96ms
step:1713/1845 train_time:102738ms step_avg:59.98ms
step:1714/1845 train_time:102825ms step_avg:59.99ms
step:1715/1845 train_time:102915ms step_avg:60.01ms
step:1716/1845 train_time:103002ms step_avg:60.02ms
step:1717/1845 train_time:103091ms step_avg:60.04ms
step:1718/1845 train_time:103178ms step_avg:60.06ms
step:1719/1845 train_time:103267ms step_avg:60.07ms
step:1720/1845 train_time:103354ms step_avg:60.09ms
step:1721/1845 train_time:103443ms step_avg:60.11ms
step:1722/1845 train_time:103530ms step_avg:60.12ms
step:1723/1845 train_time:103620ms step_avg:60.14ms
step:1724/1845 train_time:103708ms step_avg:60.16ms
step:1725/1845 train_time:103797ms step_avg:60.17ms
step:1726/1845 train_time:103885ms step_avg:60.19ms
step:1727/1845 train_time:103973ms step_avg:60.20ms
step:1728/1845 train_time:104060ms step_avg:60.22ms
step:1729/1845 train_time:104148ms step_avg:60.24ms
step:1730/1845 train_time:104235ms step_avg:60.25ms
step:1731/1845 train_time:104325ms step_avg:60.27ms
step:1732/1845 train_time:104412ms step_avg:60.28ms
step:1733/1845 train_time:104500ms step_avg:60.30ms
step:1734/1845 train_time:104588ms step_avg:60.32ms
step:1735/1845 train_time:104676ms step_avg:60.33ms
step:1736/1845 train_time:104764ms step_avg:60.35ms
step:1737/1845 train_time:104853ms step_avg:60.36ms
step:1738/1845 train_time:104940ms step_avg:60.38ms
step:1739/1845 train_time:105029ms step_avg:60.40ms
step:1740/1845 train_time:105115ms step_avg:60.41ms
step:1741/1845 train_time:105204ms step_avg:60.43ms
step:1742/1845 train_time:105291ms step_avg:60.44ms
step:1743/1845 train_time:105379ms step_avg:60.46ms
step:1744/1845 train_time:105468ms step_avg:60.47ms
step:1745/1845 train_time:105555ms step_avg:60.49ms
step:1746/1845 train_time:105643ms step_avg:60.51ms
step:1747/1845 train_time:105731ms step_avg:60.52ms
step:1748/1845 train_time:105818ms step_avg:60.54ms
step:1749/1845 train_time:105907ms step_avg:60.55ms
step:1750/1845 train_time:105994ms step_avg:60.57ms
step:1750/1845 val_loss:3.3063 train_time:106084ms step_avg:60.62ms
step:1751/1845 train_time:106104ms step_avg:60.60ms
step:1752/1845 train_time:106175ms step_avg:60.60ms
step:1753/1845 train_time:106265ms step_avg:60.62ms
step:1754/1845 train_time:106352ms step_avg:60.63ms
step:1755/1845 train_time:106441ms step_avg:60.65ms
step:1756/1845 train_time:106528ms step_avg:60.66ms
step:1757/1845 train_time:106615ms step_avg:60.68ms
step:1758/1845 train_time:106701ms step_avg:60.69ms
step:1759/1845 train_time:106789ms step_avg:60.71ms
step:1760/1845 train_time:106875ms step_avg:60.72ms
step:1761/1845 train_time:106963ms step_avg:60.74ms
step:1762/1845 train_time:107051ms step_avg:60.76ms
step:1763/1845 train_time:107143ms step_avg:60.77ms
step:1764/1845 train_time:107232ms step_avg:60.79ms
step:1765/1845 train_time:107321ms step_avg:60.81ms
step:1766/1845 train_time:107408ms step_avg:60.82ms
step:1767/1845 train_time:107497ms step_avg:60.84ms
step:1768/1845 train_time:107583ms step_avg:60.85ms
step:1769/1845 train_time:107671ms step_avg:60.87ms
step:1770/1845 train_time:107756ms step_avg:60.88ms
step:1771/1845 train_time:107845ms step_avg:60.89ms
step:1772/1845 train_time:107932ms step_avg:60.91ms
step:1773/1845 train_time:108020ms step_avg:60.93ms
step:1774/1845 train_time:108108ms step_avg:60.94ms
step:1775/1845 train_time:108199ms step_avg:60.96ms
step:1776/1845 train_time:108287ms step_avg:60.97ms
step:1777/1845 train_time:108376ms step_avg:60.99ms
step:1778/1845 train_time:108463ms step_avg:61.00ms
step:1779/1845 train_time:108551ms step_avg:61.02ms
step:1780/1845 train_time:108637ms step_avg:61.03ms
step:1781/1845 train_time:108726ms step_avg:61.05ms
step:1782/1845 train_time:108814ms step_avg:61.06ms
step:1783/1845 train_time:108902ms step_avg:61.08ms
step:1784/1845 train_time:108989ms step_avg:61.09ms
step:1785/1845 train_time:109078ms step_avg:61.11ms
step:1786/1845 train_time:109167ms step_avg:61.12ms
step:1787/1845 train_time:109257ms step_avg:61.14ms
step:1788/1845 train_time:109344ms step_avg:61.15ms
step:1789/1845 train_time:109433ms step_avg:61.17ms
step:1790/1845 train_time:109520ms step_avg:61.18ms
step:1791/1845 train_time:109608ms step_avg:61.20ms
step:1792/1845 train_time:109695ms step_avg:61.21ms
step:1793/1845 train_time:109783ms step_avg:61.23ms
step:1794/1845 train_time:109870ms step_avg:61.24ms
step:1795/1845 train_time:109959ms step_avg:61.26ms
step:1796/1845 train_time:110046ms step_avg:61.27ms
step:1797/1845 train_time:110136ms step_avg:61.29ms
step:1798/1845 train_time:110224ms step_avg:61.30ms
step:1799/1845 train_time:110314ms step_avg:61.32ms
step:1800/1845 train_time:110401ms step_avg:61.33ms
step:1801/1845 train_time:110491ms step_avg:61.35ms
step:1802/1845 train_time:110578ms step_avg:61.36ms
step:1803/1845 train_time:110666ms step_avg:61.38ms
step:1804/1845 train_time:110753ms step_avg:61.39ms
step:1805/1845 train_time:110841ms step_avg:61.41ms
step:1806/1845 train_time:110928ms step_avg:61.42ms
step:1807/1845 train_time:111017ms step_avg:61.44ms
step:1808/1845 train_time:111105ms step_avg:61.45ms
step:1809/1845 train_time:111196ms step_avg:61.47ms
step:1810/1845 train_time:111283ms step_avg:61.48ms
step:1811/1845 train_time:111372ms step_avg:61.50ms
step:1812/1845 train_time:111459ms step_avg:61.51ms
step:1813/1845 train_time:111548ms step_avg:61.53ms
step:1814/1845 train_time:111635ms step_avg:61.54ms
step:1815/1845 train_time:111724ms step_avg:61.56ms
step:1816/1845 train_time:111812ms step_avg:61.57ms
step:1817/1845 train_time:111900ms step_avg:61.59ms
step:1818/1845 train_time:111987ms step_avg:61.60ms
step:1819/1845 train_time:112076ms step_avg:61.61ms
step:1820/1845 train_time:112165ms step_avg:61.63ms
step:1821/1845 train_time:112255ms step_avg:61.64ms
step:1822/1845 train_time:112343ms step_avg:61.66ms
step:1823/1845 train_time:112432ms step_avg:61.67ms
step:1824/1845 train_time:112518ms step_avg:61.69ms
step:1825/1845 train_time:112608ms step_avg:61.70ms
step:1826/1845 train_time:112696ms step_avg:61.72ms
step:1827/1845 train_time:112784ms step_avg:61.73ms
step:1828/1845 train_time:112871ms step_avg:61.75ms
step:1829/1845 train_time:112959ms step_avg:61.76ms
step:1830/1845 train_time:113047ms step_avg:61.77ms
step:1831/1845 train_time:113136ms step_avg:61.79ms
step:1832/1845 train_time:113224ms step_avg:61.80ms
step:1833/1845 train_time:113314ms step_avg:61.82ms
step:1834/1845 train_time:113402ms step_avg:61.83ms
step:1835/1845 train_time:113492ms step_avg:61.85ms
step:1836/1845 train_time:113579ms step_avg:61.86ms
step:1837/1845 train_time:113668ms step_avg:61.88ms
step:1838/1845 train_time:113755ms step_avg:61.89ms
step:1839/1845 train_time:113845ms step_avg:61.91ms
step:1840/1845 train_time:113933ms step_avg:61.92ms
step:1841/1845 train_time:114022ms step_avg:61.93ms
step:1842/1845 train_time:114109ms step_avg:61.95ms
step:1843/1845 train_time:114198ms step_avg:61.96ms
step:1844/1845 train_time:114287ms step_avg:61.98ms
step:1845/1845 train_time:114376ms step_avg:61.99ms
step:1845/1845 val_loss:3.2795 train_time:114464ms step_avg:62.04ms
peak memory allocated: 29709 MiB reserved: 44758 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        group_sizes = [15, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

        # only include gates on layers with value embeds used on forward pass
        if layer_idx in [0,1,8,9,10]:
            self.value_embed_gate = CastedLinear(12, num_heads)
            self.value_embed_gate.weight.label = 'value_embed_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(self.value_embed_gate(x[..., :self.value_embed_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 29 06:07:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   30C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0            110W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            107W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8300 train_time:0ms step_avg:0.03ms
step:1/1845 train_time:89ms step_avg:88.70ms
step:2/1845 train_time:113ms step_avg:56.28ms
step:3/1845 train_time:136ms step_avg:45.26ms
step:4/1845 train_time:170ms step_avg:42.45ms
step:5/1845 train_time:204ms step_avg:40.87ms
step:6/1845 train_time:285ms step_avg:47.44ms
step:7/1845 train_time:309ms step_avg:44.08ms
step:8/1845 train_time:342ms step_avg:42.79ms
step:9/1845 train_time:377ms step_avg:41.85ms
step:10/1845 train_time:411ms step_avg:41.07ms
step:11/1845 train_time:446ms step_avg:40.51ms
step:12/1845 train_time:480ms step_avg:39.98ms
step:13/1845 train_time:514ms step_avg:39.57ms
step:14/1845 train_time:549ms step_avg:39.18ms
step:15/1845 train_time:583ms step_avg:38.88ms
step:16/1845 train_time:617ms step_avg:38.58ms
step:17/1845 train_time:652ms step_avg:38.35ms
step:18/1845 train_time:686ms step_avg:38.11ms
step:19/1845 train_time:721ms step_avg:37.93ms
step:20/1845 train_time:755ms step_avg:37.74ms
step:21/1845 train_time:789ms step_avg:37.59ms
step:22/1845 train_time:823ms step_avg:37.43ms
step:23/1845 train_time:858ms step_avg:37.30ms
step:24/1845 train_time:892ms step_avg:37.18ms
step:25/1845 train_time:927ms step_avg:37.09ms
step:26/1845 train_time:962ms step_avg:36.98ms
step:27/1845 train_time:996ms step_avg:36.90ms
step:28/1845 train_time:1031ms step_avg:36.81ms
step:29/1845 train_time:1065ms step_avg:36.73ms
step:30/1845 train_time:1099ms step_avg:36.64ms
step:31/1845 train_time:1134ms step_avg:36.57ms
step:32/1845 train_time:1168ms step_avg:36.50ms
step:33/1845 train_time:1203ms step_avg:36.45ms
step:34/1845 train_time:1237ms step_avg:36.39ms
step:35/1845 train_time:1273ms step_avg:36.37ms
step:36/1845 train_time:1307ms step_avg:36.31ms
step:37/1845 train_time:1342ms step_avg:36.27ms
step:38/1845 train_time:1376ms step_avg:36.21ms
step:39/1845 train_time:1411ms step_avg:36.18ms
step:40/1845 train_time:1445ms step_avg:36.12ms
step:41/1845 train_time:1480ms step_avg:36.09ms
step:42/1845 train_time:1514ms step_avg:36.05ms
step:43/1845 train_time:1549ms step_avg:36.02ms
step:44/1845 train_time:1583ms step_avg:35.98ms
step:45/1845 train_time:1618ms step_avg:35.95ms
step:46/1845 train_time:1652ms step_avg:35.92ms
step:47/1845 train_time:1687ms step_avg:35.90ms
step:48/1845 train_time:1721ms step_avg:35.86ms
step:49/1845 train_time:1756ms step_avg:35.84ms
step:50/1845 train_time:1790ms step_avg:35.80ms
step:51/1845 train_time:1825ms step_avg:35.78ms
step:52/1845 train_time:1859ms step_avg:35.75ms
step:53/1845 train_time:1894ms step_avg:35.73ms
step:54/1845 train_time:1928ms step_avg:35.71ms
step:55/1845 train_time:1963ms step_avg:35.70ms
step:56/1845 train_time:1998ms step_avg:35.67ms
step:57/1845 train_time:2032ms step_avg:35.66ms
step:58/1845 train_time:2067ms step_avg:35.63ms
step:59/1845 train_time:2101ms step_avg:35.61ms
step:60/1845 train_time:2135ms step_avg:35.59ms
step:61/1845 train_time:2170ms step_avg:35.58ms
step:62/1845 train_time:2204ms step_avg:35.55ms
step:63/1845 train_time:2239ms step_avg:35.54ms
step:64/1845 train_time:2273ms step_avg:35.52ms
step:65/1845 train_time:2308ms step_avg:35.51ms
step:66/1845 train_time:2342ms step_avg:35.49ms
step:67/1845 train_time:2377ms step_avg:35.48ms
step:68/1845 train_time:2411ms step_avg:35.46ms
step:69/1845 train_time:2446ms step_avg:35.45ms
step:70/1845 train_time:2480ms step_avg:35.44ms
step:71/1845 train_time:2515ms step_avg:35.43ms
step:72/1845 train_time:2550ms step_avg:35.41ms
step:73/1845 train_time:2584ms step_avg:35.40ms
step:74/1845 train_time:2619ms step_avg:35.39ms
step:75/1845 train_time:2653ms step_avg:35.38ms
step:76/1845 train_time:2687ms step_avg:35.36ms
step:77/1845 train_time:2722ms step_avg:35.35ms
step:78/1845 train_time:2756ms step_avg:35.33ms
step:79/1845 train_time:2791ms step_avg:35.33ms
step:80/1845 train_time:2825ms step_avg:35.31ms
step:81/1845 train_time:2860ms step_avg:35.30ms
step:82/1845 train_time:2894ms step_avg:35.29ms
step:83/1845 train_time:2929ms step_avg:35.29ms
step:84/1845 train_time:2963ms step_avg:35.27ms
step:85/1845 train_time:2998ms step_avg:35.27ms
step:86/1845 train_time:3032ms step_avg:35.25ms
step:87/1845 train_time:3067ms step_avg:35.25ms
step:88/1845 train_time:3101ms step_avg:35.24ms
step:89/1845 train_time:3136ms step_avg:35.23ms
step:90/1845 train_time:3170ms step_avg:35.22ms
step:91/1845 train_time:3205ms step_avg:35.22ms
step:92/1845 train_time:3239ms step_avg:35.20ms
step:93/1845 train_time:3274ms step_avg:35.20ms
step:94/1845 train_time:3308ms step_avg:35.19ms
step:95/1845 train_time:3343ms step_avg:35.19ms
step:96/1845 train_time:3377ms step_avg:35.17ms
step:97/1845 train_time:3412ms step_avg:35.17ms
step:98/1845 train_time:3446ms step_avg:35.16ms
step:99/1845 train_time:3481ms step_avg:35.16ms
step:100/1845 train_time:3515ms step_avg:35.15ms
step:101/1845 train_time:3549ms step_avg:35.14ms
step:102/1845 train_time:3584ms step_avg:35.13ms
step:103/1845 train_time:3618ms step_avg:35.13ms
step:104/1845 train_time:3652ms step_avg:35.12ms
step:105/1845 train_time:3688ms step_avg:35.12ms
step:106/1845 train_time:3722ms step_avg:35.11ms
step:107/1845 train_time:3757ms step_avg:35.11ms
step:108/1845 train_time:3791ms step_avg:35.10ms
step:109/1845 train_time:3826ms step_avg:35.10ms
step:110/1845 train_time:3860ms step_avg:35.09ms
step:111/1845 train_time:3894ms step_avg:35.09ms
step:112/1845 train_time:3929ms step_avg:35.08ms
step:113/1845 train_time:3963ms step_avg:35.07ms
step:114/1845 train_time:3997ms step_avg:35.07ms
step:115/1845 train_time:4032ms step_avg:35.06ms
step:116/1845 train_time:4067ms step_avg:35.06ms
step:117/1845 train_time:4102ms step_avg:35.06ms
step:118/1845 train_time:4136ms step_avg:35.05ms
step:119/1845 train_time:4170ms step_avg:35.05ms
step:120/1845 train_time:4204ms step_avg:35.04ms
step:121/1845 train_time:4239ms step_avg:35.03ms
step:122/1845 train_time:4273ms step_avg:35.03ms
step:123/1845 train_time:4308ms step_avg:35.02ms
step:124/1845 train_time:4342ms step_avg:35.02ms
step:125/1845 train_time:4377ms step_avg:35.01ms
step:126/1845 train_time:4411ms step_avg:35.01ms
step:127/1845 train_time:4445ms step_avg:35.00ms
step:128/1845 train_time:4479ms step_avg:35.00ms
step:129/1845 train_time:4514ms step_avg:34.99ms
step:130/1845 train_time:4548ms step_avg:34.99ms
step:131/1845 train_time:4583ms step_avg:34.98ms
step:132/1845 train_time:4617ms step_avg:34.98ms
step:133/1845 train_time:4652ms step_avg:34.98ms
step:134/1845 train_time:4686ms step_avg:34.97ms
step:135/1845 train_time:4720ms step_avg:34.97ms
step:136/1845 train_time:4754ms step_avg:34.96ms
step:137/1845 train_time:4789ms step_avg:34.96ms
step:138/1845 train_time:4823ms step_avg:34.95ms
step:139/1845 train_time:4858ms step_avg:34.95ms
step:140/1845 train_time:4892ms step_avg:34.94ms
step:141/1845 train_time:4927ms step_avg:34.94ms
step:142/1845 train_time:4961ms step_avg:34.93ms
step:143/1845 train_time:4995ms step_avg:34.93ms
step:144/1845 train_time:5029ms step_avg:34.93ms
step:145/1845 train_time:5064ms step_avg:34.92ms
step:146/1845 train_time:5098ms step_avg:34.92ms
step:147/1845 train_time:5133ms step_avg:34.92ms
step:148/1845 train_time:5167ms step_avg:34.91ms
step:149/1845 train_time:5201ms step_avg:34.91ms
step:150/1845 train_time:5236ms step_avg:34.90ms
step:151/1845 train_time:5270ms step_avg:34.90ms
step:152/1845 train_time:5304ms step_avg:34.90ms
step:153/1845 train_time:5339ms step_avg:34.89ms
step:154/1845 train_time:5373ms step_avg:34.89ms
step:155/1845 train_time:5408ms step_avg:34.89ms
step:156/1845 train_time:5442ms step_avg:34.88ms
step:157/1845 train_time:5477ms step_avg:34.88ms
step:158/1845 train_time:5511ms step_avg:34.88ms
step:159/1845 train_time:5546ms step_avg:34.88ms
step:160/1845 train_time:5580ms step_avg:34.87ms
step:161/1845 train_time:5615ms step_avg:34.87ms
step:162/1845 train_time:5649ms step_avg:34.87ms
step:163/1845 train_time:5683ms step_avg:34.87ms
step:164/1845 train_time:5717ms step_avg:34.86ms
step:165/1845 train_time:5752ms step_avg:34.86ms
step:166/1845 train_time:5786ms step_avg:34.86ms
step:167/1845 train_time:5820ms step_avg:34.85ms
step:168/1845 train_time:5855ms step_avg:34.85ms
step:169/1845 train_time:5890ms step_avg:34.85ms
step:170/1845 train_time:5924ms step_avg:34.85ms
step:171/1845 train_time:5959ms step_avg:34.85ms
step:172/1845 train_time:5993ms step_avg:34.84ms
step:173/1845 train_time:6028ms step_avg:34.84ms
step:174/1845 train_time:6062ms step_avg:34.84ms
step:175/1845 train_time:6096ms step_avg:34.84ms
step:176/1845 train_time:6130ms step_avg:34.83ms
step:177/1845 train_time:6165ms step_avg:34.83ms
step:178/1845 train_time:6199ms step_avg:34.83ms
step:179/1845 train_time:6234ms step_avg:34.83ms
step:180/1845 train_time:6268ms step_avg:34.82ms
step:181/1845 train_time:6303ms step_avg:34.82ms
step:182/1845 train_time:6337ms step_avg:34.82ms
step:183/1845 train_time:6371ms step_avg:34.82ms
step:184/1845 train_time:6405ms step_avg:34.81ms
step:185/1845 train_time:6440ms step_avg:34.81ms
step:186/1845 train_time:6474ms step_avg:34.81ms
step:187/1845 train_time:6509ms step_avg:34.81ms
step:188/1845 train_time:6543ms step_avg:34.80ms
step:189/1845 train_time:6577ms step_avg:34.80ms
step:190/1845 train_time:6611ms step_avg:34.80ms
step:191/1845 train_time:6646ms step_avg:34.80ms
step:192/1845 train_time:6680ms step_avg:34.79ms
step:193/1845 train_time:6715ms step_avg:34.79ms
step:194/1845 train_time:6749ms step_avg:34.79ms
step:195/1845 train_time:6783ms step_avg:34.79ms
step:196/1845 train_time:6817ms step_avg:34.78ms
step:197/1845 train_time:6852ms step_avg:34.78ms
step:198/1845 train_time:6886ms step_avg:34.78ms
step:199/1845 train_time:6921ms step_avg:34.78ms
step:200/1845 train_time:6955ms step_avg:34.77ms
step:201/1845 train_time:6989ms step_avg:34.77ms
step:202/1845 train_time:7023ms step_avg:34.77ms
step:203/1845 train_time:7058ms step_avg:34.77ms
step:204/1845 train_time:7092ms step_avg:34.77ms
step:205/1845 train_time:7127ms step_avg:34.77ms
step:206/1845 train_time:7161ms step_avg:34.76ms
step:207/1845 train_time:7196ms step_avg:34.76ms
step:208/1845 train_time:7230ms step_avg:34.76ms
step:209/1845 train_time:7264ms step_avg:34.76ms
step:210/1845 train_time:7298ms step_avg:34.75ms
step:211/1845 train_time:7333ms step_avg:34.75ms
step:212/1845 train_time:7367ms step_avg:34.75ms
step:213/1845 train_time:7401ms step_avg:34.75ms
step:214/1845 train_time:7435ms step_avg:34.74ms
step:215/1845 train_time:7470ms step_avg:34.74ms
step:216/1845 train_time:7504ms step_avg:34.74ms
step:217/1845 train_time:7538ms step_avg:34.74ms
step:218/1845 train_time:7573ms step_avg:34.74ms
step:219/1845 train_time:7607ms step_avg:34.74ms
step:220/1845 train_time:7642ms step_avg:34.73ms
step:221/1845 train_time:7676ms step_avg:34.73ms
step:222/1845 train_time:7710ms step_avg:34.73ms
step:223/1845 train_time:7745ms step_avg:34.73ms
step:224/1845 train_time:7779ms step_avg:34.73ms
step:225/1845 train_time:7813ms step_avg:34.73ms
step:226/1845 train_time:7847ms step_avg:34.72ms
step:227/1845 train_time:7882ms step_avg:34.72ms
step:228/1845 train_time:7916ms step_avg:34.72ms
step:229/1845 train_time:7951ms step_avg:34.72ms
step:230/1845 train_time:7985ms step_avg:34.72ms
step:231/1845 train_time:8019ms step_avg:34.72ms
step:232/1845 train_time:8053ms step_avg:34.71ms
step:233/1845 train_time:8088ms step_avg:34.71ms
step:234/1845 train_time:8122ms step_avg:34.71ms
step:235/1845 train_time:8156ms step_avg:34.71ms
step:236/1845 train_time:8191ms step_avg:34.71ms
step:237/1845 train_time:8225ms step_avg:34.71ms
step:238/1845 train_time:8259ms step_avg:34.70ms
step:239/1845 train_time:8293ms step_avg:34.70ms
step:240/1845 train_time:8328ms step_avg:34.70ms
step:241/1845 train_time:8362ms step_avg:34.70ms
step:242/1845 train_time:8396ms step_avg:34.70ms
step:243/1845 train_time:8431ms step_avg:34.69ms
step:244/1845 train_time:8465ms step_avg:34.69ms
step:245/1845 train_time:8499ms step_avg:34.69ms
step:246/1845 train_time:8533ms step_avg:34.69ms
step:247/1845 train_time:8568ms step_avg:34.69ms
step:248/1845 train_time:8602ms step_avg:34.69ms
step:249/1845 train_time:8637ms step_avg:34.69ms
step:250/1845 train_time:8671ms step_avg:34.68ms
step:250/1845 val_loss:4.6033 train_time:8707ms step_avg:34.83ms
step:251/1845 train_time:8728ms step_avg:34.77ms
step:252/1845 train_time:8747ms step_avg:34.71ms
step:253/1845 train_time:8776ms step_avg:34.69ms
step:254/1845 train_time:8811ms step_avg:34.69ms
step:255/1845 train_time:8847ms step_avg:34.69ms
step:256/1845 train_time:8882ms step_avg:34.69ms
step:257/1845 train_time:8917ms step_avg:34.70ms
step:258/1845 train_time:8951ms step_avg:34.69ms
step:259/1845 train_time:8985ms step_avg:34.69ms
step:260/1845 train_time:9020ms step_avg:34.69ms
step:261/1845 train_time:9054ms step_avg:34.69ms
step:262/1845 train_time:9088ms step_avg:34.69ms
step:263/1845 train_time:9122ms step_avg:34.68ms
step:264/1845 train_time:9156ms step_avg:34.68ms
step:265/1845 train_time:9190ms step_avg:34.68ms
step:266/1845 train_time:9225ms step_avg:34.68ms
step:267/1845 train_time:9259ms step_avg:34.68ms
step:268/1845 train_time:9293ms step_avg:34.68ms
step:269/1845 train_time:9327ms step_avg:34.67ms
step:270/1845 train_time:9361ms step_avg:34.67ms
step:271/1845 train_time:9395ms step_avg:34.67ms
step:272/1845 train_time:9429ms step_avg:34.67ms
step:273/1845 train_time:9464ms step_avg:34.67ms
step:274/1845 train_time:9498ms step_avg:34.66ms
step:275/1845 train_time:9532ms step_avg:34.66ms
step:276/1845 train_time:9566ms step_avg:34.66ms
step:277/1845 train_time:9600ms step_avg:34.66ms
step:278/1845 train_time:9634ms step_avg:34.66ms
step:279/1845 train_time:9669ms step_avg:34.66ms
step:280/1845 train_time:9703ms step_avg:34.65ms
step:281/1845 train_time:9737ms step_avg:34.65ms
step:282/1845 train_time:9771ms step_avg:34.65ms
step:283/1845 train_time:9806ms step_avg:34.65ms
step:284/1845 train_time:9840ms step_avg:34.65ms
step:285/1845 train_time:9875ms step_avg:34.65ms
step:286/1845 train_time:9908ms step_avg:34.65ms
step:287/1845 train_time:9943ms step_avg:34.65ms
step:288/1845 train_time:9977ms step_avg:34.64ms
step:289/1845 train_time:10012ms step_avg:34.64ms
step:290/1845 train_time:10046ms step_avg:34.64ms
step:291/1845 train_time:10081ms step_avg:34.64ms
step:292/1845 train_time:10115ms step_avg:34.64ms
step:293/1845 train_time:10149ms step_avg:34.64ms
step:294/1845 train_time:10183ms step_avg:34.64ms
step:295/1845 train_time:10218ms step_avg:34.64ms
step:296/1845 train_time:10251ms step_avg:34.63ms
step:297/1845 train_time:10286ms step_avg:34.63ms
step:298/1845 train_time:10320ms step_avg:34.63ms
step:299/1845 train_time:10354ms step_avg:34.63ms
step:300/1845 train_time:10388ms step_avg:34.63ms
step:301/1845 train_time:10423ms step_avg:34.63ms
step:302/1845 train_time:10457ms step_avg:34.63ms
step:303/1845 train_time:10491ms step_avg:34.62ms
step:304/1845 train_time:10525ms step_avg:34.62ms
step:305/1845 train_time:10560ms step_avg:34.62ms
step:306/1845 train_time:10594ms step_avg:34.62ms
step:307/1845 train_time:10628ms step_avg:34.62ms
step:308/1845 train_time:10662ms step_avg:34.62ms
step:309/1845 train_time:10697ms step_avg:34.62ms
step:310/1845 train_time:10731ms step_avg:34.61ms
step:311/1845 train_time:10765ms step_avg:34.61ms
step:312/1845 train_time:10799ms step_avg:34.61ms
step:313/1845 train_time:10833ms step_avg:34.61ms
step:314/1845 train_time:10867ms step_avg:34.61ms
step:315/1845 train_time:10901ms step_avg:34.61ms
step:316/1845 train_time:10936ms step_avg:34.61ms
step:317/1845 train_time:10970ms step_avg:34.61ms
step:318/1845 train_time:11004ms step_avg:34.60ms
step:319/1845 train_time:11039ms step_avg:34.60ms
step:320/1845 train_time:11073ms step_avg:34.60ms
step:321/1845 train_time:11107ms step_avg:34.60ms
step:322/1845 train_time:11141ms step_avg:34.60ms
step:323/1845 train_time:11175ms step_avg:34.60ms
step:324/1845 train_time:11209ms step_avg:34.60ms
step:325/1845 train_time:11244ms step_avg:34.60ms
step:326/1845 train_time:11278ms step_avg:34.59ms
step:327/1845 train_time:11312ms step_avg:34.59ms
step:328/1845 train_time:11346ms step_avg:34.59ms
step:329/1845 train_time:11380ms step_avg:34.59ms
step:330/1845 train_time:11414ms step_avg:34.59ms
step:331/1845 train_time:11449ms step_avg:34.59ms
step:332/1845 train_time:11483ms step_avg:34.59ms
step:333/1845 train_time:11517ms step_avg:34.59ms
step:334/1845 train_time:11551ms step_avg:34.58ms
step:335/1845 train_time:11585ms step_avg:34.58ms
step:336/1845 train_time:11620ms step_avg:34.58ms
step:337/1845 train_time:11654ms step_avg:34.58ms
step:338/1845 train_time:11688ms step_avg:34.58ms
step:339/1845 train_time:11723ms step_avg:34.58ms
step:340/1845 train_time:11757ms step_avg:34.58ms
step:341/1845 train_time:11791ms step_avg:34.58ms
step:342/1845 train_time:11825ms step_avg:34.58ms
step:343/1845 train_time:11860ms step_avg:34.58ms
step:344/1845 train_time:11894ms step_avg:34.57ms
step:345/1845 train_time:11928ms step_avg:34.57ms
step:346/1845 train_time:11962ms step_avg:34.57ms
step:347/1845 train_time:11997ms step_avg:34.57ms
step:348/1845 train_time:12031ms step_avg:34.57ms
step:349/1845 train_time:12065ms step_avg:34.57ms
step:350/1845 train_time:12099ms step_avg:34.57ms
step:351/1845 train_time:12134ms step_avg:34.57ms
step:352/1845 train_time:12168ms step_avg:34.57ms
step:353/1845 train_time:12202ms step_avg:34.57ms
step:354/1845 train_time:12236ms step_avg:34.57ms
step:355/1845 train_time:12271ms step_avg:34.57ms
step:356/1845 train_time:12305ms step_avg:34.56ms
step:357/1845 train_time:12339ms step_avg:34.56ms
step:358/1845 train_time:12373ms step_avg:34.56ms
step:359/1845 train_time:12408ms step_avg:34.56ms
step:360/1845 train_time:12442ms step_avg:34.56ms
step:361/1845 train_time:12477ms step_avg:34.56ms
step:362/1845 train_time:12511ms step_avg:34.56ms
step:363/1845 train_time:12545ms step_avg:34.56ms
step:364/1845 train_time:12579ms step_avg:34.56ms
step:365/1845 train_time:12614ms step_avg:34.56ms
step:366/1845 train_time:12648ms step_avg:34.56ms
step:367/1845 train_time:12683ms step_avg:34.56ms
step:368/1845 train_time:12716ms step_avg:34.56ms
step:369/1845 train_time:12751ms step_avg:34.56ms
step:370/1845 train_time:12785ms step_avg:34.55ms
step:371/1845 train_time:12819ms step_avg:34.55ms
step:372/1845 train_time:12853ms step_avg:34.55ms
step:373/1845 train_time:12888ms step_avg:34.55ms
step:374/1845 train_time:12922ms step_avg:34.55ms
step:375/1845 train_time:12956ms step_avg:34.55ms
step:376/1845 train_time:12991ms step_avg:34.55ms
step:377/1845 train_time:13025ms step_avg:34.55ms
step:378/1845 train_time:13059ms step_avg:34.55ms
step:379/1845 train_time:13093ms step_avg:34.55ms
step:380/1845 train_time:13127ms step_avg:34.54ms
step:381/1845 train_time:13161ms step_avg:34.54ms
step:382/1845 train_time:13195ms step_avg:34.54ms
step:383/1845 train_time:13230ms step_avg:34.54ms
step:384/1845 train_time:13264ms step_avg:34.54ms
step:385/1845 train_time:13298ms step_avg:34.54ms
step:386/1845 train_time:13332ms step_avg:34.54ms
step:387/1845 train_time:13366ms step_avg:34.54ms
step:388/1845 train_time:13400ms step_avg:34.54ms
step:389/1845 train_time:13435ms step_avg:34.54ms
step:390/1845 train_time:13469ms step_avg:34.54ms
step:391/1845 train_time:13503ms step_avg:34.54ms
step:392/1845 train_time:13537ms step_avg:34.53ms
step:393/1845 train_time:13572ms step_avg:34.53ms
step:394/1845 train_time:13606ms step_avg:34.53ms
step:395/1845 train_time:13640ms step_avg:34.53ms
step:396/1845 train_time:13674ms step_avg:34.53ms
step:397/1845 train_time:13709ms step_avg:34.53ms
step:398/1845 train_time:13743ms step_avg:34.53ms
step:399/1845 train_time:13777ms step_avg:34.53ms
step:400/1845 train_time:13811ms step_avg:34.53ms
step:401/1845 train_time:13846ms step_avg:34.53ms
step:402/1845 train_time:13880ms step_avg:34.53ms
step:403/1845 train_time:13915ms step_avg:34.53ms
step:404/1845 train_time:13949ms step_avg:34.53ms
step:405/1845 train_time:13983ms step_avg:34.53ms
step:406/1845 train_time:14017ms step_avg:34.53ms
step:407/1845 train_time:14052ms step_avg:34.53ms
step:408/1845 train_time:14086ms step_avg:34.52ms
step:409/1845 train_time:14120ms step_avg:34.52ms
step:410/1845 train_time:14154ms step_avg:34.52ms
step:411/1845 train_time:14189ms step_avg:34.52ms
step:412/1845 train_time:14223ms step_avg:34.52ms
step:413/1845 train_time:14257ms step_avg:34.52ms
step:414/1845 train_time:14291ms step_avg:34.52ms
step:415/1845 train_time:14325ms step_avg:34.52ms
step:416/1845 train_time:14359ms step_avg:34.52ms
step:417/1845 train_time:14394ms step_avg:34.52ms
step:418/1845 train_time:14428ms step_avg:34.52ms
step:419/1845 train_time:14462ms step_avg:34.52ms
step:420/1845 train_time:14496ms step_avg:34.52ms
step:421/1845 train_time:14531ms step_avg:34.51ms
step:422/1845 train_time:14565ms step_avg:34.51ms
step:423/1845 train_time:14599ms step_avg:34.51ms
step:424/1845 train_time:14633ms step_avg:34.51ms
step:425/1845 train_time:14667ms step_avg:34.51ms
step:426/1845 train_time:14701ms step_avg:34.51ms
step:427/1845 train_time:14736ms step_avg:34.51ms
step:428/1845 train_time:14770ms step_avg:34.51ms
step:429/1845 train_time:14804ms step_avg:34.51ms
step:430/1845 train_time:14838ms step_avg:34.51ms
step:431/1845 train_time:14872ms step_avg:34.51ms
step:432/1845 train_time:14906ms step_avg:34.51ms
step:433/1845 train_time:14941ms step_avg:34.50ms
step:434/1845 train_time:14975ms step_avg:34.50ms
step:435/1845 train_time:15009ms step_avg:34.50ms
step:436/1845 train_time:15043ms step_avg:34.50ms
step:437/1845 train_time:15078ms step_avg:34.50ms
step:438/1845 train_time:15112ms step_avg:34.50ms
step:439/1845 train_time:15146ms step_avg:34.50ms
step:440/1845 train_time:15180ms step_avg:34.50ms
step:441/1845 train_time:15214ms step_avg:34.50ms
step:442/1845 train_time:15248ms step_avg:34.50ms
step:443/1845 train_time:15283ms step_avg:34.50ms
step:444/1845 train_time:15317ms step_avg:34.50ms
step:445/1845 train_time:15351ms step_avg:34.50ms
step:446/1845 train_time:15385ms step_avg:34.50ms
step:447/1845 train_time:15419ms step_avg:34.49ms
step:448/1845 train_time:15453ms step_avg:34.49ms
step:449/1845 train_time:15488ms step_avg:34.49ms
step:450/1845 train_time:15521ms step_avg:34.49ms
step:451/1845 train_time:15556ms step_avg:34.49ms
step:452/1845 train_time:15590ms step_avg:34.49ms
step:453/1845 train_time:15624ms step_avg:34.49ms
step:454/1845 train_time:15658ms step_avg:34.49ms
step:455/1845 train_time:15693ms step_avg:34.49ms
step:456/1845 train_time:15727ms step_avg:34.49ms
step:457/1845 train_time:15761ms step_avg:34.49ms
step:458/1845 train_time:15795ms step_avg:34.49ms
step:459/1845 train_time:15829ms step_avg:34.49ms
step:460/1845 train_time:15863ms step_avg:34.49ms
step:461/1845 train_time:15897ms step_avg:34.48ms
step:462/1845 train_time:15931ms step_avg:34.48ms
step:463/1845 train_time:15966ms step_avg:34.48ms
step:464/1845 train_time:16000ms step_avg:34.48ms
step:465/1845 train_time:16034ms step_avg:34.48ms
step:466/1845 train_time:16069ms step_avg:34.48ms
step:467/1845 train_time:16103ms step_avg:34.48ms
step:468/1845 train_time:16137ms step_avg:34.48ms
step:469/1845 train_time:16171ms step_avg:34.48ms
step:470/1845 train_time:16205ms step_avg:34.48ms
step:471/1845 train_time:16240ms step_avg:34.48ms
step:472/1845 train_time:16274ms step_avg:34.48ms
step:473/1845 train_time:16308ms step_avg:34.48ms
step:474/1845 train_time:16342ms step_avg:34.48ms
step:475/1845 train_time:16377ms step_avg:34.48ms
step:476/1845 train_time:16411ms step_avg:34.48ms
step:477/1845 train_time:16445ms step_avg:34.48ms
step:478/1845 train_time:16479ms step_avg:34.47ms
step:479/1845 train_time:16513ms step_avg:34.47ms
step:480/1845 train_time:16547ms step_avg:34.47ms
step:481/1845 train_time:16582ms step_avg:34.47ms
step:482/1845 train_time:16616ms step_avg:34.47ms
step:483/1845 train_time:16650ms step_avg:34.47ms
step:484/1845 train_time:16684ms step_avg:34.47ms
step:485/1845 train_time:16718ms step_avg:34.47ms
step:486/1845 train_time:16753ms step_avg:34.47ms
step:487/1845 train_time:16787ms step_avg:34.47ms
step:488/1845 train_time:16821ms step_avg:34.47ms
step:489/1845 train_time:16856ms step_avg:34.47ms
step:490/1845 train_time:16890ms step_avg:34.47ms
step:491/1845 train_time:16924ms step_avg:34.47ms
step:492/1845 train_time:16958ms step_avg:34.47ms
step:493/1845 train_time:16992ms step_avg:34.47ms
step:494/1845 train_time:17026ms step_avg:34.47ms
step:495/1845 train_time:17061ms step_avg:34.47ms
step:496/1845 train_time:17095ms step_avg:34.47ms
step:497/1845 train_time:17129ms step_avg:34.47ms
step:498/1845 train_time:17163ms step_avg:34.46ms
step:499/1845 train_time:17198ms step_avg:34.46ms
step:500/1845 train_time:17232ms step_avg:34.46ms
step:500/1845 val_loss:4.2987 train_time:17268ms step_avg:34.54ms
step:501/1845 train_time:17287ms step_avg:34.50ms
step:502/1845 train_time:17306ms step_avg:34.47ms
step:503/1845 train_time:17338ms step_avg:34.47ms
step:504/1845 train_time:17373ms step_avg:34.47ms
step:505/1845 train_time:17409ms step_avg:34.47ms
step:506/1845 train_time:17444ms step_avg:34.47ms
step:507/1845 train_time:17480ms step_avg:34.48ms
step:508/1845 train_time:17514ms step_avg:34.48ms
step:509/1845 train_time:17549ms step_avg:34.48ms
step:510/1845 train_time:17583ms step_avg:34.48ms
step:511/1845 train_time:17617ms step_avg:34.48ms
step:512/1845 train_time:17651ms step_avg:34.48ms
step:513/1845 train_time:17686ms step_avg:34.48ms
step:514/1845 train_time:17720ms step_avg:34.47ms
step:515/1845 train_time:17754ms step_avg:34.47ms
step:516/1845 train_time:17788ms step_avg:34.47ms
step:517/1845 train_time:17823ms step_avg:34.47ms
step:518/1845 train_time:17857ms step_avg:34.47ms
step:519/1845 train_time:17891ms step_avg:34.47ms
step:520/1845 train_time:17925ms step_avg:34.47ms
step:521/1845 train_time:17959ms step_avg:34.47ms
step:522/1845 train_time:17993ms step_avg:34.47ms
step:523/1845 train_time:18028ms step_avg:34.47ms
step:524/1845 train_time:18062ms step_avg:34.47ms
step:525/1845 train_time:18096ms step_avg:34.47ms
step:526/1845 train_time:18130ms step_avg:34.47ms
step:527/1845 train_time:18164ms step_avg:34.47ms
step:528/1845 train_time:18198ms step_avg:34.47ms
step:529/1845 train_time:18232ms step_avg:34.47ms
step:530/1845 train_time:18266ms step_avg:34.47ms
step:531/1845 train_time:18301ms step_avg:34.46ms
step:532/1845 train_time:18335ms step_avg:34.46ms
step:533/1845 train_time:18369ms step_avg:34.46ms
step:534/1845 train_time:18403ms step_avg:34.46ms
step:535/1845 train_time:18437ms step_avg:34.46ms
step:536/1845 train_time:18471ms step_avg:34.46ms
step:537/1845 train_time:18506ms step_avg:34.46ms
step:538/1845 train_time:18540ms step_avg:34.46ms
step:539/1845 train_time:18575ms step_avg:34.46ms
step:540/1845 train_time:18609ms step_avg:34.46ms
step:541/1845 train_time:18643ms step_avg:34.46ms
step:542/1845 train_time:18677ms step_avg:34.46ms
step:543/1845 train_time:18712ms step_avg:34.46ms
step:544/1845 train_time:18746ms step_avg:34.46ms
step:545/1845 train_time:18780ms step_avg:34.46ms
step:546/1845 train_time:18814ms step_avg:34.46ms
step:547/1845 train_time:18848ms step_avg:34.46ms
step:548/1845 train_time:18882ms step_avg:34.46ms
step:549/1845 train_time:18917ms step_avg:34.46ms
step:550/1845 train_time:18951ms step_avg:34.46ms
step:551/1845 train_time:18985ms step_avg:34.46ms
step:552/1845 train_time:19019ms step_avg:34.46ms
step:553/1845 train_time:19054ms step_avg:34.46ms
step:554/1845 train_time:19088ms step_avg:34.45ms
step:555/1845 train_time:19122ms step_avg:34.45ms
step:556/1845 train_time:19157ms step_avg:34.45ms
step:557/1845 train_time:19191ms step_avg:34.45ms
step:558/1845 train_time:19225ms step_avg:34.45ms
step:559/1845 train_time:19259ms step_avg:34.45ms
step:560/1845 train_time:19293ms step_avg:34.45ms
step:561/1845 train_time:19328ms step_avg:34.45ms
step:562/1845 train_time:19362ms step_avg:34.45ms
step:563/1845 train_time:19396ms step_avg:34.45ms
step:564/1845 train_time:19430ms step_avg:34.45ms
step:565/1845 train_time:19464ms step_avg:34.45ms
step:566/1845 train_time:19499ms step_avg:34.45ms
step:567/1845 train_time:19533ms step_avg:34.45ms
step:568/1845 train_time:19567ms step_avg:34.45ms
step:569/1845 train_time:19602ms step_avg:34.45ms
step:570/1845 train_time:19636ms step_avg:34.45ms
step:571/1845 train_time:19670ms step_avg:34.45ms
step:572/1845 train_time:19704ms step_avg:34.45ms
step:573/1845 train_time:19738ms step_avg:34.45ms
step:574/1845 train_time:19772ms step_avg:34.45ms
step:575/1845 train_time:19807ms step_avg:34.45ms
step:576/1845 train_time:19841ms step_avg:34.45ms
step:577/1845 train_time:19875ms step_avg:34.45ms
step:578/1845 train_time:19909ms step_avg:34.44ms
step:579/1845 train_time:19943ms step_avg:34.44ms
step:580/1845 train_time:19977ms step_avg:34.44ms
step:581/1845 train_time:20012ms step_avg:34.44ms
step:582/1845 train_time:20045ms step_avg:34.44ms
step:583/1845 train_time:20080ms step_avg:34.44ms
step:584/1845 train_time:20114ms step_avg:34.44ms
step:585/1845 train_time:20149ms step_avg:34.44ms
step:586/1845 train_time:20182ms step_avg:34.44ms
step:587/1845 train_time:20217ms step_avg:34.44ms
step:588/1845 train_time:20251ms step_avg:34.44ms
step:589/1845 train_time:20286ms step_avg:34.44ms
step:590/1845 train_time:20320ms step_avg:34.44ms
step:591/1845 train_time:20354ms step_avg:34.44ms
step:592/1845 train_time:20388ms step_avg:34.44ms
step:593/1845 train_time:20423ms step_avg:34.44ms
step:594/1845 train_time:20457ms step_avg:34.44ms
step:595/1845 train_time:20491ms step_avg:34.44ms
step:596/1845 train_time:20525ms step_avg:34.44ms
step:597/1845 train_time:20559ms step_avg:34.44ms
step:598/1845 train_time:20593ms step_avg:34.44ms
step:599/1845 train_time:20628ms step_avg:34.44ms
step:600/1845 train_time:20662ms step_avg:34.44ms
step:601/1845 train_time:20696ms step_avg:34.44ms
step:602/1845 train_time:20730ms step_avg:34.44ms
step:603/1845 train_time:20765ms step_avg:34.44ms
step:604/1845 train_time:20826ms step_avg:34.48ms
step:605/1845 train_time:20888ms step_avg:34.52ms
step:606/1845 train_time:20948ms step_avg:34.57ms
step:607/1845 train_time:21010ms step_avg:34.61ms
step:608/1845 train_time:21070ms step_avg:34.66ms
step:609/1845 train_time:21132ms step_avg:34.70ms
step:610/1845 train_time:21193ms step_avg:34.74ms
step:611/1845 train_time:21256ms step_avg:34.79ms
step:612/1845 train_time:21318ms step_avg:34.83ms
step:613/1845 train_time:21382ms step_avg:34.88ms
step:614/1845 train_time:21442ms step_avg:34.92ms
step:615/1845 train_time:21505ms step_avg:34.97ms
step:616/1845 train_time:21565ms step_avg:35.01ms
step:617/1845 train_time:21629ms step_avg:35.05ms
step:618/1845 train_time:21689ms step_avg:35.10ms
step:619/1845 train_time:21751ms step_avg:35.14ms
step:620/1845 train_time:21812ms step_avg:35.18ms
step:621/1845 train_time:21875ms step_avg:35.22ms
step:622/1845 train_time:21936ms step_avg:35.27ms
step:623/1845 train_time:21999ms step_avg:35.31ms
step:624/1845 train_time:22060ms step_avg:35.35ms
step:625/1845 train_time:22123ms step_avg:35.40ms
step:626/1845 train_time:22184ms step_avg:35.44ms
step:627/1845 train_time:22247ms step_avg:35.48ms
step:628/1845 train_time:22308ms step_avg:35.52ms
step:629/1845 train_time:22370ms step_avg:35.56ms
step:630/1845 train_time:22431ms step_avg:35.61ms
step:631/1845 train_time:22494ms step_avg:35.65ms
step:632/1845 train_time:22556ms step_avg:35.69ms
step:633/1845 train_time:22619ms step_avg:35.73ms
step:634/1845 train_time:22679ms step_avg:35.77ms
step:635/1845 train_time:22742ms step_avg:35.81ms
step:636/1845 train_time:22803ms step_avg:35.85ms
step:637/1845 train_time:22866ms step_avg:35.90ms
step:638/1845 train_time:22928ms step_avg:35.94ms
step:639/1845 train_time:22990ms step_avg:35.98ms
step:640/1845 train_time:23051ms step_avg:36.02ms
step:641/1845 train_time:23113ms step_avg:36.06ms
step:642/1845 train_time:23174ms step_avg:36.10ms
step:643/1845 train_time:23236ms step_avg:36.14ms
step:644/1845 train_time:23297ms step_avg:36.18ms
step:645/1845 train_time:23360ms step_avg:36.22ms
step:646/1845 train_time:23421ms step_avg:36.25ms
step:647/1845 train_time:23483ms step_avg:36.30ms
step:648/1845 train_time:23544ms step_avg:36.33ms
step:649/1845 train_time:23606ms step_avg:36.37ms
step:650/1845 train_time:23667ms step_avg:36.41ms
step:651/1845 train_time:23729ms step_avg:36.45ms
step:652/1845 train_time:23790ms step_avg:36.49ms
step:653/1845 train_time:23852ms step_avg:36.53ms
step:654/1845 train_time:23913ms step_avg:36.56ms
step:655/1845 train_time:23975ms step_avg:36.60ms
step:656/1845 train_time:24037ms step_avg:36.64ms
step:657/1845 train_time:24099ms step_avg:36.68ms
step:658/1845 train_time:24160ms step_avg:36.72ms
step:659/1845 train_time:24223ms step_avg:36.76ms
step:660/1845 train_time:24283ms step_avg:36.79ms
step:661/1845 train_time:24346ms step_avg:36.83ms
step:662/1845 train_time:24406ms step_avg:36.87ms
step:663/1845 train_time:24469ms step_avg:36.91ms
step:664/1845 train_time:24530ms step_avg:36.94ms
step:665/1845 train_time:24593ms step_avg:36.98ms
step:666/1845 train_time:24655ms step_avg:37.02ms
step:667/1845 train_time:24718ms step_avg:37.06ms
step:668/1845 train_time:24779ms step_avg:37.09ms
step:669/1845 train_time:24842ms step_avg:37.13ms
step:670/1845 train_time:24903ms step_avg:37.17ms
step:671/1845 train_time:24965ms step_avg:37.21ms
step:672/1845 train_time:25026ms step_avg:37.24ms
step:673/1845 train_time:25088ms step_avg:37.28ms
step:674/1845 train_time:25149ms step_avg:37.31ms
step:675/1845 train_time:25211ms step_avg:37.35ms
step:676/1845 train_time:25272ms step_avg:37.38ms
step:677/1845 train_time:25335ms step_avg:37.42ms
step:678/1845 train_time:25396ms step_avg:37.46ms
step:679/1845 train_time:25459ms step_avg:37.50ms
step:680/1845 train_time:25521ms step_avg:37.53ms
step:681/1845 train_time:25583ms step_avg:37.57ms
step:682/1845 train_time:25643ms step_avg:37.60ms
step:683/1845 train_time:25706ms step_avg:37.64ms
step:684/1845 train_time:25767ms step_avg:37.67ms
step:685/1845 train_time:25830ms step_avg:37.71ms
step:686/1845 train_time:25890ms step_avg:37.74ms
step:687/1845 train_time:25953ms step_avg:37.78ms
step:688/1845 train_time:26013ms step_avg:37.81ms
step:689/1845 train_time:26075ms step_avg:37.84ms
step:690/1845 train_time:26137ms step_avg:37.88ms
step:691/1845 train_time:26199ms step_avg:37.91ms
step:692/1845 train_time:26259ms step_avg:37.95ms
step:693/1845 train_time:26322ms step_avg:37.98ms
step:694/1845 train_time:26383ms step_avg:38.02ms
step:695/1845 train_time:26445ms step_avg:38.05ms
step:696/1845 train_time:26506ms step_avg:38.08ms
step:697/1845 train_time:26568ms step_avg:38.12ms
step:698/1845 train_time:26629ms step_avg:38.15ms
step:699/1845 train_time:26692ms step_avg:38.19ms
step:700/1845 train_time:26753ms step_avg:38.22ms
step:701/1845 train_time:26816ms step_avg:38.25ms
step:702/1845 train_time:26877ms step_avg:38.29ms
step:703/1845 train_time:26940ms step_avg:38.32ms
step:704/1845 train_time:27001ms step_avg:38.35ms
step:705/1845 train_time:27063ms step_avg:38.39ms
step:706/1845 train_time:27124ms step_avg:38.42ms
step:707/1845 train_time:27187ms step_avg:38.45ms
step:708/1845 train_time:27247ms step_avg:38.49ms
step:709/1845 train_time:27310ms step_avg:38.52ms
step:710/1845 train_time:27371ms step_avg:38.55ms
step:711/1845 train_time:27433ms step_avg:38.58ms
step:712/1845 train_time:27493ms step_avg:38.61ms
step:713/1845 train_time:27555ms step_avg:38.65ms
step:714/1845 train_time:27617ms step_avg:38.68ms
step:715/1845 train_time:27680ms step_avg:38.71ms
step:716/1845 train_time:27740ms step_avg:38.74ms
step:717/1845 train_time:27803ms step_avg:38.78ms
step:718/1845 train_time:27864ms step_avg:38.81ms
step:719/1845 train_time:27927ms step_avg:38.84ms
step:720/1845 train_time:27988ms step_avg:38.87ms
step:721/1845 train_time:28050ms step_avg:38.90ms
step:722/1845 train_time:28111ms step_avg:38.93ms
step:723/1845 train_time:28173ms step_avg:38.97ms
step:724/1845 train_time:28235ms step_avg:39.00ms
step:725/1845 train_time:28297ms step_avg:39.03ms
step:726/1845 train_time:28358ms step_avg:39.06ms
step:727/1845 train_time:28420ms step_avg:39.09ms
step:728/1845 train_time:28481ms step_avg:39.12ms
step:729/1845 train_time:28543ms step_avg:39.15ms
step:730/1845 train_time:28604ms step_avg:39.18ms
step:731/1845 train_time:28667ms step_avg:39.22ms
step:732/1845 train_time:28728ms step_avg:39.25ms
step:733/1845 train_time:28791ms step_avg:39.28ms
step:734/1845 train_time:28852ms step_avg:39.31ms
step:735/1845 train_time:28915ms step_avg:39.34ms
step:736/1845 train_time:28976ms step_avg:39.37ms
step:737/1845 train_time:29038ms step_avg:39.40ms
step:738/1845 train_time:29099ms step_avg:39.43ms
step:739/1845 train_time:29162ms step_avg:39.46ms
step:740/1845 train_time:29223ms step_avg:39.49ms
step:741/1845 train_time:29286ms step_avg:39.52ms
step:742/1845 train_time:29347ms step_avg:39.55ms
step:743/1845 train_time:29410ms step_avg:39.58ms
step:744/1845 train_time:29470ms step_avg:39.61ms
step:745/1845 train_time:29532ms step_avg:39.64ms
step:746/1845 train_time:29593ms step_avg:39.67ms
step:747/1845 train_time:29656ms step_avg:39.70ms
step:748/1845 train_time:29717ms step_avg:39.73ms
step:749/1845 train_time:29780ms step_avg:39.76ms
step:750/1845 train_time:29841ms step_avg:39.79ms
step:750/1845 val_loss:4.0175 train_time:29905ms step_avg:39.87ms
step:751/1845 train_time:29924ms step_avg:39.85ms
step:752/1845 train_time:29967ms step_avg:39.85ms
step:753/1845 train_time:30033ms step_avg:39.88ms
step:754/1845 train_time:30096ms step_avg:39.92ms
step:755/1845 train_time:30160ms step_avg:39.95ms
step:756/1845 train_time:30222ms step_avg:39.98ms
step:757/1845 train_time:30284ms step_avg:40.01ms
step:758/1845 train_time:30345ms step_avg:40.03ms
step:759/1845 train_time:30407ms step_avg:40.06ms
step:760/1845 train_time:30467ms step_avg:40.09ms
step:761/1845 train_time:30529ms step_avg:40.12ms
step:762/1845 train_time:30589ms step_avg:40.14ms
step:763/1845 train_time:30651ms step_avg:40.17ms
step:764/1845 train_time:30711ms step_avg:40.20ms
step:765/1845 train_time:30773ms step_avg:40.23ms
step:766/1845 train_time:30834ms step_avg:40.25ms
step:767/1845 train_time:30897ms step_avg:40.28ms
step:768/1845 train_time:30959ms step_avg:40.31ms
step:769/1845 train_time:31022ms step_avg:40.34ms
step:770/1845 train_time:31083ms step_avg:40.37ms
step:771/1845 train_time:31146ms step_avg:40.40ms
step:772/1845 train_time:31207ms step_avg:40.42ms
step:773/1845 train_time:31270ms step_avg:40.45ms
step:774/1845 train_time:31331ms step_avg:40.48ms
step:775/1845 train_time:31394ms step_avg:40.51ms
step:776/1845 train_time:31455ms step_avg:40.53ms
step:777/1845 train_time:31517ms step_avg:40.56ms
step:778/1845 train_time:31577ms step_avg:40.59ms
step:779/1845 train_time:31640ms step_avg:40.62ms
step:780/1845 train_time:31700ms step_avg:40.64ms
step:781/1845 train_time:31763ms step_avg:40.67ms
step:782/1845 train_time:31824ms step_avg:40.70ms
step:783/1845 train_time:31887ms step_avg:40.72ms
step:784/1845 train_time:31948ms step_avg:40.75ms
step:785/1845 train_time:32011ms step_avg:40.78ms
step:786/1845 train_time:32073ms step_avg:40.80ms
step:787/1845 train_time:32136ms step_avg:40.83ms
step:788/1845 train_time:32197ms step_avg:40.86ms
step:789/1845 train_time:32260ms step_avg:40.89ms
step:790/1845 train_time:32322ms step_avg:40.91ms
step:791/1845 train_time:32384ms step_avg:40.94ms
step:792/1845 train_time:32445ms step_avg:40.97ms
step:793/1845 train_time:32507ms step_avg:40.99ms
step:794/1845 train_time:32568ms step_avg:41.02ms
step:795/1845 train_time:32630ms step_avg:41.04ms
step:796/1845 train_time:32690ms step_avg:41.07ms
step:797/1845 train_time:32752ms step_avg:41.09ms
step:798/1845 train_time:32814ms step_avg:41.12ms
step:799/1845 train_time:32876ms step_avg:41.15ms
step:800/1845 train_time:32938ms step_avg:41.17ms
step:801/1845 train_time:33000ms step_avg:41.20ms
step:802/1845 train_time:33061ms step_avg:41.22ms
step:803/1845 train_time:33123ms step_avg:41.25ms
step:804/1845 train_time:33184ms step_avg:41.27ms
step:805/1845 train_time:33247ms step_avg:41.30ms
step:806/1845 train_time:33307ms step_avg:41.32ms
step:807/1845 train_time:33370ms step_avg:41.35ms
step:808/1845 train_time:33432ms step_avg:41.38ms
step:809/1845 train_time:33494ms step_avg:41.40ms
step:810/1845 train_time:33555ms step_avg:41.43ms
step:811/1845 train_time:33617ms step_avg:41.45ms
step:812/1845 train_time:33678ms step_avg:41.48ms
step:813/1845 train_time:33740ms step_avg:41.50ms
step:814/1845 train_time:33801ms step_avg:41.52ms
step:815/1845 train_time:33864ms step_avg:41.55ms
step:816/1845 train_time:33924ms step_avg:41.57ms
step:817/1845 train_time:33987ms step_avg:41.60ms
step:818/1845 train_time:34047ms step_avg:41.62ms
step:819/1845 train_time:34110ms step_avg:41.65ms
step:820/1845 train_time:34171ms step_avg:41.67ms
step:821/1845 train_time:34234ms step_avg:41.70ms
step:822/1845 train_time:34296ms step_avg:41.72ms
step:823/1845 train_time:34359ms step_avg:41.75ms
step:824/1845 train_time:34419ms step_avg:41.77ms
step:825/1845 train_time:34482ms step_avg:41.80ms
step:826/1845 train_time:34543ms step_avg:41.82ms
step:827/1845 train_time:34606ms step_avg:41.84ms
step:828/1845 train_time:34666ms step_avg:41.87ms
step:829/1845 train_time:34728ms step_avg:41.89ms
step:830/1845 train_time:34789ms step_avg:41.91ms
step:831/1845 train_time:34852ms step_avg:41.94ms
step:832/1845 train_time:34913ms step_avg:41.96ms
step:833/1845 train_time:34976ms step_avg:41.99ms
step:834/1845 train_time:35037ms step_avg:42.01ms
step:835/1845 train_time:35099ms step_avg:42.03ms
step:836/1845 train_time:35160ms step_avg:42.06ms
step:837/1845 train_time:35222ms step_avg:42.08ms
step:838/1845 train_time:35283ms step_avg:42.10ms
step:839/1845 train_time:35345ms step_avg:42.13ms
step:840/1845 train_time:35406ms step_avg:42.15ms
step:841/1845 train_time:35468ms step_avg:42.17ms
step:842/1845 train_time:35528ms step_avg:42.20ms
step:843/1845 train_time:35592ms step_avg:42.22ms
step:844/1845 train_time:35653ms step_avg:42.24ms
step:845/1845 train_time:35715ms step_avg:42.27ms
step:846/1845 train_time:35776ms step_avg:42.29ms
step:847/1845 train_time:35839ms step_avg:42.31ms
step:848/1845 train_time:35899ms step_avg:42.33ms
step:849/1845 train_time:35962ms step_avg:42.36ms
step:850/1845 train_time:36023ms step_avg:42.38ms
step:851/1845 train_time:36085ms step_avg:42.40ms
step:852/1845 train_time:36146ms step_avg:42.43ms
step:853/1845 train_time:36209ms step_avg:42.45ms
step:854/1845 train_time:36271ms step_avg:42.47ms
step:855/1845 train_time:36334ms step_avg:42.50ms
step:856/1845 train_time:36396ms step_avg:42.52ms
step:857/1845 train_time:36458ms step_avg:42.54ms
step:858/1845 train_time:36519ms step_avg:42.56ms
step:859/1845 train_time:36582ms step_avg:42.59ms
step:860/1845 train_time:36643ms step_avg:42.61ms
step:861/1845 train_time:36706ms step_avg:42.63ms
step:862/1845 train_time:36767ms step_avg:42.65ms
step:863/1845 train_time:36829ms step_avg:42.68ms
step:864/1845 train_time:36890ms step_avg:42.70ms
step:865/1845 train_time:36952ms step_avg:42.72ms
step:866/1845 train_time:37013ms step_avg:42.74ms
step:867/1845 train_time:37076ms step_avg:42.76ms
step:868/1845 train_time:37138ms step_avg:42.79ms
step:869/1845 train_time:37200ms step_avg:42.81ms
step:870/1845 train_time:37261ms step_avg:42.83ms
step:871/1845 train_time:37323ms step_avg:42.85ms
step:872/1845 train_time:37384ms step_avg:42.87ms
step:873/1845 train_time:37447ms step_avg:42.89ms
step:874/1845 train_time:37508ms step_avg:42.91ms
step:875/1845 train_time:37570ms step_avg:42.94ms
step:876/1845 train_time:37631ms step_avg:42.96ms
step:877/1845 train_time:37693ms step_avg:42.98ms
step:878/1845 train_time:37754ms step_avg:43.00ms
step:879/1845 train_time:37817ms step_avg:43.02ms
step:880/1845 train_time:37878ms step_avg:43.04ms
step:881/1845 train_time:37941ms step_avg:43.07ms
step:882/1845 train_time:38001ms step_avg:43.09ms
step:883/1845 train_time:38063ms step_avg:43.11ms
step:884/1845 train_time:38124ms step_avg:43.13ms
step:885/1845 train_time:38186ms step_avg:43.15ms
step:886/1845 train_time:38246ms step_avg:43.17ms
step:887/1845 train_time:38309ms step_avg:43.19ms
step:888/1845 train_time:38370ms step_avg:43.21ms
step:889/1845 train_time:38432ms step_avg:43.23ms
step:890/1845 train_time:38494ms step_avg:43.25ms
step:891/1845 train_time:38557ms step_avg:43.27ms
step:892/1845 train_time:38618ms step_avg:43.29ms
step:893/1845 train_time:38680ms step_avg:43.31ms
step:894/1845 train_time:38740ms step_avg:43.33ms
step:895/1845 train_time:38803ms step_avg:43.35ms
step:896/1845 train_time:38864ms step_avg:43.38ms
step:897/1845 train_time:38926ms step_avg:43.40ms
step:898/1845 train_time:38986ms step_avg:43.41ms
step:899/1845 train_time:39049ms step_avg:43.44ms
step:900/1845 train_time:39109ms step_avg:43.45ms
step:901/1845 train_time:39172ms step_avg:43.48ms
step:902/1845 train_time:39232ms step_avg:43.49ms
step:903/1845 train_time:39295ms step_avg:43.52ms
step:904/1845 train_time:39358ms step_avg:43.54ms
step:905/1845 train_time:39419ms step_avg:43.56ms
step:906/1845 train_time:39480ms step_avg:43.58ms
step:907/1845 train_time:39542ms step_avg:43.60ms
step:908/1845 train_time:39603ms step_avg:43.62ms
step:909/1845 train_time:39666ms step_avg:43.64ms
step:910/1845 train_time:39726ms step_avg:43.66ms
step:911/1845 train_time:39788ms step_avg:43.68ms
step:912/1845 train_time:39849ms step_avg:43.69ms
step:913/1845 train_time:39911ms step_avg:43.71ms
step:914/1845 train_time:39972ms step_avg:43.73ms
step:915/1845 train_time:40035ms step_avg:43.75ms
step:916/1845 train_time:40096ms step_avg:43.77ms
step:917/1845 train_time:40159ms step_avg:43.79ms
step:918/1845 train_time:40219ms step_avg:43.81ms
step:919/1845 train_time:40282ms step_avg:43.83ms
step:920/1845 train_time:40343ms step_avg:43.85ms
step:921/1845 train_time:40406ms step_avg:43.87ms
step:922/1845 train_time:40467ms step_avg:43.89ms
step:923/1845 train_time:40529ms step_avg:43.91ms
step:924/1845 train_time:40590ms step_avg:43.93ms
step:925/1845 train_time:40654ms step_avg:43.95ms
step:926/1845 train_time:40715ms step_avg:43.97ms
step:927/1845 train_time:40778ms step_avg:43.99ms
step:928/1845 train_time:40839ms step_avg:44.01ms
step:929/1845 train_time:40901ms step_avg:44.03ms
step:930/1845 train_time:40963ms step_avg:44.05ms
step:931/1845 train_time:41026ms step_avg:44.07ms
step:932/1845 train_time:41086ms step_avg:44.08ms
step:933/1845 train_time:41148ms step_avg:44.10ms
step:934/1845 train_time:41209ms step_avg:44.12ms
step:935/1845 train_time:41271ms step_avg:44.14ms
step:936/1845 train_time:41333ms step_avg:44.16ms
step:937/1845 train_time:41396ms step_avg:44.18ms
step:938/1845 train_time:41458ms step_avg:44.20ms
step:939/1845 train_time:41520ms step_avg:44.22ms
step:940/1845 train_time:41581ms step_avg:44.24ms
step:941/1845 train_time:41645ms step_avg:44.26ms
step:942/1845 train_time:41705ms step_avg:44.27ms
step:943/1845 train_time:41768ms step_avg:44.29ms
step:944/1845 train_time:41828ms step_avg:44.31ms
step:945/1845 train_time:41890ms step_avg:44.33ms
step:946/1845 train_time:41951ms step_avg:44.35ms
step:947/1845 train_time:42014ms step_avg:44.37ms
step:948/1845 train_time:42076ms step_avg:44.38ms
step:949/1845 train_time:42138ms step_avg:44.40ms
step:950/1845 train_time:42198ms step_avg:44.42ms
step:951/1845 train_time:42261ms step_avg:44.44ms
step:952/1845 train_time:42321ms step_avg:44.46ms
step:953/1845 train_time:42384ms step_avg:44.47ms
step:954/1845 train_time:42444ms step_avg:44.49ms
step:955/1845 train_time:42506ms step_avg:44.51ms
step:956/1845 train_time:42568ms step_avg:44.53ms
step:957/1845 train_time:42629ms step_avg:44.54ms
step:958/1845 train_time:42690ms step_avg:44.56ms
step:959/1845 train_time:42753ms step_avg:44.58ms
step:960/1845 train_time:42814ms step_avg:44.60ms
step:961/1845 train_time:42875ms step_avg:44.62ms
step:962/1845 train_time:42937ms step_avg:44.63ms
step:963/1845 train_time:42999ms step_avg:44.65ms
step:964/1845 train_time:43060ms step_avg:44.67ms
step:965/1845 train_time:43122ms step_avg:44.69ms
step:966/1845 train_time:43183ms step_avg:44.70ms
step:967/1845 train_time:43245ms step_avg:44.72ms
step:968/1845 train_time:43306ms step_avg:44.74ms
step:969/1845 train_time:43368ms step_avg:44.76ms
step:970/1845 train_time:43428ms step_avg:44.77ms
step:971/1845 train_time:43491ms step_avg:44.79ms
step:972/1845 train_time:43552ms step_avg:44.81ms
step:973/1845 train_time:43615ms step_avg:44.82ms
step:974/1845 train_time:43676ms step_avg:44.84ms
step:975/1845 train_time:43738ms step_avg:44.86ms
step:976/1845 train_time:43799ms step_avg:44.88ms
step:977/1845 train_time:43861ms step_avg:44.89ms
step:978/1845 train_time:43922ms step_avg:44.91ms
step:979/1845 train_time:43985ms step_avg:44.93ms
step:980/1845 train_time:44046ms step_avg:44.94ms
step:981/1845 train_time:44108ms step_avg:44.96ms
step:982/1845 train_time:44169ms step_avg:44.98ms
step:983/1845 train_time:44232ms step_avg:45.00ms
step:984/1845 train_time:44294ms step_avg:45.01ms
step:985/1845 train_time:44356ms step_avg:45.03ms
step:986/1845 train_time:44417ms step_avg:45.05ms
step:987/1845 train_time:44479ms step_avg:45.06ms
step:988/1845 train_time:44540ms step_avg:45.08ms
step:989/1845 train_time:44602ms step_avg:45.10ms
step:990/1845 train_time:44662ms step_avg:45.11ms
step:991/1845 train_time:44725ms step_avg:45.13ms
step:992/1845 train_time:44785ms step_avg:45.15ms
step:993/1845 train_time:44847ms step_avg:45.16ms
step:994/1845 train_time:44908ms step_avg:45.18ms
step:995/1845 train_time:44970ms step_avg:45.20ms
step:996/1845 train_time:45031ms step_avg:45.21ms
step:997/1845 train_time:45094ms step_avg:45.23ms
step:998/1845 train_time:45155ms step_avg:45.25ms
step:999/1845 train_time:45218ms step_avg:45.26ms
step:1000/1845 train_time:45279ms step_avg:45.28ms
step:1000/1845 val_loss:3.7828 train_time:45342ms step_avg:45.34ms
step:1001/1845 train_time:45361ms step_avg:45.32ms
step:1002/1845 train_time:45404ms step_avg:45.31ms
step:1003/1845 train_time:45469ms step_avg:45.33ms
step:1004/1845 train_time:45532ms step_avg:45.35ms
step:1005/1845 train_time:45595ms step_avg:45.37ms
step:1006/1845 train_time:45657ms step_avg:45.38ms
step:1007/1845 train_time:45719ms step_avg:45.40ms
step:1008/1845 train_time:45780ms step_avg:45.42ms
step:1009/1845 train_time:45842ms step_avg:45.43ms
step:1010/1845 train_time:45902ms step_avg:45.45ms
step:1011/1845 train_time:45964ms step_avg:45.46ms
step:1012/1845 train_time:46024ms step_avg:45.48ms
step:1013/1845 train_time:46086ms step_avg:45.49ms
step:1014/1845 train_time:46146ms step_avg:45.51ms
step:1015/1845 train_time:46208ms step_avg:45.53ms
step:1016/1845 train_time:46269ms step_avg:45.54ms
step:1017/1845 train_time:46332ms step_avg:45.56ms
step:1018/1845 train_time:46395ms step_avg:45.57ms
step:1019/1845 train_time:46458ms step_avg:45.59ms
step:1020/1845 train_time:46520ms step_avg:45.61ms
step:1021/1845 train_time:46583ms step_avg:45.62ms
step:1022/1845 train_time:46644ms step_avg:45.64ms
step:1023/1845 train_time:46706ms step_avg:45.66ms
step:1024/1845 train_time:46767ms step_avg:45.67ms
step:1025/1845 train_time:46830ms step_avg:45.69ms
step:1026/1845 train_time:46891ms step_avg:45.70ms
step:1027/1845 train_time:46954ms step_avg:45.72ms
step:1028/1845 train_time:47014ms step_avg:45.73ms
step:1029/1845 train_time:47076ms step_avg:45.75ms
step:1030/1845 train_time:47137ms step_avg:45.76ms
step:1031/1845 train_time:47200ms step_avg:45.78ms
step:1032/1845 train_time:47261ms step_avg:45.80ms
step:1033/1845 train_time:47324ms step_avg:45.81ms
step:1034/1845 train_time:47385ms step_avg:45.83ms
step:1035/1845 train_time:47448ms step_avg:45.84ms
step:1036/1845 train_time:47510ms step_avg:45.86ms
step:1037/1845 train_time:47573ms step_avg:45.88ms
step:1038/1845 train_time:47635ms step_avg:45.89ms
step:1039/1845 train_time:47697ms step_avg:45.91ms
step:1040/1845 train_time:47758ms step_avg:45.92ms
step:1041/1845 train_time:47820ms step_avg:45.94ms
step:1042/1845 train_time:47881ms step_avg:45.95ms
step:1043/1845 train_time:47943ms step_avg:45.97ms
step:1044/1845 train_time:48004ms step_avg:45.98ms
step:1045/1845 train_time:48066ms step_avg:46.00ms
step:1046/1845 train_time:48126ms step_avg:46.01ms
step:1047/1845 train_time:48188ms step_avg:46.02ms
step:1048/1845 train_time:48249ms step_avg:46.04ms
step:1049/1845 train_time:48311ms step_avg:46.05ms
step:1050/1845 train_time:48372ms step_avg:46.07ms
step:1051/1845 train_time:48435ms step_avg:46.08ms
step:1052/1845 train_time:48496ms step_avg:46.10ms
step:1053/1845 train_time:48559ms step_avg:46.12ms
step:1054/1845 train_time:48621ms step_avg:46.13ms
step:1055/1845 train_time:48683ms step_avg:46.15ms
step:1056/1845 train_time:48745ms step_avg:46.16ms
step:1057/1845 train_time:48807ms step_avg:46.17ms
step:1058/1845 train_time:48868ms step_avg:46.19ms
step:1059/1845 train_time:48930ms step_avg:46.20ms
step:1060/1845 train_time:48992ms step_avg:46.22ms
step:1061/1845 train_time:49054ms step_avg:46.23ms
step:1062/1845 train_time:49115ms step_avg:46.25ms
step:1063/1845 train_time:49177ms step_avg:46.26ms
step:1064/1845 train_time:49238ms step_avg:46.28ms
step:1065/1845 train_time:49301ms step_avg:46.29ms
step:1066/1845 train_time:49362ms step_avg:46.31ms
step:1067/1845 train_time:49424ms step_avg:46.32ms
step:1068/1845 train_time:49485ms step_avg:46.33ms
step:1069/1845 train_time:49547ms step_avg:46.35ms
step:1070/1845 train_time:49608ms step_avg:46.36ms
step:1071/1845 train_time:49671ms step_avg:46.38ms
step:1072/1845 train_time:49732ms step_avg:46.39ms
step:1073/1845 train_time:49795ms step_avg:46.41ms
step:1074/1845 train_time:49856ms step_avg:46.42ms
step:1075/1845 train_time:49918ms step_avg:46.44ms
step:1076/1845 train_time:49980ms step_avg:46.45ms
step:1077/1845 train_time:50041ms step_avg:46.46ms
step:1078/1845 train_time:50102ms step_avg:46.48ms
step:1079/1845 train_time:50164ms step_avg:46.49ms
step:1080/1845 train_time:50224ms step_avg:46.50ms
step:1081/1845 train_time:50286ms step_avg:46.52ms
step:1082/1845 train_time:50348ms step_avg:46.53ms
step:1083/1845 train_time:50411ms step_avg:46.55ms
step:1084/1845 train_time:50472ms step_avg:46.56ms
step:1085/1845 train_time:50534ms step_avg:46.58ms
step:1086/1845 train_time:50595ms step_avg:46.59ms
step:1087/1845 train_time:50658ms step_avg:46.60ms
step:1088/1845 train_time:50719ms step_avg:46.62ms
step:1089/1845 train_time:50781ms step_avg:46.63ms
step:1090/1845 train_time:50842ms step_avg:46.64ms
step:1091/1845 train_time:50904ms step_avg:46.66ms
step:1092/1845 train_time:50964ms step_avg:46.67ms
step:1093/1845 train_time:51026ms step_avg:46.68ms
step:1094/1845 train_time:51087ms step_avg:46.70ms
step:1095/1845 train_time:51150ms step_avg:46.71ms
step:1096/1845 train_time:51212ms step_avg:46.73ms
step:1097/1845 train_time:51274ms step_avg:46.74ms
step:1098/1845 train_time:51335ms step_avg:46.75ms
step:1099/1845 train_time:51398ms step_avg:46.77ms
step:1100/1845 train_time:51459ms step_avg:46.78ms
step:1101/1845 train_time:51521ms step_avg:46.79ms
step:1102/1845 train_time:51582ms step_avg:46.81ms
step:1103/1845 train_time:51644ms step_avg:46.82ms
step:1104/1845 train_time:51705ms step_avg:46.83ms
step:1105/1845 train_time:51767ms step_avg:46.85ms
step:1106/1845 train_time:51828ms step_avg:46.86ms
step:1107/1845 train_time:51890ms step_avg:46.87ms
step:1108/1845 train_time:51951ms step_avg:46.89ms
step:1109/1845 train_time:52014ms step_avg:46.90ms
step:1110/1845 train_time:52075ms step_avg:46.91ms
step:1111/1845 train_time:52137ms step_avg:46.93ms
step:1112/1845 train_time:52198ms step_avg:46.94ms
step:1113/1845 train_time:52260ms step_avg:46.95ms
step:1114/1845 train_time:52321ms step_avg:46.97ms
step:1115/1845 train_time:52383ms step_avg:46.98ms
step:1116/1845 train_time:52444ms step_avg:46.99ms
step:1117/1845 train_time:52506ms step_avg:47.01ms
step:1118/1845 train_time:52567ms step_avg:47.02ms
step:1119/1845 train_time:52630ms step_avg:47.03ms
step:1120/1845 train_time:52691ms step_avg:47.05ms
step:1121/1845 train_time:52753ms step_avg:47.06ms
step:1122/1845 train_time:52814ms step_avg:47.07ms
step:1123/1845 train_time:52877ms step_avg:47.09ms
step:1124/1845 train_time:52939ms step_avg:47.10ms
step:1125/1845 train_time:53001ms step_avg:47.11ms
step:1126/1845 train_time:53061ms step_avg:47.12ms
step:1127/1845 train_time:53124ms step_avg:47.14ms
step:1128/1845 train_time:53184ms step_avg:47.15ms
step:1129/1845 train_time:53246ms step_avg:47.16ms
step:1130/1845 train_time:53307ms step_avg:47.17ms
step:1131/1845 train_time:53370ms step_avg:47.19ms
step:1132/1845 train_time:53431ms step_avg:47.20ms
step:1133/1845 train_time:53493ms step_avg:47.21ms
step:1134/1845 train_time:53554ms step_avg:47.23ms
step:1135/1845 train_time:53617ms step_avg:47.24ms
step:1136/1845 train_time:53677ms step_avg:47.25ms
step:1137/1845 train_time:53739ms step_avg:47.26ms
step:1138/1845 train_time:53800ms step_avg:47.28ms
step:1139/1845 train_time:53863ms step_avg:47.29ms
step:1140/1845 train_time:53924ms step_avg:47.30ms
step:1141/1845 train_time:53986ms step_avg:47.31ms
step:1142/1845 train_time:54047ms step_avg:47.33ms
step:1143/1845 train_time:54109ms step_avg:47.34ms
step:1144/1845 train_time:54170ms step_avg:47.35ms
step:1145/1845 train_time:54233ms step_avg:47.37ms
step:1146/1845 train_time:54295ms step_avg:47.38ms
step:1147/1845 train_time:54357ms step_avg:47.39ms
step:1148/1845 train_time:54418ms step_avg:47.40ms
step:1149/1845 train_time:54480ms step_avg:47.41ms
step:1150/1845 train_time:54540ms step_avg:47.43ms
step:1151/1845 train_time:54603ms step_avg:47.44ms
step:1152/1845 train_time:54663ms step_avg:47.45ms
step:1153/1845 train_time:54726ms step_avg:47.46ms
step:1154/1845 train_time:54787ms step_avg:47.48ms
step:1155/1845 train_time:54850ms step_avg:47.49ms
step:1156/1845 train_time:54911ms step_avg:47.50ms
step:1157/1845 train_time:54974ms step_avg:47.51ms
step:1158/1845 train_time:55035ms step_avg:47.53ms
step:1159/1845 train_time:55097ms step_avg:47.54ms
step:1160/1845 train_time:55158ms step_avg:47.55ms
step:1161/1845 train_time:55221ms step_avg:47.56ms
step:1162/1845 train_time:55282ms step_avg:47.57ms
step:1163/1845 train_time:55345ms step_avg:47.59ms
step:1164/1845 train_time:55406ms step_avg:47.60ms
step:1165/1845 train_time:55468ms step_avg:47.61ms
step:1166/1845 train_time:55530ms step_avg:47.62ms
step:1167/1845 train_time:55592ms step_avg:47.64ms
step:1168/1845 train_time:55654ms step_avg:47.65ms
step:1169/1845 train_time:55716ms step_avg:47.66ms
step:1170/1845 train_time:55777ms step_avg:47.67ms
step:1171/1845 train_time:55839ms step_avg:47.69ms
step:1172/1845 train_time:55901ms step_avg:47.70ms
step:1173/1845 train_time:55963ms step_avg:47.71ms
step:1174/1845 train_time:56023ms step_avg:47.72ms
step:1175/1845 train_time:56085ms step_avg:47.73ms
step:1176/1845 train_time:56146ms step_avg:47.74ms
step:1177/1845 train_time:56209ms step_avg:47.76ms
step:1178/1845 train_time:56270ms step_avg:47.77ms
step:1179/1845 train_time:56333ms step_avg:47.78ms
step:1180/1845 train_time:56394ms step_avg:47.79ms
step:1181/1845 train_time:56456ms step_avg:47.80ms
step:1182/1845 train_time:56517ms step_avg:47.81ms
step:1183/1845 train_time:56579ms step_avg:47.83ms
step:1184/1845 train_time:56640ms step_avg:47.84ms
step:1185/1845 train_time:56702ms step_avg:47.85ms
step:1186/1845 train_time:56763ms step_avg:47.86ms
step:1187/1845 train_time:56826ms step_avg:47.87ms
step:1188/1845 train_time:56887ms step_avg:47.88ms
step:1189/1845 train_time:56949ms step_avg:47.90ms
step:1190/1845 train_time:57011ms step_avg:47.91ms
step:1191/1845 train_time:57073ms step_avg:47.92ms
step:1192/1845 train_time:57134ms step_avg:47.93ms
step:1193/1845 train_time:57196ms step_avg:47.94ms
step:1194/1845 train_time:57257ms step_avg:47.95ms
step:1195/1845 train_time:57319ms step_avg:47.97ms
step:1196/1845 train_time:57380ms step_avg:47.98ms
step:1197/1845 train_time:57443ms step_avg:47.99ms
step:1198/1845 train_time:57504ms step_avg:48.00ms
step:1199/1845 train_time:57566ms step_avg:48.01ms
step:1200/1845 train_time:57626ms step_avg:48.02ms
step:1201/1845 train_time:57689ms step_avg:48.03ms
step:1202/1845 train_time:57750ms step_avg:48.04ms
step:1203/1845 train_time:57813ms step_avg:48.06ms
step:1204/1845 train_time:57874ms step_avg:48.07ms
step:1205/1845 train_time:57937ms step_avg:48.08ms
step:1206/1845 train_time:58025ms step_avg:48.11ms
step:1207/1845 train_time:58114ms step_avg:48.15ms
step:1208/1845 train_time:58200ms step_avg:48.18ms
step:1209/1845 train_time:58290ms step_avg:48.21ms
step:1210/1845 train_time:58378ms step_avg:48.25ms
step:1211/1845 train_time:58468ms step_avg:48.28ms
step:1212/1845 train_time:58556ms step_avg:48.31ms
step:1213/1845 train_time:58644ms step_avg:48.35ms
step:1214/1845 train_time:58732ms step_avg:48.38ms
step:1215/1845 train_time:58821ms step_avg:48.41ms
step:1216/1845 train_time:58908ms step_avg:48.44ms
step:1217/1845 train_time:58996ms step_avg:48.48ms
step:1218/1845 train_time:59084ms step_avg:48.51ms
step:1219/1845 train_time:59172ms step_avg:48.54ms
step:1220/1845 train_time:59260ms step_avg:48.57ms
step:1221/1845 train_time:59349ms step_avg:48.61ms
step:1222/1845 train_time:59436ms step_avg:48.64ms
step:1223/1845 train_time:59525ms step_avg:48.67ms
step:1224/1845 train_time:59612ms step_avg:48.70ms
step:1225/1845 train_time:59701ms step_avg:48.74ms
step:1226/1845 train_time:59788ms step_avg:48.77ms
step:1227/1845 train_time:59876ms step_avg:48.80ms
step:1228/1845 train_time:59963ms step_avg:48.83ms
step:1229/1845 train_time:60051ms step_avg:48.86ms
step:1230/1845 train_time:60138ms step_avg:48.89ms
step:1231/1845 train_time:60227ms step_avg:48.93ms
step:1232/1845 train_time:60315ms step_avg:48.96ms
step:1233/1845 train_time:60403ms step_avg:48.99ms
step:1234/1845 train_time:60491ms step_avg:49.02ms
step:1235/1845 train_time:60579ms step_avg:49.05ms
step:1236/1845 train_time:60667ms step_avg:49.08ms
step:1237/1845 train_time:60756ms step_avg:49.12ms
step:1238/1845 train_time:60843ms step_avg:49.15ms
step:1239/1845 train_time:60932ms step_avg:49.18ms
step:1240/1845 train_time:61020ms step_avg:49.21ms
step:1241/1845 train_time:61109ms step_avg:49.24ms
step:1242/1845 train_time:61196ms step_avg:49.27ms
step:1243/1845 train_time:61285ms step_avg:49.30ms
step:1244/1845 train_time:61373ms step_avg:49.34ms
step:1245/1845 train_time:61462ms step_avg:49.37ms
step:1246/1845 train_time:61549ms step_avg:49.40ms
step:1247/1845 train_time:61639ms step_avg:49.43ms
step:1248/1845 train_time:61726ms step_avg:49.46ms
step:1249/1845 train_time:61815ms step_avg:49.49ms
step:1250/1845 train_time:61901ms step_avg:49.52ms
step:1250/1845 val_loss:3.5407 train_time:61991ms step_avg:49.59ms
step:1251/1845 train_time:62011ms step_avg:49.57ms
step:1252/1845 train_time:62077ms step_avg:49.58ms
step:1253/1845 train_time:62166ms step_avg:49.61ms
step:1254/1845 train_time:62261ms step_avg:49.65ms
step:1255/1845 train_time:62354ms step_avg:49.68ms
step:1256/1845 train_time:62440ms step_avg:49.71ms
step:1257/1845 train_time:62528ms step_avg:49.74ms
step:1258/1845 train_time:62614ms step_avg:49.77ms
step:1259/1845 train_time:62702ms step_avg:49.80ms
step:1260/1845 train_time:62788ms step_avg:49.83ms
step:1261/1845 train_time:62876ms step_avg:49.86ms
step:1262/1845 train_time:62968ms step_avg:49.90ms
step:1263/1845 train_time:63058ms step_avg:49.93ms
step:1264/1845 train_time:63145ms step_avg:49.96ms
step:1265/1845 train_time:63237ms step_avg:49.99ms
step:1266/1845 train_time:63324ms step_avg:50.02ms
step:1267/1845 train_time:63414ms step_avg:50.05ms
step:1268/1845 train_time:63500ms step_avg:50.08ms
step:1269/1845 train_time:63587ms step_avg:50.11ms
step:1270/1845 train_time:63673ms step_avg:50.14ms
step:1271/1845 train_time:63760ms step_avg:50.17ms
step:1272/1845 train_time:63848ms step_avg:50.20ms
step:1273/1845 train_time:63937ms step_avg:50.23ms
step:1274/1845 train_time:64025ms step_avg:50.25ms
step:1275/1845 train_time:64113ms step_avg:50.28ms
step:1276/1845 train_time:64202ms step_avg:50.32ms
step:1277/1845 train_time:64290ms step_avg:50.34ms
step:1278/1845 train_time:64379ms step_avg:50.37ms
step:1279/1845 train_time:64468ms step_avg:50.40ms
step:1280/1845 train_time:64555ms step_avg:50.43ms
step:1281/1845 train_time:64643ms step_avg:50.46ms
step:1282/1845 train_time:64729ms step_avg:50.49ms
step:1283/1845 train_time:64816ms step_avg:50.52ms
step:1284/1845 train_time:64904ms step_avg:50.55ms
step:1285/1845 train_time:64993ms step_avg:50.58ms
step:1286/1845 train_time:65081ms step_avg:50.61ms
step:1287/1845 train_time:65169ms step_avg:50.64ms
step:1288/1845 train_time:65258ms step_avg:50.67ms
step:1289/1845 train_time:65346ms step_avg:50.70ms
step:1290/1845 train_time:65434ms step_avg:50.72ms
step:1291/1845 train_time:65523ms step_avg:50.75ms
step:1292/1845 train_time:65610ms step_avg:50.78ms
step:1293/1845 train_time:65699ms step_avg:50.81ms
step:1294/1845 train_time:65786ms step_avg:50.84ms
step:1295/1845 train_time:65874ms step_avg:50.87ms
step:1296/1845 train_time:65962ms step_avg:50.90ms
step:1297/1845 train_time:66050ms step_avg:50.93ms
step:1298/1845 train_time:66138ms step_avg:50.95ms
step:1299/1845 train_time:66227ms step_avg:50.98ms
step:1300/1845 train_time:66314ms step_avg:51.01ms
step:1301/1845 train_time:66403ms step_avg:51.04ms
step:1302/1845 train_time:66490ms step_avg:51.07ms
step:1303/1845 train_time:66579ms step_avg:51.10ms
step:1304/1845 train_time:66666ms step_avg:51.12ms
step:1305/1845 train_time:66754ms step_avg:51.15ms
step:1306/1845 train_time:66840ms step_avg:51.18ms
step:1307/1845 train_time:66929ms step_avg:51.21ms
step:1308/1845 train_time:67017ms step_avg:51.24ms
step:1309/1845 train_time:67106ms step_avg:51.27ms
step:1310/1845 train_time:67195ms step_avg:51.29ms
step:1311/1845 train_time:67284ms step_avg:51.32ms
step:1312/1845 train_time:67373ms step_avg:51.35ms
step:1313/1845 train_time:67461ms step_avg:51.38ms
step:1314/1845 train_time:67548ms step_avg:51.41ms
step:1315/1845 train_time:67637ms step_avg:51.44ms
step:1316/1845 train_time:67724ms step_avg:51.46ms
step:1317/1845 train_time:67812ms step_avg:51.49ms
step:1318/1845 train_time:67899ms step_avg:51.52ms
step:1319/1845 train_time:67987ms step_avg:51.54ms
step:1320/1845 train_time:68075ms step_avg:51.57ms
step:1321/1845 train_time:68163ms step_avg:51.60ms
step:1322/1845 train_time:68250ms step_avg:51.63ms
step:1323/1845 train_time:68339ms step_avg:51.65ms
step:1324/1845 train_time:68427ms step_avg:51.68ms
step:1325/1845 train_time:68515ms step_avg:51.71ms
step:1326/1845 train_time:68602ms step_avg:51.74ms
step:1327/1845 train_time:68691ms step_avg:51.76ms
step:1328/1845 train_time:68779ms step_avg:51.79ms
step:1329/1845 train_time:68867ms step_avg:51.82ms
step:1330/1845 train_time:68955ms step_avg:51.85ms
step:1331/1845 train_time:69044ms step_avg:51.87ms
step:1332/1845 train_time:69132ms step_avg:51.90ms
step:1333/1845 train_time:69221ms step_avg:51.93ms
step:1334/1845 train_time:69309ms step_avg:51.96ms
step:1335/1845 train_time:69397ms step_avg:51.98ms
step:1336/1845 train_time:69485ms step_avg:52.01ms
step:1337/1845 train_time:69573ms step_avg:52.04ms
step:1338/1845 train_time:69660ms step_avg:52.06ms
step:1339/1845 train_time:69749ms step_avg:52.09ms
step:1340/1845 train_time:69837ms step_avg:52.12ms
step:1341/1845 train_time:69924ms step_avg:52.14ms
step:1342/1845 train_time:70012ms step_avg:52.17ms
step:1343/1845 train_time:70101ms step_avg:52.20ms
step:1344/1845 train_time:70189ms step_avg:52.22ms
step:1345/1845 train_time:70278ms step_avg:52.25ms
step:1346/1845 train_time:70365ms step_avg:52.28ms
step:1347/1845 train_time:70455ms step_avg:52.30ms
step:1348/1845 train_time:70542ms step_avg:52.33ms
step:1349/1845 train_time:70629ms step_avg:52.36ms
step:1350/1845 train_time:70716ms step_avg:52.38ms
step:1351/1845 train_time:70805ms step_avg:52.41ms
step:1352/1845 train_time:70892ms step_avg:52.44ms
step:1353/1845 train_time:70980ms step_avg:52.46ms
step:1354/1845 train_time:71068ms step_avg:52.49ms
step:1355/1845 train_time:71157ms step_avg:52.51ms
step:1356/1845 train_time:71244ms step_avg:52.54ms
step:1357/1845 train_time:71333ms step_avg:52.57ms
step:1358/1845 train_time:71421ms step_avg:52.59ms
step:1359/1845 train_time:71510ms step_avg:52.62ms
step:1360/1845 train_time:71598ms step_avg:52.65ms
step:1361/1845 train_time:71685ms step_avg:52.67ms
step:1362/1845 train_time:71773ms step_avg:52.70ms
step:1363/1845 train_time:71862ms step_avg:52.72ms
step:1364/1845 train_time:71949ms step_avg:52.75ms
step:1365/1845 train_time:72037ms step_avg:52.77ms
step:1366/1845 train_time:72125ms step_avg:52.80ms
step:1367/1845 train_time:72212ms step_avg:52.83ms
step:1368/1845 train_time:72301ms step_avg:52.85ms
step:1369/1845 train_time:72389ms step_avg:52.88ms
step:1370/1845 train_time:72477ms step_avg:52.90ms
step:1371/1845 train_time:72565ms step_avg:52.93ms
step:1372/1845 train_time:72653ms step_avg:52.95ms
step:1373/1845 train_time:72741ms step_avg:52.98ms
step:1374/1845 train_time:72828ms step_avg:53.00ms
step:1375/1845 train_time:72917ms step_avg:53.03ms
step:1376/1845 train_time:73004ms step_avg:53.06ms
step:1377/1845 train_time:73092ms step_avg:53.08ms
step:1378/1845 train_time:73180ms step_avg:53.11ms
step:1379/1845 train_time:73269ms step_avg:53.13ms
step:1380/1845 train_time:73356ms step_avg:53.16ms
step:1381/1845 train_time:73445ms step_avg:53.18ms
step:1382/1845 train_time:73533ms step_avg:53.21ms
step:1383/1845 train_time:73620ms step_avg:53.23ms
step:1384/1845 train_time:73707ms step_avg:53.26ms
step:1385/1845 train_time:73795ms step_avg:53.28ms
step:1386/1845 train_time:73882ms step_avg:53.31ms
step:1387/1845 train_time:73971ms step_avg:53.33ms
step:1388/1845 train_time:74059ms step_avg:53.36ms
step:1389/1845 train_time:74147ms step_avg:53.38ms
step:1390/1845 train_time:74236ms step_avg:53.41ms
step:1391/1845 train_time:74325ms step_avg:53.43ms
step:1392/1845 train_time:74412ms step_avg:53.46ms
step:1393/1845 train_time:74501ms step_avg:53.48ms
step:1394/1845 train_time:74589ms step_avg:53.51ms
step:1395/1845 train_time:74677ms step_avg:53.53ms
step:1396/1845 train_time:74764ms step_avg:53.56ms
step:1397/1845 train_time:74853ms step_avg:53.58ms
step:1398/1845 train_time:74940ms step_avg:53.61ms
step:1399/1845 train_time:75028ms step_avg:53.63ms
step:1400/1845 train_time:75115ms step_avg:53.65ms
step:1401/1845 train_time:75204ms step_avg:53.68ms
step:1402/1845 train_time:75292ms step_avg:53.70ms
step:1403/1845 train_time:75380ms step_avg:53.73ms
step:1404/1845 train_time:75467ms step_avg:53.75ms
step:1405/1845 train_time:75556ms step_avg:53.78ms
step:1406/1845 train_time:75643ms step_avg:53.80ms
step:1407/1845 train_time:75732ms step_avg:53.82ms
step:1408/1845 train_time:75819ms step_avg:53.85ms
step:1409/1845 train_time:75907ms step_avg:53.87ms
step:1410/1845 train_time:75995ms step_avg:53.90ms
step:1411/1845 train_time:76084ms step_avg:53.92ms
step:1412/1845 train_time:76171ms step_avg:53.95ms
step:1413/1845 train_time:76260ms step_avg:53.97ms
step:1414/1845 train_time:76347ms step_avg:53.99ms
step:1415/1845 train_time:76436ms step_avg:54.02ms
step:1416/1845 train_time:76523ms step_avg:54.04ms
step:1417/1845 train_time:76611ms step_avg:54.07ms
step:1418/1845 train_time:76699ms step_avg:54.09ms
step:1419/1845 train_time:76787ms step_avg:54.11ms
step:1420/1845 train_time:76874ms step_avg:54.14ms
step:1421/1845 train_time:76963ms step_avg:54.16ms
step:1422/1845 train_time:77050ms step_avg:54.18ms
step:1423/1845 train_time:77139ms step_avg:54.21ms
step:1424/1845 train_time:77226ms step_avg:54.23ms
step:1425/1845 train_time:77314ms step_avg:54.26ms
step:1426/1845 train_time:77402ms step_avg:54.28ms
step:1427/1845 train_time:77491ms step_avg:54.30ms
step:1428/1845 train_time:77578ms step_avg:54.33ms
step:1429/1845 train_time:77666ms step_avg:54.35ms
step:1430/1845 train_time:77755ms step_avg:54.37ms
step:1431/1845 train_time:77843ms step_avg:54.40ms
step:1432/1845 train_time:77930ms step_avg:54.42ms
step:1433/1845 train_time:78018ms step_avg:54.44ms
step:1434/1845 train_time:78105ms step_avg:54.47ms
step:1435/1845 train_time:78193ms step_avg:54.49ms
step:1436/1845 train_time:78281ms step_avg:54.51ms
step:1437/1845 train_time:78369ms step_avg:54.54ms
step:1438/1845 train_time:78458ms step_avg:54.56ms
step:1439/1845 train_time:78546ms step_avg:54.58ms
step:1440/1845 train_time:78633ms step_avg:54.61ms
step:1441/1845 train_time:78722ms step_avg:54.63ms
step:1442/1845 train_time:78809ms step_avg:54.65ms
step:1443/1845 train_time:78897ms step_avg:54.68ms
step:1444/1845 train_time:78984ms step_avg:54.70ms
step:1445/1845 train_time:79072ms step_avg:54.72ms
step:1446/1845 train_time:79161ms step_avg:54.74ms
step:1447/1845 train_time:79249ms step_avg:54.77ms
step:1448/1845 train_time:79336ms step_avg:54.79ms
step:1449/1845 train_time:79424ms step_avg:54.81ms
step:1450/1845 train_time:79512ms step_avg:54.84ms
step:1451/1845 train_time:79601ms step_avg:54.86ms
step:1452/1845 train_time:79688ms step_avg:54.88ms
step:1453/1845 train_time:79777ms step_avg:54.90ms
step:1454/1845 train_time:79864ms step_avg:54.93ms
step:1455/1845 train_time:79952ms step_avg:54.95ms
step:1456/1845 train_time:80039ms step_avg:54.97ms
step:1457/1845 train_time:80127ms step_avg:54.99ms
step:1458/1845 train_time:80215ms step_avg:55.02ms
step:1459/1845 train_time:80303ms step_avg:55.04ms
step:1460/1845 train_time:80390ms step_avg:55.06ms
step:1461/1845 train_time:80479ms step_avg:55.08ms
step:1462/1845 train_time:80567ms step_avg:55.11ms
step:1463/1845 train_time:80657ms step_avg:55.13ms
step:1464/1845 train_time:80744ms step_avg:55.15ms
step:1465/1845 train_time:80832ms step_avg:55.18ms
step:1466/1845 train_time:80919ms step_avg:55.20ms
step:1467/1845 train_time:81008ms step_avg:55.22ms
step:1468/1845 train_time:81096ms step_avg:55.24ms
step:1469/1845 train_time:81184ms step_avg:55.27ms
step:1470/1845 train_time:81272ms step_avg:55.29ms
step:1471/1845 train_time:81360ms step_avg:55.31ms
step:1472/1845 train_time:81448ms step_avg:55.33ms
step:1473/1845 train_time:81537ms step_avg:55.35ms
step:1474/1845 train_time:81624ms step_avg:55.38ms
step:1475/1845 train_time:81713ms step_avg:55.40ms
step:1476/1845 train_time:81800ms step_avg:55.42ms
step:1477/1845 train_time:81889ms step_avg:55.44ms
step:1478/1845 train_time:81977ms step_avg:55.46ms
step:1479/1845 train_time:82065ms step_avg:55.49ms
step:1480/1845 train_time:82152ms step_avg:55.51ms
step:1481/1845 train_time:82241ms step_avg:55.53ms
step:1482/1845 train_time:82328ms step_avg:55.55ms
step:1483/1845 train_time:82417ms step_avg:55.57ms
step:1484/1845 train_time:82504ms step_avg:55.60ms
step:1485/1845 train_time:82593ms step_avg:55.62ms
step:1486/1845 train_time:82680ms step_avg:55.64ms
step:1487/1845 train_time:82769ms step_avg:55.66ms
step:1488/1845 train_time:82857ms step_avg:55.68ms
step:1489/1845 train_time:82945ms step_avg:55.70ms
step:1490/1845 train_time:83032ms step_avg:55.73ms
step:1491/1845 train_time:83120ms step_avg:55.75ms
step:1492/1845 train_time:83207ms step_avg:55.77ms
step:1493/1845 train_time:83295ms step_avg:55.79ms
step:1494/1845 train_time:83384ms step_avg:55.81ms
step:1495/1845 train_time:83471ms step_avg:55.83ms
step:1496/1845 train_time:83559ms step_avg:55.86ms
step:1497/1845 train_time:83648ms step_avg:55.88ms
step:1498/1845 train_time:83736ms step_avg:55.90ms
step:1499/1845 train_time:83824ms step_avg:55.92ms
step:1500/1845 train_time:83911ms step_avg:55.94ms
step:1500/1845 val_loss:3.4071 train_time:84002ms step_avg:56.00ms
step:1501/1845 train_time:84022ms step_avg:55.98ms
step:1502/1845 train_time:84090ms step_avg:55.99ms
step:1503/1845 train_time:84179ms step_avg:56.01ms
step:1504/1845 train_time:84272ms step_avg:56.03ms
step:1505/1845 train_time:84362ms step_avg:56.05ms
step:1506/1845 train_time:84447ms step_avg:56.07ms
step:1507/1845 train_time:84534ms step_avg:56.09ms
step:1508/1845 train_time:84620ms step_avg:56.11ms
step:1509/1845 train_time:84707ms step_avg:56.13ms
step:1510/1845 train_time:84794ms step_avg:56.16ms
step:1511/1845 train_time:84881ms step_avg:56.18ms
step:1512/1845 train_time:84969ms step_avg:56.20ms
step:1513/1845 train_time:85064ms step_avg:56.22ms
step:1514/1845 train_time:85154ms step_avg:56.24ms
step:1515/1845 train_time:85243ms step_avg:56.27ms
step:1516/1845 train_time:85330ms step_avg:56.29ms
step:1517/1845 train_time:85419ms step_avg:56.31ms
step:1518/1845 train_time:85505ms step_avg:56.33ms
step:1519/1845 train_time:85592ms step_avg:56.35ms
step:1520/1845 train_time:85679ms step_avg:56.37ms
step:1521/1845 train_time:85765ms step_avg:56.39ms
step:1522/1845 train_time:85852ms step_avg:56.41ms
step:1523/1845 train_time:85941ms step_avg:56.43ms
step:1524/1845 train_time:86030ms step_avg:56.45ms
step:1525/1845 train_time:86122ms step_avg:56.47ms
step:1526/1845 train_time:86210ms step_avg:56.49ms
step:1527/1845 train_time:86300ms step_avg:56.52ms
step:1528/1845 train_time:86386ms step_avg:56.54ms
step:1529/1845 train_time:86475ms step_avg:56.56ms
step:1530/1845 train_time:86562ms step_avg:56.58ms
step:1531/1845 train_time:86649ms step_avg:56.60ms
step:1532/1845 train_time:86736ms step_avg:56.62ms
step:1533/1845 train_time:86823ms step_avg:56.64ms
step:1534/1845 train_time:86910ms step_avg:56.66ms
step:1535/1845 train_time:87001ms step_avg:56.68ms
step:1536/1845 train_time:87090ms step_avg:56.70ms
step:1537/1845 train_time:87181ms step_avg:56.72ms
step:1538/1845 train_time:87268ms step_avg:56.74ms
step:1539/1845 train_time:87357ms step_avg:56.76ms
step:1540/1845 train_time:87443ms step_avg:56.78ms
step:1541/1845 train_time:87532ms step_avg:56.80ms
step:1542/1845 train_time:87619ms step_avg:56.82ms
step:1543/1845 train_time:87706ms step_avg:56.84ms
step:1544/1845 train_time:87793ms step_avg:56.86ms
step:1545/1845 train_time:87882ms step_avg:56.88ms
step:1546/1845 train_time:87969ms step_avg:56.90ms
step:1547/1845 train_time:88059ms step_avg:56.92ms
step:1548/1845 train_time:88147ms step_avg:56.94ms
step:1549/1845 train_time:88235ms step_avg:56.96ms
step:1550/1845 train_time:88323ms step_avg:56.98ms
step:1551/1845 train_time:88410ms step_avg:57.00ms
step:1552/1845 train_time:88498ms step_avg:57.02ms
step:1553/1845 train_time:88585ms step_avg:57.04ms
step:1554/1845 train_time:88672ms step_avg:57.06ms
step:1555/1845 train_time:88760ms step_avg:57.08ms
step:1556/1845 train_time:88846ms step_avg:57.10ms
step:1557/1845 train_time:88935ms step_avg:57.12ms
step:1558/1845 train_time:89022ms step_avg:57.14ms
step:1559/1845 train_time:89111ms step_avg:57.16ms
step:1560/1845 train_time:89199ms step_avg:57.18ms
step:1561/1845 train_time:89288ms step_avg:57.20ms
step:1562/1845 train_time:89376ms step_avg:57.22ms
step:1563/1845 train_time:89464ms step_avg:57.24ms
step:1564/1845 train_time:89552ms step_avg:57.26ms
step:1565/1845 train_time:89640ms step_avg:57.28ms
step:1566/1845 train_time:89727ms step_avg:57.30ms
step:1567/1845 train_time:89816ms step_avg:57.32ms
step:1568/1845 train_time:89903ms step_avg:57.34ms
step:1569/1845 train_time:89992ms step_avg:57.36ms
step:1570/1845 train_time:90080ms step_avg:57.38ms
step:1571/1845 train_time:90169ms step_avg:57.40ms
step:1572/1845 train_time:90256ms step_avg:57.41ms
step:1573/1845 train_time:90345ms step_avg:57.43ms
step:1574/1845 train_time:90432ms step_avg:57.45ms
step:1575/1845 train_time:90520ms step_avg:57.47ms
step:1576/1845 train_time:90607ms step_avg:57.49ms
step:1577/1845 train_time:90695ms step_avg:57.51ms
step:1578/1845 train_time:90782ms step_avg:57.53ms
step:1579/1845 train_time:90870ms step_avg:57.55ms
step:1580/1845 train_time:90958ms step_avg:57.57ms
step:1581/1845 train_time:91046ms step_avg:57.59ms
step:1582/1845 train_time:91134ms step_avg:57.61ms
step:1583/1845 train_time:91223ms step_avg:57.63ms
step:1584/1845 train_time:91310ms step_avg:57.64ms
step:1585/1845 train_time:91399ms step_avg:57.66ms
step:1586/1845 train_time:91486ms step_avg:57.68ms
step:1587/1845 train_time:91574ms step_avg:57.70ms
step:1588/1845 train_time:91662ms step_avg:57.72ms
step:1589/1845 train_time:91751ms step_avg:57.74ms
step:1590/1845 train_time:91838ms step_avg:57.76ms
step:1591/1845 train_time:91926ms step_avg:57.78ms
step:1592/1845 train_time:92014ms step_avg:57.80ms
step:1593/1845 train_time:92102ms step_avg:57.82ms
step:1594/1845 train_time:92189ms step_avg:57.83ms
step:1595/1845 train_time:92278ms step_avg:57.85ms
step:1596/1845 train_time:92366ms step_avg:57.87ms
step:1597/1845 train_time:92455ms step_avg:57.89ms
step:1598/1845 train_time:92541ms step_avg:57.91ms
step:1599/1845 train_time:92631ms step_avg:57.93ms
step:1600/1845 train_time:92717ms step_avg:57.95ms
step:1601/1845 train_time:92806ms step_avg:57.97ms
step:1602/1845 train_time:92893ms step_avg:57.99ms
step:1603/1845 train_time:92980ms step_avg:58.00ms
step:1604/1845 train_time:93068ms step_avg:58.02ms
step:1605/1845 train_time:93157ms step_avg:58.04ms
step:1606/1845 train_time:93245ms step_avg:58.06ms
step:1607/1845 train_time:93334ms step_avg:58.08ms
step:1608/1845 train_time:93421ms step_avg:58.10ms
step:1609/1845 train_time:93510ms step_avg:58.12ms
step:1610/1845 train_time:93597ms step_avg:58.13ms
step:1611/1845 train_time:93685ms step_avg:58.15ms
step:1612/1845 train_time:93772ms step_avg:58.17ms
step:1613/1845 train_time:93860ms step_avg:58.19ms
step:1614/1845 train_time:93948ms step_avg:58.21ms
step:1615/1845 train_time:94035ms step_avg:58.23ms
step:1616/1845 train_time:94122ms step_avg:58.24ms
step:1617/1845 train_time:94211ms step_avg:58.26ms
step:1618/1845 train_time:94298ms step_avg:58.28ms
step:1619/1845 train_time:94387ms step_avg:58.30ms
step:1620/1845 train_time:94475ms step_avg:58.32ms
step:1621/1845 train_time:94564ms step_avg:58.34ms
step:1622/1845 train_time:94651ms step_avg:58.35ms
step:1623/1845 train_time:94739ms step_avg:58.37ms
step:1624/1845 train_time:94826ms step_avg:58.39ms
step:1625/1845 train_time:94914ms step_avg:58.41ms
step:1626/1845 train_time:95002ms step_avg:58.43ms
step:1627/1845 train_time:95091ms step_avg:58.45ms
step:1628/1845 train_time:95179ms step_avg:58.46ms
step:1629/1845 train_time:95267ms step_avg:58.48ms
step:1630/1845 train_time:95355ms step_avg:58.50ms
step:1631/1845 train_time:95444ms step_avg:58.52ms
step:1632/1845 train_time:95532ms step_avg:58.54ms
step:1633/1845 train_time:95620ms step_avg:58.55ms
step:1634/1845 train_time:95707ms step_avg:58.57ms
step:1635/1845 train_time:95795ms step_avg:58.59ms
step:1636/1845 train_time:95882ms step_avg:58.61ms
step:1637/1845 train_time:95971ms step_avg:58.63ms
step:1638/1845 train_time:96059ms step_avg:58.64ms
step:1639/1845 train_time:96147ms step_avg:58.66ms
step:1640/1845 train_time:96235ms step_avg:58.68ms
step:1641/1845 train_time:96323ms step_avg:58.70ms
step:1642/1845 train_time:96411ms step_avg:58.72ms
step:1643/1845 train_time:96500ms step_avg:58.73ms
step:1644/1845 train_time:96587ms step_avg:58.75ms
step:1645/1845 train_time:96676ms step_avg:58.77ms
step:1646/1845 train_time:96763ms step_avg:58.79ms
step:1647/1845 train_time:96852ms step_avg:58.80ms
step:1648/1845 train_time:96939ms step_avg:58.82ms
step:1649/1845 train_time:97028ms step_avg:58.84ms
step:1650/1845 train_time:97116ms step_avg:58.86ms
step:1651/1845 train_time:97204ms step_avg:58.88ms
step:1652/1845 train_time:97292ms step_avg:58.89ms
step:1653/1845 train_time:97381ms step_avg:58.91ms
step:1654/1845 train_time:97469ms step_avg:58.93ms
step:1655/1845 train_time:97556ms step_avg:58.95ms
step:1656/1845 train_time:97643ms step_avg:58.96ms
step:1657/1845 train_time:97732ms step_avg:58.98ms
step:1658/1845 train_time:97820ms step_avg:59.00ms
step:1659/1845 train_time:97907ms step_avg:59.02ms
step:1660/1845 train_time:97994ms step_avg:59.03ms
step:1661/1845 train_time:98082ms step_avg:59.05ms
step:1662/1845 train_time:98170ms step_avg:59.07ms
step:1663/1845 train_time:98259ms step_avg:59.09ms
step:1664/1845 train_time:98347ms step_avg:59.10ms
step:1665/1845 train_time:98435ms step_avg:59.12ms
step:1666/1845 train_time:98521ms step_avg:59.14ms
step:1667/1845 train_time:98610ms step_avg:59.15ms
step:1668/1845 train_time:98698ms step_avg:59.17ms
step:1669/1845 train_time:98785ms step_avg:59.19ms
step:1670/1845 train_time:98872ms step_avg:59.20ms
step:1671/1845 train_time:98961ms step_avg:59.22ms
step:1672/1845 train_time:99047ms step_avg:59.24ms
step:1673/1845 train_time:99136ms step_avg:59.26ms
step:1674/1845 train_time:99223ms step_avg:59.27ms
step:1675/1845 train_time:99311ms step_avg:59.29ms
step:1676/1845 train_time:99398ms step_avg:59.31ms
step:1677/1845 train_time:99487ms step_avg:59.32ms
step:1678/1845 train_time:99574ms step_avg:59.34ms
step:1679/1845 train_time:99663ms step_avg:59.36ms
step:1680/1845 train_time:99750ms step_avg:59.38ms
step:1681/1845 train_time:99838ms step_avg:59.39ms
step:1682/1845 train_time:99925ms step_avg:59.41ms
step:1683/1845 train_time:100013ms step_avg:59.43ms
step:1684/1845 train_time:100101ms step_avg:59.44ms
step:1685/1845 train_time:100189ms step_avg:59.46ms
step:1686/1845 train_time:100277ms step_avg:59.48ms
step:1687/1845 train_time:100366ms step_avg:59.49ms
step:1688/1845 train_time:100453ms step_avg:59.51ms
step:1689/1845 train_time:100541ms step_avg:59.53ms
step:1690/1845 train_time:100628ms step_avg:59.54ms
step:1691/1845 train_time:100716ms step_avg:59.56ms
step:1692/1845 train_time:100803ms step_avg:59.58ms
step:1693/1845 train_time:100892ms step_avg:59.59ms
step:1694/1845 train_time:100980ms step_avg:59.61ms
step:1695/1845 train_time:101068ms step_avg:59.63ms
step:1696/1845 train_time:101156ms step_avg:59.64ms
step:1697/1845 train_time:101245ms step_avg:59.66ms
step:1698/1845 train_time:101332ms step_avg:59.68ms
step:1699/1845 train_time:101421ms step_avg:59.69ms
step:1700/1845 train_time:101508ms step_avg:59.71ms
step:1701/1845 train_time:101597ms step_avg:59.73ms
step:1702/1845 train_time:101685ms step_avg:59.74ms
step:1703/1845 train_time:101773ms step_avg:59.76ms
step:1704/1845 train_time:101861ms step_avg:59.78ms
step:1705/1845 train_time:101949ms step_avg:59.79ms
step:1706/1845 train_time:102037ms step_avg:59.81ms
step:1707/1845 train_time:102125ms step_avg:59.83ms
step:1708/1845 train_time:102212ms step_avg:59.84ms
step:1709/1845 train_time:102301ms step_avg:59.86ms
step:1710/1845 train_time:102388ms step_avg:59.88ms
step:1711/1845 train_time:102477ms step_avg:59.89ms
step:1712/1845 train_time:102564ms step_avg:59.91ms
step:1713/1845 train_time:102652ms step_avg:59.93ms
step:1714/1845 train_time:102740ms step_avg:59.94ms
step:1715/1845 train_time:102829ms step_avg:59.96ms
step:1716/1845 train_time:102915ms step_avg:59.97ms
step:1717/1845 train_time:103004ms step_avg:59.99ms
step:1718/1845 train_time:103092ms step_avg:60.01ms
step:1719/1845 train_time:103180ms step_avg:60.02ms
step:1720/1845 train_time:103267ms step_avg:60.04ms
step:1721/1845 train_time:103356ms step_avg:60.06ms
step:1722/1845 train_time:103443ms step_avg:60.07ms
step:1723/1845 train_time:103532ms step_avg:60.09ms
step:1724/1845 train_time:103619ms step_avg:60.10ms
step:1725/1845 train_time:103708ms step_avg:60.12ms
step:1726/1845 train_time:103795ms step_avg:60.14ms
step:1727/1845 train_time:103883ms step_avg:60.15ms
step:1728/1845 train_time:103972ms step_avg:60.17ms
step:1729/1845 train_time:104061ms step_avg:60.19ms
step:1730/1845 train_time:104149ms step_avg:60.20ms
step:1731/1845 train_time:104238ms step_avg:60.22ms
step:1732/1845 train_time:104326ms step_avg:60.23ms
step:1733/1845 train_time:104414ms step_avg:60.25ms
step:1734/1845 train_time:104501ms step_avg:60.27ms
step:1735/1845 train_time:104589ms step_avg:60.28ms
step:1736/1845 train_time:104676ms step_avg:60.30ms
step:1737/1845 train_time:104765ms step_avg:60.31ms
step:1738/1845 train_time:104852ms step_avg:60.33ms
step:1739/1845 train_time:104940ms step_avg:60.35ms
step:1740/1845 train_time:105028ms step_avg:60.36ms
step:1741/1845 train_time:105117ms step_avg:60.38ms
step:1742/1845 train_time:105204ms step_avg:60.39ms
step:1743/1845 train_time:105293ms step_avg:60.41ms
step:1744/1845 train_time:105380ms step_avg:60.42ms
step:1745/1845 train_time:105469ms step_avg:60.44ms
step:1746/1845 train_time:105556ms step_avg:60.46ms
step:1747/1845 train_time:105644ms step_avg:60.47ms
step:1748/1845 train_time:105731ms step_avg:60.49ms
step:1749/1845 train_time:105820ms step_avg:60.50ms
step:1750/1845 train_time:105907ms step_avg:60.52ms
step:1750/1845 val_loss:3.3067 train_time:105997ms step_avg:60.57ms
step:1751/1845 train_time:106016ms step_avg:60.55ms
step:1752/1845 train_time:106086ms step_avg:60.55ms
step:1753/1845 train_time:106180ms step_avg:60.57ms
step:1754/1845 train_time:106268ms step_avg:60.59ms
step:1755/1845 train_time:106355ms step_avg:60.60ms
step:1756/1845 train_time:106442ms step_avg:60.62ms
step:1757/1845 train_time:106531ms step_avg:60.63ms
step:1758/1845 train_time:106618ms step_avg:60.65ms
step:1759/1845 train_time:106705ms step_avg:60.66ms
step:1760/1845 train_time:106792ms step_avg:60.68ms
step:1761/1845 train_time:106879ms step_avg:60.69ms
step:1762/1845 train_time:106966ms step_avg:60.71ms
step:1763/1845 train_time:107056ms step_avg:60.72ms
step:1764/1845 train_time:107146ms step_avg:60.74ms
step:1765/1845 train_time:107235ms step_avg:60.76ms
step:1766/1845 train_time:107322ms step_avg:60.77ms
step:1767/1845 train_time:107411ms step_avg:60.79ms
step:1768/1845 train_time:107498ms step_avg:60.80ms
step:1769/1845 train_time:107586ms step_avg:60.82ms
step:1770/1845 train_time:107672ms step_avg:60.83ms
step:1771/1845 train_time:107759ms step_avg:60.85ms
step:1772/1845 train_time:107845ms step_avg:60.86ms
step:1773/1845 train_time:107934ms step_avg:60.88ms
step:1774/1845 train_time:108022ms step_avg:60.89ms
step:1775/1845 train_time:108113ms step_avg:60.91ms
step:1776/1845 train_time:108201ms step_avg:60.92ms
step:1777/1845 train_time:108292ms step_avg:60.94ms
step:1778/1845 train_time:108379ms step_avg:60.96ms
step:1779/1845 train_time:108468ms step_avg:60.97ms
step:1780/1845 train_time:108554ms step_avg:60.99ms
step:1781/1845 train_time:108642ms step_avg:61.00ms
step:1782/1845 train_time:108729ms step_avg:61.02ms
step:1783/1845 train_time:108816ms step_avg:61.03ms
step:1784/1845 train_time:108903ms step_avg:61.04ms
step:1785/1845 train_time:108993ms step_avg:61.06ms
step:1786/1845 train_time:109081ms step_avg:61.08ms
step:1787/1845 train_time:109170ms step_avg:61.09ms
step:1788/1845 train_time:109258ms step_avg:61.11ms
step:1789/1845 train_time:109347ms step_avg:61.12ms
step:1790/1845 train_time:109434ms step_avg:61.14ms
step:1791/1845 train_time:109523ms step_avg:61.15ms
step:1792/1845 train_time:109610ms step_avg:61.17ms
step:1793/1845 train_time:109698ms step_avg:61.18ms
step:1794/1845 train_time:109784ms step_avg:61.20ms
step:1795/1845 train_time:109872ms step_avg:61.21ms
step:1796/1845 train_time:109960ms step_avg:61.22ms
step:1797/1845 train_time:110048ms step_avg:61.24ms
step:1798/1845 train_time:110136ms step_avg:61.25ms
step:1799/1845 train_time:110226ms step_avg:61.27ms
step:1800/1845 train_time:110313ms step_avg:61.28ms
step:1801/1845 train_time:110401ms step_avg:61.30ms
step:1802/1845 train_time:110489ms step_avg:61.31ms
step:1803/1845 train_time:110578ms step_avg:61.33ms
step:1804/1845 train_time:110665ms step_avg:61.34ms
step:1805/1845 train_time:110753ms step_avg:61.36ms
step:1806/1845 train_time:110841ms step_avg:61.37ms
step:1807/1845 train_time:110930ms step_avg:61.39ms
step:1808/1845 train_time:111018ms step_avg:61.40ms
step:1809/1845 train_time:111107ms step_avg:61.42ms
step:1810/1845 train_time:111195ms step_avg:61.43ms
step:1811/1845 train_time:111283ms step_avg:61.45ms
step:1812/1845 train_time:111371ms step_avg:61.46ms
step:1813/1845 train_time:111461ms step_avg:61.48ms
step:1814/1845 train_time:111548ms step_avg:61.49ms
step:1815/1845 train_time:111637ms step_avg:61.51ms
step:1816/1845 train_time:111724ms step_avg:61.52ms
step:1817/1845 train_time:111811ms step_avg:61.54ms
step:1818/1845 train_time:111899ms step_avg:61.55ms
step:1819/1845 train_time:111988ms step_avg:61.57ms
step:1820/1845 train_time:112076ms step_avg:61.58ms
step:1821/1845 train_time:112165ms step_avg:61.60ms
step:1822/1845 train_time:112252ms step_avg:61.61ms
step:1823/1845 train_time:112340ms step_avg:61.62ms
step:1824/1845 train_time:112429ms step_avg:61.64ms
step:1825/1845 train_time:112518ms step_avg:61.65ms
step:1826/1845 train_time:112606ms step_avg:61.67ms
step:1827/1845 train_time:112694ms step_avg:61.68ms
step:1828/1845 train_time:112781ms step_avg:61.70ms
step:1829/1845 train_time:112870ms step_avg:61.71ms
step:1830/1845 train_time:112957ms step_avg:61.73ms
step:1831/1845 train_time:113047ms step_avg:61.74ms
step:1832/1845 train_time:113134ms step_avg:61.75ms
step:1833/1845 train_time:113223ms step_avg:61.77ms
step:1834/1845 train_time:113311ms step_avg:61.78ms
step:1835/1845 train_time:113400ms step_avg:61.80ms
step:1836/1845 train_time:113487ms step_avg:61.81ms
step:1837/1845 train_time:113576ms step_avg:61.83ms
step:1838/1845 train_time:113663ms step_avg:61.84ms
step:1839/1845 train_time:113752ms step_avg:61.86ms
step:1840/1845 train_time:113838ms step_avg:61.87ms
step:1841/1845 train_time:113928ms step_avg:61.88ms
step:1842/1845 train_time:114016ms step_avg:61.90ms
step:1843/1845 train_time:114105ms step_avg:61.91ms
step:1844/1845 train_time:114193ms step_avg:61.93ms
step:1845/1845 train_time:114281ms step_avg:61.94ms
step:1845/1845 val_loss:3.2800 train_time:114369ms step_avg:61.99ms
peak memory allocated: 29405 MiB reserved: 44318 MiB

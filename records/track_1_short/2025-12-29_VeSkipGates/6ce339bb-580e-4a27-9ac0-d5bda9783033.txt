import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc
os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        group_sizes = [15, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = 'gate' in ref_param.label

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

        # only include gates on layers with value embeds used on forward pass
        if layer_idx in [0,1,8,9,10]:
            self.value_embed_gate = CastedLinear(12, num_heads)
            self.value_embed_gate.weight.label = 'value_embed_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(self.value_embed_gate(x[..., :self.value_embed_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        #self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig, skew = None):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if skew is None:
            logits = 23 * torch.sigmoid((logits+5) / 7.5)
        else:
            logits = 23 * torch.sigmoid((logits+skew) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'skip_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'value_embed_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1805  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
def log_parameter_norms(model):
    norms = {}
    for name, param in model.named_parameters():
        if param.requires_grad:
            norms[f"param_norm/{name}"] = param.norm().item()
            norms[f"param_max/{name}"] = param.max().item()
            norms[f"param_min/{name}"] = param.min().item()
            if param.grad is not None:
                norms[f"grad_norm/{name}"] = param.grad.norm().item()
                norms[f"grad_max/{name}"] = param.grad.max().item()
    
    # Total norms
    total_param_norm = sum(p.norm().item() ** 2 for p in model.parameters()) ** 0.5
    total_grad_norm = sum(p.grad.norm().item() ** 2 for p in model.parameters() if p.grad is not None) ** 0.5
    
    norms["param_norm/total"] = total_param_norm
    norms["grad_norm/total"] = total_grad_norm
    
    return norms

train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

# import wandb
# if master_process:
#     wandb.init(
#         project="nano4",
#         name="baseline"
#     )

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    # if master_process and step%2==1:
    #     wandb.log({
    #         "loss": loss.item(),
    #         **log_parameter_norms(model)  # Add all norms
    #     })
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Mon Dec 29 05:47:54 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   25C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   24C    P0            109W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   23C    P0            107W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   26C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   25C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   25C    P0            110W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 601, 602, 603, 1203, 1204, 1205, 1804, 1805, 1806] for warmup
Resetting Model
step:0/1845 val_loss:10.8309 train_time:0ms step_avg:0.03ms
step:1/1845 train_time:81ms step_avg:80.51ms
step:2/1845 train_time:104ms step_avg:52.23ms
step:3/1845 train_time:125ms step_avg:41.81ms
step:4/1845 train_time:159ms step_avg:39.82ms
step:5/1845 train_time:194ms step_avg:38.70ms
step:6/1845 train_time:274ms step_avg:45.61ms
step:7/1845 train_time:398ms step_avg:56.79ms
step:8/1845 train_time:431ms step_avg:53.93ms
step:9/1845 train_time:466ms step_avg:51.74ms
step:10/1845 train_time:500ms step_avg:49.97ms
step:11/1845 train_time:534ms step_avg:48.58ms
step:12/1845 train_time:568ms step_avg:47.37ms
step:13/1845 train_time:603ms step_avg:46.37ms
step:14/1845 train_time:637ms step_avg:45.49ms
step:15/1845 train_time:671ms step_avg:44.75ms
step:16/1845 train_time:705ms step_avg:44.09ms
step:17/1845 train_time:740ms step_avg:43.54ms
step:18/1845 train_time:774ms step_avg:43.01ms
step:19/1845 train_time:809ms step_avg:42.56ms
step:20/1845 train_time:843ms step_avg:42.15ms
step:21/1845 train_time:877ms step_avg:41.78ms
step:22/1845 train_time:911ms step_avg:41.42ms
step:23/1845 train_time:946ms step_avg:41.12ms
step:24/1845 train_time:980ms step_avg:40.83ms
step:25/1845 train_time:1014ms step_avg:40.57ms
step:26/1845 train_time:1048ms step_avg:40.31ms
step:27/1845 train_time:1083ms step_avg:40.10ms
step:28/1845 train_time:1117ms step_avg:39.89ms
step:29/1845 train_time:1151ms step_avg:39.70ms
step:30/1845 train_time:1185ms step_avg:39.51ms
step:31/1845 train_time:1220ms step_avg:39.34ms
step:32/1845 train_time:1254ms step_avg:39.18ms
step:33/1845 train_time:1288ms step_avg:39.03ms
step:34/1845 train_time:1322ms step_avg:38.88ms
step:35/1845 train_time:1357ms step_avg:38.77ms
step:36/1845 train_time:1391ms step_avg:38.64ms
step:37/1845 train_time:1426ms step_avg:38.53ms
step:38/1845 train_time:1460ms step_avg:38.42ms
step:39/1845 train_time:1495ms step_avg:38.32ms
step:40/1845 train_time:1529ms step_avg:38.22ms
step:41/1845 train_time:1563ms step_avg:38.13ms
step:42/1845 train_time:1598ms step_avg:38.05ms
step:43/1845 train_time:1633ms step_avg:37.97ms
step:44/1845 train_time:1667ms step_avg:37.88ms
step:45/1845 train_time:1701ms step_avg:37.81ms
step:46/1845 train_time:1735ms step_avg:37.73ms
step:47/1845 train_time:1770ms step_avg:37.66ms
step:48/1845 train_time:1804ms step_avg:37.59ms
step:49/1845 train_time:1839ms step_avg:37.53ms
step:50/1845 train_time:1873ms step_avg:37.46ms
step:51/1845 train_time:1907ms step_avg:37.40ms
step:52/1845 train_time:1941ms step_avg:37.33ms
step:53/1845 train_time:1976ms step_avg:37.28ms
step:54/1845 train_time:2010ms step_avg:37.22ms
step:55/1845 train_time:2044ms step_avg:37.17ms
step:56/1845 train_time:2079ms step_avg:37.12ms
step:57/1845 train_time:2113ms step_avg:37.07ms
step:58/1845 train_time:2147ms step_avg:37.02ms
step:59/1845 train_time:2181ms step_avg:36.97ms
step:60/1845 train_time:2216ms step_avg:36.93ms
step:61/1845 train_time:2250ms step_avg:36.89ms
step:62/1845 train_time:2284ms step_avg:36.85ms
step:63/1845 train_time:2319ms step_avg:36.81ms
step:64/1845 train_time:2353ms step_avg:36.77ms
step:65/1845 train_time:2388ms step_avg:36.73ms
step:66/1845 train_time:2422ms step_avg:36.69ms
step:67/1845 train_time:2456ms step_avg:36.66ms
step:68/1845 train_time:2491ms step_avg:36.63ms
step:69/1845 train_time:2525ms step_avg:36.60ms
step:70/1845 train_time:2559ms step_avg:36.56ms
step:71/1845 train_time:2594ms step_avg:36.53ms
step:72/1845 train_time:2628ms step_avg:36.50ms
step:73/1845 train_time:2663ms step_avg:36.48ms
step:74/1845 train_time:2697ms step_avg:36.45ms
step:75/1845 train_time:2732ms step_avg:36.42ms
step:76/1845 train_time:2766ms step_avg:36.39ms
step:77/1845 train_time:2801ms step_avg:36.37ms
step:78/1845 train_time:2835ms step_avg:36.34ms
step:79/1845 train_time:2869ms step_avg:36.32ms
step:80/1845 train_time:2903ms step_avg:36.29ms
step:81/1845 train_time:2938ms step_avg:36.27ms
step:82/1845 train_time:2972ms step_avg:36.24ms
step:83/1845 train_time:3006ms step_avg:36.22ms
step:84/1845 train_time:3040ms step_avg:36.19ms
step:85/1845 train_time:3075ms step_avg:36.17ms
step:86/1845 train_time:3109ms step_avg:36.15ms
step:87/1845 train_time:3143ms step_avg:36.13ms
step:88/1845 train_time:3177ms step_avg:36.11ms
step:89/1845 train_time:3212ms step_avg:36.09ms
step:90/1845 train_time:3246ms step_avg:36.07ms
step:91/1845 train_time:3281ms step_avg:36.05ms
step:92/1845 train_time:3315ms step_avg:36.03ms
step:93/1845 train_time:3349ms step_avg:36.01ms
step:94/1845 train_time:3383ms step_avg:35.99ms
step:95/1845 train_time:3418ms step_avg:35.97ms
step:96/1845 train_time:3452ms step_avg:35.95ms
step:97/1845 train_time:3486ms step_avg:35.94ms
step:98/1845 train_time:3520ms step_avg:35.92ms
step:99/1845 train_time:3555ms step_avg:35.91ms
step:100/1845 train_time:3589ms step_avg:35.89ms
step:101/1845 train_time:3623ms step_avg:35.87ms
step:102/1845 train_time:3657ms step_avg:35.86ms
step:103/1845 train_time:3692ms step_avg:35.84ms
step:104/1845 train_time:3726ms step_avg:35.83ms
step:105/1845 train_time:3761ms step_avg:35.82ms
step:106/1845 train_time:3795ms step_avg:35.80ms
step:107/1845 train_time:3829ms step_avg:35.79ms
step:108/1845 train_time:3864ms step_avg:35.77ms
step:109/1845 train_time:3898ms step_avg:35.77ms
step:110/1845 train_time:3932ms step_avg:35.75ms
step:111/1845 train_time:3967ms step_avg:35.74ms
step:112/1845 train_time:4001ms step_avg:35.72ms
step:113/1845 train_time:4036ms step_avg:35.72ms
step:114/1845 train_time:4070ms step_avg:35.70ms
step:115/1845 train_time:4104ms step_avg:35.69ms
step:116/1845 train_time:4138ms step_avg:35.67ms
step:117/1845 train_time:4173ms step_avg:35.66ms
step:118/1845 train_time:4207ms step_avg:35.65ms
step:119/1845 train_time:4241ms step_avg:35.64ms
step:120/1845 train_time:4275ms step_avg:35.63ms
step:121/1845 train_time:4310ms step_avg:35.62ms
step:122/1845 train_time:4344ms step_avg:35.61ms
step:123/1845 train_time:4379ms step_avg:35.60ms
step:124/1845 train_time:4413ms step_avg:35.59ms
step:125/1845 train_time:4447ms step_avg:35.58ms
step:126/1845 train_time:4481ms step_avg:35.57ms
step:127/1845 train_time:4516ms step_avg:35.56ms
step:128/1845 train_time:4550ms step_avg:35.55ms
step:129/1845 train_time:4585ms step_avg:35.54ms
step:130/1845 train_time:4619ms step_avg:35.53ms
step:131/1845 train_time:4653ms step_avg:35.52ms
step:132/1845 train_time:4687ms step_avg:35.51ms
step:133/1845 train_time:4722ms step_avg:35.50ms
step:134/1845 train_time:4756ms step_avg:35.49ms
step:135/1845 train_time:4790ms step_avg:35.48ms
step:136/1845 train_time:4824ms step_avg:35.47ms
step:137/1845 train_time:4860ms step_avg:35.47ms
step:138/1845 train_time:4893ms step_avg:35.46ms
step:139/1845 train_time:4928ms step_avg:35.46ms
step:140/1845 train_time:4962ms step_avg:35.44ms
step:141/1845 train_time:4997ms step_avg:35.44ms
step:142/1845 train_time:5031ms step_avg:35.43ms
step:143/1845 train_time:5066ms step_avg:35.43ms
step:144/1845 train_time:5100ms step_avg:35.42ms
step:145/1845 train_time:5135ms step_avg:35.41ms
step:146/1845 train_time:5169ms step_avg:35.40ms
step:147/1845 train_time:5203ms step_avg:35.40ms
step:148/1845 train_time:5237ms step_avg:35.39ms
step:149/1845 train_time:5272ms step_avg:35.38ms
step:150/1845 train_time:5306ms step_avg:35.37ms
step:151/1845 train_time:5340ms step_avg:35.37ms
step:152/1845 train_time:5374ms step_avg:35.36ms
step:153/1845 train_time:5409ms step_avg:35.35ms
step:154/1845 train_time:5443ms step_avg:35.34ms
step:155/1845 train_time:5477ms step_avg:35.34ms
step:156/1845 train_time:5511ms step_avg:35.33ms
step:157/1845 train_time:5546ms step_avg:35.32ms
step:158/1845 train_time:5580ms step_avg:35.31ms
step:159/1845 train_time:5614ms step_avg:35.31ms
step:160/1845 train_time:5648ms step_avg:35.30ms
step:161/1845 train_time:5683ms step_avg:35.30ms
step:162/1845 train_time:5717ms step_avg:35.29ms
step:163/1845 train_time:5751ms step_avg:35.28ms
step:164/1845 train_time:5785ms step_avg:35.27ms
step:165/1845 train_time:5820ms step_avg:35.27ms
step:166/1845 train_time:5853ms step_avg:35.26ms
step:167/1845 train_time:5888ms step_avg:35.26ms
step:168/1845 train_time:5922ms step_avg:35.25ms
step:169/1845 train_time:5957ms step_avg:35.25ms
step:170/1845 train_time:5991ms step_avg:35.24ms
step:171/1845 train_time:6026ms step_avg:35.24ms
step:172/1845 train_time:6060ms step_avg:35.23ms
step:173/1845 train_time:6094ms step_avg:35.23ms
step:174/1845 train_time:6128ms step_avg:35.22ms
step:175/1845 train_time:6163ms step_avg:35.22ms
step:176/1845 train_time:6197ms step_avg:35.21ms
step:177/1845 train_time:6231ms step_avg:35.20ms
step:178/1845 train_time:6265ms step_avg:35.20ms
step:179/1845 train_time:6300ms step_avg:35.19ms
step:180/1845 train_time:6334ms step_avg:35.19ms
step:181/1845 train_time:6368ms step_avg:35.18ms
step:182/1845 train_time:6402ms step_avg:35.18ms
step:183/1845 train_time:6437ms step_avg:35.17ms
step:184/1845 train_time:6471ms step_avg:35.17ms
step:185/1845 train_time:6506ms step_avg:35.16ms
step:186/1845 train_time:6540ms step_avg:35.16ms
step:187/1845 train_time:6574ms step_avg:35.16ms
step:188/1845 train_time:6608ms step_avg:35.15ms
step:189/1845 train_time:6643ms step_avg:35.15ms
step:190/1845 train_time:6677ms step_avg:35.14ms
step:191/1845 train_time:6711ms step_avg:35.14ms
step:192/1845 train_time:6745ms step_avg:35.13ms
step:193/1845 train_time:6780ms step_avg:35.13ms
step:194/1845 train_time:6814ms step_avg:35.12ms
step:195/1845 train_time:6848ms step_avg:35.12ms
step:196/1845 train_time:6882ms step_avg:35.11ms
step:197/1845 train_time:6917ms step_avg:35.11ms
step:198/1845 train_time:6951ms step_avg:35.11ms
step:199/1845 train_time:6985ms step_avg:35.10ms
step:200/1845 train_time:7019ms step_avg:35.10ms
step:201/1845 train_time:7054ms step_avg:35.09ms
step:202/1845 train_time:7088ms step_avg:35.09ms
step:203/1845 train_time:7122ms step_avg:35.09ms
step:204/1845 train_time:7156ms step_avg:35.08ms
step:205/1845 train_time:7191ms step_avg:35.08ms
step:206/1845 train_time:7225ms step_avg:35.07ms
step:207/1845 train_time:7259ms step_avg:35.07ms
step:208/1845 train_time:7293ms step_avg:35.06ms
step:209/1845 train_time:7328ms step_avg:35.06ms
step:210/1845 train_time:7362ms step_avg:35.06ms
step:211/1845 train_time:7397ms step_avg:35.06ms
step:212/1845 train_time:7431ms step_avg:35.05ms
step:213/1845 train_time:7465ms step_avg:35.05ms
step:214/1845 train_time:7499ms step_avg:35.04ms
step:215/1845 train_time:7533ms step_avg:35.04ms
step:216/1845 train_time:7567ms step_avg:35.03ms
step:217/1845 train_time:7602ms step_avg:35.03ms
step:218/1845 train_time:7636ms step_avg:35.03ms
step:219/1845 train_time:7670ms step_avg:35.02ms
step:220/1845 train_time:7704ms step_avg:35.02ms
step:221/1845 train_time:7739ms step_avg:35.02ms
step:222/1845 train_time:7773ms step_avg:35.01ms
step:223/1845 train_time:7807ms step_avg:35.01ms
step:224/1845 train_time:7841ms step_avg:35.00ms
step:225/1845 train_time:7875ms step_avg:35.00ms
step:226/1845 train_time:7909ms step_avg:35.00ms
step:227/1845 train_time:7944ms step_avg:34.99ms
step:228/1845 train_time:7978ms step_avg:34.99ms
step:229/1845 train_time:8012ms step_avg:34.99ms
step:230/1845 train_time:8046ms step_avg:34.98ms
step:231/1845 train_time:8080ms step_avg:34.98ms
step:232/1845 train_time:8114ms step_avg:34.98ms
step:233/1845 train_time:8149ms step_avg:34.97ms
step:234/1845 train_time:8183ms step_avg:34.97ms
step:235/1845 train_time:8217ms step_avg:34.97ms
step:236/1845 train_time:8251ms step_avg:34.96ms
step:237/1845 train_time:8285ms step_avg:34.96ms
step:238/1845 train_time:8319ms step_avg:34.95ms
step:239/1845 train_time:8354ms step_avg:34.95ms
step:240/1845 train_time:8388ms step_avg:34.95ms
step:241/1845 train_time:8422ms step_avg:34.95ms
step:242/1845 train_time:8456ms step_avg:34.94ms
step:243/1845 train_time:8491ms step_avg:34.94ms
step:244/1845 train_time:8525ms step_avg:34.94ms
step:245/1845 train_time:8560ms step_avg:34.94ms
step:246/1845 train_time:8594ms step_avg:34.93ms
step:247/1845 train_time:8628ms step_avg:34.93ms
step:248/1845 train_time:8662ms step_avg:34.93ms
step:249/1845 train_time:8697ms step_avg:34.93ms
step:250/1845 train_time:8731ms step_avg:34.92ms
step:250/1845 val_loss:4.6199 train_time:8767ms step_avg:35.07ms
step:251/1845 train_time:8786ms step_avg:35.01ms
step:252/1845 train_time:8807ms step_avg:34.95ms
step:253/1845 train_time:8837ms step_avg:34.93ms
step:254/1845 train_time:8872ms step_avg:34.93ms
step:255/1845 train_time:8907ms step_avg:34.93ms
step:256/1845 train_time:8942ms step_avg:34.93ms
step:257/1845 train_time:8977ms step_avg:34.93ms
step:258/1845 train_time:9011ms step_avg:34.93ms
step:259/1845 train_time:9046ms step_avg:34.92ms
step:260/1845 train_time:9080ms step_avg:34.92ms
step:261/1845 train_time:9114ms step_avg:34.92ms
step:262/1845 train_time:9148ms step_avg:34.92ms
step:263/1845 train_time:9182ms step_avg:34.91ms
step:264/1845 train_time:9216ms step_avg:34.91ms
step:265/1845 train_time:9250ms step_avg:34.91ms
step:266/1845 train_time:9284ms step_avg:34.90ms
step:267/1845 train_time:9318ms step_avg:34.90ms
step:268/1845 train_time:9352ms step_avg:34.90ms
step:269/1845 train_time:9386ms step_avg:34.89ms
step:270/1845 train_time:9420ms step_avg:34.89ms
step:271/1845 train_time:9455ms step_avg:34.89ms
step:272/1845 train_time:9488ms step_avg:34.88ms
step:273/1845 train_time:9523ms step_avg:34.88ms
step:274/1845 train_time:9557ms step_avg:34.88ms
step:275/1845 train_time:9591ms step_avg:34.88ms
step:276/1845 train_time:9625ms step_avg:34.87ms
step:277/1845 train_time:9659ms step_avg:34.87ms
step:278/1845 train_time:9693ms step_avg:34.87ms
step:279/1845 train_time:9727ms step_avg:34.86ms
step:280/1845 train_time:9761ms step_avg:34.86ms
step:281/1845 train_time:9796ms step_avg:34.86ms
step:282/1845 train_time:9830ms step_avg:34.86ms
step:283/1845 train_time:9864ms step_avg:34.86ms
step:284/1845 train_time:9898ms step_avg:34.85ms
step:285/1845 train_time:9933ms step_avg:34.85ms
step:286/1845 train_time:9967ms step_avg:34.85ms
step:287/1845 train_time:10001ms step_avg:34.85ms
step:288/1845 train_time:10035ms step_avg:34.84ms
step:289/1845 train_time:10069ms step_avg:34.84ms
step:290/1845 train_time:10103ms step_avg:34.84ms
step:291/1845 train_time:10138ms step_avg:34.84ms
step:292/1845 train_time:10172ms step_avg:34.83ms
step:293/1845 train_time:10206ms step_avg:34.83ms
step:294/1845 train_time:10240ms step_avg:34.83ms
step:295/1845 train_time:10274ms step_avg:34.83ms
step:296/1845 train_time:10308ms step_avg:34.83ms
step:297/1845 train_time:10343ms step_avg:34.82ms
step:298/1845 train_time:10377ms step_avg:34.82ms
step:299/1845 train_time:10411ms step_avg:34.82ms
step:300/1845 train_time:10445ms step_avg:34.82ms
step:301/1845 train_time:10479ms step_avg:34.82ms
step:302/1845 train_time:10514ms step_avg:34.81ms
step:303/1845 train_time:10547ms step_avg:34.81ms
step:304/1845 train_time:10581ms step_avg:34.81ms
step:305/1845 train_time:10616ms step_avg:34.81ms
step:306/1845 train_time:10650ms step_avg:34.80ms
step:307/1845 train_time:10684ms step_avg:34.80ms
step:308/1845 train_time:10718ms step_avg:34.80ms
step:309/1845 train_time:10752ms step_avg:34.80ms
step:310/1845 train_time:10786ms step_avg:34.79ms
step:311/1845 train_time:10821ms step_avg:34.79ms
step:312/1845 train_time:10855ms step_avg:34.79ms
step:313/1845 train_time:10889ms step_avg:34.79ms
step:314/1845 train_time:10923ms step_avg:34.79ms
step:315/1845 train_time:10957ms step_avg:34.79ms
step:316/1845 train_time:10991ms step_avg:34.78ms
step:317/1845 train_time:11025ms step_avg:34.78ms
step:318/1845 train_time:11060ms step_avg:34.78ms
step:319/1845 train_time:11094ms step_avg:34.78ms
step:320/1845 train_time:11128ms step_avg:34.78ms
step:321/1845 train_time:11163ms step_avg:34.77ms
step:322/1845 train_time:11196ms step_avg:34.77ms
step:323/1845 train_time:11231ms step_avg:34.77ms
step:324/1845 train_time:11265ms step_avg:34.77ms
step:325/1845 train_time:11299ms step_avg:34.77ms
step:326/1845 train_time:11333ms step_avg:34.76ms
step:327/1845 train_time:11367ms step_avg:34.76ms
step:328/1845 train_time:11401ms step_avg:34.76ms
step:329/1845 train_time:11435ms step_avg:34.76ms
step:330/1845 train_time:11470ms step_avg:34.76ms
step:331/1845 train_time:11504ms step_avg:34.76ms
step:332/1845 train_time:11538ms step_avg:34.75ms
step:333/1845 train_time:11573ms step_avg:34.75ms
step:334/1845 train_time:11607ms step_avg:34.75ms
step:335/1845 train_time:11641ms step_avg:34.75ms
step:336/1845 train_time:11675ms step_avg:34.75ms
step:337/1845 train_time:11709ms step_avg:34.75ms
step:338/1845 train_time:11743ms step_avg:34.74ms
step:339/1845 train_time:11778ms step_avg:34.74ms
step:340/1845 train_time:11812ms step_avg:34.74ms
step:341/1845 train_time:11846ms step_avg:34.74ms
step:342/1845 train_time:11880ms step_avg:34.74ms
step:343/1845 train_time:11914ms step_avg:34.73ms
step:344/1845 train_time:11948ms step_avg:34.73ms
step:345/1845 train_time:11982ms step_avg:34.73ms
step:346/1845 train_time:12016ms step_avg:34.73ms
step:347/1845 train_time:12050ms step_avg:34.73ms
step:348/1845 train_time:12084ms step_avg:34.73ms
step:349/1845 train_time:12119ms step_avg:34.72ms
step:350/1845 train_time:12153ms step_avg:34.72ms
step:351/1845 train_time:12187ms step_avg:34.72ms
step:352/1845 train_time:12221ms step_avg:34.72ms
step:353/1845 train_time:12255ms step_avg:34.72ms
step:354/1845 train_time:12289ms step_avg:34.72ms
step:355/1845 train_time:12324ms step_avg:34.71ms
step:356/1845 train_time:12358ms step_avg:34.71ms
step:357/1845 train_time:12392ms step_avg:34.71ms
step:358/1845 train_time:12426ms step_avg:34.71ms
step:359/1845 train_time:12460ms step_avg:34.71ms
step:360/1845 train_time:12494ms step_avg:34.71ms
step:361/1845 train_time:12528ms step_avg:34.70ms
step:362/1845 train_time:12564ms step_avg:34.71ms
step:363/1845 train_time:12597ms step_avg:34.70ms
step:364/1845 train_time:12631ms step_avg:34.70ms
step:365/1845 train_time:12665ms step_avg:34.70ms
step:366/1845 train_time:12699ms step_avg:34.70ms
step:367/1845 train_time:12733ms step_avg:34.70ms
step:368/1845 train_time:12767ms step_avg:34.69ms
step:369/1845 train_time:12801ms step_avg:34.69ms
step:370/1845 train_time:12835ms step_avg:34.69ms
step:371/1845 train_time:12869ms step_avg:34.69ms
step:372/1845 train_time:12903ms step_avg:34.69ms
step:373/1845 train_time:12938ms step_avg:34.69ms
step:374/1845 train_time:12972ms step_avg:34.68ms
step:375/1845 train_time:13006ms step_avg:34.68ms
step:376/1845 train_time:13040ms step_avg:34.68ms
step:377/1845 train_time:13074ms step_avg:34.68ms
step:378/1845 train_time:13108ms step_avg:34.68ms
step:379/1845 train_time:13142ms step_avg:34.68ms
step:380/1845 train_time:13176ms step_avg:34.67ms
step:381/1845 train_time:13211ms step_avg:34.67ms
step:382/1845 train_time:13245ms step_avg:34.67ms
step:383/1845 train_time:13279ms step_avg:34.67ms
step:384/1845 train_time:13313ms step_avg:34.67ms
step:385/1845 train_time:13347ms step_avg:34.67ms
step:386/1845 train_time:13381ms step_avg:34.67ms
step:387/1845 train_time:13416ms step_avg:34.67ms
step:388/1845 train_time:13450ms step_avg:34.66ms
step:389/1845 train_time:13484ms step_avg:34.66ms
step:390/1845 train_time:13518ms step_avg:34.66ms
step:391/1845 train_time:13552ms step_avg:34.66ms
step:392/1845 train_time:13586ms step_avg:34.66ms
step:393/1845 train_time:13620ms step_avg:34.66ms
step:394/1845 train_time:13654ms step_avg:34.66ms
step:395/1845 train_time:13689ms step_avg:34.66ms
step:396/1845 train_time:13723ms step_avg:34.65ms
step:397/1845 train_time:13757ms step_avg:34.65ms
step:398/1845 train_time:13791ms step_avg:34.65ms
step:399/1845 train_time:13825ms step_avg:34.65ms
step:400/1845 train_time:13859ms step_avg:34.65ms
step:401/1845 train_time:13893ms step_avg:34.65ms
step:402/1845 train_time:13927ms step_avg:34.65ms
step:403/1845 train_time:13961ms step_avg:34.64ms
step:404/1845 train_time:13995ms step_avg:34.64ms
step:405/1845 train_time:14030ms step_avg:34.64ms
step:406/1845 train_time:14064ms step_avg:34.64ms
step:407/1845 train_time:14098ms step_avg:34.64ms
step:408/1845 train_time:14132ms step_avg:34.64ms
step:409/1845 train_time:14166ms step_avg:34.64ms
step:410/1845 train_time:14200ms step_avg:34.63ms
step:411/1845 train_time:14234ms step_avg:34.63ms
step:412/1845 train_time:14268ms step_avg:34.63ms
step:413/1845 train_time:14303ms step_avg:34.63ms
step:414/1845 train_time:14337ms step_avg:34.63ms
step:415/1845 train_time:14371ms step_avg:34.63ms
step:416/1845 train_time:14405ms step_avg:34.63ms
step:417/1845 train_time:14440ms step_avg:34.63ms
step:418/1845 train_time:14474ms step_avg:34.63ms
step:419/1845 train_time:14508ms step_avg:34.63ms
step:420/1845 train_time:14542ms step_avg:34.62ms
step:421/1845 train_time:14576ms step_avg:34.62ms
step:422/1845 train_time:14610ms step_avg:34.62ms
step:423/1845 train_time:14644ms step_avg:34.62ms
step:424/1845 train_time:14678ms step_avg:34.62ms
step:425/1845 train_time:14713ms step_avg:34.62ms
step:426/1845 train_time:14747ms step_avg:34.62ms
step:427/1845 train_time:14781ms step_avg:34.62ms
step:428/1845 train_time:14815ms step_avg:34.61ms
step:429/1845 train_time:14850ms step_avg:34.61ms
step:430/1845 train_time:14883ms step_avg:34.61ms
step:431/1845 train_time:14918ms step_avg:34.61ms
step:432/1845 train_time:14952ms step_avg:34.61ms
step:433/1845 train_time:14986ms step_avg:34.61ms
step:434/1845 train_time:15020ms step_avg:34.61ms
step:435/1845 train_time:15054ms step_avg:34.61ms
step:436/1845 train_time:15088ms step_avg:34.61ms
step:437/1845 train_time:15123ms step_avg:34.61ms
step:438/1845 train_time:15157ms step_avg:34.60ms
step:439/1845 train_time:15191ms step_avg:34.60ms
step:440/1845 train_time:15225ms step_avg:34.60ms
step:441/1845 train_time:15259ms step_avg:34.60ms
step:442/1845 train_time:15293ms step_avg:34.60ms
step:443/1845 train_time:15328ms step_avg:34.60ms
step:444/1845 train_time:15362ms step_avg:34.60ms
step:445/1845 train_time:15396ms step_avg:34.60ms
step:446/1845 train_time:15430ms step_avg:34.60ms
step:447/1845 train_time:15464ms step_avg:34.60ms
step:448/1845 train_time:15498ms step_avg:34.59ms
step:449/1845 train_time:15533ms step_avg:34.59ms
step:450/1845 train_time:15567ms step_avg:34.59ms
step:451/1845 train_time:15601ms step_avg:34.59ms
step:452/1845 train_time:15635ms step_avg:34.59ms
step:453/1845 train_time:15669ms step_avg:34.59ms
step:454/1845 train_time:15703ms step_avg:34.59ms
step:455/1845 train_time:15737ms step_avg:34.59ms
step:456/1845 train_time:15771ms step_avg:34.59ms
step:457/1845 train_time:15806ms step_avg:34.59ms
step:458/1845 train_time:15840ms step_avg:34.58ms
step:459/1845 train_time:15874ms step_avg:34.58ms
step:460/1845 train_time:15908ms step_avg:34.58ms
step:461/1845 train_time:15942ms step_avg:34.58ms
step:462/1845 train_time:15976ms step_avg:34.58ms
step:463/1845 train_time:16011ms step_avg:34.58ms
step:464/1845 train_time:16045ms step_avg:34.58ms
step:465/1845 train_time:16079ms step_avg:34.58ms
step:466/1845 train_time:16113ms step_avg:34.58ms
step:467/1845 train_time:16147ms step_avg:34.58ms
step:468/1845 train_time:16181ms step_avg:34.58ms
step:469/1845 train_time:16216ms step_avg:34.57ms
step:470/1845 train_time:16250ms step_avg:34.57ms
step:471/1845 train_time:16284ms step_avg:34.57ms
step:472/1845 train_time:16318ms step_avg:34.57ms
step:473/1845 train_time:16352ms step_avg:34.57ms
step:474/1845 train_time:16386ms step_avg:34.57ms
step:475/1845 train_time:16420ms step_avg:34.57ms
step:476/1845 train_time:16454ms step_avg:34.57ms
step:477/1845 train_time:16489ms step_avg:34.57ms
step:478/1845 train_time:16523ms step_avg:34.57ms
step:479/1845 train_time:16557ms step_avg:34.57ms
step:480/1845 train_time:16591ms step_avg:34.56ms
step:481/1845 train_time:16625ms step_avg:34.56ms
step:482/1845 train_time:16659ms step_avg:34.56ms
step:483/1845 train_time:16693ms step_avg:34.56ms
step:484/1845 train_time:16727ms step_avg:34.56ms
step:485/1845 train_time:16761ms step_avg:34.56ms
step:486/1845 train_time:16795ms step_avg:34.56ms
step:487/1845 train_time:16829ms step_avg:34.56ms
step:488/1845 train_time:16863ms step_avg:34.56ms
step:489/1845 train_time:16898ms step_avg:34.56ms
step:490/1845 train_time:16931ms step_avg:34.55ms
step:491/1845 train_time:16966ms step_avg:34.55ms
step:492/1845 train_time:17000ms step_avg:34.55ms
step:493/1845 train_time:17034ms step_avg:34.55ms
step:494/1845 train_time:17068ms step_avg:34.55ms
step:495/1845 train_time:17102ms step_avg:34.55ms
step:496/1845 train_time:17136ms step_avg:34.55ms
step:497/1845 train_time:17171ms step_avg:34.55ms
step:498/1845 train_time:17205ms step_avg:34.55ms
step:499/1845 train_time:17239ms step_avg:34.55ms
step:500/1845 train_time:17273ms step_avg:34.55ms
step:500/1845 val_loss:4.3018 train_time:17310ms step_avg:34.62ms
step:501/1845 train_time:17329ms step_avg:34.59ms
step:502/1845 train_time:17349ms step_avg:34.56ms
step:503/1845 train_time:17380ms step_avg:34.55ms
step:504/1845 train_time:17416ms step_avg:34.56ms
step:505/1845 train_time:17452ms step_avg:34.56ms
step:506/1845 train_time:17486ms step_avg:34.56ms
step:507/1845 train_time:17521ms step_avg:34.56ms
step:508/1845 train_time:17555ms step_avg:34.56ms
step:509/1845 train_time:17589ms step_avg:34.56ms
step:510/1845 train_time:17623ms step_avg:34.56ms
step:511/1845 train_time:17658ms step_avg:34.55ms
step:512/1845 train_time:17691ms step_avg:34.55ms
step:513/1845 train_time:17726ms step_avg:34.55ms
step:514/1845 train_time:17759ms step_avg:34.55ms
step:515/1845 train_time:17794ms step_avg:34.55ms
step:516/1845 train_time:17828ms step_avg:34.55ms
step:517/1845 train_time:17862ms step_avg:34.55ms
step:518/1845 train_time:17896ms step_avg:34.55ms
step:519/1845 train_time:17930ms step_avg:34.55ms
step:520/1845 train_time:17964ms step_avg:34.55ms
step:521/1845 train_time:17998ms step_avg:34.55ms
step:522/1845 train_time:18032ms step_avg:34.54ms
step:523/1845 train_time:18066ms step_avg:34.54ms
step:524/1845 train_time:18100ms step_avg:34.54ms
step:525/1845 train_time:18135ms step_avg:34.54ms
step:526/1845 train_time:18169ms step_avg:34.54ms
step:527/1845 train_time:18203ms step_avg:34.54ms
step:528/1845 train_time:18237ms step_avg:34.54ms
step:529/1845 train_time:18271ms step_avg:34.54ms
step:530/1845 train_time:18305ms step_avg:34.54ms
step:531/1845 train_time:18339ms step_avg:34.54ms
step:532/1845 train_time:18373ms step_avg:34.54ms
step:533/1845 train_time:18408ms step_avg:34.54ms
step:534/1845 train_time:18442ms step_avg:34.54ms
step:535/1845 train_time:18476ms step_avg:34.54ms
step:536/1845 train_time:18510ms step_avg:34.53ms
step:537/1845 train_time:18545ms step_avg:34.53ms
step:538/1845 train_time:18579ms step_avg:34.53ms
step:539/1845 train_time:18613ms step_avg:34.53ms
step:540/1845 train_time:18647ms step_avg:34.53ms
step:541/1845 train_time:18682ms step_avg:34.53ms
step:542/1845 train_time:18715ms step_avg:34.53ms
step:543/1845 train_time:18750ms step_avg:34.53ms
step:544/1845 train_time:18784ms step_avg:34.53ms
step:545/1845 train_time:18818ms step_avg:34.53ms
step:546/1845 train_time:18852ms step_avg:34.53ms
step:547/1845 train_time:18887ms step_avg:34.53ms
step:548/1845 train_time:18921ms step_avg:34.53ms
step:549/1845 train_time:18955ms step_avg:34.53ms
step:550/1845 train_time:18989ms step_avg:34.53ms
step:551/1845 train_time:19023ms step_avg:34.53ms
step:552/1845 train_time:19057ms step_avg:34.52ms
step:553/1845 train_time:19092ms step_avg:34.52ms
step:554/1845 train_time:19126ms step_avg:34.52ms
step:555/1845 train_time:19160ms step_avg:34.52ms
step:556/1845 train_time:19194ms step_avg:34.52ms
step:557/1845 train_time:19228ms step_avg:34.52ms
step:558/1845 train_time:19262ms step_avg:34.52ms
step:559/1845 train_time:19296ms step_avg:34.52ms
step:560/1845 train_time:19330ms step_avg:34.52ms
step:561/1845 train_time:19364ms step_avg:34.52ms
step:562/1845 train_time:19398ms step_avg:34.52ms
step:563/1845 train_time:19433ms step_avg:34.52ms
step:564/1845 train_time:19467ms step_avg:34.52ms
step:565/1845 train_time:19501ms step_avg:34.52ms
step:566/1845 train_time:19535ms step_avg:34.51ms
step:567/1845 train_time:19570ms step_avg:34.51ms
step:568/1845 train_time:19603ms step_avg:34.51ms
step:569/1845 train_time:19638ms step_avg:34.51ms
step:570/1845 train_time:19672ms step_avg:34.51ms
step:571/1845 train_time:19706ms step_avg:34.51ms
step:572/1845 train_time:19740ms step_avg:34.51ms
step:573/1845 train_time:19774ms step_avg:34.51ms
step:574/1845 train_time:19808ms step_avg:34.51ms
step:575/1845 train_time:19842ms step_avg:34.51ms
step:576/1845 train_time:19876ms step_avg:34.51ms
step:577/1845 train_time:19910ms step_avg:34.51ms
step:578/1845 train_time:19944ms step_avg:34.51ms
step:579/1845 train_time:19979ms step_avg:34.51ms
step:580/1845 train_time:20013ms step_avg:34.51ms
step:581/1845 train_time:20047ms step_avg:34.50ms
step:582/1845 train_time:20081ms step_avg:34.50ms
step:583/1845 train_time:20116ms step_avg:34.50ms
step:584/1845 train_time:20150ms step_avg:34.50ms
step:585/1845 train_time:20184ms step_avg:34.50ms
step:586/1845 train_time:20218ms step_avg:34.50ms
step:587/1845 train_time:20252ms step_avg:34.50ms
step:588/1845 train_time:20286ms step_avg:34.50ms
step:589/1845 train_time:20321ms step_avg:34.50ms
step:590/1845 train_time:20355ms step_avg:34.50ms
step:591/1845 train_time:20389ms step_avg:34.50ms
step:592/1845 train_time:20423ms step_avg:34.50ms
step:593/1845 train_time:20457ms step_avg:34.50ms
step:594/1845 train_time:20491ms step_avg:34.50ms
step:595/1845 train_time:20525ms step_avg:34.50ms
step:596/1845 train_time:20559ms step_avg:34.49ms
step:597/1845 train_time:20593ms step_avg:34.49ms
step:598/1845 train_time:20627ms step_avg:34.49ms
step:599/1845 train_time:20662ms step_avg:34.49ms
step:600/1845 train_time:20696ms step_avg:34.49ms
step:601/1845 train_time:20730ms step_avg:34.49ms
step:602/1845 train_time:20764ms step_avg:34.49ms
step:603/1845 train_time:20799ms step_avg:34.49ms
step:604/1845 train_time:20859ms step_avg:34.54ms
step:605/1845 train_time:20922ms step_avg:34.58ms
step:606/1845 train_time:20983ms step_avg:34.63ms
step:607/1845 train_time:21046ms step_avg:34.67ms
step:608/1845 train_time:21107ms step_avg:34.72ms
step:609/1845 train_time:21169ms step_avg:34.76ms
step:610/1845 train_time:21231ms step_avg:34.80ms
step:611/1845 train_time:21293ms step_avg:34.85ms
step:612/1845 train_time:21354ms step_avg:34.89ms
step:613/1845 train_time:21417ms step_avg:34.94ms
step:614/1845 train_time:21478ms step_avg:34.98ms
step:615/1845 train_time:21540ms step_avg:35.02ms
step:616/1845 train_time:21601ms step_avg:35.07ms
step:617/1845 train_time:21664ms step_avg:35.11ms
step:618/1845 train_time:21725ms step_avg:35.15ms
step:619/1845 train_time:21787ms step_avg:35.20ms
step:620/1845 train_time:21847ms step_avg:35.24ms
step:621/1845 train_time:21909ms step_avg:35.28ms
step:622/1845 train_time:21969ms step_avg:35.32ms
step:623/1845 train_time:22031ms step_avg:35.36ms
step:624/1845 train_time:22092ms step_avg:35.40ms
step:625/1845 train_time:22154ms step_avg:35.45ms
step:626/1845 train_time:22215ms step_avg:35.49ms
step:627/1845 train_time:22278ms step_avg:35.53ms
step:628/1845 train_time:22338ms step_avg:35.57ms
step:629/1845 train_time:22401ms step_avg:35.61ms
step:630/1845 train_time:22462ms step_avg:35.65ms
step:631/1845 train_time:22525ms step_avg:35.70ms
step:632/1845 train_time:22586ms step_avg:35.74ms
step:633/1845 train_time:22648ms step_avg:35.78ms
step:634/1845 train_time:22708ms step_avg:35.82ms
step:635/1845 train_time:22771ms step_avg:35.86ms
step:636/1845 train_time:22831ms step_avg:35.90ms
step:637/1845 train_time:22894ms step_avg:35.94ms
step:638/1845 train_time:22956ms step_avg:35.98ms
step:639/1845 train_time:23018ms step_avg:36.02ms
step:640/1845 train_time:23079ms step_avg:36.06ms
step:641/1845 train_time:23141ms step_avg:36.10ms
step:642/1845 train_time:23202ms step_avg:36.14ms
step:643/1845 train_time:23265ms step_avg:36.18ms
step:644/1845 train_time:23326ms step_avg:36.22ms
step:645/1845 train_time:23389ms step_avg:36.26ms
step:646/1845 train_time:23449ms step_avg:36.30ms
step:647/1845 train_time:23511ms step_avg:36.34ms
step:648/1845 train_time:23572ms step_avg:36.38ms
step:649/1845 train_time:23634ms step_avg:36.42ms
step:650/1845 train_time:23695ms step_avg:36.45ms
step:651/1845 train_time:23757ms step_avg:36.49ms
step:652/1845 train_time:23819ms step_avg:36.53ms
step:653/1845 train_time:23881ms step_avg:36.57ms
step:654/1845 train_time:23942ms step_avg:36.61ms
step:655/1845 train_time:24004ms step_avg:36.65ms
step:656/1845 train_time:24065ms step_avg:36.68ms
step:657/1845 train_time:24128ms step_avg:36.72ms
step:658/1845 train_time:24188ms step_avg:36.76ms
step:659/1845 train_time:24251ms step_avg:36.80ms
step:660/1845 train_time:24311ms step_avg:36.84ms
step:661/1845 train_time:24374ms step_avg:36.87ms
step:662/1845 train_time:24434ms step_avg:36.91ms
step:663/1845 train_time:24496ms step_avg:36.95ms
step:664/1845 train_time:24557ms step_avg:36.98ms
step:665/1845 train_time:24620ms step_avg:37.02ms
step:666/1845 train_time:24680ms step_avg:37.06ms
step:667/1845 train_time:24743ms step_avg:37.10ms
step:668/1845 train_time:24804ms step_avg:37.13ms
step:669/1845 train_time:24866ms step_avg:37.17ms
step:670/1845 train_time:24927ms step_avg:37.20ms
step:671/1845 train_time:24989ms step_avg:37.24ms
step:672/1845 train_time:25050ms step_avg:37.28ms
step:673/1845 train_time:25112ms step_avg:37.31ms
step:674/1845 train_time:25173ms step_avg:37.35ms
step:675/1845 train_time:25235ms step_avg:37.39ms
step:676/1845 train_time:25296ms step_avg:37.42ms
step:677/1845 train_time:25359ms step_avg:37.46ms
step:678/1845 train_time:25420ms step_avg:37.49ms
step:679/1845 train_time:25482ms step_avg:37.53ms
step:680/1845 train_time:25543ms step_avg:37.56ms
step:681/1845 train_time:25605ms step_avg:37.60ms
step:682/1845 train_time:25666ms step_avg:37.63ms
step:683/1845 train_time:25728ms step_avg:37.67ms
step:684/1845 train_time:25788ms step_avg:37.70ms
step:685/1845 train_time:25851ms step_avg:37.74ms
step:686/1845 train_time:25911ms step_avg:37.77ms
step:687/1845 train_time:25974ms step_avg:37.81ms
step:688/1845 train_time:26035ms step_avg:37.84ms
step:689/1845 train_time:26098ms step_avg:37.88ms
step:690/1845 train_time:26158ms step_avg:37.91ms
step:691/1845 train_time:26220ms step_avg:37.95ms
step:692/1845 train_time:26281ms step_avg:37.98ms
step:693/1845 train_time:26344ms step_avg:38.01ms
step:694/1845 train_time:26404ms step_avg:38.05ms
step:695/1845 train_time:26466ms step_avg:38.08ms
step:696/1845 train_time:26527ms step_avg:38.11ms
step:697/1845 train_time:26589ms step_avg:38.15ms
step:698/1845 train_time:26649ms step_avg:38.18ms
step:699/1845 train_time:26711ms step_avg:38.21ms
step:700/1845 train_time:26772ms step_avg:38.25ms
step:701/1845 train_time:26835ms step_avg:38.28ms
step:702/1845 train_time:26896ms step_avg:38.31ms
step:703/1845 train_time:26959ms step_avg:38.35ms
step:704/1845 train_time:27019ms step_avg:38.38ms
step:705/1845 train_time:27082ms step_avg:38.41ms
step:706/1845 train_time:27143ms step_avg:38.45ms
step:707/1845 train_time:27206ms step_avg:38.48ms
step:708/1845 train_time:27266ms step_avg:38.51ms
step:709/1845 train_time:27329ms step_avg:38.55ms
step:710/1845 train_time:27389ms step_avg:38.58ms
step:711/1845 train_time:27451ms step_avg:38.61ms
step:712/1845 train_time:27512ms step_avg:38.64ms
step:713/1845 train_time:27574ms step_avg:38.67ms
step:714/1845 train_time:27635ms step_avg:38.70ms
step:715/1845 train_time:27697ms step_avg:38.74ms
step:716/1845 train_time:27758ms step_avg:38.77ms
step:717/1845 train_time:27821ms step_avg:38.80ms
step:718/1845 train_time:27882ms step_avg:38.83ms
step:719/1845 train_time:27945ms step_avg:38.87ms
step:720/1845 train_time:28006ms step_avg:38.90ms
step:721/1845 train_time:28068ms step_avg:38.93ms
step:722/1845 train_time:28129ms step_avg:38.96ms
step:723/1845 train_time:28191ms step_avg:38.99ms
step:724/1845 train_time:28251ms step_avg:39.02ms
step:725/1845 train_time:28313ms step_avg:39.05ms
step:726/1845 train_time:28374ms step_avg:39.08ms
step:727/1845 train_time:28437ms step_avg:39.11ms
step:728/1845 train_time:28498ms step_avg:39.15ms
step:729/1845 train_time:28560ms step_avg:39.18ms
step:730/1845 train_time:28621ms step_avg:39.21ms
step:731/1845 train_time:28683ms step_avg:39.24ms
step:732/1845 train_time:28744ms step_avg:39.27ms
step:733/1845 train_time:28807ms step_avg:39.30ms
step:734/1845 train_time:28868ms step_avg:39.33ms
step:735/1845 train_time:28930ms step_avg:39.36ms
step:736/1845 train_time:28990ms step_avg:39.39ms
step:737/1845 train_time:29053ms step_avg:39.42ms
step:738/1845 train_time:29113ms step_avg:39.45ms
step:739/1845 train_time:29177ms step_avg:39.48ms
step:740/1845 train_time:29238ms step_avg:39.51ms
step:741/1845 train_time:29300ms step_avg:39.54ms
step:742/1845 train_time:29361ms step_avg:39.57ms
step:743/1845 train_time:29424ms step_avg:39.60ms
step:744/1845 train_time:29485ms step_avg:39.63ms
step:745/1845 train_time:29547ms step_avg:39.66ms
step:746/1845 train_time:29608ms step_avg:39.69ms
step:747/1845 train_time:29670ms step_avg:39.72ms
step:748/1845 train_time:29731ms step_avg:39.75ms
step:749/1845 train_time:29793ms step_avg:39.78ms
step:750/1845 train_time:29854ms step_avg:39.81ms
step:750/1845 val_loss:4.0156 train_time:29917ms step_avg:39.89ms
step:751/1845 train_time:29937ms step_avg:39.86ms
step:752/1845 train_time:29980ms step_avg:39.87ms
step:753/1845 train_time:30045ms step_avg:39.90ms
step:754/1845 train_time:30108ms step_avg:39.93ms
step:755/1845 train_time:30170ms step_avg:39.96ms
step:756/1845 train_time:30231ms step_avg:39.99ms
step:757/1845 train_time:30293ms step_avg:40.02ms
step:758/1845 train_time:30353ms step_avg:40.04ms
step:759/1845 train_time:30415ms step_avg:40.07ms
step:760/1845 train_time:30475ms step_avg:40.10ms
step:761/1845 train_time:30537ms step_avg:40.13ms
step:762/1845 train_time:30597ms step_avg:40.15ms
step:763/1845 train_time:30658ms step_avg:40.18ms
step:764/1845 train_time:30719ms step_avg:40.21ms
step:765/1845 train_time:30780ms step_avg:40.24ms
step:766/1845 train_time:30841ms step_avg:40.26ms
step:767/1845 train_time:30905ms step_avg:40.29ms
step:768/1845 train_time:30966ms step_avg:40.32ms
step:769/1845 train_time:31029ms step_avg:40.35ms
step:770/1845 train_time:31090ms step_avg:40.38ms
step:771/1845 train_time:31153ms step_avg:40.41ms
step:772/1845 train_time:31214ms step_avg:40.43ms
step:773/1845 train_time:31277ms step_avg:40.46ms
step:774/1845 train_time:31337ms step_avg:40.49ms
step:775/1845 train_time:31399ms step_avg:40.52ms
step:776/1845 train_time:31460ms step_avg:40.54ms
step:777/1845 train_time:31522ms step_avg:40.57ms
step:778/1845 train_time:31583ms step_avg:40.59ms
step:779/1845 train_time:31645ms step_avg:40.62ms
step:780/1845 train_time:31706ms step_avg:40.65ms
step:781/1845 train_time:31769ms step_avg:40.68ms
step:782/1845 train_time:31829ms step_avg:40.70ms
step:783/1845 train_time:31892ms step_avg:40.73ms
step:784/1845 train_time:31952ms step_avg:40.76ms
step:785/1845 train_time:32015ms step_avg:40.78ms
step:786/1845 train_time:32076ms step_avg:40.81ms
step:787/1845 train_time:32139ms step_avg:40.84ms
step:788/1845 train_time:32201ms step_avg:40.86ms
step:789/1845 train_time:32263ms step_avg:40.89ms
step:790/1845 train_time:32324ms step_avg:40.92ms
step:791/1845 train_time:32387ms step_avg:40.94ms
step:792/1845 train_time:32448ms step_avg:40.97ms
step:793/1845 train_time:32511ms step_avg:41.00ms
step:794/1845 train_time:32571ms step_avg:41.02ms
step:795/1845 train_time:32633ms step_avg:41.05ms
step:796/1845 train_time:32693ms step_avg:41.07ms
step:797/1845 train_time:32756ms step_avg:41.10ms
step:798/1845 train_time:32816ms step_avg:41.12ms
step:799/1845 train_time:32879ms step_avg:41.15ms
step:800/1845 train_time:32939ms step_avg:41.17ms
step:801/1845 train_time:33002ms step_avg:41.20ms
step:802/1845 train_time:33063ms step_avg:41.23ms
step:803/1845 train_time:33126ms step_avg:41.25ms
step:804/1845 train_time:33188ms step_avg:41.28ms
step:805/1845 train_time:33250ms step_avg:41.30ms
step:806/1845 train_time:33311ms step_avg:41.33ms
step:807/1845 train_time:33373ms step_avg:41.35ms
step:808/1845 train_time:33434ms step_avg:41.38ms
step:809/1845 train_time:33495ms step_avg:41.40ms
step:810/1845 train_time:33556ms step_avg:41.43ms
step:811/1845 train_time:33619ms step_avg:41.45ms
step:812/1845 train_time:33680ms step_avg:41.48ms
step:813/1845 train_time:33742ms step_avg:41.50ms
step:814/1845 train_time:33802ms step_avg:41.53ms
step:815/1845 train_time:33865ms step_avg:41.55ms
step:816/1845 train_time:33926ms step_avg:41.58ms
step:817/1845 train_time:33989ms step_avg:41.60ms
step:818/1845 train_time:34049ms step_avg:41.63ms
step:819/1845 train_time:34112ms step_avg:41.65ms
step:820/1845 train_time:34173ms step_avg:41.67ms
step:821/1845 train_time:34236ms step_avg:41.70ms
step:822/1845 train_time:34296ms step_avg:41.72ms
step:823/1845 train_time:34359ms step_avg:41.75ms
step:824/1845 train_time:34420ms step_avg:41.77ms
step:825/1845 train_time:34482ms step_avg:41.80ms
step:826/1845 train_time:34543ms step_avg:41.82ms
step:827/1845 train_time:34606ms step_avg:41.84ms
step:828/1845 train_time:34667ms step_avg:41.87ms
step:829/1845 train_time:34729ms step_avg:41.89ms
step:830/1845 train_time:34790ms step_avg:41.92ms
step:831/1845 train_time:34853ms step_avg:41.94ms
step:832/1845 train_time:34913ms step_avg:41.96ms
step:833/1845 train_time:34975ms step_avg:41.99ms
step:834/1845 train_time:35036ms step_avg:42.01ms
step:835/1845 train_time:35099ms step_avg:42.03ms
step:836/1845 train_time:35160ms step_avg:42.06ms
step:837/1845 train_time:35224ms step_avg:42.08ms
step:838/1845 train_time:35285ms step_avg:42.11ms
step:839/1845 train_time:35348ms step_avg:42.13ms
step:840/1845 train_time:35408ms step_avg:42.15ms
step:841/1845 train_time:35471ms step_avg:42.18ms
step:842/1845 train_time:35532ms step_avg:42.20ms
step:843/1845 train_time:35594ms step_avg:42.22ms
step:844/1845 train_time:35654ms step_avg:42.24ms
step:845/1845 train_time:35716ms step_avg:42.27ms
step:846/1845 train_time:35777ms step_avg:42.29ms
step:847/1845 train_time:35840ms step_avg:42.31ms
step:848/1845 train_time:35902ms step_avg:42.34ms
step:849/1845 train_time:35964ms step_avg:42.36ms
step:850/1845 train_time:36024ms step_avg:42.38ms
step:851/1845 train_time:36087ms step_avg:42.41ms
step:852/1845 train_time:36148ms step_avg:42.43ms
step:853/1845 train_time:36211ms step_avg:42.45ms
step:854/1845 train_time:36272ms step_avg:42.47ms
step:855/1845 train_time:36334ms step_avg:42.50ms
step:856/1845 train_time:36395ms step_avg:42.52ms
step:857/1845 train_time:36457ms step_avg:42.54ms
step:858/1845 train_time:36518ms step_avg:42.56ms
step:859/1845 train_time:36581ms step_avg:42.59ms
step:860/1845 train_time:36642ms step_avg:42.61ms
step:861/1845 train_time:36704ms step_avg:42.63ms
step:862/1845 train_time:36765ms step_avg:42.65ms
step:863/1845 train_time:36828ms step_avg:42.67ms
step:864/1845 train_time:36889ms step_avg:42.70ms
step:865/1845 train_time:36951ms step_avg:42.72ms
step:866/1845 train_time:37012ms step_avg:42.74ms
step:867/1845 train_time:37074ms step_avg:42.76ms
step:868/1845 train_time:37135ms step_avg:42.78ms
step:869/1845 train_time:37197ms step_avg:42.80ms
step:870/1845 train_time:37257ms step_avg:42.82ms
step:871/1845 train_time:37320ms step_avg:42.85ms
step:872/1845 train_time:37381ms step_avg:42.87ms
step:873/1845 train_time:37443ms step_avg:42.89ms
step:874/1845 train_time:37504ms step_avg:42.91ms
step:875/1845 train_time:37567ms step_avg:42.93ms
step:876/1845 train_time:37628ms step_avg:42.95ms
step:877/1845 train_time:37690ms step_avg:42.98ms
step:878/1845 train_time:37750ms step_avg:43.00ms
step:879/1845 train_time:37813ms step_avg:43.02ms
step:880/1845 train_time:37874ms step_avg:43.04ms
step:881/1845 train_time:37935ms step_avg:43.06ms
step:882/1845 train_time:37996ms step_avg:43.08ms
step:883/1845 train_time:38058ms step_avg:43.10ms
step:884/1845 train_time:38118ms step_avg:43.12ms
step:885/1845 train_time:38181ms step_avg:43.14ms
step:886/1845 train_time:38242ms step_avg:43.16ms
step:887/1845 train_time:38305ms step_avg:43.18ms
step:888/1845 train_time:38365ms step_avg:43.20ms
step:889/1845 train_time:38427ms step_avg:43.23ms
step:890/1845 train_time:38488ms step_avg:43.25ms
step:891/1845 train_time:38551ms step_avg:43.27ms
step:892/1845 train_time:38612ms step_avg:43.29ms
step:893/1845 train_time:38674ms step_avg:43.31ms
step:894/1845 train_time:38734ms step_avg:43.33ms
step:895/1845 train_time:38796ms step_avg:43.35ms
step:896/1845 train_time:38857ms step_avg:43.37ms
step:897/1845 train_time:38919ms step_avg:43.39ms
step:898/1845 train_time:38980ms step_avg:43.41ms
step:899/1845 train_time:39042ms step_avg:43.43ms
step:900/1845 train_time:39103ms step_avg:43.45ms
step:901/1845 train_time:39165ms step_avg:43.47ms
step:902/1845 train_time:39226ms step_avg:43.49ms
step:903/1845 train_time:39289ms step_avg:43.51ms
step:904/1845 train_time:39350ms step_avg:43.53ms
step:905/1845 train_time:39413ms step_avg:43.55ms
step:906/1845 train_time:39473ms step_avg:43.57ms
step:907/1845 train_time:39535ms step_avg:43.59ms
step:908/1845 train_time:39596ms step_avg:43.61ms
step:909/1845 train_time:39658ms step_avg:43.63ms
step:910/1845 train_time:39719ms step_avg:43.65ms
step:911/1845 train_time:39781ms step_avg:43.67ms
step:912/1845 train_time:39842ms step_avg:43.69ms
step:913/1845 train_time:39904ms step_avg:43.71ms
step:914/1845 train_time:39965ms step_avg:43.73ms
step:915/1845 train_time:40028ms step_avg:43.75ms
step:916/1845 train_time:40089ms step_avg:43.77ms
step:917/1845 train_time:40152ms step_avg:43.79ms
step:918/1845 train_time:40212ms step_avg:43.80ms
step:919/1845 train_time:40275ms step_avg:43.82ms
step:920/1845 train_time:40335ms step_avg:43.84ms
step:921/1845 train_time:40398ms step_avg:43.86ms
step:922/1845 train_time:40458ms step_avg:43.88ms
step:923/1845 train_time:40521ms step_avg:43.90ms
step:924/1845 train_time:40582ms step_avg:43.92ms
step:925/1845 train_time:40645ms step_avg:43.94ms
step:926/1845 train_time:40706ms step_avg:43.96ms
step:927/1845 train_time:40768ms step_avg:43.98ms
step:928/1845 train_time:40829ms step_avg:44.00ms
step:929/1845 train_time:40892ms step_avg:44.02ms
step:930/1845 train_time:40952ms step_avg:44.03ms
step:931/1845 train_time:41015ms step_avg:44.05ms
step:932/1845 train_time:41075ms step_avg:44.07ms
step:933/1845 train_time:41137ms step_avg:44.09ms
step:934/1845 train_time:41198ms step_avg:44.11ms
step:935/1845 train_time:41260ms step_avg:44.13ms
step:936/1845 train_time:41321ms step_avg:44.15ms
step:937/1845 train_time:41383ms step_avg:44.17ms
step:938/1845 train_time:41444ms step_avg:44.18ms
step:939/1845 train_time:41506ms step_avg:44.20ms
step:940/1845 train_time:41566ms step_avg:44.22ms
step:941/1845 train_time:41629ms step_avg:44.24ms
step:942/1845 train_time:41690ms step_avg:44.26ms
step:943/1845 train_time:41753ms step_avg:44.28ms
step:944/1845 train_time:41813ms step_avg:44.29ms
step:945/1845 train_time:41876ms step_avg:44.31ms
step:946/1845 train_time:41936ms step_avg:44.33ms
step:947/1845 train_time:41998ms step_avg:44.35ms
step:948/1845 train_time:42059ms step_avg:44.37ms
step:949/1845 train_time:42121ms step_avg:44.38ms
step:950/1845 train_time:42182ms step_avg:44.40ms
step:951/1845 train_time:42245ms step_avg:44.42ms
step:952/1845 train_time:42306ms step_avg:44.44ms
step:953/1845 train_time:42368ms step_avg:44.46ms
step:954/1845 train_time:42429ms step_avg:44.47ms
step:955/1845 train_time:42491ms step_avg:44.49ms
step:956/1845 train_time:42552ms step_avg:44.51ms
step:957/1845 train_time:42614ms step_avg:44.53ms
step:958/1845 train_time:42675ms step_avg:44.55ms
step:959/1845 train_time:42737ms step_avg:44.56ms
step:960/1845 train_time:42798ms step_avg:44.58ms
step:961/1845 train_time:42860ms step_avg:44.60ms
step:962/1845 train_time:42921ms step_avg:44.62ms
step:963/1845 train_time:42983ms step_avg:44.63ms
step:964/1845 train_time:43044ms step_avg:44.65ms
step:965/1845 train_time:43106ms step_avg:44.67ms
step:966/1845 train_time:43167ms step_avg:44.69ms
step:967/1845 train_time:43230ms step_avg:44.71ms
step:968/1845 train_time:43291ms step_avg:44.72ms
step:969/1845 train_time:43353ms step_avg:44.74ms
step:970/1845 train_time:43414ms step_avg:44.76ms
step:971/1845 train_time:43476ms step_avg:44.77ms
step:972/1845 train_time:43536ms step_avg:44.79ms
step:973/1845 train_time:43599ms step_avg:44.81ms
step:974/1845 train_time:43661ms step_avg:44.83ms
step:975/1845 train_time:43723ms step_avg:44.84ms
step:976/1845 train_time:43784ms step_avg:44.86ms
step:977/1845 train_time:43846ms step_avg:44.88ms
step:978/1845 train_time:43907ms step_avg:44.90ms
step:979/1845 train_time:43970ms step_avg:44.91ms
step:980/1845 train_time:44031ms step_avg:44.93ms
step:981/1845 train_time:44092ms step_avg:44.95ms
step:982/1845 train_time:44153ms step_avg:44.96ms
step:983/1845 train_time:44215ms step_avg:44.98ms
step:984/1845 train_time:44276ms step_avg:45.00ms
step:985/1845 train_time:44338ms step_avg:45.01ms
step:986/1845 train_time:44399ms step_avg:45.03ms
step:987/1845 train_time:44463ms step_avg:45.05ms
step:988/1845 train_time:44522ms step_avg:45.06ms
step:989/1845 train_time:44585ms step_avg:45.08ms
step:990/1845 train_time:44646ms step_avg:45.10ms
step:991/1845 train_time:44708ms step_avg:45.11ms
step:992/1845 train_time:44768ms step_avg:45.13ms
step:993/1845 train_time:44831ms step_avg:45.15ms
step:994/1845 train_time:44892ms step_avg:45.16ms
step:995/1845 train_time:44954ms step_avg:45.18ms
step:996/1845 train_time:45015ms step_avg:45.20ms
step:997/1845 train_time:45077ms step_avg:45.21ms
step:998/1845 train_time:45138ms step_avg:45.23ms
step:999/1845 train_time:45200ms step_avg:45.24ms
step:1000/1845 train_time:45261ms step_avg:45.26ms
step:1000/1845 val_loss:3.7869 train_time:45325ms step_avg:45.32ms
step:1001/1845 train_time:45344ms step_avg:45.30ms
step:1002/1845 train_time:45386ms step_avg:45.30ms
step:1003/1845 train_time:45451ms step_avg:45.32ms
step:1004/1845 train_time:45515ms step_avg:45.33ms
step:1005/1845 train_time:45577ms step_avg:45.35ms
step:1006/1845 train_time:45638ms step_avg:45.37ms
step:1007/1845 train_time:45700ms step_avg:45.38ms
step:1008/1845 train_time:45760ms step_avg:45.40ms
step:1009/1845 train_time:45822ms step_avg:45.41ms
step:1010/1845 train_time:45883ms step_avg:45.43ms
step:1011/1845 train_time:45945ms step_avg:45.44ms
step:1012/1845 train_time:46005ms step_avg:45.46ms
step:1013/1845 train_time:46067ms step_avg:45.48ms
step:1014/1845 train_time:46129ms step_avg:45.49ms
step:1015/1845 train_time:46191ms step_avg:45.51ms
step:1016/1845 train_time:46252ms step_avg:45.52ms
step:1017/1845 train_time:46316ms step_avg:45.54ms
step:1018/1845 train_time:46377ms step_avg:45.56ms
step:1019/1845 train_time:46440ms step_avg:45.57ms
step:1020/1845 train_time:46502ms step_avg:45.59ms
step:1021/1845 train_time:46564ms step_avg:45.61ms
step:1022/1845 train_time:46626ms step_avg:45.62ms
step:1023/1845 train_time:46688ms step_avg:45.64ms
step:1024/1845 train_time:46749ms step_avg:45.65ms
step:1025/1845 train_time:46812ms step_avg:45.67ms
step:1026/1845 train_time:46873ms step_avg:45.69ms
step:1027/1845 train_time:46936ms step_avg:45.70ms
step:1028/1845 train_time:46997ms step_avg:45.72ms
step:1029/1845 train_time:47058ms step_avg:45.73ms
step:1030/1845 train_time:47118ms step_avg:45.75ms
step:1031/1845 train_time:47180ms step_avg:45.76ms
step:1032/1845 train_time:47240ms step_avg:45.78ms
step:1033/1845 train_time:47304ms step_avg:45.79ms
step:1034/1845 train_time:47365ms step_avg:45.81ms
step:1035/1845 train_time:47428ms step_avg:45.82ms
step:1036/1845 train_time:47489ms step_avg:45.84ms
step:1037/1845 train_time:47552ms step_avg:45.86ms
step:1038/1845 train_time:47613ms step_avg:45.87ms
step:1039/1845 train_time:47676ms step_avg:45.89ms
step:1040/1845 train_time:47736ms step_avg:45.90ms
step:1041/1845 train_time:47798ms step_avg:45.92ms
step:1042/1845 train_time:47859ms step_avg:45.93ms
step:1043/1845 train_time:47921ms step_avg:45.95ms
step:1044/1845 train_time:47982ms step_avg:45.96ms
step:1045/1845 train_time:48044ms step_avg:45.97ms
step:1046/1845 train_time:48104ms step_avg:45.99ms
step:1047/1845 train_time:48166ms step_avg:46.00ms
step:1048/1845 train_time:48227ms step_avg:46.02ms
step:1049/1845 train_time:48290ms step_avg:46.03ms
step:1050/1845 train_time:48350ms step_avg:46.05ms
step:1051/1845 train_time:48413ms step_avg:46.06ms
step:1052/1845 train_time:48475ms step_avg:46.08ms
step:1053/1845 train_time:48537ms step_avg:46.09ms
step:1054/1845 train_time:48598ms step_avg:46.11ms
step:1055/1845 train_time:48661ms step_avg:46.12ms
step:1056/1845 train_time:48722ms step_avg:46.14ms
step:1057/1845 train_time:48784ms step_avg:46.15ms
step:1058/1845 train_time:48845ms step_avg:46.17ms
step:1059/1845 train_time:48908ms step_avg:46.18ms
step:1060/1845 train_time:48968ms step_avg:46.20ms
step:1061/1845 train_time:49031ms step_avg:46.21ms
step:1062/1845 train_time:49091ms step_avg:46.23ms
step:1063/1845 train_time:49153ms step_avg:46.24ms
step:1064/1845 train_time:49214ms step_avg:46.25ms
step:1065/1845 train_time:49276ms step_avg:46.27ms
step:1066/1845 train_time:49337ms step_avg:46.28ms
step:1067/1845 train_time:49399ms step_avg:46.30ms
step:1068/1845 train_time:49460ms step_avg:46.31ms
step:1069/1845 train_time:49522ms step_avg:46.33ms
step:1070/1845 train_time:49584ms step_avg:46.34ms
step:1071/1845 train_time:49646ms step_avg:46.36ms
step:1072/1845 train_time:49708ms step_avg:46.37ms
step:1073/1845 train_time:49770ms step_avg:46.38ms
step:1074/1845 train_time:49831ms step_avg:46.40ms
step:1075/1845 train_time:49894ms step_avg:46.41ms
step:1076/1845 train_time:49955ms step_avg:46.43ms
step:1077/1845 train_time:50017ms step_avg:46.44ms
step:1078/1845 train_time:50077ms step_avg:46.45ms
step:1079/1845 train_time:50140ms step_avg:46.47ms
step:1080/1845 train_time:50200ms step_avg:46.48ms
step:1081/1845 train_time:50262ms step_avg:46.50ms
step:1082/1845 train_time:50322ms step_avg:46.51ms
step:1083/1845 train_time:50385ms step_avg:46.52ms
step:1084/1845 train_time:50446ms step_avg:46.54ms
step:1085/1845 train_time:50509ms step_avg:46.55ms
step:1086/1845 train_time:50570ms step_avg:46.57ms
step:1087/1845 train_time:50632ms step_avg:46.58ms
step:1088/1845 train_time:50693ms step_avg:46.59ms
step:1089/1845 train_time:50756ms step_avg:46.61ms
step:1090/1845 train_time:50817ms step_avg:46.62ms
step:1091/1845 train_time:50880ms step_avg:46.64ms
step:1092/1845 train_time:50941ms step_avg:46.65ms
step:1093/1845 train_time:51003ms step_avg:46.66ms
step:1094/1845 train_time:51063ms step_avg:46.68ms
step:1095/1845 train_time:51126ms step_avg:46.69ms
step:1096/1845 train_time:51187ms step_avg:46.70ms
step:1097/1845 train_time:51249ms step_avg:46.72ms
step:1098/1845 train_time:51309ms step_avg:46.73ms
step:1099/1845 train_time:51372ms step_avg:46.74ms
step:1100/1845 train_time:51433ms step_avg:46.76ms
step:1101/1845 train_time:51496ms step_avg:46.77ms
step:1102/1845 train_time:51556ms step_avg:46.78ms
step:1103/1845 train_time:51619ms step_avg:46.80ms
step:1104/1845 train_time:51679ms step_avg:46.81ms
step:1105/1845 train_time:51741ms step_avg:46.82ms
step:1106/1845 train_time:51802ms step_avg:46.84ms
step:1107/1845 train_time:51864ms step_avg:46.85ms
step:1108/1845 train_time:51926ms step_avg:46.86ms
step:1109/1845 train_time:51988ms step_avg:46.88ms
step:1110/1845 train_time:52049ms step_avg:46.89ms
step:1111/1845 train_time:52112ms step_avg:46.91ms
step:1112/1845 train_time:52173ms step_avg:46.92ms
step:1113/1845 train_time:52235ms step_avg:46.93ms
step:1114/1845 train_time:52295ms step_avg:46.94ms
step:1115/1845 train_time:52358ms step_avg:46.96ms
step:1116/1845 train_time:52418ms step_avg:46.97ms
step:1117/1845 train_time:52480ms step_avg:46.98ms
step:1118/1845 train_time:52541ms step_avg:47.00ms
step:1119/1845 train_time:52603ms step_avg:47.01ms
step:1120/1845 train_time:52663ms step_avg:47.02ms
step:1121/1845 train_time:52726ms step_avg:47.03ms
step:1122/1845 train_time:52787ms step_avg:47.05ms
step:1123/1845 train_time:52849ms step_avg:47.06ms
step:1124/1845 train_time:52910ms step_avg:47.07ms
step:1125/1845 train_time:52972ms step_avg:47.09ms
step:1126/1845 train_time:53033ms step_avg:47.10ms
step:1127/1845 train_time:53095ms step_avg:47.11ms
step:1128/1845 train_time:53156ms step_avg:47.12ms
step:1129/1845 train_time:53218ms step_avg:47.14ms
step:1130/1845 train_time:53278ms step_avg:47.15ms
step:1131/1845 train_time:53340ms step_avg:47.16ms
step:1132/1845 train_time:53401ms step_avg:47.17ms
step:1133/1845 train_time:53463ms step_avg:47.19ms
step:1134/1845 train_time:53524ms step_avg:47.20ms
step:1135/1845 train_time:53586ms step_avg:47.21ms
step:1136/1845 train_time:53647ms step_avg:47.22ms
step:1137/1845 train_time:53710ms step_avg:47.24ms
step:1138/1845 train_time:53771ms step_avg:47.25ms
step:1139/1845 train_time:53833ms step_avg:47.26ms
step:1140/1845 train_time:53894ms step_avg:47.28ms
step:1141/1845 train_time:53957ms step_avg:47.29ms
step:1142/1845 train_time:54018ms step_avg:47.30ms
step:1143/1845 train_time:54080ms step_avg:47.31ms
step:1144/1845 train_time:54140ms step_avg:47.33ms
step:1145/1845 train_time:54202ms step_avg:47.34ms
step:1146/1845 train_time:54263ms step_avg:47.35ms
step:1147/1845 train_time:54326ms step_avg:47.36ms
step:1148/1845 train_time:54387ms step_avg:47.38ms
step:1149/1845 train_time:54450ms step_avg:47.39ms
step:1150/1845 train_time:54510ms step_avg:47.40ms
step:1151/1845 train_time:54573ms step_avg:47.41ms
step:1152/1845 train_time:54634ms step_avg:47.43ms
step:1153/1845 train_time:54697ms step_avg:47.44ms
step:1154/1845 train_time:54757ms step_avg:47.45ms
step:1155/1845 train_time:54819ms step_avg:47.46ms
step:1156/1845 train_time:54880ms step_avg:47.47ms
step:1157/1845 train_time:54942ms step_avg:47.49ms
step:1158/1845 train_time:55003ms step_avg:47.50ms
step:1159/1845 train_time:55065ms step_avg:47.51ms
step:1160/1845 train_time:55126ms step_avg:47.52ms
step:1161/1845 train_time:55188ms step_avg:47.54ms
step:1162/1845 train_time:55249ms step_avg:47.55ms
step:1163/1845 train_time:55312ms step_avg:47.56ms
step:1164/1845 train_time:55374ms step_avg:47.57ms
step:1165/1845 train_time:55436ms step_avg:47.58ms
step:1166/1845 train_time:55497ms step_avg:47.60ms
step:1167/1845 train_time:55559ms step_avg:47.61ms
step:1168/1845 train_time:55620ms step_avg:47.62ms
step:1169/1845 train_time:55682ms step_avg:47.63ms
step:1170/1845 train_time:55742ms step_avg:47.64ms
step:1171/1845 train_time:55805ms step_avg:47.66ms
step:1172/1845 train_time:55866ms step_avg:47.67ms
step:1173/1845 train_time:55928ms step_avg:47.68ms
step:1174/1845 train_time:55989ms step_avg:47.69ms
step:1175/1845 train_time:56052ms step_avg:47.70ms
step:1176/1845 train_time:56113ms step_avg:47.71ms
step:1177/1845 train_time:56176ms step_avg:47.73ms
step:1178/1845 train_time:56236ms step_avg:47.74ms
step:1179/1845 train_time:56298ms step_avg:47.75ms
step:1180/1845 train_time:56359ms step_avg:47.76ms
step:1181/1845 train_time:56421ms step_avg:47.77ms
step:1182/1845 train_time:56482ms step_avg:47.78ms
step:1183/1845 train_time:56544ms step_avg:47.80ms
step:1184/1845 train_time:56605ms step_avg:47.81ms
step:1185/1845 train_time:56668ms step_avg:47.82ms
step:1186/1845 train_time:56728ms step_avg:47.83ms
step:1187/1845 train_time:56791ms step_avg:47.84ms
step:1188/1845 train_time:56852ms step_avg:47.86ms
step:1189/1845 train_time:56915ms step_avg:47.87ms
step:1190/1845 train_time:56975ms step_avg:47.88ms
step:1191/1845 train_time:57038ms step_avg:47.89ms
step:1192/1845 train_time:57098ms step_avg:47.90ms
step:1193/1845 train_time:57160ms step_avg:47.91ms
step:1194/1845 train_time:57220ms step_avg:47.92ms
step:1195/1845 train_time:57283ms step_avg:47.94ms
step:1196/1845 train_time:57344ms step_avg:47.95ms
step:1197/1845 train_time:57407ms step_avg:47.96ms
step:1198/1845 train_time:57467ms step_avg:47.97ms
step:1199/1845 train_time:57530ms step_avg:47.98ms
step:1200/1845 train_time:57591ms step_avg:47.99ms
step:1201/1845 train_time:57654ms step_avg:48.00ms
step:1202/1845 train_time:57714ms step_avg:48.02ms
step:1203/1845 train_time:57777ms step_avg:48.03ms
step:1204/1845 train_time:57838ms step_avg:48.04ms
step:1205/1845 train_time:57900ms step_avg:48.05ms
step:1206/1845 train_time:57987ms step_avg:48.08ms
step:1207/1845 train_time:58076ms step_avg:48.12ms
step:1208/1845 train_time:58163ms step_avg:48.15ms
step:1209/1845 train_time:58253ms step_avg:48.18ms
step:1210/1845 train_time:58340ms step_avg:48.22ms
step:1211/1845 train_time:58429ms step_avg:48.25ms
step:1212/1845 train_time:58516ms step_avg:48.28ms
step:1213/1845 train_time:58605ms step_avg:48.31ms
step:1214/1845 train_time:58692ms step_avg:48.35ms
step:1215/1845 train_time:58782ms step_avg:48.38ms
step:1216/1845 train_time:58868ms step_avg:48.41ms
step:1217/1845 train_time:58957ms step_avg:48.44ms
step:1218/1845 train_time:59044ms step_avg:48.48ms
step:1219/1845 train_time:59132ms step_avg:48.51ms
step:1220/1845 train_time:59219ms step_avg:48.54ms
step:1221/1845 train_time:59308ms step_avg:48.57ms
step:1222/1845 train_time:59395ms step_avg:48.60ms
step:1223/1845 train_time:59484ms step_avg:48.64ms
step:1224/1845 train_time:59571ms step_avg:48.67ms
step:1225/1845 train_time:59660ms step_avg:48.70ms
step:1226/1845 train_time:59747ms step_avg:48.73ms
step:1227/1845 train_time:59835ms step_avg:48.77ms
step:1228/1845 train_time:59922ms step_avg:48.80ms
step:1229/1845 train_time:60010ms step_avg:48.83ms
step:1230/1845 train_time:60098ms step_avg:48.86ms
step:1231/1845 train_time:60187ms step_avg:48.89ms
step:1232/1845 train_time:60274ms step_avg:48.92ms
step:1233/1845 train_time:60362ms step_avg:48.96ms
step:1234/1845 train_time:60450ms step_avg:48.99ms
step:1235/1845 train_time:60538ms step_avg:49.02ms
step:1236/1845 train_time:60625ms step_avg:49.05ms
step:1237/1845 train_time:60714ms step_avg:49.08ms
step:1238/1845 train_time:60802ms step_avg:49.11ms
step:1239/1845 train_time:60890ms step_avg:49.14ms
step:1240/1845 train_time:60978ms step_avg:49.18ms
step:1241/1845 train_time:61068ms step_avg:49.21ms
step:1242/1845 train_time:61155ms step_avg:49.24ms
step:1243/1845 train_time:61244ms step_avg:49.27ms
step:1244/1845 train_time:61331ms step_avg:49.30ms
step:1245/1845 train_time:61420ms step_avg:49.33ms
step:1246/1845 train_time:61509ms step_avg:49.36ms
step:1247/1845 train_time:61598ms step_avg:49.40ms
step:1248/1845 train_time:61685ms step_avg:49.43ms
step:1249/1845 train_time:61773ms step_avg:49.46ms
step:1250/1845 train_time:61861ms step_avg:49.49ms
step:1250/1845 val_loss:3.5345 train_time:61950ms step_avg:49.56ms
step:1251/1845 train_time:61971ms step_avg:49.54ms
step:1252/1845 train_time:62039ms step_avg:49.55ms
step:1253/1845 train_time:62135ms step_avg:49.59ms
step:1254/1845 train_time:62224ms step_avg:49.62ms
step:1255/1845 train_time:62312ms step_avg:49.65ms
step:1256/1845 train_time:62399ms step_avg:49.68ms
step:1257/1845 train_time:62487ms step_avg:49.71ms
step:1258/1845 train_time:62572ms step_avg:49.74ms
step:1259/1845 train_time:62660ms step_avg:49.77ms
step:1260/1845 train_time:62746ms step_avg:49.80ms
step:1261/1845 train_time:62834ms step_avg:49.83ms
step:1262/1845 train_time:62922ms step_avg:49.86ms
step:1263/1845 train_time:63013ms step_avg:49.89ms
step:1264/1845 train_time:63103ms step_avg:49.92ms
step:1265/1845 train_time:63193ms step_avg:49.95ms
step:1266/1845 train_time:63281ms step_avg:49.98ms
step:1267/1845 train_time:63369ms step_avg:50.02ms
step:1268/1845 train_time:63455ms step_avg:50.04ms
step:1269/1845 train_time:63543ms step_avg:50.07ms
step:1270/1845 train_time:63629ms step_avg:50.10ms
step:1271/1845 train_time:63717ms step_avg:50.13ms
step:1272/1845 train_time:63805ms step_avg:50.16ms
step:1273/1845 train_time:63893ms step_avg:50.19ms
step:1274/1845 train_time:63981ms step_avg:50.22ms
step:1275/1845 train_time:64071ms step_avg:50.25ms
step:1276/1845 train_time:64160ms step_avg:50.28ms
step:1277/1845 train_time:64250ms step_avg:50.31ms
step:1278/1845 train_time:64337ms step_avg:50.34ms
step:1279/1845 train_time:64426ms step_avg:50.37ms
step:1280/1845 train_time:64512ms step_avg:50.40ms
step:1281/1845 train_time:64600ms step_avg:50.43ms
step:1282/1845 train_time:64687ms step_avg:50.46ms
step:1283/1845 train_time:64776ms step_avg:50.49ms
step:1284/1845 train_time:64863ms step_avg:50.52ms
step:1285/1845 train_time:64952ms step_avg:50.55ms
step:1286/1845 train_time:65041ms step_avg:50.58ms
step:1287/1845 train_time:65130ms step_avg:50.61ms
step:1288/1845 train_time:65219ms step_avg:50.64ms
step:1289/1845 train_time:65308ms step_avg:50.67ms
step:1290/1845 train_time:65395ms step_avg:50.69ms
step:1291/1845 train_time:65484ms step_avg:50.72ms
step:1292/1845 train_time:65570ms step_avg:50.75ms
step:1293/1845 train_time:65658ms step_avg:50.78ms
step:1294/1845 train_time:65745ms step_avg:50.81ms
step:1295/1845 train_time:65833ms step_avg:50.84ms
step:1296/1845 train_time:65921ms step_avg:50.86ms
step:1297/1845 train_time:66010ms step_avg:50.89ms
step:1298/1845 train_time:66099ms step_avg:50.92ms
step:1299/1845 train_time:66187ms step_avg:50.95ms
step:1300/1845 train_time:66274ms step_avg:50.98ms
step:1301/1845 train_time:66362ms step_avg:51.01ms
step:1302/1845 train_time:66450ms step_avg:51.04ms
step:1303/1845 train_time:66539ms step_avg:51.07ms
step:1304/1845 train_time:66627ms step_avg:51.09ms
step:1305/1845 train_time:66714ms step_avg:51.12ms
step:1306/1845 train_time:66802ms step_avg:51.15ms
step:1307/1845 train_time:66892ms step_avg:51.18ms
step:1308/1845 train_time:66981ms step_avg:51.21ms
step:1309/1845 train_time:67071ms step_avg:51.24ms
step:1310/1845 train_time:67159ms step_avg:51.27ms
step:1311/1845 train_time:67249ms step_avg:51.30ms
step:1312/1845 train_time:67336ms step_avg:51.32ms
step:1313/1845 train_time:67425ms step_avg:51.35ms
step:1314/1845 train_time:67512ms step_avg:51.38ms
step:1315/1845 train_time:67599ms step_avg:51.41ms
step:1316/1845 train_time:67686ms step_avg:51.43ms
step:1317/1845 train_time:67774ms step_avg:51.46ms
step:1318/1845 train_time:67862ms step_avg:51.49ms
step:1319/1845 train_time:67951ms step_avg:51.52ms
step:1320/1845 train_time:68039ms step_avg:51.54ms
step:1321/1845 train_time:68127ms step_avg:51.57ms
step:1322/1845 train_time:68215ms step_avg:51.60ms
step:1323/1845 train_time:68303ms step_avg:51.63ms
step:1324/1845 train_time:68391ms step_avg:51.65ms
step:1325/1845 train_time:68479ms step_avg:51.68ms
step:1326/1845 train_time:68566ms step_avg:51.71ms
step:1327/1845 train_time:68654ms step_avg:51.74ms
step:1328/1845 train_time:68742ms step_avg:51.76ms
step:1329/1845 train_time:68831ms step_avg:51.79ms
step:1330/1845 train_time:68919ms step_avg:51.82ms
step:1331/1845 train_time:69008ms step_avg:51.85ms
step:1332/1845 train_time:69095ms step_avg:51.87ms
step:1333/1845 train_time:69185ms step_avg:51.90ms
step:1334/1845 train_time:69273ms step_avg:51.93ms
step:1335/1845 train_time:69361ms step_avg:51.96ms
step:1336/1845 train_time:69448ms step_avg:51.98ms
step:1337/1845 train_time:69537ms step_avg:52.01ms
step:1338/1845 train_time:69625ms step_avg:52.04ms
step:1339/1845 train_time:69713ms step_avg:52.06ms
step:1340/1845 train_time:69800ms step_avg:52.09ms
step:1341/1845 train_time:69890ms step_avg:52.12ms
step:1342/1845 train_time:69977ms step_avg:52.14ms
step:1343/1845 train_time:70066ms step_avg:52.17ms
step:1344/1845 train_time:70153ms step_avg:52.20ms
step:1345/1845 train_time:70242ms step_avg:52.22ms
step:1346/1845 train_time:70330ms step_avg:52.25ms
step:1347/1845 train_time:70418ms step_avg:52.28ms
step:1348/1845 train_time:70505ms step_avg:52.30ms
step:1349/1845 train_time:70592ms step_avg:52.33ms
step:1350/1845 train_time:70679ms step_avg:52.35ms
step:1351/1845 train_time:70768ms step_avg:52.38ms
step:1352/1845 train_time:70855ms step_avg:52.41ms
step:1353/1845 train_time:70944ms step_avg:52.43ms
step:1354/1845 train_time:71031ms step_avg:52.46ms
step:1355/1845 train_time:71120ms step_avg:52.49ms
step:1356/1845 train_time:71207ms step_avg:52.51ms
step:1357/1845 train_time:71295ms step_avg:52.54ms
step:1358/1845 train_time:71383ms step_avg:52.56ms
step:1359/1845 train_time:71471ms step_avg:52.59ms
step:1360/1845 train_time:71559ms step_avg:52.62ms
step:1361/1845 train_time:71648ms step_avg:52.64ms
step:1362/1845 train_time:71735ms step_avg:52.67ms
step:1363/1845 train_time:71823ms step_avg:52.69ms
step:1364/1845 train_time:71911ms step_avg:52.72ms
step:1365/1845 train_time:71999ms step_avg:52.75ms
step:1366/1845 train_time:72087ms step_avg:52.77ms
step:1367/1845 train_time:72177ms step_avg:52.80ms
step:1368/1845 train_time:72264ms step_avg:52.82ms
step:1369/1845 train_time:72352ms step_avg:52.85ms
step:1370/1845 train_time:72440ms step_avg:52.88ms
step:1371/1845 train_time:72529ms step_avg:52.90ms
step:1372/1845 train_time:72616ms step_avg:52.93ms
step:1373/1845 train_time:72705ms step_avg:52.95ms
step:1374/1845 train_time:72791ms step_avg:52.98ms
step:1375/1845 train_time:72879ms step_avg:53.00ms
step:1376/1845 train_time:72966ms step_avg:53.03ms
step:1377/1845 train_time:73054ms step_avg:53.05ms
step:1378/1845 train_time:73142ms step_avg:53.08ms
step:1379/1845 train_time:73232ms step_avg:53.10ms
step:1380/1845 train_time:73319ms step_avg:53.13ms
step:1381/1845 train_time:73407ms step_avg:53.15ms
step:1382/1845 train_time:73494ms step_avg:53.18ms
step:1383/1845 train_time:73583ms step_avg:53.21ms
step:1384/1845 train_time:73669ms step_avg:53.23ms
step:1385/1845 train_time:73757ms step_avg:53.25ms
step:1386/1845 train_time:73845ms step_avg:53.28ms
step:1387/1845 train_time:73933ms step_avg:53.30ms
step:1388/1845 train_time:74020ms step_avg:53.33ms
step:1389/1845 train_time:74108ms step_avg:53.35ms
step:1390/1845 train_time:74196ms step_avg:53.38ms
step:1391/1845 train_time:74284ms step_avg:53.40ms
step:1392/1845 train_time:74371ms step_avg:53.43ms
step:1393/1845 train_time:74459ms step_avg:53.45ms
step:1394/1845 train_time:74546ms step_avg:53.48ms
step:1395/1845 train_time:74635ms step_avg:53.50ms
step:1396/1845 train_time:74722ms step_avg:53.53ms
step:1397/1845 train_time:74811ms step_avg:53.55ms
step:1398/1845 train_time:74899ms step_avg:53.58ms
step:1399/1845 train_time:74987ms step_avg:53.60ms
step:1400/1845 train_time:75074ms step_avg:53.62ms
step:1401/1845 train_time:75163ms step_avg:53.65ms
step:1402/1845 train_time:75250ms step_avg:53.67ms
step:1403/1845 train_time:75339ms step_avg:53.70ms
step:1404/1845 train_time:75427ms step_avg:53.72ms
step:1405/1845 train_time:75516ms step_avg:53.75ms
step:1406/1845 train_time:75603ms step_avg:53.77ms
step:1407/1845 train_time:75692ms step_avg:53.80ms
step:1408/1845 train_time:75779ms step_avg:53.82ms
step:1409/1845 train_time:75868ms step_avg:53.85ms
step:1410/1845 train_time:75954ms step_avg:53.87ms
step:1411/1845 train_time:76043ms step_avg:53.89ms
step:1412/1845 train_time:76130ms step_avg:53.92ms
step:1413/1845 train_time:76218ms step_avg:53.94ms
step:1414/1845 train_time:76306ms step_avg:53.96ms
step:1415/1845 train_time:76394ms step_avg:53.99ms
step:1416/1845 train_time:76482ms step_avg:54.01ms
step:1417/1845 train_time:76571ms step_avg:54.04ms
step:1418/1845 train_time:76658ms step_avg:54.06ms
step:1419/1845 train_time:76747ms step_avg:54.09ms
step:1420/1845 train_time:76834ms step_avg:54.11ms
step:1421/1845 train_time:76923ms step_avg:54.13ms
step:1422/1845 train_time:77010ms step_avg:54.16ms
step:1423/1845 train_time:77098ms step_avg:54.18ms
step:1424/1845 train_time:77185ms step_avg:54.20ms
step:1425/1845 train_time:77273ms step_avg:54.23ms
step:1426/1845 train_time:77360ms step_avg:54.25ms
step:1427/1845 train_time:77449ms step_avg:54.27ms
step:1428/1845 train_time:77536ms step_avg:54.30ms
step:1429/1845 train_time:77625ms step_avg:54.32ms
step:1430/1845 train_time:77712ms step_avg:54.34ms
step:1431/1845 train_time:77800ms step_avg:54.37ms
step:1432/1845 train_time:77888ms step_avg:54.39ms
step:1433/1845 train_time:77976ms step_avg:54.41ms
step:1434/1845 train_time:78064ms step_avg:54.44ms
step:1435/1845 train_time:78153ms step_avg:54.46ms
step:1436/1845 train_time:78241ms step_avg:54.49ms
step:1437/1845 train_time:78328ms step_avg:54.51ms
step:1438/1845 train_time:78416ms step_avg:54.53ms
step:1439/1845 train_time:78504ms step_avg:54.55ms
step:1440/1845 train_time:78591ms step_avg:54.58ms
step:1441/1845 train_time:78680ms step_avg:54.60ms
step:1442/1845 train_time:78767ms step_avg:54.62ms
step:1443/1845 train_time:78855ms step_avg:54.65ms
step:1444/1845 train_time:78942ms step_avg:54.67ms
step:1445/1845 train_time:79031ms step_avg:54.69ms
step:1446/1845 train_time:79119ms step_avg:54.72ms
step:1447/1845 train_time:79208ms step_avg:54.74ms
step:1448/1845 train_time:79295ms step_avg:54.76ms
step:1449/1845 train_time:79383ms step_avg:54.78ms
step:1450/1845 train_time:79470ms step_avg:54.81ms
step:1451/1845 train_time:79558ms step_avg:54.83ms
step:1452/1845 train_time:79646ms step_avg:54.85ms
step:1453/1845 train_time:79734ms step_avg:54.88ms
step:1454/1845 train_time:79821ms step_avg:54.90ms
step:1455/1845 train_time:79910ms step_avg:54.92ms
step:1456/1845 train_time:79997ms step_avg:54.94ms
step:1457/1845 train_time:80085ms step_avg:54.97ms
step:1458/1845 train_time:80173ms step_avg:54.99ms
step:1459/1845 train_time:80261ms step_avg:55.01ms
step:1460/1845 train_time:80349ms step_avg:55.03ms
step:1461/1845 train_time:80438ms step_avg:55.06ms
step:1462/1845 train_time:80525ms step_avg:55.08ms
step:1463/1845 train_time:80614ms step_avg:55.10ms
step:1464/1845 train_time:80701ms step_avg:55.12ms
step:1465/1845 train_time:80790ms step_avg:55.15ms
step:1466/1845 train_time:80877ms step_avg:55.17ms
step:1467/1845 train_time:80966ms step_avg:55.19ms
step:1468/1845 train_time:81054ms step_avg:55.21ms
step:1469/1845 train_time:81143ms step_avg:55.24ms
step:1470/1845 train_time:81230ms step_avg:55.26ms
step:1471/1845 train_time:81318ms step_avg:55.28ms
step:1472/1845 train_time:81406ms step_avg:55.30ms
step:1473/1845 train_time:81494ms step_avg:55.33ms
step:1474/1845 train_time:81582ms step_avg:55.35ms
step:1475/1845 train_time:81671ms step_avg:55.37ms
step:1476/1845 train_time:81758ms step_avg:55.39ms
step:1477/1845 train_time:81847ms step_avg:55.41ms
step:1478/1845 train_time:81934ms step_avg:55.44ms
step:1479/1845 train_time:82022ms step_avg:55.46ms
step:1480/1845 train_time:82110ms step_avg:55.48ms
step:1481/1845 train_time:82198ms step_avg:55.50ms
step:1482/1845 train_time:82286ms step_avg:55.52ms
step:1483/1845 train_time:82375ms step_avg:55.55ms
step:1484/1845 train_time:82463ms step_avg:55.57ms
step:1485/1845 train_time:82552ms step_avg:55.59ms
step:1486/1845 train_time:82639ms step_avg:55.61ms
step:1487/1845 train_time:82729ms step_avg:55.63ms
step:1488/1845 train_time:82817ms step_avg:55.66ms
step:1489/1845 train_time:82906ms step_avg:55.68ms
step:1490/1845 train_time:82993ms step_avg:55.70ms
step:1491/1845 train_time:83081ms step_avg:55.72ms
step:1492/1845 train_time:83168ms step_avg:55.74ms
step:1493/1845 train_time:83256ms step_avg:55.76ms
step:1494/1845 train_time:83344ms step_avg:55.79ms
step:1495/1845 train_time:83432ms step_avg:55.81ms
step:1496/1845 train_time:83519ms step_avg:55.83ms
step:1497/1845 train_time:83609ms step_avg:55.85ms
step:1498/1845 train_time:83696ms step_avg:55.87ms
step:1499/1845 train_time:83784ms step_avg:55.89ms
step:1500/1845 train_time:83872ms step_avg:55.91ms
step:1500/1845 val_loss:3.4032 train_time:83961ms step_avg:55.97ms
step:1501/1845 train_time:83980ms step_avg:55.95ms
step:1502/1845 train_time:84051ms step_avg:55.96ms
step:1503/1845 train_time:84142ms step_avg:55.98ms
step:1504/1845 train_time:84230ms step_avg:56.00ms
step:1505/1845 train_time:84318ms step_avg:56.03ms
step:1506/1845 train_time:84404ms step_avg:56.05ms
step:1507/1845 train_time:84492ms step_avg:56.07ms
step:1508/1845 train_time:84580ms step_avg:56.09ms
step:1509/1845 train_time:84668ms step_avg:56.11ms
step:1510/1845 train_time:84756ms step_avg:56.13ms
step:1511/1845 train_time:84844ms step_avg:56.15ms
step:1512/1845 train_time:84932ms step_avg:56.17ms
step:1513/1845 train_time:85023ms step_avg:56.20ms
step:1514/1845 train_time:85113ms step_avg:56.22ms
step:1515/1845 train_time:85203ms step_avg:56.24ms
step:1516/1845 train_time:85289ms step_avg:56.26ms
step:1517/1845 train_time:85377ms step_avg:56.28ms
step:1518/1845 train_time:85465ms step_avg:56.30ms
step:1519/1845 train_time:85552ms step_avg:56.32ms
step:1520/1845 train_time:85639ms step_avg:56.34ms
step:1521/1845 train_time:85726ms step_avg:56.36ms
step:1522/1845 train_time:85812ms step_avg:56.38ms
step:1523/1845 train_time:85902ms step_avg:56.40ms
step:1524/1845 train_time:85990ms step_avg:56.42ms
step:1525/1845 train_time:86080ms step_avg:56.45ms
step:1526/1845 train_time:86169ms step_avg:56.47ms
step:1527/1845 train_time:86258ms step_avg:56.49ms
step:1528/1845 train_time:86344ms step_avg:56.51ms
step:1529/1845 train_time:86432ms step_avg:56.53ms
step:1530/1845 train_time:86519ms step_avg:56.55ms
step:1531/1845 train_time:86607ms step_avg:56.57ms
step:1532/1845 train_time:86693ms step_avg:56.59ms
step:1533/1845 train_time:86782ms step_avg:56.61ms
step:1534/1845 train_time:86870ms step_avg:56.63ms
step:1535/1845 train_time:86959ms step_avg:56.65ms
step:1536/1845 train_time:87047ms step_avg:56.67ms
step:1537/1845 train_time:87136ms step_avg:56.69ms
step:1538/1845 train_time:87224ms step_avg:56.71ms
step:1539/1845 train_time:87313ms step_avg:56.73ms
step:1540/1845 train_time:87401ms step_avg:56.75ms
step:1541/1845 train_time:87489ms step_avg:56.77ms
step:1542/1845 train_time:87576ms step_avg:56.79ms
step:1543/1845 train_time:87664ms step_avg:56.81ms
step:1544/1845 train_time:87750ms step_avg:56.83ms
step:1545/1845 train_time:87838ms step_avg:56.85ms
step:1546/1845 train_time:87926ms step_avg:56.87ms
step:1547/1845 train_time:88014ms step_avg:56.89ms
step:1548/1845 train_time:88103ms step_avg:56.91ms
step:1549/1845 train_time:88191ms step_avg:56.93ms
step:1550/1845 train_time:88279ms step_avg:56.95ms
step:1551/1845 train_time:88368ms step_avg:56.97ms
step:1552/1845 train_time:88455ms step_avg:56.99ms
step:1553/1845 train_time:88544ms step_avg:57.01ms
step:1554/1845 train_time:88631ms step_avg:57.03ms
step:1555/1845 train_time:88719ms step_avg:57.05ms
step:1556/1845 train_time:88806ms step_avg:57.07ms
step:1557/1845 train_time:88895ms step_avg:57.09ms
step:1558/1845 train_time:88984ms step_avg:57.11ms
step:1559/1845 train_time:89074ms step_avg:57.14ms
step:1560/1845 train_time:89161ms step_avg:57.15ms
step:1561/1845 train_time:89249ms step_avg:57.17ms
step:1562/1845 train_time:89337ms step_avg:57.19ms
step:1563/1845 train_time:89426ms step_avg:57.21ms
step:1564/1845 train_time:89514ms step_avg:57.23ms
step:1565/1845 train_time:89602ms step_avg:57.25ms
step:1566/1845 train_time:89689ms step_avg:57.27ms
step:1567/1845 train_time:89778ms step_avg:57.29ms
step:1568/1845 train_time:89865ms step_avg:57.31ms
step:1569/1845 train_time:89954ms step_avg:57.33ms
step:1570/1845 train_time:90041ms step_avg:57.35ms
step:1571/1845 train_time:90130ms step_avg:57.37ms
step:1572/1845 train_time:90218ms step_avg:57.39ms
step:1573/1845 train_time:90306ms step_avg:57.41ms
step:1574/1845 train_time:90394ms step_avg:57.43ms
step:1575/1845 train_time:90484ms step_avg:57.45ms
step:1576/1845 train_time:90570ms step_avg:57.47ms
step:1577/1845 train_time:90660ms step_avg:57.49ms
step:1578/1845 train_time:90747ms step_avg:57.51ms
step:1579/1845 train_time:90836ms step_avg:57.53ms
step:1580/1845 train_time:90922ms step_avg:57.55ms
step:1581/1845 train_time:91012ms step_avg:57.57ms
step:1582/1845 train_time:91100ms step_avg:57.59ms
step:1583/1845 train_time:91187ms step_avg:57.60ms
step:1584/1845 train_time:91274ms step_avg:57.62ms
step:1585/1845 train_time:91363ms step_avg:57.64ms
step:1586/1845 train_time:91450ms step_avg:57.66ms
step:1587/1845 train_time:91538ms step_avg:57.68ms
step:1588/1845 train_time:91625ms step_avg:57.70ms
step:1589/1845 train_time:91714ms step_avg:57.72ms
step:1590/1845 train_time:91801ms step_avg:57.74ms
step:1591/1845 train_time:91890ms step_avg:57.76ms
step:1592/1845 train_time:91978ms step_avg:57.77ms
step:1593/1845 train_time:92067ms step_avg:57.79ms
step:1594/1845 train_time:92154ms step_avg:57.81ms
step:1595/1845 train_time:92243ms step_avg:57.83ms
step:1596/1845 train_time:92330ms step_avg:57.85ms
step:1597/1845 train_time:92418ms step_avg:57.87ms
step:1598/1845 train_time:92505ms step_avg:57.89ms
step:1599/1845 train_time:92593ms step_avg:57.91ms
step:1600/1845 train_time:92680ms step_avg:57.93ms
step:1601/1845 train_time:92768ms step_avg:57.94ms
step:1602/1845 train_time:92856ms step_avg:57.96ms
step:1603/1845 train_time:92944ms step_avg:57.98ms
step:1604/1845 train_time:93032ms step_avg:58.00ms
step:1605/1845 train_time:93122ms step_avg:58.02ms
step:1606/1845 train_time:93208ms step_avg:58.04ms
step:1607/1845 train_time:93297ms step_avg:58.06ms
step:1608/1845 train_time:93384ms step_avg:58.07ms
step:1609/1845 train_time:93473ms step_avg:58.09ms
step:1610/1845 train_time:93561ms step_avg:58.11ms
step:1611/1845 train_time:93650ms step_avg:58.13ms
step:1612/1845 train_time:93738ms step_avg:58.15ms
step:1613/1845 train_time:93825ms step_avg:58.17ms
step:1614/1845 train_time:93912ms step_avg:58.19ms
step:1615/1845 train_time:94001ms step_avg:58.20ms
step:1616/1845 train_time:94088ms step_avg:58.22ms
step:1617/1845 train_time:94176ms step_avg:58.24ms
step:1618/1845 train_time:94264ms step_avg:58.26ms
step:1619/1845 train_time:94353ms step_avg:58.28ms
step:1620/1845 train_time:94440ms step_avg:58.30ms
step:1621/1845 train_time:94528ms step_avg:58.31ms
step:1622/1845 train_time:94616ms step_avg:58.33ms
step:1623/1845 train_time:94704ms step_avg:58.35ms
step:1624/1845 train_time:94792ms step_avg:58.37ms
step:1625/1845 train_time:94881ms step_avg:58.39ms
step:1626/1845 train_time:94968ms step_avg:58.41ms
step:1627/1845 train_time:95057ms step_avg:58.42ms
step:1628/1845 train_time:95144ms step_avg:58.44ms
step:1629/1845 train_time:95233ms step_avg:58.46ms
step:1630/1845 train_time:95321ms step_avg:58.48ms
step:1631/1845 train_time:95409ms step_avg:58.50ms
step:1632/1845 train_time:95496ms step_avg:58.51ms
step:1633/1845 train_time:95585ms step_avg:58.53ms
step:1634/1845 train_time:95674ms step_avg:58.55ms
step:1635/1845 train_time:95763ms step_avg:58.57ms
step:1636/1845 train_time:95850ms step_avg:58.59ms
step:1637/1845 train_time:95938ms step_avg:58.61ms
step:1638/1845 train_time:96025ms step_avg:58.62ms
step:1639/1845 train_time:96114ms step_avg:58.64ms
step:1640/1845 train_time:96201ms step_avg:58.66ms
step:1641/1845 train_time:96290ms step_avg:58.68ms
step:1642/1845 train_time:96377ms step_avg:58.69ms
step:1643/1845 train_time:96466ms step_avg:58.71ms
step:1644/1845 train_time:96554ms step_avg:58.73ms
step:1645/1845 train_time:96642ms step_avg:58.75ms
step:1646/1845 train_time:96729ms step_avg:58.77ms
step:1647/1845 train_time:96819ms step_avg:58.79ms
step:1648/1845 train_time:96907ms step_avg:58.80ms
step:1649/1845 train_time:96996ms step_avg:58.82ms
step:1650/1845 train_time:97083ms step_avg:58.84ms
step:1651/1845 train_time:97173ms step_avg:58.86ms
step:1652/1845 train_time:97260ms step_avg:58.87ms
step:1653/1845 train_time:97349ms step_avg:58.89ms
step:1654/1845 train_time:97437ms step_avg:58.91ms
step:1655/1845 train_time:97526ms step_avg:58.93ms
step:1656/1845 train_time:97613ms step_avg:58.95ms
step:1657/1845 train_time:97702ms step_avg:58.96ms
step:1658/1845 train_time:97789ms step_avg:58.98ms
step:1659/1845 train_time:97879ms step_avg:59.00ms
step:1660/1845 train_time:97965ms step_avg:59.02ms
step:1661/1845 train_time:98054ms step_avg:59.03ms
step:1662/1845 train_time:98141ms step_avg:59.05ms
step:1663/1845 train_time:98230ms step_avg:59.07ms
step:1664/1845 train_time:98317ms step_avg:59.08ms
step:1665/1845 train_time:98405ms step_avg:59.10ms
step:1666/1845 train_time:98492ms step_avg:59.12ms
step:1667/1845 train_time:98581ms step_avg:59.14ms
step:1668/1845 train_time:98668ms step_avg:59.15ms
step:1669/1845 train_time:98757ms step_avg:59.17ms
step:1670/1845 train_time:98844ms step_avg:59.19ms
step:1671/1845 train_time:98933ms step_avg:59.21ms
step:1672/1845 train_time:99020ms step_avg:59.22ms
step:1673/1845 train_time:99109ms step_avg:59.24ms
step:1674/1845 train_time:99196ms step_avg:59.26ms
step:1675/1845 train_time:99284ms step_avg:59.27ms
step:1676/1845 train_time:99371ms step_avg:59.29ms
step:1677/1845 train_time:99461ms step_avg:59.31ms
step:1678/1845 train_time:99548ms step_avg:59.33ms
step:1679/1845 train_time:99638ms step_avg:59.34ms
step:1680/1845 train_time:99726ms step_avg:59.36ms
step:1681/1845 train_time:99813ms step_avg:59.38ms
step:1682/1845 train_time:99901ms step_avg:59.39ms
step:1683/1845 train_time:99989ms step_avg:59.41ms
step:1684/1845 train_time:100077ms step_avg:59.43ms
step:1685/1845 train_time:100165ms step_avg:59.44ms
step:1686/1845 train_time:100252ms step_avg:59.46ms
step:1687/1845 train_time:100341ms step_avg:59.48ms
step:1688/1845 train_time:100428ms step_avg:59.50ms
step:1689/1845 train_time:100518ms step_avg:59.51ms
step:1690/1845 train_time:100605ms step_avg:59.53ms
step:1691/1845 train_time:100693ms step_avg:59.55ms
step:1692/1845 train_time:100781ms step_avg:59.56ms
step:1693/1845 train_time:100869ms step_avg:59.58ms
step:1694/1845 train_time:100956ms step_avg:59.60ms
step:1695/1845 train_time:101045ms step_avg:59.61ms
step:1696/1845 train_time:101133ms step_avg:59.63ms
step:1697/1845 train_time:101220ms step_avg:59.65ms
step:1698/1845 train_time:101307ms step_avg:59.66ms
step:1699/1845 train_time:101396ms step_avg:59.68ms
step:1700/1845 train_time:101483ms step_avg:59.70ms
step:1701/1845 train_time:101572ms step_avg:59.71ms
step:1702/1845 train_time:101660ms step_avg:59.73ms
step:1703/1845 train_time:101747ms step_avg:59.75ms
step:1704/1845 train_time:101835ms step_avg:59.76ms
step:1705/1845 train_time:101923ms step_avg:59.78ms
step:1706/1845 train_time:102011ms step_avg:59.80ms
step:1707/1845 train_time:102101ms step_avg:59.81ms
step:1708/1845 train_time:102187ms step_avg:59.83ms
step:1709/1845 train_time:102276ms step_avg:59.85ms
step:1710/1845 train_time:102363ms step_avg:59.86ms
step:1711/1845 train_time:102451ms step_avg:59.88ms
step:1712/1845 train_time:102539ms step_avg:59.89ms
step:1713/1845 train_time:102628ms step_avg:59.91ms
step:1714/1845 train_time:102715ms step_avg:59.93ms
step:1715/1845 train_time:102804ms step_avg:59.94ms
step:1716/1845 train_time:102892ms step_avg:59.96ms
step:1717/1845 train_time:102981ms step_avg:59.98ms
step:1718/1845 train_time:103069ms step_avg:59.99ms
step:1719/1845 train_time:103157ms step_avg:60.01ms
step:1720/1845 train_time:103244ms step_avg:60.03ms
step:1721/1845 train_time:103332ms step_avg:60.04ms
step:1722/1845 train_time:103420ms step_avg:60.06ms
step:1723/1845 train_time:103509ms step_avg:60.07ms
step:1724/1845 train_time:103596ms step_avg:60.09ms
step:1725/1845 train_time:103685ms step_avg:60.11ms
step:1726/1845 train_time:103773ms step_avg:60.12ms
step:1727/1845 train_time:103861ms step_avg:60.14ms
step:1728/1845 train_time:103948ms step_avg:60.16ms
step:1729/1845 train_time:104037ms step_avg:60.17ms
step:1730/1845 train_time:104124ms step_avg:60.19ms
step:1731/1845 train_time:104212ms step_avg:60.20ms
step:1732/1845 train_time:104299ms step_avg:60.22ms
step:1733/1845 train_time:104387ms step_avg:60.23ms
step:1734/1845 train_time:104475ms step_avg:60.25ms
step:1735/1845 train_time:104564ms step_avg:60.27ms
step:1736/1845 train_time:104651ms step_avg:60.28ms
step:1737/1845 train_time:104740ms step_avg:60.30ms
step:1738/1845 train_time:104828ms step_avg:60.32ms
step:1739/1845 train_time:104915ms step_avg:60.33ms
step:1740/1845 train_time:105004ms step_avg:60.35ms
step:1741/1845 train_time:105092ms step_avg:60.36ms
step:1742/1845 train_time:105180ms step_avg:60.38ms
step:1743/1845 train_time:105267ms step_avg:60.39ms
step:1744/1845 train_time:105355ms step_avg:60.41ms
step:1745/1845 train_time:105444ms step_avg:60.43ms
step:1746/1845 train_time:105531ms step_avg:60.44ms
step:1747/1845 train_time:105619ms step_avg:60.46ms
step:1748/1845 train_time:105706ms step_avg:60.47ms
step:1749/1845 train_time:105794ms step_avg:60.49ms
step:1750/1845 train_time:105882ms step_avg:60.50ms
step:1750/1845 val_loss:3.3040 train_time:105972ms step_avg:60.56ms
step:1751/1845 train_time:105991ms step_avg:60.53ms
step:1752/1845 train_time:106063ms step_avg:60.54ms
step:1753/1845 train_time:106155ms step_avg:60.56ms
step:1754/1845 train_time:106244ms step_avg:60.57ms
step:1755/1845 train_time:106332ms step_avg:60.59ms
step:1756/1845 train_time:106420ms step_avg:60.60ms
step:1757/1845 train_time:106507ms step_avg:60.62ms
step:1758/1845 train_time:106594ms step_avg:60.63ms
step:1759/1845 train_time:106681ms step_avg:60.65ms
step:1760/1845 train_time:106767ms step_avg:60.66ms
step:1761/1845 train_time:106855ms step_avg:60.68ms
step:1762/1845 train_time:106942ms step_avg:60.69ms
step:1763/1845 train_time:107034ms step_avg:60.71ms
step:1764/1845 train_time:107125ms step_avg:60.73ms
step:1765/1845 train_time:107215ms step_avg:60.75ms
step:1766/1845 train_time:107304ms step_avg:60.76ms
step:1767/1845 train_time:107392ms step_avg:60.78ms
step:1768/1845 train_time:107478ms step_avg:60.79ms
step:1769/1845 train_time:107566ms step_avg:60.81ms
step:1770/1845 train_time:107652ms step_avg:60.82ms
step:1771/1845 train_time:107740ms step_avg:60.84ms
step:1772/1845 train_time:107826ms step_avg:60.85ms
step:1773/1845 train_time:107915ms step_avg:60.87ms
step:1774/1845 train_time:108004ms step_avg:60.88ms
step:1775/1845 train_time:108093ms step_avg:60.90ms
step:1776/1845 train_time:108181ms step_avg:60.91ms
step:1777/1845 train_time:108271ms step_avg:60.93ms
step:1778/1845 train_time:108359ms step_avg:60.94ms
step:1779/1845 train_time:108446ms step_avg:60.96ms
step:1780/1845 train_time:108534ms step_avg:60.97ms
step:1781/1845 train_time:108623ms step_avg:60.99ms
step:1782/1845 train_time:108710ms step_avg:61.00ms
step:1783/1845 train_time:108798ms step_avg:61.02ms
step:1784/1845 train_time:108885ms step_avg:61.03ms
step:1785/1845 train_time:108975ms step_avg:61.05ms
step:1786/1845 train_time:109063ms step_avg:61.07ms
step:1787/1845 train_time:109153ms step_avg:61.08ms
step:1788/1845 train_time:109241ms step_avg:61.10ms
step:1789/1845 train_time:109330ms step_avg:61.11ms
step:1790/1845 train_time:109417ms step_avg:61.13ms
step:1791/1845 train_time:109505ms step_avg:61.14ms
step:1792/1845 train_time:109593ms step_avg:61.16ms
step:1793/1845 train_time:109682ms step_avg:61.17ms
step:1794/1845 train_time:109770ms step_avg:61.19ms
step:1795/1845 train_time:109859ms step_avg:61.20ms
step:1796/1845 train_time:109947ms step_avg:61.22ms
step:1797/1845 train_time:110036ms step_avg:61.23ms
step:1798/1845 train_time:110124ms step_avg:61.25ms
step:1799/1845 train_time:110213ms step_avg:61.26ms
step:1800/1845 train_time:110301ms step_avg:61.28ms
step:1801/1845 train_time:110389ms step_avg:61.29ms
step:1802/1845 train_time:110477ms step_avg:61.31ms
step:1803/1845 train_time:110564ms step_avg:61.32ms
step:1804/1845 train_time:110652ms step_avg:61.34ms
step:1805/1845 train_time:110740ms step_avg:61.35ms
step:1806/1845 train_time:110827ms step_avg:61.37ms
step:1807/1845 train_time:110916ms step_avg:61.38ms
step:1808/1845 train_time:111004ms step_avg:61.40ms
step:1809/1845 train_time:111095ms step_avg:61.41ms
step:1810/1845 train_time:111183ms step_avg:61.43ms
step:1811/1845 train_time:111272ms step_avg:61.44ms
step:1812/1845 train_time:111359ms step_avg:61.46ms
step:1813/1845 train_time:111449ms step_avg:61.47ms
step:1814/1845 train_time:111535ms step_avg:61.49ms
step:1815/1845 train_time:111624ms step_avg:61.50ms
step:1816/1845 train_time:111712ms step_avg:61.52ms
step:1817/1845 train_time:111802ms step_avg:61.53ms
step:1818/1845 train_time:111890ms step_avg:61.55ms
step:1819/1845 train_time:111980ms step_avg:61.56ms
step:1820/1845 train_time:112068ms step_avg:61.58ms
step:1821/1845 train_time:112159ms step_avg:61.59ms
step:1822/1845 train_time:112246ms step_avg:61.61ms
step:1823/1845 train_time:112335ms step_avg:61.62ms
step:1824/1845 train_time:112423ms step_avg:61.64ms
step:1825/1845 train_time:112512ms step_avg:61.65ms
step:1826/1845 train_time:112599ms step_avg:61.66ms
step:1827/1845 train_time:112687ms step_avg:61.68ms
step:1828/1845 train_time:112775ms step_avg:61.69ms
step:1829/1845 train_time:112863ms step_avg:61.71ms
step:1830/1845 train_time:112951ms step_avg:61.72ms
step:1831/1845 train_time:113041ms step_avg:61.74ms
step:1832/1845 train_time:113129ms step_avg:61.75ms
step:1833/1845 train_time:113219ms step_avg:61.77ms
step:1834/1845 train_time:113307ms step_avg:61.78ms
step:1835/1845 train_time:113397ms step_avg:61.80ms
step:1836/1845 train_time:113484ms step_avg:61.81ms
step:1837/1845 train_time:113574ms step_avg:61.83ms
step:1838/1845 train_time:113661ms step_avg:61.84ms
step:1839/1845 train_time:113749ms step_avg:61.85ms
step:1840/1845 train_time:113836ms step_avg:61.87ms
step:1841/1845 train_time:113925ms step_avg:61.88ms
step:1842/1845 train_time:114014ms step_avg:61.90ms
step:1843/1845 train_time:114104ms step_avg:61.91ms
step:1844/1845 train_time:114193ms step_avg:61.93ms
step:1845/1845 train_time:114283ms step_avg:61.94ms
step:1845/1845 val_loss:3.2774 train_time:114373ms step_avg:61.99ms
peak memory allocated: 29405 MiB reserved: 44678 MiB

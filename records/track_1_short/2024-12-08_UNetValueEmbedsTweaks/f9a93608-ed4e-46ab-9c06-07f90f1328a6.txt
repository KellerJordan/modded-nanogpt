import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from pathlib import Path

import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP
# Use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5):
        self.num_process = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ["RANK"])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        params: "list[torch.Tensor]" = list(params)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [
            {
                "params": [p for p in params if p.numel() == size],
                "update_buffer": [
                    torch.empty(size, device="cuda", dtype=torch.bfloat16)
                    for _ in range(self.num_process)
                ],
            }
            for size in sizes
        ]
        super().__init__(param_groups, defaults)

    def step(self):
        for group in self.param_groups:
            lr: float = group["lr"]
            momentum: float = group["momentum"]
            nesterov: bool = group["nesterov"]
            zeropower_backend = zeropower_backends[group["backend"]]
            backend_steps: int = group["backend_steps"]
            update_buffers: "list[torch.Tensor]" = group["update_buffer"]
            # generate weight updates in distributed fashion
            params: "list[torch.Tensor]" = group["params"]
            assert len(params) % self.num_process == 0
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.num_process]:
                p = params[base_i + self.rank]
                g = p.grad
                assert g is not None
                state = self.state[p] 
                if "momentum_buffer" not in state:
                    state["momentum_buffer"] = torch.zeros_like(g)
                buf: torch.Tensor = state["momentum_buffer"]
                buf.lerp_(g, 1 - momentum)
                g = g.lerp_(buf, momentum) if nesterov else buf
                g = zeropower_backend(g, steps=backend_steps).flatten()
                update_prev()
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.num_process]
            update_prev()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.register_buffer('inv_freq', (1 / base) ** (torch.arange(0, dim, 2) / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            t = torch.arange(seq_len, device=x.device)
            freqs = torch.outer(t, self.inv_freq)
            self.seq_len_cached = seq_len
            self.cos_cached = freqs.cos()
            self.sin_cached = freqs.sin()
        cos, sin = self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
        # apply_rotary_emb(x, cos, sin)
        x1, x2 = x.chunk(2, dim=3)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, n_head):
        super().__init__()
        assert dim % n_head == 0
        self.n_head = n_head
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        # value residual lambda
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5])) # @Grad62304977
        # rotary embeddings
        self.rotary = Rotary(dim // n_head) # dim // n_head = head_dim
        # output projection
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: torch.Tensor, vi: torch.Tensor, block_mask: BlockMask) -> torch.Tensor:
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q: torch.Tensor = self.c_q(x).view(B, T, self.n_head, -1)
        k: torch.Tensor = self.c_k(x).view(B, T, self.n_head, -1)
        v: torch.Tensor = self.c_v(x).view(B, T, self.n_head, -1)
        v = self.lambdas[0] * v + self.lambdas[1] * vi.view_as(v) # @Grad62304977
        q, k = norm(q), norm(k) # QK norm suggested by @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim: int):
        super().__init__()
        self.c_fc   = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config.n_embd, config.n_head)
        self.mlp = MLP(config.n_embd)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: torch.Tensor, vi: torch.Tensor, x0: torch.Tensor, block_mask: BlockMask) -> torch.Tensor:
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x = x + self.attn(norm(x), vi, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768
    lm_head_softcap : int = 30

class GPT(nn.Module):

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.n_layer = config.n_layer
        self.lm_head_softcap = config.lm_head_softcap

        # U-net design by @brendanh0gan
        self.num_encoder_layers = config.n_layer // 2 # Half of the layers for encoder
        self.num_decoder_layers = config.n_layer - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
            # U-net structure on token value embeddings by @leloykun
            vte = nn.Embedding(config.vocab_size, config.n_embd*self.num_encoder_layers),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = CastedLinear(config.n_embd, config.vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(self, idx: torch.Tensor, target: torch.Tensor, sliding_window: torch.Tensor) -> torch.Tensor:
        BLOCK_SIZE = 128
        assert idx.ndim == 1
        docs = (idx == 50256).cumsum(0)
        docs_low = docs.reshape(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.reshape(-1, BLOCK_SIZE)[:, -1].contiguous()
        def document_sliding_window_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            window_mask = q_idx - kv_idx < sliding_window
            return causal_mask & document_mask & window_mask

        S = len(idx)
        def create_sliding_window_causal_mask(S: int, sliding_window: torch.Tensor):
            kv_idx = block_idx = torch.arange(S // BLOCK_SIZE, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_mask = q_idx >= kv_idx
            document_mask = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            window_mask = q_idx - kv_idx < ((sliding_window + BLOCK_SIZE - 1) // BLOCK_SIZE)
            dense_mask = causal_mask & document_mask & window_mask
            dense_mask = dense_mask.to(torch.int32)
            num_blocks = dense_mask.sum(dim=-1).to(torch.int32)
            indices = torch.argsort(dense_mask, dim=-1, descending=True, stable=True).to(torch.int32)
            num_blocks = num_blocks[None, None, :].contiguous()
            indices = indices[None, None, :].contiguous()
            return BlockMask.from_kv_blocks(num_blocks, indices, BLOCK_SIZE=BLOCK_SIZE, mask_mod=document_sliding_window_causal)
        block_mask = create_sliding_window_causal_mask(S, sliding_window)

        # forward the GPT model itself
        x = self.transformer.wte(idx[None]) # token embeddings of shape (b, t, n_embd)
        x = norm(x) # @Grad62304977
        x0 = x
        vi = self.transformer.vte(idx[None]).chunk(self.num_encoder_layers, dim=-1)

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.transformer.h[i](x, vi[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.transformer.h[self.num_encoder_layers + i](x, vi[self.num_encoder_layers-1-i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = self.lm_head_softcap * torch.tanh(logits / self.lm_head_softcap) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(file: Path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32)
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    return int(header[2]) # number of tokens (claimed)

def _load_data_shard(file: Path, ntok: int):
    with file.open("rb") as f:
        tokens = torch.empty(ntok, dtype=torch.uint16, pin_memory=True)
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())
        assert nbytes == 2 * ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.T = T

        # glob files that match the pattern
        self.files = sorted(Path.cwd().glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        self.ntoks = [_peek_data_shard(file) for file in self.files]
        assert min(self.ntoks) >= num_processes * T + 1
        self.ntok_total = sum(self.ntoks)

        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard], self.ntoks[self.current_shard])

    def next_batch(self):
        batch_size = self.T * self.num_processes
        buf = self.tokens[self.current_position:self.current_position+self.T+1]
        # host side async is sufficient;
        # no performance improvement was observed when introducing a separate stream.
        x = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # inputs
        y = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # targets
        # advance current position and load next shard if necessary
        self.current_position += batch_size
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        return x, y

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8 # batch size, in sequences, across all devices
    sequence_length : int = 64*1024 # sequence length, in tokens
    num_iterations : int = 1480 # number of iterations to run
    warmup_iters : int = 0
    cooldown_iters : int = 600 # number of iterations of linear warmup/cooldown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    # os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write(code)
        f.write('='*100 + '\n')
def print0(s, logonly=False):
    if master_process:
        with open(logfile, "a") as f:
            if not logonly:
                print(s)
            f.write(s+'\n')
# log information about the hardware/software environment this is running on
# and print the full `nvidia-smi` to file
print0(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:")
import subprocess
result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
print0(f'{result.stdout}', logonly=True)
print0('='*100, logonly=True)

# convenience variables
T = args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (T * ddp_world_size) == 0
val_steps = args.val_tokens // (T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (ddp_world_size) == 0
train_accumulation_steps = args.batch_size // ddp_world_size
assert train_accumulation_steps == 1

# load tokens
train_loader = DistributedDataLoader(args.input_bin, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, T, ddp_rank, ddp_world_size)
print0(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
print0(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
print0('='*100, logonly=True)
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model

# init the optimizer(s)
optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight, raw_model.transformer.vte.weight], lr=0.6, betas=(0.8, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight], lr=0.008, betas=(0.8, 0.95), fused=True)
params = list(raw_model.transformer.h.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
optimizer3 = Muon(matrix_params, lr=0.05, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.8, 0.95), fused=True)
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and cooldown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.cooldown_iters:
        return 1.0
    # 3) linear cooldown
    else:
        decay_ratio = (args.num_iterations - it) / args.cooldown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

sliding_window_size = torch.tensor(64, dtype=torch.int32, device="cuda")
sw_size_prev = 64
# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Set the sliding window size for the current step, in chunks of 64. By @fernbear.bsky.social
    sw_size =  64 * int((64 + (1792 - 64) * step / args.num_iterations) // 64)
    if sw_size != sw_size_prev:
        sliding_window_size.copy_(sw_size, non_blocking=True)
        sw_size_prev = sw_size

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                x_val, y_val = val_loader.next_batch()
                val_loss += model(x_val, y_val, sliding_window=sliding_window_size)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        print0(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        # torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    loss = model(x, y, sliding_window=sliding_window_size)
    loss.backward()
    del loss
    # advance the dataset for the next batch
    x, y = train_loader.next_batch()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer3.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{args.num_iterations} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.6.0.dev20241203+cu124 compiled for CUDA 12.4
nvidia-smi:
Sun Dec  8 08:11:43 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.6     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:65:02.0 Off |                    0 |
| N/A   37C    P0              74W / 700W |      7MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:67:02.0 Off |                    0 |
| N/A   46C    P0             129W / 700W |    533MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:69:02.0 Off |                    0 |
| N/A   46C    P0             123W / 700W |    533MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:6B:02.0 Off |                    0 |
| N/A   39C    P0             118W / 700W |    533MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:6F:02.0 Off |                    0 |
| N/A   39C    P0             117W / 700W |    533MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:71:02.0 Off |                    0 |
| N/A   46C    P0             122W / 700W |    533MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:73:02.0 Off |                    0 |
| N/A   46C    P0             127W / 700W |    533MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:75:02.0 Off |                    0 |
| N/A   39C    P0             124W / 700W |    533MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training DataLoader: total number of tokens: 3200000000 across 32 files
Validation DataLoader: total number of tokens: 100000000 across 1 files
====================================================================================================
step:0/1480 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1480 train_time:22629ms step_avg:nanms
step:2/1480 train_time:22715ms step_avg:nanms
step:3/1480 train_time:22854ms step_avg:nanms
step:4/1480 train_time:22995ms step_avg:nanms
step:5/1480 train_time:23137ms step_avg:nanms
step:6/1480 train_time:23279ms step_avg:nanms
step:7/1480 train_time:23421ms step_avg:nanms
step:8/1480 train_time:23563ms step_avg:nanms
step:9/1480 train_time:23707ms step_avg:nanms
step:10/1480 train_time:23849ms step_avg:nanms
step:11/1480 train_time:142ms step_avg:nanms
step:12/1480 train_time:286ms step_avg:nanms
step:13/1480 train_time:430ms step_avg:143.46ms
step:14/1480 train_time:573ms step_avg:143.21ms
step:15/1480 train_time:714ms step_avg:142.84ms
step:16/1480 train_time:857ms step_avg:142.78ms
step:17/1480 train_time:998ms step_avg:142.61ms
step:18/1480 train_time:1141ms step_avg:142.63ms
step:19/1480 train_time:1284ms step_avg:142.64ms
step:20/1480 train_time:1429ms step_avg:142.90ms
step:21/1480 train_time:1572ms step_avg:142.91ms
step:22/1480 train_time:1714ms step_avg:142.83ms
step:23/1480 train_time:1856ms step_avg:142.76ms
step:24/1480 train_time:1999ms step_avg:142.79ms
step:25/1480 train_time:2143ms step_avg:142.87ms
step:26/1480 train_time:2287ms step_avg:142.91ms
step:27/1480 train_time:2431ms step_avg:142.97ms
step:28/1480 train_time:2573ms step_avg:142.95ms
step:29/1480 train_time:2715ms step_avg:142.87ms
step:30/1480 train_time:2856ms step_avg:142.79ms
step:31/1480 train_time:2997ms step_avg:142.70ms
step:32/1480 train_time:3140ms step_avg:142.72ms
step:33/1480 train_time:3283ms step_avg:142.75ms
step:34/1480 train_time:3427ms step_avg:142.80ms
step:35/1480 train_time:3570ms step_avg:142.80ms
step:36/1480 train_time:3713ms step_avg:142.80ms
step:37/1480 train_time:3854ms step_avg:142.75ms
step:38/1480 train_time:3996ms step_avg:142.70ms
step:39/1480 train_time:4139ms step_avg:142.74ms
step:40/1480 train_time:4284ms step_avg:142.81ms
step:41/1480 train_time:4429ms step_avg:142.87ms
step:42/1480 train_time:4572ms step_avg:142.88ms
step:43/1480 train_time:4714ms step_avg:142.84ms
step:44/1480 train_time:4856ms step_avg:142.81ms
step:45/1480 train_time:4996ms step_avg:142.75ms
step:46/1480 train_time:5137ms step_avg:142.70ms
step:47/1480 train_time:5279ms step_avg:142.68ms
step:48/1480 train_time:5421ms step_avg:142.66ms
step:49/1480 train_time:5563ms step_avg:142.63ms
step:50/1480 train_time:5707ms step_avg:142.68ms
step:51/1480 train_time:5850ms step_avg:142.69ms
step:52/1480 train_time:5992ms step_avg:142.67ms
step:53/1480 train_time:6134ms step_avg:142.64ms
step:54/1480 train_time:6276ms step_avg:142.64ms
step:55/1480 train_time:6418ms step_avg:142.62ms
step:56/1480 train_time:6559ms step_avg:142.58ms
step:57/1480 train_time:6702ms step_avg:142.60ms
step:58/1480 train_time:6844ms step_avg:142.58ms
step:59/1480 train_time:6987ms step_avg:142.58ms
step:60/1480 train_time:7130ms step_avg:142.59ms
step:61/1480 train_time:7271ms step_avg:142.57ms
step:62/1480 train_time:7414ms step_avg:142.58ms
step:63/1480 train_time:7556ms step_avg:142.56ms
step:64/1480 train_time:7697ms step_avg:142.53ms
step:65/1480 train_time:7840ms step_avg:142.55ms
step:66/1480 train_time:7984ms step_avg:142.57ms
step:67/1480 train_time:8128ms step_avg:142.59ms
step:68/1480 train_time:8272ms step_avg:142.62ms
step:69/1480 train_time:8414ms step_avg:142.61ms
step:70/1480 train_time:8556ms step_avg:142.59ms
step:71/1480 train_time:8696ms step_avg:142.56ms
step:72/1480 train_time:8838ms step_avg:142.55ms
step:73/1480 train_time:8980ms step_avg:142.54ms
step:74/1480 train_time:9123ms step_avg:142.55ms
step:75/1480 train_time:9268ms step_avg:142.58ms
step:76/1480 train_time:9411ms step_avg:142.59ms
step:77/1480 train_time:9553ms step_avg:142.59ms
step:78/1480 train_time:9695ms step_avg:142.57ms
step:79/1480 train_time:9838ms step_avg:142.58ms
step:80/1480 train_time:9982ms step_avg:142.60ms
step:81/1480 train_time:10125ms step_avg:142.60ms
step:82/1480 train_time:10267ms step_avg:142.59ms
step:83/1480 train_time:10410ms step_avg:142.61ms
step:84/1480 train_time:10554ms step_avg:142.62ms
step:85/1480 train_time:10695ms step_avg:142.60ms
step:86/1480 train_time:10838ms step_avg:142.60ms
step:87/1480 train_time:10980ms step_avg:142.60ms
step:88/1480 train_time:11124ms step_avg:142.61ms
step:89/1480 train_time:11267ms step_avg:142.62ms
step:90/1480 train_time:11412ms step_avg:142.65ms
step:91/1480 train_time:11555ms step_avg:142.66ms
step:92/1480 train_time:11696ms step_avg:142.64ms
step:93/1480 train_time:11840ms step_avg:142.65ms
step:94/1480 train_time:11982ms step_avg:142.64ms
step:95/1480 train_time:12123ms step_avg:142.62ms
step:96/1480 train_time:12264ms step_avg:142.60ms
step:97/1480 train_time:12407ms step_avg:142.61ms
step:98/1480 train_time:12551ms step_avg:142.62ms
step:99/1480 train_time:12693ms step_avg:142.62ms
step:100/1480 train_time:12836ms step_avg:142.62ms
step:101/1480 train_time:12978ms step_avg:142.62ms
step:102/1480 train_time:13121ms step_avg:142.62ms
step:103/1480 train_time:13262ms step_avg:142.60ms
step:104/1480 train_time:13405ms step_avg:142.61ms
step:105/1480 train_time:13548ms step_avg:142.61ms
step:106/1480 train_time:13690ms step_avg:142.61ms
step:107/1480 train_time:13834ms step_avg:142.62ms
step:108/1480 train_time:13976ms step_avg:142.61ms
step:109/1480 train_time:14118ms step_avg:142.61ms
step:110/1480 train_time:14261ms step_avg:142.61ms
step:111/1480 train_time:14409ms step_avg:142.66ms
step:112/1480 train_time:14557ms step_avg:142.71ms
step:113/1480 train_time:14702ms step_avg:142.74ms
step:114/1480 train_time:14849ms step_avg:142.78ms
step:115/1480 train_time:14996ms step_avg:142.82ms
step:116/1480 train_time:15142ms step_avg:142.85ms
step:117/1480 train_time:15289ms step_avg:142.88ms
step:118/1480 train_time:15437ms step_avg:142.94ms
step:119/1480 train_time:15584ms step_avg:142.97ms
step:120/1480 train_time:15732ms step_avg:143.01ms
step:121/1480 train_time:15878ms step_avg:143.04ms
step:122/1480 train_time:16023ms step_avg:143.06ms
step:123/1480 train_time:16169ms step_avg:143.09ms
step:124/1480 train_time:16316ms step_avg:143.13ms
step:125/1480 train_time:16463ms step_avg:143.16ms
step:125/1480 val_loss:4.4207 train_time:16520ms step_avg:143.65ms
step:126/1480 train_time:16615ms step_avg:143.23ms
step:127/1480 train_time:16765ms step_avg:143.29ms
step:128/1480 train_time:16912ms step_avg:143.32ms
step:129/1480 train_time:17057ms step_avg:143.34ms
step:130/1480 train_time:17203ms step_avg:143.35ms
step:131/1480 train_time:17349ms step_avg:143.38ms
step:132/1480 train_time:17494ms step_avg:143.40ms
step:133/1480 train_time:17644ms step_avg:143.45ms
step:134/1480 train_time:17792ms step_avg:143.48ms
step:135/1480 train_time:17937ms step_avg:143.50ms
step:136/1480 train_time:18084ms step_avg:143.52ms
step:137/1480 train_time:18230ms step_avg:143.54ms
step:138/1480 train_time:18374ms step_avg:143.55ms
step:139/1480 train_time:18521ms step_avg:143.57ms
step:140/1480 train_time:18669ms step_avg:143.61ms
step:141/1480 train_time:18815ms step_avg:143.63ms
step:142/1480 train_time:18963ms step_avg:143.66ms
step:143/1480 train_time:19110ms step_avg:143.68ms
step:144/1480 train_time:19256ms step_avg:143.70ms
step:145/1480 train_time:19403ms step_avg:143.73ms
step:146/1480 train_time:19551ms step_avg:143.76ms
step:147/1480 train_time:19697ms step_avg:143.77ms
step:148/1480 train_time:19845ms step_avg:143.81ms
step:149/1480 train_time:19991ms step_avg:143.82ms
step:150/1480 train_time:20137ms step_avg:143.84ms
step:151/1480 train_time:20286ms step_avg:143.87ms
step:152/1480 train_time:20432ms step_avg:143.88ms
step:153/1480 train_time:20578ms step_avg:143.90ms
step:154/1480 train_time:20725ms step_avg:143.93ms
step:155/1480 train_time:20872ms step_avg:143.95ms
step:156/1480 train_time:21019ms step_avg:143.96ms
step:157/1480 train_time:21167ms step_avg:143.99ms
step:158/1480 train_time:21314ms step_avg:144.01ms
step:159/1480 train_time:21461ms step_avg:144.03ms
step:160/1480 train_time:21608ms step_avg:144.05ms
step:161/1480 train_time:21754ms step_avg:144.07ms
step:162/1480 train_time:21900ms step_avg:144.08ms
step:163/1480 train_time:22048ms step_avg:144.10ms
step:164/1480 train_time:22193ms step_avg:144.11ms
step:165/1480 train_time:22340ms step_avg:144.13ms
step:166/1480 train_time:22488ms step_avg:144.15ms
step:167/1480 train_time:22633ms step_avg:144.16ms
step:168/1480 train_time:22780ms step_avg:144.18ms
step:169/1480 train_time:22927ms step_avg:144.20ms
step:170/1480 train_time:23073ms step_avg:144.21ms
step:171/1480 train_time:23220ms step_avg:144.22ms
step:172/1480 train_time:23368ms step_avg:144.24ms
step:173/1480 train_time:23515ms step_avg:144.26ms
step:174/1480 train_time:23662ms step_avg:144.28ms
step:175/1480 train_time:23809ms step_avg:144.30ms
step:176/1480 train_time:23955ms step_avg:144.31ms
step:177/1480 train_time:24101ms step_avg:144.32ms
step:178/1480 train_time:24249ms step_avg:144.34ms
step:179/1480 train_time:24394ms step_avg:144.35ms
step:180/1480 train_time:24543ms step_avg:144.37ms
step:181/1480 train_time:24690ms step_avg:144.39ms
step:182/1480 train_time:24836ms step_avg:144.40ms
step:183/1480 train_time:24984ms step_avg:144.41ms
step:184/1480 train_time:25130ms step_avg:144.42ms
step:185/1480 train_time:25276ms step_avg:144.44ms
step:186/1480 train_time:25424ms step_avg:144.45ms
step:187/1480 train_time:25572ms step_avg:144.47ms
step:188/1480 train_time:25717ms step_avg:144.48ms
step:189/1480 train_time:25865ms step_avg:144.50ms
step:190/1480 train_time:26011ms step_avg:144.51ms
step:191/1480 train_time:26156ms step_avg:144.51ms
step:192/1480 train_time:26304ms step_avg:144.53ms
step:193/1480 train_time:26451ms step_avg:144.54ms
step:194/1480 train_time:26597ms step_avg:144.55ms
step:195/1480 train_time:26746ms step_avg:144.57ms
step:196/1480 train_time:26893ms step_avg:144.59ms
step:197/1480 train_time:27040ms step_avg:144.60ms
step:198/1480 train_time:27187ms step_avg:144.61ms
step:199/1480 train_time:27332ms step_avg:144.61ms
step:200/1480 train_time:27479ms step_avg:144.63ms
step:201/1480 train_time:27627ms step_avg:144.64ms
step:202/1480 train_time:27774ms step_avg:144.65ms
step:203/1480 train_time:27922ms step_avg:144.67ms
step:204/1480 train_time:28069ms step_avg:144.69ms
step:205/1480 train_time:28215ms step_avg:144.69ms
step:206/1480 train_time:28362ms step_avg:144.71ms
step:207/1480 train_time:28509ms step_avg:144.72ms
step:208/1480 train_time:28654ms step_avg:144.72ms
step:209/1480 train_time:28801ms step_avg:144.73ms
step:210/1480 train_time:28950ms step_avg:144.75ms
step:211/1480 train_time:29095ms step_avg:144.75ms
step:212/1480 train_time:29243ms step_avg:144.77ms
step:213/1480 train_time:29391ms step_avg:144.78ms
step:214/1480 train_time:29536ms step_avg:144.79ms
step:215/1480 train_time:29683ms step_avg:144.80ms
step:216/1480 train_time:29830ms step_avg:144.81ms
step:217/1480 train_time:29978ms step_avg:144.82ms
step:218/1480 train_time:30126ms step_avg:144.84ms
step:219/1480 train_time:30273ms step_avg:144.85ms
step:220/1480 train_time:30421ms step_avg:144.86ms
step:221/1480 train_time:30569ms step_avg:144.88ms
step:222/1480 train_time:30719ms step_avg:144.90ms
step:223/1480 train_time:30870ms step_avg:144.93ms
step:224/1480 train_time:31019ms step_avg:144.95ms
step:225/1480 train_time:31171ms step_avg:144.98ms
step:226/1480 train_time:31321ms step_avg:145.01ms
step:227/1480 train_time:31472ms step_avg:145.03ms
step:228/1480 train_time:31622ms step_avg:145.06ms
step:229/1480 train_time:31772ms step_avg:145.08ms
step:230/1480 train_time:31923ms step_avg:145.10ms
step:231/1480 train_time:32073ms step_avg:145.13ms
step:232/1480 train_time:32223ms step_avg:145.15ms
step:233/1480 train_time:32374ms step_avg:145.17ms
step:234/1480 train_time:32525ms step_avg:145.20ms
step:235/1480 train_time:32676ms step_avg:145.22ms
step:236/1480 train_time:32826ms step_avg:145.25ms
step:237/1480 train_time:32976ms step_avg:145.27ms
step:238/1480 train_time:33126ms step_avg:145.29ms
step:239/1480 train_time:33276ms step_avg:145.31ms
step:240/1480 train_time:33426ms step_avg:145.33ms
step:241/1480 train_time:33576ms step_avg:145.35ms
step:242/1480 train_time:33727ms step_avg:145.37ms
step:243/1480 train_time:33876ms step_avg:145.39ms
step:244/1480 train_time:34026ms step_avg:145.41ms
step:245/1480 train_time:34177ms step_avg:145.43ms
step:246/1480 train_time:34327ms step_avg:145.45ms
step:247/1480 train_time:34476ms step_avg:145.47ms
step:248/1480 train_time:34628ms step_avg:145.49ms
step:249/1480 train_time:34778ms step_avg:145.52ms
step:250/1480 train_time:34929ms step_avg:145.54ms
step:250/1480 val_loss:3.9914 train_time:34986ms step_avg:145.78ms
step:251/1480 train_time:35084ms step_avg:145.58ms
step:252/1480 train_time:35236ms step_avg:145.60ms
step:253/1480 train_time:35386ms step_avg:145.62ms
step:254/1480 train_time:35535ms step_avg:145.64ms
step:255/1480 train_time:35685ms step_avg:145.65ms
step:256/1480 train_time:35834ms step_avg:145.67ms
step:257/1480 train_time:35985ms step_avg:145.69ms
step:258/1480 train_time:36136ms step_avg:145.71ms
step:259/1480 train_time:36288ms step_avg:145.74ms
step:260/1480 train_time:36440ms step_avg:145.76ms
step:261/1480 train_time:36589ms step_avg:145.77ms
step:262/1480 train_time:36739ms step_avg:145.79ms
step:263/1480 train_time:36889ms step_avg:145.81ms
step:264/1480 train_time:37041ms step_avg:145.83ms
step:265/1480 train_time:37192ms step_avg:145.85ms
step:266/1480 train_time:37343ms step_avg:145.87ms
step:267/1480 train_time:37494ms step_avg:145.89ms
step:268/1480 train_time:37645ms step_avg:145.91ms
step:269/1480 train_time:37795ms step_avg:145.93ms
step:270/1480 train_time:37946ms step_avg:145.94ms
step:271/1480 train_time:38097ms step_avg:145.97ms
step:272/1480 train_time:38248ms step_avg:145.98ms
step:273/1480 train_time:38399ms step_avg:146.00ms
step:274/1480 train_time:38549ms step_avg:146.02ms
step:275/1480 train_time:38700ms step_avg:146.04ms
step:276/1480 train_time:38850ms step_avg:146.05ms
step:277/1480 train_time:39000ms step_avg:146.07ms
step:278/1480 train_time:39150ms step_avg:146.08ms
step:279/1480 train_time:39301ms step_avg:146.10ms
step:280/1480 train_time:39451ms step_avg:146.12ms
step:281/1480 train_time:39603ms step_avg:146.14ms
step:282/1480 train_time:39753ms step_avg:146.15ms
step:283/1480 train_time:39904ms step_avg:146.17ms
step:284/1480 train_time:40054ms step_avg:146.18ms
step:285/1480 train_time:40205ms step_avg:146.20ms
step:286/1480 train_time:40356ms step_avg:146.22ms
step:287/1480 train_time:40507ms step_avg:146.23ms
step:288/1480 train_time:40658ms step_avg:146.25ms
step:289/1480 train_time:40808ms step_avg:146.27ms
step:290/1480 train_time:40961ms step_avg:146.29ms
step:291/1480 train_time:41110ms step_avg:146.30ms
step:292/1480 train_time:41261ms step_avg:146.32ms
step:293/1480 train_time:41411ms step_avg:146.33ms
step:294/1480 train_time:41562ms step_avg:146.35ms
step:295/1480 train_time:41712ms step_avg:146.36ms
step:296/1480 train_time:41865ms step_avg:146.38ms
step:297/1480 train_time:42015ms step_avg:146.40ms
step:298/1480 train_time:42166ms step_avg:146.41ms
step:299/1480 train_time:42314ms step_avg:146.42ms
step:300/1480 train_time:42465ms step_avg:146.43ms
step:301/1480 train_time:42615ms step_avg:146.44ms
step:302/1480 train_time:42766ms step_avg:146.46ms
step:303/1480 train_time:42915ms step_avg:146.47ms
step:304/1480 train_time:43066ms step_avg:146.48ms
step:305/1480 train_time:43216ms step_avg:146.49ms
step:306/1480 train_time:43366ms step_avg:146.51ms
step:307/1480 train_time:43516ms step_avg:146.52ms
step:308/1480 train_time:43667ms step_avg:146.53ms
step:309/1480 train_time:43817ms step_avg:146.55ms
step:310/1480 train_time:43969ms step_avg:146.56ms
step:311/1480 train_time:44120ms step_avg:146.58ms
step:312/1480 train_time:44270ms step_avg:146.59ms
step:313/1480 train_time:44421ms step_avg:146.60ms
step:314/1480 train_time:44571ms step_avg:146.61ms
step:315/1480 train_time:44721ms step_avg:146.63ms
step:316/1480 train_time:44870ms step_avg:146.63ms
step:317/1480 train_time:45021ms step_avg:146.65ms
step:318/1480 train_time:45171ms step_avg:146.66ms
step:319/1480 train_time:45322ms step_avg:146.67ms
step:320/1480 train_time:45473ms step_avg:146.69ms
step:321/1480 train_time:45623ms step_avg:146.70ms
step:322/1480 train_time:45772ms step_avg:146.71ms
step:323/1480 train_time:45923ms step_avg:146.72ms
step:324/1480 train_time:46074ms step_avg:146.73ms
step:325/1480 train_time:46225ms step_avg:146.74ms
step:326/1480 train_time:46375ms step_avg:146.76ms
step:327/1480 train_time:46525ms step_avg:146.77ms
step:328/1480 train_time:46678ms step_avg:146.79ms
step:329/1480 train_time:46825ms step_avg:146.79ms
step:330/1480 train_time:46978ms step_avg:146.81ms
step:331/1480 train_time:47131ms step_avg:146.83ms
step:332/1480 train_time:47285ms step_avg:146.85ms
step:333/1480 train_time:47439ms step_avg:146.87ms
step:334/1480 train_time:47593ms step_avg:146.89ms
step:335/1480 train_time:47747ms step_avg:146.91ms
step:336/1480 train_time:47903ms step_avg:146.94ms
step:337/1480 train_time:48057ms step_avg:146.96ms
step:338/1480 train_time:48210ms step_avg:146.98ms
step:339/1480 train_time:48365ms step_avg:147.01ms
step:340/1480 train_time:48518ms step_avg:147.02ms
step:341/1480 train_time:48672ms step_avg:147.04ms
step:342/1480 train_time:48825ms step_avg:147.06ms
step:343/1480 train_time:48979ms step_avg:147.08ms
step:344/1480 train_time:49134ms step_avg:147.11ms
step:345/1480 train_time:49290ms step_avg:147.13ms
step:346/1480 train_time:49444ms step_avg:147.16ms
step:347/1480 train_time:49599ms step_avg:147.18ms
step:348/1480 train_time:49753ms step_avg:147.20ms
step:349/1480 train_time:49906ms step_avg:147.21ms
step:350/1480 train_time:50060ms step_avg:147.23ms
step:351/1480 train_time:50214ms step_avg:147.26ms
step:352/1480 train_time:50368ms step_avg:147.28ms
step:353/1480 train_time:50523ms step_avg:147.30ms
step:354/1480 train_time:50677ms step_avg:147.32ms
step:355/1480 train_time:50831ms step_avg:147.34ms
step:356/1480 train_time:50985ms step_avg:147.36ms
step:357/1480 train_time:51139ms step_avg:147.37ms
step:358/1480 train_time:51294ms step_avg:147.40ms
step:359/1480 train_time:51450ms step_avg:147.42ms
step:360/1480 train_time:51606ms step_avg:147.44ms
step:361/1480 train_time:51760ms step_avg:147.47ms
step:362/1480 train_time:51914ms step_avg:147.48ms
step:363/1480 train_time:52067ms step_avg:147.50ms
step:364/1480 train_time:52221ms step_avg:147.52ms
step:365/1480 train_time:52376ms step_avg:147.54ms
step:366/1480 train_time:52529ms step_avg:147.55ms
step:367/1480 train_time:52684ms step_avg:147.57ms
step:368/1480 train_time:52838ms step_avg:147.59ms
step:369/1480 train_time:52992ms step_avg:147.61ms
step:370/1480 train_time:53144ms step_avg:147.62ms
step:371/1480 train_time:53298ms step_avg:147.64ms
step:372/1480 train_time:53453ms step_avg:147.66ms
step:373/1480 train_time:53606ms step_avg:147.68ms
step:374/1480 train_time:53759ms step_avg:147.69ms
step:375/1480 train_time:53911ms step_avg:147.70ms
step:375/1480 val_loss:3.8023 train_time:53973ms step_avg:147.87ms
step:376/1480 train_time:54075ms step_avg:147.75ms
step:377/1480 train_time:54230ms step_avg:147.77ms
step:378/1480 train_time:54383ms step_avg:147.78ms
step:379/1480 train_time:54535ms step_avg:147.79ms
step:380/1480 train_time:54687ms step_avg:147.80ms
step:381/1480 train_time:54839ms step_avg:147.81ms
step:382/1480 train_time:54993ms step_avg:147.83ms
step:383/1480 train_time:55149ms step_avg:147.85ms
step:384/1480 train_time:55302ms step_avg:147.87ms
step:385/1480 train_time:55458ms step_avg:147.89ms
step:386/1480 train_time:55610ms step_avg:147.90ms
step:387/1480 train_time:55764ms step_avg:147.92ms
step:388/1480 train_time:55917ms step_avg:147.93ms
step:389/1480 train_time:56071ms step_avg:147.94ms
step:390/1480 train_time:56226ms step_avg:147.96ms
step:391/1480 train_time:56380ms step_avg:147.98ms
step:392/1480 train_time:56535ms step_avg:148.00ms
step:393/1480 train_time:56688ms step_avg:148.01ms
step:394/1480 train_time:56841ms step_avg:148.02ms
step:395/1480 train_time:56995ms step_avg:148.04ms
step:396/1480 train_time:57148ms step_avg:148.05ms
step:397/1480 train_time:57301ms step_avg:148.06ms
step:398/1480 train_time:57455ms step_avg:148.08ms
step:399/1480 train_time:57609ms step_avg:148.09ms
step:400/1480 train_time:57763ms step_avg:148.11ms
step:401/1480 train_time:57916ms step_avg:148.12ms
step:402/1480 train_time:58070ms step_avg:148.14ms
step:403/1480 train_time:58223ms step_avg:148.15ms
step:404/1480 train_time:58377ms step_avg:148.16ms
step:405/1480 train_time:58530ms step_avg:148.18ms
step:406/1480 train_time:58685ms step_avg:148.19ms
step:407/1480 train_time:58838ms step_avg:148.21ms
step:408/1480 train_time:58992ms step_avg:148.22ms
step:409/1480 train_time:59146ms step_avg:148.24ms
step:410/1480 train_time:59300ms step_avg:148.25ms
step:411/1480 train_time:59454ms step_avg:148.26ms
step:412/1480 train_time:59608ms step_avg:148.28ms
step:413/1480 train_time:59762ms step_avg:148.29ms
step:414/1480 train_time:59916ms step_avg:148.31ms
step:415/1480 train_time:60071ms step_avg:148.32ms
step:416/1480 train_time:60224ms step_avg:148.34ms
step:417/1480 train_time:60378ms step_avg:148.35ms
step:418/1480 train_time:60531ms step_avg:148.36ms
step:419/1480 train_time:60685ms step_avg:148.37ms
step:420/1480 train_time:60838ms step_avg:148.39ms
step:421/1480 train_time:60992ms step_avg:148.40ms
step:422/1480 train_time:61146ms step_avg:148.41ms
step:423/1480 train_time:61300ms step_avg:148.43ms
step:424/1480 train_time:61456ms step_avg:148.44ms
step:425/1480 train_time:61610ms step_avg:148.46ms
step:426/1480 train_time:61764ms step_avg:148.47ms
step:427/1480 train_time:61917ms step_avg:148.48ms
step:428/1480 train_time:62069ms step_avg:148.49ms
step:429/1480 train_time:62222ms step_avg:148.50ms
step:430/1480 train_time:62377ms step_avg:148.52ms
step:431/1480 train_time:62531ms step_avg:148.53ms
step:432/1480 train_time:62686ms step_avg:148.54ms
step:433/1480 train_time:62838ms step_avg:148.55ms
step:434/1480 train_time:62992ms step_avg:148.57ms
step:435/1480 train_time:63145ms step_avg:148.58ms
step:436/1480 train_time:63299ms step_avg:148.59ms
step:437/1480 train_time:63453ms step_avg:148.60ms
step:438/1480 train_time:63607ms step_avg:148.61ms
step:439/1480 train_time:63762ms step_avg:148.63ms
step:440/1480 train_time:63917ms step_avg:148.64ms
step:441/1480 train_time:64073ms step_avg:148.66ms
step:442/1480 train_time:64231ms step_avg:148.68ms
step:443/1480 train_time:64389ms step_avg:148.71ms
step:444/1480 train_time:64546ms step_avg:148.72ms
step:445/1480 train_time:64701ms step_avg:148.74ms
step:446/1480 train_time:64857ms step_avg:148.75ms
step:447/1480 train_time:65013ms step_avg:148.77ms
step:448/1480 train_time:65169ms step_avg:148.79ms
step:449/1480 train_time:65327ms step_avg:148.81ms
step:450/1480 train_time:65485ms step_avg:148.83ms
step:451/1480 train_time:65644ms step_avg:148.85ms
step:452/1480 train_time:65800ms step_avg:148.87ms
step:453/1480 train_time:65956ms step_avg:148.89ms
step:454/1480 train_time:66114ms step_avg:148.90ms
step:455/1480 train_time:66270ms step_avg:148.92ms
step:456/1480 train_time:66425ms step_avg:148.93ms
step:457/1480 train_time:66582ms step_avg:148.95ms
step:458/1480 train_time:66739ms step_avg:148.97ms
step:459/1480 train_time:66896ms step_avg:148.99ms
step:460/1480 train_time:67054ms step_avg:149.01ms
step:461/1480 train_time:67212ms step_avg:149.03ms
step:462/1480 train_time:67370ms step_avg:149.05ms
step:463/1480 train_time:67526ms step_avg:149.07ms
step:464/1480 train_time:67683ms step_avg:149.08ms
step:465/1480 train_time:67839ms step_avg:149.10ms
step:466/1480 train_time:67997ms step_avg:149.12ms
step:467/1480 train_time:68156ms step_avg:149.14ms
step:468/1480 train_time:68313ms step_avg:149.16ms
step:469/1480 train_time:68471ms step_avg:149.17ms
step:470/1480 train_time:68630ms step_avg:149.20ms
step:471/1480 train_time:68787ms step_avg:149.21ms
step:472/1480 train_time:68944ms step_avg:149.23ms
step:473/1480 train_time:69100ms step_avg:149.24ms
step:474/1480 train_time:69256ms step_avg:149.26ms
step:475/1480 train_time:69413ms step_avg:149.28ms
step:476/1480 train_time:69571ms step_avg:149.29ms
step:477/1480 train_time:69727ms step_avg:149.31ms
step:478/1480 train_time:69883ms step_avg:149.32ms
step:479/1480 train_time:70040ms step_avg:149.34ms
step:480/1480 train_time:70198ms step_avg:149.36ms
step:481/1480 train_time:70355ms step_avg:149.37ms
step:482/1480 train_time:70512ms step_avg:149.39ms
step:483/1480 train_time:70667ms step_avg:149.40ms
step:484/1480 train_time:70822ms step_avg:149.41ms
step:485/1480 train_time:70980ms step_avg:149.43ms
step:486/1480 train_time:71137ms step_avg:149.45ms
step:487/1480 train_time:71296ms step_avg:149.47ms
step:488/1480 train_time:71454ms step_avg:149.49ms
step:489/1480 train_time:71610ms step_avg:149.50ms
step:490/1480 train_time:71767ms step_avg:149.51ms
step:491/1480 train_time:71923ms step_avg:149.53ms
step:492/1480 train_time:72079ms step_avg:149.54ms
step:493/1480 train_time:72236ms step_avg:149.56ms
step:494/1480 train_time:72394ms step_avg:149.57ms
step:495/1480 train_time:72553ms step_avg:149.59ms
step:496/1480 train_time:72711ms step_avg:149.61ms
step:497/1480 train_time:72868ms step_avg:149.63ms
step:498/1480 train_time:73025ms step_avg:149.64ms
step:499/1480 train_time:73182ms step_avg:149.66ms
step:500/1480 train_time:73339ms step_avg:149.67ms
step:500/1480 val_loss:3.6809 train_time:73400ms step_avg:149.80ms
step:501/1480 train_time:73496ms step_avg:149.69ms
step:502/1480 train_time:73654ms step_avg:149.70ms
step:503/1480 train_time:73810ms step_avg:149.72ms
step:504/1480 train_time:73966ms step_avg:149.73ms
step:505/1480 train_time:74122ms step_avg:149.74ms
step:506/1480 train_time:74280ms step_avg:149.76ms
step:507/1480 train_time:74438ms step_avg:149.77ms
step:508/1480 train_time:74595ms step_avg:149.79ms
step:509/1480 train_time:74752ms step_avg:149.80ms
step:510/1480 train_time:74908ms step_avg:149.82ms
step:511/1480 train_time:75065ms step_avg:149.83ms
step:512/1480 train_time:75223ms step_avg:149.85ms
step:513/1480 train_time:75380ms step_avg:149.86ms
step:514/1480 train_time:75537ms step_avg:149.88ms
step:515/1480 train_time:75695ms step_avg:149.89ms
step:516/1480 train_time:75855ms step_avg:149.91ms
step:517/1480 train_time:76012ms step_avg:149.92ms
step:518/1480 train_time:76168ms step_avg:149.94ms
step:519/1480 train_time:76324ms step_avg:149.95ms
step:520/1480 train_time:76480ms step_avg:149.96ms
step:521/1480 train_time:76637ms step_avg:149.97ms
step:522/1480 train_time:76794ms step_avg:149.99ms
step:523/1480 train_time:76950ms step_avg:150.00ms
step:524/1480 train_time:77108ms step_avg:150.01ms
step:525/1480 train_time:77265ms step_avg:150.03ms
step:526/1480 train_time:77422ms step_avg:150.04ms
step:527/1480 train_time:77579ms step_avg:150.06ms
step:528/1480 train_time:77737ms step_avg:150.07ms
step:529/1480 train_time:77893ms step_avg:150.08ms
step:530/1480 train_time:78049ms step_avg:150.09ms
step:531/1480 train_time:78206ms step_avg:150.11ms
step:532/1480 train_time:78363ms step_avg:150.12ms
step:533/1480 train_time:78520ms step_avg:150.13ms
step:534/1480 train_time:78676ms step_avg:150.14ms
step:535/1480 train_time:78833ms step_avg:150.16ms
step:536/1480 train_time:78990ms step_avg:150.17ms
step:537/1480 train_time:79147ms step_avg:150.18ms
step:538/1480 train_time:79303ms step_avg:150.19ms
step:539/1480 train_time:79461ms step_avg:150.21ms
step:540/1480 train_time:79619ms step_avg:150.22ms
step:541/1480 train_time:79776ms step_avg:150.24ms
step:542/1480 train_time:79933ms step_avg:150.25ms
step:543/1480 train_time:80090ms step_avg:150.26ms
step:544/1480 train_time:80247ms step_avg:150.28ms
step:545/1480 train_time:80403ms step_avg:150.29ms
step:546/1480 train_time:80559ms step_avg:150.30ms
step:547/1480 train_time:80715ms step_avg:150.31ms
step:548/1480 train_time:80872ms step_avg:150.32ms
step:549/1480 train_time:81027ms step_avg:150.33ms
step:550/1480 train_time:81186ms step_avg:150.34ms
step:551/1480 train_time:81345ms step_avg:150.36ms
step:552/1480 train_time:81504ms step_avg:150.38ms
step:553/1480 train_time:81666ms step_avg:150.40ms
step:554/1480 train_time:81826ms step_avg:150.42ms
step:555/1480 train_time:81987ms step_avg:150.44ms
step:556/1480 train_time:82146ms step_avg:150.45ms
step:557/1480 train_time:82305ms step_avg:150.47ms
step:558/1480 train_time:82465ms step_avg:150.48ms
step:559/1480 train_time:82625ms step_avg:150.50ms
step:560/1480 train_time:82786ms step_avg:150.52ms
step:561/1480 train_time:82946ms step_avg:150.54ms
step:562/1480 train_time:83105ms step_avg:150.55ms
step:563/1480 train_time:83264ms step_avg:150.57ms
step:564/1480 train_time:83423ms step_avg:150.58ms
step:565/1480 train_time:83583ms step_avg:150.60ms
step:566/1480 train_time:83743ms step_avg:150.62ms
step:567/1480 train_time:83903ms step_avg:150.63ms
step:568/1480 train_time:84065ms step_avg:150.65ms
step:569/1480 train_time:84224ms step_avg:150.67ms
step:570/1480 train_time:84384ms step_avg:150.69ms
step:571/1480 train_time:84544ms step_avg:150.70ms
step:572/1480 train_time:84703ms step_avg:150.72ms
step:573/1480 train_time:84864ms step_avg:150.73ms
step:574/1480 train_time:85024ms step_avg:150.75ms
step:575/1480 train_time:85187ms step_avg:150.77ms
step:576/1480 train_time:85346ms step_avg:150.79ms
step:577/1480 train_time:85505ms step_avg:150.80ms
step:578/1480 train_time:85664ms step_avg:150.82ms
step:579/1480 train_time:85824ms step_avg:150.83ms
step:580/1480 train_time:85984ms step_avg:150.85ms
step:581/1480 train_time:86144ms step_avg:150.87ms
step:582/1480 train_time:86306ms step_avg:150.88ms
step:583/1480 train_time:86465ms step_avg:150.90ms
step:584/1480 train_time:86624ms step_avg:150.91ms
step:585/1480 train_time:86784ms step_avg:150.93ms
step:586/1480 train_time:86944ms step_avg:150.94ms
step:587/1480 train_time:87104ms step_avg:150.96ms
step:588/1480 train_time:87263ms step_avg:150.97ms
step:589/1480 train_time:87424ms step_avg:150.99ms
step:590/1480 train_time:87585ms step_avg:151.01ms
step:591/1480 train_time:87745ms step_avg:151.02ms
step:592/1480 train_time:87904ms step_avg:151.04ms
step:593/1480 train_time:88066ms step_avg:151.06ms
step:594/1480 train_time:88226ms step_avg:151.07ms
step:595/1480 train_time:88388ms step_avg:151.09ms
step:596/1480 train_time:88549ms step_avg:151.11ms
step:597/1480 train_time:88707ms step_avg:151.12ms
step:598/1480 train_time:88865ms step_avg:151.13ms
step:599/1480 train_time:89025ms step_avg:151.15ms
step:600/1480 train_time:89186ms step_avg:151.16ms
step:601/1480 train_time:89344ms step_avg:151.17ms
step:602/1480 train_time:89504ms step_avg:151.19ms
step:603/1480 train_time:89665ms step_avg:151.21ms
step:604/1480 train_time:89825ms step_avg:151.22ms
step:605/1480 train_time:89985ms step_avg:151.24ms
step:606/1480 train_time:90147ms step_avg:151.25ms
step:607/1480 train_time:90307ms step_avg:151.27ms
step:608/1480 train_time:90466ms step_avg:151.28ms
step:609/1480 train_time:90625ms step_avg:151.29ms
step:610/1480 train_time:90784ms step_avg:151.31ms
step:611/1480 train_time:90945ms step_avg:151.32ms
step:612/1480 train_time:91104ms step_avg:151.34ms
step:613/1480 train_time:91266ms step_avg:151.35ms
step:614/1480 train_time:91426ms step_avg:151.37ms
step:615/1480 train_time:91585ms step_avg:151.38ms
step:616/1480 train_time:91744ms step_avg:151.39ms
step:617/1480 train_time:91904ms step_avg:151.41ms
step:618/1480 train_time:92064ms step_avg:151.42ms
step:619/1480 train_time:92224ms step_avg:151.43ms
step:620/1480 train_time:92385ms step_avg:151.45ms
step:621/1480 train_time:92545ms step_avg:151.47ms
step:622/1480 train_time:92705ms step_avg:151.48ms
step:623/1480 train_time:92866ms step_avg:151.49ms
step:624/1480 train_time:93026ms step_avg:151.51ms
step:625/1480 train_time:93186ms step_avg:151.52ms
step:625/1480 val_loss:3.6005 train_time:93249ms step_avg:151.63ms
step:626/1480 train_time:93349ms step_avg:151.54ms
step:627/1480 train_time:93510ms step_avg:151.56ms
step:628/1480 train_time:93669ms step_avg:151.57ms
step:629/1480 train_time:93827ms step_avg:151.58ms
step:630/1480 train_time:93986ms step_avg:151.59ms
step:631/1480 train_time:94144ms step_avg:151.60ms
step:632/1480 train_time:94303ms step_avg:151.61ms
step:633/1480 train_time:94464ms step_avg:151.63ms
step:634/1480 train_time:94624ms step_avg:151.64ms
step:635/1480 train_time:94783ms step_avg:151.65ms
step:636/1480 train_time:94944ms step_avg:151.67ms
step:637/1480 train_time:95104ms step_avg:151.68ms
step:638/1480 train_time:95261ms step_avg:151.69ms
step:639/1480 train_time:95421ms step_avg:151.70ms
step:640/1480 train_time:95580ms step_avg:151.71ms
step:641/1480 train_time:95737ms step_avg:151.72ms
step:642/1480 train_time:95896ms step_avg:151.73ms
step:643/1480 train_time:96055ms step_avg:151.75ms
step:644/1480 train_time:96213ms step_avg:151.76ms
step:645/1480 train_time:96371ms step_avg:151.77ms
step:646/1480 train_time:96532ms step_avg:151.78ms
step:647/1480 train_time:96692ms step_avg:151.79ms
step:648/1480 train_time:96853ms step_avg:151.81ms
step:649/1480 train_time:97013ms step_avg:151.82ms
step:650/1480 train_time:97172ms step_avg:151.83ms
step:651/1480 train_time:97332ms step_avg:151.84ms
step:652/1480 train_time:97493ms step_avg:151.86ms
step:653/1480 train_time:97652ms step_avg:151.87ms
step:654/1480 train_time:97813ms step_avg:151.88ms
step:655/1480 train_time:97972ms step_avg:151.89ms
step:656/1480 train_time:98133ms step_avg:151.91ms
step:657/1480 train_time:98292ms step_avg:151.92ms
step:658/1480 train_time:98452ms step_avg:151.93ms
step:659/1480 train_time:98614ms step_avg:151.95ms
step:660/1480 train_time:98776ms step_avg:151.96ms
step:661/1480 train_time:98937ms step_avg:151.98ms
step:662/1480 train_time:99097ms step_avg:151.99ms
step:663/1480 train_time:99256ms step_avg:152.00ms
step:664/1480 train_time:99417ms step_avg:152.01ms
step:665/1480 train_time:99579ms step_avg:152.03ms
step:666/1480 train_time:99739ms step_avg:152.04ms
step:667/1480 train_time:99899ms step_avg:152.05ms
step:668/1480 train_time:100061ms step_avg:152.07ms
step:669/1480 train_time:100223ms step_avg:152.08ms
step:670/1480 train_time:100383ms step_avg:152.10ms
step:671/1480 train_time:100545ms step_avg:152.11ms
step:672/1480 train_time:100707ms step_avg:152.13ms
step:673/1480 train_time:100871ms step_avg:152.14ms
step:674/1480 train_time:101035ms step_avg:152.16ms
step:675/1480 train_time:101197ms step_avg:152.18ms
step:676/1480 train_time:101358ms step_avg:152.19ms
step:677/1480 train_time:101519ms step_avg:152.20ms
step:678/1480 train_time:101679ms step_avg:152.21ms
step:679/1480 train_time:101840ms step_avg:152.23ms
step:680/1480 train_time:102004ms step_avg:152.24ms
step:681/1480 train_time:102166ms step_avg:152.26ms
step:682/1480 train_time:102329ms step_avg:152.28ms
step:683/1480 train_time:102492ms step_avg:152.29ms
step:684/1480 train_time:102654ms step_avg:152.31ms
step:685/1480 train_time:102817ms step_avg:152.32ms
step:686/1480 train_time:102979ms step_avg:152.34ms
step:687/1480 train_time:103139ms step_avg:152.35ms
step:688/1480 train_time:103302ms step_avg:152.36ms
step:689/1480 train_time:103464ms step_avg:152.38ms
step:690/1480 train_time:103629ms step_avg:152.40ms
step:691/1480 train_time:103792ms step_avg:152.41ms
step:692/1480 train_time:103954ms step_avg:152.43ms
step:693/1480 train_time:104116ms step_avg:152.44ms
step:694/1480 train_time:104276ms step_avg:152.45ms
step:695/1480 train_time:104436ms step_avg:152.46ms
step:696/1480 train_time:104597ms step_avg:152.47ms
step:697/1480 train_time:104759ms step_avg:152.49ms
step:698/1480 train_time:104920ms step_avg:152.50ms
step:699/1480 train_time:105082ms step_avg:152.51ms
step:700/1480 train_time:105243ms step_avg:152.53ms
step:701/1480 train_time:105404ms step_avg:152.54ms
step:702/1480 train_time:105566ms step_avg:152.55ms
step:703/1480 train_time:105727ms step_avg:152.56ms
step:704/1480 train_time:105889ms step_avg:152.58ms
step:705/1480 train_time:106052ms step_avg:152.59ms
step:706/1480 train_time:106215ms step_avg:152.61ms
step:707/1480 train_time:106376ms step_avg:152.62ms
step:708/1480 train_time:106536ms step_avg:152.63ms
step:709/1480 train_time:106696ms step_avg:152.64ms
step:710/1480 train_time:106857ms step_avg:152.65ms
step:711/1480 train_time:107020ms step_avg:152.67ms
step:712/1480 train_time:107185ms step_avg:152.69ms
step:713/1480 train_time:107350ms step_avg:152.70ms
step:714/1480 train_time:107513ms step_avg:152.72ms
step:715/1480 train_time:107674ms step_avg:152.73ms
step:716/1480 train_time:107834ms step_avg:152.74ms
step:717/1480 train_time:107996ms step_avg:152.75ms
step:718/1480 train_time:108156ms step_avg:152.76ms
step:719/1480 train_time:108315ms step_avg:152.77ms
step:720/1480 train_time:108477ms step_avg:152.78ms
step:721/1480 train_time:108638ms step_avg:152.80ms
step:722/1480 train_time:108799ms step_avg:152.81ms
step:723/1480 train_time:108959ms step_avg:152.82ms
step:724/1480 train_time:109121ms step_avg:152.83ms
step:725/1480 train_time:109284ms step_avg:152.84ms
step:726/1480 train_time:109449ms step_avg:152.86ms
step:727/1480 train_time:109613ms step_avg:152.88ms
step:728/1480 train_time:109774ms step_avg:152.89ms
step:729/1480 train_time:109935ms step_avg:152.90ms
step:730/1480 train_time:110097ms step_avg:152.91ms
step:731/1480 train_time:110258ms step_avg:152.92ms
step:732/1480 train_time:110418ms step_avg:152.93ms
step:733/1480 train_time:110578ms step_avg:152.94ms
step:734/1480 train_time:110740ms step_avg:152.96ms
step:735/1480 train_time:110900ms step_avg:152.97ms
step:736/1480 train_time:111063ms step_avg:152.98ms
step:737/1480 train_time:111222ms step_avg:152.99ms
step:738/1480 train_time:111385ms step_avg:153.00ms
step:739/1480 train_time:111545ms step_avg:153.01ms
step:740/1480 train_time:111711ms step_avg:153.03ms
step:741/1480 train_time:111874ms step_avg:153.04ms
step:742/1480 train_time:112036ms step_avg:153.05ms
step:743/1480 train_time:112196ms step_avg:153.06ms
step:744/1480 train_time:112359ms step_avg:153.08ms
step:745/1480 train_time:112524ms step_avg:153.09ms
step:746/1480 train_time:112683ms step_avg:153.10ms
step:747/1480 train_time:112847ms step_avg:153.12ms
step:748/1480 train_time:113014ms step_avg:153.14ms
step:749/1480 train_time:113177ms step_avg:153.15ms
step:750/1480 train_time:113336ms step_avg:153.16ms
step:750/1480 val_loss:3.5454 train_time:113399ms step_avg:153.24ms
step:751/1480 train_time:113501ms step_avg:153.17ms
step:752/1480 train_time:113662ms step_avg:153.18ms
step:753/1480 train_time:113823ms step_avg:153.19ms
step:754/1480 train_time:113983ms step_avg:153.20ms
step:755/1480 train_time:114143ms step_avg:153.21ms
step:756/1480 train_time:114304ms step_avg:153.22ms
step:757/1480 train_time:114466ms step_avg:153.23ms
step:758/1480 train_time:114627ms step_avg:153.24ms
step:759/1480 train_time:114789ms step_avg:153.26ms
step:760/1480 train_time:114950ms step_avg:153.27ms
step:761/1480 train_time:115114ms step_avg:153.28ms
step:762/1480 train_time:115276ms step_avg:153.29ms
step:763/1480 train_time:115438ms step_avg:153.30ms
step:764/1480 train_time:115601ms step_avg:153.32ms
step:765/1480 train_time:115762ms step_avg:153.33ms
step:766/1480 train_time:115925ms step_avg:153.34ms
step:767/1480 train_time:116087ms step_avg:153.35ms
step:768/1480 train_time:116248ms step_avg:153.36ms
step:769/1480 train_time:116412ms step_avg:153.38ms
step:770/1480 train_time:116577ms step_avg:153.39ms
step:771/1480 train_time:116739ms step_avg:153.40ms
step:772/1480 train_time:116902ms step_avg:153.41ms
step:773/1480 train_time:117063ms step_avg:153.42ms
step:774/1480 train_time:117225ms step_avg:153.44ms
step:775/1480 train_time:117387ms step_avg:153.45ms
step:776/1480 train_time:117550ms step_avg:153.46ms
step:777/1480 train_time:117718ms step_avg:153.48ms
step:778/1480 train_time:117882ms step_avg:153.49ms
step:779/1480 train_time:118043ms step_avg:153.50ms
step:780/1480 train_time:118206ms step_avg:153.51ms
step:781/1480 train_time:118368ms step_avg:153.53ms
step:782/1480 train_time:118532ms step_avg:153.54ms
step:783/1480 train_time:118694ms step_avg:153.55ms
step:784/1480 train_time:118859ms step_avg:153.56ms
step:785/1480 train_time:119022ms step_avg:153.58ms
step:786/1480 train_time:119186ms step_avg:153.59ms
step:787/1480 train_time:119349ms step_avg:153.60ms
step:788/1480 train_time:119515ms step_avg:153.62ms
step:789/1480 train_time:119678ms step_avg:153.63ms
step:790/1480 train_time:119842ms step_avg:153.64ms
step:791/1480 train_time:120008ms step_avg:153.66ms
step:792/1480 train_time:120172ms step_avg:153.67ms
step:793/1480 train_time:120335ms step_avg:153.68ms
step:794/1480 train_time:120500ms step_avg:153.70ms
step:795/1480 train_time:120664ms step_avg:153.71ms
step:796/1480 train_time:120828ms step_avg:153.73ms
step:797/1480 train_time:120992ms step_avg:153.74ms
step:798/1480 train_time:121155ms step_avg:153.75ms
step:799/1480 train_time:121322ms step_avg:153.77ms
step:800/1480 train_time:121485ms step_avg:153.78ms
step:801/1480 train_time:121647ms step_avg:153.79ms
step:802/1480 train_time:121814ms step_avg:153.81ms
step:803/1480 train_time:121978ms step_avg:153.82ms
step:804/1480 train_time:122141ms step_avg:153.83ms
step:805/1480 train_time:122306ms step_avg:153.84ms
step:806/1480 train_time:122467ms step_avg:153.85ms
step:807/1480 train_time:122628ms step_avg:153.86ms
step:808/1480 train_time:122791ms step_avg:153.87ms
step:809/1480 train_time:122956ms step_avg:153.89ms
step:810/1480 train_time:123120ms step_avg:153.90ms
step:811/1480 train_time:123283ms step_avg:153.91ms
step:812/1480 train_time:123446ms step_avg:153.92ms
step:813/1480 train_time:123607ms step_avg:153.93ms
step:814/1480 train_time:123769ms step_avg:153.94ms
step:815/1480 train_time:123933ms step_avg:153.95ms
step:816/1480 train_time:124099ms step_avg:153.97ms
step:817/1480 train_time:124261ms step_avg:153.98ms
step:818/1480 train_time:124423ms step_avg:153.99ms
step:819/1480 train_time:124586ms step_avg:154.00ms
step:820/1480 train_time:124749ms step_avg:154.01ms
step:821/1480 train_time:124912ms step_avg:154.02ms
step:822/1480 train_time:125076ms step_avg:154.03ms
step:823/1480 train_time:125239ms step_avg:154.05ms
step:824/1480 train_time:125402ms step_avg:154.06ms
step:825/1480 train_time:125565ms step_avg:154.07ms
step:826/1480 train_time:125729ms step_avg:154.08ms
step:827/1480 train_time:125892ms step_avg:154.09ms
step:828/1480 train_time:126055ms step_avg:154.10ms
step:829/1480 train_time:126220ms step_avg:154.11ms
step:830/1480 train_time:126384ms step_avg:154.13ms
step:831/1480 train_time:126547ms step_avg:154.14ms
step:832/1480 train_time:126710ms step_avg:154.15ms
step:833/1480 train_time:126876ms step_avg:154.16ms
step:834/1480 train_time:127042ms step_avg:154.18ms
step:835/1480 train_time:127205ms step_avg:154.19ms
step:836/1480 train_time:127371ms step_avg:154.20ms
step:837/1480 train_time:127534ms step_avg:154.21ms
step:838/1480 train_time:127699ms step_avg:154.23ms
step:839/1480 train_time:127861ms step_avg:154.24ms
step:840/1480 train_time:128022ms step_avg:154.24ms
step:841/1480 train_time:128184ms step_avg:154.25ms
step:842/1480 train_time:128346ms step_avg:154.26ms
step:843/1480 train_time:128508ms step_avg:154.27ms
step:844/1480 train_time:128670ms step_avg:154.28ms
step:845/1480 train_time:128837ms step_avg:154.30ms
step:846/1480 train_time:129002ms step_avg:154.31ms
step:847/1480 train_time:129165ms step_avg:154.32ms
step:848/1480 train_time:129327ms step_avg:154.33ms
step:849/1480 train_time:129490ms step_avg:154.34ms
step:850/1480 train_time:129652ms step_avg:154.35ms
step:851/1480 train_time:129821ms step_avg:154.36ms
step:852/1480 train_time:129983ms step_avg:154.37ms
step:853/1480 train_time:130145ms step_avg:154.38ms
step:854/1480 train_time:130309ms step_avg:154.39ms
step:855/1480 train_time:130473ms step_avg:154.41ms
step:856/1480 train_time:130634ms step_avg:154.41ms
step:857/1480 train_time:130801ms step_avg:154.43ms
step:858/1480 train_time:130966ms step_avg:154.44ms
step:859/1480 train_time:131129ms step_avg:154.45ms
step:860/1480 train_time:131290ms step_avg:154.46ms
step:861/1480 train_time:131457ms step_avg:154.47ms
step:862/1480 train_time:131625ms step_avg:154.49ms
step:863/1480 train_time:131794ms step_avg:154.51ms
step:864/1480 train_time:131958ms step_avg:154.52ms
step:865/1480 train_time:132121ms step_avg:154.53ms
step:866/1480 train_time:132286ms step_avg:154.54ms
step:867/1480 train_time:132448ms step_avg:154.55ms
step:868/1480 train_time:132610ms step_avg:154.56ms
step:869/1480 train_time:132772ms step_avg:154.57ms
step:870/1480 train_time:132937ms step_avg:154.58ms
step:871/1480 train_time:133100ms step_avg:154.59ms
step:872/1480 train_time:133263ms step_avg:154.60ms
step:873/1480 train_time:133425ms step_avg:154.61ms
step:874/1480 train_time:133590ms step_avg:154.62ms
step:875/1480 train_time:133755ms step_avg:154.63ms
step:875/1480 val_loss:3.4988 train_time:133823ms step_avg:154.71ms
step:876/1480 train_time:133924ms step_avg:154.65ms
step:877/1480 train_time:134089ms step_avg:154.66ms
step:878/1480 train_time:134252ms step_avg:154.67ms
step:879/1480 train_time:134414ms step_avg:154.68ms
step:880/1480 train_time:134577ms step_avg:154.69ms
step:881/1480 train_time:134739ms step_avg:154.69ms
step:882/1480 train_time:134905ms step_avg:154.71ms
step:883/1480 train_time:135072ms step_avg:154.72ms
step:884/1480 train_time:135237ms step_avg:154.73ms
step:885/1480 train_time:135402ms step_avg:154.75ms
step:886/1480 train_time:135569ms step_avg:154.76ms
step:887/1480 train_time:135736ms step_avg:154.77ms
step:888/1480 train_time:135909ms step_avg:154.79ms
step:889/1480 train_time:136078ms step_avg:154.81ms
step:890/1480 train_time:136241ms step_avg:154.82ms
step:891/1480 train_time:136407ms step_avg:154.83ms
step:892/1480 train_time:136572ms step_avg:154.84ms
step:893/1480 train_time:136735ms step_avg:154.85ms
step:894/1480 train_time:136900ms step_avg:154.86ms
step:895/1480 train_time:137068ms step_avg:154.88ms
step:896/1480 train_time:137232ms step_avg:154.89ms
step:897/1480 train_time:137398ms step_avg:154.90ms
step:898/1480 train_time:137566ms step_avg:154.92ms
step:899/1480 train_time:137731ms step_avg:154.93ms
step:900/1480 train_time:137893ms step_avg:154.94ms
step:901/1480 train_time:138058ms step_avg:154.95ms
step:902/1480 train_time:138223ms step_avg:154.96ms
step:903/1480 train_time:138394ms step_avg:154.98ms
step:904/1480 train_time:138562ms step_avg:154.99ms
step:905/1480 train_time:138725ms step_avg:155.00ms
step:906/1480 train_time:138891ms step_avg:155.01ms
step:907/1480 train_time:139059ms step_avg:155.03ms
step:908/1480 train_time:139223ms step_avg:155.04ms
step:909/1480 train_time:139390ms step_avg:155.05ms
step:910/1480 train_time:139561ms step_avg:155.07ms
step:911/1480 train_time:139725ms step_avg:155.08ms
step:912/1480 train_time:139891ms step_avg:155.09ms
step:913/1480 train_time:140058ms step_avg:155.10ms
step:914/1480 train_time:140226ms step_avg:155.12ms
step:915/1480 train_time:140394ms step_avg:155.13ms
step:916/1480 train_time:140560ms step_avg:155.14ms
step:917/1480 train_time:140724ms step_avg:155.15ms
step:918/1480 train_time:140892ms step_avg:155.17ms
step:919/1480 train_time:141062ms step_avg:155.18ms
step:920/1480 train_time:141229ms step_avg:155.20ms
step:921/1480 train_time:141395ms step_avg:155.21ms
step:922/1480 train_time:141565ms step_avg:155.22ms
step:923/1480 train_time:141728ms step_avg:155.23ms
step:924/1480 train_time:141892ms step_avg:155.24ms
step:925/1480 train_time:142058ms step_avg:155.25ms
step:926/1480 train_time:142221ms step_avg:155.26ms
step:927/1480 train_time:142387ms step_avg:155.27ms
step:928/1480 train_time:142552ms step_avg:155.29ms
step:929/1480 train_time:142717ms step_avg:155.30ms
step:930/1480 train_time:142883ms step_avg:155.31ms
step:931/1480 train_time:143047ms step_avg:155.32ms
step:932/1480 train_time:143212ms step_avg:155.33ms
step:933/1480 train_time:143380ms step_avg:155.34ms
step:934/1480 train_time:143548ms step_avg:155.36ms
step:935/1480 train_time:143715ms step_avg:155.37ms
step:936/1480 train_time:143885ms step_avg:155.38ms
step:937/1480 train_time:144054ms step_avg:155.40ms
step:938/1480 train_time:144217ms step_avg:155.41ms
step:939/1480 train_time:144387ms step_avg:155.42ms
step:940/1480 train_time:144553ms step_avg:155.43ms
step:941/1480 train_time:144717ms step_avg:155.44ms
step:942/1480 train_time:144883ms step_avg:155.45ms
step:943/1480 train_time:145053ms step_avg:155.47ms
step:944/1480 train_time:145226ms step_avg:155.49ms
step:945/1480 train_time:145389ms step_avg:155.50ms
step:946/1480 train_time:145557ms step_avg:155.51ms
step:947/1480 train_time:145727ms step_avg:155.52ms
step:948/1480 train_time:145892ms step_avg:155.53ms
step:949/1480 train_time:146057ms step_avg:155.54ms
step:950/1480 train_time:146221ms step_avg:155.55ms
step:951/1480 train_time:146390ms step_avg:155.57ms
step:952/1480 train_time:146554ms step_avg:155.58ms
step:953/1480 train_time:146721ms step_avg:155.59ms
step:954/1480 train_time:146891ms step_avg:155.60ms
step:955/1480 train_time:147053ms step_avg:155.61ms
step:956/1480 train_time:147216ms step_avg:155.62ms
step:957/1480 train_time:147386ms step_avg:155.64ms
step:958/1480 train_time:147556ms step_avg:155.65ms
step:959/1480 train_time:147720ms step_avg:155.66ms
step:960/1480 train_time:147887ms step_avg:155.67ms
step:961/1480 train_time:148052ms step_avg:155.68ms
step:962/1480 train_time:148215ms step_avg:155.69ms
step:963/1480 train_time:148380ms step_avg:155.70ms
step:964/1480 train_time:148550ms step_avg:155.71ms
step:965/1480 train_time:148714ms step_avg:155.72ms
step:966/1480 train_time:148877ms step_avg:155.73ms
step:967/1480 train_time:149042ms step_avg:155.74ms
step:968/1480 train_time:149206ms step_avg:155.75ms
step:969/1480 train_time:149372ms step_avg:155.76ms
step:970/1480 train_time:149534ms step_avg:155.76ms
step:971/1480 train_time:149698ms step_avg:155.77ms
step:972/1480 train_time:149866ms step_avg:155.79ms
step:973/1480 train_time:150030ms step_avg:155.79ms
step:974/1480 train_time:150198ms step_avg:155.81ms
step:975/1480 train_time:150364ms step_avg:155.82ms
step:976/1480 train_time:150529ms step_avg:155.83ms
step:977/1480 train_time:150692ms step_avg:155.83ms
step:978/1480 train_time:150856ms step_avg:155.84ms
step:979/1480 train_time:151021ms step_avg:155.85ms
step:980/1480 train_time:151187ms step_avg:155.86ms
step:981/1480 train_time:151355ms step_avg:155.88ms
step:982/1480 train_time:151518ms step_avg:155.88ms
step:983/1480 train_time:151685ms step_avg:155.89ms
step:984/1480 train_time:151850ms step_avg:155.90ms
step:985/1480 train_time:152014ms step_avg:155.91ms
step:986/1480 train_time:152179ms step_avg:155.92ms
step:987/1480 train_time:152343ms step_avg:155.93ms
step:988/1480 train_time:152511ms step_avg:155.94ms
step:989/1480 train_time:152675ms step_avg:155.95ms
step:990/1480 train_time:152847ms step_avg:155.97ms
step:991/1480 train_time:153016ms step_avg:155.98ms
step:992/1480 train_time:153192ms step_avg:156.00ms
step:993/1480 train_time:153371ms step_avg:156.02ms
step:994/1480 train_time:153535ms step_avg:156.03ms
step:995/1480 train_time:153700ms step_avg:156.04ms
step:996/1480 train_time:153863ms step_avg:156.05ms
step:997/1480 train_time:154027ms step_avg:156.06ms
step:998/1480 train_time:154189ms step_avg:156.06ms
step:999/1480 train_time:154355ms step_avg:156.07ms
step:1000/1480 train_time:154528ms step_avg:156.09ms
step:1000/1480 val_loss:3.4361 train_time:154596ms step_avg:156.16ms
step:1001/1480 train_time:154699ms step_avg:156.10ms
step:1002/1480 train_time:154863ms step_avg:156.11ms
step:1003/1480 train_time:155036ms step_avg:156.13ms
step:1004/1480 train_time:155203ms step_avg:156.14ms
step:1005/1480 train_time:155370ms step_avg:156.15ms
step:1006/1480 train_time:155537ms step_avg:156.16ms
step:1007/1480 train_time:155702ms step_avg:156.17ms
step:1008/1480 train_time:155868ms step_avg:156.18ms
step:1009/1480 train_time:156042ms step_avg:156.20ms
step:1010/1480 train_time:156208ms step_avg:156.21ms
step:1011/1480 train_time:156374ms step_avg:156.22ms
step:1012/1480 train_time:156539ms step_avg:156.23ms
step:1013/1480 train_time:156709ms step_avg:156.24ms
step:1014/1480 train_time:156877ms step_avg:156.25ms
step:1015/1480 train_time:157045ms step_avg:156.26ms
step:1016/1480 train_time:157214ms step_avg:156.28ms
step:1017/1480 train_time:157386ms step_avg:156.29ms
step:1018/1480 train_time:157554ms step_avg:156.30ms
step:1019/1480 train_time:157722ms step_avg:156.31ms
step:1020/1480 train_time:157890ms step_avg:156.33ms
step:1021/1480 train_time:158056ms step_avg:156.34ms
step:1022/1480 train_time:158223ms step_avg:156.35ms
step:1023/1480 train_time:158391ms step_avg:156.36ms
step:1024/1480 train_time:158559ms step_avg:156.37ms
step:1025/1480 train_time:158729ms step_avg:156.38ms
step:1026/1480 train_time:158896ms step_avg:156.39ms
step:1027/1480 train_time:159061ms step_avg:156.40ms
step:1028/1480 train_time:159234ms step_avg:156.42ms
step:1029/1480 train_time:159408ms step_avg:156.44ms
step:1030/1480 train_time:159577ms step_avg:156.45ms
step:1031/1480 train_time:159740ms step_avg:156.45ms
step:1032/1480 train_time:159914ms step_avg:156.47ms
step:1033/1480 train_time:160080ms step_avg:156.48ms
step:1034/1480 train_time:160249ms step_avg:156.49ms
step:1035/1480 train_time:160417ms step_avg:156.50ms
step:1036/1480 train_time:160581ms step_avg:156.51ms
step:1037/1480 train_time:160747ms step_avg:156.52ms
step:1038/1480 train_time:160915ms step_avg:156.53ms
step:1039/1480 train_time:161084ms step_avg:156.54ms
step:1040/1480 train_time:161250ms step_avg:156.55ms
step:1041/1480 train_time:161419ms step_avg:156.57ms
step:1042/1480 train_time:161582ms step_avg:156.57ms
step:1043/1480 train_time:161745ms step_avg:156.58ms
step:1044/1480 train_time:161911ms step_avg:156.59ms
step:1045/1480 train_time:162081ms step_avg:156.60ms
step:1046/1480 train_time:162247ms step_avg:156.61ms
step:1047/1480 train_time:162416ms step_avg:156.62ms
step:1048/1480 train_time:162582ms step_avg:156.63ms
step:1049/1480 train_time:162749ms step_avg:156.64ms
step:1050/1480 train_time:162919ms step_avg:156.65ms
step:1051/1480 train_time:163088ms step_avg:156.66ms
step:1052/1480 train_time:163257ms step_avg:156.68ms
step:1053/1480 train_time:163423ms step_avg:156.69ms
step:1054/1480 train_time:163592ms step_avg:156.70ms
step:1055/1480 train_time:163759ms step_avg:156.71ms
step:1056/1480 train_time:163923ms step_avg:156.71ms
step:1057/1480 train_time:164090ms step_avg:156.72ms
step:1058/1480 train_time:164260ms step_avg:156.74ms
step:1059/1480 train_time:164432ms step_avg:156.75ms
step:1060/1480 train_time:164600ms step_avg:156.76ms
step:1061/1480 train_time:164762ms step_avg:156.77ms
step:1062/1480 train_time:164929ms step_avg:156.78ms
step:1063/1480 train_time:165095ms step_avg:156.79ms
step:1064/1480 train_time:165259ms step_avg:156.79ms
step:1065/1480 train_time:165426ms step_avg:156.80ms
step:1066/1480 train_time:165594ms step_avg:156.81ms
step:1067/1480 train_time:165763ms step_avg:156.82ms
step:1068/1480 train_time:165930ms step_avg:156.83ms
step:1069/1480 train_time:166100ms step_avg:156.85ms
step:1070/1480 train_time:166265ms step_avg:156.85ms
step:1071/1480 train_time:166438ms step_avg:156.87ms
step:1072/1480 train_time:166603ms step_avg:156.88ms
step:1073/1480 train_time:166767ms step_avg:156.88ms
step:1074/1480 train_time:166935ms step_avg:156.89ms
step:1075/1480 train_time:167106ms step_avg:156.91ms
step:1076/1480 train_time:167276ms step_avg:156.92ms
step:1077/1480 train_time:167441ms step_avg:156.93ms
step:1078/1480 train_time:167616ms step_avg:156.94ms
step:1079/1480 train_time:167789ms step_avg:156.96ms
step:1080/1480 train_time:167958ms step_avg:156.97ms
step:1081/1480 train_time:168124ms step_avg:156.98ms
step:1082/1480 train_time:168294ms step_avg:156.99ms
step:1083/1480 train_time:168460ms step_avg:157.00ms
step:1084/1480 train_time:168627ms step_avg:157.01ms
step:1085/1480 train_time:168797ms step_avg:157.02ms
step:1086/1480 train_time:168963ms step_avg:157.03ms
step:1087/1480 train_time:169130ms step_avg:157.04ms
step:1088/1480 train_time:169300ms step_avg:157.05ms
step:1089/1480 train_time:169472ms step_avg:157.06ms
step:1090/1480 train_time:169642ms step_avg:157.08ms
step:1091/1480 train_time:169811ms step_avg:157.09ms
step:1092/1480 train_time:169980ms step_avg:157.10ms
step:1093/1480 train_time:170148ms step_avg:157.11ms
step:1094/1480 train_time:170314ms step_avg:157.12ms
step:1095/1480 train_time:170478ms step_avg:157.12ms
step:1096/1480 train_time:170645ms step_avg:157.13ms
step:1097/1480 train_time:170816ms step_avg:157.14ms
step:1098/1480 train_time:170987ms step_avg:157.16ms
step:1099/1480 train_time:171158ms step_avg:157.17ms
step:1100/1480 train_time:171330ms step_avg:157.18ms
step:1101/1480 train_time:171501ms step_avg:157.20ms
step:1102/1480 train_time:171674ms step_avg:157.21ms
step:1103/1480 train_time:171849ms step_avg:157.23ms
step:1104/1480 train_time:172017ms step_avg:157.24ms
step:1105/1480 train_time:172185ms step_avg:157.25ms
step:1106/1480 train_time:172354ms step_avg:157.26ms
step:1107/1480 train_time:172522ms step_avg:157.27ms
step:1108/1480 train_time:172687ms step_avg:157.27ms
step:1109/1480 train_time:172855ms step_avg:157.28ms
step:1110/1480 train_time:173021ms step_avg:157.29ms
step:1111/1480 train_time:173189ms step_avg:157.30ms
step:1112/1480 train_time:173360ms step_avg:157.31ms
step:1113/1480 train_time:173538ms step_avg:157.33ms
step:1114/1480 train_time:173709ms step_avg:157.35ms
step:1115/1480 train_time:173881ms step_avg:157.36ms
step:1116/1480 train_time:174049ms step_avg:157.37ms
step:1117/1480 train_time:174221ms step_avg:157.38ms
step:1118/1480 train_time:174395ms step_avg:157.40ms
step:1119/1480 train_time:174560ms step_avg:157.40ms
step:1120/1480 train_time:174728ms step_avg:157.41ms
step:1121/1480 train_time:174900ms step_avg:157.43ms
step:1122/1480 train_time:175067ms step_avg:157.43ms
step:1123/1480 train_time:175233ms step_avg:157.44ms
step:1124/1480 train_time:175400ms step_avg:157.45ms
step:1125/1480 train_time:175570ms step_avg:157.46ms
step:1125/1480 val_loss:3.3809 train_time:175638ms step_avg:157.52ms
step:1126/1480 train_time:175740ms step_avg:157.47ms
step:1127/1480 train_time:175910ms step_avg:157.48ms
step:1128/1480 train_time:176082ms step_avg:157.50ms
step:1129/1480 train_time:176257ms step_avg:157.51ms
step:1130/1480 train_time:176426ms step_avg:157.52ms
step:1131/1480 train_time:176605ms step_avg:157.54ms
step:1132/1480 train_time:176771ms step_avg:157.55ms
step:1133/1480 train_time:176945ms step_avg:157.56ms
step:1134/1480 train_time:177116ms step_avg:157.58ms
step:1135/1480 train_time:177283ms step_avg:157.58ms
step:1136/1480 train_time:177453ms step_avg:157.60ms
step:1137/1480 train_time:177623ms step_avg:157.61ms
step:1138/1480 train_time:177794ms step_avg:157.62ms
step:1139/1480 train_time:177962ms step_avg:157.63ms
step:1140/1480 train_time:178130ms step_avg:157.64ms
step:1141/1480 train_time:178303ms step_avg:157.65ms
step:1142/1480 train_time:178470ms step_avg:157.66ms
step:1143/1480 train_time:178640ms step_avg:157.67ms
step:1144/1480 train_time:178808ms step_avg:157.68ms
step:1145/1480 train_time:178974ms step_avg:157.69ms
step:1146/1480 train_time:179143ms step_avg:157.70ms
step:1147/1480 train_time:179310ms step_avg:157.70ms
step:1148/1480 train_time:179480ms step_avg:157.71ms
step:1149/1480 train_time:179649ms step_avg:157.73ms
step:1150/1480 train_time:179819ms step_avg:157.74ms
step:1151/1480 train_time:179989ms step_avg:157.75ms
step:1152/1480 train_time:180162ms step_avg:157.76ms
step:1153/1480 train_time:180336ms step_avg:157.77ms
step:1154/1480 train_time:180503ms step_avg:157.78ms
step:1155/1480 train_time:180674ms step_avg:157.79ms
step:1156/1480 train_time:180855ms step_avg:157.81ms
step:1157/1480 train_time:181024ms step_avg:157.82ms
step:1158/1480 train_time:181188ms step_avg:157.83ms
step:1159/1480 train_time:181357ms step_avg:157.84ms
step:1160/1480 train_time:181523ms step_avg:157.85ms
step:1161/1480 train_time:181694ms step_avg:157.86ms
step:1162/1480 train_time:181864ms step_avg:157.87ms
step:1163/1480 train_time:182033ms step_avg:157.88ms
step:1164/1480 train_time:182202ms step_avg:157.89ms
step:1165/1480 train_time:182367ms step_avg:157.89ms
step:1166/1480 train_time:182537ms step_avg:157.90ms
step:1167/1480 train_time:182706ms step_avg:157.91ms
step:1168/1480 train_time:182873ms step_avg:157.92ms
step:1169/1480 train_time:183043ms step_avg:157.93ms
step:1170/1480 train_time:183213ms step_avg:157.94ms
step:1171/1480 train_time:183380ms step_avg:157.95ms
step:1172/1480 train_time:183547ms step_avg:157.96ms
step:1173/1480 train_time:183720ms step_avg:157.97ms
step:1174/1480 train_time:183901ms step_avg:157.99ms
step:1175/1480 train_time:184072ms step_avg:158.00ms
step:1176/1480 train_time:184243ms step_avg:158.01ms
step:1177/1480 train_time:184419ms step_avg:158.03ms
step:1178/1480 train_time:184586ms step_avg:158.04ms
step:1179/1480 train_time:184753ms step_avg:158.04ms
step:1180/1480 train_time:184932ms step_avg:158.06ms
step:1181/1480 train_time:185102ms step_avg:158.07ms
step:1182/1480 train_time:185269ms step_avg:158.08ms
step:1183/1480 train_time:185440ms step_avg:158.09ms
step:1184/1480 train_time:185608ms step_avg:158.10ms
step:1185/1480 train_time:185782ms step_avg:158.11ms
step:1186/1480 train_time:185952ms step_avg:158.12ms
step:1187/1480 train_time:186137ms step_avg:158.14ms
step:1188/1480 train_time:186303ms step_avg:158.15ms
step:1189/1480 train_time:186474ms step_avg:158.16ms
step:1190/1480 train_time:186641ms step_avg:158.17ms
step:1191/1480 train_time:186811ms step_avg:158.18ms
step:1192/1480 train_time:186978ms step_avg:158.19ms
step:1193/1480 train_time:187143ms step_avg:158.19ms
step:1194/1480 train_time:187313ms step_avg:158.20ms
step:1195/1480 train_time:187487ms step_avg:158.22ms
step:1196/1480 train_time:187670ms step_avg:158.24ms
step:1197/1480 train_time:187842ms step_avg:158.25ms
step:1198/1480 train_time:188023ms step_avg:158.27ms
step:1199/1480 train_time:188192ms step_avg:158.28ms
step:1200/1480 train_time:188361ms step_avg:158.29ms
step:1201/1480 train_time:188528ms step_avg:158.29ms
step:1202/1480 train_time:188710ms step_avg:158.31ms
step:1203/1480 train_time:188885ms step_avg:158.33ms
step:1204/1480 train_time:189060ms step_avg:158.34ms
step:1205/1480 train_time:189228ms step_avg:158.35ms
step:1206/1480 train_time:189399ms step_avg:158.36ms
step:1207/1480 train_time:189568ms step_avg:158.37ms
step:1208/1480 train_time:189736ms step_avg:158.38ms
step:1209/1480 train_time:189911ms step_avg:158.39ms
step:1210/1480 train_time:190086ms step_avg:158.40ms
step:1211/1480 train_time:190259ms step_avg:158.42ms
step:1212/1480 train_time:190429ms step_avg:158.43ms
step:1213/1480 train_time:190604ms step_avg:158.44ms
step:1214/1480 train_time:190782ms step_avg:158.46ms
step:1215/1480 train_time:190956ms step_avg:158.47ms
step:1216/1480 train_time:191125ms step_avg:158.48ms
step:1217/1480 train_time:191299ms step_avg:158.49ms
step:1218/1480 train_time:191467ms step_avg:158.50ms
step:1219/1480 train_time:191648ms step_avg:158.52ms
step:1220/1480 train_time:191818ms step_avg:158.53ms
step:1221/1480 train_time:191987ms step_avg:158.54ms
step:1222/1480 train_time:192155ms step_avg:158.54ms
step:1223/1480 train_time:192323ms step_avg:158.55ms
step:1224/1480 train_time:192502ms step_avg:158.57ms
step:1225/1480 train_time:192675ms step_avg:158.58ms
step:1226/1480 train_time:192850ms step_avg:158.59ms
step:1227/1480 train_time:193024ms step_avg:158.61ms
step:1228/1480 train_time:193195ms step_avg:158.62ms
step:1229/1480 train_time:193366ms step_avg:158.63ms
step:1230/1480 train_time:193546ms step_avg:158.64ms
step:1231/1480 train_time:193722ms step_avg:158.66ms
step:1232/1480 train_time:193895ms step_avg:158.67ms
step:1233/1480 train_time:194064ms step_avg:158.68ms
step:1234/1480 train_time:194234ms step_avg:158.69ms
step:1235/1480 train_time:194409ms step_avg:158.70ms
step:1236/1480 train_time:194579ms step_avg:158.71ms
step:1237/1480 train_time:194749ms step_avg:158.72ms
step:1238/1480 train_time:194936ms step_avg:158.74ms
step:1239/1480 train_time:195107ms step_avg:158.75ms
step:1240/1480 train_time:195279ms step_avg:158.76ms
step:1241/1480 train_time:195450ms step_avg:158.77ms
step:1242/1480 train_time:195620ms step_avg:158.78ms
step:1243/1480 train_time:195795ms step_avg:158.80ms
step:1244/1480 train_time:195961ms step_avg:158.80ms
step:1245/1480 train_time:196129ms step_avg:158.81ms
step:1246/1480 train_time:196301ms step_avg:158.82ms
step:1247/1480 train_time:196470ms step_avg:158.83ms
step:1248/1480 train_time:196640ms step_avg:158.84ms
step:1249/1480 train_time:196807ms step_avg:158.84ms
step:1250/1480 train_time:196976ms step_avg:158.85ms
step:1250/1480 val_loss:3.3317 train_time:197049ms step_avg:158.91ms
step:1251/1480 train_time:197158ms step_avg:158.87ms
step:1252/1480 train_time:197329ms step_avg:158.88ms
step:1253/1480 train_time:197496ms step_avg:158.89ms
step:1254/1480 train_time:197666ms step_avg:158.90ms
step:1255/1480 train_time:197853ms step_avg:158.92ms
step:1256/1480 train_time:198026ms step_avg:158.93ms
step:1257/1480 train_time:198196ms step_avg:158.94ms
step:1258/1480 train_time:198371ms step_avg:158.95ms
step:1259/1480 train_time:198543ms step_avg:158.96ms
step:1260/1480 train_time:198711ms step_avg:158.97ms
step:1261/1480 train_time:198883ms step_avg:158.98ms
step:1262/1480 train_time:199059ms step_avg:158.99ms
step:1263/1480 train_time:199233ms step_avg:159.01ms
step:1264/1480 train_time:199400ms step_avg:159.01ms
step:1265/1480 train_time:199568ms step_avg:159.02ms
step:1266/1480 train_time:199739ms step_avg:159.03ms
step:1267/1480 train_time:199910ms step_avg:159.04ms
step:1268/1480 train_time:200081ms step_avg:159.05ms
step:1269/1480 train_time:200257ms step_avg:159.06ms
step:1270/1480 train_time:200424ms step_avg:159.07ms
step:1271/1480 train_time:200594ms step_avg:159.08ms
step:1272/1480 train_time:200760ms step_avg:159.08ms
step:1273/1480 train_time:200932ms step_avg:159.09ms
step:1274/1480 train_time:201103ms step_avg:159.10ms
step:1275/1480 train_time:201271ms step_avg:159.11ms
step:1276/1480 train_time:201436ms step_avg:159.11ms
step:1277/1480 train_time:201608ms step_avg:159.12ms
step:1278/1480 train_time:201777ms step_avg:159.13ms
step:1279/1480 train_time:201950ms step_avg:159.14ms
step:1280/1480 train_time:202130ms step_avg:159.16ms
step:1281/1480 train_time:202299ms step_avg:159.16ms
step:1282/1480 train_time:202465ms step_avg:159.17ms
step:1283/1480 train_time:202636ms step_avg:159.18ms
step:1284/1480 train_time:202805ms step_avg:159.19ms
step:1285/1480 train_time:202974ms step_avg:159.20ms
step:1286/1480 train_time:203146ms step_avg:159.21ms
step:1287/1480 train_time:203319ms step_avg:159.22ms
step:1288/1480 train_time:203492ms step_avg:159.23ms
step:1289/1480 train_time:203674ms step_avg:159.25ms
step:1290/1480 train_time:203854ms step_avg:159.26ms
step:1291/1480 train_time:204028ms step_avg:159.27ms
step:1292/1480 train_time:204202ms step_avg:159.28ms
step:1293/1480 train_time:204379ms step_avg:159.30ms
step:1294/1480 train_time:204553ms step_avg:159.31ms
step:1295/1480 train_time:204726ms step_avg:159.32ms
step:1296/1480 train_time:204900ms step_avg:159.33ms
step:1297/1480 train_time:205073ms step_avg:159.34ms
step:1298/1480 train_time:205244ms step_avg:159.35ms
step:1299/1480 train_time:205414ms step_avg:159.36ms
step:1300/1480 train_time:205581ms step_avg:159.36ms
step:1301/1480 train_time:205751ms step_avg:159.37ms
step:1302/1480 train_time:205925ms step_avg:159.38ms
step:1303/1480 train_time:206099ms step_avg:159.40ms
step:1304/1480 train_time:206274ms step_avg:159.41ms
step:1305/1480 train_time:206442ms step_avg:159.41ms
step:1306/1480 train_time:206616ms step_avg:159.43ms
step:1307/1480 train_time:206783ms step_avg:159.43ms
step:1308/1480 train_time:206953ms step_avg:159.44ms
step:1309/1480 train_time:207126ms step_avg:159.45ms
step:1310/1480 train_time:207294ms step_avg:159.46ms
step:1311/1480 train_time:207461ms step_avg:159.46ms
step:1312/1480 train_time:207635ms step_avg:159.47ms
step:1313/1480 train_time:207804ms step_avg:159.48ms
step:1314/1480 train_time:207977ms step_avg:159.49ms
step:1315/1480 train_time:208148ms step_avg:159.50ms
step:1316/1480 train_time:208314ms step_avg:159.51ms
step:1317/1480 train_time:208485ms step_avg:159.51ms
step:1318/1480 train_time:208665ms step_avg:159.53ms
step:1319/1480 train_time:208841ms step_avg:159.54ms
step:1320/1480 train_time:209017ms step_avg:159.56ms
step:1321/1480 train_time:209190ms step_avg:159.57ms
step:1322/1480 train_time:209371ms step_avg:159.58ms
step:1323/1480 train_time:209542ms step_avg:159.59ms
step:1324/1480 train_time:209718ms step_avg:159.60ms
step:1325/1480 train_time:209899ms step_avg:159.62ms
step:1326/1480 train_time:210075ms step_avg:159.63ms
step:1327/1480 train_time:210245ms step_avg:159.64ms
step:1328/1480 train_time:210416ms step_avg:159.65ms
step:1329/1480 train_time:210612ms step_avg:159.68ms
step:1330/1480 train_time:210792ms step_avg:159.69ms
step:1331/1480 train_time:210962ms step_avg:159.70ms
step:1332/1480 train_time:211137ms step_avg:159.71ms
step:1333/1480 train_time:211312ms step_avg:159.72ms
step:1334/1480 train_time:211481ms step_avg:159.73ms
step:1335/1480 train_time:211653ms step_avg:159.74ms
step:1336/1480 train_time:211836ms step_avg:159.76ms
step:1337/1480 train_time:212011ms step_avg:159.77ms
step:1338/1480 train_time:212182ms step_avg:159.78ms
step:1339/1480 train_time:212357ms step_avg:159.79ms
step:1340/1480 train_time:212529ms step_avg:159.80ms
step:1341/1480 train_time:212697ms step_avg:159.80ms
step:1342/1480 train_time:212872ms step_avg:159.81ms
step:1343/1480 train_time:213042ms step_avg:159.82ms
step:1344/1480 train_time:213215ms step_avg:159.83ms
step:1345/1480 train_time:213391ms step_avg:159.84ms
step:1346/1480 train_time:213560ms step_avg:159.85ms
step:1347/1480 train_time:213732ms step_avg:159.86ms
step:1348/1480 train_time:213900ms step_avg:159.87ms
step:1349/1480 train_time:214070ms step_avg:159.87ms
step:1350/1480 train_time:214245ms step_avg:159.88ms
step:1351/1480 train_time:214415ms step_avg:159.89ms
step:1352/1480 train_time:214585ms step_avg:159.90ms
step:1353/1480 train_time:214760ms step_avg:159.91ms
step:1354/1480 train_time:214932ms step_avg:159.92ms
step:1355/1480 train_time:215099ms step_avg:159.92ms
step:1356/1480 train_time:215272ms step_avg:159.93ms
step:1357/1480 train_time:215444ms step_avg:159.94ms
step:1358/1480 train_time:215616ms step_avg:159.95ms
step:1359/1480 train_time:215787ms step_avg:159.96ms
step:1360/1480 train_time:215960ms step_avg:159.97ms
step:1361/1480 train_time:216138ms step_avg:159.98ms
step:1362/1480 train_time:216314ms step_avg:160.00ms
step:1363/1480 train_time:216493ms step_avg:160.01ms
step:1364/1480 train_time:216660ms step_avg:160.01ms
step:1365/1480 train_time:216829ms step_avg:160.02ms
step:1366/1480 train_time:217001ms step_avg:160.03ms
step:1367/1480 train_time:217173ms step_avg:160.04ms
step:1368/1480 train_time:217346ms step_avg:160.05ms
step:1369/1480 train_time:217527ms step_avg:160.06ms
step:1370/1480 train_time:217705ms step_avg:160.08ms
step:1371/1480 train_time:217877ms step_avg:160.09ms
step:1372/1480 train_time:218055ms step_avg:160.10ms
step:1373/1480 train_time:218223ms step_avg:160.11ms
step:1374/1480 train_time:218399ms step_avg:160.12ms
step:1375/1480 train_time:218571ms step_avg:160.13ms
step:1375/1480 val_loss:3.2933 train_time:218638ms step_avg:160.17ms
step:1376/1480 train_time:218747ms step_avg:160.14ms
step:1377/1480 train_time:218918ms step_avg:160.14ms
step:1378/1480 train_time:219086ms step_avg:160.15ms
step:1379/1480 train_time:219262ms step_avg:160.16ms
step:1380/1480 train_time:219435ms step_avg:160.17ms
step:1381/1480 train_time:219616ms step_avg:160.19ms
step:1382/1480 train_time:219788ms step_avg:160.20ms
step:1383/1480 train_time:219960ms step_avg:160.20ms
step:1384/1480 train_time:220138ms step_avg:160.22ms
step:1385/1480 train_time:220304ms step_avg:160.22ms
step:1386/1480 train_time:220474ms step_avg:160.23ms
step:1387/1480 train_time:220646ms step_avg:160.24ms
step:1388/1480 train_time:220813ms step_avg:160.24ms
step:1389/1480 train_time:220989ms step_avg:160.25ms
step:1390/1480 train_time:221157ms step_avg:160.26ms
step:1391/1480 train_time:221327ms step_avg:160.27ms
step:1392/1480 train_time:221500ms step_avg:160.27ms
step:1393/1480 train_time:221671ms step_avg:160.28ms
step:1394/1480 train_time:221842ms step_avg:160.29ms
step:1395/1480 train_time:222010ms step_avg:160.30ms
step:1396/1480 train_time:222178ms step_avg:160.30ms
step:1397/1480 train_time:222346ms step_avg:160.31ms
step:1398/1480 train_time:222512ms step_avg:160.31ms
step:1399/1480 train_time:222683ms step_avg:160.32ms
step:1400/1480 train_time:222860ms step_avg:160.33ms
step:1401/1480 train_time:223027ms step_avg:160.34ms
step:1402/1480 train_time:223198ms step_avg:160.34ms
step:1403/1480 train_time:223374ms step_avg:160.35ms
step:1404/1480 train_time:223545ms step_avg:160.36ms
step:1405/1480 train_time:223718ms step_avg:160.37ms
step:1406/1480 train_time:223892ms step_avg:160.38ms
step:1407/1480 train_time:224063ms step_avg:160.39ms
step:1408/1480 train_time:224232ms step_avg:160.40ms
step:1409/1480 train_time:224415ms step_avg:160.41ms
step:1410/1480 train_time:224585ms step_avg:160.42ms
step:1411/1480 train_time:224753ms step_avg:160.42ms
step:1412/1480 train_time:224923ms step_avg:160.43ms
step:1413/1480 train_time:225091ms step_avg:160.44ms
step:1414/1480 train_time:225264ms step_avg:160.44ms
step:1415/1480 train_time:225437ms step_avg:160.45ms
step:1416/1480 train_time:225624ms step_avg:160.47ms
step:1417/1480 train_time:225798ms step_avg:160.48ms
step:1418/1480 train_time:225970ms step_avg:160.49ms
step:1419/1480 train_time:226145ms step_avg:160.50ms
step:1420/1480 train_time:226320ms step_avg:160.51ms
step:1421/1480 train_time:226493ms step_avg:160.52ms
step:1422/1480 train_time:226666ms step_avg:160.53ms
step:1423/1480 train_time:226835ms step_avg:160.53ms
step:1424/1480 train_time:227012ms step_avg:160.55ms
step:1425/1480 train_time:227193ms step_avg:160.56ms
step:1426/1480 train_time:227365ms step_avg:160.57ms
step:1427/1480 train_time:227539ms step_avg:160.58ms
step:1428/1480 train_time:227708ms step_avg:160.58ms
step:1429/1480 train_time:227876ms step_avg:160.59ms
step:1430/1480 train_time:228050ms step_avg:160.60ms
step:1431/1480 train_time:228225ms step_avg:160.61ms
step:1432/1480 train_time:228402ms step_avg:160.62ms
step:1433/1480 train_time:228580ms step_avg:160.63ms
step:1434/1480 train_time:228762ms step_avg:160.65ms
step:1435/1480 train_time:228936ms step_avg:160.66ms
step:1436/1480 train_time:229110ms step_avg:160.67ms
step:1437/1480 train_time:229282ms step_avg:160.67ms
step:1438/1480 train_time:229450ms step_avg:160.68ms
step:1439/1480 train_time:229625ms step_avg:160.69ms
step:1440/1480 train_time:229794ms step_avg:160.70ms
step:1441/1480 train_time:229966ms step_avg:160.70ms
step:1442/1480 train_time:230142ms step_avg:160.71ms
step:1443/1480 train_time:230332ms step_avg:160.73ms
step:1444/1480 train_time:230503ms step_avg:160.74ms
step:1445/1480 train_time:230674ms step_avg:160.75ms
step:1446/1480 train_time:230850ms step_avg:160.76ms
step:1447/1480 train_time:231029ms step_avg:160.77ms
step:1448/1480 train_time:231201ms step_avg:160.78ms
step:1449/1480 train_time:231374ms step_avg:160.79ms
step:1450/1480 train_time:231548ms step_avg:160.80ms
step:1451/1480 train_time:231719ms step_avg:160.80ms
step:1452/1480 train_time:231892ms step_avg:160.81ms
step:1453/1480 train_time:232062ms step_avg:160.82ms
step:1454/1480 train_time:232233ms step_avg:160.83ms
step:1455/1480 train_time:232412ms step_avg:160.84ms
step:1456/1480 train_time:232584ms step_avg:160.85ms
step:1457/1480 train_time:232754ms step_avg:160.85ms
step:1458/1480 train_time:232926ms step_avg:160.86ms
step:1459/1480 train_time:233102ms step_avg:160.87ms
step:1460/1480 train_time:233273ms step_avg:160.88ms
step:1461/1480 train_time:233447ms step_avg:160.89ms
step:1462/1480 train_time:233618ms step_avg:160.89ms
step:1463/1480 train_time:233794ms step_avg:160.90ms
step:1464/1480 train_time:233970ms step_avg:160.91ms
step:1465/1480 train_time:234142ms step_avg:160.92ms
step:1466/1480 train_time:234312ms step_avg:160.93ms
step:1467/1480 train_time:234486ms step_avg:160.94ms
step:1468/1480 train_time:234655ms step_avg:160.94ms
step:1469/1480 train_time:234828ms step_avg:160.95ms
step:1470/1480 train_time:235008ms step_avg:160.96ms
step:1471/1480 train_time:235193ms step_avg:160.98ms
step:1472/1480 train_time:235373ms step_avg:160.99ms
step:1473/1480 train_time:235545ms step_avg:161.00ms
step:1474/1480 train_time:235724ms step_avg:161.01ms
step:1475/1480 train_time:235904ms step_avg:161.03ms
step:1476/1480 train_time:236076ms step_avg:161.03ms
step:1477/1480 train_time:236260ms step_avg:161.05ms
step:1478/1480 train_time:236442ms step_avg:161.06ms
step:1479/1480 train_time:236614ms step_avg:161.07ms
step:1480/1480 train_time:236787ms step_avg:161.08ms
step:1480/1480 val_loss:3.2747 train_time:236858ms step_avg:161.13ms

import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from pathlib import Path

import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP
# Use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5):
        self.num_process = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ["RANK"])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        params: "list[torch.Tensor]" = list(params)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [
            {
                "params": [p for p in params if p.numel() == size],
                "update_buffer": [
                    torch.empty(size, device="cuda", dtype=torch.bfloat16)
                    for _ in range(self.num_process)
                ],
            }
            for size in sizes
        ]
        super().__init__(param_groups, defaults)

    def step(self):
        for group in self.param_groups:
            lr: float = group["lr"]
            momentum: float = group["momentum"]
            nesterov: bool = group["nesterov"]
            zeropower_backend = zeropower_backends[group["backend"]]
            backend_steps: int = group["backend_steps"]
            update_buffers: "list[torch.Tensor]" = group["update_buffer"]
            # generate weight updates in distributed fashion
            params: "list[torch.Tensor]" = group["params"]
            assert len(params) % self.num_process == 0
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.num_process]:
                p = params[base_i + self.rank]
                g = p.grad
                assert g is not None
                state = self.state[p] 
                if "momentum_buffer" not in state:
                    state["momentum_buffer"] = torch.zeros_like(g)
                buf: torch.Tensor = state["momentum_buffer"]
                buf.lerp_(g, 1 - momentum)
                g = g.lerp_(buf, momentum) if nesterov else buf
                g = zeropower_backend(g, steps=backend_steps).flatten()
                update_prev()
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.num_process]
            update_prev()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.register_buffer('inv_freq', (1 / base) ** (torch.arange(0, dim, 2) / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            t = torch.arange(seq_len, device=x.device)
            freqs = torch.outer(t, self.inv_freq)
            self.seq_len_cached = seq_len
            self.cos_cached = freqs.cos()
            self.sin_cached = freqs.sin()
        cos, sin = self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
        # apply_rotary_emb(x, cos, sin)
        x1, x2 = x.chunk(2, dim=3)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, n_head):
        super().__init__()
        assert dim % n_head == 0
        self.n_head = n_head
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        # value residual lambda
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5])) # @Grad62304977
        # rotary embeddings
        self.rotary = Rotary(dim // n_head) # dim // n_head = head_dim
        # output projection
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: torch.Tensor, vi: torch.Tensor, block_mask: BlockMask) -> torch.Tensor:
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q: torch.Tensor = self.c_q(x).view(B, T, self.n_head, -1)
        k: torch.Tensor = self.c_k(x).view(B, T, self.n_head, -1)
        v: torch.Tensor = self.c_v(x).view(B, T, self.n_head, -1)
        v = self.lambdas[0] * v + self.lambdas[1] * vi.view_as(v) # @Grad62304977
        q, k = norm(q), norm(k) # QK norm suggested by @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim: int):
        super().__init__()
        self.c_fc   = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config.n_embd, config.n_head)
        self.mlp = MLP(config.n_embd)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: torch.Tensor, vi: torch.Tensor, x0: torch.Tensor, block_mask: BlockMask) -> torch.Tensor:
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x = x + self.attn(norm(x), vi, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768
    lm_head_softcap : int = 30

class GPT(nn.Module):

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.n_layer = config.n_layer
        self.lm_head_softcap = config.lm_head_softcap

        # U-net design by @brendanh0gan
        self.num_encoder_layers = config.n_layer // 2 # Half of the layers for encoder
        self.num_decoder_layers = config.n_layer - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
            # U-net structure on token value embeddings by @leloykun
            vte = nn.Embedding(config.vocab_size, config.n_embd*self.num_encoder_layers),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = CastedLinear(config.n_embd, config.vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(self, idx: torch.Tensor, target: torch.Tensor, sliding_window: torch.Tensor) -> torch.Tensor:
        BLOCK_SIZE = 128
        assert idx.ndim == 1
        docs = (idx == 50256).cumsum(0)
        docs_low = docs.reshape(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.reshape(-1, BLOCK_SIZE)[:, -1].contiguous()
        def document_sliding_window_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            window_mask = q_idx - kv_idx < sliding_window
            return causal_mask & document_mask & window_mask

        S = len(idx)
        def create_sliding_window_causal_mask(S: int, sliding_window: torch.Tensor):
            kv_idx = block_idx = torch.arange(S // BLOCK_SIZE, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_mask = q_idx >= kv_idx
            document_mask = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            window_mask = q_idx - kv_idx < ((sliding_window + BLOCK_SIZE - 1) // BLOCK_SIZE)
            dense_mask = causal_mask & document_mask & window_mask
            dense_mask = dense_mask.to(torch.int32)
            num_blocks = dense_mask.sum(dim=-1).to(torch.int32)
            indices = torch.argsort(dense_mask, dim=-1, descending=True, stable=True).to(torch.int32)
            num_blocks = num_blocks[None, None, :].contiguous()
            indices = indices[None, None, :].contiguous()
            return BlockMask.from_kv_blocks(num_blocks, indices, BLOCK_SIZE=BLOCK_SIZE, mask_mod=document_sliding_window_causal)
        block_mask = create_sliding_window_causal_mask(S, sliding_window)

        # forward the GPT model itself
        x = self.transformer.wte(idx[None]) # token embeddings of shape (b, t, n_embd)
        x = norm(x) # @Grad62304977
        x0 = x
        vi = self.transformer.vte(idx[None]).chunk(self.num_encoder_layers, dim=-1)

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.transformer.h[i](x, vi[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.transformer.h[self.num_encoder_layers + i](x, vi[self.num_encoder_layers-1-i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = self.lm_head_softcap * torch.tanh(logits / self.lm_head_softcap) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(file: Path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32)
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    return int(header[2]) # number of tokens (claimed)

def _load_data_shard(file: Path, ntok: int):
    with file.open("rb") as f:
        tokens = torch.empty(ntok, dtype=torch.uint16, pin_memory=True)
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())
        assert nbytes == 2 * ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.T = T

        # glob files that match the pattern
        self.files = sorted(Path.cwd().glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        self.ntoks = [_peek_data_shard(file) for file in self.files]
        assert min(self.ntoks) >= num_processes * T + 1
        self.ntok_total = sum(self.ntoks)

        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard], self.ntoks[self.current_shard])

    def next_batch(self):
        batch_size = self.T * self.num_processes
        buf = self.tokens[self.current_position:self.current_position+self.T+1]
        # host side async is sufficient;
        # no performance improvement was observed when introducing a separate stream.
        x = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # inputs
        y = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # targets
        # advance current position and load next shard if necessary
        self.current_position += batch_size
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        return x, y

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8 # batch size, in sequences, across all devices
    sequence_length : int = 64*1024 # sequence length, in tokens
    num_iterations : int = 1480 # number of iterations to run
    warmup_iters : int = 0
    cooldown_iters : int = 600 # number of iterations of linear warmup/cooldown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    # os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write(code)
        f.write('='*100 + '\n')
def print0(s, logonly=False):
    if master_process:
        with open(logfile, "a") as f:
            if not logonly:
                print(s)
            f.write(s+'\n')
# log information about the hardware/software environment this is running on
# and print the full `nvidia-smi` to file
print0(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:")
import subprocess
result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
print0(f'{result.stdout}', logonly=True)
print0('='*100, logonly=True)

# convenience variables
T = args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (T * ddp_world_size) == 0
val_steps = args.val_tokens // (T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (ddp_world_size) == 0
train_accumulation_steps = args.batch_size // ddp_world_size
assert train_accumulation_steps == 1

# load tokens
train_loader = DistributedDataLoader(args.input_bin, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, T, ddp_rank, ddp_world_size)
print0(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
print0(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
print0('='*100, logonly=True)
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model

# init the optimizer(s)
optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight, raw_model.transformer.vte.weight], lr=0.6, betas=(0.8, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight], lr=0.008, betas=(0.8, 0.95), fused=True)
params = list(raw_model.transformer.h.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
optimizer3 = Muon(matrix_params, lr=0.05, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.8, 0.95), fused=True)
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and cooldown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.cooldown_iters:
        return 1.0
    # 3) linear cooldown
    else:
        decay_ratio = (args.num_iterations - it) / args.cooldown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

sliding_window_size = torch.tensor(64, dtype=torch.int32, device="cuda")
sw_size_prev = 64
# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Set the sliding window size for the current step, in chunks of 64. By @fernbear.bsky.social
    sw_size =  64 * int((64 + (1792 - 64) * step / args.num_iterations) // 64)
    if sw_size != sw_size_prev:
        sliding_window_size.copy_(sw_size, non_blocking=True)
        sw_size_prev = sw_size

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                x_val, y_val = val_loader.next_batch()
                val_loss += model(x_val, y_val, sliding_window=sliding_window_size)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        print0(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        # torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    loss = model(x, y, sliding_window=sliding_window_size)
    loss.backward()
    del loss
    # advance the dataset for the next batch
    x, y = train_loader.next_batch()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer3.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{args.num_iterations} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.6.0.dev20241203+cu124 compiled for CUDA 12.4
nvidia-smi:
Sun Dec  8 09:00:05 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.6     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:65:02.0 Off |                    0 |
| N/A   36C    P0              74W / 700W |      7MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:67:02.0 Off |                    0 |
| N/A   45C    P0              93W / 700W |     27MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:69:02.0 Off |                    0 |
| N/A   45C    P0             114W / 700W |    533MiB / 81559MiB |      1%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:6B:02.0 Off |                    0 |
| N/A   39C    P0             118W / 700W |    533MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:6F:02.0 Off |                    0 |
| N/A   39C    P0             117W / 700W |    533MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:71:02.0 Off |                    0 |
| N/A   45C    P0             122W / 700W |    533MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:73:02.0 Off |                    0 |
| N/A   46C    P0             104W / 700W |     27MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:75:02.0 Off |                    0 |
| N/A   38C    P0             124W / 700W |    533MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training DataLoader: total number of tokens: 3200000000 across 32 files
Validation DataLoader: total number of tokens: 100000000 across 1 files
====================================================================================================
step:0/1480 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1480 train_time:23474ms step_avg:nanms
step:2/1480 train_time:23571ms step_avg:nanms
step:3/1480 train_time:23711ms step_avg:nanms
step:4/1480 train_time:23852ms step_avg:nanms
step:5/1480 train_time:23992ms step_avg:nanms
step:6/1480 train_time:24133ms step_avg:nanms
step:7/1480 train_time:24274ms step_avg:nanms
step:8/1480 train_time:24417ms step_avg:nanms
step:9/1480 train_time:24562ms step_avg:nanms
step:10/1480 train_time:24707ms step_avg:nanms
step:11/1480 train_time:143ms step_avg:nanms
step:12/1480 train_time:284ms step_avg:nanms
step:13/1480 train_time:426ms step_avg:141.90ms
step:14/1480 train_time:567ms step_avg:141.70ms
step:15/1480 train_time:708ms step_avg:141.69ms
step:16/1480 train_time:853ms step_avg:142.18ms
step:17/1480 train_time:1000ms step_avg:142.89ms
step:18/1480 train_time:1144ms step_avg:143.00ms
step:19/1480 train_time:1287ms step_avg:143.05ms
step:20/1480 train_time:1429ms step_avg:142.90ms
step:21/1480 train_time:1570ms step_avg:142.71ms
step:22/1480 train_time:1711ms step_avg:142.60ms
step:23/1480 train_time:1853ms step_avg:142.56ms
step:24/1480 train_time:1998ms step_avg:142.69ms
step:25/1480 train_time:2142ms step_avg:142.81ms
step:26/1480 train_time:2285ms step_avg:142.80ms
step:27/1480 train_time:2427ms step_avg:142.75ms
step:28/1480 train_time:2568ms step_avg:142.65ms
step:29/1480 train_time:2709ms step_avg:142.57ms
step:30/1480 train_time:2850ms step_avg:142.50ms
step:31/1480 train_time:2992ms step_avg:142.49ms
step:32/1480 train_time:3135ms step_avg:142.50ms
step:33/1480 train_time:3277ms step_avg:142.50ms
step:34/1480 train_time:3421ms step_avg:142.55ms
step:35/1480 train_time:3563ms step_avg:142.52ms
step:36/1480 train_time:3704ms step_avg:142.47ms
step:37/1480 train_time:3845ms step_avg:142.42ms
step:38/1480 train_time:3988ms step_avg:142.41ms
step:39/1480 train_time:4130ms step_avg:142.42ms
step:40/1480 train_time:4273ms step_avg:142.44ms
step:41/1480 train_time:4416ms step_avg:142.45ms
step:42/1480 train_time:4559ms step_avg:142.47ms
step:43/1480 train_time:4703ms step_avg:142.50ms
step:44/1480 train_time:4846ms step_avg:142.52ms
step:45/1480 train_time:4988ms step_avg:142.52ms
step:46/1480 train_time:5130ms step_avg:142.51ms
step:47/1480 train_time:5274ms step_avg:142.54ms
step:48/1480 train_time:5417ms step_avg:142.54ms
step:49/1480 train_time:5559ms step_avg:142.53ms
step:50/1480 train_time:5702ms step_avg:142.55ms
step:51/1480 train_time:5845ms step_avg:142.56ms
step:52/1480 train_time:5985ms step_avg:142.51ms
step:53/1480 train_time:6127ms step_avg:142.50ms
step:54/1480 train_time:6271ms step_avg:142.51ms
step:55/1480 train_time:6415ms step_avg:142.55ms
step:56/1480 train_time:6559ms step_avg:142.58ms
step:57/1480 train_time:6703ms step_avg:142.61ms
step:58/1480 train_time:6846ms step_avg:142.63ms
step:59/1480 train_time:6988ms step_avg:142.62ms
step:60/1480 train_time:7130ms step_avg:142.59ms
step:61/1480 train_time:7271ms step_avg:142.56ms
step:62/1480 train_time:7412ms step_avg:142.53ms
step:63/1480 train_time:7552ms step_avg:142.50ms
step:64/1480 train_time:7694ms step_avg:142.49ms
step:65/1480 train_time:7838ms step_avg:142.51ms
step:66/1480 train_time:7981ms step_avg:142.51ms
step:67/1480 train_time:8124ms step_avg:142.53ms
step:68/1480 train_time:8266ms step_avg:142.52ms
step:69/1480 train_time:8408ms step_avg:142.52ms
step:70/1480 train_time:8550ms step_avg:142.49ms
step:71/1480 train_time:8691ms step_avg:142.47ms
step:72/1480 train_time:8834ms step_avg:142.49ms
step:73/1480 train_time:8979ms step_avg:142.52ms
step:74/1480 train_time:9123ms step_avg:142.55ms
step:75/1480 train_time:9266ms step_avg:142.56ms
step:76/1480 train_time:9407ms step_avg:142.54ms
step:77/1480 train_time:9549ms step_avg:142.52ms
step:78/1480 train_time:9689ms step_avg:142.49ms
step:79/1480 train_time:9831ms step_avg:142.48ms
step:80/1480 train_time:9975ms step_avg:142.50ms
step:81/1480 train_time:10119ms step_avg:142.51ms
step:82/1480 train_time:10261ms step_avg:142.52ms
step:83/1480 train_time:10405ms step_avg:142.53ms
step:84/1480 train_time:10545ms step_avg:142.50ms
step:85/1480 train_time:10686ms step_avg:142.48ms
step:86/1480 train_time:10828ms step_avg:142.48ms
step:87/1480 train_time:10971ms step_avg:142.47ms
step:88/1480 train_time:11114ms step_avg:142.48ms
step:89/1480 train_time:11256ms step_avg:142.49ms
step:90/1480 train_time:11400ms step_avg:142.50ms
step:91/1480 train_time:11543ms step_avg:142.50ms
step:92/1480 train_time:11684ms step_avg:142.49ms
step:93/1480 train_time:11826ms step_avg:142.48ms
step:94/1480 train_time:11968ms step_avg:142.47ms
step:95/1480 train_time:12110ms step_avg:142.47ms
step:96/1480 train_time:12251ms step_avg:142.45ms
step:97/1480 train_time:12395ms step_avg:142.47ms
step:98/1480 train_time:12539ms step_avg:142.48ms
step:99/1480 train_time:12682ms step_avg:142.49ms
step:100/1480 train_time:12825ms step_avg:142.50ms
step:101/1480 train_time:12966ms step_avg:142.48ms
step:102/1480 train_time:13108ms step_avg:142.48ms
step:103/1480 train_time:13249ms step_avg:142.46ms
step:104/1480 train_time:13391ms step_avg:142.46ms
step:105/1480 train_time:13534ms step_avg:142.46ms
step:106/1480 train_time:13678ms step_avg:142.48ms
step:107/1480 train_time:13821ms step_avg:142.49ms
step:108/1480 train_time:13963ms step_avg:142.48ms
step:109/1480 train_time:14105ms step_avg:142.48ms
step:110/1480 train_time:14247ms step_avg:142.47ms
step:111/1480 train_time:14390ms step_avg:142.48ms
step:112/1480 train_time:14536ms step_avg:142.51ms
step:113/1480 train_time:14684ms step_avg:142.56ms
step:114/1480 train_time:14830ms step_avg:142.60ms
step:115/1480 train_time:14977ms step_avg:142.64ms
step:116/1480 train_time:15124ms step_avg:142.68ms
step:117/1480 train_time:15270ms step_avg:142.71ms
step:118/1480 train_time:15417ms step_avg:142.75ms
step:119/1480 train_time:15564ms step_avg:142.79ms
step:120/1480 train_time:15712ms step_avg:142.83ms
step:121/1480 train_time:15859ms step_avg:142.88ms
step:122/1480 train_time:16007ms step_avg:142.92ms
step:123/1480 train_time:16152ms step_avg:142.94ms
step:124/1480 train_time:16300ms step_avg:142.98ms
step:125/1480 train_time:16447ms step_avg:143.02ms
step:125/1480 val_loss:4.4200 train_time:16504ms step_avg:143.51ms
step:126/1480 train_time:16600ms step_avg:143.10ms
step:127/1480 train_time:16746ms step_avg:143.13ms
step:128/1480 train_time:16893ms step_avg:143.16ms
step:129/1480 train_time:17038ms step_avg:143.18ms
step:130/1480 train_time:17182ms step_avg:143.19ms
step:131/1480 train_time:17329ms step_avg:143.21ms
step:132/1480 train_time:17476ms step_avg:143.25ms
step:133/1480 train_time:17624ms step_avg:143.29ms
step:134/1480 train_time:17771ms step_avg:143.32ms
step:135/1480 train_time:17918ms step_avg:143.34ms
step:136/1480 train_time:18064ms step_avg:143.36ms
step:137/1480 train_time:18211ms step_avg:143.39ms
step:138/1480 train_time:18357ms step_avg:143.42ms
step:139/1480 train_time:18504ms step_avg:143.44ms
step:140/1480 train_time:18652ms step_avg:143.48ms
step:141/1480 train_time:18799ms step_avg:143.50ms
step:142/1480 train_time:18945ms step_avg:143.52ms
step:143/1480 train_time:19092ms step_avg:143.55ms
step:144/1480 train_time:19239ms step_avg:143.57ms
step:145/1480 train_time:19384ms step_avg:143.59ms
step:146/1480 train_time:19534ms step_avg:143.63ms
step:147/1480 train_time:19680ms step_avg:143.65ms
step:148/1480 train_time:19826ms step_avg:143.67ms
step:149/1480 train_time:19974ms step_avg:143.70ms
step:150/1480 train_time:20121ms step_avg:143.72ms
step:151/1480 train_time:20270ms step_avg:143.76ms
step:152/1480 train_time:20417ms step_avg:143.78ms
step:153/1480 train_time:20562ms step_avg:143.79ms
step:154/1480 train_time:20708ms step_avg:143.81ms
step:155/1480 train_time:20855ms step_avg:143.83ms
step:156/1480 train_time:21001ms step_avg:143.84ms
step:157/1480 train_time:21148ms step_avg:143.87ms
step:158/1480 train_time:21296ms step_avg:143.89ms
step:159/1480 train_time:21441ms step_avg:143.90ms
step:160/1480 train_time:21587ms step_avg:143.91ms
step:161/1480 train_time:21734ms step_avg:143.93ms
step:162/1480 train_time:21880ms step_avg:143.94ms
step:163/1480 train_time:22027ms step_avg:143.97ms
step:164/1480 train_time:22177ms step_avg:144.00ms
step:165/1480 train_time:22323ms step_avg:144.02ms
step:166/1480 train_time:22471ms step_avg:144.05ms
step:167/1480 train_time:22618ms step_avg:144.06ms
step:168/1480 train_time:22764ms step_avg:144.08ms
step:169/1480 train_time:22912ms step_avg:144.10ms
step:170/1480 train_time:23059ms step_avg:144.12ms
step:171/1480 train_time:23205ms step_avg:144.13ms
step:172/1480 train_time:23352ms step_avg:144.15ms
step:173/1480 train_time:23500ms step_avg:144.17ms
step:174/1480 train_time:23647ms step_avg:144.19ms
step:175/1480 train_time:23794ms step_avg:144.21ms
step:176/1480 train_time:23941ms step_avg:144.22ms
step:177/1480 train_time:24088ms step_avg:144.24ms
step:178/1480 train_time:24236ms step_avg:144.26ms
step:179/1480 train_time:24382ms step_avg:144.27ms
step:180/1480 train_time:24530ms step_avg:144.29ms
step:181/1480 train_time:24677ms step_avg:144.31ms
step:182/1480 train_time:24823ms step_avg:144.32ms
step:183/1480 train_time:24970ms step_avg:144.34ms
step:184/1480 train_time:25117ms step_avg:144.35ms
step:185/1480 train_time:25262ms step_avg:144.36ms
step:186/1480 train_time:25409ms step_avg:144.37ms
step:187/1480 train_time:25557ms step_avg:144.39ms
step:188/1480 train_time:25702ms step_avg:144.39ms
step:189/1480 train_time:25849ms step_avg:144.41ms
step:190/1480 train_time:25996ms step_avg:144.42ms
step:191/1480 train_time:26142ms step_avg:144.43ms
step:192/1480 train_time:26289ms step_avg:144.44ms
step:193/1480 train_time:26437ms step_avg:144.46ms
step:194/1480 train_time:26582ms step_avg:144.47ms
step:195/1480 train_time:26729ms step_avg:144.48ms
step:196/1480 train_time:26876ms step_avg:144.50ms
step:197/1480 train_time:27024ms step_avg:144.51ms
step:198/1480 train_time:27171ms step_avg:144.53ms
step:199/1480 train_time:27317ms step_avg:144.54ms
step:200/1480 train_time:27463ms step_avg:144.54ms
step:201/1480 train_time:27611ms step_avg:144.56ms
step:202/1480 train_time:27758ms step_avg:144.57ms
step:203/1480 train_time:27904ms step_avg:144.58ms
step:204/1480 train_time:28052ms step_avg:144.60ms
step:205/1480 train_time:28198ms step_avg:144.61ms
step:206/1480 train_time:28344ms step_avg:144.61ms
step:207/1480 train_time:28491ms step_avg:144.63ms
step:208/1480 train_time:28638ms step_avg:144.64ms
step:209/1480 train_time:28784ms step_avg:144.64ms
step:210/1480 train_time:28931ms step_avg:144.65ms
step:211/1480 train_time:29079ms step_avg:144.67ms
step:212/1480 train_time:29226ms step_avg:144.68ms
step:213/1480 train_time:29374ms step_avg:144.70ms
step:214/1480 train_time:29520ms step_avg:144.70ms
step:215/1480 train_time:29666ms step_avg:144.71ms
step:216/1480 train_time:29812ms step_avg:144.72ms
step:217/1480 train_time:29959ms step_avg:144.73ms
step:218/1480 train_time:30106ms step_avg:144.74ms
step:219/1480 train_time:30253ms step_avg:144.75ms
step:220/1480 train_time:30401ms step_avg:144.77ms
step:221/1480 train_time:30551ms step_avg:144.79ms
step:222/1480 train_time:30701ms step_avg:144.81ms
step:223/1480 train_time:30852ms step_avg:144.85ms
step:224/1480 train_time:31002ms step_avg:144.87ms
step:225/1480 train_time:31152ms step_avg:144.89ms
step:226/1480 train_time:31303ms step_avg:144.92ms
step:227/1480 train_time:31454ms step_avg:144.95ms
step:228/1480 train_time:31603ms step_avg:144.97ms
step:229/1480 train_time:31754ms step_avg:145.00ms
step:230/1480 train_time:31903ms step_avg:145.01ms
step:231/1480 train_time:32053ms step_avg:145.04ms
step:232/1480 train_time:32202ms step_avg:145.06ms
step:233/1480 train_time:32353ms step_avg:145.08ms
step:234/1480 train_time:32502ms step_avg:145.10ms
step:235/1480 train_time:32654ms step_avg:145.13ms
step:236/1480 train_time:32803ms step_avg:145.15ms
step:237/1480 train_time:32954ms step_avg:145.17ms
step:238/1480 train_time:33103ms step_avg:145.19ms
step:239/1480 train_time:33255ms step_avg:145.22ms
step:240/1480 train_time:33404ms step_avg:145.23ms
step:241/1480 train_time:33554ms step_avg:145.26ms
step:242/1480 train_time:33704ms step_avg:145.28ms
step:243/1480 train_time:33855ms step_avg:145.30ms
step:244/1480 train_time:34004ms step_avg:145.32ms
step:245/1480 train_time:34155ms step_avg:145.34ms
step:246/1480 train_time:34305ms step_avg:145.36ms
step:247/1480 train_time:34456ms step_avg:145.39ms
step:248/1480 train_time:34607ms step_avg:145.41ms
step:249/1480 train_time:34761ms step_avg:145.44ms
step:250/1480 train_time:34912ms step_avg:145.47ms
step:250/1480 val_loss:4.0012 train_time:34971ms step_avg:145.71ms
step:251/1480 train_time:35069ms step_avg:145.52ms
step:252/1480 train_time:35220ms step_avg:145.54ms
step:253/1480 train_time:35371ms step_avg:145.56ms
step:254/1480 train_time:35519ms step_avg:145.57ms
step:255/1480 train_time:35669ms step_avg:145.59ms
step:256/1480 train_time:35818ms step_avg:145.60ms
step:257/1480 train_time:35968ms step_avg:145.62ms
step:258/1480 train_time:36119ms step_avg:145.64ms
step:259/1480 train_time:36272ms step_avg:145.67ms
step:260/1480 train_time:36423ms step_avg:145.69ms
step:261/1480 train_time:36573ms step_avg:145.71ms
step:262/1480 train_time:36723ms step_avg:145.73ms
step:263/1480 train_time:36872ms step_avg:145.74ms
step:264/1480 train_time:37023ms step_avg:145.76ms
step:265/1480 train_time:37174ms step_avg:145.78ms
step:266/1480 train_time:37325ms step_avg:145.80ms
step:267/1480 train_time:37476ms step_avg:145.82ms
step:268/1480 train_time:37627ms step_avg:145.84ms
step:269/1480 train_time:37776ms step_avg:145.86ms
step:270/1480 train_time:37926ms step_avg:145.87ms
step:271/1480 train_time:38077ms step_avg:145.89ms
step:272/1480 train_time:38228ms step_avg:145.91ms
step:273/1480 train_time:38378ms step_avg:145.92ms
step:274/1480 train_time:38529ms step_avg:145.94ms
step:275/1480 train_time:38679ms step_avg:145.96ms
step:276/1480 train_time:38830ms step_avg:145.98ms
step:277/1480 train_time:38979ms step_avg:145.99ms
step:278/1480 train_time:39130ms step_avg:146.01ms
step:279/1480 train_time:39279ms step_avg:146.02ms
step:280/1480 train_time:39431ms step_avg:146.04ms
step:281/1480 train_time:39582ms step_avg:146.06ms
step:282/1480 train_time:39732ms step_avg:146.08ms
step:283/1480 train_time:39882ms step_avg:146.09ms
step:284/1480 train_time:40033ms step_avg:146.10ms
step:285/1480 train_time:40183ms step_avg:146.12ms
step:286/1480 train_time:40333ms step_avg:146.13ms
step:287/1480 train_time:40484ms step_avg:146.15ms
step:288/1480 train_time:40635ms step_avg:146.17ms
step:289/1480 train_time:40787ms step_avg:146.19ms
step:290/1480 train_time:40937ms step_avg:146.20ms
step:291/1480 train_time:41087ms step_avg:146.22ms
step:292/1480 train_time:41237ms step_avg:146.23ms
step:293/1480 train_time:41388ms step_avg:146.25ms
step:294/1480 train_time:41539ms step_avg:146.26ms
step:295/1480 train_time:41690ms step_avg:146.28ms
step:296/1480 train_time:41840ms step_avg:146.30ms
step:297/1480 train_time:41991ms step_avg:146.31ms
step:298/1480 train_time:42141ms step_avg:146.32ms
step:299/1480 train_time:42292ms step_avg:146.34ms
step:300/1480 train_time:42442ms step_avg:146.35ms
step:301/1480 train_time:42593ms step_avg:146.37ms
step:302/1480 train_time:42742ms step_avg:146.38ms
step:303/1480 train_time:42894ms step_avg:146.39ms
step:304/1480 train_time:43044ms step_avg:146.41ms
step:305/1480 train_time:43194ms step_avg:146.42ms
step:306/1480 train_time:43344ms step_avg:146.43ms
step:307/1480 train_time:43495ms step_avg:146.45ms
step:308/1480 train_time:43645ms step_avg:146.46ms
step:309/1480 train_time:43796ms step_avg:146.48ms
step:310/1480 train_time:43947ms step_avg:146.49ms
step:311/1480 train_time:44097ms step_avg:146.50ms
step:312/1480 train_time:44248ms step_avg:146.52ms
step:313/1480 train_time:44398ms step_avg:146.53ms
step:314/1480 train_time:44548ms step_avg:146.54ms
step:315/1480 train_time:44698ms step_avg:146.55ms
step:316/1480 train_time:44850ms step_avg:146.57ms
step:317/1480 train_time:45000ms step_avg:146.58ms
step:318/1480 train_time:45151ms step_avg:146.60ms
step:319/1480 train_time:45301ms step_avg:146.61ms
step:320/1480 train_time:45453ms step_avg:146.62ms
step:321/1480 train_time:45602ms step_avg:146.63ms
step:322/1480 train_time:45754ms step_avg:146.65ms
step:323/1480 train_time:45903ms step_avg:146.66ms
step:324/1480 train_time:46054ms step_avg:146.67ms
step:325/1480 train_time:46204ms step_avg:146.68ms
step:326/1480 train_time:46356ms step_avg:146.70ms
step:327/1480 train_time:46507ms step_avg:146.71ms
step:328/1480 train_time:46657ms step_avg:146.72ms
step:329/1480 train_time:46808ms step_avg:146.73ms
step:330/1480 train_time:46961ms step_avg:146.75ms
step:331/1480 train_time:47114ms step_avg:146.77ms
step:332/1480 train_time:47268ms step_avg:146.79ms
step:333/1480 train_time:47422ms step_avg:146.82ms
step:334/1480 train_time:47576ms step_avg:146.84ms
step:335/1480 train_time:47730ms step_avg:146.86ms
step:336/1480 train_time:47885ms step_avg:146.89ms
step:337/1480 train_time:48038ms step_avg:146.90ms
step:338/1480 train_time:48191ms step_avg:146.92ms
step:339/1480 train_time:48344ms step_avg:146.94ms
step:340/1480 train_time:48498ms step_avg:146.96ms
step:341/1480 train_time:48653ms step_avg:146.99ms
step:342/1480 train_time:48807ms step_avg:147.01ms
step:343/1480 train_time:48962ms step_avg:147.03ms
step:344/1480 train_time:49115ms step_avg:147.05ms
step:345/1480 train_time:49269ms step_avg:147.07ms
step:346/1480 train_time:49424ms step_avg:147.10ms
step:347/1480 train_time:49577ms step_avg:147.11ms
step:348/1480 train_time:49730ms step_avg:147.13ms
step:349/1480 train_time:49884ms step_avg:147.15ms
step:350/1480 train_time:50038ms step_avg:147.17ms
step:351/1480 train_time:50192ms step_avg:147.19ms
step:352/1480 train_time:50346ms step_avg:147.21ms
step:353/1480 train_time:50500ms step_avg:147.23ms
step:354/1480 train_time:50652ms step_avg:147.25ms
step:355/1480 train_time:50805ms step_avg:147.26ms
step:356/1480 train_time:50958ms step_avg:147.28ms
step:357/1480 train_time:51112ms step_avg:147.30ms
step:358/1480 train_time:51265ms step_avg:147.31ms
step:359/1480 train_time:51419ms step_avg:147.33ms
step:360/1480 train_time:51575ms step_avg:147.36ms
step:361/1480 train_time:51729ms step_avg:147.37ms
step:362/1480 train_time:51884ms step_avg:147.40ms
step:363/1480 train_time:52037ms step_avg:147.41ms
step:364/1480 train_time:52191ms step_avg:147.43ms
step:365/1480 train_time:52347ms step_avg:147.46ms
step:366/1480 train_time:52501ms step_avg:147.47ms
step:367/1480 train_time:52654ms step_avg:147.49ms
step:368/1480 train_time:52808ms step_avg:147.51ms
step:369/1480 train_time:52962ms step_avg:147.53ms
step:370/1480 train_time:53115ms step_avg:147.54ms
step:371/1480 train_time:53270ms step_avg:147.56ms
step:372/1480 train_time:53424ms step_avg:147.58ms
step:373/1480 train_time:53579ms step_avg:147.60ms
step:374/1480 train_time:53732ms step_avg:147.62ms
step:375/1480 train_time:53886ms step_avg:147.63ms
step:375/1480 val_loss:3.8129 train_time:53947ms step_avg:147.80ms
step:376/1480 train_time:54045ms step_avg:147.66ms
step:377/1480 train_time:54200ms step_avg:147.68ms
step:378/1480 train_time:54353ms step_avg:147.70ms
step:379/1480 train_time:54506ms step_avg:147.71ms
step:380/1480 train_time:54659ms step_avg:147.73ms
step:381/1480 train_time:54811ms step_avg:147.74ms
step:382/1480 train_time:54965ms step_avg:147.76ms
step:383/1480 train_time:55121ms step_avg:147.78ms
step:384/1480 train_time:55273ms step_avg:147.79ms
step:385/1480 train_time:55427ms step_avg:147.81ms
step:386/1480 train_time:55582ms step_avg:147.82ms
step:387/1480 train_time:55736ms step_avg:147.84ms
step:388/1480 train_time:55889ms step_avg:147.85ms
step:389/1480 train_time:56044ms step_avg:147.87ms
step:390/1480 train_time:56198ms step_avg:147.89ms
step:391/1480 train_time:56351ms step_avg:147.90ms
step:392/1480 train_time:56504ms step_avg:147.92ms
step:393/1480 train_time:56659ms step_avg:147.93ms
step:394/1480 train_time:56812ms step_avg:147.95ms
step:395/1480 train_time:56967ms step_avg:147.97ms
step:396/1480 train_time:57121ms step_avg:147.98ms
step:397/1480 train_time:57274ms step_avg:147.99ms
step:398/1480 train_time:57428ms step_avg:148.01ms
step:399/1480 train_time:57584ms step_avg:148.03ms
step:400/1480 train_time:57738ms step_avg:148.05ms
step:401/1480 train_time:57892ms step_avg:148.06ms
step:402/1480 train_time:58047ms step_avg:148.08ms
step:403/1480 train_time:58202ms step_avg:148.10ms
step:404/1480 train_time:58355ms step_avg:148.11ms
step:405/1480 train_time:58509ms step_avg:148.12ms
step:406/1480 train_time:58662ms step_avg:148.14ms
step:407/1480 train_time:58816ms step_avg:148.15ms
step:408/1480 train_time:58971ms step_avg:148.17ms
step:409/1480 train_time:59125ms step_avg:148.18ms
step:410/1480 train_time:59280ms step_avg:148.20ms
step:411/1480 train_time:59432ms step_avg:148.21ms
step:412/1480 train_time:59587ms step_avg:148.23ms
step:413/1480 train_time:59741ms step_avg:148.24ms
step:414/1480 train_time:59895ms step_avg:148.25ms
step:415/1480 train_time:60048ms step_avg:148.27ms
step:416/1480 train_time:60202ms step_avg:148.28ms
step:417/1480 train_time:60356ms step_avg:148.29ms
step:418/1480 train_time:60509ms step_avg:148.31ms
step:419/1480 train_time:60661ms step_avg:148.32ms
step:420/1480 train_time:60815ms step_avg:148.33ms
step:421/1480 train_time:60969ms step_avg:148.34ms
step:422/1480 train_time:61123ms step_avg:148.36ms
step:423/1480 train_time:61276ms step_avg:148.37ms
step:424/1480 train_time:61429ms step_avg:148.38ms
step:425/1480 train_time:61583ms step_avg:148.39ms
step:426/1480 train_time:61738ms step_avg:148.41ms
step:427/1480 train_time:61892ms step_avg:148.42ms
step:428/1480 train_time:62046ms step_avg:148.44ms
step:429/1480 train_time:62200ms step_avg:148.45ms
step:430/1480 train_time:62354ms step_avg:148.46ms
step:431/1480 train_time:62507ms step_avg:148.47ms
step:432/1480 train_time:62661ms step_avg:148.48ms
step:433/1480 train_time:62814ms step_avg:148.50ms
step:434/1480 train_time:62967ms step_avg:148.51ms
step:435/1480 train_time:63121ms step_avg:148.52ms
step:436/1480 train_time:63276ms step_avg:148.53ms
step:437/1480 train_time:63429ms step_avg:148.55ms
step:438/1480 train_time:63582ms step_avg:148.56ms
step:439/1480 train_time:63735ms step_avg:148.57ms
step:440/1480 train_time:63890ms step_avg:148.58ms
step:441/1480 train_time:64046ms step_avg:148.60ms
step:442/1480 train_time:64204ms step_avg:148.62ms
step:443/1480 train_time:64361ms step_avg:148.64ms
step:444/1480 train_time:64515ms step_avg:148.65ms
step:445/1480 train_time:64670ms step_avg:148.67ms
step:446/1480 train_time:64826ms step_avg:148.68ms
step:447/1480 train_time:64984ms step_avg:148.70ms
step:448/1480 train_time:65139ms step_avg:148.72ms
step:449/1480 train_time:65299ms step_avg:148.74ms
step:450/1480 train_time:65456ms step_avg:148.76ms
step:451/1480 train_time:65613ms step_avg:148.78ms
step:452/1480 train_time:65768ms step_avg:148.80ms
step:453/1480 train_time:65925ms step_avg:148.81ms
step:454/1480 train_time:66081ms step_avg:148.83ms
step:455/1480 train_time:66236ms step_avg:148.85ms
step:456/1480 train_time:66393ms step_avg:148.86ms
step:457/1480 train_time:66550ms step_avg:148.88ms
step:458/1480 train_time:66707ms step_avg:148.90ms
step:459/1480 train_time:66866ms step_avg:148.92ms
step:460/1480 train_time:67022ms step_avg:148.94ms
step:461/1480 train_time:67178ms step_avg:148.95ms
step:462/1480 train_time:67335ms step_avg:148.97ms
step:463/1480 train_time:67491ms step_avg:148.99ms
step:464/1480 train_time:67647ms step_avg:149.00ms
step:465/1480 train_time:67804ms step_avg:149.02ms
step:466/1480 train_time:67963ms step_avg:149.04ms
step:467/1480 train_time:68121ms step_avg:149.06ms
step:468/1480 train_time:68278ms step_avg:149.08ms
step:469/1480 train_time:68433ms step_avg:149.09ms
step:470/1480 train_time:68589ms step_avg:149.11ms
step:471/1480 train_time:68746ms step_avg:149.12ms
step:472/1480 train_time:68903ms step_avg:149.14ms
step:473/1480 train_time:69060ms step_avg:149.16ms
step:474/1480 train_time:69217ms step_avg:149.17ms
step:475/1480 train_time:69374ms step_avg:149.19ms
step:476/1480 train_time:69530ms step_avg:149.21ms
step:477/1480 train_time:69687ms step_avg:149.22ms
step:478/1480 train_time:69843ms step_avg:149.24ms
step:479/1480 train_time:69999ms step_avg:149.25ms
step:480/1480 train_time:70157ms step_avg:149.27ms
step:481/1480 train_time:70313ms step_avg:149.28ms
step:482/1480 train_time:70471ms step_avg:149.30ms
step:483/1480 train_time:70627ms step_avg:149.32ms
step:484/1480 train_time:70787ms step_avg:149.34ms
step:485/1480 train_time:70945ms step_avg:149.36ms
step:486/1480 train_time:71103ms step_avg:149.38ms
step:487/1480 train_time:71262ms step_avg:149.40ms
step:488/1480 train_time:71419ms step_avg:149.41ms
step:489/1480 train_time:71574ms step_avg:149.42ms
step:490/1480 train_time:71730ms step_avg:149.44ms
step:491/1480 train_time:71888ms step_avg:149.45ms
step:492/1480 train_time:72045ms step_avg:149.47ms
step:493/1480 train_time:72202ms step_avg:149.49ms
step:494/1480 train_time:72360ms step_avg:149.50ms
step:495/1480 train_time:72516ms step_avg:149.52ms
step:496/1480 train_time:72674ms step_avg:149.53ms
step:497/1480 train_time:72830ms step_avg:149.55ms
step:498/1480 train_time:72989ms step_avg:149.57ms
step:499/1480 train_time:73147ms step_avg:149.58ms
step:500/1480 train_time:73304ms step_avg:149.60ms
step:500/1480 val_loss:3.6930 train_time:73367ms step_avg:149.73ms
step:501/1480 train_time:73467ms step_avg:149.63ms
step:502/1480 train_time:73626ms step_avg:149.65ms
step:503/1480 train_time:73782ms step_avg:149.66ms
step:504/1480 train_time:73938ms step_avg:149.67ms
step:505/1480 train_time:74092ms step_avg:149.68ms
step:506/1480 train_time:74248ms step_avg:149.69ms
step:507/1480 train_time:74405ms step_avg:149.71ms
step:508/1480 train_time:74564ms step_avg:149.73ms
step:509/1480 train_time:74720ms step_avg:149.74ms
step:510/1480 train_time:74877ms step_avg:149.75ms
step:511/1480 train_time:75034ms step_avg:149.77ms
step:512/1480 train_time:75191ms step_avg:149.78ms
step:513/1480 train_time:75348ms step_avg:149.80ms
step:514/1480 train_time:75507ms step_avg:149.81ms
step:515/1480 train_time:75666ms step_avg:149.83ms
step:516/1480 train_time:75825ms step_avg:149.85ms
step:517/1480 train_time:75984ms step_avg:149.87ms
step:518/1480 train_time:76141ms step_avg:149.88ms
step:519/1480 train_time:76297ms step_avg:149.90ms
step:520/1480 train_time:76455ms step_avg:149.91ms
step:521/1480 train_time:76611ms step_avg:149.92ms
step:522/1480 train_time:76769ms step_avg:149.94ms
step:523/1480 train_time:76926ms step_avg:149.95ms
step:524/1480 train_time:77083ms step_avg:149.97ms
step:525/1480 train_time:77242ms step_avg:149.98ms
step:526/1480 train_time:77399ms step_avg:150.00ms
step:527/1480 train_time:77556ms step_avg:150.01ms
step:528/1480 train_time:77712ms step_avg:150.02ms
step:529/1480 train_time:77870ms step_avg:150.04ms
step:530/1480 train_time:78027ms step_avg:150.05ms
step:531/1480 train_time:78185ms step_avg:150.07ms
step:532/1480 train_time:78343ms step_avg:150.08ms
step:533/1480 train_time:78498ms step_avg:150.09ms
step:534/1480 train_time:78654ms step_avg:150.10ms
step:535/1480 train_time:78811ms step_avg:150.12ms
step:536/1480 train_time:78969ms step_avg:150.13ms
step:537/1480 train_time:79126ms step_avg:150.14ms
step:538/1480 train_time:79284ms step_avg:150.16ms
step:539/1480 train_time:79443ms step_avg:150.17ms
step:540/1480 train_time:79601ms step_avg:150.19ms
step:541/1480 train_time:79757ms step_avg:150.20ms
step:542/1480 train_time:79912ms step_avg:150.21ms
step:543/1480 train_time:80069ms step_avg:150.22ms
step:544/1480 train_time:80227ms step_avg:150.24ms
step:545/1480 train_time:80384ms step_avg:150.25ms
step:546/1480 train_time:80542ms step_avg:150.26ms
step:547/1480 train_time:80698ms step_avg:150.28ms
step:548/1480 train_time:80856ms step_avg:150.29ms
step:549/1480 train_time:81011ms step_avg:150.30ms
step:550/1480 train_time:81169ms step_avg:150.31ms
step:551/1480 train_time:81328ms step_avg:150.33ms
step:552/1480 train_time:81487ms step_avg:150.34ms
step:553/1480 train_time:81646ms step_avg:150.36ms
step:554/1480 train_time:81807ms step_avg:150.38ms
step:555/1480 train_time:81969ms step_avg:150.40ms
step:556/1480 train_time:82127ms step_avg:150.42ms
step:557/1480 train_time:82288ms step_avg:150.43ms
step:558/1480 train_time:82448ms step_avg:150.45ms
step:559/1480 train_time:82607ms step_avg:150.47ms
step:560/1480 train_time:82768ms step_avg:150.49ms
step:561/1480 train_time:82928ms step_avg:150.51ms
step:562/1480 train_time:83087ms step_avg:150.52ms
step:563/1480 train_time:83246ms step_avg:150.54ms
step:564/1480 train_time:83406ms step_avg:150.55ms
step:565/1480 train_time:83566ms step_avg:150.57ms
step:566/1480 train_time:83727ms step_avg:150.59ms
step:567/1480 train_time:83886ms step_avg:150.60ms
step:568/1480 train_time:84046ms step_avg:150.62ms
step:569/1480 train_time:84206ms step_avg:150.64ms
step:570/1480 train_time:84366ms step_avg:150.65ms
step:571/1480 train_time:84525ms step_avg:150.67ms
step:572/1480 train_time:84686ms step_avg:150.69ms
step:573/1480 train_time:84847ms step_avg:150.71ms
step:574/1480 train_time:85007ms step_avg:150.72ms
step:575/1480 train_time:85169ms step_avg:150.74ms
step:576/1480 train_time:85328ms step_avg:150.76ms
step:577/1480 train_time:85487ms step_avg:150.77ms
step:578/1480 train_time:85647ms step_avg:150.79ms
step:579/1480 train_time:85807ms step_avg:150.80ms
step:580/1480 train_time:85968ms step_avg:150.82ms
step:581/1480 train_time:86128ms step_avg:150.84ms
step:582/1480 train_time:86287ms step_avg:150.85ms
step:583/1480 train_time:86447ms step_avg:150.87ms
step:584/1480 train_time:86607ms step_avg:150.88ms
step:585/1480 train_time:86768ms step_avg:150.90ms
step:586/1480 train_time:86927ms step_avg:150.92ms
step:587/1480 train_time:87088ms step_avg:150.93ms
step:588/1480 train_time:87247ms step_avg:150.95ms
step:589/1480 train_time:87408ms step_avg:150.96ms
step:590/1480 train_time:87569ms step_avg:150.98ms
step:591/1480 train_time:87728ms step_avg:151.00ms
step:592/1480 train_time:87889ms step_avg:151.01ms
step:593/1480 train_time:88050ms step_avg:151.03ms
step:594/1480 train_time:88209ms step_avg:151.04ms
step:595/1480 train_time:88370ms step_avg:151.06ms
step:596/1480 train_time:88531ms step_avg:151.08ms
step:597/1480 train_time:88690ms step_avg:151.09ms
step:598/1480 train_time:88848ms step_avg:151.10ms
step:599/1480 train_time:89007ms step_avg:151.12ms
step:600/1480 train_time:89167ms step_avg:151.13ms
step:601/1480 train_time:89327ms step_avg:151.14ms
step:602/1480 train_time:89486ms step_avg:151.16ms
step:603/1480 train_time:89648ms step_avg:151.18ms
step:604/1480 train_time:89807ms step_avg:151.19ms
step:605/1480 train_time:89967ms step_avg:151.20ms
step:606/1480 train_time:90129ms step_avg:151.22ms
step:607/1480 train_time:90290ms step_avg:151.24ms
step:608/1480 train_time:90449ms step_avg:151.25ms
step:609/1480 train_time:90608ms step_avg:151.27ms
step:610/1480 train_time:90768ms step_avg:151.28ms
step:611/1480 train_time:90929ms step_avg:151.30ms
step:612/1480 train_time:91088ms step_avg:151.31ms
step:613/1480 train_time:91248ms step_avg:151.32ms
step:614/1480 train_time:91407ms step_avg:151.34ms
step:615/1480 train_time:91566ms step_avg:151.35ms
step:616/1480 train_time:91724ms step_avg:151.36ms
step:617/1480 train_time:91886ms step_avg:151.38ms
step:618/1480 train_time:92046ms step_avg:151.39ms
step:619/1480 train_time:92206ms step_avg:151.40ms
step:620/1480 train_time:92367ms step_avg:151.42ms
step:621/1480 train_time:92527ms step_avg:151.44ms
step:622/1480 train_time:92687ms step_avg:151.45ms
step:623/1480 train_time:92849ms step_avg:151.47ms
step:624/1480 train_time:93008ms step_avg:151.48ms
step:625/1480 train_time:93167ms step_avg:151.49ms
step:625/1480 val_loss:3.6101 train_time:93230ms step_avg:151.59ms
step:626/1480 train_time:93331ms step_avg:151.51ms
step:627/1480 train_time:93492ms step_avg:151.53ms
step:628/1480 train_time:93650ms step_avg:151.54ms
step:629/1480 train_time:93808ms step_avg:151.55ms
step:630/1480 train_time:93967ms step_avg:151.56ms
step:631/1480 train_time:94126ms step_avg:151.57ms
step:632/1480 train_time:94285ms step_avg:151.58ms
step:633/1480 train_time:94445ms step_avg:151.60ms
step:634/1480 train_time:94605ms step_avg:151.61ms
step:635/1480 train_time:94764ms step_avg:151.62ms
step:636/1480 train_time:94924ms step_avg:151.64ms
step:637/1480 train_time:95084ms step_avg:151.65ms
step:638/1480 train_time:95240ms step_avg:151.66ms
step:639/1480 train_time:95399ms step_avg:151.67ms
step:640/1480 train_time:95559ms step_avg:151.68ms
step:641/1480 train_time:95717ms step_avg:151.69ms
step:642/1480 train_time:95874ms step_avg:151.70ms
step:643/1480 train_time:96034ms step_avg:151.71ms
step:644/1480 train_time:96193ms step_avg:151.72ms
step:645/1480 train_time:96352ms step_avg:151.73ms
step:646/1480 train_time:96512ms step_avg:151.75ms
step:647/1480 train_time:96672ms step_avg:151.76ms
step:648/1480 train_time:96833ms step_avg:151.78ms
step:649/1480 train_time:96993ms step_avg:151.79ms
step:650/1480 train_time:97153ms step_avg:151.80ms
step:651/1480 train_time:97313ms step_avg:151.81ms
step:652/1480 train_time:97473ms step_avg:151.83ms
step:653/1480 train_time:97633ms step_avg:151.84ms
step:654/1480 train_time:97792ms step_avg:151.85ms
step:655/1480 train_time:97952ms step_avg:151.86ms
step:656/1480 train_time:98112ms step_avg:151.88ms
step:657/1480 train_time:98272ms step_avg:151.89ms
step:658/1480 train_time:98431ms step_avg:151.90ms
step:659/1480 train_time:98593ms step_avg:151.92ms
step:660/1480 train_time:98755ms step_avg:151.93ms
step:661/1480 train_time:98917ms step_avg:151.95ms
step:662/1480 train_time:99076ms step_avg:151.96ms
step:663/1480 train_time:99235ms step_avg:151.97ms
step:664/1480 train_time:99397ms step_avg:151.98ms
step:665/1480 train_time:99559ms step_avg:152.00ms
step:666/1480 train_time:99718ms step_avg:152.01ms
step:667/1480 train_time:99880ms step_avg:152.02ms
step:668/1480 train_time:100041ms step_avg:152.04ms
step:669/1480 train_time:100205ms step_avg:152.06ms
step:670/1480 train_time:100365ms step_avg:152.07ms
step:671/1480 train_time:100527ms step_avg:152.08ms
step:672/1480 train_time:100690ms step_avg:152.10ms
step:673/1480 train_time:100853ms step_avg:152.12ms
step:674/1480 train_time:101015ms step_avg:152.13ms
step:675/1480 train_time:101176ms step_avg:152.14ms
step:676/1480 train_time:101338ms step_avg:152.16ms
step:677/1480 train_time:101499ms step_avg:152.17ms
step:678/1480 train_time:101658ms step_avg:152.18ms
step:679/1480 train_time:101819ms step_avg:152.20ms
step:680/1480 train_time:101980ms step_avg:152.21ms
step:681/1480 train_time:102140ms step_avg:152.22ms
step:682/1480 train_time:102303ms step_avg:152.24ms
step:683/1480 train_time:102464ms step_avg:152.25ms
step:684/1480 train_time:102626ms step_avg:152.26ms
step:685/1480 train_time:102788ms step_avg:152.28ms
step:686/1480 train_time:102951ms step_avg:152.29ms
step:687/1480 train_time:103114ms step_avg:152.31ms
step:688/1480 train_time:103276ms step_avg:152.32ms
step:689/1480 train_time:103439ms step_avg:152.34ms
step:690/1480 train_time:103603ms step_avg:152.36ms
step:691/1480 train_time:103763ms step_avg:152.37ms
step:692/1480 train_time:103926ms step_avg:152.38ms
step:693/1480 train_time:104089ms step_avg:152.40ms
step:694/1480 train_time:104251ms step_avg:152.41ms
step:695/1480 train_time:104414ms step_avg:152.43ms
step:696/1480 train_time:104574ms step_avg:152.44ms
step:697/1480 train_time:104736ms step_avg:152.45ms
step:698/1480 train_time:104896ms step_avg:152.47ms
step:699/1480 train_time:105060ms step_avg:152.48ms
step:700/1480 train_time:105221ms step_avg:152.49ms
step:701/1480 train_time:105382ms step_avg:152.51ms
step:702/1480 train_time:105541ms step_avg:152.52ms
step:703/1480 train_time:105701ms step_avg:152.53ms
step:704/1480 train_time:105861ms step_avg:152.54ms
step:705/1480 train_time:106023ms step_avg:152.55ms
step:706/1480 train_time:106188ms step_avg:152.57ms
step:707/1480 train_time:106350ms step_avg:152.58ms
step:708/1480 train_time:106511ms step_avg:152.60ms
step:709/1480 train_time:106673ms step_avg:152.61ms
step:710/1480 train_time:106835ms step_avg:152.62ms
step:711/1480 train_time:106997ms step_avg:152.63ms
step:712/1480 train_time:107162ms step_avg:152.65ms
step:713/1480 train_time:107324ms step_avg:152.67ms
step:714/1480 train_time:107487ms step_avg:152.68ms
step:715/1480 train_time:107646ms step_avg:152.69ms
step:716/1480 train_time:107807ms step_avg:152.70ms
step:717/1480 train_time:107970ms step_avg:152.72ms
step:718/1480 train_time:108131ms step_avg:152.73ms
step:719/1480 train_time:108292ms step_avg:152.74ms
step:720/1480 train_time:108455ms step_avg:152.75ms
step:721/1480 train_time:108616ms step_avg:152.77ms
step:722/1480 train_time:108777ms step_avg:152.78ms
step:723/1480 train_time:108938ms step_avg:152.79ms
step:724/1480 train_time:109100ms step_avg:152.80ms
step:725/1480 train_time:109264ms step_avg:152.82ms
step:726/1480 train_time:109428ms step_avg:152.83ms
step:727/1480 train_time:109592ms step_avg:152.85ms
step:728/1480 train_time:109753ms step_avg:152.86ms
step:729/1480 train_time:109914ms step_avg:152.87ms
step:730/1480 train_time:110076ms step_avg:152.88ms
step:731/1480 train_time:110238ms step_avg:152.90ms
step:732/1480 train_time:110397ms step_avg:152.91ms
step:733/1480 train_time:110559ms step_avg:152.92ms
step:734/1480 train_time:110721ms step_avg:152.93ms
step:735/1480 train_time:110881ms step_avg:152.94ms
step:736/1480 train_time:111042ms step_avg:152.95ms
step:737/1480 train_time:111204ms step_avg:152.96ms
step:738/1480 train_time:111366ms step_avg:152.97ms
step:739/1480 train_time:111528ms step_avg:152.99ms
step:740/1480 train_time:111696ms step_avg:153.01ms
step:741/1480 train_time:111859ms step_avg:153.02ms
step:742/1480 train_time:112021ms step_avg:153.03ms
step:743/1480 train_time:112181ms step_avg:153.04ms
step:744/1480 train_time:112345ms step_avg:153.06ms
step:745/1480 train_time:112511ms step_avg:153.08ms
step:746/1480 train_time:112671ms step_avg:153.09ms
step:747/1480 train_time:112835ms step_avg:153.10ms
step:748/1480 train_time:113000ms step_avg:153.12ms
step:749/1480 train_time:113162ms step_avg:153.13ms
step:750/1480 train_time:113322ms step_avg:153.14ms
step:750/1480 val_loss:3.5541 train_time:113387ms step_avg:153.23ms
step:751/1480 train_time:113489ms step_avg:153.16ms
step:752/1480 train_time:113651ms step_avg:153.17ms
step:753/1480 train_time:113811ms step_avg:153.18ms
step:754/1480 train_time:113973ms step_avg:153.19ms
step:755/1480 train_time:114133ms step_avg:153.20ms
step:756/1480 train_time:114295ms step_avg:153.21ms
step:757/1480 train_time:114459ms step_avg:153.22ms
step:758/1480 train_time:114618ms step_avg:153.23ms
step:759/1480 train_time:114780ms step_avg:153.24ms
step:760/1480 train_time:114943ms step_avg:153.26ms
step:761/1480 train_time:115108ms step_avg:153.27ms
step:762/1480 train_time:115270ms step_avg:153.28ms
step:763/1480 train_time:115432ms step_avg:153.30ms
step:764/1480 train_time:115594ms step_avg:153.31ms
step:765/1480 train_time:115755ms step_avg:153.32ms
step:766/1480 train_time:115919ms step_avg:153.33ms
step:767/1480 train_time:116080ms step_avg:153.34ms
step:768/1480 train_time:116244ms step_avg:153.36ms
step:769/1480 train_time:116406ms step_avg:153.37ms
step:770/1480 train_time:116570ms step_avg:153.38ms
step:771/1480 train_time:116732ms step_avg:153.39ms
step:772/1480 train_time:116893ms step_avg:153.40ms
step:773/1480 train_time:117056ms step_avg:153.42ms
step:774/1480 train_time:117218ms step_avg:153.43ms
step:775/1480 train_time:117380ms step_avg:153.44ms
step:776/1480 train_time:117545ms step_avg:153.45ms
step:777/1480 train_time:117711ms step_avg:153.47ms
step:778/1480 train_time:117874ms step_avg:153.48ms
step:779/1480 train_time:118034ms step_avg:153.49ms
step:780/1480 train_time:118198ms step_avg:153.50ms
step:781/1480 train_time:118362ms step_avg:153.52ms
step:782/1480 train_time:118526ms step_avg:153.53ms
step:783/1480 train_time:118688ms step_avg:153.54ms
step:784/1480 train_time:118851ms step_avg:153.55ms
step:785/1480 train_time:119013ms step_avg:153.57ms
step:786/1480 train_time:119178ms step_avg:153.58ms
step:787/1480 train_time:119340ms step_avg:153.59ms
step:788/1480 train_time:119505ms step_avg:153.60ms
step:789/1480 train_time:119668ms step_avg:153.62ms
step:790/1480 train_time:119834ms step_avg:153.63ms
step:791/1480 train_time:120001ms step_avg:153.65ms
step:792/1480 train_time:120167ms step_avg:153.67ms
step:793/1480 train_time:120329ms step_avg:153.68ms
step:794/1480 train_time:120493ms step_avg:153.69ms
step:795/1480 train_time:120657ms step_avg:153.70ms
step:796/1480 train_time:120823ms step_avg:153.72ms
step:797/1480 train_time:120988ms step_avg:153.73ms
step:798/1480 train_time:121152ms step_avg:153.75ms
step:799/1480 train_time:121318ms step_avg:153.76ms
step:800/1480 train_time:121480ms step_avg:153.77ms
step:801/1480 train_time:121643ms step_avg:153.78ms
step:802/1480 train_time:121812ms step_avg:153.80ms
step:803/1480 train_time:121974ms step_avg:153.81ms
step:804/1480 train_time:122138ms step_avg:153.83ms
step:805/1480 train_time:122304ms step_avg:153.84ms
step:806/1480 train_time:122465ms step_avg:153.85ms
step:807/1480 train_time:122627ms step_avg:153.86ms
step:808/1480 train_time:122790ms step_avg:153.87ms
step:809/1480 train_time:122953ms step_avg:153.88ms
step:810/1480 train_time:123114ms step_avg:153.89ms
step:811/1480 train_time:123277ms step_avg:153.90ms
step:812/1480 train_time:123439ms step_avg:153.91ms
step:813/1480 train_time:123600ms step_avg:153.92ms
step:814/1480 train_time:123765ms step_avg:153.94ms
step:815/1480 train_time:123928ms step_avg:153.95ms
step:816/1480 train_time:124092ms step_avg:153.96ms
step:817/1480 train_time:124254ms step_avg:153.97ms
step:818/1480 train_time:124416ms step_avg:153.98ms
step:819/1480 train_time:124579ms step_avg:153.99ms
step:820/1480 train_time:124744ms step_avg:154.01ms
step:821/1480 train_time:124906ms step_avg:154.02ms
step:822/1480 train_time:125071ms step_avg:154.03ms
step:823/1480 train_time:125232ms step_avg:154.04ms
step:824/1480 train_time:125395ms step_avg:154.05ms
step:825/1480 train_time:125558ms step_avg:154.06ms
step:826/1480 train_time:125726ms step_avg:154.08ms
step:827/1480 train_time:125891ms step_avg:154.09ms
step:828/1480 train_time:126053ms step_avg:154.10ms
step:829/1480 train_time:126217ms step_avg:154.11ms
step:830/1480 train_time:126381ms step_avg:154.12ms
step:831/1480 train_time:126545ms step_avg:154.14ms
step:832/1480 train_time:126710ms step_avg:154.15ms
step:833/1480 train_time:126874ms step_avg:154.16ms
step:834/1480 train_time:127037ms step_avg:154.17ms
step:835/1480 train_time:127200ms step_avg:154.18ms
step:836/1480 train_time:127366ms step_avg:154.20ms
step:837/1480 train_time:127529ms step_avg:154.21ms
step:838/1480 train_time:127694ms step_avg:154.22ms
step:839/1480 train_time:127856ms step_avg:154.23ms
step:840/1480 train_time:128017ms step_avg:154.24ms
step:841/1480 train_time:128178ms step_avg:154.25ms
step:842/1480 train_time:128341ms step_avg:154.26ms
step:843/1480 train_time:128505ms step_avg:154.27ms
step:844/1480 train_time:128667ms step_avg:154.28ms
step:845/1480 train_time:128831ms step_avg:154.29ms
step:846/1480 train_time:128995ms step_avg:154.30ms
step:847/1480 train_time:129159ms step_avg:154.31ms
step:848/1480 train_time:129320ms step_avg:154.32ms
step:849/1480 train_time:129484ms step_avg:154.33ms
step:850/1480 train_time:129647ms step_avg:154.34ms
step:851/1480 train_time:129812ms step_avg:154.35ms
step:852/1480 train_time:129974ms step_avg:154.36ms
step:853/1480 train_time:130137ms step_avg:154.37ms
step:854/1480 train_time:130301ms step_avg:154.39ms
step:855/1480 train_time:130466ms step_avg:154.40ms
step:856/1480 train_time:130628ms step_avg:154.41ms
step:857/1480 train_time:130794ms step_avg:154.42ms
step:858/1480 train_time:130960ms step_avg:154.43ms
step:859/1480 train_time:131124ms step_avg:154.45ms
step:860/1480 train_time:131287ms step_avg:154.45ms
step:861/1480 train_time:131453ms step_avg:154.47ms
step:862/1480 train_time:131624ms step_avg:154.49ms
step:863/1480 train_time:131792ms step_avg:154.50ms
step:864/1480 train_time:131954ms step_avg:154.51ms
step:865/1480 train_time:132114ms step_avg:154.52ms
step:866/1480 train_time:132279ms step_avg:154.53ms
step:867/1480 train_time:132442ms step_avg:154.54ms
step:868/1480 train_time:132605ms step_avg:154.55ms
step:869/1480 train_time:132770ms step_avg:154.56ms
step:870/1480 train_time:132933ms step_avg:154.57ms
step:871/1480 train_time:133095ms step_avg:154.58ms
step:872/1480 train_time:133258ms step_avg:154.59ms
step:873/1480 train_time:133423ms step_avg:154.60ms
step:874/1480 train_time:133591ms step_avg:154.62ms
step:875/1480 train_time:133755ms step_avg:154.63ms
step:875/1480 val_loss:3.5075 train_time:133819ms step_avg:154.70ms
step:876/1480 train_time:133919ms step_avg:154.64ms
step:877/1480 train_time:134084ms step_avg:154.65ms
step:878/1480 train_time:134248ms step_avg:154.66ms
step:879/1480 train_time:134412ms step_avg:154.67ms
step:880/1480 train_time:134575ms step_avg:154.68ms
step:881/1480 train_time:134738ms step_avg:154.69ms
step:882/1480 train_time:134902ms step_avg:154.70ms
step:883/1480 train_time:135068ms step_avg:154.72ms
step:884/1480 train_time:135236ms step_avg:154.73ms
step:885/1480 train_time:135401ms step_avg:154.74ms
step:886/1480 train_time:135566ms step_avg:154.76ms
step:887/1480 train_time:135736ms step_avg:154.77ms
step:888/1480 train_time:135908ms step_avg:154.79ms
step:889/1480 train_time:136075ms step_avg:154.81ms
step:890/1480 train_time:136238ms step_avg:154.82ms
step:891/1480 train_time:136403ms step_avg:154.83ms
step:892/1480 train_time:136568ms step_avg:154.84ms
step:893/1480 train_time:136732ms step_avg:154.85ms
step:894/1480 train_time:136899ms step_avg:154.86ms
step:895/1480 train_time:137065ms step_avg:154.88ms
step:896/1480 train_time:137228ms step_avg:154.89ms
step:897/1480 train_time:137395ms step_avg:154.90ms
step:898/1480 train_time:137560ms step_avg:154.91ms
step:899/1480 train_time:137724ms step_avg:154.92ms
step:900/1480 train_time:137888ms step_avg:154.93ms
step:901/1480 train_time:138052ms step_avg:154.94ms
step:902/1480 train_time:138216ms step_avg:154.95ms
step:903/1480 train_time:138388ms step_avg:154.97ms
step:904/1480 train_time:138553ms step_avg:154.98ms
step:905/1480 train_time:138715ms step_avg:154.99ms
step:906/1480 train_time:138880ms step_avg:155.00ms
step:907/1480 train_time:139049ms step_avg:155.02ms
step:908/1480 train_time:139212ms step_avg:155.02ms
step:909/1480 train_time:139377ms step_avg:155.04ms
step:910/1480 train_time:139549ms step_avg:155.05ms
step:911/1480 train_time:139714ms step_avg:155.07ms
step:912/1480 train_time:139880ms step_avg:155.08ms
step:913/1480 train_time:140047ms step_avg:155.09ms
step:914/1480 train_time:140215ms step_avg:155.10ms
step:915/1480 train_time:140384ms step_avg:155.12ms
step:916/1480 train_time:140548ms step_avg:155.13ms
step:917/1480 train_time:140713ms step_avg:155.14ms
step:918/1480 train_time:140881ms step_avg:155.15ms
step:919/1480 train_time:141051ms step_avg:155.17ms
step:920/1480 train_time:141216ms step_avg:155.18ms
step:921/1480 train_time:141382ms step_avg:155.19ms
step:922/1480 train_time:141550ms step_avg:155.21ms
step:923/1480 train_time:141713ms step_avg:155.22ms
step:924/1480 train_time:141877ms step_avg:155.23ms
step:925/1480 train_time:142042ms step_avg:155.24ms
step:926/1480 train_time:142206ms step_avg:155.25ms
step:927/1480 train_time:142370ms step_avg:155.26ms
step:928/1480 train_time:142536ms step_avg:155.27ms
step:929/1480 train_time:142701ms step_avg:155.28ms
step:930/1480 train_time:142868ms step_avg:155.29ms
step:931/1480 train_time:143033ms step_avg:155.30ms
step:932/1480 train_time:143198ms step_avg:155.31ms
step:933/1480 train_time:143365ms step_avg:155.32ms
step:934/1480 train_time:143532ms step_avg:155.34ms
step:935/1480 train_time:143702ms step_avg:155.35ms
step:936/1480 train_time:143869ms step_avg:155.37ms
step:937/1480 train_time:144038ms step_avg:155.38ms
step:938/1480 train_time:144201ms step_avg:155.39ms
step:939/1480 train_time:144371ms step_avg:155.41ms
step:940/1480 train_time:144538ms step_avg:155.42ms
step:941/1480 train_time:144702ms step_avg:155.43ms
step:942/1480 train_time:144868ms step_avg:155.44ms
step:943/1480 train_time:145038ms step_avg:155.45ms
step:944/1480 train_time:145211ms step_avg:155.47ms
step:945/1480 train_time:145375ms step_avg:155.48ms
step:946/1480 train_time:145544ms step_avg:155.50ms
step:947/1480 train_time:145712ms step_avg:155.51ms
step:948/1480 train_time:145877ms step_avg:155.52ms
step:949/1480 train_time:146042ms step_avg:155.53ms
step:950/1480 train_time:146205ms step_avg:155.54ms
step:951/1480 train_time:146372ms step_avg:155.55ms
step:952/1480 train_time:146538ms step_avg:155.56ms
step:953/1480 train_time:146705ms step_avg:155.57ms
step:954/1480 train_time:146873ms step_avg:155.59ms
step:955/1480 train_time:147036ms step_avg:155.59ms
step:956/1480 train_time:147201ms step_avg:155.60ms
step:957/1480 train_time:147371ms step_avg:155.62ms
step:958/1480 train_time:147542ms step_avg:155.63ms
step:959/1480 train_time:147708ms step_avg:155.65ms
step:960/1480 train_time:147875ms step_avg:155.66ms
step:961/1480 train_time:148039ms step_avg:155.67ms
step:962/1480 train_time:148202ms step_avg:155.67ms
step:963/1480 train_time:148366ms step_avg:155.68ms
step:964/1480 train_time:148535ms step_avg:155.70ms
step:965/1480 train_time:148699ms step_avg:155.71ms
step:966/1480 train_time:148863ms step_avg:155.71ms
step:967/1480 train_time:149027ms step_avg:155.72ms
step:968/1480 train_time:149192ms step_avg:155.73ms
step:969/1480 train_time:149358ms step_avg:155.74ms
step:970/1480 train_time:149521ms step_avg:155.75ms
step:971/1480 train_time:149686ms step_avg:155.76ms
step:972/1480 train_time:149852ms step_avg:155.77ms
step:973/1480 train_time:150016ms step_avg:155.78ms
step:974/1480 train_time:150184ms step_avg:155.79ms
step:975/1480 train_time:150349ms step_avg:155.80ms
step:976/1480 train_time:150515ms step_avg:155.81ms
step:977/1480 train_time:150678ms step_avg:155.82ms
step:978/1480 train_time:150843ms step_avg:155.83ms
step:979/1480 train_time:151010ms step_avg:155.84ms
step:980/1480 train_time:151177ms step_avg:155.85ms
step:981/1480 train_time:151345ms step_avg:155.86ms
step:982/1480 train_time:151509ms step_avg:155.87ms
step:983/1480 train_time:151675ms step_avg:155.88ms
step:984/1480 train_time:151838ms step_avg:155.89ms
step:985/1480 train_time:152006ms step_avg:155.90ms
step:986/1480 train_time:152173ms step_avg:155.91ms
step:987/1480 train_time:152336ms step_avg:155.92ms
step:988/1480 train_time:152503ms step_avg:155.93ms
step:989/1480 train_time:152669ms step_avg:155.94ms
step:990/1480 train_time:152840ms step_avg:155.96ms
step:991/1480 train_time:153007ms step_avg:155.97ms
step:992/1480 train_time:153180ms step_avg:155.99ms
step:993/1480 train_time:153357ms step_avg:156.01ms
step:994/1480 train_time:153522ms step_avg:156.02ms
step:995/1480 train_time:153685ms step_avg:156.03ms
step:996/1480 train_time:153849ms step_avg:156.03ms
step:997/1480 train_time:154015ms step_avg:156.04ms
step:998/1480 train_time:154177ms step_avg:156.05ms
step:999/1480 train_time:154344ms step_avg:156.06ms
step:1000/1480 train_time:154514ms step_avg:156.07ms
step:1000/1480 val_loss:3.4442 train_time:154580ms step_avg:156.14ms
step:1001/1480 train_time:154682ms step_avg:156.09ms
step:1002/1480 train_time:154849ms step_avg:156.10ms
step:1003/1480 train_time:155019ms step_avg:156.11ms
step:1004/1480 train_time:155187ms step_avg:156.12ms
step:1005/1480 train_time:155356ms step_avg:156.14ms
step:1006/1480 train_time:155522ms step_avg:156.15ms
step:1007/1480 train_time:155687ms step_avg:156.16ms
step:1008/1480 train_time:155855ms step_avg:156.17ms
step:1009/1480 train_time:156030ms step_avg:156.19ms
step:1010/1480 train_time:156195ms step_avg:156.20ms
step:1011/1480 train_time:156361ms step_avg:156.20ms
step:1012/1480 train_time:156528ms step_avg:156.22ms
step:1013/1480 train_time:156699ms step_avg:156.23ms
step:1014/1480 train_time:156866ms step_avg:156.24ms
step:1015/1480 train_time:157036ms step_avg:156.25ms
step:1016/1480 train_time:157203ms step_avg:156.27ms
step:1017/1480 train_time:157375ms step_avg:156.28ms
step:1018/1480 train_time:157543ms step_avg:156.29ms
step:1019/1480 train_time:157712ms step_avg:156.31ms
step:1020/1480 train_time:157881ms step_avg:156.32ms
step:1021/1480 train_time:158045ms step_avg:156.33ms
step:1022/1480 train_time:158213ms step_avg:156.34ms
step:1023/1480 train_time:158379ms step_avg:156.35ms
step:1024/1480 train_time:158545ms step_avg:156.36ms
step:1025/1480 train_time:158714ms step_avg:156.37ms
step:1026/1480 train_time:158878ms step_avg:156.38ms
step:1027/1480 train_time:159046ms step_avg:156.39ms
step:1028/1480 train_time:159218ms step_avg:156.40ms
step:1029/1480 train_time:159393ms step_avg:156.42ms
step:1030/1480 train_time:159559ms step_avg:156.43ms
step:1031/1480 train_time:159723ms step_avg:156.44ms
step:1032/1480 train_time:159896ms step_avg:156.45ms
step:1033/1480 train_time:160060ms step_avg:156.46ms
step:1034/1480 train_time:160229ms step_avg:156.47ms
step:1035/1480 train_time:160396ms step_avg:156.48ms
step:1036/1480 train_time:160562ms step_avg:156.49ms
step:1037/1480 train_time:160731ms step_avg:156.51ms
step:1038/1480 train_time:160899ms step_avg:156.52ms
step:1039/1480 train_time:161071ms step_avg:156.53ms
step:1040/1480 train_time:161236ms step_avg:156.54ms
step:1041/1480 train_time:161403ms step_avg:156.55ms
step:1042/1480 train_time:161567ms step_avg:156.56ms
step:1043/1480 train_time:161733ms step_avg:156.57ms
step:1044/1480 train_time:161898ms step_avg:156.57ms
step:1045/1480 train_time:162068ms step_avg:156.59ms
step:1046/1480 train_time:162236ms step_avg:156.60ms
step:1047/1480 train_time:162401ms step_avg:156.61ms
step:1048/1480 train_time:162569ms step_avg:156.62ms
step:1049/1480 train_time:162734ms step_avg:156.63ms
step:1050/1480 train_time:162903ms step_avg:156.64ms
step:1051/1480 train_time:163074ms step_avg:156.65ms
step:1052/1480 train_time:163240ms step_avg:156.66ms
step:1053/1480 train_time:163407ms step_avg:156.67ms
step:1054/1480 train_time:163575ms step_avg:156.68ms
step:1055/1480 train_time:163739ms step_avg:156.69ms
step:1056/1480 train_time:163903ms step_avg:156.70ms
step:1057/1480 train_time:164070ms step_avg:156.70ms
step:1058/1480 train_time:164237ms step_avg:156.72ms
step:1059/1480 train_time:164410ms step_avg:156.73ms
step:1060/1480 train_time:164578ms step_avg:156.74ms
step:1061/1480 train_time:164742ms step_avg:156.75ms
step:1062/1480 train_time:164910ms step_avg:156.76ms
step:1063/1480 train_time:165077ms step_avg:156.77ms
step:1064/1480 train_time:165240ms step_avg:156.77ms
step:1065/1480 train_time:165407ms step_avg:156.78ms
step:1066/1480 train_time:165574ms step_avg:156.79ms
step:1067/1480 train_time:165742ms step_avg:156.80ms
step:1068/1480 train_time:165907ms step_avg:156.81ms
step:1069/1480 train_time:166077ms step_avg:156.82ms
step:1070/1480 train_time:166242ms step_avg:156.83ms
step:1071/1480 train_time:166415ms step_avg:156.85ms
step:1072/1480 train_time:166581ms step_avg:156.86ms
step:1073/1480 train_time:166745ms step_avg:156.86ms
step:1074/1480 train_time:166912ms step_avg:156.87ms
step:1075/1480 train_time:167083ms step_avg:156.89ms
step:1076/1480 train_time:167253ms step_avg:156.90ms
step:1077/1480 train_time:167418ms step_avg:156.91ms
step:1078/1480 train_time:167592ms step_avg:156.92ms
step:1079/1480 train_time:167765ms step_avg:156.94ms
step:1080/1480 train_time:167936ms step_avg:156.95ms
step:1081/1480 train_time:168102ms step_avg:156.96ms
step:1082/1480 train_time:168269ms step_avg:156.97ms
step:1083/1480 train_time:168436ms step_avg:156.98ms
step:1084/1480 train_time:168602ms step_avg:156.99ms
step:1085/1480 train_time:168772ms step_avg:157.00ms
step:1086/1480 train_time:168940ms step_avg:157.01ms
step:1087/1480 train_time:169106ms step_avg:157.02ms
step:1088/1480 train_time:169277ms step_avg:157.03ms
step:1089/1480 train_time:169450ms step_avg:157.04ms
step:1090/1480 train_time:169621ms step_avg:157.06ms
step:1091/1480 train_time:169788ms step_avg:157.07ms
step:1092/1480 train_time:169956ms step_avg:157.08ms
step:1093/1480 train_time:170126ms step_avg:157.09ms
step:1094/1480 train_time:170293ms step_avg:157.10ms
step:1095/1480 train_time:170457ms step_avg:157.10ms
step:1096/1480 train_time:170626ms step_avg:157.11ms
step:1097/1480 train_time:170794ms step_avg:157.12ms
step:1098/1480 train_time:170964ms step_avg:157.14ms
step:1099/1480 train_time:171136ms step_avg:157.15ms
step:1100/1480 train_time:171308ms step_avg:157.16ms
step:1101/1480 train_time:171478ms step_avg:157.17ms
step:1102/1480 train_time:171651ms step_avg:157.19ms
step:1103/1480 train_time:171828ms step_avg:157.21ms
step:1104/1480 train_time:171997ms step_avg:157.22ms
step:1105/1480 train_time:172169ms step_avg:157.23ms
step:1106/1480 train_time:172338ms step_avg:157.24ms
step:1107/1480 train_time:172507ms step_avg:157.25ms
step:1108/1480 train_time:172673ms step_avg:157.26ms
step:1109/1480 train_time:172838ms step_avg:157.27ms
step:1110/1480 train_time:173003ms step_avg:157.28ms
step:1111/1480 train_time:173171ms step_avg:157.29ms
step:1112/1480 train_time:173340ms step_avg:157.30ms
step:1113/1480 train_time:173519ms step_avg:157.32ms
step:1114/1480 train_time:173692ms step_avg:157.33ms
step:1115/1480 train_time:173863ms step_avg:157.34ms
step:1116/1480 train_time:174031ms step_avg:157.35ms
step:1117/1480 train_time:174202ms step_avg:157.36ms
step:1118/1480 train_time:174378ms step_avg:157.38ms
step:1119/1480 train_time:174546ms step_avg:157.39ms
step:1120/1480 train_time:174714ms step_avg:157.40ms
step:1121/1480 train_time:174884ms step_avg:157.41ms
step:1122/1480 train_time:175051ms step_avg:157.42ms
step:1123/1480 train_time:175217ms step_avg:157.43ms
step:1124/1480 train_time:175386ms step_avg:157.44ms
step:1125/1480 train_time:175554ms step_avg:157.45ms
step:1125/1480 val_loss:3.3901 train_time:175622ms step_avg:157.51ms
step:1126/1480 train_time:175724ms step_avg:157.46ms
step:1127/1480 train_time:175894ms step_avg:157.47ms
step:1128/1480 train_time:176064ms step_avg:157.48ms
step:1129/1480 train_time:176237ms step_avg:157.50ms
step:1130/1480 train_time:176405ms step_avg:157.50ms
step:1131/1480 train_time:176582ms step_avg:157.52ms
step:1132/1480 train_time:176747ms step_avg:157.53ms
step:1133/1480 train_time:176921ms step_avg:157.54ms
step:1134/1480 train_time:177092ms step_avg:157.56ms
step:1135/1480 train_time:177259ms step_avg:157.56ms
step:1136/1480 train_time:177432ms step_avg:157.58ms
step:1137/1480 train_time:177603ms step_avg:157.59ms
step:1138/1480 train_time:177776ms step_avg:157.60ms
step:1139/1480 train_time:177945ms step_avg:157.61ms
step:1140/1480 train_time:178113ms step_avg:157.62ms
step:1141/1480 train_time:178287ms step_avg:157.64ms
step:1142/1480 train_time:178455ms step_avg:157.65ms
step:1143/1480 train_time:178627ms step_avg:157.66ms
step:1144/1480 train_time:178796ms step_avg:157.67ms
step:1145/1480 train_time:178961ms step_avg:157.67ms
step:1146/1480 train_time:179132ms step_avg:157.69ms
step:1147/1480 train_time:179301ms step_avg:157.70ms
step:1148/1480 train_time:179470ms step_avg:157.71ms
step:1149/1480 train_time:179640ms step_avg:157.72ms
step:1150/1480 train_time:179809ms step_avg:157.73ms
step:1151/1480 train_time:179980ms step_avg:157.74ms
step:1152/1480 train_time:180153ms step_avg:157.75ms
step:1153/1480 train_time:180327ms step_avg:157.77ms
step:1154/1480 train_time:180495ms step_avg:157.78ms
step:1155/1480 train_time:180666ms step_avg:157.79ms
step:1156/1480 train_time:180845ms step_avg:157.81ms
step:1157/1480 train_time:181014ms step_avg:157.82ms
step:1158/1480 train_time:181180ms step_avg:157.82ms
step:1159/1480 train_time:181349ms step_avg:157.83ms
step:1160/1480 train_time:181514ms step_avg:157.84ms
step:1161/1480 train_time:181684ms step_avg:157.85ms
step:1162/1480 train_time:181854ms step_avg:157.86ms
step:1163/1480 train_time:182023ms step_avg:157.87ms
step:1164/1480 train_time:182192ms step_avg:157.88ms
step:1165/1480 train_time:182357ms step_avg:157.89ms
step:1166/1480 train_time:182527ms step_avg:157.90ms
step:1167/1480 train_time:182696ms step_avg:157.90ms
step:1168/1480 train_time:182863ms step_avg:157.91ms
step:1169/1480 train_time:183033ms step_avg:157.92ms
step:1170/1480 train_time:183201ms step_avg:157.93ms
step:1171/1480 train_time:183370ms step_avg:157.94ms
step:1172/1480 train_time:183536ms step_avg:157.95ms
step:1173/1480 train_time:183709ms step_avg:157.96ms
step:1174/1480 train_time:183891ms step_avg:157.98ms
step:1175/1480 train_time:184062ms step_avg:157.99ms
step:1176/1480 train_time:184234ms step_avg:158.01ms
step:1177/1480 train_time:184412ms step_avg:158.02ms
step:1178/1480 train_time:184578ms step_avg:158.03ms
step:1179/1480 train_time:184745ms step_avg:158.04ms
step:1180/1480 train_time:184923ms step_avg:158.05ms
step:1181/1480 train_time:185094ms step_avg:158.07ms
step:1182/1480 train_time:185261ms step_avg:158.07ms
step:1183/1480 train_time:185432ms step_avg:158.08ms
step:1184/1480 train_time:185600ms step_avg:158.09ms
step:1185/1480 train_time:185774ms step_avg:158.11ms
step:1186/1480 train_time:185945ms step_avg:158.12ms
step:1187/1480 train_time:186129ms step_avg:158.14ms
step:1188/1480 train_time:186296ms step_avg:158.15ms
step:1189/1480 train_time:186467ms step_avg:158.16ms
step:1190/1480 train_time:186634ms step_avg:158.16ms
step:1191/1480 train_time:186806ms step_avg:158.18ms
step:1192/1480 train_time:186974ms step_avg:158.18ms
step:1193/1480 train_time:187140ms step_avg:158.19ms
step:1194/1480 train_time:187310ms step_avg:158.20ms
step:1195/1480 train_time:187485ms step_avg:158.22ms
step:1196/1480 train_time:187668ms step_avg:158.24ms
step:1197/1480 train_time:187837ms step_avg:158.24ms
step:1198/1480 train_time:188017ms step_avg:158.26ms
step:1199/1480 train_time:188189ms step_avg:158.27ms
step:1200/1480 train_time:188357ms step_avg:158.28ms
step:1201/1480 train_time:188523ms step_avg:158.29ms
step:1202/1480 train_time:188704ms step_avg:158.31ms
step:1203/1480 train_time:188878ms step_avg:158.32ms
step:1204/1480 train_time:189054ms step_avg:158.34ms
step:1205/1480 train_time:189222ms step_avg:158.34ms
step:1206/1480 train_time:189391ms step_avg:158.35ms
step:1207/1480 train_time:189560ms step_avg:158.36ms
step:1208/1480 train_time:189728ms step_avg:158.37ms
step:1209/1480 train_time:189902ms step_avg:158.38ms
step:1210/1480 train_time:190077ms step_avg:158.40ms
step:1211/1480 train_time:190253ms step_avg:158.41ms
step:1212/1480 train_time:190425ms step_avg:158.42ms
step:1213/1480 train_time:190598ms step_avg:158.44ms
step:1214/1480 train_time:190776ms step_avg:158.45ms
step:1215/1480 train_time:190949ms step_avg:158.46ms
step:1216/1480 train_time:191119ms step_avg:158.47ms
step:1217/1480 train_time:191292ms step_avg:158.49ms
step:1218/1480 train_time:191461ms step_avg:158.49ms
step:1219/1480 train_time:191638ms step_avg:158.51ms
step:1220/1480 train_time:191808ms step_avg:158.52ms
step:1221/1480 train_time:191978ms step_avg:158.53ms
step:1222/1480 train_time:192145ms step_avg:158.54ms
step:1223/1480 train_time:192314ms step_avg:158.54ms
step:1224/1480 train_time:192495ms step_avg:158.56ms
step:1225/1480 train_time:192666ms step_avg:158.57ms
step:1226/1480 train_time:192838ms step_avg:158.58ms
step:1227/1480 train_time:193010ms step_avg:158.59ms
step:1228/1480 train_time:193178ms step_avg:158.60ms
step:1229/1480 train_time:193352ms step_avg:158.61ms
step:1230/1480 train_time:193530ms step_avg:158.63ms
step:1231/1480 train_time:193705ms step_avg:158.64ms
step:1232/1480 train_time:193880ms step_avg:158.66ms
step:1233/1480 train_time:194051ms step_avg:158.67ms
step:1234/1480 train_time:194220ms step_avg:158.68ms
step:1235/1480 train_time:194395ms step_avg:158.69ms
step:1236/1480 train_time:194562ms step_avg:158.70ms
step:1237/1480 train_time:194732ms step_avg:158.71ms
step:1238/1480 train_time:194917ms step_avg:158.73ms
step:1239/1480 train_time:195087ms step_avg:158.74ms
step:1240/1480 train_time:195257ms step_avg:158.75ms
step:1241/1480 train_time:195431ms step_avg:158.76ms
step:1242/1480 train_time:195601ms step_avg:158.77ms
step:1243/1480 train_time:195776ms step_avg:158.78ms
step:1244/1480 train_time:195943ms step_avg:158.79ms
step:1245/1480 train_time:196113ms step_avg:158.80ms
step:1246/1480 train_time:196284ms step_avg:158.81ms
step:1247/1480 train_time:196454ms step_avg:158.81ms
step:1248/1480 train_time:196623ms step_avg:158.82ms
step:1249/1480 train_time:196792ms step_avg:158.83ms
step:1250/1480 train_time:196961ms step_avg:158.84ms
step:1250/1480 val_loss:3.3395 train_time:197034ms step_avg:158.90ms
step:1251/1480 train_time:197141ms step_avg:158.86ms
step:1252/1480 train_time:197310ms step_avg:158.86ms
step:1253/1480 train_time:197477ms step_avg:158.87ms
step:1254/1480 train_time:197649ms step_avg:158.88ms
step:1255/1480 train_time:197835ms step_avg:158.90ms
step:1256/1480 train_time:198010ms step_avg:158.92ms
step:1257/1480 train_time:198182ms step_avg:158.93ms
step:1258/1480 train_time:198357ms step_avg:158.94ms
step:1259/1480 train_time:198528ms step_avg:158.95ms
step:1260/1480 train_time:198695ms step_avg:158.96ms
step:1261/1480 train_time:198867ms step_avg:158.97ms
step:1262/1480 train_time:199041ms step_avg:158.98ms
step:1263/1480 train_time:199215ms step_avg:158.99ms
step:1264/1480 train_time:199381ms step_avg:159.00ms
step:1265/1480 train_time:199548ms step_avg:159.00ms
step:1266/1480 train_time:199720ms step_avg:159.01ms
step:1267/1480 train_time:199890ms step_avg:159.02ms
step:1268/1480 train_time:200061ms step_avg:159.03ms
step:1269/1480 train_time:200238ms step_avg:159.05ms
step:1270/1480 train_time:200408ms step_avg:159.05ms
step:1271/1480 train_time:200577ms step_avg:159.06ms
step:1272/1480 train_time:200744ms step_avg:159.07ms
step:1273/1480 train_time:200914ms step_avg:159.08ms
step:1274/1480 train_time:201088ms step_avg:159.09ms
step:1275/1480 train_time:201255ms step_avg:159.09ms
step:1276/1480 train_time:201421ms step_avg:159.10ms
step:1277/1480 train_time:201593ms step_avg:159.11ms
step:1278/1480 train_time:201762ms step_avg:159.12ms
step:1279/1480 train_time:201935ms step_avg:159.13ms
step:1280/1480 train_time:202115ms step_avg:159.15ms
step:1281/1480 train_time:202285ms step_avg:159.15ms
step:1282/1480 train_time:202451ms step_avg:159.16ms
step:1283/1480 train_time:202622ms step_avg:159.17ms
step:1284/1480 train_time:202793ms step_avg:159.18ms
step:1285/1480 train_time:202962ms step_avg:159.19ms
step:1286/1480 train_time:203132ms step_avg:159.19ms
step:1287/1480 train_time:203303ms step_avg:159.20ms
step:1288/1480 train_time:203475ms step_avg:159.21ms
step:1289/1480 train_time:203656ms step_avg:159.23ms
step:1290/1480 train_time:203836ms step_avg:159.25ms
step:1291/1480 train_time:204010ms step_avg:159.26ms
step:1292/1480 train_time:204185ms step_avg:159.27ms
step:1293/1480 train_time:204360ms step_avg:159.28ms
step:1294/1480 train_time:204531ms step_avg:159.29ms
step:1295/1480 train_time:204701ms step_avg:159.30ms
step:1296/1480 train_time:204875ms step_avg:159.31ms
step:1297/1480 train_time:205048ms step_avg:159.32ms
step:1298/1480 train_time:205217ms step_avg:159.33ms
step:1299/1480 train_time:205389ms step_avg:159.34ms
step:1300/1480 train_time:205556ms step_avg:159.35ms
step:1301/1480 train_time:205725ms step_avg:159.35ms
step:1302/1480 train_time:205900ms step_avg:159.36ms
step:1303/1480 train_time:206077ms step_avg:159.38ms
step:1304/1480 train_time:206250ms step_avg:159.39ms
step:1305/1480 train_time:206419ms step_avg:159.40ms
step:1306/1480 train_time:206594ms step_avg:159.41ms
step:1307/1480 train_time:206762ms step_avg:159.42ms
step:1308/1480 train_time:206931ms step_avg:159.42ms
step:1309/1480 train_time:207104ms step_avg:159.43ms
step:1310/1480 train_time:207272ms step_avg:159.44ms
step:1311/1480 train_time:207440ms step_avg:159.45ms
step:1312/1480 train_time:207613ms step_avg:159.46ms
step:1313/1480 train_time:207782ms step_avg:159.46ms
step:1314/1480 train_time:207954ms step_avg:159.47ms
step:1315/1480 train_time:208124ms step_avg:159.48ms
step:1316/1480 train_time:208292ms step_avg:159.49ms
step:1317/1480 train_time:208462ms step_avg:159.50ms
step:1318/1480 train_time:208641ms step_avg:159.51ms
step:1319/1480 train_time:208816ms step_avg:159.52ms
step:1320/1480 train_time:208993ms step_avg:159.54ms
step:1321/1480 train_time:209167ms step_avg:159.55ms
step:1322/1480 train_time:209349ms step_avg:159.56ms
step:1323/1480 train_time:209521ms step_avg:159.57ms
step:1324/1480 train_time:209697ms step_avg:159.59ms
step:1325/1480 train_time:209877ms step_avg:159.60ms
step:1326/1480 train_time:210052ms step_avg:159.61ms
step:1327/1480 train_time:210222ms step_avg:159.62ms
step:1328/1480 train_time:210394ms step_avg:159.63ms
step:1329/1480 train_time:210591ms step_avg:159.66ms
step:1330/1480 train_time:210771ms step_avg:159.67ms
step:1331/1480 train_time:210941ms step_avg:159.68ms
step:1332/1480 train_time:211115ms step_avg:159.69ms
step:1333/1480 train_time:211290ms step_avg:159.71ms
step:1334/1480 train_time:211460ms step_avg:159.71ms
step:1335/1480 train_time:211628ms step_avg:159.72ms
step:1336/1480 train_time:211812ms step_avg:159.74ms
step:1337/1480 train_time:211985ms step_avg:159.75ms
step:1338/1480 train_time:212156ms step_avg:159.76ms
step:1339/1480 train_time:212331ms step_avg:159.77ms
step:1340/1480 train_time:212504ms step_avg:159.78ms
step:1341/1480 train_time:212671ms step_avg:159.78ms
step:1342/1480 train_time:212844ms step_avg:159.79ms
step:1343/1480 train_time:213013ms step_avg:159.80ms
step:1344/1480 train_time:213187ms step_avg:159.81ms
step:1345/1480 train_time:213366ms step_avg:159.82ms
step:1346/1480 train_time:213534ms step_avg:159.83ms
step:1347/1480 train_time:213705ms step_avg:159.84ms
step:1348/1480 train_time:213874ms step_avg:159.85ms
step:1349/1480 train_time:214044ms step_avg:159.85ms
step:1350/1480 train_time:214218ms step_avg:159.86ms
step:1351/1480 train_time:214389ms step_avg:159.87ms
step:1352/1480 train_time:214559ms step_avg:159.88ms
step:1353/1480 train_time:214736ms step_avg:159.89ms
step:1354/1480 train_time:214909ms step_avg:159.90ms
step:1355/1480 train_time:215076ms step_avg:159.91ms
step:1356/1480 train_time:215249ms step_avg:159.92ms
step:1357/1480 train_time:215422ms step_avg:159.93ms
step:1358/1480 train_time:215595ms step_avg:159.94ms
step:1359/1480 train_time:215767ms step_avg:159.95ms
step:1360/1480 train_time:215940ms step_avg:159.96ms
step:1361/1480 train_time:216118ms step_avg:159.97ms
step:1362/1480 train_time:216292ms step_avg:159.98ms
step:1363/1480 train_time:216475ms step_avg:160.00ms
step:1364/1480 train_time:216646ms step_avg:160.00ms
step:1365/1480 train_time:216813ms step_avg:160.01ms
step:1366/1480 train_time:216985ms step_avg:160.02ms
step:1367/1480 train_time:217154ms step_avg:160.03ms
step:1368/1480 train_time:217327ms step_avg:160.03ms
step:1369/1480 train_time:217509ms step_avg:160.05ms
step:1370/1480 train_time:217686ms step_avg:160.06ms
step:1371/1480 train_time:217856ms step_avg:160.07ms
step:1372/1480 train_time:218033ms step_avg:160.08ms
step:1373/1480 train_time:218203ms step_avg:160.09ms
step:1374/1480 train_time:218379ms step_avg:160.10ms
step:1375/1480 train_time:218550ms step_avg:160.11ms
step:1375/1480 val_loss:3.3003 train_time:218617ms step_avg:160.16ms
step:1376/1480 train_time:218724ms step_avg:160.12ms
step:1377/1480 train_time:218896ms step_avg:160.13ms
step:1378/1480 train_time:219064ms step_avg:160.13ms
step:1379/1480 train_time:219240ms step_avg:160.15ms
step:1380/1480 train_time:219415ms step_avg:160.16ms
step:1381/1480 train_time:219597ms step_avg:160.17ms
step:1382/1480 train_time:219769ms step_avg:160.18ms
step:1383/1480 train_time:219940ms step_avg:160.19ms
step:1384/1480 train_time:220119ms step_avg:160.20ms
step:1385/1480 train_time:220284ms step_avg:160.21ms
step:1386/1480 train_time:220455ms step_avg:160.21ms
step:1387/1480 train_time:220625ms step_avg:160.22ms
step:1388/1480 train_time:220794ms step_avg:160.23ms
step:1389/1480 train_time:220967ms step_avg:160.24ms
step:1390/1480 train_time:221135ms step_avg:160.24ms
step:1391/1480 train_time:221305ms step_avg:160.25ms
step:1392/1480 train_time:221477ms step_avg:160.26ms
step:1393/1480 train_time:221647ms step_avg:160.27ms
step:1394/1480 train_time:221819ms step_avg:160.27ms
step:1395/1480 train_time:221987ms step_avg:160.28ms
step:1396/1480 train_time:222156ms step_avg:160.29ms
step:1397/1480 train_time:222323ms step_avg:160.29ms
step:1398/1480 train_time:222491ms step_avg:160.30ms
step:1399/1480 train_time:222659ms step_avg:160.30ms
step:1400/1480 train_time:222836ms step_avg:160.31ms
step:1401/1480 train_time:223002ms step_avg:160.32ms
step:1402/1480 train_time:223174ms step_avg:160.33ms
step:1403/1480 train_time:223351ms step_avg:160.34ms
step:1404/1480 train_time:223522ms step_avg:160.35ms
step:1405/1480 train_time:223698ms step_avg:160.36ms
step:1406/1480 train_time:223872ms step_avg:160.37ms
step:1407/1480 train_time:224039ms step_avg:160.37ms
step:1408/1480 train_time:224207ms step_avg:160.38ms
step:1409/1480 train_time:224391ms step_avg:160.39ms
step:1410/1480 train_time:224559ms step_avg:160.40ms
step:1411/1480 train_time:224729ms step_avg:160.41ms
step:1412/1480 train_time:224899ms step_avg:160.41ms
step:1413/1480 train_time:225069ms step_avg:160.42ms
step:1414/1480 train_time:225241ms step_avg:160.43ms
step:1415/1480 train_time:225418ms step_avg:160.44ms
step:1416/1480 train_time:225605ms step_avg:160.46ms
step:1417/1480 train_time:225779ms step_avg:160.47ms
step:1418/1480 train_time:225952ms step_avg:160.48ms
step:1419/1480 train_time:226126ms step_avg:160.49ms
step:1420/1480 train_time:226303ms step_avg:160.50ms
step:1421/1480 train_time:226477ms step_avg:160.51ms
step:1422/1480 train_time:226648ms step_avg:160.52ms
step:1423/1480 train_time:226818ms step_avg:160.52ms
step:1424/1480 train_time:226996ms step_avg:160.53ms
step:1425/1480 train_time:227176ms step_avg:160.55ms
step:1426/1480 train_time:227349ms step_avg:160.56ms
step:1427/1480 train_time:227524ms step_avg:160.57ms
step:1428/1480 train_time:227696ms step_avg:160.58ms
step:1429/1480 train_time:227864ms step_avg:160.58ms
step:1430/1480 train_time:228039ms step_avg:160.59ms
step:1431/1480 train_time:228217ms step_avg:160.60ms
step:1432/1480 train_time:228394ms step_avg:160.61ms
step:1433/1480 train_time:228571ms step_avg:160.63ms
step:1434/1480 train_time:228752ms step_avg:160.64ms
step:1435/1480 train_time:228927ms step_avg:160.65ms
step:1436/1480 train_time:229100ms step_avg:160.66ms
step:1437/1480 train_time:229269ms step_avg:160.66ms
step:1438/1480 train_time:229438ms step_avg:160.67ms
step:1439/1480 train_time:229613ms step_avg:160.68ms
step:1440/1480 train_time:229782ms step_avg:160.69ms
step:1441/1480 train_time:229953ms step_avg:160.69ms
step:1442/1480 train_time:230131ms step_avg:160.71ms
step:1443/1480 train_time:230323ms step_avg:160.73ms
step:1444/1480 train_time:230494ms step_avg:160.73ms
step:1445/1480 train_time:230664ms step_avg:160.74ms
step:1446/1480 train_time:230840ms step_avg:160.75ms
step:1447/1480 train_time:231019ms step_avg:160.76ms
step:1448/1480 train_time:231190ms step_avg:160.77ms
step:1449/1480 train_time:231362ms step_avg:160.78ms
step:1450/1480 train_time:231536ms step_avg:160.79ms
step:1451/1480 train_time:231707ms step_avg:160.80ms
step:1452/1480 train_time:231881ms step_avg:160.80ms
step:1453/1480 train_time:232050ms step_avg:160.81ms
step:1454/1480 train_time:232223ms step_avg:160.82ms
step:1455/1480 train_time:232402ms step_avg:160.83ms
step:1456/1480 train_time:232575ms step_avg:160.84ms
step:1457/1480 train_time:232747ms step_avg:160.85ms
step:1458/1480 train_time:232919ms step_avg:160.86ms
step:1459/1480 train_time:233096ms step_avg:160.87ms
step:1460/1480 train_time:233266ms step_avg:160.87ms
step:1461/1480 train_time:233440ms step_avg:160.88ms
step:1462/1480 train_time:233612ms step_avg:160.89ms
step:1463/1480 train_time:233788ms step_avg:160.90ms
step:1464/1480 train_time:233962ms step_avg:160.91ms
step:1465/1480 train_time:234134ms step_avg:160.92ms
step:1466/1480 train_time:234304ms step_avg:160.92ms
step:1467/1480 train_time:234479ms step_avg:160.93ms
step:1468/1480 train_time:234649ms step_avg:160.94ms
step:1469/1480 train_time:234824ms step_avg:160.95ms
step:1470/1480 train_time:235004ms step_avg:160.96ms
step:1471/1480 train_time:235190ms step_avg:160.98ms
step:1472/1480 train_time:235369ms step_avg:160.99ms
step:1473/1480 train_time:235541ms step_avg:161.00ms
step:1474/1480 train_time:235719ms step_avg:161.01ms
step:1475/1480 train_time:235900ms step_avg:161.02ms
step:1476/1480 train_time:236073ms step_avg:161.03ms
step:1477/1480 train_time:236257ms step_avg:161.05ms
step:1478/1480 train_time:236441ms step_avg:161.06ms
step:1479/1480 train_time:236617ms step_avg:161.07ms
step:1480/1480 train_time:236789ms step_avg:161.08ms
step:1480/1480 val_loss:3.2815 train_time:236860ms step_avg:161.13ms

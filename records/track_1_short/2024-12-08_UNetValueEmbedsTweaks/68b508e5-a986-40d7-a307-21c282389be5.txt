import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from pathlib import Path

import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP
# Use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5):
        self.num_process = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ["RANK"])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        params: "list[torch.Tensor]" = list(params)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [
            {
                "params": [p for p in params if p.numel() == size],
                "update_buffer": [
                    torch.empty(size, device="cuda", dtype=torch.bfloat16)
                    for _ in range(self.num_process)
                ],
            }
            for size in sizes
        ]
        super().__init__(param_groups, defaults)

    def step(self):
        for group in self.param_groups:
            lr: float = group["lr"]
            momentum: float = group["momentum"]
            nesterov: bool = group["nesterov"]
            zeropower_backend = zeropower_backends[group["backend"]]
            backend_steps: int = group["backend_steps"]
            update_buffers: "list[torch.Tensor]" = group["update_buffer"]
            # generate weight updates in distributed fashion
            params: "list[torch.Tensor]" = group["params"]
            assert len(params) % self.num_process == 0
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.num_process]:
                p = params[base_i + self.rank]
                g = p.grad
                assert g is not None
                state = self.state[p] 
                if "momentum_buffer" not in state:
                    state["momentum_buffer"] = torch.zeros_like(g)
                buf: torch.Tensor = state["momentum_buffer"]
                buf.lerp_(g, 1 - momentum)
                g = g.lerp_(buf, momentum) if nesterov else buf
                g = zeropower_backend(g, steps=backend_steps).flatten()
                update_prev()
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.num_process]
            update_prev()


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.register_buffer('inv_freq', (1 / base) ** (torch.arange(0, dim, 2) / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            t = torch.arange(seq_len, device=x.device)
            freqs = torch.outer(t, self.inv_freq)
            self.seq_len_cached = seq_len
            self.cos_cached = freqs.cos()
            self.sin_cached = freqs.sin()
        cos, sin = self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
        # apply_rotary_emb(x, cos, sin)
        x1, x2 = x.chunk(2, dim=3)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, n_head):
        super().__init__()
        assert dim % n_head == 0
        self.n_head = n_head
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        # value residual lambda
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5])) # @Grad62304977
        # rotary embeddings
        self.rotary = Rotary(dim // n_head) # dim // n_head = head_dim
        # output projection
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: torch.Tensor, vi: torch.Tensor, block_mask: BlockMask) -> torch.Tensor:
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q: torch.Tensor = self.c_q(x).view(B, T, self.n_head, -1)
        k: torch.Tensor = self.c_k(x).view(B, T, self.n_head, -1)
        v: torch.Tensor = self.c_v(x).view(B, T, self.n_head, -1)
        v = self.lambdas[0] * v + self.lambdas[1] * vi.view_as(v) # @Grad62304977
        q, k = norm(q), norm(k) # QK norm suggested by @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim: int):
        super().__init__()
        self.c_fc   = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config.n_embd, config.n_head)
        self.mlp = MLP(config.n_embd)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: torch.Tensor, vi: torch.Tensor, x0: torch.Tensor, block_mask: BlockMask) -> torch.Tensor:
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x = x + self.attn(norm(x), vi, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768
    lm_head_softcap : int = 30

class GPT(nn.Module):

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.n_layer = config.n_layer
        self.lm_head_softcap = config.lm_head_softcap

        # U-net design by @brendanh0gan
        self.num_encoder_layers = config.n_layer // 2 # Half of the layers for encoder
        self.num_decoder_layers = config.n_layer - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
            # U-net structure on token value embeddings by @leloykun
            vte = nn.Embedding(config.vocab_size, config.n_embd*self.num_encoder_layers),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = CastedLinear(config.n_embd, config.vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(self, idx: torch.Tensor, target: torch.Tensor, sliding_window: torch.Tensor) -> torch.Tensor:
        BLOCK_SIZE = 128
        assert idx.ndim == 1
        docs = (idx == 50256).cumsum(0)
        docs_low = docs.reshape(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.reshape(-1, BLOCK_SIZE)[:, -1].contiguous()
        def document_sliding_window_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            window_mask = q_idx - kv_idx < sliding_window
            return causal_mask & document_mask & window_mask

        S = len(idx)
        def create_sliding_window_causal_mask(S: int, sliding_window: torch.Tensor):
            kv_idx = block_idx = torch.arange(S // BLOCK_SIZE, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_mask = q_idx >= kv_idx
            document_mask = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            window_mask = q_idx - kv_idx < ((sliding_window + BLOCK_SIZE - 1) // BLOCK_SIZE)
            dense_mask = causal_mask & document_mask & window_mask
            dense_mask = dense_mask.to(torch.int32)
            num_blocks = dense_mask.sum(dim=-1).to(torch.int32)
            indices = torch.argsort(dense_mask, dim=-1, descending=True, stable=True).to(torch.int32)
            num_blocks = num_blocks[None, None, :].contiguous()
            indices = indices[None, None, :].contiguous()
            return BlockMask.from_kv_blocks(num_blocks, indices, BLOCK_SIZE=BLOCK_SIZE, mask_mod=document_sliding_window_causal)
        block_mask = create_sliding_window_causal_mask(S, sliding_window)

        # forward the GPT model itself
        x = self.transformer.wte(idx[None]) # token embeddings of shape (b, t, n_embd)
        x = norm(x) # @Grad62304977
        x0 = x
        vi = self.transformer.vte(idx[None]).chunk(self.num_encoder_layers, dim=-1)

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.transformer.h[i](x, vi[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.transformer.h[self.num_encoder_layers + i](x, vi[self.num_encoder_layers-1-i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = self.lm_head_softcap * torch.tanh(logits / self.lm_head_softcap) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(file: Path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32)
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    return int(header[2]) # number of tokens (claimed)

def _load_data_shard(file: Path, ntok: int):
    with file.open("rb") as f:
        tokens = torch.empty(ntok, dtype=torch.uint16, pin_memory=True)
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())
        assert nbytes == 2 * ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.T = T

        # glob files that match the pattern
        self.files = sorted(Path.cwd().glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        self.ntoks = [_peek_data_shard(file) for file in self.files]
        assert min(self.ntoks) >= num_processes * T + 1
        self.ntok_total = sum(self.ntoks)

        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard], self.ntoks[self.current_shard])

    def next_batch(self):
        batch_size = self.T * self.num_processes
        buf = self.tokens[self.current_position:self.current_position+self.T+1]
        # host side async is sufficient;
        # no performance improvement was observed when introducing a separate stream.
        x = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # inputs
        y = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # targets
        # advance current position and load next shard if necessary
        self.current_position += batch_size
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        return x, y

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8 # batch size, in sequences, across all devices
    sequence_length : int = 64*1024 # sequence length, in tokens
    num_iterations : int = 1480 # number of iterations to run
    warmup_iters : int = 0
    cooldown_iters : int = 600 # number of iterations of linear warmup/cooldown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    # os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write(code)
        f.write('='*100 + '\n')
def print0(s, logonly=False):
    if master_process:
        with open(logfile, "a") as f:
            if not logonly:
                print(s)
            f.write(s+'\n')
# log information about the hardware/software environment this is running on
# and print the full `nvidia-smi` to file
print0(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:")
import subprocess
result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
print0(f'{result.stdout}', logonly=True)
print0('='*100, logonly=True)

# convenience variables
T = args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (T * ddp_world_size) == 0
val_steps = args.val_tokens // (T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (ddp_world_size) == 0
train_accumulation_steps = args.batch_size // ddp_world_size
assert train_accumulation_steps == 1

# load tokens
train_loader = DistributedDataLoader(args.input_bin, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, T, ddp_rank, ddp_world_size)
print0(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
print0(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
print0('='*100, logonly=True)
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model

# init the optimizer(s)
optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight, raw_model.transformer.vte.weight], lr=0.6, betas=(0.8, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight], lr=0.008, betas=(0.8, 0.95), fused=True)
params = list(raw_model.transformer.h.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
optimizer3 = Muon(matrix_params, lr=0.05, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.8, 0.95), fused=True)
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and cooldown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.cooldown_iters:
        return 1.0
    # 3) linear cooldown
    else:
        decay_ratio = (args.num_iterations - it) / args.cooldown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

sliding_window_size = torch.tensor(64, dtype=torch.int32, device="cuda")
sw_size_prev = 64
# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Set the sliding window size for the current step, in chunks of 64. By @fernbear.bsky.social
    sw_size =  64 * int((64 + (1792 - 64) * step / args.num_iterations) // 64)
    if sw_size != sw_size_prev:
        sliding_window_size.copy_(sw_size, non_blocking=True)
        sw_size_prev = sw_size

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                x_val, y_val = val_loader.next_batch()
                val_loss += model(x_val, y_val, sliding_window=sliding_window_size)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        print0(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        # torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    loss = model(x, y, sliding_window=sliding_window_size)
    loss.backward()
    del loss
    # advance the dataset for the next batch
    x, y = train_loader.next_batch()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer3.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{args.num_iterations} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.6.0.dev20241203+cu124 compiled for CUDA 12.4
nvidia-smi:
Sun Dec  8 08:49:20 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.6     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:65:02.0 Off |                    0 |
| N/A   37C    P0              74W / 700W |      7MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:67:02.0 Off |                    0 |
| N/A   46C    P0             109W / 700W |     26MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:69:02.0 Off |                    0 |
| N/A   46C    P0             124W / 700W |     45MiB / 81559MiB |      2%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:6B:02.0 Off |                    0 |
| N/A   39C    P0              81W / 700W |     26MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:6F:02.0 Off |                    0 |
| N/A   38C    P0              73W / 700W |      7MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:71:02.0 Off |                    0 |
| N/A   45C    P0              75W / 700W |      7MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:73:02.0 Off |                    0 |
| N/A   46C    P0              99W / 700W |     27MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:75:02.0 Off |                    0 |
| N/A   39C    P0             124W / 700W |    533MiB / 81559MiB |      1%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training DataLoader: total number of tokens: 3200000000 across 32 files
Validation DataLoader: total number of tokens: 100000000 across 1 files
====================================================================================================
step:0/1480 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1480 train_time:23244ms step_avg:nanms
step:2/1480 train_time:23330ms step_avg:nanms
step:3/1480 train_time:23468ms step_avg:nanms
step:4/1480 train_time:23609ms step_avg:nanms
step:5/1480 train_time:23749ms step_avg:nanms
step:6/1480 train_time:23891ms step_avg:nanms
step:7/1480 train_time:24031ms step_avg:nanms
step:8/1480 train_time:24174ms step_avg:nanms
step:9/1480 train_time:24322ms step_avg:nanms
step:10/1480 train_time:24466ms step_avg:nanms
step:11/1480 train_time:141ms step_avg:nanms
step:12/1480 train_time:283ms step_avg:nanms
step:13/1480 train_time:424ms step_avg:141.32ms
step:14/1480 train_time:564ms step_avg:141.11ms
step:15/1480 train_time:707ms step_avg:141.35ms
step:16/1480 train_time:851ms step_avg:141.76ms
step:17/1480 train_time:994ms step_avg:141.99ms
step:18/1480 train_time:1138ms step_avg:142.21ms
step:19/1480 train_time:1280ms step_avg:142.26ms
step:20/1480 train_time:1423ms step_avg:142.31ms
step:21/1480 train_time:1564ms step_avg:142.15ms
step:22/1480 train_time:1705ms step_avg:142.07ms
step:23/1480 train_time:1848ms step_avg:142.18ms
step:24/1480 train_time:1993ms step_avg:142.37ms
step:25/1480 train_time:2138ms step_avg:142.51ms
step:26/1480 train_time:2281ms step_avg:142.59ms
step:27/1480 train_time:2425ms step_avg:142.63ms
step:28/1480 train_time:2565ms step_avg:142.52ms
step:29/1480 train_time:2707ms step_avg:142.49ms
step:30/1480 train_time:2851ms step_avg:142.53ms
step:31/1480 train_time:2996ms step_avg:142.64ms
step:32/1480 train_time:3141ms step_avg:142.77ms
step:33/1480 train_time:3285ms step_avg:142.81ms
step:34/1480 train_time:3427ms step_avg:142.80ms
step:35/1480 train_time:3568ms step_avg:142.73ms
step:36/1480 train_time:3709ms step_avg:142.67ms
step:37/1480 train_time:3854ms step_avg:142.74ms
step:38/1480 train_time:3997ms step_avg:142.76ms
step:39/1480 train_time:4141ms step_avg:142.80ms
step:40/1480 train_time:4284ms step_avg:142.81ms
step:41/1480 train_time:4426ms step_avg:142.79ms
step:42/1480 train_time:4568ms step_avg:142.74ms
step:43/1480 train_time:4710ms step_avg:142.73ms
step:44/1480 train_time:4854ms step_avg:142.77ms
step:45/1480 train_time:4998ms step_avg:142.80ms
step:46/1480 train_time:5141ms step_avg:142.79ms
step:47/1480 train_time:5284ms step_avg:142.80ms
step:48/1480 train_time:5426ms step_avg:142.80ms
step:49/1480 train_time:5568ms step_avg:142.76ms
step:50/1480 train_time:5712ms step_avg:142.81ms
step:51/1480 train_time:5856ms step_avg:142.84ms
step:52/1480 train_time:5999ms step_avg:142.84ms
step:53/1480 train_time:6143ms step_avg:142.87ms
step:54/1480 train_time:6286ms step_avg:142.86ms
step:55/1480 train_time:6428ms step_avg:142.84ms
step:56/1480 train_time:6571ms step_avg:142.84ms
step:57/1480 train_time:6714ms step_avg:142.85ms
step:58/1480 train_time:6858ms step_avg:142.88ms
step:59/1480 train_time:7001ms step_avg:142.88ms
step:60/1480 train_time:7145ms step_avg:142.89ms
step:61/1480 train_time:7286ms step_avg:142.85ms
step:62/1480 train_time:7427ms step_avg:142.83ms
step:63/1480 train_time:7570ms step_avg:142.83ms
step:64/1480 train_time:7715ms step_avg:142.87ms
step:65/1480 train_time:7858ms step_avg:142.88ms
step:66/1480 train_time:8001ms step_avg:142.88ms
step:67/1480 train_time:8145ms step_avg:142.90ms
step:68/1480 train_time:8287ms step_avg:142.88ms
step:69/1480 train_time:8428ms step_avg:142.85ms
step:70/1480 train_time:8569ms step_avg:142.82ms
step:71/1480 train_time:8713ms step_avg:142.83ms
step:72/1480 train_time:8857ms step_avg:142.85ms
step:73/1480 train_time:9000ms step_avg:142.85ms
step:74/1480 train_time:9143ms step_avg:142.86ms
step:75/1480 train_time:9285ms step_avg:142.85ms
step:76/1480 train_time:9427ms step_avg:142.83ms
step:77/1480 train_time:9568ms step_avg:142.80ms
step:78/1480 train_time:9711ms step_avg:142.81ms
step:79/1480 train_time:9856ms step_avg:142.84ms
step:80/1480 train_time:10000ms step_avg:142.86ms
step:81/1480 train_time:10143ms step_avg:142.85ms
step:82/1480 train_time:10285ms step_avg:142.85ms
step:83/1480 train_time:10428ms step_avg:142.85ms
step:84/1480 train_time:10571ms step_avg:142.86ms
step:85/1480 train_time:10715ms step_avg:142.87ms
step:86/1480 train_time:10860ms step_avg:142.89ms
step:87/1480 train_time:11002ms step_avg:142.89ms
step:88/1480 train_time:11146ms step_avg:142.90ms
step:89/1480 train_time:11288ms step_avg:142.88ms
step:90/1480 train_time:11428ms step_avg:142.86ms
step:91/1480 train_time:11569ms step_avg:142.83ms
step:92/1480 train_time:11711ms step_avg:142.82ms
step:93/1480 train_time:11855ms step_avg:142.83ms
step:94/1480 train_time:11998ms step_avg:142.84ms
step:95/1480 train_time:12141ms step_avg:142.84ms
step:96/1480 train_time:12284ms step_avg:142.83ms
step:97/1480 train_time:12425ms step_avg:142.81ms
step:98/1480 train_time:12566ms step_avg:142.79ms
step:99/1480 train_time:12708ms step_avg:142.79ms
step:100/1480 train_time:12852ms step_avg:142.80ms
step:101/1480 train_time:12996ms step_avg:142.82ms
step:102/1480 train_time:13140ms step_avg:142.83ms
step:103/1480 train_time:13283ms step_avg:142.83ms
step:104/1480 train_time:13425ms step_avg:142.82ms
step:105/1480 train_time:13566ms step_avg:142.80ms
step:106/1480 train_time:13708ms step_avg:142.79ms
step:107/1480 train_time:13850ms step_avg:142.78ms
step:108/1480 train_time:13993ms step_avg:142.79ms
step:109/1480 train_time:14139ms step_avg:142.82ms
step:110/1480 train_time:14280ms step_avg:142.80ms
step:111/1480 train_time:14425ms step_avg:142.82ms
step:112/1480 train_time:14571ms step_avg:142.85ms
step:113/1480 train_time:14720ms step_avg:142.91ms
step:114/1480 train_time:14866ms step_avg:142.94ms
step:115/1480 train_time:15014ms step_avg:142.99ms
step:116/1480 train_time:15162ms step_avg:143.04ms
step:117/1480 train_time:15308ms step_avg:143.07ms
step:118/1480 train_time:15455ms step_avg:143.10ms
step:119/1480 train_time:15602ms step_avg:143.14ms
step:120/1480 train_time:15748ms step_avg:143.17ms
step:121/1480 train_time:15896ms step_avg:143.21ms
step:122/1480 train_time:16044ms step_avg:143.25ms
step:123/1480 train_time:16191ms step_avg:143.28ms
step:124/1480 train_time:16340ms step_avg:143.33ms
step:125/1480 train_time:16486ms step_avg:143.36ms
step:125/1480 val_loss:4.4164 train_time:16544ms step_avg:143.86ms
step:126/1480 train_time:16640ms step_avg:143.45ms
step:127/1480 train_time:16791ms step_avg:143.51ms
step:128/1480 train_time:16936ms step_avg:143.53ms
step:129/1480 train_time:17083ms step_avg:143.56ms
step:130/1480 train_time:17229ms step_avg:143.58ms
step:131/1480 train_time:17375ms step_avg:143.60ms
step:132/1480 train_time:17521ms step_avg:143.62ms
step:133/1480 train_time:17672ms step_avg:143.67ms
step:134/1480 train_time:17818ms step_avg:143.69ms
step:135/1480 train_time:17966ms step_avg:143.72ms
step:136/1480 train_time:18112ms step_avg:143.75ms
step:137/1480 train_time:18258ms step_avg:143.76ms
step:138/1480 train_time:18404ms step_avg:143.78ms
step:139/1480 train_time:18552ms step_avg:143.81ms
step:140/1480 train_time:18698ms step_avg:143.83ms
step:141/1480 train_time:18846ms step_avg:143.86ms
step:142/1480 train_time:18993ms step_avg:143.89ms
step:143/1480 train_time:19140ms step_avg:143.91ms
step:144/1480 train_time:19288ms step_avg:143.94ms
step:145/1480 train_time:19433ms step_avg:143.95ms
step:146/1480 train_time:19580ms step_avg:143.97ms
step:147/1480 train_time:19728ms step_avg:144.00ms
step:148/1480 train_time:19874ms step_avg:144.02ms
step:149/1480 train_time:20020ms step_avg:144.03ms
step:150/1480 train_time:20168ms step_avg:144.06ms
step:151/1480 train_time:20314ms step_avg:144.07ms
step:152/1480 train_time:20459ms step_avg:144.08ms
step:153/1480 train_time:20606ms step_avg:144.10ms
step:154/1480 train_time:20754ms step_avg:144.12ms
step:155/1480 train_time:20901ms step_avg:144.14ms
step:156/1480 train_time:21049ms step_avg:144.17ms
step:157/1480 train_time:21195ms step_avg:144.18ms
step:158/1480 train_time:21342ms step_avg:144.20ms
step:159/1480 train_time:21489ms step_avg:144.22ms
step:160/1480 train_time:21635ms step_avg:144.23ms
step:161/1480 train_time:21781ms step_avg:144.24ms
step:162/1480 train_time:21928ms step_avg:144.26ms
step:163/1480 train_time:22074ms step_avg:144.28ms
step:164/1480 train_time:22221ms step_avg:144.29ms
step:165/1480 train_time:22370ms step_avg:144.32ms
step:166/1480 train_time:22515ms step_avg:144.33ms
step:167/1480 train_time:22663ms step_avg:144.35ms
step:168/1480 train_time:22810ms step_avg:144.37ms
step:169/1480 train_time:22956ms step_avg:144.38ms
step:170/1480 train_time:23103ms step_avg:144.39ms
step:171/1480 train_time:23250ms step_avg:144.41ms
step:172/1480 train_time:23396ms step_avg:144.42ms
step:173/1480 train_time:23543ms step_avg:144.44ms
step:174/1480 train_time:23691ms step_avg:144.46ms
step:175/1480 train_time:23836ms step_avg:144.46ms
step:176/1480 train_time:23985ms step_avg:144.49ms
step:177/1480 train_time:24132ms step_avg:144.50ms
step:178/1480 train_time:24279ms step_avg:144.52ms
step:179/1480 train_time:24426ms step_avg:144.53ms
step:180/1480 train_time:24573ms step_avg:144.55ms
step:181/1480 train_time:24719ms step_avg:144.56ms
step:182/1480 train_time:24867ms step_avg:144.57ms
step:183/1480 train_time:25013ms step_avg:144.59ms
step:184/1480 train_time:25159ms step_avg:144.59ms
step:185/1480 train_time:25307ms step_avg:144.61ms
step:186/1480 train_time:25454ms step_avg:144.62ms
step:187/1480 train_time:25598ms step_avg:144.62ms
step:188/1480 train_time:25747ms step_avg:144.65ms
step:189/1480 train_time:25894ms step_avg:144.66ms
step:190/1480 train_time:26039ms step_avg:144.66ms
step:191/1480 train_time:26187ms step_avg:144.68ms
step:192/1480 train_time:26334ms step_avg:144.69ms
step:193/1480 train_time:26481ms step_avg:144.71ms
step:194/1480 train_time:26628ms step_avg:144.72ms
step:195/1480 train_time:26774ms step_avg:144.73ms
step:196/1480 train_time:26921ms step_avg:144.74ms
step:197/1480 train_time:27069ms step_avg:144.75ms
step:198/1480 train_time:27214ms step_avg:144.76ms
step:199/1480 train_time:27361ms step_avg:144.77ms
step:200/1480 train_time:27508ms step_avg:144.78ms
step:201/1480 train_time:27655ms step_avg:144.79ms
step:202/1480 train_time:27799ms step_avg:144.79ms
step:203/1480 train_time:27947ms step_avg:144.80ms
step:204/1480 train_time:28094ms step_avg:144.81ms
step:205/1480 train_time:28242ms step_avg:144.83ms
step:206/1480 train_time:28390ms step_avg:144.85ms
step:207/1480 train_time:28536ms step_avg:144.85ms
step:208/1480 train_time:28683ms step_avg:144.86ms
step:209/1480 train_time:28830ms step_avg:144.87ms
step:210/1480 train_time:28976ms step_avg:144.88ms
step:211/1480 train_time:29123ms step_avg:144.89ms
step:212/1480 train_time:29270ms step_avg:144.90ms
step:213/1480 train_time:29416ms step_avg:144.91ms
step:214/1480 train_time:29564ms step_avg:144.92ms
step:215/1480 train_time:29710ms step_avg:144.93ms
step:216/1480 train_time:29855ms step_avg:144.93ms
step:217/1480 train_time:30001ms step_avg:144.93ms
step:218/1480 train_time:30150ms step_avg:144.95ms
step:219/1480 train_time:30295ms step_avg:144.95ms
step:220/1480 train_time:30442ms step_avg:144.96ms
step:221/1480 train_time:30592ms step_avg:144.98ms
step:222/1480 train_time:30742ms step_avg:145.01ms
step:223/1480 train_time:30894ms step_avg:145.04ms
step:224/1480 train_time:31044ms step_avg:145.06ms
step:225/1480 train_time:31195ms step_avg:145.09ms
step:226/1480 train_time:31345ms step_avg:145.12ms
step:227/1480 train_time:31496ms step_avg:145.14ms
step:228/1480 train_time:31648ms step_avg:145.17ms
step:229/1480 train_time:31798ms step_avg:145.20ms
step:230/1480 train_time:31949ms step_avg:145.22ms
step:231/1480 train_time:32097ms step_avg:145.24ms
step:232/1480 train_time:32250ms step_avg:145.27ms
step:233/1480 train_time:32399ms step_avg:145.29ms
step:234/1480 train_time:32550ms step_avg:145.31ms
step:235/1480 train_time:32700ms step_avg:145.34ms
step:236/1480 train_time:32851ms step_avg:145.36ms
step:237/1480 train_time:33001ms step_avg:145.38ms
step:238/1480 train_time:33152ms step_avg:145.40ms
step:239/1480 train_time:33301ms step_avg:145.42ms
step:240/1480 train_time:33453ms step_avg:145.45ms
step:241/1480 train_time:33601ms step_avg:145.46ms
step:242/1480 train_time:33752ms step_avg:145.48ms
step:243/1480 train_time:33901ms step_avg:145.50ms
step:244/1480 train_time:34053ms step_avg:145.53ms
step:245/1480 train_time:34203ms step_avg:145.55ms
step:246/1480 train_time:34353ms step_avg:145.56ms
step:247/1480 train_time:34504ms step_avg:145.58ms
step:248/1480 train_time:34655ms step_avg:145.61ms
step:249/1480 train_time:34804ms step_avg:145.62ms
step:250/1480 train_time:34955ms step_avg:145.65ms
step:250/1480 val_loss:3.9931 train_time:35013ms step_avg:145.89ms
step:251/1480 train_time:35111ms step_avg:145.69ms
step:252/1480 train_time:35264ms step_avg:145.72ms
step:253/1480 train_time:35414ms step_avg:145.74ms
step:254/1480 train_time:35563ms step_avg:145.75ms
step:255/1480 train_time:35712ms step_avg:145.76ms
step:256/1480 train_time:35861ms step_avg:145.78ms
step:257/1480 train_time:36012ms step_avg:145.80ms
step:258/1480 train_time:36164ms step_avg:145.82ms
step:259/1480 train_time:36315ms step_avg:145.85ms
step:260/1480 train_time:36467ms step_avg:145.87ms
step:261/1480 train_time:36617ms step_avg:145.88ms
step:262/1480 train_time:36767ms step_avg:145.90ms
step:263/1480 train_time:36916ms step_avg:145.91ms
step:264/1480 train_time:37068ms step_avg:145.94ms
step:265/1480 train_time:37218ms step_avg:145.95ms
step:266/1480 train_time:37370ms step_avg:145.98ms
step:267/1480 train_time:37520ms step_avg:145.99ms
step:268/1480 train_time:37672ms step_avg:146.02ms
step:269/1480 train_time:37822ms step_avg:146.03ms
step:270/1480 train_time:37972ms step_avg:146.04ms
step:271/1480 train_time:38121ms step_avg:146.06ms
step:272/1480 train_time:38273ms step_avg:146.08ms
step:273/1480 train_time:38422ms step_avg:146.09ms
step:274/1480 train_time:38573ms step_avg:146.11ms
step:275/1480 train_time:38725ms step_avg:146.13ms
step:276/1480 train_time:38875ms step_avg:146.15ms
step:277/1480 train_time:39024ms step_avg:146.16ms
step:278/1480 train_time:39175ms step_avg:146.17ms
step:279/1480 train_time:39325ms step_avg:146.19ms
step:280/1480 train_time:39477ms step_avg:146.21ms
step:281/1480 train_time:39628ms step_avg:146.23ms
step:282/1480 train_time:39778ms step_avg:146.24ms
step:283/1480 train_time:39927ms step_avg:146.25ms
step:284/1480 train_time:40077ms step_avg:146.27ms
step:285/1480 train_time:40228ms step_avg:146.28ms
step:286/1480 train_time:40378ms step_avg:146.30ms
step:287/1480 train_time:40529ms step_avg:146.32ms
step:288/1480 train_time:40680ms step_avg:146.33ms
step:289/1480 train_time:40831ms step_avg:146.35ms
step:290/1480 train_time:40981ms step_avg:146.36ms
step:291/1480 train_time:41131ms step_avg:146.37ms
step:292/1480 train_time:41281ms step_avg:146.38ms
step:293/1480 train_time:41431ms step_avg:146.40ms
step:294/1480 train_time:41582ms step_avg:146.41ms
step:295/1480 train_time:41733ms step_avg:146.43ms
step:296/1480 train_time:41884ms step_avg:146.45ms
step:297/1480 train_time:42034ms step_avg:146.46ms
step:298/1480 train_time:42186ms step_avg:146.48ms
step:299/1480 train_time:42337ms step_avg:146.50ms
step:300/1480 train_time:42489ms step_avg:146.51ms
step:301/1480 train_time:42638ms step_avg:146.52ms
step:302/1480 train_time:42791ms step_avg:146.54ms
step:303/1480 train_time:42939ms step_avg:146.55ms
step:304/1480 train_time:43090ms step_avg:146.56ms
step:305/1480 train_time:43239ms step_avg:146.57ms
step:306/1480 train_time:43392ms step_avg:146.59ms
step:307/1480 train_time:43540ms step_avg:146.60ms
step:308/1480 train_time:43691ms step_avg:146.62ms
step:309/1480 train_time:43842ms step_avg:146.63ms
step:310/1480 train_time:43994ms step_avg:146.65ms
step:311/1480 train_time:44144ms step_avg:146.66ms
step:312/1480 train_time:44294ms step_avg:146.67ms
step:313/1480 train_time:44446ms step_avg:146.69ms
step:314/1480 train_time:44596ms step_avg:146.70ms
step:315/1480 train_time:44746ms step_avg:146.71ms
step:316/1480 train_time:44897ms step_avg:146.72ms
step:317/1480 train_time:45048ms step_avg:146.74ms
step:318/1480 train_time:45200ms step_avg:146.75ms
step:319/1480 train_time:45350ms step_avg:146.76ms
step:320/1480 train_time:45501ms step_avg:146.78ms
step:321/1480 train_time:45652ms step_avg:146.79ms
step:322/1480 train_time:45802ms step_avg:146.80ms
step:323/1480 train_time:45953ms step_avg:146.81ms
step:324/1480 train_time:46103ms step_avg:146.83ms
step:325/1480 train_time:46254ms step_avg:146.84ms
step:326/1480 train_time:46405ms step_avg:146.85ms
step:327/1480 train_time:46555ms step_avg:146.86ms
step:328/1480 train_time:46707ms step_avg:146.88ms
step:329/1480 train_time:46857ms step_avg:146.89ms
step:330/1480 train_time:47011ms step_avg:146.91ms
step:331/1480 train_time:47166ms step_avg:146.93ms
step:332/1480 train_time:47319ms step_avg:146.95ms
step:333/1480 train_time:47473ms step_avg:146.97ms
step:334/1480 train_time:47625ms step_avg:146.99ms
step:335/1480 train_time:47779ms step_avg:147.01ms
step:336/1480 train_time:47933ms step_avg:147.03ms
step:337/1480 train_time:48088ms step_avg:147.06ms
step:338/1480 train_time:48241ms step_avg:147.08ms
step:339/1480 train_time:48395ms step_avg:147.10ms
step:340/1480 train_time:48548ms step_avg:147.12ms
step:341/1480 train_time:48702ms step_avg:147.14ms
step:342/1480 train_time:48856ms step_avg:147.16ms
step:343/1480 train_time:49011ms step_avg:147.18ms
step:344/1480 train_time:49166ms step_avg:147.20ms
step:345/1480 train_time:49321ms step_avg:147.23ms
step:346/1480 train_time:49476ms step_avg:147.25ms
step:347/1480 train_time:49629ms step_avg:147.27ms
step:348/1480 train_time:49783ms step_avg:147.29ms
step:349/1480 train_time:49936ms step_avg:147.30ms
step:350/1480 train_time:50091ms step_avg:147.33ms
step:351/1480 train_time:50245ms step_avg:147.35ms
step:352/1480 train_time:50400ms step_avg:147.37ms
step:353/1480 train_time:50553ms step_avg:147.38ms
step:354/1480 train_time:50706ms step_avg:147.40ms
step:355/1480 train_time:50860ms step_avg:147.42ms
step:356/1480 train_time:51013ms step_avg:147.44ms
step:357/1480 train_time:51167ms step_avg:147.45ms
step:358/1480 train_time:51320ms step_avg:147.47ms
step:359/1480 train_time:51475ms step_avg:147.49ms
step:360/1480 train_time:51629ms step_avg:147.51ms
step:361/1480 train_time:51784ms step_avg:147.53ms
step:362/1480 train_time:51939ms step_avg:147.55ms
step:363/1480 train_time:52092ms step_avg:147.57ms
step:364/1480 train_time:52247ms step_avg:147.59ms
step:365/1480 train_time:52402ms step_avg:147.61ms
step:366/1480 train_time:52555ms step_avg:147.63ms
step:367/1480 train_time:52709ms step_avg:147.64ms
step:368/1480 train_time:52863ms step_avg:147.66ms
step:369/1480 train_time:53016ms step_avg:147.68ms
step:370/1480 train_time:53170ms step_avg:147.69ms
step:371/1480 train_time:53323ms step_avg:147.71ms
step:372/1480 train_time:53478ms step_avg:147.73ms
step:373/1480 train_time:53633ms step_avg:147.75ms
step:374/1480 train_time:53786ms step_avg:147.76ms
step:375/1480 train_time:53938ms step_avg:147.78ms
step:375/1480 val_loss:3.8099 train_time:54000ms step_avg:147.95ms
step:376/1480 train_time:54097ms step_avg:147.81ms
step:377/1480 train_time:54251ms step_avg:147.82ms
step:378/1480 train_time:54405ms step_avg:147.84ms
step:379/1480 train_time:54556ms step_avg:147.85ms
step:380/1480 train_time:54710ms step_avg:147.86ms
step:381/1480 train_time:54862ms step_avg:147.88ms
step:382/1480 train_time:55016ms step_avg:147.89ms
step:383/1480 train_time:55173ms step_avg:147.92ms
step:384/1480 train_time:55327ms step_avg:147.93ms
step:385/1480 train_time:55481ms step_avg:147.95ms
step:386/1480 train_time:55635ms step_avg:147.97ms
step:387/1480 train_time:55789ms step_avg:147.98ms
step:388/1480 train_time:55942ms step_avg:148.00ms
step:389/1480 train_time:56095ms step_avg:148.01ms
step:390/1480 train_time:56249ms step_avg:148.02ms
step:391/1480 train_time:56403ms step_avg:148.04ms
step:392/1480 train_time:56556ms step_avg:148.05ms
step:393/1480 train_time:56710ms step_avg:148.07ms
step:394/1480 train_time:56863ms step_avg:148.08ms
step:395/1480 train_time:57016ms step_avg:148.09ms
step:396/1480 train_time:57171ms step_avg:148.11ms
step:397/1480 train_time:57325ms step_avg:148.13ms
step:398/1480 train_time:57479ms step_avg:148.14ms
step:399/1480 train_time:57632ms step_avg:148.16ms
step:400/1480 train_time:57787ms step_avg:148.17ms
step:401/1480 train_time:57942ms step_avg:148.19ms
step:402/1480 train_time:58098ms step_avg:148.21ms
step:403/1480 train_time:58249ms step_avg:148.22ms
step:404/1480 train_time:58404ms step_avg:148.23ms
step:405/1480 train_time:58558ms step_avg:148.25ms
step:406/1480 train_time:58712ms step_avg:148.26ms
step:407/1480 train_time:58868ms step_avg:148.28ms
step:408/1480 train_time:59022ms step_avg:148.30ms
step:409/1480 train_time:59176ms step_avg:148.31ms
step:410/1480 train_time:59329ms step_avg:148.32ms
step:411/1480 train_time:59483ms step_avg:148.34ms
step:412/1480 train_time:59636ms step_avg:148.35ms
step:413/1480 train_time:59789ms step_avg:148.36ms
step:414/1480 train_time:59944ms step_avg:148.38ms
step:415/1480 train_time:60098ms step_avg:148.39ms
step:416/1480 train_time:60251ms step_avg:148.40ms
step:417/1480 train_time:60405ms step_avg:148.42ms
step:418/1480 train_time:60559ms step_avg:148.43ms
step:419/1480 train_time:60712ms step_avg:148.44ms
step:420/1480 train_time:60867ms step_avg:148.46ms
step:421/1480 train_time:61020ms step_avg:148.47ms
step:422/1480 train_time:61175ms step_avg:148.48ms
step:423/1480 train_time:61329ms step_avg:148.50ms
step:424/1480 train_time:61487ms step_avg:148.52ms
step:425/1480 train_time:61639ms step_avg:148.53ms
step:426/1480 train_time:61793ms step_avg:148.54ms
step:427/1480 train_time:61946ms step_avg:148.55ms
step:428/1480 train_time:62101ms step_avg:148.57ms
step:429/1480 train_time:62254ms step_avg:148.58ms
step:430/1480 train_time:62409ms step_avg:148.59ms
step:431/1480 train_time:62563ms step_avg:148.61ms
step:432/1480 train_time:62716ms step_avg:148.62ms
step:433/1480 train_time:62871ms step_avg:148.63ms
step:434/1480 train_time:63025ms step_avg:148.64ms
step:435/1480 train_time:63179ms step_avg:148.66ms
step:436/1480 train_time:63332ms step_avg:148.67ms
step:437/1480 train_time:63485ms step_avg:148.68ms
step:438/1480 train_time:63638ms step_avg:148.69ms
step:439/1480 train_time:63792ms step_avg:148.70ms
step:440/1480 train_time:63947ms step_avg:148.72ms
step:441/1480 train_time:64106ms step_avg:148.74ms
step:442/1480 train_time:64264ms step_avg:148.76ms
step:443/1480 train_time:64421ms step_avg:148.78ms
step:444/1480 train_time:64575ms step_avg:148.79ms
step:445/1480 train_time:64731ms step_avg:148.81ms
step:446/1480 train_time:64886ms step_avg:148.82ms
step:447/1480 train_time:65041ms step_avg:148.84ms
step:448/1480 train_time:65196ms step_avg:148.85ms
step:449/1480 train_time:65354ms step_avg:148.87ms
step:450/1480 train_time:65513ms step_avg:148.89ms
step:451/1480 train_time:65670ms step_avg:148.91ms
step:452/1480 train_time:65828ms step_avg:148.93ms
step:453/1480 train_time:65985ms step_avg:148.95ms
step:454/1480 train_time:66141ms step_avg:148.97ms
step:455/1480 train_time:66297ms step_avg:148.98ms
step:456/1480 train_time:66452ms step_avg:149.00ms
step:457/1480 train_time:66609ms step_avg:149.01ms
step:458/1480 train_time:66765ms step_avg:149.03ms
step:459/1480 train_time:66923ms step_avg:149.05ms
step:460/1480 train_time:67079ms step_avg:149.07ms
step:461/1480 train_time:67236ms step_avg:149.08ms
step:462/1480 train_time:67392ms step_avg:149.10ms
step:463/1480 train_time:67549ms step_avg:149.11ms
step:464/1480 train_time:67705ms step_avg:149.13ms
step:465/1480 train_time:67862ms step_avg:149.15ms
step:466/1480 train_time:68018ms step_avg:149.16ms
step:467/1480 train_time:68177ms step_avg:149.18ms
step:468/1480 train_time:68333ms step_avg:149.20ms
step:469/1480 train_time:68490ms step_avg:149.22ms
step:470/1480 train_time:68647ms step_avg:149.23ms
step:471/1480 train_time:68804ms step_avg:149.25ms
step:472/1480 train_time:68960ms step_avg:149.27ms
step:473/1480 train_time:69116ms step_avg:149.28ms
step:474/1480 train_time:69272ms step_avg:149.29ms
step:475/1480 train_time:69429ms step_avg:149.31ms
step:476/1480 train_time:69586ms step_avg:149.33ms
step:477/1480 train_time:69742ms step_avg:149.34ms
step:478/1480 train_time:69899ms step_avg:149.36ms
step:479/1480 train_time:70054ms step_avg:149.37ms
step:480/1480 train_time:70211ms step_avg:149.39ms
step:481/1480 train_time:70368ms step_avg:149.40ms
step:482/1480 train_time:70525ms step_avg:149.42ms
step:483/1480 train_time:70680ms step_avg:149.43ms
step:484/1480 train_time:70836ms step_avg:149.44ms
step:485/1480 train_time:70993ms step_avg:149.46ms
step:486/1480 train_time:71150ms step_avg:149.47ms
step:487/1480 train_time:71308ms step_avg:149.49ms
step:488/1480 train_time:71465ms step_avg:149.51ms
step:489/1480 train_time:71620ms step_avg:149.52ms
step:490/1480 train_time:71775ms step_avg:149.53ms
step:491/1480 train_time:71931ms step_avg:149.54ms
step:492/1480 train_time:72089ms step_avg:149.56ms
step:493/1480 train_time:72245ms step_avg:149.58ms
step:494/1480 train_time:72404ms step_avg:149.60ms
step:495/1480 train_time:72561ms step_avg:149.61ms
step:496/1480 train_time:72718ms step_avg:149.63ms
step:497/1480 train_time:72873ms step_avg:149.64ms
step:498/1480 train_time:73030ms step_avg:149.65ms
step:499/1480 train_time:73188ms step_avg:149.67ms
step:500/1480 train_time:73347ms step_avg:149.69ms
step:500/1480 val_loss:3.6888 train_time:73410ms step_avg:149.82ms
step:501/1480 train_time:73509ms step_avg:149.71ms
step:502/1480 train_time:73669ms step_avg:149.73ms
step:503/1480 train_time:73824ms step_avg:149.74ms
step:504/1480 train_time:73978ms step_avg:149.75ms
step:505/1480 train_time:74133ms step_avg:149.76ms
step:506/1480 train_time:74289ms step_avg:149.78ms
step:507/1480 train_time:74445ms step_avg:149.79ms
step:508/1480 train_time:74602ms step_avg:149.80ms
step:509/1480 train_time:74759ms step_avg:149.82ms
step:510/1480 train_time:74916ms step_avg:149.83ms
step:511/1480 train_time:75072ms step_avg:149.84ms
step:512/1480 train_time:75230ms step_avg:149.86ms
step:513/1480 train_time:75386ms step_avg:149.87ms
step:514/1480 train_time:75544ms step_avg:149.89ms
step:515/1480 train_time:75701ms step_avg:149.90ms
step:516/1480 train_time:75860ms step_avg:149.92ms
step:517/1480 train_time:76017ms step_avg:149.93ms
step:518/1480 train_time:76174ms step_avg:149.95ms
step:519/1480 train_time:76329ms step_avg:149.96ms
step:520/1480 train_time:76489ms step_avg:149.98ms
step:521/1480 train_time:76646ms step_avg:149.99ms
step:522/1480 train_time:76804ms step_avg:150.01ms
step:523/1480 train_time:76961ms step_avg:150.02ms
step:524/1480 train_time:77117ms step_avg:150.03ms
step:525/1480 train_time:77273ms step_avg:150.04ms
step:526/1480 train_time:77430ms step_avg:150.06ms
step:527/1480 train_time:77587ms step_avg:150.07ms
step:528/1480 train_time:77743ms step_avg:150.08ms
step:529/1480 train_time:77898ms step_avg:150.09ms
step:530/1480 train_time:78055ms step_avg:150.11ms
step:531/1480 train_time:78212ms step_avg:150.12ms
step:532/1480 train_time:78368ms step_avg:150.13ms
step:533/1480 train_time:78525ms step_avg:150.14ms
step:534/1480 train_time:78682ms step_avg:150.16ms
step:535/1480 train_time:78838ms step_avg:150.17ms
step:536/1480 train_time:78996ms step_avg:150.18ms
step:537/1480 train_time:79152ms step_avg:150.19ms
step:538/1480 train_time:79309ms step_avg:150.21ms
step:539/1480 train_time:79468ms step_avg:150.22ms
step:540/1480 train_time:79627ms step_avg:150.24ms
step:541/1480 train_time:79783ms step_avg:150.25ms
step:542/1480 train_time:79939ms step_avg:150.26ms
step:543/1480 train_time:80095ms step_avg:150.27ms
step:544/1480 train_time:80252ms step_avg:150.28ms
step:545/1480 train_time:80409ms step_avg:150.30ms
step:546/1480 train_time:80569ms step_avg:150.32ms
step:547/1480 train_time:80725ms step_avg:150.32ms
step:548/1480 train_time:80883ms step_avg:150.34ms
step:549/1480 train_time:81040ms step_avg:150.35ms
step:550/1480 train_time:81198ms step_avg:150.37ms
step:551/1480 train_time:81357ms step_avg:150.38ms
step:552/1480 train_time:81517ms step_avg:150.40ms
step:553/1480 train_time:81676ms step_avg:150.42ms
step:554/1480 train_time:81834ms step_avg:150.43ms
step:555/1480 train_time:81994ms step_avg:150.45ms
step:556/1480 train_time:82152ms step_avg:150.46ms
step:557/1480 train_time:82312ms step_avg:150.48ms
step:558/1480 train_time:82473ms step_avg:150.50ms
step:559/1480 train_time:82631ms step_avg:150.51ms
step:560/1480 train_time:82791ms step_avg:150.53ms
step:561/1480 train_time:82950ms step_avg:150.54ms
step:562/1480 train_time:83110ms step_avg:150.56ms
step:563/1480 train_time:83269ms step_avg:150.58ms
step:564/1480 train_time:83429ms step_avg:150.59ms
step:565/1480 train_time:83588ms step_avg:150.61ms
step:566/1480 train_time:83749ms step_avg:150.63ms
step:567/1480 train_time:83909ms step_avg:150.65ms
step:568/1480 train_time:84069ms step_avg:150.66ms
step:569/1480 train_time:84228ms step_avg:150.68ms
step:570/1480 train_time:84388ms step_avg:150.69ms
step:571/1480 train_time:84549ms step_avg:150.71ms
step:572/1480 train_time:84708ms step_avg:150.73ms
step:573/1480 train_time:84870ms step_avg:150.75ms
step:574/1480 train_time:85031ms step_avg:150.76ms
step:575/1480 train_time:85192ms step_avg:150.78ms
step:576/1480 train_time:85352ms step_avg:150.80ms
step:577/1480 train_time:85511ms step_avg:150.81ms
step:578/1480 train_time:85670ms step_avg:150.83ms
step:579/1480 train_time:85830ms step_avg:150.84ms
step:580/1480 train_time:85991ms step_avg:150.86ms
step:581/1480 train_time:86152ms step_avg:150.88ms
step:582/1480 train_time:86311ms step_avg:150.89ms
step:583/1480 train_time:86471ms step_avg:150.91ms
step:584/1480 train_time:86631ms step_avg:150.93ms
step:585/1480 train_time:86789ms step_avg:150.94ms
step:586/1480 train_time:86951ms step_avg:150.96ms
step:587/1480 train_time:87110ms step_avg:150.97ms
step:588/1480 train_time:87269ms step_avg:150.98ms
step:589/1480 train_time:87429ms step_avg:151.00ms
step:590/1480 train_time:87591ms step_avg:151.02ms
step:591/1480 train_time:87751ms step_avg:151.03ms
step:592/1480 train_time:87911ms step_avg:151.05ms
step:593/1480 train_time:88072ms step_avg:151.07ms
step:594/1480 train_time:88232ms step_avg:151.08ms
step:595/1480 train_time:88394ms step_avg:151.10ms
step:596/1480 train_time:88555ms step_avg:151.12ms
step:597/1480 train_time:88714ms step_avg:151.13ms
step:598/1480 train_time:88872ms step_avg:151.14ms
step:599/1480 train_time:89029ms step_avg:151.15ms
step:600/1480 train_time:89191ms step_avg:151.17ms
step:601/1480 train_time:89351ms step_avg:151.19ms
step:602/1480 train_time:89511ms step_avg:151.20ms
step:603/1480 train_time:89672ms step_avg:151.22ms
step:604/1480 train_time:89831ms step_avg:151.23ms
step:605/1480 train_time:89990ms step_avg:151.24ms
step:606/1480 train_time:90152ms step_avg:151.26ms
step:607/1480 train_time:90312ms step_avg:151.28ms
step:608/1480 train_time:90472ms step_avg:151.29ms
step:609/1480 train_time:90632ms step_avg:151.31ms
step:610/1480 train_time:90792ms step_avg:151.32ms
step:611/1480 train_time:90952ms step_avg:151.33ms
step:612/1480 train_time:91112ms step_avg:151.35ms
step:613/1480 train_time:91272ms step_avg:151.36ms
step:614/1480 train_time:91431ms step_avg:151.38ms
step:615/1480 train_time:91591ms step_avg:151.39ms
step:616/1480 train_time:91750ms step_avg:151.40ms
step:617/1480 train_time:91910ms step_avg:151.42ms
step:618/1480 train_time:92069ms step_avg:151.43ms
step:619/1480 train_time:92230ms step_avg:151.45ms
step:620/1480 train_time:92390ms step_avg:151.46ms
step:621/1480 train_time:92550ms step_avg:151.47ms
step:622/1480 train_time:92709ms step_avg:151.49ms
step:623/1480 train_time:92870ms step_avg:151.50ms
step:624/1480 train_time:93030ms step_avg:151.51ms
step:625/1480 train_time:93190ms step_avg:151.53ms
step:625/1480 val_loss:3.6060 train_time:93253ms step_avg:151.63ms
step:626/1480 train_time:93353ms step_avg:151.55ms
step:627/1480 train_time:93514ms step_avg:151.56ms
step:628/1480 train_time:93672ms step_avg:151.57ms
step:629/1480 train_time:93830ms step_avg:151.58ms
step:630/1480 train_time:93987ms step_avg:151.59ms
step:631/1480 train_time:94145ms step_avg:151.60ms
step:632/1480 train_time:94302ms step_avg:151.61ms
step:633/1480 train_time:94463ms step_avg:151.63ms
step:634/1480 train_time:94623ms step_avg:151.64ms
step:635/1480 train_time:94781ms step_avg:151.65ms
step:636/1480 train_time:94939ms step_avg:151.66ms
step:637/1480 train_time:95100ms step_avg:151.67ms
step:638/1480 train_time:95260ms step_avg:151.69ms
step:639/1480 train_time:95419ms step_avg:151.70ms
step:640/1480 train_time:95579ms step_avg:151.71ms
step:641/1480 train_time:95740ms step_avg:151.73ms
step:642/1480 train_time:95899ms step_avg:151.74ms
step:643/1480 train_time:96061ms step_avg:151.75ms
step:644/1480 train_time:96220ms step_avg:151.77ms
step:645/1480 train_time:96379ms step_avg:151.78ms
step:646/1480 train_time:96539ms step_avg:151.79ms
step:647/1480 train_time:96700ms step_avg:151.81ms
step:648/1480 train_time:96862ms step_avg:151.82ms
step:649/1480 train_time:97022ms step_avg:151.83ms
step:650/1480 train_time:97181ms step_avg:151.84ms
step:651/1480 train_time:97341ms step_avg:151.86ms
step:652/1480 train_time:97501ms step_avg:151.87ms
step:653/1480 train_time:97660ms step_avg:151.88ms
step:654/1480 train_time:97821ms step_avg:151.90ms
step:655/1480 train_time:97980ms step_avg:151.91ms
step:656/1480 train_time:98140ms step_avg:151.92ms
step:657/1480 train_time:98300ms step_avg:151.93ms
step:658/1480 train_time:98461ms step_avg:151.95ms
step:659/1480 train_time:98622ms step_avg:151.96ms
step:660/1480 train_time:98783ms step_avg:151.97ms
step:661/1480 train_time:98946ms step_avg:151.99ms
step:662/1480 train_time:99106ms step_avg:152.00ms
step:663/1480 train_time:99265ms step_avg:152.01ms
step:664/1480 train_time:99426ms step_avg:152.03ms
step:665/1480 train_time:99589ms step_avg:152.04ms
step:666/1480 train_time:99749ms step_avg:152.06ms
step:667/1480 train_time:99910ms step_avg:152.07ms
step:668/1480 train_time:100073ms step_avg:152.09ms
step:669/1480 train_time:100236ms step_avg:152.10ms
step:670/1480 train_time:100397ms step_avg:152.12ms
step:671/1480 train_time:100560ms step_avg:152.13ms
step:672/1480 train_time:100723ms step_avg:152.15ms
step:673/1480 train_time:100885ms step_avg:152.16ms
step:674/1480 train_time:101047ms step_avg:152.18ms
step:675/1480 train_time:101208ms step_avg:152.19ms
step:676/1480 train_time:101370ms step_avg:152.21ms
step:677/1480 train_time:101530ms step_avg:152.22ms
step:678/1480 train_time:101690ms step_avg:152.23ms
step:679/1480 train_time:101851ms step_avg:152.24ms
step:680/1480 train_time:102011ms step_avg:152.26ms
step:681/1480 train_time:102171ms step_avg:152.27ms
step:682/1480 train_time:102334ms step_avg:152.28ms
step:683/1480 train_time:102497ms step_avg:152.30ms
step:684/1480 train_time:102660ms step_avg:152.31ms
step:685/1480 train_time:102824ms step_avg:152.33ms
step:686/1480 train_time:102986ms step_avg:152.35ms
step:687/1480 train_time:103147ms step_avg:152.36ms
step:688/1480 train_time:103310ms step_avg:152.37ms
step:689/1480 train_time:103473ms step_avg:152.39ms
step:690/1480 train_time:103637ms step_avg:152.41ms
step:691/1480 train_time:103799ms step_avg:152.42ms
step:692/1480 train_time:103963ms step_avg:152.44ms
step:693/1480 train_time:104124ms step_avg:152.45ms
step:694/1480 train_time:104285ms step_avg:152.46ms
step:695/1480 train_time:104446ms step_avg:152.48ms
step:696/1480 train_time:104606ms step_avg:152.49ms
step:697/1480 train_time:104769ms step_avg:152.50ms
step:698/1480 train_time:104929ms step_avg:152.51ms
step:699/1480 train_time:105092ms step_avg:152.53ms
step:700/1480 train_time:105255ms step_avg:152.54ms
step:701/1480 train_time:105416ms step_avg:152.56ms
step:702/1480 train_time:105575ms step_avg:152.57ms
step:703/1480 train_time:105736ms step_avg:152.58ms
step:704/1480 train_time:105897ms step_avg:152.59ms
step:705/1480 train_time:106061ms step_avg:152.61ms
step:706/1480 train_time:106225ms step_avg:152.62ms
step:707/1480 train_time:106386ms step_avg:152.63ms
step:708/1480 train_time:106545ms step_avg:152.64ms
step:709/1480 train_time:106708ms step_avg:152.66ms
step:710/1480 train_time:106867ms step_avg:152.67ms
step:711/1480 train_time:107030ms step_avg:152.68ms
step:712/1480 train_time:107196ms step_avg:152.70ms
step:713/1480 train_time:107359ms step_avg:152.72ms
step:714/1480 train_time:107522ms step_avg:152.73ms
step:715/1480 train_time:107683ms step_avg:152.74ms
step:716/1480 train_time:107843ms step_avg:152.75ms
step:717/1480 train_time:108005ms step_avg:152.77ms
step:718/1480 train_time:108164ms step_avg:152.77ms
step:719/1480 train_time:108324ms step_avg:152.78ms
step:720/1480 train_time:108485ms step_avg:152.80ms
step:721/1480 train_time:108646ms step_avg:152.81ms
step:722/1480 train_time:108807ms step_avg:152.82ms
step:723/1480 train_time:108967ms step_avg:152.83ms
step:724/1480 train_time:109129ms step_avg:152.84ms
step:725/1480 train_time:109291ms step_avg:152.85ms
step:726/1480 train_time:109455ms step_avg:152.87ms
step:727/1480 train_time:109618ms step_avg:152.88ms
step:728/1480 train_time:109779ms step_avg:152.90ms
step:729/1480 train_time:109942ms step_avg:152.91ms
step:730/1480 train_time:110104ms step_avg:152.92ms
step:731/1480 train_time:110265ms step_avg:152.93ms
step:732/1480 train_time:110425ms step_avg:152.94ms
step:733/1480 train_time:110586ms step_avg:152.95ms
step:734/1480 train_time:110748ms step_avg:152.97ms
step:735/1480 train_time:110909ms step_avg:152.98ms
step:736/1480 train_time:111071ms step_avg:152.99ms
step:737/1480 train_time:111233ms step_avg:153.00ms
step:738/1480 train_time:111395ms step_avg:153.02ms
step:739/1480 train_time:111556ms step_avg:153.03ms
step:740/1480 train_time:111722ms step_avg:153.04ms
step:741/1480 train_time:111886ms step_avg:153.06ms
step:742/1480 train_time:112047ms step_avg:153.07ms
step:743/1480 train_time:112209ms step_avg:153.08ms
step:744/1480 train_time:112370ms step_avg:153.09ms
step:745/1480 train_time:112536ms step_avg:153.11ms
step:746/1480 train_time:112698ms step_avg:153.12ms
step:747/1480 train_time:112860ms step_avg:153.13ms
step:748/1480 train_time:113026ms step_avg:153.15ms
step:749/1480 train_time:113189ms step_avg:153.17ms
step:750/1480 train_time:113348ms step_avg:153.17ms
step:750/1480 val_loss:3.5515 train_time:113413ms step_avg:153.26ms
step:751/1480 train_time:113515ms step_avg:153.19ms
step:752/1480 train_time:113679ms step_avg:153.21ms
step:753/1480 train_time:113840ms step_avg:153.22ms
step:754/1480 train_time:114001ms step_avg:153.23ms
step:755/1480 train_time:114162ms step_avg:153.24ms
step:756/1480 train_time:114325ms step_avg:153.25ms
step:757/1480 train_time:114488ms step_avg:153.26ms
step:758/1480 train_time:114648ms step_avg:153.27ms
step:759/1480 train_time:114809ms step_avg:153.28ms
step:760/1480 train_time:114969ms step_avg:153.29ms
step:761/1480 train_time:115131ms step_avg:153.30ms
step:762/1480 train_time:115291ms step_avg:153.31ms
step:763/1480 train_time:115451ms step_avg:153.32ms
step:764/1480 train_time:115612ms step_avg:153.33ms
step:765/1480 train_time:115773ms step_avg:153.34ms
step:766/1480 train_time:115937ms step_avg:153.36ms
step:767/1480 train_time:116101ms step_avg:153.37ms
step:768/1480 train_time:116264ms step_avg:153.38ms
step:769/1480 train_time:116427ms step_avg:153.40ms
step:770/1480 train_time:116589ms step_avg:153.41ms
step:771/1480 train_time:116754ms step_avg:153.42ms
step:772/1480 train_time:116915ms step_avg:153.43ms
step:773/1480 train_time:117078ms step_avg:153.44ms
step:774/1480 train_time:117241ms step_avg:153.46ms
step:775/1480 train_time:117405ms step_avg:153.47ms
step:776/1480 train_time:117569ms step_avg:153.48ms
step:777/1480 train_time:117734ms step_avg:153.50ms
step:778/1480 train_time:117898ms step_avg:153.51ms
step:779/1480 train_time:118061ms step_avg:153.53ms
step:780/1480 train_time:118225ms step_avg:153.54ms
step:781/1480 train_time:118387ms step_avg:153.55ms
step:782/1480 train_time:118550ms step_avg:153.56ms
step:783/1480 train_time:118711ms step_avg:153.57ms
step:784/1480 train_time:118874ms step_avg:153.58ms
step:785/1480 train_time:119036ms step_avg:153.60ms
step:786/1480 train_time:119204ms step_avg:153.61ms
step:787/1480 train_time:119367ms step_avg:153.63ms
step:788/1480 train_time:119531ms step_avg:153.64ms
step:789/1480 train_time:119693ms step_avg:153.65ms
step:790/1480 train_time:119858ms step_avg:153.66ms
step:791/1480 train_time:120026ms step_avg:153.68ms
step:792/1480 train_time:120189ms step_avg:153.69ms
step:793/1480 train_time:120350ms step_avg:153.70ms
step:794/1480 train_time:120515ms step_avg:153.72ms
step:795/1480 train_time:120682ms step_avg:153.74ms
step:796/1480 train_time:120849ms step_avg:153.75ms
step:797/1480 train_time:121011ms step_avg:153.76ms
step:798/1480 train_time:121175ms step_avg:153.77ms
step:799/1480 train_time:121340ms step_avg:153.79ms
step:800/1480 train_time:121504ms step_avg:153.80ms
step:801/1480 train_time:121667ms step_avg:153.81ms
step:802/1480 train_time:121832ms step_avg:153.83ms
step:803/1480 train_time:121994ms step_avg:153.84ms
step:804/1480 train_time:122157ms step_avg:153.85ms
step:805/1480 train_time:122321ms step_avg:153.86ms
step:806/1480 train_time:122483ms step_avg:153.87ms
step:807/1480 train_time:122644ms step_avg:153.88ms
step:808/1480 train_time:122808ms step_avg:153.89ms
step:809/1480 train_time:122970ms step_avg:153.91ms
step:810/1480 train_time:123132ms step_avg:153.92ms
step:811/1480 train_time:123293ms step_avg:153.92ms
step:812/1480 train_time:123458ms step_avg:153.94ms
step:813/1480 train_time:123620ms step_avg:153.95ms
step:814/1480 train_time:123784ms step_avg:153.96ms
step:815/1480 train_time:123947ms step_avg:153.97ms
step:816/1480 train_time:124112ms step_avg:153.99ms
step:817/1480 train_time:124273ms step_avg:153.99ms
step:818/1480 train_time:124435ms step_avg:154.00ms
step:819/1480 train_time:124599ms step_avg:154.02ms
step:820/1480 train_time:124762ms step_avg:154.03ms
step:821/1480 train_time:124925ms step_avg:154.04ms
step:822/1480 train_time:125088ms step_avg:154.05ms
step:823/1480 train_time:125250ms step_avg:154.06ms
step:824/1480 train_time:125414ms step_avg:154.07ms
step:825/1480 train_time:125577ms step_avg:154.08ms
step:826/1480 train_time:125745ms step_avg:154.10ms
step:827/1480 train_time:125909ms step_avg:154.11ms
step:828/1480 train_time:126072ms step_avg:154.12ms
step:829/1480 train_time:126235ms step_avg:154.13ms
step:830/1480 train_time:126399ms step_avg:154.15ms
step:831/1480 train_time:126564ms step_avg:154.16ms
step:832/1480 train_time:126727ms step_avg:154.17ms
step:833/1480 train_time:126891ms step_avg:154.18ms
step:834/1480 train_time:127054ms step_avg:154.19ms
step:835/1480 train_time:127218ms step_avg:154.20ms
step:836/1480 train_time:127384ms step_avg:154.22ms
step:837/1480 train_time:127547ms step_avg:154.23ms
step:838/1480 train_time:127710ms step_avg:154.24ms
step:839/1480 train_time:127872ms step_avg:154.25ms
step:840/1480 train_time:128033ms step_avg:154.26ms
step:841/1480 train_time:128193ms step_avg:154.26ms
step:842/1480 train_time:128358ms step_avg:154.28ms
step:843/1480 train_time:128522ms step_avg:154.29ms
step:844/1480 train_time:128685ms step_avg:154.30ms
step:845/1480 train_time:128848ms step_avg:154.31ms
step:846/1480 train_time:129013ms step_avg:154.32ms
step:847/1480 train_time:129176ms step_avg:154.33ms
step:848/1480 train_time:129336ms step_avg:154.34ms
step:849/1480 train_time:129500ms step_avg:154.35ms
step:850/1480 train_time:129662ms step_avg:154.36ms
step:851/1480 train_time:129827ms step_avg:154.37ms
step:852/1480 train_time:129988ms step_avg:154.38ms
step:853/1480 train_time:130150ms step_avg:154.39ms
step:854/1480 train_time:130315ms step_avg:154.40ms
step:855/1480 train_time:130479ms step_avg:154.41ms
step:856/1480 train_time:130642ms step_avg:154.42ms
step:857/1480 train_time:130807ms step_avg:154.44ms
step:858/1480 train_time:130973ms step_avg:154.45ms
step:859/1480 train_time:131136ms step_avg:154.46ms
step:860/1480 train_time:131298ms step_avg:154.47ms
step:861/1480 train_time:131464ms step_avg:154.48ms
step:862/1480 train_time:131633ms step_avg:154.50ms
step:863/1480 train_time:131803ms step_avg:154.52ms
step:864/1480 train_time:131967ms step_avg:154.53ms
step:865/1480 train_time:132128ms step_avg:154.54ms
step:866/1480 train_time:132294ms step_avg:154.55ms
step:867/1480 train_time:132459ms step_avg:154.56ms
step:868/1480 train_time:132621ms step_avg:154.57ms
step:869/1480 train_time:132784ms step_avg:154.58ms
step:870/1480 train_time:132947ms step_avg:154.59ms
step:871/1480 train_time:133110ms step_avg:154.60ms
step:872/1480 train_time:133272ms step_avg:154.61ms
step:873/1480 train_time:133435ms step_avg:154.62ms
step:874/1480 train_time:133604ms step_avg:154.63ms
step:875/1480 train_time:133769ms step_avg:154.65ms
step:875/1480 val_loss:3.5075 train_time:133833ms step_avg:154.72ms
step:876/1480 train_time:133935ms step_avg:154.66ms
step:877/1480 train_time:134099ms step_avg:154.67ms
step:878/1480 train_time:134262ms step_avg:154.68ms
step:879/1480 train_time:134425ms step_avg:154.69ms
step:880/1480 train_time:134590ms step_avg:154.70ms
step:881/1480 train_time:134753ms step_avg:154.71ms
step:882/1480 train_time:134918ms step_avg:154.72ms
step:883/1480 train_time:135083ms step_avg:154.73ms
step:884/1480 train_time:135251ms step_avg:154.75ms
step:885/1480 train_time:135416ms step_avg:154.76ms
step:886/1480 train_time:135582ms step_avg:154.77ms
step:887/1480 train_time:135750ms step_avg:154.79ms
step:888/1480 train_time:135923ms step_avg:154.81ms
step:889/1480 train_time:136090ms step_avg:154.82ms
step:890/1480 train_time:136252ms step_avg:154.83ms
step:891/1480 train_time:136418ms step_avg:154.84ms
step:892/1480 train_time:136584ms step_avg:154.86ms
step:893/1480 train_time:136746ms step_avg:154.87ms
step:894/1480 train_time:136914ms step_avg:154.88ms
step:895/1480 train_time:137079ms step_avg:154.89ms
step:896/1480 train_time:137243ms step_avg:154.90ms
step:897/1480 train_time:137408ms step_avg:154.91ms
step:898/1480 train_time:137575ms step_avg:154.93ms
step:899/1480 train_time:137739ms step_avg:154.94ms
step:900/1480 train_time:137902ms step_avg:154.95ms
step:901/1480 train_time:138066ms step_avg:154.96ms
step:902/1480 train_time:138231ms step_avg:154.97ms
step:903/1480 train_time:138404ms step_avg:154.99ms
step:904/1480 train_time:138570ms step_avg:155.00ms
step:905/1480 train_time:138733ms step_avg:155.01ms
step:906/1480 train_time:138900ms step_avg:155.02ms
step:907/1480 train_time:139069ms step_avg:155.04ms
step:908/1480 train_time:139233ms step_avg:155.05ms
step:909/1480 train_time:139399ms step_avg:155.06ms
step:910/1480 train_time:139572ms step_avg:155.08ms
step:911/1480 train_time:139737ms step_avg:155.09ms
step:912/1480 train_time:139903ms step_avg:155.10ms
step:913/1480 train_time:140071ms step_avg:155.12ms
step:914/1480 train_time:140239ms step_avg:155.13ms
step:915/1480 train_time:140409ms step_avg:155.15ms
step:916/1480 train_time:140574ms step_avg:155.16ms
step:917/1480 train_time:140737ms step_avg:155.17ms
step:918/1480 train_time:140904ms step_avg:155.18ms
step:919/1480 train_time:141075ms step_avg:155.20ms
step:920/1480 train_time:141241ms step_avg:155.21ms
step:921/1480 train_time:141408ms step_avg:155.22ms
step:922/1480 train_time:141576ms step_avg:155.24ms
step:923/1480 train_time:141738ms step_avg:155.24ms
step:924/1480 train_time:141902ms step_avg:155.25ms
step:925/1480 train_time:142069ms step_avg:155.27ms
step:926/1480 train_time:142233ms step_avg:155.28ms
step:927/1480 train_time:142396ms step_avg:155.29ms
step:928/1480 train_time:142562ms step_avg:155.30ms
step:929/1480 train_time:142728ms step_avg:155.31ms
step:930/1480 train_time:142895ms step_avg:155.32ms
step:931/1480 train_time:143059ms step_avg:155.33ms
step:932/1480 train_time:143225ms step_avg:155.34ms
step:933/1480 train_time:143393ms step_avg:155.36ms
step:934/1480 train_time:143560ms step_avg:155.37ms
step:935/1480 train_time:143730ms step_avg:155.38ms
step:936/1480 train_time:143898ms step_avg:155.40ms
step:937/1480 train_time:144068ms step_avg:155.41ms
step:938/1480 train_time:144232ms step_avg:155.42ms
step:939/1480 train_time:144400ms step_avg:155.44ms
step:940/1480 train_time:144567ms step_avg:155.45ms
step:941/1480 train_time:144730ms step_avg:155.46ms
step:942/1480 train_time:144895ms step_avg:155.47ms
step:943/1480 train_time:145064ms step_avg:155.48ms
step:944/1480 train_time:145237ms step_avg:155.50ms
step:945/1480 train_time:145400ms step_avg:155.51ms
step:946/1480 train_time:145569ms step_avg:155.52ms
step:947/1480 train_time:145737ms step_avg:155.54ms
step:948/1480 train_time:145901ms step_avg:155.55ms
step:949/1480 train_time:146067ms step_avg:155.56ms
step:950/1480 train_time:146231ms step_avg:155.57ms
step:951/1480 train_time:146400ms step_avg:155.58ms
step:952/1480 train_time:146565ms step_avg:155.59ms
step:953/1480 train_time:146731ms step_avg:155.60ms
step:954/1480 train_time:146899ms step_avg:155.61ms
step:955/1480 train_time:147063ms step_avg:155.62ms
step:956/1480 train_time:147227ms step_avg:155.63ms
step:957/1480 train_time:147396ms step_avg:155.65ms
step:958/1480 train_time:147567ms step_avg:155.66ms
step:959/1480 train_time:147733ms step_avg:155.67ms
step:960/1480 train_time:147900ms step_avg:155.68ms
step:961/1480 train_time:148063ms step_avg:155.69ms
step:962/1480 train_time:148229ms step_avg:155.70ms
step:963/1480 train_time:148395ms step_avg:155.71ms
step:964/1480 train_time:148563ms step_avg:155.73ms
step:965/1480 train_time:148727ms step_avg:155.74ms
step:966/1480 train_time:148893ms step_avg:155.75ms
step:967/1480 train_time:149057ms step_avg:155.75ms
step:968/1480 train_time:149221ms step_avg:155.76ms
step:969/1480 train_time:149388ms step_avg:155.77ms
step:970/1480 train_time:149554ms step_avg:155.78ms
step:971/1480 train_time:149717ms step_avg:155.79ms
step:972/1480 train_time:149880ms step_avg:155.80ms
step:973/1480 train_time:150045ms step_avg:155.81ms
step:974/1480 train_time:150214ms step_avg:155.82ms
step:975/1480 train_time:150381ms step_avg:155.83ms
step:976/1480 train_time:150544ms step_avg:155.84ms
step:977/1480 train_time:150708ms step_avg:155.85ms
step:978/1480 train_time:150875ms step_avg:155.86ms
step:979/1480 train_time:151042ms step_avg:155.87ms
step:980/1480 train_time:151208ms step_avg:155.88ms
step:981/1480 train_time:151377ms step_avg:155.90ms
step:982/1480 train_time:151540ms step_avg:155.91ms
step:983/1480 train_time:151705ms step_avg:155.91ms
step:984/1480 train_time:151870ms step_avg:155.92ms
step:985/1480 train_time:152037ms step_avg:155.94ms
step:986/1480 train_time:152202ms step_avg:155.94ms
step:987/1480 train_time:152366ms step_avg:155.95ms
step:988/1480 train_time:152534ms step_avg:155.97ms
step:989/1480 train_time:152699ms step_avg:155.97ms
step:990/1480 train_time:152869ms step_avg:155.99ms
step:991/1480 train_time:153037ms step_avg:156.00ms
step:992/1480 train_time:153211ms step_avg:156.02ms
step:993/1480 train_time:153387ms step_avg:156.04ms
step:994/1480 train_time:153552ms step_avg:156.05ms
step:995/1480 train_time:153717ms step_avg:156.06ms
step:996/1480 train_time:153879ms step_avg:156.06ms
step:997/1480 train_time:154045ms step_avg:156.07ms
step:998/1480 train_time:154209ms step_avg:156.08ms
step:999/1480 train_time:154375ms step_avg:156.09ms
step:1000/1480 train_time:154543ms step_avg:156.10ms
step:1000/1480 val_loss:3.4429 train_time:154611ms step_avg:156.17ms
step:1001/1480 train_time:154713ms step_avg:156.12ms
step:1002/1480 train_time:154879ms step_avg:156.13ms
step:1003/1480 train_time:155051ms step_avg:156.14ms
step:1004/1480 train_time:155218ms step_avg:156.16ms
step:1005/1480 train_time:155387ms step_avg:156.17ms
step:1006/1480 train_time:155555ms step_avg:156.18ms
step:1007/1480 train_time:155720ms step_avg:156.19ms
step:1008/1480 train_time:155888ms step_avg:156.20ms
step:1009/1480 train_time:156061ms step_avg:156.22ms
step:1010/1480 train_time:156226ms step_avg:156.23ms
step:1011/1480 train_time:156391ms step_avg:156.23ms
step:1012/1480 train_time:156555ms step_avg:156.24ms
step:1013/1480 train_time:156726ms step_avg:156.26ms
step:1014/1480 train_time:156894ms step_avg:156.27ms
step:1015/1480 train_time:157063ms step_avg:156.28ms
step:1016/1480 train_time:157233ms step_avg:156.29ms
step:1017/1480 train_time:157403ms step_avg:156.31ms
step:1018/1480 train_time:157572ms step_avg:156.32ms
step:1019/1480 train_time:157740ms step_avg:156.33ms
step:1020/1480 train_time:157910ms step_avg:156.35ms
step:1021/1480 train_time:158075ms step_avg:156.35ms
step:1022/1480 train_time:158241ms step_avg:156.36ms
step:1023/1480 train_time:158410ms step_avg:156.38ms
step:1024/1480 train_time:158576ms step_avg:156.39ms
step:1025/1480 train_time:158747ms step_avg:156.40ms
step:1026/1480 train_time:158913ms step_avg:156.41ms
step:1027/1480 train_time:159078ms step_avg:156.42ms
step:1028/1480 train_time:159252ms step_avg:156.44ms
step:1029/1480 train_time:159426ms step_avg:156.45ms
step:1030/1480 train_time:159593ms step_avg:156.46ms
step:1031/1480 train_time:159757ms step_avg:156.47ms
step:1032/1480 train_time:159930ms step_avg:156.49ms
step:1033/1480 train_time:160096ms step_avg:156.50ms
step:1034/1480 train_time:160263ms step_avg:156.51ms
step:1035/1480 train_time:160430ms step_avg:156.52ms
step:1036/1480 train_time:160596ms step_avg:156.53ms
step:1037/1480 train_time:160764ms step_avg:156.54ms
step:1038/1480 train_time:160932ms step_avg:156.55ms
step:1039/1480 train_time:161104ms step_avg:156.56ms
step:1040/1480 train_time:161271ms step_avg:156.57ms
step:1041/1480 train_time:161437ms step_avg:156.58ms
step:1042/1480 train_time:161600ms step_avg:156.59ms
step:1043/1480 train_time:161766ms step_avg:156.60ms
step:1044/1480 train_time:161931ms step_avg:156.61ms
step:1045/1480 train_time:162099ms step_avg:156.62ms
step:1046/1480 train_time:162269ms step_avg:156.63ms
step:1047/1480 train_time:162435ms step_avg:156.64ms
step:1048/1480 train_time:162602ms step_avg:156.65ms
step:1049/1480 train_time:162769ms step_avg:156.66ms
step:1050/1480 train_time:162937ms step_avg:156.67ms
step:1051/1480 train_time:163108ms step_avg:156.68ms
step:1052/1480 train_time:163275ms step_avg:156.69ms
step:1053/1480 train_time:163443ms step_avg:156.70ms
step:1054/1480 train_time:163612ms step_avg:156.72ms
step:1055/1480 train_time:163777ms step_avg:156.72ms
step:1056/1480 train_time:163940ms step_avg:156.73ms
step:1057/1480 train_time:164108ms step_avg:156.74ms
step:1058/1480 train_time:164276ms step_avg:156.75ms
step:1059/1480 train_time:164447ms step_avg:156.77ms
step:1060/1480 train_time:164616ms step_avg:156.78ms
step:1061/1480 train_time:164780ms step_avg:156.78ms
step:1062/1480 train_time:164947ms step_avg:156.79ms
step:1063/1480 train_time:165113ms step_avg:156.80ms
step:1064/1480 train_time:165276ms step_avg:156.81ms
step:1065/1480 train_time:165444ms step_avg:156.82ms
step:1066/1480 train_time:165612ms step_avg:156.83ms
step:1067/1480 train_time:165780ms step_avg:156.84ms
step:1068/1480 train_time:165945ms step_avg:156.85ms
step:1069/1480 train_time:166115ms step_avg:156.86ms
step:1070/1480 train_time:166281ms step_avg:156.87ms
step:1071/1480 train_time:166453ms step_avg:156.88ms
step:1072/1480 train_time:166619ms step_avg:156.89ms
step:1073/1480 train_time:166783ms step_avg:156.90ms
step:1074/1480 train_time:166950ms step_avg:156.91ms
step:1075/1480 train_time:167120ms step_avg:156.92ms
step:1076/1480 train_time:167288ms step_avg:156.93ms
step:1077/1480 train_time:167454ms step_avg:156.94ms
step:1078/1480 train_time:167628ms step_avg:156.96ms
step:1079/1480 train_time:167801ms step_avg:156.97ms
step:1080/1480 train_time:167972ms step_avg:156.98ms
step:1081/1480 train_time:168138ms step_avg:156.99ms
step:1082/1480 train_time:168305ms step_avg:157.00ms
step:1083/1480 train_time:168472ms step_avg:157.01ms
step:1084/1480 train_time:168638ms step_avg:157.02ms
step:1085/1480 train_time:168809ms step_avg:157.03ms
step:1086/1480 train_time:168977ms step_avg:157.04ms
step:1087/1480 train_time:169143ms step_avg:157.05ms
step:1088/1480 train_time:169313ms step_avg:157.06ms
step:1089/1480 train_time:169485ms step_avg:157.08ms
step:1090/1480 train_time:169655ms step_avg:157.09ms
step:1091/1480 train_time:169823ms step_avg:157.10ms
step:1092/1480 train_time:169991ms step_avg:157.11ms
step:1093/1480 train_time:170158ms step_avg:157.12ms
step:1094/1480 train_time:170324ms step_avg:157.13ms
step:1095/1480 train_time:170488ms step_avg:157.13ms
step:1096/1480 train_time:170656ms step_avg:157.14ms
step:1097/1480 train_time:170825ms step_avg:157.15ms
step:1098/1480 train_time:170995ms step_avg:157.16ms
step:1099/1480 train_time:171166ms step_avg:157.18ms
step:1100/1480 train_time:171338ms step_avg:157.19ms
step:1101/1480 train_time:171509ms step_avg:157.20ms
step:1102/1480 train_time:171679ms step_avg:157.22ms
step:1103/1480 train_time:171855ms step_avg:157.23ms
step:1104/1480 train_time:172022ms step_avg:157.24ms
step:1105/1480 train_time:172194ms step_avg:157.25ms
step:1106/1480 train_time:172361ms step_avg:157.26ms
step:1107/1480 train_time:172531ms step_avg:157.28ms
step:1108/1480 train_time:172696ms step_avg:157.28ms
step:1109/1480 train_time:172861ms step_avg:157.29ms
step:1110/1480 train_time:173028ms step_avg:157.30ms
step:1111/1480 train_time:173194ms step_avg:157.31ms
step:1112/1480 train_time:173364ms step_avg:157.32ms
step:1113/1480 train_time:173543ms step_avg:157.34ms
step:1114/1480 train_time:173716ms step_avg:157.35ms
step:1115/1480 train_time:173891ms step_avg:157.37ms
step:1116/1480 train_time:174058ms step_avg:157.38ms
step:1117/1480 train_time:174231ms step_avg:157.39ms
step:1118/1480 train_time:174405ms step_avg:157.40ms
step:1119/1480 train_time:174570ms step_avg:157.41ms
step:1120/1480 train_time:174738ms step_avg:157.42ms
step:1121/1480 train_time:174909ms step_avg:157.43ms
step:1122/1480 train_time:175075ms step_avg:157.44ms
step:1123/1480 train_time:175241ms step_avg:157.45ms
step:1124/1480 train_time:175411ms step_avg:157.46ms
step:1125/1480 train_time:175578ms step_avg:157.47ms
step:1125/1480 val_loss:3.3871 train_time:175647ms step_avg:157.53ms
step:1126/1480 train_time:175748ms step_avg:157.48ms
step:1127/1480 train_time:175919ms step_avg:157.49ms
step:1128/1480 train_time:176090ms step_avg:157.50ms
step:1129/1480 train_time:176263ms step_avg:157.52ms
step:1130/1480 train_time:176432ms step_avg:157.53ms
step:1131/1480 train_time:176609ms step_avg:157.55ms
step:1132/1480 train_time:176774ms step_avg:157.55ms
step:1133/1480 train_time:176947ms step_avg:157.57ms
step:1134/1480 train_time:177118ms step_avg:157.58ms
step:1135/1480 train_time:177285ms step_avg:157.59ms
step:1136/1480 train_time:177456ms step_avg:157.60ms
step:1137/1480 train_time:177627ms step_avg:157.61ms
step:1138/1480 train_time:177800ms step_avg:157.62ms
step:1139/1480 train_time:177968ms step_avg:157.63ms
step:1140/1480 train_time:178138ms step_avg:157.64ms
step:1141/1480 train_time:178310ms step_avg:157.66ms
step:1142/1480 train_time:178478ms step_avg:157.67ms
step:1143/1480 train_time:178647ms step_avg:157.68ms
step:1144/1480 train_time:178816ms step_avg:157.69ms
step:1145/1480 train_time:178982ms step_avg:157.69ms
step:1146/1480 train_time:179152ms step_avg:157.70ms
step:1147/1480 train_time:179321ms step_avg:157.71ms
step:1148/1480 train_time:179488ms step_avg:157.72ms
step:1149/1480 train_time:179657ms step_avg:157.73ms
step:1150/1480 train_time:179827ms step_avg:157.74ms
step:1151/1480 train_time:180001ms step_avg:157.76ms
step:1152/1480 train_time:180171ms step_avg:157.77ms
step:1153/1480 train_time:180345ms step_avg:157.78ms
step:1154/1480 train_time:180511ms step_avg:157.79ms
step:1155/1480 train_time:180683ms step_avg:157.80ms
step:1156/1480 train_time:180861ms step_avg:157.82ms
step:1157/1480 train_time:181030ms step_avg:157.83ms
step:1158/1480 train_time:181197ms step_avg:157.84ms
step:1159/1480 train_time:181364ms step_avg:157.85ms
step:1160/1480 train_time:181529ms step_avg:157.85ms
step:1161/1480 train_time:181700ms step_avg:157.86ms
step:1162/1480 train_time:181870ms step_avg:157.87ms
step:1163/1480 train_time:182040ms step_avg:157.88ms
step:1164/1480 train_time:182209ms step_avg:157.89ms
step:1165/1480 train_time:182374ms step_avg:157.90ms
step:1166/1480 train_time:182544ms step_avg:157.91ms
step:1167/1480 train_time:182711ms step_avg:157.92ms
step:1168/1480 train_time:182879ms step_avg:157.93ms
step:1169/1480 train_time:183048ms step_avg:157.94ms
step:1170/1480 train_time:183218ms step_avg:157.95ms
step:1171/1480 train_time:183385ms step_avg:157.95ms
step:1172/1480 train_time:183552ms step_avg:157.96ms
step:1173/1480 train_time:183723ms step_avg:157.97ms
step:1174/1480 train_time:183904ms step_avg:157.99ms
step:1175/1480 train_time:184075ms step_avg:158.00ms
step:1176/1480 train_time:184246ms step_avg:158.02ms
step:1177/1480 train_time:184423ms step_avg:158.03ms
step:1178/1480 train_time:184589ms step_avg:158.04ms
step:1179/1480 train_time:184753ms step_avg:158.04ms
step:1180/1480 train_time:184934ms step_avg:158.06ms
step:1181/1480 train_time:185105ms step_avg:158.07ms
step:1182/1480 train_time:185273ms step_avg:158.08ms
step:1183/1480 train_time:185444ms step_avg:158.09ms
step:1184/1480 train_time:185613ms step_avg:158.10ms
step:1185/1480 train_time:185785ms step_avg:158.11ms
step:1186/1480 train_time:185955ms step_avg:158.13ms
step:1187/1480 train_time:186140ms step_avg:158.15ms
step:1188/1480 train_time:186308ms step_avg:158.16ms
step:1189/1480 train_time:186480ms step_avg:158.17ms
step:1190/1480 train_time:186647ms step_avg:158.18ms
step:1191/1480 train_time:186819ms step_avg:158.19ms
step:1192/1480 train_time:186985ms step_avg:158.19ms
step:1193/1480 train_time:187152ms step_avg:158.20ms
step:1194/1480 train_time:187323ms step_avg:158.21ms
step:1195/1480 train_time:187496ms step_avg:158.22ms
step:1196/1480 train_time:187679ms step_avg:158.25ms
step:1197/1480 train_time:187850ms step_avg:158.26ms
step:1198/1480 train_time:188032ms step_avg:158.28ms
step:1199/1480 train_time:188203ms step_avg:158.29ms
step:1200/1480 train_time:188373ms step_avg:158.30ms
step:1201/1480 train_time:188541ms step_avg:158.30ms
step:1202/1480 train_time:188722ms step_avg:158.32ms
step:1203/1480 train_time:188899ms step_avg:158.34ms
step:1204/1480 train_time:189075ms step_avg:158.35ms
step:1205/1480 train_time:189243ms step_avg:158.36ms
step:1206/1480 train_time:189412ms step_avg:158.37ms
step:1207/1480 train_time:189583ms step_avg:158.38ms
step:1208/1480 train_time:189750ms step_avg:158.39ms
step:1209/1480 train_time:189924ms step_avg:158.40ms
step:1210/1480 train_time:190099ms step_avg:158.42ms
step:1211/1480 train_time:190275ms step_avg:158.43ms
step:1212/1480 train_time:190447ms step_avg:158.44ms
step:1213/1480 train_time:190619ms step_avg:158.45ms
step:1214/1480 train_time:190797ms step_avg:158.47ms
step:1215/1480 train_time:190972ms step_avg:158.48ms
step:1216/1480 train_time:191142ms step_avg:158.49ms
step:1217/1480 train_time:191315ms step_avg:158.50ms
step:1218/1480 train_time:191484ms step_avg:158.51ms
step:1219/1480 train_time:191663ms step_avg:158.53ms
step:1220/1480 train_time:191831ms step_avg:158.54ms
step:1221/1480 train_time:192001ms step_avg:158.55ms
step:1222/1480 train_time:192168ms step_avg:158.55ms
step:1223/1480 train_time:192341ms step_avg:158.57ms
step:1224/1480 train_time:192521ms step_avg:158.58ms
step:1225/1480 train_time:192692ms step_avg:158.59ms
step:1226/1480 train_time:192865ms step_avg:158.61ms
step:1227/1480 train_time:193038ms step_avg:158.62ms
step:1228/1480 train_time:193207ms step_avg:158.63ms
step:1229/1480 train_time:193379ms step_avg:158.64ms
step:1230/1480 train_time:193558ms step_avg:158.65ms
step:1231/1480 train_time:193734ms step_avg:158.67ms
step:1232/1480 train_time:193909ms step_avg:158.68ms
step:1233/1480 train_time:194080ms step_avg:158.69ms
step:1234/1480 train_time:194249ms step_avg:158.70ms
step:1235/1480 train_time:194424ms step_avg:158.71ms
step:1236/1480 train_time:194591ms step_avg:158.72ms
step:1237/1480 train_time:194763ms step_avg:158.73ms
step:1238/1480 train_time:194947ms step_avg:158.75ms
step:1239/1480 train_time:195120ms step_avg:158.76ms
step:1240/1480 train_time:195289ms step_avg:158.77ms
step:1241/1480 train_time:195462ms step_avg:158.78ms
step:1242/1480 train_time:195632ms step_avg:158.79ms
step:1243/1480 train_time:195805ms step_avg:158.80ms
step:1244/1480 train_time:195972ms step_avg:158.81ms
step:1245/1480 train_time:196143ms step_avg:158.82ms
step:1246/1480 train_time:196312ms step_avg:158.83ms
step:1247/1480 train_time:196482ms step_avg:158.84ms
step:1248/1480 train_time:196650ms step_avg:158.84ms
step:1249/1480 train_time:196818ms step_avg:158.85ms
step:1250/1480 train_time:196987ms step_avg:158.86ms
step:1250/1480 val_loss:3.3373 train_time:197060ms step_avg:158.92ms
step:1251/1480 train_time:197168ms step_avg:158.88ms
step:1252/1480 train_time:197339ms step_avg:158.89ms
step:1253/1480 train_time:197505ms step_avg:158.89ms
step:1254/1480 train_time:197675ms step_avg:158.90ms
step:1255/1480 train_time:197861ms step_avg:158.92ms
step:1256/1480 train_time:198034ms step_avg:158.94ms
step:1257/1480 train_time:198203ms step_avg:158.94ms
step:1258/1480 train_time:198378ms step_avg:158.96ms
step:1259/1480 train_time:198549ms step_avg:158.97ms
step:1260/1480 train_time:198716ms step_avg:158.97ms
step:1261/1480 train_time:198888ms step_avg:158.98ms
step:1262/1480 train_time:199065ms step_avg:159.00ms
step:1263/1480 train_time:199238ms step_avg:159.01ms
step:1264/1480 train_time:199404ms step_avg:159.01ms
step:1265/1480 train_time:199572ms step_avg:159.02ms
step:1266/1480 train_time:199745ms step_avg:159.03ms
step:1267/1480 train_time:199916ms step_avg:159.04ms
step:1268/1480 train_time:200086ms step_avg:159.05ms
step:1269/1480 train_time:200264ms step_avg:159.07ms
step:1270/1480 train_time:200434ms step_avg:159.07ms
step:1271/1480 train_time:200603ms step_avg:159.08ms
step:1272/1480 train_time:200768ms step_avg:159.09ms
step:1273/1480 train_time:200940ms step_avg:159.10ms
step:1274/1480 train_time:201111ms step_avg:159.11ms
step:1275/1480 train_time:201280ms step_avg:159.11ms
step:1276/1480 train_time:201445ms step_avg:159.12ms
step:1277/1480 train_time:201616ms step_avg:159.13ms
step:1278/1480 train_time:201784ms step_avg:159.14ms
step:1279/1480 train_time:201955ms step_avg:159.15ms
step:1280/1480 train_time:202134ms step_avg:159.16ms
step:1281/1480 train_time:202302ms step_avg:159.17ms
step:1282/1480 train_time:202468ms step_avg:159.17ms
step:1283/1480 train_time:202641ms step_avg:159.18ms
step:1284/1480 train_time:202809ms step_avg:159.19ms
step:1285/1480 train_time:202978ms step_avg:159.20ms
step:1286/1480 train_time:203147ms step_avg:159.21ms
step:1287/1480 train_time:203319ms step_avg:159.22ms
step:1288/1480 train_time:203489ms step_avg:159.22ms
step:1289/1480 train_time:203673ms step_avg:159.24ms
step:1290/1480 train_time:203853ms step_avg:159.26ms
step:1291/1480 train_time:204027ms step_avg:159.27ms
step:1292/1480 train_time:204202ms step_avg:159.28ms
step:1293/1480 train_time:204377ms step_avg:159.30ms
step:1294/1480 train_time:204547ms step_avg:159.30ms
step:1295/1480 train_time:204719ms step_avg:159.31ms
step:1296/1480 train_time:204893ms step_avg:159.33ms
step:1297/1480 train_time:205064ms step_avg:159.34ms
step:1298/1480 train_time:205235ms step_avg:159.34ms
step:1299/1480 train_time:205405ms step_avg:159.35ms
step:1300/1480 train_time:205571ms step_avg:159.36ms
step:1301/1480 train_time:205742ms step_avg:159.37ms
step:1302/1480 train_time:205916ms step_avg:159.38ms
step:1303/1480 train_time:206092ms step_avg:159.39ms
step:1304/1480 train_time:206266ms step_avg:159.40ms
step:1305/1480 train_time:206434ms step_avg:159.41ms
step:1306/1480 train_time:206608ms step_avg:159.42ms
step:1307/1480 train_time:206776ms step_avg:159.43ms
step:1308/1480 train_time:206945ms step_avg:159.43ms
step:1309/1480 train_time:207117ms step_avg:159.44ms
step:1310/1480 train_time:207286ms step_avg:159.45ms
step:1311/1480 train_time:207456ms step_avg:159.46ms
step:1312/1480 train_time:207627ms step_avg:159.47ms
step:1313/1480 train_time:207797ms step_avg:159.48ms
step:1314/1480 train_time:207970ms step_avg:159.49ms
step:1315/1480 train_time:208140ms step_avg:159.49ms
step:1316/1480 train_time:208306ms step_avg:159.50ms
step:1317/1480 train_time:208477ms step_avg:159.51ms
step:1318/1480 train_time:208657ms step_avg:159.52ms
step:1319/1480 train_time:208831ms step_avg:159.54ms
step:1320/1480 train_time:209007ms step_avg:159.55ms
step:1321/1480 train_time:209181ms step_avg:159.56ms
step:1322/1480 train_time:209361ms step_avg:159.57ms
step:1323/1480 train_time:209533ms step_avg:159.58ms
step:1324/1480 train_time:209707ms step_avg:159.59ms
step:1325/1480 train_time:209888ms step_avg:159.61ms
step:1326/1480 train_time:210065ms step_avg:159.62ms
step:1327/1480 train_time:210235ms step_avg:159.63ms
step:1328/1480 train_time:210405ms step_avg:159.64ms
step:1329/1480 train_time:210601ms step_avg:159.67ms
step:1330/1480 train_time:210781ms step_avg:159.68ms
step:1331/1480 train_time:210952ms step_avg:159.69ms
step:1332/1480 train_time:211126ms step_avg:159.70ms
step:1333/1480 train_time:211302ms step_avg:159.71ms
step:1334/1480 train_time:211473ms step_avg:159.72ms
step:1335/1480 train_time:211642ms step_avg:159.73ms
step:1336/1480 train_time:211826ms step_avg:159.75ms
step:1337/1480 train_time:212002ms step_avg:159.76ms
step:1338/1480 train_time:212175ms step_avg:159.77ms
step:1339/1480 train_time:212348ms step_avg:159.78ms
step:1340/1480 train_time:212520ms step_avg:159.79ms
step:1341/1480 train_time:212690ms step_avg:159.80ms
step:1342/1480 train_time:212864ms step_avg:159.81ms
step:1343/1480 train_time:213036ms step_avg:159.82ms
step:1344/1480 train_time:213207ms step_avg:159.83ms
step:1345/1480 train_time:213386ms step_avg:159.84ms
step:1346/1480 train_time:213557ms step_avg:159.85ms
step:1347/1480 train_time:213726ms step_avg:159.85ms
step:1348/1480 train_time:213896ms step_avg:159.86ms
step:1349/1480 train_time:214066ms step_avg:159.87ms
step:1350/1480 train_time:214241ms step_avg:159.88ms
step:1351/1480 train_time:214411ms step_avg:159.89ms
step:1352/1480 train_time:214582ms step_avg:159.90ms
step:1353/1480 train_time:214759ms step_avg:159.91ms
step:1354/1480 train_time:214930ms step_avg:159.92ms
step:1355/1480 train_time:215099ms step_avg:159.92ms
step:1356/1480 train_time:215270ms step_avg:159.93ms
step:1357/1480 train_time:215443ms step_avg:159.94ms
step:1358/1480 train_time:215616ms step_avg:159.95ms
step:1359/1480 train_time:215790ms step_avg:159.96ms
step:1360/1480 train_time:215965ms step_avg:159.97ms
step:1361/1480 train_time:216145ms step_avg:159.99ms
step:1362/1480 train_time:216322ms step_avg:160.00ms
step:1363/1480 train_time:216501ms step_avg:160.02ms
step:1364/1480 train_time:216670ms step_avg:160.02ms
step:1365/1480 train_time:216836ms step_avg:160.03ms
step:1366/1480 train_time:217008ms step_avg:160.04ms
step:1367/1480 train_time:217180ms step_avg:160.04ms
step:1368/1480 train_time:217351ms step_avg:160.05ms
step:1369/1480 train_time:217532ms step_avg:160.07ms
step:1370/1480 train_time:217708ms step_avg:160.08ms
step:1371/1480 train_time:217879ms step_avg:160.09ms
step:1372/1480 train_time:218058ms step_avg:160.10ms
step:1373/1480 train_time:218226ms step_avg:160.11ms
step:1374/1480 train_time:218401ms step_avg:160.12ms
step:1375/1480 train_time:218570ms step_avg:160.12ms
step:1375/1480 val_loss:3.2986 train_time:218638ms step_avg:160.17ms
step:1376/1480 train_time:218744ms step_avg:160.13ms
step:1377/1480 train_time:218916ms step_avg:160.14ms
step:1378/1480 train_time:219084ms step_avg:160.15ms
step:1379/1480 train_time:219262ms step_avg:160.16ms
step:1380/1480 train_time:219436ms step_avg:160.17ms
step:1381/1480 train_time:219621ms step_avg:160.19ms
step:1382/1480 train_time:219792ms step_avg:160.20ms
step:1383/1480 train_time:219964ms step_avg:160.21ms
step:1384/1480 train_time:220142ms step_avg:160.22ms
step:1385/1480 train_time:220309ms step_avg:160.22ms
step:1386/1480 train_time:220480ms step_avg:160.23ms
step:1387/1480 train_time:220650ms step_avg:160.24ms
step:1388/1480 train_time:220820ms step_avg:160.25ms
step:1389/1480 train_time:220992ms step_avg:160.26ms
step:1390/1480 train_time:221161ms step_avg:160.26ms
step:1391/1480 train_time:221331ms step_avg:160.27ms
step:1392/1480 train_time:221503ms step_avg:160.28ms
step:1393/1480 train_time:221674ms step_avg:160.29ms
step:1394/1480 train_time:221845ms step_avg:160.29ms
step:1395/1480 train_time:222014ms step_avg:160.30ms
step:1396/1480 train_time:222183ms step_avg:160.31ms
step:1397/1480 train_time:222349ms step_avg:160.31ms
step:1398/1480 train_time:222516ms step_avg:160.31ms
step:1399/1480 train_time:222686ms step_avg:160.32ms
step:1400/1480 train_time:222863ms step_avg:160.33ms
step:1401/1480 train_time:223027ms step_avg:160.34ms
step:1402/1480 train_time:223199ms step_avg:160.34ms
step:1403/1480 train_time:223375ms step_avg:160.36ms
step:1404/1480 train_time:223546ms step_avg:160.36ms
step:1405/1480 train_time:223721ms step_avg:160.37ms
step:1406/1480 train_time:223895ms step_avg:160.38ms
step:1407/1480 train_time:224063ms step_avg:160.39ms
step:1408/1480 train_time:224231ms step_avg:160.39ms
step:1409/1480 train_time:224414ms step_avg:160.41ms
step:1410/1480 train_time:224583ms step_avg:160.42ms
step:1411/1480 train_time:224749ms step_avg:160.42ms
step:1412/1480 train_time:224919ms step_avg:160.43ms
step:1413/1480 train_time:225090ms step_avg:160.43ms
step:1414/1480 train_time:225261ms step_avg:160.44ms
step:1415/1480 train_time:225436ms step_avg:160.45ms
step:1416/1480 train_time:225621ms step_avg:160.47ms
step:1417/1480 train_time:225795ms step_avg:160.48ms
step:1418/1480 train_time:225966ms step_avg:160.49ms
step:1419/1480 train_time:226140ms step_avg:160.50ms
step:1420/1480 train_time:226315ms step_avg:160.51ms
step:1421/1480 train_time:226488ms step_avg:160.52ms
step:1422/1480 train_time:226661ms step_avg:160.52ms
step:1423/1480 train_time:226830ms step_avg:160.53ms
step:1424/1480 train_time:227008ms step_avg:160.54ms
step:1425/1480 train_time:227188ms step_avg:160.56ms
step:1426/1480 train_time:227359ms step_avg:160.56ms
step:1427/1480 train_time:227533ms step_avg:160.57ms
step:1428/1480 train_time:227703ms step_avg:160.58ms
step:1429/1480 train_time:227872ms step_avg:160.59ms
step:1430/1480 train_time:228046ms step_avg:160.60ms
step:1431/1480 train_time:228221ms step_avg:160.61ms
step:1432/1480 train_time:228400ms step_avg:160.62ms
step:1433/1480 train_time:228580ms step_avg:160.63ms
step:1434/1480 train_time:228759ms step_avg:160.65ms
step:1435/1480 train_time:228936ms step_avg:160.66ms
step:1436/1480 train_time:229109ms step_avg:160.67ms
step:1437/1480 train_time:229281ms step_avg:160.67ms
step:1438/1480 train_time:229449ms step_avg:160.68ms
step:1439/1480 train_time:229623ms step_avg:160.69ms
step:1440/1480 train_time:229791ms step_avg:160.69ms
step:1441/1480 train_time:229962ms step_avg:160.70ms
step:1442/1480 train_time:230140ms step_avg:160.71ms
step:1443/1480 train_time:230328ms step_avg:160.73ms
step:1444/1480 train_time:230499ms step_avg:160.74ms
step:1445/1480 train_time:230670ms step_avg:160.75ms
step:1446/1480 train_time:230845ms step_avg:160.76ms
step:1447/1480 train_time:231022ms step_avg:160.77ms
step:1448/1480 train_time:231194ms step_avg:160.77ms
step:1449/1480 train_time:231367ms step_avg:160.78ms
step:1450/1480 train_time:231541ms step_avg:160.79ms
step:1451/1480 train_time:231712ms step_avg:160.80ms
step:1452/1480 train_time:231886ms step_avg:160.81ms
step:1453/1480 train_time:232055ms step_avg:160.81ms
step:1454/1480 train_time:232228ms step_avg:160.82ms
step:1455/1480 train_time:232407ms step_avg:160.84ms
step:1456/1480 train_time:232580ms step_avg:160.84ms
step:1457/1480 train_time:232749ms step_avg:160.85ms
step:1458/1480 train_time:232920ms step_avg:160.86ms
step:1459/1480 train_time:233096ms step_avg:160.87ms
step:1460/1480 train_time:233267ms step_avg:160.87ms
step:1461/1480 train_time:233441ms step_avg:160.88ms
step:1462/1480 train_time:233612ms step_avg:160.89ms
step:1463/1480 train_time:233788ms step_avg:160.90ms
step:1464/1480 train_time:233964ms step_avg:160.91ms
step:1465/1480 train_time:234136ms step_avg:160.92ms
step:1466/1480 train_time:234306ms step_avg:160.92ms
step:1467/1480 train_time:234481ms step_avg:160.93ms
step:1468/1480 train_time:234650ms step_avg:160.94ms
step:1469/1480 train_time:234824ms step_avg:160.95ms
step:1470/1480 train_time:235005ms step_avg:160.96ms
step:1471/1480 train_time:235190ms step_avg:160.98ms
step:1472/1480 train_time:235371ms step_avg:160.99ms
step:1473/1480 train_time:235542ms step_avg:161.00ms
step:1474/1480 train_time:235720ms step_avg:161.01ms
step:1475/1480 train_time:235900ms step_avg:161.02ms
step:1476/1480 train_time:236072ms step_avg:161.03ms
step:1477/1480 train_time:236256ms step_avg:161.05ms
step:1478/1480 train_time:236440ms step_avg:161.06ms
step:1479/1480 train_time:236614ms step_avg:161.07ms
step:1480/1480 train_time:236786ms step_avg:161.08ms
step:1480/1480 val_loss:3.2799 train_time:236859ms step_avg:161.13ms

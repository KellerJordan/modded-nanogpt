import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977

        # This call computes relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# Fused Softcapped Cross Entropy

@triton.jit
def fused_softcapped_entropy_fwd_kernel(
    logits_ptr, losses_ptr, lse_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)
    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    
    max_val = -float('inf')
    sum_exp = 0.0
    
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)
        z = A * tl.sigmoid((val + B) / C)
        z = tl.where(mask, z, -float('inf'))
        curr_max = tl.max(z, axis=0)
        new_max = tl.maximum(max_val, curr_max)
        sum_exp = sum_exp * tl.exp(max_val - new_max) + tl.sum(tl.exp(z - new_max), axis=0)
        max_val = new_max
    
    lse = max_val + tl.log(sum_exp)
    tl.store(lse_ptr + row_idx, lse)
    
    total_loss = 0.0
    for k in range(n_predict):
        target_idx = row_idx + k
        if target_idx < n_rows:
            weight = tl.load(mtp_weights_ptr + k)
            if weight > 0:
                target = tl.load(targets_ptr + target_idx).to(tl.int32)
                if target >= 0 and target < n_cols:
                    val_target = tl.load(logits_row_ptr + target).to(tl.float32)
                    z_target = A * tl.sigmoid((val_target + B) / C)
                    total_loss += weight * (lse - z_target)
    
    tl.store(losses_ptr + row_idx, total_loss)

@triton.jit
def fused_softcapped_entropy_bwd_kernel(
    grad_input_ptr, grad_output_ptr, lse_ptr, logits_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v, stride_grad_n, stride_grad_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)

    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    grad_row_ptr = grad_input_ptr + row_idx * stride_grad_n
    
    lse = tl.load(lse_ptr + row_idx)
    grad_loss = tl.load(grad_output_ptr + row_idx)
    
    S_w = 0.0
    for k in range(n_predict):
        if row_idx + k < n_rows:
            S_w += tl.load(mtp_weights_ptr + k)
            
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=0.0).to(tl.float32)
        u = (val + B) / C
        sigmoid_u = tl.sigmoid(u)
        z = A * sigmoid_u
        p = tl.exp(z - lse)
        
        term1 = S_w * p
        term2 = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
        for k in range(n_predict):
            if row_idx + k < n_rows:
                target = tl.load(targets_ptr + row_idx + k).to(tl.int32)
                weight = tl.load(mtp_weights_ptr + k)
                term2 += tl.where(cols == target, weight, 0.0)
        
        grad_z = grad_loss * (term1 - term2)
        dz_dx = (1.0 / C) * z * (1.0 - sigmoid_u)
        grad_x = grad_z * dz_dx
        tl.store(grad_row_ptr + cols, grad_x.to(tl.bfloat16), mask=mask)

class FusedSoftcappedCrossEntropy(torch.autograd.Function):
    @staticmethod
    def forward(ctx, logits, targets, mtp_weights, A=23.0, B=5.0, C=7.5):
        n_rows, n_cols = logits.shape
        if mtp_weights is None:
             mtp_weights = torch.tensor([1.0], device=logits.device, dtype=torch.float32)
        n_predict = mtp_weights.shape[0]

        losses = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        lse = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        
        logits = logits.contiguous()
        targets = targets.contiguous()
        mtp_weights = mtp_weights.contiguous()

        grid = (n_rows,)
        fused_softcapped_entropy_fwd_kernel[grid](
            logits, losses, lse, targets, mtp_weights,
            logits.stride(0), logits.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        
        ctx.save_for_backward(logits, targets, mtp_weights, lse)
        ctx.params = (A, B, C)
        return losses

    @staticmethod
    def backward(ctx, grad_output):
        logits, targets, mtp_weights, lse = ctx.saved_tensors
        A, B, C = ctx.params
        n_rows, n_cols = logits.shape
        n_predict = mtp_weights.shape[0]
        
        grad_input = torch.empty((n_rows, n_cols), dtype=torch.bfloat16, device=logits.device)
        grad_output = grad_output.contiguous()
        
        grid = (n_rows,)
        fused_softcapped_entropy_bwd_kernel[grid](
            grad_input, grad_output, lse, logits, targets, mtp_weights,
            logits.stride(0), logits.stride(1), grad_input.stride(0), grad_input.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        return grad_input, None, None, None, None, None

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if self.training:
            losses = FusedSoftcappedCrossEntropy.apply(logits.view(-1, logits.size(-1)), target_seq, mtp_weights)
            loss = losses.sum()
        else:
            logits = 23 * torch.sigmoid((logits + 5) / 7.5)
            logits_for_loss = logits.float()
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Jan  8 2026, 06:52:19) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan 15 22:26:57 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   43C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   34C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   42C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   41C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   40C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    366568      C   /usr/bin/python3                             1510MiB |
|    1   N/A  N/A    366569      C   /usr/bin/python3                             1510MiB |
|    2   N/A  N/A    366570      C   /usr/bin/python3                             1510MiB |
|    3   N/A  N/A    366571      C   /usr/bin/python3                             1510MiB |
|    4   N/A  N/A    366572      C   /usr/bin/python3                             1510MiB |
|    5   N/A  N/A    366573      C   /usr/bin/python3                             1510MiB |
|    6   N/A  N/A    366574      C   /usr/bin/python3                             1510MiB |
|    7   N/A  N/A    366575      C   /usr/bin/python3                             1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8307 train_time:0ms step_avg:0.05ms
step:1/1775 train_time:80ms step_avg:79.63ms
step:2/1775 train_time:108ms step_avg:54.09ms
step:3/1775 train_time:131ms step_avg:43.59ms
step:4/1775 train_time:155ms step_avg:38.67ms
step:5/1775 train_time:176ms step_avg:35.13ms
step:6/1775 train_time:257ms step_avg:42.79ms
step:7/1775 train_time:277ms step_avg:39.62ms
step:8/1775 train_time:305ms step_avg:38.07ms
step:9/1775 train_time:326ms step_avg:36.26ms
step:10/1775 train_time:354ms step_avg:35.45ms
step:11/1775 train_time:385ms step_avg:35.02ms
step:12/1775 train_time:418ms step_avg:34.85ms
step:13/1775 train_time:449ms step_avg:34.55ms
step:14/1775 train_time:483ms step_avg:34.49ms
step:15/1775 train_time:514ms step_avg:34.26ms
step:16/1775 train_time:547ms step_avg:34.19ms
step:17/1775 train_time:578ms step_avg:33.99ms
step:18/1775 train_time:611ms step_avg:33.94ms
step:19/1775 train_time:642ms step_avg:33.80ms
step:20/1775 train_time:675ms step_avg:33.77ms
step:21/1775 train_time:706ms step_avg:33.63ms
step:22/1775 train_time:743ms step_avg:33.79ms
step:23/1775 train_time:770ms step_avg:33.49ms
step:24/1775 train_time:804ms step_avg:33.50ms
step:25/1775 train_time:835ms step_avg:33.40ms
step:26/1775 train_time:868ms step_avg:33.39ms
step:27/1775 train_time:899ms step_avg:33.30ms
step:28/1775 train_time:932ms step_avg:33.29ms
step:29/1775 train_time:963ms step_avg:33.21ms
step:30/1775 train_time:997ms step_avg:33.24ms
step:31/1775 train_time:1029ms step_avg:33.20ms
step:32/1775 train_time:1064ms step_avg:33.24ms
step:33/1775 train_time:1095ms step_avg:33.19ms
step:34/1775 train_time:1129ms step_avg:33.21ms
step:35/1775 train_time:1162ms step_avg:33.19ms
step:36/1775 train_time:1196ms step_avg:33.23ms
step:37/1775 train_time:1228ms step_avg:33.20ms
step:38/1775 train_time:1263ms step_avg:33.24ms
step:39/1775 train_time:1295ms step_avg:33.20ms
step:40/1775 train_time:1329ms step_avg:33.23ms
step:41/1775 train_time:1361ms step_avg:33.19ms
step:42/1775 train_time:1397ms step_avg:33.25ms
step:43/1775 train_time:1425ms step_avg:33.14ms
step:44/1775 train_time:1458ms step_avg:33.14ms
step:45/1775 train_time:1490ms step_avg:33.11ms
step:46/1775 train_time:1524ms step_avg:33.13ms
step:47/1775 train_time:1555ms step_avg:33.08ms
step:48/1775 train_time:1588ms step_avg:33.08ms
step:49/1775 train_time:1619ms step_avg:33.05ms
step:50/1775 train_time:1653ms step_avg:33.06ms
step:51/1775 train_time:1684ms step_avg:33.01ms
step:52/1775 train_time:1717ms step_avg:33.03ms
step:53/1775 train_time:1749ms step_avg:32.99ms
step:54/1775 train_time:1782ms step_avg:33.00ms
step:55/1775 train_time:1813ms step_avg:32.96ms
step:56/1775 train_time:1847ms step_avg:32.98ms
step:57/1775 train_time:1878ms step_avg:32.94ms
step:58/1775 train_time:1911ms step_avg:32.95ms
step:59/1775 train_time:1942ms step_avg:32.92ms
step:60/1775 train_time:1976ms step_avg:32.94ms
step:61/1775 train_time:2007ms step_avg:32.90ms
step:62/1775 train_time:2041ms step_avg:32.91ms
step:63/1775 train_time:2073ms step_avg:32.90ms
step:64/1775 train_time:2106ms step_avg:32.91ms
step:65/1775 train_time:2137ms step_avg:32.88ms
step:66/1775 train_time:2172ms step_avg:32.90ms
step:67/1775 train_time:2204ms step_avg:32.89ms
step:68/1775 train_time:2237ms step_avg:32.90ms
step:69/1775 train_time:2269ms step_avg:32.88ms
step:70/1775 train_time:2303ms step_avg:32.90ms
step:71/1775 train_time:2335ms step_avg:32.88ms
step:72/1775 train_time:2368ms step_avg:32.89ms
step:73/1775 train_time:2400ms step_avg:32.87ms
step:74/1775 train_time:2433ms step_avg:32.88ms
step:75/1775 train_time:2465ms step_avg:32.87ms
step:76/1775 train_time:2499ms step_avg:32.88ms
step:77/1775 train_time:2530ms step_avg:32.86ms
step:78/1775 train_time:2564ms step_avg:32.88ms
step:79/1775 train_time:2596ms step_avg:32.86ms
step:80/1775 train_time:2629ms step_avg:32.86ms
step:81/1775 train_time:2660ms step_avg:32.85ms
step:82/1775 train_time:2694ms step_avg:32.85ms
step:83/1775 train_time:2725ms step_avg:32.83ms
step:84/1775 train_time:2758ms step_avg:32.84ms
step:85/1775 train_time:2790ms step_avg:32.82ms
step:86/1775 train_time:2824ms step_avg:32.84ms
step:87/1775 train_time:2855ms step_avg:32.82ms
step:88/1775 train_time:2889ms step_avg:32.82ms
step:89/1775 train_time:2919ms step_avg:32.80ms
step:90/1775 train_time:2953ms step_avg:32.81ms
step:91/1775 train_time:2984ms step_avg:32.79ms
step:92/1775 train_time:3018ms step_avg:32.80ms
step:93/1775 train_time:3049ms step_avg:32.78ms
step:94/1775 train_time:3082ms step_avg:32.79ms
step:95/1775 train_time:3114ms step_avg:32.77ms
step:96/1775 train_time:3147ms step_avg:32.79ms
step:97/1775 train_time:3179ms step_avg:32.77ms
step:98/1775 train_time:3212ms step_avg:32.78ms
step:99/1775 train_time:3244ms step_avg:32.77ms
step:100/1775 train_time:3278ms step_avg:32.78ms
step:101/1775 train_time:3309ms step_avg:32.76ms
step:102/1775 train_time:3343ms step_avg:32.77ms
step:103/1775 train_time:3374ms step_avg:32.76ms
step:104/1775 train_time:3408ms step_avg:32.77ms
step:105/1775 train_time:3440ms step_avg:32.76ms
step:106/1775 train_time:3473ms step_avg:32.77ms
step:107/1775 train_time:3505ms step_avg:32.76ms
step:108/1775 train_time:3539ms step_avg:32.76ms
step:109/1775 train_time:3570ms step_avg:32.75ms
step:110/1775 train_time:3604ms step_avg:32.76ms
step:111/1775 train_time:3635ms step_avg:32.75ms
step:112/1775 train_time:3668ms step_avg:32.75ms
step:113/1775 train_time:3700ms step_avg:32.74ms
step:114/1775 train_time:3733ms step_avg:32.75ms
step:115/1775 train_time:3765ms step_avg:32.74ms
step:116/1775 train_time:3799ms step_avg:32.75ms
step:117/1775 train_time:3830ms step_avg:32.73ms
step:118/1775 train_time:3864ms step_avg:32.75ms
step:119/1775 train_time:3895ms step_avg:32.73ms
step:120/1775 train_time:3929ms step_avg:32.74ms
step:121/1775 train_time:3961ms step_avg:32.73ms
step:122/1775 train_time:3994ms step_avg:32.74ms
step:123/1775 train_time:4025ms step_avg:32.72ms
step:124/1775 train_time:4059ms step_avg:32.73ms
step:125/1775 train_time:4090ms step_avg:32.72ms
step:126/1775 train_time:4124ms step_avg:32.73ms
step:127/1775 train_time:4156ms step_avg:32.72ms
step:128/1775 train_time:4189ms step_avg:32.72ms
step:129/1775 train_time:4220ms step_avg:32.71ms
step:130/1775 train_time:4255ms step_avg:32.73ms
step:131/1775 train_time:4285ms step_avg:32.71ms
step:132/1775 train_time:4320ms step_avg:32.72ms
step:133/1775 train_time:4351ms step_avg:32.71ms
step:134/1775 train_time:4384ms step_avg:32.72ms
step:135/1775 train_time:4417ms step_avg:32.72ms
step:136/1775 train_time:4450ms step_avg:32.72ms
step:137/1775 train_time:4481ms step_avg:32.71ms
step:138/1775 train_time:4515ms step_avg:32.72ms
step:139/1775 train_time:4547ms step_avg:32.71ms
step:140/1775 train_time:4581ms step_avg:32.72ms
step:141/1775 train_time:4612ms step_avg:32.71ms
step:142/1775 train_time:4645ms step_avg:32.71ms
step:143/1775 train_time:4677ms step_avg:32.71ms
step:144/1775 train_time:4710ms step_avg:32.71ms
step:145/1775 train_time:4742ms step_avg:32.70ms
step:146/1775 train_time:4775ms step_avg:32.71ms
step:147/1775 train_time:4807ms step_avg:32.70ms
step:148/1775 train_time:4841ms step_avg:32.71ms
step:149/1775 train_time:4872ms step_avg:32.70ms
step:150/1775 train_time:4906ms step_avg:32.70ms
step:151/1775 train_time:4937ms step_avg:32.69ms
step:152/1775 train_time:4970ms step_avg:32.70ms
step:153/1775 train_time:5001ms step_avg:32.69ms
step:154/1775 train_time:5035ms step_avg:32.69ms
step:155/1775 train_time:5067ms step_avg:32.69ms
step:156/1775 train_time:5100ms step_avg:32.69ms
step:157/1775 train_time:5131ms step_avg:32.68ms
step:158/1775 train_time:5165ms step_avg:32.69ms
step:159/1775 train_time:5196ms step_avg:32.68ms
step:160/1775 train_time:5230ms step_avg:32.69ms
step:161/1775 train_time:5261ms step_avg:32.68ms
step:162/1775 train_time:5295ms step_avg:32.68ms
step:163/1775 train_time:5327ms step_avg:32.68ms
step:164/1775 train_time:5360ms step_avg:32.68ms
step:165/1775 train_time:5392ms step_avg:32.68ms
step:166/1775 train_time:5426ms step_avg:32.68ms
step:167/1775 train_time:5457ms step_avg:32.68ms
step:168/1775 train_time:5490ms step_avg:32.68ms
step:169/1775 train_time:5521ms step_avg:32.67ms
step:170/1775 train_time:5555ms step_avg:32.67ms
step:171/1775 train_time:5586ms step_avg:32.67ms
step:172/1775 train_time:5620ms step_avg:32.68ms
step:173/1775 train_time:5651ms step_avg:32.67ms
step:174/1775 train_time:5685ms step_avg:32.67ms
step:175/1775 train_time:5716ms step_avg:32.66ms
step:176/1775 train_time:5749ms step_avg:32.66ms
step:177/1775 train_time:5781ms step_avg:32.66ms
step:178/1775 train_time:5814ms step_avg:32.67ms
step:179/1775 train_time:5845ms step_avg:32.66ms
step:180/1775 train_time:5879ms step_avg:32.66ms
step:181/1775 train_time:5910ms step_avg:32.65ms
step:182/1775 train_time:5944ms step_avg:32.66ms
step:183/1775 train_time:5975ms step_avg:32.65ms
step:184/1775 train_time:6009ms step_avg:32.66ms
step:185/1775 train_time:6040ms step_avg:32.65ms
step:186/1775 train_time:6073ms step_avg:32.65ms
step:187/1775 train_time:6105ms step_avg:32.64ms
step:188/1775 train_time:6138ms step_avg:32.65ms
step:189/1775 train_time:6169ms step_avg:32.64ms
step:190/1775 train_time:6203ms step_avg:32.65ms
step:191/1775 train_time:6235ms step_avg:32.64ms
step:192/1775 train_time:6269ms step_avg:32.65ms
step:193/1775 train_time:6300ms step_avg:32.64ms
step:194/1775 train_time:6334ms step_avg:32.65ms
step:195/1775 train_time:6366ms step_avg:32.65ms
step:196/1775 train_time:6399ms step_avg:32.65ms
step:197/1775 train_time:6430ms step_avg:32.64ms
step:198/1775 train_time:6464ms step_avg:32.65ms
step:199/1775 train_time:6495ms step_avg:32.64ms
step:200/1775 train_time:6529ms step_avg:32.64ms
step:201/1775 train_time:6560ms step_avg:32.64ms
step:202/1775 train_time:6594ms step_avg:32.64ms
step:203/1775 train_time:6625ms step_avg:32.64ms
step:204/1775 train_time:6658ms step_avg:32.64ms
step:205/1775 train_time:6689ms step_avg:32.63ms
step:206/1775 train_time:6724ms step_avg:32.64ms
step:207/1775 train_time:6755ms step_avg:32.63ms
step:208/1775 train_time:6788ms step_avg:32.64ms
step:209/1775 train_time:6820ms step_avg:32.63ms
step:210/1775 train_time:6852ms step_avg:32.63ms
step:211/1775 train_time:6885ms step_avg:32.63ms
step:212/1775 train_time:6918ms step_avg:32.63ms
step:213/1775 train_time:6949ms step_avg:32.62ms
step:214/1775 train_time:6983ms step_avg:32.63ms
step:215/1775 train_time:7014ms step_avg:32.62ms
step:216/1775 train_time:7047ms step_avg:32.63ms
step:217/1775 train_time:7079ms step_avg:32.62ms
step:218/1775 train_time:7112ms step_avg:32.62ms
step:219/1775 train_time:7143ms step_avg:32.62ms
step:220/1775 train_time:7177ms step_avg:32.62ms
step:221/1775 train_time:7208ms step_avg:32.61ms
step:222/1775 train_time:7241ms step_avg:32.62ms
step:223/1775 train_time:7273ms step_avg:32.62ms
step:224/1775 train_time:7306ms step_avg:32.62ms
step:225/1775 train_time:7337ms step_avg:32.61ms
step:226/1775 train_time:7371ms step_avg:32.61ms
step:227/1775 train_time:7402ms step_avg:32.61ms
step:228/1775 train_time:7436ms step_avg:32.61ms
step:229/1775 train_time:7467ms step_avg:32.61ms
step:230/1775 train_time:7500ms step_avg:32.61ms
step:231/1775 train_time:7531ms step_avg:32.60ms
step:232/1775 train_time:7565ms step_avg:32.61ms
step:233/1775 train_time:7596ms step_avg:32.60ms
step:234/1775 train_time:7629ms step_avg:32.60ms
step:235/1775 train_time:7661ms step_avg:32.60ms
step:236/1775 train_time:7695ms step_avg:32.60ms
step:237/1775 train_time:7726ms step_avg:32.60ms
step:238/1775 train_time:7759ms step_avg:32.60ms
step:239/1775 train_time:7790ms step_avg:32.59ms
step:240/1775 train_time:7824ms step_avg:32.60ms
step:241/1775 train_time:7856ms step_avg:32.60ms
step:242/1775 train_time:7889ms step_avg:32.60ms
step:243/1775 train_time:7920ms step_avg:32.59ms
step:244/1775 train_time:7954ms step_avg:32.60ms
step:245/1775 train_time:7985ms step_avg:32.59ms
step:246/1775 train_time:8018ms step_avg:32.60ms
step:247/1775 train_time:8049ms step_avg:32.59ms
step:248/1775 train_time:8083ms step_avg:32.59ms
step:249/1775 train_time:8115ms step_avg:32.59ms
step:250/1775 train_time:8148ms step_avg:32.59ms
step:250/1775 val_loss:4.6033 train_time:8190ms step_avg:32.76ms
step:251/1775 train_time:8219ms step_avg:32.75ms
step:252/1775 train_time:8243ms step_avg:32.71ms
step:253/1775 train_time:8263ms step_avg:32.66ms
step:254/1775 train_time:8285ms step_avg:32.62ms
step:255/1775 train_time:8314ms step_avg:32.60ms
step:256/1775 train_time:8348ms step_avg:32.61ms
step:257/1775 train_time:8380ms step_avg:32.61ms
step:258/1775 train_time:8413ms step_avg:32.61ms
step:259/1775 train_time:8444ms step_avg:32.60ms
step:260/1775 train_time:8477ms step_avg:32.60ms
step:261/1775 train_time:8508ms step_avg:32.60ms
step:262/1775 train_time:8541ms step_avg:32.60ms
step:263/1775 train_time:8573ms step_avg:32.60ms
step:264/1775 train_time:8606ms step_avg:32.60ms
step:265/1775 train_time:8637ms step_avg:32.59ms
step:266/1775 train_time:8671ms step_avg:32.60ms
step:267/1775 train_time:8702ms step_avg:32.59ms
step:268/1775 train_time:8735ms step_avg:32.59ms
step:269/1775 train_time:8766ms step_avg:32.59ms
step:270/1775 train_time:8799ms step_avg:32.59ms
step:271/1775 train_time:8831ms step_avg:32.59ms
step:272/1775 train_time:8863ms step_avg:32.59ms
step:273/1775 train_time:8894ms step_avg:32.58ms
step:274/1775 train_time:8927ms step_avg:32.58ms
step:275/1775 train_time:8959ms step_avg:32.58ms
step:276/1775 train_time:8992ms step_avg:32.58ms
step:277/1775 train_time:9023ms step_avg:32.57ms
step:278/1775 train_time:9056ms step_avg:32.58ms
step:279/1775 train_time:9088ms step_avg:32.57ms
step:280/1775 train_time:9121ms step_avg:32.58ms
step:281/1775 train_time:9153ms step_avg:32.57ms
step:282/1775 train_time:9188ms step_avg:32.58ms
step:283/1775 train_time:9219ms step_avg:32.58ms
step:284/1775 train_time:9254ms step_avg:32.58ms
step:285/1775 train_time:9285ms step_avg:32.58ms
step:286/1775 train_time:9319ms step_avg:32.58ms
step:287/1775 train_time:9351ms step_avg:32.58ms
step:288/1775 train_time:9384ms step_avg:32.58ms
step:289/1775 train_time:9415ms step_avg:32.58ms
step:290/1775 train_time:9449ms step_avg:32.58ms
step:291/1775 train_time:9480ms step_avg:32.58ms
step:292/1775 train_time:9513ms step_avg:32.58ms
step:293/1775 train_time:9545ms step_avg:32.58ms
step:294/1775 train_time:9578ms step_avg:32.58ms
step:295/1775 train_time:9609ms step_avg:32.57ms
step:296/1775 train_time:9642ms step_avg:32.57ms
step:297/1775 train_time:9674ms step_avg:32.57ms
step:298/1775 train_time:9708ms step_avg:32.58ms
step:299/1775 train_time:9739ms step_avg:32.57ms
step:300/1775 train_time:9772ms step_avg:32.57ms
step:301/1775 train_time:9804ms step_avg:32.57ms
step:302/1775 train_time:9837ms step_avg:32.57ms
step:303/1775 train_time:9868ms step_avg:32.57ms
step:304/1775 train_time:9901ms step_avg:32.57ms
step:305/1775 train_time:9932ms step_avg:32.56ms
step:306/1775 train_time:9966ms step_avg:32.57ms
step:307/1775 train_time:9997ms step_avg:32.56ms
step:308/1775 train_time:10030ms step_avg:32.57ms
step:309/1775 train_time:10061ms step_avg:32.56ms
step:310/1775 train_time:10095ms step_avg:32.57ms
step:311/1775 train_time:10127ms step_avg:32.56ms
step:312/1775 train_time:10160ms step_avg:32.56ms
step:313/1775 train_time:10192ms step_avg:32.56ms
step:314/1775 train_time:10226ms step_avg:32.57ms
step:315/1775 train_time:10257ms step_avg:32.56ms
step:316/1775 train_time:10291ms step_avg:32.57ms
step:317/1775 train_time:10322ms step_avg:32.56ms
step:318/1775 train_time:10356ms step_avg:32.57ms
step:319/1775 train_time:10387ms step_avg:32.56ms
step:320/1775 train_time:10420ms step_avg:32.56ms
step:321/1775 train_time:10452ms step_avg:32.56ms
step:322/1775 train_time:10486ms step_avg:32.56ms
step:323/1775 train_time:10517ms step_avg:32.56ms
step:324/1775 train_time:10550ms step_avg:32.56ms
step:325/1775 train_time:10582ms step_avg:32.56ms
step:326/1775 train_time:10615ms step_avg:32.56ms
step:327/1775 train_time:10646ms step_avg:32.56ms
step:328/1775 train_time:10679ms step_avg:32.56ms
step:329/1775 train_time:10710ms step_avg:32.55ms
step:330/1775 train_time:10743ms step_avg:32.55ms
step:331/1775 train_time:10775ms step_avg:32.55ms
step:332/1775 train_time:10808ms step_avg:32.55ms
step:333/1775 train_time:10839ms step_avg:32.55ms
step:334/1775 train_time:10872ms step_avg:32.55ms
step:335/1775 train_time:10903ms step_avg:32.54ms
step:336/1775 train_time:10936ms step_avg:32.55ms
step:337/1775 train_time:10967ms step_avg:32.54ms
step:338/1775 train_time:11000ms step_avg:32.55ms
step:339/1775 train_time:11032ms step_avg:32.54ms
step:340/1775 train_time:11066ms step_avg:32.55ms
step:341/1775 train_time:11097ms step_avg:32.54ms
step:342/1775 train_time:11130ms step_avg:32.55ms
step:343/1775 train_time:11161ms step_avg:32.54ms
step:344/1775 train_time:11195ms step_avg:32.54ms
step:345/1775 train_time:11226ms step_avg:32.54ms
step:346/1775 train_time:11260ms step_avg:32.54ms
step:347/1775 train_time:11291ms step_avg:32.54ms
step:348/1775 train_time:11325ms step_avg:32.54ms
step:349/1775 train_time:11356ms step_avg:32.54ms
step:350/1775 train_time:11390ms step_avg:32.54ms
step:351/1775 train_time:11421ms step_avg:32.54ms
step:352/1775 train_time:11454ms step_avg:32.54ms
step:353/1775 train_time:11486ms step_avg:32.54ms
step:354/1775 train_time:11520ms step_avg:32.54ms
step:355/1775 train_time:11551ms step_avg:32.54ms
step:356/1775 train_time:11585ms step_avg:32.54ms
step:357/1775 train_time:11616ms step_avg:32.54ms
step:358/1775 train_time:11649ms step_avg:32.54ms
step:359/1775 train_time:11681ms step_avg:32.54ms
step:360/1775 train_time:11715ms step_avg:32.54ms
step:361/1775 train_time:11747ms step_avg:32.54ms
step:362/1775 train_time:11780ms step_avg:32.54ms
step:363/1775 train_time:11811ms step_avg:32.54ms
step:364/1775 train_time:11845ms step_avg:32.54ms
step:365/1775 train_time:11876ms step_avg:32.54ms
step:366/1775 train_time:11909ms step_avg:32.54ms
step:367/1775 train_time:11940ms step_avg:32.53ms
step:368/1775 train_time:11973ms step_avg:32.54ms
step:369/1775 train_time:12004ms step_avg:32.53ms
step:370/1775 train_time:12037ms step_avg:32.53ms
step:371/1775 train_time:12069ms step_avg:32.53ms
step:372/1775 train_time:12102ms step_avg:32.53ms
step:373/1775 train_time:12133ms step_avg:32.53ms
step:374/1775 train_time:12167ms step_avg:32.53ms
step:375/1775 train_time:12198ms step_avg:32.53ms
step:376/1775 train_time:12232ms step_avg:32.53ms
step:377/1775 train_time:12263ms step_avg:32.53ms
step:378/1775 train_time:12296ms step_avg:32.53ms
step:379/1775 train_time:12327ms step_avg:32.53ms
step:380/1775 train_time:12361ms step_avg:32.53ms
step:381/1775 train_time:12392ms step_avg:32.53ms
step:382/1775 train_time:12426ms step_avg:32.53ms
step:383/1775 train_time:12457ms step_avg:32.53ms
step:384/1775 train_time:12491ms step_avg:32.53ms
step:385/1775 train_time:12523ms step_avg:32.53ms
step:386/1775 train_time:12556ms step_avg:32.53ms
step:387/1775 train_time:12588ms step_avg:32.53ms
step:388/1775 train_time:12621ms step_avg:32.53ms
step:389/1775 train_time:12652ms step_avg:32.52ms
step:390/1775 train_time:12685ms step_avg:32.53ms
step:391/1775 train_time:12717ms step_avg:32.52ms
step:392/1775 train_time:12750ms step_avg:32.53ms
step:393/1775 train_time:12781ms step_avg:32.52ms
step:394/1775 train_time:12815ms step_avg:32.53ms
step:395/1775 train_time:12846ms step_avg:32.52ms
step:396/1775 train_time:12880ms step_avg:32.52ms
step:397/1775 train_time:12910ms step_avg:32.52ms
step:398/1775 train_time:12943ms step_avg:32.52ms
step:399/1775 train_time:12975ms step_avg:32.52ms
step:400/1775 train_time:13008ms step_avg:32.52ms
step:401/1775 train_time:13039ms step_avg:32.52ms
step:402/1775 train_time:13073ms step_avg:32.52ms
step:403/1775 train_time:13104ms step_avg:32.52ms
step:404/1775 train_time:13137ms step_avg:32.52ms
step:405/1775 train_time:13168ms step_avg:32.51ms
step:406/1775 train_time:13202ms step_avg:32.52ms
step:407/1775 train_time:13233ms step_avg:32.51ms
step:408/1775 train_time:13266ms step_avg:32.52ms
step:409/1775 train_time:13298ms step_avg:32.51ms
step:410/1775 train_time:13331ms step_avg:32.52ms
step:411/1775 train_time:13362ms step_avg:32.51ms
step:412/1775 train_time:13396ms step_avg:32.51ms
step:413/1775 train_time:13427ms step_avg:32.51ms
step:414/1775 train_time:13460ms step_avg:32.51ms
step:415/1775 train_time:13492ms step_avg:32.51ms
step:416/1775 train_time:13526ms step_avg:32.51ms
step:417/1775 train_time:13557ms step_avg:32.51ms
step:418/1775 train_time:13590ms step_avg:32.51ms
step:419/1775 train_time:13621ms step_avg:32.51ms
step:420/1775 train_time:13655ms step_avg:32.51ms
step:421/1775 train_time:13686ms step_avg:32.51ms
step:422/1775 train_time:13720ms step_avg:32.51ms
step:423/1775 train_time:13751ms step_avg:32.51ms
step:424/1775 train_time:13785ms step_avg:32.51ms
step:425/1775 train_time:13815ms step_avg:32.51ms
step:426/1775 train_time:13849ms step_avg:32.51ms
step:427/1775 train_time:13880ms step_avg:32.51ms
step:428/1775 train_time:13913ms step_avg:32.51ms
step:429/1775 train_time:13945ms step_avg:32.51ms
step:430/1775 train_time:13979ms step_avg:32.51ms
step:431/1775 train_time:14009ms step_avg:32.50ms
step:432/1775 train_time:14042ms step_avg:32.51ms
step:433/1775 train_time:14074ms step_avg:32.50ms
step:434/1775 train_time:14107ms step_avg:32.51ms
step:435/1775 train_time:14138ms step_avg:32.50ms
step:436/1775 train_time:14171ms step_avg:32.50ms
step:437/1775 train_time:14202ms step_avg:32.50ms
step:438/1775 train_time:14236ms step_avg:32.50ms
step:439/1775 train_time:14267ms step_avg:32.50ms
step:440/1775 train_time:14301ms step_avg:32.50ms
step:441/1775 train_time:14332ms step_avg:32.50ms
step:442/1775 train_time:14366ms step_avg:32.50ms
step:443/1775 train_time:14397ms step_avg:32.50ms
step:444/1775 train_time:14431ms step_avg:32.50ms
step:445/1775 train_time:14462ms step_avg:32.50ms
step:446/1775 train_time:14495ms step_avg:32.50ms
step:447/1775 train_time:14526ms step_avg:32.50ms
step:448/1775 train_time:14560ms step_avg:32.50ms
step:449/1775 train_time:14591ms step_avg:32.50ms
step:450/1775 train_time:14625ms step_avg:32.50ms
step:451/1775 train_time:14656ms step_avg:32.50ms
step:452/1775 train_time:14689ms step_avg:32.50ms
step:453/1775 train_time:14721ms step_avg:32.50ms
step:454/1775 train_time:14755ms step_avg:32.50ms
step:455/1775 train_time:14786ms step_avg:32.50ms
step:456/1775 train_time:14820ms step_avg:32.50ms
step:457/1775 train_time:14851ms step_avg:32.50ms
step:458/1775 train_time:14885ms step_avg:32.50ms
step:459/1775 train_time:14916ms step_avg:32.50ms
step:460/1775 train_time:14949ms step_avg:32.50ms
step:461/1775 train_time:14980ms step_avg:32.50ms
step:462/1775 train_time:15014ms step_avg:32.50ms
step:463/1775 train_time:15045ms step_avg:32.50ms
step:464/1775 train_time:15078ms step_avg:32.50ms
step:465/1775 train_time:15109ms step_avg:32.49ms
step:466/1775 train_time:15142ms step_avg:32.49ms
step:467/1775 train_time:15173ms step_avg:32.49ms
step:468/1775 train_time:15207ms step_avg:32.49ms
step:469/1775 train_time:15238ms step_avg:32.49ms
step:470/1775 train_time:15272ms step_avg:32.49ms
step:471/1775 train_time:15304ms step_avg:32.49ms
step:472/1775 train_time:15337ms step_avg:32.49ms
step:473/1775 train_time:15368ms step_avg:32.49ms
step:474/1775 train_time:15401ms step_avg:32.49ms
step:475/1775 train_time:15433ms step_avg:32.49ms
step:476/1775 train_time:15467ms step_avg:32.49ms
step:477/1775 train_time:15498ms step_avg:32.49ms
step:478/1775 train_time:15532ms step_avg:32.49ms
step:479/1775 train_time:15564ms step_avg:32.49ms
step:480/1775 train_time:15597ms step_avg:32.49ms
step:481/1775 train_time:15629ms step_avg:32.49ms
step:482/1775 train_time:15662ms step_avg:32.49ms
step:483/1775 train_time:15693ms step_avg:32.49ms
step:484/1775 train_time:15726ms step_avg:32.49ms
step:485/1775 train_time:15757ms step_avg:32.49ms
step:486/1775 train_time:15790ms step_avg:32.49ms
step:487/1775 train_time:15821ms step_avg:32.49ms
step:488/1775 train_time:15855ms step_avg:32.49ms
step:489/1775 train_time:15886ms step_avg:32.49ms
step:490/1775 train_time:15919ms step_avg:32.49ms
step:491/1775 train_time:15951ms step_avg:32.49ms
step:492/1775 train_time:15984ms step_avg:32.49ms
step:493/1775 train_time:16015ms step_avg:32.49ms
step:494/1775 train_time:16049ms step_avg:32.49ms
step:495/1775 train_time:16080ms step_avg:32.49ms
step:496/1775 train_time:16114ms step_avg:32.49ms
step:497/1775 train_time:16145ms step_avg:32.49ms
step:498/1775 train_time:16178ms step_avg:32.49ms
step:499/1775 train_time:16209ms step_avg:32.48ms
step:500/1775 train_time:16243ms step_avg:32.49ms
step:500/1775 val_loss:4.2723 train_time:16284ms step_avg:32.57ms
step:501/1775 train_time:16308ms step_avg:32.55ms
step:502/1775 train_time:16330ms step_avg:32.53ms
step:503/1775 train_time:16350ms step_avg:32.50ms
step:504/1775 train_time:16375ms step_avg:32.49ms
step:505/1775 train_time:16407ms step_avg:32.49ms
step:506/1775 train_time:16441ms step_avg:32.49ms
step:507/1775 train_time:16472ms step_avg:32.49ms
step:508/1775 train_time:16505ms step_avg:32.49ms
step:509/1775 train_time:16537ms step_avg:32.49ms
step:510/1775 train_time:16571ms step_avg:32.49ms
step:511/1775 train_time:16601ms step_avg:32.49ms
step:512/1775 train_time:16634ms step_avg:32.49ms
step:513/1775 train_time:16665ms step_avg:32.49ms
step:514/1775 train_time:16699ms step_avg:32.49ms
step:515/1775 train_time:16729ms step_avg:32.48ms
step:516/1775 train_time:16763ms step_avg:32.49ms
step:517/1775 train_time:16794ms step_avg:32.48ms
step:518/1775 train_time:16827ms step_avg:32.49ms
step:519/1775 train_time:16858ms step_avg:32.48ms
step:520/1775 train_time:16891ms step_avg:32.48ms
step:521/1775 train_time:16922ms step_avg:32.48ms
step:522/1775 train_time:16956ms step_avg:32.48ms
step:523/1775 train_time:16986ms step_avg:32.48ms
step:524/1775 train_time:17019ms step_avg:32.48ms
step:525/1775 train_time:17050ms step_avg:32.48ms
step:526/1775 train_time:17084ms step_avg:32.48ms
step:527/1775 train_time:17115ms step_avg:32.48ms
step:528/1775 train_time:17147ms step_avg:32.48ms
step:529/1775 train_time:17179ms step_avg:32.48ms
step:530/1775 train_time:17213ms step_avg:32.48ms
step:531/1775 train_time:17246ms step_avg:32.48ms
step:532/1775 train_time:17281ms step_avg:32.48ms
step:533/1775 train_time:17313ms step_avg:32.48ms
step:534/1775 train_time:17347ms step_avg:32.48ms
step:535/1775 train_time:17379ms step_avg:32.48ms
step:536/1775 train_time:17412ms step_avg:32.49ms
step:537/1775 train_time:17444ms step_avg:32.48ms
step:538/1775 train_time:17477ms step_avg:32.49ms
step:539/1775 train_time:17508ms step_avg:32.48ms
step:540/1775 train_time:17542ms step_avg:32.48ms
step:541/1775 train_time:17572ms step_avg:32.48ms
step:542/1775 train_time:17606ms step_avg:32.48ms
step:543/1775 train_time:17637ms step_avg:32.48ms
step:544/1775 train_time:17671ms step_avg:32.48ms
step:545/1775 train_time:17702ms step_avg:32.48ms
step:546/1775 train_time:17735ms step_avg:32.48ms
step:547/1775 train_time:17766ms step_avg:32.48ms
step:548/1775 train_time:17799ms step_avg:32.48ms
step:549/1775 train_time:17830ms step_avg:32.48ms
step:550/1775 train_time:17863ms step_avg:32.48ms
step:551/1775 train_time:17894ms step_avg:32.48ms
step:552/1775 train_time:17928ms step_avg:32.48ms
step:553/1775 train_time:17959ms step_avg:32.48ms
step:554/1775 train_time:17993ms step_avg:32.48ms
step:555/1775 train_time:18024ms step_avg:32.47ms
step:556/1775 train_time:18057ms step_avg:32.48ms
step:557/1775 train_time:18088ms step_avg:32.47ms
step:558/1775 train_time:18121ms step_avg:32.47ms
step:559/1775 train_time:18152ms step_avg:32.47ms
step:560/1775 train_time:18187ms step_avg:32.48ms
step:561/1775 train_time:18218ms step_avg:32.47ms
step:562/1775 train_time:18251ms step_avg:32.48ms
step:563/1775 train_time:18283ms step_avg:32.47ms
step:564/1775 train_time:18317ms step_avg:32.48ms
step:565/1775 train_time:18348ms step_avg:32.47ms
step:566/1775 train_time:18381ms step_avg:32.48ms
step:567/1775 train_time:18413ms step_avg:32.47ms
step:568/1775 train_time:18447ms step_avg:32.48ms
step:569/1775 train_time:18479ms step_avg:32.48ms
step:570/1775 train_time:18512ms step_avg:32.48ms
step:571/1775 train_time:18543ms step_avg:32.48ms
step:572/1775 train_time:18576ms step_avg:32.48ms
step:573/1775 train_time:18608ms step_avg:32.47ms
step:574/1775 train_time:18641ms step_avg:32.48ms
step:575/1775 train_time:18672ms step_avg:32.47ms
step:576/1775 train_time:18707ms step_avg:32.48ms
step:577/1775 train_time:18737ms step_avg:32.47ms
step:578/1775 train_time:18771ms step_avg:32.48ms
step:579/1775 train_time:18802ms step_avg:32.47ms
step:580/1775 train_time:18842ms step_avg:32.49ms
step:581/1775 train_time:18895ms step_avg:32.52ms
step:582/1775 train_time:18955ms step_avg:32.57ms
step:583/1775 train_time:19013ms step_avg:32.61ms
step:584/1775 train_time:19073ms step_avg:32.66ms
step:585/1775 train_time:19131ms step_avg:32.70ms
step:586/1775 train_time:19191ms step_avg:32.75ms
step:587/1775 train_time:19249ms step_avg:32.79ms
step:588/1775 train_time:19309ms step_avg:32.84ms
step:589/1775 train_time:19367ms step_avg:32.88ms
step:590/1775 train_time:19428ms step_avg:32.93ms
step:591/1775 train_time:19486ms step_avg:32.97ms
step:592/1775 train_time:19549ms step_avg:33.02ms
step:593/1775 train_time:19606ms step_avg:33.06ms
step:594/1775 train_time:19666ms step_avg:33.11ms
step:595/1775 train_time:19724ms step_avg:33.15ms
step:596/1775 train_time:19784ms step_avg:33.20ms
step:597/1775 train_time:19842ms step_avg:33.24ms
step:598/1775 train_time:19903ms step_avg:33.28ms
step:599/1775 train_time:19962ms step_avg:33.32ms
step:600/1775 train_time:20022ms step_avg:33.37ms
step:601/1775 train_time:20079ms step_avg:33.41ms
step:602/1775 train_time:20140ms step_avg:33.46ms
step:603/1775 train_time:20198ms step_avg:33.50ms
step:604/1775 train_time:20259ms step_avg:33.54ms
step:605/1775 train_time:20316ms step_avg:33.58ms
step:606/1775 train_time:20376ms step_avg:33.62ms
step:607/1775 train_time:20435ms step_avg:33.67ms
step:608/1775 train_time:20497ms step_avg:33.71ms
step:609/1775 train_time:20555ms step_avg:33.75ms
step:610/1775 train_time:20616ms step_avg:33.80ms
step:611/1775 train_time:20674ms step_avg:33.84ms
step:612/1775 train_time:20735ms step_avg:33.88ms
step:613/1775 train_time:20793ms step_avg:33.92ms
step:614/1775 train_time:20854ms step_avg:33.96ms
step:615/1775 train_time:20911ms step_avg:34.00ms
step:616/1775 train_time:20971ms step_avg:34.04ms
step:617/1775 train_time:21028ms step_avg:34.08ms
step:618/1775 train_time:21090ms step_avg:34.13ms
step:619/1775 train_time:21148ms step_avg:34.16ms
step:620/1775 train_time:21208ms step_avg:34.21ms
step:621/1775 train_time:21266ms step_avg:34.25ms
step:622/1775 train_time:21328ms step_avg:34.29ms
step:623/1775 train_time:21385ms step_avg:34.33ms
step:624/1775 train_time:21446ms step_avg:34.37ms
step:625/1775 train_time:21504ms step_avg:34.41ms
step:626/1775 train_time:21564ms step_avg:34.45ms
step:627/1775 train_time:21623ms step_avg:34.49ms
step:628/1775 train_time:21683ms step_avg:34.53ms
step:629/1775 train_time:21740ms step_avg:34.56ms
step:630/1775 train_time:21801ms step_avg:34.60ms
step:631/1775 train_time:21858ms step_avg:34.64ms
step:632/1775 train_time:21918ms step_avg:34.68ms
step:633/1775 train_time:21975ms step_avg:34.72ms
step:634/1775 train_time:22036ms step_avg:34.76ms
step:635/1775 train_time:22093ms step_avg:34.79ms
step:636/1775 train_time:22155ms step_avg:34.84ms
step:637/1775 train_time:22213ms step_avg:34.87ms
step:638/1775 train_time:22273ms step_avg:34.91ms
step:639/1775 train_time:22331ms step_avg:34.95ms
step:640/1775 train_time:22391ms step_avg:34.99ms
step:641/1775 train_time:22450ms step_avg:35.02ms
step:642/1775 train_time:22510ms step_avg:35.06ms
step:643/1775 train_time:22568ms step_avg:35.10ms
step:644/1775 train_time:22628ms step_avg:35.14ms
step:645/1775 train_time:22687ms step_avg:35.17ms
step:646/1775 train_time:22749ms step_avg:35.21ms
step:647/1775 train_time:22807ms step_avg:35.25ms
step:648/1775 train_time:22868ms step_avg:35.29ms
step:649/1775 train_time:22925ms step_avg:35.32ms
step:650/1775 train_time:22987ms step_avg:35.37ms
step:651/1775 train_time:23046ms step_avg:35.40ms
step:652/1775 train_time:23106ms step_avg:35.44ms
step:653/1775 train_time:23163ms step_avg:35.47ms
step:654/1775 train_time:23224ms step_avg:35.51ms
step:655/1775 train_time:23282ms step_avg:35.54ms
step:656/1775 train_time:23343ms step_avg:35.58ms
step:657/1775 train_time:23401ms step_avg:35.62ms
step:658/1775 train_time:23461ms step_avg:35.66ms
step:659/1775 train_time:23520ms step_avg:35.69ms
step:660/1775 train_time:23581ms step_avg:35.73ms
step:661/1775 train_time:23639ms step_avg:35.76ms
step:662/1775 train_time:23699ms step_avg:35.80ms
step:663/1775 train_time:23758ms step_avg:35.83ms
step:664/1775 train_time:23818ms step_avg:35.87ms
step:665/1775 train_time:23876ms step_avg:35.90ms
step:666/1775 train_time:23936ms step_avg:35.94ms
step:667/1775 train_time:23995ms step_avg:35.97ms
step:668/1775 train_time:24056ms step_avg:36.01ms
step:669/1775 train_time:24113ms step_avg:36.04ms
step:670/1775 train_time:24173ms step_avg:36.08ms
step:671/1775 train_time:24231ms step_avg:36.11ms
step:672/1775 train_time:24290ms step_avg:36.15ms
step:673/1775 train_time:24349ms step_avg:36.18ms
step:674/1775 train_time:24410ms step_avg:36.22ms
step:675/1775 train_time:24468ms step_avg:36.25ms
step:676/1775 train_time:24528ms step_avg:36.28ms
step:677/1775 train_time:24587ms step_avg:36.32ms
step:678/1775 train_time:24649ms step_avg:36.35ms
step:679/1775 train_time:24707ms step_avg:36.39ms
step:680/1775 train_time:24768ms step_avg:36.42ms
step:681/1775 train_time:24826ms step_avg:36.46ms
step:682/1775 train_time:24885ms step_avg:36.49ms
step:683/1775 train_time:24943ms step_avg:36.52ms
step:684/1775 train_time:25003ms step_avg:36.55ms
step:685/1775 train_time:25062ms step_avg:36.59ms
step:686/1775 train_time:25122ms step_avg:36.62ms
step:687/1775 train_time:25180ms step_avg:36.65ms
step:688/1775 train_time:25241ms step_avg:36.69ms
step:689/1775 train_time:25300ms step_avg:36.72ms
step:690/1775 train_time:25360ms step_avg:36.75ms
step:691/1775 train_time:25419ms step_avg:36.79ms
step:692/1775 train_time:25480ms step_avg:36.82ms
step:693/1775 train_time:25538ms step_avg:36.85ms
step:694/1775 train_time:25598ms step_avg:36.88ms
step:695/1775 train_time:25656ms step_avg:36.92ms
step:696/1775 train_time:25717ms step_avg:36.95ms
step:697/1775 train_time:25775ms step_avg:36.98ms
step:698/1775 train_time:25835ms step_avg:37.01ms
step:699/1775 train_time:25892ms step_avg:37.04ms
step:700/1775 train_time:25953ms step_avg:37.08ms
step:701/1775 train_time:26010ms step_avg:37.10ms
step:702/1775 train_time:26070ms step_avg:37.14ms
step:703/1775 train_time:26128ms step_avg:37.17ms
step:704/1775 train_time:26189ms step_avg:37.20ms
step:705/1775 train_time:26247ms step_avg:37.23ms
step:706/1775 train_time:26307ms step_avg:37.26ms
step:707/1775 train_time:26365ms step_avg:37.29ms
step:708/1775 train_time:26426ms step_avg:37.32ms
step:709/1775 train_time:26484ms step_avg:37.35ms
step:710/1775 train_time:26545ms step_avg:37.39ms
step:711/1775 train_time:26603ms step_avg:37.42ms
step:712/1775 train_time:26663ms step_avg:37.45ms
step:713/1775 train_time:26721ms step_avg:37.48ms
step:714/1775 train_time:26782ms step_avg:37.51ms
step:715/1775 train_time:26839ms step_avg:37.54ms
step:716/1775 train_time:26899ms step_avg:37.57ms
step:717/1775 train_time:26957ms step_avg:37.60ms
step:718/1775 train_time:27017ms step_avg:37.63ms
step:719/1775 train_time:27076ms step_avg:37.66ms
step:720/1775 train_time:27137ms step_avg:37.69ms
step:721/1775 train_time:27195ms step_avg:37.72ms
step:722/1775 train_time:27256ms step_avg:37.75ms
step:723/1775 train_time:27313ms step_avg:37.78ms
step:724/1775 train_time:27374ms step_avg:37.81ms
step:725/1775 train_time:27432ms step_avg:37.84ms
step:726/1775 train_time:27492ms step_avg:37.87ms
step:727/1775 train_time:27550ms step_avg:37.90ms
step:728/1775 train_time:27610ms step_avg:37.93ms
step:729/1775 train_time:27668ms step_avg:37.95ms
step:730/1775 train_time:27729ms step_avg:37.98ms
step:731/1775 train_time:27787ms step_avg:38.01ms
step:732/1775 train_time:27849ms step_avg:38.04ms
step:733/1775 train_time:27906ms step_avg:38.07ms
step:734/1775 train_time:27967ms step_avg:38.10ms
step:735/1775 train_time:28027ms step_avg:38.13ms
step:736/1775 train_time:28087ms step_avg:38.16ms
step:737/1775 train_time:28146ms step_avg:38.19ms
step:738/1775 train_time:28206ms step_avg:38.22ms
step:739/1775 train_time:28264ms step_avg:38.25ms
step:740/1775 train_time:28325ms step_avg:38.28ms
step:741/1775 train_time:28384ms step_avg:38.30ms
step:742/1775 train_time:28444ms step_avg:38.33ms
step:743/1775 train_time:28501ms step_avg:38.36ms
step:744/1775 train_time:28562ms step_avg:38.39ms
step:745/1775 train_time:28621ms step_avg:38.42ms
step:746/1775 train_time:28681ms step_avg:38.45ms
step:747/1775 train_time:28739ms step_avg:38.47ms
step:748/1775 train_time:28800ms step_avg:38.50ms
step:749/1775 train_time:28858ms step_avg:38.53ms
step:750/1775 train_time:28919ms step_avg:38.56ms
step:750/1775 val_loss:3.9857 train_time:28990ms step_avg:38.65ms
step:751/1775 train_time:29016ms step_avg:38.64ms
step:752/1775 train_time:29039ms step_avg:38.62ms
step:753/1775 train_time:29098ms step_avg:38.64ms
step:754/1775 train_time:29164ms step_avg:38.68ms
step:755/1775 train_time:29222ms step_avg:38.70ms
step:756/1775 train_time:29282ms step_avg:38.73ms
step:757/1775 train_time:29339ms step_avg:38.76ms
step:758/1775 train_time:29399ms step_avg:38.79ms
step:759/1775 train_time:29457ms step_avg:38.81ms
step:760/1775 train_time:29516ms step_avg:38.84ms
step:761/1775 train_time:29573ms step_avg:38.86ms
step:762/1775 train_time:29634ms step_avg:38.89ms
step:763/1775 train_time:29691ms step_avg:38.91ms
step:764/1775 train_time:29750ms step_avg:38.94ms
step:765/1775 train_time:29807ms step_avg:38.96ms
step:766/1775 train_time:29866ms step_avg:38.99ms
step:767/1775 train_time:29924ms step_avg:39.01ms
step:768/1775 train_time:29986ms step_avg:39.04ms
step:769/1775 train_time:30045ms step_avg:39.07ms
step:770/1775 train_time:30107ms step_avg:39.10ms
step:771/1775 train_time:30166ms step_avg:39.13ms
step:772/1775 train_time:30227ms step_avg:39.15ms
step:773/1775 train_time:30286ms step_avg:39.18ms
step:774/1775 train_time:30345ms step_avg:39.21ms
step:775/1775 train_time:30403ms step_avg:39.23ms
step:776/1775 train_time:30464ms step_avg:39.26ms
step:777/1775 train_time:30522ms step_avg:39.28ms
step:778/1775 train_time:30584ms step_avg:39.31ms
step:779/1775 train_time:30641ms step_avg:39.33ms
step:780/1775 train_time:30701ms step_avg:39.36ms
step:781/1775 train_time:30759ms step_avg:39.38ms
step:782/1775 train_time:30820ms step_avg:39.41ms
step:783/1775 train_time:30877ms step_avg:39.43ms
step:784/1775 train_time:30937ms step_avg:39.46ms
step:785/1775 train_time:30997ms step_avg:39.49ms
step:786/1775 train_time:31057ms step_avg:39.51ms
step:787/1775 train_time:31116ms step_avg:39.54ms
step:788/1775 train_time:31177ms step_avg:39.56ms
step:789/1775 train_time:31236ms step_avg:39.59ms
step:790/1775 train_time:31296ms step_avg:39.62ms
step:791/1775 train_time:31354ms step_avg:39.64ms
step:792/1775 train_time:31415ms step_avg:39.67ms
step:793/1775 train_time:31473ms step_avg:39.69ms
step:794/1775 train_time:31533ms step_avg:39.71ms
step:795/1775 train_time:31591ms step_avg:39.74ms
step:796/1775 train_time:31651ms step_avg:39.76ms
step:797/1775 train_time:31709ms step_avg:39.78ms
step:798/1775 train_time:31769ms step_avg:39.81ms
step:799/1775 train_time:31826ms step_avg:39.83ms
step:800/1775 train_time:31886ms step_avg:39.86ms
step:801/1775 train_time:31944ms step_avg:39.88ms
step:802/1775 train_time:32004ms step_avg:39.91ms
step:803/1775 train_time:32063ms step_avg:39.93ms
step:804/1775 train_time:32124ms step_avg:39.96ms
step:805/1775 train_time:32184ms step_avg:39.98ms
step:806/1775 train_time:32244ms step_avg:40.01ms
step:807/1775 train_time:32303ms step_avg:40.03ms
step:808/1775 train_time:32364ms step_avg:40.05ms
step:809/1775 train_time:32422ms step_avg:40.08ms
step:810/1775 train_time:32483ms step_avg:40.10ms
step:811/1775 train_time:32541ms step_avg:40.12ms
step:812/1775 train_time:32601ms step_avg:40.15ms
step:813/1775 train_time:32659ms step_avg:40.17ms
step:814/1775 train_time:32718ms step_avg:40.19ms
step:815/1775 train_time:32777ms step_avg:40.22ms
step:816/1775 train_time:32836ms step_avg:40.24ms
step:817/1775 train_time:32894ms step_avg:40.26ms
step:818/1775 train_time:32954ms step_avg:40.29ms
step:819/1775 train_time:33012ms step_avg:40.31ms
step:820/1775 train_time:33073ms step_avg:40.33ms
step:821/1775 train_time:33131ms step_avg:40.35ms
step:822/1775 train_time:33193ms step_avg:40.38ms
step:823/1775 train_time:33251ms step_avg:40.40ms
step:824/1775 train_time:33313ms step_avg:40.43ms
step:825/1775 train_time:33370ms step_avg:40.45ms
step:826/1775 train_time:33432ms step_avg:40.47ms
step:827/1775 train_time:33490ms step_avg:40.50ms
step:828/1775 train_time:33549ms step_avg:40.52ms
step:829/1775 train_time:33607ms step_avg:40.54ms
step:830/1775 train_time:33667ms step_avg:40.56ms
step:831/1775 train_time:33725ms step_avg:40.58ms
step:832/1775 train_time:33785ms step_avg:40.61ms
step:833/1775 train_time:33843ms step_avg:40.63ms
step:834/1775 train_time:33904ms step_avg:40.65ms
step:835/1775 train_time:33962ms step_avg:40.67ms
step:836/1775 train_time:34023ms step_avg:40.70ms
step:837/1775 train_time:34082ms step_avg:40.72ms
step:838/1775 train_time:34142ms step_avg:40.74ms
step:839/1775 train_time:34199ms step_avg:40.76ms
step:840/1775 train_time:34261ms step_avg:40.79ms
step:841/1775 train_time:34319ms step_avg:40.81ms
step:842/1775 train_time:34380ms step_avg:40.83ms
step:843/1775 train_time:34438ms step_avg:40.85ms
step:844/1775 train_time:34498ms step_avg:40.87ms
step:845/1775 train_time:34557ms step_avg:40.90ms
step:846/1775 train_time:34617ms step_avg:40.92ms
step:847/1775 train_time:34675ms step_avg:40.94ms
step:848/1775 train_time:34736ms step_avg:40.96ms
step:849/1775 train_time:34793ms step_avg:40.98ms
step:850/1775 train_time:34853ms step_avg:41.00ms
step:851/1775 train_time:34912ms step_avg:41.02ms
step:852/1775 train_time:34973ms step_avg:41.05ms
step:853/1775 train_time:35030ms step_avg:41.07ms
step:854/1775 train_time:35092ms step_avg:41.09ms
step:855/1775 train_time:35148ms step_avg:41.11ms
step:856/1775 train_time:35209ms step_avg:41.13ms
step:857/1775 train_time:35267ms step_avg:41.15ms
step:858/1775 train_time:35327ms step_avg:41.17ms
step:859/1775 train_time:35385ms step_avg:41.19ms
step:860/1775 train_time:35446ms step_avg:41.22ms
step:861/1775 train_time:35505ms step_avg:41.24ms
step:862/1775 train_time:35564ms step_avg:41.26ms
step:863/1775 train_time:35623ms step_avg:41.28ms
step:864/1775 train_time:35685ms step_avg:41.30ms
step:865/1775 train_time:35743ms step_avg:41.32ms
step:866/1775 train_time:35803ms step_avg:41.34ms
step:867/1775 train_time:35861ms step_avg:41.36ms
step:868/1775 train_time:35922ms step_avg:41.38ms
step:869/1775 train_time:35981ms step_avg:41.40ms
step:870/1775 train_time:36040ms step_avg:41.43ms
step:871/1775 train_time:36099ms step_avg:41.45ms
step:872/1775 train_time:36160ms step_avg:41.47ms
step:873/1775 train_time:36218ms step_avg:41.49ms
step:874/1775 train_time:36278ms step_avg:41.51ms
step:875/1775 train_time:36336ms step_avg:41.53ms
step:876/1775 train_time:36396ms step_avg:41.55ms
step:877/1775 train_time:36455ms step_avg:41.57ms
step:878/1775 train_time:36514ms step_avg:41.59ms
step:879/1775 train_time:36572ms step_avg:41.61ms
step:880/1775 train_time:36633ms step_avg:41.63ms
step:881/1775 train_time:36691ms step_avg:41.65ms
step:882/1775 train_time:36752ms step_avg:41.67ms
step:883/1775 train_time:36809ms step_avg:41.69ms
step:884/1775 train_time:36870ms step_avg:41.71ms
step:885/1775 train_time:36928ms step_avg:41.73ms
step:886/1775 train_time:36988ms step_avg:41.75ms
step:887/1775 train_time:37046ms step_avg:41.77ms
step:888/1775 train_time:37106ms step_avg:41.79ms
step:889/1775 train_time:37164ms step_avg:41.80ms
step:890/1775 train_time:37226ms step_avg:41.83ms
step:891/1775 train_time:37284ms step_avg:41.85ms
step:892/1775 train_time:37345ms step_avg:41.87ms
step:893/1775 train_time:37404ms step_avg:41.89ms
step:894/1775 train_time:37465ms step_avg:41.91ms
step:895/1775 train_time:37522ms step_avg:41.92ms
step:896/1775 train_time:37584ms step_avg:41.95ms
step:897/1775 train_time:37641ms step_avg:41.96ms
step:898/1775 train_time:37702ms step_avg:41.98ms
step:899/1775 train_time:37760ms step_avg:42.00ms
step:900/1775 train_time:37820ms step_avg:42.02ms
step:901/1775 train_time:37878ms step_avg:42.04ms
step:902/1775 train_time:37938ms step_avg:42.06ms
step:903/1775 train_time:37996ms step_avg:42.08ms
step:904/1775 train_time:38057ms step_avg:42.10ms
step:905/1775 train_time:38116ms step_avg:42.12ms
step:906/1775 train_time:38176ms step_avg:42.14ms
step:907/1775 train_time:38235ms step_avg:42.16ms
step:908/1775 train_time:38295ms step_avg:42.17ms
step:909/1775 train_time:38353ms step_avg:42.19ms
step:910/1775 train_time:38414ms step_avg:42.21ms
step:911/1775 train_time:38472ms step_avg:42.23ms
step:912/1775 train_time:38533ms step_avg:42.25ms
step:913/1775 train_time:38591ms step_avg:42.27ms
step:914/1775 train_time:38651ms step_avg:42.29ms
step:915/1775 train_time:38709ms step_avg:42.30ms
step:916/1775 train_time:38769ms step_avg:42.32ms
step:917/1775 train_time:38826ms step_avg:42.34ms
step:918/1775 train_time:38887ms step_avg:42.36ms
step:919/1775 train_time:38944ms step_avg:42.38ms
step:920/1775 train_time:39004ms step_avg:42.40ms
step:921/1775 train_time:39062ms step_avg:42.41ms
step:922/1775 train_time:39124ms step_avg:42.43ms
step:923/1775 train_time:39182ms step_avg:42.45ms
step:924/1775 train_time:39243ms step_avg:42.47ms
step:925/1775 train_time:39301ms step_avg:42.49ms
step:926/1775 train_time:39361ms step_avg:42.51ms
step:927/1775 train_time:39419ms step_avg:42.52ms
step:928/1775 train_time:39480ms step_avg:42.54ms
step:929/1775 train_time:39539ms step_avg:42.56ms
step:930/1775 train_time:39599ms step_avg:42.58ms
step:931/1775 train_time:39657ms step_avg:42.60ms
step:932/1775 train_time:39717ms step_avg:42.62ms
step:933/1775 train_time:39774ms step_avg:42.63ms
step:934/1775 train_time:39835ms step_avg:42.65ms
step:935/1775 train_time:39893ms step_avg:42.67ms
step:936/1775 train_time:39953ms step_avg:42.69ms
step:937/1775 train_time:40011ms step_avg:42.70ms
step:938/1775 train_time:40072ms step_avg:42.72ms
step:939/1775 train_time:40130ms step_avg:42.74ms
step:940/1775 train_time:40191ms step_avg:42.76ms
step:941/1775 train_time:40249ms step_avg:42.77ms
step:942/1775 train_time:40309ms step_avg:42.79ms
step:943/1775 train_time:40367ms step_avg:42.81ms
step:944/1775 train_time:40427ms step_avg:42.83ms
step:945/1775 train_time:40485ms step_avg:42.84ms
step:946/1775 train_time:40545ms step_avg:42.86ms
step:947/1775 train_time:40604ms step_avg:42.88ms
step:948/1775 train_time:40665ms step_avg:42.90ms
step:949/1775 train_time:40723ms step_avg:42.91ms
step:950/1775 train_time:40785ms step_avg:42.93ms
step:951/1775 train_time:40842ms step_avg:42.95ms
step:952/1775 train_time:40902ms step_avg:42.96ms
step:953/1775 train_time:40960ms step_avg:42.98ms
step:954/1775 train_time:41020ms step_avg:43.00ms
step:955/1775 train_time:41079ms step_avg:43.02ms
step:956/1775 train_time:41140ms step_avg:43.03ms
step:957/1775 train_time:41198ms step_avg:43.05ms
step:958/1775 train_time:41258ms step_avg:43.07ms
step:959/1775 train_time:41316ms step_avg:43.08ms
step:960/1775 train_time:41377ms step_avg:43.10ms
step:961/1775 train_time:41435ms step_avg:43.12ms
step:962/1775 train_time:41495ms step_avg:43.13ms
step:963/1775 train_time:41554ms step_avg:43.15ms
step:964/1775 train_time:41614ms step_avg:43.17ms
step:965/1775 train_time:41672ms step_avg:43.18ms
step:966/1775 train_time:41732ms step_avg:43.20ms
step:967/1775 train_time:41790ms step_avg:43.22ms
step:968/1775 train_time:41850ms step_avg:43.23ms
step:969/1775 train_time:41908ms step_avg:43.25ms
step:970/1775 train_time:41968ms step_avg:43.27ms
step:971/1775 train_time:42026ms step_avg:43.28ms
step:972/1775 train_time:42087ms step_avg:43.30ms
step:973/1775 train_time:42144ms step_avg:43.31ms
step:974/1775 train_time:42204ms step_avg:43.33ms
step:975/1775 train_time:42262ms step_avg:43.35ms
step:976/1775 train_time:42323ms step_avg:43.36ms
step:977/1775 train_time:42382ms step_avg:43.38ms
step:978/1775 train_time:42443ms step_avg:43.40ms
step:979/1775 train_time:42502ms step_avg:43.41ms
step:980/1775 train_time:42563ms step_avg:43.43ms
step:981/1775 train_time:42621ms step_avg:43.45ms
step:982/1775 train_time:42682ms step_avg:43.46ms
step:983/1775 train_time:42740ms step_avg:43.48ms
step:984/1775 train_time:42800ms step_avg:43.50ms
step:985/1775 train_time:42859ms step_avg:43.51ms
step:986/1775 train_time:42919ms step_avg:43.53ms
step:987/1775 train_time:42977ms step_avg:43.54ms
step:988/1775 train_time:43037ms step_avg:43.56ms
step:989/1775 train_time:43095ms step_avg:43.57ms
step:990/1775 train_time:43155ms step_avg:43.59ms
step:991/1775 train_time:43215ms step_avg:43.61ms
step:992/1775 train_time:43275ms step_avg:43.62ms
step:993/1775 train_time:43333ms step_avg:43.64ms
step:994/1775 train_time:43394ms step_avg:43.66ms
step:995/1775 train_time:43452ms step_avg:43.67ms
step:996/1775 train_time:43513ms step_avg:43.69ms
step:997/1775 train_time:43571ms step_avg:43.70ms
step:998/1775 train_time:43631ms step_avg:43.72ms
step:999/1775 train_time:43689ms step_avg:43.73ms
step:1000/1775 train_time:43748ms step_avg:43.75ms
step:1000/1775 val_loss:3.7351 train_time:43819ms step_avg:43.82ms
step:1001/1775 train_time:43846ms step_avg:43.80ms
step:1002/1775 train_time:43870ms step_avg:43.78ms
step:1003/1775 train_time:43926ms step_avg:43.80ms
step:1004/1775 train_time:43988ms step_avg:43.81ms
step:1005/1775 train_time:44047ms step_avg:43.83ms
step:1006/1775 train_time:44107ms step_avg:43.84ms
step:1007/1775 train_time:44165ms step_avg:43.86ms
step:1008/1775 train_time:44224ms step_avg:43.87ms
step:1009/1775 train_time:44282ms step_avg:43.89ms
step:1010/1775 train_time:44341ms step_avg:43.90ms
step:1011/1775 train_time:44398ms step_avg:43.92ms
step:1012/1775 train_time:44457ms step_avg:43.93ms
step:1013/1775 train_time:44515ms step_avg:43.94ms
step:1014/1775 train_time:44575ms step_avg:43.96ms
step:1015/1775 train_time:44633ms step_avg:43.97ms
step:1016/1775 train_time:44692ms step_avg:43.99ms
step:1017/1775 train_time:44750ms step_avg:44.00ms
step:1018/1775 train_time:44812ms step_avg:44.02ms
step:1019/1775 train_time:44870ms step_avg:44.03ms
step:1020/1775 train_time:44933ms step_avg:44.05ms
step:1021/1775 train_time:44991ms step_avg:44.07ms
step:1022/1775 train_time:45051ms step_avg:44.08ms
step:1023/1775 train_time:45110ms step_avg:44.10ms
step:1024/1775 train_time:45171ms step_avg:44.11ms
step:1025/1775 train_time:45229ms step_avg:44.13ms
step:1026/1775 train_time:45289ms step_avg:44.14ms
step:1027/1775 train_time:45347ms step_avg:44.16ms
step:1028/1775 train_time:45407ms step_avg:44.17ms
step:1029/1775 train_time:45464ms step_avg:44.18ms
step:1030/1775 train_time:45524ms step_avg:44.20ms
step:1031/1775 train_time:45582ms step_avg:44.21ms
step:1032/1775 train_time:45641ms step_avg:44.23ms
step:1033/1775 train_time:45699ms step_avg:44.24ms
step:1034/1775 train_time:45760ms step_avg:44.25ms
step:1035/1775 train_time:45821ms step_avg:44.27ms
step:1036/1775 train_time:45881ms step_avg:44.29ms
step:1037/1775 train_time:45940ms step_avg:44.30ms
step:1038/1775 train_time:46002ms step_avg:44.32ms
step:1039/1775 train_time:46061ms step_avg:44.33ms
step:1040/1775 train_time:46123ms step_avg:44.35ms
step:1041/1775 train_time:46181ms step_avg:44.36ms
step:1042/1775 train_time:46242ms step_avg:44.38ms
step:1043/1775 train_time:46299ms step_avg:44.39ms
step:1044/1775 train_time:46360ms step_avg:44.41ms
step:1045/1775 train_time:46417ms step_avg:44.42ms
step:1046/1775 train_time:46477ms step_avg:44.43ms
step:1047/1775 train_time:46535ms step_avg:44.45ms
step:1048/1775 train_time:46596ms step_avg:44.46ms
step:1049/1775 train_time:46652ms step_avg:44.47ms
step:1050/1775 train_time:46712ms step_avg:44.49ms
step:1051/1775 train_time:46770ms step_avg:44.50ms
step:1052/1775 train_time:46831ms step_avg:44.52ms
step:1053/1775 train_time:46888ms step_avg:44.53ms
step:1054/1775 train_time:46950ms step_avg:44.54ms
step:1055/1775 train_time:47009ms step_avg:44.56ms
step:1056/1775 train_time:47071ms step_avg:44.57ms
step:1057/1775 train_time:47129ms step_avg:44.59ms
step:1058/1775 train_time:47190ms step_avg:44.60ms
step:1059/1775 train_time:47248ms step_avg:44.62ms
step:1060/1775 train_time:47309ms step_avg:44.63ms
step:1061/1775 train_time:47367ms step_avg:44.64ms
step:1062/1775 train_time:47427ms step_avg:44.66ms
step:1063/1775 train_time:47484ms step_avg:44.67ms
step:1064/1775 train_time:47544ms step_avg:44.68ms
step:1065/1775 train_time:47602ms step_avg:44.70ms
step:1066/1775 train_time:47662ms step_avg:44.71ms
step:1067/1775 train_time:47721ms step_avg:44.72ms
step:1068/1775 train_time:47781ms step_avg:44.74ms
step:1069/1775 train_time:47839ms step_avg:44.75ms
step:1070/1775 train_time:47900ms step_avg:44.77ms
step:1071/1775 train_time:47959ms step_avg:44.78ms
step:1072/1775 train_time:48022ms step_avg:44.80ms
step:1073/1775 train_time:48080ms step_avg:44.81ms
step:1074/1775 train_time:48140ms step_avg:44.82ms
step:1075/1775 train_time:48198ms step_avg:44.84ms
step:1076/1775 train_time:48260ms step_avg:44.85ms
step:1077/1775 train_time:48317ms step_avg:44.86ms
step:1078/1775 train_time:48378ms step_avg:44.88ms
step:1079/1775 train_time:48435ms step_avg:44.89ms
step:1080/1775 train_time:48495ms step_avg:44.90ms
step:1081/1775 train_time:48553ms step_avg:44.91ms
step:1082/1775 train_time:48612ms step_avg:44.93ms
step:1083/1775 train_time:48671ms step_avg:44.94ms
step:1084/1775 train_time:48731ms step_avg:44.95ms
step:1085/1775 train_time:48789ms step_avg:44.97ms
step:1086/1775 train_time:48849ms step_avg:44.98ms
step:1087/1775 train_time:48908ms step_avg:44.99ms
step:1088/1775 train_time:48968ms step_avg:45.01ms
step:1089/1775 train_time:49027ms step_avg:45.02ms
step:1090/1775 train_time:49087ms step_avg:45.03ms
step:1091/1775 train_time:49144ms step_avg:45.05ms
step:1092/1775 train_time:49204ms step_avg:45.06ms
step:1093/1775 train_time:49263ms step_avg:45.07ms
step:1094/1775 train_time:49324ms step_avg:45.09ms
step:1095/1775 train_time:49382ms step_avg:45.10ms
step:1096/1775 train_time:49442ms step_avg:45.11ms
step:1097/1775 train_time:49499ms step_avg:45.12ms
step:1098/1775 train_time:49560ms step_avg:45.14ms
step:1099/1775 train_time:49619ms step_avg:45.15ms
step:1100/1775 train_time:49681ms step_avg:45.16ms
step:1101/1775 train_time:49738ms step_avg:45.18ms
step:1102/1775 train_time:49798ms step_avg:45.19ms
step:1103/1775 train_time:49856ms step_avg:45.20ms
step:1104/1775 train_time:49918ms step_avg:45.22ms
step:1105/1775 train_time:49976ms step_avg:45.23ms
step:1106/1775 train_time:50037ms step_avg:45.24ms
step:1107/1775 train_time:50096ms step_avg:45.25ms
step:1108/1775 train_time:50156ms step_avg:45.27ms
step:1109/1775 train_time:50215ms step_avg:45.28ms
step:1110/1775 train_time:50275ms step_avg:45.29ms
step:1111/1775 train_time:50333ms step_avg:45.30ms
step:1112/1775 train_time:50394ms step_avg:45.32ms
step:1113/1775 train_time:50452ms step_avg:45.33ms
step:1114/1775 train_time:50513ms step_avg:45.34ms
step:1115/1775 train_time:50570ms step_avg:45.35ms
step:1116/1775 train_time:50631ms step_avg:45.37ms
step:1117/1775 train_time:50688ms step_avg:45.38ms
step:1118/1775 train_time:50749ms step_avg:45.39ms
step:1119/1775 train_time:50806ms step_avg:45.40ms
step:1120/1775 train_time:50867ms step_avg:45.42ms
step:1121/1775 train_time:50925ms step_avg:45.43ms
step:1122/1775 train_time:50984ms step_avg:45.44ms
step:1123/1775 train_time:51043ms step_avg:45.45ms
step:1124/1775 train_time:51103ms step_avg:45.47ms
step:1125/1775 train_time:51162ms step_avg:45.48ms
step:1126/1775 train_time:51222ms step_avg:45.49ms
step:1127/1775 train_time:51279ms step_avg:45.50ms
step:1128/1775 train_time:51340ms step_avg:45.51ms
step:1129/1775 train_time:51398ms step_avg:45.53ms
step:1130/1775 train_time:51458ms step_avg:45.54ms
step:1131/1775 train_time:51516ms step_avg:45.55ms
step:1132/1775 train_time:51576ms step_avg:45.56ms
step:1133/1775 train_time:51635ms step_avg:45.57ms
step:1134/1775 train_time:51696ms step_avg:45.59ms
step:1135/1775 train_time:51755ms step_avg:45.60ms
step:1136/1775 train_time:51815ms step_avg:45.61ms
step:1137/1775 train_time:51872ms step_avg:45.62ms
step:1138/1775 train_time:51934ms step_avg:45.64ms
step:1139/1775 train_time:51992ms step_avg:45.65ms
step:1140/1775 train_time:52053ms step_avg:45.66ms
step:1141/1775 train_time:52110ms step_avg:45.67ms
step:1142/1775 train_time:52171ms step_avg:45.68ms
step:1143/1775 train_time:52229ms step_avg:45.69ms
step:1144/1775 train_time:52290ms step_avg:45.71ms
step:1145/1775 train_time:52348ms step_avg:45.72ms
step:1146/1775 train_time:52409ms step_avg:45.73ms
step:1147/1775 train_time:52466ms step_avg:45.74ms
step:1148/1775 train_time:52527ms step_avg:45.76ms
step:1149/1775 train_time:52584ms step_avg:45.76ms
step:1150/1775 train_time:52644ms step_avg:45.78ms
step:1151/1775 train_time:52702ms step_avg:45.79ms
step:1152/1775 train_time:52763ms step_avg:45.80ms
step:1153/1775 train_time:52821ms step_avg:45.81ms
step:1154/1775 train_time:52882ms step_avg:45.83ms
step:1155/1775 train_time:52940ms step_avg:45.84ms
step:1156/1775 train_time:53002ms step_avg:45.85ms
step:1157/1775 train_time:53059ms step_avg:45.86ms
step:1158/1775 train_time:53124ms step_avg:45.88ms
step:1159/1775 train_time:53207ms step_avg:45.91ms
step:1160/1775 train_time:53293ms step_avg:45.94ms
step:1161/1775 train_time:53376ms step_avg:45.97ms
step:1162/1775 train_time:53462ms step_avg:46.01ms
step:1163/1775 train_time:53547ms step_avg:46.04ms
step:1164/1775 train_time:53633ms step_avg:46.08ms
step:1165/1775 train_time:53716ms step_avg:46.11ms
step:1166/1775 train_time:53803ms step_avg:46.14ms
step:1167/1775 train_time:53887ms step_avg:46.18ms
step:1168/1775 train_time:53973ms step_avg:46.21ms
step:1169/1775 train_time:54057ms step_avg:46.24ms
step:1170/1775 train_time:54145ms step_avg:46.28ms
step:1171/1775 train_time:54228ms step_avg:46.31ms
step:1172/1775 train_time:54313ms step_avg:46.34ms
step:1173/1775 train_time:54397ms step_avg:46.37ms
step:1174/1775 train_time:54483ms step_avg:46.41ms
step:1175/1775 train_time:54567ms step_avg:46.44ms
step:1176/1775 train_time:54654ms step_avg:46.47ms
step:1177/1775 train_time:54737ms step_avg:46.51ms
step:1178/1775 train_time:54824ms step_avg:46.54ms
step:1179/1775 train_time:54909ms step_avg:46.57ms
step:1180/1775 train_time:54995ms step_avg:46.61ms
step:1181/1775 train_time:55079ms step_avg:46.64ms
step:1182/1775 train_time:55164ms step_avg:46.67ms
step:1183/1775 train_time:55249ms step_avg:46.70ms
step:1184/1775 train_time:55334ms step_avg:46.74ms
step:1185/1775 train_time:55418ms step_avg:46.77ms
step:1186/1775 train_time:55506ms step_avg:46.80ms
step:1187/1775 train_time:55589ms step_avg:46.83ms
step:1188/1775 train_time:55675ms step_avg:46.86ms
step:1189/1775 train_time:55759ms step_avg:46.90ms
step:1190/1775 train_time:55845ms step_avg:46.93ms
step:1191/1775 train_time:55928ms step_avg:46.96ms
step:1192/1775 train_time:56016ms step_avg:46.99ms
step:1193/1775 train_time:56100ms step_avg:47.02ms
step:1194/1775 train_time:56187ms step_avg:47.06ms
step:1195/1775 train_time:56270ms step_avg:47.09ms
step:1196/1775 train_time:56355ms step_avg:47.12ms
step:1197/1775 train_time:56439ms step_avg:47.15ms
step:1198/1775 train_time:56525ms step_avg:47.18ms
step:1199/1775 train_time:56609ms step_avg:47.21ms
step:1200/1775 train_time:56695ms step_avg:47.25ms
step:1201/1775 train_time:56779ms step_avg:47.28ms
step:1202/1775 train_time:56865ms step_avg:47.31ms
step:1203/1775 train_time:56949ms step_avg:47.34ms
step:1204/1775 train_time:57034ms step_avg:47.37ms
step:1205/1775 train_time:57118ms step_avg:47.40ms
step:1206/1775 train_time:57205ms step_avg:47.43ms
step:1207/1775 train_time:57290ms step_avg:47.46ms
step:1208/1775 train_time:57375ms step_avg:47.50ms
step:1209/1775 train_time:57458ms step_avg:47.53ms
step:1210/1775 train_time:57545ms step_avg:47.56ms
step:1211/1775 train_time:57629ms step_avg:47.59ms
step:1212/1775 train_time:57715ms step_avg:47.62ms
step:1213/1775 train_time:57799ms step_avg:47.65ms
step:1214/1775 train_time:57885ms step_avg:47.68ms
step:1215/1775 train_time:57970ms step_avg:47.71ms
step:1216/1775 train_time:58056ms step_avg:47.74ms
step:1217/1775 train_time:58140ms step_avg:47.77ms
step:1218/1775 train_time:58227ms step_avg:47.81ms
step:1219/1775 train_time:58311ms step_avg:47.83ms
step:1220/1775 train_time:58396ms step_avg:47.87ms
step:1221/1775 train_time:58480ms step_avg:47.89ms
step:1222/1775 train_time:58566ms step_avg:47.93ms
step:1223/1775 train_time:58650ms step_avg:47.96ms
step:1224/1775 train_time:58738ms step_avg:47.99ms
step:1225/1775 train_time:58821ms step_avg:48.02ms
step:1226/1775 train_time:58906ms step_avg:48.05ms
step:1227/1775 train_time:58991ms step_avg:48.08ms
step:1228/1775 train_time:59076ms step_avg:48.11ms
step:1229/1775 train_time:59160ms step_avg:48.14ms
step:1230/1775 train_time:59247ms step_avg:48.17ms
step:1231/1775 train_time:59330ms step_avg:48.20ms
step:1232/1775 train_time:59417ms step_avg:48.23ms
step:1233/1775 train_time:59502ms step_avg:48.26ms
step:1234/1775 train_time:59588ms step_avg:48.29ms
step:1235/1775 train_time:59672ms step_avg:48.32ms
step:1236/1775 train_time:59758ms step_avg:48.35ms
step:1237/1775 train_time:59841ms step_avg:48.38ms
step:1238/1775 train_time:59927ms step_avg:48.41ms
step:1239/1775 train_time:60010ms step_avg:48.43ms
step:1240/1775 train_time:60096ms step_avg:48.46ms
step:1241/1775 train_time:60181ms step_avg:48.49ms
step:1242/1775 train_time:60267ms step_avg:48.52ms
step:1243/1775 train_time:60351ms step_avg:48.55ms
step:1244/1775 train_time:60438ms step_avg:48.58ms
step:1245/1775 train_time:60523ms step_avg:48.61ms
step:1246/1775 train_time:60608ms step_avg:48.64ms
step:1247/1775 train_time:60692ms step_avg:48.67ms
step:1248/1775 train_time:60778ms step_avg:48.70ms
step:1249/1775 train_time:60861ms step_avg:48.73ms
step:1250/1775 train_time:60947ms step_avg:48.76ms
step:1250/1775 val_loss:3.5053 train_time:61047ms step_avg:48.84ms
step:1251/1775 train_time:61073ms step_avg:48.82ms
step:1252/1775 train_time:61119ms step_avg:48.82ms
step:1253/1775 train_time:61208ms step_avg:48.85ms
step:1254/1775 train_time:61295ms step_avg:48.88ms
step:1255/1775 train_time:61378ms step_avg:48.91ms
step:1256/1775 train_time:61464ms step_avg:48.94ms
step:1257/1775 train_time:61546ms step_avg:48.96ms
step:1258/1775 train_time:61632ms step_avg:48.99ms
step:1259/1775 train_time:61714ms step_avg:49.02ms
step:1260/1775 train_time:61798ms step_avg:49.05ms
step:1261/1775 train_time:61883ms step_avg:49.07ms
step:1262/1775 train_time:61970ms step_avg:49.10ms
step:1263/1775 train_time:62055ms step_avg:49.13ms
step:1264/1775 train_time:62145ms step_avg:49.17ms
step:1265/1775 train_time:62232ms step_avg:49.20ms
step:1266/1775 train_time:62319ms step_avg:49.23ms
step:1267/1775 train_time:62402ms step_avg:49.25ms
step:1268/1775 train_time:62488ms step_avg:49.28ms
step:1269/1775 train_time:62571ms step_avg:49.31ms
step:1270/1775 train_time:62655ms step_avg:49.33ms
step:1271/1775 train_time:62739ms step_avg:49.36ms
step:1272/1775 train_time:62824ms step_avg:49.39ms
step:1273/1775 train_time:62907ms step_avg:49.42ms
step:1274/1775 train_time:62994ms step_avg:49.45ms
step:1275/1775 train_time:63078ms step_avg:49.47ms
step:1276/1775 train_time:63167ms step_avg:49.50ms
step:1277/1775 train_time:63253ms step_avg:49.53ms
step:1278/1775 train_time:63339ms step_avg:49.56ms
step:1279/1775 train_time:63422ms step_avg:49.59ms
step:1280/1775 train_time:63507ms step_avg:49.61ms
step:1281/1775 train_time:63591ms step_avg:49.64ms
step:1282/1775 train_time:63676ms step_avg:49.67ms
step:1283/1775 train_time:63758ms step_avg:49.69ms
step:1284/1775 train_time:63844ms step_avg:49.72ms
step:1285/1775 train_time:63926ms step_avg:49.75ms
step:1286/1775 train_time:64015ms step_avg:49.78ms
step:1287/1775 train_time:64100ms step_avg:49.81ms
step:1288/1775 train_time:64186ms step_avg:49.83ms
step:1289/1775 train_time:64271ms step_avg:49.86ms
step:1290/1775 train_time:64356ms step_avg:49.89ms
step:1291/1775 train_time:64440ms step_avg:49.91ms
step:1292/1775 train_time:64526ms step_avg:49.94ms
step:1293/1775 train_time:64611ms step_avg:49.97ms
step:1294/1775 train_time:64696ms step_avg:50.00ms
step:1295/1775 train_time:64780ms step_avg:50.02ms
step:1296/1775 train_time:64865ms step_avg:50.05ms
step:1297/1775 train_time:64948ms step_avg:50.08ms
step:1298/1775 train_time:65035ms step_avg:50.10ms
step:1299/1775 train_time:65119ms step_avg:50.13ms
step:1300/1775 train_time:65208ms step_avg:50.16ms
step:1301/1775 train_time:65292ms step_avg:50.19ms
step:1302/1775 train_time:65380ms step_avg:50.21ms
step:1303/1775 train_time:65462ms step_avg:50.24ms
step:1304/1775 train_time:65548ms step_avg:50.27ms
step:1305/1775 train_time:65631ms step_avg:50.29ms
step:1306/1775 train_time:65717ms step_avg:50.32ms
step:1307/1775 train_time:65800ms step_avg:50.34ms
step:1308/1775 train_time:65886ms step_avg:50.37ms
step:1309/1775 train_time:65971ms step_avg:50.40ms
step:1310/1775 train_time:66056ms step_avg:50.42ms
step:1311/1775 train_time:66141ms step_avg:50.45ms
step:1312/1775 train_time:66228ms step_avg:50.48ms
step:1313/1775 train_time:66314ms step_avg:50.51ms
step:1314/1775 train_time:66399ms step_avg:50.53ms
step:1315/1775 train_time:66482ms step_avg:50.56ms
step:1316/1775 train_time:66568ms step_avg:50.58ms
step:1317/1775 train_time:66651ms step_avg:50.61ms
step:1318/1775 train_time:66737ms step_avg:50.64ms
step:1319/1775 train_time:66821ms step_avg:50.66ms
step:1320/1775 train_time:66907ms step_avg:50.69ms
step:1321/1775 train_time:66991ms step_avg:50.71ms
step:1322/1775 train_time:67078ms step_avg:50.74ms
step:1323/1775 train_time:67161ms step_avg:50.76ms
step:1324/1775 train_time:67249ms step_avg:50.79ms
step:1325/1775 train_time:67333ms step_avg:50.82ms
step:1326/1775 train_time:67419ms step_avg:50.84ms
step:1327/1775 train_time:67503ms step_avg:50.87ms
step:1328/1775 train_time:67590ms step_avg:50.90ms
step:1329/1775 train_time:67674ms step_avg:50.92ms
step:1330/1775 train_time:67759ms step_avg:50.95ms
step:1331/1775 train_time:67843ms step_avg:50.97ms
step:1332/1775 train_time:67930ms step_avg:51.00ms
step:1333/1775 train_time:68014ms step_avg:51.02ms
step:1334/1775 train_time:68100ms step_avg:51.05ms
step:1335/1775 train_time:68184ms step_avg:51.07ms
step:1336/1775 train_time:68272ms step_avg:51.10ms
step:1337/1775 train_time:68355ms step_avg:51.13ms
step:1338/1775 train_time:68442ms step_avg:51.15ms
step:1339/1775 train_time:68526ms step_avg:51.18ms
step:1340/1775 train_time:68614ms step_avg:51.20ms
step:1341/1775 train_time:68697ms step_avg:51.23ms
step:1342/1775 train_time:68782ms step_avg:51.25ms
step:1343/1775 train_time:68866ms step_avg:51.28ms
step:1344/1775 train_time:68952ms step_avg:51.30ms
step:1345/1775 train_time:69036ms step_avg:51.33ms
step:1346/1775 train_time:69123ms step_avg:51.35ms
step:1347/1775 train_time:69207ms step_avg:51.38ms
step:1348/1775 train_time:69294ms step_avg:51.41ms
step:1349/1775 train_time:69378ms step_avg:51.43ms
step:1350/1775 train_time:69464ms step_avg:51.45ms
step:1351/1775 train_time:69548ms step_avg:51.48ms
step:1352/1775 train_time:69634ms step_avg:51.50ms
step:1353/1775 train_time:69717ms step_avg:51.53ms
step:1354/1775 train_time:69805ms step_avg:51.55ms
step:1355/1775 train_time:69888ms step_avg:51.58ms
step:1356/1775 train_time:69974ms step_avg:51.60ms
step:1357/1775 train_time:70058ms step_avg:51.63ms
step:1358/1775 train_time:70144ms step_avg:51.65ms
step:1359/1775 train_time:70227ms step_avg:51.68ms
step:1360/1775 train_time:70315ms step_avg:51.70ms
step:1361/1775 train_time:70399ms step_avg:51.73ms
step:1362/1775 train_time:70484ms step_avg:51.75ms
step:1363/1775 train_time:70568ms step_avg:51.77ms
step:1364/1775 train_time:70653ms step_avg:51.80ms
step:1365/1775 train_time:70737ms step_avg:51.82ms
step:1366/1775 train_time:70823ms step_avg:51.85ms
step:1367/1775 train_time:70908ms step_avg:51.87ms
step:1368/1775 train_time:70993ms step_avg:51.90ms
step:1369/1775 train_time:71077ms step_avg:51.92ms
step:1370/1775 train_time:71163ms step_avg:51.94ms
step:1371/1775 train_time:71247ms step_avg:51.97ms
step:1372/1775 train_time:71334ms step_avg:51.99ms
step:1373/1775 train_time:71416ms step_avg:52.01ms
step:1374/1775 train_time:71503ms step_avg:52.04ms
step:1375/1775 train_time:71586ms step_avg:52.06ms
step:1376/1775 train_time:71673ms step_avg:52.09ms
step:1377/1775 train_time:71756ms step_avg:52.11ms
step:1378/1775 train_time:71843ms step_avg:52.14ms
step:1379/1775 train_time:71925ms step_avg:52.16ms
step:1380/1775 train_time:72012ms step_avg:52.18ms
step:1381/1775 train_time:72096ms step_avg:52.21ms
step:1382/1775 train_time:72183ms step_avg:52.23ms
step:1383/1775 train_time:72267ms step_avg:52.25ms
step:1384/1775 train_time:72353ms step_avg:52.28ms
step:1385/1775 train_time:72437ms step_avg:52.30ms
step:1386/1775 train_time:72524ms step_avg:52.33ms
step:1387/1775 train_time:72608ms step_avg:52.35ms
step:1388/1775 train_time:72694ms step_avg:52.37ms
step:1389/1775 train_time:72778ms step_avg:52.40ms
step:1390/1775 train_time:72864ms step_avg:52.42ms
step:1391/1775 train_time:72947ms step_avg:52.44ms
step:1392/1775 train_time:73034ms step_avg:52.47ms
step:1393/1775 train_time:73117ms step_avg:52.49ms
step:1394/1775 train_time:73203ms step_avg:52.51ms
step:1395/1775 train_time:73287ms step_avg:52.54ms
step:1396/1775 train_time:73375ms step_avg:52.56ms
step:1397/1775 train_time:73457ms step_avg:52.58ms
step:1398/1775 train_time:73544ms step_avg:52.61ms
step:1399/1775 train_time:73626ms step_avg:52.63ms
step:1400/1775 train_time:73715ms step_avg:52.65ms
step:1401/1775 train_time:73798ms step_avg:52.68ms
step:1402/1775 train_time:73885ms step_avg:52.70ms
step:1403/1775 train_time:73968ms step_avg:52.72ms
step:1404/1775 train_time:74054ms step_avg:52.75ms
step:1405/1775 train_time:74138ms step_avg:52.77ms
step:1406/1775 train_time:74225ms step_avg:52.79ms
step:1407/1775 train_time:74309ms step_avg:52.81ms
step:1408/1775 train_time:74395ms step_avg:52.84ms
step:1409/1775 train_time:74479ms step_avg:52.86ms
step:1410/1775 train_time:74565ms step_avg:52.88ms
step:1411/1775 train_time:74648ms step_avg:52.90ms
step:1412/1775 train_time:74734ms step_avg:52.93ms
step:1413/1775 train_time:74817ms step_avg:52.95ms
step:1414/1775 train_time:74904ms step_avg:52.97ms
step:1415/1775 train_time:74987ms step_avg:52.99ms
step:1416/1775 train_time:75074ms step_avg:53.02ms
step:1417/1775 train_time:75156ms step_avg:53.04ms
step:1418/1775 train_time:75244ms step_avg:53.06ms
step:1419/1775 train_time:75328ms step_avg:53.08ms
step:1420/1775 train_time:75416ms step_avg:53.11ms
step:1421/1775 train_time:75499ms step_avg:53.13ms
step:1422/1775 train_time:75585ms step_avg:53.15ms
step:1423/1775 train_time:75670ms step_avg:53.18ms
step:1424/1775 train_time:75755ms step_avg:53.20ms
step:1425/1775 train_time:75838ms step_avg:53.22ms
step:1426/1775 train_time:75924ms step_avg:53.24ms
step:1427/1775 train_time:76008ms step_avg:53.26ms
step:1428/1775 train_time:76094ms step_avg:53.29ms
step:1429/1775 train_time:76177ms step_avg:53.31ms
step:1430/1775 train_time:76263ms step_avg:53.33ms
step:1431/1775 train_time:76346ms step_avg:53.35ms
step:1432/1775 train_time:76433ms step_avg:53.37ms
step:1433/1775 train_time:76517ms step_avg:53.40ms
step:1434/1775 train_time:76603ms step_avg:53.42ms
step:1435/1775 train_time:76687ms step_avg:53.44ms
step:1436/1775 train_time:76774ms step_avg:53.46ms
step:1437/1775 train_time:76857ms step_avg:53.48ms
step:1438/1775 train_time:76943ms step_avg:53.51ms
step:1439/1775 train_time:77026ms step_avg:53.53ms
step:1440/1775 train_time:77112ms step_avg:53.55ms
step:1441/1775 train_time:77194ms step_avg:53.57ms
step:1442/1775 train_time:77282ms step_avg:53.59ms
step:1443/1775 train_time:77365ms step_avg:53.61ms
step:1444/1775 train_time:77451ms step_avg:53.64ms
step:1445/1775 train_time:77535ms step_avg:53.66ms
step:1446/1775 train_time:77622ms step_avg:53.68ms
step:1447/1775 train_time:77705ms step_avg:53.70ms
step:1448/1775 train_time:77792ms step_avg:53.72ms
step:1449/1775 train_time:77875ms step_avg:53.74ms
step:1450/1775 train_time:77961ms step_avg:53.77ms
step:1451/1775 train_time:78045ms step_avg:53.79ms
step:1452/1775 train_time:78131ms step_avg:53.81ms
step:1453/1775 train_time:78214ms step_avg:53.83ms
step:1454/1775 train_time:78301ms step_avg:53.85ms
step:1455/1775 train_time:78384ms step_avg:53.87ms
step:1456/1775 train_time:78471ms step_avg:53.90ms
step:1457/1775 train_time:78555ms step_avg:53.92ms
step:1458/1775 train_time:78641ms step_avg:53.94ms
step:1459/1775 train_time:78725ms step_avg:53.96ms
step:1460/1775 train_time:78811ms step_avg:53.98ms
step:1461/1775 train_time:78895ms step_avg:54.00ms
step:1462/1775 train_time:78981ms step_avg:54.02ms
step:1463/1775 train_time:79065ms step_avg:54.04ms
step:1464/1775 train_time:79151ms step_avg:54.06ms
step:1465/1775 train_time:79236ms step_avg:54.09ms
step:1466/1775 train_time:79322ms step_avg:54.11ms
step:1467/1775 train_time:79405ms step_avg:54.13ms
step:1468/1775 train_time:79491ms step_avg:54.15ms
step:1469/1775 train_time:79575ms step_avg:54.17ms
step:1470/1775 train_time:79661ms step_avg:54.19ms
step:1471/1775 train_time:79745ms step_avg:54.21ms
step:1472/1775 train_time:79832ms step_avg:54.23ms
step:1473/1775 train_time:79916ms step_avg:54.25ms
step:1474/1775 train_time:80002ms step_avg:54.28ms
step:1475/1775 train_time:80085ms step_avg:54.30ms
step:1476/1775 train_time:80171ms step_avg:54.32ms
step:1477/1775 train_time:80254ms step_avg:54.34ms
step:1478/1775 train_time:80341ms step_avg:54.36ms
step:1479/1775 train_time:80424ms step_avg:54.38ms
step:1480/1775 train_time:80511ms step_avg:54.40ms
step:1481/1775 train_time:80595ms step_avg:54.42ms
step:1482/1775 train_time:80681ms step_avg:54.44ms
step:1483/1775 train_time:80765ms step_avg:54.46ms
step:1484/1775 train_time:80851ms step_avg:54.48ms
step:1485/1775 train_time:80935ms step_avg:54.50ms
step:1486/1775 train_time:81021ms step_avg:54.52ms
step:1487/1775 train_time:81104ms step_avg:54.54ms
step:1488/1775 train_time:81192ms step_avg:54.56ms
step:1489/1775 train_time:81274ms step_avg:54.58ms
step:1490/1775 train_time:81360ms step_avg:54.60ms
step:1491/1775 train_time:81444ms step_avg:54.62ms
step:1492/1775 train_time:81531ms step_avg:54.65ms
step:1493/1775 train_time:81615ms step_avg:54.67ms
step:1494/1775 train_time:81702ms step_avg:54.69ms
step:1495/1775 train_time:81785ms step_avg:54.71ms
step:1496/1775 train_time:81872ms step_avg:54.73ms
step:1497/1775 train_time:81955ms step_avg:54.75ms
step:1498/1775 train_time:82042ms step_avg:54.77ms
step:1499/1775 train_time:82125ms step_avg:54.79ms
step:1500/1775 train_time:82212ms step_avg:54.81ms
step:1500/1775 val_loss:3.3767 train_time:82309ms step_avg:54.87ms
step:1501/1775 train_time:82329ms step_avg:54.85ms
step:1502/1775 train_time:82385ms step_avg:54.85ms
step:1503/1775 train_time:82470ms step_avg:54.87ms
step:1504/1775 train_time:82556ms step_avg:54.89ms
step:1505/1775 train_time:82641ms step_avg:54.91ms
step:1506/1775 train_time:82726ms step_avg:54.93ms
step:1507/1775 train_time:82808ms step_avg:54.95ms
step:1508/1775 train_time:82895ms step_avg:54.97ms
step:1509/1775 train_time:82978ms step_avg:54.99ms
step:1510/1775 train_time:83064ms step_avg:55.01ms
step:1511/1775 train_time:83146ms step_avg:55.03ms
step:1512/1775 train_time:83233ms step_avg:55.05ms
step:1513/1775 train_time:83319ms step_avg:55.07ms
step:1514/1775 train_time:83408ms step_avg:55.09ms
step:1515/1775 train_time:83493ms step_avg:55.11ms
step:1516/1775 train_time:83580ms step_avg:55.13ms
step:1517/1775 train_time:83663ms step_avg:55.15ms
step:1518/1775 train_time:83749ms step_avg:55.17ms
step:1519/1775 train_time:83832ms step_avg:55.19ms
step:1520/1775 train_time:83918ms step_avg:55.21ms
step:1521/1775 train_time:84001ms step_avg:55.23ms
step:1522/1775 train_time:84086ms step_avg:55.25ms
step:1523/1775 train_time:84169ms step_avg:55.27ms
step:1524/1775 train_time:84256ms step_avg:55.29ms
step:1525/1775 train_time:84342ms step_avg:55.31ms
step:1526/1775 train_time:84429ms step_avg:55.33ms
step:1527/1775 train_time:84512ms step_avg:55.34ms
step:1528/1775 train_time:84600ms step_avg:55.37ms
step:1529/1775 train_time:84683ms step_avg:55.38ms
step:1530/1775 train_time:84768ms step_avg:55.40ms
step:1531/1775 train_time:84851ms step_avg:55.42ms
step:1532/1775 train_time:84937ms step_avg:55.44ms
step:1533/1775 train_time:85021ms step_avg:55.46ms
step:1534/1775 train_time:85106ms step_avg:55.48ms
step:1535/1775 train_time:85188ms step_avg:55.50ms
step:1536/1775 train_time:85275ms step_avg:55.52ms
step:1537/1775 train_time:85360ms step_avg:55.54ms
step:1538/1775 train_time:85445ms step_avg:55.56ms
step:1539/1775 train_time:85531ms step_avg:55.58ms
step:1540/1775 train_time:85618ms step_avg:55.60ms
step:1541/1775 train_time:85703ms step_avg:55.61ms
step:1542/1775 train_time:85788ms step_avg:55.63ms
step:1543/1775 train_time:85871ms step_avg:55.65ms
step:1544/1775 train_time:85957ms step_avg:55.67ms
step:1545/1775 train_time:86040ms step_avg:55.69ms
step:1546/1775 train_time:86125ms step_avg:55.71ms
step:1547/1775 train_time:86209ms step_avg:55.73ms
step:1548/1775 train_time:86296ms step_avg:55.75ms
step:1549/1775 train_time:86381ms step_avg:55.77ms
step:1550/1775 train_time:86467ms step_avg:55.79ms
step:1551/1775 train_time:86551ms step_avg:55.80ms
step:1552/1775 train_time:86639ms step_avg:55.82ms
step:1553/1775 train_time:86722ms step_avg:55.84ms
step:1554/1775 train_time:86808ms step_avg:55.86ms
step:1555/1775 train_time:86891ms step_avg:55.88ms
step:1556/1775 train_time:86978ms step_avg:55.90ms
step:1557/1775 train_time:87062ms step_avg:55.92ms
step:1558/1775 train_time:87147ms step_avg:55.94ms
step:1559/1775 train_time:87230ms step_avg:55.95ms
step:1560/1775 train_time:87317ms step_avg:55.97ms
step:1561/1775 train_time:87401ms step_avg:55.99ms
step:1562/1775 train_time:87487ms step_avg:56.01ms
step:1563/1775 train_time:87572ms step_avg:56.03ms
step:1564/1775 train_time:87658ms step_avg:56.05ms
step:1565/1775 train_time:87742ms step_avg:56.07ms
step:1566/1775 train_time:87827ms step_avg:56.08ms
step:1567/1775 train_time:87911ms step_avg:56.10ms
step:1568/1775 train_time:87998ms step_avg:56.12ms
step:1569/1775 train_time:88081ms step_avg:56.14ms
step:1570/1775 train_time:88166ms step_avg:56.16ms
step:1571/1775 train_time:88250ms step_avg:56.17ms
step:1572/1775 train_time:88337ms step_avg:56.19ms
step:1573/1775 train_time:88421ms step_avg:56.21ms
step:1574/1775 train_time:88507ms step_avg:56.23ms
step:1575/1775 train_time:88591ms step_avg:56.25ms
step:1576/1775 train_time:88678ms step_avg:56.27ms
step:1577/1775 train_time:88762ms step_avg:56.29ms
step:1578/1775 train_time:88847ms step_avg:56.30ms
step:1579/1775 train_time:88932ms step_avg:56.32ms
step:1580/1775 train_time:89019ms step_avg:56.34ms
step:1581/1775 train_time:89102ms step_avg:56.36ms
step:1582/1775 train_time:89187ms step_avg:56.38ms
step:1583/1775 train_time:89271ms step_avg:56.39ms
step:1584/1775 train_time:89357ms step_avg:56.41ms
step:1585/1775 train_time:89442ms step_avg:56.43ms
step:1586/1775 train_time:89529ms step_avg:56.45ms
step:1587/1775 train_time:89612ms step_avg:56.47ms
step:1588/1775 train_time:89699ms step_avg:56.49ms
step:1589/1775 train_time:89782ms step_avg:56.50ms
step:1590/1775 train_time:89870ms step_avg:56.52ms
step:1591/1775 train_time:89954ms step_avg:56.54ms
step:1592/1775 train_time:90042ms step_avg:56.56ms
step:1593/1775 train_time:90124ms step_avg:56.58ms
step:1594/1775 train_time:90211ms step_avg:56.59ms
step:1595/1775 train_time:90295ms step_avg:56.61ms
step:1596/1775 train_time:90381ms step_avg:56.63ms
step:1597/1775 train_time:90465ms step_avg:56.65ms
step:1598/1775 train_time:90552ms step_avg:56.67ms
step:1599/1775 train_time:90635ms step_avg:56.68ms
step:1600/1775 train_time:90724ms step_avg:56.70ms
step:1601/1775 train_time:90807ms step_avg:56.72ms
step:1602/1775 train_time:90893ms step_avg:56.74ms
step:1603/1775 train_time:90977ms step_avg:56.75ms
step:1604/1775 train_time:91062ms step_avg:56.77ms
step:1605/1775 train_time:91146ms step_avg:56.79ms
step:1606/1775 train_time:91233ms step_avg:56.81ms
step:1607/1775 train_time:91317ms step_avg:56.82ms
step:1608/1775 train_time:91404ms step_avg:56.84ms
step:1609/1775 train_time:91486ms step_avg:56.86ms
step:1610/1775 train_time:91573ms step_avg:56.88ms
step:1611/1775 train_time:91656ms step_avg:56.89ms
step:1612/1775 train_time:91744ms step_avg:56.91ms
step:1613/1775 train_time:91827ms step_avg:56.93ms
step:1614/1775 train_time:91914ms step_avg:56.95ms
step:1615/1775 train_time:91997ms step_avg:56.96ms
step:1616/1775 train_time:92083ms step_avg:56.98ms
step:1617/1775 train_time:92166ms step_avg:57.00ms
step:1618/1775 train_time:92253ms step_avg:57.02ms
step:1619/1775 train_time:92337ms step_avg:57.03ms
step:1620/1775 train_time:92424ms step_avg:57.05ms
step:1621/1775 train_time:92506ms step_avg:57.07ms
step:1622/1775 train_time:92592ms step_avg:57.09ms
step:1623/1775 train_time:92677ms step_avg:57.10ms
step:1624/1775 train_time:92763ms step_avg:57.12ms
step:1625/1775 train_time:92846ms step_avg:57.14ms
step:1626/1775 train_time:92933ms step_avg:57.15ms
step:1627/1775 train_time:93016ms step_avg:57.17ms
step:1628/1775 train_time:93103ms step_avg:57.19ms
step:1629/1775 train_time:93186ms step_avg:57.20ms
step:1630/1775 train_time:93274ms step_avg:57.22ms
step:1631/1775 train_time:93358ms step_avg:57.24ms
step:1632/1775 train_time:93444ms step_avg:57.26ms
step:1633/1775 train_time:93527ms step_avg:57.27ms
step:1634/1775 train_time:93614ms step_avg:57.29ms
step:1635/1775 train_time:93698ms step_avg:57.31ms
step:1636/1775 train_time:93784ms step_avg:57.32ms
step:1637/1775 train_time:93868ms step_avg:57.34ms
step:1638/1775 train_time:93954ms step_avg:57.36ms
step:1639/1775 train_time:94037ms step_avg:57.37ms
step:1640/1775 train_time:94124ms step_avg:57.39ms
step:1641/1775 train_time:94207ms step_avg:57.41ms
step:1642/1775 train_time:94294ms step_avg:57.43ms
step:1643/1775 train_time:94377ms step_avg:57.44ms
step:1644/1775 train_time:94463ms step_avg:57.46ms
step:1645/1775 train_time:94546ms step_avg:57.47ms
step:1646/1775 train_time:94633ms step_avg:57.49ms
step:1647/1775 train_time:94717ms step_avg:57.51ms
step:1648/1775 train_time:94805ms step_avg:57.53ms
step:1649/1775 train_time:94887ms step_avg:57.54ms
step:1650/1775 train_time:94973ms step_avg:57.56ms
step:1651/1775 train_time:95057ms step_avg:57.58ms
step:1652/1775 train_time:95144ms step_avg:57.59ms
step:1653/1775 train_time:95228ms step_avg:57.61ms
step:1654/1775 train_time:95314ms step_avg:57.63ms
step:1655/1775 train_time:95399ms step_avg:57.64ms
step:1656/1775 train_time:95484ms step_avg:57.66ms
step:1657/1775 train_time:95568ms step_avg:57.68ms
step:1658/1775 train_time:95653ms step_avg:57.69ms
step:1659/1775 train_time:95738ms step_avg:57.71ms
step:1660/1775 train_time:95824ms step_avg:57.73ms
step:1661/1775 train_time:95907ms step_avg:57.74ms
step:1662/1775 train_time:95994ms step_avg:57.76ms
step:1663/1775 train_time:96078ms step_avg:57.77ms
step:1664/1775 train_time:96164ms step_avg:57.79ms
step:1665/1775 train_time:96247ms step_avg:57.81ms
step:1666/1775 train_time:96334ms step_avg:57.82ms
step:1667/1775 train_time:96418ms step_avg:57.84ms
step:1668/1775 train_time:96505ms step_avg:57.86ms
step:1669/1775 train_time:96588ms step_avg:57.87ms
step:1670/1775 train_time:96674ms step_avg:57.89ms
step:1671/1775 train_time:96759ms step_avg:57.90ms
step:1672/1775 train_time:96844ms step_avg:57.92ms
step:1673/1775 train_time:96928ms step_avg:57.94ms
step:1674/1775 train_time:97015ms step_avg:57.95ms
step:1675/1775 train_time:97099ms step_avg:57.97ms
step:1676/1775 train_time:97184ms step_avg:57.99ms
step:1677/1775 train_time:97268ms step_avg:58.00ms
step:1678/1775 train_time:97354ms step_avg:58.02ms
step:1679/1775 train_time:97439ms step_avg:58.03ms
step:1680/1775 train_time:97526ms step_avg:58.05ms
step:1681/1775 train_time:97609ms step_avg:58.07ms
step:1682/1775 train_time:97696ms step_avg:58.08ms
step:1683/1775 train_time:97779ms step_avg:58.10ms
step:1684/1775 train_time:97865ms step_avg:58.11ms
step:1685/1775 train_time:97949ms step_avg:58.13ms
step:1686/1775 train_time:98035ms step_avg:58.15ms
step:1687/1775 train_time:98119ms step_avg:58.16ms
step:1688/1775 train_time:98205ms step_avg:58.18ms
step:1689/1775 train_time:98288ms step_avg:58.19ms
step:1690/1775 train_time:98375ms step_avg:58.21ms
step:1691/1775 train_time:98458ms step_avg:58.22ms
step:1692/1775 train_time:98544ms step_avg:58.24ms
step:1693/1775 train_time:98628ms step_avg:58.26ms
step:1694/1775 train_time:98714ms step_avg:58.27ms
step:1695/1775 train_time:98800ms step_avg:58.29ms
step:1696/1775 train_time:98884ms step_avg:58.30ms
step:1697/1775 train_time:98967ms step_avg:58.32ms
step:1698/1775 train_time:99053ms step_avg:58.34ms
step:1699/1775 train_time:99139ms step_avg:58.35ms
step:1700/1775 train_time:99224ms step_avg:58.37ms
step:1701/1775 train_time:99308ms step_avg:58.38ms
step:1702/1775 train_time:99395ms step_avg:58.40ms
step:1703/1775 train_time:99479ms step_avg:58.41ms
step:1704/1775 train_time:99564ms step_avg:58.43ms
step:1705/1775 train_time:99649ms step_avg:58.44ms
step:1706/1775 train_time:99736ms step_avg:58.46ms
step:1707/1775 train_time:99820ms step_avg:58.48ms
step:1708/1775 train_time:99905ms step_avg:58.49ms
step:1709/1775 train_time:99988ms step_avg:58.51ms
step:1710/1775 train_time:100075ms step_avg:58.52ms
step:1711/1775 train_time:100160ms step_avg:58.54ms
step:1712/1775 train_time:100246ms step_avg:58.55ms
step:1713/1775 train_time:100329ms step_avg:58.57ms
step:1714/1775 train_time:100415ms step_avg:58.59ms
step:1715/1775 train_time:100500ms step_avg:58.60ms
step:1716/1775 train_time:100585ms step_avg:58.62ms
step:1717/1775 train_time:100669ms step_avg:58.63ms
step:1718/1775 train_time:100755ms step_avg:58.65ms
step:1719/1775 train_time:100839ms step_avg:58.66ms
step:1720/1775 train_time:100925ms step_avg:58.68ms
step:1721/1775 train_time:101009ms step_avg:58.69ms
step:1722/1775 train_time:101095ms step_avg:58.71ms
step:1723/1775 train_time:101179ms step_avg:58.72ms
step:1724/1775 train_time:101265ms step_avg:58.74ms
step:1725/1775 train_time:101349ms step_avg:58.75ms
step:1726/1775 train_time:101437ms step_avg:58.77ms
step:1727/1775 train_time:101521ms step_avg:58.78ms
step:1728/1775 train_time:101607ms step_avg:58.80ms
step:1729/1775 train_time:101691ms step_avg:58.81ms
step:1730/1775 train_time:101777ms step_avg:58.83ms
step:1731/1775 train_time:101861ms step_avg:58.85ms
step:1732/1775 train_time:101947ms step_avg:58.86ms
step:1733/1775 train_time:102031ms step_avg:58.88ms
step:1734/1775 train_time:102117ms step_avg:58.89ms
step:1735/1775 train_time:102202ms step_avg:58.91ms
step:1736/1775 train_time:102291ms step_avg:58.92ms
step:1737/1775 train_time:102375ms step_avg:58.94ms
step:1738/1775 train_time:102462ms step_avg:58.95ms
step:1739/1775 train_time:102547ms step_avg:58.97ms
step:1740/1775 train_time:102634ms step_avg:58.99ms
step:1741/1775 train_time:102718ms step_avg:59.00ms
step:1742/1775 train_time:102805ms step_avg:59.02ms
step:1743/1775 train_time:102888ms step_avg:59.03ms
step:1744/1775 train_time:102975ms step_avg:59.05ms
step:1745/1775 train_time:103059ms step_avg:59.06ms
step:1746/1775 train_time:103145ms step_avg:59.07ms
step:1747/1775 train_time:103230ms step_avg:59.09ms
step:1748/1775 train_time:103316ms step_avg:59.11ms
step:1749/1775 train_time:103402ms step_avg:59.12ms
step:1750/1775 train_time:103488ms step_avg:59.14ms
step:1750/1775 val_loss:3.2853 train_time:103586ms step_avg:59.19ms
step:1751/1775 train_time:103606ms step_avg:59.17ms
step:1752/1775 train_time:103660ms step_avg:59.17ms
step:1753/1775 train_time:103749ms step_avg:59.18ms
step:1754/1775 train_time:103836ms step_avg:59.20ms
step:1755/1775 train_time:103919ms step_avg:59.21ms
step:1756/1775 train_time:104004ms step_avg:59.23ms
step:1757/1775 train_time:104087ms step_avg:59.24ms
step:1758/1775 train_time:104172ms step_avg:59.26ms
step:1759/1775 train_time:104256ms step_avg:59.27ms
step:1760/1775 train_time:104342ms step_avg:59.29ms
step:1761/1775 train_time:104425ms step_avg:59.30ms
step:1762/1775 train_time:104512ms step_avg:59.31ms
step:1763/1775 train_time:104597ms step_avg:59.33ms
step:1764/1775 train_time:104687ms step_avg:59.35ms
step:1765/1775 train_time:104774ms step_avg:59.36ms
step:1766/1775 train_time:104861ms step_avg:59.38ms
step:1767/1775 train_time:104945ms step_avg:59.39ms
step:1768/1775 train_time:105030ms step_avg:59.41ms
step:1769/1775 train_time:105112ms step_avg:59.42ms
step:1770/1775 train_time:105199ms step_avg:59.43ms
step:1771/1775 train_time:105283ms step_avg:59.45ms
step:1772/1775 train_time:105368ms step_avg:59.46ms
step:1773/1775 train_time:105451ms step_avg:59.48ms
step:1774/1775 train_time:105539ms step_avg:59.49ms
step:1775/1775 train_time:105626ms step_avg:59.51ms
step:1775/1775 val_loss:3.2790 train_time:105726ms step_avg:59.56ms
peak memory allocated: 29148 MiB reserved: 44838 MiB

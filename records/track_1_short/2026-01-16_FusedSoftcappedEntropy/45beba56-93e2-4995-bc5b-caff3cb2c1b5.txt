import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977

        # This call computes relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# Fused Softcapped Cross Entropy

@triton.jit
def fused_softcapped_entropy_fwd_kernel(
    logits_ptr, losses_ptr, lse_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)
    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    
    max_val = -float('inf')
    sum_exp = 0.0
    
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)
        z = A * tl.sigmoid((val + B) / C)
        z = tl.where(mask, z, -float('inf'))
        curr_max = tl.max(z, axis=0)
        new_max = tl.maximum(max_val, curr_max)
        sum_exp = sum_exp * tl.exp(max_val - new_max) + tl.sum(tl.exp(z - new_max), axis=0)
        max_val = new_max
    
    lse = max_val + tl.log(sum_exp)
    tl.store(lse_ptr + row_idx, lse)
    
    total_loss = 0.0
    for k in range(n_predict):
        target_idx = row_idx + k
        if target_idx < n_rows:
            weight = tl.load(mtp_weights_ptr + k)
            if weight > 0:
                target = tl.load(targets_ptr + target_idx).to(tl.int32)
                if target >= 0 and target < n_cols:
                    val_target = tl.load(logits_row_ptr + target).to(tl.float32)
                    z_target = A * tl.sigmoid((val_target + B) / C)
                    total_loss += weight * (lse - z_target)
    
    tl.store(losses_ptr + row_idx, total_loss)

@triton.jit
def fused_softcapped_entropy_bwd_kernel(
    grad_input_ptr, grad_output_ptr, lse_ptr, logits_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v, stride_grad_n, stride_grad_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)

    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    grad_row_ptr = grad_input_ptr + row_idx * stride_grad_n
    
    lse = tl.load(lse_ptr + row_idx)
    grad_loss = tl.load(grad_output_ptr + row_idx)
    
    S_w = 0.0
    for k in range(n_predict):
        if row_idx + k < n_rows:
            S_w += tl.load(mtp_weights_ptr + k)
            
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=0.0).to(tl.float32)
        u = (val + B) / C
        sigmoid_u = tl.sigmoid(u)
        z = A * sigmoid_u
        p = tl.exp(z - lse)
        
        term1 = S_w * p
        term2 = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
        for k in range(n_predict):
            if row_idx + k < n_rows:
                target = tl.load(targets_ptr + row_idx + k).to(tl.int32)
                weight = tl.load(mtp_weights_ptr + k)
                term2 += tl.where(cols == target, weight, 0.0)
        
        grad_z = grad_loss * (term1 - term2)
        dz_dx = (1.0 / C) * z * (1.0 - sigmoid_u)
        grad_x = grad_z * dz_dx
        tl.store(grad_row_ptr + cols, grad_x.to(tl.bfloat16), mask=mask)

class FusedSoftcappedCrossEntropy(torch.autograd.Function):
    @staticmethod
    def forward(ctx, logits, targets, mtp_weights, A=23.0, B=5.0, C=7.5):
        n_rows, n_cols = logits.shape
        if mtp_weights is None:
             mtp_weights = torch.tensor([1.0], device=logits.device, dtype=torch.float32)
        n_predict = mtp_weights.shape[0]

        losses = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        lse = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        
        logits = logits.contiguous()
        targets = targets.contiguous()
        mtp_weights = mtp_weights.contiguous()

        grid = (n_rows,)
        fused_softcapped_entropy_fwd_kernel[grid](
            logits, losses, lse, targets, mtp_weights,
            logits.stride(0), logits.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        
        ctx.save_for_backward(logits, targets, mtp_weights, lse)
        ctx.params = (A, B, C)
        return losses

    @staticmethod
    def backward(ctx, grad_output):
        logits, targets, mtp_weights, lse = ctx.saved_tensors
        A, B, C = ctx.params
        n_rows, n_cols = logits.shape
        n_predict = mtp_weights.shape[0]
        
        grad_input = torch.empty((n_rows, n_cols), dtype=torch.bfloat16, device=logits.device)
        grad_output = grad_output.contiguous()
        
        grid = (n_rows,)
        fused_softcapped_entropy_bwd_kernel[grid](
            grad_input, grad_output, lse, logits, targets, mtp_weights,
            logits.stride(0), logits.stride(1), grad_input.stride(0), grad_input.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        return grad_input, None, None, None, None, None

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if self.training:
            losses = FusedSoftcappedCrossEntropy.apply(logits.view(-1, logits.size(-1)), target_seq, mtp_weights)
            loss = losses.sum()
        else:
            logits = 23 * torch.sigmoid((logits + 5) / 7.5)
            logits_for_loss = logits.float()
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Jan  8 2026, 06:52:19) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan 15 22:00:51 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   43C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   34C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   43C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   41C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   40C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    245749      C   /usr/bin/python3                             1510MiB |
|    1   N/A  N/A    245750      C   /usr/bin/python3                             1510MiB |
|    2   N/A  N/A    245751      C   /usr/bin/python3                             1510MiB |
|    3   N/A  N/A    245752      C   /usr/bin/python3                             1510MiB |
|    4   N/A  N/A    245753      C   /usr/bin/python3                             1510MiB |
|    5   N/A  N/A    245754      C   /usr/bin/python3                             1510MiB |
|    6   N/A  N/A    245755      C   /usr/bin/python3                             1510MiB |
|    7   N/A  N/A    245756      C   /usr/bin/python3                             1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8309 train_time:0ms step_avg:0.03ms
step:1/1775 train_time:77ms step_avg:76.76ms
step:2/1775 train_time:99ms step_avg:49.41ms
step:3/1775 train_time:118ms step_avg:39.39ms
step:4/1775 train_time:142ms step_avg:35.50ms
step:5/1775 train_time:173ms step_avg:34.56ms
step:6/1775 train_time:257ms step_avg:42.81ms
step:7/1775 train_time:275ms step_avg:39.27ms
step:8/1775 train_time:294ms step_avg:36.80ms
step:9/1775 train_time:325ms step_avg:36.14ms
step:10/1775 train_time:358ms step_avg:35.83ms
step:11/1775 train_time:389ms step_avg:35.39ms
step:12/1775 train_time:423ms step_avg:35.22ms
step:13/1775 train_time:454ms step_avg:34.91ms
step:14/1775 train_time:487ms step_avg:34.79ms
step:15/1775 train_time:519ms step_avg:34.58ms
step:16/1775 train_time:552ms step_avg:34.49ms
step:17/1775 train_time:583ms step_avg:34.28ms
step:18/1775 train_time:616ms step_avg:34.22ms
step:19/1775 train_time:647ms step_avg:34.06ms
step:20/1775 train_time:681ms step_avg:34.03ms
step:21/1775 train_time:712ms step_avg:33.88ms
step:22/1775 train_time:745ms step_avg:33.85ms
step:23/1775 train_time:776ms step_avg:33.74ms
step:24/1775 train_time:809ms step_avg:33.73ms
step:25/1775 train_time:841ms step_avg:33.62ms
step:26/1775 train_time:874ms step_avg:33.61ms
step:27/1775 train_time:905ms step_avg:33.51ms
step:28/1775 train_time:938ms step_avg:33.51ms
step:29/1775 train_time:969ms step_avg:33.42ms
step:30/1775 train_time:1003ms step_avg:33.44ms
step:31/1775 train_time:1035ms step_avg:33.39ms
step:32/1775 train_time:1069ms step_avg:33.41ms
step:33/1775 train_time:1100ms step_avg:33.35ms
step:34/1775 train_time:1135ms step_avg:33.38ms
step:35/1775 train_time:1168ms step_avg:33.38ms
step:36/1775 train_time:1203ms step_avg:33.40ms
step:37/1775 train_time:1234ms step_avg:33.36ms
step:38/1775 train_time:1268ms step_avg:33.37ms
step:39/1775 train_time:1300ms step_avg:33.33ms
step:40/1775 train_time:1334ms step_avg:33.36ms
step:41/1775 train_time:1366ms step_avg:33.31ms
step:42/1775 train_time:1399ms step_avg:33.32ms
step:43/1775 train_time:1430ms step_avg:33.27ms
step:44/1775 train_time:1464ms step_avg:33.28ms
step:45/1775 train_time:1496ms step_avg:33.25ms
step:46/1775 train_time:1530ms step_avg:33.25ms
step:47/1775 train_time:1561ms step_avg:33.20ms
step:48/1775 train_time:1595ms step_avg:33.22ms
step:49/1775 train_time:1626ms step_avg:33.18ms
step:50/1775 train_time:1659ms step_avg:33.18ms
step:51/1775 train_time:1690ms step_avg:33.14ms
step:52/1775 train_time:1724ms step_avg:33.15ms
step:53/1775 train_time:1755ms step_avg:33.11ms
step:54/1775 train_time:1788ms step_avg:33.11ms
step:55/1775 train_time:1819ms step_avg:33.08ms
step:56/1775 train_time:1853ms step_avg:33.09ms
step:57/1775 train_time:1884ms step_avg:33.06ms
step:58/1775 train_time:1918ms step_avg:33.06ms
step:59/1775 train_time:1949ms step_avg:33.04ms
step:60/1775 train_time:1983ms step_avg:33.04ms
step:61/1775 train_time:2014ms step_avg:33.01ms
step:62/1775 train_time:2047ms step_avg:33.02ms
step:63/1775 train_time:2079ms step_avg:33.00ms
step:64/1775 train_time:2113ms step_avg:33.02ms
step:65/1775 train_time:2145ms step_avg:33.00ms
step:66/1775 train_time:2179ms step_avg:33.02ms
step:67/1775 train_time:2210ms step_avg:32.99ms
step:68/1775 train_time:2244ms step_avg:33.00ms
step:69/1775 train_time:2277ms step_avg:32.99ms
step:70/1775 train_time:2310ms step_avg:33.00ms
step:71/1775 train_time:2342ms step_avg:32.98ms
step:72/1775 train_time:2376ms step_avg:33.00ms
step:73/1775 train_time:2407ms step_avg:32.98ms
step:74/1775 train_time:2441ms step_avg:32.98ms
step:75/1775 train_time:2472ms step_avg:32.96ms
step:76/1775 train_time:2506ms step_avg:32.97ms
step:77/1775 train_time:2537ms step_avg:32.95ms
step:78/1775 train_time:2571ms step_avg:32.96ms
step:79/1775 train_time:2602ms step_avg:32.94ms
step:80/1775 train_time:2636ms step_avg:32.95ms
step:81/1775 train_time:2667ms step_avg:32.93ms
step:82/1775 train_time:2701ms step_avg:32.93ms
step:83/1775 train_time:2732ms step_avg:32.91ms
step:84/1775 train_time:2765ms step_avg:32.92ms
step:85/1775 train_time:2797ms step_avg:32.90ms
step:86/1775 train_time:2830ms step_avg:32.91ms
step:87/1775 train_time:2861ms step_avg:32.89ms
step:88/1775 train_time:2894ms step_avg:32.89ms
step:89/1775 train_time:2926ms step_avg:32.88ms
step:90/1775 train_time:2959ms step_avg:32.88ms
step:91/1775 train_time:2991ms step_avg:32.86ms
step:92/1775 train_time:3024ms step_avg:32.87ms
step:93/1775 train_time:3056ms step_avg:32.86ms
step:94/1775 train_time:3090ms step_avg:32.87ms
step:95/1775 train_time:3121ms step_avg:32.86ms
step:96/1775 train_time:3155ms step_avg:32.87ms
step:97/1775 train_time:3187ms step_avg:32.85ms
step:98/1775 train_time:3221ms step_avg:32.87ms
step:99/1775 train_time:3252ms step_avg:32.85ms
step:100/1775 train_time:3285ms step_avg:32.85ms
step:101/1775 train_time:3317ms step_avg:32.84ms
step:102/1775 train_time:3351ms step_avg:32.85ms
step:103/1775 train_time:3383ms step_avg:32.84ms
step:104/1775 train_time:3417ms step_avg:32.85ms
step:105/1775 train_time:3448ms step_avg:32.84ms
step:106/1775 train_time:3482ms step_avg:32.85ms
step:107/1775 train_time:3513ms step_avg:32.83ms
step:108/1775 train_time:3547ms step_avg:32.84ms
step:109/1775 train_time:3578ms step_avg:32.83ms
step:110/1775 train_time:3612ms step_avg:32.84ms
step:111/1775 train_time:3643ms step_avg:32.82ms
step:112/1775 train_time:3677ms step_avg:32.83ms
step:113/1775 train_time:3709ms step_avg:32.82ms
step:114/1775 train_time:3742ms step_avg:32.83ms
step:115/1775 train_time:3773ms step_avg:32.81ms
step:116/1775 train_time:3807ms step_avg:32.81ms
step:117/1775 train_time:3838ms step_avg:32.80ms
step:118/1775 train_time:3871ms step_avg:32.81ms
step:119/1775 train_time:3903ms step_avg:32.80ms
step:120/1775 train_time:3936ms step_avg:32.80ms
step:121/1775 train_time:3968ms step_avg:32.79ms
step:122/1775 train_time:4001ms step_avg:32.80ms
step:123/1775 train_time:4033ms step_avg:32.78ms
step:124/1775 train_time:4066ms step_avg:32.79ms
step:125/1775 train_time:4098ms step_avg:32.78ms
step:126/1775 train_time:4131ms step_avg:32.79ms
step:127/1775 train_time:4162ms step_avg:32.78ms
step:128/1775 train_time:4197ms step_avg:32.79ms
step:129/1775 train_time:4228ms step_avg:32.78ms
step:130/1775 train_time:4262ms step_avg:32.78ms
step:131/1775 train_time:4294ms step_avg:32.78ms
step:132/1775 train_time:4327ms step_avg:32.78ms
step:133/1775 train_time:4359ms step_avg:32.77ms
step:134/1775 train_time:4393ms step_avg:32.78ms
step:135/1775 train_time:4424ms step_avg:32.77ms
step:136/1775 train_time:4458ms step_avg:32.78ms
step:137/1775 train_time:4490ms step_avg:32.77ms
step:138/1775 train_time:4523ms step_avg:32.78ms
step:139/1775 train_time:4555ms step_avg:32.77ms
step:140/1775 train_time:4589ms step_avg:32.78ms
step:141/1775 train_time:4620ms step_avg:32.77ms
step:142/1775 train_time:4653ms step_avg:32.77ms
step:143/1775 train_time:4684ms step_avg:32.76ms
step:144/1775 train_time:4718ms step_avg:32.76ms
step:145/1775 train_time:4749ms step_avg:32.75ms
step:146/1775 train_time:4783ms step_avg:32.76ms
step:147/1775 train_time:4814ms step_avg:32.75ms
step:148/1775 train_time:4847ms step_avg:32.75ms
step:149/1775 train_time:4879ms step_avg:32.74ms
step:150/1775 train_time:4913ms step_avg:32.75ms
step:151/1775 train_time:4944ms step_avg:32.74ms
step:152/1775 train_time:4978ms step_avg:32.75ms
step:153/1775 train_time:5009ms step_avg:32.74ms
step:154/1775 train_time:5043ms step_avg:32.74ms
step:155/1775 train_time:5074ms step_avg:32.73ms
step:156/1775 train_time:5107ms step_avg:32.74ms
step:157/1775 train_time:5139ms step_avg:32.73ms
step:158/1775 train_time:5172ms step_avg:32.73ms
step:159/1775 train_time:5203ms step_avg:32.73ms
step:160/1775 train_time:5237ms step_avg:32.73ms
step:161/1775 train_time:5269ms step_avg:32.72ms
step:162/1775 train_time:5302ms step_avg:32.73ms
step:163/1775 train_time:5334ms step_avg:32.73ms
step:164/1775 train_time:5368ms step_avg:32.73ms
step:165/1775 train_time:5399ms step_avg:32.72ms
step:166/1775 train_time:5433ms step_avg:32.73ms
step:167/1775 train_time:5465ms step_avg:32.72ms
step:168/1775 train_time:5498ms step_avg:32.73ms
step:169/1775 train_time:5530ms step_avg:32.72ms
step:170/1775 train_time:5563ms step_avg:32.73ms
step:171/1775 train_time:5595ms step_avg:32.72ms
step:172/1775 train_time:5629ms step_avg:32.72ms
step:173/1775 train_time:5660ms step_avg:32.72ms
step:174/1775 train_time:5693ms step_avg:32.72ms
step:175/1775 train_time:5725ms step_avg:32.71ms
step:176/1775 train_time:5759ms step_avg:32.72ms
step:177/1775 train_time:5790ms step_avg:32.71ms
step:178/1775 train_time:5824ms step_avg:32.72ms
step:179/1775 train_time:5855ms step_avg:32.71ms
step:180/1775 train_time:5888ms step_avg:32.71ms
step:181/1775 train_time:5920ms step_avg:32.71ms
step:182/1775 train_time:5953ms step_avg:32.71ms
step:183/1775 train_time:5985ms step_avg:32.70ms
step:184/1775 train_time:6018ms step_avg:32.71ms
step:185/1775 train_time:6050ms step_avg:32.70ms
step:186/1775 train_time:6083ms step_avg:32.71ms
step:187/1775 train_time:6114ms step_avg:32.70ms
step:188/1775 train_time:6148ms step_avg:32.70ms
step:189/1775 train_time:6179ms step_avg:32.69ms
step:190/1775 train_time:6213ms step_avg:32.70ms
step:191/1775 train_time:6244ms step_avg:32.69ms
step:192/1775 train_time:6278ms step_avg:32.70ms
step:193/1775 train_time:6309ms step_avg:32.69ms
step:194/1775 train_time:6342ms step_avg:32.69ms
step:195/1775 train_time:6374ms step_avg:32.69ms
step:196/1775 train_time:6408ms step_avg:32.69ms
step:197/1775 train_time:6439ms step_avg:32.69ms
step:198/1775 train_time:6473ms step_avg:32.69ms
step:199/1775 train_time:6504ms step_avg:32.68ms
step:200/1775 train_time:6538ms step_avg:32.69ms
step:201/1775 train_time:6569ms step_avg:32.68ms
step:202/1775 train_time:6602ms step_avg:32.68ms
step:203/1775 train_time:6634ms step_avg:32.68ms
step:204/1775 train_time:6667ms step_avg:32.68ms
step:205/1775 train_time:6698ms step_avg:32.68ms
step:206/1775 train_time:6732ms step_avg:32.68ms
step:207/1775 train_time:6763ms step_avg:32.67ms
step:208/1775 train_time:6797ms step_avg:32.68ms
step:209/1775 train_time:6828ms step_avg:32.67ms
step:210/1775 train_time:6862ms step_avg:32.67ms
step:211/1775 train_time:6893ms step_avg:32.67ms
step:212/1775 train_time:6927ms step_avg:32.68ms
step:213/1775 train_time:6958ms step_avg:32.67ms
step:214/1775 train_time:6992ms step_avg:32.67ms
step:215/1775 train_time:7023ms step_avg:32.67ms
step:216/1775 train_time:7056ms step_avg:32.67ms
step:217/1775 train_time:7088ms step_avg:32.66ms
step:218/1775 train_time:7121ms step_avg:32.67ms
step:219/1775 train_time:7152ms step_avg:32.66ms
step:220/1775 train_time:7186ms step_avg:32.66ms
step:221/1775 train_time:7217ms step_avg:32.66ms
step:222/1775 train_time:7250ms step_avg:32.66ms
step:223/1775 train_time:7282ms step_avg:32.65ms
step:224/1775 train_time:7316ms step_avg:32.66ms
step:225/1775 train_time:7347ms step_avg:32.65ms
step:226/1775 train_time:7380ms step_avg:32.66ms
step:227/1775 train_time:7412ms step_avg:32.65ms
step:228/1775 train_time:7446ms step_avg:32.66ms
step:229/1775 train_time:7477ms step_avg:32.65ms
step:230/1775 train_time:7511ms step_avg:32.66ms
step:231/1775 train_time:7542ms step_avg:32.65ms
step:232/1775 train_time:7575ms step_avg:32.65ms
step:233/1775 train_time:7607ms step_avg:32.65ms
step:234/1775 train_time:7640ms step_avg:32.65ms
step:235/1775 train_time:7672ms step_avg:32.65ms
step:236/1775 train_time:7705ms step_avg:32.65ms
step:237/1775 train_time:7737ms step_avg:32.65ms
step:238/1775 train_time:7771ms step_avg:32.65ms
step:239/1775 train_time:7803ms step_avg:32.65ms
step:240/1775 train_time:7836ms step_avg:32.65ms
step:241/1775 train_time:7868ms step_avg:32.65ms
step:242/1775 train_time:7901ms step_avg:32.65ms
step:243/1775 train_time:7932ms step_avg:32.64ms
step:244/1775 train_time:7966ms step_avg:32.65ms
step:245/1775 train_time:7997ms step_avg:32.64ms
step:246/1775 train_time:8031ms step_avg:32.65ms
step:247/1775 train_time:8062ms step_avg:32.64ms
step:248/1775 train_time:8096ms step_avg:32.64ms
step:249/1775 train_time:8127ms step_avg:32.64ms
step:250/1775 train_time:8160ms step_avg:32.64ms
step:250/1775 val_loss:4.6062 train_time:8201ms step_avg:32.80ms
step:251/1775 train_time:8221ms step_avg:32.75ms
step:252/1775 train_time:8241ms step_avg:32.70ms
step:253/1775 train_time:8258ms step_avg:32.64ms
step:254/1775 train_time:8293ms step_avg:32.65ms
step:255/1775 train_time:8326ms step_avg:32.65ms
step:256/1775 train_time:8361ms step_avg:32.66ms
step:257/1775 train_time:8393ms step_avg:32.66ms
step:258/1775 train_time:8428ms step_avg:32.67ms
step:259/1775 train_time:8459ms step_avg:32.66ms
step:260/1775 train_time:8493ms step_avg:32.67ms
step:261/1775 train_time:8524ms step_avg:32.66ms
step:262/1775 train_time:8557ms step_avg:32.66ms
step:263/1775 train_time:8589ms step_avg:32.66ms
step:264/1775 train_time:8622ms step_avg:32.66ms
step:265/1775 train_time:8654ms step_avg:32.66ms
step:266/1775 train_time:8687ms step_avg:32.66ms
step:267/1775 train_time:8718ms step_avg:32.65ms
step:268/1775 train_time:8751ms step_avg:32.65ms
step:269/1775 train_time:8782ms step_avg:32.65ms
step:270/1775 train_time:8815ms step_avg:32.65ms
step:271/1775 train_time:8846ms step_avg:32.64ms
step:272/1775 train_time:8879ms step_avg:32.64ms
step:273/1775 train_time:8910ms step_avg:32.64ms
step:274/1775 train_time:8943ms step_avg:32.64ms
step:275/1775 train_time:8974ms step_avg:32.63ms
step:276/1775 train_time:9007ms step_avg:32.63ms
step:277/1775 train_time:9038ms step_avg:32.63ms
step:278/1775 train_time:9071ms step_avg:32.63ms
step:279/1775 train_time:9102ms step_avg:32.62ms
step:280/1775 train_time:9136ms step_avg:32.63ms
step:281/1775 train_time:9167ms step_avg:32.62ms
step:282/1775 train_time:9202ms step_avg:32.63ms
step:283/1775 train_time:9233ms step_avg:32.63ms
step:284/1775 train_time:9267ms step_avg:32.63ms
step:285/1775 train_time:9299ms step_avg:32.63ms
step:286/1775 train_time:9333ms step_avg:32.63ms
step:287/1775 train_time:9365ms step_avg:32.63ms
step:288/1775 train_time:9399ms step_avg:32.64ms
step:289/1775 train_time:9431ms step_avg:32.63ms
step:290/1775 train_time:9465ms step_avg:32.64ms
step:291/1775 train_time:9496ms step_avg:32.63ms
step:292/1775 train_time:9530ms step_avg:32.64ms
step:293/1775 train_time:9561ms step_avg:32.63ms
step:294/1775 train_time:9595ms step_avg:32.64ms
step:295/1775 train_time:9627ms step_avg:32.63ms
step:296/1775 train_time:9660ms step_avg:32.63ms
step:297/1775 train_time:9691ms step_avg:32.63ms
step:298/1775 train_time:9724ms step_avg:32.63ms
step:299/1775 train_time:9756ms step_avg:32.63ms
step:300/1775 train_time:9789ms step_avg:32.63ms
step:301/1775 train_time:9820ms step_avg:32.63ms
step:302/1775 train_time:9854ms step_avg:32.63ms
step:303/1775 train_time:9885ms step_avg:32.62ms
step:304/1775 train_time:9918ms step_avg:32.62ms
step:305/1775 train_time:9949ms step_avg:32.62ms
step:306/1775 train_time:9982ms step_avg:32.62ms
step:307/1775 train_time:10013ms step_avg:32.62ms
step:308/1775 train_time:10046ms step_avg:32.62ms
step:309/1775 train_time:10077ms step_avg:32.61ms
step:310/1775 train_time:10111ms step_avg:32.62ms
step:311/1775 train_time:10142ms step_avg:32.61ms
step:312/1775 train_time:10176ms step_avg:32.61ms
step:313/1775 train_time:10207ms step_avg:32.61ms
step:314/1775 train_time:10241ms step_avg:32.61ms
step:315/1775 train_time:10273ms step_avg:32.61ms
step:316/1775 train_time:10307ms step_avg:32.62ms
step:317/1775 train_time:10339ms step_avg:32.61ms
step:318/1775 train_time:10372ms step_avg:32.62ms
step:319/1775 train_time:10404ms step_avg:32.61ms
step:320/1775 train_time:10438ms step_avg:32.62ms
step:321/1775 train_time:10469ms step_avg:32.61ms
step:322/1775 train_time:10503ms step_avg:32.62ms
step:323/1775 train_time:10534ms step_avg:32.61ms
step:324/1775 train_time:10568ms step_avg:32.62ms
step:325/1775 train_time:10599ms step_avg:32.61ms
step:326/1775 train_time:10633ms step_avg:32.62ms
step:327/1775 train_time:10665ms step_avg:32.61ms
step:328/1775 train_time:10698ms step_avg:32.62ms
step:329/1775 train_time:10730ms step_avg:32.61ms
step:330/1775 train_time:10763ms step_avg:32.62ms
step:331/1775 train_time:10794ms step_avg:32.61ms
step:332/1775 train_time:10828ms step_avg:32.61ms
step:333/1775 train_time:10859ms step_avg:32.61ms
step:334/1775 train_time:10892ms step_avg:32.61ms
step:335/1775 train_time:10923ms step_avg:32.61ms
step:336/1775 train_time:10956ms step_avg:32.61ms
step:337/1775 train_time:10987ms step_avg:32.60ms
step:338/1775 train_time:11021ms step_avg:32.61ms
step:339/1775 train_time:11052ms step_avg:32.60ms
step:340/1775 train_time:11086ms step_avg:32.60ms
step:341/1775 train_time:11117ms step_avg:32.60ms
step:342/1775 train_time:11150ms step_avg:32.60ms
step:343/1775 train_time:11182ms step_avg:32.60ms
step:344/1775 train_time:11215ms step_avg:32.60ms
step:345/1775 train_time:11247ms step_avg:32.60ms
step:346/1775 train_time:11280ms step_avg:32.60ms
step:347/1775 train_time:11311ms step_avg:32.60ms
step:348/1775 train_time:11346ms step_avg:32.60ms
step:349/1775 train_time:11377ms step_avg:32.60ms
step:350/1775 train_time:11411ms step_avg:32.60ms
step:351/1775 train_time:11442ms step_avg:32.60ms
step:352/1775 train_time:11476ms step_avg:32.60ms
step:353/1775 train_time:11507ms step_avg:32.60ms
step:354/1775 train_time:11541ms step_avg:32.60ms
step:355/1775 train_time:11572ms step_avg:32.60ms
step:356/1775 train_time:11605ms step_avg:32.60ms
step:357/1775 train_time:11636ms step_avg:32.59ms
step:358/1775 train_time:11670ms step_avg:32.60ms
step:359/1775 train_time:11701ms step_avg:32.59ms
step:360/1775 train_time:11735ms step_avg:32.60ms
step:361/1775 train_time:11767ms step_avg:32.59ms
step:362/1775 train_time:11800ms step_avg:32.60ms
step:363/1775 train_time:11831ms step_avg:32.59ms
step:364/1775 train_time:11865ms step_avg:32.60ms
step:365/1775 train_time:11896ms step_avg:32.59ms
step:366/1775 train_time:11929ms step_avg:32.59ms
step:367/1775 train_time:11961ms step_avg:32.59ms
step:368/1775 train_time:11994ms step_avg:32.59ms
step:369/1775 train_time:12026ms step_avg:32.59ms
step:370/1775 train_time:12059ms step_avg:32.59ms
step:371/1775 train_time:12090ms step_avg:32.59ms
step:372/1775 train_time:12124ms step_avg:32.59ms
step:373/1775 train_time:12155ms step_avg:32.59ms
step:374/1775 train_time:12189ms step_avg:32.59ms
step:375/1775 train_time:12220ms step_avg:32.59ms
step:376/1775 train_time:12253ms step_avg:32.59ms
step:377/1775 train_time:12284ms step_avg:32.58ms
step:378/1775 train_time:12318ms step_avg:32.59ms
step:379/1775 train_time:12349ms step_avg:32.58ms
step:380/1775 train_time:12383ms step_avg:32.59ms
step:381/1775 train_time:12414ms step_avg:32.58ms
step:382/1775 train_time:12448ms step_avg:32.59ms
step:383/1775 train_time:12480ms step_avg:32.58ms
step:384/1775 train_time:12513ms step_avg:32.59ms
step:385/1775 train_time:12544ms step_avg:32.58ms
step:386/1775 train_time:12577ms step_avg:32.58ms
step:387/1775 train_time:12609ms step_avg:32.58ms
step:388/1775 train_time:12643ms step_avg:32.58ms
step:389/1775 train_time:12673ms step_avg:32.58ms
step:390/1775 train_time:12707ms step_avg:32.58ms
step:391/1775 train_time:12738ms step_avg:32.58ms
step:392/1775 train_time:12771ms step_avg:32.58ms
step:393/1775 train_time:12803ms step_avg:32.58ms
step:394/1775 train_time:12836ms step_avg:32.58ms
step:395/1775 train_time:12868ms step_avg:32.58ms
step:396/1775 train_time:12902ms step_avg:32.58ms
step:397/1775 train_time:12933ms step_avg:32.58ms
step:398/1775 train_time:12966ms step_avg:32.58ms
step:399/1775 train_time:12998ms step_avg:32.58ms
step:400/1775 train_time:13031ms step_avg:32.58ms
step:401/1775 train_time:13063ms step_avg:32.58ms
step:402/1775 train_time:13096ms step_avg:32.58ms
step:403/1775 train_time:13127ms step_avg:32.57ms
step:404/1775 train_time:13161ms step_avg:32.58ms
step:405/1775 train_time:13192ms step_avg:32.57ms
step:406/1775 train_time:13226ms step_avg:32.58ms
step:407/1775 train_time:13257ms step_avg:32.57ms
step:408/1775 train_time:13291ms step_avg:32.57ms
step:409/1775 train_time:13322ms step_avg:32.57ms
step:410/1775 train_time:13356ms step_avg:32.58ms
step:411/1775 train_time:13387ms step_avg:32.57ms
step:412/1775 train_time:13420ms step_avg:32.57ms
step:413/1775 train_time:13452ms step_avg:32.57ms
step:414/1775 train_time:13486ms step_avg:32.57ms
step:415/1775 train_time:13517ms step_avg:32.57ms
step:416/1775 train_time:13551ms step_avg:32.58ms
step:417/1775 train_time:13582ms step_avg:32.57ms
step:418/1775 train_time:13616ms step_avg:32.57ms
step:419/1775 train_time:13647ms step_avg:32.57ms
step:420/1775 train_time:13681ms step_avg:32.57ms
step:421/1775 train_time:13712ms step_avg:32.57ms
step:422/1775 train_time:13745ms step_avg:32.57ms
step:423/1775 train_time:13777ms step_avg:32.57ms
step:424/1775 train_time:13810ms step_avg:32.57ms
step:425/1775 train_time:13841ms step_avg:32.57ms
step:426/1775 train_time:13875ms step_avg:32.57ms
step:427/1775 train_time:13907ms step_avg:32.57ms
step:428/1775 train_time:13940ms step_avg:32.57ms
step:429/1775 train_time:13971ms step_avg:32.57ms
step:430/1775 train_time:14005ms step_avg:32.57ms
step:431/1775 train_time:14036ms step_avg:32.57ms
step:432/1775 train_time:14070ms step_avg:32.57ms
step:433/1775 train_time:14101ms step_avg:32.57ms
step:434/1775 train_time:14135ms step_avg:32.57ms
step:435/1775 train_time:14166ms step_avg:32.57ms
step:436/1775 train_time:14200ms step_avg:32.57ms
step:437/1775 train_time:14231ms step_avg:32.57ms
step:438/1775 train_time:14264ms step_avg:32.57ms
step:439/1775 train_time:14295ms step_avg:32.56ms
step:440/1775 train_time:14329ms step_avg:32.57ms
step:441/1775 train_time:14360ms step_avg:32.56ms
step:442/1775 train_time:14394ms step_avg:32.57ms
step:443/1775 train_time:14425ms step_avg:32.56ms
step:444/1775 train_time:14459ms step_avg:32.56ms
step:445/1775 train_time:14490ms step_avg:32.56ms
step:446/1775 train_time:14524ms step_avg:32.57ms
step:447/1775 train_time:14556ms step_avg:32.56ms
step:448/1775 train_time:14589ms step_avg:32.56ms
step:449/1775 train_time:14621ms step_avg:32.56ms
step:450/1775 train_time:14654ms step_avg:32.56ms
step:451/1775 train_time:14685ms step_avg:32.56ms
step:452/1775 train_time:14718ms step_avg:32.56ms
step:453/1775 train_time:14750ms step_avg:32.56ms
step:454/1775 train_time:14783ms step_avg:32.56ms
step:455/1775 train_time:14814ms step_avg:32.56ms
step:456/1775 train_time:14848ms step_avg:32.56ms
step:457/1775 train_time:14879ms step_avg:32.56ms
step:458/1775 train_time:14912ms step_avg:32.56ms
step:459/1775 train_time:14944ms step_avg:32.56ms
step:460/1775 train_time:14977ms step_avg:32.56ms
step:461/1775 train_time:15008ms step_avg:32.56ms
step:462/1775 train_time:15042ms step_avg:32.56ms
step:463/1775 train_time:15073ms step_avg:32.56ms
step:464/1775 train_time:15107ms step_avg:32.56ms
step:465/1775 train_time:15138ms step_avg:32.55ms
step:466/1775 train_time:15171ms step_avg:32.56ms
step:467/1775 train_time:15203ms step_avg:32.56ms
step:468/1775 train_time:15236ms step_avg:32.56ms
step:469/1775 train_time:15268ms step_avg:32.55ms
step:470/1775 train_time:15301ms step_avg:32.56ms
step:471/1775 train_time:15332ms step_avg:32.55ms
step:472/1775 train_time:15366ms step_avg:32.55ms
step:473/1775 train_time:15397ms step_avg:32.55ms
step:474/1775 train_time:15431ms step_avg:32.55ms
step:475/1775 train_time:15462ms step_avg:32.55ms
step:476/1775 train_time:15496ms step_avg:32.55ms
step:477/1775 train_time:15527ms step_avg:32.55ms
step:478/1775 train_time:15560ms step_avg:32.55ms
step:479/1775 train_time:15592ms step_avg:32.55ms
step:480/1775 train_time:15625ms step_avg:32.55ms
step:481/1775 train_time:15657ms step_avg:32.55ms
step:482/1775 train_time:15691ms step_avg:32.55ms
step:483/1775 train_time:15722ms step_avg:32.55ms
step:484/1775 train_time:15755ms step_avg:32.55ms
step:485/1775 train_time:15787ms step_avg:32.55ms
step:486/1775 train_time:15821ms step_avg:32.55ms
step:487/1775 train_time:15852ms step_avg:32.55ms
step:488/1775 train_time:15885ms step_avg:32.55ms
step:489/1775 train_time:15917ms step_avg:32.55ms
step:490/1775 train_time:15951ms step_avg:32.55ms
step:491/1775 train_time:15982ms step_avg:32.55ms
step:492/1775 train_time:16015ms step_avg:32.55ms
step:493/1775 train_time:16047ms step_avg:32.55ms
step:494/1775 train_time:16080ms step_avg:32.55ms
step:495/1775 train_time:16111ms step_avg:32.55ms
step:496/1775 train_time:16145ms step_avg:32.55ms
step:497/1775 train_time:16176ms step_avg:32.55ms
step:498/1775 train_time:16210ms step_avg:32.55ms
step:499/1775 train_time:16242ms step_avg:32.55ms
step:500/1775 train_time:16275ms step_avg:32.55ms
step:500/1775 val_loss:4.2740 train_time:16316ms step_avg:32.63ms
step:501/1775 train_time:16335ms step_avg:32.61ms
step:502/1775 train_time:16355ms step_avg:32.58ms
step:503/1775 train_time:16373ms step_avg:32.55ms
step:504/1775 train_time:16407ms step_avg:32.55ms
step:505/1775 train_time:16441ms step_avg:32.56ms
step:506/1775 train_time:16477ms step_avg:32.56ms
step:507/1775 train_time:16509ms step_avg:32.56ms
step:508/1775 train_time:16543ms step_avg:32.56ms
step:509/1775 train_time:16574ms step_avg:32.56ms
step:510/1775 train_time:16607ms step_avg:32.56ms
step:511/1775 train_time:16639ms step_avg:32.56ms
step:512/1775 train_time:16672ms step_avg:32.56ms
step:513/1775 train_time:16703ms step_avg:32.56ms
step:514/1775 train_time:16736ms step_avg:32.56ms
step:515/1775 train_time:16767ms step_avg:32.56ms
step:516/1775 train_time:16801ms step_avg:32.56ms
step:517/1775 train_time:16832ms step_avg:32.56ms
step:518/1775 train_time:16865ms step_avg:32.56ms
step:519/1775 train_time:16896ms step_avg:32.55ms
step:520/1775 train_time:16929ms step_avg:32.56ms
step:521/1775 train_time:16960ms step_avg:32.55ms
step:522/1775 train_time:16993ms step_avg:32.55ms
step:523/1775 train_time:17024ms step_avg:32.55ms
step:524/1775 train_time:17057ms step_avg:32.55ms
step:525/1775 train_time:17088ms step_avg:32.55ms
step:526/1775 train_time:17122ms step_avg:32.55ms
step:527/1775 train_time:17153ms step_avg:32.55ms
step:528/1775 train_time:17186ms step_avg:32.55ms
step:529/1775 train_time:17217ms step_avg:32.55ms
step:530/1775 train_time:17250ms step_avg:32.55ms
step:531/1775 train_time:17282ms step_avg:32.55ms
step:532/1775 train_time:17317ms step_avg:32.55ms
step:533/1775 train_time:17348ms step_avg:32.55ms
step:534/1775 train_time:17382ms step_avg:32.55ms
step:535/1775 train_time:17415ms step_avg:32.55ms
step:536/1775 train_time:17448ms step_avg:32.55ms
step:537/1775 train_time:17481ms step_avg:32.55ms
step:538/1775 train_time:17514ms step_avg:32.55ms
step:539/1775 train_time:17546ms step_avg:32.55ms
step:540/1775 train_time:17580ms step_avg:32.56ms
step:541/1775 train_time:17611ms step_avg:32.55ms
step:542/1775 train_time:17645ms step_avg:32.55ms
step:543/1775 train_time:17676ms step_avg:32.55ms
step:544/1775 train_time:17709ms step_avg:32.55ms
step:545/1775 train_time:17741ms step_avg:32.55ms
step:546/1775 train_time:17774ms step_avg:32.55ms
step:547/1775 train_time:17805ms step_avg:32.55ms
step:548/1775 train_time:17839ms step_avg:32.55ms
step:549/1775 train_time:17870ms step_avg:32.55ms
step:550/1775 train_time:17903ms step_avg:32.55ms
step:551/1775 train_time:17933ms step_avg:32.55ms
step:552/1775 train_time:17967ms step_avg:32.55ms
step:553/1775 train_time:17998ms step_avg:32.55ms
step:554/1775 train_time:18031ms step_avg:32.55ms
step:555/1775 train_time:18062ms step_avg:32.54ms
step:556/1775 train_time:18095ms step_avg:32.55ms
step:557/1775 train_time:18126ms step_avg:32.54ms
step:558/1775 train_time:18160ms step_avg:32.54ms
step:559/1775 train_time:18191ms step_avg:32.54ms
step:560/1775 train_time:18224ms step_avg:32.54ms
step:561/1775 train_time:18256ms step_avg:32.54ms
step:562/1775 train_time:18289ms step_avg:32.54ms
step:563/1775 train_time:18321ms step_avg:32.54ms
step:564/1775 train_time:18355ms step_avg:32.54ms
step:565/1775 train_time:18386ms step_avg:32.54ms
step:566/1775 train_time:18420ms step_avg:32.54ms
step:567/1775 train_time:18452ms step_avg:32.54ms
step:568/1775 train_time:18486ms step_avg:32.55ms
step:569/1775 train_time:18517ms step_avg:32.54ms
step:570/1775 train_time:18551ms step_avg:32.54ms
step:571/1775 train_time:18582ms step_avg:32.54ms
step:572/1775 train_time:18615ms step_avg:32.54ms
step:573/1775 train_time:18647ms step_avg:32.54ms
step:574/1775 train_time:18680ms step_avg:32.54ms
step:575/1775 train_time:18711ms step_avg:32.54ms
step:576/1775 train_time:18744ms step_avg:32.54ms
step:577/1775 train_time:18776ms step_avg:32.54ms
step:578/1775 train_time:18809ms step_avg:32.54ms
step:579/1775 train_time:18841ms step_avg:32.54ms
step:580/1775 train_time:18877ms step_avg:32.55ms
step:581/1775 train_time:18935ms step_avg:32.59ms
step:582/1775 train_time:18994ms step_avg:32.64ms
step:583/1775 train_time:19052ms step_avg:32.68ms
step:584/1775 train_time:19112ms step_avg:32.73ms
step:585/1775 train_time:19170ms step_avg:32.77ms
step:586/1775 train_time:19229ms step_avg:32.81ms
step:587/1775 train_time:19287ms step_avg:32.86ms
step:588/1775 train_time:19348ms step_avg:32.90ms
step:589/1775 train_time:19406ms step_avg:32.95ms
step:590/1775 train_time:19466ms step_avg:32.99ms
step:591/1775 train_time:19525ms step_avg:33.04ms
step:592/1775 train_time:19585ms step_avg:33.08ms
step:593/1775 train_time:19643ms step_avg:33.13ms
step:594/1775 train_time:19704ms step_avg:33.17ms
step:595/1775 train_time:19761ms step_avg:33.21ms
step:596/1775 train_time:19822ms step_avg:33.26ms
step:597/1775 train_time:19879ms step_avg:33.30ms
step:598/1775 train_time:19940ms step_avg:33.34ms
step:599/1775 train_time:19998ms step_avg:33.39ms
step:600/1775 train_time:20058ms step_avg:33.43ms
step:601/1775 train_time:20117ms step_avg:33.47ms
step:602/1775 train_time:20177ms step_avg:33.52ms
step:603/1775 train_time:20237ms step_avg:33.56ms
step:604/1775 train_time:20298ms step_avg:33.61ms
step:605/1775 train_time:20357ms step_avg:33.65ms
step:606/1775 train_time:20417ms step_avg:33.69ms
step:607/1775 train_time:20476ms step_avg:33.73ms
step:608/1775 train_time:20538ms step_avg:33.78ms
step:609/1775 train_time:20598ms step_avg:33.82ms
step:610/1775 train_time:20658ms step_avg:33.87ms
step:611/1775 train_time:20717ms step_avg:33.91ms
step:612/1775 train_time:20777ms step_avg:33.95ms
step:613/1775 train_time:20835ms step_avg:33.99ms
step:614/1775 train_time:20895ms step_avg:34.03ms
step:615/1775 train_time:20953ms step_avg:34.07ms
step:616/1775 train_time:21013ms step_avg:34.11ms
step:617/1775 train_time:21070ms step_avg:34.15ms
step:618/1775 train_time:21130ms step_avg:34.19ms
step:619/1775 train_time:21189ms step_avg:34.23ms
step:620/1775 train_time:21248ms step_avg:34.27ms
step:621/1775 train_time:21306ms step_avg:34.31ms
step:622/1775 train_time:21366ms step_avg:34.35ms
step:623/1775 train_time:21424ms step_avg:34.39ms
step:624/1775 train_time:21484ms step_avg:34.43ms
step:625/1775 train_time:21543ms step_avg:34.47ms
step:626/1775 train_time:21604ms step_avg:34.51ms
step:627/1775 train_time:21660ms step_avg:34.55ms
step:628/1775 train_time:21720ms step_avg:34.59ms
step:629/1775 train_time:21778ms step_avg:34.62ms
step:630/1775 train_time:21839ms step_avg:34.67ms
step:631/1775 train_time:21898ms step_avg:34.70ms
step:632/1775 train_time:21958ms step_avg:34.74ms
step:633/1775 train_time:22016ms step_avg:34.78ms
step:634/1775 train_time:22077ms step_avg:34.82ms
step:635/1775 train_time:22136ms step_avg:34.86ms
step:636/1775 train_time:22198ms step_avg:34.90ms
step:637/1775 train_time:22257ms step_avg:34.94ms
step:638/1775 train_time:22317ms step_avg:34.98ms
step:639/1775 train_time:22376ms step_avg:35.02ms
step:640/1775 train_time:22438ms step_avg:35.06ms
step:641/1775 train_time:22496ms step_avg:35.10ms
step:642/1775 train_time:22558ms step_avg:35.14ms
step:643/1775 train_time:22616ms step_avg:35.17ms
step:644/1775 train_time:22677ms step_avg:35.21ms
step:645/1775 train_time:22735ms step_avg:35.25ms
step:646/1775 train_time:22795ms step_avg:35.29ms
step:647/1775 train_time:22854ms step_avg:35.32ms
step:648/1775 train_time:22914ms step_avg:35.36ms
step:649/1775 train_time:22972ms step_avg:35.40ms
step:650/1775 train_time:23032ms step_avg:35.43ms
step:651/1775 train_time:23090ms step_avg:35.47ms
step:652/1775 train_time:23151ms step_avg:35.51ms
step:653/1775 train_time:23210ms step_avg:35.54ms
step:654/1775 train_time:23269ms step_avg:35.58ms
step:655/1775 train_time:23328ms step_avg:35.61ms
step:656/1775 train_time:23389ms step_avg:35.65ms
step:657/1775 train_time:23448ms step_avg:35.69ms
step:658/1775 train_time:23508ms step_avg:35.73ms
step:659/1775 train_time:23566ms step_avg:35.76ms
step:660/1775 train_time:23628ms step_avg:35.80ms
step:661/1775 train_time:23687ms step_avg:35.84ms
step:662/1775 train_time:23747ms step_avg:35.87ms
step:663/1775 train_time:23805ms step_avg:35.91ms
step:664/1775 train_time:23866ms step_avg:35.94ms
step:665/1775 train_time:23923ms step_avg:35.97ms
step:666/1775 train_time:23983ms step_avg:36.01ms
step:667/1775 train_time:24041ms step_avg:36.04ms
step:668/1775 train_time:24102ms step_avg:36.08ms
step:669/1775 train_time:24160ms step_avg:36.11ms
step:670/1775 train_time:24220ms step_avg:36.15ms
step:671/1775 train_time:24277ms step_avg:36.18ms
step:672/1775 train_time:24339ms step_avg:36.22ms
step:673/1775 train_time:24399ms step_avg:36.25ms
step:674/1775 train_time:24459ms step_avg:36.29ms
step:675/1775 train_time:24517ms step_avg:36.32ms
step:676/1775 train_time:24578ms step_avg:36.36ms
step:677/1775 train_time:24638ms step_avg:36.39ms
step:678/1775 train_time:24698ms step_avg:36.43ms
step:679/1775 train_time:24757ms step_avg:36.46ms
step:680/1775 train_time:24817ms step_avg:36.50ms
step:681/1775 train_time:24876ms step_avg:36.53ms
step:682/1775 train_time:24936ms step_avg:36.56ms
step:683/1775 train_time:24995ms step_avg:36.60ms
step:684/1775 train_time:25055ms step_avg:36.63ms
step:685/1775 train_time:25113ms step_avg:36.66ms
step:686/1775 train_time:25174ms step_avg:36.70ms
step:687/1775 train_time:25232ms step_avg:36.73ms
step:688/1775 train_time:25293ms step_avg:36.76ms
step:689/1775 train_time:25351ms step_avg:36.79ms
step:690/1775 train_time:25413ms step_avg:36.83ms
step:691/1775 train_time:25470ms step_avg:36.86ms
step:692/1775 train_time:25531ms step_avg:36.89ms
step:693/1775 train_time:25589ms step_avg:36.93ms
step:694/1775 train_time:25650ms step_avg:36.96ms
step:695/1775 train_time:25709ms step_avg:36.99ms
step:696/1775 train_time:25769ms step_avg:37.02ms
step:697/1775 train_time:25826ms step_avg:37.05ms
step:698/1775 train_time:25888ms step_avg:37.09ms
step:699/1775 train_time:25946ms step_avg:37.12ms
step:700/1775 train_time:26007ms step_avg:37.15ms
step:701/1775 train_time:26064ms step_avg:37.18ms
step:702/1775 train_time:26124ms step_avg:37.21ms
step:703/1775 train_time:26182ms step_avg:37.24ms
step:704/1775 train_time:26242ms step_avg:37.28ms
step:705/1775 train_time:26300ms step_avg:37.31ms
step:706/1775 train_time:26361ms step_avg:37.34ms
step:707/1775 train_time:26419ms step_avg:37.37ms
step:708/1775 train_time:26481ms step_avg:37.40ms
step:709/1775 train_time:26539ms step_avg:37.43ms
step:710/1775 train_time:26600ms step_avg:37.46ms
step:711/1775 train_time:26659ms step_avg:37.49ms
step:712/1775 train_time:26720ms step_avg:37.53ms
step:713/1775 train_time:26778ms step_avg:37.56ms
step:714/1775 train_time:26839ms step_avg:37.59ms
step:715/1775 train_time:26898ms step_avg:37.62ms
step:716/1775 train_time:26958ms step_avg:37.65ms
step:717/1775 train_time:27017ms step_avg:37.68ms
step:718/1775 train_time:27077ms step_avg:37.71ms
step:719/1775 train_time:27135ms step_avg:37.74ms
step:720/1775 train_time:27197ms step_avg:37.77ms
step:721/1775 train_time:27254ms step_avg:37.80ms
step:722/1775 train_time:27315ms step_avg:37.83ms
step:723/1775 train_time:27372ms step_avg:37.86ms
step:724/1775 train_time:27432ms step_avg:37.89ms
step:725/1775 train_time:27490ms step_avg:37.92ms
step:726/1775 train_time:27551ms step_avg:37.95ms
step:727/1775 train_time:27610ms step_avg:37.98ms
step:728/1775 train_time:27671ms step_avg:38.01ms
step:729/1775 train_time:27729ms step_avg:38.04ms
step:730/1775 train_time:27791ms step_avg:38.07ms
step:731/1775 train_time:27849ms step_avg:38.10ms
step:732/1775 train_time:27910ms step_avg:38.13ms
step:733/1775 train_time:27967ms step_avg:38.15ms
step:734/1775 train_time:28028ms step_avg:38.19ms
step:735/1775 train_time:28086ms step_avg:38.21ms
step:736/1775 train_time:28147ms step_avg:38.24ms
step:737/1775 train_time:28205ms step_avg:38.27ms
step:738/1775 train_time:28266ms step_avg:38.30ms
step:739/1775 train_time:28323ms step_avg:38.33ms
step:740/1775 train_time:28384ms step_avg:38.36ms
step:741/1775 train_time:28441ms step_avg:38.38ms
step:742/1775 train_time:28502ms step_avg:38.41ms
step:743/1775 train_time:28559ms step_avg:38.44ms
step:744/1775 train_time:28620ms step_avg:38.47ms
step:745/1775 train_time:28680ms step_avg:38.50ms
step:746/1775 train_time:28739ms step_avg:38.52ms
step:747/1775 train_time:28799ms step_avg:38.55ms
step:748/1775 train_time:28859ms step_avg:38.58ms
step:749/1775 train_time:28917ms step_avg:38.61ms
step:750/1775 train_time:28978ms step_avg:38.64ms
step:750/1775 val_loss:4.0061 train_time:29049ms step_avg:38.73ms
step:751/1775 train_time:29069ms step_avg:38.71ms
step:752/1775 train_time:29098ms step_avg:38.69ms
step:753/1775 train_time:29160ms step_avg:38.73ms
step:754/1775 train_time:29226ms step_avg:38.76ms
step:755/1775 train_time:29288ms step_avg:38.79ms
step:756/1775 train_time:29349ms step_avg:38.82ms
step:757/1775 train_time:29407ms step_avg:38.85ms
step:758/1775 train_time:29466ms step_avg:38.87ms
step:759/1775 train_time:29525ms step_avg:38.90ms
step:760/1775 train_time:29583ms step_avg:38.93ms
step:761/1775 train_time:29640ms step_avg:38.95ms
step:762/1775 train_time:29700ms step_avg:38.98ms
step:763/1775 train_time:29757ms step_avg:39.00ms
step:764/1775 train_time:29817ms step_avg:39.03ms
step:765/1775 train_time:29874ms step_avg:39.05ms
step:766/1775 train_time:29933ms step_avg:39.08ms
step:767/1775 train_time:29991ms step_avg:39.10ms
step:768/1775 train_time:30051ms step_avg:39.13ms
step:769/1775 train_time:30110ms step_avg:39.16ms
step:770/1775 train_time:30173ms step_avg:39.19ms
step:771/1775 train_time:30234ms step_avg:39.21ms
step:772/1775 train_time:30296ms step_avg:39.24ms
step:773/1775 train_time:30356ms step_avg:39.27ms
step:774/1775 train_time:30417ms step_avg:39.30ms
step:775/1775 train_time:30476ms step_avg:39.32ms
step:776/1775 train_time:30537ms step_avg:39.35ms
step:777/1775 train_time:30594ms step_avg:39.37ms
step:778/1775 train_time:30654ms step_avg:39.40ms
step:779/1775 train_time:30711ms step_avg:39.42ms
step:780/1775 train_time:30772ms step_avg:39.45ms
step:781/1775 train_time:30829ms step_avg:39.47ms
step:782/1775 train_time:30889ms step_avg:39.50ms
step:783/1775 train_time:30947ms step_avg:39.52ms
step:784/1775 train_time:31007ms step_avg:39.55ms
step:785/1775 train_time:31065ms step_avg:39.57ms
step:786/1775 train_time:31126ms step_avg:39.60ms
step:787/1775 train_time:31185ms step_avg:39.63ms
step:788/1775 train_time:31246ms step_avg:39.65ms
step:789/1775 train_time:31305ms step_avg:39.68ms
step:790/1775 train_time:31366ms step_avg:39.70ms
step:791/1775 train_time:31423ms step_avg:39.73ms
step:792/1775 train_time:31483ms step_avg:39.75ms
step:793/1775 train_time:31540ms step_avg:39.77ms
step:794/1775 train_time:31600ms step_avg:39.80ms
step:795/1775 train_time:31658ms step_avg:39.82ms
step:796/1775 train_time:31719ms step_avg:39.85ms
step:797/1775 train_time:31776ms step_avg:39.87ms
step:798/1775 train_time:31836ms step_avg:39.89ms
step:799/1775 train_time:31894ms step_avg:39.92ms
step:800/1775 train_time:31955ms step_avg:39.94ms
step:801/1775 train_time:32014ms step_avg:39.97ms
step:802/1775 train_time:32074ms step_avg:39.99ms
step:803/1775 train_time:32132ms step_avg:40.01ms
step:804/1775 train_time:32194ms step_avg:40.04ms
step:805/1775 train_time:32253ms step_avg:40.07ms
step:806/1775 train_time:32315ms step_avg:40.09ms
step:807/1775 train_time:32374ms step_avg:40.12ms
step:808/1775 train_time:32436ms step_avg:40.14ms
step:809/1775 train_time:32493ms step_avg:40.16ms
step:810/1775 train_time:32554ms step_avg:40.19ms
step:811/1775 train_time:32613ms step_avg:40.21ms
step:812/1775 train_time:32673ms step_avg:40.24ms
step:813/1775 train_time:32730ms step_avg:40.26ms
step:814/1775 train_time:32790ms step_avg:40.28ms
step:815/1775 train_time:32847ms step_avg:40.30ms
step:816/1775 train_time:32907ms step_avg:40.33ms
step:817/1775 train_time:32965ms step_avg:40.35ms
step:818/1775 train_time:33027ms step_avg:40.37ms
step:819/1775 train_time:33084ms step_avg:40.40ms
step:820/1775 train_time:33144ms step_avg:40.42ms
step:821/1775 train_time:33202ms step_avg:40.44ms
step:822/1775 train_time:33262ms step_avg:40.46ms
step:823/1775 train_time:33320ms step_avg:40.49ms
step:824/1775 train_time:33381ms step_avg:40.51ms
step:825/1775 train_time:33440ms step_avg:40.53ms
step:826/1775 train_time:33500ms step_avg:40.56ms
step:827/1775 train_time:33558ms step_avg:40.58ms
step:828/1775 train_time:33618ms step_avg:40.60ms
step:829/1775 train_time:33676ms step_avg:40.62ms
step:830/1775 train_time:33737ms step_avg:40.65ms
step:831/1775 train_time:33795ms step_avg:40.67ms
step:832/1775 train_time:33857ms step_avg:40.69ms
step:833/1775 train_time:33916ms step_avg:40.72ms
step:834/1775 train_time:33976ms step_avg:40.74ms
step:835/1775 train_time:34035ms step_avg:40.76ms
step:836/1775 train_time:34096ms step_avg:40.78ms
step:837/1775 train_time:34155ms step_avg:40.81ms
step:838/1775 train_time:34216ms step_avg:40.83ms
step:839/1775 train_time:34274ms step_avg:40.85ms
step:840/1775 train_time:34335ms step_avg:40.88ms
step:841/1775 train_time:34393ms step_avg:40.89ms
step:842/1775 train_time:34453ms step_avg:40.92ms
step:843/1775 train_time:34512ms step_avg:40.94ms
step:844/1775 train_time:34573ms step_avg:40.96ms
step:845/1775 train_time:34631ms step_avg:40.98ms
step:846/1775 train_time:34690ms step_avg:41.01ms
step:847/1775 train_time:34748ms step_avg:41.02ms
step:848/1775 train_time:34809ms step_avg:41.05ms
step:849/1775 train_time:34866ms step_avg:41.07ms
step:850/1775 train_time:34927ms step_avg:41.09ms
step:851/1775 train_time:34984ms step_avg:41.11ms
step:852/1775 train_time:35044ms step_avg:41.13ms
step:853/1775 train_time:35102ms step_avg:41.15ms
step:854/1775 train_time:35162ms step_avg:41.17ms
step:855/1775 train_time:35220ms step_avg:41.19ms
step:856/1775 train_time:35281ms step_avg:41.22ms
step:857/1775 train_time:35339ms step_avg:41.24ms
step:858/1775 train_time:35401ms step_avg:41.26ms
step:859/1775 train_time:35458ms step_avg:41.28ms
step:860/1775 train_time:35519ms step_avg:41.30ms
step:861/1775 train_time:35577ms step_avg:41.32ms
step:862/1775 train_time:35638ms step_avg:41.34ms
step:863/1775 train_time:35695ms step_avg:41.36ms
step:864/1775 train_time:35757ms step_avg:41.39ms
step:865/1775 train_time:35816ms step_avg:41.41ms
step:866/1775 train_time:35876ms step_avg:41.43ms
step:867/1775 train_time:35933ms step_avg:41.45ms
step:868/1775 train_time:35994ms step_avg:41.47ms
step:869/1775 train_time:36052ms step_avg:41.49ms
step:870/1775 train_time:36113ms step_avg:41.51ms
step:871/1775 train_time:36172ms step_avg:41.53ms
step:872/1775 train_time:36233ms step_avg:41.55ms
step:873/1775 train_time:36292ms step_avg:41.57ms
step:874/1775 train_time:36353ms step_avg:41.59ms
step:875/1775 train_time:36411ms step_avg:41.61ms
step:876/1775 train_time:36472ms step_avg:41.63ms
step:877/1775 train_time:36530ms step_avg:41.65ms
step:878/1775 train_time:36590ms step_avg:41.67ms
step:879/1775 train_time:36648ms step_avg:41.69ms
step:880/1775 train_time:36709ms step_avg:41.71ms
step:881/1775 train_time:36767ms step_avg:41.73ms
step:882/1775 train_time:36827ms step_avg:41.75ms
step:883/1775 train_time:36884ms step_avg:41.77ms
step:884/1775 train_time:36944ms step_avg:41.79ms
step:885/1775 train_time:37003ms step_avg:41.81ms
step:886/1775 train_time:37063ms step_avg:41.83ms
step:887/1775 train_time:37121ms step_avg:41.85ms
step:888/1775 train_time:37181ms step_avg:41.87ms
step:889/1775 train_time:37239ms step_avg:41.89ms
step:890/1775 train_time:37299ms step_avg:41.91ms
step:891/1775 train_time:37358ms step_avg:41.93ms
step:892/1775 train_time:37419ms step_avg:41.95ms
step:893/1775 train_time:37479ms step_avg:41.97ms
step:894/1775 train_time:37538ms step_avg:41.99ms
step:895/1775 train_time:37597ms step_avg:42.01ms
step:896/1775 train_time:37657ms step_avg:42.03ms
step:897/1775 train_time:37716ms step_avg:42.05ms
step:898/1775 train_time:37777ms step_avg:42.07ms
step:899/1775 train_time:37835ms step_avg:42.09ms
step:900/1775 train_time:37896ms step_avg:42.11ms
step:901/1775 train_time:37954ms step_avg:42.12ms
step:902/1775 train_time:38014ms step_avg:42.14ms
step:903/1775 train_time:38072ms step_avg:42.16ms
step:904/1775 train_time:38134ms step_avg:42.18ms
step:905/1775 train_time:38192ms step_avg:42.20ms
step:906/1775 train_time:38252ms step_avg:42.22ms
step:907/1775 train_time:38311ms step_avg:42.24ms
step:908/1775 train_time:38372ms step_avg:42.26ms
step:909/1775 train_time:38430ms step_avg:42.28ms
step:910/1775 train_time:38491ms step_avg:42.30ms
step:911/1775 train_time:38549ms step_avg:42.31ms
step:912/1775 train_time:38609ms step_avg:42.33ms
step:913/1775 train_time:38667ms step_avg:42.35ms
step:914/1775 train_time:38728ms step_avg:42.37ms
step:915/1775 train_time:38785ms step_avg:42.39ms
step:916/1775 train_time:38845ms step_avg:42.41ms
step:917/1775 train_time:38903ms step_avg:42.42ms
step:918/1775 train_time:38963ms step_avg:42.44ms
step:919/1775 train_time:39021ms step_avg:42.46ms
step:920/1775 train_time:39081ms step_avg:42.48ms
step:921/1775 train_time:39140ms step_avg:42.50ms
step:922/1775 train_time:39202ms step_avg:42.52ms
step:923/1775 train_time:39259ms step_avg:42.53ms
step:924/1775 train_time:39321ms step_avg:42.55ms
step:925/1775 train_time:39379ms step_avg:42.57ms
step:926/1775 train_time:39440ms step_avg:42.59ms
step:927/1775 train_time:39499ms step_avg:42.61ms
step:928/1775 train_time:39559ms step_avg:42.63ms
step:929/1775 train_time:39617ms step_avg:42.65ms
step:930/1775 train_time:39677ms step_avg:42.66ms
step:931/1775 train_time:39737ms step_avg:42.68ms
step:932/1775 train_time:39797ms step_avg:42.70ms
step:933/1775 train_time:39856ms step_avg:42.72ms
step:934/1775 train_time:39918ms step_avg:42.74ms
step:935/1775 train_time:39975ms step_avg:42.75ms
step:936/1775 train_time:40036ms step_avg:42.77ms
step:937/1775 train_time:40095ms step_avg:42.79ms
step:938/1775 train_time:40156ms step_avg:42.81ms
step:939/1775 train_time:40214ms step_avg:42.83ms
step:940/1775 train_time:40274ms step_avg:42.85ms
step:941/1775 train_time:40332ms step_avg:42.86ms
step:942/1775 train_time:40392ms step_avg:42.88ms
step:943/1775 train_time:40450ms step_avg:42.89ms
step:944/1775 train_time:40510ms step_avg:42.91ms
step:945/1775 train_time:40569ms step_avg:42.93ms
step:946/1775 train_time:40629ms step_avg:42.95ms
step:947/1775 train_time:40686ms step_avg:42.96ms
step:948/1775 train_time:40746ms step_avg:42.98ms
step:949/1775 train_time:40804ms step_avg:43.00ms
step:950/1775 train_time:40863ms step_avg:43.01ms
step:951/1775 train_time:40922ms step_avg:43.03ms
step:952/1775 train_time:40981ms step_avg:43.05ms
step:953/1775 train_time:41040ms step_avg:43.06ms
step:954/1775 train_time:41100ms step_avg:43.08ms
step:955/1775 train_time:41158ms step_avg:43.10ms
step:956/1775 train_time:41218ms step_avg:43.12ms
step:957/1775 train_time:41277ms step_avg:43.13ms
step:958/1775 train_time:41337ms step_avg:43.15ms
step:959/1775 train_time:41395ms step_avg:43.16ms
step:960/1775 train_time:41456ms step_avg:43.18ms
step:961/1775 train_time:41515ms step_avg:43.20ms
step:962/1775 train_time:41575ms step_avg:43.22ms
step:963/1775 train_time:41634ms step_avg:43.23ms
step:964/1775 train_time:41696ms step_avg:43.25ms
step:965/1775 train_time:41754ms step_avg:43.27ms
step:966/1775 train_time:41815ms step_avg:43.29ms
step:967/1775 train_time:41873ms step_avg:43.30ms
step:968/1775 train_time:41935ms step_avg:43.32ms
step:969/1775 train_time:41993ms step_avg:43.34ms
step:970/1775 train_time:42053ms step_avg:43.35ms
step:971/1775 train_time:42112ms step_avg:43.37ms
step:972/1775 train_time:42173ms step_avg:43.39ms
step:973/1775 train_time:42231ms step_avg:43.40ms
step:974/1775 train_time:42291ms step_avg:43.42ms
step:975/1775 train_time:42349ms step_avg:43.44ms
step:976/1775 train_time:42411ms step_avg:43.45ms
step:977/1775 train_time:42469ms step_avg:43.47ms
step:978/1775 train_time:42530ms step_avg:43.49ms
step:979/1775 train_time:42587ms step_avg:43.50ms
step:980/1775 train_time:42648ms step_avg:43.52ms
step:981/1775 train_time:42706ms step_avg:43.53ms
step:982/1775 train_time:42766ms step_avg:43.55ms
step:983/1775 train_time:42825ms step_avg:43.57ms
step:984/1775 train_time:42885ms step_avg:43.58ms
step:985/1775 train_time:42943ms step_avg:43.60ms
step:986/1775 train_time:43003ms step_avg:43.61ms
step:987/1775 train_time:43060ms step_avg:43.63ms
step:988/1775 train_time:43120ms step_avg:43.64ms
step:989/1775 train_time:43178ms step_avg:43.66ms
step:990/1775 train_time:43238ms step_avg:43.68ms
step:991/1775 train_time:43297ms step_avg:43.69ms
step:992/1775 train_time:43358ms step_avg:43.71ms
step:993/1775 train_time:43418ms step_avg:43.72ms
step:994/1775 train_time:43478ms step_avg:43.74ms
step:995/1775 train_time:43537ms step_avg:43.76ms
step:996/1775 train_time:43598ms step_avg:43.77ms
step:997/1775 train_time:43657ms step_avg:43.79ms
step:998/1775 train_time:43717ms step_avg:43.81ms
step:999/1775 train_time:43776ms step_avg:43.82ms
step:1000/1775 train_time:43836ms step_avg:43.84ms
step:1000/1775 val_loss:3.7358 train_time:43907ms step_avg:43.91ms
step:1001/1775 train_time:43927ms step_avg:43.88ms
step:1002/1775 train_time:43957ms step_avg:43.87ms
step:1003/1775 train_time:44020ms step_avg:43.89ms
step:1004/1775 train_time:44082ms step_avg:43.91ms
step:1005/1775 train_time:44141ms step_avg:43.92ms
step:1006/1775 train_time:44201ms step_avg:43.94ms
step:1007/1775 train_time:44257ms step_avg:43.95ms
step:1008/1775 train_time:44318ms step_avg:43.97ms
step:1009/1775 train_time:44375ms step_avg:43.98ms
step:1010/1775 train_time:44435ms step_avg:43.99ms
step:1011/1775 train_time:44492ms step_avg:44.01ms
step:1012/1775 train_time:44553ms step_avg:44.02ms
step:1013/1775 train_time:44610ms step_avg:44.04ms
step:1014/1775 train_time:44670ms step_avg:44.05ms
step:1015/1775 train_time:44728ms step_avg:44.07ms
step:1016/1775 train_time:44788ms step_avg:44.08ms
step:1017/1775 train_time:44846ms step_avg:44.10ms
step:1018/1775 train_time:44907ms step_avg:44.11ms
step:1019/1775 train_time:44967ms step_avg:44.13ms
step:1020/1775 train_time:45030ms step_avg:44.15ms
step:1021/1775 train_time:45090ms step_avg:44.16ms
step:1022/1775 train_time:45151ms step_avg:44.18ms
step:1023/1775 train_time:45209ms step_avg:44.19ms
step:1024/1775 train_time:45271ms step_avg:44.21ms
step:1025/1775 train_time:45329ms step_avg:44.22ms
step:1026/1775 train_time:45389ms step_avg:44.24ms
step:1027/1775 train_time:45447ms step_avg:44.25ms
step:1028/1775 train_time:45507ms step_avg:44.27ms
step:1029/1775 train_time:45564ms step_avg:44.28ms
step:1030/1775 train_time:45625ms step_avg:44.30ms
step:1031/1775 train_time:45682ms step_avg:44.31ms
step:1032/1775 train_time:45741ms step_avg:44.32ms
step:1033/1775 train_time:45799ms step_avg:44.34ms
step:1034/1775 train_time:45859ms step_avg:44.35ms
step:1035/1775 train_time:45918ms step_avg:44.36ms
step:1036/1775 train_time:45979ms step_avg:44.38ms
step:1037/1775 train_time:46039ms step_avg:44.40ms
step:1038/1775 train_time:46099ms step_avg:44.41ms
step:1039/1775 train_time:46156ms step_avg:44.42ms
step:1040/1775 train_time:46217ms step_avg:44.44ms
step:1041/1775 train_time:46275ms step_avg:44.45ms
step:1042/1775 train_time:46335ms step_avg:44.47ms
step:1043/1775 train_time:46392ms step_avg:44.48ms
step:1044/1775 train_time:46454ms step_avg:44.50ms
step:1045/1775 train_time:46512ms step_avg:44.51ms
step:1046/1775 train_time:46573ms step_avg:44.52ms
step:1047/1775 train_time:46631ms step_avg:44.54ms
step:1048/1775 train_time:46691ms step_avg:44.55ms
step:1049/1775 train_time:46748ms step_avg:44.56ms
step:1050/1775 train_time:46808ms step_avg:44.58ms
step:1051/1775 train_time:46866ms step_avg:44.59ms
step:1052/1775 train_time:46928ms step_avg:44.61ms
step:1053/1775 train_time:46986ms step_avg:44.62ms
step:1054/1775 train_time:47049ms step_avg:44.64ms
step:1055/1775 train_time:47107ms step_avg:44.65ms
step:1056/1775 train_time:47169ms step_avg:44.67ms
step:1057/1775 train_time:47227ms step_avg:44.68ms
step:1058/1775 train_time:47289ms step_avg:44.70ms
step:1059/1775 train_time:47347ms step_avg:44.71ms
step:1060/1775 train_time:47407ms step_avg:44.72ms
step:1061/1775 train_time:47465ms step_avg:44.74ms
step:1062/1775 train_time:47526ms step_avg:44.75ms
step:1063/1775 train_time:47583ms step_avg:44.76ms
step:1064/1775 train_time:47644ms step_avg:44.78ms
step:1065/1775 train_time:47701ms step_avg:44.79ms
step:1066/1775 train_time:47762ms step_avg:44.80ms
step:1067/1775 train_time:47820ms step_avg:44.82ms
step:1068/1775 train_time:47880ms step_avg:44.83ms
step:1069/1775 train_time:47938ms step_avg:44.84ms
step:1070/1775 train_time:47999ms step_avg:44.86ms
step:1071/1775 train_time:48058ms step_avg:44.87ms
step:1072/1775 train_time:48117ms step_avg:44.89ms
step:1073/1775 train_time:48175ms step_avg:44.90ms
step:1074/1775 train_time:48236ms step_avg:44.91ms
step:1075/1775 train_time:48294ms step_avg:44.92ms
step:1076/1775 train_time:48354ms step_avg:44.94ms
step:1077/1775 train_time:48412ms step_avg:44.95ms
step:1078/1775 train_time:48472ms step_avg:44.96ms
step:1079/1775 train_time:48531ms step_avg:44.98ms
step:1080/1775 train_time:48591ms step_avg:44.99ms
step:1081/1775 train_time:48648ms step_avg:45.00ms
step:1082/1775 train_time:48709ms step_avg:45.02ms
step:1083/1775 train_time:48767ms step_avg:45.03ms
step:1084/1775 train_time:48827ms step_avg:45.04ms
step:1085/1775 train_time:48885ms step_avg:45.06ms
step:1086/1775 train_time:48947ms step_avg:45.07ms
step:1087/1775 train_time:49005ms step_avg:45.08ms
step:1088/1775 train_time:49066ms step_avg:45.10ms
step:1089/1775 train_time:49125ms step_avg:45.11ms
step:1090/1775 train_time:49186ms step_avg:45.12ms
step:1091/1775 train_time:49244ms step_avg:45.14ms
step:1092/1775 train_time:49305ms step_avg:45.15ms
step:1093/1775 train_time:49363ms step_avg:45.16ms
step:1094/1775 train_time:49424ms step_avg:45.18ms
step:1095/1775 train_time:49482ms step_avg:45.19ms
step:1096/1775 train_time:49543ms step_avg:45.20ms
step:1097/1775 train_time:49600ms step_avg:45.21ms
step:1098/1775 train_time:49660ms step_avg:45.23ms
step:1099/1775 train_time:49718ms step_avg:45.24ms
step:1100/1775 train_time:49778ms step_avg:45.25ms
step:1101/1775 train_time:49837ms step_avg:45.26ms
step:1102/1775 train_time:49898ms step_avg:45.28ms
step:1103/1775 train_time:49956ms step_avg:45.29ms
step:1104/1775 train_time:50017ms step_avg:45.31ms
step:1105/1775 train_time:50076ms step_avg:45.32ms
step:1106/1775 train_time:50136ms step_avg:45.33ms
step:1107/1775 train_time:50194ms step_avg:45.34ms
step:1108/1775 train_time:50254ms step_avg:45.36ms
step:1109/1775 train_time:50313ms step_avg:45.37ms
step:1110/1775 train_time:50374ms step_avg:45.38ms
step:1111/1775 train_time:50431ms step_avg:45.39ms
step:1112/1775 train_time:50492ms step_avg:45.41ms
step:1113/1775 train_time:50550ms step_avg:45.42ms
step:1114/1775 train_time:50610ms step_avg:45.43ms
step:1115/1775 train_time:50667ms step_avg:45.44ms
step:1116/1775 train_time:50728ms step_avg:45.46ms
step:1117/1775 train_time:50786ms step_avg:45.47ms
step:1118/1775 train_time:50847ms step_avg:45.48ms
step:1119/1775 train_time:50904ms step_avg:45.49ms
step:1120/1775 train_time:50965ms step_avg:45.50ms
step:1121/1775 train_time:51024ms step_avg:45.52ms
step:1122/1775 train_time:51085ms step_avg:45.53ms
step:1123/1775 train_time:51144ms step_avg:45.54ms
step:1124/1775 train_time:51204ms step_avg:45.56ms
step:1125/1775 train_time:51262ms step_avg:45.57ms
step:1126/1775 train_time:51322ms step_avg:45.58ms
step:1127/1775 train_time:51380ms step_avg:45.59ms
step:1128/1775 train_time:51441ms step_avg:45.60ms
step:1129/1775 train_time:51498ms step_avg:45.61ms
step:1130/1775 train_time:51558ms step_avg:45.63ms
step:1131/1775 train_time:51616ms step_avg:45.64ms
step:1132/1775 train_time:51676ms step_avg:45.65ms
step:1133/1775 train_time:51735ms step_avg:45.66ms
step:1134/1775 train_time:51795ms step_avg:45.67ms
step:1135/1775 train_time:51852ms step_avg:45.68ms
step:1136/1775 train_time:51913ms step_avg:45.70ms
step:1137/1775 train_time:51973ms step_avg:45.71ms
step:1138/1775 train_time:52034ms step_avg:45.72ms
step:1139/1775 train_time:52093ms step_avg:45.74ms
step:1140/1775 train_time:52153ms step_avg:45.75ms
step:1141/1775 train_time:52211ms step_avg:45.76ms
step:1142/1775 train_time:52273ms step_avg:45.77ms
step:1143/1775 train_time:52331ms step_avg:45.78ms
step:1144/1775 train_time:52392ms step_avg:45.80ms
step:1145/1775 train_time:52450ms step_avg:45.81ms
step:1146/1775 train_time:52510ms step_avg:45.82ms
step:1147/1775 train_time:52568ms step_avg:45.83ms
step:1148/1775 train_time:52629ms step_avg:45.84ms
step:1149/1775 train_time:52686ms step_avg:45.85ms
step:1150/1775 train_time:52747ms step_avg:45.87ms
step:1151/1775 train_time:52804ms step_avg:45.88ms
step:1152/1775 train_time:52865ms step_avg:45.89ms
step:1153/1775 train_time:52922ms step_avg:45.90ms
step:1154/1775 train_time:52983ms step_avg:45.91ms
step:1155/1775 train_time:53040ms step_avg:45.92ms
step:1156/1775 train_time:53100ms step_avg:45.93ms
step:1157/1775 train_time:53158ms step_avg:45.94ms
step:1158/1775 train_time:53221ms step_avg:45.96ms
step:1159/1775 train_time:53305ms step_avg:45.99ms
step:1160/1775 train_time:53392ms step_avg:46.03ms
step:1161/1775 train_time:53474ms step_avg:46.06ms
step:1162/1775 train_time:53561ms step_avg:46.09ms
step:1163/1775 train_time:53645ms step_avg:46.13ms
step:1164/1775 train_time:53731ms step_avg:46.16ms
step:1165/1775 train_time:53815ms step_avg:46.19ms
step:1166/1775 train_time:53901ms step_avg:46.23ms
step:1167/1775 train_time:53986ms step_avg:46.26ms
step:1168/1775 train_time:54072ms step_avg:46.29ms
step:1169/1775 train_time:54156ms step_avg:46.33ms
step:1170/1775 train_time:54243ms step_avg:46.36ms
step:1171/1775 train_time:54327ms step_avg:46.39ms
step:1172/1775 train_time:54412ms step_avg:46.43ms
step:1173/1775 train_time:54496ms step_avg:46.46ms
step:1174/1775 train_time:54583ms step_avg:46.49ms
step:1175/1775 train_time:54668ms step_avg:46.53ms
step:1176/1775 train_time:54753ms step_avg:46.56ms
step:1177/1775 train_time:54837ms step_avg:46.59ms
step:1178/1775 train_time:54923ms step_avg:46.62ms
step:1179/1775 train_time:55007ms step_avg:46.66ms
step:1180/1775 train_time:55093ms step_avg:46.69ms
step:1181/1775 train_time:55177ms step_avg:46.72ms
step:1182/1775 train_time:55264ms step_avg:46.75ms
step:1183/1775 train_time:55347ms step_avg:46.79ms
step:1184/1775 train_time:55434ms step_avg:46.82ms
step:1185/1775 train_time:55517ms step_avg:46.85ms
step:1186/1775 train_time:55604ms step_avg:46.88ms
step:1187/1775 train_time:55688ms step_avg:46.92ms
step:1188/1775 train_time:55774ms step_avg:46.95ms
step:1189/1775 train_time:55857ms step_avg:46.98ms
step:1190/1775 train_time:55944ms step_avg:47.01ms
step:1191/1775 train_time:56028ms step_avg:47.04ms
step:1192/1775 train_time:56114ms step_avg:47.08ms
step:1193/1775 train_time:56199ms step_avg:47.11ms
step:1194/1775 train_time:56285ms step_avg:47.14ms
step:1195/1775 train_time:56369ms step_avg:47.17ms
step:1196/1775 train_time:56455ms step_avg:47.20ms
step:1197/1775 train_time:56538ms step_avg:47.23ms
step:1198/1775 train_time:56625ms step_avg:47.27ms
step:1199/1775 train_time:56709ms step_avg:47.30ms
step:1200/1775 train_time:56794ms step_avg:47.33ms
step:1201/1775 train_time:56878ms step_avg:47.36ms
step:1202/1775 train_time:56965ms step_avg:47.39ms
step:1203/1775 train_time:57048ms step_avg:47.42ms
step:1204/1775 train_time:57134ms step_avg:47.45ms
step:1205/1775 train_time:57218ms step_avg:47.48ms
step:1206/1775 train_time:57306ms step_avg:47.52ms
step:1207/1775 train_time:57390ms step_avg:47.55ms
step:1208/1775 train_time:57475ms step_avg:47.58ms
step:1209/1775 train_time:57559ms step_avg:47.61ms
step:1210/1775 train_time:57646ms step_avg:47.64ms
step:1211/1775 train_time:57729ms step_avg:47.67ms
step:1212/1775 train_time:57816ms step_avg:47.70ms
step:1213/1775 train_time:57900ms step_avg:47.73ms
step:1214/1775 train_time:57986ms step_avg:47.76ms
step:1215/1775 train_time:58070ms step_avg:47.79ms
step:1216/1775 train_time:58157ms step_avg:47.83ms
step:1217/1775 train_time:58240ms step_avg:47.86ms
step:1218/1775 train_time:58328ms step_avg:47.89ms
step:1219/1775 train_time:58410ms step_avg:47.92ms
step:1220/1775 train_time:58497ms step_avg:47.95ms
step:1221/1775 train_time:58581ms step_avg:47.98ms
step:1222/1775 train_time:58668ms step_avg:48.01ms
step:1223/1775 train_time:58750ms step_avg:48.04ms
step:1224/1775 train_time:58837ms step_avg:48.07ms
step:1225/1775 train_time:58920ms step_avg:48.10ms
step:1226/1775 train_time:59008ms step_avg:48.13ms
step:1227/1775 train_time:59091ms step_avg:48.16ms
step:1228/1775 train_time:59177ms step_avg:48.19ms
step:1229/1775 train_time:59261ms step_avg:48.22ms
step:1230/1775 train_time:59348ms step_avg:48.25ms
step:1231/1775 train_time:59432ms step_avg:48.28ms
step:1232/1775 train_time:59519ms step_avg:48.31ms
step:1233/1775 train_time:59603ms step_avg:48.34ms
step:1234/1775 train_time:59689ms step_avg:48.37ms
step:1235/1775 train_time:59773ms step_avg:48.40ms
step:1236/1775 train_time:59859ms step_avg:48.43ms
step:1237/1775 train_time:59942ms step_avg:48.46ms
step:1238/1775 train_time:60028ms step_avg:48.49ms
step:1239/1775 train_time:60112ms step_avg:48.52ms
step:1240/1775 train_time:60199ms step_avg:48.55ms
step:1241/1775 train_time:60285ms step_avg:48.58ms
step:1242/1775 train_time:60371ms step_avg:48.61ms
step:1243/1775 train_time:60454ms step_avg:48.64ms
step:1244/1775 train_time:60540ms step_avg:48.67ms
step:1245/1775 train_time:60623ms step_avg:48.69ms
step:1246/1775 train_time:60710ms step_avg:48.72ms
step:1247/1775 train_time:60792ms step_avg:48.75ms
step:1248/1775 train_time:60879ms step_avg:48.78ms
step:1249/1775 train_time:60963ms step_avg:48.81ms
step:1250/1775 train_time:61049ms step_avg:48.84ms
step:1250/1775 val_loss:3.5104 train_time:61149ms step_avg:48.92ms
step:1251/1775 train_time:61170ms step_avg:48.90ms
step:1252/1775 train_time:61221ms step_avg:48.90ms
step:1253/1775 train_time:61311ms step_avg:48.93ms
step:1254/1775 train_time:61399ms step_avg:48.96ms
step:1255/1775 train_time:61484ms step_avg:48.99ms
step:1256/1775 train_time:61572ms step_avg:49.02ms
step:1257/1775 train_time:61655ms step_avg:49.05ms
step:1258/1775 train_time:61739ms step_avg:49.08ms
step:1259/1775 train_time:61821ms step_avg:49.10ms
step:1260/1775 train_time:61907ms step_avg:49.13ms
step:1261/1775 train_time:61991ms step_avg:49.16ms
step:1262/1775 train_time:62077ms step_avg:49.19ms
step:1263/1775 train_time:62160ms step_avg:49.22ms
step:1264/1775 train_time:62248ms step_avg:49.25ms
step:1265/1775 train_time:62336ms step_avg:49.28ms
step:1266/1775 train_time:62424ms step_avg:49.31ms
step:1267/1775 train_time:62509ms step_avg:49.34ms
step:1268/1775 train_time:62597ms step_avg:49.37ms
step:1269/1775 train_time:62679ms step_avg:49.39ms
step:1270/1775 train_time:62764ms step_avg:49.42ms
step:1271/1775 train_time:62846ms step_avg:49.45ms
step:1272/1775 train_time:62932ms step_avg:49.47ms
step:1273/1775 train_time:63015ms step_avg:49.50ms
step:1274/1775 train_time:63101ms step_avg:49.53ms
step:1275/1775 train_time:63186ms step_avg:49.56ms
step:1276/1775 train_time:63274ms step_avg:49.59ms
step:1277/1775 train_time:63358ms step_avg:49.61ms
step:1278/1775 train_time:63445ms step_avg:49.64ms
step:1279/1775 train_time:63529ms step_avg:49.67ms
step:1280/1775 train_time:63616ms step_avg:49.70ms
step:1281/1775 train_time:63699ms step_avg:49.73ms
step:1282/1775 train_time:63784ms step_avg:49.75ms
step:1283/1775 train_time:63867ms step_avg:49.78ms
step:1284/1775 train_time:63952ms step_avg:49.81ms
step:1285/1775 train_time:64035ms step_avg:49.83ms
step:1286/1775 train_time:64122ms step_avg:49.86ms
step:1287/1775 train_time:64207ms step_avg:49.89ms
step:1288/1775 train_time:64296ms step_avg:49.92ms
step:1289/1775 train_time:64380ms step_avg:49.95ms
step:1290/1775 train_time:64466ms step_avg:49.97ms
step:1291/1775 train_time:64550ms step_avg:50.00ms
step:1292/1775 train_time:64637ms step_avg:50.03ms
step:1293/1775 train_time:64721ms step_avg:50.05ms
step:1294/1775 train_time:64807ms step_avg:50.08ms
step:1295/1775 train_time:64889ms step_avg:50.11ms
step:1296/1775 train_time:64974ms step_avg:50.13ms
step:1297/1775 train_time:65058ms step_avg:50.16ms
step:1298/1775 train_time:65144ms step_avg:50.19ms
step:1299/1775 train_time:65229ms step_avg:50.21ms
step:1300/1775 train_time:65317ms step_avg:50.24ms
step:1301/1775 train_time:65400ms step_avg:50.27ms
step:1302/1775 train_time:65488ms step_avg:50.30ms
step:1303/1775 train_time:65572ms step_avg:50.32ms
step:1304/1775 train_time:65658ms step_avg:50.35ms
step:1305/1775 train_time:65742ms step_avg:50.38ms
step:1306/1775 train_time:65829ms step_avg:50.40ms
step:1307/1775 train_time:65911ms step_avg:50.43ms
step:1308/1775 train_time:65998ms step_avg:50.46ms
step:1309/1775 train_time:66082ms step_avg:50.48ms
step:1310/1775 train_time:66168ms step_avg:50.51ms
step:1311/1775 train_time:66251ms step_avg:50.53ms
step:1312/1775 train_time:66339ms step_avg:50.56ms
step:1313/1775 train_time:66422ms step_avg:50.59ms
step:1314/1775 train_time:66509ms step_avg:50.62ms
step:1315/1775 train_time:66594ms step_avg:50.64ms
step:1316/1775 train_time:66680ms step_avg:50.67ms
step:1317/1775 train_time:66763ms step_avg:50.69ms
step:1318/1775 train_time:66849ms step_avg:50.72ms
step:1319/1775 train_time:66934ms step_avg:50.75ms
step:1320/1775 train_time:67019ms step_avg:50.77ms
step:1321/1775 train_time:67103ms step_avg:50.80ms
step:1322/1775 train_time:67190ms step_avg:50.82ms
step:1323/1775 train_time:67275ms step_avg:50.85ms
step:1324/1775 train_time:67361ms step_avg:50.88ms
step:1325/1775 train_time:67445ms step_avg:50.90ms
step:1326/1775 train_time:67532ms step_avg:50.93ms
step:1327/1775 train_time:67616ms step_avg:50.95ms
step:1328/1775 train_time:67701ms step_avg:50.98ms
step:1329/1775 train_time:67785ms step_avg:51.00ms
step:1330/1775 train_time:67872ms step_avg:51.03ms
step:1331/1775 train_time:67954ms step_avg:51.06ms
step:1332/1775 train_time:68040ms step_avg:51.08ms
step:1333/1775 train_time:68123ms step_avg:51.10ms
step:1334/1775 train_time:68210ms step_avg:51.13ms
step:1335/1775 train_time:68295ms step_avg:51.16ms
step:1336/1775 train_time:68381ms step_avg:51.18ms
step:1337/1775 train_time:68465ms step_avg:51.21ms
step:1338/1775 train_time:68552ms step_avg:51.23ms
step:1339/1775 train_time:68636ms step_avg:51.26ms
step:1340/1775 train_time:68722ms step_avg:51.28ms
step:1341/1775 train_time:68806ms step_avg:51.31ms
step:1342/1775 train_time:68892ms step_avg:51.34ms
step:1343/1775 train_time:68977ms step_avg:51.36ms
step:1344/1775 train_time:69062ms step_avg:51.39ms
step:1345/1775 train_time:69145ms step_avg:51.41ms
step:1346/1775 train_time:69233ms step_avg:51.44ms
step:1347/1775 train_time:69318ms step_avg:51.46ms
step:1348/1775 train_time:69403ms step_avg:51.49ms
step:1349/1775 train_time:69486ms step_avg:51.51ms
step:1350/1775 train_time:69573ms step_avg:51.54ms
step:1351/1775 train_time:69657ms step_avg:51.56ms
step:1352/1775 train_time:69744ms step_avg:51.59ms
step:1353/1775 train_time:69827ms step_avg:51.61ms
step:1354/1775 train_time:69914ms step_avg:51.64ms
step:1355/1775 train_time:69998ms step_avg:51.66ms
step:1356/1775 train_time:70084ms step_avg:51.68ms
step:1357/1775 train_time:70167ms step_avg:51.71ms
step:1358/1775 train_time:70254ms step_avg:51.73ms
step:1359/1775 train_time:70338ms step_avg:51.76ms
step:1360/1775 train_time:70423ms step_avg:51.78ms
step:1361/1775 train_time:70506ms step_avg:51.80ms
step:1362/1775 train_time:70593ms step_avg:51.83ms
step:1363/1775 train_time:70678ms step_avg:51.85ms
step:1364/1775 train_time:70762ms step_avg:51.88ms
step:1365/1775 train_time:70846ms step_avg:51.90ms
step:1366/1775 train_time:70933ms step_avg:51.93ms
step:1367/1775 train_time:71018ms step_avg:51.95ms
step:1368/1775 train_time:71103ms step_avg:51.98ms
step:1369/1775 train_time:71188ms step_avg:52.00ms
step:1370/1775 train_time:71275ms step_avg:52.03ms
step:1371/1775 train_time:71359ms step_avg:52.05ms
step:1372/1775 train_time:71445ms step_avg:52.07ms
step:1373/1775 train_time:71528ms step_avg:52.10ms
step:1374/1775 train_time:71614ms step_avg:52.12ms
step:1375/1775 train_time:71698ms step_avg:52.14ms
step:1376/1775 train_time:71784ms step_avg:52.17ms
step:1377/1775 train_time:71867ms step_avg:52.19ms
step:1378/1775 train_time:71954ms step_avg:52.22ms
step:1379/1775 train_time:72038ms step_avg:52.24ms
step:1380/1775 train_time:72124ms step_avg:52.26ms
step:1381/1775 train_time:72207ms step_avg:52.29ms
step:1382/1775 train_time:72294ms step_avg:52.31ms
step:1383/1775 train_time:72379ms step_avg:52.34ms
step:1384/1775 train_time:72465ms step_avg:52.36ms
step:1385/1775 train_time:72549ms step_avg:52.38ms
step:1386/1775 train_time:72635ms step_avg:52.41ms
step:1387/1775 train_time:72718ms step_avg:52.43ms
step:1388/1775 train_time:72803ms step_avg:52.45ms
step:1389/1775 train_time:72887ms step_avg:52.47ms
step:1390/1775 train_time:72975ms step_avg:52.50ms
step:1391/1775 train_time:73058ms step_avg:52.52ms
step:1392/1775 train_time:73144ms step_avg:52.55ms
step:1393/1775 train_time:73226ms step_avg:52.57ms
step:1394/1775 train_time:73314ms step_avg:52.59ms
step:1395/1775 train_time:73397ms step_avg:52.61ms
step:1396/1775 train_time:73484ms step_avg:52.64ms
step:1397/1775 train_time:73567ms step_avg:52.66ms
step:1398/1775 train_time:73654ms step_avg:52.69ms
step:1399/1775 train_time:73738ms step_avg:52.71ms
step:1400/1775 train_time:73823ms step_avg:52.73ms
step:1401/1775 train_time:73907ms step_avg:52.75ms
step:1402/1775 train_time:73994ms step_avg:52.78ms
step:1403/1775 train_time:74078ms step_avg:52.80ms
step:1404/1775 train_time:74163ms step_avg:52.82ms
step:1405/1775 train_time:74246ms step_avg:52.84ms
step:1406/1775 train_time:74333ms step_avg:52.87ms
step:1407/1775 train_time:74417ms step_avg:52.89ms
step:1408/1775 train_time:74504ms step_avg:52.91ms
step:1409/1775 train_time:74587ms step_avg:52.94ms
step:1410/1775 train_time:74674ms step_avg:52.96ms
step:1411/1775 train_time:74757ms step_avg:52.98ms
step:1412/1775 train_time:74844ms step_avg:53.01ms
step:1413/1775 train_time:74927ms step_avg:53.03ms
step:1414/1775 train_time:75014ms step_avg:53.05ms
step:1415/1775 train_time:75097ms step_avg:53.07ms
step:1416/1775 train_time:75184ms step_avg:53.10ms
step:1417/1775 train_time:75267ms step_avg:53.12ms
step:1418/1775 train_time:75353ms step_avg:53.14ms
step:1419/1775 train_time:75437ms step_avg:53.16ms
step:1420/1775 train_time:75524ms step_avg:53.19ms
step:1421/1775 train_time:75608ms step_avg:53.21ms
step:1422/1775 train_time:75695ms step_avg:53.23ms
step:1423/1775 train_time:75779ms step_avg:53.25ms
step:1424/1775 train_time:75865ms step_avg:53.28ms
step:1425/1775 train_time:75949ms step_avg:53.30ms
step:1426/1775 train_time:76036ms step_avg:53.32ms
step:1427/1775 train_time:76119ms step_avg:53.34ms
step:1428/1775 train_time:76204ms step_avg:53.36ms
step:1429/1775 train_time:76288ms step_avg:53.39ms
step:1430/1775 train_time:76376ms step_avg:53.41ms
step:1431/1775 train_time:76458ms step_avg:53.43ms
step:1432/1775 train_time:76546ms step_avg:53.45ms
step:1433/1775 train_time:76630ms step_avg:53.48ms
step:1434/1775 train_time:76717ms step_avg:53.50ms
step:1435/1775 train_time:76800ms step_avg:53.52ms
step:1436/1775 train_time:76886ms step_avg:53.54ms
step:1437/1775 train_time:76970ms step_avg:53.56ms
step:1438/1775 train_time:77056ms step_avg:53.59ms
step:1439/1775 train_time:77140ms step_avg:53.61ms
step:1440/1775 train_time:77225ms step_avg:53.63ms
step:1441/1775 train_time:77309ms step_avg:53.65ms
step:1442/1775 train_time:77396ms step_avg:53.67ms
step:1443/1775 train_time:77479ms step_avg:53.69ms
step:1444/1775 train_time:77565ms step_avg:53.72ms
step:1445/1775 train_time:77651ms step_avg:53.74ms
step:1446/1775 train_time:77737ms step_avg:53.76ms
step:1447/1775 train_time:77820ms step_avg:53.78ms
step:1448/1775 train_time:77906ms step_avg:53.80ms
step:1449/1775 train_time:77990ms step_avg:53.82ms
step:1450/1775 train_time:78077ms step_avg:53.85ms
step:1451/1775 train_time:78160ms step_avg:53.87ms
step:1452/1775 train_time:78246ms step_avg:53.89ms
step:1453/1775 train_time:78330ms step_avg:53.91ms
step:1454/1775 train_time:78418ms step_avg:53.93ms
step:1455/1775 train_time:78501ms step_avg:53.95ms
step:1456/1775 train_time:78587ms step_avg:53.97ms
step:1457/1775 train_time:78671ms step_avg:54.00ms
step:1458/1775 train_time:78757ms step_avg:54.02ms
step:1459/1775 train_time:78842ms step_avg:54.04ms
step:1460/1775 train_time:78928ms step_avg:54.06ms
step:1461/1775 train_time:79013ms step_avg:54.08ms
step:1462/1775 train_time:79099ms step_avg:54.10ms
step:1463/1775 train_time:79182ms step_avg:54.12ms
step:1464/1775 train_time:79269ms step_avg:54.15ms
step:1465/1775 train_time:79353ms step_avg:54.17ms
step:1466/1775 train_time:79438ms step_avg:54.19ms
step:1467/1775 train_time:79522ms step_avg:54.21ms
step:1468/1775 train_time:79609ms step_avg:54.23ms
step:1469/1775 train_time:79692ms step_avg:54.25ms
step:1470/1775 train_time:79779ms step_avg:54.27ms
step:1471/1775 train_time:79862ms step_avg:54.29ms
step:1472/1775 train_time:79949ms step_avg:54.31ms
step:1473/1775 train_time:80032ms step_avg:54.33ms
step:1474/1775 train_time:80118ms step_avg:54.35ms
step:1475/1775 train_time:80201ms step_avg:54.37ms
step:1476/1775 train_time:80287ms step_avg:54.40ms
step:1477/1775 train_time:80372ms step_avg:54.42ms
step:1478/1775 train_time:80457ms step_avg:54.44ms
step:1479/1775 train_time:80541ms step_avg:54.46ms
step:1480/1775 train_time:80627ms step_avg:54.48ms
step:1481/1775 train_time:80712ms step_avg:54.50ms
step:1482/1775 train_time:80799ms step_avg:54.52ms
step:1483/1775 train_time:80882ms step_avg:54.54ms
step:1484/1775 train_time:80969ms step_avg:54.56ms
step:1485/1775 train_time:81053ms step_avg:54.58ms
step:1486/1775 train_time:81139ms step_avg:54.60ms
step:1487/1775 train_time:81222ms step_avg:54.62ms
step:1488/1775 train_time:81309ms step_avg:54.64ms
step:1489/1775 train_time:81392ms step_avg:54.66ms
step:1490/1775 train_time:81479ms step_avg:54.68ms
step:1491/1775 train_time:81562ms step_avg:54.70ms
step:1492/1775 train_time:81649ms step_avg:54.72ms
step:1493/1775 train_time:81733ms step_avg:54.74ms
step:1494/1775 train_time:81820ms step_avg:54.77ms
step:1495/1775 train_time:81904ms step_avg:54.79ms
step:1496/1775 train_time:81992ms step_avg:54.81ms
step:1497/1775 train_time:82077ms step_avg:54.83ms
step:1498/1775 train_time:82162ms step_avg:54.85ms
step:1499/1775 train_time:82246ms step_avg:54.87ms
step:1500/1775 train_time:82331ms step_avg:54.89ms
step:1500/1775 val_loss:3.3792 train_time:82429ms step_avg:54.95ms
step:1501/1775 train_time:82449ms step_avg:54.93ms
step:1502/1775 train_time:82507ms step_avg:54.93ms
step:1503/1775 train_time:82598ms step_avg:54.96ms
step:1504/1775 train_time:82686ms step_avg:54.98ms
step:1505/1775 train_time:82769ms step_avg:55.00ms
step:1506/1775 train_time:82855ms step_avg:55.02ms
step:1507/1775 train_time:82937ms step_avg:55.03ms
step:1508/1775 train_time:83023ms step_avg:55.05ms
step:1509/1775 train_time:83105ms step_avg:55.07ms
step:1510/1775 train_time:83189ms step_avg:55.09ms
step:1511/1775 train_time:83271ms step_avg:55.11ms
step:1512/1775 train_time:83358ms step_avg:55.13ms
step:1513/1775 train_time:83444ms step_avg:55.15ms
step:1514/1775 train_time:83532ms step_avg:55.17ms
step:1515/1775 train_time:83620ms step_avg:55.19ms
step:1516/1775 train_time:83708ms step_avg:55.22ms
step:1517/1775 train_time:83792ms step_avg:55.24ms
step:1518/1775 train_time:83877ms step_avg:55.26ms
step:1519/1775 train_time:83960ms step_avg:55.27ms
step:1520/1775 train_time:84047ms step_avg:55.29ms
step:1521/1775 train_time:84129ms step_avg:55.31ms
step:1522/1775 train_time:84215ms step_avg:55.33ms
step:1523/1775 train_time:84297ms step_avg:55.35ms
step:1524/1775 train_time:84384ms step_avg:55.37ms
step:1525/1775 train_time:84468ms step_avg:55.39ms
step:1526/1775 train_time:84556ms step_avg:55.41ms
step:1527/1775 train_time:84642ms step_avg:55.43ms
step:1528/1775 train_time:84729ms step_avg:55.45ms
step:1529/1775 train_time:84813ms step_avg:55.47ms
step:1530/1775 train_time:84898ms step_avg:55.49ms
step:1531/1775 train_time:84982ms step_avg:55.51ms
step:1532/1775 train_time:85067ms step_avg:55.53ms
step:1533/1775 train_time:85150ms step_avg:55.54ms
step:1534/1775 train_time:85236ms step_avg:55.56ms
step:1535/1775 train_time:85320ms step_avg:55.58ms
step:1536/1775 train_time:85407ms step_avg:55.60ms
step:1537/1775 train_time:85492ms step_avg:55.62ms
step:1538/1775 train_time:85578ms step_avg:55.64ms
step:1539/1775 train_time:85664ms step_avg:55.66ms
step:1540/1775 train_time:85750ms step_avg:55.68ms
step:1541/1775 train_time:85834ms step_avg:55.70ms
step:1542/1775 train_time:85920ms step_avg:55.72ms
step:1543/1775 train_time:86003ms step_avg:55.74ms
step:1544/1775 train_time:86088ms step_avg:55.76ms
step:1545/1775 train_time:86170ms step_avg:55.77ms
step:1546/1775 train_time:86257ms step_avg:55.79ms
step:1547/1775 train_time:86341ms step_avg:55.81ms
step:1548/1775 train_time:86427ms step_avg:55.83ms
step:1549/1775 train_time:86511ms step_avg:55.85ms
step:1550/1775 train_time:86598ms step_avg:55.87ms
step:1551/1775 train_time:86684ms step_avg:55.89ms
step:1552/1775 train_time:86770ms step_avg:55.91ms
step:1553/1775 train_time:86853ms step_avg:55.93ms
step:1554/1775 train_time:86940ms step_avg:55.95ms
step:1555/1775 train_time:87024ms step_avg:55.96ms
step:1556/1775 train_time:87110ms step_avg:55.98ms
step:1557/1775 train_time:87192ms step_avg:56.00ms
step:1558/1775 train_time:87279ms step_avg:56.02ms
step:1559/1775 train_time:87363ms step_avg:56.04ms
step:1560/1775 train_time:87449ms step_avg:56.06ms
step:1561/1775 train_time:87532ms step_avg:56.07ms
step:1562/1775 train_time:87620ms step_avg:56.09ms
step:1563/1775 train_time:87705ms step_avg:56.11ms
step:1564/1775 train_time:87791ms step_avg:56.13ms
step:1565/1775 train_time:87873ms step_avg:56.15ms
step:1566/1775 train_time:87960ms step_avg:56.17ms
step:1567/1775 train_time:88044ms step_avg:56.19ms
step:1568/1775 train_time:88129ms step_avg:56.20ms
step:1569/1775 train_time:88213ms step_avg:56.22ms
step:1570/1775 train_time:88301ms step_avg:56.24ms
step:1571/1775 train_time:88385ms step_avg:56.26ms
step:1572/1775 train_time:88470ms step_avg:56.28ms
step:1573/1775 train_time:88554ms step_avg:56.30ms
step:1574/1775 train_time:88642ms step_avg:56.32ms
step:1575/1775 train_time:88726ms step_avg:56.33ms
step:1576/1775 train_time:88812ms step_avg:56.35ms
step:1577/1775 train_time:88896ms step_avg:56.37ms
step:1578/1775 train_time:88983ms step_avg:56.39ms
step:1579/1775 train_time:89066ms step_avg:56.41ms
step:1580/1775 train_time:89152ms step_avg:56.43ms
step:1581/1775 train_time:89235ms step_avg:56.44ms
step:1582/1775 train_time:89321ms step_avg:56.46ms
step:1583/1775 train_time:89406ms step_avg:56.48ms
step:1584/1775 train_time:89491ms step_avg:56.50ms
step:1585/1775 train_time:89574ms step_avg:56.51ms
step:1586/1775 train_time:89663ms step_avg:56.53ms
step:1587/1775 train_time:89746ms step_avg:56.55ms
step:1588/1775 train_time:89833ms step_avg:56.57ms
step:1589/1775 train_time:89918ms step_avg:56.59ms
step:1590/1775 train_time:90004ms step_avg:56.61ms
step:1591/1775 train_time:90086ms step_avg:56.62ms
step:1592/1775 train_time:90173ms step_avg:56.64ms
step:1593/1775 train_time:90257ms step_avg:56.66ms
step:1594/1775 train_time:90344ms step_avg:56.68ms
step:1595/1775 train_time:90427ms step_avg:56.69ms
step:1596/1775 train_time:90513ms step_avg:56.71ms
step:1597/1775 train_time:90597ms step_avg:56.73ms
step:1598/1775 train_time:90685ms step_avg:56.75ms
step:1599/1775 train_time:90768ms step_avg:56.77ms
step:1600/1775 train_time:90855ms step_avg:56.78ms
step:1601/1775 train_time:90938ms step_avg:56.80ms
step:1602/1775 train_time:91024ms step_avg:56.82ms
step:1603/1775 train_time:91107ms step_avg:56.84ms
step:1604/1775 train_time:91193ms step_avg:56.85ms
step:1605/1775 train_time:91278ms step_avg:56.87ms
step:1606/1775 train_time:91363ms step_avg:56.89ms
step:1607/1775 train_time:91446ms step_avg:56.91ms
step:1608/1775 train_time:91533ms step_avg:56.92ms
step:1609/1775 train_time:91617ms step_avg:56.94ms
step:1610/1775 train_time:91704ms step_avg:56.96ms
step:1611/1775 train_time:91787ms step_avg:56.98ms
step:1612/1775 train_time:91874ms step_avg:56.99ms
step:1613/1775 train_time:91958ms step_avg:57.01ms
step:1614/1775 train_time:92043ms step_avg:57.03ms
step:1615/1775 train_time:92127ms step_avg:57.04ms
step:1616/1775 train_time:92213ms step_avg:57.06ms
step:1617/1775 train_time:92297ms step_avg:57.08ms
step:1618/1775 train_time:92385ms step_avg:57.10ms
step:1619/1775 train_time:92467ms step_avg:57.11ms
step:1620/1775 train_time:92555ms step_avg:57.13ms
step:1621/1775 train_time:92638ms step_avg:57.15ms
step:1622/1775 train_time:92725ms step_avg:57.17ms
step:1623/1775 train_time:92808ms step_avg:57.18ms
step:1624/1775 train_time:92895ms step_avg:57.20ms
step:1625/1775 train_time:92978ms step_avg:57.22ms
step:1626/1775 train_time:93064ms step_avg:57.24ms
step:1627/1775 train_time:93148ms step_avg:57.25ms
step:1628/1775 train_time:93235ms step_avg:57.27ms
step:1629/1775 train_time:93321ms step_avg:57.29ms
step:1630/1775 train_time:93407ms step_avg:57.30ms
step:1631/1775 train_time:93491ms step_avg:57.32ms
step:1632/1775 train_time:93577ms step_avg:57.34ms
step:1633/1775 train_time:93661ms step_avg:57.36ms
step:1634/1775 train_time:93747ms step_avg:57.37ms
step:1635/1775 train_time:93832ms step_avg:57.39ms
step:1636/1775 train_time:93918ms step_avg:57.41ms
step:1637/1775 train_time:94001ms step_avg:57.42ms
step:1638/1775 train_time:94088ms step_avg:57.44ms
step:1639/1775 train_time:94171ms step_avg:57.46ms
step:1640/1775 train_time:94259ms step_avg:57.48ms
step:1641/1775 train_time:94343ms step_avg:57.49ms
step:1642/1775 train_time:94429ms step_avg:57.51ms
step:1643/1775 train_time:94512ms step_avg:57.52ms
step:1644/1775 train_time:94598ms step_avg:57.54ms
step:1645/1775 train_time:94683ms step_avg:57.56ms
step:1646/1775 train_time:94768ms step_avg:57.57ms
step:1647/1775 train_time:94851ms step_avg:57.59ms
step:1648/1775 train_time:94938ms step_avg:57.61ms
step:1649/1775 train_time:95023ms step_avg:57.62ms
step:1650/1775 train_time:95109ms step_avg:57.64ms
step:1651/1775 train_time:95193ms step_avg:57.66ms
step:1652/1775 train_time:95278ms step_avg:57.67ms
step:1653/1775 train_time:95362ms step_avg:57.69ms
step:1654/1775 train_time:95449ms step_avg:57.71ms
step:1655/1775 train_time:95532ms step_avg:57.72ms
step:1656/1775 train_time:95620ms step_avg:57.74ms
step:1657/1775 train_time:95703ms step_avg:57.76ms
step:1658/1775 train_time:95789ms step_avg:57.77ms
step:1659/1775 train_time:95873ms step_avg:57.79ms
step:1660/1775 train_time:95959ms step_avg:57.81ms
step:1661/1775 train_time:96043ms step_avg:57.82ms
step:1662/1775 train_time:96129ms step_avg:57.84ms
step:1663/1775 train_time:96213ms step_avg:57.85ms
step:1664/1775 train_time:96299ms step_avg:57.87ms
step:1665/1775 train_time:96384ms step_avg:57.89ms
step:1666/1775 train_time:96469ms step_avg:57.90ms
step:1667/1775 train_time:96552ms step_avg:57.92ms
step:1668/1775 train_time:96639ms step_avg:57.94ms
step:1669/1775 train_time:96723ms step_avg:57.95ms
step:1670/1775 train_time:96808ms step_avg:57.97ms
step:1671/1775 train_time:96892ms step_avg:57.98ms
step:1672/1775 train_time:96981ms step_avg:58.00ms
step:1673/1775 train_time:97065ms step_avg:58.02ms
step:1674/1775 train_time:97151ms step_avg:58.04ms
step:1675/1775 train_time:97234ms step_avg:58.05ms
step:1676/1775 train_time:97322ms step_avg:58.07ms
step:1677/1775 train_time:97405ms step_avg:58.08ms
step:1678/1775 train_time:97490ms step_avg:58.10ms
step:1679/1775 train_time:97574ms step_avg:58.11ms
step:1680/1775 train_time:97660ms step_avg:58.13ms
step:1681/1775 train_time:97744ms step_avg:58.15ms
step:1682/1775 train_time:97829ms step_avg:58.16ms
step:1683/1775 train_time:97913ms step_avg:58.18ms
step:1684/1775 train_time:98001ms step_avg:58.20ms
step:1685/1775 train_time:98085ms step_avg:58.21ms
step:1686/1775 train_time:98171ms step_avg:58.23ms
step:1687/1775 train_time:98254ms step_avg:58.24ms
step:1688/1775 train_time:98340ms step_avg:58.26ms
step:1689/1775 train_time:98425ms step_avg:58.27ms
step:1690/1775 train_time:98511ms step_avg:58.29ms
step:1691/1775 train_time:98595ms step_avg:58.31ms
step:1692/1775 train_time:98682ms step_avg:58.32ms
step:1693/1775 train_time:98765ms step_avg:58.34ms
step:1694/1775 train_time:98851ms step_avg:58.35ms
step:1695/1775 train_time:98935ms step_avg:58.37ms
step:1696/1775 train_time:99022ms step_avg:58.39ms
step:1697/1775 train_time:99105ms step_avg:58.40ms
step:1698/1775 train_time:99192ms step_avg:58.42ms
step:1699/1775 train_time:99275ms step_avg:58.43ms
step:1700/1775 train_time:99362ms step_avg:58.45ms
step:1701/1775 train_time:99445ms step_avg:58.46ms
step:1702/1775 train_time:99532ms step_avg:58.48ms
step:1703/1775 train_time:99616ms step_avg:58.49ms
step:1704/1775 train_time:99703ms step_avg:58.51ms
step:1705/1775 train_time:99786ms step_avg:58.53ms
step:1706/1775 train_time:99871ms step_avg:58.54ms
step:1707/1775 train_time:99955ms step_avg:58.56ms
step:1708/1775 train_time:100042ms step_avg:58.57ms
step:1709/1775 train_time:100124ms step_avg:58.59ms
step:1710/1775 train_time:100211ms step_avg:58.60ms
step:1711/1775 train_time:100294ms step_avg:58.62ms
step:1712/1775 train_time:100382ms step_avg:58.63ms
step:1713/1775 train_time:100465ms step_avg:58.65ms
step:1714/1775 train_time:100551ms step_avg:58.66ms
step:1715/1775 train_time:100635ms step_avg:58.68ms
step:1716/1775 train_time:100722ms step_avg:58.70ms
step:1717/1775 train_time:100804ms step_avg:58.71ms
step:1718/1775 train_time:100892ms step_avg:58.73ms
step:1719/1775 train_time:100976ms step_avg:58.74ms
step:1720/1775 train_time:101061ms step_avg:58.76ms
step:1721/1775 train_time:101144ms step_avg:58.77ms
step:1722/1775 train_time:101231ms step_avg:58.79ms
step:1723/1775 train_time:101316ms step_avg:58.80ms
step:1724/1775 train_time:101402ms step_avg:58.82ms
step:1725/1775 train_time:101486ms step_avg:58.83ms
step:1726/1775 train_time:101571ms step_avg:58.85ms
step:1727/1775 train_time:101655ms step_avg:58.86ms
step:1728/1775 train_time:101742ms step_avg:58.88ms
step:1729/1775 train_time:101826ms step_avg:58.89ms
step:1730/1775 train_time:101912ms step_avg:58.91ms
step:1731/1775 train_time:101996ms step_avg:58.92ms
step:1732/1775 train_time:102084ms step_avg:58.94ms
step:1733/1775 train_time:102167ms step_avg:58.95ms
step:1734/1775 train_time:102254ms step_avg:58.97ms
step:1735/1775 train_time:102337ms step_avg:58.98ms
step:1736/1775 train_time:102428ms step_avg:59.00ms
step:1737/1775 train_time:102513ms step_avg:59.02ms
step:1738/1775 train_time:102599ms step_avg:59.03ms
step:1739/1775 train_time:102684ms step_avg:59.05ms
step:1740/1775 train_time:102770ms step_avg:59.06ms
step:1741/1775 train_time:102854ms step_avg:59.08ms
step:1742/1775 train_time:102942ms step_avg:59.09ms
step:1743/1775 train_time:103026ms step_avg:59.11ms
step:1744/1775 train_time:103112ms step_avg:59.12ms
step:1745/1775 train_time:103196ms step_avg:59.14ms
step:1746/1775 train_time:103283ms step_avg:59.15ms
step:1747/1775 train_time:103367ms step_avg:59.17ms
step:1748/1775 train_time:103454ms step_avg:59.18ms
step:1749/1775 train_time:103538ms step_avg:59.20ms
step:1750/1775 train_time:103625ms step_avg:59.21ms
step:1750/1775 val_loss:3.2870 train_time:103723ms step_avg:59.27ms
step:1751/1775 train_time:103742ms step_avg:59.25ms
step:1752/1775 train_time:103798ms step_avg:59.25ms
step:1753/1775 train_time:103889ms step_avg:59.26ms
step:1754/1775 train_time:103977ms step_avg:59.28ms
step:1755/1775 train_time:104063ms step_avg:59.30ms
step:1756/1775 train_time:104149ms step_avg:59.31ms
step:1757/1775 train_time:104232ms step_avg:59.32ms
step:1758/1775 train_time:104316ms step_avg:59.34ms
step:1759/1775 train_time:104400ms step_avg:59.35ms
step:1760/1775 train_time:104486ms step_avg:59.37ms
step:1761/1775 train_time:104568ms step_avg:59.38ms
step:1762/1775 train_time:104654ms step_avg:59.39ms
step:1763/1775 train_time:104741ms step_avg:59.41ms
step:1764/1775 train_time:104832ms step_avg:59.43ms
step:1765/1775 train_time:104920ms step_avg:59.44ms
step:1766/1775 train_time:105008ms step_avg:59.46ms
step:1767/1775 train_time:105093ms step_avg:59.48ms
step:1768/1775 train_time:105180ms step_avg:59.49ms
step:1769/1775 train_time:105263ms step_avg:59.50ms
step:1770/1775 train_time:105350ms step_avg:59.52ms
step:1771/1775 train_time:105432ms step_avg:59.53ms
step:1772/1775 train_time:105517ms step_avg:59.55ms
step:1773/1775 train_time:105600ms step_avg:59.56ms
step:1774/1775 train_time:105687ms step_avg:59.58ms
step:1775/1775 train_time:105772ms step_avg:59.59ms
step:1775/1775 val_loss:3.2807 train_time:105875ms step_avg:59.65ms
peak memory allocated: 29148 MiB reserved: 45178 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977

        # This call computes relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# Fused Softcapped Cross Entropy

@triton.jit
def fused_softcapped_entropy_fwd_kernel(
    logits_ptr, losses_ptr, lse_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)
    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    
    max_val = -float('inf')
    sum_exp = 0.0
    
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)
        z = A * tl.sigmoid((val + B) / C)
        z = tl.where(mask, z, -float('inf'))
        curr_max = tl.max(z, axis=0)
        new_max = tl.maximum(max_val, curr_max)
        sum_exp = sum_exp * tl.exp(max_val - new_max) + tl.sum(tl.exp(z - new_max), axis=0)
        max_val = new_max
    
    lse = max_val + tl.log(sum_exp)
    tl.store(lse_ptr + row_idx, lse)
    
    total_loss = 0.0
    for k in range(n_predict):
        target_idx = row_idx + k
        if target_idx < n_rows:
            weight = tl.load(mtp_weights_ptr + k)
            if weight > 0:
                target = tl.load(targets_ptr + target_idx).to(tl.int32)
                if target >= 0 and target < n_cols:
                    val_target = tl.load(logits_row_ptr + target).to(tl.float32)
                    z_target = A * tl.sigmoid((val_target + B) / C)
                    total_loss += weight * (lse - z_target)
    
    tl.store(losses_ptr + row_idx, total_loss)

@triton.jit
def fused_softcapped_entropy_bwd_kernel(
    grad_input_ptr, grad_output_ptr, lse_ptr, logits_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v, stride_grad_n, stride_grad_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)

    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    grad_row_ptr = grad_input_ptr + row_idx * stride_grad_n
    
    lse = tl.load(lse_ptr + row_idx)
    grad_loss = tl.load(grad_output_ptr + row_idx)
    
    S_w = 0.0
    for k in range(n_predict):
        if row_idx + k < n_rows:
            S_w += tl.load(mtp_weights_ptr + k)
            
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=0.0).to(tl.float32)
        u = (val + B) / C
        sigmoid_u = tl.sigmoid(u)
        z = A * sigmoid_u
        p = tl.exp(z - lse)
        
        term1 = S_w * p
        term2 = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
        for k in range(n_predict):
            if row_idx + k < n_rows:
                target = tl.load(targets_ptr + row_idx + k).to(tl.int32)
                weight = tl.load(mtp_weights_ptr + k)
                term2 += tl.where(cols == target, weight, 0.0)
        
        grad_z = grad_loss * (term1 - term2)
        dz_dx = (1.0 / C) * z * (1.0 - sigmoid_u)
        grad_x = grad_z * dz_dx
        tl.store(grad_row_ptr + cols, grad_x.to(tl.bfloat16), mask=mask)

class FusedSoftcappedCrossEntropy(torch.autograd.Function):
    @staticmethod
    def forward(ctx, logits, targets, mtp_weights, A=23.0, B=5.0, C=7.5):
        n_rows, n_cols = logits.shape
        if mtp_weights is None:
             mtp_weights = torch.tensor([1.0], device=logits.device, dtype=torch.float32)
        n_predict = mtp_weights.shape[0]

        losses = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        lse = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        
        logits = logits.contiguous()
        targets = targets.contiguous()
        mtp_weights = mtp_weights.contiguous()

        grid = (n_rows,)
        fused_softcapped_entropy_fwd_kernel[grid](
            logits, losses, lse, targets, mtp_weights,
            logits.stride(0), logits.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        
        ctx.save_for_backward(logits, targets, mtp_weights, lse)
        ctx.params = (A, B, C)
        return losses

    @staticmethod
    def backward(ctx, grad_output):
        logits, targets, mtp_weights, lse = ctx.saved_tensors
        A, B, C = ctx.params
        n_rows, n_cols = logits.shape
        n_predict = mtp_weights.shape[0]
        
        grad_input = torch.empty((n_rows, n_cols), dtype=torch.bfloat16, device=logits.device)
        grad_output = grad_output.contiguous()
        
        grid = (n_rows,)
        fused_softcapped_entropy_bwd_kernel[grid](
            grad_input, grad_output, lse, logits, targets, mtp_weights,
            logits.stride(0), logits.stride(1), grad_input.stride(0), grad_input.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        return grad_input, None, None, None, None, None

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if self.training:
            losses = FusedSoftcappedCrossEntropy.apply(logits.view(-1, logits.size(-1)), target_seq, mtp_weights)
            loss = losses.sum()
        else:
            logits = 23 * torch.sigmoid((logits + 5) / 7.5)
            logits_for_loss = logits.float()
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Jan  8 2026, 06:52:19) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan 15 22:08:19 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   43C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   34C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   43C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   41C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   40C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    280285      C   /usr/bin/python3                             1510MiB |
|    1   N/A  N/A    280286      C   /usr/bin/python3                             1510MiB |
|    2   N/A  N/A    280287      C   /usr/bin/python3                             1510MiB |
|    3   N/A  N/A    280288      C   /usr/bin/python3                             1510MiB |
|    4   N/A  N/A    280289      C   /usr/bin/python3                             1510MiB |
|    5   N/A  N/A    280290      C   /usr/bin/python3                             1510MiB |
|    6   N/A  N/A    280291      C   /usr/bin/python3                             1510MiB |
|    7   N/A  N/A    280292      C   /usr/bin/python3                             1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8308 train_time:0ms step_avg:0.03ms
step:1/1775 train_time:73ms step_avg:73.15ms
step:2/1775 train_time:98ms step_avg:48.79ms
step:3/1775 train_time:118ms step_avg:39.47ms
step:4/1775 train_time:146ms step_avg:36.38ms
step:5/1775 train_time:176ms step_avg:35.21ms
step:6/1775 train_time:267ms step_avg:44.54ms
step:7/1775 train_time:286ms step_avg:40.81ms
step:8/1775 train_time:312ms step_avg:39.00ms
step:9/1775 train_time:343ms step_avg:38.10ms
step:10/1775 train_time:376ms step_avg:37.57ms
step:11/1775 train_time:407ms step_avg:36.96ms
step:12/1775 train_time:440ms step_avg:36.66ms
step:13/1775 train_time:471ms step_avg:36.26ms
step:14/1775 train_time:504ms step_avg:36.02ms
step:15/1775 train_time:535ms step_avg:35.69ms
step:16/1775 train_time:568ms step_avg:35.53ms
step:17/1775 train_time:600ms step_avg:35.28ms
step:18/1775 train_time:633ms step_avg:35.19ms
step:19/1775 train_time:664ms step_avg:34.97ms
step:20/1775 train_time:698ms step_avg:34.88ms
step:21/1775 train_time:729ms step_avg:34.72ms
step:22/1775 train_time:762ms step_avg:34.65ms
step:23/1775 train_time:793ms step_avg:34.48ms
step:24/1775 train_time:826ms step_avg:34.42ms
step:25/1775 train_time:857ms step_avg:34.29ms
step:26/1775 train_time:890ms step_avg:34.25ms
step:27/1775 train_time:922ms step_avg:34.13ms
step:28/1775 train_time:955ms step_avg:34.09ms
step:29/1775 train_time:986ms step_avg:33.99ms
step:30/1775 train_time:1020ms step_avg:34.00ms
step:31/1775 train_time:1052ms step_avg:33.93ms
step:32/1775 train_time:1085ms step_avg:33.90ms
step:33/1775 train_time:1117ms step_avg:33.84ms
step:34/1775 train_time:1151ms step_avg:33.86ms
step:35/1775 train_time:1184ms step_avg:33.82ms
step:36/1775 train_time:1218ms step_avg:33.85ms
step:37/1775 train_time:1251ms step_avg:33.80ms
step:38/1775 train_time:1285ms step_avg:33.82ms
step:39/1775 train_time:1317ms step_avg:33.76ms
step:40/1775 train_time:1350ms step_avg:33.76ms
step:41/1775 train_time:1382ms step_avg:33.71ms
step:42/1775 train_time:1416ms step_avg:33.71ms
step:43/1775 train_time:1448ms step_avg:33.67ms
step:44/1775 train_time:1482ms step_avg:33.67ms
step:45/1775 train_time:1513ms step_avg:33.61ms
step:46/1775 train_time:1546ms step_avg:33.61ms
step:47/1775 train_time:1577ms step_avg:33.56ms
step:48/1775 train_time:1611ms step_avg:33.57ms
step:49/1775 train_time:1642ms step_avg:33.51ms
step:50/1775 train_time:1675ms step_avg:33.51ms
step:51/1775 train_time:1707ms step_avg:33.47ms
step:52/1775 train_time:1741ms step_avg:33.47ms
step:53/1775 train_time:1772ms step_avg:33.44ms
step:54/1775 train_time:1805ms step_avg:33.43ms
step:55/1775 train_time:1836ms step_avg:33.39ms
step:56/1775 train_time:1870ms step_avg:33.39ms
step:57/1775 train_time:1901ms step_avg:33.35ms
step:58/1775 train_time:1935ms step_avg:33.36ms
step:59/1775 train_time:1966ms step_avg:33.33ms
step:60/1775 train_time:2000ms step_avg:33.33ms
step:61/1775 train_time:2031ms step_avg:33.30ms
step:62/1775 train_time:2064ms step_avg:33.29ms
step:63/1775 train_time:2096ms step_avg:33.26ms
step:64/1775 train_time:2129ms step_avg:33.27ms
step:65/1775 train_time:2161ms step_avg:33.25ms
step:66/1775 train_time:2195ms step_avg:33.25ms
step:67/1775 train_time:2227ms step_avg:33.23ms
step:68/1775 train_time:2261ms step_avg:33.25ms
step:69/1775 train_time:2293ms step_avg:33.23ms
step:70/1775 train_time:2326ms step_avg:33.23ms
step:71/1775 train_time:2358ms step_avg:33.21ms
step:72/1775 train_time:2392ms step_avg:33.22ms
step:73/1775 train_time:2423ms step_avg:33.19ms
step:74/1775 train_time:2456ms step_avg:33.20ms
step:75/1775 train_time:2488ms step_avg:33.17ms
step:76/1775 train_time:2522ms step_avg:33.18ms
step:77/1775 train_time:2553ms step_avg:33.16ms
step:78/1775 train_time:2587ms step_avg:33.16ms
step:79/1775 train_time:2618ms step_avg:33.14ms
step:80/1775 train_time:2651ms step_avg:33.14ms
step:81/1775 train_time:2683ms step_avg:33.12ms
step:82/1775 train_time:2716ms step_avg:33.13ms
step:83/1775 train_time:2748ms step_avg:33.11ms
step:84/1775 train_time:2782ms step_avg:33.12ms
step:85/1775 train_time:2813ms step_avg:33.09ms
step:86/1775 train_time:2846ms step_avg:33.09ms
step:87/1775 train_time:2877ms step_avg:33.07ms
step:88/1775 train_time:2911ms step_avg:33.08ms
step:89/1775 train_time:2942ms step_avg:33.06ms
step:90/1775 train_time:2976ms step_avg:33.07ms
step:91/1775 train_time:3007ms step_avg:33.04ms
step:92/1775 train_time:3040ms step_avg:33.05ms
step:93/1775 train_time:3072ms step_avg:33.03ms
step:94/1775 train_time:3105ms step_avg:33.03ms
step:95/1775 train_time:3137ms step_avg:33.02ms
step:96/1775 train_time:3171ms step_avg:33.03ms
step:97/1775 train_time:3203ms step_avg:33.02ms
step:98/1775 train_time:3237ms step_avg:33.03ms
step:99/1775 train_time:3269ms step_avg:33.02ms
step:100/1775 train_time:3302ms step_avg:33.02ms
step:101/1775 train_time:3333ms step_avg:33.00ms
step:102/1775 train_time:3367ms step_avg:33.01ms
step:103/1775 train_time:3399ms step_avg:33.00ms
step:104/1775 train_time:3432ms step_avg:33.00ms
step:105/1775 train_time:3464ms step_avg:32.99ms
step:106/1775 train_time:3497ms step_avg:32.99ms
step:107/1775 train_time:3529ms step_avg:32.98ms
step:108/1775 train_time:3562ms step_avg:32.98ms
step:109/1775 train_time:3594ms step_avg:32.97ms
step:110/1775 train_time:3627ms step_avg:32.98ms
step:111/1775 train_time:3659ms step_avg:32.96ms
step:112/1775 train_time:3692ms step_avg:32.97ms
step:113/1775 train_time:3723ms step_avg:32.95ms
step:114/1775 train_time:3757ms step_avg:32.96ms
step:115/1775 train_time:3788ms step_avg:32.94ms
step:116/1775 train_time:3821ms step_avg:32.94ms
step:117/1775 train_time:3852ms step_avg:32.93ms
step:118/1775 train_time:3886ms step_avg:32.93ms
step:119/1775 train_time:3917ms step_avg:32.92ms
step:120/1775 train_time:3951ms step_avg:32.92ms
step:121/1775 train_time:3983ms step_avg:32.91ms
step:122/1775 train_time:4016ms step_avg:32.92ms
step:123/1775 train_time:4048ms step_avg:32.91ms
step:124/1775 train_time:4082ms step_avg:32.92ms
step:125/1775 train_time:4113ms step_avg:32.91ms
step:126/1775 train_time:4147ms step_avg:32.91ms
step:127/1775 train_time:4178ms step_avg:32.90ms
step:128/1775 train_time:4212ms step_avg:32.91ms
step:129/1775 train_time:4244ms step_avg:32.90ms
step:130/1775 train_time:4277ms step_avg:32.90ms
step:131/1775 train_time:4309ms step_avg:32.89ms
step:132/1775 train_time:4342ms step_avg:32.90ms
step:133/1775 train_time:4373ms step_avg:32.88ms
step:134/1775 train_time:4407ms step_avg:32.89ms
step:135/1775 train_time:4439ms step_avg:32.88ms
step:136/1775 train_time:4473ms step_avg:32.89ms
step:137/1775 train_time:4504ms step_avg:32.87ms
step:138/1775 train_time:4537ms step_avg:32.88ms
step:139/1775 train_time:4569ms step_avg:32.87ms
step:140/1775 train_time:4602ms step_avg:32.87ms
step:141/1775 train_time:4633ms step_avg:32.86ms
step:142/1775 train_time:4666ms step_avg:32.86ms
step:143/1775 train_time:4698ms step_avg:32.85ms
step:144/1775 train_time:4732ms step_avg:32.86ms
step:145/1775 train_time:4763ms step_avg:32.85ms
step:146/1775 train_time:4796ms step_avg:32.85ms
step:147/1775 train_time:4828ms step_avg:32.84ms
step:148/1775 train_time:4861ms step_avg:32.85ms
step:149/1775 train_time:4893ms step_avg:32.84ms
step:150/1775 train_time:4926ms step_avg:32.84ms
step:151/1775 train_time:4957ms step_avg:32.83ms
step:152/1775 train_time:4991ms step_avg:32.83ms
step:153/1775 train_time:5022ms step_avg:32.82ms
step:154/1775 train_time:5055ms step_avg:32.83ms
step:155/1775 train_time:5087ms step_avg:32.82ms
step:156/1775 train_time:5121ms step_avg:32.82ms
step:157/1775 train_time:5152ms step_avg:32.82ms
step:158/1775 train_time:5185ms step_avg:32.82ms
step:159/1775 train_time:5217ms step_avg:32.81ms
step:160/1775 train_time:5250ms step_avg:32.81ms
step:161/1775 train_time:5281ms step_avg:32.80ms
step:162/1775 train_time:5315ms step_avg:32.81ms
step:163/1775 train_time:5346ms step_avg:32.80ms
step:164/1775 train_time:5380ms step_avg:32.80ms
step:165/1775 train_time:5411ms step_avg:32.80ms
step:166/1775 train_time:5445ms step_avg:32.80ms
step:167/1775 train_time:5476ms step_avg:32.79ms
step:168/1775 train_time:5510ms step_avg:32.79ms
step:169/1775 train_time:5542ms step_avg:32.79ms
step:170/1775 train_time:5575ms step_avg:32.79ms
step:171/1775 train_time:5606ms step_avg:32.79ms
step:172/1775 train_time:5640ms step_avg:32.79ms
step:173/1775 train_time:5672ms step_avg:32.78ms
step:174/1775 train_time:5705ms step_avg:32.79ms
step:175/1775 train_time:5736ms step_avg:32.77ms
step:176/1775 train_time:5769ms step_avg:32.78ms
step:177/1775 train_time:5801ms step_avg:32.77ms
step:178/1775 train_time:5834ms step_avg:32.78ms
step:179/1775 train_time:5866ms step_avg:32.77ms
step:180/1775 train_time:5899ms step_avg:32.77ms
step:181/1775 train_time:5931ms step_avg:32.77ms
step:182/1775 train_time:5964ms step_avg:32.77ms
step:183/1775 train_time:5995ms step_avg:32.76ms
step:184/1775 train_time:6029ms step_avg:32.77ms
step:185/1775 train_time:6060ms step_avg:32.76ms
step:186/1775 train_time:6093ms step_avg:32.76ms
step:187/1775 train_time:6125ms step_avg:32.75ms
step:188/1775 train_time:6158ms step_avg:32.76ms
step:189/1775 train_time:6189ms step_avg:32.75ms
step:190/1775 train_time:6223ms step_avg:32.75ms
step:191/1775 train_time:6254ms step_avg:32.75ms
step:192/1775 train_time:6288ms step_avg:32.75ms
step:193/1775 train_time:6319ms step_avg:32.74ms
step:194/1775 train_time:6353ms step_avg:32.75ms
step:195/1775 train_time:6384ms step_avg:32.74ms
step:196/1775 train_time:6418ms step_avg:32.75ms
step:197/1775 train_time:6449ms step_avg:32.74ms
step:198/1775 train_time:6483ms step_avg:32.74ms
step:199/1775 train_time:6514ms step_avg:32.73ms
step:200/1775 train_time:6547ms step_avg:32.74ms
step:201/1775 train_time:6579ms step_avg:32.73ms
step:202/1775 train_time:6613ms step_avg:32.74ms
step:203/1775 train_time:6644ms step_avg:32.73ms
step:204/1775 train_time:6678ms step_avg:32.73ms
step:205/1775 train_time:6709ms step_avg:32.73ms
step:206/1775 train_time:6743ms step_avg:32.73ms
step:207/1775 train_time:6774ms step_avg:32.72ms
step:208/1775 train_time:6807ms step_avg:32.73ms
step:209/1775 train_time:6839ms step_avg:32.72ms
step:210/1775 train_time:6872ms step_avg:32.73ms
step:211/1775 train_time:6903ms step_avg:32.72ms
step:212/1775 train_time:6937ms step_avg:32.72ms
step:213/1775 train_time:6968ms step_avg:32.71ms
step:214/1775 train_time:7001ms step_avg:32.72ms
step:215/1775 train_time:7032ms step_avg:32.71ms
step:216/1775 train_time:7066ms step_avg:32.71ms
step:217/1775 train_time:7097ms step_avg:32.71ms
step:218/1775 train_time:7131ms step_avg:32.71ms
step:219/1775 train_time:7162ms step_avg:32.71ms
step:220/1775 train_time:7197ms step_avg:32.71ms
step:221/1775 train_time:7228ms step_avg:32.71ms
step:222/1775 train_time:7261ms step_avg:32.71ms
step:223/1775 train_time:7293ms step_avg:32.70ms
step:224/1775 train_time:7326ms step_avg:32.71ms
step:225/1775 train_time:7358ms step_avg:32.70ms
step:226/1775 train_time:7391ms step_avg:32.71ms
step:227/1775 train_time:7423ms step_avg:32.70ms
step:228/1775 train_time:7456ms step_avg:32.70ms
step:229/1775 train_time:7487ms step_avg:32.70ms
step:230/1775 train_time:7521ms step_avg:32.70ms
step:231/1775 train_time:7552ms step_avg:32.69ms
step:232/1775 train_time:7585ms step_avg:32.69ms
step:233/1775 train_time:7617ms step_avg:32.69ms
step:234/1775 train_time:7650ms step_avg:32.69ms
step:235/1775 train_time:7682ms step_avg:32.69ms
step:236/1775 train_time:7716ms step_avg:32.69ms
step:237/1775 train_time:7748ms step_avg:32.69ms
step:238/1775 train_time:7781ms step_avg:32.69ms
step:239/1775 train_time:7812ms step_avg:32.69ms
step:240/1775 train_time:7845ms step_avg:32.69ms
step:241/1775 train_time:7876ms step_avg:32.68ms
step:242/1775 train_time:7910ms step_avg:32.68ms
step:243/1775 train_time:7942ms step_avg:32.68ms
step:244/1775 train_time:7975ms step_avg:32.68ms
step:245/1775 train_time:8006ms step_avg:32.68ms
step:246/1775 train_time:8040ms step_avg:32.68ms
step:247/1775 train_time:8071ms step_avg:32.68ms
step:248/1775 train_time:8105ms step_avg:32.68ms
step:249/1775 train_time:8137ms step_avg:32.68ms
step:250/1775 train_time:8171ms step_avg:32.68ms
step:250/1775 val_loss:4.6188 train_time:8211ms step_avg:32.85ms
step:251/1775 train_time:8232ms step_avg:32.80ms
step:252/1775 train_time:8252ms step_avg:32.75ms
step:253/1775 train_time:8271ms step_avg:32.69ms
step:254/1775 train_time:8305ms step_avg:32.70ms
step:255/1775 train_time:8337ms step_avg:32.70ms
step:256/1775 train_time:8372ms step_avg:32.70ms
step:257/1775 train_time:8403ms step_avg:32.70ms
step:258/1775 train_time:8436ms step_avg:32.70ms
step:259/1775 train_time:8468ms step_avg:32.69ms
step:260/1775 train_time:8501ms step_avg:32.70ms
step:261/1775 train_time:8533ms step_avg:32.69ms
step:262/1775 train_time:8567ms step_avg:32.70ms
step:263/1775 train_time:8598ms step_avg:32.69ms
step:264/1775 train_time:8632ms step_avg:32.70ms
step:265/1775 train_time:8663ms step_avg:32.69ms
step:266/1775 train_time:8697ms step_avg:32.69ms
step:267/1775 train_time:8728ms step_avg:32.69ms
step:268/1775 train_time:8761ms step_avg:32.69ms
step:269/1775 train_time:8792ms step_avg:32.68ms
step:270/1775 train_time:8825ms step_avg:32.69ms
step:271/1775 train_time:8856ms step_avg:32.68ms
step:272/1775 train_time:8889ms step_avg:32.68ms
step:273/1775 train_time:8920ms step_avg:32.67ms
step:274/1775 train_time:8953ms step_avg:32.68ms
step:275/1775 train_time:8984ms step_avg:32.67ms
step:276/1775 train_time:9017ms step_avg:32.67ms
step:277/1775 train_time:9048ms step_avg:32.66ms
step:278/1775 train_time:9081ms step_avg:32.66ms
step:279/1775 train_time:9112ms step_avg:32.66ms
step:280/1775 train_time:9146ms step_avg:32.66ms
step:281/1775 train_time:9178ms step_avg:32.66ms
step:282/1775 train_time:9212ms step_avg:32.67ms
step:283/1775 train_time:9244ms step_avg:32.66ms
step:284/1775 train_time:9278ms step_avg:32.67ms
step:285/1775 train_time:9310ms step_avg:32.67ms
step:286/1775 train_time:9343ms step_avg:32.67ms
step:287/1775 train_time:9375ms step_avg:32.67ms
step:288/1775 train_time:9409ms step_avg:32.67ms
step:289/1775 train_time:9441ms step_avg:32.67ms
step:290/1775 train_time:9474ms step_avg:32.67ms
step:291/1775 train_time:9505ms step_avg:32.66ms
step:292/1775 train_time:9538ms step_avg:32.67ms
step:293/1775 train_time:9570ms step_avg:32.66ms
step:294/1775 train_time:9603ms step_avg:32.66ms
step:295/1775 train_time:9635ms step_avg:32.66ms
step:296/1775 train_time:9668ms step_avg:32.66ms
step:297/1775 train_time:9699ms step_avg:32.66ms
step:298/1775 train_time:9733ms step_avg:32.66ms
step:299/1775 train_time:9763ms step_avg:32.65ms
step:300/1775 train_time:9797ms step_avg:32.66ms
step:301/1775 train_time:9828ms step_avg:32.65ms
step:302/1775 train_time:9861ms step_avg:32.65ms
step:303/1775 train_time:9892ms step_avg:32.65ms
step:304/1775 train_time:9926ms step_avg:32.65ms
step:305/1775 train_time:9957ms step_avg:32.64ms
step:306/1775 train_time:9990ms step_avg:32.65ms
step:307/1775 train_time:10021ms step_avg:32.64ms
step:308/1775 train_time:10054ms step_avg:32.64ms
step:309/1775 train_time:10085ms step_avg:32.64ms
step:310/1775 train_time:10119ms step_avg:32.64ms
step:311/1775 train_time:10150ms step_avg:32.64ms
step:312/1775 train_time:10184ms step_avg:32.64ms
step:313/1775 train_time:10215ms step_avg:32.64ms
step:314/1775 train_time:10249ms step_avg:32.64ms
step:315/1775 train_time:10280ms step_avg:32.64ms
step:316/1775 train_time:10314ms step_avg:32.64ms
step:317/1775 train_time:10346ms step_avg:32.64ms
step:318/1775 train_time:10380ms step_avg:32.64ms
step:319/1775 train_time:10411ms step_avg:32.64ms
step:320/1775 train_time:10445ms step_avg:32.64ms
step:321/1775 train_time:10476ms step_avg:32.64ms
step:322/1775 train_time:10510ms step_avg:32.64ms
step:323/1775 train_time:10542ms step_avg:32.64ms
step:324/1775 train_time:10576ms step_avg:32.64ms
step:325/1775 train_time:10607ms step_avg:32.64ms
step:326/1775 train_time:10641ms step_avg:32.64ms
step:327/1775 train_time:10672ms step_avg:32.64ms
step:328/1775 train_time:10706ms step_avg:32.64ms
step:329/1775 train_time:10737ms step_avg:32.63ms
step:330/1775 train_time:10770ms step_avg:32.64ms
step:331/1775 train_time:10801ms step_avg:32.63ms
step:332/1775 train_time:10835ms step_avg:32.63ms
step:333/1775 train_time:10865ms step_avg:32.63ms
step:334/1775 train_time:10898ms step_avg:32.63ms
step:335/1775 train_time:10929ms step_avg:32.63ms
step:336/1775 train_time:10963ms step_avg:32.63ms
step:337/1775 train_time:10994ms step_avg:32.62ms
step:338/1775 train_time:11027ms step_avg:32.62ms
step:339/1775 train_time:11058ms step_avg:32.62ms
step:340/1775 train_time:11092ms step_avg:32.62ms
step:341/1775 train_time:11123ms step_avg:32.62ms
step:342/1775 train_time:11156ms step_avg:32.62ms
step:343/1775 train_time:11187ms step_avg:32.62ms
step:344/1775 train_time:11220ms step_avg:32.62ms
step:345/1775 train_time:11252ms step_avg:32.62ms
step:346/1775 train_time:11286ms step_avg:32.62ms
step:347/1775 train_time:11317ms step_avg:32.61ms
step:348/1775 train_time:11351ms step_avg:32.62ms
step:349/1775 train_time:11382ms step_avg:32.61ms
step:350/1775 train_time:11416ms step_avg:32.62ms
step:351/1775 train_time:11448ms step_avg:32.61ms
step:352/1775 train_time:11481ms step_avg:32.62ms
step:353/1775 train_time:11513ms step_avg:32.61ms
step:354/1775 train_time:11546ms step_avg:32.62ms
step:355/1775 train_time:11577ms step_avg:32.61ms
step:356/1775 train_time:11611ms step_avg:32.62ms
step:357/1775 train_time:11643ms step_avg:32.61ms
step:358/1775 train_time:11676ms step_avg:32.61ms
step:359/1775 train_time:11707ms step_avg:32.61ms
step:360/1775 train_time:11741ms step_avg:32.61ms
step:361/1775 train_time:11772ms step_avg:32.61ms
step:362/1775 train_time:11806ms step_avg:32.61ms
step:363/1775 train_time:11837ms step_avg:32.61ms
step:364/1775 train_time:11871ms step_avg:32.61ms
step:365/1775 train_time:11902ms step_avg:32.61ms
step:366/1775 train_time:11935ms step_avg:32.61ms
step:367/1775 train_time:11967ms step_avg:32.61ms
step:368/1775 train_time:11999ms step_avg:32.61ms
step:369/1775 train_time:12030ms step_avg:32.60ms
step:370/1775 train_time:12064ms step_avg:32.61ms
step:371/1775 train_time:12095ms step_avg:32.60ms
step:372/1775 train_time:12129ms step_avg:32.60ms
step:373/1775 train_time:12160ms step_avg:32.60ms
step:374/1775 train_time:12194ms step_avg:32.60ms
step:375/1775 train_time:12225ms step_avg:32.60ms
step:376/1775 train_time:12259ms step_avg:32.60ms
step:377/1775 train_time:12290ms step_avg:32.60ms
step:378/1775 train_time:12324ms step_avg:32.60ms
step:379/1775 train_time:12355ms step_avg:32.60ms
step:380/1775 train_time:12388ms step_avg:32.60ms
step:381/1775 train_time:12420ms step_avg:32.60ms
step:382/1775 train_time:12454ms step_avg:32.60ms
step:383/1775 train_time:12486ms step_avg:32.60ms
step:384/1775 train_time:12519ms step_avg:32.60ms
step:385/1775 train_time:12550ms step_avg:32.60ms
step:386/1775 train_time:12584ms step_avg:32.60ms
step:387/1775 train_time:12615ms step_avg:32.60ms
step:388/1775 train_time:12648ms step_avg:32.60ms
step:389/1775 train_time:12679ms step_avg:32.59ms
step:390/1775 train_time:12713ms step_avg:32.60ms
step:391/1775 train_time:12745ms step_avg:32.60ms
step:392/1775 train_time:12778ms step_avg:32.60ms
step:393/1775 train_time:12809ms step_avg:32.59ms
step:394/1775 train_time:12843ms step_avg:32.60ms
step:395/1775 train_time:12874ms step_avg:32.59ms
step:396/1775 train_time:12908ms step_avg:32.59ms
step:397/1775 train_time:12939ms step_avg:32.59ms
step:398/1775 train_time:12972ms step_avg:32.59ms
step:399/1775 train_time:13003ms step_avg:32.59ms
step:400/1775 train_time:13037ms step_avg:32.59ms
step:401/1775 train_time:13068ms step_avg:32.59ms
step:402/1775 train_time:13101ms step_avg:32.59ms
step:403/1775 train_time:13133ms step_avg:32.59ms
step:404/1775 train_time:13166ms step_avg:32.59ms
step:405/1775 train_time:13197ms step_avg:32.59ms
step:406/1775 train_time:13230ms step_avg:32.59ms
step:407/1775 train_time:13262ms step_avg:32.58ms
step:408/1775 train_time:13296ms step_avg:32.59ms
step:409/1775 train_time:13327ms step_avg:32.58ms
step:410/1775 train_time:13360ms step_avg:32.59ms
step:411/1775 train_time:13392ms step_avg:32.58ms
step:412/1775 train_time:13426ms step_avg:32.59ms
step:413/1775 train_time:13457ms step_avg:32.58ms
step:414/1775 train_time:13491ms step_avg:32.59ms
step:415/1775 train_time:13522ms step_avg:32.58ms
step:416/1775 train_time:13555ms step_avg:32.59ms
step:417/1775 train_time:13586ms step_avg:32.58ms
step:418/1775 train_time:13620ms step_avg:32.58ms
step:419/1775 train_time:13651ms step_avg:32.58ms
step:420/1775 train_time:13685ms step_avg:32.58ms
step:421/1775 train_time:13716ms step_avg:32.58ms
step:422/1775 train_time:13749ms step_avg:32.58ms
step:423/1775 train_time:13780ms step_avg:32.58ms
step:424/1775 train_time:13814ms step_avg:32.58ms
step:425/1775 train_time:13845ms step_avg:32.58ms
step:426/1775 train_time:13879ms step_avg:32.58ms
step:427/1775 train_time:13909ms step_avg:32.57ms
step:428/1775 train_time:13943ms step_avg:32.58ms
step:429/1775 train_time:13974ms step_avg:32.57ms
step:430/1775 train_time:14007ms step_avg:32.57ms
step:431/1775 train_time:14038ms step_avg:32.57ms
step:432/1775 train_time:14072ms step_avg:32.57ms
step:433/1775 train_time:14103ms step_avg:32.57ms
step:434/1775 train_time:14137ms step_avg:32.57ms
step:435/1775 train_time:14168ms step_avg:32.57ms
step:436/1775 train_time:14201ms step_avg:32.57ms
step:437/1775 train_time:14233ms step_avg:32.57ms
step:438/1775 train_time:14267ms step_avg:32.57ms
step:439/1775 train_time:14298ms step_avg:32.57ms
step:440/1775 train_time:14332ms step_avg:32.57ms
step:441/1775 train_time:14364ms step_avg:32.57ms
step:442/1775 train_time:14397ms step_avg:32.57ms
step:443/1775 train_time:14428ms step_avg:32.57ms
step:444/1775 train_time:14461ms step_avg:32.57ms
step:445/1775 train_time:14493ms step_avg:32.57ms
step:446/1775 train_time:14527ms step_avg:32.57ms
step:447/1775 train_time:14558ms step_avg:32.57ms
step:448/1775 train_time:14591ms step_avg:32.57ms
step:449/1775 train_time:14622ms step_avg:32.57ms
step:450/1775 train_time:14655ms step_avg:32.57ms
step:451/1775 train_time:14687ms step_avg:32.56ms
step:452/1775 train_time:14719ms step_avg:32.57ms
step:453/1775 train_time:14751ms step_avg:32.56ms
step:454/1775 train_time:14785ms step_avg:32.57ms
step:455/1775 train_time:14816ms step_avg:32.56ms
step:456/1775 train_time:14850ms step_avg:32.56ms
step:457/1775 train_time:14881ms step_avg:32.56ms
step:458/1775 train_time:14914ms step_avg:32.56ms
step:459/1775 train_time:14946ms step_avg:32.56ms
step:460/1775 train_time:14979ms step_avg:32.56ms
step:461/1775 train_time:15010ms step_avg:32.56ms
step:462/1775 train_time:15044ms step_avg:32.56ms
step:463/1775 train_time:15076ms step_avg:32.56ms
step:464/1775 train_time:15109ms step_avg:32.56ms
step:465/1775 train_time:15141ms step_avg:32.56ms
step:466/1775 train_time:15175ms step_avg:32.56ms
step:467/1775 train_time:15206ms step_avg:32.56ms
step:468/1775 train_time:15240ms step_avg:32.56ms
step:469/1775 train_time:15271ms step_avg:32.56ms
step:470/1775 train_time:15305ms step_avg:32.56ms
step:471/1775 train_time:15336ms step_avg:32.56ms
step:472/1775 train_time:15369ms step_avg:32.56ms
step:473/1775 train_time:15400ms step_avg:32.56ms
step:474/1775 train_time:15434ms step_avg:32.56ms
step:475/1775 train_time:15465ms step_avg:32.56ms
step:476/1775 train_time:15498ms step_avg:32.56ms
step:477/1775 train_time:15529ms step_avg:32.56ms
step:478/1775 train_time:15563ms step_avg:32.56ms
step:479/1775 train_time:15595ms step_avg:32.56ms
step:480/1775 train_time:15629ms step_avg:32.56ms
step:481/1775 train_time:15659ms step_avg:32.56ms
step:482/1775 train_time:15693ms step_avg:32.56ms
step:483/1775 train_time:15724ms step_avg:32.55ms
step:484/1775 train_time:15757ms step_avg:32.56ms
step:485/1775 train_time:15788ms step_avg:32.55ms
step:486/1775 train_time:15821ms step_avg:32.55ms
step:487/1775 train_time:15852ms step_avg:32.55ms
step:488/1775 train_time:15886ms step_avg:32.55ms
step:489/1775 train_time:15917ms step_avg:32.55ms
step:490/1775 train_time:15950ms step_avg:32.55ms
step:491/1775 train_time:15982ms step_avg:32.55ms
step:492/1775 train_time:16015ms step_avg:32.55ms
step:493/1775 train_time:16046ms step_avg:32.55ms
step:494/1775 train_time:16079ms step_avg:32.55ms
step:495/1775 train_time:16111ms step_avg:32.55ms
step:496/1775 train_time:16144ms step_avg:32.55ms
step:497/1775 train_time:16176ms step_avg:32.55ms
step:498/1775 train_time:16209ms step_avg:32.55ms
step:499/1775 train_time:16240ms step_avg:32.55ms
step:500/1775 train_time:16274ms step_avg:32.55ms
step:500/1775 val_loss:4.2741 train_time:16315ms step_avg:32.63ms
step:501/1775 train_time:16336ms step_avg:32.61ms
step:502/1775 train_time:16356ms step_avg:32.58ms
step:503/1775 train_time:16374ms step_avg:32.55ms
step:504/1775 train_time:16408ms step_avg:32.56ms
step:505/1775 train_time:16441ms step_avg:32.56ms
step:506/1775 train_time:16475ms step_avg:32.56ms
step:507/1775 train_time:16507ms step_avg:32.56ms
step:508/1775 train_time:16541ms step_avg:32.56ms
step:509/1775 train_time:16572ms step_avg:32.56ms
step:510/1775 train_time:16606ms step_avg:32.56ms
step:511/1775 train_time:16637ms step_avg:32.56ms
step:512/1775 train_time:16671ms step_avg:32.56ms
step:513/1775 train_time:16702ms step_avg:32.56ms
step:514/1775 train_time:16735ms step_avg:32.56ms
step:515/1775 train_time:16766ms step_avg:32.55ms
step:516/1775 train_time:16799ms step_avg:32.56ms
step:517/1775 train_time:16830ms step_avg:32.55ms
step:518/1775 train_time:16864ms step_avg:32.56ms
step:519/1775 train_time:16895ms step_avg:32.55ms
step:520/1775 train_time:16928ms step_avg:32.55ms
step:521/1775 train_time:16958ms step_avg:32.55ms
step:522/1775 train_time:16991ms step_avg:32.55ms
step:523/1775 train_time:17022ms step_avg:32.55ms
step:524/1775 train_time:17055ms step_avg:32.55ms
step:525/1775 train_time:17086ms step_avg:32.55ms
step:526/1775 train_time:17120ms step_avg:32.55ms
step:527/1775 train_time:17151ms step_avg:32.54ms
step:528/1775 train_time:17184ms step_avg:32.55ms
step:529/1775 train_time:17215ms step_avg:32.54ms
step:530/1775 train_time:17249ms step_avg:32.55ms
step:531/1775 train_time:17281ms step_avg:32.54ms
step:532/1775 train_time:17314ms step_avg:32.54ms
step:533/1775 train_time:17345ms step_avg:32.54ms
step:534/1775 train_time:17379ms step_avg:32.54ms
step:535/1775 train_time:17410ms step_avg:32.54ms
step:536/1775 train_time:17445ms step_avg:32.55ms
step:537/1775 train_time:17476ms step_avg:32.54ms
step:538/1775 train_time:17510ms step_avg:32.55ms
step:539/1775 train_time:17542ms step_avg:32.55ms
step:540/1775 train_time:17575ms step_avg:32.55ms
step:541/1775 train_time:17607ms step_avg:32.55ms
step:542/1775 train_time:17640ms step_avg:32.55ms
step:543/1775 train_time:17671ms step_avg:32.54ms
step:544/1775 train_time:17705ms step_avg:32.55ms
step:545/1775 train_time:17736ms step_avg:32.54ms
step:546/1775 train_time:17770ms step_avg:32.55ms
step:547/1775 train_time:17800ms step_avg:32.54ms
step:548/1775 train_time:17833ms step_avg:32.54ms
step:549/1775 train_time:17865ms step_avg:32.54ms
step:550/1775 train_time:17898ms step_avg:32.54ms
step:551/1775 train_time:17929ms step_avg:32.54ms
step:552/1775 train_time:17962ms step_avg:32.54ms
step:553/1775 train_time:17993ms step_avg:32.54ms
step:554/1775 train_time:18026ms step_avg:32.54ms
step:555/1775 train_time:18057ms step_avg:32.54ms
step:556/1775 train_time:18090ms step_avg:32.54ms
step:557/1775 train_time:18121ms step_avg:32.53ms
step:558/1775 train_time:18154ms step_avg:32.53ms
step:559/1775 train_time:18185ms step_avg:32.53ms
step:560/1775 train_time:18219ms step_avg:32.53ms
step:561/1775 train_time:18250ms step_avg:32.53ms
step:562/1775 train_time:18284ms step_avg:32.53ms
step:563/1775 train_time:18315ms step_avg:32.53ms
step:564/1775 train_time:18349ms step_avg:32.53ms
step:565/1775 train_time:18381ms step_avg:32.53ms
step:566/1775 train_time:18415ms step_avg:32.53ms
step:567/1775 train_time:18447ms step_avg:32.53ms
step:568/1775 train_time:18481ms step_avg:32.54ms
step:569/1775 train_time:18512ms step_avg:32.53ms
step:570/1775 train_time:18545ms step_avg:32.54ms
step:571/1775 train_time:18577ms step_avg:32.53ms
step:572/1775 train_time:18610ms step_avg:32.53ms
step:573/1775 train_time:18641ms step_avg:32.53ms
step:574/1775 train_time:18674ms step_avg:32.53ms
step:575/1775 train_time:18705ms step_avg:32.53ms
step:576/1775 train_time:18739ms step_avg:32.53ms
step:577/1775 train_time:18770ms step_avg:32.53ms
step:578/1775 train_time:18803ms step_avg:32.53ms
step:579/1775 train_time:18835ms step_avg:32.53ms
step:580/1775 train_time:18871ms step_avg:32.54ms
step:581/1775 train_time:18929ms step_avg:32.58ms
step:582/1775 train_time:18988ms step_avg:32.63ms
step:583/1775 train_time:19045ms step_avg:32.67ms
step:584/1775 train_time:19107ms step_avg:32.72ms
step:585/1775 train_time:19164ms step_avg:32.76ms
step:586/1775 train_time:19226ms step_avg:32.81ms
step:587/1775 train_time:19284ms step_avg:32.85ms
step:588/1775 train_time:19344ms step_avg:32.90ms
step:589/1775 train_time:19403ms step_avg:32.94ms
step:590/1775 train_time:19463ms step_avg:32.99ms
step:591/1775 train_time:19522ms step_avg:33.03ms
step:592/1775 train_time:19583ms step_avg:33.08ms
step:593/1775 train_time:19641ms step_avg:33.12ms
step:594/1775 train_time:19703ms step_avg:33.17ms
step:595/1775 train_time:19761ms step_avg:33.21ms
step:596/1775 train_time:19822ms step_avg:33.26ms
step:597/1775 train_time:19879ms step_avg:33.30ms
step:598/1775 train_time:19940ms step_avg:33.34ms
step:599/1775 train_time:19998ms step_avg:33.38ms
step:600/1775 train_time:20058ms step_avg:33.43ms
step:601/1775 train_time:20116ms step_avg:33.47ms
step:602/1775 train_time:20176ms step_avg:33.52ms
step:603/1775 train_time:20234ms step_avg:33.56ms
step:604/1775 train_time:20295ms step_avg:33.60ms
step:605/1775 train_time:20353ms step_avg:33.64ms
step:606/1775 train_time:20413ms step_avg:33.69ms
step:607/1775 train_time:20471ms step_avg:33.73ms
step:608/1775 train_time:20531ms step_avg:33.77ms
step:609/1775 train_time:20589ms step_avg:33.81ms
step:610/1775 train_time:20650ms step_avg:33.85ms
step:611/1775 train_time:20707ms step_avg:33.89ms
step:612/1775 train_time:20767ms step_avg:33.93ms
step:613/1775 train_time:20825ms step_avg:33.97ms
step:614/1775 train_time:20886ms step_avg:34.02ms
step:615/1775 train_time:20944ms step_avg:34.06ms
step:616/1775 train_time:21005ms step_avg:34.10ms
step:617/1775 train_time:21063ms step_avg:34.14ms
step:618/1775 train_time:21123ms step_avg:34.18ms
step:619/1775 train_time:21181ms step_avg:34.22ms
step:620/1775 train_time:21242ms step_avg:34.26ms
step:621/1775 train_time:21300ms step_avg:34.30ms
step:622/1775 train_time:21360ms step_avg:34.34ms
step:623/1775 train_time:21419ms step_avg:34.38ms
step:624/1775 train_time:21479ms step_avg:34.42ms
step:625/1775 train_time:21538ms step_avg:34.46ms
step:626/1775 train_time:21599ms step_avg:34.50ms
step:627/1775 train_time:21657ms step_avg:34.54ms
step:628/1775 train_time:21718ms step_avg:34.58ms
step:629/1775 train_time:21775ms step_avg:34.62ms
step:630/1775 train_time:21837ms step_avg:34.66ms
step:631/1775 train_time:21894ms step_avg:34.70ms
step:632/1775 train_time:21955ms step_avg:34.74ms
step:633/1775 train_time:22012ms step_avg:34.77ms
step:634/1775 train_time:22072ms step_avg:34.81ms
step:635/1775 train_time:22129ms step_avg:34.85ms
step:636/1775 train_time:22189ms step_avg:34.89ms
step:637/1775 train_time:22247ms step_avg:34.92ms
step:638/1775 train_time:22307ms step_avg:34.96ms
step:639/1775 train_time:22365ms step_avg:35.00ms
step:640/1775 train_time:22425ms step_avg:35.04ms
step:641/1775 train_time:22483ms step_avg:35.07ms
step:642/1775 train_time:22543ms step_avg:35.11ms
step:643/1775 train_time:22602ms step_avg:35.15ms
step:644/1775 train_time:22662ms step_avg:35.19ms
step:645/1775 train_time:22721ms step_avg:35.23ms
step:646/1775 train_time:22782ms step_avg:35.27ms
step:647/1775 train_time:22840ms step_avg:35.30ms
step:648/1775 train_time:22900ms step_avg:35.34ms
step:649/1775 train_time:22959ms step_avg:35.38ms
step:650/1775 train_time:23019ms step_avg:35.41ms
step:651/1775 train_time:23076ms step_avg:35.45ms
step:652/1775 train_time:23137ms step_avg:35.49ms
step:653/1775 train_time:23195ms step_avg:35.52ms
step:654/1775 train_time:23256ms step_avg:35.56ms
step:655/1775 train_time:23314ms step_avg:35.59ms
step:656/1775 train_time:23374ms step_avg:35.63ms
step:657/1775 train_time:23432ms step_avg:35.66ms
step:658/1775 train_time:23491ms step_avg:35.70ms
step:659/1775 train_time:23549ms step_avg:35.73ms
step:660/1775 train_time:23609ms step_avg:35.77ms
step:661/1775 train_time:23666ms step_avg:35.80ms
step:662/1775 train_time:23727ms step_avg:35.84ms
step:663/1775 train_time:23785ms step_avg:35.87ms
step:664/1775 train_time:23845ms step_avg:35.91ms
step:665/1775 train_time:23903ms step_avg:35.94ms
step:666/1775 train_time:23964ms step_avg:35.98ms
step:667/1775 train_time:24022ms step_avg:36.02ms
step:668/1775 train_time:24083ms step_avg:36.05ms
step:669/1775 train_time:24142ms step_avg:36.09ms
step:670/1775 train_time:24203ms step_avg:36.12ms
step:671/1775 train_time:24261ms step_avg:36.16ms
step:672/1775 train_time:24321ms step_avg:36.19ms
step:673/1775 train_time:24379ms step_avg:36.23ms
step:674/1775 train_time:24440ms step_avg:36.26ms
step:675/1775 train_time:24498ms step_avg:36.29ms
step:676/1775 train_time:24558ms step_avg:36.33ms
step:677/1775 train_time:24617ms step_avg:36.36ms
step:678/1775 train_time:24679ms step_avg:36.40ms
step:679/1775 train_time:24736ms step_avg:36.43ms
step:680/1775 train_time:24797ms step_avg:36.47ms
step:681/1775 train_time:24855ms step_avg:36.50ms
step:682/1775 train_time:24915ms step_avg:36.53ms
step:683/1775 train_time:24973ms step_avg:36.56ms
step:684/1775 train_time:25034ms step_avg:36.60ms
step:685/1775 train_time:25092ms step_avg:36.63ms
step:686/1775 train_time:25152ms step_avg:36.66ms
step:687/1775 train_time:25210ms step_avg:36.70ms
step:688/1775 train_time:25269ms step_avg:36.73ms
step:689/1775 train_time:25328ms step_avg:36.76ms
step:690/1775 train_time:25387ms step_avg:36.79ms
step:691/1775 train_time:25445ms step_avg:36.82ms
step:692/1775 train_time:25506ms step_avg:36.86ms
step:693/1775 train_time:25563ms step_avg:36.89ms
step:694/1775 train_time:25623ms step_avg:36.92ms
step:695/1775 train_time:25682ms step_avg:36.95ms
step:696/1775 train_time:25744ms step_avg:36.99ms
step:697/1775 train_time:25802ms step_avg:37.02ms
step:698/1775 train_time:25862ms step_avg:37.05ms
step:699/1775 train_time:25920ms step_avg:37.08ms
step:700/1775 train_time:25980ms step_avg:37.11ms
step:701/1775 train_time:26039ms step_avg:37.14ms
step:702/1775 train_time:26099ms step_avg:37.18ms
step:703/1775 train_time:26157ms step_avg:37.21ms
step:704/1775 train_time:26218ms step_avg:37.24ms
step:705/1775 train_time:26274ms step_avg:37.27ms
step:706/1775 train_time:26336ms step_avg:37.30ms
step:707/1775 train_time:26394ms step_avg:37.33ms
step:708/1775 train_time:26455ms step_avg:37.37ms
step:709/1775 train_time:26513ms step_avg:37.39ms
step:710/1775 train_time:26573ms step_avg:37.43ms
step:711/1775 train_time:26632ms step_avg:37.46ms
step:712/1775 train_time:26693ms step_avg:37.49ms
step:713/1775 train_time:26750ms step_avg:37.52ms
step:714/1775 train_time:26810ms step_avg:37.55ms
step:715/1775 train_time:26868ms step_avg:37.58ms
step:716/1775 train_time:26929ms step_avg:37.61ms
step:717/1775 train_time:26987ms step_avg:37.64ms
step:718/1775 train_time:27047ms step_avg:37.67ms
step:719/1775 train_time:27106ms step_avg:37.70ms
step:720/1775 train_time:27166ms step_avg:37.73ms
step:721/1775 train_time:27223ms step_avg:37.76ms
step:722/1775 train_time:27284ms step_avg:37.79ms
step:723/1775 train_time:27342ms step_avg:37.82ms
step:724/1775 train_time:27403ms step_avg:37.85ms
step:725/1775 train_time:27461ms step_avg:37.88ms
step:726/1775 train_time:27523ms step_avg:37.91ms
step:727/1775 train_time:27581ms step_avg:37.94ms
step:728/1775 train_time:27642ms step_avg:37.97ms
step:729/1775 train_time:27701ms step_avg:38.00ms
step:730/1775 train_time:27761ms step_avg:38.03ms
step:731/1775 train_time:27819ms step_avg:38.06ms
step:732/1775 train_time:27879ms step_avg:38.09ms
step:733/1775 train_time:27937ms step_avg:38.11ms
step:734/1775 train_time:27998ms step_avg:38.14ms
step:735/1775 train_time:28056ms step_avg:38.17ms
step:736/1775 train_time:28116ms step_avg:38.20ms
step:737/1775 train_time:28174ms step_avg:38.23ms
step:738/1775 train_time:28234ms step_avg:38.26ms
step:739/1775 train_time:28292ms step_avg:38.28ms
step:740/1775 train_time:28351ms step_avg:38.31ms
step:741/1775 train_time:28409ms step_avg:38.34ms
step:742/1775 train_time:28469ms step_avg:38.37ms
step:743/1775 train_time:28527ms step_avg:38.39ms
step:744/1775 train_time:28587ms step_avg:38.42ms
step:745/1775 train_time:28644ms step_avg:38.45ms
step:746/1775 train_time:28705ms step_avg:38.48ms
step:747/1775 train_time:28762ms step_avg:38.50ms
step:748/1775 train_time:28823ms step_avg:38.53ms
step:749/1775 train_time:28881ms step_avg:38.56ms
step:750/1775 train_time:28943ms step_avg:38.59ms
step:750/1775 val_loss:3.9990 train_time:29012ms step_avg:38.68ms
step:751/1775 train_time:29034ms step_avg:38.66ms
step:752/1775 train_time:29064ms step_avg:38.65ms
step:753/1775 train_time:29123ms step_avg:38.68ms
step:754/1775 train_time:29185ms step_avg:38.71ms
step:755/1775 train_time:29243ms step_avg:38.73ms
step:756/1775 train_time:29302ms step_avg:38.76ms
step:757/1775 train_time:29361ms step_avg:38.79ms
step:758/1775 train_time:29420ms step_avg:38.81ms
step:759/1775 train_time:29478ms step_avg:38.84ms
step:760/1775 train_time:29538ms step_avg:38.87ms
step:761/1775 train_time:29594ms step_avg:38.89ms
step:762/1775 train_time:29656ms step_avg:38.92ms
step:763/1775 train_time:29714ms step_avg:38.94ms
step:764/1775 train_time:29773ms step_avg:38.97ms
step:765/1775 train_time:29831ms step_avg:38.99ms
step:766/1775 train_time:29892ms step_avg:39.02ms
step:767/1775 train_time:29950ms step_avg:39.05ms
step:768/1775 train_time:30013ms step_avg:39.08ms
step:769/1775 train_time:30073ms step_avg:39.11ms
step:770/1775 train_time:30136ms step_avg:39.14ms
step:771/1775 train_time:30195ms step_avg:39.16ms
step:772/1775 train_time:30256ms step_avg:39.19ms
step:773/1775 train_time:30315ms step_avg:39.22ms
step:774/1775 train_time:30375ms step_avg:39.24ms
step:775/1775 train_time:30434ms step_avg:39.27ms
step:776/1775 train_time:30494ms step_avg:39.30ms
step:777/1775 train_time:30551ms step_avg:39.32ms
step:778/1775 train_time:30610ms step_avg:39.34ms
step:779/1775 train_time:30668ms step_avg:39.37ms
step:780/1775 train_time:30728ms step_avg:39.40ms
step:781/1775 train_time:30786ms step_avg:39.42ms
step:782/1775 train_time:30846ms step_avg:39.44ms
step:783/1775 train_time:30903ms step_avg:39.47ms
step:784/1775 train_time:30963ms step_avg:39.49ms
step:785/1775 train_time:31023ms step_avg:39.52ms
step:786/1775 train_time:31083ms step_avg:39.55ms
step:787/1775 train_time:31141ms step_avg:39.57ms
step:788/1775 train_time:31202ms step_avg:39.60ms
step:789/1775 train_time:31260ms step_avg:39.62ms
step:790/1775 train_time:31320ms step_avg:39.65ms
step:791/1775 train_time:31379ms step_avg:39.67ms
step:792/1775 train_time:31439ms step_avg:39.70ms
step:793/1775 train_time:31497ms step_avg:39.72ms
step:794/1775 train_time:31557ms step_avg:39.74ms
step:795/1775 train_time:31616ms step_avg:39.77ms
step:796/1775 train_time:31675ms step_avg:39.79ms
step:797/1775 train_time:31734ms step_avg:39.82ms
step:798/1775 train_time:31794ms step_avg:39.84ms
step:799/1775 train_time:31851ms step_avg:39.86ms
step:800/1775 train_time:31913ms step_avg:39.89ms
step:801/1775 train_time:31972ms step_avg:39.91ms
step:802/1775 train_time:32033ms step_avg:39.94ms
step:803/1775 train_time:32093ms step_avg:39.97ms
step:804/1775 train_time:32154ms step_avg:39.99ms
step:805/1775 train_time:32213ms step_avg:40.02ms
step:806/1775 train_time:32274ms step_avg:40.04ms
step:807/1775 train_time:32333ms step_avg:40.07ms
step:808/1775 train_time:32394ms step_avg:40.09ms
step:809/1775 train_time:32452ms step_avg:40.11ms
step:810/1775 train_time:32513ms step_avg:40.14ms
step:811/1775 train_time:32572ms step_avg:40.16ms
step:812/1775 train_time:32632ms step_avg:40.19ms
step:813/1775 train_time:32689ms step_avg:40.21ms
step:814/1775 train_time:32750ms step_avg:40.23ms
step:815/1775 train_time:32807ms step_avg:40.25ms
step:816/1775 train_time:32866ms step_avg:40.28ms
step:817/1775 train_time:32925ms step_avg:40.30ms
step:818/1775 train_time:32986ms step_avg:40.32ms
step:819/1775 train_time:33044ms step_avg:40.35ms
step:820/1775 train_time:33103ms step_avg:40.37ms
step:821/1775 train_time:33162ms step_avg:40.39ms
step:822/1775 train_time:33222ms step_avg:40.42ms
step:823/1775 train_time:33280ms step_avg:40.44ms
step:824/1775 train_time:33341ms step_avg:40.46ms
step:825/1775 train_time:33398ms step_avg:40.48ms
step:826/1775 train_time:33459ms step_avg:40.51ms
step:827/1775 train_time:33517ms step_avg:40.53ms
step:828/1775 train_time:33577ms step_avg:40.55ms
step:829/1775 train_time:33635ms step_avg:40.57ms
step:830/1775 train_time:33696ms step_avg:40.60ms
step:831/1775 train_time:33755ms step_avg:40.62ms
step:832/1775 train_time:33815ms step_avg:40.64ms
step:833/1775 train_time:33874ms step_avg:40.66ms
step:834/1775 train_time:33935ms step_avg:40.69ms
step:835/1775 train_time:33992ms step_avg:40.71ms
step:836/1775 train_time:34053ms step_avg:40.73ms
step:837/1775 train_time:34112ms step_avg:40.76ms
step:838/1775 train_time:34174ms step_avg:40.78ms
step:839/1775 train_time:34233ms step_avg:40.80ms
step:840/1775 train_time:34293ms step_avg:40.83ms
step:841/1775 train_time:34352ms step_avg:40.85ms
step:842/1775 train_time:34412ms step_avg:40.87ms
step:843/1775 train_time:34469ms step_avg:40.89ms
step:844/1775 train_time:34530ms step_avg:40.91ms
step:845/1775 train_time:34588ms step_avg:40.93ms
step:846/1775 train_time:34648ms step_avg:40.96ms
step:847/1775 train_time:34705ms step_avg:40.97ms
step:848/1775 train_time:34764ms step_avg:41.00ms
step:849/1775 train_time:34823ms step_avg:41.02ms
step:850/1775 train_time:34883ms step_avg:41.04ms
step:851/1775 train_time:34941ms step_avg:41.06ms
step:852/1775 train_time:35001ms step_avg:41.08ms
step:853/1775 train_time:35059ms step_avg:41.10ms
step:854/1775 train_time:35120ms step_avg:41.12ms
step:855/1775 train_time:35178ms step_avg:41.14ms
step:856/1775 train_time:35239ms step_avg:41.17ms
step:857/1775 train_time:35297ms step_avg:41.19ms
step:858/1775 train_time:35357ms step_avg:41.21ms
step:859/1775 train_time:35416ms step_avg:41.23ms
step:860/1775 train_time:35476ms step_avg:41.25ms
step:861/1775 train_time:35534ms step_avg:41.27ms
step:862/1775 train_time:35595ms step_avg:41.29ms
step:863/1775 train_time:35652ms step_avg:41.31ms
step:864/1775 train_time:35713ms step_avg:41.33ms
step:865/1775 train_time:35771ms step_avg:41.35ms
step:866/1775 train_time:35831ms step_avg:41.38ms
step:867/1775 train_time:35889ms step_avg:41.39ms
step:868/1775 train_time:35950ms step_avg:41.42ms
step:869/1775 train_time:36008ms step_avg:41.44ms
step:870/1775 train_time:36070ms step_avg:41.46ms
step:871/1775 train_time:36128ms step_avg:41.48ms
step:872/1775 train_time:36189ms step_avg:41.50ms
step:873/1775 train_time:36247ms step_avg:41.52ms
step:874/1775 train_time:36307ms step_avg:41.54ms
step:875/1775 train_time:36365ms step_avg:41.56ms
step:876/1775 train_time:36426ms step_avg:41.58ms
step:877/1775 train_time:36483ms step_avg:41.60ms
step:878/1775 train_time:36544ms step_avg:41.62ms
step:879/1775 train_time:36602ms step_avg:41.64ms
step:880/1775 train_time:36662ms step_avg:41.66ms
step:881/1775 train_time:36720ms step_avg:41.68ms
step:882/1775 train_time:36781ms step_avg:41.70ms
step:883/1775 train_time:36839ms step_avg:41.72ms
step:884/1775 train_time:36900ms step_avg:41.74ms
step:885/1775 train_time:36958ms step_avg:41.76ms
step:886/1775 train_time:37018ms step_avg:41.78ms
step:887/1775 train_time:37077ms step_avg:41.80ms
step:888/1775 train_time:37138ms step_avg:41.82ms
step:889/1775 train_time:37196ms step_avg:41.84ms
step:890/1775 train_time:37256ms step_avg:41.86ms
step:891/1775 train_time:37314ms step_avg:41.88ms
step:892/1775 train_time:37376ms step_avg:41.90ms
step:893/1775 train_time:37434ms step_avg:41.92ms
step:894/1775 train_time:37494ms step_avg:41.94ms
step:895/1775 train_time:37553ms step_avg:41.96ms
step:896/1775 train_time:37613ms step_avg:41.98ms
step:897/1775 train_time:37672ms step_avg:42.00ms
step:898/1775 train_time:37732ms step_avg:42.02ms
step:899/1775 train_time:37790ms step_avg:42.04ms
step:900/1775 train_time:37851ms step_avg:42.06ms
step:901/1775 train_time:37909ms step_avg:42.07ms
step:902/1775 train_time:37968ms step_avg:42.09ms
step:903/1775 train_time:38027ms step_avg:42.11ms
step:904/1775 train_time:38087ms step_avg:42.13ms
step:905/1775 train_time:38144ms step_avg:42.15ms
step:906/1775 train_time:38204ms step_avg:42.17ms
step:907/1775 train_time:38261ms step_avg:42.18ms
step:908/1775 train_time:38322ms step_avg:42.20ms
step:909/1775 train_time:38379ms step_avg:42.22ms
step:910/1775 train_time:38439ms step_avg:42.24ms
step:911/1775 train_time:38496ms step_avg:42.26ms
step:912/1775 train_time:38557ms step_avg:42.28ms
step:913/1775 train_time:38615ms step_avg:42.29ms
step:914/1775 train_time:38676ms step_avg:42.32ms
step:915/1775 train_time:38734ms step_avg:42.33ms
step:916/1775 train_time:38796ms step_avg:42.35ms
step:917/1775 train_time:38854ms step_avg:42.37ms
step:918/1775 train_time:38915ms step_avg:42.39ms
step:919/1775 train_time:38974ms step_avg:42.41ms
step:920/1775 train_time:39035ms step_avg:42.43ms
step:921/1775 train_time:39093ms step_avg:42.45ms
step:922/1775 train_time:39154ms step_avg:42.47ms
step:923/1775 train_time:39212ms step_avg:42.48ms
step:924/1775 train_time:39273ms step_avg:42.50ms
step:925/1775 train_time:39331ms step_avg:42.52ms
step:926/1775 train_time:39391ms step_avg:42.54ms
step:927/1775 train_time:39449ms step_avg:42.56ms
step:928/1775 train_time:39510ms step_avg:42.57ms
step:929/1775 train_time:39567ms step_avg:42.59ms
step:930/1775 train_time:39628ms step_avg:42.61ms
step:931/1775 train_time:39686ms step_avg:42.63ms
step:932/1775 train_time:39747ms step_avg:42.65ms
step:933/1775 train_time:39804ms step_avg:42.66ms
step:934/1775 train_time:39865ms step_avg:42.68ms
step:935/1775 train_time:39923ms step_avg:42.70ms
step:936/1775 train_time:39983ms step_avg:42.72ms
step:937/1775 train_time:40041ms step_avg:42.73ms
step:938/1775 train_time:40101ms step_avg:42.75ms
step:939/1775 train_time:40159ms step_avg:42.77ms
step:940/1775 train_time:40220ms step_avg:42.79ms
step:941/1775 train_time:40278ms step_avg:42.80ms
step:942/1775 train_time:40338ms step_avg:42.82ms
step:943/1775 train_time:40396ms step_avg:42.84ms
step:944/1775 train_time:40457ms step_avg:42.86ms
step:945/1775 train_time:40515ms step_avg:42.87ms
step:946/1775 train_time:40576ms step_avg:42.89ms
step:947/1775 train_time:40635ms step_avg:42.91ms
step:948/1775 train_time:40695ms step_avg:42.93ms
step:949/1775 train_time:40754ms step_avg:42.94ms
step:950/1775 train_time:40814ms step_avg:42.96ms
step:951/1775 train_time:40872ms step_avg:42.98ms
step:952/1775 train_time:40933ms step_avg:43.00ms
step:953/1775 train_time:40991ms step_avg:43.01ms
step:954/1775 train_time:41052ms step_avg:43.03ms
step:955/1775 train_time:41110ms step_avg:43.05ms
step:956/1775 train_time:41171ms step_avg:43.07ms
step:957/1775 train_time:41228ms step_avg:43.08ms
step:958/1775 train_time:41289ms step_avg:43.10ms
step:959/1775 train_time:41347ms step_avg:43.11ms
step:960/1775 train_time:41408ms step_avg:43.13ms
step:961/1775 train_time:41465ms step_avg:43.15ms
step:962/1775 train_time:41525ms step_avg:43.16ms
step:963/1775 train_time:41583ms step_avg:43.18ms
step:964/1775 train_time:41642ms step_avg:43.20ms
step:965/1775 train_time:41700ms step_avg:43.21ms
step:966/1775 train_time:41760ms step_avg:43.23ms
step:967/1775 train_time:41818ms step_avg:43.25ms
step:968/1775 train_time:41879ms step_avg:43.26ms
step:969/1775 train_time:41937ms step_avg:43.28ms
step:970/1775 train_time:41998ms step_avg:43.30ms
step:971/1775 train_time:42056ms step_avg:43.31ms
step:972/1775 train_time:42117ms step_avg:43.33ms
step:973/1775 train_time:42176ms step_avg:43.35ms
step:974/1775 train_time:42235ms step_avg:43.36ms
step:975/1775 train_time:42294ms step_avg:43.38ms
step:976/1775 train_time:42354ms step_avg:43.40ms
step:977/1775 train_time:42414ms step_avg:43.41ms
step:978/1775 train_time:42473ms step_avg:43.43ms
step:979/1775 train_time:42531ms step_avg:43.44ms
step:980/1775 train_time:42593ms step_avg:43.46ms
step:981/1775 train_time:42651ms step_avg:43.48ms
step:982/1775 train_time:42711ms step_avg:43.49ms
step:983/1775 train_time:42769ms step_avg:43.51ms
step:984/1775 train_time:42831ms step_avg:43.53ms
step:985/1775 train_time:42889ms step_avg:43.54ms
step:986/1775 train_time:42950ms step_avg:43.56ms
step:987/1775 train_time:43007ms step_avg:43.57ms
step:988/1775 train_time:43068ms step_avg:43.59ms
step:989/1775 train_time:43126ms step_avg:43.61ms
step:990/1775 train_time:43187ms step_avg:43.62ms
step:991/1775 train_time:43244ms step_avg:43.64ms
step:992/1775 train_time:43305ms step_avg:43.65ms
step:993/1775 train_time:43363ms step_avg:43.67ms
step:994/1775 train_time:43423ms step_avg:43.68ms
step:995/1775 train_time:43480ms step_avg:43.70ms
step:996/1775 train_time:43541ms step_avg:43.72ms
step:997/1775 train_time:43598ms step_avg:43.73ms
step:998/1775 train_time:43659ms step_avg:43.75ms
step:999/1775 train_time:43718ms step_avg:43.76ms
step:1000/1775 train_time:43777ms step_avg:43.78ms
step:1000/1775 val_loss:3.7377 train_time:43847ms step_avg:43.85ms
step:1001/1775 train_time:43868ms step_avg:43.82ms
step:1002/1775 train_time:43896ms step_avg:43.81ms
step:1003/1775 train_time:43955ms step_avg:43.82ms
step:1004/1775 train_time:44019ms step_avg:43.84ms
step:1005/1775 train_time:44078ms step_avg:43.86ms
step:1006/1775 train_time:44138ms step_avg:43.88ms
step:1007/1775 train_time:44196ms step_avg:43.89ms
step:1008/1775 train_time:44255ms step_avg:43.90ms
step:1009/1775 train_time:44312ms step_avg:43.92ms
step:1010/1775 train_time:44372ms step_avg:43.93ms
step:1011/1775 train_time:44428ms step_avg:43.94ms
step:1012/1775 train_time:44489ms step_avg:43.96ms
step:1013/1775 train_time:44546ms step_avg:43.97ms
step:1014/1775 train_time:44605ms step_avg:43.99ms
step:1015/1775 train_time:44663ms step_avg:44.00ms
step:1016/1775 train_time:44724ms step_avg:44.02ms
step:1017/1775 train_time:44783ms step_avg:44.03ms
step:1018/1775 train_time:44845ms step_avg:44.05ms
step:1019/1775 train_time:44906ms step_avg:44.07ms
step:1020/1775 train_time:44967ms step_avg:44.09ms
step:1021/1775 train_time:45026ms step_avg:44.10ms
step:1022/1775 train_time:45088ms step_avg:44.12ms
step:1023/1775 train_time:45147ms step_avg:44.13ms
step:1024/1775 train_time:45207ms step_avg:44.15ms
step:1025/1775 train_time:45264ms step_avg:44.16ms
step:1026/1775 train_time:45324ms step_avg:44.18ms
step:1027/1775 train_time:45383ms step_avg:44.19ms
step:1028/1775 train_time:45443ms step_avg:44.21ms
step:1029/1775 train_time:45500ms step_avg:44.22ms
step:1030/1775 train_time:45559ms step_avg:44.23ms
step:1031/1775 train_time:45617ms step_avg:44.25ms
step:1032/1775 train_time:45677ms step_avg:44.26ms
step:1033/1775 train_time:45735ms step_avg:44.27ms
step:1034/1775 train_time:45796ms step_avg:44.29ms
step:1035/1775 train_time:45854ms step_avg:44.30ms
step:1036/1775 train_time:45915ms step_avg:44.32ms
step:1037/1775 train_time:45975ms step_avg:44.33ms
step:1038/1775 train_time:46035ms step_avg:44.35ms
step:1039/1775 train_time:46093ms step_avg:44.36ms
step:1040/1775 train_time:46155ms step_avg:44.38ms
step:1041/1775 train_time:46212ms step_avg:44.39ms
step:1042/1775 train_time:46272ms step_avg:44.41ms
step:1043/1775 train_time:46329ms step_avg:44.42ms
step:1044/1775 train_time:46389ms step_avg:44.43ms
step:1045/1775 train_time:46446ms step_avg:44.45ms
step:1046/1775 train_time:46506ms step_avg:44.46ms
step:1047/1775 train_time:46563ms step_avg:44.47ms
step:1048/1775 train_time:46623ms step_avg:44.49ms
step:1049/1775 train_time:46681ms step_avg:44.50ms
step:1050/1775 train_time:46742ms step_avg:44.52ms
step:1051/1775 train_time:46802ms step_avg:44.53ms
step:1052/1775 train_time:46864ms step_avg:44.55ms
step:1053/1775 train_time:46923ms step_avg:44.56ms
step:1054/1775 train_time:46985ms step_avg:44.58ms
step:1055/1775 train_time:47044ms step_avg:44.59ms
step:1056/1775 train_time:47105ms step_avg:44.61ms
step:1057/1775 train_time:47164ms step_avg:44.62ms
step:1058/1775 train_time:47224ms step_avg:44.64ms
step:1059/1775 train_time:47282ms step_avg:44.65ms
step:1060/1775 train_time:47342ms step_avg:44.66ms
step:1061/1775 train_time:47399ms step_avg:44.67ms
step:1062/1775 train_time:47459ms step_avg:44.69ms
step:1063/1775 train_time:47516ms step_avg:44.70ms
step:1064/1775 train_time:47576ms step_avg:44.71ms
step:1065/1775 train_time:47633ms step_avg:44.73ms
step:1066/1775 train_time:47693ms step_avg:44.74ms
step:1067/1775 train_time:47751ms step_avg:44.75ms
step:1068/1775 train_time:47810ms step_avg:44.77ms
step:1069/1775 train_time:47869ms step_avg:44.78ms
step:1070/1775 train_time:47930ms step_avg:44.79ms
step:1071/1775 train_time:47989ms step_avg:44.81ms
step:1072/1775 train_time:48049ms step_avg:44.82ms
step:1073/1775 train_time:48107ms step_avg:44.83ms
step:1074/1775 train_time:48167ms step_avg:44.85ms
step:1075/1775 train_time:48225ms step_avg:44.86ms
step:1076/1775 train_time:48286ms step_avg:44.88ms
step:1077/1775 train_time:48345ms step_avg:44.89ms
step:1078/1775 train_time:48404ms step_avg:44.90ms
step:1079/1775 train_time:48463ms step_avg:44.91ms
step:1080/1775 train_time:48522ms step_avg:44.93ms
step:1081/1775 train_time:48580ms step_avg:44.94ms
step:1082/1775 train_time:48640ms step_avg:44.95ms
step:1083/1775 train_time:48697ms step_avg:44.97ms
step:1084/1775 train_time:48758ms step_avg:44.98ms
step:1085/1775 train_time:48816ms step_avg:44.99ms
step:1086/1775 train_time:48877ms step_avg:45.01ms
step:1087/1775 train_time:48935ms step_avg:45.02ms
step:1088/1775 train_time:48996ms step_avg:45.03ms
step:1089/1775 train_time:49055ms step_avg:45.05ms
step:1090/1775 train_time:49115ms step_avg:45.06ms
step:1091/1775 train_time:49172ms step_avg:45.07ms
step:1092/1775 train_time:49234ms step_avg:45.09ms
step:1093/1775 train_time:49292ms step_avg:45.10ms
step:1094/1775 train_time:49353ms step_avg:45.11ms
step:1095/1775 train_time:49410ms step_avg:45.12ms
step:1096/1775 train_time:49470ms step_avg:45.14ms
step:1097/1775 train_time:49527ms step_avg:45.15ms
step:1098/1775 train_time:49588ms step_avg:45.16ms
step:1099/1775 train_time:49646ms step_avg:45.17ms
step:1100/1775 train_time:49706ms step_avg:45.19ms
step:1101/1775 train_time:49765ms step_avg:45.20ms
step:1102/1775 train_time:49825ms step_avg:45.21ms
step:1103/1775 train_time:49883ms step_avg:45.22ms
step:1104/1775 train_time:49943ms step_avg:45.24ms
step:1105/1775 train_time:50002ms step_avg:45.25ms
step:1106/1775 train_time:50063ms step_avg:45.26ms
step:1107/1775 train_time:50122ms step_avg:45.28ms
step:1108/1775 train_time:50182ms step_avg:45.29ms
step:1109/1775 train_time:50240ms step_avg:45.30ms
step:1110/1775 train_time:50302ms step_avg:45.32ms
step:1111/1775 train_time:50359ms step_avg:45.33ms
step:1112/1775 train_time:50420ms step_avg:45.34ms
step:1113/1775 train_time:50477ms step_avg:45.35ms
step:1114/1775 train_time:50537ms step_avg:45.37ms
step:1115/1775 train_time:50595ms step_avg:45.38ms
step:1116/1775 train_time:50655ms step_avg:45.39ms
step:1117/1775 train_time:50713ms step_avg:45.40ms
step:1118/1775 train_time:50773ms step_avg:45.41ms
step:1119/1775 train_time:50831ms step_avg:45.43ms
step:1120/1775 train_time:50891ms step_avg:45.44ms
step:1121/1775 train_time:50949ms step_avg:45.45ms
step:1122/1775 train_time:51009ms step_avg:45.46ms
step:1123/1775 train_time:51067ms step_avg:45.47ms
step:1124/1775 train_time:51127ms step_avg:45.49ms
step:1125/1775 train_time:51186ms step_avg:45.50ms
step:1126/1775 train_time:51246ms step_avg:45.51ms
step:1127/1775 train_time:51304ms step_avg:45.52ms
step:1128/1775 train_time:51365ms step_avg:45.54ms
step:1129/1775 train_time:51423ms step_avg:45.55ms
step:1130/1775 train_time:51483ms step_avg:45.56ms
step:1131/1775 train_time:51542ms step_avg:45.57ms
step:1132/1775 train_time:51602ms step_avg:45.58ms
step:1133/1775 train_time:51660ms step_avg:45.60ms
step:1134/1775 train_time:51721ms step_avg:45.61ms
step:1135/1775 train_time:51778ms step_avg:45.62ms
step:1136/1775 train_time:51839ms step_avg:45.63ms
step:1137/1775 train_time:51897ms step_avg:45.64ms
step:1138/1775 train_time:51958ms step_avg:45.66ms
step:1139/1775 train_time:52015ms step_avg:45.67ms
step:1140/1775 train_time:52077ms step_avg:45.68ms
step:1141/1775 train_time:52133ms step_avg:45.69ms
step:1142/1775 train_time:52194ms step_avg:45.70ms
step:1143/1775 train_time:52253ms step_avg:45.72ms
step:1144/1775 train_time:52312ms step_avg:45.73ms
step:1145/1775 train_time:52371ms step_avg:45.74ms
step:1146/1775 train_time:52430ms step_avg:45.75ms
step:1147/1775 train_time:52490ms step_avg:45.76ms
step:1148/1775 train_time:52549ms step_avg:45.77ms
step:1149/1775 train_time:52607ms step_avg:45.79ms
step:1150/1775 train_time:52668ms step_avg:45.80ms
step:1151/1775 train_time:52727ms step_avg:45.81ms
step:1152/1775 train_time:52787ms step_avg:45.82ms
step:1153/1775 train_time:52845ms step_avg:45.83ms
step:1154/1775 train_time:52906ms step_avg:45.85ms
step:1155/1775 train_time:52965ms step_avg:45.86ms
step:1156/1775 train_time:53025ms step_avg:45.87ms
step:1157/1775 train_time:53083ms step_avg:45.88ms
step:1158/1775 train_time:53146ms step_avg:45.89ms
step:1159/1775 train_time:53230ms step_avg:45.93ms
step:1160/1775 train_time:53317ms step_avg:45.96ms
step:1161/1775 train_time:53400ms step_avg:46.00ms
step:1162/1775 train_time:53486ms step_avg:46.03ms
step:1163/1775 train_time:53570ms step_avg:46.06ms
step:1164/1775 train_time:53656ms step_avg:46.10ms
step:1165/1775 train_time:53741ms step_avg:46.13ms
step:1166/1775 train_time:53826ms step_avg:46.16ms
step:1167/1775 train_time:53911ms step_avg:46.20ms
step:1168/1775 train_time:53997ms step_avg:46.23ms
step:1169/1775 train_time:54080ms step_avg:46.26ms
step:1170/1775 train_time:54166ms step_avg:46.30ms
step:1171/1775 train_time:54249ms step_avg:46.33ms
step:1172/1775 train_time:54337ms step_avg:46.36ms
step:1173/1775 train_time:54421ms step_avg:46.40ms
step:1174/1775 train_time:54507ms step_avg:46.43ms
step:1175/1775 train_time:54590ms step_avg:46.46ms
step:1176/1775 train_time:54676ms step_avg:46.49ms
step:1177/1775 train_time:54759ms step_avg:46.52ms
step:1178/1775 train_time:54846ms step_avg:46.56ms
step:1179/1775 train_time:54930ms step_avg:46.59ms
step:1180/1775 train_time:55016ms step_avg:46.62ms
step:1181/1775 train_time:55101ms step_avg:46.66ms
step:1182/1775 train_time:55186ms step_avg:46.69ms
step:1183/1775 train_time:55270ms step_avg:46.72ms
step:1184/1775 train_time:55358ms step_avg:46.75ms
step:1185/1775 train_time:55441ms step_avg:46.79ms
step:1186/1775 train_time:55527ms step_avg:46.82ms
step:1187/1775 train_time:55611ms step_avg:46.85ms
step:1188/1775 train_time:55698ms step_avg:46.88ms
step:1189/1775 train_time:55780ms step_avg:46.91ms
step:1190/1775 train_time:55867ms step_avg:46.95ms
step:1191/1775 train_time:55951ms step_avg:46.98ms
step:1192/1775 train_time:56040ms step_avg:47.01ms
step:1193/1775 train_time:56124ms step_avg:47.04ms
step:1194/1775 train_time:56209ms step_avg:47.08ms
step:1195/1775 train_time:56293ms step_avg:47.11ms
step:1196/1775 train_time:56379ms step_avg:47.14ms
step:1197/1775 train_time:56463ms step_avg:47.17ms
step:1198/1775 train_time:56548ms step_avg:47.20ms
step:1199/1775 train_time:56633ms step_avg:47.23ms
step:1200/1775 train_time:56719ms step_avg:47.27ms
step:1201/1775 train_time:56803ms step_avg:47.30ms
step:1202/1775 train_time:56888ms step_avg:47.33ms
step:1203/1775 train_time:56972ms step_avg:47.36ms
step:1204/1775 train_time:57059ms step_avg:47.39ms
step:1205/1775 train_time:57142ms step_avg:47.42ms
step:1206/1775 train_time:57229ms step_avg:47.45ms
step:1207/1775 train_time:57313ms step_avg:47.48ms
step:1208/1775 train_time:57399ms step_avg:47.52ms
step:1209/1775 train_time:57482ms step_avg:47.54ms
step:1210/1775 train_time:57568ms step_avg:47.58ms
step:1211/1775 train_time:57652ms step_avg:47.61ms
step:1212/1775 train_time:57739ms step_avg:47.64ms
step:1213/1775 train_time:57822ms step_avg:47.67ms
step:1214/1775 train_time:57909ms step_avg:47.70ms
step:1215/1775 train_time:57992ms step_avg:47.73ms
step:1216/1775 train_time:58078ms step_avg:47.76ms
step:1217/1775 train_time:58163ms step_avg:47.79ms
step:1218/1775 train_time:58249ms step_avg:47.82ms
step:1219/1775 train_time:58332ms step_avg:47.85ms
step:1220/1775 train_time:58419ms step_avg:47.88ms
step:1221/1775 train_time:58502ms step_avg:47.91ms
step:1222/1775 train_time:58588ms step_avg:47.94ms
step:1223/1775 train_time:58671ms step_avg:47.97ms
step:1224/1775 train_time:58759ms step_avg:48.01ms
step:1225/1775 train_time:58843ms step_avg:48.04ms
step:1226/1775 train_time:58929ms step_avg:48.07ms
step:1227/1775 train_time:59012ms step_avg:48.09ms
step:1228/1775 train_time:59099ms step_avg:48.13ms
step:1229/1775 train_time:59182ms step_avg:48.15ms
step:1230/1775 train_time:59269ms step_avg:48.19ms
step:1231/1775 train_time:59352ms step_avg:48.21ms
step:1232/1775 train_time:59439ms step_avg:48.25ms
step:1233/1775 train_time:59522ms step_avg:48.27ms
step:1234/1775 train_time:59608ms step_avg:48.30ms
step:1235/1775 train_time:59692ms step_avg:48.33ms
step:1236/1775 train_time:59778ms step_avg:48.36ms
step:1237/1775 train_time:59860ms step_avg:48.39ms
step:1238/1775 train_time:59947ms step_avg:48.42ms
step:1239/1775 train_time:60033ms step_avg:48.45ms
step:1240/1775 train_time:60120ms step_avg:48.48ms
step:1241/1775 train_time:60204ms step_avg:48.51ms
step:1242/1775 train_time:60289ms step_avg:48.54ms
step:1243/1775 train_time:60373ms step_avg:48.57ms
step:1244/1775 train_time:60459ms step_avg:48.60ms
step:1245/1775 train_time:60542ms step_avg:48.63ms
step:1246/1775 train_time:60629ms step_avg:48.66ms
step:1247/1775 train_time:60713ms step_avg:48.69ms
step:1248/1775 train_time:60801ms step_avg:48.72ms
step:1249/1775 train_time:60883ms step_avg:48.75ms
step:1250/1775 train_time:60969ms step_avg:48.78ms
step:1250/1775 val_loss:3.5028 train_time:61069ms step_avg:48.86ms
step:1251/1775 train_time:61090ms step_avg:48.83ms
step:1252/1775 train_time:61144ms step_avg:48.84ms
step:1253/1775 train_time:61231ms step_avg:48.87ms
step:1254/1775 train_time:61319ms step_avg:48.90ms
step:1255/1775 train_time:61402ms step_avg:48.93ms
step:1256/1775 train_time:61490ms step_avg:48.96ms
step:1257/1775 train_time:61572ms step_avg:48.98ms
step:1258/1775 train_time:61658ms step_avg:49.01ms
step:1259/1775 train_time:61740ms step_avg:49.04ms
step:1260/1775 train_time:61825ms step_avg:49.07ms
step:1261/1775 train_time:61908ms step_avg:49.09ms
step:1262/1775 train_time:61996ms step_avg:49.12ms
step:1263/1775 train_time:62083ms step_avg:49.16ms
step:1264/1775 train_time:62172ms step_avg:49.19ms
step:1265/1775 train_time:62256ms step_avg:49.21ms
step:1266/1775 train_time:62344ms step_avg:49.25ms
step:1267/1775 train_time:62428ms step_avg:49.27ms
step:1268/1775 train_time:62514ms step_avg:49.30ms
step:1269/1775 train_time:62598ms step_avg:49.33ms
step:1270/1775 train_time:62683ms step_avg:49.36ms
step:1271/1775 train_time:62766ms step_avg:49.38ms
step:1272/1775 train_time:62852ms step_avg:49.41ms
step:1273/1775 train_time:62935ms step_avg:49.44ms
step:1274/1775 train_time:63023ms step_avg:49.47ms
step:1275/1775 train_time:63107ms step_avg:49.50ms
step:1276/1775 train_time:63196ms step_avg:49.53ms
step:1277/1775 train_time:63282ms step_avg:49.56ms
step:1278/1775 train_time:63368ms step_avg:49.58ms
step:1279/1775 train_time:63451ms step_avg:49.61ms
step:1280/1775 train_time:63538ms step_avg:49.64ms
step:1281/1775 train_time:63621ms step_avg:49.67ms
step:1282/1775 train_time:63706ms step_avg:49.69ms
step:1283/1775 train_time:63788ms step_avg:49.72ms
step:1284/1775 train_time:63873ms step_avg:49.75ms
step:1285/1775 train_time:63957ms step_avg:49.77ms
step:1286/1775 train_time:64044ms step_avg:49.80ms
step:1287/1775 train_time:64129ms step_avg:49.83ms
step:1288/1775 train_time:64218ms step_avg:49.86ms
step:1289/1775 train_time:64302ms step_avg:49.89ms
step:1290/1775 train_time:64389ms step_avg:49.91ms
step:1291/1775 train_time:64472ms step_avg:49.94ms
step:1292/1775 train_time:64559ms step_avg:49.97ms
step:1293/1775 train_time:64642ms step_avg:49.99ms
step:1294/1775 train_time:64727ms step_avg:50.02ms
step:1295/1775 train_time:64810ms step_avg:50.05ms
step:1296/1775 train_time:64896ms step_avg:50.07ms
step:1297/1775 train_time:64981ms step_avg:50.10ms
step:1298/1775 train_time:65067ms step_avg:50.13ms
step:1299/1775 train_time:65151ms step_avg:50.15ms
step:1300/1775 train_time:65240ms step_avg:50.18ms
step:1301/1775 train_time:65324ms step_avg:50.21ms
step:1302/1775 train_time:65410ms step_avg:50.24ms
step:1303/1775 train_time:65494ms step_avg:50.26ms
step:1304/1775 train_time:65581ms step_avg:50.29ms
step:1305/1775 train_time:65663ms step_avg:50.32ms
step:1306/1775 train_time:65748ms step_avg:50.34ms
step:1307/1775 train_time:65831ms step_avg:50.37ms
step:1308/1775 train_time:65918ms step_avg:50.40ms
step:1309/1775 train_time:66000ms step_avg:50.42ms
step:1310/1775 train_time:66087ms step_avg:50.45ms
step:1311/1775 train_time:66172ms step_avg:50.47ms
step:1312/1775 train_time:66261ms step_avg:50.50ms
step:1313/1775 train_time:66346ms step_avg:50.53ms
step:1314/1775 train_time:66431ms step_avg:50.56ms
step:1315/1775 train_time:66515ms step_avg:50.58ms
step:1316/1775 train_time:66601ms step_avg:50.61ms
step:1317/1775 train_time:66683ms step_avg:50.63ms
step:1318/1775 train_time:66771ms step_avg:50.66ms
step:1319/1775 train_time:66854ms step_avg:50.69ms
step:1320/1775 train_time:66942ms step_avg:50.71ms
step:1321/1775 train_time:67025ms step_avg:50.74ms
step:1322/1775 train_time:67110ms step_avg:50.76ms
step:1323/1775 train_time:67195ms step_avg:50.79ms
step:1324/1775 train_time:67285ms step_avg:50.82ms
step:1325/1775 train_time:67368ms step_avg:50.84ms
step:1326/1775 train_time:67455ms step_avg:50.87ms
step:1327/1775 train_time:67539ms step_avg:50.90ms
step:1328/1775 train_time:67624ms step_avg:50.92ms
step:1329/1775 train_time:67707ms step_avg:50.95ms
step:1330/1775 train_time:67793ms step_avg:50.97ms
step:1331/1775 train_time:67877ms step_avg:51.00ms
step:1332/1775 train_time:67964ms step_avg:51.02ms
step:1333/1775 train_time:68048ms step_avg:51.05ms
step:1334/1775 train_time:68135ms step_avg:51.08ms
step:1335/1775 train_time:68220ms step_avg:51.10ms
step:1336/1775 train_time:68306ms step_avg:51.13ms
step:1337/1775 train_time:68391ms step_avg:51.15ms
step:1338/1775 train_time:68478ms step_avg:51.18ms
step:1339/1775 train_time:68561ms step_avg:51.20ms
step:1340/1775 train_time:68647ms step_avg:51.23ms
step:1341/1775 train_time:68730ms step_avg:51.25ms
step:1342/1775 train_time:68816ms step_avg:51.28ms
step:1343/1775 train_time:68899ms step_avg:51.30ms
step:1344/1775 train_time:68985ms step_avg:51.33ms
step:1345/1775 train_time:69069ms step_avg:51.35ms
step:1346/1775 train_time:69157ms step_avg:51.38ms
step:1347/1775 train_time:69242ms step_avg:51.40ms
step:1348/1775 train_time:69328ms step_avg:51.43ms
step:1349/1775 train_time:69412ms step_avg:51.45ms
step:1350/1775 train_time:69498ms step_avg:51.48ms
step:1351/1775 train_time:69581ms step_avg:51.50ms
step:1352/1775 train_time:69666ms step_avg:51.53ms
step:1353/1775 train_time:69749ms step_avg:51.55ms
step:1354/1775 train_time:69836ms step_avg:51.58ms
step:1355/1775 train_time:69920ms step_avg:51.60ms
step:1356/1775 train_time:70005ms step_avg:51.63ms
step:1357/1775 train_time:70089ms step_avg:51.65ms
step:1358/1775 train_time:70176ms step_avg:51.68ms
step:1359/1775 train_time:70261ms step_avg:51.70ms
step:1360/1775 train_time:70346ms step_avg:51.73ms
step:1361/1775 train_time:70430ms step_avg:51.75ms
step:1362/1775 train_time:70518ms step_avg:51.78ms
step:1363/1775 train_time:70602ms step_avg:51.80ms
step:1364/1775 train_time:70687ms step_avg:51.82ms
step:1365/1775 train_time:70770ms step_avg:51.85ms
step:1366/1775 train_time:70858ms step_avg:51.87ms
step:1367/1775 train_time:70942ms step_avg:51.90ms
step:1368/1775 train_time:71028ms step_avg:51.92ms
step:1369/1775 train_time:71111ms step_avg:51.94ms
step:1370/1775 train_time:71197ms step_avg:51.97ms
step:1371/1775 train_time:71281ms step_avg:51.99ms
step:1372/1775 train_time:71368ms step_avg:52.02ms
step:1373/1775 train_time:71453ms step_avg:52.04ms
step:1374/1775 train_time:71539ms step_avg:52.07ms
step:1375/1775 train_time:71623ms step_avg:52.09ms
step:1376/1775 train_time:71708ms step_avg:52.11ms
step:1377/1775 train_time:71791ms step_avg:52.14ms
step:1378/1775 train_time:71878ms step_avg:52.16ms
step:1379/1775 train_time:71962ms step_avg:52.18ms
step:1380/1775 train_time:72048ms step_avg:52.21ms
step:1381/1775 train_time:72132ms step_avg:52.23ms
step:1382/1775 train_time:72219ms step_avg:52.26ms
step:1383/1775 train_time:72302ms step_avg:52.28ms
step:1384/1775 train_time:72388ms step_avg:52.30ms
step:1385/1775 train_time:72472ms step_avg:52.33ms
step:1386/1775 train_time:72559ms step_avg:52.35ms
step:1387/1775 train_time:72642ms step_avg:52.37ms
step:1388/1775 train_time:72727ms step_avg:52.40ms
step:1389/1775 train_time:72811ms step_avg:52.42ms
step:1390/1775 train_time:72899ms step_avg:52.45ms
step:1391/1775 train_time:72983ms step_avg:52.47ms
step:1392/1775 train_time:73068ms step_avg:52.49ms
step:1393/1775 train_time:73152ms step_avg:52.51ms
step:1394/1775 train_time:73238ms step_avg:52.54ms
step:1395/1775 train_time:73322ms step_avg:52.56ms
step:1396/1775 train_time:73409ms step_avg:52.59ms
step:1397/1775 train_time:73492ms step_avg:52.61ms
step:1398/1775 train_time:73579ms step_avg:52.63ms
step:1399/1775 train_time:73662ms step_avg:52.65ms
step:1400/1775 train_time:73748ms step_avg:52.68ms
step:1401/1775 train_time:73832ms step_avg:52.70ms
step:1402/1775 train_time:73920ms step_avg:52.72ms
step:1403/1775 train_time:74003ms step_avg:52.75ms
step:1404/1775 train_time:74089ms step_avg:52.77ms
step:1405/1775 train_time:74172ms step_avg:52.79ms
step:1406/1775 train_time:74260ms step_avg:52.82ms
step:1407/1775 train_time:74343ms step_avg:52.84ms
step:1408/1775 train_time:74430ms step_avg:52.86ms
step:1409/1775 train_time:74513ms step_avg:52.88ms
step:1410/1775 train_time:74600ms step_avg:52.91ms
step:1411/1775 train_time:74684ms step_avg:52.93ms
step:1412/1775 train_time:74770ms step_avg:52.95ms
step:1413/1775 train_time:74853ms step_avg:52.97ms
step:1414/1775 train_time:74941ms step_avg:53.00ms
step:1415/1775 train_time:75024ms step_avg:53.02ms
step:1416/1775 train_time:75111ms step_avg:53.04ms
step:1417/1775 train_time:75194ms step_avg:53.07ms
step:1418/1775 train_time:75281ms step_avg:53.09ms
step:1419/1775 train_time:75364ms step_avg:53.11ms
step:1420/1775 train_time:75452ms step_avg:53.13ms
step:1421/1775 train_time:75535ms step_avg:53.16ms
step:1422/1775 train_time:75622ms step_avg:53.18ms
step:1423/1775 train_time:75705ms step_avg:53.20ms
step:1424/1775 train_time:75790ms step_avg:53.22ms
step:1425/1775 train_time:75874ms step_avg:53.24ms
step:1426/1775 train_time:75961ms step_avg:53.27ms
step:1427/1775 train_time:76045ms step_avg:53.29ms
step:1428/1775 train_time:76131ms step_avg:53.31ms
step:1429/1775 train_time:76214ms step_avg:53.33ms
step:1430/1775 train_time:76301ms step_avg:53.36ms
step:1431/1775 train_time:76384ms step_avg:53.38ms
step:1432/1775 train_time:76471ms step_avg:53.40ms
step:1433/1775 train_time:76555ms step_avg:53.42ms
step:1434/1775 train_time:76641ms step_avg:53.45ms
step:1435/1775 train_time:76724ms step_avg:53.47ms
step:1436/1775 train_time:76810ms step_avg:53.49ms
step:1437/1775 train_time:76894ms step_avg:53.51ms
step:1438/1775 train_time:76981ms step_avg:53.53ms
step:1439/1775 train_time:77064ms step_avg:53.55ms
step:1440/1775 train_time:77150ms step_avg:53.58ms
step:1441/1775 train_time:77233ms step_avg:53.60ms
step:1442/1775 train_time:77320ms step_avg:53.62ms
step:1443/1775 train_time:77403ms step_avg:53.64ms
step:1444/1775 train_time:77490ms step_avg:53.66ms
step:1445/1775 train_time:77573ms step_avg:53.68ms
step:1446/1775 train_time:77660ms step_avg:53.71ms
step:1447/1775 train_time:77743ms step_avg:53.73ms
step:1448/1775 train_time:77829ms step_avg:53.75ms
step:1449/1775 train_time:77914ms step_avg:53.77ms
step:1450/1775 train_time:78000ms step_avg:53.79ms
step:1451/1775 train_time:78084ms step_avg:53.81ms
step:1452/1775 train_time:78169ms step_avg:53.84ms
step:1453/1775 train_time:78253ms step_avg:53.86ms
step:1454/1775 train_time:78340ms step_avg:53.88ms
step:1455/1775 train_time:78423ms step_avg:53.90ms
step:1456/1775 train_time:78511ms step_avg:53.92ms
step:1457/1775 train_time:78594ms step_avg:53.94ms
step:1458/1775 train_time:78680ms step_avg:53.96ms
step:1459/1775 train_time:78764ms step_avg:53.98ms
step:1460/1775 train_time:78850ms step_avg:54.01ms
step:1461/1775 train_time:78935ms step_avg:54.03ms
step:1462/1775 train_time:79022ms step_avg:54.05ms
step:1463/1775 train_time:79106ms step_avg:54.07ms
step:1464/1775 train_time:79191ms step_avg:54.09ms
step:1465/1775 train_time:79274ms step_avg:54.11ms
step:1466/1775 train_time:79362ms step_avg:54.14ms
step:1467/1775 train_time:79446ms step_avg:54.16ms
step:1468/1775 train_time:79532ms step_avg:54.18ms
step:1469/1775 train_time:79616ms step_avg:54.20ms
step:1470/1775 train_time:79703ms step_avg:54.22ms
step:1471/1775 train_time:79787ms step_avg:54.24ms
step:1472/1775 train_time:79873ms step_avg:54.26ms
step:1473/1775 train_time:79958ms step_avg:54.28ms
step:1474/1775 train_time:80044ms step_avg:54.30ms
step:1475/1775 train_time:80127ms step_avg:54.32ms
step:1476/1775 train_time:80213ms step_avg:54.34ms
step:1477/1775 train_time:80297ms step_avg:54.37ms
step:1478/1775 train_time:80384ms step_avg:54.39ms
step:1479/1775 train_time:80466ms step_avg:54.41ms
step:1480/1775 train_time:80553ms step_avg:54.43ms
step:1481/1775 train_time:80638ms step_avg:54.45ms
step:1482/1775 train_time:80724ms step_avg:54.47ms
step:1483/1775 train_time:80807ms step_avg:54.49ms
step:1484/1775 train_time:80894ms step_avg:54.51ms
step:1485/1775 train_time:80979ms step_avg:54.53ms
step:1486/1775 train_time:81064ms step_avg:54.55ms
step:1487/1775 train_time:81148ms step_avg:54.57ms
step:1488/1775 train_time:81235ms step_avg:54.59ms
step:1489/1775 train_time:81319ms step_avg:54.61ms
step:1490/1775 train_time:81405ms step_avg:54.63ms
step:1491/1775 train_time:81488ms step_avg:54.65ms
step:1492/1775 train_time:81575ms step_avg:54.68ms
step:1493/1775 train_time:81661ms step_avg:54.70ms
step:1494/1775 train_time:81746ms step_avg:54.72ms
step:1495/1775 train_time:81831ms step_avg:54.74ms
step:1496/1775 train_time:81918ms step_avg:54.76ms
step:1497/1775 train_time:82001ms step_avg:54.78ms
step:1498/1775 train_time:82087ms step_avg:54.80ms
step:1499/1775 train_time:82170ms step_avg:54.82ms
step:1500/1775 train_time:82256ms step_avg:54.84ms
step:1500/1775 val_loss:3.3739 train_time:82354ms step_avg:54.90ms
step:1501/1775 train_time:82375ms step_avg:54.88ms
step:1502/1775 train_time:82429ms step_avg:54.88ms
step:1503/1775 train_time:82515ms step_avg:54.90ms
step:1504/1775 train_time:82605ms step_avg:54.92ms
step:1505/1775 train_time:82688ms step_avg:54.94ms
step:1506/1775 train_time:82775ms step_avg:54.96ms
step:1507/1775 train_time:82858ms step_avg:54.98ms
step:1508/1775 train_time:82944ms step_avg:55.00ms
step:1509/1775 train_time:83025ms step_avg:55.02ms
step:1510/1775 train_time:83111ms step_avg:55.04ms
step:1511/1775 train_time:83194ms step_avg:55.06ms
step:1512/1775 train_time:83280ms step_avg:55.08ms
step:1513/1775 train_time:83365ms step_avg:55.10ms
step:1514/1775 train_time:83454ms step_avg:55.12ms
step:1515/1775 train_time:83540ms step_avg:55.14ms
step:1516/1775 train_time:83626ms step_avg:55.16ms
step:1517/1775 train_time:83710ms step_avg:55.18ms
step:1518/1775 train_time:83797ms step_avg:55.20ms
step:1519/1775 train_time:83880ms step_avg:55.22ms
step:1520/1775 train_time:83965ms step_avg:55.24ms
step:1521/1775 train_time:84048ms step_avg:55.26ms
step:1522/1775 train_time:84134ms step_avg:55.28ms
step:1523/1775 train_time:84216ms step_avg:55.30ms
step:1524/1775 train_time:84304ms step_avg:55.32ms
step:1525/1775 train_time:84389ms step_avg:55.34ms
step:1526/1775 train_time:84476ms step_avg:55.36ms
step:1527/1775 train_time:84561ms step_avg:55.38ms
step:1528/1775 train_time:84648ms step_avg:55.40ms
step:1529/1775 train_time:84731ms step_avg:55.42ms
step:1530/1775 train_time:84817ms step_avg:55.44ms
step:1531/1775 train_time:84901ms step_avg:55.45ms
step:1532/1775 train_time:84987ms step_avg:55.47ms
step:1533/1775 train_time:85070ms step_avg:55.49ms
step:1534/1775 train_time:85157ms step_avg:55.51ms
step:1535/1775 train_time:85240ms step_avg:55.53ms
step:1536/1775 train_time:85326ms step_avg:55.55ms
step:1537/1775 train_time:85410ms step_avg:55.57ms
step:1538/1775 train_time:85498ms step_avg:55.59ms
step:1539/1775 train_time:85583ms step_avg:55.61ms
step:1540/1775 train_time:85669ms step_avg:55.63ms
step:1541/1775 train_time:85753ms step_avg:55.65ms
step:1542/1775 train_time:85840ms step_avg:55.67ms
step:1543/1775 train_time:85922ms step_avg:55.68ms
step:1544/1775 train_time:86008ms step_avg:55.70ms
step:1545/1775 train_time:86094ms step_avg:55.72ms
step:1546/1775 train_time:86180ms step_avg:55.74ms
step:1547/1775 train_time:86264ms step_avg:55.76ms
step:1548/1775 train_time:86350ms step_avg:55.78ms
step:1549/1775 train_time:86433ms step_avg:55.80ms
step:1550/1775 train_time:86520ms step_avg:55.82ms
step:1551/1775 train_time:86604ms step_avg:55.84ms
step:1552/1775 train_time:86692ms step_avg:55.86ms
step:1553/1775 train_time:86775ms step_avg:55.88ms
step:1554/1775 train_time:86862ms step_avg:55.90ms
step:1555/1775 train_time:86946ms step_avg:55.91ms
step:1556/1775 train_time:87032ms step_avg:55.93ms
step:1557/1775 train_time:87115ms step_avg:55.95ms
step:1558/1775 train_time:87201ms step_avg:55.97ms
step:1559/1775 train_time:87284ms step_avg:55.99ms
step:1560/1775 train_time:87371ms step_avg:56.01ms
step:1561/1775 train_time:87455ms step_avg:56.03ms
step:1562/1775 train_time:87542ms step_avg:56.05ms
step:1563/1775 train_time:87626ms step_avg:56.06ms
step:1564/1775 train_time:87712ms step_avg:56.08ms
step:1565/1775 train_time:87796ms step_avg:56.10ms
step:1566/1775 train_time:87883ms step_avg:56.12ms
step:1567/1775 train_time:87966ms step_avg:56.14ms
step:1568/1775 train_time:88052ms step_avg:56.16ms
step:1569/1775 train_time:88137ms step_avg:56.17ms
step:1570/1775 train_time:88223ms step_avg:56.19ms
step:1571/1775 train_time:88306ms step_avg:56.21ms
step:1572/1775 train_time:88392ms step_avg:56.23ms
step:1573/1775 train_time:88476ms step_avg:56.25ms
step:1574/1775 train_time:88563ms step_avg:56.27ms
step:1575/1775 train_time:88646ms step_avg:56.28ms
step:1576/1775 train_time:88733ms step_avg:56.30ms
step:1577/1775 train_time:88817ms step_avg:56.32ms
step:1578/1775 train_time:88903ms step_avg:56.34ms
step:1579/1775 train_time:88986ms step_avg:56.36ms
step:1580/1775 train_time:89073ms step_avg:56.38ms
step:1581/1775 train_time:89156ms step_avg:56.39ms
step:1582/1775 train_time:89243ms step_avg:56.41ms
step:1583/1775 train_time:89326ms step_avg:56.43ms
step:1584/1775 train_time:89413ms step_avg:56.45ms
step:1585/1775 train_time:89496ms step_avg:56.46ms
step:1586/1775 train_time:89584ms step_avg:56.48ms
step:1587/1775 train_time:89668ms step_avg:56.50ms
step:1588/1775 train_time:89755ms step_avg:56.52ms
step:1589/1775 train_time:89840ms step_avg:56.54ms
step:1590/1775 train_time:89925ms step_avg:56.56ms
step:1591/1775 train_time:90008ms step_avg:56.57ms
step:1592/1775 train_time:90095ms step_avg:56.59ms
step:1593/1775 train_time:90178ms step_avg:56.61ms
step:1594/1775 train_time:90265ms step_avg:56.63ms
step:1595/1775 train_time:90348ms step_avg:56.64ms
step:1596/1775 train_time:90436ms step_avg:56.66ms
step:1597/1775 train_time:90520ms step_avg:56.68ms
step:1598/1775 train_time:90605ms step_avg:56.70ms
step:1599/1775 train_time:90689ms step_avg:56.72ms
step:1600/1775 train_time:90777ms step_avg:56.74ms
step:1601/1775 train_time:90862ms step_avg:56.75ms
step:1602/1775 train_time:90947ms step_avg:56.77ms
step:1603/1775 train_time:91029ms step_avg:56.79ms
step:1604/1775 train_time:91115ms step_avg:56.80ms
step:1605/1775 train_time:91200ms step_avg:56.82ms
step:1606/1775 train_time:91285ms step_avg:56.84ms
step:1607/1775 train_time:91370ms step_avg:56.86ms
step:1608/1775 train_time:91457ms step_avg:56.88ms
step:1609/1775 train_time:91541ms step_avg:56.89ms
step:1610/1775 train_time:91627ms step_avg:56.91ms
step:1611/1775 train_time:91711ms step_avg:56.93ms
step:1612/1775 train_time:91799ms step_avg:56.95ms
step:1613/1775 train_time:91883ms step_avg:56.96ms
step:1614/1775 train_time:91968ms step_avg:56.98ms
step:1615/1775 train_time:92052ms step_avg:57.00ms
step:1616/1775 train_time:92138ms step_avg:57.02ms
step:1617/1775 train_time:92221ms step_avg:57.03ms
step:1618/1775 train_time:92307ms step_avg:57.05ms
step:1619/1775 train_time:92390ms step_avg:57.07ms
step:1620/1775 train_time:92477ms step_avg:57.08ms
step:1621/1775 train_time:92562ms step_avg:57.10ms
step:1622/1775 train_time:92648ms step_avg:57.12ms
step:1623/1775 train_time:92732ms step_avg:57.14ms
step:1624/1775 train_time:92819ms step_avg:57.15ms
step:1625/1775 train_time:92902ms step_avg:57.17ms
step:1626/1775 train_time:92987ms step_avg:57.19ms
step:1627/1775 train_time:93070ms step_avg:57.20ms
step:1628/1775 train_time:93157ms step_avg:57.22ms
step:1629/1775 train_time:93242ms step_avg:57.24ms
step:1630/1775 train_time:93328ms step_avg:57.26ms
step:1631/1775 train_time:93412ms step_avg:57.27ms
step:1632/1775 train_time:93498ms step_avg:57.29ms
step:1633/1775 train_time:93582ms step_avg:57.31ms
step:1634/1775 train_time:93667ms step_avg:57.32ms
step:1635/1775 train_time:93751ms step_avg:57.34ms
step:1636/1775 train_time:93838ms step_avg:57.36ms
step:1637/1775 train_time:93921ms step_avg:57.37ms
step:1638/1775 train_time:94007ms step_avg:57.39ms
step:1639/1775 train_time:94091ms step_avg:57.41ms
step:1640/1775 train_time:94178ms step_avg:57.43ms
step:1641/1775 train_time:94262ms step_avg:57.44ms
step:1642/1775 train_time:94348ms step_avg:57.46ms
step:1643/1775 train_time:94431ms step_avg:57.47ms
step:1644/1775 train_time:94517ms step_avg:57.49ms
step:1645/1775 train_time:94602ms step_avg:57.51ms
step:1646/1775 train_time:94688ms step_avg:57.53ms
step:1647/1775 train_time:94771ms step_avg:57.54ms
step:1648/1775 train_time:94858ms step_avg:57.56ms
step:1649/1775 train_time:94942ms step_avg:57.58ms
step:1650/1775 train_time:95027ms step_avg:57.59ms
step:1651/1775 train_time:95111ms step_avg:57.61ms
step:1652/1775 train_time:95196ms step_avg:57.62ms
step:1653/1775 train_time:95281ms step_avg:57.64ms
step:1654/1775 train_time:95367ms step_avg:57.66ms
step:1655/1775 train_time:95450ms step_avg:57.67ms
step:1656/1775 train_time:95538ms step_avg:57.69ms
step:1657/1775 train_time:95621ms step_avg:57.71ms
step:1658/1775 train_time:95707ms step_avg:57.72ms
step:1659/1775 train_time:95791ms step_avg:57.74ms
step:1660/1775 train_time:95879ms step_avg:57.76ms
step:1661/1775 train_time:95962ms step_avg:57.77ms
step:1662/1775 train_time:96048ms step_avg:57.79ms
step:1663/1775 train_time:96131ms step_avg:57.81ms
step:1664/1775 train_time:96218ms step_avg:57.82ms
step:1665/1775 train_time:96301ms step_avg:57.84ms
step:1666/1775 train_time:96387ms step_avg:57.86ms
step:1667/1775 train_time:96471ms step_avg:57.87ms
step:1668/1775 train_time:96559ms step_avg:57.89ms
step:1669/1775 train_time:96643ms step_avg:57.90ms
step:1670/1775 train_time:96728ms step_avg:57.92ms
step:1671/1775 train_time:96811ms step_avg:57.94ms
step:1672/1775 train_time:96898ms step_avg:57.95ms
step:1673/1775 train_time:96983ms step_avg:57.97ms
step:1674/1775 train_time:97068ms step_avg:57.99ms
step:1675/1775 train_time:97151ms step_avg:58.00ms
step:1676/1775 train_time:97238ms step_avg:58.02ms
step:1677/1775 train_time:97322ms step_avg:58.03ms
step:1678/1775 train_time:97408ms step_avg:58.05ms
step:1679/1775 train_time:97491ms step_avg:58.06ms
step:1680/1775 train_time:97577ms step_avg:58.08ms
step:1681/1775 train_time:97661ms step_avg:58.10ms
step:1682/1775 train_time:97747ms step_avg:58.11ms
step:1683/1775 train_time:97830ms step_avg:58.13ms
step:1684/1775 train_time:97917ms step_avg:58.15ms
step:1685/1775 train_time:98001ms step_avg:58.16ms
step:1686/1775 train_time:98087ms step_avg:58.18ms
step:1687/1775 train_time:98171ms step_avg:58.19ms
step:1688/1775 train_time:98258ms step_avg:58.21ms
step:1689/1775 train_time:98342ms step_avg:58.22ms
step:1690/1775 train_time:98427ms step_avg:58.24ms
step:1691/1775 train_time:98511ms step_avg:58.26ms
step:1692/1775 train_time:98597ms step_avg:58.27ms
step:1693/1775 train_time:98682ms step_avg:58.29ms
step:1694/1775 train_time:98767ms step_avg:58.30ms
step:1695/1775 train_time:98851ms step_avg:58.32ms
step:1696/1775 train_time:98939ms step_avg:58.34ms
step:1697/1775 train_time:99022ms step_avg:58.35ms
step:1698/1775 train_time:99108ms step_avg:58.37ms
step:1699/1775 train_time:99192ms step_avg:58.38ms
step:1700/1775 train_time:99278ms step_avg:58.40ms
step:1701/1775 train_time:99362ms step_avg:58.41ms
step:1702/1775 train_time:99448ms step_avg:58.43ms
step:1703/1775 train_time:99531ms step_avg:58.44ms
step:1704/1775 train_time:99618ms step_avg:58.46ms
step:1705/1775 train_time:99702ms step_avg:58.48ms
step:1706/1775 train_time:99787ms step_avg:58.49ms
step:1707/1775 train_time:99872ms step_avg:58.51ms
step:1708/1775 train_time:99959ms step_avg:58.52ms
step:1709/1775 train_time:100042ms step_avg:58.54ms
step:1710/1775 train_time:100128ms step_avg:58.55ms
step:1711/1775 train_time:100212ms step_avg:58.57ms
step:1712/1775 train_time:100298ms step_avg:58.59ms
step:1713/1775 train_time:100382ms step_avg:58.60ms
step:1714/1775 train_time:100469ms step_avg:58.62ms
step:1715/1775 train_time:100553ms step_avg:58.63ms
step:1716/1775 train_time:100640ms step_avg:58.65ms
step:1717/1775 train_time:100722ms step_avg:58.66ms
step:1718/1775 train_time:100809ms step_avg:58.68ms
step:1719/1775 train_time:100893ms step_avg:58.69ms
step:1720/1775 train_time:100979ms step_avg:58.71ms
step:1721/1775 train_time:101063ms step_avg:58.72ms
step:1722/1775 train_time:101148ms step_avg:58.74ms
step:1723/1775 train_time:101232ms step_avg:58.75ms
step:1724/1775 train_time:101319ms step_avg:58.77ms
step:1725/1775 train_time:101403ms step_avg:58.78ms
step:1726/1775 train_time:101488ms step_avg:58.80ms
step:1727/1775 train_time:101572ms step_avg:58.81ms
step:1728/1775 train_time:101658ms step_avg:58.83ms
step:1729/1775 train_time:101742ms step_avg:58.84ms
step:1730/1775 train_time:101827ms step_avg:58.86ms
step:1731/1775 train_time:101911ms step_avg:58.87ms
step:1732/1775 train_time:101999ms step_avg:58.89ms
step:1733/1775 train_time:102083ms step_avg:58.91ms
step:1734/1775 train_time:102168ms step_avg:58.92ms
step:1735/1775 train_time:102251ms step_avg:58.93ms
step:1736/1775 train_time:102342ms step_avg:58.95ms
step:1737/1775 train_time:102426ms step_avg:58.97ms
step:1738/1775 train_time:102514ms step_avg:58.98ms
step:1739/1775 train_time:102599ms step_avg:59.00ms
step:1740/1775 train_time:102685ms step_avg:59.01ms
step:1741/1775 train_time:102769ms step_avg:59.03ms
step:1742/1775 train_time:102855ms step_avg:59.04ms
step:1743/1775 train_time:102940ms step_avg:59.06ms
step:1744/1775 train_time:103026ms step_avg:59.07ms
step:1745/1775 train_time:103111ms step_avg:59.09ms
step:1746/1775 train_time:103198ms step_avg:59.11ms
step:1747/1775 train_time:103283ms step_avg:59.12ms
step:1748/1775 train_time:103369ms step_avg:59.14ms
step:1749/1775 train_time:103453ms step_avg:59.15ms
step:1750/1775 train_time:103539ms step_avg:59.17ms
step:1750/1775 val_loss:3.2831 train_time:103639ms step_avg:59.22ms
step:1751/1775 train_time:103659ms step_avg:59.20ms
step:1752/1775 train_time:103713ms step_avg:59.20ms
step:1753/1775 train_time:103802ms step_avg:59.21ms
step:1754/1775 train_time:103893ms step_avg:59.23ms
step:1755/1775 train_time:103977ms step_avg:59.25ms
step:1756/1775 train_time:104062ms step_avg:59.26ms
step:1757/1775 train_time:104145ms step_avg:59.27ms
step:1758/1775 train_time:104230ms step_avg:59.29ms
step:1759/1775 train_time:104314ms step_avg:59.30ms
step:1760/1775 train_time:104399ms step_avg:59.32ms
step:1761/1775 train_time:104481ms step_avg:59.33ms
step:1762/1775 train_time:104567ms step_avg:59.35ms
step:1763/1775 train_time:104654ms step_avg:59.36ms
step:1764/1775 train_time:104742ms step_avg:59.38ms
step:1765/1775 train_time:104831ms step_avg:59.39ms
step:1766/1775 train_time:104919ms step_avg:59.41ms
step:1767/1775 train_time:105004ms step_avg:59.42ms
step:1768/1775 train_time:105090ms step_avg:59.44ms
step:1769/1775 train_time:105173ms step_avg:59.45ms
step:1770/1775 train_time:105259ms step_avg:59.47ms
step:1771/1775 train_time:105342ms step_avg:59.48ms
step:1772/1775 train_time:105429ms step_avg:59.50ms
step:1773/1775 train_time:105512ms step_avg:59.51ms
step:1774/1775 train_time:105599ms step_avg:59.53ms
step:1775/1775 train_time:105684ms step_avg:59.54ms
step:1775/1775 val_loss:3.2768 train_time:105786ms step_avg:59.60ms
peak memory allocated: 29148 MiB reserved: 44858 MiB

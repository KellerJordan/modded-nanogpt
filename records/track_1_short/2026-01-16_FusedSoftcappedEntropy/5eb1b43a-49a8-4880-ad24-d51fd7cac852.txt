import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977

        # This call computes relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# Fused Softcapped Cross Entropy

@triton.jit
def fused_softcapped_entropy_fwd_kernel(
    logits_ptr, losses_ptr, lse_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)
    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    
    max_val = -float('inf')
    sum_exp = 0.0
    
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)
        z = A * tl.sigmoid((val + B) / C)
        z = tl.where(mask, z, -float('inf'))
        curr_max = tl.max(z, axis=0)
        new_max = tl.maximum(max_val, curr_max)
        sum_exp = sum_exp * tl.exp(max_val - new_max) + tl.sum(tl.exp(z - new_max), axis=0)
        max_val = new_max
    
    lse = max_val + tl.log(sum_exp)
    tl.store(lse_ptr + row_idx, lse)
    
    total_loss = 0.0
    for k in range(n_predict):
        target_idx = row_idx + k
        if target_idx < n_rows:
            weight = tl.load(mtp_weights_ptr + k)
            if weight > 0:
                target = tl.load(targets_ptr + target_idx).to(tl.int32)
                if target >= 0 and target < n_cols:
                    val_target = tl.load(logits_row_ptr + target).to(tl.float32)
                    z_target = A * tl.sigmoid((val_target + B) / C)
                    total_loss += weight * (lse - z_target)
    
    tl.store(losses_ptr + row_idx, total_loss)

@triton.jit
def fused_softcapped_entropy_bwd_kernel(
    grad_input_ptr, grad_output_ptr, lse_ptr, logits_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v, stride_grad_n, stride_grad_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)

    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    grad_row_ptr = grad_input_ptr + row_idx * stride_grad_n
    
    lse = tl.load(lse_ptr + row_idx)
    grad_loss = tl.load(grad_output_ptr + row_idx)
    
    S_w = 0.0
    for k in range(n_predict):
        if row_idx + k < n_rows:
            S_w += tl.load(mtp_weights_ptr + k)
            
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=0.0).to(tl.float32)
        u = (val + B) / C
        sigmoid_u = tl.sigmoid(u)
        z = A * sigmoid_u
        p = tl.exp(z - lse)
        
        term1 = S_w * p
        term2 = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
        for k in range(n_predict):
            if row_idx + k < n_rows:
                target = tl.load(targets_ptr + row_idx + k).to(tl.int32)
                weight = tl.load(mtp_weights_ptr + k)
                term2 += tl.where(cols == target, weight, 0.0)
        
        grad_z = grad_loss * (term1 - term2)
        dz_dx = (1.0 / C) * z * (1.0 - sigmoid_u)
        grad_x = grad_z * dz_dx
        tl.store(grad_row_ptr + cols, grad_x.to(tl.bfloat16), mask=mask)

class FusedSoftcappedCrossEntropy(torch.autograd.Function):
    @staticmethod
    def forward(ctx, logits, targets, mtp_weights, A=23.0, B=5.0, C=7.5):
        n_rows, n_cols = logits.shape
        if mtp_weights is None:
             mtp_weights = torch.tensor([1.0], device=logits.device, dtype=torch.float32)
        n_predict = mtp_weights.shape[0]

        losses = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        lse = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        
        logits = logits.contiguous()
        targets = targets.contiguous()
        mtp_weights = mtp_weights.contiguous()

        grid = (n_rows,)
        fused_softcapped_entropy_fwd_kernel[grid](
            logits, losses, lse, targets, mtp_weights,
            logits.stride(0), logits.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        
        ctx.save_for_backward(logits, targets, mtp_weights, lse)
        ctx.params = (A, B, C)
        return losses

    @staticmethod
    def backward(ctx, grad_output):
        logits, targets, mtp_weights, lse = ctx.saved_tensors
        A, B, C = ctx.params
        n_rows, n_cols = logits.shape
        n_predict = mtp_weights.shape[0]
        
        grad_input = torch.empty((n_rows, n_cols), dtype=torch.bfloat16, device=logits.device)
        grad_output = grad_output.contiguous()
        
        grid = (n_rows,)
        fused_softcapped_entropy_bwd_kernel[grid](
            grad_input, grad_output, lse, logits, targets, mtp_weights,
            logits.stride(0), logits.stride(1), grad_input.stride(0), grad_input.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        return grad_input, None, None, None, None, None

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if self.training:
            losses = FusedSoftcappedCrossEntropy.apply(logits.view(-1, logits.size(-1)), target_seq, mtp_weights)
            loss = losses.sum()
        else:
            logits = 23 * torch.sigmoid((logits + 5) / 7.5)
            logits_for_loss = logits.float()
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Jan  8 2026, 06:52:19) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan 15 22:19:29 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   43C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   36C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   34C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   43C    P0            131W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   36C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   41C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   40C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    332072      C   /usr/bin/python3                             1510MiB |
|    1   N/A  N/A    332073      C   /usr/bin/python3                             1510MiB |
|    2   N/A  N/A    332074      C   /usr/bin/python3                             1510MiB |
|    3   N/A  N/A    332075      C   /usr/bin/python3                             1510MiB |
|    4   N/A  N/A    332076      C   /usr/bin/python3                             1510MiB |
|    5   N/A  N/A    332077      C   /usr/bin/python3                             1510MiB |
|    6   N/A  N/A    332078      C   /usr/bin/python3                             1510MiB |
|    7   N/A  N/A    332079      C   /usr/bin/python3                             1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8297 train_time:0ms step_avg:0.03ms
step:1/1775 train_time:90ms step_avg:89.92ms
step:2/1775 train_time:112ms step_avg:56.15ms
step:3/1775 train_time:132ms step_avg:44.03ms
step:4/1775 train_time:154ms step_avg:38.62ms
step:5/1775 train_time:185ms step_avg:37.07ms
step:6/1775 train_time:277ms step_avg:46.09ms
step:7/1775 train_time:295ms step_avg:42.11ms
step:8/1775 train_time:314ms step_avg:39.24ms
step:9/1775 train_time:343ms step_avg:38.12ms
step:10/1775 train_time:376ms step_avg:37.58ms
step:11/1775 train_time:407ms step_avg:36.97ms
step:12/1775 train_time:440ms step_avg:36.65ms
step:13/1775 train_time:471ms step_avg:36.21ms
step:14/1775 train_time:504ms step_avg:36.02ms
step:15/1775 train_time:535ms step_avg:35.68ms
step:16/1775 train_time:568ms step_avg:35.52ms
step:17/1775 train_time:600ms step_avg:35.27ms
step:18/1775 train_time:633ms step_avg:35.15ms
step:19/1775 train_time:664ms step_avg:34.96ms
step:20/1775 train_time:698ms step_avg:34.88ms
step:21/1775 train_time:729ms step_avg:34.70ms
step:22/1775 train_time:762ms step_avg:34.63ms
step:23/1775 train_time:793ms step_avg:34.48ms
step:24/1775 train_time:827ms step_avg:34.44ms
step:25/1775 train_time:857ms step_avg:34.30ms
step:26/1775 train_time:891ms step_avg:34.25ms
step:27/1775 train_time:921ms step_avg:34.13ms
step:28/1775 train_time:955ms step_avg:34.10ms
step:29/1775 train_time:986ms step_avg:34.00ms
step:30/1775 train_time:1019ms step_avg:33.98ms
step:31/1775 train_time:1051ms step_avg:33.90ms
step:32/1775 train_time:1084ms step_avg:33.89ms
step:33/1775 train_time:1116ms step_avg:33.82ms
step:34/1775 train_time:1150ms step_avg:33.82ms
step:35/1775 train_time:1182ms step_avg:33.78ms
step:36/1775 train_time:1217ms step_avg:33.81ms
step:37/1775 train_time:1250ms step_avg:33.77ms
step:38/1775 train_time:1284ms step_avg:33.80ms
step:39/1775 train_time:1316ms step_avg:33.74ms
step:40/1775 train_time:1350ms step_avg:33.74ms
step:41/1775 train_time:1381ms step_avg:33.69ms
step:42/1775 train_time:1415ms step_avg:33.68ms
step:43/1775 train_time:1446ms step_avg:33.63ms
step:44/1775 train_time:1479ms step_avg:33.61ms
step:45/1775 train_time:1511ms step_avg:33.57ms
step:46/1775 train_time:1544ms step_avg:33.58ms
step:47/1775 train_time:1576ms step_avg:33.52ms
step:48/1775 train_time:1609ms step_avg:33.53ms
step:49/1775 train_time:1641ms step_avg:33.49ms
step:50/1775 train_time:1674ms step_avg:33.49ms
step:51/1775 train_time:1706ms step_avg:33.45ms
step:52/1775 train_time:1739ms step_avg:33.44ms
step:53/1775 train_time:1770ms step_avg:33.40ms
step:54/1775 train_time:1804ms step_avg:33.41ms
step:55/1775 train_time:1835ms step_avg:33.36ms
step:56/1775 train_time:1868ms step_avg:33.35ms
step:57/1775 train_time:1899ms step_avg:33.32ms
step:58/1775 train_time:1933ms step_avg:33.33ms
step:59/1775 train_time:1964ms step_avg:33.29ms
step:60/1775 train_time:1997ms step_avg:33.29ms
step:61/1775 train_time:2029ms step_avg:33.26ms
step:62/1775 train_time:2062ms step_avg:33.27ms
step:63/1775 train_time:2094ms step_avg:33.24ms
step:64/1775 train_time:2128ms step_avg:33.24ms
step:65/1775 train_time:2159ms step_avg:33.22ms
step:66/1775 train_time:2193ms step_avg:33.23ms
step:67/1775 train_time:2226ms step_avg:33.22ms
step:68/1775 train_time:2259ms step_avg:33.23ms
step:69/1775 train_time:2291ms step_avg:33.21ms
step:70/1775 train_time:2325ms step_avg:33.22ms
step:71/1775 train_time:2356ms step_avg:33.19ms
step:72/1775 train_time:2390ms step_avg:33.20ms
step:73/1775 train_time:2422ms step_avg:33.18ms
step:74/1775 train_time:2456ms step_avg:33.18ms
step:75/1775 train_time:2487ms step_avg:33.15ms
step:76/1775 train_time:2521ms step_avg:33.17ms
step:77/1775 train_time:2552ms step_avg:33.15ms
step:78/1775 train_time:2586ms step_avg:33.16ms
step:79/1775 train_time:2617ms step_avg:33.13ms
step:80/1775 train_time:2652ms step_avg:33.14ms
step:81/1775 train_time:2683ms step_avg:33.12ms
step:82/1775 train_time:2716ms step_avg:33.12ms
step:83/1775 train_time:2747ms step_avg:33.10ms
step:84/1775 train_time:2781ms step_avg:33.10ms
step:85/1775 train_time:2812ms step_avg:33.08ms
step:86/1775 train_time:2846ms step_avg:33.09ms
step:87/1775 train_time:2876ms step_avg:33.06ms
step:88/1775 train_time:2910ms step_avg:33.07ms
step:89/1775 train_time:2942ms step_avg:33.05ms
step:90/1775 train_time:2975ms step_avg:33.05ms
step:91/1775 train_time:3006ms step_avg:33.03ms
step:92/1775 train_time:3039ms step_avg:33.03ms
step:93/1775 train_time:3070ms step_avg:33.01ms
step:94/1775 train_time:3103ms step_avg:33.01ms
step:95/1775 train_time:3135ms step_avg:33.00ms
step:96/1775 train_time:3169ms step_avg:33.01ms
step:97/1775 train_time:3201ms step_avg:33.00ms
step:98/1775 train_time:3234ms step_avg:33.00ms
step:99/1775 train_time:3266ms step_avg:32.99ms
step:100/1775 train_time:3300ms step_avg:33.00ms
step:101/1775 train_time:3331ms step_avg:32.98ms
step:102/1775 train_time:3366ms step_avg:33.00ms
step:103/1775 train_time:3397ms step_avg:32.98ms
step:104/1775 train_time:3430ms step_avg:32.98ms
step:105/1775 train_time:3462ms step_avg:32.97ms
step:106/1775 train_time:3495ms step_avg:32.98ms
step:107/1775 train_time:3527ms step_avg:32.97ms
step:108/1775 train_time:3561ms step_avg:32.97ms
step:109/1775 train_time:3592ms step_avg:32.96ms
step:110/1775 train_time:3626ms step_avg:32.96ms
step:111/1775 train_time:3657ms step_avg:32.95ms
step:112/1775 train_time:3690ms step_avg:32.95ms
step:113/1775 train_time:3722ms step_avg:32.93ms
step:114/1775 train_time:3755ms step_avg:32.94ms
step:115/1775 train_time:3786ms step_avg:32.92ms
step:116/1775 train_time:3819ms step_avg:32.92ms
step:117/1775 train_time:3851ms step_avg:32.91ms
step:118/1775 train_time:3885ms step_avg:32.92ms
step:119/1775 train_time:3916ms step_avg:32.90ms
step:120/1775 train_time:3949ms step_avg:32.91ms
step:121/1775 train_time:3981ms step_avg:32.90ms
step:122/1775 train_time:4014ms step_avg:32.90ms
step:123/1775 train_time:4045ms step_avg:32.89ms
step:124/1775 train_time:4078ms step_avg:32.89ms
step:125/1775 train_time:4110ms step_avg:32.88ms
step:126/1775 train_time:4144ms step_avg:32.89ms
step:127/1775 train_time:4175ms step_avg:32.87ms
step:128/1775 train_time:4208ms step_avg:32.88ms
step:129/1775 train_time:4240ms step_avg:32.87ms
step:130/1775 train_time:4274ms step_avg:32.87ms
step:131/1775 train_time:4305ms step_avg:32.86ms
step:132/1775 train_time:4339ms step_avg:32.87ms
step:133/1775 train_time:4370ms step_avg:32.86ms
step:134/1775 train_time:4405ms step_avg:32.87ms
step:135/1775 train_time:4436ms step_avg:32.86ms
step:136/1775 train_time:4470ms step_avg:32.86ms
step:137/1775 train_time:4501ms step_avg:32.86ms
step:138/1775 train_time:4535ms step_avg:32.86ms
step:139/1775 train_time:4566ms step_avg:32.85ms
step:140/1775 train_time:4599ms step_avg:32.85ms
step:141/1775 train_time:4631ms step_avg:32.84ms
step:142/1775 train_time:4664ms step_avg:32.85ms
step:143/1775 train_time:4695ms step_avg:32.83ms
step:144/1775 train_time:4729ms step_avg:32.84ms
step:145/1775 train_time:4760ms step_avg:32.83ms
step:146/1775 train_time:4793ms step_avg:32.83ms
step:147/1775 train_time:4824ms step_avg:32.82ms
step:148/1775 train_time:4858ms step_avg:32.82ms
step:149/1775 train_time:4889ms step_avg:32.81ms
step:150/1775 train_time:4922ms step_avg:32.82ms
step:151/1775 train_time:4954ms step_avg:32.81ms
step:152/1775 train_time:4987ms step_avg:32.81ms
step:153/1775 train_time:5018ms step_avg:32.80ms
step:154/1775 train_time:5052ms step_avg:32.80ms
step:155/1775 train_time:5083ms step_avg:32.79ms
step:156/1775 train_time:5116ms step_avg:32.80ms
step:157/1775 train_time:5148ms step_avg:32.79ms
step:158/1775 train_time:5181ms step_avg:32.79ms
step:159/1775 train_time:5212ms step_avg:32.78ms
step:160/1775 train_time:5246ms step_avg:32.79ms
step:161/1775 train_time:5277ms step_avg:32.78ms
step:162/1775 train_time:5311ms step_avg:32.78ms
step:163/1775 train_time:5343ms step_avg:32.78ms
step:164/1775 train_time:5376ms step_avg:32.78ms
step:165/1775 train_time:5408ms step_avg:32.77ms
step:166/1775 train_time:5442ms step_avg:32.78ms
step:167/1775 train_time:5473ms step_avg:32.77ms
step:168/1775 train_time:5507ms step_avg:32.78ms
step:169/1775 train_time:5538ms step_avg:32.77ms
step:170/1775 train_time:5572ms step_avg:32.78ms
step:171/1775 train_time:5604ms step_avg:32.77ms
step:172/1775 train_time:5637ms step_avg:32.77ms
step:173/1775 train_time:5668ms step_avg:32.77ms
step:174/1775 train_time:5702ms step_avg:32.77ms
step:175/1775 train_time:5733ms step_avg:32.76ms
step:176/1775 train_time:5766ms step_avg:32.76ms
step:177/1775 train_time:5798ms step_avg:32.76ms
step:178/1775 train_time:5831ms step_avg:32.76ms
step:179/1775 train_time:5863ms step_avg:32.75ms
step:180/1775 train_time:5896ms step_avg:32.76ms
step:181/1775 train_time:5927ms step_avg:32.75ms
step:182/1775 train_time:5961ms step_avg:32.75ms
step:183/1775 train_time:5993ms step_avg:32.75ms
step:184/1775 train_time:6026ms step_avg:32.75ms
step:185/1775 train_time:6057ms step_avg:32.74ms
step:186/1775 train_time:6090ms step_avg:32.74ms
step:187/1775 train_time:6122ms step_avg:32.74ms
step:188/1775 train_time:6155ms step_avg:32.74ms
step:189/1775 train_time:6186ms step_avg:32.73ms
step:190/1775 train_time:6219ms step_avg:32.73ms
step:191/1775 train_time:6251ms step_avg:32.73ms
step:192/1775 train_time:6284ms step_avg:32.73ms
step:193/1775 train_time:6315ms step_avg:32.72ms
step:194/1775 train_time:6349ms step_avg:32.73ms
step:195/1775 train_time:6380ms step_avg:32.72ms
step:196/1775 train_time:6414ms step_avg:32.72ms
step:197/1775 train_time:6445ms step_avg:32.71ms
step:198/1775 train_time:6478ms step_avg:32.72ms
step:199/1775 train_time:6509ms step_avg:32.71ms
step:200/1775 train_time:6544ms step_avg:32.72ms
step:201/1775 train_time:6575ms step_avg:32.71ms
step:202/1775 train_time:6608ms step_avg:32.71ms
step:203/1775 train_time:6639ms step_avg:32.71ms
step:204/1775 train_time:6673ms step_avg:32.71ms
step:205/1775 train_time:6704ms step_avg:32.70ms
step:206/1775 train_time:6738ms step_avg:32.71ms
step:207/1775 train_time:6769ms step_avg:32.70ms
step:208/1775 train_time:6803ms step_avg:32.71ms
step:209/1775 train_time:6834ms step_avg:32.70ms
step:210/1775 train_time:6867ms step_avg:32.70ms
step:211/1775 train_time:6899ms step_avg:32.70ms
step:212/1775 train_time:6933ms step_avg:32.70ms
step:213/1775 train_time:6964ms step_avg:32.69ms
step:214/1775 train_time:6997ms step_avg:32.70ms
step:215/1775 train_time:7028ms step_avg:32.69ms
step:216/1775 train_time:7062ms step_avg:32.69ms
step:217/1775 train_time:7093ms step_avg:32.69ms
step:218/1775 train_time:7126ms step_avg:32.69ms
step:219/1775 train_time:7157ms step_avg:32.68ms
step:220/1775 train_time:7191ms step_avg:32.69ms
step:221/1775 train_time:7223ms step_avg:32.68ms
step:222/1775 train_time:7256ms step_avg:32.69ms
step:223/1775 train_time:7288ms step_avg:32.68ms
step:224/1775 train_time:7321ms step_avg:32.68ms
step:225/1775 train_time:7352ms step_avg:32.68ms
step:226/1775 train_time:7386ms step_avg:32.68ms
step:227/1775 train_time:7417ms step_avg:32.67ms
step:228/1775 train_time:7451ms step_avg:32.68ms
step:229/1775 train_time:7482ms step_avg:32.67ms
step:230/1775 train_time:7515ms step_avg:32.68ms
step:231/1775 train_time:7547ms step_avg:32.67ms
step:232/1775 train_time:7580ms step_avg:32.67ms
step:233/1775 train_time:7612ms step_avg:32.67ms
step:234/1775 train_time:7646ms step_avg:32.67ms
step:235/1775 train_time:7677ms step_avg:32.67ms
step:236/1775 train_time:7711ms step_avg:32.67ms
step:237/1775 train_time:7742ms step_avg:32.67ms
step:238/1775 train_time:7776ms step_avg:32.67ms
step:239/1775 train_time:7807ms step_avg:32.67ms
step:240/1775 train_time:7840ms step_avg:32.67ms
step:241/1775 train_time:7872ms step_avg:32.66ms
step:242/1775 train_time:7906ms step_avg:32.67ms
step:243/1775 train_time:7937ms step_avg:32.66ms
step:244/1775 train_time:7970ms step_avg:32.66ms
step:245/1775 train_time:8001ms step_avg:32.66ms
step:246/1775 train_time:8035ms step_avg:32.66ms
step:247/1775 train_time:8066ms step_avg:32.66ms
step:248/1775 train_time:8100ms step_avg:32.66ms
step:249/1775 train_time:8132ms step_avg:32.66ms
step:250/1775 train_time:8165ms step_avg:32.66ms
step:250/1775 val_loss:4.6145 train_time:8207ms step_avg:32.83ms
step:251/1775 train_time:8226ms step_avg:32.77ms
step:252/1775 train_time:8245ms step_avg:32.72ms
step:253/1775 train_time:8264ms step_avg:32.67ms
step:254/1775 train_time:8298ms step_avg:32.67ms
step:255/1775 train_time:8331ms step_avg:32.67ms
step:256/1775 train_time:8367ms step_avg:32.68ms
step:257/1775 train_time:8400ms step_avg:32.68ms
step:258/1775 train_time:8434ms step_avg:32.69ms
step:259/1775 train_time:8466ms step_avg:32.69ms
step:260/1775 train_time:8500ms step_avg:32.69ms
step:261/1775 train_time:8531ms step_avg:32.69ms
step:262/1775 train_time:8565ms step_avg:32.69ms
step:263/1775 train_time:8596ms step_avg:32.69ms
step:264/1775 train_time:8630ms step_avg:32.69ms
step:265/1775 train_time:8661ms step_avg:32.68ms
step:266/1775 train_time:8694ms step_avg:32.68ms
step:267/1775 train_time:8725ms step_avg:32.68ms
step:268/1775 train_time:8757ms step_avg:32.68ms
step:269/1775 train_time:8789ms step_avg:32.67ms
step:270/1775 train_time:8822ms step_avg:32.67ms
step:271/1775 train_time:8853ms step_avg:32.67ms
step:272/1775 train_time:8885ms step_avg:32.67ms
step:273/1775 train_time:8916ms step_avg:32.66ms
step:274/1775 train_time:8950ms step_avg:32.66ms
step:275/1775 train_time:8981ms step_avg:32.66ms
step:276/1775 train_time:9014ms step_avg:32.66ms
step:277/1775 train_time:9045ms step_avg:32.65ms
step:278/1775 train_time:9078ms step_avg:32.66ms
step:279/1775 train_time:9109ms step_avg:32.65ms
step:280/1775 train_time:9142ms step_avg:32.65ms
step:281/1775 train_time:9173ms step_avg:32.64ms
step:282/1775 train_time:9207ms step_avg:32.65ms
step:283/1775 train_time:9239ms step_avg:32.65ms
step:284/1775 train_time:9273ms step_avg:32.65ms
step:285/1775 train_time:9305ms step_avg:32.65ms
step:286/1775 train_time:9339ms step_avg:32.65ms
step:287/1775 train_time:9371ms step_avg:32.65ms
step:288/1775 train_time:9405ms step_avg:32.66ms
step:289/1775 train_time:9436ms step_avg:32.65ms
step:290/1775 train_time:9471ms step_avg:32.66ms
step:291/1775 train_time:9502ms step_avg:32.65ms
step:292/1775 train_time:9536ms step_avg:32.66ms
step:293/1775 train_time:9567ms step_avg:32.65ms
step:294/1775 train_time:9601ms step_avg:32.66ms
step:295/1775 train_time:9632ms step_avg:32.65ms
step:296/1775 train_time:9666ms step_avg:32.65ms
step:297/1775 train_time:9696ms step_avg:32.65ms
step:298/1775 train_time:9730ms step_avg:32.65ms
step:299/1775 train_time:9761ms step_avg:32.65ms
step:300/1775 train_time:9794ms step_avg:32.65ms
step:301/1775 train_time:9825ms step_avg:32.64ms
step:302/1775 train_time:9858ms step_avg:32.64ms
step:303/1775 train_time:9890ms step_avg:32.64ms
step:304/1775 train_time:9923ms step_avg:32.64ms
step:305/1775 train_time:9954ms step_avg:32.64ms
step:306/1775 train_time:9987ms step_avg:32.64ms
step:307/1775 train_time:10018ms step_avg:32.63ms
step:308/1775 train_time:10052ms step_avg:32.64ms
step:309/1775 train_time:10082ms step_avg:32.63ms
step:310/1775 train_time:10115ms step_avg:32.63ms
step:311/1775 train_time:10146ms step_avg:32.62ms
step:312/1775 train_time:10179ms step_avg:32.63ms
step:313/1775 train_time:10210ms step_avg:32.62ms
step:314/1775 train_time:10245ms step_avg:32.63ms
step:315/1775 train_time:10276ms step_avg:32.62ms
step:316/1775 train_time:10310ms step_avg:32.63ms
step:317/1775 train_time:10342ms step_avg:32.62ms
step:318/1775 train_time:10376ms step_avg:32.63ms
step:319/1775 train_time:10407ms step_avg:32.62ms
step:320/1775 train_time:10441ms step_avg:32.63ms
step:321/1775 train_time:10473ms step_avg:32.63ms
step:322/1775 train_time:10507ms step_avg:32.63ms
step:323/1775 train_time:10538ms step_avg:32.62ms
step:324/1775 train_time:10572ms step_avg:32.63ms
step:325/1775 train_time:10603ms step_avg:32.63ms
step:326/1775 train_time:10637ms step_avg:32.63ms
step:327/1775 train_time:10668ms step_avg:32.62ms
step:328/1775 train_time:10702ms step_avg:32.63ms
step:329/1775 train_time:10733ms step_avg:32.62ms
step:330/1775 train_time:10767ms step_avg:32.63ms
step:331/1775 train_time:10798ms step_avg:32.62ms
step:332/1775 train_time:10831ms step_avg:32.62ms
step:333/1775 train_time:10862ms step_avg:32.62ms
step:334/1775 train_time:10895ms step_avg:32.62ms
step:335/1775 train_time:10926ms step_avg:32.62ms
step:336/1775 train_time:10960ms step_avg:32.62ms
step:337/1775 train_time:10991ms step_avg:32.61ms
step:338/1775 train_time:11024ms step_avg:32.62ms
step:339/1775 train_time:11055ms step_avg:32.61ms
step:340/1775 train_time:11090ms step_avg:32.62ms
step:341/1775 train_time:11121ms step_avg:32.61ms
step:342/1775 train_time:11154ms step_avg:32.61ms
step:343/1775 train_time:11185ms step_avg:32.61ms
step:344/1775 train_time:11219ms step_avg:32.61ms
step:345/1775 train_time:11250ms step_avg:32.61ms
step:346/1775 train_time:11284ms step_avg:32.61ms
step:347/1775 train_time:11315ms step_avg:32.61ms
step:348/1775 train_time:11349ms step_avg:32.61ms
step:349/1775 train_time:11380ms step_avg:32.61ms
step:350/1775 train_time:11414ms step_avg:32.61ms
step:351/1775 train_time:11446ms step_avg:32.61ms
step:352/1775 train_time:11479ms step_avg:32.61ms
step:353/1775 train_time:11510ms step_avg:32.61ms
step:354/1775 train_time:11544ms step_avg:32.61ms
step:355/1775 train_time:11575ms step_avg:32.61ms
step:356/1775 train_time:11609ms step_avg:32.61ms
step:357/1775 train_time:11641ms step_avg:32.61ms
step:358/1775 train_time:11674ms step_avg:32.61ms
step:359/1775 train_time:11705ms step_avg:32.61ms
step:360/1775 train_time:11739ms step_avg:32.61ms
step:361/1775 train_time:11770ms step_avg:32.60ms
step:362/1775 train_time:11804ms step_avg:32.61ms
step:363/1775 train_time:11835ms step_avg:32.60ms
step:364/1775 train_time:11869ms step_avg:32.61ms
step:365/1775 train_time:11900ms step_avg:32.60ms
step:366/1775 train_time:11934ms step_avg:32.61ms
step:367/1775 train_time:11965ms step_avg:32.60ms
step:368/1775 train_time:11998ms step_avg:32.60ms
step:369/1775 train_time:12029ms step_avg:32.60ms
step:370/1775 train_time:12063ms step_avg:32.60ms
step:371/1775 train_time:12093ms step_avg:32.60ms
step:372/1775 train_time:12127ms step_avg:32.60ms
step:373/1775 train_time:12159ms step_avg:32.60ms
step:374/1775 train_time:12192ms step_avg:32.60ms
step:375/1775 train_time:12224ms step_avg:32.60ms
step:376/1775 train_time:12257ms step_avg:32.60ms
step:377/1775 train_time:12288ms step_avg:32.59ms
step:378/1775 train_time:12321ms step_avg:32.60ms
step:379/1775 train_time:12352ms step_avg:32.59ms
step:380/1775 train_time:12386ms step_avg:32.59ms
step:381/1775 train_time:12417ms step_avg:32.59ms
step:382/1775 train_time:12450ms step_avg:32.59ms
step:383/1775 train_time:12482ms step_avg:32.59ms
step:384/1775 train_time:12516ms step_avg:32.59ms
step:385/1775 train_time:12547ms step_avg:32.59ms
step:386/1775 train_time:12581ms step_avg:32.59ms
step:387/1775 train_time:12612ms step_avg:32.59ms
step:388/1775 train_time:12647ms step_avg:32.59ms
step:389/1775 train_time:12678ms step_avg:32.59ms
step:390/1775 train_time:12712ms step_avg:32.59ms
step:391/1775 train_time:12743ms step_avg:32.59ms
step:392/1775 train_time:12777ms step_avg:32.59ms
step:393/1775 train_time:12808ms step_avg:32.59ms
step:394/1775 train_time:12841ms step_avg:32.59ms
step:395/1775 train_time:12872ms step_avg:32.59ms
step:396/1775 train_time:12906ms step_avg:32.59ms
step:397/1775 train_time:12937ms step_avg:32.59ms
step:398/1775 train_time:12970ms step_avg:32.59ms
step:399/1775 train_time:13001ms step_avg:32.59ms
step:400/1775 train_time:13035ms step_avg:32.59ms
step:401/1775 train_time:13066ms step_avg:32.58ms
step:402/1775 train_time:13099ms step_avg:32.58ms
step:403/1775 train_time:13130ms step_avg:32.58ms
step:404/1775 train_time:13164ms step_avg:32.58ms
step:405/1775 train_time:13195ms step_avg:32.58ms
step:406/1775 train_time:13229ms step_avg:32.58ms
step:407/1775 train_time:13261ms step_avg:32.58ms
step:408/1775 train_time:13294ms step_avg:32.58ms
step:409/1775 train_time:13325ms step_avg:32.58ms
step:410/1775 train_time:13358ms step_avg:32.58ms
step:411/1775 train_time:13390ms step_avg:32.58ms
step:412/1775 train_time:13423ms step_avg:32.58ms
step:413/1775 train_time:13454ms step_avg:32.58ms
step:414/1775 train_time:13488ms step_avg:32.58ms
step:415/1775 train_time:13520ms step_avg:32.58ms
step:416/1775 train_time:13553ms step_avg:32.58ms
step:417/1775 train_time:13584ms step_avg:32.58ms
step:418/1775 train_time:13617ms step_avg:32.58ms
step:419/1775 train_time:13649ms step_avg:32.57ms
step:420/1775 train_time:13683ms step_avg:32.58ms
step:421/1775 train_time:13714ms step_avg:32.57ms
step:422/1775 train_time:13747ms step_avg:32.58ms
step:423/1775 train_time:13778ms step_avg:32.57ms
step:424/1775 train_time:13812ms step_avg:32.58ms
step:425/1775 train_time:13843ms step_avg:32.57ms
step:426/1775 train_time:13877ms step_avg:32.57ms
step:427/1775 train_time:13908ms step_avg:32.57ms
step:428/1775 train_time:13942ms step_avg:32.57ms
step:429/1775 train_time:13973ms step_avg:32.57ms
step:430/1775 train_time:14006ms step_avg:32.57ms
step:431/1775 train_time:14037ms step_avg:32.57ms
step:432/1775 train_time:14071ms step_avg:32.57ms
step:433/1775 train_time:14102ms step_avg:32.57ms
step:434/1775 train_time:14135ms step_avg:32.57ms
step:435/1775 train_time:14167ms step_avg:32.57ms
step:436/1775 train_time:14200ms step_avg:32.57ms
step:437/1775 train_time:14231ms step_avg:32.57ms
step:438/1775 train_time:14265ms step_avg:32.57ms
step:439/1775 train_time:14296ms step_avg:32.56ms
step:440/1775 train_time:14330ms step_avg:32.57ms
step:441/1775 train_time:14360ms step_avg:32.56ms
step:442/1775 train_time:14394ms step_avg:32.57ms
step:443/1775 train_time:14425ms step_avg:32.56ms
step:444/1775 train_time:14459ms step_avg:32.56ms
step:445/1775 train_time:14490ms step_avg:32.56ms
step:446/1775 train_time:14523ms step_avg:32.56ms
step:447/1775 train_time:14555ms step_avg:32.56ms
step:448/1775 train_time:14588ms step_avg:32.56ms
step:449/1775 train_time:14619ms step_avg:32.56ms
step:450/1775 train_time:14653ms step_avg:32.56ms
step:451/1775 train_time:14685ms step_avg:32.56ms
step:452/1775 train_time:14717ms step_avg:32.56ms
step:453/1775 train_time:14749ms step_avg:32.56ms
step:454/1775 train_time:14783ms step_avg:32.56ms
step:455/1775 train_time:14814ms step_avg:32.56ms
step:456/1775 train_time:14848ms step_avg:32.56ms
step:457/1775 train_time:14879ms step_avg:32.56ms
step:458/1775 train_time:14912ms step_avg:32.56ms
step:459/1775 train_time:14944ms step_avg:32.56ms
step:460/1775 train_time:14977ms step_avg:32.56ms
step:461/1775 train_time:15009ms step_avg:32.56ms
step:462/1775 train_time:15042ms step_avg:32.56ms
step:463/1775 train_time:15073ms step_avg:32.56ms
step:464/1775 train_time:15107ms step_avg:32.56ms
step:465/1775 train_time:15138ms step_avg:32.55ms
step:466/1775 train_time:15171ms step_avg:32.56ms
step:467/1775 train_time:15203ms step_avg:32.55ms
step:468/1775 train_time:15236ms step_avg:32.56ms
step:469/1775 train_time:15268ms step_avg:32.55ms
step:470/1775 train_time:15301ms step_avg:32.56ms
step:471/1775 train_time:15333ms step_avg:32.55ms
step:472/1775 train_time:15366ms step_avg:32.56ms
step:473/1775 train_time:15398ms step_avg:32.55ms
step:474/1775 train_time:15432ms step_avg:32.56ms
step:475/1775 train_time:15463ms step_avg:32.55ms
step:476/1775 train_time:15496ms step_avg:32.56ms
step:477/1775 train_time:15527ms step_avg:32.55ms
step:478/1775 train_time:15561ms step_avg:32.55ms
step:479/1775 train_time:15592ms step_avg:32.55ms
step:480/1775 train_time:15625ms step_avg:32.55ms
step:481/1775 train_time:15656ms step_avg:32.55ms
step:482/1775 train_time:15690ms step_avg:32.55ms
step:483/1775 train_time:15723ms step_avg:32.55ms
step:484/1775 train_time:15755ms step_avg:32.55ms
step:485/1775 train_time:15787ms step_avg:32.55ms
step:486/1775 train_time:15820ms step_avg:32.55ms
step:487/1775 train_time:15852ms step_avg:32.55ms
step:488/1775 train_time:15886ms step_avg:32.55ms
step:489/1775 train_time:15918ms step_avg:32.55ms
step:490/1775 train_time:15951ms step_avg:32.55ms
step:491/1775 train_time:15982ms step_avg:32.55ms
step:492/1775 train_time:16016ms step_avg:32.55ms
step:493/1775 train_time:16047ms step_avg:32.55ms
step:494/1775 train_time:16081ms step_avg:32.55ms
step:495/1775 train_time:16112ms step_avg:32.55ms
step:496/1775 train_time:16146ms step_avg:32.55ms
step:497/1775 train_time:16177ms step_avg:32.55ms
step:498/1775 train_time:16210ms step_avg:32.55ms
step:499/1775 train_time:16242ms step_avg:32.55ms
step:500/1775 train_time:16275ms step_avg:32.55ms
step:500/1775 val_loss:4.2756 train_time:16317ms step_avg:32.63ms
step:501/1775 train_time:16336ms step_avg:32.61ms
step:502/1775 train_time:16355ms step_avg:32.58ms
step:503/1775 train_time:16377ms step_avg:32.56ms
step:504/1775 train_time:16410ms step_avg:32.56ms
step:505/1775 train_time:16444ms step_avg:32.56ms
step:506/1775 train_time:16479ms step_avg:32.57ms
step:507/1775 train_time:16510ms step_avg:32.56ms
step:508/1775 train_time:16544ms step_avg:32.57ms
step:509/1775 train_time:16575ms step_avg:32.56ms
step:510/1775 train_time:16609ms step_avg:32.57ms
step:511/1775 train_time:16640ms step_avg:32.56ms
step:512/1775 train_time:16673ms step_avg:32.56ms
step:513/1775 train_time:16704ms step_avg:32.56ms
step:514/1775 train_time:16737ms step_avg:32.56ms
step:515/1775 train_time:16768ms step_avg:32.56ms
step:516/1775 train_time:16802ms step_avg:32.56ms
step:517/1775 train_time:16833ms step_avg:32.56ms
step:518/1775 train_time:16866ms step_avg:32.56ms
step:519/1775 train_time:16897ms step_avg:32.56ms
step:520/1775 train_time:16930ms step_avg:32.56ms
step:521/1775 train_time:16961ms step_avg:32.55ms
step:522/1775 train_time:16994ms step_avg:32.56ms
step:523/1775 train_time:17025ms step_avg:32.55ms
step:524/1775 train_time:17058ms step_avg:32.55ms
step:525/1775 train_time:17088ms step_avg:32.55ms
step:526/1775 train_time:17121ms step_avg:32.55ms
step:527/1775 train_time:17152ms step_avg:32.55ms
step:528/1775 train_time:17185ms step_avg:32.55ms
step:529/1775 train_time:17216ms step_avg:32.54ms
step:530/1775 train_time:17250ms step_avg:32.55ms
step:531/1775 train_time:17281ms step_avg:32.54ms
step:532/1775 train_time:17315ms step_avg:32.55ms
step:533/1775 train_time:17347ms step_avg:32.55ms
step:534/1775 train_time:17382ms step_avg:32.55ms
step:535/1775 train_time:17413ms step_avg:32.55ms
step:536/1775 train_time:17447ms step_avg:32.55ms
step:537/1775 train_time:17479ms step_avg:32.55ms
step:538/1775 train_time:17513ms step_avg:32.55ms
step:539/1775 train_time:17544ms step_avg:32.55ms
step:540/1775 train_time:17578ms step_avg:32.55ms
step:541/1775 train_time:17610ms step_avg:32.55ms
step:542/1775 train_time:17644ms step_avg:32.55ms
step:543/1775 train_time:17675ms step_avg:32.55ms
step:544/1775 train_time:17709ms step_avg:32.55ms
step:545/1775 train_time:17740ms step_avg:32.55ms
step:546/1775 train_time:17773ms step_avg:32.55ms
step:547/1775 train_time:17804ms step_avg:32.55ms
step:548/1775 train_time:17838ms step_avg:32.55ms
step:549/1775 train_time:17869ms step_avg:32.55ms
step:550/1775 train_time:17902ms step_avg:32.55ms
step:551/1775 train_time:17933ms step_avg:32.55ms
step:552/1775 train_time:17967ms step_avg:32.55ms
step:553/1775 train_time:17998ms step_avg:32.55ms
step:554/1775 train_time:18031ms step_avg:32.55ms
step:555/1775 train_time:18062ms step_avg:32.54ms
step:556/1775 train_time:18095ms step_avg:32.54ms
step:557/1775 train_time:18127ms step_avg:32.54ms
step:558/1775 train_time:18159ms step_avg:32.54ms
step:559/1775 train_time:18191ms step_avg:32.54ms
step:560/1775 train_time:18225ms step_avg:32.54ms
step:561/1775 train_time:18256ms step_avg:32.54ms
step:562/1775 train_time:18289ms step_avg:32.54ms
step:563/1775 train_time:18320ms step_avg:32.54ms
step:564/1775 train_time:18353ms step_avg:32.54ms
step:565/1775 train_time:18385ms step_avg:32.54ms
step:566/1775 train_time:18419ms step_avg:32.54ms
step:567/1775 train_time:18451ms step_avg:32.54ms
step:568/1775 train_time:18485ms step_avg:32.54ms
step:569/1775 train_time:18516ms step_avg:32.54ms
step:570/1775 train_time:18549ms step_avg:32.54ms
step:571/1775 train_time:18581ms step_avg:32.54ms
step:572/1775 train_time:18614ms step_avg:32.54ms
step:573/1775 train_time:18646ms step_avg:32.54ms
step:574/1775 train_time:18679ms step_avg:32.54ms
step:575/1775 train_time:18710ms step_avg:32.54ms
step:576/1775 train_time:18744ms step_avg:32.54ms
step:577/1775 train_time:18775ms step_avg:32.54ms
step:578/1775 train_time:18808ms step_avg:32.54ms
step:579/1775 train_time:18839ms step_avg:32.54ms
step:580/1775 train_time:18875ms step_avg:32.54ms
step:581/1775 train_time:18933ms step_avg:32.59ms
step:582/1775 train_time:18993ms step_avg:32.63ms
step:583/1775 train_time:19050ms step_avg:32.68ms
step:584/1775 train_time:19110ms step_avg:32.72ms
step:585/1775 train_time:19168ms step_avg:32.77ms
step:586/1775 train_time:19228ms step_avg:32.81ms
step:587/1775 train_time:19285ms step_avg:32.85ms
step:588/1775 train_time:19347ms step_avg:32.90ms
step:589/1775 train_time:19405ms step_avg:32.95ms
step:590/1775 train_time:19466ms step_avg:32.99ms
step:591/1775 train_time:19525ms step_avg:33.04ms
step:592/1775 train_time:19585ms step_avg:33.08ms
step:593/1775 train_time:19643ms step_avg:33.13ms
step:594/1775 train_time:19704ms step_avg:33.17ms
step:595/1775 train_time:19762ms step_avg:33.21ms
step:596/1775 train_time:19823ms step_avg:33.26ms
step:597/1775 train_time:19881ms step_avg:33.30ms
step:598/1775 train_time:19941ms step_avg:33.35ms
step:599/1775 train_time:19999ms step_avg:33.39ms
step:600/1775 train_time:20059ms step_avg:33.43ms
step:601/1775 train_time:20117ms step_avg:33.47ms
step:602/1775 train_time:20178ms step_avg:33.52ms
step:603/1775 train_time:20235ms step_avg:33.56ms
step:604/1775 train_time:20296ms step_avg:33.60ms
step:605/1775 train_time:20354ms step_avg:33.64ms
step:606/1775 train_time:20414ms step_avg:33.69ms
step:607/1775 train_time:20472ms step_avg:33.73ms
step:608/1775 train_time:20532ms step_avg:33.77ms
step:609/1775 train_time:20591ms step_avg:33.81ms
step:610/1775 train_time:20651ms step_avg:33.85ms
step:611/1775 train_time:20709ms step_avg:33.89ms
step:612/1775 train_time:20771ms step_avg:33.94ms
step:613/1775 train_time:20829ms step_avg:33.98ms
step:614/1775 train_time:20890ms step_avg:34.02ms
step:615/1775 train_time:20948ms step_avg:34.06ms
step:616/1775 train_time:21008ms step_avg:34.10ms
step:617/1775 train_time:21066ms step_avg:34.14ms
step:618/1775 train_time:21126ms step_avg:34.18ms
step:619/1775 train_time:21184ms step_avg:34.22ms
step:620/1775 train_time:21244ms step_avg:34.26ms
step:621/1775 train_time:21302ms step_avg:34.30ms
step:622/1775 train_time:21364ms step_avg:34.35ms
step:623/1775 train_time:21421ms step_avg:34.38ms
step:624/1775 train_time:21483ms step_avg:34.43ms
step:625/1775 train_time:21542ms step_avg:34.47ms
step:626/1775 train_time:21602ms step_avg:34.51ms
step:627/1775 train_time:21660ms step_avg:34.55ms
step:628/1775 train_time:21721ms step_avg:34.59ms
step:629/1775 train_time:21779ms step_avg:34.62ms
step:630/1775 train_time:21839ms step_avg:34.67ms
step:631/1775 train_time:21897ms step_avg:34.70ms
step:632/1775 train_time:21958ms step_avg:34.74ms
step:633/1775 train_time:22015ms step_avg:34.78ms
step:634/1775 train_time:22074ms step_avg:34.82ms
step:635/1775 train_time:22132ms step_avg:34.85ms
step:636/1775 train_time:22192ms step_avg:34.89ms
step:637/1775 train_time:22249ms step_avg:34.93ms
step:638/1775 train_time:22310ms step_avg:34.97ms
step:639/1775 train_time:22368ms step_avg:35.00ms
step:640/1775 train_time:22428ms step_avg:35.04ms
step:641/1775 train_time:22485ms step_avg:35.08ms
step:642/1775 train_time:22547ms step_avg:35.12ms
step:643/1775 train_time:22604ms step_avg:35.15ms
step:644/1775 train_time:22666ms step_avg:35.19ms
step:645/1775 train_time:22725ms step_avg:35.23ms
step:646/1775 train_time:22786ms step_avg:35.27ms
step:647/1775 train_time:22844ms step_avg:35.31ms
step:648/1775 train_time:22904ms step_avg:35.35ms
step:649/1775 train_time:22962ms step_avg:35.38ms
step:650/1775 train_time:23022ms step_avg:35.42ms
step:651/1775 train_time:23081ms step_avg:35.46ms
step:652/1775 train_time:23142ms step_avg:35.49ms
step:653/1775 train_time:23199ms step_avg:35.53ms
step:654/1775 train_time:23259ms step_avg:35.56ms
step:655/1775 train_time:23317ms step_avg:35.60ms
step:656/1775 train_time:23378ms step_avg:35.64ms
step:657/1775 train_time:23436ms step_avg:35.67ms
step:658/1775 train_time:23497ms step_avg:35.71ms
step:659/1775 train_time:23555ms step_avg:35.74ms
step:660/1775 train_time:23615ms step_avg:35.78ms
step:661/1775 train_time:23672ms step_avg:35.81ms
step:662/1775 train_time:23734ms step_avg:35.85ms
step:663/1775 train_time:23791ms step_avg:35.88ms
step:664/1775 train_time:23851ms step_avg:35.92ms
step:665/1775 train_time:23908ms step_avg:35.95ms
step:666/1775 train_time:23969ms step_avg:35.99ms
step:667/1775 train_time:24027ms step_avg:36.02ms
step:668/1775 train_time:24087ms step_avg:36.06ms
step:669/1775 train_time:24145ms step_avg:36.09ms
step:670/1775 train_time:24207ms step_avg:36.13ms
step:671/1775 train_time:24264ms step_avg:36.16ms
step:672/1775 train_time:24324ms step_avg:36.20ms
step:673/1775 train_time:24384ms step_avg:36.23ms
step:674/1775 train_time:24445ms step_avg:36.27ms
step:675/1775 train_time:24503ms step_avg:36.30ms
step:676/1775 train_time:24563ms step_avg:36.34ms
step:677/1775 train_time:24622ms step_avg:36.37ms
step:678/1775 train_time:24682ms step_avg:36.40ms
step:679/1775 train_time:24741ms step_avg:36.44ms
step:680/1775 train_time:24801ms step_avg:36.47ms
step:681/1775 train_time:24861ms step_avg:36.51ms
step:682/1775 train_time:24920ms step_avg:36.54ms
step:683/1775 train_time:24979ms step_avg:36.57ms
step:684/1775 train_time:25039ms step_avg:36.61ms
step:685/1775 train_time:25097ms step_avg:36.64ms
step:686/1775 train_time:25159ms step_avg:36.67ms
step:687/1775 train_time:25215ms step_avg:36.70ms
step:688/1775 train_time:25275ms step_avg:36.74ms
step:689/1775 train_time:25333ms step_avg:36.77ms
step:690/1775 train_time:25392ms step_avg:36.80ms
step:691/1775 train_time:25452ms step_avg:36.83ms
step:692/1775 train_time:25511ms step_avg:36.87ms
step:693/1775 train_time:25568ms step_avg:36.89ms
step:694/1775 train_time:25630ms step_avg:36.93ms
step:695/1775 train_time:25688ms step_avg:36.96ms
step:696/1775 train_time:25748ms step_avg:36.99ms
step:697/1775 train_time:25806ms step_avg:37.02ms
step:698/1775 train_time:25866ms step_avg:37.06ms
step:699/1775 train_time:25924ms step_avg:37.09ms
step:700/1775 train_time:25984ms step_avg:37.12ms
step:701/1775 train_time:26043ms step_avg:37.15ms
step:702/1775 train_time:26103ms step_avg:37.18ms
step:703/1775 train_time:26161ms step_avg:37.21ms
step:704/1775 train_time:26221ms step_avg:37.25ms
step:705/1775 train_time:26280ms step_avg:37.28ms
step:706/1775 train_time:26340ms step_avg:37.31ms
step:707/1775 train_time:26399ms step_avg:37.34ms
step:708/1775 train_time:26461ms step_avg:37.37ms
step:709/1775 train_time:26518ms step_avg:37.40ms
step:710/1775 train_time:26579ms step_avg:37.44ms
step:711/1775 train_time:26637ms step_avg:37.46ms
step:712/1775 train_time:26698ms step_avg:37.50ms
step:713/1775 train_time:26757ms step_avg:37.53ms
step:714/1775 train_time:26817ms step_avg:37.56ms
step:715/1775 train_time:26874ms step_avg:37.59ms
step:716/1775 train_time:26934ms step_avg:37.62ms
step:717/1775 train_time:26992ms step_avg:37.65ms
step:718/1775 train_time:27052ms step_avg:37.68ms
step:719/1775 train_time:27110ms step_avg:37.70ms
step:720/1775 train_time:27171ms step_avg:37.74ms
step:721/1775 train_time:27229ms step_avg:37.77ms
step:722/1775 train_time:27290ms step_avg:37.80ms
step:723/1775 train_time:27349ms step_avg:37.83ms
step:724/1775 train_time:27409ms step_avg:37.86ms
step:725/1775 train_time:27466ms step_avg:37.88ms
step:726/1775 train_time:27528ms step_avg:37.92ms
step:727/1775 train_time:27586ms step_avg:37.94ms
step:728/1775 train_time:27647ms step_avg:37.98ms
step:729/1775 train_time:27704ms step_avg:38.00ms
step:730/1775 train_time:27765ms step_avg:38.03ms
step:731/1775 train_time:27823ms step_avg:38.06ms
step:732/1775 train_time:27884ms step_avg:38.09ms
step:733/1775 train_time:27943ms step_avg:38.12ms
step:734/1775 train_time:28003ms step_avg:38.15ms
step:735/1775 train_time:28061ms step_avg:38.18ms
step:736/1775 train_time:28122ms step_avg:38.21ms
step:737/1775 train_time:28180ms step_avg:38.24ms
step:738/1775 train_time:28240ms step_avg:38.27ms
step:739/1775 train_time:28298ms step_avg:38.29ms
step:740/1775 train_time:28359ms step_avg:38.32ms
step:741/1775 train_time:28417ms step_avg:38.35ms
step:742/1775 train_time:28478ms step_avg:38.38ms
step:743/1775 train_time:28536ms step_avg:38.41ms
step:744/1775 train_time:28596ms step_avg:38.44ms
step:745/1775 train_time:28654ms step_avg:38.46ms
step:746/1775 train_time:28714ms step_avg:38.49ms
step:747/1775 train_time:28772ms step_avg:38.52ms
step:748/1775 train_time:28832ms step_avg:38.55ms
step:749/1775 train_time:28889ms step_avg:38.57ms
step:750/1775 train_time:28951ms step_avg:38.60ms
step:750/1775 val_loss:4.0017 train_time:29021ms step_avg:38.69ms
step:751/1775 train_time:29041ms step_avg:38.67ms
step:752/1775 train_time:29070ms step_avg:38.66ms
step:753/1775 train_time:29132ms step_avg:38.69ms
step:754/1775 train_time:29194ms step_avg:38.72ms
step:755/1775 train_time:29252ms step_avg:38.74ms
step:756/1775 train_time:29313ms step_avg:38.77ms
step:757/1775 train_time:29370ms step_avg:38.80ms
step:758/1775 train_time:29429ms step_avg:38.82ms
step:759/1775 train_time:29486ms step_avg:38.85ms
step:760/1775 train_time:29546ms step_avg:38.88ms
step:761/1775 train_time:29603ms step_avg:38.90ms
step:762/1775 train_time:29663ms step_avg:38.93ms
step:763/1775 train_time:29720ms step_avg:38.95ms
step:764/1775 train_time:29780ms step_avg:38.98ms
step:765/1775 train_time:29836ms step_avg:39.00ms
step:766/1775 train_time:29896ms step_avg:39.03ms
step:767/1775 train_time:29953ms step_avg:39.05ms
step:768/1775 train_time:30013ms step_avg:39.08ms
step:769/1775 train_time:30072ms step_avg:39.11ms
step:770/1775 train_time:30133ms step_avg:39.13ms
step:771/1775 train_time:30192ms step_avg:39.16ms
step:772/1775 train_time:30253ms step_avg:39.19ms
step:773/1775 train_time:30311ms step_avg:39.21ms
step:774/1775 train_time:30371ms step_avg:39.24ms
step:775/1775 train_time:30428ms step_avg:39.26ms
step:776/1775 train_time:30488ms step_avg:39.29ms
step:777/1775 train_time:30545ms step_avg:39.31ms
step:778/1775 train_time:30605ms step_avg:39.34ms
step:779/1775 train_time:30663ms step_avg:39.36ms
step:780/1775 train_time:30723ms step_avg:39.39ms
step:781/1775 train_time:30780ms step_avg:39.41ms
step:782/1775 train_time:30840ms step_avg:39.44ms
step:783/1775 train_time:30897ms step_avg:39.46ms
step:784/1775 train_time:30957ms step_avg:39.49ms
step:785/1775 train_time:31016ms step_avg:39.51ms
step:786/1775 train_time:31078ms step_avg:39.54ms
step:787/1775 train_time:31137ms step_avg:39.56ms
step:788/1775 train_time:31198ms step_avg:39.59ms
step:789/1775 train_time:31257ms step_avg:39.62ms
step:790/1775 train_time:31320ms step_avg:39.65ms
step:791/1775 train_time:31378ms step_avg:39.67ms
step:792/1775 train_time:31439ms step_avg:39.70ms
step:793/1775 train_time:31497ms step_avg:39.72ms
step:794/1775 train_time:31558ms step_avg:39.75ms
step:795/1775 train_time:31615ms step_avg:39.77ms
step:796/1775 train_time:31675ms step_avg:39.79ms
step:797/1775 train_time:31732ms step_avg:39.81ms
step:798/1775 train_time:31791ms step_avg:39.84ms
step:799/1775 train_time:31849ms step_avg:39.86ms
step:800/1775 train_time:31909ms step_avg:39.89ms
step:801/1775 train_time:31967ms step_avg:39.91ms
step:802/1775 train_time:32028ms step_avg:39.94ms
step:803/1775 train_time:32087ms step_avg:39.96ms
step:804/1775 train_time:32147ms step_avg:39.98ms
step:805/1775 train_time:32206ms step_avg:40.01ms
step:806/1775 train_time:32269ms step_avg:40.04ms
step:807/1775 train_time:32326ms step_avg:40.06ms
step:808/1775 train_time:32386ms step_avg:40.08ms
step:809/1775 train_time:32444ms step_avg:40.10ms
step:810/1775 train_time:32504ms step_avg:40.13ms
step:811/1775 train_time:32564ms step_avg:40.15ms
step:812/1775 train_time:32624ms step_avg:40.18ms
step:813/1775 train_time:32680ms step_avg:40.20ms
step:814/1775 train_time:32739ms step_avg:40.22ms
step:815/1775 train_time:32797ms step_avg:40.24ms
step:816/1775 train_time:32857ms step_avg:40.27ms
step:817/1775 train_time:32914ms step_avg:40.29ms
step:818/1775 train_time:32975ms step_avg:40.31ms
step:819/1775 train_time:33033ms step_avg:40.33ms
step:820/1775 train_time:33093ms step_avg:40.36ms
step:821/1775 train_time:33151ms step_avg:40.38ms
step:822/1775 train_time:33211ms step_avg:40.40ms
step:823/1775 train_time:33269ms step_avg:40.42ms
step:824/1775 train_time:33329ms step_avg:40.45ms
step:825/1775 train_time:33387ms step_avg:40.47ms
step:826/1775 train_time:33449ms step_avg:40.49ms
step:827/1775 train_time:33506ms step_avg:40.52ms
step:828/1775 train_time:33567ms step_avg:40.54ms
step:829/1775 train_time:33625ms step_avg:40.56ms
step:830/1775 train_time:33686ms step_avg:40.59ms
step:831/1775 train_time:33744ms step_avg:40.61ms
step:832/1775 train_time:33804ms step_avg:40.63ms
step:833/1775 train_time:33862ms step_avg:40.65ms
step:834/1775 train_time:33923ms step_avg:40.67ms
step:835/1775 train_time:33982ms step_avg:40.70ms
step:836/1775 train_time:34042ms step_avg:40.72ms
step:837/1775 train_time:34101ms step_avg:40.74ms
step:838/1775 train_time:34161ms step_avg:40.76ms
step:839/1775 train_time:34218ms step_avg:40.78ms
step:840/1775 train_time:34279ms step_avg:40.81ms
step:841/1775 train_time:34337ms step_avg:40.83ms
step:842/1775 train_time:34400ms step_avg:40.85ms
step:843/1775 train_time:34458ms step_avg:40.88ms
step:844/1775 train_time:34518ms step_avg:40.90ms
step:845/1775 train_time:34576ms step_avg:40.92ms
step:846/1775 train_time:34636ms step_avg:40.94ms
step:847/1775 train_time:34695ms step_avg:40.96ms
step:848/1775 train_time:34755ms step_avg:40.98ms
step:849/1775 train_time:34811ms step_avg:41.00ms
step:850/1775 train_time:34872ms step_avg:41.03ms
step:851/1775 train_time:34930ms step_avg:41.05ms
step:852/1775 train_time:34990ms step_avg:41.07ms
step:853/1775 train_time:35048ms step_avg:41.09ms
step:854/1775 train_time:35109ms step_avg:41.11ms
step:855/1775 train_time:35167ms step_avg:41.13ms
step:856/1775 train_time:35227ms step_avg:41.15ms
step:857/1775 train_time:35285ms step_avg:41.17ms
step:858/1775 train_time:35346ms step_avg:41.20ms
step:859/1775 train_time:35405ms step_avg:41.22ms
step:860/1775 train_time:35465ms step_avg:41.24ms
step:861/1775 train_time:35523ms step_avg:41.26ms
step:862/1775 train_time:35584ms step_avg:41.28ms
step:863/1775 train_time:35642ms step_avg:41.30ms
step:864/1775 train_time:35703ms step_avg:41.32ms
step:865/1775 train_time:35761ms step_avg:41.34ms
step:866/1775 train_time:35821ms step_avg:41.36ms
step:867/1775 train_time:35880ms step_avg:41.38ms
step:868/1775 train_time:35941ms step_avg:41.41ms
step:869/1775 train_time:35999ms step_avg:41.43ms
step:870/1775 train_time:36059ms step_avg:41.45ms
step:871/1775 train_time:36117ms step_avg:41.47ms
step:872/1775 train_time:36179ms step_avg:41.49ms
step:873/1775 train_time:36237ms step_avg:41.51ms
step:874/1775 train_time:36298ms step_avg:41.53ms
step:875/1775 train_time:36356ms step_avg:41.55ms
step:876/1775 train_time:36417ms step_avg:41.57ms
step:877/1775 train_time:36475ms step_avg:41.59ms
step:878/1775 train_time:36536ms step_avg:41.61ms
step:879/1775 train_time:36594ms step_avg:41.63ms
step:880/1775 train_time:36655ms step_avg:41.65ms
step:881/1775 train_time:36713ms step_avg:41.67ms
step:882/1775 train_time:36773ms step_avg:41.69ms
step:883/1775 train_time:36830ms step_avg:41.71ms
step:884/1775 train_time:36890ms step_avg:41.73ms
step:885/1775 train_time:36948ms step_avg:41.75ms
step:886/1775 train_time:37008ms step_avg:41.77ms
step:887/1775 train_time:37066ms step_avg:41.79ms
step:888/1775 train_time:37126ms step_avg:41.81ms
step:889/1775 train_time:37185ms step_avg:41.83ms
step:890/1775 train_time:37245ms step_avg:41.85ms
step:891/1775 train_time:37303ms step_avg:41.87ms
step:892/1775 train_time:37364ms step_avg:41.89ms
step:893/1775 train_time:37423ms step_avg:41.91ms
step:894/1775 train_time:37483ms step_avg:41.93ms
step:895/1775 train_time:37541ms step_avg:41.95ms
step:896/1775 train_time:37602ms step_avg:41.97ms
step:897/1775 train_time:37661ms step_avg:41.99ms
step:898/1775 train_time:37721ms step_avg:42.01ms
step:899/1775 train_time:37779ms step_avg:42.02ms
step:900/1775 train_time:37838ms step_avg:42.04ms
step:901/1775 train_time:37896ms step_avg:42.06ms
step:902/1775 train_time:37958ms step_avg:42.08ms
step:903/1775 train_time:38015ms step_avg:42.10ms
step:904/1775 train_time:38076ms step_avg:42.12ms
step:905/1775 train_time:38134ms step_avg:42.14ms
step:906/1775 train_time:38194ms step_avg:42.16ms
step:907/1775 train_time:38252ms step_avg:42.17ms
step:908/1775 train_time:38312ms step_avg:42.19ms
step:909/1775 train_time:38370ms step_avg:42.21ms
step:910/1775 train_time:38430ms step_avg:42.23ms
step:911/1775 train_time:38488ms step_avg:42.25ms
step:912/1775 train_time:38548ms step_avg:42.27ms
step:913/1775 train_time:38607ms step_avg:42.29ms
step:914/1775 train_time:38668ms step_avg:42.31ms
step:915/1775 train_time:38726ms step_avg:42.32ms
step:916/1775 train_time:38786ms step_avg:42.34ms
step:917/1775 train_time:38845ms step_avg:42.36ms
step:918/1775 train_time:38905ms step_avg:42.38ms
step:919/1775 train_time:38964ms step_avg:42.40ms
step:920/1775 train_time:39025ms step_avg:42.42ms
step:921/1775 train_time:39082ms step_avg:42.43ms
step:922/1775 train_time:39143ms step_avg:42.45ms
step:923/1775 train_time:39202ms step_avg:42.47ms
step:924/1775 train_time:39262ms step_avg:42.49ms
step:925/1775 train_time:39321ms step_avg:42.51ms
step:926/1775 train_time:39381ms step_avg:42.53ms
step:927/1775 train_time:39439ms step_avg:42.55ms
step:928/1775 train_time:39500ms step_avg:42.56ms
step:929/1775 train_time:39557ms step_avg:42.58ms
step:930/1775 train_time:39618ms step_avg:42.60ms
step:931/1775 train_time:39676ms step_avg:42.62ms
step:932/1775 train_time:39736ms step_avg:42.64ms
step:933/1775 train_time:39794ms step_avg:42.65ms
step:934/1775 train_time:39854ms step_avg:42.67ms
step:935/1775 train_time:39912ms step_avg:42.69ms
step:936/1775 train_time:39973ms step_avg:42.71ms
step:937/1775 train_time:40030ms step_avg:42.72ms
step:938/1775 train_time:40090ms step_avg:42.74ms
step:939/1775 train_time:40149ms step_avg:42.76ms
step:940/1775 train_time:40209ms step_avg:42.78ms
step:941/1775 train_time:40267ms step_avg:42.79ms
step:942/1775 train_time:40327ms step_avg:42.81ms
step:943/1775 train_time:40385ms step_avg:42.83ms
step:944/1775 train_time:40447ms step_avg:42.85ms
step:945/1775 train_time:40505ms step_avg:42.86ms
step:946/1775 train_time:40567ms step_avg:42.88ms
step:947/1775 train_time:40625ms step_avg:42.90ms
step:948/1775 train_time:40686ms step_avg:42.92ms
step:949/1775 train_time:40743ms step_avg:42.93ms
step:950/1775 train_time:40804ms step_avg:42.95ms
step:951/1775 train_time:40862ms step_avg:42.97ms
step:952/1775 train_time:40922ms step_avg:42.99ms
step:953/1775 train_time:40980ms step_avg:43.00ms
step:954/1775 train_time:41040ms step_avg:43.02ms
step:955/1775 train_time:41098ms step_avg:43.03ms
step:956/1775 train_time:41158ms step_avg:43.05ms
step:957/1775 train_time:41217ms step_avg:43.07ms
step:958/1775 train_time:41278ms step_avg:43.09ms
step:959/1775 train_time:41335ms step_avg:43.10ms
step:960/1775 train_time:41397ms step_avg:43.12ms
step:961/1775 train_time:41455ms step_avg:43.14ms
step:962/1775 train_time:41515ms step_avg:43.16ms
step:963/1775 train_time:41573ms step_avg:43.17ms
step:964/1775 train_time:41633ms step_avg:43.19ms
step:965/1775 train_time:41692ms step_avg:43.20ms
step:966/1775 train_time:41752ms step_avg:43.22ms
step:967/1775 train_time:41810ms step_avg:43.24ms
step:968/1775 train_time:41870ms step_avg:43.25ms
step:969/1775 train_time:41928ms step_avg:43.27ms
step:970/1775 train_time:41989ms step_avg:43.29ms
step:971/1775 train_time:42048ms step_avg:43.30ms
step:972/1775 train_time:42108ms step_avg:43.32ms
step:973/1775 train_time:42166ms step_avg:43.34ms
step:974/1775 train_time:42226ms step_avg:43.35ms
step:975/1775 train_time:42285ms step_avg:43.37ms
step:976/1775 train_time:42344ms step_avg:43.39ms
step:977/1775 train_time:42403ms step_avg:43.40ms
step:978/1775 train_time:42466ms step_avg:43.42ms
step:979/1775 train_time:42523ms step_avg:43.44ms
step:980/1775 train_time:42584ms step_avg:43.45ms
step:981/1775 train_time:42642ms step_avg:43.47ms
step:982/1775 train_time:42703ms step_avg:43.49ms
step:983/1775 train_time:42762ms step_avg:43.50ms
step:984/1775 train_time:42823ms step_avg:43.52ms
step:985/1775 train_time:42881ms step_avg:43.53ms
step:986/1775 train_time:42941ms step_avg:43.55ms
step:987/1775 train_time:42999ms step_avg:43.57ms
step:988/1775 train_time:43060ms step_avg:43.58ms
step:989/1775 train_time:43118ms step_avg:43.60ms
step:990/1775 train_time:43178ms step_avg:43.61ms
step:991/1775 train_time:43235ms step_avg:43.63ms
step:992/1775 train_time:43297ms step_avg:43.65ms
step:993/1775 train_time:43355ms step_avg:43.66ms
step:994/1775 train_time:43415ms step_avg:43.68ms
step:995/1775 train_time:43473ms step_avg:43.69ms
step:996/1775 train_time:43533ms step_avg:43.71ms
step:997/1775 train_time:43590ms step_avg:43.72ms
step:998/1775 train_time:43651ms step_avg:43.74ms
step:999/1775 train_time:43709ms step_avg:43.75ms
step:1000/1775 train_time:43769ms step_avg:43.77ms
step:1000/1775 val_loss:3.7313 train_time:43838ms step_avg:43.84ms
step:1001/1775 train_time:43858ms step_avg:43.81ms
step:1002/1775 train_time:43887ms step_avg:43.80ms
step:1003/1775 train_time:43948ms step_avg:43.82ms
step:1004/1775 train_time:44015ms step_avg:43.84ms
step:1005/1775 train_time:44074ms step_avg:43.85ms
step:1006/1775 train_time:44134ms step_avg:43.87ms
step:1007/1775 train_time:44193ms step_avg:43.89ms
step:1008/1775 train_time:44252ms step_avg:43.90ms
step:1009/1775 train_time:44310ms step_avg:43.91ms
step:1010/1775 train_time:44369ms step_avg:43.93ms
step:1011/1775 train_time:44427ms step_avg:43.94ms
step:1012/1775 train_time:44486ms step_avg:43.96ms
step:1013/1775 train_time:44543ms step_avg:43.97ms
step:1014/1775 train_time:44603ms step_avg:43.99ms
step:1015/1775 train_time:44660ms step_avg:44.00ms
step:1016/1775 train_time:44720ms step_avg:44.02ms
step:1017/1775 train_time:44777ms step_avg:44.03ms
step:1018/1775 train_time:44837ms step_avg:44.04ms
step:1019/1775 train_time:44897ms step_avg:44.06ms
step:1020/1775 train_time:44959ms step_avg:44.08ms
step:1021/1775 train_time:45018ms step_avg:44.09ms
step:1022/1775 train_time:45080ms step_avg:44.11ms
step:1023/1775 train_time:45138ms step_avg:44.12ms
step:1024/1775 train_time:45199ms step_avg:44.14ms
step:1025/1775 train_time:45255ms step_avg:44.15ms
step:1026/1775 train_time:45314ms step_avg:44.17ms
step:1027/1775 train_time:45372ms step_avg:44.18ms
step:1028/1775 train_time:45432ms step_avg:44.19ms
step:1029/1775 train_time:45490ms step_avg:44.21ms
step:1030/1775 train_time:45550ms step_avg:44.22ms
step:1031/1775 train_time:45608ms step_avg:44.24ms
step:1032/1775 train_time:45668ms step_avg:44.25ms
step:1033/1775 train_time:45726ms step_avg:44.27ms
step:1034/1775 train_time:45788ms step_avg:44.28ms
step:1035/1775 train_time:45847ms step_avg:44.30ms
step:1036/1775 train_time:45908ms step_avg:44.31ms
step:1037/1775 train_time:45967ms step_avg:44.33ms
step:1038/1775 train_time:46029ms step_avg:44.34ms
step:1039/1775 train_time:46089ms step_avg:44.36ms
step:1040/1775 train_time:46150ms step_avg:44.37ms
step:1041/1775 train_time:46208ms step_avg:44.39ms
step:1042/1775 train_time:46268ms step_avg:44.40ms
step:1043/1775 train_time:46325ms step_avg:44.42ms
step:1044/1775 train_time:46386ms step_avg:44.43ms
step:1045/1775 train_time:46443ms step_avg:44.44ms
step:1046/1775 train_time:46503ms step_avg:44.46ms
step:1047/1775 train_time:46560ms step_avg:44.47ms
step:1048/1775 train_time:46620ms step_avg:44.48ms
step:1049/1775 train_time:46677ms step_avg:44.50ms
step:1050/1775 train_time:46736ms step_avg:44.51ms
step:1051/1775 train_time:46795ms step_avg:44.52ms
step:1052/1775 train_time:46855ms step_avg:44.54ms
step:1053/1775 train_time:46913ms step_avg:44.55ms
step:1054/1775 train_time:46974ms step_avg:44.57ms
step:1055/1775 train_time:47033ms step_avg:44.58ms
step:1056/1775 train_time:47095ms step_avg:44.60ms
step:1057/1775 train_time:47152ms step_avg:44.61ms
step:1058/1775 train_time:47213ms step_avg:44.62ms
step:1059/1775 train_time:47270ms step_avg:44.64ms
step:1060/1775 train_time:47331ms step_avg:44.65ms
step:1061/1775 train_time:47389ms step_avg:44.66ms
step:1062/1775 train_time:47449ms step_avg:44.68ms
step:1063/1775 train_time:47507ms step_avg:44.69ms
step:1064/1775 train_time:47567ms step_avg:44.71ms
step:1065/1775 train_time:47624ms step_avg:44.72ms
step:1066/1775 train_time:47684ms step_avg:44.73ms
step:1067/1775 train_time:47742ms step_avg:44.74ms
step:1068/1775 train_time:47803ms step_avg:44.76ms
step:1069/1775 train_time:47861ms step_avg:44.77ms
step:1070/1775 train_time:47923ms step_avg:44.79ms
step:1071/1775 train_time:47982ms step_avg:44.80ms
step:1072/1775 train_time:48043ms step_avg:44.82ms
step:1073/1775 train_time:48102ms step_avg:44.83ms
step:1074/1775 train_time:48163ms step_avg:44.84ms
step:1075/1775 train_time:48221ms step_avg:44.86ms
step:1076/1775 train_time:48282ms step_avg:44.87ms
step:1077/1775 train_time:48340ms step_avg:44.88ms
step:1078/1775 train_time:48400ms step_avg:44.90ms
step:1079/1775 train_time:48457ms step_avg:44.91ms
step:1080/1775 train_time:48517ms step_avg:44.92ms
step:1081/1775 train_time:48576ms step_avg:44.94ms
step:1082/1775 train_time:48635ms step_avg:44.95ms
step:1083/1775 train_time:48693ms step_avg:44.96ms
step:1084/1775 train_time:48753ms step_avg:44.98ms
step:1085/1775 train_time:48811ms step_avg:44.99ms
step:1086/1775 train_time:48872ms step_avg:45.00ms
step:1087/1775 train_time:48930ms step_avg:45.01ms
step:1088/1775 train_time:48991ms step_avg:45.03ms
step:1089/1775 train_time:49049ms step_avg:45.04ms
step:1090/1775 train_time:49109ms step_avg:45.05ms
step:1091/1775 train_time:49169ms step_avg:45.07ms
step:1092/1775 train_time:49229ms step_avg:45.08ms
step:1093/1775 train_time:49287ms step_avg:45.09ms
step:1094/1775 train_time:49348ms step_avg:45.11ms
step:1095/1775 train_time:49406ms step_avg:45.12ms
step:1096/1775 train_time:49467ms step_avg:45.13ms
step:1097/1775 train_time:49525ms step_avg:45.15ms
step:1098/1775 train_time:49584ms step_avg:45.16ms
step:1099/1775 train_time:49642ms step_avg:45.17ms
step:1100/1775 train_time:49703ms step_avg:45.18ms
step:1101/1775 train_time:49759ms step_avg:45.19ms
step:1102/1775 train_time:49820ms step_avg:45.21ms
step:1103/1775 train_time:49878ms step_avg:45.22ms
step:1104/1775 train_time:49939ms step_avg:45.23ms
step:1105/1775 train_time:49996ms step_avg:45.25ms
step:1106/1775 train_time:50056ms step_avg:45.26ms
step:1107/1775 train_time:50114ms step_avg:45.27ms
step:1108/1775 train_time:50174ms step_avg:45.28ms
step:1109/1775 train_time:50232ms step_avg:45.29ms
step:1110/1775 train_time:50293ms step_avg:45.31ms
step:1111/1775 train_time:50351ms step_avg:45.32ms
step:1112/1775 train_time:50412ms step_avg:45.33ms
step:1113/1775 train_time:50469ms step_avg:45.35ms
step:1114/1775 train_time:50529ms step_avg:45.36ms
step:1115/1775 train_time:50587ms step_avg:45.37ms
step:1116/1775 train_time:50648ms step_avg:45.38ms
step:1117/1775 train_time:50708ms step_avg:45.40ms
step:1118/1775 train_time:50768ms step_avg:45.41ms
step:1119/1775 train_time:50827ms step_avg:45.42ms
step:1120/1775 train_time:50887ms step_avg:45.43ms
step:1121/1775 train_time:50945ms step_avg:45.45ms
step:1122/1775 train_time:51005ms step_avg:45.46ms
step:1123/1775 train_time:51062ms step_avg:45.47ms
step:1124/1775 train_time:51123ms step_avg:45.48ms
step:1125/1775 train_time:51181ms step_avg:45.49ms
step:1126/1775 train_time:51242ms step_avg:45.51ms
step:1127/1775 train_time:51300ms step_avg:45.52ms
step:1128/1775 train_time:51360ms step_avg:45.53ms
step:1129/1775 train_time:51418ms step_avg:45.54ms
step:1130/1775 train_time:51478ms step_avg:45.56ms
step:1131/1775 train_time:51537ms step_avg:45.57ms
step:1132/1775 train_time:51597ms step_avg:45.58ms
step:1133/1775 train_time:51655ms step_avg:45.59ms
step:1134/1775 train_time:51715ms step_avg:45.60ms
step:1135/1775 train_time:51774ms step_avg:45.62ms
step:1136/1775 train_time:51835ms step_avg:45.63ms
step:1137/1775 train_time:51893ms step_avg:45.64ms
step:1138/1775 train_time:51953ms step_avg:45.65ms
step:1139/1775 train_time:52011ms step_avg:45.66ms
step:1140/1775 train_time:52071ms step_avg:45.68ms
step:1141/1775 train_time:52130ms step_avg:45.69ms
step:1142/1775 train_time:52193ms step_avg:45.70ms
step:1143/1775 train_time:52251ms step_avg:45.71ms
step:1144/1775 train_time:52311ms step_avg:45.73ms
step:1145/1775 train_time:52369ms step_avg:45.74ms
step:1146/1775 train_time:52430ms step_avg:45.75ms
step:1147/1775 train_time:52488ms step_avg:45.76ms
step:1148/1775 train_time:52548ms step_avg:45.77ms
step:1149/1775 train_time:52606ms step_avg:45.78ms
step:1150/1775 train_time:52667ms step_avg:45.80ms
step:1151/1775 train_time:52725ms step_avg:45.81ms
step:1152/1775 train_time:52786ms step_avg:45.82ms
step:1153/1775 train_time:52844ms step_avg:45.83ms
step:1154/1775 train_time:52904ms step_avg:45.84ms
step:1155/1775 train_time:52962ms step_avg:45.85ms
step:1156/1775 train_time:53021ms step_avg:45.87ms
step:1157/1775 train_time:53079ms step_avg:45.88ms
step:1158/1775 train_time:53143ms step_avg:45.89ms
step:1159/1775 train_time:53227ms step_avg:45.92ms
step:1160/1775 train_time:53313ms step_avg:45.96ms
step:1161/1775 train_time:53397ms step_avg:45.99ms
step:1162/1775 train_time:53483ms step_avg:46.03ms
step:1163/1775 train_time:53566ms step_avg:46.06ms
step:1164/1775 train_time:53653ms step_avg:46.09ms
step:1165/1775 train_time:53737ms step_avg:46.13ms
step:1166/1775 train_time:53824ms step_avg:46.16ms
step:1167/1775 train_time:53907ms step_avg:46.19ms
step:1168/1775 train_time:53994ms step_avg:46.23ms
step:1169/1775 train_time:54077ms step_avg:46.26ms
step:1170/1775 train_time:54163ms step_avg:46.29ms
step:1171/1775 train_time:54247ms step_avg:46.33ms
step:1172/1775 train_time:54333ms step_avg:46.36ms
step:1173/1775 train_time:54417ms step_avg:46.39ms
step:1174/1775 train_time:54503ms step_avg:46.43ms
step:1175/1775 train_time:54586ms step_avg:46.46ms
step:1176/1775 train_time:54673ms step_avg:46.49ms
step:1177/1775 train_time:54756ms step_avg:46.52ms
step:1178/1775 train_time:54843ms step_avg:46.56ms
step:1179/1775 train_time:54926ms step_avg:46.59ms
step:1180/1775 train_time:55012ms step_avg:46.62ms
step:1181/1775 train_time:55097ms step_avg:46.65ms
step:1182/1775 train_time:55182ms step_avg:46.69ms
step:1183/1775 train_time:55266ms step_avg:46.72ms
step:1184/1775 train_time:55353ms step_avg:46.75ms
step:1185/1775 train_time:55437ms step_avg:46.78ms
step:1186/1775 train_time:55524ms step_avg:46.82ms
step:1187/1775 train_time:55606ms step_avg:46.85ms
step:1188/1775 train_time:55693ms step_avg:46.88ms
step:1189/1775 train_time:55776ms step_avg:46.91ms
step:1190/1775 train_time:55865ms step_avg:46.95ms
step:1191/1775 train_time:55948ms step_avg:46.98ms
step:1192/1775 train_time:56034ms step_avg:47.01ms
step:1193/1775 train_time:56118ms step_avg:47.04ms
step:1194/1775 train_time:56205ms step_avg:47.07ms
step:1195/1775 train_time:56288ms step_avg:47.10ms
step:1196/1775 train_time:56375ms step_avg:47.14ms
step:1197/1775 train_time:56458ms step_avg:47.17ms
step:1198/1775 train_time:56545ms step_avg:47.20ms
step:1199/1775 train_time:56629ms step_avg:47.23ms
step:1200/1775 train_time:56715ms step_avg:47.26ms
step:1201/1775 train_time:56799ms step_avg:47.29ms
step:1202/1775 train_time:56885ms step_avg:47.33ms
step:1203/1775 train_time:56968ms step_avg:47.36ms
step:1204/1775 train_time:57054ms step_avg:47.39ms
step:1205/1775 train_time:57138ms step_avg:47.42ms
step:1206/1775 train_time:57225ms step_avg:47.45ms
step:1207/1775 train_time:57309ms step_avg:47.48ms
step:1208/1775 train_time:57395ms step_avg:47.51ms
step:1209/1775 train_time:57478ms step_avg:47.54ms
step:1210/1775 train_time:57564ms step_avg:47.57ms
step:1211/1775 train_time:57647ms step_avg:47.60ms
step:1212/1775 train_time:57735ms step_avg:47.64ms
step:1213/1775 train_time:57818ms step_avg:47.67ms
step:1214/1775 train_time:57903ms step_avg:47.70ms
step:1215/1775 train_time:57988ms step_avg:47.73ms
step:1216/1775 train_time:58074ms step_avg:47.76ms
step:1217/1775 train_time:58160ms step_avg:47.79ms
step:1218/1775 train_time:58245ms step_avg:47.82ms
step:1219/1775 train_time:58328ms step_avg:47.85ms
step:1220/1775 train_time:58414ms step_avg:47.88ms
step:1221/1775 train_time:58497ms step_avg:47.91ms
step:1222/1775 train_time:58584ms step_avg:47.94ms
step:1223/1775 train_time:58667ms step_avg:47.97ms
step:1224/1775 train_time:58754ms step_avg:48.00ms
step:1225/1775 train_time:58837ms step_avg:48.03ms
step:1226/1775 train_time:58924ms step_avg:48.06ms
step:1227/1775 train_time:59007ms step_avg:48.09ms
step:1228/1775 train_time:59094ms step_avg:48.12ms
step:1229/1775 train_time:59177ms step_avg:48.15ms
step:1230/1775 train_time:59264ms step_avg:48.18ms
step:1231/1775 train_time:59348ms step_avg:48.21ms
step:1232/1775 train_time:59434ms step_avg:48.24ms
step:1233/1775 train_time:59517ms step_avg:48.27ms
step:1234/1775 train_time:59603ms step_avg:48.30ms
step:1235/1775 train_time:59687ms step_avg:48.33ms
step:1236/1775 train_time:59773ms step_avg:48.36ms
step:1237/1775 train_time:59858ms step_avg:48.39ms
step:1238/1775 train_time:59944ms step_avg:48.42ms
step:1239/1775 train_time:60027ms step_avg:48.45ms
step:1240/1775 train_time:60114ms step_avg:48.48ms
step:1241/1775 train_time:60199ms step_avg:48.51ms
step:1242/1775 train_time:60285ms step_avg:48.54ms
step:1243/1775 train_time:60368ms step_avg:48.57ms
step:1244/1775 train_time:60454ms step_avg:48.60ms
step:1245/1775 train_time:60538ms step_avg:48.63ms
step:1246/1775 train_time:60626ms step_avg:48.66ms
step:1247/1775 train_time:60708ms step_avg:48.68ms
step:1248/1775 train_time:60794ms step_avg:48.71ms
step:1249/1775 train_time:60879ms step_avg:48.74ms
step:1250/1775 train_time:60966ms step_avg:48.77ms
step:1250/1775 val_loss:3.5053 train_time:61063ms step_avg:48.85ms
step:1251/1775 train_time:61084ms step_avg:48.83ms
step:1252/1775 train_time:61140ms step_avg:48.83ms
step:1253/1775 train_time:61231ms step_avg:48.87ms
step:1254/1775 train_time:61318ms step_avg:48.90ms
step:1255/1775 train_time:61402ms step_avg:48.93ms
step:1256/1775 train_time:61486ms step_avg:48.95ms
step:1257/1775 train_time:61569ms step_avg:48.98ms
step:1258/1775 train_time:61654ms step_avg:49.01ms
step:1259/1775 train_time:61736ms step_avg:49.04ms
step:1260/1775 train_time:61821ms step_avg:49.06ms
step:1261/1775 train_time:61904ms step_avg:49.09ms
step:1262/1775 train_time:61990ms step_avg:49.12ms
step:1263/1775 train_time:62075ms step_avg:49.15ms
step:1264/1775 train_time:62167ms step_avg:49.18ms
step:1265/1775 train_time:62252ms step_avg:49.21ms
step:1266/1775 train_time:62341ms step_avg:49.24ms
step:1267/1775 train_time:62425ms step_avg:49.27ms
step:1268/1775 train_time:62510ms step_avg:49.30ms
step:1269/1775 train_time:62593ms step_avg:49.32ms
step:1270/1775 train_time:62678ms step_avg:49.35ms
step:1271/1775 train_time:62762ms step_avg:49.38ms
step:1272/1775 train_time:62847ms step_avg:49.41ms
step:1273/1775 train_time:62930ms step_avg:49.43ms
step:1274/1775 train_time:63017ms step_avg:49.46ms
step:1275/1775 train_time:63101ms step_avg:49.49ms
step:1276/1775 train_time:63192ms step_avg:49.52ms
step:1277/1775 train_time:63278ms step_avg:49.55ms
step:1278/1775 train_time:63364ms step_avg:49.58ms
step:1279/1775 train_time:63447ms step_avg:49.61ms
step:1280/1775 train_time:63533ms step_avg:49.64ms
step:1281/1775 train_time:63616ms step_avg:49.66ms
step:1282/1775 train_time:63702ms step_avg:49.69ms
step:1283/1775 train_time:63784ms step_avg:49.71ms
step:1284/1775 train_time:63870ms step_avg:49.74ms
step:1285/1775 train_time:63952ms step_avg:49.77ms
step:1286/1775 train_time:64039ms step_avg:49.80ms
step:1287/1775 train_time:64125ms step_avg:49.82ms
step:1288/1775 train_time:64212ms step_avg:49.85ms
step:1289/1775 train_time:64297ms step_avg:49.88ms
step:1290/1775 train_time:64385ms step_avg:49.91ms
step:1291/1775 train_time:64468ms step_avg:49.94ms
step:1292/1775 train_time:64553ms step_avg:49.96ms
step:1293/1775 train_time:64637ms step_avg:49.99ms
step:1294/1775 train_time:64722ms step_avg:50.02ms
step:1295/1775 train_time:64804ms step_avg:50.04ms
step:1296/1775 train_time:64888ms step_avg:50.07ms
step:1297/1775 train_time:64973ms step_avg:50.09ms
step:1298/1775 train_time:65060ms step_avg:50.12ms
step:1299/1775 train_time:65145ms step_avg:50.15ms
step:1300/1775 train_time:65234ms step_avg:50.18ms
step:1301/1775 train_time:65317ms step_avg:50.20ms
step:1302/1775 train_time:65403ms step_avg:50.23ms
step:1303/1775 train_time:65487ms step_avg:50.26ms
step:1304/1775 train_time:65574ms step_avg:50.29ms
step:1305/1775 train_time:65656ms step_avg:50.31ms
step:1306/1775 train_time:65742ms step_avg:50.34ms
step:1307/1775 train_time:65825ms step_avg:50.36ms
step:1308/1775 train_time:65910ms step_avg:50.39ms
step:1309/1775 train_time:65994ms step_avg:50.42ms
step:1310/1775 train_time:66082ms step_avg:50.44ms
step:1311/1775 train_time:66166ms step_avg:50.47ms
step:1312/1775 train_time:66254ms step_avg:50.50ms
step:1313/1775 train_time:66338ms step_avg:50.52ms
step:1314/1775 train_time:66425ms step_avg:50.55ms
step:1315/1775 train_time:66508ms step_avg:50.58ms
step:1316/1775 train_time:66594ms step_avg:50.60ms
step:1317/1775 train_time:66676ms step_avg:50.63ms
step:1318/1775 train_time:66764ms step_avg:50.66ms
step:1319/1775 train_time:66847ms step_avg:50.68ms
step:1320/1775 train_time:66934ms step_avg:50.71ms
step:1321/1775 train_time:67017ms step_avg:50.73ms
step:1322/1775 train_time:67104ms step_avg:50.76ms
step:1323/1775 train_time:67188ms step_avg:50.78ms
step:1324/1775 train_time:67275ms step_avg:50.81ms
step:1325/1775 train_time:67359ms step_avg:50.84ms
step:1326/1775 train_time:67446ms step_avg:50.86ms
step:1327/1775 train_time:67531ms step_avg:50.89ms
step:1328/1775 train_time:67615ms step_avg:50.92ms
step:1329/1775 train_time:67700ms step_avg:50.94ms
step:1330/1775 train_time:67785ms step_avg:50.97ms
step:1331/1775 train_time:67868ms step_avg:50.99ms
step:1332/1775 train_time:67954ms step_avg:51.02ms
step:1333/1775 train_time:68037ms step_avg:51.04ms
step:1334/1775 train_time:68124ms step_avg:51.07ms
step:1335/1775 train_time:68208ms step_avg:51.09ms
step:1336/1775 train_time:68295ms step_avg:51.12ms
step:1337/1775 train_time:68379ms step_avg:51.14ms
step:1338/1775 train_time:68465ms step_avg:51.17ms
step:1339/1775 train_time:68550ms step_avg:51.19ms
step:1340/1775 train_time:68635ms step_avg:51.22ms
step:1341/1775 train_time:68717ms step_avg:51.24ms
step:1342/1775 train_time:68804ms step_avg:51.27ms
step:1343/1775 train_time:68888ms step_avg:51.29ms
step:1344/1775 train_time:68973ms step_avg:51.32ms
step:1345/1775 train_time:69057ms step_avg:51.34ms
step:1346/1775 train_time:69144ms step_avg:51.37ms
step:1347/1775 train_time:69228ms step_avg:51.39ms
step:1348/1775 train_time:69314ms step_avg:51.42ms
step:1349/1775 train_time:69398ms step_avg:51.44ms
step:1350/1775 train_time:69483ms step_avg:51.47ms
step:1351/1775 train_time:69568ms step_avg:51.49ms
step:1352/1775 train_time:69653ms step_avg:51.52ms
step:1353/1775 train_time:69736ms step_avg:51.54ms
step:1354/1775 train_time:69823ms step_avg:51.57ms
step:1355/1775 train_time:69905ms step_avg:51.59ms
step:1356/1775 train_time:69992ms step_avg:51.62ms
step:1357/1775 train_time:70075ms step_avg:51.64ms
step:1358/1775 train_time:70162ms step_avg:51.67ms
step:1359/1775 train_time:70246ms step_avg:51.69ms
step:1360/1775 train_time:70333ms step_avg:51.72ms
step:1361/1775 train_time:70415ms step_avg:51.74ms
step:1362/1775 train_time:70501ms step_avg:51.76ms
step:1363/1775 train_time:70586ms step_avg:51.79ms
step:1364/1775 train_time:70673ms step_avg:51.81ms
step:1365/1775 train_time:70756ms step_avg:51.84ms
step:1366/1775 train_time:70842ms step_avg:51.86ms
step:1367/1775 train_time:70925ms step_avg:51.88ms
step:1368/1775 train_time:71012ms step_avg:51.91ms
step:1369/1775 train_time:71095ms step_avg:51.93ms
step:1370/1775 train_time:71182ms step_avg:51.96ms
step:1371/1775 train_time:71266ms step_avg:51.98ms
step:1372/1775 train_time:71353ms step_avg:52.01ms
step:1373/1775 train_time:71436ms step_avg:52.03ms
step:1374/1775 train_time:71522ms step_avg:52.05ms
step:1375/1775 train_time:71606ms step_avg:52.08ms
step:1376/1775 train_time:71692ms step_avg:52.10ms
step:1377/1775 train_time:71775ms step_avg:52.12ms
step:1378/1775 train_time:71861ms step_avg:52.15ms
step:1379/1775 train_time:71945ms step_avg:52.17ms
step:1380/1775 train_time:72033ms step_avg:52.20ms
step:1381/1775 train_time:72115ms step_avg:52.22ms
step:1382/1775 train_time:72201ms step_avg:52.24ms
step:1383/1775 train_time:72285ms step_avg:52.27ms
step:1384/1775 train_time:72372ms step_avg:52.29ms
step:1385/1775 train_time:72455ms step_avg:52.31ms
step:1386/1775 train_time:72542ms step_avg:52.34ms
step:1387/1775 train_time:72624ms step_avg:52.36ms
step:1388/1775 train_time:72711ms step_avg:52.39ms
step:1389/1775 train_time:72794ms step_avg:52.41ms
step:1390/1775 train_time:72881ms step_avg:52.43ms
step:1391/1775 train_time:72964ms step_avg:52.45ms
step:1392/1775 train_time:73051ms step_avg:52.48ms
step:1393/1775 train_time:73135ms step_avg:52.50ms
step:1394/1775 train_time:73221ms step_avg:52.53ms
step:1395/1775 train_time:73304ms step_avg:52.55ms
step:1396/1775 train_time:73391ms step_avg:52.57ms
step:1397/1775 train_time:73475ms step_avg:52.59ms
step:1398/1775 train_time:73560ms step_avg:52.62ms
step:1399/1775 train_time:73644ms step_avg:52.64ms
step:1400/1775 train_time:73731ms step_avg:52.66ms
step:1401/1775 train_time:73814ms step_avg:52.69ms
step:1402/1775 train_time:73900ms step_avg:52.71ms
step:1403/1775 train_time:73983ms step_avg:52.73ms
step:1404/1775 train_time:74069ms step_avg:52.76ms
step:1405/1775 train_time:74153ms step_avg:52.78ms
step:1406/1775 train_time:74239ms step_avg:52.80ms
step:1407/1775 train_time:74322ms step_avg:52.82ms
step:1408/1775 train_time:74409ms step_avg:52.85ms
step:1409/1775 train_time:74492ms step_avg:52.87ms
step:1410/1775 train_time:74578ms step_avg:52.89ms
step:1411/1775 train_time:74663ms step_avg:52.91ms
step:1412/1775 train_time:74749ms step_avg:52.94ms
step:1413/1775 train_time:74833ms step_avg:52.96ms
step:1414/1775 train_time:74918ms step_avg:52.98ms
step:1415/1775 train_time:75002ms step_avg:53.00ms
step:1416/1775 train_time:75087ms step_avg:53.03ms
step:1417/1775 train_time:75171ms step_avg:53.05ms
step:1418/1775 train_time:75257ms step_avg:53.07ms
step:1419/1775 train_time:75342ms step_avg:53.09ms
step:1420/1775 train_time:75430ms step_avg:53.12ms
step:1421/1775 train_time:75513ms step_avg:53.14ms
step:1422/1775 train_time:75599ms step_avg:53.16ms
step:1423/1775 train_time:75681ms step_avg:53.18ms
step:1424/1775 train_time:75768ms step_avg:53.21ms
step:1425/1775 train_time:75852ms step_avg:53.23ms
step:1426/1775 train_time:75938ms step_avg:53.25ms
step:1427/1775 train_time:76021ms step_avg:53.27ms
step:1428/1775 train_time:76106ms step_avg:53.30ms
step:1429/1775 train_time:76191ms step_avg:53.32ms
step:1430/1775 train_time:76278ms step_avg:53.34ms
step:1431/1775 train_time:76363ms step_avg:53.36ms
step:1432/1775 train_time:76450ms step_avg:53.39ms
step:1433/1775 train_time:76534ms step_avg:53.41ms
step:1434/1775 train_time:76620ms step_avg:53.43ms
step:1435/1775 train_time:76705ms step_avg:53.45ms
step:1436/1775 train_time:76791ms step_avg:53.48ms
step:1437/1775 train_time:76874ms step_avg:53.50ms
step:1438/1775 train_time:76960ms step_avg:53.52ms
step:1439/1775 train_time:77044ms step_avg:53.54ms
step:1440/1775 train_time:77130ms step_avg:53.56ms
step:1441/1775 train_time:77214ms step_avg:53.58ms
step:1442/1775 train_time:77300ms step_avg:53.61ms
step:1443/1775 train_time:77383ms step_avg:53.63ms
step:1444/1775 train_time:77471ms step_avg:53.65ms
step:1445/1775 train_time:77554ms step_avg:53.67ms
step:1446/1775 train_time:77640ms step_avg:53.69ms
step:1447/1775 train_time:77724ms step_avg:53.71ms
step:1448/1775 train_time:77810ms step_avg:53.74ms
step:1449/1775 train_time:77894ms step_avg:53.76ms
step:1450/1775 train_time:77979ms step_avg:53.78ms
step:1451/1775 train_time:78063ms step_avg:53.80ms
step:1452/1775 train_time:78150ms step_avg:53.82ms
step:1453/1775 train_time:78234ms step_avg:53.84ms
step:1454/1775 train_time:78320ms step_avg:53.87ms
step:1455/1775 train_time:78404ms step_avg:53.89ms
step:1456/1775 train_time:78491ms step_avg:53.91ms
step:1457/1775 train_time:78574ms step_avg:53.93ms
step:1458/1775 train_time:78660ms step_avg:53.95ms
step:1459/1775 train_time:78744ms step_avg:53.97ms
step:1460/1775 train_time:78832ms step_avg:53.99ms
step:1461/1775 train_time:78914ms step_avg:54.01ms
step:1462/1775 train_time:79001ms step_avg:54.04ms
step:1463/1775 train_time:79085ms step_avg:54.06ms
step:1464/1775 train_time:79172ms step_avg:54.08ms
step:1465/1775 train_time:79254ms step_avg:54.10ms
step:1466/1775 train_time:79341ms step_avg:54.12ms
step:1467/1775 train_time:79426ms step_avg:54.14ms
step:1468/1775 train_time:79512ms step_avg:54.16ms
step:1469/1775 train_time:79595ms step_avg:54.18ms
step:1470/1775 train_time:79682ms step_avg:54.21ms
step:1471/1775 train_time:79766ms step_avg:54.23ms
step:1472/1775 train_time:79852ms step_avg:54.25ms
step:1473/1775 train_time:79936ms step_avg:54.27ms
step:1474/1775 train_time:80022ms step_avg:54.29ms
step:1475/1775 train_time:80105ms step_avg:54.31ms
step:1476/1775 train_time:80192ms step_avg:54.33ms
step:1477/1775 train_time:80275ms step_avg:54.35ms
step:1478/1775 train_time:80361ms step_avg:54.37ms
step:1479/1775 train_time:80444ms step_avg:54.39ms
step:1480/1775 train_time:80531ms step_avg:54.41ms
step:1481/1775 train_time:80614ms step_avg:54.43ms
step:1482/1775 train_time:80700ms step_avg:54.45ms
step:1483/1775 train_time:80783ms step_avg:54.47ms
step:1484/1775 train_time:80870ms step_avg:54.49ms
step:1485/1775 train_time:80954ms step_avg:54.51ms
step:1486/1775 train_time:81040ms step_avg:54.54ms
step:1487/1775 train_time:81124ms step_avg:54.56ms
step:1488/1775 train_time:81210ms step_avg:54.58ms
step:1489/1775 train_time:81293ms step_avg:54.60ms
step:1490/1775 train_time:81379ms step_avg:54.62ms
step:1491/1775 train_time:81463ms step_avg:54.64ms
step:1492/1775 train_time:81550ms step_avg:54.66ms
step:1493/1775 train_time:81633ms step_avg:54.68ms
step:1494/1775 train_time:81718ms step_avg:54.70ms
step:1495/1775 train_time:81803ms step_avg:54.72ms
step:1496/1775 train_time:81889ms step_avg:54.74ms
step:1497/1775 train_time:81973ms step_avg:54.76ms
step:1498/1775 train_time:82059ms step_avg:54.78ms
step:1499/1775 train_time:82142ms step_avg:54.80ms
step:1500/1775 train_time:82229ms step_avg:54.82ms
step:1500/1775 val_loss:3.3754 train_time:82326ms step_avg:54.88ms
step:1501/1775 train_time:82346ms step_avg:54.86ms
step:1502/1775 train_time:82399ms step_avg:54.86ms
step:1503/1775 train_time:82492ms step_avg:54.88ms
step:1504/1775 train_time:82581ms step_avg:54.91ms
step:1505/1775 train_time:82665ms step_avg:54.93ms
step:1506/1775 train_time:82750ms step_avg:54.95ms
step:1507/1775 train_time:82832ms step_avg:54.96ms
step:1508/1775 train_time:82916ms step_avg:54.98ms
step:1509/1775 train_time:82999ms step_avg:55.00ms
step:1510/1775 train_time:83086ms step_avg:55.02ms
step:1511/1775 train_time:83168ms step_avg:55.04ms
step:1512/1775 train_time:83253ms step_avg:55.06ms
step:1513/1775 train_time:83337ms step_avg:55.08ms
step:1514/1775 train_time:83428ms step_avg:55.10ms
step:1515/1775 train_time:83514ms step_avg:55.12ms
step:1516/1775 train_time:83603ms step_avg:55.15ms
step:1517/1775 train_time:83688ms step_avg:55.17ms
step:1518/1775 train_time:83772ms step_avg:55.19ms
step:1519/1775 train_time:83854ms step_avg:55.20ms
step:1520/1775 train_time:83940ms step_avg:55.22ms
step:1521/1775 train_time:84022ms step_avg:55.24ms
step:1522/1775 train_time:84109ms step_avg:55.26ms
step:1523/1775 train_time:84191ms step_avg:55.28ms
step:1524/1775 train_time:84277ms step_avg:55.30ms
step:1525/1775 train_time:84361ms step_avg:55.32ms
step:1526/1775 train_time:84450ms step_avg:55.34ms
step:1527/1775 train_time:84534ms step_avg:55.36ms
step:1528/1775 train_time:84621ms step_avg:55.38ms
step:1529/1775 train_time:84706ms step_avg:55.40ms
step:1530/1775 train_time:84791ms step_avg:55.42ms
step:1531/1775 train_time:84874ms step_avg:55.44ms
step:1532/1775 train_time:84960ms step_avg:55.46ms
step:1533/1775 train_time:85044ms step_avg:55.48ms
step:1534/1775 train_time:85129ms step_avg:55.49ms
step:1535/1775 train_time:85212ms step_avg:55.51ms
step:1536/1775 train_time:85298ms step_avg:55.53ms
step:1537/1775 train_time:85384ms step_avg:55.55ms
step:1538/1775 train_time:85470ms step_avg:55.57ms
step:1539/1775 train_time:85554ms step_avg:55.59ms
step:1540/1775 train_time:85642ms step_avg:55.61ms
step:1541/1775 train_time:85726ms step_avg:55.63ms
step:1542/1775 train_time:85812ms step_avg:55.65ms
step:1543/1775 train_time:85895ms step_avg:55.67ms
step:1544/1775 train_time:85982ms step_avg:55.69ms
step:1545/1775 train_time:86065ms step_avg:55.71ms
step:1546/1775 train_time:86151ms step_avg:55.73ms
step:1547/1775 train_time:86235ms step_avg:55.74ms
step:1548/1775 train_time:86323ms step_avg:55.76ms
step:1549/1775 train_time:86408ms step_avg:55.78ms
step:1550/1775 train_time:86493ms step_avg:55.80ms
step:1551/1775 train_time:86579ms step_avg:55.82ms
step:1552/1775 train_time:86666ms step_avg:55.84ms
step:1553/1775 train_time:86749ms step_avg:55.86ms
step:1554/1775 train_time:86836ms step_avg:55.88ms
step:1555/1775 train_time:86919ms step_avg:55.90ms
step:1556/1775 train_time:87005ms step_avg:55.92ms
step:1557/1775 train_time:87088ms step_avg:55.93ms
step:1558/1775 train_time:87173ms step_avg:55.95ms
step:1559/1775 train_time:87258ms step_avg:55.97ms
step:1560/1775 train_time:87344ms step_avg:55.99ms
step:1561/1775 train_time:87428ms step_avg:56.01ms
step:1562/1775 train_time:87515ms step_avg:56.03ms
step:1563/1775 train_time:87600ms step_avg:56.05ms
step:1564/1775 train_time:87686ms step_avg:56.07ms
step:1565/1775 train_time:87770ms step_avg:56.08ms
step:1566/1775 train_time:87856ms step_avg:56.10ms
step:1567/1775 train_time:87939ms step_avg:56.12ms
step:1568/1775 train_time:88025ms step_avg:56.14ms
step:1569/1775 train_time:88108ms step_avg:56.16ms
step:1570/1775 train_time:88194ms step_avg:56.17ms
step:1571/1775 train_time:88278ms step_avg:56.19ms
step:1572/1775 train_time:88365ms step_avg:56.21ms
step:1573/1775 train_time:88449ms step_avg:56.23ms
step:1574/1775 train_time:88535ms step_avg:56.25ms
step:1575/1775 train_time:88619ms step_avg:56.27ms
step:1576/1775 train_time:88707ms step_avg:56.29ms
step:1577/1775 train_time:88790ms step_avg:56.30ms
step:1578/1775 train_time:88877ms step_avg:56.32ms
step:1579/1775 train_time:88960ms step_avg:56.34ms
step:1580/1775 train_time:89047ms step_avg:56.36ms
step:1581/1775 train_time:89129ms step_avg:56.38ms
step:1582/1775 train_time:89215ms step_avg:56.39ms
step:1583/1775 train_time:89299ms step_avg:56.41ms
step:1584/1775 train_time:89386ms step_avg:56.43ms
step:1585/1775 train_time:89470ms step_avg:56.45ms
step:1586/1775 train_time:89557ms step_avg:56.47ms
step:1587/1775 train_time:89640ms step_avg:56.48ms
step:1588/1775 train_time:89727ms step_avg:56.50ms
step:1589/1775 train_time:89811ms step_avg:56.52ms
step:1590/1775 train_time:89897ms step_avg:56.54ms
step:1591/1775 train_time:89980ms step_avg:56.56ms
step:1592/1775 train_time:90067ms step_avg:56.58ms
step:1593/1775 train_time:90151ms step_avg:56.59ms
step:1594/1775 train_time:90236ms step_avg:56.61ms
step:1595/1775 train_time:90321ms step_avg:56.63ms
step:1596/1775 train_time:90407ms step_avg:56.65ms
step:1597/1775 train_time:90492ms step_avg:56.66ms
step:1598/1775 train_time:90579ms step_avg:56.68ms
step:1599/1775 train_time:90663ms step_avg:56.70ms
step:1600/1775 train_time:90750ms step_avg:56.72ms
step:1601/1775 train_time:90832ms step_avg:56.73ms
step:1602/1775 train_time:90919ms step_avg:56.75ms
step:1603/1775 train_time:91002ms step_avg:56.77ms
step:1604/1775 train_time:91089ms step_avg:56.79ms
step:1605/1775 train_time:91171ms step_avg:56.80ms
step:1606/1775 train_time:91258ms step_avg:56.82ms
step:1607/1775 train_time:91342ms step_avg:56.84ms
step:1608/1775 train_time:91429ms step_avg:56.86ms
step:1609/1775 train_time:91511ms step_avg:56.87ms
step:1610/1775 train_time:91598ms step_avg:56.89ms
step:1611/1775 train_time:91682ms step_avg:56.91ms
step:1612/1775 train_time:91770ms step_avg:56.93ms
step:1613/1775 train_time:91854ms step_avg:56.95ms
step:1614/1775 train_time:91940ms step_avg:56.96ms
step:1615/1775 train_time:92023ms step_avg:56.98ms
step:1616/1775 train_time:92109ms step_avg:57.00ms
step:1617/1775 train_time:92193ms step_avg:57.02ms
step:1618/1775 train_time:92280ms step_avg:57.03ms
step:1619/1775 train_time:92364ms step_avg:57.05ms
step:1620/1775 train_time:92450ms step_avg:57.07ms
step:1621/1775 train_time:92533ms step_avg:57.08ms
step:1622/1775 train_time:92619ms step_avg:57.10ms
step:1623/1775 train_time:92704ms step_avg:57.12ms
step:1624/1775 train_time:92790ms step_avg:57.14ms
step:1625/1775 train_time:92873ms step_avg:57.15ms
step:1626/1775 train_time:92959ms step_avg:57.17ms
step:1627/1775 train_time:93043ms step_avg:57.19ms
step:1628/1775 train_time:93129ms step_avg:57.20ms
step:1629/1775 train_time:93213ms step_avg:57.22ms
step:1630/1775 train_time:93299ms step_avg:57.24ms
step:1631/1775 train_time:93384ms step_avg:57.26ms
step:1632/1775 train_time:93471ms step_avg:57.27ms
step:1633/1775 train_time:93555ms step_avg:57.29ms
step:1634/1775 train_time:93640ms step_avg:57.31ms
step:1635/1775 train_time:93724ms step_avg:57.32ms
step:1636/1775 train_time:93810ms step_avg:57.34ms
step:1637/1775 train_time:93893ms step_avg:57.36ms
step:1638/1775 train_time:93981ms step_avg:57.38ms
step:1639/1775 train_time:94065ms step_avg:57.39ms
step:1640/1775 train_time:94151ms step_avg:57.41ms
step:1641/1775 train_time:94235ms step_avg:57.43ms
step:1642/1775 train_time:94322ms step_avg:57.44ms
step:1643/1775 train_time:94406ms step_avg:57.46ms
step:1644/1775 train_time:94492ms step_avg:57.48ms
step:1645/1775 train_time:94576ms step_avg:57.49ms
step:1646/1775 train_time:94663ms step_avg:57.51ms
step:1647/1775 train_time:94746ms step_avg:57.53ms
step:1648/1775 train_time:94831ms step_avg:57.54ms
step:1649/1775 train_time:94915ms step_avg:57.56ms
step:1650/1775 train_time:95001ms step_avg:57.58ms
step:1651/1775 train_time:95086ms step_avg:57.59ms
step:1652/1775 train_time:95172ms step_avg:57.61ms
step:1653/1775 train_time:95256ms step_avg:57.63ms
step:1654/1775 train_time:95342ms step_avg:57.64ms
step:1655/1775 train_time:95426ms step_avg:57.66ms
step:1656/1775 train_time:95513ms step_avg:57.68ms
step:1657/1775 train_time:95596ms step_avg:57.69ms
step:1658/1775 train_time:95683ms step_avg:57.71ms
step:1659/1775 train_time:95768ms step_avg:57.73ms
step:1660/1775 train_time:95854ms step_avg:57.74ms
step:1661/1775 train_time:95936ms step_avg:57.76ms
step:1662/1775 train_time:96022ms step_avg:57.77ms
step:1663/1775 train_time:96106ms step_avg:57.79ms
step:1664/1775 train_time:96192ms step_avg:57.81ms
step:1665/1775 train_time:96276ms step_avg:57.82ms
step:1666/1775 train_time:96362ms step_avg:57.84ms
step:1667/1775 train_time:96446ms step_avg:57.86ms
step:1668/1775 train_time:96532ms step_avg:57.87ms
step:1669/1775 train_time:96616ms step_avg:57.89ms
step:1670/1775 train_time:96702ms step_avg:57.91ms
step:1671/1775 train_time:96786ms step_avg:57.92ms
step:1672/1775 train_time:96872ms step_avg:57.94ms
step:1673/1775 train_time:96955ms step_avg:57.95ms
step:1674/1775 train_time:97043ms step_avg:57.97ms
step:1675/1775 train_time:97127ms step_avg:57.99ms
step:1676/1775 train_time:97213ms step_avg:58.00ms
step:1677/1775 train_time:97297ms step_avg:58.02ms
step:1678/1775 train_time:97383ms step_avg:58.04ms
step:1679/1775 train_time:97467ms step_avg:58.05ms
step:1680/1775 train_time:97553ms step_avg:58.07ms
step:1681/1775 train_time:97637ms step_avg:58.08ms
step:1682/1775 train_time:97723ms step_avg:58.10ms
step:1683/1775 train_time:97806ms step_avg:58.11ms
step:1684/1775 train_time:97892ms step_avg:58.13ms
step:1685/1775 train_time:97976ms step_avg:58.15ms
step:1686/1775 train_time:98062ms step_avg:58.16ms
step:1687/1775 train_time:98147ms step_avg:58.18ms
step:1688/1775 train_time:98233ms step_avg:58.19ms
step:1689/1775 train_time:98316ms step_avg:58.21ms
step:1690/1775 train_time:98402ms step_avg:58.23ms
step:1691/1775 train_time:98487ms step_avg:58.24ms
step:1692/1775 train_time:98573ms step_avg:58.26ms
step:1693/1775 train_time:98656ms step_avg:58.27ms
step:1694/1775 train_time:98743ms step_avg:58.29ms
step:1695/1775 train_time:98827ms step_avg:58.31ms
step:1696/1775 train_time:98913ms step_avg:58.32ms
step:1697/1775 train_time:98996ms step_avg:58.34ms
step:1698/1775 train_time:99082ms step_avg:58.35ms
step:1699/1775 train_time:99167ms step_avg:58.37ms
step:1700/1775 train_time:99252ms step_avg:58.38ms
step:1701/1775 train_time:99336ms step_avg:58.40ms
step:1702/1775 train_time:99422ms step_avg:58.41ms
step:1703/1775 train_time:99506ms step_avg:58.43ms
step:1704/1775 train_time:99593ms step_avg:58.45ms
step:1705/1775 train_time:99678ms step_avg:58.46ms
step:1706/1775 train_time:99763ms step_avg:58.48ms
step:1707/1775 train_time:99849ms step_avg:58.49ms
step:1708/1775 train_time:99934ms step_avg:58.51ms
step:1709/1775 train_time:100017ms step_avg:58.52ms
step:1710/1775 train_time:100105ms step_avg:58.54ms
step:1711/1775 train_time:100189ms step_avg:58.56ms
step:1712/1775 train_time:100275ms step_avg:58.57ms
step:1713/1775 train_time:100358ms step_avg:58.59ms
step:1714/1775 train_time:100444ms step_avg:58.60ms
step:1715/1775 train_time:100528ms step_avg:58.62ms
step:1716/1775 train_time:100615ms step_avg:58.63ms
step:1717/1775 train_time:100698ms step_avg:58.65ms
step:1718/1775 train_time:100784ms step_avg:58.66ms
step:1719/1775 train_time:100868ms step_avg:58.68ms
step:1720/1775 train_time:100954ms step_avg:58.69ms
step:1721/1775 train_time:101037ms step_avg:58.71ms
step:1722/1775 train_time:101123ms step_avg:58.72ms
step:1723/1775 train_time:101207ms step_avg:58.74ms
step:1724/1775 train_time:101293ms step_avg:58.75ms
step:1725/1775 train_time:101377ms step_avg:58.77ms
step:1726/1775 train_time:101463ms step_avg:58.79ms
step:1727/1775 train_time:101548ms step_avg:58.80ms
step:1728/1775 train_time:101634ms step_avg:58.82ms
step:1729/1775 train_time:101718ms step_avg:58.83ms
step:1730/1775 train_time:101804ms step_avg:58.85ms
step:1731/1775 train_time:101889ms step_avg:58.86ms
step:1732/1775 train_time:101975ms step_avg:58.88ms
step:1733/1775 train_time:102059ms step_avg:58.89ms
step:1734/1775 train_time:102145ms step_avg:58.91ms
step:1735/1775 train_time:102229ms step_avg:58.92ms
step:1736/1775 train_time:102319ms step_avg:58.94ms
step:1737/1775 train_time:102402ms step_avg:58.95ms
step:1738/1775 train_time:102490ms step_avg:58.97ms
step:1739/1775 train_time:102573ms step_avg:58.98ms
step:1740/1775 train_time:102660ms step_avg:59.00ms
step:1741/1775 train_time:102744ms step_avg:59.01ms
step:1742/1775 train_time:102831ms step_avg:59.03ms
step:1743/1775 train_time:102915ms step_avg:59.04ms
step:1744/1775 train_time:103002ms step_avg:59.06ms
step:1745/1775 train_time:103088ms step_avg:59.08ms
step:1746/1775 train_time:103173ms step_avg:59.09ms
step:1747/1775 train_time:103257ms step_avg:59.11ms
step:1748/1775 train_time:103344ms step_avg:59.12ms
step:1749/1775 train_time:103428ms step_avg:59.14ms
step:1750/1775 train_time:103515ms step_avg:59.15ms
step:1750/1775 val_loss:3.2841 train_time:103613ms step_avg:59.21ms
step:1751/1775 train_time:103632ms step_avg:59.18ms
step:1752/1775 train_time:103687ms step_avg:59.18ms
step:1753/1775 train_time:103782ms step_avg:59.20ms
step:1754/1775 train_time:103868ms step_avg:59.22ms
step:1755/1775 train_time:103952ms step_avg:59.23ms
step:1756/1775 train_time:104037ms step_avg:59.25ms
step:1757/1775 train_time:104120ms step_avg:59.26ms
step:1758/1775 train_time:104205ms step_avg:59.27ms
step:1759/1775 train_time:104289ms step_avg:59.29ms
step:1760/1775 train_time:104374ms step_avg:59.30ms
step:1761/1775 train_time:104457ms step_avg:59.32ms
step:1762/1775 train_time:104543ms step_avg:59.33ms
step:1763/1775 train_time:104630ms step_avg:59.35ms
step:1764/1775 train_time:104720ms step_avg:59.37ms
step:1765/1775 train_time:104806ms step_avg:59.38ms
step:1766/1775 train_time:104896ms step_avg:59.40ms
step:1767/1775 train_time:104979ms step_avg:59.41ms
step:1768/1775 train_time:105065ms step_avg:59.43ms
step:1769/1775 train_time:105150ms step_avg:59.44ms
step:1770/1775 train_time:105235ms step_avg:59.45ms
step:1771/1775 train_time:105317ms step_avg:59.47ms
step:1772/1775 train_time:105403ms step_avg:59.48ms
step:1773/1775 train_time:105486ms step_avg:59.50ms
step:1774/1775 train_time:105572ms step_avg:59.51ms
step:1775/1775 train_time:105658ms step_avg:59.53ms
step:1775/1775 val_loss:3.2776 train_time:105759ms step_avg:59.58ms
peak memory allocated: 29148 MiB reserved: 44798 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977

        # This call computes relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# Fused Softcapped Cross Entropy

@triton.jit
def fused_softcapped_entropy_fwd_kernel(
    logits_ptr, losses_ptr, lse_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)
    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    
    max_val = -float('inf')
    sum_exp = 0.0
    
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)
        z = A * tl.sigmoid((val + B) / C)
        z = tl.where(mask, z, -float('inf'))
        curr_max = tl.max(z, axis=0)
        new_max = tl.maximum(max_val, curr_max)
        sum_exp = sum_exp * tl.exp(max_val - new_max) + tl.sum(tl.exp(z - new_max), axis=0)
        max_val = new_max
    
    lse = max_val + tl.log(sum_exp)
    tl.store(lse_ptr + row_idx, lse)
    
    total_loss = 0.0
    for k in range(n_predict):
        target_idx = row_idx + k
        if target_idx < n_rows:
            weight = tl.load(mtp_weights_ptr + k)
            if weight > 0:
                target = tl.load(targets_ptr + target_idx).to(tl.int32)
                if target >= 0 and target < n_cols:
                    val_target = tl.load(logits_row_ptr + target).to(tl.float32)
                    z_target = A * tl.sigmoid((val_target + B) / C)
                    total_loss += weight * (lse - z_target)
    
    tl.store(losses_ptr + row_idx, total_loss)

@triton.jit
def fused_softcapped_entropy_bwd_kernel(
    grad_input_ptr, grad_output_ptr, lse_ptr, logits_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v, stride_grad_n, stride_grad_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)

    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    grad_row_ptr = grad_input_ptr + row_idx * stride_grad_n
    
    lse = tl.load(lse_ptr + row_idx)
    grad_loss = tl.load(grad_output_ptr + row_idx)
    
    S_w = 0.0
    for k in range(n_predict):
        if row_idx + k < n_rows:
            S_w += tl.load(mtp_weights_ptr + k)
            
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=0.0).to(tl.float32)
        u = (val + B) / C
        sigmoid_u = tl.sigmoid(u)
        z = A * sigmoid_u
        p = tl.exp(z - lse)
        
        term1 = S_w * p
        term2 = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
        for k in range(n_predict):
            if row_idx + k < n_rows:
                target = tl.load(targets_ptr + row_idx + k).to(tl.int32)
                weight = tl.load(mtp_weights_ptr + k)
                term2 += tl.where(cols == target, weight, 0.0)
        
        grad_z = grad_loss * (term1 - term2)
        dz_dx = (1.0 / C) * z * (1.0 - sigmoid_u)
        grad_x = grad_z * dz_dx
        tl.store(grad_row_ptr + cols, grad_x.to(tl.bfloat16), mask=mask)

class FusedSoftcappedCrossEntropy(torch.autograd.Function):
    @staticmethod
    def forward(ctx, logits, targets, mtp_weights, A=23.0, B=5.0, C=7.5):
        n_rows, n_cols = logits.shape
        if mtp_weights is None:
             mtp_weights = torch.tensor([1.0], device=logits.device, dtype=torch.float32)
        n_predict = mtp_weights.shape[0]

        losses = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        lse = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        
        logits = logits.contiguous()
        targets = targets.contiguous()
        mtp_weights = mtp_weights.contiguous()

        grid = (n_rows,)
        fused_softcapped_entropy_fwd_kernel[grid](
            logits, losses, lse, targets, mtp_weights,
            logits.stride(0), logits.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        
        ctx.save_for_backward(logits, targets, mtp_weights, lse)
        ctx.params = (A, B, C)
        return losses

    @staticmethod
    def backward(ctx, grad_output):
        logits, targets, mtp_weights, lse = ctx.saved_tensors
        A, B, C = ctx.params
        n_rows, n_cols = logits.shape
        n_predict = mtp_weights.shape[0]
        
        grad_input = torch.empty((n_rows, n_cols), dtype=torch.bfloat16, device=logits.device)
        grad_output = grad_output.contiguous()
        
        grid = (n_rows,)
        fused_softcapped_entropy_bwd_kernel[grid](
            grad_input, grad_output, lse, logits, targets, mtp_weights,
            logits.stride(0), logits.stride(1), grad_input.stride(0), grad_input.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        return grad_input, None, None, None, None, None

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if self.training:
            losses = FusedSoftcappedCrossEntropy.apply(logits.view(-1, logits.size(-1)), target_seq, mtp_weights)
            loss = losses.sum()
        else:
            logits = 23 * torch.sigmoid((logits + 5) / 7.5)
            logits_for_loss = logits.float()
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Jan  8 2026, 06:52:19) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan 15 21:51:29 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   36C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   31C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   33C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   35C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   38C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    206102      C   /usr/bin/python3                             1510MiB |
|    1   N/A  N/A    206103      C   /usr/bin/python3                             1510MiB |
|    2   N/A  N/A    206104      C   /usr/bin/python3                             1510MiB |
|    3   N/A  N/A    206105      C   /usr/bin/python3                             1510MiB |
|    4   N/A  N/A    206106      C   /usr/bin/python3                             1510MiB |
|    5   N/A  N/A    206107      C   /usr/bin/python3                             1510MiB |
|    6   N/A  N/A    206108      C   /usr/bin/python3                             1510MiB |
|    7   N/A  N/A    206109      C   /usr/bin/python3                             1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8288 train_time:0ms step_avg:0.04ms
step:1/1775 train_time:74ms step_avg:74.30ms
step:2/1775 train_time:99ms step_avg:49.61ms
step:3/1775 train_time:121ms step_avg:40.18ms
step:4/1775 train_time:146ms step_avg:36.54ms
step:5/1775 train_time:177ms step_avg:35.34ms
step:6/1775 train_time:260ms step_avg:43.29ms
step:7/1775 train_time:279ms step_avg:39.86ms
step:8/1775 train_time:302ms step_avg:37.79ms
step:9/1775 train_time:333ms step_avg:37.00ms
step:10/1775 train_time:366ms step_avg:36.57ms
step:11/1775 train_time:397ms step_avg:36.08ms
step:12/1775 train_time:430ms step_avg:35.83ms
step:13/1775 train_time:461ms step_avg:35.44ms
step:14/1775 train_time:494ms step_avg:35.28ms
step:15/1775 train_time:525ms step_avg:35.00ms
step:16/1775 train_time:558ms step_avg:34.88ms
step:17/1775 train_time:589ms step_avg:34.67ms
step:18/1775 train_time:623ms step_avg:34.61ms
step:19/1775 train_time:654ms step_avg:34.43ms
step:20/1775 train_time:687ms step_avg:34.37ms
step:21/1775 train_time:719ms step_avg:34.22ms
step:22/1775 train_time:752ms step_avg:34.18ms
step:23/1775 train_time:783ms step_avg:34.04ms
step:24/1775 train_time:816ms step_avg:34.00ms
step:25/1775 train_time:847ms step_avg:33.88ms
step:26/1775 train_time:880ms step_avg:33.86ms
step:27/1775 train_time:911ms step_avg:33.75ms
step:28/1775 train_time:945ms step_avg:33.74ms
step:29/1775 train_time:976ms step_avg:33.65ms
step:30/1775 train_time:1009ms step_avg:33.63ms
step:31/1775 train_time:1040ms step_avg:33.56ms
step:32/1775 train_time:1074ms step_avg:33.55ms
step:33/1775 train_time:1105ms step_avg:33.49ms
step:34/1775 train_time:1139ms step_avg:33.49ms
step:35/1775 train_time:1171ms step_avg:33.46ms
step:36/1775 train_time:1205ms step_avg:33.48ms
step:37/1775 train_time:1237ms step_avg:33.44ms
step:38/1775 train_time:1272ms step_avg:33.47ms
step:39/1775 train_time:1304ms step_avg:33.43ms
step:40/1775 train_time:1339ms step_avg:33.48ms
step:41/1775 train_time:1370ms step_avg:33.42ms
step:42/1775 train_time:1404ms step_avg:33.42ms
step:43/1775 train_time:1436ms step_avg:33.39ms
step:44/1775 train_time:1469ms step_avg:33.38ms
step:45/1775 train_time:1500ms step_avg:33.33ms
step:46/1775 train_time:1533ms step_avg:33.33ms
step:47/1775 train_time:1564ms step_avg:33.28ms
step:48/1775 train_time:1598ms step_avg:33.29ms
step:49/1775 train_time:1629ms step_avg:33.25ms
step:50/1775 train_time:1662ms step_avg:33.25ms
step:51/1775 train_time:1693ms step_avg:33.20ms
step:52/1775 train_time:1727ms step_avg:33.21ms
step:53/1775 train_time:1758ms step_avg:33.18ms
step:54/1775 train_time:1792ms step_avg:33.18ms
step:55/1775 train_time:1823ms step_avg:33.14ms
step:56/1775 train_time:1857ms step_avg:33.16ms
step:57/1775 train_time:1888ms step_avg:33.12ms
step:58/1775 train_time:1921ms step_avg:33.13ms
step:59/1775 train_time:1953ms step_avg:33.09ms
step:60/1775 train_time:1986ms step_avg:33.10ms
step:61/1775 train_time:2017ms step_avg:33.07ms
step:62/1775 train_time:2050ms step_avg:33.07ms
step:63/1775 train_time:2082ms step_avg:33.04ms
step:64/1775 train_time:2115ms step_avg:33.05ms
step:65/1775 train_time:2146ms step_avg:33.02ms
step:66/1775 train_time:2180ms step_avg:33.03ms
step:67/1775 train_time:2211ms step_avg:33.00ms
step:68/1775 train_time:2245ms step_avg:33.02ms
step:69/1775 train_time:2277ms step_avg:33.00ms
step:70/1775 train_time:2311ms step_avg:33.01ms
step:71/1775 train_time:2343ms step_avg:33.00ms
step:72/1775 train_time:2377ms step_avg:33.01ms
step:73/1775 train_time:2408ms step_avg:32.99ms
step:74/1775 train_time:2442ms step_avg:33.00ms
step:75/1775 train_time:2474ms step_avg:32.98ms
step:76/1775 train_time:2507ms step_avg:32.98ms
step:77/1775 train_time:2538ms step_avg:32.96ms
step:78/1775 train_time:2572ms step_avg:32.97ms
step:79/1775 train_time:2603ms step_avg:32.95ms
step:80/1775 train_time:2637ms step_avg:32.96ms
step:81/1775 train_time:2668ms step_avg:32.94ms
step:82/1775 train_time:2701ms step_avg:32.94ms
step:83/1775 train_time:2733ms step_avg:32.93ms
step:84/1775 train_time:2766ms step_avg:32.93ms
step:85/1775 train_time:2798ms step_avg:32.91ms
step:86/1775 train_time:2831ms step_avg:32.92ms
step:87/1775 train_time:2862ms step_avg:32.90ms
step:88/1775 train_time:2896ms step_avg:32.91ms
step:89/1775 train_time:2927ms step_avg:32.89ms
step:90/1775 train_time:2960ms step_avg:32.89ms
step:91/1775 train_time:2992ms step_avg:32.88ms
step:92/1775 train_time:3025ms step_avg:32.88ms
step:93/1775 train_time:3056ms step_avg:32.86ms
step:94/1775 train_time:3089ms step_avg:32.86ms
step:95/1775 train_time:3120ms step_avg:32.85ms
step:96/1775 train_time:3154ms step_avg:32.86ms
step:97/1775 train_time:3185ms step_avg:32.84ms
step:98/1775 train_time:3219ms step_avg:32.85ms
step:99/1775 train_time:3251ms step_avg:32.83ms
step:100/1775 train_time:3285ms step_avg:32.85ms
step:101/1775 train_time:3316ms step_avg:32.83ms
step:102/1775 train_time:3350ms step_avg:32.85ms
step:103/1775 train_time:3382ms step_avg:32.83ms
step:104/1775 train_time:3415ms step_avg:32.84ms
step:105/1775 train_time:3447ms step_avg:32.83ms
step:106/1775 train_time:3480ms step_avg:32.83ms
step:107/1775 train_time:3512ms step_avg:32.82ms
step:108/1775 train_time:3545ms step_avg:32.83ms
step:109/1775 train_time:3577ms step_avg:32.82ms
step:110/1775 train_time:3610ms step_avg:32.82ms
step:111/1775 train_time:3641ms step_avg:32.80ms
step:112/1775 train_time:3675ms step_avg:32.81ms
step:113/1775 train_time:3706ms step_avg:32.80ms
step:114/1775 train_time:3740ms step_avg:32.80ms
step:115/1775 train_time:3771ms step_avg:32.79ms
step:116/1775 train_time:3804ms step_avg:32.80ms
step:117/1775 train_time:3836ms step_avg:32.78ms
step:118/1775 train_time:3869ms step_avg:32.79ms
step:119/1775 train_time:3900ms step_avg:32.77ms
step:120/1775 train_time:3933ms step_avg:32.78ms
step:121/1775 train_time:3965ms step_avg:32.77ms
step:122/1775 train_time:3998ms step_avg:32.77ms
step:123/1775 train_time:4030ms step_avg:32.76ms
step:124/1775 train_time:4063ms step_avg:32.77ms
step:125/1775 train_time:4095ms step_avg:32.76ms
step:126/1775 train_time:4128ms step_avg:32.76ms
step:127/1775 train_time:4160ms step_avg:32.75ms
step:128/1775 train_time:4193ms step_avg:32.75ms
step:129/1775 train_time:4224ms step_avg:32.74ms
step:130/1775 train_time:4258ms step_avg:32.75ms
step:131/1775 train_time:4289ms step_avg:32.74ms
step:132/1775 train_time:4323ms step_avg:32.75ms
step:133/1775 train_time:4355ms step_avg:32.74ms
step:134/1775 train_time:4388ms step_avg:32.75ms
step:135/1775 train_time:4420ms step_avg:32.74ms
step:136/1775 train_time:4453ms step_avg:32.74ms
step:137/1775 train_time:4485ms step_avg:32.73ms
step:138/1775 train_time:4519ms step_avg:32.74ms
step:139/1775 train_time:4550ms step_avg:32.73ms
step:140/1775 train_time:4583ms step_avg:32.73ms
step:141/1775 train_time:4614ms step_avg:32.72ms
step:142/1775 train_time:4648ms step_avg:32.73ms
step:143/1775 train_time:4679ms step_avg:32.72ms
step:144/1775 train_time:4712ms step_avg:32.72ms
step:145/1775 train_time:4743ms step_avg:32.71ms
step:146/1775 train_time:4777ms step_avg:32.72ms
step:147/1775 train_time:4808ms step_avg:32.71ms
step:148/1775 train_time:4842ms step_avg:32.72ms
step:149/1775 train_time:4874ms step_avg:32.71ms
step:150/1775 train_time:4907ms step_avg:32.71ms
step:151/1775 train_time:4938ms step_avg:32.70ms
step:152/1775 train_time:4971ms step_avg:32.71ms
step:153/1775 train_time:5003ms step_avg:32.70ms
step:154/1775 train_time:5036ms step_avg:32.70ms
step:155/1775 train_time:5067ms step_avg:32.69ms
step:156/1775 train_time:5101ms step_avg:32.70ms
step:157/1775 train_time:5132ms step_avg:32.69ms
step:158/1775 train_time:5165ms step_avg:32.69ms
step:159/1775 train_time:5196ms step_avg:32.68ms
step:160/1775 train_time:5230ms step_avg:32.69ms
step:161/1775 train_time:5262ms step_avg:32.68ms
step:162/1775 train_time:5296ms step_avg:32.69ms
step:163/1775 train_time:5327ms step_avg:32.68ms
step:164/1775 train_time:5361ms step_avg:32.69ms
step:165/1775 train_time:5391ms step_avg:32.68ms
step:166/1775 train_time:5425ms step_avg:32.68ms
step:167/1775 train_time:5457ms step_avg:32.68ms
step:168/1775 train_time:5491ms step_avg:32.68ms
step:169/1775 train_time:5522ms step_avg:32.67ms
step:170/1775 train_time:5556ms step_avg:32.68ms
step:171/1775 train_time:5587ms step_avg:32.67ms
step:172/1775 train_time:5621ms step_avg:32.68ms
step:173/1775 train_time:5652ms step_avg:32.67ms
step:174/1775 train_time:5686ms step_avg:32.68ms
step:175/1775 train_time:5717ms step_avg:32.67ms
step:176/1775 train_time:5750ms step_avg:32.67ms
step:177/1775 train_time:5782ms step_avg:32.66ms
step:178/1775 train_time:5816ms step_avg:32.67ms
step:179/1775 train_time:5847ms step_avg:32.66ms
step:180/1775 train_time:5880ms step_avg:32.66ms
step:181/1775 train_time:5911ms step_avg:32.66ms
step:182/1775 train_time:5945ms step_avg:32.66ms
step:183/1775 train_time:5976ms step_avg:32.65ms
step:184/1775 train_time:6009ms step_avg:32.66ms
step:185/1775 train_time:6041ms step_avg:32.65ms
step:186/1775 train_time:6074ms step_avg:32.65ms
step:187/1775 train_time:6104ms step_avg:32.64ms
step:188/1775 train_time:6138ms step_avg:32.65ms
step:189/1775 train_time:6170ms step_avg:32.64ms
step:190/1775 train_time:6203ms step_avg:32.65ms
step:191/1775 train_time:6235ms step_avg:32.64ms
step:192/1775 train_time:6268ms step_avg:32.65ms
step:193/1775 train_time:6299ms step_avg:32.64ms
step:194/1775 train_time:6334ms step_avg:32.65ms
step:195/1775 train_time:6365ms step_avg:32.64ms
step:196/1775 train_time:6398ms step_avg:32.64ms
step:197/1775 train_time:6429ms step_avg:32.64ms
step:198/1775 train_time:6463ms step_avg:32.64ms
step:199/1775 train_time:6495ms step_avg:32.64ms
step:200/1775 train_time:6528ms step_avg:32.64ms
step:201/1775 train_time:6560ms step_avg:32.64ms
step:202/1775 train_time:6593ms step_avg:32.64ms
step:203/1775 train_time:6625ms step_avg:32.63ms
step:204/1775 train_time:6658ms step_avg:32.64ms
step:205/1775 train_time:6689ms step_avg:32.63ms
step:206/1775 train_time:6722ms step_avg:32.63ms
step:207/1775 train_time:6753ms step_avg:32.63ms
step:208/1775 train_time:6787ms step_avg:32.63ms
step:209/1775 train_time:6818ms step_avg:32.62ms
step:210/1775 train_time:6852ms step_avg:32.63ms
step:211/1775 train_time:6883ms step_avg:32.62ms
step:212/1775 train_time:6917ms step_avg:32.63ms
step:213/1775 train_time:6948ms step_avg:32.62ms
step:214/1775 train_time:6982ms step_avg:32.62ms
step:215/1775 train_time:7013ms step_avg:32.62ms
step:216/1775 train_time:7047ms step_avg:32.62ms
step:217/1775 train_time:7078ms step_avg:32.62ms
step:218/1775 train_time:7111ms step_avg:32.62ms
step:219/1775 train_time:7142ms step_avg:32.61ms
step:220/1775 train_time:7176ms step_avg:32.62ms
step:221/1775 train_time:7207ms step_avg:32.61ms
step:222/1775 train_time:7240ms step_avg:32.61ms
step:223/1775 train_time:7271ms step_avg:32.61ms
step:224/1775 train_time:7305ms step_avg:32.61ms
step:225/1775 train_time:7336ms step_avg:32.60ms
step:226/1775 train_time:7369ms step_avg:32.61ms
step:227/1775 train_time:7401ms step_avg:32.60ms
step:228/1775 train_time:7434ms step_avg:32.61ms
step:229/1775 train_time:7465ms step_avg:32.60ms
step:230/1775 train_time:7499ms step_avg:32.60ms
step:231/1775 train_time:7530ms step_avg:32.60ms
step:232/1775 train_time:7563ms step_avg:32.60ms
step:233/1775 train_time:7595ms step_avg:32.59ms
step:234/1775 train_time:7628ms step_avg:32.60ms
step:235/1775 train_time:7659ms step_avg:32.59ms
step:236/1775 train_time:7693ms step_avg:32.60ms
step:237/1775 train_time:7724ms step_avg:32.59ms
step:238/1775 train_time:7757ms step_avg:32.59ms
step:239/1775 train_time:7788ms step_avg:32.59ms
step:240/1775 train_time:7821ms step_avg:32.59ms
step:241/1775 train_time:7852ms step_avg:32.58ms
step:242/1775 train_time:7886ms step_avg:32.59ms
step:243/1775 train_time:7918ms step_avg:32.58ms
step:244/1775 train_time:7951ms step_avg:32.59ms
step:245/1775 train_time:7982ms step_avg:32.58ms
step:246/1775 train_time:8016ms step_avg:32.58ms
step:247/1775 train_time:8047ms step_avg:32.58ms
step:248/1775 train_time:8081ms step_avg:32.58ms
step:249/1775 train_time:8112ms step_avg:32.58ms
step:250/1775 train_time:8145ms step_avg:32.58ms
step:250/1775 val_loss:4.6002 train_time:8185ms step_avg:32.74ms
step:251/1775 train_time:8205ms step_avg:32.69ms
step:252/1775 train_time:8225ms step_avg:32.64ms
step:253/1775 train_time:8243ms step_avg:32.58ms
step:254/1775 train_time:8276ms step_avg:32.58ms
step:255/1775 train_time:8307ms step_avg:32.58ms
step:256/1775 train_time:8342ms step_avg:32.59ms
step:257/1775 train_time:8374ms step_avg:32.58ms
step:258/1775 train_time:8407ms step_avg:32.59ms
step:259/1775 train_time:8440ms step_avg:32.59ms
step:260/1775 train_time:8473ms step_avg:32.59ms
step:261/1775 train_time:8504ms step_avg:32.58ms
step:262/1775 train_time:8537ms step_avg:32.58ms
step:263/1775 train_time:8568ms step_avg:32.58ms
step:264/1775 train_time:8602ms step_avg:32.58ms
step:265/1775 train_time:8633ms step_avg:32.58ms
step:266/1775 train_time:8666ms step_avg:32.58ms
step:267/1775 train_time:8697ms step_avg:32.57ms
step:268/1775 train_time:8730ms step_avg:32.57ms
step:269/1775 train_time:8761ms step_avg:32.57ms
step:270/1775 train_time:8794ms step_avg:32.57ms
step:271/1775 train_time:8826ms step_avg:32.57ms
step:272/1775 train_time:8859ms step_avg:32.57ms
step:273/1775 train_time:8890ms step_avg:32.56ms
step:274/1775 train_time:8923ms step_avg:32.57ms
step:275/1775 train_time:8954ms step_avg:32.56ms
step:276/1775 train_time:8987ms step_avg:32.56ms
step:277/1775 train_time:9018ms step_avg:32.56ms
step:278/1775 train_time:9052ms step_avg:32.56ms
step:279/1775 train_time:9083ms step_avg:32.55ms
step:280/1775 train_time:9116ms step_avg:32.56ms
step:281/1775 train_time:9148ms step_avg:32.55ms
step:282/1775 train_time:9182ms step_avg:32.56ms
step:283/1775 train_time:9213ms step_avg:32.56ms
step:284/1775 train_time:9248ms step_avg:32.56ms
step:285/1775 train_time:9280ms step_avg:32.56ms
step:286/1775 train_time:9314ms step_avg:32.57ms
step:287/1775 train_time:9346ms step_avg:32.56ms
step:288/1775 train_time:9379ms step_avg:32.57ms
step:289/1775 train_time:9411ms step_avg:32.56ms
step:290/1775 train_time:9445ms step_avg:32.57ms
step:291/1775 train_time:9475ms step_avg:32.56ms
step:292/1775 train_time:9509ms step_avg:32.57ms
step:293/1775 train_time:9541ms step_avg:32.56ms
step:294/1775 train_time:9574ms step_avg:32.56ms
step:295/1775 train_time:9605ms step_avg:32.56ms
step:296/1775 train_time:9638ms step_avg:32.56ms
step:297/1775 train_time:9670ms step_avg:32.56ms
step:298/1775 train_time:9703ms step_avg:32.56ms
step:299/1775 train_time:9734ms step_avg:32.56ms
step:300/1775 train_time:9767ms step_avg:32.56ms
step:301/1775 train_time:9798ms step_avg:32.55ms
step:302/1775 train_time:9831ms step_avg:32.55ms
step:303/1775 train_time:9862ms step_avg:32.55ms
step:304/1775 train_time:9896ms step_avg:32.55ms
step:305/1775 train_time:9926ms step_avg:32.55ms
step:306/1775 train_time:9960ms step_avg:32.55ms
step:307/1775 train_time:9991ms step_avg:32.54ms
step:308/1775 train_time:10024ms step_avg:32.55ms
step:309/1775 train_time:10055ms step_avg:32.54ms
step:310/1775 train_time:10089ms step_avg:32.54ms
step:311/1775 train_time:10119ms step_avg:32.54ms
step:312/1775 train_time:10153ms step_avg:32.54ms
step:313/1775 train_time:10185ms step_avg:32.54ms
step:314/1775 train_time:10218ms step_avg:32.54ms
step:315/1775 train_time:10250ms step_avg:32.54ms
step:316/1775 train_time:10284ms step_avg:32.54ms
step:317/1775 train_time:10315ms step_avg:32.54ms
step:318/1775 train_time:10349ms step_avg:32.54ms
step:319/1775 train_time:10380ms step_avg:32.54ms
step:320/1775 train_time:10414ms step_avg:32.54ms
step:321/1775 train_time:10445ms step_avg:32.54ms
step:322/1775 train_time:10479ms step_avg:32.54ms
step:323/1775 train_time:10510ms step_avg:32.54ms
step:324/1775 train_time:10544ms step_avg:32.54ms
step:325/1775 train_time:10575ms step_avg:32.54ms
step:326/1775 train_time:10608ms step_avg:32.54ms
step:327/1775 train_time:10640ms step_avg:32.54ms
step:328/1775 train_time:10673ms step_avg:32.54ms
step:329/1775 train_time:10703ms step_avg:32.53ms
step:330/1775 train_time:10737ms step_avg:32.54ms
step:331/1775 train_time:10768ms step_avg:32.53ms
step:332/1775 train_time:10801ms step_avg:32.53ms
step:333/1775 train_time:10832ms step_avg:32.53ms
step:334/1775 train_time:10866ms step_avg:32.53ms
step:335/1775 train_time:10896ms step_avg:32.53ms
step:336/1775 train_time:10930ms step_avg:32.53ms
step:337/1775 train_time:10961ms step_avg:32.53ms
step:338/1775 train_time:10995ms step_avg:32.53ms
step:339/1775 train_time:11026ms step_avg:32.53ms
step:340/1775 train_time:11059ms step_avg:32.53ms
step:341/1775 train_time:11091ms step_avg:32.52ms
step:342/1775 train_time:11124ms step_avg:32.53ms
step:343/1775 train_time:11155ms step_avg:32.52ms
step:344/1775 train_time:11188ms step_avg:32.52ms
step:345/1775 train_time:11220ms step_avg:32.52ms
step:346/1775 train_time:11253ms step_avg:32.52ms
step:347/1775 train_time:11285ms step_avg:32.52ms
step:348/1775 train_time:11318ms step_avg:32.52ms
step:349/1775 train_time:11350ms step_avg:32.52ms
step:350/1775 train_time:11384ms step_avg:32.52ms
step:351/1775 train_time:11415ms step_avg:32.52ms
step:352/1775 train_time:11448ms step_avg:32.52ms
step:353/1775 train_time:11480ms step_avg:32.52ms
step:354/1775 train_time:11513ms step_avg:32.52ms
step:355/1775 train_time:11545ms step_avg:32.52ms
step:356/1775 train_time:11578ms step_avg:32.52ms
step:357/1775 train_time:11610ms step_avg:32.52ms
step:358/1775 train_time:11644ms step_avg:32.52ms
step:359/1775 train_time:11675ms step_avg:32.52ms
step:360/1775 train_time:11709ms step_avg:32.52ms
step:361/1775 train_time:11740ms step_avg:32.52ms
step:362/1775 train_time:11773ms step_avg:32.52ms
step:363/1775 train_time:11804ms step_avg:32.52ms
step:364/1775 train_time:11838ms step_avg:32.52ms
step:365/1775 train_time:11869ms step_avg:32.52ms
step:366/1775 train_time:11902ms step_avg:32.52ms
step:367/1775 train_time:11934ms step_avg:32.52ms
step:368/1775 train_time:11967ms step_avg:32.52ms
step:369/1775 train_time:11999ms step_avg:32.52ms
step:370/1775 train_time:12032ms step_avg:32.52ms
step:371/1775 train_time:12063ms step_avg:32.51ms
step:372/1775 train_time:12096ms step_avg:32.52ms
step:373/1775 train_time:12128ms step_avg:32.51ms
step:374/1775 train_time:12161ms step_avg:32.52ms
step:375/1775 train_time:12193ms step_avg:32.51ms
step:376/1775 train_time:12226ms step_avg:32.52ms
step:377/1775 train_time:12257ms step_avg:32.51ms
step:378/1775 train_time:12290ms step_avg:32.51ms
step:379/1775 train_time:12321ms step_avg:32.51ms
step:380/1775 train_time:12355ms step_avg:32.51ms
step:381/1775 train_time:12387ms step_avg:32.51ms
step:382/1775 train_time:12420ms step_avg:32.51ms
step:383/1775 train_time:12451ms step_avg:32.51ms
step:384/1775 train_time:12485ms step_avg:32.51ms
step:385/1775 train_time:12516ms step_avg:32.51ms
step:386/1775 train_time:12550ms step_avg:32.51ms
step:387/1775 train_time:12581ms step_avg:32.51ms
step:388/1775 train_time:12614ms step_avg:32.51ms
step:389/1775 train_time:12645ms step_avg:32.51ms
step:390/1775 train_time:12678ms step_avg:32.51ms
step:391/1775 train_time:12710ms step_avg:32.51ms
step:392/1775 train_time:12744ms step_avg:32.51ms
step:393/1775 train_time:12775ms step_avg:32.51ms
step:394/1775 train_time:12808ms step_avg:32.51ms
step:395/1775 train_time:12839ms step_avg:32.50ms
step:396/1775 train_time:12873ms step_avg:32.51ms
step:397/1775 train_time:12904ms step_avg:32.50ms
step:398/1775 train_time:12937ms step_avg:32.51ms
step:399/1775 train_time:12969ms step_avg:32.50ms
step:400/1775 train_time:13002ms step_avg:32.50ms
step:401/1775 train_time:13033ms step_avg:32.50ms
step:402/1775 train_time:13067ms step_avg:32.50ms
step:403/1775 train_time:13098ms step_avg:32.50ms
step:404/1775 train_time:13131ms step_avg:32.50ms
step:405/1775 train_time:13162ms step_avg:32.50ms
step:406/1775 train_time:13196ms step_avg:32.50ms
step:407/1775 train_time:13227ms step_avg:32.50ms
step:408/1775 train_time:13260ms step_avg:32.50ms
step:409/1775 train_time:13291ms step_avg:32.50ms
step:410/1775 train_time:13324ms step_avg:32.50ms
step:411/1775 train_time:13355ms step_avg:32.49ms
step:412/1775 train_time:13389ms step_avg:32.50ms
step:413/1775 train_time:13420ms step_avg:32.49ms
step:414/1775 train_time:13453ms step_avg:32.50ms
step:415/1775 train_time:13484ms step_avg:32.49ms
step:416/1775 train_time:13518ms step_avg:32.49ms
step:417/1775 train_time:13550ms step_avg:32.49ms
step:418/1775 train_time:13583ms step_avg:32.49ms
step:419/1775 train_time:13614ms step_avg:32.49ms
step:420/1775 train_time:13647ms step_avg:32.49ms
step:421/1775 train_time:13679ms step_avg:32.49ms
step:422/1775 train_time:13712ms step_avg:32.49ms
step:423/1775 train_time:13744ms step_avg:32.49ms
step:424/1775 train_time:13777ms step_avg:32.49ms
step:425/1775 train_time:13809ms step_avg:32.49ms
step:426/1775 train_time:13842ms step_avg:32.49ms
step:427/1775 train_time:13874ms step_avg:32.49ms
step:428/1775 train_time:13907ms step_avg:32.49ms
step:429/1775 train_time:13938ms step_avg:32.49ms
step:430/1775 train_time:13972ms step_avg:32.49ms
step:431/1775 train_time:14003ms step_avg:32.49ms
step:432/1775 train_time:14036ms step_avg:32.49ms
step:433/1775 train_time:14068ms step_avg:32.49ms
step:434/1775 train_time:14101ms step_avg:32.49ms
step:435/1775 train_time:14132ms step_avg:32.49ms
step:436/1775 train_time:14166ms step_avg:32.49ms
step:437/1775 train_time:14198ms step_avg:32.49ms
step:438/1775 train_time:14231ms step_avg:32.49ms
step:439/1775 train_time:14262ms step_avg:32.49ms
step:440/1775 train_time:14295ms step_avg:32.49ms
step:441/1775 train_time:14326ms step_avg:32.49ms
step:442/1775 train_time:14360ms step_avg:32.49ms
step:443/1775 train_time:14391ms step_avg:32.49ms
step:444/1775 train_time:14425ms step_avg:32.49ms
step:445/1775 train_time:14456ms step_avg:32.49ms
step:446/1775 train_time:14490ms step_avg:32.49ms
step:447/1775 train_time:14521ms step_avg:32.48ms
step:448/1775 train_time:14554ms step_avg:32.49ms
step:449/1775 train_time:14585ms step_avg:32.48ms
step:450/1775 train_time:14618ms step_avg:32.48ms
step:451/1775 train_time:14650ms step_avg:32.48ms
step:452/1775 train_time:14684ms step_avg:32.49ms
step:453/1775 train_time:14714ms step_avg:32.48ms
step:454/1775 train_time:14747ms step_avg:32.48ms
step:455/1775 train_time:14779ms step_avg:32.48ms
step:456/1775 train_time:14812ms step_avg:32.48ms
step:457/1775 train_time:14844ms step_avg:32.48ms
step:458/1775 train_time:14877ms step_avg:32.48ms
step:459/1775 train_time:14908ms step_avg:32.48ms
step:460/1775 train_time:14941ms step_avg:32.48ms
step:461/1775 train_time:14972ms step_avg:32.48ms
step:462/1775 train_time:15006ms step_avg:32.48ms
step:463/1775 train_time:15037ms step_avg:32.48ms
step:464/1775 train_time:15070ms step_avg:32.48ms
step:465/1775 train_time:15101ms step_avg:32.48ms
step:466/1775 train_time:15135ms step_avg:32.48ms
step:467/1775 train_time:15167ms step_avg:32.48ms
step:468/1775 train_time:15200ms step_avg:32.48ms
step:469/1775 train_time:15231ms step_avg:32.48ms
step:470/1775 train_time:15265ms step_avg:32.48ms
step:471/1775 train_time:15296ms step_avg:32.48ms
step:472/1775 train_time:15329ms step_avg:32.48ms
step:473/1775 train_time:15361ms step_avg:32.48ms
step:474/1775 train_time:15394ms step_avg:32.48ms
step:475/1775 train_time:15426ms step_avg:32.47ms
step:476/1775 train_time:15459ms step_avg:32.48ms
step:477/1775 train_time:15490ms step_avg:32.47ms
step:478/1775 train_time:15524ms step_avg:32.48ms
step:479/1775 train_time:15555ms step_avg:32.47ms
step:480/1775 train_time:15588ms step_avg:32.48ms
step:481/1775 train_time:15620ms step_avg:32.47ms
step:482/1775 train_time:15653ms step_avg:32.48ms
step:483/1775 train_time:15684ms step_avg:32.47ms
step:484/1775 train_time:15717ms step_avg:32.47ms
step:485/1775 train_time:15749ms step_avg:32.47ms
step:486/1775 train_time:15782ms step_avg:32.47ms
step:487/1775 train_time:15814ms step_avg:32.47ms
step:488/1775 train_time:15847ms step_avg:32.47ms
step:489/1775 train_time:15878ms step_avg:32.47ms
step:490/1775 train_time:15912ms step_avg:32.47ms
step:491/1775 train_time:15943ms step_avg:32.47ms
step:492/1775 train_time:15976ms step_avg:32.47ms
step:493/1775 train_time:16008ms step_avg:32.47ms
step:494/1775 train_time:16041ms step_avg:32.47ms
step:495/1775 train_time:16073ms step_avg:32.47ms
step:496/1775 train_time:16106ms step_avg:32.47ms
step:497/1775 train_time:16137ms step_avg:32.47ms
step:498/1775 train_time:16171ms step_avg:32.47ms
step:499/1775 train_time:16202ms step_avg:32.47ms
step:500/1775 train_time:16235ms step_avg:32.47ms
step:500/1775 val_loss:4.2668 train_time:16276ms step_avg:32.55ms
step:501/1775 train_time:16296ms step_avg:32.53ms
step:502/1775 train_time:16316ms step_avg:32.50ms
step:503/1775 train_time:16334ms step_avg:32.47ms
step:504/1775 train_time:16368ms step_avg:32.48ms
step:505/1775 train_time:16402ms step_avg:32.48ms
step:506/1775 train_time:16436ms step_avg:32.48ms
step:507/1775 train_time:16468ms step_avg:32.48ms
step:508/1775 train_time:16501ms step_avg:32.48ms
step:509/1775 train_time:16532ms step_avg:32.48ms
step:510/1775 train_time:16565ms step_avg:32.48ms
step:511/1775 train_time:16596ms step_avg:32.48ms
step:512/1775 train_time:16630ms step_avg:32.48ms
step:513/1775 train_time:16660ms step_avg:32.48ms
step:514/1775 train_time:16694ms step_avg:32.48ms
step:515/1775 train_time:16725ms step_avg:32.48ms
step:516/1775 train_time:16758ms step_avg:32.48ms
step:517/1775 train_time:16789ms step_avg:32.47ms
step:518/1775 train_time:16822ms step_avg:32.47ms
step:519/1775 train_time:16852ms step_avg:32.47ms
step:520/1775 train_time:16885ms step_avg:32.47ms
step:521/1775 train_time:16916ms step_avg:32.47ms
step:522/1775 train_time:16949ms step_avg:32.47ms
step:523/1775 train_time:16980ms step_avg:32.47ms
step:524/1775 train_time:17013ms step_avg:32.47ms
step:525/1775 train_time:17044ms step_avg:32.47ms
step:526/1775 train_time:17078ms step_avg:32.47ms
step:527/1775 train_time:17109ms step_avg:32.47ms
step:528/1775 train_time:17142ms step_avg:32.47ms
step:529/1775 train_time:17173ms step_avg:32.46ms
step:530/1775 train_time:17206ms step_avg:32.46ms
step:531/1775 train_time:17239ms step_avg:32.46ms
step:532/1775 train_time:17273ms step_avg:32.47ms
step:533/1775 train_time:17304ms step_avg:32.47ms
step:534/1775 train_time:17339ms step_avg:32.47ms
step:535/1775 train_time:17370ms step_avg:32.47ms
step:536/1775 train_time:17403ms step_avg:32.47ms
step:537/1775 train_time:17435ms step_avg:32.47ms
step:538/1775 train_time:17468ms step_avg:32.47ms
step:539/1775 train_time:17499ms step_avg:32.47ms
step:540/1775 train_time:17533ms step_avg:32.47ms
step:541/1775 train_time:17564ms step_avg:32.47ms
step:542/1775 train_time:17598ms step_avg:32.47ms
step:543/1775 train_time:17630ms step_avg:32.47ms
step:544/1775 train_time:17663ms step_avg:32.47ms
step:545/1775 train_time:17694ms step_avg:32.47ms
step:546/1775 train_time:17727ms step_avg:32.47ms
step:547/1775 train_time:17759ms step_avg:32.47ms
step:548/1775 train_time:17792ms step_avg:32.47ms
step:549/1775 train_time:17823ms step_avg:32.46ms
step:550/1775 train_time:17857ms step_avg:32.47ms
step:551/1775 train_time:17889ms step_avg:32.47ms
step:552/1775 train_time:17922ms step_avg:32.47ms
step:553/1775 train_time:17953ms step_avg:32.46ms
step:554/1775 train_time:17985ms step_avg:32.46ms
step:555/1775 train_time:18016ms step_avg:32.46ms
step:556/1775 train_time:18049ms step_avg:32.46ms
step:557/1775 train_time:18080ms step_avg:32.46ms
step:558/1775 train_time:18113ms step_avg:32.46ms
step:559/1775 train_time:18145ms step_avg:32.46ms
step:560/1775 train_time:18178ms step_avg:32.46ms
step:561/1775 train_time:18210ms step_avg:32.46ms
step:562/1775 train_time:18243ms step_avg:32.46ms
step:563/1775 train_time:18274ms step_avg:32.46ms
step:564/1775 train_time:18308ms step_avg:32.46ms
step:565/1775 train_time:18340ms step_avg:32.46ms
step:566/1775 train_time:18373ms step_avg:32.46ms
step:567/1775 train_time:18405ms step_avg:32.46ms
step:568/1775 train_time:18439ms step_avg:32.46ms
step:569/1775 train_time:18470ms step_avg:32.46ms
step:570/1775 train_time:18503ms step_avg:32.46ms
step:571/1775 train_time:18535ms step_avg:32.46ms
step:572/1775 train_time:18568ms step_avg:32.46ms
step:573/1775 train_time:18599ms step_avg:32.46ms
step:574/1775 train_time:18633ms step_avg:32.46ms
step:575/1775 train_time:18664ms step_avg:32.46ms
step:576/1775 train_time:18697ms step_avg:32.46ms
step:577/1775 train_time:18728ms step_avg:32.46ms
step:578/1775 train_time:18762ms step_avg:32.46ms
step:579/1775 train_time:18794ms step_avg:32.46ms
step:580/1775 train_time:18830ms step_avg:32.47ms
step:581/1775 train_time:18888ms step_avg:32.51ms
step:582/1775 train_time:18947ms step_avg:32.55ms
step:583/1775 train_time:19004ms step_avg:32.60ms
step:584/1775 train_time:19064ms step_avg:32.64ms
step:585/1775 train_time:19122ms step_avg:32.69ms
step:586/1775 train_time:19181ms step_avg:32.73ms
step:587/1775 train_time:19239ms step_avg:32.78ms
step:588/1775 train_time:19301ms step_avg:32.82ms
step:589/1775 train_time:19360ms step_avg:32.87ms
step:590/1775 train_time:19420ms step_avg:32.91ms
step:591/1775 train_time:19477ms step_avg:32.96ms
step:592/1775 train_time:19538ms step_avg:33.00ms
step:593/1775 train_time:19597ms step_avg:33.05ms
step:594/1775 train_time:19657ms step_avg:33.09ms
step:595/1775 train_time:19714ms step_avg:33.13ms
step:596/1775 train_time:19775ms step_avg:33.18ms
step:597/1775 train_time:19834ms step_avg:33.22ms
step:598/1775 train_time:19895ms step_avg:33.27ms
step:599/1775 train_time:19953ms step_avg:33.31ms
step:600/1775 train_time:20013ms step_avg:33.35ms
step:601/1775 train_time:20070ms step_avg:33.39ms
step:602/1775 train_time:20130ms step_avg:33.44ms
step:603/1775 train_time:20187ms step_avg:33.48ms
step:604/1775 train_time:20248ms step_avg:33.52ms
step:605/1775 train_time:20307ms step_avg:33.56ms
step:606/1775 train_time:20367ms step_avg:33.61ms
step:607/1775 train_time:20424ms step_avg:33.65ms
step:608/1775 train_time:20485ms step_avg:33.69ms
step:609/1775 train_time:20544ms step_avg:33.73ms
step:610/1775 train_time:20605ms step_avg:33.78ms
step:611/1775 train_time:20663ms step_avg:33.82ms
step:612/1775 train_time:20724ms step_avg:33.86ms
step:613/1775 train_time:20781ms step_avg:33.90ms
step:614/1775 train_time:20841ms step_avg:33.94ms
step:615/1775 train_time:20899ms step_avg:33.98ms
step:616/1775 train_time:20960ms step_avg:34.03ms
step:617/1775 train_time:21017ms step_avg:34.06ms
step:618/1775 train_time:21078ms step_avg:34.11ms
step:619/1775 train_time:21136ms step_avg:34.15ms
step:620/1775 train_time:21198ms step_avg:34.19ms
step:621/1775 train_time:21256ms step_avg:34.23ms
step:622/1775 train_time:21317ms step_avg:34.27ms
step:623/1775 train_time:21376ms step_avg:34.31ms
step:624/1775 train_time:21437ms step_avg:34.35ms
step:625/1775 train_time:21496ms step_avg:34.39ms
step:626/1775 train_time:21557ms step_avg:34.44ms
step:627/1775 train_time:21615ms step_avg:34.47ms
step:628/1775 train_time:21677ms step_avg:34.52ms
step:629/1775 train_time:21735ms step_avg:34.55ms
step:630/1775 train_time:21795ms step_avg:34.59ms
step:631/1775 train_time:21853ms step_avg:34.63ms
step:632/1775 train_time:21914ms step_avg:34.67ms
step:633/1775 train_time:21971ms step_avg:34.71ms
step:634/1775 train_time:22031ms step_avg:34.75ms
step:635/1775 train_time:22089ms step_avg:34.79ms
step:636/1775 train_time:22149ms step_avg:34.83ms
step:637/1775 train_time:22207ms step_avg:34.86ms
step:638/1775 train_time:22268ms step_avg:34.90ms
step:639/1775 train_time:22325ms step_avg:34.94ms
step:640/1775 train_time:22385ms step_avg:34.98ms
step:641/1775 train_time:22443ms step_avg:35.01ms
step:642/1775 train_time:22503ms step_avg:35.05ms
step:643/1775 train_time:22562ms step_avg:35.09ms
step:644/1775 train_time:22622ms step_avg:35.13ms
step:645/1775 train_time:22680ms step_avg:35.16ms
step:646/1775 train_time:22740ms step_avg:35.20ms
step:647/1775 train_time:22798ms step_avg:35.24ms
step:648/1775 train_time:22858ms step_avg:35.27ms
step:649/1775 train_time:22916ms step_avg:35.31ms
step:650/1775 train_time:22976ms step_avg:35.35ms
step:651/1775 train_time:23034ms step_avg:35.38ms
step:652/1775 train_time:23095ms step_avg:35.42ms
step:653/1775 train_time:23154ms step_avg:35.46ms
step:654/1775 train_time:23215ms step_avg:35.50ms
step:655/1775 train_time:23274ms step_avg:35.53ms
step:656/1775 train_time:23334ms step_avg:35.57ms
step:657/1775 train_time:23392ms step_avg:35.60ms
step:658/1775 train_time:23453ms step_avg:35.64ms
step:659/1775 train_time:23512ms step_avg:35.68ms
step:660/1775 train_time:23573ms step_avg:35.72ms
step:661/1775 train_time:23631ms step_avg:35.75ms
step:662/1775 train_time:23691ms step_avg:35.79ms
step:663/1775 train_time:23748ms step_avg:35.82ms
step:664/1775 train_time:23809ms step_avg:35.86ms
step:665/1775 train_time:23867ms step_avg:35.89ms
step:666/1775 train_time:23927ms step_avg:35.93ms
step:667/1775 train_time:23985ms step_avg:35.96ms
step:668/1775 train_time:24045ms step_avg:36.00ms
step:669/1775 train_time:24103ms step_avg:36.03ms
step:670/1775 train_time:24164ms step_avg:36.07ms
step:671/1775 train_time:24222ms step_avg:36.10ms
step:672/1775 train_time:24281ms step_avg:36.13ms
step:673/1775 train_time:24339ms step_avg:36.17ms
step:674/1775 train_time:24399ms step_avg:36.20ms
step:675/1775 train_time:24458ms step_avg:36.23ms
step:676/1775 train_time:24519ms step_avg:36.27ms
step:677/1775 train_time:24577ms step_avg:36.30ms
step:678/1775 train_time:24637ms step_avg:36.34ms
step:679/1775 train_time:24696ms step_avg:36.37ms
step:680/1775 train_time:24757ms step_avg:36.41ms
step:681/1775 train_time:24814ms step_avg:36.44ms
step:682/1775 train_time:24875ms step_avg:36.47ms
step:683/1775 train_time:24934ms step_avg:36.51ms
step:684/1775 train_time:24994ms step_avg:36.54ms
step:685/1775 train_time:25052ms step_avg:36.57ms
step:686/1775 train_time:25113ms step_avg:36.61ms
step:687/1775 train_time:25171ms step_avg:36.64ms
step:688/1775 train_time:25232ms step_avg:36.67ms
step:689/1775 train_time:25289ms step_avg:36.70ms
step:690/1775 train_time:25349ms step_avg:36.74ms
step:691/1775 train_time:25408ms step_avg:36.77ms
step:692/1775 train_time:25468ms step_avg:36.80ms
step:693/1775 train_time:25525ms step_avg:36.83ms
step:694/1775 train_time:25587ms step_avg:36.87ms
step:695/1775 train_time:25645ms step_avg:36.90ms
step:696/1775 train_time:25705ms step_avg:36.93ms
step:697/1775 train_time:25762ms step_avg:36.96ms
step:698/1775 train_time:25822ms step_avg:36.99ms
step:699/1775 train_time:25880ms step_avg:37.02ms
step:700/1775 train_time:25940ms step_avg:37.06ms
step:701/1775 train_time:25998ms step_avg:37.09ms
step:702/1775 train_time:26058ms step_avg:37.12ms
step:703/1775 train_time:26115ms step_avg:37.15ms
step:704/1775 train_time:26177ms step_avg:37.18ms
step:705/1775 train_time:26235ms step_avg:37.21ms
step:706/1775 train_time:26297ms step_avg:37.25ms
step:707/1775 train_time:26354ms step_avg:37.28ms
step:708/1775 train_time:26415ms step_avg:37.31ms
step:709/1775 train_time:26474ms step_avg:37.34ms
step:710/1775 train_time:26535ms step_avg:37.37ms
step:711/1775 train_time:26594ms step_avg:37.40ms
step:712/1775 train_time:26655ms step_avg:37.44ms
step:713/1775 train_time:26713ms step_avg:37.47ms
step:714/1775 train_time:26773ms step_avg:37.50ms
step:715/1775 train_time:26831ms step_avg:37.53ms
step:716/1775 train_time:26892ms step_avg:37.56ms
step:717/1775 train_time:26951ms step_avg:37.59ms
step:718/1775 train_time:27011ms step_avg:37.62ms
step:719/1775 train_time:27069ms step_avg:37.65ms
step:720/1775 train_time:27128ms step_avg:37.68ms
step:721/1775 train_time:27186ms step_avg:37.71ms
step:722/1775 train_time:27247ms step_avg:37.74ms
step:723/1775 train_time:27304ms step_avg:37.76ms
step:724/1775 train_time:27364ms step_avg:37.80ms
step:725/1775 train_time:27422ms step_avg:37.82ms
step:726/1775 train_time:27483ms step_avg:37.86ms
step:727/1775 train_time:27541ms step_avg:37.88ms
step:728/1775 train_time:27602ms step_avg:37.91ms
step:729/1775 train_time:27660ms step_avg:37.94ms
step:730/1775 train_time:27721ms step_avg:37.97ms
step:731/1775 train_time:27778ms step_avg:38.00ms
step:732/1775 train_time:27839ms step_avg:38.03ms
step:733/1775 train_time:27897ms step_avg:38.06ms
step:734/1775 train_time:27957ms step_avg:38.09ms
step:735/1775 train_time:28014ms step_avg:38.11ms
step:736/1775 train_time:28075ms step_avg:38.14ms
step:737/1775 train_time:28134ms step_avg:38.17ms
step:738/1775 train_time:28194ms step_avg:38.20ms
step:739/1775 train_time:28252ms step_avg:38.23ms
step:740/1775 train_time:28313ms step_avg:38.26ms
step:741/1775 train_time:28371ms step_avg:38.29ms
step:742/1775 train_time:28432ms step_avg:38.32ms
step:743/1775 train_time:28491ms step_avg:38.35ms
step:744/1775 train_time:28551ms step_avg:38.37ms
step:745/1775 train_time:28609ms step_avg:38.40ms
step:746/1775 train_time:28670ms step_avg:38.43ms
step:747/1775 train_time:28727ms step_avg:38.46ms
step:748/1775 train_time:28787ms step_avg:38.49ms
step:749/1775 train_time:28845ms step_avg:38.51ms
step:750/1775 train_time:28905ms step_avg:38.54ms
step:750/1775 val_loss:3.9915 train_time:28976ms step_avg:38.63ms
step:751/1775 train_time:28997ms step_avg:38.61ms
step:752/1775 train_time:29026ms step_avg:38.60ms
step:753/1775 train_time:29085ms step_avg:38.63ms
step:754/1775 train_time:29147ms step_avg:38.66ms
step:755/1775 train_time:29206ms step_avg:38.68ms
step:756/1775 train_time:29267ms step_avg:38.71ms
step:757/1775 train_time:29324ms step_avg:38.74ms
step:758/1775 train_time:29383ms step_avg:38.76ms
step:759/1775 train_time:29441ms step_avg:38.79ms
step:760/1775 train_time:29501ms step_avg:38.82ms
step:761/1775 train_time:29558ms step_avg:38.84ms
step:762/1775 train_time:29617ms step_avg:38.87ms
step:763/1775 train_time:29674ms step_avg:38.89ms
step:764/1775 train_time:29734ms step_avg:38.92ms
step:765/1775 train_time:29792ms step_avg:38.94ms
step:766/1775 train_time:29852ms step_avg:38.97ms
step:767/1775 train_time:29910ms step_avg:39.00ms
step:768/1775 train_time:29971ms step_avg:39.03ms
step:769/1775 train_time:30033ms step_avg:39.06ms
step:770/1775 train_time:30095ms step_avg:39.08ms
step:771/1775 train_time:30154ms step_avg:39.11ms
step:772/1775 train_time:30214ms step_avg:39.14ms
step:773/1775 train_time:30273ms step_avg:39.16ms
step:774/1775 train_time:30334ms step_avg:39.19ms
step:775/1775 train_time:30392ms step_avg:39.22ms
step:776/1775 train_time:30452ms step_avg:39.24ms
step:777/1775 train_time:30510ms step_avg:39.27ms
step:778/1775 train_time:30570ms step_avg:39.29ms
step:779/1775 train_time:30628ms step_avg:39.32ms
step:780/1775 train_time:30687ms step_avg:39.34ms
step:781/1775 train_time:30744ms step_avg:39.36ms
step:782/1775 train_time:30804ms step_avg:39.39ms
step:783/1775 train_time:30861ms step_avg:39.41ms
step:784/1775 train_time:30922ms step_avg:39.44ms
step:785/1775 train_time:30981ms step_avg:39.47ms
step:786/1775 train_time:31043ms step_avg:39.49ms
step:787/1775 train_time:31102ms step_avg:39.52ms
step:788/1775 train_time:31164ms step_avg:39.55ms
step:789/1775 train_time:31221ms step_avg:39.57ms
step:790/1775 train_time:31282ms step_avg:39.60ms
step:791/1775 train_time:31340ms step_avg:39.62ms
step:792/1775 train_time:31399ms step_avg:39.65ms
step:793/1775 train_time:31457ms step_avg:39.67ms
step:794/1775 train_time:31517ms step_avg:39.69ms
step:795/1775 train_time:31574ms step_avg:39.72ms
step:796/1775 train_time:31634ms step_avg:39.74ms
step:797/1775 train_time:31692ms step_avg:39.76ms
step:798/1775 train_time:31752ms step_avg:39.79ms
step:799/1775 train_time:31810ms step_avg:39.81ms
step:800/1775 train_time:31870ms step_avg:39.84ms
step:801/1775 train_time:31929ms step_avg:39.86ms
step:802/1775 train_time:31991ms step_avg:39.89ms
step:803/1775 train_time:32050ms step_avg:39.91ms
step:804/1775 train_time:32111ms step_avg:39.94ms
step:805/1775 train_time:32169ms step_avg:39.96ms
step:806/1775 train_time:32230ms step_avg:39.99ms
step:807/1775 train_time:32289ms step_avg:40.01ms
step:808/1775 train_time:32349ms step_avg:40.04ms
step:809/1775 train_time:32407ms step_avg:40.06ms
step:810/1775 train_time:32466ms step_avg:40.08ms
step:811/1775 train_time:32525ms step_avg:40.11ms
step:812/1775 train_time:32585ms step_avg:40.13ms
step:813/1775 train_time:32643ms step_avg:40.15ms
step:814/1775 train_time:32704ms step_avg:40.18ms
step:815/1775 train_time:32761ms step_avg:40.20ms
step:816/1775 train_time:32822ms step_avg:40.22ms
step:817/1775 train_time:32879ms step_avg:40.24ms
step:818/1775 train_time:32941ms step_avg:40.27ms
step:819/1775 train_time:32999ms step_avg:40.29ms
step:820/1775 train_time:33058ms step_avg:40.31ms
step:821/1775 train_time:33115ms step_avg:40.34ms
step:822/1775 train_time:33177ms step_avg:40.36ms
step:823/1775 train_time:33235ms step_avg:40.38ms
step:824/1775 train_time:33295ms step_avg:40.41ms
step:825/1775 train_time:33353ms step_avg:40.43ms
step:826/1775 train_time:33415ms step_avg:40.45ms
step:827/1775 train_time:33473ms step_avg:40.47ms
step:828/1775 train_time:33533ms step_avg:40.50ms
step:829/1775 train_time:33591ms step_avg:40.52ms
step:830/1775 train_time:33651ms step_avg:40.54ms
step:831/1775 train_time:33710ms step_avg:40.57ms
step:832/1775 train_time:33770ms step_avg:40.59ms
step:833/1775 train_time:33827ms step_avg:40.61ms
step:834/1775 train_time:33888ms step_avg:40.63ms
step:835/1775 train_time:33946ms step_avg:40.65ms
step:836/1775 train_time:34006ms step_avg:40.68ms
step:837/1775 train_time:34064ms step_avg:40.70ms
step:838/1775 train_time:34124ms step_avg:40.72ms
step:839/1775 train_time:34184ms step_avg:40.74ms
step:840/1775 train_time:34245ms step_avg:40.77ms
step:841/1775 train_time:34302ms step_avg:40.79ms
step:842/1775 train_time:34363ms step_avg:40.81ms
step:843/1775 train_time:34422ms step_avg:40.83ms
step:844/1775 train_time:34482ms step_avg:40.86ms
step:845/1775 train_time:34539ms step_avg:40.87ms
step:846/1775 train_time:34600ms step_avg:40.90ms
step:847/1775 train_time:34658ms step_avg:40.92ms
step:848/1775 train_time:34718ms step_avg:40.94ms
step:849/1775 train_time:34775ms step_avg:40.96ms
step:850/1775 train_time:34836ms step_avg:40.98ms
step:851/1775 train_time:34893ms step_avg:41.00ms
step:852/1775 train_time:34954ms step_avg:41.03ms
step:853/1775 train_time:35012ms step_avg:41.05ms
step:854/1775 train_time:35073ms step_avg:41.07ms
step:855/1775 train_time:35132ms step_avg:41.09ms
step:856/1775 train_time:35192ms step_avg:41.11ms
step:857/1775 train_time:35251ms step_avg:41.13ms
step:858/1775 train_time:35312ms step_avg:41.16ms
step:859/1775 train_time:35371ms step_avg:41.18ms
step:860/1775 train_time:35431ms step_avg:41.20ms
step:861/1775 train_time:35490ms step_avg:41.22ms
step:862/1775 train_time:35550ms step_avg:41.24ms
step:863/1775 train_time:35607ms step_avg:41.26ms
step:864/1775 train_time:35666ms step_avg:41.28ms
step:865/1775 train_time:35724ms step_avg:41.30ms
step:866/1775 train_time:35785ms step_avg:41.32ms
step:867/1775 train_time:35842ms step_avg:41.34ms
step:868/1775 train_time:35903ms step_avg:41.36ms
step:869/1775 train_time:35960ms step_avg:41.38ms
step:870/1775 train_time:36021ms step_avg:41.40ms
step:871/1775 train_time:36080ms step_avg:41.42ms
step:872/1775 train_time:36140ms step_avg:41.44ms
step:873/1775 train_time:36198ms step_avg:41.46ms
step:874/1775 train_time:36258ms step_avg:41.49ms
step:875/1775 train_time:36316ms step_avg:41.50ms
step:876/1775 train_time:36376ms step_avg:41.52ms
step:877/1775 train_time:36435ms step_avg:41.55ms
step:878/1775 train_time:36496ms step_avg:41.57ms
step:879/1775 train_time:36553ms step_avg:41.58ms
step:880/1775 train_time:36614ms step_avg:41.61ms
step:881/1775 train_time:36672ms step_avg:41.63ms
step:882/1775 train_time:36733ms step_avg:41.65ms
step:883/1775 train_time:36791ms step_avg:41.67ms
step:884/1775 train_time:36852ms step_avg:41.69ms
step:885/1775 train_time:36911ms step_avg:41.71ms
step:886/1775 train_time:36971ms step_avg:41.73ms
step:887/1775 train_time:37031ms step_avg:41.75ms
step:888/1775 train_time:37091ms step_avg:41.77ms
step:889/1775 train_time:37149ms step_avg:41.79ms
step:890/1775 train_time:37210ms step_avg:41.81ms
step:891/1775 train_time:37268ms step_avg:41.83ms
step:892/1775 train_time:37328ms step_avg:41.85ms
step:893/1775 train_time:37387ms step_avg:41.87ms
step:894/1775 train_time:37447ms step_avg:41.89ms
step:895/1775 train_time:37506ms step_avg:41.91ms
step:896/1775 train_time:37565ms step_avg:41.93ms
step:897/1775 train_time:37623ms step_avg:41.94ms
step:898/1775 train_time:37682ms step_avg:41.96ms
step:899/1775 train_time:37741ms step_avg:41.98ms
step:900/1775 train_time:37800ms step_avg:42.00ms
step:901/1775 train_time:37859ms step_avg:42.02ms
step:902/1775 train_time:37918ms step_avg:42.04ms
step:903/1775 train_time:37976ms step_avg:42.06ms
step:904/1775 train_time:38037ms step_avg:42.08ms
step:905/1775 train_time:38096ms step_avg:42.09ms
step:906/1775 train_time:38155ms step_avg:42.11ms
step:907/1775 train_time:38213ms step_avg:42.13ms
step:908/1775 train_time:38274ms step_avg:42.15ms
step:909/1775 train_time:38332ms step_avg:42.17ms
step:910/1775 train_time:38394ms step_avg:42.19ms
step:911/1775 train_time:38452ms step_avg:42.21ms
step:912/1775 train_time:38513ms step_avg:42.23ms
step:913/1775 train_time:38572ms step_avg:42.25ms
step:914/1775 train_time:38632ms step_avg:42.27ms
step:915/1775 train_time:38690ms step_avg:42.28ms
step:916/1775 train_time:38751ms step_avg:42.30ms
step:917/1775 train_time:38809ms step_avg:42.32ms
step:918/1775 train_time:38869ms step_avg:42.34ms
step:919/1775 train_time:38928ms step_avg:42.36ms
step:920/1775 train_time:38988ms step_avg:42.38ms
step:921/1775 train_time:39047ms step_avg:42.40ms
step:922/1775 train_time:39107ms step_avg:42.42ms
step:923/1775 train_time:39165ms step_avg:42.43ms
step:924/1775 train_time:39226ms step_avg:42.45ms
step:925/1775 train_time:39284ms step_avg:42.47ms
step:926/1775 train_time:39345ms step_avg:42.49ms
step:927/1775 train_time:39403ms step_avg:42.51ms
step:928/1775 train_time:39463ms step_avg:42.53ms
step:929/1775 train_time:39521ms step_avg:42.54ms
step:930/1775 train_time:39582ms step_avg:42.56ms
step:931/1775 train_time:39639ms step_avg:42.58ms
step:932/1775 train_time:39700ms step_avg:42.60ms
step:933/1775 train_time:39757ms step_avg:42.61ms
step:934/1775 train_time:39817ms step_avg:42.63ms
step:935/1775 train_time:39876ms step_avg:42.65ms
step:936/1775 train_time:39935ms step_avg:42.67ms
step:937/1775 train_time:39993ms step_avg:42.68ms
step:938/1775 train_time:40055ms step_avg:42.70ms
step:939/1775 train_time:40112ms step_avg:42.72ms
step:940/1775 train_time:40173ms step_avg:42.74ms
step:941/1775 train_time:40232ms step_avg:42.75ms
step:942/1775 train_time:40292ms step_avg:42.77ms
step:943/1775 train_time:40351ms step_avg:42.79ms
step:944/1775 train_time:40412ms step_avg:42.81ms
step:945/1775 train_time:40471ms step_avg:42.83ms
step:946/1775 train_time:40532ms step_avg:42.85ms
step:947/1775 train_time:40591ms step_avg:42.86ms
step:948/1775 train_time:40650ms step_avg:42.88ms
step:949/1775 train_time:40707ms step_avg:42.89ms
step:950/1775 train_time:40768ms step_avg:42.91ms
step:951/1775 train_time:40825ms step_avg:42.93ms
step:952/1775 train_time:40886ms step_avg:42.95ms
step:953/1775 train_time:40943ms step_avg:42.96ms
step:954/1775 train_time:41003ms step_avg:42.98ms
step:955/1775 train_time:41061ms step_avg:43.00ms
step:956/1775 train_time:41122ms step_avg:43.01ms
step:957/1775 train_time:41181ms step_avg:43.03ms
step:958/1775 train_time:41240ms step_avg:43.05ms
step:959/1775 train_time:41299ms step_avg:43.06ms
step:960/1775 train_time:41359ms step_avg:43.08ms
step:961/1775 train_time:41418ms step_avg:43.10ms
step:962/1775 train_time:41477ms step_avg:43.12ms
step:963/1775 train_time:41535ms step_avg:43.13ms
step:964/1775 train_time:41595ms step_avg:43.15ms
step:965/1775 train_time:41653ms step_avg:43.16ms
step:966/1775 train_time:41714ms step_avg:43.18ms
step:967/1775 train_time:41772ms step_avg:43.20ms
step:968/1775 train_time:41832ms step_avg:43.21ms
step:969/1775 train_time:41890ms step_avg:43.23ms
step:970/1775 train_time:41950ms step_avg:43.25ms
step:971/1775 train_time:42009ms step_avg:43.26ms
step:972/1775 train_time:42071ms step_avg:43.28ms
step:973/1775 train_time:42130ms step_avg:43.30ms
step:974/1775 train_time:42190ms step_avg:43.32ms
step:975/1775 train_time:42249ms step_avg:43.33ms
step:976/1775 train_time:42309ms step_avg:43.35ms
step:977/1775 train_time:42367ms step_avg:43.36ms
step:978/1775 train_time:42427ms step_avg:43.38ms
step:979/1775 train_time:42485ms step_avg:43.40ms
step:980/1775 train_time:42546ms step_avg:43.41ms
step:981/1775 train_time:42604ms step_avg:43.43ms
step:982/1775 train_time:42664ms step_avg:43.45ms
step:983/1775 train_time:42722ms step_avg:43.46ms
step:984/1775 train_time:42782ms step_avg:43.48ms
step:985/1775 train_time:42839ms step_avg:43.49ms
step:986/1775 train_time:42900ms step_avg:43.51ms
step:987/1775 train_time:42957ms step_avg:43.52ms
step:988/1775 train_time:43018ms step_avg:43.54ms
step:989/1775 train_time:43075ms step_avg:43.55ms
step:990/1775 train_time:43136ms step_avg:43.57ms
step:991/1775 train_time:43194ms step_avg:43.59ms
step:992/1775 train_time:43255ms step_avg:43.60ms
step:993/1775 train_time:43313ms step_avg:43.62ms
step:994/1775 train_time:43374ms step_avg:43.64ms
step:995/1775 train_time:43432ms step_avg:43.65ms
step:996/1775 train_time:43493ms step_avg:43.67ms
step:997/1775 train_time:43551ms step_avg:43.68ms
step:998/1775 train_time:43612ms step_avg:43.70ms
step:999/1775 train_time:43671ms step_avg:43.71ms
step:1000/1775 train_time:43731ms step_avg:43.73ms
step:1000/1775 val_loss:3.7268 train_time:43801ms step_avg:43.80ms
step:1001/1775 train_time:43825ms step_avg:43.78ms
step:1002/1775 train_time:43852ms step_avg:43.76ms
step:1003/1775 train_time:43913ms step_avg:43.78ms
step:1004/1775 train_time:43974ms step_avg:43.80ms
step:1005/1775 train_time:44038ms step_avg:43.82ms
step:1006/1775 train_time:44097ms step_avg:43.83ms
step:1007/1775 train_time:44154ms step_avg:43.85ms
step:1008/1775 train_time:44213ms step_avg:43.86ms
step:1009/1775 train_time:44270ms step_avg:43.87ms
step:1010/1775 train_time:44329ms step_avg:43.89ms
step:1011/1775 train_time:44387ms step_avg:43.90ms
step:1012/1775 train_time:44446ms step_avg:43.92ms
step:1013/1775 train_time:44504ms step_avg:43.93ms
step:1014/1775 train_time:44564ms step_avg:43.95ms
step:1015/1775 train_time:44621ms step_avg:43.96ms
step:1016/1775 train_time:44681ms step_avg:43.98ms
step:1017/1775 train_time:44738ms step_avg:43.99ms
step:1018/1775 train_time:44799ms step_avg:44.01ms
step:1019/1775 train_time:44859ms step_avg:44.02ms
step:1020/1775 train_time:44920ms step_avg:44.04ms
step:1021/1775 train_time:44980ms step_avg:44.05ms
step:1022/1775 train_time:45040ms step_avg:44.07ms
step:1023/1775 train_time:45097ms step_avg:44.08ms
step:1024/1775 train_time:45158ms step_avg:44.10ms
step:1025/1775 train_time:45216ms step_avg:44.11ms
step:1026/1775 train_time:45275ms step_avg:44.13ms
step:1027/1775 train_time:45333ms step_avg:44.14ms
step:1028/1775 train_time:45393ms step_avg:44.16ms
step:1029/1775 train_time:45450ms step_avg:44.17ms
step:1030/1775 train_time:45510ms step_avg:44.18ms
step:1031/1775 train_time:45567ms step_avg:44.20ms
step:1032/1775 train_time:45627ms step_avg:44.21ms
step:1033/1775 train_time:45685ms step_avg:44.23ms
step:1034/1775 train_time:45746ms step_avg:44.24ms
step:1035/1775 train_time:45805ms step_avg:44.26ms
step:1036/1775 train_time:45867ms step_avg:44.27ms
step:1037/1775 train_time:45925ms step_avg:44.29ms
step:1038/1775 train_time:45986ms step_avg:44.30ms
step:1039/1775 train_time:46046ms step_avg:44.32ms
step:1040/1775 train_time:46108ms step_avg:44.33ms
step:1041/1775 train_time:46167ms step_avg:44.35ms
step:1042/1775 train_time:46227ms step_avg:44.36ms
step:1043/1775 train_time:46285ms step_avg:44.38ms
step:1044/1775 train_time:46346ms step_avg:44.39ms
step:1045/1775 train_time:46402ms step_avg:44.40ms
step:1046/1775 train_time:46462ms step_avg:44.42ms
step:1047/1775 train_time:46519ms step_avg:44.43ms
step:1048/1775 train_time:46579ms step_avg:44.45ms
step:1049/1775 train_time:46637ms step_avg:44.46ms
step:1050/1775 train_time:46697ms step_avg:44.47ms
step:1051/1775 train_time:46755ms step_avg:44.49ms
step:1052/1775 train_time:46816ms step_avg:44.50ms
step:1053/1775 train_time:46874ms step_avg:44.52ms
step:1054/1775 train_time:46936ms step_avg:44.53ms
step:1055/1775 train_time:46995ms step_avg:44.54ms
step:1056/1775 train_time:47055ms step_avg:44.56ms
step:1057/1775 train_time:47114ms step_avg:44.57ms
step:1058/1775 train_time:47175ms step_avg:44.59ms
step:1059/1775 train_time:47233ms step_avg:44.60ms
step:1060/1775 train_time:47292ms step_avg:44.61ms
step:1061/1775 train_time:47350ms step_avg:44.63ms
step:1062/1775 train_time:47410ms step_avg:44.64ms
step:1063/1775 train_time:47469ms step_avg:44.66ms
step:1064/1775 train_time:47528ms step_avg:44.67ms
step:1065/1775 train_time:47586ms step_avg:44.68ms
step:1066/1775 train_time:47647ms step_avg:44.70ms
step:1067/1775 train_time:47705ms step_avg:44.71ms
step:1068/1775 train_time:47765ms step_avg:44.72ms
step:1069/1775 train_time:47824ms step_avg:44.74ms
step:1070/1775 train_time:47885ms step_avg:44.75ms
step:1071/1775 train_time:47943ms step_avg:44.76ms
step:1072/1775 train_time:48003ms step_avg:44.78ms
step:1073/1775 train_time:48062ms step_avg:44.79ms
step:1074/1775 train_time:48123ms step_avg:44.81ms
step:1075/1775 train_time:48181ms step_avg:44.82ms
step:1076/1775 train_time:48242ms step_avg:44.83ms
step:1077/1775 train_time:48299ms step_avg:44.85ms
step:1078/1775 train_time:48360ms step_avg:44.86ms
step:1079/1775 train_time:48418ms step_avg:44.87ms
step:1080/1775 train_time:48478ms step_avg:44.89ms
step:1081/1775 train_time:48535ms step_avg:44.90ms
step:1082/1775 train_time:48595ms step_avg:44.91ms
step:1083/1775 train_time:48652ms step_avg:44.92ms
step:1084/1775 train_time:48712ms step_avg:44.94ms
step:1085/1775 train_time:48770ms step_avg:44.95ms
step:1086/1775 train_time:48830ms step_avg:44.96ms
step:1087/1775 train_time:48889ms step_avg:44.98ms
step:1088/1775 train_time:48950ms step_avg:44.99ms
step:1089/1775 train_time:49008ms step_avg:45.00ms
step:1090/1775 train_time:49070ms step_avg:45.02ms
step:1091/1775 train_time:49128ms step_avg:45.03ms
step:1092/1775 train_time:49188ms step_avg:45.04ms
step:1093/1775 train_time:49246ms step_avg:45.06ms
step:1094/1775 train_time:49307ms step_avg:45.07ms
step:1095/1775 train_time:49365ms step_avg:45.08ms
step:1096/1775 train_time:49426ms step_avg:45.10ms
step:1097/1775 train_time:49484ms step_avg:45.11ms
step:1098/1775 train_time:49544ms step_avg:45.12ms
step:1099/1775 train_time:49602ms step_avg:45.13ms
step:1100/1775 train_time:49661ms step_avg:45.15ms
step:1101/1775 train_time:49719ms step_avg:45.16ms
step:1102/1775 train_time:49779ms step_avg:45.17ms
step:1103/1775 train_time:49837ms step_avg:45.18ms
step:1104/1775 train_time:49898ms step_avg:45.20ms
step:1105/1775 train_time:49956ms step_avg:45.21ms
step:1106/1775 train_time:50017ms step_avg:45.22ms
step:1107/1775 train_time:50075ms step_avg:45.24ms
step:1108/1775 train_time:50135ms step_avg:45.25ms
step:1109/1775 train_time:50194ms step_avg:45.26ms
step:1110/1775 train_time:50254ms step_avg:45.27ms
step:1111/1775 train_time:50311ms step_avg:45.28ms
step:1112/1775 train_time:50372ms step_avg:45.30ms
step:1113/1775 train_time:50430ms step_avg:45.31ms
step:1114/1775 train_time:50489ms step_avg:45.32ms
step:1115/1775 train_time:50547ms step_avg:45.33ms
step:1116/1775 train_time:50607ms step_avg:45.35ms
step:1117/1775 train_time:50666ms step_avg:45.36ms
step:1118/1775 train_time:50727ms step_avg:45.37ms
step:1119/1775 train_time:50785ms step_avg:45.38ms
step:1120/1775 train_time:50846ms step_avg:45.40ms
step:1121/1775 train_time:50905ms step_avg:45.41ms
step:1122/1775 train_time:50965ms step_avg:45.42ms
step:1123/1775 train_time:51025ms step_avg:45.44ms
step:1124/1775 train_time:51085ms step_avg:45.45ms
step:1125/1775 train_time:51142ms step_avg:45.46ms
step:1126/1775 train_time:51202ms step_avg:45.47ms
step:1127/1775 train_time:51260ms step_avg:45.48ms
step:1128/1775 train_time:51321ms step_avg:45.50ms
step:1129/1775 train_time:51378ms step_avg:45.51ms
step:1130/1775 train_time:51440ms step_avg:45.52ms
step:1131/1775 train_time:51498ms step_avg:45.53ms
step:1132/1775 train_time:51558ms step_avg:45.55ms
step:1133/1775 train_time:51615ms step_avg:45.56ms
step:1134/1775 train_time:51676ms step_avg:45.57ms
step:1135/1775 train_time:51733ms step_avg:45.58ms
step:1136/1775 train_time:51793ms step_avg:45.59ms
step:1137/1775 train_time:51851ms step_avg:45.60ms
step:1138/1775 train_time:51912ms step_avg:45.62ms
step:1139/1775 train_time:51970ms step_avg:45.63ms
step:1140/1775 train_time:52031ms step_avg:45.64ms
step:1141/1775 train_time:52088ms step_avg:45.65ms
step:1142/1775 train_time:52149ms step_avg:45.66ms
step:1143/1775 train_time:52207ms step_avg:45.68ms
step:1144/1775 train_time:52268ms step_avg:45.69ms
step:1145/1775 train_time:52328ms step_avg:45.70ms
step:1146/1775 train_time:52388ms step_avg:45.71ms
step:1147/1775 train_time:52446ms step_avg:45.72ms
step:1148/1775 train_time:52507ms step_avg:45.74ms
step:1149/1775 train_time:52565ms step_avg:45.75ms
step:1150/1775 train_time:52626ms step_avg:45.76ms
step:1151/1775 train_time:52684ms step_avg:45.77ms
step:1152/1775 train_time:52744ms step_avg:45.78ms
step:1153/1775 train_time:52801ms step_avg:45.79ms
step:1154/1775 train_time:52862ms step_avg:45.81ms
step:1155/1775 train_time:52920ms step_avg:45.82ms
step:1156/1775 train_time:52981ms step_avg:45.83ms
step:1157/1775 train_time:53039ms step_avg:45.84ms
step:1158/1775 train_time:53103ms step_avg:45.86ms
step:1159/1775 train_time:53186ms step_avg:45.89ms
step:1160/1775 train_time:53274ms step_avg:45.93ms
step:1161/1775 train_time:53359ms step_avg:45.96ms
step:1162/1775 train_time:53443ms step_avg:45.99ms
step:1163/1775 train_time:53527ms step_avg:46.03ms
step:1164/1775 train_time:53613ms step_avg:46.06ms
step:1165/1775 train_time:53697ms step_avg:46.09ms
step:1166/1775 train_time:53784ms step_avg:46.13ms
step:1167/1775 train_time:53866ms step_avg:46.16ms
step:1168/1775 train_time:53953ms step_avg:46.19ms
step:1169/1775 train_time:54038ms step_avg:46.23ms
step:1170/1775 train_time:54124ms step_avg:46.26ms
step:1171/1775 train_time:54207ms step_avg:46.29ms
step:1172/1775 train_time:54295ms step_avg:46.33ms
step:1173/1775 train_time:54378ms step_avg:46.36ms
step:1174/1775 train_time:54465ms step_avg:46.39ms
step:1175/1775 train_time:54547ms step_avg:46.42ms
step:1176/1775 train_time:54634ms step_avg:46.46ms
step:1177/1775 train_time:54717ms step_avg:46.49ms
step:1178/1775 train_time:54802ms step_avg:46.52ms
step:1179/1775 train_time:54885ms step_avg:46.55ms
step:1180/1775 train_time:54972ms step_avg:46.59ms
step:1181/1775 train_time:55056ms step_avg:46.62ms
step:1182/1775 train_time:55142ms step_avg:46.65ms
step:1183/1775 train_time:55226ms step_avg:46.68ms
step:1184/1775 train_time:55312ms step_avg:46.72ms
step:1185/1775 train_time:55396ms step_avg:46.75ms
step:1186/1775 train_time:55483ms step_avg:46.78ms
step:1187/1775 train_time:55566ms step_avg:46.81ms
step:1188/1775 train_time:55652ms step_avg:46.85ms
step:1189/1775 train_time:55735ms step_avg:46.88ms
step:1190/1775 train_time:55821ms step_avg:46.91ms
step:1191/1775 train_time:55905ms step_avg:46.94ms
step:1192/1775 train_time:55992ms step_avg:46.97ms
step:1193/1775 train_time:56076ms step_avg:47.00ms
step:1194/1775 train_time:56163ms step_avg:47.04ms
step:1195/1775 train_time:56246ms step_avg:47.07ms
step:1196/1775 train_time:56334ms step_avg:47.10ms
step:1197/1775 train_time:56416ms step_avg:47.13ms
step:1198/1775 train_time:56502ms step_avg:47.16ms
step:1199/1775 train_time:56585ms step_avg:47.19ms
step:1200/1775 train_time:56671ms step_avg:47.23ms
step:1201/1775 train_time:56755ms step_avg:47.26ms
step:1202/1775 train_time:56841ms step_avg:47.29ms
step:1203/1775 train_time:56925ms step_avg:47.32ms
step:1204/1775 train_time:57011ms step_avg:47.35ms
step:1205/1775 train_time:57095ms step_avg:47.38ms
step:1206/1775 train_time:57182ms step_avg:47.41ms
step:1207/1775 train_time:57265ms step_avg:47.44ms
step:1208/1775 train_time:57352ms step_avg:47.48ms
step:1209/1775 train_time:57435ms step_avg:47.51ms
step:1210/1775 train_time:57521ms step_avg:47.54ms
step:1211/1775 train_time:57604ms step_avg:47.57ms
step:1212/1775 train_time:57690ms step_avg:47.60ms
step:1213/1775 train_time:57774ms step_avg:47.63ms
step:1214/1775 train_time:57860ms step_avg:47.66ms
step:1215/1775 train_time:57943ms step_avg:47.69ms
step:1216/1775 train_time:58029ms step_avg:47.72ms
step:1217/1775 train_time:58114ms step_avg:47.75ms
step:1218/1775 train_time:58201ms step_avg:47.78ms
step:1219/1775 train_time:58284ms step_avg:47.81ms
step:1220/1775 train_time:58370ms step_avg:47.84ms
step:1221/1775 train_time:58454ms step_avg:47.87ms
step:1222/1775 train_time:58540ms step_avg:47.90ms
step:1223/1775 train_time:58623ms step_avg:47.93ms
step:1224/1775 train_time:58709ms step_avg:47.96ms
step:1225/1775 train_time:58793ms step_avg:47.99ms
step:1226/1775 train_time:58880ms step_avg:48.03ms
step:1227/1775 train_time:58964ms step_avg:48.06ms
step:1228/1775 train_time:59048ms step_avg:48.08ms
step:1229/1775 train_time:59132ms step_avg:48.11ms
step:1230/1775 train_time:59219ms step_avg:48.15ms
step:1231/1775 train_time:59302ms step_avg:48.17ms
step:1232/1775 train_time:59389ms step_avg:48.21ms
step:1233/1775 train_time:59473ms step_avg:48.23ms
step:1234/1775 train_time:59559ms step_avg:48.27ms
step:1235/1775 train_time:59642ms step_avg:48.29ms
step:1236/1775 train_time:59727ms step_avg:48.32ms
step:1237/1775 train_time:59811ms step_avg:48.35ms
step:1238/1775 train_time:59899ms step_avg:48.38ms
step:1239/1775 train_time:59982ms step_avg:48.41ms
step:1240/1775 train_time:60068ms step_avg:48.44ms
step:1241/1775 train_time:60153ms step_avg:48.47ms
step:1242/1775 train_time:60239ms step_avg:48.50ms
step:1243/1775 train_time:60323ms step_avg:48.53ms
step:1244/1775 train_time:60408ms step_avg:48.56ms
step:1245/1775 train_time:60492ms step_avg:48.59ms
step:1246/1775 train_time:60580ms step_avg:48.62ms
step:1247/1775 train_time:60664ms step_avg:48.65ms
step:1248/1775 train_time:60749ms step_avg:48.68ms
step:1249/1775 train_time:60833ms step_avg:48.71ms
step:1250/1775 train_time:60919ms step_avg:48.74ms
step:1250/1775 val_loss:3.5037 train_time:61017ms step_avg:48.81ms
step:1251/1775 train_time:61039ms step_avg:48.79ms
step:1252/1775 train_time:61092ms step_avg:48.80ms
step:1253/1775 train_time:61180ms step_avg:48.83ms
step:1254/1775 train_time:61267ms step_avg:48.86ms
step:1255/1775 train_time:61351ms step_avg:48.88ms
step:1256/1775 train_time:61435ms step_avg:48.91ms
step:1257/1775 train_time:61520ms step_avg:48.94ms
step:1258/1775 train_time:61606ms step_avg:48.97ms
step:1259/1775 train_time:61688ms step_avg:49.00ms
step:1260/1775 train_time:61773ms step_avg:49.03ms
step:1261/1775 train_time:61855ms step_avg:49.05ms
step:1262/1775 train_time:61941ms step_avg:49.08ms
step:1263/1775 train_time:62028ms step_avg:49.11ms
step:1264/1775 train_time:62116ms step_avg:49.14ms
step:1265/1775 train_time:62200ms step_avg:49.17ms
step:1266/1775 train_time:62288ms step_avg:49.20ms
step:1267/1775 train_time:62371ms step_avg:49.23ms
step:1268/1775 train_time:62457ms step_avg:49.26ms
step:1269/1775 train_time:62540ms step_avg:49.28ms
step:1270/1775 train_time:62626ms step_avg:49.31ms
step:1271/1775 train_time:62709ms step_avg:49.34ms
step:1272/1775 train_time:62794ms step_avg:49.37ms
step:1273/1775 train_time:62877ms step_avg:49.39ms
step:1274/1775 train_time:62964ms step_avg:49.42ms
step:1275/1775 train_time:63049ms step_avg:49.45ms
step:1276/1775 train_time:63136ms step_avg:49.48ms
step:1277/1775 train_time:63222ms step_avg:49.51ms
step:1278/1775 train_time:63307ms step_avg:49.54ms
step:1279/1775 train_time:63391ms step_avg:49.56ms
step:1280/1775 train_time:63476ms step_avg:49.59ms
step:1281/1775 train_time:63559ms step_avg:49.62ms
step:1282/1775 train_time:63645ms step_avg:49.65ms
step:1283/1775 train_time:63728ms step_avg:49.67ms
step:1284/1775 train_time:63813ms step_avg:49.70ms
step:1285/1775 train_time:63896ms step_avg:49.72ms
step:1286/1775 train_time:63984ms step_avg:49.75ms
step:1287/1775 train_time:64067ms step_avg:49.78ms
step:1288/1775 train_time:64155ms step_avg:49.81ms
step:1289/1775 train_time:64240ms step_avg:49.84ms
step:1290/1775 train_time:64327ms step_avg:49.87ms
step:1291/1775 train_time:64411ms step_avg:49.89ms
step:1292/1775 train_time:64496ms step_avg:49.92ms
step:1293/1775 train_time:64580ms step_avg:49.95ms
step:1294/1775 train_time:64665ms step_avg:49.97ms
step:1295/1775 train_time:64748ms step_avg:50.00ms
step:1296/1775 train_time:64833ms step_avg:50.03ms
step:1297/1775 train_time:64917ms step_avg:50.05ms
step:1298/1775 train_time:65004ms step_avg:50.08ms
step:1299/1775 train_time:65089ms step_avg:50.11ms
step:1300/1775 train_time:65175ms step_avg:50.13ms
step:1301/1775 train_time:65259ms step_avg:50.16ms
step:1302/1775 train_time:65346ms step_avg:50.19ms
step:1303/1775 train_time:65430ms step_avg:50.21ms
step:1304/1775 train_time:65516ms step_avg:50.24ms
step:1305/1775 train_time:65600ms step_avg:50.27ms
step:1306/1775 train_time:65686ms step_avg:50.30ms
step:1307/1775 train_time:65768ms step_avg:50.32ms
step:1308/1775 train_time:65854ms step_avg:50.35ms
step:1309/1775 train_time:65937ms step_avg:50.37ms
step:1310/1775 train_time:66024ms step_avg:50.40ms
step:1311/1775 train_time:66109ms step_avg:50.43ms
step:1312/1775 train_time:66197ms step_avg:50.45ms
step:1313/1775 train_time:66281ms step_avg:50.48ms
step:1314/1775 train_time:66367ms step_avg:50.51ms
step:1315/1775 train_time:66450ms step_avg:50.53ms
step:1316/1775 train_time:66537ms step_avg:50.56ms
step:1317/1775 train_time:66621ms step_avg:50.59ms
step:1318/1775 train_time:66708ms step_avg:50.61ms
step:1319/1775 train_time:66790ms step_avg:50.64ms
step:1320/1775 train_time:66876ms step_avg:50.66ms
step:1321/1775 train_time:66960ms step_avg:50.69ms
step:1322/1775 train_time:67047ms step_avg:50.72ms
step:1323/1775 train_time:67131ms step_avg:50.74ms
step:1324/1775 train_time:67217ms step_avg:50.77ms
step:1325/1775 train_time:67301ms step_avg:50.79ms
step:1326/1775 train_time:67389ms step_avg:50.82ms
step:1327/1775 train_time:67472ms step_avg:50.85ms
step:1328/1775 train_time:67557ms step_avg:50.87ms
step:1329/1775 train_time:67640ms step_avg:50.90ms
step:1330/1775 train_time:67727ms step_avg:50.92ms
step:1331/1775 train_time:67810ms step_avg:50.95ms
step:1332/1775 train_time:67896ms step_avg:50.97ms
step:1333/1775 train_time:67979ms step_avg:51.00ms
step:1334/1775 train_time:68065ms step_avg:51.02ms
step:1335/1775 train_time:68149ms step_avg:51.05ms
step:1336/1775 train_time:68236ms step_avg:51.07ms
step:1337/1775 train_time:68320ms step_avg:51.10ms
step:1338/1775 train_time:68407ms step_avg:51.13ms
step:1339/1775 train_time:68490ms step_avg:51.15ms
step:1340/1775 train_time:68576ms step_avg:51.18ms
step:1341/1775 train_time:68661ms step_avg:51.20ms
step:1342/1775 train_time:68746ms step_avg:51.23ms
step:1343/1775 train_time:68829ms step_avg:51.25ms
step:1344/1775 train_time:68915ms step_avg:51.28ms
step:1345/1775 train_time:69000ms step_avg:51.30ms
step:1346/1775 train_time:69087ms step_avg:51.33ms
step:1347/1775 train_time:69170ms step_avg:51.35ms
step:1348/1775 train_time:69257ms step_avg:51.38ms
step:1349/1775 train_time:69340ms step_avg:51.40ms
step:1350/1775 train_time:69426ms step_avg:51.43ms
step:1351/1775 train_time:69510ms step_avg:51.45ms
step:1352/1775 train_time:69596ms step_avg:51.48ms
step:1353/1775 train_time:69679ms step_avg:51.50ms
step:1354/1775 train_time:69765ms step_avg:51.53ms
step:1355/1775 train_time:69849ms step_avg:51.55ms
step:1356/1775 train_time:69935ms step_avg:51.57ms
step:1357/1775 train_time:70018ms step_avg:51.60ms
step:1358/1775 train_time:70104ms step_avg:51.62ms
step:1359/1775 train_time:70189ms step_avg:51.65ms
step:1360/1775 train_time:70275ms step_avg:51.67ms
step:1361/1775 train_time:70358ms step_avg:51.70ms
step:1362/1775 train_time:70444ms step_avg:51.72ms
step:1363/1775 train_time:70528ms step_avg:51.74ms
step:1364/1775 train_time:70613ms step_avg:51.77ms
step:1365/1775 train_time:70696ms step_avg:51.79ms
step:1366/1775 train_time:70783ms step_avg:51.82ms
step:1367/1775 train_time:70867ms step_avg:51.84ms
step:1368/1775 train_time:70953ms step_avg:51.87ms
step:1369/1775 train_time:71037ms step_avg:51.89ms
step:1370/1775 train_time:71124ms step_avg:51.92ms
step:1371/1775 train_time:71208ms step_avg:51.94ms
step:1372/1775 train_time:71294ms step_avg:51.96ms
step:1373/1775 train_time:71378ms step_avg:51.99ms
step:1374/1775 train_time:71465ms step_avg:52.01ms
step:1375/1775 train_time:71549ms step_avg:52.04ms
step:1376/1775 train_time:71634ms step_avg:52.06ms
step:1377/1775 train_time:71717ms step_avg:52.08ms
step:1378/1775 train_time:71805ms step_avg:52.11ms
step:1379/1775 train_time:71888ms step_avg:52.13ms
step:1380/1775 train_time:71974ms step_avg:52.15ms
step:1381/1775 train_time:72058ms step_avg:52.18ms
step:1382/1775 train_time:72144ms step_avg:52.20ms
step:1383/1775 train_time:72228ms step_avg:52.23ms
step:1384/1775 train_time:72313ms step_avg:52.25ms
step:1385/1775 train_time:72398ms step_avg:52.27ms
step:1386/1775 train_time:72485ms step_avg:52.30ms
step:1387/1775 train_time:72567ms step_avg:52.32ms
step:1388/1775 train_time:72653ms step_avg:52.34ms
step:1389/1775 train_time:72737ms step_avg:52.37ms
step:1390/1775 train_time:72825ms step_avg:52.39ms
step:1391/1775 train_time:72908ms step_avg:52.41ms
step:1392/1775 train_time:72993ms step_avg:52.44ms
step:1393/1775 train_time:73076ms step_avg:52.46ms
step:1394/1775 train_time:73163ms step_avg:52.48ms
step:1395/1775 train_time:73248ms step_avg:52.51ms
step:1396/1775 train_time:73333ms step_avg:52.53ms
step:1397/1775 train_time:73417ms step_avg:52.55ms
step:1398/1775 train_time:73503ms step_avg:52.58ms
step:1399/1775 train_time:73587ms step_avg:52.60ms
step:1400/1775 train_time:73672ms step_avg:52.62ms
step:1401/1775 train_time:73756ms step_avg:52.65ms
step:1402/1775 train_time:73843ms step_avg:52.67ms
step:1403/1775 train_time:73927ms step_avg:52.69ms
step:1404/1775 train_time:74013ms step_avg:52.72ms
step:1405/1775 train_time:74096ms step_avg:52.74ms
step:1406/1775 train_time:74184ms step_avg:52.76ms
step:1407/1775 train_time:74267ms step_avg:52.78ms
step:1408/1775 train_time:74353ms step_avg:52.81ms
step:1409/1775 train_time:74437ms step_avg:52.83ms
step:1410/1775 train_time:74523ms step_avg:52.85ms
step:1411/1775 train_time:74608ms step_avg:52.88ms
step:1412/1775 train_time:74693ms step_avg:52.90ms
step:1413/1775 train_time:74776ms step_avg:52.92ms
step:1414/1775 train_time:74862ms step_avg:52.94ms
step:1415/1775 train_time:74947ms step_avg:52.97ms
step:1416/1775 train_time:75033ms step_avg:52.99ms
step:1417/1775 train_time:75116ms step_avg:53.01ms
step:1418/1775 train_time:75204ms step_avg:53.04ms
step:1419/1775 train_time:75288ms step_avg:53.06ms
step:1420/1775 train_time:75374ms step_avg:53.08ms
step:1421/1775 train_time:75458ms step_avg:53.10ms
step:1422/1775 train_time:75544ms step_avg:53.13ms
step:1423/1775 train_time:75628ms step_avg:53.15ms
step:1424/1775 train_time:75713ms step_avg:53.17ms
step:1425/1775 train_time:75797ms step_avg:53.19ms
step:1426/1775 train_time:75883ms step_avg:53.21ms
step:1427/1775 train_time:75967ms step_avg:53.24ms
step:1428/1775 train_time:76052ms step_avg:53.26ms
step:1429/1775 train_time:76135ms step_avg:53.28ms
step:1430/1775 train_time:76224ms step_avg:53.30ms
step:1431/1775 train_time:76307ms step_avg:53.32ms
step:1432/1775 train_time:76394ms step_avg:53.35ms
step:1433/1775 train_time:76478ms step_avg:53.37ms
step:1434/1775 train_time:76564ms step_avg:53.39ms
step:1435/1775 train_time:76648ms step_avg:53.41ms
step:1436/1775 train_time:76733ms step_avg:53.44ms
step:1437/1775 train_time:76818ms step_avg:53.46ms
step:1438/1775 train_time:76906ms step_avg:53.48ms
step:1439/1775 train_time:76990ms step_avg:53.50ms
step:1440/1775 train_time:77075ms step_avg:53.52ms
step:1441/1775 train_time:77158ms step_avg:53.54ms
step:1442/1775 train_time:77246ms step_avg:53.57ms
step:1443/1775 train_time:77329ms step_avg:53.59ms
step:1444/1775 train_time:77415ms step_avg:53.61ms
step:1445/1775 train_time:77498ms step_avg:53.63ms
step:1446/1775 train_time:77585ms step_avg:53.65ms
step:1447/1775 train_time:77668ms step_avg:53.67ms
step:1448/1775 train_time:77754ms step_avg:53.70ms
step:1449/1775 train_time:77838ms step_avg:53.72ms
step:1450/1775 train_time:77924ms step_avg:53.74ms
step:1451/1775 train_time:78008ms step_avg:53.76ms
step:1452/1775 train_time:78094ms step_avg:53.78ms
step:1453/1775 train_time:78178ms step_avg:53.80ms
step:1454/1775 train_time:78265ms step_avg:53.83ms
step:1455/1775 train_time:78348ms step_avg:53.85ms
step:1456/1775 train_time:78434ms step_avg:53.87ms
step:1457/1775 train_time:78519ms step_avg:53.89ms
step:1458/1775 train_time:78605ms step_avg:53.91ms
step:1459/1775 train_time:78689ms step_avg:53.93ms
step:1460/1775 train_time:78774ms step_avg:53.95ms
step:1461/1775 train_time:78857ms step_avg:53.97ms
step:1462/1775 train_time:78944ms step_avg:54.00ms
step:1463/1775 train_time:79029ms step_avg:54.02ms
step:1464/1775 train_time:79114ms step_avg:54.04ms
step:1465/1775 train_time:79198ms step_avg:54.06ms
step:1466/1775 train_time:79286ms step_avg:54.08ms
step:1467/1775 train_time:79369ms step_avg:54.10ms
step:1468/1775 train_time:79455ms step_avg:54.12ms
step:1469/1775 train_time:79539ms step_avg:54.15ms
step:1470/1775 train_time:79626ms step_avg:54.17ms
step:1471/1775 train_time:79710ms step_avg:54.19ms
step:1472/1775 train_time:79797ms step_avg:54.21ms
step:1473/1775 train_time:79881ms step_avg:54.23ms
step:1474/1775 train_time:79968ms step_avg:54.25ms
step:1475/1775 train_time:80052ms step_avg:54.27ms
step:1476/1775 train_time:80137ms step_avg:54.29ms
step:1477/1775 train_time:80222ms step_avg:54.31ms
step:1478/1775 train_time:80309ms step_avg:54.34ms
step:1479/1775 train_time:80392ms step_avg:54.36ms
step:1480/1775 train_time:80477ms step_avg:54.38ms
step:1481/1775 train_time:80560ms step_avg:54.40ms
step:1482/1775 train_time:80647ms step_avg:54.42ms
step:1483/1775 train_time:80730ms step_avg:54.44ms
step:1484/1775 train_time:80816ms step_avg:54.46ms
step:1485/1775 train_time:80901ms step_avg:54.48ms
step:1486/1775 train_time:80987ms step_avg:54.50ms
step:1487/1775 train_time:81070ms step_avg:54.52ms
step:1488/1775 train_time:81156ms step_avg:54.54ms
step:1489/1775 train_time:81239ms step_avg:54.56ms
step:1490/1775 train_time:81328ms step_avg:54.58ms
step:1491/1775 train_time:81411ms step_avg:54.60ms
step:1492/1775 train_time:81497ms step_avg:54.62ms
step:1493/1775 train_time:81581ms step_avg:54.64ms
step:1494/1775 train_time:81667ms step_avg:54.66ms
step:1495/1775 train_time:81750ms step_avg:54.68ms
step:1496/1775 train_time:81837ms step_avg:54.70ms
step:1497/1775 train_time:81921ms step_avg:54.72ms
step:1498/1775 train_time:82008ms step_avg:54.74ms
step:1499/1775 train_time:82092ms step_avg:54.76ms
step:1500/1775 train_time:82178ms step_avg:54.79ms
step:1500/1775 val_loss:3.3754 train_time:82275ms step_avg:54.85ms
step:1501/1775 train_time:82296ms step_avg:54.83ms
step:1502/1775 train_time:82350ms step_avg:54.83ms
step:1503/1775 train_time:82434ms step_avg:54.85ms
step:1504/1775 train_time:82522ms step_avg:54.87ms
step:1505/1775 train_time:82606ms step_avg:54.89ms
step:1506/1775 train_time:82692ms step_avg:54.91ms
step:1507/1775 train_time:82774ms step_avg:54.93ms
step:1508/1775 train_time:82860ms step_avg:54.95ms
step:1509/1775 train_time:82945ms step_avg:54.97ms
step:1510/1775 train_time:83031ms step_avg:54.99ms
step:1511/1775 train_time:83114ms step_avg:55.01ms
step:1512/1775 train_time:83199ms step_avg:55.03ms
step:1513/1775 train_time:83284ms step_avg:55.05ms
step:1514/1775 train_time:83373ms step_avg:55.07ms
step:1515/1775 train_time:83458ms step_avg:55.09ms
step:1516/1775 train_time:83544ms step_avg:55.11ms
step:1517/1775 train_time:83628ms step_avg:55.13ms
step:1518/1775 train_time:83714ms step_avg:55.15ms
step:1519/1775 train_time:83797ms step_avg:55.17ms
step:1520/1775 train_time:83883ms step_avg:55.19ms
step:1521/1775 train_time:83964ms step_avg:55.20ms
step:1522/1775 train_time:84051ms step_avg:55.22ms
step:1523/1775 train_time:84134ms step_avg:55.24ms
step:1524/1775 train_time:84220ms step_avg:55.26ms
step:1525/1775 train_time:84306ms step_avg:55.28ms
step:1526/1775 train_time:84393ms step_avg:55.30ms
step:1527/1775 train_time:84477ms step_avg:55.32ms
step:1528/1775 train_time:84562ms step_avg:55.34ms
step:1529/1775 train_time:84647ms step_avg:55.36ms
step:1530/1775 train_time:84733ms step_avg:55.38ms
step:1531/1775 train_time:84816ms step_avg:55.40ms
step:1532/1775 train_time:84902ms step_avg:55.42ms
step:1533/1775 train_time:84985ms step_avg:55.44ms
step:1534/1775 train_time:85071ms step_avg:55.46ms
step:1535/1775 train_time:85154ms step_avg:55.48ms
step:1536/1775 train_time:85241ms step_avg:55.50ms
step:1537/1775 train_time:85325ms step_avg:55.51ms
step:1538/1775 train_time:85412ms step_avg:55.53ms
step:1539/1775 train_time:85496ms step_avg:55.55ms
step:1540/1775 train_time:85583ms step_avg:55.57ms
step:1541/1775 train_time:85667ms step_avg:55.59ms
step:1542/1775 train_time:85753ms step_avg:55.61ms
step:1543/1775 train_time:85836ms step_avg:55.63ms
step:1544/1775 train_time:85922ms step_avg:55.65ms
step:1545/1775 train_time:86005ms step_avg:55.67ms
step:1546/1775 train_time:86090ms step_avg:55.69ms
step:1547/1775 train_time:86174ms step_avg:55.70ms
step:1548/1775 train_time:86261ms step_avg:55.72ms
step:1549/1775 train_time:86346ms step_avg:55.74ms
step:1550/1775 train_time:86432ms step_avg:55.76ms
step:1551/1775 train_time:86517ms step_avg:55.78ms
step:1552/1775 train_time:86603ms step_avg:55.80ms
step:1553/1775 train_time:86687ms step_avg:55.82ms
step:1554/1775 train_time:86773ms step_avg:55.84ms
step:1555/1775 train_time:86856ms step_avg:55.86ms
step:1556/1775 train_time:86943ms step_avg:55.88ms
step:1557/1775 train_time:87026ms step_avg:55.89ms
step:1558/1775 train_time:87112ms step_avg:55.91ms
step:1559/1775 train_time:87195ms step_avg:55.93ms
step:1560/1775 train_time:87282ms step_avg:55.95ms
step:1561/1775 train_time:87366ms step_avg:55.97ms
step:1562/1775 train_time:87453ms step_avg:55.99ms
step:1563/1775 train_time:87536ms step_avg:56.01ms
step:1564/1775 train_time:87623ms step_avg:56.02ms
step:1565/1775 train_time:87706ms step_avg:56.04ms
step:1566/1775 train_time:87792ms step_avg:56.06ms
step:1567/1775 train_time:87875ms step_avg:56.08ms
step:1568/1775 train_time:87961ms step_avg:56.10ms
step:1569/1775 train_time:88044ms step_avg:56.11ms
step:1570/1775 train_time:88132ms step_avg:56.13ms
step:1571/1775 train_time:88214ms step_avg:56.15ms
step:1572/1775 train_time:88301ms step_avg:56.17ms
step:1573/1775 train_time:88385ms step_avg:56.19ms
step:1574/1775 train_time:88472ms step_avg:56.21ms
step:1575/1775 train_time:88555ms step_avg:56.23ms
step:1576/1775 train_time:88642ms step_avg:56.24ms
step:1577/1775 train_time:88725ms step_avg:56.26ms
step:1578/1775 train_time:88811ms step_avg:56.28ms
step:1579/1775 train_time:88894ms step_avg:56.30ms
step:1580/1775 train_time:88981ms step_avg:56.32ms
step:1581/1775 train_time:89065ms step_avg:56.33ms
step:1582/1775 train_time:89152ms step_avg:56.35ms
step:1583/1775 train_time:89235ms step_avg:56.37ms
step:1584/1775 train_time:89321ms step_avg:56.39ms
step:1585/1775 train_time:89405ms step_avg:56.41ms
step:1586/1775 train_time:89492ms step_avg:56.43ms
step:1587/1775 train_time:89575ms step_avg:56.44ms
step:1588/1775 train_time:89662ms step_avg:56.46ms
step:1589/1775 train_time:89746ms step_avg:56.48ms
step:1590/1775 train_time:89832ms step_avg:56.50ms
step:1591/1775 train_time:89915ms step_avg:56.51ms
step:1592/1775 train_time:90001ms step_avg:56.53ms
step:1593/1775 train_time:90085ms step_avg:56.55ms
step:1594/1775 train_time:90171ms step_avg:56.57ms
step:1595/1775 train_time:90255ms step_avg:56.59ms
step:1596/1775 train_time:90342ms step_avg:56.61ms
step:1597/1775 train_time:90427ms step_avg:56.62ms
step:1598/1775 train_time:90513ms step_avg:56.64ms
step:1599/1775 train_time:90596ms step_avg:56.66ms
step:1600/1775 train_time:90683ms step_avg:56.68ms
step:1601/1775 train_time:90766ms step_avg:56.69ms
step:1602/1775 train_time:90852ms step_avg:56.71ms
step:1603/1775 train_time:90935ms step_avg:56.73ms
step:1604/1775 train_time:91021ms step_avg:56.75ms
step:1605/1775 train_time:91105ms step_avg:56.76ms
step:1606/1775 train_time:91191ms step_avg:56.78ms
step:1607/1775 train_time:91275ms step_avg:56.80ms
step:1608/1775 train_time:91362ms step_avg:56.82ms
step:1609/1775 train_time:91447ms step_avg:56.83ms
step:1610/1775 train_time:91533ms step_avg:56.85ms
step:1611/1775 train_time:91615ms step_avg:56.87ms
step:1612/1775 train_time:91702ms step_avg:56.89ms
step:1613/1775 train_time:91786ms step_avg:56.90ms
step:1614/1775 train_time:91872ms step_avg:56.92ms
step:1615/1775 train_time:91955ms step_avg:56.94ms
step:1616/1775 train_time:92041ms step_avg:56.96ms
step:1617/1775 train_time:92125ms step_avg:56.97ms
step:1618/1775 train_time:92213ms step_avg:56.99ms
step:1619/1775 train_time:92295ms step_avg:57.01ms
step:1620/1775 train_time:92382ms step_avg:57.03ms
step:1621/1775 train_time:92467ms step_avg:57.04ms
step:1622/1775 train_time:92552ms step_avg:57.06ms
step:1623/1775 train_time:92635ms step_avg:57.08ms
step:1624/1775 train_time:92722ms step_avg:57.09ms
step:1625/1775 train_time:92806ms step_avg:57.11ms
step:1626/1775 train_time:92891ms step_avg:57.13ms
step:1627/1775 train_time:92974ms step_avg:57.14ms
step:1628/1775 train_time:93061ms step_avg:57.16ms
step:1629/1775 train_time:93145ms step_avg:57.18ms
step:1630/1775 train_time:93232ms step_avg:57.20ms
step:1631/1775 train_time:93316ms step_avg:57.21ms
step:1632/1775 train_time:93404ms step_avg:57.23ms
step:1633/1775 train_time:93488ms step_avg:57.25ms
step:1634/1775 train_time:93573ms step_avg:57.27ms
step:1635/1775 train_time:93656ms step_avg:57.28ms
step:1636/1775 train_time:93742ms step_avg:57.30ms
step:1637/1775 train_time:93826ms step_avg:57.32ms
step:1638/1775 train_time:93913ms step_avg:57.33ms
step:1639/1775 train_time:93996ms step_avg:57.35ms
step:1640/1775 train_time:94082ms step_avg:57.37ms
step:1641/1775 train_time:94166ms step_avg:57.38ms
step:1642/1775 train_time:94252ms step_avg:57.40ms
step:1643/1775 train_time:94335ms step_avg:57.42ms
step:1644/1775 train_time:94422ms step_avg:57.43ms
step:1645/1775 train_time:94506ms step_avg:57.45ms
step:1646/1775 train_time:94592ms step_avg:57.47ms
step:1647/1775 train_time:94675ms step_avg:57.48ms
step:1648/1775 train_time:94762ms step_avg:57.50ms
step:1649/1775 train_time:94846ms step_avg:57.52ms
step:1650/1775 train_time:94933ms step_avg:57.53ms
step:1651/1775 train_time:95017ms step_avg:57.55ms
step:1652/1775 train_time:95103ms step_avg:57.57ms
step:1653/1775 train_time:95186ms step_avg:57.58ms
step:1654/1775 train_time:95273ms step_avg:57.60ms
step:1655/1775 train_time:95356ms step_avg:57.62ms
step:1656/1775 train_time:95442ms step_avg:57.63ms
step:1657/1775 train_time:95526ms step_avg:57.65ms
step:1658/1775 train_time:95612ms step_avg:57.67ms
step:1659/1775 train_time:95695ms step_avg:57.68ms
step:1660/1775 train_time:95782ms step_avg:57.70ms
step:1661/1775 train_time:95864ms step_avg:57.71ms
step:1662/1775 train_time:95951ms step_avg:57.73ms
step:1663/1775 train_time:96035ms step_avg:57.75ms
step:1664/1775 train_time:96121ms step_avg:57.76ms
step:1665/1775 train_time:96204ms step_avg:57.78ms
step:1666/1775 train_time:96291ms step_avg:57.80ms
step:1667/1775 train_time:96374ms step_avg:57.81ms
step:1668/1775 train_time:96461ms step_avg:57.83ms
step:1669/1775 train_time:96545ms step_avg:57.85ms
step:1670/1775 train_time:96632ms step_avg:57.86ms
step:1671/1775 train_time:96716ms step_avg:57.88ms
step:1672/1775 train_time:96800ms step_avg:57.89ms
step:1673/1775 train_time:96883ms step_avg:57.91ms
step:1674/1775 train_time:96971ms step_avg:57.93ms
step:1675/1775 train_time:97054ms step_avg:57.94ms
step:1676/1775 train_time:97141ms step_avg:57.96ms
step:1677/1775 train_time:97225ms step_avg:57.98ms
step:1678/1775 train_time:97312ms step_avg:57.99ms
step:1679/1775 train_time:97395ms step_avg:58.01ms
step:1680/1775 train_time:97481ms step_avg:58.02ms
step:1681/1775 train_time:97564ms step_avg:58.04ms
step:1682/1775 train_time:97651ms step_avg:58.06ms
step:1683/1775 train_time:97734ms step_avg:58.07ms
step:1684/1775 train_time:97820ms step_avg:58.09ms
step:1685/1775 train_time:97903ms step_avg:58.10ms
step:1686/1775 train_time:97990ms step_avg:58.12ms
step:1687/1775 train_time:98073ms step_avg:58.13ms
step:1688/1775 train_time:98159ms step_avg:58.15ms
step:1689/1775 train_time:98244ms step_avg:58.17ms
step:1690/1775 train_time:98331ms step_avg:58.18ms
step:1691/1775 train_time:98413ms step_avg:58.20ms
step:1692/1775 train_time:98499ms step_avg:58.21ms
step:1693/1775 train_time:98583ms step_avg:58.23ms
step:1694/1775 train_time:98670ms step_avg:58.25ms
step:1695/1775 train_time:98754ms step_avg:58.26ms
step:1696/1775 train_time:98839ms step_avg:58.28ms
step:1697/1775 train_time:98922ms step_avg:58.29ms
step:1698/1775 train_time:99009ms step_avg:58.31ms
step:1699/1775 train_time:99092ms step_avg:58.32ms
step:1700/1775 train_time:99178ms step_avg:58.34ms
step:1701/1775 train_time:99261ms step_avg:58.35ms
step:1702/1775 train_time:99349ms step_avg:58.37ms
step:1703/1775 train_time:99432ms step_avg:58.39ms
step:1704/1775 train_time:99519ms step_avg:58.40ms
step:1705/1775 train_time:99603ms step_avg:58.42ms
step:1706/1775 train_time:99690ms step_avg:58.43ms
step:1707/1775 train_time:99772ms step_avg:58.45ms
step:1708/1775 train_time:99858ms step_avg:58.47ms
step:1709/1775 train_time:99941ms step_avg:58.48ms
step:1710/1775 train_time:100029ms step_avg:58.50ms
step:1711/1775 train_time:100112ms step_avg:58.51ms
step:1712/1775 train_time:100197ms step_avg:58.53ms
step:1713/1775 train_time:100282ms step_avg:58.54ms
step:1714/1775 train_time:100368ms step_avg:58.56ms
step:1715/1775 train_time:100452ms step_avg:58.57ms
step:1716/1775 train_time:100537ms step_avg:58.59ms
step:1717/1775 train_time:100621ms step_avg:58.60ms
step:1718/1775 train_time:100708ms step_avg:58.62ms
step:1719/1775 train_time:100791ms step_avg:58.63ms
step:1720/1775 train_time:100877ms step_avg:58.65ms
step:1721/1775 train_time:100961ms step_avg:58.66ms
step:1722/1775 train_time:101050ms step_avg:58.68ms
step:1723/1775 train_time:101133ms step_avg:58.70ms
step:1724/1775 train_time:101218ms step_avg:58.71ms
step:1725/1775 train_time:101302ms step_avg:58.73ms
step:1726/1775 train_time:101389ms step_avg:58.74ms
step:1727/1775 train_time:101472ms step_avg:58.76ms
step:1728/1775 train_time:101557ms step_avg:58.77ms
step:1729/1775 train_time:101642ms step_avg:58.79ms
step:1730/1775 train_time:101729ms step_avg:58.80ms
step:1731/1775 train_time:101814ms step_avg:58.82ms
step:1732/1775 train_time:101898ms step_avg:58.83ms
step:1733/1775 train_time:101982ms step_avg:58.85ms
step:1734/1775 train_time:102069ms step_avg:58.86ms
step:1735/1775 train_time:102152ms step_avg:58.88ms
step:1736/1775 train_time:102242ms step_avg:58.89ms
step:1737/1775 train_time:102326ms step_avg:58.91ms
step:1738/1775 train_time:102414ms step_avg:58.93ms
step:1739/1775 train_time:102498ms step_avg:58.94ms
step:1740/1775 train_time:102583ms step_avg:58.96ms
step:1741/1775 train_time:102668ms step_avg:58.97ms
step:1742/1775 train_time:102755ms step_avg:58.99ms
step:1743/1775 train_time:102838ms step_avg:59.00ms
step:1744/1775 train_time:102925ms step_avg:59.02ms
step:1745/1775 train_time:103010ms step_avg:59.03ms
step:1746/1775 train_time:103096ms step_avg:59.05ms
step:1747/1775 train_time:103181ms step_avg:59.06ms
step:1748/1775 train_time:103268ms step_avg:59.08ms
step:1749/1775 train_time:103352ms step_avg:59.09ms
step:1750/1775 train_time:103438ms step_avg:59.11ms
step:1750/1775 val_loss:3.2840 train_time:103536ms step_avg:59.16ms
step:1751/1775 train_time:103558ms step_avg:59.14ms
step:1752/1775 train_time:103612ms step_avg:59.14ms
step:1753/1775 train_time:103699ms step_avg:59.16ms
step:1754/1775 train_time:103785ms step_avg:59.17ms
step:1755/1775 train_time:103869ms step_avg:59.18ms
step:1756/1775 train_time:103954ms step_avg:59.20ms
step:1757/1775 train_time:104037ms step_avg:59.21ms
step:1758/1775 train_time:104123ms step_avg:59.23ms
step:1759/1775 train_time:104206ms step_avg:59.24ms
step:1760/1775 train_time:104294ms step_avg:59.26ms
step:1761/1775 train_time:104378ms step_avg:59.27ms
step:1762/1775 train_time:104466ms step_avg:59.29ms
step:1763/1775 train_time:104553ms step_avg:59.30ms
step:1764/1775 train_time:104640ms step_avg:59.32ms
step:1765/1775 train_time:104724ms step_avg:59.33ms
step:1766/1775 train_time:104812ms step_avg:59.35ms
step:1767/1775 train_time:104896ms step_avg:59.36ms
step:1768/1775 train_time:104982ms step_avg:59.38ms
step:1769/1775 train_time:105066ms step_avg:59.39ms
step:1770/1775 train_time:105152ms step_avg:59.41ms
step:1771/1775 train_time:105236ms step_avg:59.42ms
step:1772/1775 train_time:105323ms step_avg:59.44ms
step:1773/1775 train_time:105407ms step_avg:59.45ms
step:1774/1775 train_time:105494ms step_avg:59.47ms
step:1775/1775 train_time:105580ms step_avg:59.48ms
step:1775/1775 val_loss:3.2774 train_time:105677ms step_avg:59.54ms
peak memory allocated: 29148 MiB reserved: 45198 MiB

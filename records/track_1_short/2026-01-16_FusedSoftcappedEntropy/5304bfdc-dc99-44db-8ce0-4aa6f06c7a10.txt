import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977

        # This call computes relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# Fused Softcapped Cross Entropy

@triton.jit
def fused_softcapped_entropy_fwd_kernel(
    logits_ptr, losses_ptr, lse_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)
    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    
    max_val = -float('inf')
    sum_exp = 0.0
    
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)
        z = A * tl.sigmoid((val + B) / C)
        z = tl.where(mask, z, -float('inf'))
        curr_max = tl.max(z, axis=0)
        new_max = tl.maximum(max_val, curr_max)
        sum_exp = sum_exp * tl.exp(max_val - new_max) + tl.sum(tl.exp(z - new_max), axis=0)
        max_val = new_max
    
    lse = max_val + tl.log(sum_exp)
    tl.store(lse_ptr + row_idx, lse)
    
    total_loss = 0.0
    for k in range(n_predict):
        target_idx = row_idx + k
        if target_idx < n_rows:
            weight = tl.load(mtp_weights_ptr + k)
            if weight > 0:
                target = tl.load(targets_ptr + target_idx).to(tl.int32)
                if target >= 0 and target < n_cols:
                    val_target = tl.load(logits_row_ptr + target).to(tl.float32)
                    z_target = A * tl.sigmoid((val_target + B) / C)
                    total_loss += weight * (lse - z_target)
    
    tl.store(losses_ptr + row_idx, total_loss)

@triton.jit
def fused_softcapped_entropy_bwd_kernel(
    grad_input_ptr, grad_output_ptr, lse_ptr, logits_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v, stride_grad_n, stride_grad_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)

    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    grad_row_ptr = grad_input_ptr + row_idx * stride_grad_n
    
    lse = tl.load(lse_ptr + row_idx)
    grad_loss = tl.load(grad_output_ptr + row_idx)
    
    S_w = 0.0
    for k in range(n_predict):
        if row_idx + k < n_rows:
            S_w += tl.load(mtp_weights_ptr + k)
            
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=0.0).to(tl.float32)
        u = (val + B) / C
        sigmoid_u = tl.sigmoid(u)
        z = A * sigmoid_u
        p = tl.exp(z - lse)
        
        term1 = S_w * p
        term2 = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
        for k in range(n_predict):
            if row_idx + k < n_rows:
                target = tl.load(targets_ptr + row_idx + k).to(tl.int32)
                weight = tl.load(mtp_weights_ptr + k)
                term2 += tl.where(cols == target, weight, 0.0)
        
        grad_z = grad_loss * (term1 - term2)
        dz_dx = (1.0 / C) * z * (1.0 - sigmoid_u)
        grad_x = grad_z * dz_dx
        tl.store(grad_row_ptr + cols, grad_x.to(tl.bfloat16), mask=mask)

class FusedSoftcappedCrossEntropy(torch.autograd.Function):
    @staticmethod
    def forward(ctx, logits, targets, mtp_weights, A=23.0, B=5.0, C=7.5):
        n_rows, n_cols = logits.shape
        if mtp_weights is None:
             mtp_weights = torch.tensor([1.0], device=logits.device, dtype=torch.float32)
        n_predict = mtp_weights.shape[0]

        losses = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        lse = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        
        logits = logits.contiguous()
        targets = targets.contiguous()
        mtp_weights = mtp_weights.contiguous()

        grid = (n_rows,)
        fused_softcapped_entropy_fwd_kernel[grid](
            logits, losses, lse, targets, mtp_weights,
            logits.stride(0), logits.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        
        ctx.save_for_backward(logits, targets, mtp_weights, lse)
        ctx.params = (A, B, C)
        return losses

    @staticmethod
    def backward(ctx, grad_output):
        logits, targets, mtp_weights, lse = ctx.saved_tensors
        A, B, C = ctx.params
        n_rows, n_cols = logits.shape
        n_predict = mtp_weights.shape[0]
        
        grad_input = torch.empty((n_rows, n_cols), dtype=torch.bfloat16, device=logits.device)
        grad_output = grad_output.contiguous()
        
        grid = (n_rows,)
        fused_softcapped_entropy_bwd_kernel[grid](
            grad_input, grad_output, lse, logits, targets, mtp_weights,
            logits.stride(0), logits.stride(1), grad_input.stride(0), grad_input.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        return grad_input, None, None, None, None, None

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if self.training:
            losses = FusedSoftcappedCrossEntropy.apply(logits.view(-1, logits.size(-1)), target_seq, mtp_weights)
            loss = losses.sum()
        else:
            logits = 23 * torch.sigmoid((logits + 5) / 7.5)
            logits_for_loss = logits.float()
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Jan  8 2026, 06:52:19) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan 15 22:15:47 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   44C    P0            132W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   36C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   34C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   42C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   43C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   35C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   41C    P0            133W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   40C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    314833      C   /usr/bin/python3                             1510MiB |
|    1   N/A  N/A    314834      C   /usr/bin/python3                             1510MiB |
|    2   N/A  N/A    314836      C   /usr/bin/python3                             1510MiB |
|    3   N/A  N/A    314837      C   /usr/bin/python3                             1510MiB |
|    4   N/A  N/A    314838      C   /usr/bin/python3                             1510MiB |
|    5   N/A  N/A    314839      C   /usr/bin/python3                             1510MiB |
|    6   N/A  N/A    314840      C   /usr/bin/python3                             1510MiB |
|    7   N/A  N/A    314841      C   /usr/bin/python3                             1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8293 train_time:0ms step_avg:0.04ms
step:1/1775 train_time:75ms step_avg:74.54ms
step:2/1775 train_time:98ms step_avg:48.98ms
step:3/1775 train_time:118ms step_avg:39.43ms
step:4/1775 train_time:140ms step_avg:35.00ms
step:5/1775 train_time:170ms step_avg:34.05ms
step:6/1775 train_time:259ms step_avg:43.14ms
step:7/1775 train_time:280ms step_avg:40.01ms
step:8/1775 train_time:300ms step_avg:37.47ms
step:9/1775 train_time:329ms step_avg:36.53ms
step:10/1775 train_time:362ms step_avg:36.16ms
step:11/1775 train_time:393ms step_avg:35.70ms
step:12/1775 train_time:426ms step_avg:35.51ms
step:13/1775 train_time:457ms step_avg:35.16ms
step:14/1775 train_time:490ms step_avg:35.01ms
step:15/1775 train_time:521ms step_avg:34.75ms
step:16/1775 train_time:555ms step_avg:34.67ms
step:17/1775 train_time:586ms step_avg:34.47ms
step:18/1775 train_time:619ms step_avg:34.40ms
step:19/1775 train_time:650ms step_avg:34.22ms
step:20/1775 train_time:683ms step_avg:34.17ms
step:21/1775 train_time:715ms step_avg:34.03ms
step:22/1775 train_time:748ms step_avg:34.00ms
step:23/1775 train_time:779ms step_avg:33.88ms
step:24/1775 train_time:813ms step_avg:33.88ms
step:25/1775 train_time:844ms step_avg:33.77ms
step:26/1775 train_time:878ms step_avg:33.75ms
step:27/1775 train_time:909ms step_avg:33.66ms
step:28/1775 train_time:942ms step_avg:33.63ms
step:29/1775 train_time:973ms step_avg:33.55ms
step:30/1775 train_time:1006ms step_avg:33.55ms
step:31/1775 train_time:1038ms step_avg:33.48ms
step:32/1775 train_time:1071ms step_avg:33.48ms
step:33/1775 train_time:1103ms step_avg:33.43ms
step:34/1775 train_time:1137ms step_avg:33.43ms
step:35/1775 train_time:1169ms step_avg:33.41ms
step:36/1775 train_time:1204ms step_avg:33.45ms
step:37/1775 train_time:1236ms step_avg:33.42ms
step:38/1775 train_time:1270ms step_avg:33.43ms
step:39/1775 train_time:1302ms step_avg:33.39ms
step:40/1775 train_time:1336ms step_avg:33.41ms
step:41/1775 train_time:1368ms step_avg:33.36ms
step:42/1775 train_time:1401ms step_avg:33.37ms
step:43/1775 train_time:1433ms step_avg:33.32ms
step:44/1775 train_time:1467ms step_avg:33.33ms
step:45/1775 train_time:1498ms step_avg:33.29ms
step:46/1775 train_time:1532ms step_avg:33.30ms
step:47/1775 train_time:1563ms step_avg:33.26ms
step:48/1775 train_time:1597ms step_avg:33.28ms
step:49/1775 train_time:1628ms step_avg:33.23ms
step:50/1775 train_time:1661ms step_avg:33.23ms
step:51/1775 train_time:1692ms step_avg:33.18ms
step:52/1775 train_time:1725ms step_avg:33.18ms
step:53/1775 train_time:1757ms step_avg:33.15ms
step:54/1775 train_time:1791ms step_avg:33.17ms
step:55/1775 train_time:1822ms step_avg:33.13ms
step:56/1775 train_time:1855ms step_avg:33.13ms
step:57/1775 train_time:1886ms step_avg:33.09ms
step:58/1775 train_time:1920ms step_avg:33.11ms
step:59/1775 train_time:1951ms step_avg:33.07ms
step:60/1775 train_time:1984ms step_avg:33.07ms
step:61/1775 train_time:2016ms step_avg:33.05ms
step:62/1775 train_time:2050ms step_avg:33.07ms
step:63/1775 train_time:2082ms step_avg:33.05ms
step:64/1775 train_time:2116ms step_avg:33.07ms
step:65/1775 train_time:2148ms step_avg:33.04ms
step:66/1775 train_time:2181ms step_avg:33.05ms
step:67/1775 train_time:2214ms step_avg:33.04ms
step:68/1775 train_time:2248ms step_avg:33.05ms
step:69/1775 train_time:2279ms step_avg:33.03ms
step:70/1775 train_time:2314ms step_avg:33.05ms
step:71/1775 train_time:2345ms step_avg:33.03ms
step:72/1775 train_time:2379ms step_avg:33.04ms
step:73/1775 train_time:2411ms step_avg:33.02ms
step:74/1775 train_time:2444ms step_avg:33.03ms
step:75/1775 train_time:2476ms step_avg:33.01ms
step:76/1775 train_time:2509ms step_avg:33.02ms
step:77/1775 train_time:2541ms step_avg:32.99ms
step:78/1775 train_time:2575ms step_avg:33.01ms
step:79/1775 train_time:2606ms step_avg:32.98ms
step:80/1775 train_time:2639ms step_avg:32.99ms
step:81/1775 train_time:2670ms step_avg:32.97ms
step:82/1775 train_time:2704ms step_avg:32.97ms
step:83/1775 train_time:2735ms step_avg:32.95ms
step:84/1775 train_time:2768ms step_avg:32.95ms
step:85/1775 train_time:2799ms step_avg:32.93ms
step:86/1775 train_time:2833ms step_avg:32.94ms
step:87/1775 train_time:2864ms step_avg:32.92ms
step:88/1775 train_time:2897ms step_avg:32.92ms
step:89/1775 train_time:2928ms step_avg:32.90ms
step:90/1775 train_time:2962ms step_avg:32.91ms
step:91/1775 train_time:2994ms step_avg:32.90ms
step:92/1775 train_time:3027ms step_avg:32.90ms
step:93/1775 train_time:3058ms step_avg:32.89ms
step:94/1775 train_time:3092ms step_avg:32.90ms
step:95/1775 train_time:3124ms step_avg:32.88ms
step:96/1775 train_time:3157ms step_avg:32.89ms
step:97/1775 train_time:3189ms step_avg:32.87ms
step:98/1775 train_time:3222ms step_avg:32.88ms
step:99/1775 train_time:3254ms step_avg:32.87ms
step:100/1775 train_time:3288ms step_avg:32.88ms
step:101/1775 train_time:3320ms step_avg:32.87ms
step:102/1775 train_time:3354ms step_avg:32.88ms
step:103/1775 train_time:3386ms step_avg:32.87ms
step:104/1775 train_time:3420ms step_avg:32.88ms
step:105/1775 train_time:3451ms step_avg:32.87ms
step:106/1775 train_time:3484ms step_avg:32.87ms
step:107/1775 train_time:3516ms step_avg:32.86ms
step:108/1775 train_time:3550ms step_avg:32.87ms
step:109/1775 train_time:3581ms step_avg:32.85ms
step:110/1775 train_time:3615ms step_avg:32.87ms
step:111/1775 train_time:3647ms step_avg:32.85ms
step:112/1775 train_time:3680ms step_avg:32.85ms
step:113/1775 train_time:3711ms step_avg:32.84ms
step:114/1775 train_time:3744ms step_avg:32.84ms
step:115/1775 train_time:3776ms step_avg:32.83ms
step:116/1775 train_time:3810ms step_avg:32.84ms
step:117/1775 train_time:3840ms step_avg:32.82ms
step:118/1775 train_time:3874ms step_avg:32.83ms
step:119/1775 train_time:3906ms step_avg:32.82ms
step:120/1775 train_time:3939ms step_avg:32.82ms
step:121/1775 train_time:3970ms step_avg:32.81ms
step:122/1775 train_time:4003ms step_avg:32.81ms
step:123/1775 train_time:4035ms step_avg:32.80ms
step:124/1775 train_time:4068ms step_avg:32.81ms
step:125/1775 train_time:4099ms step_avg:32.79ms
step:126/1775 train_time:4133ms step_avg:32.80ms
step:127/1775 train_time:4164ms step_avg:32.79ms
step:128/1775 train_time:4197ms step_avg:32.79ms
step:129/1775 train_time:4229ms step_avg:32.78ms
step:130/1775 train_time:4262ms step_avg:32.79ms
step:131/1775 train_time:4294ms step_avg:32.78ms
step:132/1775 train_time:4328ms step_avg:32.78ms
step:133/1775 train_time:4359ms step_avg:32.77ms
step:134/1775 train_time:4393ms step_avg:32.78ms
step:135/1775 train_time:4424ms step_avg:32.77ms
step:136/1775 train_time:4458ms step_avg:32.78ms
step:137/1775 train_time:4489ms step_avg:32.77ms
step:138/1775 train_time:4523ms step_avg:32.77ms
step:139/1775 train_time:4554ms step_avg:32.77ms
step:140/1775 train_time:4588ms step_avg:32.77ms
step:141/1775 train_time:4620ms step_avg:32.76ms
step:142/1775 train_time:4653ms step_avg:32.77ms
step:143/1775 train_time:4685ms step_avg:32.76ms
step:144/1775 train_time:4718ms step_avg:32.77ms
step:145/1775 train_time:4749ms step_avg:32.75ms
step:146/1775 train_time:4783ms step_avg:32.76ms
step:147/1775 train_time:4814ms step_avg:32.75ms
step:148/1775 train_time:4848ms step_avg:32.75ms
step:149/1775 train_time:4879ms step_avg:32.74ms
step:150/1775 train_time:4912ms step_avg:32.75ms
step:151/1775 train_time:4943ms step_avg:32.74ms
step:152/1775 train_time:4977ms step_avg:32.74ms
step:153/1775 train_time:5008ms step_avg:32.73ms
step:154/1775 train_time:5041ms step_avg:32.73ms
step:155/1775 train_time:5073ms step_avg:32.73ms
step:156/1775 train_time:5107ms step_avg:32.73ms
step:157/1775 train_time:5137ms step_avg:32.72ms
step:158/1775 train_time:5170ms step_avg:32.72ms
step:159/1775 train_time:5202ms step_avg:32.72ms
step:160/1775 train_time:5236ms step_avg:32.73ms
step:161/1775 train_time:5268ms step_avg:32.72ms
step:162/1775 train_time:5301ms step_avg:32.72ms
step:163/1775 train_time:5333ms step_avg:32.72ms
step:164/1775 train_time:5367ms step_avg:32.72ms
step:165/1775 train_time:5398ms step_avg:32.71ms
step:166/1775 train_time:5432ms step_avg:32.72ms
step:167/1775 train_time:5463ms step_avg:32.71ms
step:168/1775 train_time:5497ms step_avg:32.72ms
step:169/1775 train_time:5528ms step_avg:32.71ms
step:170/1775 train_time:5562ms step_avg:32.72ms
step:171/1775 train_time:5594ms step_avg:32.72ms
step:172/1775 train_time:5628ms step_avg:32.72ms
step:173/1775 train_time:5659ms step_avg:32.71ms
step:174/1775 train_time:5693ms step_avg:32.72ms
step:175/1775 train_time:5724ms step_avg:32.71ms
step:176/1775 train_time:5758ms step_avg:32.71ms
step:177/1775 train_time:5789ms step_avg:32.71ms
step:178/1775 train_time:5822ms step_avg:32.71ms
step:179/1775 train_time:5854ms step_avg:32.70ms
step:180/1775 train_time:5887ms step_avg:32.71ms
step:181/1775 train_time:5918ms step_avg:32.70ms
step:182/1775 train_time:5951ms step_avg:32.70ms
step:183/1775 train_time:5983ms step_avg:32.70ms
step:184/1775 train_time:6017ms step_avg:32.70ms
step:185/1775 train_time:6048ms step_avg:32.69ms
step:186/1775 train_time:6081ms step_avg:32.69ms
step:187/1775 train_time:6113ms step_avg:32.69ms
step:188/1775 train_time:6147ms step_avg:32.70ms
step:189/1775 train_time:6178ms step_avg:32.69ms
step:190/1775 train_time:6213ms step_avg:32.70ms
step:191/1775 train_time:6244ms step_avg:32.69ms
step:192/1775 train_time:6278ms step_avg:32.70ms
step:193/1775 train_time:6309ms step_avg:32.69ms
step:194/1775 train_time:6342ms step_avg:32.69ms
step:195/1775 train_time:6374ms step_avg:32.69ms
step:196/1775 train_time:6407ms step_avg:32.69ms
step:197/1775 train_time:6438ms step_avg:32.68ms
step:198/1775 train_time:6472ms step_avg:32.68ms
step:199/1775 train_time:6503ms step_avg:32.68ms
step:200/1775 train_time:6537ms step_avg:32.68ms
step:201/1775 train_time:6568ms step_avg:32.68ms
step:202/1775 train_time:6601ms step_avg:32.68ms
step:203/1775 train_time:6633ms step_avg:32.67ms
step:204/1775 train_time:6666ms step_avg:32.68ms
step:205/1775 train_time:6697ms step_avg:32.67ms
step:206/1775 train_time:6731ms step_avg:32.67ms
step:207/1775 train_time:6762ms step_avg:32.67ms
step:208/1775 train_time:6796ms step_avg:32.67ms
step:209/1775 train_time:6827ms step_avg:32.67ms
step:210/1775 train_time:6861ms step_avg:32.67ms
step:211/1775 train_time:6892ms step_avg:32.66ms
step:212/1775 train_time:6925ms step_avg:32.67ms
step:213/1775 train_time:6957ms step_avg:32.66ms
step:214/1775 train_time:6990ms step_avg:32.66ms
step:215/1775 train_time:7021ms step_avg:32.66ms
step:216/1775 train_time:7055ms step_avg:32.66ms
step:217/1775 train_time:7087ms step_avg:32.66ms
step:218/1775 train_time:7120ms step_avg:32.66ms
step:219/1775 train_time:7151ms step_avg:32.65ms
step:220/1775 train_time:7184ms step_avg:32.66ms
step:221/1775 train_time:7216ms step_avg:32.65ms
step:222/1775 train_time:7249ms step_avg:32.65ms
step:223/1775 train_time:7280ms step_avg:32.65ms
step:224/1775 train_time:7314ms step_avg:32.65ms
step:225/1775 train_time:7345ms step_avg:32.64ms
step:226/1775 train_time:7379ms step_avg:32.65ms
step:227/1775 train_time:7410ms step_avg:32.64ms
step:228/1775 train_time:7443ms step_avg:32.65ms
step:229/1775 train_time:7474ms step_avg:32.64ms
step:230/1775 train_time:7508ms step_avg:32.64ms
step:231/1775 train_time:7539ms step_avg:32.64ms
step:232/1775 train_time:7573ms step_avg:32.64ms
step:233/1775 train_time:7604ms step_avg:32.64ms
step:234/1775 train_time:7638ms step_avg:32.64ms
step:235/1775 train_time:7670ms step_avg:32.64ms
step:236/1775 train_time:7703ms step_avg:32.64ms
step:237/1775 train_time:7734ms step_avg:32.63ms
step:238/1775 train_time:7768ms step_avg:32.64ms
step:239/1775 train_time:7800ms step_avg:32.63ms
step:240/1775 train_time:7833ms step_avg:32.64ms
step:241/1775 train_time:7865ms step_avg:32.63ms
step:242/1775 train_time:7898ms step_avg:32.64ms
step:243/1775 train_time:7929ms step_avg:32.63ms
step:244/1775 train_time:7962ms step_avg:32.63ms
step:245/1775 train_time:7994ms step_avg:32.63ms
step:246/1775 train_time:8028ms step_avg:32.63ms
step:247/1775 train_time:8059ms step_avg:32.63ms
step:248/1775 train_time:8092ms step_avg:32.63ms
step:249/1775 train_time:8124ms step_avg:32.63ms
step:250/1775 train_time:8157ms step_avg:32.63ms
step:250/1775 val_loss:4.6017 train_time:8199ms step_avg:32.79ms
step:251/1775 train_time:8219ms step_avg:32.74ms
step:252/1775 train_time:8239ms step_avg:32.69ms
step:253/1775 train_time:8258ms step_avg:32.64ms
step:254/1775 train_time:8291ms step_avg:32.64ms
step:255/1775 train_time:8325ms step_avg:32.65ms
step:256/1775 train_time:8359ms step_avg:32.65ms
step:257/1775 train_time:8390ms step_avg:32.65ms
step:258/1775 train_time:8424ms step_avg:32.65ms
step:259/1775 train_time:8456ms step_avg:32.65ms
step:260/1775 train_time:8489ms step_avg:32.65ms
step:261/1775 train_time:8520ms step_avg:32.64ms
step:262/1775 train_time:8553ms step_avg:32.65ms
step:263/1775 train_time:8584ms step_avg:32.64ms
step:264/1775 train_time:8617ms step_avg:32.64ms
step:265/1775 train_time:8649ms step_avg:32.64ms
step:266/1775 train_time:8683ms step_avg:32.64ms
step:267/1775 train_time:8714ms step_avg:32.64ms
step:268/1775 train_time:8747ms step_avg:32.64ms
step:269/1775 train_time:8778ms step_avg:32.63ms
step:270/1775 train_time:8811ms step_avg:32.63ms
step:271/1775 train_time:8842ms step_avg:32.63ms
step:272/1775 train_time:8875ms step_avg:32.63ms
step:273/1775 train_time:8906ms step_avg:32.62ms
step:274/1775 train_time:8939ms step_avg:32.62ms
step:275/1775 train_time:8970ms step_avg:32.62ms
step:276/1775 train_time:9004ms step_avg:32.62ms
step:277/1775 train_time:9035ms step_avg:32.62ms
step:278/1775 train_time:9068ms step_avg:32.62ms
step:279/1775 train_time:9099ms step_avg:32.61ms
step:280/1775 train_time:9132ms step_avg:32.61ms
step:281/1775 train_time:9164ms step_avg:32.61ms
step:282/1775 train_time:9198ms step_avg:32.62ms
step:283/1775 train_time:9229ms step_avg:32.61ms
step:284/1775 train_time:9264ms step_avg:32.62ms
step:285/1775 train_time:9296ms step_avg:32.62ms
step:286/1775 train_time:9330ms step_avg:32.62ms
step:287/1775 train_time:9361ms step_avg:32.62ms
step:288/1775 train_time:9395ms step_avg:32.62ms
step:289/1775 train_time:9427ms step_avg:32.62ms
step:290/1775 train_time:9461ms step_avg:32.62ms
step:291/1775 train_time:9492ms step_avg:32.62ms
step:292/1775 train_time:9525ms step_avg:32.62ms
step:293/1775 train_time:9556ms step_avg:32.61ms
step:294/1775 train_time:9589ms step_avg:32.62ms
step:295/1775 train_time:9621ms step_avg:32.61ms
step:296/1775 train_time:9654ms step_avg:32.61ms
step:297/1775 train_time:9686ms step_avg:32.61ms
step:298/1775 train_time:9719ms step_avg:32.61ms
step:299/1775 train_time:9749ms step_avg:32.61ms
step:300/1775 train_time:9783ms step_avg:32.61ms
step:301/1775 train_time:9814ms step_avg:32.60ms
step:302/1775 train_time:9847ms step_avg:32.61ms
step:303/1775 train_time:9878ms step_avg:32.60ms
step:304/1775 train_time:9911ms step_avg:32.60ms
step:305/1775 train_time:9942ms step_avg:32.60ms
step:306/1775 train_time:9976ms step_avg:32.60ms
step:307/1775 train_time:10007ms step_avg:32.60ms
step:308/1775 train_time:10040ms step_avg:32.60ms
step:309/1775 train_time:10072ms step_avg:32.59ms
step:310/1775 train_time:10106ms step_avg:32.60ms
step:311/1775 train_time:10137ms step_avg:32.60ms
step:312/1775 train_time:10171ms step_avg:32.60ms
step:313/1775 train_time:10202ms step_avg:32.60ms
step:314/1775 train_time:10236ms step_avg:32.60ms
step:315/1775 train_time:10267ms step_avg:32.60ms
step:316/1775 train_time:10301ms step_avg:32.60ms
step:317/1775 train_time:10332ms step_avg:32.59ms
step:318/1775 train_time:10366ms step_avg:32.60ms
step:319/1775 train_time:10398ms step_avg:32.59ms
step:320/1775 train_time:10431ms step_avg:32.60ms
step:321/1775 train_time:10463ms step_avg:32.60ms
step:322/1775 train_time:10497ms step_avg:32.60ms
step:323/1775 train_time:10528ms step_avg:32.59ms
step:324/1775 train_time:10561ms step_avg:32.60ms
step:325/1775 train_time:10592ms step_avg:32.59ms
step:326/1775 train_time:10626ms step_avg:32.60ms
step:327/1775 train_time:10657ms step_avg:32.59ms
step:328/1775 train_time:10690ms step_avg:32.59ms
step:329/1775 train_time:10722ms step_avg:32.59ms
step:330/1775 train_time:10755ms step_avg:32.59ms
step:331/1775 train_time:10787ms step_avg:32.59ms
step:332/1775 train_time:10820ms step_avg:32.59ms
step:333/1775 train_time:10851ms step_avg:32.59ms
step:334/1775 train_time:10885ms step_avg:32.59ms
step:335/1775 train_time:10917ms step_avg:32.59ms
step:336/1775 train_time:10949ms step_avg:32.59ms
step:337/1775 train_time:10980ms step_avg:32.58ms
step:338/1775 train_time:11014ms step_avg:32.58ms
step:339/1775 train_time:11045ms step_avg:32.58ms
step:340/1775 train_time:11079ms step_avg:32.58ms
step:341/1775 train_time:11110ms step_avg:32.58ms
step:342/1775 train_time:11143ms step_avg:32.58ms
step:343/1775 train_time:11175ms step_avg:32.58ms
step:344/1775 train_time:11209ms step_avg:32.58ms
step:345/1775 train_time:11240ms step_avg:32.58ms
step:346/1775 train_time:11273ms step_avg:32.58ms
step:347/1775 train_time:11304ms step_avg:32.58ms
step:348/1775 train_time:11338ms step_avg:32.58ms
step:349/1775 train_time:11369ms step_avg:32.58ms
step:350/1775 train_time:11403ms step_avg:32.58ms
step:351/1775 train_time:11435ms step_avg:32.58ms
step:352/1775 train_time:11468ms step_avg:32.58ms
step:353/1775 train_time:11499ms step_avg:32.58ms
step:354/1775 train_time:11532ms step_avg:32.58ms
step:355/1775 train_time:11564ms step_avg:32.57ms
step:356/1775 train_time:11597ms step_avg:32.58ms
step:357/1775 train_time:11629ms step_avg:32.57ms
step:358/1775 train_time:11663ms step_avg:32.58ms
step:359/1775 train_time:11695ms step_avg:32.58ms
step:360/1775 train_time:11728ms step_avg:32.58ms
step:361/1775 train_time:11759ms step_avg:32.57ms
step:362/1775 train_time:11792ms step_avg:32.57ms
step:363/1775 train_time:11824ms step_avg:32.57ms
step:364/1775 train_time:11857ms step_avg:32.58ms
step:365/1775 train_time:11888ms step_avg:32.57ms
step:366/1775 train_time:11921ms step_avg:32.57ms
step:367/1775 train_time:11952ms step_avg:32.57ms
step:368/1775 train_time:11986ms step_avg:32.57ms
step:369/1775 train_time:12017ms step_avg:32.57ms
step:370/1775 train_time:12050ms step_avg:32.57ms
step:371/1775 train_time:12081ms step_avg:32.56ms
step:372/1775 train_time:12114ms step_avg:32.56ms
step:373/1775 train_time:12145ms step_avg:32.56ms
step:374/1775 train_time:12179ms step_avg:32.56ms
step:375/1775 train_time:12210ms step_avg:32.56ms
step:376/1775 train_time:12245ms step_avg:32.57ms
step:377/1775 train_time:12276ms step_avg:32.56ms
step:378/1775 train_time:12310ms step_avg:32.57ms
step:379/1775 train_time:12341ms step_avg:32.56ms
step:380/1775 train_time:12375ms step_avg:32.57ms
step:381/1775 train_time:12407ms step_avg:32.56ms
step:382/1775 train_time:12440ms step_avg:32.57ms
step:383/1775 train_time:12471ms step_avg:32.56ms
step:384/1775 train_time:12505ms step_avg:32.56ms
step:385/1775 train_time:12537ms step_avg:32.56ms
step:386/1775 train_time:12570ms step_avg:32.56ms
step:387/1775 train_time:12601ms step_avg:32.56ms
step:388/1775 train_time:12634ms step_avg:32.56ms
step:389/1775 train_time:12666ms step_avg:32.56ms
step:390/1775 train_time:12699ms step_avg:32.56ms
step:391/1775 train_time:12730ms step_avg:32.56ms
step:392/1775 train_time:12764ms step_avg:32.56ms
step:393/1775 train_time:12795ms step_avg:32.56ms
step:394/1775 train_time:12828ms step_avg:32.56ms
step:395/1775 train_time:12859ms step_avg:32.55ms
step:396/1775 train_time:12892ms step_avg:32.56ms
step:397/1775 train_time:12924ms step_avg:32.55ms
step:398/1775 train_time:12958ms step_avg:32.56ms
step:399/1775 train_time:12989ms step_avg:32.55ms
step:400/1775 train_time:13022ms step_avg:32.56ms
step:401/1775 train_time:13054ms step_avg:32.55ms
step:402/1775 train_time:13087ms step_avg:32.55ms
step:403/1775 train_time:13117ms step_avg:32.55ms
step:404/1775 train_time:13150ms step_avg:32.55ms
step:405/1775 train_time:13181ms step_avg:32.55ms
step:406/1775 train_time:13216ms step_avg:32.55ms
step:407/1775 train_time:13247ms step_avg:32.55ms
step:408/1775 train_time:13280ms step_avg:32.55ms
step:409/1775 train_time:13312ms step_avg:32.55ms
step:410/1775 train_time:13346ms step_avg:32.55ms
step:411/1775 train_time:13377ms step_avg:32.55ms
step:412/1775 train_time:13410ms step_avg:32.55ms
step:413/1775 train_time:13441ms step_avg:32.55ms
step:414/1775 train_time:13475ms step_avg:32.55ms
step:415/1775 train_time:13507ms step_avg:32.55ms
step:416/1775 train_time:13541ms step_avg:32.55ms
step:417/1775 train_time:13572ms step_avg:32.55ms
step:418/1775 train_time:13605ms step_avg:32.55ms
step:419/1775 train_time:13636ms step_avg:32.55ms
step:420/1775 train_time:13670ms step_avg:32.55ms
step:421/1775 train_time:13701ms step_avg:32.54ms
step:422/1775 train_time:13734ms step_avg:32.54ms
step:423/1775 train_time:13765ms step_avg:32.54ms
step:424/1775 train_time:13798ms step_avg:32.54ms
step:425/1775 train_time:13829ms step_avg:32.54ms
step:426/1775 train_time:13864ms step_avg:32.54ms
step:427/1775 train_time:13895ms step_avg:32.54ms
step:428/1775 train_time:13928ms step_avg:32.54ms
step:429/1775 train_time:13959ms step_avg:32.54ms
step:430/1775 train_time:13993ms step_avg:32.54ms
step:431/1775 train_time:14024ms step_avg:32.54ms
step:432/1775 train_time:14057ms step_avg:32.54ms
step:433/1775 train_time:14088ms step_avg:32.54ms
step:434/1775 train_time:14122ms step_avg:32.54ms
step:435/1775 train_time:14153ms step_avg:32.54ms
step:436/1775 train_time:14187ms step_avg:32.54ms
step:437/1775 train_time:14218ms step_avg:32.54ms
step:438/1775 train_time:14252ms step_avg:32.54ms
step:439/1775 train_time:14284ms step_avg:32.54ms
step:440/1775 train_time:14317ms step_avg:32.54ms
step:441/1775 train_time:14348ms step_avg:32.54ms
step:442/1775 train_time:14382ms step_avg:32.54ms
step:443/1775 train_time:14413ms step_avg:32.54ms
step:444/1775 train_time:14447ms step_avg:32.54ms
step:445/1775 train_time:14479ms step_avg:32.54ms
step:446/1775 train_time:14512ms step_avg:32.54ms
step:447/1775 train_time:14544ms step_avg:32.54ms
step:448/1775 train_time:14577ms step_avg:32.54ms
step:449/1775 train_time:14609ms step_avg:32.54ms
step:450/1775 train_time:14643ms step_avg:32.54ms
step:451/1775 train_time:14674ms step_avg:32.54ms
step:452/1775 train_time:14708ms step_avg:32.54ms
step:453/1775 train_time:14739ms step_avg:32.54ms
step:454/1775 train_time:14772ms step_avg:32.54ms
step:455/1775 train_time:14803ms step_avg:32.53ms
step:456/1775 train_time:14837ms step_avg:32.54ms
step:457/1775 train_time:14868ms step_avg:32.53ms
step:458/1775 train_time:14902ms step_avg:32.54ms
step:459/1775 train_time:14933ms step_avg:32.53ms
step:460/1775 train_time:14969ms step_avg:32.54ms
step:461/1775 train_time:14998ms step_avg:32.53ms
step:462/1775 train_time:15031ms step_avg:32.53ms
step:463/1775 train_time:15062ms step_avg:32.53ms
step:464/1775 train_time:15095ms step_avg:32.53ms
step:465/1775 train_time:15127ms step_avg:32.53ms
step:466/1775 train_time:15161ms step_avg:32.53ms
step:467/1775 train_time:15193ms step_avg:32.53ms
step:468/1775 train_time:15226ms step_avg:32.53ms
step:469/1775 train_time:15257ms step_avg:32.53ms
step:470/1775 train_time:15290ms step_avg:32.53ms
step:471/1775 train_time:15322ms step_avg:32.53ms
step:472/1775 train_time:15355ms step_avg:32.53ms
step:473/1775 train_time:15387ms step_avg:32.53ms
step:474/1775 train_time:15420ms step_avg:32.53ms
step:475/1775 train_time:15451ms step_avg:32.53ms
step:476/1775 train_time:15485ms step_avg:32.53ms
step:477/1775 train_time:15516ms step_avg:32.53ms
step:478/1775 train_time:15549ms step_avg:32.53ms
step:479/1775 train_time:15580ms step_avg:32.53ms
step:480/1775 train_time:15614ms step_avg:32.53ms
step:481/1775 train_time:15645ms step_avg:32.53ms
step:482/1775 train_time:15679ms step_avg:32.53ms
step:483/1775 train_time:15710ms step_avg:32.52ms
step:484/1775 train_time:15744ms step_avg:32.53ms
step:485/1775 train_time:15775ms step_avg:32.53ms
step:486/1775 train_time:15808ms step_avg:32.53ms
step:487/1775 train_time:15839ms step_avg:32.52ms
step:488/1775 train_time:15873ms step_avg:32.53ms
step:489/1775 train_time:15905ms step_avg:32.52ms
step:490/1775 train_time:15938ms step_avg:32.53ms
step:491/1775 train_time:15969ms step_avg:32.52ms
step:492/1775 train_time:16003ms step_avg:32.53ms
step:493/1775 train_time:16034ms step_avg:32.52ms
step:494/1775 train_time:16067ms step_avg:32.52ms
step:495/1775 train_time:16098ms step_avg:32.52ms
step:496/1775 train_time:16132ms step_avg:32.52ms
step:497/1775 train_time:16163ms step_avg:32.52ms
step:498/1775 train_time:16197ms step_avg:32.52ms
step:499/1775 train_time:16228ms step_avg:32.52ms
step:500/1775 train_time:16262ms step_avg:32.52ms
step:500/1775 val_loss:4.2728 train_time:16303ms step_avg:32.61ms
step:501/1775 train_time:16324ms step_avg:32.58ms
step:502/1775 train_time:16344ms step_avg:32.56ms
step:503/1775 train_time:16362ms step_avg:32.53ms
step:504/1775 train_time:16395ms step_avg:32.53ms
step:505/1775 train_time:16427ms step_avg:32.53ms
step:506/1775 train_time:16462ms step_avg:32.53ms
step:507/1775 train_time:16494ms step_avg:32.53ms
step:508/1775 train_time:16528ms step_avg:32.53ms
step:509/1775 train_time:16559ms step_avg:32.53ms
step:510/1775 train_time:16593ms step_avg:32.53ms
step:511/1775 train_time:16624ms step_avg:32.53ms
step:512/1775 train_time:16657ms step_avg:32.53ms
step:513/1775 train_time:16688ms step_avg:32.53ms
step:514/1775 train_time:16721ms step_avg:32.53ms
step:515/1775 train_time:16753ms step_avg:32.53ms
step:516/1775 train_time:16786ms step_avg:32.53ms
step:517/1775 train_time:16817ms step_avg:32.53ms
step:518/1775 train_time:16851ms step_avg:32.53ms
step:519/1775 train_time:16881ms step_avg:32.53ms
step:520/1775 train_time:16915ms step_avg:32.53ms
step:521/1775 train_time:16946ms step_avg:32.53ms
step:522/1775 train_time:16979ms step_avg:32.53ms
step:523/1775 train_time:17010ms step_avg:32.52ms
step:524/1775 train_time:17043ms step_avg:32.53ms
step:525/1775 train_time:17075ms step_avg:32.52ms
step:526/1775 train_time:17108ms step_avg:32.52ms
step:527/1775 train_time:17139ms step_avg:32.52ms
step:528/1775 train_time:17172ms step_avg:32.52ms
step:529/1775 train_time:17203ms step_avg:32.52ms
step:530/1775 train_time:17237ms step_avg:32.52ms
step:531/1775 train_time:17268ms step_avg:32.52ms
step:532/1775 train_time:17302ms step_avg:32.52ms
step:533/1775 train_time:17333ms step_avg:32.52ms
step:534/1775 train_time:17368ms step_avg:32.52ms
step:535/1775 train_time:17399ms step_avg:32.52ms
step:536/1775 train_time:17433ms step_avg:32.52ms
step:537/1775 train_time:17465ms step_avg:32.52ms
step:538/1775 train_time:17498ms step_avg:32.52ms
step:539/1775 train_time:17530ms step_avg:32.52ms
step:540/1775 train_time:17564ms step_avg:32.53ms
step:541/1775 train_time:17595ms step_avg:32.52ms
step:542/1775 train_time:17628ms step_avg:32.52ms
step:543/1775 train_time:17659ms step_avg:32.52ms
step:544/1775 train_time:17693ms step_avg:32.52ms
step:545/1775 train_time:17725ms step_avg:32.52ms
step:546/1775 train_time:17758ms step_avg:32.52ms
step:547/1775 train_time:17790ms step_avg:32.52ms
step:548/1775 train_time:17823ms step_avg:32.52ms
step:549/1775 train_time:17854ms step_avg:32.52ms
step:550/1775 train_time:17887ms step_avg:32.52ms
step:551/1775 train_time:17918ms step_avg:32.52ms
step:552/1775 train_time:17951ms step_avg:32.52ms
step:553/1775 train_time:17982ms step_avg:32.52ms
step:554/1775 train_time:18016ms step_avg:32.52ms
step:555/1775 train_time:18047ms step_avg:32.52ms
step:556/1775 train_time:18080ms step_avg:32.52ms
step:557/1775 train_time:18110ms step_avg:32.51ms
step:558/1775 train_time:18144ms step_avg:32.52ms
step:559/1775 train_time:18175ms step_avg:32.51ms
step:560/1775 train_time:18208ms step_avg:32.51ms
step:561/1775 train_time:18239ms step_avg:32.51ms
step:562/1775 train_time:18273ms step_avg:32.51ms
step:563/1775 train_time:18306ms step_avg:32.51ms
step:564/1775 train_time:18339ms step_avg:32.52ms
step:565/1775 train_time:18370ms step_avg:32.51ms
step:566/1775 train_time:18404ms step_avg:32.52ms
step:567/1775 train_time:18436ms step_avg:32.51ms
step:568/1775 train_time:18469ms step_avg:32.52ms
step:569/1775 train_time:18500ms step_avg:32.51ms
step:570/1775 train_time:18534ms step_avg:32.52ms
step:571/1775 train_time:18566ms step_avg:32.51ms
step:572/1775 train_time:18599ms step_avg:32.52ms
step:573/1775 train_time:18631ms step_avg:32.52ms
step:574/1775 train_time:18665ms step_avg:32.52ms
step:575/1775 train_time:18696ms step_avg:32.51ms
step:576/1775 train_time:18729ms step_avg:32.52ms
step:577/1775 train_time:18761ms step_avg:32.51ms
step:578/1775 train_time:18794ms step_avg:32.52ms
step:579/1775 train_time:18825ms step_avg:32.51ms
step:580/1775 train_time:18860ms step_avg:32.52ms
step:581/1775 train_time:18919ms step_avg:32.56ms
step:582/1775 train_time:18978ms step_avg:32.61ms
step:583/1775 train_time:19034ms step_avg:32.65ms
step:584/1775 train_time:19095ms step_avg:32.70ms
step:585/1775 train_time:19153ms step_avg:32.74ms
step:586/1775 train_time:19212ms step_avg:32.79ms
step:587/1775 train_time:19270ms step_avg:32.83ms
step:588/1775 train_time:19331ms step_avg:32.88ms
step:589/1775 train_time:19390ms step_avg:32.92ms
step:590/1775 train_time:19451ms step_avg:32.97ms
step:591/1775 train_time:19509ms step_avg:33.01ms
step:592/1775 train_time:19570ms step_avg:33.06ms
step:593/1775 train_time:19628ms step_avg:33.10ms
step:594/1775 train_time:19689ms step_avg:33.15ms
step:595/1775 train_time:19747ms step_avg:33.19ms
step:596/1775 train_time:19808ms step_avg:33.23ms
step:597/1775 train_time:19867ms step_avg:33.28ms
step:598/1775 train_time:19927ms step_avg:33.32ms
step:599/1775 train_time:19985ms step_avg:33.36ms
step:600/1775 train_time:20045ms step_avg:33.41ms
step:601/1775 train_time:20102ms step_avg:33.45ms
step:602/1775 train_time:20163ms step_avg:33.49ms
step:603/1775 train_time:20220ms step_avg:33.53ms
step:604/1775 train_time:20281ms step_avg:33.58ms
step:605/1775 train_time:20338ms step_avg:33.62ms
step:606/1775 train_time:20399ms step_avg:33.66ms
step:607/1775 train_time:20458ms step_avg:33.70ms
step:608/1775 train_time:20519ms step_avg:33.75ms
step:609/1775 train_time:20577ms step_avg:33.79ms
step:610/1775 train_time:20637ms step_avg:33.83ms
step:611/1775 train_time:20694ms step_avg:33.87ms
step:612/1775 train_time:20755ms step_avg:33.91ms
step:613/1775 train_time:20813ms step_avg:33.95ms
step:614/1775 train_time:20874ms step_avg:34.00ms
step:615/1775 train_time:20932ms step_avg:34.04ms
step:616/1775 train_time:20992ms step_avg:34.08ms
step:617/1775 train_time:21049ms step_avg:34.12ms
step:618/1775 train_time:21110ms step_avg:34.16ms
step:619/1775 train_time:21168ms step_avg:34.20ms
step:620/1775 train_time:21228ms step_avg:34.24ms
step:621/1775 train_time:21287ms step_avg:34.28ms
step:622/1775 train_time:21347ms step_avg:34.32ms
step:623/1775 train_time:21405ms step_avg:34.36ms
step:624/1775 train_time:21466ms step_avg:34.40ms
step:625/1775 train_time:21524ms step_avg:34.44ms
step:626/1775 train_time:21586ms step_avg:34.48ms
step:627/1775 train_time:21644ms step_avg:34.52ms
step:628/1775 train_time:21705ms step_avg:34.56ms
step:629/1775 train_time:21763ms step_avg:34.60ms
step:630/1775 train_time:21824ms step_avg:34.64ms
step:631/1775 train_time:21881ms step_avg:34.68ms
step:632/1775 train_time:21942ms step_avg:34.72ms
step:633/1775 train_time:21999ms step_avg:34.75ms
step:634/1775 train_time:22059ms step_avg:34.79ms
step:635/1775 train_time:22116ms step_avg:34.83ms
step:636/1775 train_time:22176ms step_avg:34.87ms
step:637/1775 train_time:22235ms step_avg:34.91ms
step:638/1775 train_time:22295ms step_avg:34.94ms
step:639/1775 train_time:22354ms step_avg:34.98ms
step:640/1775 train_time:22414ms step_avg:35.02ms
step:641/1775 train_time:22472ms step_avg:35.06ms
step:642/1775 train_time:22534ms step_avg:35.10ms
step:643/1775 train_time:22592ms step_avg:35.14ms
step:644/1775 train_time:22652ms step_avg:35.17ms
step:645/1775 train_time:22710ms step_avg:35.21ms
step:646/1775 train_time:22771ms step_avg:35.25ms
step:647/1775 train_time:22830ms step_avg:35.29ms
step:648/1775 train_time:22891ms step_avg:35.33ms
step:649/1775 train_time:22948ms step_avg:35.36ms
step:650/1775 train_time:23008ms step_avg:35.40ms
step:651/1775 train_time:23067ms step_avg:35.43ms
step:652/1775 train_time:23127ms step_avg:35.47ms
step:653/1775 train_time:23185ms step_avg:35.51ms
step:654/1775 train_time:23245ms step_avg:35.54ms
step:655/1775 train_time:23303ms step_avg:35.58ms
step:656/1775 train_time:23364ms step_avg:35.62ms
step:657/1775 train_time:23422ms step_avg:35.65ms
step:658/1775 train_time:23483ms step_avg:35.69ms
step:659/1775 train_time:23541ms step_avg:35.72ms
step:660/1775 train_time:23602ms step_avg:35.76ms
step:661/1775 train_time:23660ms step_avg:35.79ms
step:662/1775 train_time:23721ms step_avg:35.83ms
step:663/1775 train_time:23779ms step_avg:35.87ms
step:664/1775 train_time:23839ms step_avg:35.90ms
step:665/1775 train_time:23896ms step_avg:35.93ms
step:666/1775 train_time:23957ms step_avg:35.97ms
step:667/1775 train_time:24015ms step_avg:36.00ms
step:668/1775 train_time:24075ms step_avg:36.04ms
step:669/1775 train_time:24132ms step_avg:36.07ms
step:670/1775 train_time:24193ms step_avg:36.11ms
step:671/1775 train_time:24251ms step_avg:36.14ms
step:672/1775 train_time:24312ms step_avg:36.18ms
step:673/1775 train_time:24370ms step_avg:36.21ms
step:674/1775 train_time:24431ms step_avg:36.25ms
step:675/1775 train_time:24489ms step_avg:36.28ms
step:676/1775 train_time:24550ms step_avg:36.32ms
step:677/1775 train_time:24608ms step_avg:36.35ms
step:678/1775 train_time:24670ms step_avg:36.39ms
step:679/1775 train_time:24729ms step_avg:36.42ms
step:680/1775 train_time:24789ms step_avg:36.45ms
step:681/1775 train_time:24847ms step_avg:36.49ms
step:682/1775 train_time:24908ms step_avg:36.52ms
step:683/1775 train_time:24967ms step_avg:36.55ms
step:684/1775 train_time:25027ms step_avg:36.59ms
step:685/1775 train_time:25085ms step_avg:36.62ms
step:686/1775 train_time:25146ms step_avg:36.66ms
step:687/1775 train_time:25204ms step_avg:36.69ms
step:688/1775 train_time:25264ms step_avg:36.72ms
step:689/1775 train_time:25322ms step_avg:36.75ms
step:690/1775 train_time:25383ms step_avg:36.79ms
step:691/1775 train_time:25441ms step_avg:36.82ms
step:692/1775 train_time:25501ms step_avg:36.85ms
step:693/1775 train_time:25559ms step_avg:36.88ms
step:694/1775 train_time:25619ms step_avg:36.91ms
step:695/1775 train_time:25677ms step_avg:36.95ms
step:696/1775 train_time:25737ms step_avg:36.98ms
step:697/1775 train_time:25795ms step_avg:37.01ms
step:698/1775 train_time:25854ms step_avg:37.04ms
step:699/1775 train_time:25913ms step_avg:37.07ms
step:700/1775 train_time:25974ms step_avg:37.11ms
step:701/1775 train_time:26031ms step_avg:37.13ms
step:702/1775 train_time:26093ms step_avg:37.17ms
step:703/1775 train_time:26150ms step_avg:37.20ms
step:704/1775 train_time:26210ms step_avg:37.23ms
step:705/1775 train_time:26269ms step_avg:37.26ms
step:706/1775 train_time:26330ms step_avg:37.30ms
step:707/1775 train_time:26388ms step_avg:37.32ms
step:708/1775 train_time:26449ms step_avg:37.36ms
step:709/1775 train_time:26509ms step_avg:37.39ms
step:710/1775 train_time:26570ms step_avg:37.42ms
step:711/1775 train_time:26629ms step_avg:37.45ms
step:712/1775 train_time:26689ms step_avg:37.48ms
step:713/1775 train_time:26747ms step_avg:37.51ms
step:714/1775 train_time:26808ms step_avg:37.55ms
step:715/1775 train_time:26867ms step_avg:37.58ms
step:716/1775 train_time:26927ms step_avg:37.61ms
step:717/1775 train_time:26985ms step_avg:37.64ms
step:718/1775 train_time:27045ms step_avg:37.67ms
step:719/1775 train_time:27102ms step_avg:37.69ms
step:720/1775 train_time:27163ms step_avg:37.73ms
step:721/1775 train_time:27220ms step_avg:37.75ms
step:722/1775 train_time:27280ms step_avg:37.78ms
step:723/1775 train_time:27337ms step_avg:37.81ms
step:724/1775 train_time:27398ms step_avg:37.84ms
step:725/1775 train_time:27456ms step_avg:37.87ms
step:726/1775 train_time:27516ms step_avg:37.90ms
step:727/1775 train_time:27574ms step_avg:37.93ms
step:728/1775 train_time:27635ms step_avg:37.96ms
step:729/1775 train_time:27692ms step_avg:37.99ms
step:730/1775 train_time:27753ms step_avg:38.02ms
step:731/1775 train_time:27811ms step_avg:38.04ms
step:732/1775 train_time:27871ms step_avg:38.08ms
step:733/1775 train_time:27929ms step_avg:38.10ms
step:734/1775 train_time:27990ms step_avg:38.13ms
step:735/1775 train_time:28048ms step_avg:38.16ms
step:736/1775 train_time:28108ms step_avg:38.19ms
step:737/1775 train_time:28165ms step_avg:38.22ms
step:738/1775 train_time:28225ms step_avg:38.24ms
step:739/1775 train_time:28283ms step_avg:38.27ms
step:740/1775 train_time:28345ms step_avg:38.30ms
step:741/1775 train_time:28402ms step_avg:38.33ms
step:742/1775 train_time:28464ms step_avg:38.36ms
step:743/1775 train_time:28522ms step_avg:38.39ms
step:744/1775 train_time:28582ms step_avg:38.42ms
step:745/1775 train_time:28641ms step_avg:38.44ms
step:746/1775 train_time:28701ms step_avg:38.47ms
step:747/1775 train_time:28759ms step_avg:38.50ms
step:748/1775 train_time:28819ms step_avg:38.53ms
step:749/1775 train_time:28877ms step_avg:38.55ms
step:750/1775 train_time:28937ms step_avg:38.58ms
step:750/1775 val_loss:4.0021 train_time:29008ms step_avg:38.68ms
step:751/1775 train_time:29029ms step_avg:38.65ms
step:752/1775 train_time:29058ms step_avg:38.64ms
step:753/1775 train_time:29120ms step_avg:38.67ms
step:754/1775 train_time:29180ms step_avg:38.70ms
step:755/1775 train_time:29240ms step_avg:38.73ms
step:756/1775 train_time:29301ms step_avg:38.76ms
step:757/1775 train_time:29359ms step_avg:38.78ms
step:758/1775 train_time:29418ms step_avg:38.81ms
step:759/1775 train_time:29474ms step_avg:38.83ms
step:760/1775 train_time:29534ms step_avg:38.86ms
step:761/1775 train_time:29590ms step_avg:38.88ms
step:762/1775 train_time:29651ms step_avg:38.91ms
step:763/1775 train_time:29708ms step_avg:38.94ms
step:764/1775 train_time:29768ms step_avg:38.96ms
step:765/1775 train_time:29825ms step_avg:38.99ms
step:766/1775 train_time:29885ms step_avg:39.01ms
step:767/1775 train_time:29944ms step_avg:39.04ms
step:768/1775 train_time:30007ms step_avg:39.07ms
step:769/1775 train_time:30068ms step_avg:39.10ms
step:770/1775 train_time:30130ms step_avg:39.13ms
step:771/1775 train_time:30189ms step_avg:39.16ms
step:772/1775 train_time:30250ms step_avg:39.18ms
step:773/1775 train_time:30309ms step_avg:39.21ms
step:774/1775 train_time:30369ms step_avg:39.24ms
step:775/1775 train_time:30427ms step_avg:39.26ms
step:776/1775 train_time:30488ms step_avg:39.29ms
step:777/1775 train_time:30546ms step_avg:39.31ms
step:778/1775 train_time:30606ms step_avg:39.34ms
step:779/1775 train_time:30663ms step_avg:39.36ms
step:780/1775 train_time:30723ms step_avg:39.39ms
step:781/1775 train_time:30780ms step_avg:39.41ms
step:782/1775 train_time:30840ms step_avg:39.44ms
step:783/1775 train_time:30897ms step_avg:39.46ms
step:784/1775 train_time:30958ms step_avg:39.49ms
step:785/1775 train_time:31016ms step_avg:39.51ms
step:786/1775 train_time:31078ms step_avg:39.54ms
step:787/1775 train_time:31136ms step_avg:39.56ms
step:788/1775 train_time:31196ms step_avg:39.59ms
step:789/1775 train_time:31255ms step_avg:39.61ms
step:790/1775 train_time:31314ms step_avg:39.64ms
step:791/1775 train_time:31372ms step_avg:39.66ms
step:792/1775 train_time:31432ms step_avg:39.69ms
step:793/1775 train_time:31489ms step_avg:39.71ms
step:794/1775 train_time:31549ms step_avg:39.73ms
step:795/1775 train_time:31609ms step_avg:39.76ms
step:796/1775 train_time:31668ms step_avg:39.78ms
step:797/1775 train_time:31726ms step_avg:39.81ms
step:798/1775 train_time:31786ms step_avg:39.83ms
step:799/1775 train_time:31843ms step_avg:39.85ms
step:800/1775 train_time:31903ms step_avg:39.88ms
step:801/1775 train_time:31961ms step_avg:39.90ms
step:802/1775 train_time:32023ms step_avg:39.93ms
step:803/1775 train_time:32082ms step_avg:39.95ms
step:804/1775 train_time:32143ms step_avg:39.98ms
step:805/1775 train_time:32202ms step_avg:40.00ms
step:806/1775 train_time:32264ms step_avg:40.03ms
step:807/1775 train_time:32321ms step_avg:40.05ms
step:808/1775 train_time:32382ms step_avg:40.08ms
step:809/1775 train_time:32440ms step_avg:40.10ms
step:810/1775 train_time:32501ms step_avg:40.12ms
step:811/1775 train_time:32559ms step_avg:40.15ms
step:812/1775 train_time:32619ms step_avg:40.17ms
step:813/1775 train_time:32676ms step_avg:40.19ms
step:814/1775 train_time:32737ms step_avg:40.22ms
step:815/1775 train_time:32794ms step_avg:40.24ms
step:816/1775 train_time:32854ms step_avg:40.26ms
step:817/1775 train_time:32912ms step_avg:40.28ms
step:818/1775 train_time:32973ms step_avg:40.31ms
step:819/1775 train_time:33030ms step_avg:40.33ms
step:820/1775 train_time:33091ms step_avg:40.35ms
step:821/1775 train_time:33150ms step_avg:40.38ms
step:822/1775 train_time:33211ms step_avg:40.40ms
step:823/1775 train_time:33270ms step_avg:40.42ms
step:824/1775 train_time:33329ms step_avg:40.45ms
step:825/1775 train_time:33388ms step_avg:40.47ms
step:826/1775 train_time:33449ms step_avg:40.49ms
step:827/1775 train_time:33507ms step_avg:40.52ms
step:828/1775 train_time:33568ms step_avg:40.54ms
step:829/1775 train_time:33625ms step_avg:40.56ms
step:830/1775 train_time:33685ms step_avg:40.58ms
step:831/1775 train_time:33743ms step_avg:40.61ms
step:832/1775 train_time:33804ms step_avg:40.63ms
step:833/1775 train_time:33862ms step_avg:40.65ms
step:834/1775 train_time:33922ms step_avg:40.67ms
step:835/1775 train_time:33980ms step_avg:40.69ms
step:836/1775 train_time:34041ms step_avg:40.72ms
step:837/1775 train_time:34099ms step_avg:40.74ms
step:838/1775 train_time:34162ms step_avg:40.77ms
step:839/1775 train_time:34219ms step_avg:40.79ms
step:840/1775 train_time:34280ms step_avg:40.81ms
step:841/1775 train_time:34338ms step_avg:40.83ms
step:842/1775 train_time:34399ms step_avg:40.85ms
step:843/1775 train_time:34456ms step_avg:40.87ms
step:844/1775 train_time:34517ms step_avg:40.90ms
step:845/1775 train_time:34575ms step_avg:40.92ms
step:846/1775 train_time:34634ms step_avg:40.94ms
step:847/1775 train_time:34692ms step_avg:40.96ms
step:848/1775 train_time:34753ms step_avg:40.98ms
step:849/1775 train_time:34810ms step_avg:41.00ms
step:850/1775 train_time:34871ms step_avg:41.02ms
step:851/1775 train_time:34930ms step_avg:41.05ms
step:852/1775 train_time:34990ms step_avg:41.07ms
step:853/1775 train_time:35048ms step_avg:41.09ms
step:854/1775 train_time:35109ms step_avg:41.11ms
step:855/1775 train_time:35167ms step_avg:41.13ms
step:856/1775 train_time:35228ms step_avg:41.15ms
step:857/1775 train_time:35287ms step_avg:41.17ms
step:858/1775 train_time:35347ms step_avg:41.20ms
step:859/1775 train_time:35406ms step_avg:41.22ms
step:860/1775 train_time:35466ms step_avg:41.24ms
step:861/1775 train_time:35524ms step_avg:41.26ms
step:862/1775 train_time:35584ms step_avg:41.28ms
step:863/1775 train_time:35642ms step_avg:41.30ms
step:864/1775 train_time:35703ms step_avg:41.32ms
step:865/1775 train_time:35761ms step_avg:41.34ms
step:866/1775 train_time:35821ms step_avg:41.36ms
step:867/1775 train_time:35879ms step_avg:41.38ms
step:868/1775 train_time:35939ms step_avg:41.40ms
step:869/1775 train_time:35997ms step_avg:41.42ms
step:870/1775 train_time:36058ms step_avg:41.45ms
step:871/1775 train_time:36116ms step_avg:41.47ms
step:872/1775 train_time:36176ms step_avg:41.49ms
step:873/1775 train_time:36234ms step_avg:41.50ms
step:874/1775 train_time:36294ms step_avg:41.53ms
step:875/1775 train_time:36353ms step_avg:41.55ms
step:876/1775 train_time:36414ms step_avg:41.57ms
step:877/1775 train_time:36472ms step_avg:41.59ms
step:878/1775 train_time:36533ms step_avg:41.61ms
step:879/1775 train_time:36591ms step_avg:41.63ms
step:880/1775 train_time:36652ms step_avg:41.65ms
step:881/1775 train_time:36710ms step_avg:41.67ms
step:882/1775 train_time:36771ms step_avg:41.69ms
step:883/1775 train_time:36829ms step_avg:41.71ms
step:884/1775 train_time:36889ms step_avg:41.73ms
step:885/1775 train_time:36948ms step_avg:41.75ms
step:886/1775 train_time:37008ms step_avg:41.77ms
step:887/1775 train_time:37067ms step_avg:41.79ms
step:888/1775 train_time:37127ms step_avg:41.81ms
step:889/1775 train_time:37185ms step_avg:41.83ms
step:890/1775 train_time:37246ms step_avg:41.85ms
step:891/1775 train_time:37305ms step_avg:41.87ms
step:892/1775 train_time:37366ms step_avg:41.89ms
step:893/1775 train_time:37424ms step_avg:41.91ms
step:894/1775 train_time:37483ms step_avg:41.93ms
step:895/1775 train_time:37541ms step_avg:41.95ms
step:896/1775 train_time:37603ms step_avg:41.97ms
step:897/1775 train_time:37661ms step_avg:41.99ms
step:898/1775 train_time:37722ms step_avg:42.01ms
step:899/1775 train_time:37780ms step_avg:42.02ms
step:900/1775 train_time:37839ms step_avg:42.04ms
step:901/1775 train_time:37897ms step_avg:42.06ms
step:902/1775 train_time:37958ms step_avg:42.08ms
step:903/1775 train_time:38016ms step_avg:42.10ms
step:904/1775 train_time:38076ms step_avg:42.12ms
step:905/1775 train_time:38133ms step_avg:42.14ms
step:906/1775 train_time:38193ms step_avg:42.16ms
step:907/1775 train_time:38252ms step_avg:42.17ms
step:908/1775 train_time:38313ms step_avg:42.19ms
step:909/1775 train_time:38371ms step_avg:42.21ms
step:910/1775 train_time:38432ms step_avg:42.23ms
step:911/1775 train_time:38490ms step_avg:42.25ms
step:912/1775 train_time:38552ms step_avg:42.27ms
step:913/1775 train_time:38609ms step_avg:42.29ms
step:914/1775 train_time:38670ms step_avg:42.31ms
step:915/1775 train_time:38729ms step_avg:42.33ms
step:916/1775 train_time:38789ms step_avg:42.35ms
step:917/1775 train_time:38847ms step_avg:42.36ms
step:918/1775 train_time:38907ms step_avg:42.38ms
step:919/1775 train_time:38965ms step_avg:42.40ms
step:920/1775 train_time:39027ms step_avg:42.42ms
step:921/1775 train_time:39085ms step_avg:42.44ms
step:922/1775 train_time:39146ms step_avg:42.46ms
step:923/1775 train_time:39204ms step_avg:42.47ms
step:924/1775 train_time:39265ms step_avg:42.49ms
step:925/1775 train_time:39324ms step_avg:42.51ms
step:926/1775 train_time:39384ms step_avg:42.53ms
step:927/1775 train_time:39442ms step_avg:42.55ms
step:928/1775 train_time:39502ms step_avg:42.57ms
step:929/1775 train_time:39560ms step_avg:42.58ms
step:930/1775 train_time:39620ms step_avg:42.60ms
step:931/1775 train_time:39679ms step_avg:42.62ms
step:932/1775 train_time:39738ms step_avg:42.64ms
step:933/1775 train_time:39796ms step_avg:42.65ms
step:934/1775 train_time:39856ms step_avg:42.67ms
step:935/1775 train_time:39915ms step_avg:42.69ms
step:936/1775 train_time:39975ms step_avg:42.71ms
step:937/1775 train_time:40033ms step_avg:42.72ms
step:938/1775 train_time:40093ms step_avg:42.74ms
step:939/1775 train_time:40151ms step_avg:42.76ms
step:940/1775 train_time:40212ms step_avg:42.78ms
step:941/1775 train_time:40270ms step_avg:42.79ms
step:942/1775 train_time:40330ms step_avg:42.81ms
step:943/1775 train_time:40389ms step_avg:42.83ms
step:944/1775 train_time:40450ms step_avg:42.85ms
step:945/1775 train_time:40508ms step_avg:42.87ms
step:946/1775 train_time:40568ms step_avg:42.88ms
step:947/1775 train_time:40627ms step_avg:42.90ms
step:948/1775 train_time:40687ms step_avg:42.92ms
step:949/1775 train_time:40745ms step_avg:42.93ms
step:950/1775 train_time:40806ms step_avg:42.95ms
step:951/1775 train_time:40865ms step_avg:42.97ms
step:952/1775 train_time:40926ms step_avg:42.99ms
step:953/1775 train_time:40985ms step_avg:43.01ms
step:954/1775 train_time:41045ms step_avg:43.02ms
step:955/1775 train_time:41102ms step_avg:43.04ms
step:956/1775 train_time:41163ms step_avg:43.06ms
step:957/1775 train_time:41221ms step_avg:43.07ms
step:958/1775 train_time:41282ms step_avg:43.09ms
step:959/1775 train_time:41339ms step_avg:43.11ms
step:960/1775 train_time:41400ms step_avg:43.13ms
step:961/1775 train_time:41458ms step_avg:43.14ms
step:962/1775 train_time:41517ms step_avg:43.16ms
step:963/1775 train_time:41576ms step_avg:43.17ms
step:964/1775 train_time:41635ms step_avg:43.19ms
step:965/1775 train_time:41693ms step_avg:43.20ms
step:966/1775 train_time:41754ms step_avg:43.22ms
step:967/1775 train_time:41812ms step_avg:43.24ms
step:968/1775 train_time:41873ms step_avg:43.26ms
step:969/1775 train_time:41931ms step_avg:43.27ms
step:970/1775 train_time:41991ms step_avg:43.29ms
step:971/1775 train_time:42049ms step_avg:43.31ms
step:972/1775 train_time:42110ms step_avg:43.32ms
step:973/1775 train_time:42169ms step_avg:43.34ms
step:974/1775 train_time:42229ms step_avg:43.36ms
step:975/1775 train_time:42287ms step_avg:43.37ms
step:976/1775 train_time:42348ms step_avg:43.39ms
step:977/1775 train_time:42407ms step_avg:43.41ms
step:978/1775 train_time:42468ms step_avg:43.42ms
step:979/1775 train_time:42526ms step_avg:43.44ms
step:980/1775 train_time:42586ms step_avg:43.46ms
step:981/1775 train_time:42644ms step_avg:43.47ms
step:982/1775 train_time:42705ms step_avg:43.49ms
step:983/1775 train_time:42763ms step_avg:43.50ms
step:984/1775 train_time:42824ms step_avg:43.52ms
step:985/1775 train_time:42883ms step_avg:43.54ms
step:986/1775 train_time:42943ms step_avg:43.55ms
step:987/1775 train_time:43001ms step_avg:43.57ms
step:988/1775 train_time:43062ms step_avg:43.59ms
step:989/1775 train_time:43120ms step_avg:43.60ms
step:990/1775 train_time:43181ms step_avg:43.62ms
step:991/1775 train_time:43239ms step_avg:43.63ms
step:992/1775 train_time:43299ms step_avg:43.65ms
step:993/1775 train_time:43357ms step_avg:43.66ms
step:994/1775 train_time:43417ms step_avg:43.68ms
step:995/1775 train_time:43475ms step_avg:43.69ms
step:996/1775 train_time:43535ms step_avg:43.71ms
step:997/1775 train_time:43594ms step_avg:43.73ms
step:998/1775 train_time:43654ms step_avg:43.74ms
step:999/1775 train_time:43712ms step_avg:43.76ms
step:1000/1775 train_time:43773ms step_avg:43.77ms
step:1000/1775 val_loss:3.7360 train_time:43842ms step_avg:43.84ms
step:1001/1775 train_time:43863ms step_avg:43.82ms
step:1002/1775 train_time:43892ms step_avg:43.80ms
step:1003/1775 train_time:43952ms step_avg:43.82ms
step:1004/1775 train_time:44015ms step_avg:43.84ms
step:1005/1775 train_time:44073ms step_avg:43.85ms
step:1006/1775 train_time:44132ms step_avg:43.87ms
step:1007/1775 train_time:44191ms step_avg:43.88ms
step:1008/1775 train_time:44250ms step_avg:43.90ms
step:1009/1775 train_time:44307ms step_avg:43.91ms
step:1010/1775 train_time:44367ms step_avg:43.93ms
step:1011/1775 train_time:44424ms step_avg:43.94ms
step:1012/1775 train_time:44485ms step_avg:43.96ms
step:1013/1775 train_time:44542ms step_avg:43.97ms
step:1014/1775 train_time:44602ms step_avg:43.99ms
step:1015/1775 train_time:44660ms step_avg:44.00ms
step:1016/1775 train_time:44720ms step_avg:44.02ms
step:1017/1775 train_time:44777ms step_avg:44.03ms
step:1018/1775 train_time:44841ms step_avg:44.05ms
step:1019/1775 train_time:44900ms step_avg:44.06ms
step:1020/1775 train_time:44963ms step_avg:44.08ms
step:1021/1775 train_time:45024ms step_avg:44.10ms
step:1022/1775 train_time:45086ms step_avg:44.12ms
step:1023/1775 train_time:45145ms step_avg:44.13ms
step:1024/1775 train_time:45207ms step_avg:44.15ms
step:1025/1775 train_time:45264ms step_avg:44.16ms
step:1026/1775 train_time:45324ms step_avg:44.18ms
step:1027/1775 train_time:45383ms step_avg:44.19ms
step:1028/1775 train_time:45443ms step_avg:44.21ms
step:1029/1775 train_time:45500ms step_avg:44.22ms
step:1030/1775 train_time:45560ms step_avg:44.23ms
step:1031/1775 train_time:45617ms step_avg:44.25ms
step:1032/1775 train_time:45677ms step_avg:44.26ms
step:1033/1775 train_time:45734ms step_avg:44.27ms
step:1034/1775 train_time:45795ms step_avg:44.29ms
step:1035/1775 train_time:45853ms step_avg:44.30ms
step:1036/1775 train_time:45913ms step_avg:44.32ms
step:1037/1775 train_time:45971ms step_avg:44.33ms
step:1038/1775 train_time:46032ms step_avg:44.35ms
step:1039/1775 train_time:46091ms step_avg:44.36ms
step:1040/1775 train_time:46151ms step_avg:44.38ms
step:1041/1775 train_time:46209ms step_avg:44.39ms
step:1042/1775 train_time:46269ms step_avg:44.40ms
step:1043/1775 train_time:46326ms step_avg:44.42ms
step:1044/1775 train_time:46387ms step_avg:44.43ms
step:1045/1775 train_time:46445ms step_avg:44.44ms
step:1046/1775 train_time:46505ms step_avg:44.46ms
step:1047/1775 train_time:46564ms step_avg:44.47ms
step:1048/1775 train_time:46624ms step_avg:44.49ms
step:1049/1775 train_time:46683ms step_avg:44.50ms
step:1050/1775 train_time:46744ms step_avg:44.52ms
step:1051/1775 train_time:46803ms step_avg:44.53ms
step:1052/1775 train_time:46863ms step_avg:44.55ms
step:1053/1775 train_time:46922ms step_avg:44.56ms
step:1054/1775 train_time:46983ms step_avg:44.58ms
step:1055/1775 train_time:47041ms step_avg:44.59ms
step:1056/1775 train_time:47102ms step_avg:44.60ms
step:1057/1775 train_time:47161ms step_avg:44.62ms
step:1058/1775 train_time:47222ms step_avg:44.63ms
step:1059/1775 train_time:47280ms step_avg:44.65ms
step:1060/1775 train_time:47340ms step_avg:44.66ms
step:1061/1775 train_time:47398ms step_avg:44.67ms
step:1062/1775 train_time:47458ms step_avg:44.69ms
step:1063/1775 train_time:47516ms step_avg:44.70ms
step:1064/1775 train_time:47576ms step_avg:44.71ms
step:1065/1775 train_time:47634ms step_avg:44.73ms
step:1066/1775 train_time:47694ms step_avg:44.74ms
step:1067/1775 train_time:47751ms step_avg:44.75ms
step:1068/1775 train_time:47812ms step_avg:44.77ms
step:1069/1775 train_time:47870ms step_avg:44.78ms
step:1070/1775 train_time:47930ms step_avg:44.79ms
step:1071/1775 train_time:47988ms step_avg:44.81ms
step:1072/1775 train_time:48049ms step_avg:44.82ms
step:1073/1775 train_time:48108ms step_avg:44.84ms
step:1074/1775 train_time:48167ms step_avg:44.85ms
step:1075/1775 train_time:48226ms step_avg:44.86ms
step:1076/1775 train_time:48286ms step_avg:44.88ms
step:1077/1775 train_time:48344ms step_avg:44.89ms
step:1078/1775 train_time:48405ms step_avg:44.90ms
step:1079/1775 train_time:48463ms step_avg:44.92ms
step:1080/1775 train_time:48523ms step_avg:44.93ms
step:1081/1775 train_time:48581ms step_avg:44.94ms
step:1082/1775 train_time:48641ms step_avg:44.95ms
step:1083/1775 train_time:48699ms step_avg:44.97ms
step:1084/1775 train_time:48759ms step_avg:44.98ms
step:1085/1775 train_time:48818ms step_avg:44.99ms
step:1086/1775 train_time:48879ms step_avg:45.01ms
step:1087/1775 train_time:48937ms step_avg:45.02ms
step:1088/1775 train_time:48998ms step_avg:45.04ms
step:1089/1775 train_time:49056ms step_avg:45.05ms
step:1090/1775 train_time:49117ms step_avg:45.06ms
step:1091/1775 train_time:49174ms step_avg:45.07ms
step:1092/1775 train_time:49235ms step_avg:45.09ms
step:1093/1775 train_time:49293ms step_avg:45.10ms
step:1094/1775 train_time:49354ms step_avg:45.11ms
step:1095/1775 train_time:49412ms step_avg:45.12ms
step:1096/1775 train_time:49472ms step_avg:45.14ms
step:1097/1775 train_time:49530ms step_avg:45.15ms
step:1098/1775 train_time:49591ms step_avg:45.17ms
step:1099/1775 train_time:49650ms step_avg:45.18ms
step:1100/1775 train_time:49710ms step_avg:45.19ms
step:1101/1775 train_time:49767ms step_avg:45.20ms
step:1102/1775 train_time:49827ms step_avg:45.22ms
step:1103/1775 train_time:49885ms step_avg:45.23ms
step:1104/1775 train_time:49945ms step_avg:45.24ms
step:1105/1775 train_time:50005ms step_avg:45.25ms
step:1106/1775 train_time:50064ms step_avg:45.27ms
step:1107/1775 train_time:50123ms step_avg:45.28ms
step:1108/1775 train_time:50184ms step_avg:45.29ms
step:1109/1775 train_time:50242ms step_avg:45.30ms
step:1110/1775 train_time:50303ms step_avg:45.32ms
step:1111/1775 train_time:50361ms step_avg:45.33ms
step:1112/1775 train_time:50422ms step_avg:45.34ms
step:1113/1775 train_time:50480ms step_avg:45.36ms
step:1114/1775 train_time:50541ms step_avg:45.37ms
step:1115/1775 train_time:50598ms step_avg:45.38ms
step:1116/1775 train_time:50659ms step_avg:45.39ms
step:1117/1775 train_time:50716ms step_avg:45.40ms
step:1118/1775 train_time:50776ms step_avg:45.42ms
step:1119/1775 train_time:50834ms step_avg:45.43ms
step:1120/1775 train_time:50895ms step_avg:45.44ms
step:1121/1775 train_time:50953ms step_avg:45.45ms
step:1122/1775 train_time:51013ms step_avg:45.47ms
step:1123/1775 train_time:51070ms step_avg:45.48ms
step:1124/1775 train_time:51130ms step_avg:45.49ms
step:1125/1775 train_time:51189ms step_avg:45.50ms
step:1126/1775 train_time:51249ms step_avg:45.51ms
step:1127/1775 train_time:51307ms step_avg:45.53ms
step:1128/1775 train_time:51367ms step_avg:45.54ms
step:1129/1775 train_time:51425ms step_avg:45.55ms
step:1130/1775 train_time:51487ms step_avg:45.56ms
step:1131/1775 train_time:51545ms step_avg:45.57ms
step:1132/1775 train_time:51607ms step_avg:45.59ms
step:1133/1775 train_time:51665ms step_avg:45.60ms
step:1134/1775 train_time:51725ms step_avg:45.61ms
step:1135/1775 train_time:51784ms step_avg:45.62ms
step:1136/1775 train_time:51844ms step_avg:45.64ms
step:1137/1775 train_time:51902ms step_avg:45.65ms
step:1138/1775 train_time:51962ms step_avg:45.66ms
step:1139/1775 train_time:52020ms step_avg:45.67ms
step:1140/1775 train_time:52080ms step_avg:45.68ms
step:1141/1775 train_time:52138ms step_avg:45.70ms
step:1142/1775 train_time:52199ms step_avg:45.71ms
step:1143/1775 train_time:52257ms step_avg:45.72ms
step:1144/1775 train_time:52318ms step_avg:45.73ms
step:1145/1775 train_time:52376ms step_avg:45.74ms
step:1146/1775 train_time:52437ms step_avg:45.76ms
step:1147/1775 train_time:52495ms step_avg:45.77ms
step:1148/1775 train_time:52556ms step_avg:45.78ms
step:1149/1775 train_time:52613ms step_avg:45.79ms
step:1150/1775 train_time:52673ms step_avg:45.80ms
step:1151/1775 train_time:52730ms step_avg:45.81ms
step:1152/1775 train_time:52791ms step_avg:45.83ms
step:1153/1775 train_time:52848ms step_avg:45.84ms
step:1154/1775 train_time:52910ms step_avg:45.85ms
step:1155/1775 train_time:52967ms step_avg:45.86ms
step:1156/1775 train_time:53028ms step_avg:45.87ms
step:1157/1775 train_time:53086ms step_avg:45.88ms
step:1158/1775 train_time:53150ms step_avg:45.90ms
step:1159/1775 train_time:53234ms step_avg:45.93ms
step:1160/1775 train_time:53320ms step_avg:45.97ms
step:1161/1775 train_time:53403ms step_avg:46.00ms
step:1162/1775 train_time:53490ms step_avg:46.03ms
step:1163/1775 train_time:53575ms step_avg:46.07ms
step:1164/1775 train_time:53662ms step_avg:46.10ms
step:1165/1775 train_time:53746ms step_avg:46.13ms
step:1166/1775 train_time:53832ms step_avg:46.17ms
step:1167/1775 train_time:53916ms step_avg:46.20ms
step:1168/1775 train_time:54002ms step_avg:46.23ms
step:1169/1775 train_time:54086ms step_avg:46.27ms
step:1170/1775 train_time:54173ms step_avg:46.30ms
step:1171/1775 train_time:54257ms step_avg:46.33ms
step:1172/1775 train_time:54342ms step_avg:46.37ms
step:1173/1775 train_time:54425ms step_avg:46.40ms
step:1174/1775 train_time:54512ms step_avg:46.43ms
step:1175/1775 train_time:54595ms step_avg:46.46ms
step:1176/1775 train_time:54682ms step_avg:46.50ms
step:1177/1775 train_time:54765ms step_avg:46.53ms
step:1178/1775 train_time:54852ms step_avg:46.56ms
step:1179/1775 train_time:54937ms step_avg:46.60ms
step:1180/1775 train_time:55022ms step_avg:46.63ms
step:1181/1775 train_time:55105ms step_avg:46.66ms
step:1182/1775 train_time:55192ms step_avg:46.69ms
step:1183/1775 train_time:55276ms step_avg:46.73ms
step:1184/1775 train_time:55363ms step_avg:46.76ms
step:1185/1775 train_time:55447ms step_avg:46.79ms
step:1186/1775 train_time:55534ms step_avg:46.82ms
step:1187/1775 train_time:55618ms step_avg:46.86ms
step:1188/1775 train_time:55703ms step_avg:46.89ms
step:1189/1775 train_time:55787ms step_avg:46.92ms
step:1190/1775 train_time:55874ms step_avg:46.95ms
step:1191/1775 train_time:55958ms step_avg:46.98ms
step:1192/1775 train_time:56045ms step_avg:47.02ms
step:1193/1775 train_time:56127ms step_avg:47.05ms
step:1194/1775 train_time:56214ms step_avg:47.08ms
step:1195/1775 train_time:56299ms step_avg:47.11ms
step:1196/1775 train_time:56384ms step_avg:47.14ms
step:1197/1775 train_time:56469ms step_avg:47.18ms
step:1198/1775 train_time:56554ms step_avg:47.21ms
step:1199/1775 train_time:56639ms step_avg:47.24ms
step:1200/1775 train_time:56724ms step_avg:47.27ms
step:1201/1775 train_time:56808ms step_avg:47.30ms
step:1202/1775 train_time:56894ms step_avg:47.33ms
step:1203/1775 train_time:56980ms step_avg:47.36ms
step:1204/1775 train_time:57066ms step_avg:47.40ms
step:1205/1775 train_time:57149ms step_avg:47.43ms
step:1206/1775 train_time:57235ms step_avg:47.46ms
step:1207/1775 train_time:57319ms step_avg:47.49ms
step:1208/1775 train_time:57404ms step_avg:47.52ms
step:1209/1775 train_time:57488ms step_avg:47.55ms
step:1210/1775 train_time:57577ms step_avg:47.58ms
step:1211/1775 train_time:57661ms step_avg:47.61ms
step:1212/1775 train_time:57747ms step_avg:47.65ms
step:1213/1775 train_time:57829ms step_avg:47.67ms
step:1214/1775 train_time:57915ms step_avg:47.71ms
step:1215/1775 train_time:57999ms step_avg:47.74ms
step:1216/1775 train_time:58084ms step_avg:47.77ms
step:1217/1775 train_time:58170ms step_avg:47.80ms
step:1218/1775 train_time:58255ms step_avg:47.83ms
step:1219/1775 train_time:58341ms step_avg:47.86ms
step:1220/1775 train_time:58426ms step_avg:47.89ms
step:1221/1775 train_time:58509ms step_avg:47.92ms
step:1222/1775 train_time:58595ms step_avg:47.95ms
step:1223/1775 train_time:58680ms step_avg:47.98ms
step:1224/1775 train_time:58766ms step_avg:48.01ms
step:1225/1775 train_time:58850ms step_avg:48.04ms
step:1226/1775 train_time:58937ms step_avg:48.07ms
step:1227/1775 train_time:59020ms step_avg:48.10ms
step:1228/1775 train_time:59105ms step_avg:48.13ms
step:1229/1775 train_time:59189ms step_avg:48.16ms
step:1230/1775 train_time:59275ms step_avg:48.19ms
step:1231/1775 train_time:59360ms step_avg:48.22ms
step:1232/1775 train_time:59446ms step_avg:48.25ms
step:1233/1775 train_time:59531ms step_avg:48.28ms
step:1234/1775 train_time:59618ms step_avg:48.31ms
step:1235/1775 train_time:59702ms step_avg:48.34ms
step:1236/1775 train_time:59789ms step_avg:48.37ms
step:1237/1775 train_time:59871ms step_avg:48.40ms
step:1238/1775 train_time:59958ms step_avg:48.43ms
step:1239/1775 train_time:60041ms step_avg:48.46ms
step:1240/1775 train_time:60128ms step_avg:48.49ms
step:1241/1775 train_time:60212ms step_avg:48.52ms
step:1242/1775 train_time:60298ms step_avg:48.55ms
step:1243/1775 train_time:60381ms step_avg:48.58ms
step:1244/1775 train_time:60467ms step_avg:48.61ms
step:1245/1775 train_time:60551ms step_avg:48.64ms
step:1246/1775 train_time:60639ms step_avg:48.67ms
step:1247/1775 train_time:60721ms step_avg:48.69ms
step:1248/1775 train_time:60807ms step_avg:48.72ms
step:1249/1775 train_time:60892ms step_avg:48.75ms
step:1250/1775 train_time:60979ms step_avg:48.78ms
step:1250/1775 val_loss:3.5064 train_time:61077ms step_avg:48.86ms
step:1251/1775 train_time:61097ms step_avg:48.84ms
step:1252/1775 train_time:61150ms step_avg:48.84ms
step:1253/1775 train_time:61236ms step_avg:48.87ms
step:1254/1775 train_time:61324ms step_avg:48.90ms
step:1255/1775 train_time:61408ms step_avg:48.93ms
step:1256/1775 train_time:61495ms step_avg:48.96ms
step:1257/1775 train_time:61577ms step_avg:48.99ms
step:1258/1775 train_time:61663ms step_avg:49.02ms
step:1259/1775 train_time:61745ms step_avg:49.04ms
step:1260/1775 train_time:61831ms step_avg:49.07ms
step:1261/1775 train_time:61915ms step_avg:49.10ms
step:1262/1775 train_time:62001ms step_avg:49.13ms
step:1263/1775 train_time:62088ms step_avg:49.16ms
step:1264/1775 train_time:62176ms step_avg:49.19ms
step:1265/1775 train_time:62260ms step_avg:49.22ms
step:1266/1775 train_time:62348ms step_avg:49.25ms
step:1267/1775 train_time:62432ms step_avg:49.28ms
step:1268/1775 train_time:62518ms step_avg:49.30ms
step:1269/1775 train_time:62600ms step_avg:49.33ms
step:1270/1775 train_time:62686ms step_avg:49.36ms
step:1271/1775 train_time:62770ms step_avg:49.39ms
step:1272/1775 train_time:62856ms step_avg:49.41ms
step:1273/1775 train_time:62939ms step_avg:49.44ms
step:1274/1775 train_time:63026ms step_avg:49.47ms
step:1275/1775 train_time:63111ms step_avg:49.50ms
step:1276/1775 train_time:63198ms step_avg:49.53ms
step:1277/1775 train_time:63282ms step_avg:49.56ms
step:1278/1775 train_time:63369ms step_avg:49.58ms
step:1279/1775 train_time:63453ms step_avg:49.61ms
step:1280/1775 train_time:63539ms step_avg:49.64ms
step:1281/1775 train_time:63623ms step_avg:49.67ms
step:1282/1775 train_time:63708ms step_avg:49.69ms
step:1283/1775 train_time:63791ms step_avg:49.72ms
step:1284/1775 train_time:63878ms step_avg:49.75ms
step:1285/1775 train_time:63961ms step_avg:49.78ms
step:1286/1775 train_time:64049ms step_avg:49.80ms
step:1287/1775 train_time:64132ms step_avg:49.83ms
step:1288/1775 train_time:64220ms step_avg:49.86ms
step:1289/1775 train_time:64303ms step_avg:49.89ms
step:1290/1775 train_time:64391ms step_avg:49.92ms
step:1291/1775 train_time:64476ms step_avg:49.94ms
step:1292/1775 train_time:64560ms step_avg:49.97ms
step:1293/1775 train_time:64644ms step_avg:50.00ms
step:1294/1775 train_time:64731ms step_avg:50.02ms
step:1295/1775 train_time:64814ms step_avg:50.05ms
step:1296/1775 train_time:64900ms step_avg:50.08ms
step:1297/1775 train_time:64982ms step_avg:50.10ms
step:1298/1775 train_time:65070ms step_avg:50.13ms
step:1299/1775 train_time:65154ms step_avg:50.16ms
step:1300/1775 train_time:65241ms step_avg:50.19ms
step:1301/1775 train_time:65325ms step_avg:50.21ms
step:1302/1775 train_time:65413ms step_avg:50.24ms
step:1303/1775 train_time:65496ms step_avg:50.27ms
step:1304/1775 train_time:65582ms step_avg:50.29ms
step:1305/1775 train_time:65666ms step_avg:50.32ms
step:1306/1775 train_time:65753ms step_avg:50.35ms
step:1307/1775 train_time:65837ms step_avg:50.37ms
step:1308/1775 train_time:65921ms step_avg:50.40ms
step:1309/1775 train_time:66005ms step_avg:50.42ms
step:1310/1775 train_time:66092ms step_avg:50.45ms
step:1311/1775 train_time:66177ms step_avg:50.48ms
step:1312/1775 train_time:66264ms step_avg:50.51ms
step:1313/1775 train_time:66348ms step_avg:50.53ms
step:1314/1775 train_time:66434ms step_avg:50.56ms
step:1315/1775 train_time:66518ms step_avg:50.58ms
step:1316/1775 train_time:66604ms step_avg:50.61ms
step:1317/1775 train_time:66687ms step_avg:50.64ms
step:1318/1775 train_time:66776ms step_avg:50.66ms
step:1319/1775 train_time:66860ms step_avg:50.69ms
step:1320/1775 train_time:66945ms step_avg:50.72ms
step:1321/1775 train_time:67028ms step_avg:50.74ms
step:1322/1775 train_time:67114ms step_avg:50.77ms
step:1323/1775 train_time:67199ms step_avg:50.79ms
step:1324/1775 train_time:67286ms step_avg:50.82ms
step:1325/1775 train_time:67369ms step_avg:50.84ms
step:1326/1775 train_time:67457ms step_avg:50.87ms
step:1327/1775 train_time:67539ms step_avg:50.90ms
step:1328/1775 train_time:67625ms step_avg:50.92ms
step:1329/1775 train_time:67708ms step_avg:50.95ms
step:1330/1775 train_time:67796ms step_avg:50.97ms
step:1331/1775 train_time:67879ms step_avg:51.00ms
step:1332/1775 train_time:67966ms step_avg:51.03ms
step:1333/1775 train_time:68049ms step_avg:51.05ms
step:1334/1775 train_time:68136ms step_avg:51.08ms
step:1335/1775 train_time:68219ms step_avg:51.10ms
step:1336/1775 train_time:68306ms step_avg:51.13ms
step:1337/1775 train_time:68390ms step_avg:51.15ms
step:1338/1775 train_time:68477ms step_avg:51.18ms
step:1339/1775 train_time:68560ms step_avg:51.20ms
step:1340/1775 train_time:68646ms step_avg:51.23ms
step:1341/1775 train_time:68730ms step_avg:51.25ms
step:1342/1775 train_time:68817ms step_avg:51.28ms
step:1343/1775 train_time:68900ms step_avg:51.30ms
step:1344/1775 train_time:68986ms step_avg:51.33ms
step:1345/1775 train_time:69070ms step_avg:51.35ms
step:1346/1775 train_time:69156ms step_avg:51.38ms
step:1347/1775 train_time:69239ms step_avg:51.40ms
step:1348/1775 train_time:69326ms step_avg:51.43ms
step:1349/1775 train_time:69410ms step_avg:51.45ms
step:1350/1775 train_time:69496ms step_avg:51.48ms
step:1351/1775 train_time:69580ms step_avg:51.50ms
step:1352/1775 train_time:69666ms step_avg:51.53ms
step:1353/1775 train_time:69749ms step_avg:51.55ms
step:1354/1775 train_time:69835ms step_avg:51.58ms
step:1355/1775 train_time:69919ms step_avg:51.60ms
step:1356/1775 train_time:70006ms step_avg:51.63ms
step:1357/1775 train_time:70089ms step_avg:51.65ms
step:1358/1775 train_time:70177ms step_avg:51.68ms
step:1359/1775 train_time:70260ms step_avg:51.70ms
step:1360/1775 train_time:70346ms step_avg:51.72ms
step:1361/1775 train_time:70429ms step_avg:51.75ms
step:1362/1775 train_time:70516ms step_avg:51.77ms
step:1363/1775 train_time:70599ms step_avg:51.80ms
step:1364/1775 train_time:70684ms step_avg:51.82ms
step:1365/1775 train_time:70768ms step_avg:51.84ms
step:1366/1775 train_time:70855ms step_avg:51.87ms
step:1367/1775 train_time:70938ms step_avg:51.89ms
step:1368/1775 train_time:71025ms step_avg:51.92ms
step:1369/1775 train_time:71109ms step_avg:51.94ms
step:1370/1775 train_time:71195ms step_avg:51.97ms
step:1371/1775 train_time:71279ms step_avg:51.99ms
step:1372/1775 train_time:71366ms step_avg:52.02ms
step:1373/1775 train_time:71450ms step_avg:52.04ms
step:1374/1775 train_time:71536ms step_avg:52.06ms
step:1375/1775 train_time:71619ms step_avg:52.09ms
step:1376/1775 train_time:71705ms step_avg:52.11ms
step:1377/1775 train_time:71791ms step_avg:52.14ms
step:1378/1775 train_time:71876ms step_avg:52.16ms
step:1379/1775 train_time:71960ms step_avg:52.18ms
step:1380/1775 train_time:72046ms step_avg:52.21ms
step:1381/1775 train_time:72129ms step_avg:52.23ms
step:1382/1775 train_time:72216ms step_avg:52.26ms
step:1383/1775 train_time:72300ms step_avg:52.28ms
step:1384/1775 train_time:72387ms step_avg:52.30ms
step:1385/1775 train_time:72471ms step_avg:52.33ms
step:1386/1775 train_time:72558ms step_avg:52.35ms
step:1387/1775 train_time:72640ms step_avg:52.37ms
step:1388/1775 train_time:72726ms step_avg:52.40ms
step:1389/1775 train_time:72810ms step_avg:52.42ms
step:1390/1775 train_time:72898ms step_avg:52.44ms
step:1391/1775 train_time:72981ms step_avg:52.47ms
step:1392/1775 train_time:73067ms step_avg:52.49ms
step:1393/1775 train_time:73152ms step_avg:52.51ms
step:1394/1775 train_time:73238ms step_avg:52.54ms
step:1395/1775 train_time:73321ms step_avg:52.56ms
step:1396/1775 train_time:73406ms step_avg:52.58ms
step:1397/1775 train_time:73491ms step_avg:52.61ms
step:1398/1775 train_time:73577ms step_avg:52.63ms
step:1399/1775 train_time:73660ms step_avg:52.65ms
step:1400/1775 train_time:73747ms step_avg:52.68ms
step:1401/1775 train_time:73829ms step_avg:52.70ms
step:1402/1775 train_time:73917ms step_avg:52.72ms
step:1403/1775 train_time:73999ms step_avg:52.74ms
step:1404/1775 train_time:74085ms step_avg:52.77ms
step:1405/1775 train_time:74169ms step_avg:52.79ms
step:1406/1775 train_time:74257ms step_avg:52.81ms
step:1407/1775 train_time:74339ms step_avg:52.83ms
step:1408/1775 train_time:74426ms step_avg:52.86ms
step:1409/1775 train_time:74510ms step_avg:52.88ms
step:1410/1775 train_time:74597ms step_avg:52.91ms
step:1411/1775 train_time:74681ms step_avg:52.93ms
step:1412/1775 train_time:74767ms step_avg:52.95ms
step:1413/1775 train_time:74850ms step_avg:52.97ms
step:1414/1775 train_time:74937ms step_avg:53.00ms
step:1415/1775 train_time:75020ms step_avg:53.02ms
step:1416/1775 train_time:75107ms step_avg:53.04ms
step:1417/1775 train_time:75191ms step_avg:53.06ms
step:1418/1775 train_time:75278ms step_avg:53.09ms
step:1419/1775 train_time:75362ms step_avg:53.11ms
step:1420/1775 train_time:75448ms step_avg:53.13ms
step:1421/1775 train_time:75531ms step_avg:53.15ms
step:1422/1775 train_time:75618ms step_avg:53.18ms
step:1423/1775 train_time:75701ms step_avg:53.20ms
step:1424/1775 train_time:75786ms step_avg:53.22ms
step:1425/1775 train_time:75870ms step_avg:53.24ms
step:1426/1775 train_time:75957ms step_avg:53.27ms
step:1427/1775 train_time:76040ms step_avg:53.29ms
step:1428/1775 train_time:76127ms step_avg:53.31ms
step:1429/1775 train_time:76211ms step_avg:53.33ms
step:1430/1775 train_time:76299ms step_avg:53.36ms
step:1431/1775 train_time:76381ms step_avg:53.38ms
step:1432/1775 train_time:76467ms step_avg:53.40ms
step:1433/1775 train_time:76552ms step_avg:53.42ms
step:1434/1775 train_time:76638ms step_avg:53.44ms
step:1435/1775 train_time:76721ms step_avg:53.46ms
step:1436/1775 train_time:76808ms step_avg:53.49ms
step:1437/1775 train_time:76890ms step_avg:53.51ms
step:1438/1775 train_time:76977ms step_avg:53.53ms
step:1439/1775 train_time:77061ms step_avg:53.55ms
step:1440/1775 train_time:77147ms step_avg:53.57ms
step:1441/1775 train_time:77231ms step_avg:53.60ms
step:1442/1775 train_time:77317ms step_avg:53.62ms
step:1443/1775 train_time:77400ms step_avg:53.64ms
step:1444/1775 train_time:77487ms step_avg:53.66ms
step:1445/1775 train_time:77571ms step_avg:53.68ms
step:1446/1775 train_time:77658ms step_avg:53.71ms
step:1447/1775 train_time:77740ms step_avg:53.72ms
step:1448/1775 train_time:77827ms step_avg:53.75ms
step:1449/1775 train_time:77910ms step_avg:53.77ms
step:1450/1775 train_time:77996ms step_avg:53.79ms
step:1451/1775 train_time:78080ms step_avg:53.81ms
step:1452/1775 train_time:78167ms step_avg:53.83ms
step:1453/1775 train_time:78251ms step_avg:53.85ms
step:1454/1775 train_time:78337ms step_avg:53.88ms
step:1455/1775 train_time:78420ms step_avg:53.90ms
step:1456/1775 train_time:78507ms step_avg:53.92ms
step:1457/1775 train_time:78590ms step_avg:53.94ms
step:1458/1775 train_time:78677ms step_avg:53.96ms
step:1459/1775 train_time:78760ms step_avg:53.98ms
step:1460/1775 train_time:78847ms step_avg:54.00ms
step:1461/1775 train_time:78930ms step_avg:54.02ms
step:1462/1775 train_time:79017ms step_avg:54.05ms
step:1463/1775 train_time:79100ms step_avg:54.07ms
step:1464/1775 train_time:79186ms step_avg:54.09ms
step:1465/1775 train_time:79271ms step_avg:54.11ms
step:1466/1775 train_time:79357ms step_avg:54.13ms
step:1467/1775 train_time:79440ms step_avg:54.15ms
step:1468/1775 train_time:79527ms step_avg:54.17ms
step:1469/1775 train_time:79610ms step_avg:54.19ms
step:1470/1775 train_time:79697ms step_avg:54.22ms
step:1471/1775 train_time:79780ms step_avg:54.24ms
step:1472/1775 train_time:79866ms step_avg:54.26ms
step:1473/1775 train_time:79950ms step_avg:54.28ms
step:1474/1775 train_time:80036ms step_avg:54.30ms
step:1475/1775 train_time:80119ms step_avg:54.32ms
step:1476/1775 train_time:80205ms step_avg:54.34ms
step:1477/1775 train_time:80289ms step_avg:54.36ms
step:1478/1775 train_time:80376ms step_avg:54.38ms
step:1479/1775 train_time:80459ms step_avg:54.40ms
step:1480/1775 train_time:80545ms step_avg:54.42ms
step:1481/1775 train_time:80629ms step_avg:54.44ms
step:1482/1775 train_time:80717ms step_avg:54.46ms
step:1483/1775 train_time:80800ms step_avg:54.48ms
step:1484/1775 train_time:80886ms step_avg:54.51ms
step:1485/1775 train_time:80969ms step_avg:54.52ms
step:1486/1775 train_time:81056ms step_avg:54.55ms
step:1487/1775 train_time:81139ms step_avg:54.57ms
step:1488/1775 train_time:81225ms step_avg:54.59ms
step:1489/1775 train_time:81308ms step_avg:54.61ms
step:1490/1775 train_time:81395ms step_avg:54.63ms
step:1491/1775 train_time:81479ms step_avg:54.65ms
step:1492/1775 train_time:81565ms step_avg:54.67ms
step:1493/1775 train_time:81649ms step_avg:54.69ms
step:1494/1775 train_time:81736ms step_avg:54.71ms
step:1495/1775 train_time:81819ms step_avg:54.73ms
step:1496/1775 train_time:81905ms step_avg:54.75ms
step:1497/1775 train_time:81990ms step_avg:54.77ms
step:1498/1775 train_time:82076ms step_avg:54.79ms
step:1499/1775 train_time:82159ms step_avg:54.81ms
step:1500/1775 train_time:82244ms step_avg:54.83ms
step:1500/1775 val_loss:3.3778 train_time:82342ms step_avg:54.89ms
step:1501/1775 train_time:82363ms step_avg:54.87ms
step:1502/1775 train_time:82415ms step_avg:54.87ms
step:1503/1775 train_time:82502ms step_avg:54.89ms
step:1504/1775 train_time:82593ms step_avg:54.92ms
step:1505/1775 train_time:82678ms step_avg:54.94ms
step:1506/1775 train_time:82762ms step_avg:54.96ms
step:1507/1775 train_time:82847ms step_avg:54.97ms
step:1508/1775 train_time:82932ms step_avg:55.00ms
step:1509/1775 train_time:83016ms step_avg:55.01ms
step:1510/1775 train_time:83102ms step_avg:55.03ms
step:1511/1775 train_time:83185ms step_avg:55.05ms
step:1512/1775 train_time:83272ms step_avg:55.07ms
step:1513/1775 train_time:83356ms step_avg:55.09ms
step:1514/1775 train_time:83444ms step_avg:55.12ms
step:1515/1775 train_time:83531ms step_avg:55.14ms
step:1516/1775 train_time:83619ms step_avg:55.16ms
step:1517/1775 train_time:83703ms step_avg:55.18ms
step:1518/1775 train_time:83789ms step_avg:55.20ms
step:1519/1775 train_time:83871ms step_avg:55.21ms
step:1520/1775 train_time:83957ms step_avg:55.23ms
step:1521/1775 train_time:84040ms step_avg:55.25ms
step:1522/1775 train_time:84125ms step_avg:55.27ms
step:1523/1775 train_time:84208ms step_avg:55.29ms
step:1524/1775 train_time:84296ms step_avg:55.31ms
step:1525/1775 train_time:84381ms step_avg:55.33ms
step:1526/1775 train_time:84467ms step_avg:55.35ms
step:1527/1775 train_time:84552ms step_avg:55.37ms
step:1528/1775 train_time:84640ms step_avg:55.39ms
step:1529/1775 train_time:84723ms step_avg:55.41ms
step:1530/1775 train_time:84810ms step_avg:55.43ms
step:1531/1775 train_time:84892ms step_avg:55.45ms
step:1532/1775 train_time:84979ms step_avg:55.47ms
step:1533/1775 train_time:85061ms step_avg:55.49ms
step:1534/1775 train_time:85147ms step_avg:55.51ms
step:1535/1775 train_time:85231ms step_avg:55.52ms
step:1536/1775 train_time:85319ms step_avg:55.55ms
step:1537/1775 train_time:85403ms step_avg:55.56ms
step:1538/1775 train_time:85489ms step_avg:55.58ms
step:1539/1775 train_time:85573ms step_avg:55.60ms
step:1540/1775 train_time:85660ms step_avg:55.62ms
step:1541/1775 train_time:85743ms step_avg:55.64ms
step:1542/1775 train_time:85830ms step_avg:55.66ms
step:1543/1775 train_time:85913ms step_avg:55.68ms
step:1544/1775 train_time:85998ms step_avg:55.70ms
step:1545/1775 train_time:86082ms step_avg:55.72ms
step:1546/1775 train_time:86168ms step_avg:55.74ms
step:1547/1775 train_time:86251ms step_avg:55.75ms
step:1548/1775 train_time:86338ms step_avg:55.77ms
step:1549/1775 train_time:86423ms step_avg:55.79ms
step:1550/1775 train_time:86509ms step_avg:55.81ms
step:1551/1775 train_time:86593ms step_avg:55.83ms
step:1552/1775 train_time:86679ms step_avg:55.85ms
step:1553/1775 train_time:86762ms step_avg:55.87ms
step:1554/1775 train_time:86849ms step_avg:55.89ms
step:1555/1775 train_time:86932ms step_avg:55.90ms
step:1556/1775 train_time:87018ms step_avg:55.92ms
step:1557/1775 train_time:87102ms step_avg:55.94ms
step:1558/1775 train_time:87188ms step_avg:55.96ms
step:1559/1775 train_time:87271ms step_avg:55.98ms
step:1560/1775 train_time:87357ms step_avg:56.00ms
step:1561/1775 train_time:87442ms step_avg:56.02ms
step:1562/1775 train_time:87528ms step_avg:56.04ms
step:1563/1775 train_time:87613ms step_avg:56.05ms
step:1564/1775 train_time:87699ms step_avg:56.07ms
step:1565/1775 train_time:87782ms step_avg:56.09ms
step:1566/1775 train_time:87868ms step_avg:56.11ms
step:1567/1775 train_time:87951ms step_avg:56.13ms
step:1568/1775 train_time:88038ms step_avg:56.15ms
step:1569/1775 train_time:88121ms step_avg:56.16ms
step:1570/1775 train_time:88207ms step_avg:56.18ms
step:1571/1775 train_time:88290ms step_avg:56.20ms
step:1572/1775 train_time:88378ms step_avg:56.22ms
step:1573/1775 train_time:88462ms step_avg:56.24ms
step:1574/1775 train_time:88549ms step_avg:56.26ms
step:1575/1775 train_time:88631ms step_avg:56.27ms
step:1576/1775 train_time:88719ms step_avg:56.29ms
step:1577/1775 train_time:88802ms step_avg:56.31ms
step:1578/1775 train_time:88888ms step_avg:56.33ms
step:1579/1775 train_time:88971ms step_avg:56.35ms
step:1580/1775 train_time:89057ms step_avg:56.37ms
step:1581/1775 train_time:89140ms step_avg:56.38ms
step:1582/1775 train_time:89226ms step_avg:56.40ms
step:1583/1775 train_time:89311ms step_avg:56.42ms
step:1584/1775 train_time:89397ms step_avg:56.44ms
step:1585/1775 train_time:89481ms step_avg:56.46ms
step:1586/1775 train_time:89567ms step_avg:56.47ms
step:1587/1775 train_time:89650ms step_avg:56.49ms
step:1588/1775 train_time:89738ms step_avg:56.51ms
step:1589/1775 train_time:89820ms step_avg:56.53ms
step:1590/1775 train_time:89906ms step_avg:56.54ms
step:1591/1775 train_time:89990ms step_avg:56.56ms
step:1592/1775 train_time:90078ms step_avg:56.58ms
step:1593/1775 train_time:90160ms step_avg:56.60ms
step:1594/1775 train_time:90246ms step_avg:56.62ms
step:1595/1775 train_time:90330ms step_avg:56.63ms
step:1596/1775 train_time:90417ms step_avg:56.65ms
step:1597/1775 train_time:90502ms step_avg:56.67ms
step:1598/1775 train_time:90586ms step_avg:56.69ms
step:1599/1775 train_time:90671ms step_avg:56.70ms
step:1600/1775 train_time:90757ms step_avg:56.72ms
step:1601/1775 train_time:90840ms step_avg:56.74ms
step:1602/1775 train_time:90925ms step_avg:56.76ms
step:1603/1775 train_time:91010ms step_avg:56.77ms
step:1604/1775 train_time:91096ms step_avg:56.79ms
step:1605/1775 train_time:91180ms step_avg:56.81ms
step:1606/1775 train_time:91265ms step_avg:56.83ms
step:1607/1775 train_time:91349ms step_avg:56.84ms
step:1608/1775 train_time:91436ms step_avg:56.86ms
step:1609/1775 train_time:91520ms step_avg:56.88ms
step:1610/1775 train_time:91605ms step_avg:56.90ms
step:1611/1775 train_time:91689ms step_avg:56.91ms
step:1612/1775 train_time:91776ms step_avg:56.93ms
step:1613/1775 train_time:91859ms step_avg:56.95ms
step:1614/1775 train_time:91945ms step_avg:56.97ms
step:1615/1775 train_time:92029ms step_avg:56.98ms
step:1616/1775 train_time:92115ms step_avg:57.00ms
step:1617/1775 train_time:92200ms step_avg:57.02ms
step:1618/1775 train_time:92285ms step_avg:57.04ms
step:1619/1775 train_time:92368ms step_avg:57.05ms
step:1620/1775 train_time:92455ms step_avg:57.07ms
step:1621/1775 train_time:92540ms step_avg:57.09ms
step:1622/1775 train_time:92626ms step_avg:57.11ms
step:1623/1775 train_time:92710ms step_avg:57.12ms
step:1624/1775 train_time:92795ms step_avg:57.14ms
step:1625/1775 train_time:92880ms step_avg:57.16ms
step:1626/1775 train_time:92965ms step_avg:57.17ms
step:1627/1775 train_time:93048ms step_avg:57.19ms
step:1628/1775 train_time:93134ms step_avg:57.21ms
step:1629/1775 train_time:93220ms step_avg:57.23ms
step:1630/1775 train_time:93306ms step_avg:57.24ms
step:1631/1775 train_time:93390ms step_avg:57.26ms
step:1632/1775 train_time:93478ms step_avg:57.28ms
step:1633/1775 train_time:93561ms step_avg:57.29ms
step:1634/1775 train_time:93648ms step_avg:57.31ms
step:1635/1775 train_time:93731ms step_avg:57.33ms
step:1636/1775 train_time:93817ms step_avg:57.35ms
step:1637/1775 train_time:93901ms step_avg:57.36ms
step:1638/1775 train_time:93987ms step_avg:57.38ms
step:1639/1775 train_time:94071ms step_avg:57.40ms
step:1640/1775 train_time:94157ms step_avg:57.41ms
step:1641/1775 train_time:94241ms step_avg:57.43ms
step:1642/1775 train_time:94328ms step_avg:57.45ms
step:1643/1775 train_time:94412ms step_avg:57.46ms
step:1644/1775 train_time:94498ms step_avg:57.48ms
step:1645/1775 train_time:94582ms step_avg:57.50ms
step:1646/1775 train_time:94668ms step_avg:57.51ms
step:1647/1775 train_time:94751ms step_avg:57.53ms
step:1648/1775 train_time:94838ms step_avg:57.55ms
step:1649/1775 train_time:94922ms step_avg:57.56ms
step:1650/1775 train_time:95008ms step_avg:57.58ms
step:1651/1775 train_time:95092ms step_avg:57.60ms
step:1652/1775 train_time:95179ms step_avg:57.61ms
step:1653/1775 train_time:95262ms step_avg:57.63ms
step:1654/1775 train_time:95348ms step_avg:57.65ms
step:1655/1775 train_time:95432ms step_avg:57.66ms
step:1656/1775 train_time:95519ms step_avg:57.68ms
step:1657/1775 train_time:95603ms step_avg:57.70ms
step:1658/1775 train_time:95689ms step_avg:57.71ms
step:1659/1775 train_time:95773ms step_avg:57.73ms
step:1660/1775 train_time:95859ms step_avg:57.75ms
step:1661/1775 train_time:95942ms step_avg:57.76ms
step:1662/1775 train_time:96029ms step_avg:57.78ms
step:1663/1775 train_time:96112ms step_avg:57.79ms
step:1664/1775 train_time:96199ms step_avg:57.81ms
step:1665/1775 train_time:96282ms step_avg:57.83ms
step:1666/1775 train_time:96369ms step_avg:57.84ms
step:1667/1775 train_time:96453ms step_avg:57.86ms
step:1668/1775 train_time:96540ms step_avg:57.88ms
step:1669/1775 train_time:96623ms step_avg:57.89ms
step:1670/1775 train_time:96708ms step_avg:57.91ms
step:1671/1775 train_time:96792ms step_avg:57.92ms
step:1672/1775 train_time:96879ms step_avg:57.94ms
step:1673/1775 train_time:96962ms step_avg:57.96ms
step:1674/1775 train_time:97049ms step_avg:57.97ms
step:1675/1775 train_time:97133ms step_avg:57.99ms
step:1676/1775 train_time:97219ms step_avg:58.01ms
step:1677/1775 train_time:97303ms step_avg:58.02ms
step:1678/1775 train_time:97389ms step_avg:58.04ms
step:1679/1775 train_time:97473ms step_avg:58.05ms
step:1680/1775 train_time:97559ms step_avg:58.07ms
step:1681/1775 train_time:97642ms step_avg:58.09ms
step:1682/1775 train_time:97728ms step_avg:58.10ms
step:1683/1775 train_time:97812ms step_avg:58.12ms
step:1684/1775 train_time:97899ms step_avg:58.13ms
step:1685/1775 train_time:97983ms step_avg:58.15ms
step:1686/1775 train_time:98068ms step_avg:58.17ms
step:1687/1775 train_time:98151ms step_avg:58.18ms
step:1688/1775 train_time:98237ms step_avg:58.20ms
step:1689/1775 train_time:98321ms step_avg:58.21ms
step:1690/1775 train_time:98407ms step_avg:58.23ms
step:1691/1775 train_time:98490ms step_avg:58.24ms
step:1692/1775 train_time:98577ms step_avg:58.26ms
step:1693/1775 train_time:98661ms step_avg:58.28ms
step:1694/1775 train_time:98748ms step_avg:58.29ms
step:1695/1775 train_time:98831ms step_avg:58.31ms
step:1696/1775 train_time:98919ms step_avg:58.32ms
step:1697/1775 train_time:99002ms step_avg:58.34ms
step:1698/1775 train_time:99088ms step_avg:58.36ms
step:1699/1775 train_time:99171ms step_avg:58.37ms
step:1700/1775 train_time:99257ms step_avg:58.39ms
step:1701/1775 train_time:99341ms step_avg:58.40ms
step:1702/1775 train_time:99427ms step_avg:58.42ms
step:1703/1775 train_time:99512ms step_avg:58.43ms
step:1704/1775 train_time:99598ms step_avg:58.45ms
step:1705/1775 train_time:99683ms step_avg:58.46ms
step:1706/1775 train_time:99768ms step_avg:58.48ms
step:1707/1775 train_time:99852ms step_avg:58.50ms
step:1708/1775 train_time:99939ms step_avg:58.51ms
step:1709/1775 train_time:100021ms step_avg:58.53ms
step:1710/1775 train_time:100107ms step_avg:58.54ms
step:1711/1775 train_time:100191ms step_avg:58.56ms
step:1712/1775 train_time:100278ms step_avg:58.57ms
step:1713/1775 train_time:100361ms step_avg:58.59ms
step:1714/1775 train_time:100448ms step_avg:58.60ms
step:1715/1775 train_time:100533ms step_avg:58.62ms
step:1716/1775 train_time:100620ms step_avg:58.64ms
step:1717/1775 train_time:100703ms step_avg:58.65ms
step:1718/1775 train_time:100789ms step_avg:58.67ms
step:1719/1775 train_time:100872ms step_avg:58.68ms
step:1720/1775 train_time:100958ms step_avg:58.70ms
step:1721/1775 train_time:101041ms step_avg:58.71ms
step:1722/1775 train_time:101127ms step_avg:58.73ms
step:1723/1775 train_time:101212ms step_avg:58.74ms
step:1724/1775 train_time:101298ms step_avg:58.76ms
step:1725/1775 train_time:101382ms step_avg:58.77ms
step:1726/1775 train_time:101468ms step_avg:58.79ms
step:1727/1775 train_time:101551ms step_avg:58.80ms
step:1728/1775 train_time:101639ms step_avg:58.82ms
step:1729/1775 train_time:101722ms step_avg:58.83ms
step:1730/1775 train_time:101807ms step_avg:58.85ms
step:1731/1775 train_time:101891ms step_avg:58.86ms
step:1732/1775 train_time:101978ms step_avg:58.88ms
step:1733/1775 train_time:102061ms step_avg:58.89ms
step:1734/1775 train_time:102148ms step_avg:58.91ms
step:1735/1775 train_time:102230ms step_avg:58.92ms
step:1736/1775 train_time:102321ms step_avg:58.94ms
step:1737/1775 train_time:102405ms step_avg:58.96ms
step:1738/1775 train_time:102493ms step_avg:58.97ms
step:1739/1775 train_time:102579ms step_avg:58.99ms
step:1740/1775 train_time:102664ms step_avg:59.00ms
step:1741/1775 train_time:102749ms step_avg:59.02ms
step:1742/1775 train_time:102835ms step_avg:59.03ms
step:1743/1775 train_time:102920ms step_avg:59.05ms
step:1744/1775 train_time:103006ms step_avg:59.06ms
step:1745/1775 train_time:103090ms step_avg:59.08ms
step:1746/1775 train_time:103176ms step_avg:59.09ms
step:1747/1775 train_time:103261ms step_avg:59.11ms
step:1748/1775 train_time:103349ms step_avg:59.12ms
step:1749/1775 train_time:103432ms step_avg:59.14ms
step:1750/1775 train_time:103519ms step_avg:59.15ms
step:1750/1775 val_loss:3.2864 train_time:103617ms step_avg:59.21ms
step:1751/1775 train_time:103637ms step_avg:59.19ms
step:1752/1775 train_time:103689ms step_avg:59.18ms
step:1753/1775 train_time:103777ms step_avg:59.20ms
step:1754/1775 train_time:103865ms step_avg:59.22ms
step:1755/1775 train_time:103950ms step_avg:59.23ms
step:1756/1775 train_time:104035ms step_avg:59.25ms
step:1757/1775 train_time:104118ms step_avg:59.26ms
step:1758/1775 train_time:104203ms step_avg:59.27ms
step:1759/1775 train_time:104286ms step_avg:59.29ms
step:1760/1775 train_time:104373ms step_avg:59.30ms
step:1761/1775 train_time:104456ms step_avg:59.32ms
step:1762/1775 train_time:104545ms step_avg:59.33ms
step:1763/1775 train_time:104632ms step_avg:59.35ms
step:1764/1775 train_time:104721ms step_avg:59.37ms
step:1765/1775 train_time:104806ms step_avg:59.38ms
step:1766/1775 train_time:104893ms step_avg:59.40ms
step:1767/1775 train_time:104977ms step_avg:59.41ms
step:1768/1775 train_time:105062ms step_avg:59.42ms
step:1769/1775 train_time:105145ms step_avg:59.44ms
step:1770/1775 train_time:105232ms step_avg:59.45ms
step:1771/1775 train_time:105314ms step_avg:59.47ms
step:1772/1775 train_time:105401ms step_avg:59.48ms
step:1773/1775 train_time:105485ms step_avg:59.50ms
step:1774/1775 train_time:105572ms step_avg:59.51ms
step:1775/1775 train_time:105658ms step_avg:59.53ms
step:1775/1775 val_loss:3.2798 train_time:105758ms step_avg:59.58ms
peak memory allocated: 29148 MiB reserved: 44838 MiB
